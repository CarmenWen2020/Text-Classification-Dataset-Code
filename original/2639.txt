Graph classification problem is becoming one of research hotspots in the realm of graph mining, which has been widely used in cheminformatics, bioinformatics and social network analytics. Existing approaches, such as graph kernel methods and graph Convolutional Neural Network, are facing the challenges of non-interpretability and high dimensionality. To address the problems, we propose a novel recurrent attention model, called g-Inspector, which applies the attention mechanism to investigate the significance of each region to make the results interpretable. It also takes a shift operation to guide the inspector agent to discover the next relevant region, so that the model sequentially loads small regions instead of the entire large graph, to solve the high dimensionality problem. The experiments conducted on standard graph datasets show the effectiveness of our g-Inspector in graph classification problems.
SECTION 1Introduction
As a prevalent data structure, graph has been widely used to model key features and complex relationships for complicated objects, such as molecular graph structures, biological protein-protein interaction networks and social networks [1]. Many complex problems in the areas of cheminformatics, bioinformatics and social network analytics can be formulated as graph problems and solved by leveraging graph mining techniques [2]. In this field, graph classification has been a main research branch and a hot topic as well, which assigns labels to graphs [3].

Graph classification is of highly practical significance in a wide variety of real-world applications, such as drug activity predictions, toxicology tests, and categorizing scientific publications [4], [5], [6]. For example, in molecular medicine, representing a chemical compound as a graph can help quickly judge whether the compound has the inhibiting effect on a specific cancer cell, which can significantly reduce time, efforts and excessive resource on drug testing [4], [7].

Existing approaches for graph classification can be roughly divided into three categories. The first category is subgraph methods, which classifies graph object by subgraph patterns [3], [8]. The second category is graph kernel which leverages kernel methods to compute the similarity among graphs [9], [10]. The similarity is computed based on the entire graph, and all the subgraph structures in the graphs were treated fairly. It is ambiguous how each structure contributes to the classification. The third category is graph neural networks (GNNs), which applies deep learning based methods on graph domain [11], [12], [13], [14], [15], [16]. It is hard to know the significance of each feature for the classification, since the features are automatically integrated through deep neural networks. Taken together, the methods in both of the categories cannot decide which structure is important to the graph classification, and thereby it is difficult to interpret the results. Consequently, it is hard to retrieve valuable knowledge from graphs to support many practical applications. Meanwhile, there is no clear boundary among multiple different classes.

Graph objects can be with up to billions of vertices and edges in practical applications, such as various social networks. Not all the subgraph structures are helpful for classification. It is natural to select the relevant structures and ignore the irrelevant ones so as to make results interpretable. Motivated by the idea, we introduce an attention mechanism to select a series of task relevant regions to pay attention to. In this task, a region refers to a neighborhood subgraph structure of a given central vertex.

Attention has become an essential part of deep learning models and achieved great success in various tasks, e.g., machine translation [17], image classification [18], caption generation [19] and speech recognition [20]. The attention mechanism helps to reduce the number of parameters, and improve computational efficiency [21]. Moreover, adopting the attention mechanism make results interpretable [19]. One can understand the process of the model by visualizing the regions where the attention is paid to.

Unfortunately, the attention mechanism cannot be directly applied for graph classification. In the aforementioned work, the data is well-structured and with determined and low dimensionality, such as image, text or video. However, the size of different graphs is variational, and a space with a high dimensionality equal to that of the largest graph is required to represent different graphs.

In this paper, to address the problem, we propose a novel recurrent attention model, called g-Inspector, where we introduce an inspector module applying the attention to investigate the significance of each region to classification. We also design a shift operation to guide the inspector to discover the next relevant region under the rule of reinforcement learning. By the recurrent process consisting of attention inspection and shift operation, we sequentially load small relevant regions instead of the entire graph, to avoid the high dimensionality problem. The main contributions of this paper are as follows:

The recurrent attention model, g-Inspector, overcomes the challenge of high dimensionality problem. Moreover, it is interpretable owing to the ability of measuring the contribution of each region to the classification.

We introduce a shift operation across the graph which works under the reinforcement learning rule. It adaptively selects a series of task relevant regions instead of searching the entire graph, and significantly prunes the search space.

The experiments conducted on public graph datasets verify the accuracy, scalability and parameter selection of the model.

To better illustrate our work, we organize the remaining part of this paper as follows. Section 2 briefly reviews the related researches from literature and summarizes their advantages and disadvantages in solving graph classification problem. Section 3, formalizes this problem, introduces the g-Inspector model and its training method. Section 4 reports our experimental result on open datasets. The conclusion and future work is discussed in Section 5.

SECTION 2Related Work
In this section, we briefly review the literature on graph classification, which can be roughly divided into three categories: subgraph methods, graph kernels and GNNs. Besides, we take a quick look at the research studies on attention mechanism and its application on graphs.

As the first category, subgraph methods leverage subgraph patterns for graph classification. For example, gSpan explored depth-first search to mine frequent subgraphs [22]. gSSC performed semi-supervised feature selection for graphs in a progressive way together with the subgraph feature mining process [4]. Pan et al. [3] introduced a boosting method, especially designed fro cost-sensitive classification. An incremental subgraph feature selecting technique is proposed in [8]. The major drawback of these subgraph methods is the limitation on scalability. The high-dimension challenge comes with the growth of graph size has not been explored well and has high computational complexity.

The second category on graph classification is graph kernel based methods. The original purpose of the kernels on graphs was to measure the similarity of nodes on a single graph [23]. Graph kernels allow kernel-based learning approaches such as SVMs to work directly on graphs. Some variants have considered that the similarity can be measured between different graphs [24], subgraphs or certain graphlets [10]. However, the features extracted from graphs that are used to compute the kernel matrix are not independent [9], and all graph kernels have a training complexity at least quadratic in the number of graphs which is infeasible for large-scale problems [25].

In the third category, graph neural networks extend existing neural network methods to work with graph-structured data. Diffusion CNNs [13] learned diffusion-based representations from graph-structured data by scanning a diffusion process across each node. NgramCNN [12] studied the Ngram on a graph adjacency matrix and diagonal convolution to extract subgraph features. PSCN [11] constructed the receptive fields by extracting locally connected regions from graphs. DGCNN [26] proposed an end-to-end deep learning architecture with a spatial graph convolution layer and a SortPooling layer. DiffPool [27] is a differentiable graph pooling module which can be combined with existing GNNs to extract the hierarchical representations of graphs. GNN is computationally expensive because it required many iterations between gradient descent steps, and the networks behavior is hard to explain.

Some studies have tried to apply the attention mechanism on graphs for graph representing [28], [29]. Choi et al. proposed GRAM [28], an attention model for healthcare representation learning, which is based on medical ontology and only applicable on directed acyclic graphs. Petar et al. introduced GATS [29], graph attention networks which focused on node classification of graph-structured data by using attention to compute the hidden representation of each node. Lee et al. proposed MCN [30], a motif-based graph attention model for node classification task, which generalizes GCNs by using weighted multi-hop motif-induced adjacencies to capture higher-order neighborhoods. Compared with these studies, we use an attention mechanism for graph classification instead of using graph representation directly. Specifically, our model uses an inspector module to select attention regions instead of learning attention weights.

We have noticed the publication of another research applying attention on graphs, GAM [31], which is independent and simultaneous with ours. Coincidentally, both their proposed model GAM and our work are inspired by the recent work on the visual attention-based model, RAM [18]. However, GAM is significantly different from ours, and the main differences are as follows: (1) Our g-Inspector has better scalability. It integrates graph structure feature, vertex/edge attributes at the same time, while GAM can only handle the vertex attributes. (2) At each time step, our g-Inspector extracts a subgraph instead of visiting a single node in GAM. More subgraph information can be collected and inferred, which makes it possible for the inspector to gain more information about the graph and make better decisions. (3) In GAM, the step module takes a step from the current vertex to one of its one-hop neighborhood. Our g-Inspector takes the shift operation to expand the scope of observation rather than just a small portion of the graph.

SECTION 3Proposed Method
3.1 Problem Definition
A graph G is defined by a tuple (V,E,fv,fe,Sv,Se) where V={vi}|V|i=1 is the set of vertices and E⊆V×V is the set of edges. An edge in E connecting vertex vi with vertex vj can be denoted as (vi,vj), where vi,vj∈V. Sv is the set of vertex-label, or called node attribute, and Se the set of edge-label, or called edge attribute, respectively. fv:V→Sv is the assignment from a vertex to a vertex-label. Similarly, fe:E→Se is the assignment mapping from an edge to an edge-label. Both Sv and Se can either be qualitative attribute or quantitative attribute. Consider that by modeling a social network as a graph, the vertex represents a person, and the edge represents the friendship relation. The person's gender, age and preference are the vertex-labels. A qualitative attribute is the gender, whose Sv = {Male, Female, Others}. A quantitative attribute is the age, whose Sv is the integer number>0. In this way, our problem framework covers both the qualitative and quantitative attributes.

With graph and basic elements formally defined, we can describe the following classification problem.

Definition 1 (Graph Classification).
Given a collection of graphs G={G1,…,Gn}, let L={l1,…,lK} denote the set of K class labels, learn an assigning function f:G→L that maps a graph to one class label in L.

Graph classification tasks are associated with the whole graph, which aim to estimate the label of entire graph object. Like other classification problems, the key is to extract the features of input, namely the graph, and then quantify their contributions to the class labels. The challenges come from three aspects.

Feature Complexity Challenge It is difficult to simultaneously cover two different kinds of graph features, one of which is the connection information from subgraph structures, and another the vertex/edge attributes. For example, GAM [31] can only handle the graphs with attributes. Our idea is to encode the attributes by merging vertex-labels, edge-labels and the graph embedding vectors together.

High Dimensionality Challenge The size of a graph object can be very large. Loading the entire graph cause the high dimensionality problem. Our idea is to sequentially load small relevant regions of the graph via an inspection network.

Interpretable Ability Challenge Most existing methods cannot interpret the results. In g-Inspector, we introduce an attention mechanism to measure the contribution of each region, making the results interpretable.

Facing these challenges, we combine together these possible ideas to build a complete model, called g-Inspector.

3.2 Method Overview
The overview of our is shown in Fig. 1, which consists of four parts. (1) Encoder of graphs to embed the graph structure and vertex/edge-label. (2) Inspection network, containing the inspector to capture a part of the graph, and taking shift operation to efficiently shift the inspector on the graph. (3) Core network to keep the memory from previously observed subgraph features and merge the newly captured information by a recurrent cell. (4) Action network to guide the shift of the inspector and predict the classification label. These four parts are detailed illustrated one-by-one in the rest of this section.

Fig. 1. - 
Method overview.
Fig. 1.
Method overview.

Show All

3.3 Encoder
To overcome the feature complexity challenge, we leverage an encoder to merge the structural information with the attributes of vertexes and edges. For a graph G, each vertex v is encoded as a vector M(v) consisting of three parts.

ψ(v;G) is the embedding vector of v in the graph G.

ϕ(fv(v);θvϕ) is the encoding of the vertex attribute parameterized by θvϕ.

E(vi,v)[ϕ(fe((vi,v));θeϕ)] is the expectation on encoding of the edge attribute parameterized by θvϕ. (vi,v) denotes any edge connecting v, and whose encoding is ϕ(fe((vi,v));θeϕ.

In summary, given a vertex v in graph G, M(v) represents the vertex feature with both structural information and attributes considered. Without considering the attributes of vertex or edge, we only take the structure of graph into account in practice. For the implementation, we employ DeepWalk as ψ, which can encode the relationship between the vertices in a continuous vector space with a relatively low dimensionality [32]. Other embedding algorithms, e.g., node2vec could be a good choice. The slightly promotion or demotion of accuracy caused by replacing different embedding algorithm such as replacing Deepwalk by node2vec is not our major technical contribution.

3.4 Inspection Network
The inspection network is the essential component of our method, which observes a task relevant region of graph and shifts its central vertex under the instructions from the action network. A region refers to a neighborhood subgraph structure of a given central vertex.

3.4.1 Inspector
Given the whole graph G, the inspector collects the subgraph features from a region determined by a central vertex v. To specify the region of v, we leverage the concept of neighborhood.

Definition 2 (k-hop neighborhood).
∀v∈V, let Nk(v) denote the k-hop neighborhood of vertex v. Nk(v) can be defined in a recursive form
Nk(v)=⎧⎩⎨⎪⎪{v}{u|u∈(V−⋃k−1iNi(v))∧∃v′∈Nk−1(v),(u,v′)∈E} if k=0ow.(1)
View Source

With the help of neighborhood Nk(v), we can describe the region around v from variant granularity.

Definition 3 (k-order region).
For a vertex v in graph G, its k-order region is the vertex set Rk(v) which is also recursively defined
Rk(v)={N0(v)Nk(v)⋃Rk−1(v) if k=0ow.(2)
View Source

For the smallest granularity, the region has only one vertex, namely v itself.

To take use of these regions, given the central vertex v, the inspector mixes the vertex features from its k-order region in two ways: The first and straight-forward way is the stacked k-order region which take average of all regions whose order is less than k.
ρ(v;G)=∑i=1kEv′∈Ri(v)[M(v′)].(3)
View SourceRight-click on figure for MathML and additional features.The second way is introducing the decay factor γρ∈(0,1) as the stacked decay k-order region and give the i-order region with weight γi−1ρ.
ρ(v;G)=∑i=1γi−1ρEv′∈Ri(v)[M(v′)].(4)
View SourceIn practice, the former way is easy to implement by setting k as a constant but hard to generalize on different graph datasets. Therefore, by default, we take use of Eq. (4) and stop summation at n-order region if γnρ is smaller than a constant threshold.

In summary, ρ(v;G) represents the mixed features from all observing regions around vertex v by inspector.

3.4.2 Shift on Graph
The inspector takes only the features from the region around v. In order to observe more information, we can shift the inspector on the graph. The procedure is elaborated upon below. Given the entire graph G, consider that at the time step t, the inspector stays at vt, and then ρ(vt;G) is observed. At the next step t+1, the inspector shifts from vt to vt+1 under the instruction dt, which is generated from action network and will be discussed later. At vt+1, ρ(vt+1;G) is observed. To explain how to get vt+1 from vt in detail, we introduce a distance metric at the vertex.

Definition 4 (Distance between Vertices).
Given a graph G, the distance rvi,vj between vi∈V and vj∈V is the euclidean distance ∥M(vi)−M(vj)∥2 on feature matrix.

Based on distance metric, we prepare a ranking structure D representing the relatively distance between each two vertices. for each vertex vi, we rank all vertices in a descending order in terms of the distance from vi. We use notation D(vi,j) to represent the vertex having jth smallest distance from vi. It ensures that
∀vi∈V,k∈1,…,|V|,rvi,D(vi,k)≥rvi,D(vi,k−1).(5)
View SourceRight-click on figure for MathML and additional features.Note that D is independent with the inspector and can be computed off-line. With the help of D, we can localize any vertex by describing the order and location with respect to another vertex. For example, consider the graph in Fig. 2a, we can localize the green vertex, by pointing out the relative position with respect to the red one (v1), namely D(v1,3). Based on this mechanism, we can define the atomic shift operation on an integer instruction d.


Fig. 2.
(a) Atomic Shift Operation: Given a graph G, the start vertex vi and a instruction d=3. The inspector shifts to the green vertex with the 3rd smallest distance, i.e., D(vi,3)=vj. (b) Serial Shift Operation: A serial shift operation dt−1 with m-dimensionality, consists of m atomic shift operations.

Show All

Definition 5 (Atomic Shift).
Given the central vt of the inspector at the time step t, the next observation central vt+1=D(vt,d) is determined by instruction d.

To improve the efficiency, we consider the serial shift at one step. For the serial shift, the instruction d={di}mi=1 is a vector, used to guide the shift of the inspector, where each dimension is a scalar corresponding to an atomic operation.

At each time step t, the initial position of the inspector is at the previous central vertex vt−1, according to the instruction vector dt−1={d1t−1,…,dmt−1}. The inspector takes the first shift operation from the current vertex vt−1 to an intermediate vertex calculated by D(vt−1,d1t−1), then uses this intermediate vertex as the start of the second atomic shift operation to get the next start vertex according to d2t−1. The rest can be repeated in the same manner. After m times of atomic shift operations, the inspector moves from the original start node vt−1 to vt, see Fig. 2b.

Definition 6 (Serial Shift).
Given the central vertex vt−1 of the inspector at time step t−1 and instruction dt−1, the next observation central vertex vt=v^m where
v^i={vt−1D(v^i−1,dit−1) if i=1ow.(6)
View Source

The detailed procedure is as follows.

At each time step t, the initial position of the inspector is at the previous central vertex vt−1. And then the inspector receives the instruction vector dt−1={d1t−1,…,dmt−1}. Setting the intermediate vertex v^1=vt−1 and repeating step 2 by taking iterator i∈{1,…,m}.

Taking an atomic shift operation on v^i−1 under instruction dit−1 and setting the result as the intermediate vertex v^i=D(v^i−1,dit−1).

When i=m, setting vt=v^m.

For a large graph object, the serial shift is more efficient, which is studied in the experimental part. In the following discussion, we use vt=fj(vt−1,dt−1;G) to represent the shift procedure.

The structure of the inspection network, shown in Fig. 3, which integrates two features as a feature vector at by a two-layer neural network. The first feature is the vertex feature ρ(vt;G) derived from the region by the inspector. The second feature is the instruction dt. The inspection network, illustrated in Fig. 3, is formalized as follows
zdzσat=σ(dt−1;θ0a)=σ(ρ(vt;G);θ1a)=σ([zd,zσ];θ2a),(7)
View SourceRight-click on figure for MathML and additional features.where zσ and zd are intermediate variables, θa={θ0a,θ1a,θ2a} is the network parameter, [⋅,⋅] is the vector concatenation operation and σ(⋅) is the activation function.

Fig. 3. - 
Inspection network.
Fig. 3.
Inspection network.

Show All

In summary, the inspector module leverages the shift operation, especially serial shift operation, to observe a serial of regions of original graph under the instructions and compute the features at.

3.4.3 Inspector Initialization
In the inspector module, despite the trainable parameters θa, the output features at is impacted by two factors: the instructions and the initial vector v0. The initial vector, representing the inspector center v at time temp t=0, determines which region is observed at first.

For each input graph, three methods available can be used to initialize the inspector center v0. The simplest one is to randomly select a vertex from the input graph. An ideal method is to choose v0 from the dense area of the input graph. Also, we can also get a vertex v0 under the expert's guidance.

Our g-Inspector is not sensitive to the initial value. Each vertex and its neighbor region has an implicit contribution score to classification label. If initializing the start at the vertex with high score, the model can easily achieve good accuracy. If at the vertex with low score, our g-Inspector will shift quickly leaving current vertex by the shift instruction vector with a large value. And this policy is well trained by reinforcement rule in training phase.

3.5 Core Network
The core network in the g-Inspector model, is a recurrent network maintaining a hidden state to keep the memory from previously observed subgraph features. In the model, the number of hidden layers, denoted by T, indicates that the inspector has observed the graph for T times.

The hidden state, initialized randomly and integrates its previous memory ht−1 with the feature at obtained by the inspection network. Formally speaking, ht=fh(ht−1,at;θh) where θh is the neural network parameter. The hidden state explicitly encodes the inspector's knowledge of the graph, such as subgraph structures and attributes.

3.6 Action Networks
The hidden state ht contains all information that the inspector has collected from previous observations. Two actions are executed based on the hidden state at each step:

Generating the instruction dt=fd(ht;θd) by the shift network, determining where the inspector will explore in the next step.

Predicting the label l^t of the input graph by the classification network which applies softmax function.
Pr(l^t=i)=expfc(ht;θc)i∑jexpfc(ht;θc)j.(8)
View SourceRight-click on figure for MathML and additional features.

Besides, the action network provides guidance as to the time point to stop observing the graph when the inspector has collected sufficient information to make the accurate prediction.

From the perspective of reinforcement learning, the inspector is an agent interacting with the environment, i.e., the input graph, via a Partially Observable Markov Decision Process (POMDP). At time step t, the inspector, collects the partially observation at (see Eq. (7)), receives the reward signal rt from the environment, and makes two actions: generating the shift instruction dt and predicting label l^t. Given the state, actions generated from actions networks are implicitly determined by the policy π((dt,l^t)|s1:t;θd,θc) where s1:t is the memory on history s1:t=a1,d1,l^1,…,at−1,dt−1,l^t−1,at, which can be replaced by the hidden state ht. The parameters of policy are estimated by optimizing the accumulative reward R=∑Tt=1rt. In our graph classification task, when the classification result is correct, let rT=1, otherwise let rT=0.

Taking the g-Inspector as a whole agent, the entire policy, denoted by π((dt,l^t,ht)|ρ(v,G),ht−1;Θ) is determined by Θ={θh,θa,θd,θc}. Then, the total reward J(Θ)=Ep(s1:T;Θ)[R] is also the function of Θ.

Theorem 3.1 (Sufficient Condition on Classification).
Given the graph G, the optimal Θ which maximizes J(Θ) is the optimal classifying mapping by assigning f(G;Θ)=l^T.

The Theorem 3.1 shows that optimal solution Θ=argΘmaxJ(Θ) is the sufficient condition for the optimal graph classification.

Proof.
Assume that exists another Θ′ and a graph G whose correct label is l, such that π(l^T=l|G;Θ′)>π(l^T=l|G;Θ). Namely, the expectation Ep(s1:T;Θ′)[R]>Ep(s1:T;Θ)[R]. According to the definition of J, J(Θ′)>J(Θ) which is against with Θ=argΘmaxJ(Θ). QED

With the Theorem 3.1, we can solve argΘmaxJ(Θ) to archive the solution of proposed problem in Section 3.1.

3.7 Training
In this section, we will introduce the training algorithm of our model. As mentioned above, the inspector needs to learn a policy by tuning the parameters Θ={θh,θa,θd,θc} in the conjunction of the core network, the inspection network and the action networks. Since each policy of the inspector induces a distribution of the possible interaction sequences s1:T, we want to maximize the total reward under this distribution: J(Θ)=Ep(s1:T;Θ)[R], where p(s1:T;Θ) depends on the policy representing the occurrence probability of the interaction sequence s1:T.

Since the interaction sequences may be very large and difficult to exhaustive calculate, it is a non-trivial task to maximize J. Fortunately, by considering the problem as a POMDP, we use the technique introduced in [33], to obtain a sample approximation of the gradient of J as shown in [18], given by
∇ΘJ≈1MJ∑i=1MJ∑t=1T−1∇Θlogπ(dit,l^it|si1:t;Θ)γT−tJRi,(9)
View SourceRight-click on figure for MathML and additional features.where si1:t is the interaction sequence generated by the inspector according to the current policy for i=1,…,MJ episodes, and γJ∈(0,1) is a discount factor. The Equation (9) is also known as the REINFORCE rule [34], which obtains samples of interaction sequences s1:T by the inspector with current policy. Then, we adjust the parameters Θ of the inspector. It increases the log-probability of the chosen actions that result in a correct prediction and decreases the log-probability of low rewards corresponding actions. The policy trained in this way allows the agent to increase the chance to shift the inspector towards a relevant region the next time when facing the same state. To compute ∇Θlogπ(dit,l^it|si1:t;Θ), we deal with the gradient of the network at each time step which can be computed by back-propagation [35]. In particular, our inspector observes the graph in the first T−1 step, therefore, we adjust the log-probability for t=1,…,T−1.

To reduce the variance, we add a baseline b to obtain an optimized ∇ΘJ which is equal to the Equation (9) in expectation but has lower variance [18]
1M∑i=1M∑t=1T−1∇Θlogπ(dit,l^it|si1:t;Θ)(γT−tRi−bit),(10)
View SourceRight-click on figure for MathML and additional features.where bit=fb(si1:t;θb) to compute the cumulative reward depend on si1:t(hit). Training the parameters according to Equation (10), allows us to increase the log-probability of the chosen actions that result in greater cumulative reward, and decrease the log-probability of the actions with smaller obtained cumulative than that of baseline. The parameter θb of fb is trained by reducing the mean squared error of Ri−bit.

SECTION 4Experiment
This section reports the experimental results to show the advantages of our g-Inspector. We compare the performance of our model with other approaches. Also, we examine an example to illustrate the effectiveness of the attention mechanism in graph classification problem, and discuss the scalability and interpretabe ability of our model. At last the source code of g-Inspector is open-sourced.1

4.1 Experimental Setup
4.1.1 Datasets
We evaluate our model on three bioinformatics datasets: MUTAG, NCI1, ENZYMES, and two social network datasets: IMDB-BINARY and IMDB-MULTI. The MUTAG dataset consists of 188 chemical compounds divided into 2 classes according to their mutagenic effect on a bacterium. The NCI1 dataset was published by National Cancer Institute(NCI), which includes 4,110 chemical compounds and is divided into 2 classes on the activation against non-small cell lung cancer. ENZYMES has 600 protein tertiary structures, each of which belongs to one of the 6 classes. IMDB-BINARY and IMDB-MULTI are movie collaboration datasets collected from IMDB, where nodes represent actors/actresses and the edges connect the actors/actresses who appear in the same movie.

4.1.2 Compared Methods
We compare g-Inspector against the following representative approaches: Random Walk kernel(RW) [36], Graphlet kernel (GK) [10], Deep Graph Kernel(DGK)[9] (Code from [37]), DCNN [13] (Code from [38]), DGCNN [26], PSCN [11], GAM and GAM-mem [31]. Note that the codes for GAM and GAM-mem are neither released on the Internet nor accessible from the authors, we refer to the performance results reported in [31].

4.1.3 Implementation
Our g-Inspector is implemented by Python 2.7 and TensorFlow 1.4.1, and executed on the server with 32 GB memory, 2.4 GHz Intel CPU. In all experiments, the following setting are the same. We use DeepWalk [32] to represent each node as a 64-dimensionality vector. In the inspection network, we select a random node in the input graph to initialize the start central vertex v0 and the shift instruction d0 is initialized by a uniform distribution. Both the feature vector a and the hidden state are with the same dimensionality (256). In the training procedure, we set the number of samples MJ=20 and the discount factor γJ=1. We train the g-Inspector using stochastic gradient descent with a batch size =20, the learning rate of 10−3, and momentum of 0.9.

Beside the basic implementation of g-Inspector, we implement a multi-agent version, called g-Inspector-mem. It contains multiple inspectors to explore the graph independently and concurrently. Then it integrates their observations on the same graph into a memory cell by averaging their hidden states. At last, the classification network, see Section 3.6, leverages the memory cell to conduct the prediction. This extension is similar to GAM-mem [31].

4.2 Performance Comparisons
The performance results are reported in Table 1, in which each column represents a dataset, the second to the forth rows represent the statistical report, and the rest represent the compared methods. For GAM, we compare the single agent version (GAM) and its multi-agent version (GAM-mem). The experiments are repeated 10 times in the same setting over 10-fold cross-validation. All results are presented as the average classification accuracy ± standard deviation.

TABLE 1 Statistics of Datasets and Accuracy for g-Inspector Compared With Baselines
Table 1- 
Statistics of Datasets and Accuracy for g-Inspector Compared With Baselines
We make three-fold interesting observations. First, the g-Inspector outperforms the other methods on MUTAG, ENZYMES, and IMDB-MULTI, with accuracy gain on average by 16.30 percent and up to 22.53 percent. Concretely, using g-Inspector, the accuracy gain for MUTAG is 13.00 percent, for ENZYMES is 22.53 percent and for IMDB-MULTI is 13.38 percent. For the two left datasets NCI1 and IMDB-BINARY, g-Inspector is also competitive.

Second, the execution time of g-Inspector is short than DGK on all the datasets, and also shorter than DCNN on NCI1. This is mainly because the forward procedure of g-Inspector is efficient. For example, DGK takes 1.089 seconds, 14 times of 0.077 of g-Inspector, and 12 times of 0.092 of g-Inspector-mem.

Third, the g-Inspector is more effective and efficient than GAM. For model design, g-Inspector moves to a farther vertex by the shift operation, compared with GAM moving to a one-hop neighborhood vertex at one step. From the experiment results, (1) multi-agent version (both GAM-mem and g-Inspector-mem) achieves higher classification accuracy than the basic version; (2) our g-Inspector gets 65.63 percent which is better than 64.17 percent of GAM; and (3) g-Inspector-mem gets 67.79 percent which is better than 67.71 percent of GAM-mem.

4.3 Result Illustration of Attention Mechanism
In order to better understand the nature of the attention mechanism, we inspect an compound C14H23NO2 from MUTAG from MUTAG. Fig. 4a illustrates the structure of this compound and the shift process of the inspector. In C14H23NO2 from MUTAG, the nitro structure consisting of vertex 14, 15 and 16, determines the class of this compound. In this case, the inspector shifts from the initial carbon atom at the vertex 3, and takes 6 shift operations over time. Fig. 4b demonstrates the classification accuracy at each time step. For example, at the first time step, the vertex 3 and its neighborhoods, see Fig. 4b, 4c, 4d, 4e, 4f, 4g, 4h, and 4i, are observed, and the current classification accuracy is only 59.9 percent. When the inspector shifts to the vertex 7, it can observe the one-hop neighborhood, see (b-ii), which contains the nitrogen atom vertex 14, and the classification accuracy can reach 86.39 percent. When the inspector shifts to the oxygen atom vertex 15, it achieves the best classification accuracy of 91.19 percent, because both an oxygen vertex 15 and the nitrogen vertex 14 can be observed, see (b-iii). When the inspector shifts from the vertex 15 to the vertex 8, see (b-iv), the accuracy is reduced to 83.59 percent, in practice, we used an early-stop to stop the inspector's shift and output the optimal classification result at (b-iii).

Fig. 4. - 
(a) The inspector shift process on C14H23NO2 from MUTAG. (b) The classification accuracy varies over the shift of inspector.
Fig. 4.
(a) The inspector shift process on C14H23NO2 from MUTAG. (b) The classification accuracy varies over the shift of inspector.

Show All

4.4 Interpretable Ability of g-Inspector
Compared with other graph classification methods, our proposed model can make the results interpretable. Specifically, our g-Inspector can help to measure the contribution of each subgraph region to find out the key subgraph regions for classification. Our g-Inspector can assign each observed region a contribution score by normalizing the gain of the corresponding classification accuracy. For example, the scores of the four regions shown in Fig. 4b are 0.19, 0.27, 0.28, 0.26. It concludes that the third region (b-iii) is more significant. We can discover the significant regions for a classification task by intersections of the high-score regions from each graph in a set, e.g., the nitro structure in MUTAG, which making the results interpretable.

4.5 Performance on Different Amounts of Training Data
It is essential to investigate the performance with different size of training data and test data. We design this experiment by dividing the datasets into sub-datasets with different proportion of training set, varying from 90 to 50 percent, on NCI1, ENZYMES, IMDB-BINARY and IMDB-MULTI. The results are reported in Fig. 5, where the x-axis represents the training size proportion and the left and right sides of the y-axis represent the test time and accuracy respectively. It supports that (1) the test time keep steady because it is independent with training set size; and (2) the accuracy increases a little with the growth of training set proportion. When the training set is extremely small, e.g., 50 percent for IMDB-BINARY, the classification accuracy is still competitive (66 percent, close to most baselines).

Fig. 5. - 
The performance with different size of training set on four datasets.
Fig. 5.
The performance with different size of training set on four datasets.

Show All

4.6 Step Number Tuning
This experiment examined the effectiveness and efficiency of the model with different # hidden layer, denoted by T, which represents the maximum step number of shift operation. In this experiment, we set m=3, vary the parameter T from 1 to 11, on NCI1, ENZYMES, IMDB-BINARY and IMDB-MULTI, and report the results in Fig. 6. The x-axis represents parameter T and the left and right sides of the y-axis are the test time and accuracy, respectively. We make two observations: (1) For most datasets, the execution time grows as T increases. The larger T means more recurrent layers in the core net, resulting in larger time complexity in computation for forward procedure of the g-Inspector. (2) The optimal setting of T varies from dataset to dataset. For IMDB-BINARY, the classification accuracy grows when T≤8 but decreases when T>8. For other datasets, the classification accuracy grows when T≤7 and decreases when T>7. A very large T increases the difficulty of pre-stop and causes overfitting on the training set.

Fig. 6. - 
The performance with different hidden layers number $T$T on four datasets.
Fig. 6.
The performance with different hidden layers number T on four datasets.

Show All

4.7 Shift Instruction Length Tuning
We conducted another experiment to report the performance with varying the instruction length m from 1, 11, on NCI1, ENZYMES, IMDB-BINARY and IMDB-MULTI. Fig. 7 illustrates the results on both accuracy and test time. We make the following interesting observations: (1) The test time increases greatly when the length varies from 1 to 7, as shown by the height of the bar(s) in Fig. 7. It is because the longer instruction vector requires more computation in both action network and inception network. (2) With the growth of m, the accuracy increases at first, achieves the optimal value and then keeps steady or even decreases. For example, for ENZYMES, the accuracy is 37 percent at m=3 and decreases to 32 percent at m≥6. Because the longer instruction, meaning more atomic shift operations at a time step, represents higher efficiency and provides greater chance to scan more regions in graph. It eventually obtains features observed by the inspector and promotes the classification accuracy. When m is too large, the effectiveness will be canceled due to the over-fitting.

Fig. 7. - 
The performance with different inspector shift instruction length $m$m on four datasets.
Fig. 7.
The performance with different inspector shift instruction length m on four datasets.

Show All

4.8 Scalability on Graph Number
To evaluate the scalability of g-Inspector, we conducted an experiment varying the graph number in datasets and reported the performance. In this experiment, we control the graph number by randomly removing and duplicating graph objectives in test set. The experiment results on accuracy and test time are reported in Fig. 8. There are two observations: (1) The test time grows linearly with respect to the growing of graph number. For example, in NCI1, the test time is around 0.25 seconds when graph number is 1000. And the test time reaches 1.7 seconds when graph number is 8000. (2) The accuracy keeps steady with different graph number. For example, in IMDB-BINARY, the accuracy is about 65 percent when graph number is either 300 or 2,400. The experiment results show that g-Inspector works well for a mass of graph objectives.

Fig. 8. - 
The performance with different graph number on four datasets.
Fig. 8.
The performance with different graph number on four datasets.

Show All

4.9 Scalability on Vertex Number
The last experiment is about the scalability of g-Inspector on vertex number of graph. The key advantage of g-Inspector is that it just loads a small region of the graph object instead of loading the entire graph via the inspection network. It means that it has the good time complexity for big graph object. To evaluate this scalability, we conduct this experiment by evaluate g-Inspector on graph objects having different vertex number. We generate the big graph objects by duplicating the nodes and connecting edges from original graph. The experiment results are illustrated by Fig. 9, from which we can make the observation: Both test time and accuracy remain unchanged with the growth of #vertices. For example, in NCI1, the accuracy is always 68 percent and the test time is 0.013 seconds when #vertices = 100, or 800.

Fig. 9. - 
The performance with different vertex number on four datasets.
Fig. 9.
The performance with different vertex number on four datasets.

Show All

4.10 Ablations
We perform ablation experiments to validate several key parts of g-Inspector. First, we design an experiment to investigate the impact of embedding algorithm by employing node2vec as ψ, which can preserve higher-order proximity between nodes to a low-dimensional space the same as Deepwalk [39]. We compare the normal version (g-Inspector) using DeepWalk as the encoder and a variant version g-Inspector(node2vec), shown in Table 2. The g-Inspector(node2vec) denotes the model using node2vec as the encoder. When comparing g-Inspector and g-Inspector(node2vec) in Table 2, we can see that the accuracy decreased on average by 1.27 percent up to 2.40 percent on all datasets. Even the accuracy on different version is slightly different, our g-Inspector is always competitive with baseline. The accuracy promotion or demotion caused by replacing different embedding algorithm is not our major technical contribution.

TABLE 2 Statistics of Datasets and Accuracy for g-Inspector Compared With Baselines
Table 2- 
Statistics of Datasets and Accuracy for g-Inspector Compared With Baselines
Second, we design an experiment to investigate the effectiveness of the shift operation. More specifically, we compare the normal version (g-Inspector), and the other two variant versions, shown in Table 2. The g-Inspector(no-shift) denotes the model in which the inspector cannot be shifted. The g-Inspector(atomic-shift) denotes the model where only atomic shift operation is permitted and serial shift operation is forbidden. When comparing g-Inspector and g-Inspector(no-shift) in Table 2, we can see that enabling the inspector shift makes the accuracy higher on average by 7.88 percent up to 14 percent on all datasets. The shift operation makes the inspector observe more than one region of graph to retrieve valuable information for classification. Compared to atomic shit, serial shift operation provides accuracy improvement on average by 4.87 percent and up to 7.75 percent. The serial shift operation speeds up the shift process, and observes more regions subject to the constraint steps.

Third, we design an experiment to investigate the impact of the size of observing regions by inspector, which was defined in the k-order region. We compare the normal version (g-Inspector) where k=1 and a variant version g-Inspector(0-order) where k=0, shown in Table 2. The g-Inspector(0-order) denotes the model in which the inspector only observes a single node at once time. When comparing g-Inspector and g-Inspector(0-order) in Table 2, we can see that enabling the inspector to observe a wider range makes the accuracy higher on average by 1.70 percent up to 2.73 percent on all datasets. When the scope of observation is wider, the inspector can collect more information to makes better decisions.

SECTION 5Conclusion
To address the challenges of graph classification, we proposed a recurrent attention model on graphs, called g-Inspector. We introduced an inspector module applying the attention to investigate the significance of each region to classification. It is interpretable owing to the ability of measuring the contribution of each region to the classification. Besides, we introduced a shift operation across the graph selecting a series of task relevant regions instead of searching the entire graph, avoiding the high dimensionality problem. Our experimental results showed that our g-Inspector model is competitive and achieves a higher accuracy compared with existing methods.