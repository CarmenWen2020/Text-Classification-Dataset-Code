Many big data applications produce a massive amount of high-dimensional, real-time, and evolving streaming data. Clustering such data streams with both effectiveness and efficiency are critical for these applications. Although there are well-known data stream clustering algorithms that are based on the popular online-offline framework, these algorithms still face some major challenges. Several critical questions are still not answer satisfactorily: How to perform dimensionality reduction effectively and efficiently in the online dynamic environment? How to enable the clustering algorithm to achieve complete real-time online processing? How to make algorithm parameters learn in a self-supervised or self-adaptive manner to cope with high-speed evolving streams? In this paper, we focus on tackling these challenges by proposing a fully online data stream clustering algorithm (called ESA-Stream) that can learn parameters online dynamically in a self-adaptive manner, speedup dimensionality reduction, and cluster data streams effectively and efficiently in an online and dynamic environment. Experiments on a wide range of synthetic and real-world data streams show that ESA-Stream outperforms state-of-the-art baselines considerably in both effectiveness and efficiency.
SECTION 1Introduction
With an ever-increasing number of applications involving data streams, e.g., data from smart phones, network monitoring, stock quotes, Internet of Things (IoT), etc. Unsupervised clustering of data streams has become an important problem for machine learning and big data analysis [1], [2], [3], [4]. As data streams are data-intensive, temporally ordered, and rapidly evolving, efficiently and effectively clustering data streams presents a major challenge for storage capacity, processing time, computational modeling, etc. [5], [6], [7].

Most existing data stream clustering solutions typically follow a two-stage (online and offline) framework [8]. In the online stage, the system receives and summarizes the data into micro-clusters or grid cells. In the offline stage, these micro-clusters are merged into the final macro-clusters. Since most existing algorithms consider offline processing to be less time sensitive, traditional clustering algorithms such as k-means, DBSCAN [9], etc., are commonly used. Although a dominant framework, this two-stage online-offline approach has some key weaknesses. It is incapable of responding to high-speed and high-dimensional evolving data streams effectively as (1) it can miss drifting concepts due to the fact that the two-stage processing model cannot truly achieve real-time and online processing, (2) its manually fixed parameters cannot keep up with dynamic changes in the data stream [7], [10], [11], [12], [13], and (3) it often has no mechanism to speedup dimensionality reduction in an online and dynamic environment. To overcome these weaknesses, this paper proposes a fully online and light-weight data stream clustering method, called ESA-Stream1 (Efficient Self Adaptive Stream). Its key parameters are learned/generated online in a self-adaptive manner. It also has a grid density centroid-based method to help speedup dimensionality reduction and to cluster more efficiently and accurately. Experimental results with both synthetic and real-world data streams show that ESA-Stream can efficiently and effectively tackle those weaknesses. Moreover, ESA-Stream exhibits a significantly better performance in both running time and result quality compared to state-of-the-art baseline algorithms. In summary, the main contributions of this paper are:

It presents a fully online data stream clustering method ESA-Stream, which does not need the traditional time-consuming offline stage and can detect arbitrarily shaped clusters efficiently and effectively.

It proposes an efficient self-adaptive technique to learn parameter settings dynamically during clustering to keep up with the drifting or evolving data streams. We are not aware of any existing work that is able to do this.

It introduces the notions of the grid density centroid-based and grid feature vector to speedup dimensionality reduction and to cluster data streams in an efficient and fully online manner.

The rest of this paper is organized as follows. Literature review is given in Section 2. Preliminary concepts and definitions are presented in Section 3. Section 4 gives a high-level overview of the proposed solution. The proposed methods in ESA-Stream are detailed in Section 5. Empirical evaluation is reported in Section 6. Section 7 concludes the paper.

SECTION 2Related Work
As mentioned above, most existing data stream clustering algorithms adopt the two-stage online-offline framework. According to their clustering methods in the offline stage, existing algorithms can be divided into the following categories: partition-based clustering algorithms [26], [27], [28], [29], hierarchical-based clustering algorithms [30], density-based/density grid-based algorithms [15], [16], [19], [21], [31], etc. Since the density-based/density grid-based algorithms can produce arbitrarily shaped clusters with high efficiency, they are widely favored. Our ESA-Stream algorithm also uses the density grid-based clustering method. Hence, our discussion below focuses on the latest two-stage density-based/density grid-based data stream clustering algorithms.

Most existing data stream clustering algorithms are designed to cluster specific domain data streams [16], [23], [32], [33] or to detect changes in data streams [34], [35]. Due to the inherent complex nature of data streams and the stringent clustering requirements (Section 3.1) of real-time and high-dimensional evolving streams, several representative works have attempted to deal with the weaknesses of the existing algorithms discussed in the introduction section.

Improving the Efficiency of the Offline Stage: To improve the efficiency of the offline stage, many density grid-based clustering solutions [14], [15], [22] following the online-offline clustering framework have been proposed. These algorithms combine the advantages of density-based and grid-based clustering approaches and are often referred to as density grid-based hybrid clustering algorithms. These methods can discover arbitrarily shaped clusters and detect outliers. They also work fairly efficiently as they depend only on the number of grids with data points, which is much smaller than the total number of data points in the input stream. In these approaches, the data space is partitioned into small segments called grids. Each data point from the data stream is mapped into a grid, which is then clustered with others based on their densities. The representative work following this strategy is D-Stream I [14], which was improved in D-Stream II [15] with a grid attraction mechanism, resulting in higher clustering quality. Although D-Stream I and II have good performances to some streams, they do not work well on high-dimensional evolving data streams.

In recent years, several researchers also exploited powerful computing platforms such as GPU-based or Apache Storm-based (Apache Storm is a distributed data streams computing framework) approaches [17], [36] to improve clustering efficiency for very fast streams. Moreover, there are also several distributed computing platforms, e.g., HadoopStreaming2 and RStorm3 for efficiently processing data streams [37]. Our work is not to build a platform, but to propose a fully online and light-weight data stream clustering method/framework.

On developing one-stage algorithms, [2] introduced a one-stage online clustering algorithm SNCStream based on Social Network Theory. It used homophyly to find non-hyperspherical clusters. [20] presented a micro-cluster-based online clustering component that explicitly captures the density between micro-clusters via a shared density graph. Similar to the method in [2], [19] proposed a fully online clustering algorithm CEDAS for evolving streams with arbitrarily shaped clusters. Although these online methods have improved the clustering efficiency to some extent, none of them can handle high-dimensional data streams.

High-Dimensional data Stream Clustering: Due to the curse-of-dimensionality [38], the distance between a pair of data points in a high-dimensional data space are numerically quite similar, resulting in no cluster or group. To address this issue, clustering algorithms such as subspace projection [21], [39] and index tree, called PKS-Stream [22], were proposed. However, the dimensionality reduction methods in these techniques are computationally very expensive and not suitable for real-time and dynamically evolving data streams. Some recent works [23], [24], [25] also addressed high-dimensional clustering over dynamic streams based on k-median or k-means. However, they require the user specified number of clusters k and are suitable only for finding hypersphere clusters

The main characteristics of the above algorithms are summarized in Table 1, from which we can make the following observations.

To improve the efficiency of clustering and dimensionality reduction of data streams, existing algorithms have made some progresses. But there is still room for major improvements. More importantly, none of the existing algorithms can fully meet the basic requirements of data stream clustering (see Section 3.1).

We found no work that learns clustering parameter settings adaptively and automatically in order to keep up with the real-time ever-changing data streams.

TABLE 1 Main Characteristics of the Recent Data Stream Algorithms
Table 1- 
Main Characteristics of the Recent Data Stream Algorithms
SECTION 3Preliminary and Definitions
3.1 Data Stream Clustering and Its Basic Requirements
We refer to those massive, unbounded sequences of data points, continuously generated by some applications at rapid rates as data streams [8], [11]. More formally, a data stream S is a massive sequence of data points, each of which is associated with a particular time stamp, x1,x2,…,xn, i.e., X={xi}, where xi ∈ Rd,1≤d∈Z+,1≤i≤n, and n→∞.

The intrinsic nature of data streams calls for the following requirements for clustering algorithms of data streams [10], [40], [41], [42]:

Since a data stream is a massive and unbounded sequence of data points, it is impossible to store the entire data stream. There must be a compact representation for them.

A stream is generally of high-speed and continually evolving, which requires real-time processing. As a result, traditional clustering methods that follow multi-scan procedures become infeasible. Data points from the stream can only be read sequentially once, and new data points from the stream must be processed quickly (i.e., online processing) and incrementally [43].

The algorithms should quickly identify outliers and cope with concept drift [32], [35], [44].

Data streams are often inherently high-dimensional, so stream clustering algorithms should be able to efficiently process high-dimensional data [24].

From our literature survey, we can see that there is no existing algorithm that satisfies all these requirements simultaneously. In this paper, we aim to propose a solution that attempts to meet all these requirements at the same time.

3.2 Grids and Related Definitions
We define that the data points from a stream with d dimensions belong to the space S=S1×...×Sd, where Si is the space for the ith dimension, which is evenly divided into pi segments. So, the space S is partitioned into N=∏di=1pi grids. For a grid g, we denote it as g=(j1,…,jd), where ji∈{1,…,pi} and (j1,…,jd) is used as the ID for g.

To capture the dynamic and evolving nature of data streams, we assign each data point xi in the stream a density coefficient, which decays as time elapses. If data point xi arrives at time stamp tc, the density coefficient of xi at time t (t>tc), denoted as D(xi,t), is defined as follows
D(xi,t)=λt−tc,(1)
View SourceRight-click on figure for MathML and additional features.where λ∈(0,1) is a constant called the decay factor.

For a grid g at a given time tc, let E(g,tc) be the set of data points that are mapped to g at or before tc. The grid density D(g,tc) of g is defined as the sum of the density coefficients of all data points that are mapped to g at or before tc, which is defined as
D(g,tc)=∑xi∈E(g,tc)D(xi,tc).(2)
View SourceRight-click on figure for MathML and additional features.Since data continuously arrive and keep evolving, the density of a grid changes constantly. However, it is proven that it is unnecessary to update the density of a grid at every timestamp [14] for its quickly calculating. If a new data point is mapped into a grid g at time t, the density of g can be updated as follows [15]:
D(g,t)=λt−tcD(g,tc)+1(t>tc),(3)
View SourceRight-click on figure for MathML and additional features.where D(g,tc) is the density of g at last moment tc before time t (see Eq. (2)).

In this paper, two self-learning and self-adaptive grid density threshold parameters, namely Dl (lower limit) and Du (upper limit), are used to divide grids into three categories [14], [15] for more accurate clustering and detecting outliers.

At time t, for a grid g, we call it a dense grid if its grid density D(g,t) (see Eqs. (2) and (3)) is greater than or equal to Du. If its grid density D(g,t) is less than or equal to Dl but greater than 0, we call it a sparse grid (not including empty grids and as noise). Otherwise, it is called a transitional grid. Note that Dl and Du are two key parameters, we will show in Section 5.1 that both of them can be computed/learned in a unsupervised or self-adaptive manner.

SECTION 4Overview of the Proposed ESA-Stream
The purposed ESA-Stream framework is outlined in Fig. 1. With five components cooperating with each other, ESA-Stream can cluster online real-time and high-dimensional evolving data streams efficiently and effectively. The functions of the five components are described as follows.

Data Stream Acceptor: It accepts each data point from a stream in real-time, and performs a min-max normalization for each dimension of the data point.

Grid Manager: It first maps or projects the normalized data point to a grid, then calculates and updates the density and feature vector (see Section 5.2 for details) of the grid online at a time interval gap between two consecutive clustering updates (see Section 5.1.3). It then calculates and produces a grid density centroid data stream. If the grid density centroid data stream is high-dimensional, its dimensionality should be reduced efficiently with our proposed Algorithm 1 GCB-PCA (see Section 5.2.2 for details).

Parameters Online Learner: It keeps on learning and updating the parameters (i.e., Dl, Du and gap) online when new data points in the stream arrive.

Parallel Clustering Engine: After each gap, it intelligently clusters the grid density centroid data (not the original data points) from Grid Manager in parallel with the strategy ECS shown in Section 5.3.2. It then generates the intermediate clustering results.

Result Integrator: It integrates the intermediate clustering results and outputs the final clusters for gap.

Fig. 1. - 
Stream clustering pipeline of ESA-Stream.
Fig. 1.
Stream clustering pipeline of ESA-Stream.

Show All

With collaboration among the five pipelined components shown in Fig. 1, ESA-Stream achieves efficient self-adaptive online data stream clustering.

According to the above ESA-Stream component functions and Figs. 1 and 2, we can identify its following unique features, which are further proved by our theoretical analysis later and extensive experiments.

Fully real-time online: The data stream is processed fully online in real time, without any offline stage.

Online-learning parameters: With the component Parameters Online Learner and the interactions between components, we highlight the fact that the ESA-Stream is an intelligent framework whose parameters are online-learning and online-adaptive to cope with the concept drift during the streaming process.

Light-weight but high-efficiency: Thanks to the strategies of grid density centroid-based clustering with dimensionality reduction (see Section 5.2.2 for details) and divide-and-conquer, the ESA-Stream is a light-weight yet efficient stream clustering framework.

Fig. 2. - 
The framework of ESA-Stream algorithm.
Fig. 2.
The framework of ESA-Stream algorithm.

Show All

SECTION 5Key Strategies Embedded in ESA-Stream
5.1 Self-Adaptive Parameter Learning
As indicated above, ESA-Stream has three key parameters, grid density lower and upper thresholds Dl,Du, and optimal time interval gap for the grids examination in order to update the clusters. Since fixed parameters from the user in all existing algorithms are unsuitable for dynamic and evolving data streams, we propose the following methods to learn and adapt these parameters automatically.

5.1.1 Grid Density Lower Threshold Dl
[14] and [15] proved that when t→∞, the sum of all grids densities is 1/(1−λ) from time 0 to t. With this knowledge, we propose a method to compute Dl. Suppose the total number of non-empty grids (i.e., dense grids, transitional grids, and sparse grids) is Mt at time t. In general, Mt is much less than the total number of grids N (Mt≪N) due to the existing large number of empty grids. Then the average density of these three types of grids should be 1/(Mt(1−λ)). We assume that the three types of grids follow a normal distribution (which is also verified in our initial experiments). Since dense, transitional and sparse are all statistical concepts relative to the mean value, we define those non-empty grids with less than two-thirds of the average grid density as the sparse grids based on extensive experimentation and verification. Thus, the grid density lower threshold Dl at time t is defined below.
Dl(t)=2/(3Mt(1−λ)).(4)
View Source

5.1.2 Grid Density Upper Threshold Du
Similarly, grids whose density is higher than the average density of non-sparse grids (also non-empty) can be regarded as dense grids. Based on this idea and experimental verification, we define Du as follows, where gi represents the such grid whose density D(gi,t) is greater than Dl at time t.
Du(t)=avg(D(gi,t)).(5)
View Source

5.1.3 gap: Time Interval for Two Consecutive Cluster Updates
As a data stream constantly evolves, the densities of the grids will change [34] from time to time. The density of each grid should be examined and the clusters should be updated as the data evolves. We define the optimal length of the time interval between two consecutive grid examinations and cluster updates/adjustments as the gap. The question is how to compute the gap.

Generally, the value for gap cannot be too large or too small. If it is too large, dynamical changes of the data stream will not be captured. If it is too small, it will result in very frequent and needless computations in the clustering phase. We propose the following technique to determine the value of the gap between two consecutive clustering updates. In particular, we consider the minimum time needed for a dense grid to degenerate to a sparse grid as well as that needed for a sparse grid to become a dense grid. Then we set gap to be the smaller one of the two values.

Lemma 5.1.
For any dense grid g, the minimum time needed for g to become a sparse grid is
δ1=⌊logλ(Dl/Du)⌋.(6)
View SourceRight-click on figure for MathML and additional features.

Proof.
Suppose that at time t, the grid g is a dense grid, then
D(g,t)≥Du.(7)
View SourceRight-click on figure for MathML and additional features.Suppose after δt1 time, g becomes a sparse grid, then we have:
D(g,t+δt1)≤Dl.(8)
View SourceLet E(g,t) be the set of data points mapped in g at time t. Since some data points may have mapped into g from time t to δt1, E(g,t)⊆E(g,t+δt1). And we have:
D(g,t+δt1)=∑x∈E(g,t+δt1)D(x,t+δt1)≥∑x∈E(g,t)D(x,t+δt1)=∑x∈E(g,t)λδt1D(x,t)=λδt1D(g,t).(9)
View SourceCombining Eqs. (7), 8, and (9), we get:
Dl≥D(g,t+δt1)≥λδt1D(g,t)≥λδt1Du,(10)
View Sourcewhich yields:
δt1≥logλ(Dl/Du).(11)
View SourceRight-click on figure for MathML and additional features.Hence, for any dense grid g, the minimum time needed for g to become a sparse grid is δ1=⌊δt1⌋.

Lemma 5.2.
For any sparse grid g, the minimum time needed for g to become a dense grid is
δ2=⌊logλ(1−DuMt(1−λ)1−DlMt(1−λ))⌋.(12)
View Source

Proof.
According to the definition of grid types in Section 3.2, if a grid g is a sparse grid at time t, we have:
D(g,t)≤Dl.(13)
View SourceRight-click on figure for MathML and additional features.Suppose after δt2 time, g becomes a dense grid, then we have:
D(g,t+δt2)≥Du.(14)
View SourceRight-click on figure for MathML and additional features.It is noted that E(g,t+δt2) can be divided into those points in E(g,t) and those come after t. In this case, there are new data points arrived for any of the time steps from t+1 until t+δt2. The average density increment of each grid g is δdensity=∑δt2−1i=0λi/Mt. Therefore, we have:
D(g,t+δt2)=∑x∈E(g,t+δt2)D(x,t+δt2)≤∑x∈E(g,t)D(x,t+δt2)+∑δt2−1i=0λiMt=λδt2D(g,t)+1−λδt2Mt(1−λ).(15)
View SourceSince a sparse grid g at time t becomes a dense grid after time t+δt2, based on Eqs. (13)-(15), we have:
Du≤D(g,t+δt2)≤λδt2D(g,t)+1−λδt2Mt(1−λ)≤λδt2Dl+1−λδt2Mt(1−λ).(16)
View SourceSolving Eq. (16) yields:
δt2≥logλ(1−DuMt(1−λ)1−DlMt(1−λ)).(17)
View SourceThus, δ2=⌊δt2⌋.

Based on δ1 and δ2, we set gap as Eq. (18) so that any change of a grid from dense to sparse or vice versa can be recognized in real-time.
gap=min(δ1,δ2)=⌊logλ(min(DlDu,1−DuMt(1−λ)1−DlMt(1−λ)))⌋.(18)
View Source

Note that each new gap (resp., δ1,δ2) is computed and updated based on the changes over all the grids during the last gap. That is, after a period of gap(i), we check the density changes over all the grids and compute the next gap, gap(i+1)=min(δ(i)1,δ(i)2).

Clearly, since Mt and D(gi,t) for the formulation of Dl, Du and gap are all captured online by the component Parameters Online Learner, the three parameters, Dl, Du and gap, of ESA-Stream are learned online in an unsupervised and self-adaptive manner for evolving data streams.

5.2 Grid Manager
Most density grid-based clustering algorithms simply map data points to each grid and compute its density for clustering. This method can lose the position information of the data points within a grid and lead to low clustering quality.

As illustrated in Fig. 3, grids 1 to 6 are dense grids, but grids 3 and 4 are separated by small areas of much lower density. Grids 7 to 10 are all transitional grids, but they are close to each other. In the existing density grid-based clustering algorithms, grids 1 to 6 will improperly form a cluster, while grids 7 to 10 will produce no cluster. These problems can be alleviated with a finer resolution of the grids or by the method [15] based on attractions of all data points between adjacent grids. However, our analysis and experiments show that the above methods are unsuitable for high-speed streams due to their high computation cost.


Fig. 3.
The schematic diagram of the grid data distribution and their Chebyshev distances between grid density centroids (a large red dot represents the grid density centroid of a grid).

Show All

5.2.1 Grid Density Centroid and Grid Feature Vector
For efficient and effective clustering, inspired by the Centroidal Equation in physics [45], we first introduce the concept of grid density centroid Dcen(g,t), which is used to represent all data points mapped into a grid g. It is defined as:
Dcen(g,t)=xj+∑xi∈E(g,t)D(xi,t)⋅xi1+∑xi∈E(g,t)D(xi,t)=xj+∑xi∈E(g,t)D(xi,t)⋅xi(1+D(g,t)) (i≠j),(19)
View SourceRight-click on figure for MathML and additional features.where xj=(xj,1,…,xj,d) is a new d-dimensional data point mapped into a grid g at t, and xi=(xi,1,…,xi,d) is a d-dimensional data point mapped into the grid g before t. From Eq. (19), we can see that comparing with the traditional centroid, the grid density centroid not only preserves the coordinates information for the data points in the grid, but also takes into account both the positional distribution and density for the points in its computation.

In order to record the necessary information about grid g, we introduce another concept of grid feature vector Vg, which is a tetrad tuple defined as
Vg=(ID,D(g,t),Dcen(g,t),tu),(20)
View SourceRight-click on figure for MathML and additional features.where ID is a serial number of grid g, and tu is the last updated time of D(g,t) and Dcen(g,t) of the grid g.

Note that Dcen(g,t) and Vg are updated in each gap by Eqs. (3), (19), and (20).

We will see later that the grid density centroid Dcen(g,t) of g plays a central role in substantially improving the clustering efficiency and effectiveness. First, dimensionality reduction based on the grid density centroid of grid g rather than the original data points is much more efficient because the centroid summarizes all the data points in a grid and it also preserves the positional distribution and density of points in grid g. For high-dimensional data streams, dimensionality reduction needs to be carried out efficiently. Second, grids merging for clustering based on the distance of grid density centroids substantially improves the clustering result quality using the merge method proposed below. Note that in grid merging, Dcen(g,t) in Vg should be replaced with D′cen(g,t) (the grid density centroid of g after dimensionality reduction).

5.2.2 Grid Centroid-Based Dimensionality Reduction
As we know, for the high-dimensional data streams, the curse-of-dimensionality is bound to happen. On the one hand, data points from a high-dimensional data stream are sparsely distributed in the space with almost equidistant resulting in no cluster or group. On the other hand, as the dimensionality increases, the total number of grids grows exponentially, which leads to the drastic drop of computational performance of the density grid-based clustering algorithms. Although some dimensionality reduction techniques, such as subspace projection and index tree are used in some high-dimensional data streams clustering algorithms, these dimensionality reduction methods [39], [46] use all data points of a grid and is computationally very expensive and not suitable for real-time evolving data streams. To overcome these weaknesses, we propose an efficient grid centroid-based dimensionality reduction method, called GCB-PCA(Grid Centroid-Based Principal Component Analysis) shown in Algorithm 1.

In each time interval gap, GCB-PCA performs a dimensionality reduction with the grid density centroid, which embeds the information of the density distribution along with locations of all data points mapped in the grid and is a representative point of all points in the grid. Even without parallel computation, our GCB-PCA can achieve very high efficiency, which is further verified later by our experiments in Section 6.

5.3 Parallel Clustering Engine
5.3.1 Grid Merging Strategy
In order to avoid the issue of the spatial distance measurement in high-dimensional data streams, we use Chebyshev distance for grid merging. Let Dis(gi,gj) (i≠j) be the Chebyshev distance between two adjacent grids gi and gj. Intuitively, if one of the following conditions is satisfied (designed empirically), the two adjacent grids gi and gj can be merged as an intermediate micro-cluster.

Dis(gi,gj)≤4/3∗len, if both gi and gj are dense grids (i.e., the density centroids for adjacent dense grids should be at most 4/3∗len away, such as the distance of grids 1 and 2 shown in Fig. 3). Here len is the segmented or grid length of each dimension for a grid. Note that, like all other grid-based approaches, len is manually set.

Dis(gi,gj)≤1.0∗len, if gi is a dense grid and gj is a transitional grid, or vise versa (such as the distance of grids 2 and 3 shown in Fig. 3);

Dis(gi,gj)≤2/3∗len, if both gi and gj are transitional grids, and D(gi,t)+D(gj,t)≥Du (such as the distance of grids 7 and 8 shown in Fig. 3).

Note that with this grids merging strategy, GMS for short, the three clusters shown in Fig. 3 can be recognized. As we will see in the experiment section, the strategy works effectively with real-world data.

Algorithm 1. GCB-PCA
Input: Non-sparse grid g feature vector Vg set SVg in the original d-dimensional data space S;

Output: Non-empty grid g′ feature vector Vg′ set S′Vg′ and grid centroid set S′Dcen(g′,t) in the reduced d′-dimensional data space S′ (d′<<d);

S′Vg′←∅; S′Dcen(g′,t)←∅; // Initialization

SDcen(g,t)← extract Dcen(g,t) of each Vg in SVg to construct the grid centroid set SDcen(g,t);

for each Dcen(g,t)∈SDcen(g,t) do

D′cen(g′,t)← reduce the dimensionality d based on grid centroids Dcen(g,t) to d′ using PCA;

S′Dcen(g′,t)←S′Dcen(g′,t)∪{D′cen(g′,t)};

end for

for each D′cen(g′,t)∈S′Dcen(g′,t) do

⟨g,g′⟩← combine the correspondence between a grid g in the original space S and a grid g′ in the reduced-dimension space S′ based on the results of Line 4;

Calculate D(g′,t) based on D(g,t) and ⟨g,g′⟩;

Calculate Vg′;

if Vg′∉S′Vg′ then

S′Vg′←S′Vg′∪{Vg′};

else

Update D(g′,t), Dcen(g′,t), and Vg′ by Eqs. (3), (19) and (20);

end if

end for

return S′Vg′ and S′Dcen(g′,t).

5.3.2 Efficient Clustering
In the density grid-based clustering algorithms, for a non-sparse grid g, all of its 2d direct adjacent grids or (3d-1)-2d indirect (diagonal neighbor) ones (shown in Fig. 3) must be traversed to form its temporary macro-clusters based on a grid merging strategy. Only considering all 2d direct adjacent grids will result in a low clustering accuracy. Moreover, for a high-dimensional data stream, the number for all adjacent grids to be traversed (i.e., 3d−1) will increase exponentially with d, which results in a huge computational cost. To achieve a good balance between computational efficiency and effectiveness for high-dimensional data streams, we developed a new Efficient Clustering Strategy, namely ECS, which has four steps as shown in Fig. 4.

Fig. 4. - 
The framework of ECS.
Fig. 4.
The framework of ECS.

Show All

Fig. 5 shows an example for ECS. Fig. 5a shows that a 2D-space is evenly divided into grids, including both dense and transitional ones. Assume we evenly split the 24 grids into 3 groups (following Step 1 in Fig. 4), namely SVg1, SVg2 and SVg3 (they are not clusters). Then we apply the GMS Strategy from each grid in SVg1 following BFS in parallel, resulting in a MST tree (Minimum Spanning Tree), i.e., temporary micro-cluster Ct1 in Fig. 5b. Note that, in Step 2 of ECS, during traversing all 3d-1 neighbors of each grid, we restrict that each grid must not be traversed more than once. In this way, the search space will get smaller and smaller during traversal. In most cases, the search space may become empty before SVgm is iterated. Then this step will intelligently perform an early-stop. All these strategies will save a significant amount of time in practice. We then immediately update the search space from SS1 to SS2 by eliminating the grids that have been merged. Afterwards, we carry on the merging process from each grid in SVg2, which leads to Ct2 and Ct3. Note that as the grids {2,3,8,13} have already been eliminated from the search space, there are only 2 grids in Ct2. After generating Ct2 and Ct3, as all the grids have been merged, the search space is updated to SS3=∅. Therefore, we do not need to perform merging from SVg3 anymore. The process stops here. Finally, we merge Ct1 and Ct2 into C1 as they share a common grid 9 following Step 3. The pipeline is shown in Fig. 5c. The pseudo code of ECS is given in Algorithm 2 corresponding to the ECS framework in Fig. 4.


Fig. 5.
An example of clustering grids efficiently.

Show All

5.4 The Complete ESA-Stream Algorithm
5.4.1 Algorithm Details
Hereby, the pseudo code for our ESA-Stream algorithm is shown as Algorithm 3.

Before processing the data stream X, ESA-Stream initializes the algorithm parameters (Lines 1-3). Because ESA-Stream does not accept any data points in the data stream during the initialization phase, the total number of non-empty grids M(t)=0, Eqs. (4), (5), and (18) are not available. Therefore, among these parameters, Dl, Du and gap are initialized to 0. It should be noted that the initial values of Dl, Du, and gap will not affect the subsequent clustering results, because these parameters will be self-adaptive (Line 15). tg is the clustering analysis moment and initialized to 10,000. This means that ESA-Stream clusters 10,000 points for the first time, and then tg is updated with the update value of gap (Line 24). Moreover, in order to store the grid feature tuple of non-empty grids, ESA-Stream uses the HashMap object gridList with ID as the key and Vg as the value.

Algorithm 2. ECS
Input: Non-sparse grid g feature tuple set SVg;

Output: Clustering result set SC;

SVg1, ..., SVgm ← evenly divide SVg into m parts;

for each clustering thread Proci,1≤i≤m,

start

SCti←∅;

for each Vg∈SVgi do

if Vg is traversed then

continue;

else

Ct← clustering Vg with GMS Strategy;

end if

SCti←SCti∪{Ct};

end for

Send SCti to the main thread Proc0;

end

The thread Proc0 performs:

start

SC←⋃mi=1SCti;

for i←1 to |SC| do

for j←i to |SC| do

if Cti∩Ctj≠∅ then

Cti←Cti∪Ctj; Ctj←∅;

end if

end for

end for

end

return SC;

After the initialization phase, ESA-Stream accepts each data point in the order of its arrival. A data point xi in the stream X will be mapped into the gird g, and then ESA-Stream will determine the ID of the gird g with the grid's length len (Lines 5-6, corresponding to Step 2 of Fig. 2). If ⟨ID,Vg⟩ exists in gridList, ESA-Stream will update the grid feature tuple. Otherwise we initialize the grid feature tuple and insert it into the grid list (Lines 7-13, corresponding to Step 2 of Fig. 2).

Algorithm 3. ESA-Stream
Input: Data stream X, decay factor λ, grid's length len;

Output: Clustering result set SC;

t←0; Dl←0; Du←0; gap←0

tg←10000; // tg is the latest clustering time

gridList←∅;

for each xi∈X do

ID← map xi into the gird g with the grid's len;

Calculate Vg;

if ⟨ID,Vg⟩∉gridList then

D(g,t)←1; Dcen(g,t)←xi;

Vg←(ID,D(g,t),Dcen(g,t),t);

gridList←gridList∪{⟨ID,Vg⟩} ;

else

Update D(g,t), Dcen(g,t) in Vg by Eqs. (3), (19);

end if

if t==tg then

Update Dl, Du and gap by Eqs. (4), (5) and (18);

Detect and remove sporadic grids from gridList;

SVg← extract all non-sparse grid feature tuple Vg in gridList to construct the grid feature tuple set;

if X is high-dimensional (dimensionality ≥4) judged by EAS-Stream's Data Stream Acceptor then

S′Vg′← call function GCB-PCA(SVg);

SC← call function ECS(S′Vg′);

else

SC← call function ECS(SVg);

end if

tg←tg+gap;

Output SC;

end if

t←t+1;

end for

When time tg elapses, ESA-Stream updates Dl, Du and gap (Line 15, corresponding to Step 3 of Fig. 2). Then, ESA-Stream removes the sporadic grids and extract all non-sparse grids’ Vg data of gridList to construct the grid feature tuple set SVg (Lines 16-17, corresponding to Step 4 of Fig. 2). If X is high-dimensional (the dimensionality ≥4), the grid feature tuple set S′Vg′ is constructed after dimension reduction by calling function GCB-PCA(SVg) (Line 19, corresponding to Step 5 of Fig. 2).

In the clustering analysis phase, ESA-Stream calls the function ECS(S′Vg′) to cluster reduced dimensional S′Vg′. If X is not high-dimensional, ESA-Stream calls the function ECS(SVg) to cluster original dimensional SVg (Lines 20-22, corresponding to Steps 6-8 of Fig. 2). Finally, ESA-Stream outputs the clustering result set SC (Line 25, corresponding to Step 9 of Fig. 2).

5.4.2 Time and Space Complexity
For each data point of data stream X in the original d-dimensional data space, O(d) is needed for mapping data point into the grid and calculating/updating the grid feature tuple. Therefore, the time complexity of data stream reading is O(d|X|).

Before clustering the grid feature tuple data, ESA-Stream removed noise grids, i.e., the sporadic grid feature tuples, and constructed the non-sparse grid feature tuple set SVg by traversing all non-empty grid feature tuples. Suppose the number of the non-empty grids is Mti at time gap(i), the time complexity of constructing SVg equals O(Mti).

If X is high-dimensional data stream in the time interval gap(i), ESA-Stream could call the function GCB-PCA(SVg) to reduce its dimensionality. The main operations of GCB-PCA consist of using PCA to reduce grid density centroid's dimension of each grid feature tuple in SVg with time O(d′M2ti), and constructing the grid feature tuple set S′Vg′ in the reduced d′-dimensional data space with time O(Mti). Therefore, the time complexity of GCB-PCA is O(d′M2ti)+O(Mti). It is noted that some grid density-centroids from original d-dimensional space S should be mapped into the same grid in the reduced d′-dimensional data space S′ (d′<<d). Thus, the size M′ti of S′Vg′ must be much less than the size Mti of Vg, i.e., M′ti≪Mti.

When gap(i) comes, ESA-Stream could call Algorithm 2 to carry out stream clustering. As the grid density-centroid clustering is adopted, the clustering time complexity is O(dMtilog(Mti)). Corresponding to the reduced dimensional space S′, the time complexity of Algorithm 2 should be O(d′M′tilog(M′ti)).

Above all, suppose all of the number of gap(i) is Ngap (1≤i≤Ngap). If data stream X is high-dimensional, the total time complexity of ESA-Stream equals O(d|X|) + ∑Ngapi=1O(Mti) + ∑Ngapi=1O(d′M2ti) + ∑Ngapi=1O(d′M′tilog(M′ti)). Otherwise, the total time complexity is O(d|X|) + ∑Ngapi=1O(Mti) + ∑Ngapi=1O(dMtilog(Mti)).

Since theoretical analysis and experimental results show that M′ti≪Mti, d′≪d and ∑Ngapi=1M2ti≪|X|, thus, O(d|X|)+ ∑Ngapi=1O(Mti) + ∑Ngapi=1O(d′M2ti) + ∑ci=1O(d′M′tilog(M′ti)) ≈ O(d|X|), and O(d|X|) + ∑Ngapi=1O(Mti) + ∑Ngapi=1O(dMtilog(Mti)) ≈ O(d|X|). From the above discussion, we conclude that the time complexity of ESA-Stream is O(d|X|) regardless of whether the dimension is reduced.

Similarly, the storage space of grid feature tuples is O(Mt) at time gap(i), the storage space of GCB-PCA is O(Mt)+O(M′t)=O(Mt)(M′t<<Mt), and the storage space of ECS is at most O(Mt). Therefore, the space complexity of ESA-Stream is O(Mt)+O(Mt)+O(Mt)=O(Mt) whether it's dimensionally has been reduced or not.

SECTION 6Experimental Results
To evaluate the performance of ESA-Stream, we use some synthetic and real-world data streams, which are shown in Table 2. All experiments are conducted on a HP ProDesk 480 G1 MT Mini Tower (Intel i5-4570, 4 cores/chip, 1 thread/core, 3.20 GHz and 8GB RAM ) running Win 7. ESA-Stream with 4 threads and all baselines are implemented in Scala 2.11.

TABLE 2 The Experimental Datasets
Table 2- 
The Experimental Datasets
As shown in Table 2, we created 5 synthetic datasets. To verify the ability of recognizing arbitrary shaped clusters, filtering outliers, and coping with concept drifts, we created the synthetic dataset DB2, shown in Fig. 6. Like the evaluation of the other high-dimensional data stream clustering algorithms, the synthetic datasets RBF5∼RBF40 are generated using the Radial Basis Function (RBF) generator of MOA4 to ensure clusters/groups in high-dimensional space can be generated. The RBF generator creates the user-given number of drifting concepts (or clusters) and the data points in each cluster according to the user-specified standard deviations in Gaussian distribution. The real-world traffic dataset records traffics from the city of Aarhus5. 449 traffic sensors are deployed in the city which produce new values every 5 min. This dataset covers a period of 2 months and contains several attributes, e.g., number of cars, average speed, timestamp and 2-dimensional location of cars. The datasets Forest Covertype and KDD Cup 996 are commonly used for supervised learning, where the class labels are adopted as the ground-truth for evaluating our clustering results. All the datasets are normalized for each dimension.


Fig. 6.
Data distribution of DB2 and the result of ESA-Stream.

Show All

All baselines are parameterized following their original papers shown in Table 3. GD-Stream [16], HDDStream [21] and D-Stream II [15] follow the online-offline framework with density grid-based algorithm clustering, while CEDAS [19] is an online clustering algorithm using DBSCAN. All the four algorithms are all state-of-the-art clustering algorithms for data streams [12]. Unlike other baselines, HDDStream was designed to cluster high-dimensional streams. GD-Stream and D-Stream II parameterize gap as follows:
gap=⌊logλ(max(ClCm,N−CmN−Cl))⌋,(21)
View SourceRight-click on figure for MathML and additional features.where N is the total number of grids (see Table 3 for other parameters). Note that (1) although gap is computed, it is fixed and cannot adapt to the evolving streams, and (2) for high-dimensional streams, N is huge, then gap=⌊logλ(1)⌋=0 according to Eq. (21). Therefore, for datasets where dimension is over 5, Eq. (21) cannot be applied, we set gap = 10000 in GD-Stream and D-Stream II for these cases instead.

TABLE 3 Baseline Algorithms and Their Parameters
Table 3- 
Baseline Algorithms and Their Parameters
In our ESA-Stream system, gap is computed as min(δ1,δ2) (see Eq. (18)), and the parameters Dl, Du and gap are all learned in an unsupervised self-adaptive manner automatically from the data streams.

Evaluation Metrics. The effectiveness is evaluated using purity except for the Traffic dataset. Purity is defined as
purity=1N∑kmaxj|Ck∩Lj|,(22)
View SourceRight-click on figure for MathML and additional features.where Lj is the ground-truth class assignment for the samples.

As the Traffic dataset has no ground-truth class labels and no prior probability density, purity and the measure in [47] do not apply. Therefore, we use the popular SC (Silhouette Coefficient):
SC=1N∑i=1Nb(xi)−a(xi)max{a(xi),b(xi)},(23)
View Sourcewhere a(xi) (resp., b(xi)) is the mean distance between xi and all other points in the same cluster (resp., next nearest cluster). Running time efficiency is measured in seconds.

6.1 Ability to Handle Evolving Data Streams
In the first set of experiments on dataset DB2 with 70 K outliers, we test all algorithms on their ability to capture the dynamic evolution of data clusters and to remove outliers. For this purpose, we generated 7 clusters (with their data points) with arbitrary shapes. The clusters randomly appear and disappear over time, which simulates concept drifts [44], i.e., the dynamic evolution of data clusters. The speed of the data stream is 100 K data points per second, with equal interval of arrival.

First, we check our ESA-Stream's clustering result in Fig. 7 at three different timestamps t1, t2 and t3 as well as the final clustering result shown in Fig. 6. The results clearly show that ESA-Stream can respond and adapt timely to the dynamic evolving high-speed data stream shown in Fig. 7, and that it has high purity of 96.1 percent shown in Table 4 and is effectively immune to outliers.

TABLE 4 Running Time (T) and Purity/SC (P) of Algorithms
Table 4- 
Running Time (T) and Purity/SC (P) of Algorithms
Fig. 7. - 
The results of ESA-Stream for evolving data stream DB$_2$2.
Fig. 7.
The results of ESA-Stream for evolving data stream DB2.

Show All

Second, we evaluate all baselines. The results are shown in Table 4 and Fig. 8. As the drifting concepts (or clusters) in the data stream occur over time, some clusters appear and others disappear over time. Since all baselines cannot learn clustering parameters adaptively, they have very weak ability to handle evolving data streams. The experiments shown in Table 4 and Fig. 8 clearly demonstrate that (1) with a constant gap (10000, 30000 or 50000), all baselines have low clustering purity, (2) the larger the constant gap is, the lower the average clustering purity, and (3) using the computed gap, ESA-Stream has much higher purity due to its parameter online learning. Since ESA-Stream can timely and dynamically detects drifting concepts 83 times, its purity is 15, 19 and 24 percent higher than the baselines on their constant gaps of 10000, 30000 and 50000, respectively. These observations show that the gap with the online-learning and self-adaptive ability is the key to detect/capture drifting concepts in data streams over time.


Fig. 8.
Purities of ESA-Stream and baselines with different gap models for evolving data stream DB2.

Show All

6.2 Effectiveness and Efficiency
For the second set of experiments, we report the clustering results of all algorithms with regard to both the running time and effectiveness (measured in purity or the SC for the Traffic dataset). In particular, we carried out clustering over all datasets in Table 2, where the speed of data streams is set to 100K/second and len=0.075 for each dimension, except for len=0.02 on DB2. From the experiments shown in Table 4, comparing with the baselines, on average, the purity is improved by 15.4 and 8.9 percents on synthetic and real-world datasets, respectively; the running time is reduced by one ninth and one sixth on the synthetic and real-world datasets, respectively. Moreover, as the dimensionality and data volume increases, the running times of the algorithms except ESA-Stream dramatically increase, and clustering purity or SC drop drastically. Although CEDAS was designed as fully online clustering algorithm, experiments show that it is incapable of handling high-speed and high-dimensional evolving data streams due to its weaknesses of fixed parameter settings and not-reduced dimensionality.

To further compare the precision performance (Purity) of ESA-Stream with the baselines, we conducted statistical tests (t-test, α=0.05). The p-values of ESA-Stream and the four baselines CEDAS, HDDStream, GD-Stream and D-Stream II are 0.0002, 0.0084, 0.0015 and 0.0013, respectively, from which it can be clearly seen that our ESA-Stream is significantly better than the baselines in effectiveness.

6.3 Dimensionality Scalability
We now verify dimensionality scalability of all algorithms on all datasets shown in Table 1. Through the results in Fig. 9, we can see that the proposed ESA-Stream exhibits excellent dimensionality scalability compared to baselines, which is attributed to its grid centroids and the self-learned gap (i.e., dimensionality reduction using PCA is done at the end of each gap for the next clustering update). Note that the baselines of CEDAS, GD-Stream and D-Stream II have no dimensionality reduction mechanism. They only leverage fast traversal of the limited 2d adjacent grids/micro-clusters to attempt to alleviate the “curse-of-dimensionality” problem. However, the results in Fig. 9 show that, without the dimensionality reduction, they can't cope with the “curse-of-dimensionality”, leading to poor purity. Intuitively, one trivial approach for addressing the problem is to simply apply dimensionality reduction, e.g., PCA, before performing CEDAS, GD-Stream and D-Stream II. However, we argue that this strategy is impractical due to the following reasons. First, only when all the data samples are present, PCA is able to learn the principle components. It cannot work on streaming scenario where new data samples dynamically keep on arriving. Second, due to the first reason, the only approach to apply PCA is to implement PCA over all the samples in each gap, which definitely introduces large cost and cannot satisfy the requirements for stream processing.

Fig. 9. - 
Running time and purity of algorithms with various dimensionalities from $5d$5d-$40d$40d data streams RBF$_5$5 and RBF$_{40}$40.
Fig. 9.
Running time and purity of algorithms with various dimensionalities from 5d-40d data streams RBF5 and RBF40.

Show All

Although HDDStream is designed to perform dimensionality reduction with its subspace projection method, the method is very time-consuming and hard for high-speed streams. Our grid density centroid based method enables PCA to effectively and efficiently reduces the data stream's dimensionality with rapid evolving streams.

6.4 Parametric Sensitivity
Since ESA-Stream adopts the commonly used density grid-based clustering idea, like other algorithms, the parameter len (grid granularity) needs to be preset. Existing algorithms are quite sensitive to len. The running time on a high-dimensional stream dramatically increases with the decrease of len. However, thanks to our data dimensionality reduction and grids merging methods, ESA-Stream is not very sensitive to variations of len when len is not too large (e.g., from 0.05 to 0.25), which is shown in Fig. 10. Note that there is no len in CEDAS and HDDStream as they are not density grid-based algorithms and are not compared here.

Fig. 10. - 
The running time and purity of algorithms with various values of grids’ $len$len under data stream KDD Cup 99.
Fig. 10.
The running time and purity of algorithms with various values of grids’ len under data stream KDD Cup 99.

Show All

All the experiments showed that ESA-Stream is far superior to all baselines in clustering efficiency and quality.

SECTION 7Conclusion
Although many stream clustering algorithms have been proposed, there are still major gaps between existing solutions and practical requirements. To bridge the gaps, we present a real-time online and light-weight clustering framework, ESA-Stream, for high-speed and high-dimensional evolving data streams. This framework is built upon a series of new ideas: self-learning or self-adaptation in setting parameters in the online dynamic environment, efficient dimensionality reduction and clustering based on grid density centroids, etc., designed to address specific challenges. ESA-Stream has several desirable features, i.e., fully real-time online, dynamic and online self-adaptive learning of parameters, and light-weight and high-efficiency. Empirical evaluations showed that ESA-Stream outperforms state-of-the-art data streams clustering algorithms in both efficiency and result quality.

Although ESA-Stream has demonstrated its effectiveness in clustering real-time and high-dimensional evolving data streams, there are still some gaps for ESA-Stream to become a truly intelligent machine learning framework. To cover the gaps, embedding a lifelong learning mechanism [48] into ESA-Stream is a promising future research direction.

