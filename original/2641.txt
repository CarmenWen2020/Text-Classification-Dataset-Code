Network reliability measures the probability that a target node is reachable from a source node in an uncertain graph, i.e., a graph where every edge is associated with a probability of existence. In this paper, we investigate the novel and fundamental problem of adding a small number of edges in the uncertain network for maximizing the reliability between a given pair of nodes. We study the NP -hardness and the approximation hardness of our problem, and design effective, scalable solutions. Furthermore, we consider extended versions of our problem (e.g., multiple source and target nodes can be provided as input) to support and demonstrate a wider family of queries and applications, including sensor network reliability maximization and social influence maximization. Experimental results validate the effectiveness and efficiency of the proposed algorithms.

SECTION 1Introduction
Rich expressiveness of probabilistic graphs and their utility to model the inherent uncertainty in a wide range of applications have prompted a large number of research works on probabilistic graphs by the data management research communities. In recent years, researchers in this community have proposed efficient algorithms for solving several interesting problems, e.g., finding k-nearest neighbors [1], answering reachability queries [2], and designing networks [3] — all in an uncertain graph setting. Uncertainty in a graph arises due to many reasons, including noisy measurements of an edge metric [4], edge imputation using inference and prediction models [5], [6], and explicit manipulation of edges, e.g., for privacy purposes [7].

In an uncertain graph setting, Network Reliability is a well-studied problem [8], [9], which requires to measure the probability that a target node is reachable from a source node. Reliability has been widely studied in device networks, i.e., networks whose nodes are electronic devices and the (physical) links between such devices have a probability of failure [10]. More recently, the attention has been shifted to social, communication, transportation, genomic, and logistic networks [11], [12], [13]. Applications of reliability estimation include computing the packet delivery probability from a source to a sink node in a wireless sensor network, measuring information diffusion probability from an early adopter to a target customer in a social influence network, predicting new interactions by finding all proteins that are evidently (i.e., with high probability) reachable from a core (source) set of proteins in a protein-interaction network, as well as estimating on-demand delivery probability via different routes from an inventory to warehouses or customers in a road network, among many others.

In this paper, we investigate the novel problem of adding a small number of edges in an uncertain network for maximizing the reliability between a given pair of nodes. We refer to such edges as shortcut edges and the problem of identifying the best set of k edges as the budgeted reliability maximization problem. Our problem falls under the broad category of uncertain networks design [3], optimization [14], and modification [15] problems, yet surprisingly this specific problem has not been studied in the past.

The budgeted reliability maximization problem is critical in the context of many physical networks, such as transportation and communication networks. In mobile ad-hoc networks, the connectivity between sensor nodes and devices is estimated using noisy measurements, thus leading to edges naturally associated with a probability of existence [13]. Road networks can be modeled as uncertain graphs because of unexpected traffic congestion [12]. In these networks, creating new connections between nodes (e.g., building new roads, flyovers, adding Ethernet cables) is limited by physical constraints and budget. One can introduce only k new edges where k is decided based on resource constraints. Thus, our goal is to intelligently add k new edges such that the reliability between a pair of important nodes is maximized [3]. Furthermore, in social networks, finding k best shortcut edges could maximize the information diffusion probability from an early adopter to a target customer [16], thus the network host can actively recommend these links to the respective users. In case of protein-interaction networks, interactions are established for a limited number of proteins through noisy and error-prone experiments—each edge is associated with a probability accounting for the existence of the interaction. Therefore, finding the top-k shortcut edges can assist in de-noising protein-interaction networks [17].

Challenges and Contributions. Unfortunately, budgeted reliability maximization problem is non-trivial. In fact, a simpler problem to compute the exact reliability over uncertain graphs is #P-complete [8], [9]. Our thorough investigation of the budgeted reliability maximization problem have yielded the following theoretical results: (1) we prove that, even assuming polynomial-time sampling methods to estimate reliability (such as, Monte Carlo sampling [18], or more sophisticated recursive stratified sampling [19]), our problem of computing a set of k shortcut edges that maximizes the reliability between two nodes remains NP-hard; (2) the budgeted reliability maximization problem is hard to approximate, as (i) it does not admit any PTAS, and (ii) the underlying objective function is neither submodular, nor supermodular. The above pessimistic results are useful to comprehend the computation challenges associated with finding even an approximate solution to this problem, let alone an optimal one. For instance, lack of submodular (or supermodular) property prevents us from using an iterative hill-climbing based greedy algorithm that maximizes the marginal gain at every iteration to obtain a solution with approximation guarantees. Moreover, a hill-climbing algorithm would be inefficient due to repeated computation of marginal gains for all candidate edges (which are, in fact, missing edges in the input graph, and can be O(n2) in numbers for a sparse graph) at every iteration.

By considering the computation challenges as we have discussed above, in this paper we propose a practical algorithm for budgeted reliability maximization problem. Our proposed solution systematically minimizes the search space by only considering missing edges between nodes that have reasonably high reliability from the source node and to the target node. Next, we extract several highly-reliable paths between source and target nodes, after including those limited number of candidate edges in the input graph. This is motivated by the observation that what really matters in computing the reliability between two nodes is the set of paths connecting source to target, not the individual edges in the graph [20], [21], [22]. Our algorithm then iteratively selects these paths so as to achieve maximum improvement in reliability while satisfying the constraint on the number of new edges (k) to be added.

We also consider a restricted version of our problem, which approximates the reliability by considering only the most reliable path between the source and the target node [21], [22]. We prove that improving the probability of the most reliable path can be solved exactly in polynomial time, which yields an efficient algorithm for the restricted version of our problem. Finally, we focus on generalizations where multiple source and target nodes can be provided as input, thus opening the stage to a wider family of queries and applications, e.g., network modification for targeted influence maximization [16], [23].

The main contributions of this paper are as follows.

We study the novel and fundamental problem of maximizing the reliability between a given pair of nodes by adding a small number of edges in an uncertain graph. Our problem is NP-hard, and is also hard to approximate, even when polynomial-time reliability estimation is employed (Section 2).

We design effective and efficient solutions for our problem. Our algorithms first apply reliability-based search space elimination, then fill the remaining graph with missing edges, and finally select the top-k edges to add based on most reliable paths (Section 5).

We further consider a restricted and one extended version of our problem to support a wider family of queries. In the restricted version, the reliability is estimated only by the most reliable path, thus it can be solved in polynomial-time. In the extended version, multiple sources and targets can be provided as input. The proposed algorithms are generalized to multiple-source-target case. (Sections 4 and 6).

We conduct a thorough experimental evaluation with several real-world and synthetic graphs to demonstrate the effectiveness, efficiency, and scalability of our algorithms, and illustrate the usefulness of our problem in critical applications such as sensor network reliability maximization and influence maximization in social networks (Section 8).

SECTION 2Preliminaries
2.1 Problem Formulation
An uncertain graph G is a triple (V,E,p), where V is a set of n nodes, E⊆V×V is a set of m directed edges, and p(e)∈[0,1] is the probability that the edge e∈E exists. Following bulk of the literature on uncertain graphs [1], [2], [8], [9], [20], [24], we assume that edge probabilities are independent of each other. Therefore, we employ the well-established possible world semantics: The uncertain graph G yields 2m deterministic graphs G⊑G. Each possible world G=(V,EG) is a certain instance of the uncertain graph G, where EG⊆E and is obtained by independent sampling of the edges. Its probability of being observed is given as
Pr(G)=∏e∈EGp(e)∏e∈E∖EG(1−p(e)).(1)
View SourceGiven a source node s∈V, a target node t∈V, the reliability R(s,t,G), also known as the s-t reliability, is defined as the probability that t is reachable from s in G. Formally, for a possible graph G⊑G, let IG(s,t) be an indicator function taking the value 1 if there exists a path from s to t in G, and 0 otherwise. R(s,t,G) is computed as follows:
R(s,t,G)=R(s,t,(V,E,p))=∑G⊑G[IG(s,t)×Pr(G)].(2)
View SourceThe problem that we study in this work is stated below.

Problem 1 (Single-source-target budgeted reliability maximization).
Given an uncertain graph G=(V,E,p), a source node s∈V, a target node t∈V, a probability threshold ζ∈(0,1], and a small positive integer k, find the top-k edges to add in G, each with probability p(e)=ζ, so that the reliability from s to t is maximized
E∗=argmaxE1⊆V×V∖ER(s,t,(V,E∪E1,p))s. t.|E1|=k;andp(e)=ζ∀e∈E1.(3)
View Source

For simplicity, we adopt a fixed probability threshold ζ on new edges. The intuition is that when establishing a new edge, generally we consider a connection with the best/average possible reliability, e.g., an Ethernet cable with the highest reliability in case of LAN, or the average link reliability of sensor edges as we have used in the discussed case study. However, if the user provides probability values for the missing edges as part of input, our proposed algorithm (Section 5) will work smoothly as it can simply use those values instead of ζ when finding the most reliable paths (see our experimental evaluation in Table 14, Section 8.2).

Remarks.
Due to various physical and resource constraints, in practice, it might not be possible to consider all missing edges in the input graph as candidates for our problem. In a social network it is often realistic to recommend new connections between users who are within 2-3 hops. In a communication network, a new edge can be added only if the two nodes are within a certain geographical distance. While we analyze the complexity of our problem and develop algorithms for the generalized case, that is, all missing edges can potentially be candidate edges, in our solution as well as in experiments we provision for a threshold distance h: Two nodes can be added by a new edge only if they are within h-hops away. Note that (1) when h is the diameter of the graph (i.e., maximum shortest-path distance between any pair of nodes), this is essentially equivalent to the generalized case. (2) Smaller values of h reduces search space, thereby improving efficiency. In our experiments, we analyze scalability of our methods for different values of h.

TABLE 1 Table of Notations (Used in Section 2: Preliminaries)

TABLE 2 Reliability Gains of Three Possible Solutions for the Example in Fig. 3 Under Different Setting

TABLE 3 Table of Notations (Used in Sections 4, 5, 6).

TABLE 4 Reliability Gain and Running Time Comparison Without Search Space Elimination

TABLE 5 Reliability Gain and Running Time Comparison After Search Space Elimination

TABLE 6 Properties of Datasets, SPL Denotes Shortest Path Length, and C. Coe. Denotes Clustering Coefficient

TABLE 7 Single-Source-Target Reliability Maximization on Different Real Datasets

TABLE 8 Single-Source-Target Reliability Maximization on Different Synthetic Datasets

TABLE 9 Comparison With the Exact Solution (ES), k=3k=3, ζ=0.33ζ=0.33, r=54r=54, l=30l=30, Intel Lab Data

TABLE 10 Reliability Gain and Running Time Comparison With Varying Budget on #New Edges kk

TABLE 11 Reliability Gain and Running Time Comparison With Varying Budget on #New Edges kk

TABLE 12 Reliability Gain, Running Time Comparison With Varying Probability ζζ on New Edges

TABLE 13 Reliability Gain, Running Time Comparison With Varying Probability ζζ on New Edges

TABLE 14 Analysis With Different Probabilities on New Edges

2.2 Hardness of the Problem
Problem 1 depends on reliability computation in uncertain graphs, which is #P-complete [8], [9]. Thus, single-source-target budgeted reliability maximization problem is hard as well. However, as reliability can be estimated in polynomial time via Monte Carlo (MC) sampling [18], or more sophisticated recursive stratified sampling [19], the key question is whether Problem 1 remains hard even if polynomial-time reliability estimation methods are employed. Due to combinatorial nature of our problem, and assuming O(n2) missing edges in a sparse graph, one can design an exact solution that compares the s-t reliability gain for (n2k) possible ways of adding k new edges, and then reports the best one. However, this is clearly infeasible for large networks. We, in fact, prove that our problem is NP-hard, and it does not admit any PTAS. Moreover, Problem 1 is neither submodular, nor supermodular for inclusion of edges.

Theorem 1.
Problem 1 is NP-hard in the number of newly added edges, k.

Proof.
We prove NP-hardness by a reduction from the MAX k-COVER problem, which is NP-hard. In MAX k-COVER, there is a collection of subsets S={S1,S2,…,Sh} of a ground set U={u1,u2,…,ur}, where Si⊆U for all i∈[1…h]. The objective is to find a subset S∗⊂S of size k such that maximum number of elements in U can be covered by S∗, i.e., so as to maximize |⋃Si∈S∗Si|. For an instance of MAX k-COVER, we construct an instance of our Problem 1 in polynomial time as follows (Fig. 1).


Fig. 1.
Reduction from MAX k-COVER to single s-t budgeted reliability maximization problem.

Show All

We create an uncertain graph G with a source node s and a target node t. For each element ui in U, we add a node in G. Each ui is connected to t by an edge with probability p, such that 0<p<1. Then, we also add each Si in S as a node of G. Node Si is connected to node uj with probability 1 if and only if uj∈Si. All other edges in G except those from s to all Si have probability 0.

Thus, the candidate set of edges to add in G for maximizing reliability from s to t are those edges from s to all Si. Without them, there is no path from s to t with non-zero probability. Let ζ=1, after k of these edges are selected, q out of r elements in U are now reachable from s, then the s-t reliability =1−(1−p)q, which monotonically increases with larger q. This implies that Problem 1 and MAX k-COVER are equivalent here. If there exists a polynomial time solution for Problem 1, the MAX k-COVER can be solved in polynomial time too. The theorem follows.

Moreover, Problem 1 is also hard to approximate.

Theorem 2.
Problem 1 does not admit any PTAS, unless P =NP.

Proof.
A problem is said to admit a Polynomial Time Approximation Scheme (PTAS) if the problem admits a polynomial-time constant-factor approximation algorithm for every constant β∈(0,1). We prove the theorem by showing that one can find at least one value of β such that, if a β-approximation algorithm for Problem 1 exists, then we can solve the well-known SET COVER problem in polynomial time. Since SET COVER is NP-hard, this can happen only if P = NP.

In SET COVER, there is a collection of subsets S={S1,S2, …,Sh} of a ground set U={u1,u2,…,ur}, where Si⊆U for all i∈[1…h]. The decision version of SET COVER asks if there is a subset S∗⊂S of size k such that all elements in U can be covered by S∗.

Given an instance of SET COVER, we construct an instance of our problem in polynomial time by following the same method as in NP-hardness proof (Fig. 1). In the SET COVER instance, if there is a solution with k subsets, then the optimal solution OPT of our problem will add k edges such that the s-t reliability after edge addition is: 1−(1−p)r. This is because |U|=r. In contrast, if no solution with k subsets exists for SET COVER, then OPT will produce s-t reliability at most: 1−(1−p)r−1 (because at least one of uj would not be covered).

Let there be a polynomial-time β-approximation algorithm, Approx, for Problem 1, such that 0<β<1. Based on the definition of approximation ratio, Approx will produce s-t reliability at least β times to that produced by OPT. Now, let us consider the inequality: 1−(1−p)r−1<β[1−(1−p)r]. If this inequality has a solution for some values of β and p, then by simply running Approx on our instance of Problem 1, and checking the s-t reliability of the solution returned by Approx, one can answer SET COVER in polynomial time: a solution to SET COVER exists iff the solution given by Approx has s-t reliability ≥β[1−(1−p)r]. Thus, to prove the theorem, we require to show that a solution to that inequality exists.

Our inequality has a solution iff β>1−(1−p)r−11−(1−p)r. One can verify that 1−(1−p)r−11−(1−p)r<1, for all r≥1 and p>0. This implies that there will always be a value of β∈(0,1) and p for which β>1−(1−p)r−11−(1−p)r is satisfied, regardless of r. Hence, there exists at least one value of β such that the inequality [1−(1−p)r−1]<β[1−(1−p)r] has a solution, and, based on the above argument, such that no β-approximation algorithm for Problem 1 can exist. The theorem follows.

We further show that neither submodularity nor supermodularity holds for the objective function of Problem 1, and demonstrate with the following counter example. Therefore, standard greedy hill-climbing algorithms do not directly come with approximation guarantees for Problem 1.

Lemma 1.
The objective function of Problem 1 is neither submodular, nor supermodular w.r.t inclusion of edges.

For any set X⊆Y and all elements x∉Y, a set function f is submodular if f(X∪{x})−f(X)≥f(Y∪{x})−f(Y). For supermodularity, the inequality is reversed.

Let us consider the example in Fig. 2: s is the source node and t is the target nodes. Assume the node set V={s,A,t}. Let X={st}, Y={st,sA} be two edge sets. We have R(s,t,(V,X,p))=R(s,t,(V,Y,p))=0.5. We find that R(s,t,(V,X∪{At},p))=0.5, R(s,t,(V,Y∪{At},p))=1−(1−0.5)[1−0.52]≈0.63. Clearly, submodularity does not hold in this example.


Fig. 2.
Example for non-submodularity, non-supermodularity.

Show All

Next, considering X′={sA}, Y′={sA,st}, we have R(s,t,(V,X′,p))=0 and R(s,t,(V,Y′,p))=0.5. Then, R(s,t,(V,X′∪{At},p))=0.25, R(s,t,(V,Y′∪{At},p))≈0.63, thus supermodularity also does not hold.

2.3 Characterization of the Problem
We next show that the optimal solution to our problem varies based on most input parameters, even if the other set of input parameters remains the same, thereby making it non-trivial to utilize pre-existing solutions of past queries, as well as indexing-based or incremental methods.

Observation 1.
The optimal solution for Problem 1 may vary with different input probability threshold ζ.

Observation 2.
The optimal solution for Problem 1 may vary when the edge probabilities in the original graph change.

Observation 3.
When k1<k2, the optimal solution for Problem 1 with k1 may not be a subset of that with k2.

All these three observations can be demonstrated with the example given in Fig. 3, as follows.


Fig. 3.
Example for the problem characterization.

Show All

Example 1.
In Fig. 3, there are edges AB and At, both with probability α (0<α<1), in this graph. And the edge directly connecting s and t can not exist, e.g., no direct flight can be established between two airports if they are too far away. Clearly, original reliability between s and t is 0. {sA,sB,Bt} is the candidate set of edges to add for improving the s-t reliability.

If budget k=1, {sA} is always the optimal solution. Its reliability is αζ, which is larger than both α2ζ for solution {sB} and 0 for solution {Bt}.

If budget k=2, there are 3 possible solutions, {sA,sB}, {sA,Bt}, and {sB,Bt}. The reliability between s and t after adding them can be calculated as follows:
R(s,t,(V,E∪{sA,sB},p))=[1−(1−ζ)(1−α⋅ζ)]⋅αR(s,t,(V,E∪{sA,Bt},p))=ζ⋅[1−(1−α)(1−α⋅ζ)]R(s,t,(V,E∪{sB,Bt},p))=ζ⋅[1−(1−ζ)(1−α2)].
View SourceTable 2 presents the reliability of these solutions with different α and ζ. Clearly, rows 1 and 2 have same α and different ζ, and their optimal solutions are different: {sA,sB} and {sB,Bt}, respectively. This confirms our Observation 1. Similarly, we have same ζ but different α in rows 1 and 3, and obtain different optimal solutions. Therefore, we draw Observation 2. Moreover, {sA}, the optimal solution when k=1, is not a subset of the optimal solution {sB,Bt} when k=2 if α=0.5, ζ=0.7, which implies our Observation 3.

Finally, we conclude this section with an interesting observation below: The direct edge st, if missing in the input graph, will always be in the top-k optimal solution. In other words, when the direct st edge is missing and if it can be added, for the top-1 solution, adding the direct st edge is the best solution.

Observation 4.
If the direct edge from s to t, st, is missing in the input graph, and is allowed to be added, st will always be included in the top-k optimal solution.

Proof.
Let G be a possible world (i.e., deterministic graph) of the original uncertain graph G. Following Equation (2), the s-t reliability is calculated as: ∑G⊑G[IG(s,t)×Pr(G)]. After adding k missing edges, G will partition into 2k new possible worlds: {G1,G2, …,G2k}. Pr(G)=∑2ki=0Pr(Gi). Clearly, when t is reachable from s in G, it will still be reachable from s in each of {G1,G2,…,G2k}, thus IGi(s,t) will continue to be 1. Therefore, we only investigate those G containing no path from s to t, where the s-t reliability can be improved with new edges.

Suppose {e1,e2,…,ek} is an optimal solution without st. For any Gi in {G1,G2,…,G2k} obtained from some G, where t was originally not reachable from s, we consider another solution by replacing ej (1≤j≤k) with st. (1) If ej exists in Gi, IGi may or may not be 1. However, when replacing ej with st, IGi will always return 1, and improve the reliability; (2) If ej is absent in Gi, the value of IGi depends only on other edges in the solution set. Replacing ej with st will not impact the reliability. Therefore, replacing ej with st will result in a new solution which has reliability gain at least as large as the earlier one. This implies that st, if allowed, can always be added in the optimal solution.

SECTION 3Baseline Methods
In this section, we first present several baseline methods, that are straightforward, and demonstrate how they suffer from both effectiveness and efficiency issues. The discussions will be instrumental in developing a more accurate and scalable solution in Sections 4 and 5.

3.1 Individual Top-k Method
In the most straightforward approach, we consider every candidate edge one by one, check the reliability gain due to its addition in the input graph with probability ζ, and select the top-k edges with highest individual reliability gains.

Time Complexity. The reliability can be estimated in polynomial time via Monte Carlo (MC) sampling. It samples Z deterministic graphs from the input uncertain graph, and estimates the reliability of an s-t pair as ratio of samples in which the target is reachable from the source. The reachability in a deterministic graph can be evaluated via breadth first search (BFS) in time O(n+m), where n and m denote the number of nodes and edges in the input graph, respectively. Thus, the time complexity of MC sampling for each newly added edge is O(Z(n+m)). Since real-world networks are generally sparse, the number of candidate edges is nearly O(n2). Therefore, the overall complexity of individual top-k baseline is: O(n2Z(n+m)+n2logk), where the last term is due to top-k search.

Shortcomings. (1) To achieve reasonable accuracy, MC sampling requires around thousands of samples [2], [11]. Performing this for O(n2) times is not scalable for large graphs. (2) Once an edge is added into the input graph, the reliability gain of adding other candidate edges may change. Hence, selecting the top-k edges based on individual reliability gains results in low-quality solution.

Algorithm 1. Hill Climbing
Require: source node s, target node t in uncertain graph G=(V,E,p), a budget k for new edges, a probability threshold ζ.

Ensure: A set of k edges E1 (each with probability ζ) to add in G for maximizing the s-t reliability

Construct a set of candidate edges E+=V×V∖E, each with probability ζ

E1←∅

while |E1|<k do

e∗=argmaxe∈E+∖E1[R(s,t,(V,E∪E1∪{e},p))

−R(s,t,(V,E∪E1,p))]

E1←E1∪{e∗}

end while

return E1

3.2 Hill Climbing Method
A better-quality solution would be the hill climbing algorithm (Algorithm 1): It greedily adds the edge that provides the maximum marginal gain to the s-t reliability at the current round, until total k new edges have been selected. In particular, consider that a set E1⊆V×V∖E of new edges have been already included, in the next iteration the hill climbing baseline selects a new edge e∈V×V∖(E∪E1), with p(e)=ζ, such that
e∗=argmaxe∈V×V∖(E∪E1)[R(s,t,(V,E∪E1∪{e},p))−R(s,t,(V,E∪E1,p))].(4)
View SourceSince Problem 1 is neither submodular nor supermodular, this approach does not provide approximation guarantees.

Time Complexity. Assuming the number of missing edges to be O(n2), coupled with MC sampling, the time complexity of each iteration of this approach is O(n2Z(n+m)). For total k iterations, overall complexity is O(n2kZ(n+m)).

Shortcomings. Hill climbing also suffers from efficiency and accuracy issues. (1) This is more inefficient compared to individual top-k baseline. (2) For accuracy, hill climbing still suffers from the cold start problem: At initial rounds, there would be several new edges with marginal reliability gain zero (or, quite small), resulting in random selections, which in turn produces sub-optimal solutions at later stages.

3.3 Centrality-Based Method
Another intuitive approach is to find highly central nodes in the input graph, and connect them by new edges if they are not already connected, until the budget k on new edges is exhausted. In particular, we consider (1) degree centrality, that is, nodes having higher aggregated edge probabilities considering all incoming and outgoing edges; (2) betweenness centrality, that is, nodes having larger number of shortest paths passing through. Such nodes are also known as the hub nodes: Connecting these hub nodes help in reducing network distances (as well as improving reliability over uncertain graphs).

Time Complexity. For degree centrality, it requires going through all nodes and checking their in/out going edges, which costs O(m+n) time. To calculate the betweenness centrality of all nodes, Brandes’ algorithm [25] takes O(nm). Then, it ranks the nodes based on their aggregated edge probabilities, which consumes O(nlogn) time.

Shortcomings. Although the method (in particular, degree centrality) is efficient, and in general improves the s-t reliability, it is not customized for a specific s-t pair. This often results in low-quality solution.

3.4 Eigenvalue-Based Method
Wang et al. [26] studied the importance of the largest eigenvalue of graph topology in the dissemination process over real networks. To model the virus propagation in a network, they assumed a fixed infection rate β for an infected node to pass the virus to its neighbor, and another fixed curing rate δ for an infected node. Then, they proved that if βδ<1λ, where λ is the largest eigenvalue of the adjacency matrix of this network, the virus will die out in this network. Therefore, one can optimize the leading eigenvalue to control the virus dissemination in a network, e.g., with smaller λ, smaller curing rate δ is required for the same infecting rate β.

Recently, Chen et al. studied the problem of maximizing the largest eigenvalue of a network by edge-addition [16]. They proved that the eigenvalue gain of adding a set of k new edges E1 can be approximated by ∑ex∈E1u(ix)v(jx), where u and v are the corresponding left and right eigenvectors with the leading eigenvalue of the original adjacency matrix (ix and jx are the two end points of the new edge ex). They also proved that each ex in optimal E1 has left endpoint from the subset of (k+din) nodes with the highest left eigen-score u(ix), and right endpoint from the subset of (k+dout) nodes with the highest right eigen-score v(jx), where din and dout are the maximum in-degree and out-degree in the original graph, respectively. Therefore, one can find the optimal k new edges to increase the eigenvalue of the input graph by the following steps (Algorithm 2): First, calculate the largest eigenvalue of the input graph, and the corresponding left and right eigenvectors. Then, compute the maximum in-degree and out-degree of this graph, and find the subset of nodes I with top-(k+din) left eigen-scores and the subset of nodes J with top-(k+dout) right eigen-scores. Finally, connect the nodes from I to J (if no such edge exists in the original graph), and select the top-k pairs with largest eigen-scores u(ix)v(jx).

Algorithm 2. Eigenvalue-Based Method
Require: The adjacency matrix A of the input uncertain graph G=(V,E,p)

Ensure:A set of k edges E1 (each with probability ζ) to add in G for maximizing the largest eigenvalue of the input matrix A

Compute the largest eigenvalue λ of the input matrix A

Compute the maximum in-degree din and out-degree dout of the input graph G

Find the subset of nodes I with top-(k+din) left eigen-scores and the subset of nodes J with top-(k+dout) right eigen-scores.

Connect the nodes from I to J (if no such edges exists in G), which results in a set of edges E+

Select the top-k edges E1 in E+ with largest u(i)v(j), where u and v are the corresponding left and right eigenvectors with the leading eigenvalue λ of the original adjacency matrix A

return E1

Time Complexity. The first step can be solved with power iteration method in O(n) time. Finding maximum in/out degrees takes O(n+m) time, and finding subset I and J requires O(n(din+k)) and O(n(dout+k)) times, respectively, which can be written as O(nt), t=max(k,din,dout). The final step consumes O(kt2) time. Therefore, the overall time complexity is O(m+nt+kt2).

Shortcomings. (1) This method is not customized for a specific s-t pair, and may report low-quality solutions. (2) To the best of our knowledge, there is no equivalent transformation from virus propagation threshold βδ to the s-t reliability. Therefore, maximizing virus propagation may not be equivalent to maximizing the s-t reliability.

SECTION 4A Simplified Problem: Improve the Most Reliable Path
Due to limitations of baseline approaches as discussed in Section 3, we now explore an orthogonal direction following the notion of the most reliable path. The idea that we shall develop in this section will be the basis of our ultimate solution (to be introduced in Section 5) for the budgeted reliability maximization problem. A path between a source s and a target node t in an uncertain graph G is called the most reliable path MRP(s,t,G) if the probability of that path (i.e., product of edge probabilities on that path) is maximum in comparison with all other paths between these two nodes
MRP(s,t,G)=argmaxP∈P(s,t,G)∏e∈Pp(e),(5)
View SourceP(s,t,G) denotes the set of all paths from s to t in G. The problem that we investigate here is a simplified version of our original problem (i.e., Problem 1) as stated next.

4Problem 2 (Single-source-target most reliable path improvement).
Given an uncertain graph G=(V,E,p), a source s∈V, a target t∈V, a probability threshold ζ∈(0,1], and a small positive integer k, find the top-k edges to add in G, each new edge e having probability p(e)=ζ, such that the probability of the most reliable path from s to t in the updated graph is maximized
E∗=argmaxE1⊆V×V∖E∏e∈MRP(s,t,(V,E∪E1,p))p(e)s. t.|E1|=k;andp(e)=ζ∀e∈E1.(6)
View Source

Notice that the probability of the most reliable path from s to t cannot be larger than the s-t reliability. Thus, Problem 2 might be considered as a simplified version of the budgeted reliability maximization problem. Nevertheless, as reported in earlier studies [21], [22], the most reliable path often provides a good approximation to the reliability between a pair of nodes. Thus, our intuition is simple: If the solution of Problem 2 is more efficient and results in higher-quality top-k edges (compared to baselines for the original budgeted reliability maximization problem), then we can augment this idea (e.g., instead of the most reliable path, one may consider multiple highly-reliable paths from s to t) to develop even better-quality solution for the budged reliability maximization problem.

Fortunately, Problem 2 can be solved exactly in polynomial time. We shall provide a constructive proof, which can also be used as an algorithm for Problem 2.

4Theorem 3.
Problem 2 can be solved in polynomial time.

4Proof.
First, we color all existing edges in the input graph G as blue (Algorithm 3). Then, we add all missing edges to the graph, each with edge probability ζ (thus, resulting in a complete graph), and color new edges as red. Name this new graph as G¯¯¯. The goal of Problem 2 is to find the most reliable path from s to t containing at most k red edges (with zero or more blue edges), if any. Notice that we can convert the uncertain graph G¯¯¯ into an weighted graph G0, which has same set of edges and nodes as G¯¯¯, and the weight of each edge e in G0 is: w(e)=−logp(e). Equivalently, we aim at finding the shortest path from s to t in G0 containing at most k red edges (with zero or more blue edges), if any.

To find such paths, k identical copies of G0 are made (denoted as G0,G1,G2,…,Gk), and are updated as follows.

Remove all red edges from Gk.

For every 0≤i≤k−1

For every red edge ej=(va,vb) in Gi

Remove ej from Gi

Draw a new edge from va in Gi to vb in Gi+1

Now, we employ the Dijkstra's algorithm to find the shortest paths from s in G0 to every t in Gi (0≤i≤k). Each shortest path from s in G0 to t in Gi corresponds to a path in the original graph G¯¯¯ with at most i red edges . We refer to these paths (if they exist) as P0,P1,…,Pk, respectively.

Consider a function W that gets as input a path, and returns the aggregate weight of edges on that path. If for every 1≤i≤k, we have W(P0)≤W(Pi), then adding no k′≤k edges to G can improve the probability of the most reliable path from s to t. Otherwise, we find P=argmin1≤i≤kW(Pi), and consider all red edges in P. Adding these edges to G will result in the maximum probability of the most reliable path from s to t.

The time required for the above method is due to running the Dijkstra's algorithm for k+1 times over a graph with (k+1)n nodes and (k+1)n2 edges. Hence, the overall time complexity of our method is O(k2n2+k2nlog(kn)), which is polynomial in input size. The theorem follows.

SECTION Algorithm 3.Improve the Most Reliable Path
Require: source node s, target node t in uncertain graph G=(V,E,p), a budget k for new edges, a probability threshold ζ.

Ensure:A set of k edges E1 (each with probability ζ) to add in G for maximizing the probability of the most reliable path from s to t

Color all existing edges in G as blue

Add all missing edges to the graph, each with edge probablity ζ, and color them as red. The new graph is G¯¯¯

Convert G¯¯¯ into a weighted graph G0 by assigning weight w(e)=−logp(e)

Make k identical copies of G0: {G0,G1,…,Gk}

for j from k to 0 do

Remove all red edges from Gk

for Every 0≤i≤k−1 do

for Every red edge ej=(va,vb) in Gi do

Remove ej from Gi

Draw a new edge from va in Gi to vb in Gi+1

end for

end for

end for

Find the shortest paths {P0,P2,…,Pk} from s in G0 to every t in Gi(0≤i≤k)

P=argmin1≤i≤kW(Pi)

return The set of red edges on P as E1

Comparison With Baselines. As shown in Table 4, solving the simplified most reliable path problem (Problem 2) is much faster than both baselines: Individual Top-k and Hill Climbing for the original problem (Problem 1). As expected, the improvement in s-t reliability via most reliable path-based solution is 0.26, which is lower but comparable to that of Hill Climbing: 0.32. However, the most reliable path approach terminates in 467 seconds, while Hill Climbing consumes about 4.7 days. For other two baselines, Centrality-based and Eigenvalue-based, the most reliable path method significantly outperforms them in reliability gain.

SECTION 5Proposed Solution: Budgeted Reliability Maximization
In this section, we present our method ultimately designed for an effective and efficient solution to the single-source-target budgeted reliability maximization (Problem 1). Due to the success of the most reliable path technique as detailed in Section 4, our final solution is developed based on a similar notion by employing multiple reliable paths, and further improved in two ways: (1) Reduction of search space by identifying only the most relevant candidate edges for a given s-t pair, and (2) improving solution quality by considering multiple highly reliable paths from s to t. In the following, we discuss various steps of our framework, and demonstrate accuracy and efficiency improvements against previous baselines.

5.1 Search Space Elimination
5.1.1 Reliability-Based Search Space Elimination
In a sparse input graph G, one can have as many as O(n2) candidate edges. However, given a specific s-t query, all candidate edges may not be equally relevant. In particular, let us consider two nodes u and v: Both have low reliability either from source s, or to target t; then adding an edge between u and v will not improve the s-t reliability significantly. Therefore, we select “relevant” candidate edges as follows (Algorithm 4). (1) We find the top-r nodes with the highest reliability from s. Similarly, we compute the top-r nodes having the highest reliability to t. Let us refer to these sets as C(s) and C(t), respectively. Notice that s∈C(s) and t∈C(t). (2) For two distinct nodes u,v, such that u∈C(s), v∈C(t), and u,v are not connected in the input graph G, then we consider the new edge (u,v), with edge probability p(u,v)=ζ, as a candidate edge. We denote by E+ the set of relevant candidate edges. Thus, we reduce the number of candidate edges from O(n2) to only O(r2).

The time complexity of this step is O(Z(n+m)+nlogr+r2). The first term is due to MC sampling to compute the reliability of all nodes from s and to t, and the second term is due to sorting all nodes based on these reliability values.

Algorithm 4. Search Space Elimination
Require: source node s, target node t in uncertain graph G=(V,E,p), a number threshold r

Ensure: A set of edges E+ as the search space

Find the top-r nodes C(s) with highest reliability from the source node s

Find the top-r nodes C(t) with highest reliability to the target node t

E+←{(u,v)|u≠v,u∈C(s),v∈C(t),(u,v)∉E}

return E+

5.1.2 Top-l Most-Reliable Paths Selection
Given the success of the most reliable path-based approach (Section 4), we further improve it with multiple highly reliable paths. Recent research has shown that what really matters in computing the reliability between two nodes is the set of highly reliable paths between them [20], [21], [22].

On adding the relevant candidate edges E+, we refer to the updated graph as G+=(V,E∪E+,p). Next, we find the top-l most reliable paths from s to t with the Eppstein's algorithm [20], [27] within O(m+nlogn+l). If a new edge does not appear in any of these top-l paths, it can be removed from E+. This further reduces the search space.

Example 2.
Let us demonstrate “search space elimination” with Fig. 4


Fig. 4.
Run-through example for the proposed algorithm.

Show All

. Suppose we set r=3, l=3, and ζ=0.5. First, we select the top-3 nodes with highest reliability from source s. Clearly, {s,A,B} will be selected. Similarly, {B,C,t} are the top-3 nodes with highest reliability to target t. Node D, E, F, and G will be eliminated, and we obtain a graph presented in Fig. 4b. Then, we select top-3 most reliable paths between s and t after adding all missing edges (dotted lines) with given probability ζ=0.5 in Fig. 4b. They will be {sBt,sCBt,sCt} (in decreasing order). Node A does not appear in any of these paths, and will be eliminated. Finally, we have a simplified graph shown in Fig. 4c.
Benefits of Search Space Elimination. As shown in Table 5, our search space elimination methods can save about 99% of running time for the baselines: Individual Top-k and Hill Climbing without accuracy loss. For Centrality-based and Eigenvalue-based baselines, both efficiency and accuracy get improved, because these baselines are now applied over a smaller and more relevant (to a specific s-t pair) subgraph. After including the time cost for conducting search space elimination: 16 seconds, the overall running time for most reliable path method and our proposed algorithms can be reduced by over 70% without accuracy loss.

5.2 Top-k Edges Selection
Our next objective is to find the top-k edges from the reduced set E+ of candidate edges, so to maximize the s-t reliability. We formulate the problem as follows.

Problem 3 (Budgeted Path Selection).
Given the set P of the top-l most reliable paths from s to t in the updated graph G+, find a path set P∗⊆P such that
P∗=argmaxP1⊆PR(s,t,P1)s. t.|{e:e∈E+∩P1}|≤k.(7)
View Source

For Problem 3, R(s,t,P1) denotes the s-t reliability on the subgraph induced by the path set P1. In other words, we find a path set P∗⊆P that maximize the s-t reliability, while also satisfying the constraint on k, the number of newly-added edges. Unfortunately, this problem is NP-hard as well, which can be proved by a reduction from the MAX k-COVER. Since the proof is analogous to the one in Theorem 1, we omit this for brevity. Instead, we design two practical and effective solutions as given below.

5.2.1 Individual Path-Based Edge Selection
The algorithm is shown as Algorithm 5. First, we combine all paths from P that do not have any candidate edges from E+ (line 5). We refer to these paths as P1, and the subgraph induced by P1 as G∗. Then, in each successive round, we iteratively include a remaining path P∗ from P∖P1 into G∗ which maximally increases the reliability (estimated via MC-sampling) from s to t in G∗ (line 7), while still maintaining the budget k on the number of included candidate edges in G∗ . It can be formulated as
P∗=argmaxP∈P∖P1R(s,t,P1∪{P}).(8)
View SourceWe ensure that the number of included candidate edges from E+ in P1 does not exceed k via line 11-16. The included candidate edges in G∗ are reported as our solution.

Algorithm 5. Individual Path-Based Edge Selection
Require: source node s, target node t in uncertain graph G=(V,E,p), a budget k for new edges, a probability threshold ζ, a candidate number threshold r, a most reliable path number threshold l

Ensure: A set of k edges E1 (each with probability ζ) to add in G for maximizing the s-t reliability

Invoke Algorithm 4 to obtain the candidate edge set E+

G+=(V,E∪E+,p), each edge in E+ is assigned a probability of ζ

Find the top-l most reliable paths P from s to t in G+

E1←∅, P1←∅

Move those the paths which do not contain any edge in E+ from P into P1

while |E1|<k do

P∗=argmaxP∈P∖P1R(s,t,P1∪{P})

P1←P1∪{P∗}

Extract the set of edges EP∗ on path P∗

E1←E1∪(EP∗∩E+)

for P in P do

Extract the set of edges EP on path P

if |E1∪(EP∩E+)|>k then

Remove P from P

end if

end for

end while

return E1

Let us denote by n′ and m′ the number of nodes and edges, respectively, in the subgraph induced by the top-l most-reliable path set P, and T the number of MC samples required in each iteration. We need at most k iterations, thus the overall time complexity is O(kZ|P|(n′+m′)).

Algorithm 6. Path Batch Construction
Require: A set of most reliable paths P, a set of candidate edges E+

Ensure: A set of path batches PB

PB←∅

for Path P∈P do

Extract the set of edges EP on path P

Compute the label L=EP∩E+ for path P

if PBL∉PB then

PBL←∅

PB←PB∪{PBL}

end if

PBL←PBL∪{P}

end for

return PB

5.2.2 Path Batches-Based Edge Selection
The effectiveness of individual path selection can be improved by considering the relationships between paths in P. The intuitions are: (1) different paths can share same set of candidate edges; (2) the candidate edge set of a path can be a subset of that for another path; and (3) different paths may have different number of candidate edges to be included.

Therefore, we design a path batch-based (instead of individual path-based) edge selection algorithm. First, we go through all paths in P. If two paths share same set of candidate edges, they will be put into the same “path batch” (Algorithm 6). Each path batch is labeled by its candidate edge set (line 4, Algorithm 6), and in our algorithm we include a path batch in every round. In general, it follows the same procedure of Algorithm 5, but invokes Algorithm 6 after line 5. In all of follow-up steps of Algorithm 5, we shall select path batch PB instead of path P. When evaluating the marginal gain of a path batch, all other path batches whose candidate edge set is a subset of it, shall also be included in G∗ in the current round. The marginal gain of this batch is normalized by the size of its candidate edge set. The detailed procedure is shown in the following example.

Example 3.
Consider Example 2 and Fig. 4, we are now selecting top-2 edges from 3 candidate edges {sB,sC, Bt}. If selecting paths individually, path sBt has the highest marginal gain 0.25 and will be selected in the first round. As budget k=2 is exhausted, the solution set is {sB,Bt}. However, path sCt has reliability gain 0.15, and only adds 1 new edge. Its marginal gain per new edge is higher than that of sBt. Further, by considering it in batch path selection manner, including path sCBt will also activate path sCt The reliability gain of adding them in batch is 0.3075, and the marginal gain per new edge is 0.1538, which is the winner of this round, and we find the optimal solution {sC,Bt} in this example. The reliability gains for the other 2 possible solution are 0.28 for {sB,Bt}, 0.18 for {sB,sC}. This demonstrates the effectiveness of path batch selection procedure.

Benefits of Path Batches-Based Edge Selection. As shown in Table 5, path batch selection and Hill Climbing have similar reliability gain, while path batch selection consumes significantly less running time. Comparing with individual path inclusion, path batch selection has some improvement in reliability gain, with comparable running time.

5.3 Improvement via Advanced Sampling
Recently, several advanced sampling methods have been proposed for estimating s-t reliability, including lazy propagation [28], recursive sampling [2], recursive stratified sampling (RSS) [19], and probabilistic tree [29]. While our problem and the proposed solution are orthogonal to the specific sampling method used, its efficiency can further be improved by employing more sophisticated sampling strategies [30]. In particular, instead of MC sampling, we shall consider RSS in the experiments, both for our proposed method and for the baselines. The detailed description and the analysis can be found in our extended version [31].

SECTION 6Multiple-Source-Target Reliability Maximization
In practice, queries may consist of multiple source and/or target nodes, rather than a single s-t pair. For example, in targeted marketing [22], [23] via social networks, the campaigner wants to maximize the information diffusion from a group of early adopters to a set of target customers. For such real-world applications, we extend our problem to adapt to multiple source/target nodes. In particular, we focus on maximizing an aggregate function (e.g., average, maximum, minimum) over reliability of all s-t pairs.

6Problem 4 (Multiple-source-target budgeted reliability maximization).
Given an uncertain graph G=(V,E,p), a set of source nodes S⊂V, a set of target nodes T⊂V, a probability threshold ζ∈(0,1], and a small positive integer k, find the top-k edges to add in G, each new edge having probability p(e)=ζ, such that an aggregate function F over reliability of all s-t pairs (s∈S, t∈T) is maximized
E∗=argmaxE1⊆V×V∖EFs,t∈S×T(R(s,t,(V,E∪E1,p)))s. t.|E1|=k;andp(e)=ζ∀e∈E1.(9)
View Source

Due to NP-hardness of Problem 1, its generalization, Problem 4 is also NP-hard. In the following sections, we consider three widely-used aggregate functions: average, minimum, maximum; and design efficient solutions.

6.1 Maximizing the Average Reliability
Our objective is
argmaxE1⊆V×V∖E1|S||T|∑s,t∈S×TR(s,t,(V,E∪E1,p)).(10)
View SourceNote that this is equivalent to maximizing the sum of reliability of all s-t pairs. For targeted marketing in social networks, a campaigner would like to maximize the spread of information to the entire target group; and therefore, she would prefer to maximize the average reliability.

Similar to the single-source-target budgeted reliability maximization problem, we first compute the reliable sets from source and target nodes, that is, C(s) for all s∈S, and C(t) for all t∈T. Next, for each pair of distinct nodes u,v, such that u∈C(s),∀s∈S, v∈C(t),∀t∈T, and u,v are not connected in the input graph G, we consider a new edge (u,v), having edge probability p(u,v)=ζ, as a relevant candidate edge. We denote by E+ the set of relevant candidate edges, and after adding them to G, we refer to the updated graph as G+=(V,E∪E+,p).

Now, for each s-t pair, we identify the top-l most reliable paths in G+. Then we have total |S||T|l paths in this set, and the path set might contain more than k new edges. Therefore, we employ the path batches-based edge selection method (Section 5.2.2): The algorithm iteratively includes path batches that maximize the marginal gain considering our current objective function (Equation (10)), while maintaining the budget k on the number of newly inserted edges.

Time Complexity. Let O(P1) denote the time complexity of reliability-based search space elimination, O(P2) denote that of top-l most-reliable paths selection, and O(P3) denote that of path batches-based edge selection, for the single-source-target case. The time complexity of the proposed algorithm for average multiple-source-target budgeted reliability maximization problem is O((|S|+|T|)P1+|S||T|(P2+P3)). We need to evaluate all nodes’ reliability from/to each source/target, which results in the first term. The second term is due to applying top-l path selection algorithm |S||T| times for each s-t pair, and the path set size will be |S||T| times of that for single-source-target problem.

6.2 Maximizing the Minimum Reliability
Our objective is
argmaxE1⊆V×V∖Emins,t∈S×TR(s,t,(V,E∪E1,p)).(11)
View SourceIn other words, we aim at including k new edges such that the reliability of the s-t pair having the lowest reliability (after the addition of k edges) is maximized. In the targeted marketing setting, this can happen during complementary influence maximization [32], where multiple products are being campaigned simultaneously, and they are complementary in nature: Buying a product could boost the probability of buying another. Now, consider that each source node (e.g., an early adopter) is campaigning a different, but complementary product. The campaigner would prefer to maximize the minimum spread of her campaign from any of the early adopters to any of her target users, because only a small percentage of the users who have heard about a campaign will buy the corresponding product.

To solve this problem, we first estimate the s-t reliability for each pair in S×T over the input graph G. We sort these s-t pairs in ascending order in a priority queue based on their current reliability. Next, in successive rounds, we keep improving the reliability of the pair having the smallest current reliability, until the budget on k new edges can be exhausted. In particular, at any point in our algorithm, we know which s-t pair has the minimum reliability. We extract this pair from the top of the priority queue, and improve its reliability with the addition of a batch of suitable, new edges. For this purpose, we employ our algorithm for the single-source-target pair (discussed in Section 5). The batch size can be set as k1<<k. Note that the addition of new edges not only updates the reliability of the current pair, instead this will also increase the reliability of other s-t pairs.

Thus, after adding a batch of k1 new edges, we re-compute the reliability of all s-t pairs and re-organize them in the priority queue. Once again, we extract the pair from the top of the priority queue, and improve its reliability with the addition of k1 suitable, new edges. We repeat the above steps. Ultimately, we terminate the algorithm when we exhaust our budget of adding total k new edges.

Time Complexity. The time complexity of our algorithm is O((|S|+|T|)P1+kk1AP1+kk1P2+P3), A=min(|S|,|T|). Similar to maximizing the average reliability, we need evaluating all nodes’ reliability from/to each source/target. All s-t pair's original reliability can also be known through this process. However, after improving the reliability of the currently selected s-t pair by adding k1 edges, we need to update the reliability of all s-t pairs. This will happen kk1 times, and the cost is kk1AP1, A=min(|S|,|T|). The top-l paths selection will be operated kk1 times. The complexity of executing top-k1 edge selection is O(k1kP3), and it will happen kk1 times. Thus, the total time cost for edge selection remains O(P3).

6.3 Maximizing the Maximum Reliability
Our objective function is
argmaxE1⊆V×V∖Emaxs,t∈S×TR(s,t,(V,E∪E1,p)).(12)
View SourceIn the targeted marketing scenario, let us again consider complementary influence maximization [32], where each source user (e.g., an early adopter) is campaigning a different, but complementary product. However, each target user is now a celebrity in Twitter. Hence, the campaigner wants at least one target user to be influenced by one of her products. In other words, the campaigner would be willing to maximize the spread of information from at least one early adopter to at least one target customer.

Note that if S∩T≠ϕ, the problem is trivial, as the maximum reliability is already one. Therefore, below we consider the case when S∩T=ϕ. A straightforward solution to our problem would be to separately consider each s-t pair from S×T, improve its reliability by adding k new, suitable edges. Then, we pick the pair having the maximum final reliability, and report those k new edges that were selected for this s-t pair. However, the time complexity of this approach is O(|S||T|) times to that of a single s-t pair.

Next, we develop a more efficient algorithm without significantly affecting the quality. Our approach is similar to that of maximizing the minimum reliability (discussed in Section 6.2). In each round, we maximize the reliability (by adding k1<<k new edges) of the pair having the current maximum reliability. After this, we re-compute the reliability of all pairs, and again pick the one which has the current maximum reliability. We terminate the algorithm when we exhaust our budget of adding total k new edges.

Time Complexity. The time complexity will be the same as that of maximizing the minimal reliability, which is O((|S|+|T|)P1+kk1AP1+kk1P2+P3), A=min(|S|,|T|).

SECTION 7Related Work
Network Design Problems. Network design, optimization, and modification are widely studied research topics, where one modifies the network structure or attributes, targeting at some objective metrics or functions.

There exist many different metrics to characterize the “goodness” of the network, including average shortest paths [33], [34], ratio of connected nodes [35], relative size of the largest connected component and average size of other components [33], network flow and delay [36], centrality [37], average path length [38], and spectral measures [16]. Spectral measures are derived from the adjacency and the Laplacian matrices of a graph. For example, [16] optimized the leading eigenvalue of a network by edge addition/deletion, due to the finding that the leading eigenvalue of the underlying graph is the key metric in determining the so-called “epidemic threshold” for a variety of dissemination models [26]. However, such global metric is not query-specific. In real-world, users may tend to optimize the network in a way that is relevant only to themselves, e.g., a campaigner would like to improve the influence [39] of her product to her target customers, but not that of all similar products (from other competitors), and neither to other users who are not her targets. Moreover, many network metrics studied in the past cannot be easily generalized to probabilistic scenarios (e.g., connected component size). Our objective, reliability, is a fundamental metric to capture the probability that a given target node is reachable from a specific source node in an uncertain graph. Furthermore, we show that it is possible to generalize our objective to multiple-source-target cases in order characterize a larger region in the network.

The major network manipulation operations include node addition/deletion [33], [34], edge addition/deletion [16], [37], [39], edge rewiring [34], and updating edge weights [36]. Our goal is to improve the reliability between s-t pairs in a network: In our application scenarios, adding new edges is usually more practical. For example, it is often not realistic to set up a new airport only to improve the reliability of connections between two existing airports, rather establishing some new flights is much easier. In this paper, we study the problem of maximizing the reliability between a given pair of nodes by adding a small number of new edges. Altering the existing edge probabilities is not investigated here, and can be an interesting future research direction on this problem.

Reliability in Uncertain Networks. Due to the #P-hardness of s-t reliability estimation problem, various efficient sampling approaches have been proposed in the literature. Monte Carlo (MC) sampling [18] is a fundamental approach, which samples Z possible worlds from the input uncertain graph, and approximates the s-t reliability with the ratio of possible world in which t is reachable from s. One may combine MC sampling with BFS from the source node to further improve its efficiency [2]. [28] proposed Lazy Propagation, which utilizes geometric distribution to avoid frequent probing of edges. BFSSharing improves the efficiency with offline indexes. Recursive sampling [2] and recursive stratified sampling [19] reduces the estimator variance by recursively partitioning the search space. Less samples are required for them to achieve the same variance as previous methods, thereby improving the efficiency. More recently, ProbTree index [29] was designed to support faster s-t reliability queries over uncertain graphs. Our problem and the proposed solution are orthogonal to the specific sampling method used, we have demonstrated in Section 5 and in our extended version [31] that its efficiency can be improved by employing recursive stratified sampling.

Orthogonal directions to our problem include adaptive edge testing [40] and crowdsourcing [41] for reducing uncertainty. In this work, we focus on improving s-t reliability by adding a limited number of new edges.

SECTION 8Experimental Results
We perform experiments to demonstrate effectiveness, efficiency, scalability, and memory usage of our algorithms. We report sensitivity analysis by varying all input parameters in this section. The code is implemented in C++, executed on a single core, 40GB, 2.40 GHz Xeon server.

8.1 Experimental Setup
Real-World Datasets. We use 5 real-world graphs, consisting of 3 social and 2 device networks (Table 6). (1)Intel Lab Data (http://db.csail.mit.edu/labdata/labdata.html). It is a collection of sensor communication data with 54 sensors deployed in the Intel Berkeley Research Lab between February 28th and April 5th, 2004. (2) LastFM (www.last.fm). It is a musical social network, where users listen to musics, and share them with friends. An edge between two users exists if they communicate at least once. (3) AS_Topology (http://data.caida.org/datasets/topology/ark/ipv4/). An autonomous system (AS) is a collection of connected Internet Protocol (IP) routing prefixes under the control of one or more network operators on behalf of a single administrative entity, e.g., a university. The AS connections are established with BGP protocol. It may fail due to various reasons, e.g., failure when one AS updates its connection configuration to ensure stricter security setting, while some of its peers can no longer satisfy it, or some connections are cancelled manually by the AS administrator. We downloaded one network snapshot per month, from January 2008 to December 2017. (4) DBLP (https://dblp.uni-trier.de/xml/). It is a well-known collaboration network. We downloaded it on March 31, 2017. Each node is an author and edges denote their co-author relations. (5) Twitter (http://snap.stanford.edu/data/). This is a widely used social network: Nodes are users and edges are re-tweets.

Synthetic Datasets. In order to study the effects of network properties to the performance of our algorithms, we generate 8 synthetic datasets, with the help of NetworkX package (https://networkx.github.io). They can be categorized into following 4 kinds of networks, each having 2 instances with different number of edges. (1) Random. We generate the random networks with the well-known Erdős-Rényi model, that is, the edge between every pair of nodes exists with a fixed proabablity p. Here, the p value for Random_1 is 5×10−6, and the one for Random_2 is 1×10−5. The random network tends to have a small average shortest path length, and a small clustering coefficient. The degrees of nodes in general follow Poisson distribution. (2) Regular. We adopt the k-regular networks model here. In a k-regular network, every node has same degree of k. In particular, the k value for Regular_1 is 5, and the one for Regular_2 is 10. In a regualr network, the average shortest path length is usually high, and the clustering coefficient is also high. (3) SmallWorld. In a small-world network, most nodes are not neighbors of one another, but the neighbors of any given node are likely to be neighbors of each other, and most nodes can be reached from every other node by a small number of hops or steps. Thus, it always has a small shortest path length, and a high clustering coefficient. The widely-used Watts-Strogatz model for generating small-world networks starts with a k-regular lattice, and re-writes its edge connections with probability p to obtain a small-world graph. We adopt k=5 and k=10 for our two instances, respectively. The p is set to 0.3. (4) ScaleFree. A scale-free network is a network whose degree distribution follows a power law. It usually has relatively smaller shortest path length, and higher clustering coefficient. Our scale-free networks are generated with Barabási-Albert preferential attachment model, where a graph of n nodes is grown by attaching new nodes each with m edges that are preferentially attached to existing nodes with high degree. For ScaleFree_2, the m is set to be 5. In order to maintain a consistent number of edges as all previous networks, we make slight change in the source code to allow alternating m=2 and m=3 during the generation. Real-world social networks are mostly both small-world and scale-free.

Edge Probability Models. Our problems and solutions are orthogonal to the specific way of assigning edge probabilities. We adopt some widely-used models for generating edge probabilities in our evaluation. (1) Intel Lab Data and (3) AS_Topology. The edge probabilities in these two datasets are real probabilities. For Intel Lab Data, the probabilities on edges denote the percentages of messages from a sender successfully reached to a receiver. For AS_Topology, once an AS connection (i.e., an edge) is observed for the first time, we calculate the ratio of snapshots containing this connection within all follow-up snapshots as the probability of the existence for this edge. (2) LastFM. The probability on any edge is the inverse of the out-degree of the node from which that edge is outgoing. (4) DBLP and (5) Twitter. We assign the edge probability following 1−e−t/μ, which is an exponential cdf of mean μ to a count t [2]. In DBLP, t denotes the count of the collaborations between two authors. In Twitter, t is the count of re-tweet actions. We set μ=20. (6) Synthetic Datasets. For 4 kinds of synthetic datasets, we assign a probability for each edge uniformly at random from range (0,0.6].

Queries. For single-source-target queries, we select 100 different s-t pairs from each dataset. In practice, if two nodes are too close to each other, their original reliability will be naturally high; thus, it might be unnecessary to improve their reliability further. Thus, we select a source node uniformly at random, and find all its neighbors within 3-5 hops. A target node is chosen from those neighbors randomly. The sensitivity analysis about the distance between s and t is provided in Section 8.2

For multiple-source-target queries, we first generate a single-source-target query s-t. Then, for that s, we find all its neighbors that are within 5-hops away, and randomly select q of them into the source set S (i.e., source set size=q). Similarly, we pick q of the within 5-hop neighbors of t as the target set T, uniformly at random. We ensure that the source set and the target set do not overlap. Finally, 100 different source-target sets are generated.

Parameters Setup. (1) Budget on #new edges(k). For single s-t queries, we vary k from 5 to 50, and use 10 as default. For multiple-source-target queries, we vary k from 10 to 500, use 100 as default. (2) Probability on new edges (ζ). We vary ζ from 0.3 to 0.7, and use 0.5 as default. (3) Number of candidate nodes (r). We vary r from 20 to 300, and use 100 as default. (4) Number of most−reliable paths (l). We vary l from 10 to 50, and use 30 as default. (5) #Sources and #targets. For multiple-source-target queries, we vary source and target set sizes from 3 to 500. (6) The ratio of k1k. For multiple-source-target case with Max and Min aggregate functions, we further have a parameter k1 as the budget for the current selected pair. We vary k1 from 5 to 30 percent of k, and use 10 percent as default. (7) Distance constraint for new edges. In practice, some missing edges cannot be candidate edges due to physical constraints. For example, in our case study in Introduction, only short distance connections (i.e., <15 meters) are allowed to be established. In social networks, if two users have no common friends, it is unlikely that they will start communicating. In current experiments, we assume that a missing edge can be added only if its two endpoints are ≤h-hops in the input graph. We vary h from 2 to 5, and use 3 as default. For real-world applications, one can easily set this constraint based on her requirements.

Competing Methods. For single-source-target query, our ultimate method: path batches-based edge selection (BE) is compared with individual path-based edge selection (IP), most reliable path (MRP), and our best baseline: Hill Climbing (HC). For baselines, the reliability gain is our major concern. Although HC is not efficient, it outperforms others in reliability gain (Tables 4 and 5).

For multiple-source-target case, we employ Hill Climbing (HC), Eigenvalue-based Optimization (EO) [16], and two more recent methods, ESSSP [37] and IMA [39], as competitors. Both ESSSP and IMA follow the same manner of adding a budget of new edges into the graph, each with a fixed probability. The former aims at reducing the sum of expected shortest path length of each source-target pair, while the later attempts to increase the influence spread of the source nodes in the target nodes.

Moreover, on the smallest dataset, Intel Lab Data, we have the exact solution (ES) as a competitor, which enumerates all possible combinations of k missing edges, and find the cone with highest reliability gain. All of them are coupled with our search space elimination strategy (Section 5.1.1) and an advanced sampling method: RSS (Section 5.3).

Performance Metrics. (1) Reliability gain. We compute reliability gain due to k new edges for each pair of source and target nodes, and report the average reliability gain over 100 distinct s-t pairs. (2) Running time. We report the end-to-end running time, averaged over 100 queries. (3) Memory usage. We report the average memory usage of running each query.

8.2 Single-Source-Target Results
Comparison With the Exact Solution. The exact solution (ES) enumerates all possible combinations of k missing edges, and finds the one with the highest reliability gain. The number of missing edges can reach O(n2) in sparse graph, and results in (n2k) possible choices, which makes it extremely inefficient in larger graphs. However, due to the small size of Intel Lab Data, we can apply such exhaustive search, and empirically compare with our proposed solution, BE. We follow the setting used in our case study in Section 8.4.1, that is, only 3 new short distance (≤15 meters) links are allowed to be established, each have the average probability 0.33. 30 distinct pairs of sensors, which are remote and with lower original reliabilities, are selected as queries.

As shown in Table 9, our proposed solution, BE, exhibits very close performance against the exact solution (ES) in reliability gain. It returns same set of edges as ES, in 25 out of 30 queries. However, the running time of BE are at least three orders of magnitude faster than ES. This demonstrates both the effectiveness and the efficiency of our methods.

Comparison of All Competing Methods on Different Real Datasets With Default Parameters. In Table 7, we present the reliability gain obtained by four methods, and the corresponding running time and memory usage, on various datasets with default parameters. Clearly, our ultimate method, path batches-based edge selection (BE) outperforms others. For reliability gain, it wins on all datasets. On Twitter, the advantage of BE is more prominent. The reason is that Twitter is a sparser graph compared to other datasets, and the highly reliable paths connecting source to target are more likely to contain more than one missing edges — this fact enhances the impact of path batches. Individual path-based edge selection (IP) always has lower reliability gain compared to BE. The polynomial-time solution, MRP for the restricted version of our problem has the lowest reliability gain among these methods, as expected.

Considering the running time, IP is the best one. However, BE is only about 10-20 seconds slower than IP across all the datasets. Both of them are about an order of magnitude faster than the baseline HC. The memory usages of IP and BE are similar, while MRP costs slightly less memory.

Comparison of All Competing Methods on Different Synthetic Datasets With Default Parameters. Similar to previous part, we present the results on synthetic datasets in Table 8. Our ultimate method, BE, still outperforms others on all datasets. For reliability gain, it first confirms our finding on the real datasets that the reliability gain tends to be higher on sparser graphs, regardless of the kind of network. Further, it can be observed that we can achieve higher reliability gain on regular networks. It is well-known that establishing a few short cut edges can sharply reduce the average shortest path length in a network, and transforming it gradually into a small-world graph. The original path length is higher in a regular graph, which allows more for improvement.

The running time on random graphs is the highest, while that of regular graphs is lowest. The top-r candidate nodes, C(s) and C(t), tend to be farther from s or to t on regular graphs. Since our s-t query pairs are 3-5 hops away, C(s) and C(t) will have more overlap on regular graphs, which can reduce the number of candidate edges. Furthermore, clustering coefficients are high on regular graphs, thus more edges have existed from C(s) to C(t). This again reduces the number of candidate edges. These are the reasons for the smaller running time of our methods on regular graphs. The random graphs have the contrary properties, therefore it is slower to excute our algorithms there.

Varying the Budget k on #New Edges. We present the results on LastFM and DBLP datasets in Tables 10 and 11, respectively. The reliability gain tends to increase with larger k. Such growth is more significant when k is small, for example the reliability gain increases from 0.27 to 0.33 when k increases from 5 to 10, while only 0.02 increase can be obtained when permitting k from 20 to 30, on LastFM. The reliability gain nearly saturates at k=20 on DBLP. On both datasets, BE outperforms others in reliability gain, no matter how large is k. The reliability gain of MRP converges at the beginning, because we only consider the most reliable path in this restricted version. A path containing larger number of new edges tends to have longer length and lower probability, thus it is unlikely to be the most reliable path.

For running time, MRP, IP, and BE are comparable, and all of them can finish within 200 seconds with the largest k=50. HC is ≈100× slower. The running time of MRP increases faster than IP and BE with larger k, since it requires k copies of the original graph to find the most reliable paths with exactly 0 to k missing edges, although this does not help improve the solution quality in practice.

Varying Probability ζ on New Edges. The experimental results on AS_Topology and Twitter are provided in Tables 12 and 13, respectively. The reliability grows almost linearly with the probability threshold ζ. Sometimes, the growth rate may be even higher (e.g., on Twitter). The reason is that the optimal solution set of edges may change with different ζ (Observation 1), and a sharp increase may happen when shifting from a set of edges to another (Example 1). The running times of all the methods are not sensitive to different ζ. However, with larger ζ, the running time slightly increases.

Table 14 provides additional analysis about the probabilities on new edges. Here, instead of a fixed threshold ζ, we allow different probabilities on different new edges. Probabilities on new edges are generated uniformly at random in different range, or generated following normal distribution N(0.5,0.038) (99 percent of value generated are in range (0,1)). It can be viewed that the results are very similar to all our previous study with fixed threshold ζ. This confirms that our proposed algorithm, BE works well even when different probabilities for the missing edges are provided as input.

Varying #Candidtae Nodes (r). In reliability-based search space elimination, we only keep the top-r nodes C(s) with the highest reliability from s, and the top-r nodes C(t) with the highest reliability to t. C(s) and C(t) are candidate node sets, and only those missing edges from a node in C(s) to a node in C(t) will be considered as candidate edges. As demonstrated in Table 16, small r incurs low-quality result, due to the excessive elimination. The accuracy does not keep improving if r exceeds 80 and 100, respectively for LastFM and DBLP. We find out that r=100 is sufficient for all the methods to work on all datasets in our experiments.

TABLE 15 Reliability Gain and Running Time Comparison With Varying #Candidate Nodes rr

TABLE 16 Reliability Gain and Running Time Comparison With Varying #Candidate Nodes rr

Time 1 denotes the time cost for search space elimination, and Time 2 is the time cost for top-k edges selection. As shown in Tables 15 and 16, when varying r, Time 1 increases sharply with larger r. Although the time cost of checking all nodes’ reliability from/to a node is not relevant to r, we need to add at most O(r2) missing edges after determining the candidate nodes. Since we shall also verify the distance constraint before adding a missing edge, the time cost of adding edges is non-trivial. When r≤100, the increasing rate for the running time of search space elimination is modest. Together with the previous finding that r=100 can ensure a good accuracy, we set r=100 as default in other experiments.

The running times for top-k edge selection (Time 2) for methods IP and BE increase little with larger r, since they estimate the reliability gain of missing edges only on the subgraph induced by a few most-reliable paths. Time 2 for MRP, on the other hand, increases linearly with r, since the size of each copy of graph is linear to r. The time cost of edge selection by HC is also linear to r at the beginning, and slows down with larger r. This is because that, although the time complexity of sampling is linear to the graph size theoretically, when coupling with BFS search, low-reliability nodes added later are less frequent to be explored during the sampling.

Varying Distance (d) Between Query Nodes s and t. We further select queries where each s-t pair are exactly d-hops away in the input graph. As shown in Table 17, the original reliability decreases with larger d. And the reliability gain at d=3 and d=4 is about the highest, for both HC and BE.

TABLE 17 Reliability Gain and Running Time Comparison With Varying Distance dd Between Query Nodes

The running time is small either with too large or too small d. For small d, the candidate node sets C(s) and C(t) are likely to have a large overlap, thus less missing edges are found. However, the distance between nodes in C(s) and C(t) tends to increase with larger d, thus the distance constraint may forbid many missing edges from being added into the graphs. The running time of HC is more sensitive to d, since it iterates over each new edge.

Varying Distance Constraint (h) for Newly Added Edges. We constrain that a missing edge can only be added if the distance between its two endpoints in the original graph is at most h hops. Smaller h prevents more edges from being added. As shown in Table 18, with larger h, we can obtain more reliability improvement. However, this allows many remote links to be established, which may not be realistic in practice. Moreover, many candidate edges also increase the running time, both for HC and BE.

Varying #Most Reliable Paths (l). Table 19 demonstrate the sensitivity analysis of our IP and BE methods to the number of most reliable paths, l. The reliability gain increases with larger l, and saturates at around l=30. The running time is linear to l. Thus, we set l=30 as default in the rest of our experiments.

Scalability Analysis. We conduct scalability analysis of our method, BE by varying the graph size on the largest dataset, Twitter. We select 1M, 2M, 3M, 4M, 5M, and 6M nodes uniformly at random to generate 6 subgraphs, and apply our algorithm on them. Table 20 shows that the running time and the memory usage are both linear to the graph size, which confirms good scalability of BE.

TABLE 18 Reliability Gain and Running Time Comparison With Varying Distance Constraint hh for New Edges

TABLE 19 Reliability Gain and Running Time Comparison With Varying #Most-Reliable Paths ll

TABLE 20 Scalability Analysis of BE

8.3 Multiple-Source-Target Results
Varying #Source-Target Nodes. We present the results on the largest dataset, Twitter, in Tables 21, 22, and 23, for the aggregate functions: Minimum, Maximum, and Average, respectively. The baselines ESSSP and IMA can also run in single-source-target scenario, and we keep their results in the first row of Table 23. Our purposed method, BE, significantly outperforms the baselines, HC, EO, ESSSP, and IMA in reliability gain, and runs at least 40× and 2× faster than HC, with Average and Minimum/Maximum aggregate functions, respectively. The running times of EO, ESSSP, IMA, and BE are comparable. In general, the running time of our method, BE is almost linear to the number of source/target nodes.

TABLE 21 Reliability Gain and Running Time Comparison for Multiple-Source-Target Pairs

TABLE 22 Reliability Gain and Running Time Comparison for Multiple-Source-Target Pairs

TABLE 23 Reliability Gain and Running Time Comparison for Multiple-Source-Target Pairs

Furthermore, our method results in higher reliability gain with all 3 aggregate functions when comparing with EO, especially for Minimum and Maximum. This is because EO is not query-specific. EO optimizes the leading eigenvalue of a graph, which is a global metric and may have little to do with the query pair having Minimum or Maximum reliability. The performance of IMA algorithm is closed to our method with Average aggregate function, since its objective, influence spread, can be considered as a variant of average aggregated reliability (see Section 8.4.2). When we have only one source-target pair, the objective of IMA becomes exactly the same as ours, which results in the same reliability gain, as shown in Table 23. The performance of ESSSP is always worse than our method.

Varying the Budget k on #New Edges. Similar to single-source-target case, we vary k, now in a larger scale: 10 to 500, and present the result in Fig. 5. The reliability gains for all three aggregate functions increase with larger k. The running time of BE with Minimum/Maximum aggregate function is less sensitive to a larger k, since the complexity of their top-k edge selection part remains the same as single-source-target case, while the search space elimination part scales up. On the contrary, the running time of BE with Averageis almost linear to k. However, Average is still less time consuming than Minimum/Maximum with large k.


Fig. 5.
Reliability gain and running time comparison with varying budget on #new edges k. ζ=0.5, r=100 l=30, #Sources=#Targets=100, Twitter.

Show All

8.4 Case Study and Application
We demonstrate the effectiveness of our proposed methods via a case study about improving the s-t reliability in a sensor network, and an application of maximizing average multi-source-target reliability in influence maximization.

8.4.1 Case Study in Sensor Network
The Intel Lab Data (http://db.csail.mit.edu/labdata/labdata.html) dataset contains the sensor network information with 54 sensors deployed in the Intel Berkeley Research Lab (map given in Figs. 6 and 7) between February 28th and April 5th, 2004. The probabilities on links denote the percentages of messages from a sender successfully reached to a receiver. The average link probability is 0.33 (ignoring edge probabilities which are lower than 0.1).


Fig. 6.
Improving the reliability from sensor 21 (right) to 46 (left) with 3 new links (marked by dotted lines).

Show All


Fig. 7.
Improving the reliability from sensor 15 to 40 (on the diagonal) with 3 new links (marked by dotted lines).

Show All

Assume that our goal is to maximize the reliability from: (1) a sensor on the right hand side of the lab to a sensor on the left hand side (e.g., from sensor 21 to 46, with original reliability 0.40); (2) between two sensors on the diagonal of the lab (e.g., from sensor 15 to 40, with original reliability 0.28). Due to budget constraints, only 3 new links are allowed for each case. We further assume that the probability of each new link would be the same as the average edge probability of the original dataset, that is, 0.33. We notice that if two sensors are more than 20 meters away, the original link probability between them is usually close to 0. Thus, we only allow establishing new links between a pair of sensors that are at most 15 meters away.

Fig. 6 demonstrates the solution obtained by our algorithm for case (1). Only those links with probabilities higher than the average value (0.33) are shown in the figure, and the thickness represents link probabilities. Clearly, sensor 46 has very weak connections from outside, while sensor 21 is connected with the bottom part of the lab, where a very dense network exists. Therefore, our solution for this case is to connect sensor 46 with sensors in the bottom part of the lab. By establishing three links 2 to 46, 35 to 46, and 37 to 46, we improve the reliability between 21 to 46 from 0.40 to 0.88.

For case (2), notice in Fig. 7 that the sensors in the center part of lab are well-connected, and the links in this region are thicker than those in the bottom part. The source sensor 15 has a few connections with sensors in the bottom part, but no link with those in the center part. The destination sensor 40 has limited connections beyond its physical neighbors. Existing configuration offers a poor reliability of 0.28 for the connection between source and destination which we like to improve. The smart decision made by our algorithm is as follows: First, connect sensor 35 to 40, thus making sensor 35 a bridge between the center and the bottom region of the network; Second, enable connection from sensor 15 to the center part (by establishing link from 15 to 10, and from 15 to 11). This results in 0.58 overall reliability from sensor 15 to 40, which is more than double of the original reliability value. These results illustrate how our proposed solution for the budgeted reliability maximization problem can be useful in solving real-life problems.

8.4.2 Application in Influence Maximization
In social influence maximization following the widely-used independent cascade model [11], when some node u first becomes active at step t, it gets a single chance to activate each of its currently inactive out-neighbors v at step t+1, with probability p(u,v). Initially, only the source nodes are active, and the activation continues in discrete steps. When no more nodes can be activated, the number of active nodes in target set is referred to as the influence spread. With possible world notation, the influence spread from source set S to target set T can be formulated as
Inf(S,T)=∑G⊑G[Pr(G)∑t∈TIG(S,t)].(13)
View SourceAs discussed in Section 6.1, the average reliability from S to T is
Ravg(S,T)=1|S||T|∑G⊑G[Pr(G)∑s,t∈S×TIG(s,t)].(14)
View SourceClearly, in each possible world, if we only check whether there is at least one path to t from any s∈S, instead of counting the exact number of s∈S which has a path to t, our problem becomes equivalent to the (targeted) influence maximization problem. Adding a new edge in this network implies recommending and/or establishing collaboration with an author in the real-world [42].

In DBLP dataset, we select a set of junior researchers in Databases area, containing 1,000 authors randomly selected from all the authors with 1-3 papers in [SIGMOD, VLDB, ICDE]. Similarly, we choose 50 senior researchers with more than 10 papers in [SIGMOD, VLDB, ICDE], uniformly at random. The expected influence spread from the senior to the junior group is around 462, using the IC model. Next, we aim at maximally improving the influence spread from the senior group to the junior group, by adding up to 100 new edges. As shown in Fig. 8, our method outperforms Eigenvalue-based optimization (EO) [16], and results in about 326 more influenced authors within the junior set.


Fig. 8.
Influence spread comparison in Databases area. ζ=0.5, r=100, l=30, k1k=10%, DBLP.

Show All

SECTION 9Conclusion
In this paper, we introduced and investigated the novel and fundamental problem of maximizing the reliability between a given pair of nodes in an uncertain graph by adding a small number of edges. We proved that this problem is NP-hard and also hard to approximate. Several interesting observations are presented to characterize it. Our purposed solution first eliminates the search space based on original reliability, and then selects the top-k edges following an iterative most-reliable path-batches inclusion algorithm. We further studied one restricted and several extended versions of the problem, to support a wider family of queries. The experimental results validated the effectiveness, efficiency, and scalability of our method, and rich real-world case studies demonstrated the usefulness of our budgeted reliability maximization problem. In future, a total reliability budget on new edges, instead of a fixed/ individual budget on each new edge, can be considered. This will add more complexity on selecting proper candidate edges and allocating reliability budget to them.