Subsequence clustering is an important issue in time series data mining. Observing that most time series consist of various patterns with different unknown lengths, we propose an optimization framework to adaptively estimate the lengths and representations for different patterns. Our framework minimizes the inner subsequence cluster errors with respect to subsequence clusters and segmentation under time series cover constraint where the subsequence cluster lengths can be variable. To optimize our framework, we first generate abundant initial subsequence clusters with different lengths. Then, three cluster operations, i.e., cluster splitting, combination and removing, are used to iteratively refine the cluster lengths and representations by respectively splitting clusters consisting of different patterns, joining neighboring clusters belonging to the same pattern and removing clusters to the predefined cluster number. During each cluster refinement, we employ an efficient algorithm to alternatively optimize subsequence clusters and segmentation based on dynamic programming. Our method can automatically and efficiently extract the unknown variable-length subsequence clusters in the time series. Comparative results with the state-of-the-art are conducted on various synthetic and real time series, and quantitative and qualitative performances demonstrate the effectiveness of our method.
SECTION 1Introduction
Time series clustering is an important task in the field of time series data mining [1], [2]. Without any labels on the time series, it can extract interesting patterns and provide valuable information for various applications, such as business, engineering and medical [3]. Time series clustering is also critical to be employed in many other tasks, such as outlier detection [4] and segmentation [5] in time series.

Time series clustering is generally divided into two different sub-tasks [6]. The whole time series clustering is to cluster each complete time series in the time series database into different groups and the subsequence clustering is to cluster subsequences in a time series into different groups. In contrast to the whole time series clustering, subsequence clustering needs first to extract subsequences and then employ clustering on them. Though extensively studied in the past decade [7], there are still challenging issues in subsequence clustering, such as subsequence clustering for unknown variable-length patterns in time series.

The difficulty of subsequence clustering for variable-length patterns lies in the adaptive subsequence extraction where accurate subsequence length settings, subsequence localization and segmentation are required to reveal different patterns in time series. Inappropriate subsequence extraction will produce undesired results in subsequence clustering as pointed in [8] and [9]. Taking outlier detection based on subsequence clustering [4] as an example, if the subsequence length is set too small and accordingly the pattern subsequences in the time series are inaccurately extracted, then some pattern information is lost and this may result in the indistinguishability between the outliers and the normals. Otherwise, if the subsequence length is set too large, the subsequences will contain both the patterns and some additional disturbances that may identify the normals as outliers.

Traditional subsequence clustering methods extract subsequences with fixed length by sliding window and then apply clustering methods on the extracted subsequences. For example, Kmeans [10], hierarchical clustering [11] and self-organizing maps [12] are all employed to cluster the highly overlapping subsequences via sliding window. These methods generally produce meaningless results due to plenty of trivial matches between neighboring subsequences in sliding window subsequence extraction.

To exclude the trivial matches in subsequence extraction for clustering, motif discovery methods are proposed to extract similar distant subsequences for fixed or variable lengths. In [8], Keogh et al. extract motifs with a fixed subsequence length. Then, Nunthanid et al. [13] extract all the motifs with various subsequence lengths by enumeration and estimate the most significant motifs in each overlapping motif group. The motif enumeration is later speed up by using a novel bound on the similarity function across subsequence lengths in [14] and by introducing lower bound distance profile in [15] and [16] which can effectively and efficiently extract interesting patterns. Blalock and Guttag [17] also extract motifs with unknown but roughly constrained subsequence length by identifying sets of features that repeat in the same temporal arrangement. Rakthanmanon et al. [18] extract motifs under the minimal description length principle to save the bits of subsequence clusters to represent the time series. For motif discovery methods, their aims are to find non-trivial similar patterns in the time series. If we need to cluster the whole time series into different subsequence patterns, the extracted motifs may not represent the whole time series and thus may not produce the desired clusters. Besides, the motifs with various lengths are usually enumerated with some heuristic rules, it would be better if a principle framework can be adopted.

Besides motif discovery methods, there are some other subsequence clustering methods based on time series segmentation. Toeplitz inverse covariance-based clustering method [19] is proposed to represent the subsequences in a fixed length window by a MRF model for subsequence segmentation and clustering. Then, context-aware segmentation and clustering method [5] is further proposed to incorporate the MRF model in an optimization framework to simultaneously optimize the subsequence clusters and segmentation. For these methods, the time series are required to satisfy some assumptions. For example, the subsequences in the same segmentation are assumed to be randomly generated from a gaussian distribution with a Toeplitz inverse covariance-based kernel in [19] and [5].

In this paper, we propose a novel adaptive subsequence clustering model to cluster the unknown variable-length subsequence patterns. In our model, we formulate the subsequence clustering as an optimization framework with respect to the subsequence clusters and segmentation. To optimize our model, three cluster operations, i.e., cluster splitting, combination and removing, are adopted to adjust the lengths and representations for different clusters. When the clusters are adjusted, an efficient algorithm based on dynamic programming is employed to alternatively optimize the subsequence clusters and segmentation. Our optimization method can progressively refine the subsequence clusters and finally converge to estimate subsequence clusters with suitable lengths.

Our main contributions are listed below:

We propose an optimization framework for unknown variable-length subsequence clustering. Our model can automatically estimate the lengths and representations of variable-length patterns in the time series. This makes it suitable to various practical applications where only the cluster numbers and the ranges of subsequence cluster lengths are provided.

We propose an efficient algorithm to optimize our model via three cluster operations, i.e., cluster splitting, combination and removing, by progressively refining the subsequence clusters during the iteration. While cluster splitting splits subsequence clusters containing different patterns, cluster combination joins neighboring clusters belonging to the same pattern and cluster removing reduces the cluster number to the predefined value. During the cluster refinement, we also propose an efficient algorithm to alternatively optimize subsequence clusters and segmentation based on dynamic programming.

The reminder of our paper is organized as follows: In Section 2, a novel framework for unknown variable-length subsequence clustering is introduced and the corresponding optimization algorithm is described in Section 3. Our experimental results and performance analysis are reported in Section 4. Finally, our study is concluded in Section 5.

SECTION 2Our method
In this section, we first describe the motivation for variable-length subsequence clustering. Then, our adaptive subsequence clustering framework is accordingly constructed and analyzed in details.

2.1 Preliminaries
In this subsection, formulation and description of the problem will be displayed. Subsequence clustering is to cluster subsequences in a time series into some groups with each group representing a pattern. Specifically, given a time series X1:T with time step T, the aim is to extract k clusters C={Ci,i=1,…,k} to represent different patterns. The task can be solved by minimizing an objective function f(X1:T,C) under some constraints where f(X1:T,C) evaluates the quality of subsequence clusters. For each cluster Ci, the cluster center μi and the subsequences x∈ Ci are estimated together with the subsequence length li as illustrated in Fig. 1.

Fig. 1. - 
Subsequence clustering. Given a time series $X_{1:T}$X1:T, subsequence clustering estimates cluster center $\boldsymbol{\mu ^{i}}$μi and the subsequences $\mathrm {x}\in$x∈ $C^{i}$Ci together with the subsequence length $l_{i}$li for each cluster $C^{i}$Ci.
Fig. 1.
Subsequence clustering. Given a time series X1:T, subsequence clustering estimates cluster center μi and the subsequences x∈ Ci together with the subsequence length li for each cluster Ci.

Show All

Generally, the cluster center μi, subsequences x∈ Ci and length li are all unknown for each i=1,…,k. They couple together making the subsequence clustering in time series challenging. In this paper, we propose a novel framework to simultaneously estimate them.

2.2 Motivation
In many situations, time series contain various patterns with different lengths, e.g., temperature time series generated in the processes of quick heating, slow heating, quick cooling and slow cooling in air conditioner. One toy example to illustrate our motivation is shown in Fig. 2 where the time series consists of two patterns with different lengths. For ideal subsequence clustering in Fig. 2, the two patterns should be adaptively estimated as the subsequence clusters.

Fig. 2. - 
Our motivation. Time series usually consist of various patterns with different lengths. Subsequence clustering should adaptively extract the lengths and representations of various patterns.
Fig. 2.
Our motivation. Time series usually consist of various patterns with different lengths. Subsequence clustering should adaptively extract the lengths and representations of various patterns.

Show All

There are some interesting observations in Fig. 2. One is that only one fixed subsequence length is not suitable to simultaneously represent both pattern 1 and pattern 2. If the subsequence length is set to the length of pattern 1, then the subsequences of pattern 2 with larger length will be split and thus some information of pattern 2 is lost. Otherwise, if the subsequences length is set to the length of pattern 2, then subsequences of pattern 1 will contain both pattern 1 and some other parts from pattern 1 or pattern 2, and thus some disturbances exist for pattern 1. Therefore, the subsequence lengths in Fig. 2 should not be fixed but adaptively estimated.

The second observation is that accurate time series segmentation is critical for subsequence clustering. Even when the different subsequence lengths in Fig. 2 are known but the time series are not accurately segmented, i.e., some segmented subsequences are only parts of a pattern or combinations of two patterns, the clustering on these segmented subsequences will still not produce satisfying results.

The third observation is that subsequence splitting and combination are essential for accurate time series segmentation. If a subsequence consists of parts of two different patterns, it should be split into two subsequences. Besides, if two subsequences are next to each other and belong to the same pattern, they should be combined into one. Only with such splitting and combination, accurate segmentation and thus desired subsequence clustering can be obtained.

Based on our observations, we propose a novel framework to simultaneously optimize the subsequence clusters and segmentation to adaptively extract the variable-length patterns in the time series.

2.3 Adaptive Subsequence Clustering Model
Let X be a time series with length T, our aim is to estimate the unknown variable-length subsequence clusters C and the subsequence partition Z in X for subsequence clustering. To this end, we propose an adaptive subsequence clustering model by minimizing the following objective function f(C,Z) under the time series cover constraint:
minC,Zf(C,Z)=∑i=1k∑x∈Ci||x−μi||2,s.t.∪x=X,lmin≤|μi|≤lmax,(1)
View SourceRight-click on figure for MathML and additional features.where k is the subsequence cluster number, Ci is the ith subsequence cluster, μi is the cluster center of Ci with length |μi| bounded by lmin and lmax, Z={x|x∈Ci,i=1,…,k} is the subsequence partition set of X where x∈Ci indicates that x is a subsequence belonging to cluster Ci, ∪x=X is the time series cover constraint that every point of X must be in at least one subsequence x.

In our model (1), the objective function is to minimize the inner cluster errors. Thus, similar subsequences will group together to form a cluster. Three points make our model different from existing subsequence clustering models. First, the subsequence cluster lengths are unknown and generally different in our model. Therefore, our model can extract the variable-length subsequence patterns in Fig. 2.

Second, our model introduces the subsequence partition Z to be optimized. As explained in Section 2.2, accurate subsequence segmentation is critical to produce the desired subsequence clusters. Therefore, our model simultaneously optimizes the subsequence clusters and subsequence partition. The subsequence partition Z in our model can be highly irregular, i.e., only the subsequence length constraint and time series cover constraint are enforced, to match various patterns in time series.

Third, our model is optimized under time series cover constraint. In the constraint, each point of X is only required to exist in one of partitioned subsequences. This implies that our model can naturally exclude the trivial matches in the sliding window subsequence extraction. Therefore, we can avoid the disadvantage of sliding window based methods and do the similar work like motif discovery methods to find non-trivial similar subsequences in a different way.

Specifically, in model (1), if all the subsequences x are pre-extracted with the same fixed length, our model then becomes the classical Kmeans algorithm [20].

SECTION 3Optimization
There is no closed-form solution to directly optimize our model (1) due to simultaneously estimating lengths and representations of subsequence clusters, and subsequence partition Z under additional constraints. Directly checking all possible subsequence combinations to form clusters is obviously computationally prohibitive due to combinatorial explosion. Instead, we optimize it in two nested routines.

In the inner routine, we optimize (1) by an alternative algorithm based on dynamic programming under fixed subsequence cluster lengths and cluster number for further cluster refinement in outer routine. In the outer routine, we extend the inner routine by progressively refining subsequence cluster lengths and cluster number via three cluster operations, i.e., cluster splitting, combination and removing, to adaptively estimate the subsequence clusters. In the following subsections, we first describe the inner routine without cluster splitting, combination and removing. Then, we illustrate the outer routine to optimize our model (1).

3.1 Optimization Without Cluster Splitting, Combination and Removing
In (1), when the cluster number k and subsequence cluster length |μi| are known and fixed for each cluster Ci(i=1,…,k), then the constraint lmin≤|μi|≤lmax will be replaced by |μi|=li where li is fixed. In such cases, there is an efficient algorithm to alternatively optimize Z and C in (1). The whole algorithm is summarized in Algorithm 1.

Algorithm 1. Subsequence Clustering Algorithm Without Cluster Splitting, Combination and Removing
Input: time series: X1:T, initial cluster centers: C0, max iteration: maxIter, minimal threshold: ϵ;

Output: subsequence clusters C, subsequence partition Z;

for n=1:maxIter do

Given Cn−1, estimate Zn by (2), (3) and (4);

Given Zn, update Cn by (5);

if d(Cn−1,Cn)≤ϵ then

Break;

end if

end for

Return Cn,Zn;

Optimizing Z Given C. Given subsequence clusters C={μi,i=1,…,k}, the corresponding subsequence partition Z can be efficiently and globally optimized by dynamic programming. Let l0 be the minimal cluster length in C, the initial subsequence partition Zl0 and model loss Dl0 at position l0 are determined by
Zl0=X1:l0,Dl0=mini||X1:l0−μi||2,s.t.,|μi|=l0.(2)
View SourceRight-click on figure for MathML and additional features.Let Zp and Dp be the optimal subsequence partition and model loss at position p=1,…,t−1, then the optimal Zt and Dt at position t are computed by the following equations to minimize Dt under time series cover constraint:
Dt=DJ+||Xt+1−|μI|:t−μI||2Zt={ZJ,Xt+1−|μI|:t},(3)
View SourceRight-click on figure for MathML and additional features.where
I,J=argmini=1:k,j=t−|μi|:t−1Dj+||Xt+1−|μi|:t−μi||2(4)
View SourceRight-click on figure for MathML and additional features.

In (3) and (4), we obtain the optimal partition Zt by checking all the subsequence partitions to cover the time series X1:t, and the subsequence partition with the minimal model loss is chosen. When t=T, the optimal partition Z=ZT is then obtained for the whole time series X1:T.

Optimizing C Given Z. Given partition Z, the global minimization of C is obtained by the following equations:
Ci={x|d(x,μi)≤d(x,μj),∀x∈Z,|μj|=|μi|=|x|}μi=1|Ci|∑x∈Cix.(5)
View SourceThat is, for each partitioned subsequence x, we estimate its corresponding cluster label by computing its distance to the clusters, and then subsequently update the clusters.

Convergence. In Algorithm 1, the subsequence partition Z is globally optimized given C in (2), (3) and (4), and the subsequence clusters C are globally optimization given Z in (5). Thus, our objective function (1) decreases in each iteration of subsequence clusters and partition optimization, and finally converges to a local minimum in Algorithm 1.

Algorithm 1 is only applicable when the subsequence cluster lengths are given. However, for our model (1), the subsequence lengths are not known in advance. Thus, we extend Algorithm 1 to progressively refine subsequence clusters under unknown subsequence lengths via three cluster operations, i.e., cluster splitting, combination and removing. The algorithm is summarized in Algorithm 2.

In Algorithm 2, we first give a proper subsequence cluster initialization by providing plenty of initial clusters for different fixed lengths. Then, we optimize our model to generate new clusters with more suitable lengths and representations by cluster splitting and combination under fixed cluster number. After optimization at a fixed cluster number, we remove one cluster and begin new optimization at smaller cluster number until to the predefined number. Finally, we obtain the adaptive subsequence clusters with different lengths. The more details of our Algorithm 2 are described in the following subsections.

Algorithm 2. Adaptive Subsequence Clustering Algorithm in a Time Series
Input: time series: X1:T, cluster number: k, minimal and maximal subsequence lengths: lmin,lmax;

Output: subsequence clusters C, subsequence partition Z;

oldC=Initialization(X,k,lmin,lmax);

oldK=ClusterNumber(oldC);

oldZ=SubsequencePartition(oldC);

oldLoss=ComputeLoss(X,oldC,oldZ);

while oldK>=k do

C=ClusterSplit(X,C);

C=ClusterCombine(X,C);

Z=SubsequencePartition(C);

curLoss=ComputeLoss(X,C,Z);

curK=ClusterNumber(C);

if curK==oldK and curLoss>oldLoss then

% Converge if loss increases at fixed cluster number

tmpC=RemoveOneCluster(X,C);

curK=ClusterNumber(tmpC);

if curK>=k then

C=tmpC;

else

Break;

end if

else if curK>oldK then

% Remove additional cluster due to cluster splitting

tmpC=RemoveOneCluster(X,C);

tmpZ=SubsequencePartition(tmpC);

tmpLoss=ComputeLoss(X,tmpC,tmpZ);

if tmpLoss<=curLoss then

C=tmpC;

else

C=RemoveOneCluster(X,C);

end if

end if

oldC=C;

oldK=ClusterNumber(oldC);

oldZ=SubsequencePartition(oldC);

oldLoss=ComputeLoss(X,oldC,oldZ);

end while

Z=SubsequencePartition(C);

Return C,Z;

**************************************************************

ComputeLoss(): estimated by (1);

SubsequencePartition(): estimated by (2), (3) and (4);

Initialization(): estimated in Section 3.2;

ClusterSplit(): estimated in Section 3.3;

ClusterCombine(): estimated in Section 3.4;

RemoveOneCluster(): estimated in Section 3.5;

3.2 Cluster Initialization
A proper initialization is necessary in Algorithm 2 because our model (1) is locally optimized. With subsequence lengths unknown and different, initialization should contain clusters of different lengths which may finally produce satisfying local or even global optimization. Thus, we greedily extract clusters for each fixed length from the minimal subsequence length to the maximal subsequence length.

For each fixed length l, subsequence clusters are estimated by minimizing the following objective function which is optimized by Algorithm 1:
minC,Z∑i=1k∑x∈Ci||x−μi||2,s.t.∪x=X,|μi|=l(6)
View SourceRight-click on figure for MathML and additional features.

All the extracted subsequence clusters with different fixed lengths are gathered to form the initial clusters. There is large redundancy in such initial clusters. However, applying Algorithm 1 on initial clusters can reduce the redundancy because some clusters are unnecessary to minimize the objective function (1) and thus are removed naturally.

Note that it is usually not necessary to extract all the subsequence clusters for each fixed length between the minimal and maximal subsequence length. Our cluster splitting and combination operations can adjust the cluster lengths to produce new clusters with more suitable lengths.

3.3 Cluster Splitting
Cluster splitting is necessary when a cluster contains different pattern parts. As illustrated in Fig. 3, cluster μi contains both the parts of pattern 1 and pattern 2. In such case, we split μi into two clusters to extract different pattern parts.

Fig. 3. - 
Cluster splitting. Cluster $C^{i}$Ci contains parts from both pattern 1 and pattern 2, it should be split to extract different pattern parts.
Fig. 3.
Cluster splitting. Cluster Ci contains parts from both pattern 1 and pattern 2, it should be split to extract different pattern parts.

Show All

There are two issues in cluster splitting. One is which cluster to split. We choose the cluster with maximal inner cluster error to split because it is likely to contain different pattern parts. Another is how the chosen cluster is split. For each feasible split position p in the chosen cluster μi where 1≤p≤|μi|−1, we compute the model loss by replacing μi by μi1:p and μip+1:l according to Algorithm 1. The position p with the minimal model loss is used to split the chosen cluster.

After cluster splitting, one cluster is split into two clusters to separate different pattern parts. These pattern parts are then joined together for further refinement by cluster combination.

3.4 Cluster Combination
Cluster combination is useful when two neighboring clusters belong to the same pattern. As illustrated in Fig. 4, both the clusters Ci and Cj belong to pattern 1. It is reasonable to combine them together.


Fig. 4.
Cluster combination. Two neighboring clusters Ci and Cj belong to the same pattern 1, and they should be joined into one cluster.

Show All

There are also two issues in cluster combination. One issue is which cluster pair to combine. Let P1(Ci|Cj) be the probability of cluster Cj where the pre-cluster is Ci, P2(Ci|Cj) be the probability of cluster Ci where the next cluster is Cj. The two probabilities are computed as:
P1(Ci|Cj)=nijni,P2(Ci|Cj)=nijnj,(7)
View SourceRight-click on figure for MathML and additional features.where ni is the subsequence number of cluster Ci and nij is the number of subsequence pairs of cluster Cj next to Ci. As illustrated in Fig. 4, the neighboring clusters Ci and Cj in the same pattern almost appear together indicating large P1(Ci|Cj) or P2(Ci|Cj). Let P(Ci|Cj)=max(P1,P2). If P(Ci|Cj) comes from P1, then we combine Ci and Cj to replace Cj. Otherwise, if P(Ci|Cj) comes from P2, we combine Ci and Cj to replace Ci.

Another issue is how to combine two clusters. Generally, cluster Ci and Cj to be combined are overlapped. Thus, we can not directly join them together to generate a new cluster. Similar to cluster splitting, we search for the combination position p with the minimal model loss. For a combination position p, the new cluster is generated by averaging Ci and Cj at corresponding positions, and the model loss is obtained by optimization Algorithm 1.

3.5 Cluster Removing
In (1), we need to extract the predefined k subsequence clusters. In cluster initialization, we provide much larger number of initial clusters than k. In cluster splitting and combination, cluster number will not naturally decrease to k too. Thus, in Algorithm 2, we gradually reduce the cluster number by cluster removing until to the predefined k.

We apply cluster removing in two cases in Algorithm 2. One is when our optimization algorithm converges under some fixed cluster number, we need to remove one cluster to begin new optimization at smaller cluster number. Another is when the cluster number increases by one in cluster splitting, we remove one cluster to continue the optimization at the fixed cluster number. Cluster removing ensures the cluster number decreasing in Algorithm 2. For cluster removing, we will remove the cluster with the minimal loss of our model (1) if it is removed by Algorithm 1.

Fig. 5 illustrates one example of our optimization process in Algorithm 2. The input time series in Fig. 5a is generated by randomly concentrating two patterns with different lengths. In cluster initialization in Fig. 5b, six clusters are extracted with some pattern fragments, such as the pink and green clusters, which are automatically combined in Fig. 5c. Then, the black and greed clusters in Fig. 5c are further refined by cluster combination and splitting to generate more suitable clusters in Fig. 5d. In Fig. 5d, the blur cluster is split into two clusters in Fig. 5e in which the green cluster are further split and finally generates satisfying results with two patterns in Fig. 5f.

Fig. 5. - 
Optimization process in Algorithm 2. (a) Input time series, (b) cluster initialization, (c)-(f) clustering results at different cluster number during optimization. Different colors in (b)-(f) represent different clusters. As the cluster number decreases by cluster removing, the subsequence clusters are progressively refined by cluster splitting and combination. Finally, satisfying results are obtained in (f).
Fig. 5.
Optimization process in Algorithm 2. (a) Input time series, (b) cluster initialization, (c)-(f) clustering results at different cluster number during optimization. Different colors in (b)-(f) represent different clusters. As the cluster number decreases by cluster removing, the subsequence clusters are progressively refined by cluster splitting and combination. Finally, satisfying results are obtained in (f).

Show All

Convergence. In Algorithm 2, our model (1) is gradually optimized from the large initial cluster number to the predefined cluster number. At each fixed cluster number, we have proved the convergence of the optimization without cluster splitting, combination and removing in Section 3.1. For cluster splitting and combination, we will only continue the optimization when the model loss decreases. When the loss increases, we begin a new optimization at smaller cluster number by cluster removing. Thus, our model loss decreases at each fixed cluster number and will finally converge when the predefined cluster number is reached.

3.6 Computational Complexity
We estimate space and time complexity for Algorithms 1 and 2. In Algorithm 1, we only need to store the input time series X1:T, the k cluster centers and the partitioned subsequences Z, thus the space complexity is O(T+klmax+Tlmax). In Algorithm 1, the main computation cost in optimizing Z given C is to compute (4) for t=1,…,T which in the worst case is O(Tklmax), the main computation cost in optimizing C given Z is to estimate the cluster labels which in the worst case is O(Tklmax) where maximal T partitioned subsequences of length lmax are used to estimate the k cluster labels. Both the time and space complexity of Algorithm 1 are linear to time series length T.

In Algorithm 2, we only need to store the input time series X1:T, at most (lmax−lmin)k cluster centers with maximal length lmax and the partitioned subsequences Z, thus the space complexity is O(T+kl2max+Tlmax). The time complexity consists of computation costs on cluster initialization, splitting, combination and removing. For cluster initialization, computation cost for each fixed length l is O(Tkl) according to Algorithm 1, thus the whole cost for l=lmin,…,lmax is O(Tkl2max). For cluster splitting, the main cost is to estimate the model loss for the most lmax split positions by Algorithm 1 which has at most (lmax−lmin)k clusters, thus the cost in the worst case is O(lmaxT(lmax−lmin)klmax), i.e., O(Tkl3max). For cluster combination, the main cost is to estimate the model loss at most 2lmax−1 combination positions given combined cluster pair by Algorithm 1 which has at most (lmax−lmin)k clusters, thus the cost is O((2lmax−1)T(lmax−lmin)klmax), i.e., O(Tkl3max). For cluster removing, at most (lmax−lmin)k clusters are evaluated for removing by Algorithm 1, thus the cost in the worst case is O((lmax−lmin)kT(lmax−lmin)klmax), i.e., O(Tk2l3max). Thus, the time complexity of Algorithm 2 in the worst case is O(Tkl2max+Tkl3max+Tkl3max+Tk2l3max). Both the space and complexity of Algorithm 2 are linear to time series length T.

SECTION 4Experiments
In this section, we demonstrate our method on synthetic and real time series. First, the comparative methods and evaluation methodology are described. Then, quantitative and qualitative comparisons are conducted and analyzed. Finally, the robustness of our model parameters is test and some issues of our method are discussed.

4.1 Comparison Methods and Evaluation Methodology
In the experiments, we compare with three kinds of subsequence clustering methods. The first kinds are the methods based on sliding window subsequence extraction with different clustering methods on them. The second kinds are motif extraction methods under different principles to extract similar interesting patterns which is also the aim of subsequence clustering. The third kinds are the methods based on time series segmentation to segment the time series into clusters to reveal different patterns in it. Though some different from traditional subsequence clustering, the motif extraction methods and segmentation methods also aim to extract patterns in time series and thus are compared with our method by evaluating the extracted patterns. The comparison methods are listed below:

Kmeans—standard Kmeans algorithm with sliding window subsequences

DTWGAK—dynamic time warping (DTW)-based clustering method using a global alignment kernel with sliding window subsequences [21]

NeuralGas—artificial neural network clustering method based on self-organizing maps with sliding window subsequences [22]

Kmotif—standard k-motif extraction method [8]

MDLKmotif—k-motif extraction method based on minimal description length [18]

VLmotif—variable-length k-motif extraction method by enumeration [13]

VALMOD—variable-length k-motif extraction method based on lower bound distance profile [15]

EXTRACT—semi-supervised repeated event discovery method for time series [17]

TICC—Toeplitz inverse covariance-based clustering method for time series [19]

CASC—context-aware segmentation and clustering method for time series [5]

where Kmeans, DTWGAK and NeuralGas are sliding window subsequence extraction based methods, Kmotif, MDLKmotif, VLmotif, VALMOD and EXTRACT are motif extraction methods, TICC and CASC are time series segmentation based methods. For all these methods, the parameters are set default in the original implementation by the authors except for cluster numbers and subsequence lengths and length ranges which are set the same as ours if necessary. Specifically, the cluster numbers and subsequence lengths of Kmeans, DTWGAK and NeuralGas, the subsequence lengths of Kmotif, the subsequence length ranges of MDLKmotif, VLmotif, VALMOD and EXTRACT, the cluster numbers and the subsequence length ranges of TICC and CASC, are all the same to ours.

Quantity metrics and qualitative performances are used for comparisons. For quantity metrics, we compute the cluster assignment error between the estimated time series cluster labels and the ground truth cluster labels. For time series X1:T, let the ground truth and estimated cluster label for position i=1,…,T are Ai and Bi respectively, then the cluster assignment error err is defined as
err=∑Ti=11Ai≠BiT,(8)
View SourceRight-click on figure for MathML and additional features.where 1(.) is the indication function. The smaller cluster assignment error is, the better result is obtained. Cluster assignment error is used for all the methods except for Kmotif, MDLKmotif, VLmotif, VALMOD and EXTRACT which do not produce cluster labels for time series.

For qualitative performances, the subsequence cluster centers, subsequence cluster samples and subsequence segmentation are visually evaluated. For our method, all the three visual performances can be obtained and evaluated. For sliding window subsequence clustering methods, i.e., Kmeans, DTWGAK and NeuralGas, the subsequence cluster samples are highly overlapped and can be readily indicated by subsequence segmentation, thus only the subsequence cluster centers and segmentation are evaluated. For Kmotif, MDLKmotif, VLmotif, VALMOD and EXTRACT, their aims are to extract similar subsequences and do not produce segmentations, thus only the subsequence cluster centers and samples are evaluated. For TICC and CASC, they do not actually produce the traditional subsequence cluster centers, thus only the subsequence segmentations are evaluated.

4.2 Results on Synthetic Time Series
First, we evaluate our method on synthetic time series with the ground truth cluster labels. The first synthetic time series is constructed by randomly concentrating three different patterns of lengths 10, 15 and 30 with small noise on them in Fig. 6a. By setting the cluster number to 3, the minimal and maximal subsequence lengths to 5 and 60 respectively, our method accurately estimates the different pattern lengths and representations in Fig. 6c. With the adaptively extracted subsequence clusters, we obtain the ideal subsequence clustering results in Fig. 6d.

Fig. 6. - 
Adaptive subsequence clustering on a synthetic time series. (a) Three ground truth subsequence patterns with lengths of 10, 15 and 30, (b) test time series generated by randomly concentrating the three patterns in (a), (c) our cluster centers, (d) subsequence clustering result.
Fig. 6.
Adaptive subsequence clustering on a synthetic time series. (a) Three ground truth subsequence patterns with lengths of 10, 15 and 30, (b) test time series generated by randomly concentrating the three patterns in (a), (c) our cluster centers, (d) subsequence clustering result.

Show All

Fig. 7 illustrates the iterative process of model loss and cluster number. Initially, thirteen subsequence clusters are generated and the model loss is large due to the unsuitable cluster lengths and representations. Then, our method splits and combines clusters for refinement, and the model loss decreases. At iteration 5, the loss is almost zero to ideally cluster the time series with twelve clusters. By gradually decreasing the cluster number, at iteration 27, the cluster number is reduced to the predefined three, but the loss increases due to unsatisfying clusters. However, by cluster splitting and combination, the clusters are then adjusted and desired cluster results are obtained at iteration 30.

Fig. 7. - 
Adaptive subsequence clustering iteration on a synthetic time series. (a) Model loss during iteration, (b) cluster number during iteration.
Fig. 7.
Adaptive subsequence clustering iteration on a synthetic time series. (a) Model loss during iteration, (b) cluster number during iteration.

Show All

Fig. 8, 9, and 10 illustrate the qualitative comparisons with other methods on the synthetic time series in Fig. 6. Among them, the cluster center comparisons are shown in Fig. 8. For sliding window based methods, the extracted subsequence clusters do not produce meaningful subsequence patterns in Fig. 8d, 8e, and 8f. Our method produces the ideal subsequence patterns with the accurate lengths and representations in Fig. 8c.


Fig. 8.
Subsequence clustering center comparisons on a synthetic time series. (a) Input time series, (b) subsequence clusters ground truth, (c) our result, (d) Kmeans result, (e) DTWGAK result, (f) NeuralGas result.

Show All

Fig. 9. - 
Extracted subsequence sample comparisons on a synthetic time series. (a) Input time series, (b) subsequence samples ground truth, (c) our result, (d) Kmotif result, (e) MDLKmotif result, (f) VLmotif result, (g) VALMOD result, (h) EXTRACT result.
Fig. 9.
Extracted subsequence sample comparisons on a synthetic time series. (a) Input time series, (b) subsequence samples ground truth, (c) our result, (d) Kmotif result, (e) MDLKmotif result, (f) VLmotif result, (g) VALMOD result, (h) EXTRACT result.

Show All

Fig. 10. - 
Subsequence segmentation comparisons on a synthetic time series. (a) Input time series, (b) subsequence segmentation ground truth, (c) our result, (d) Kmeans result, (e) DTWGAK result, (f) NeuralGas result, (g) TICC result, (h) CASC result.
Fig. 10.
Subsequence segmentation comparisons on a synthetic time series. (a) Input time series, (b) subsequence segmentation ground truth, (c) our result, (d) Kmeans result, (e) DTWGAK result, (f) NeuralGas result, (g) TICC result, (h) CASC result.

Show All

The subsequence cluster sample comparisons are shown in Fig. 9. For motif-based methods, i.e., Kmotif, MDLKmotif, VLmotif, VALMOD and EXTRACT, while they are efficient and successful in extracting interesting motifs, their subsequence samples may be some parts or combinations of the ground-truth patterns in Fig. 9d, 8e, 8f, 8g, and 8h because there exists many repeated patterns in the time series. Our method produces the ideal subsequence patterns in Fig. 9c. When there are various similar patterns in the time series, our method can extract the most basic subsequence patterns to explain the time series with few subsequence clusters.

The subsequence segmentation comparisons are shown in Fig. 10. For Kmeans, DTWGAK and NeuralGas, the ground-truth patterns are segmented into different parts in Fig. 10d, 10e, and 10f. This is because the fixed length sliding window subsequences can not accurately represent the variable-length patterns. Similar results appear in TICC and CASC in Fig. 10g and 10h. This may be due to their assumptions are not well satisfied in our time series that the subsequences in a segmentation are normally generated from a probability distribution. Our method produces the ideal segmentation to the ground-truth in Fig. 10c.

For quantity evaluation, Table 1 lists the cluster assignment error comparison for the time series in Fig. 6 where the ground-truth cluster labels are given in the time series generation process. We obtain the ideal cluster assignment.

TABLE 1 Cluster Assignment Error Comparisons on a Synthetic Time Series (%)
Table 1- 
Cluster Assignment Error Comparisons on a Synthetic Time Series (%)
Another synthetic time series is constructed by randomly concentrating three randomly generated patterns with some noise on them in Fig. 11. The three pattern lengths are 10, 20 and 30 respectively. By setting the cluster number 3, the minimal and maximal subsequence length 5 and 60, we produce accurate clustering results are in Fig. 11c and 11d.


Fig. 11.
Adaptive subsequence clustering on a synthetic time series. (a) Three randomly generated patterns with lengths of 10, 20 and 30, (b) test time series generated by randomly concentrating three patterns in (a), (c) our subsequence cluster centers, (d) our subsequence clustering result.

Show All

4.3 Results on Real Time Series
In this subsection, we evaluate our method on open-source real time series. One time series is from GestureMidAir dataset in UCR Time Series Classification Archive [23]. The time series are sequences of 3D points to represent different gestures. We concentrate the time series samples from two kinds of gestures as the test time series, and then extract subsequence clusters. The results are shown in Fig. 12 where the cluster number is set to two, and the minimal and maximal subsequence length are set to 10 and 40 respectively. Though there are variations in the same gesture pattern in Fig. 12b, our method produces almost the identical clustering results with the ground truth in Fig. 12d.

Fig. 12. - 
Adaptive subsequence clustering on GestureMidAir dataset. (a) Input time series, (b) cluster labels ground truth, (c) extracted subsequence clusters, (d) clustering result.
Fig. 12.
Adaptive subsequence clustering on GestureMidAir dataset. (a) Input time series, (b) cluster labels ground truth, (c) extracted subsequence clusters, (d) clustering result.

Show All

The cluster center comparisons on GestureMidAir dataset are illustrated in Fig. 13. For sliding window based methods, they do not produce meaningful subsequence clusters in Fig. 13d, 13e, and 13f. However, we extract the meaningful subsequence clusters in Fig. 13c to represent the different length upper-trend and down-trend patterns in the original time series.

Fig. 13. - 
Subsequence clustering center comparisons on GestureMidAir dataset. (a) Input time series, (b) subsequence clusters ground truth, (c) our result, (d) Kmeans result, (e) DTWGAK result, (f) NeuralGas result.
Fig. 13.
Subsequence clustering center comparisons on GestureMidAir dataset. (a) Input time series, (b) subsequence clusters ground truth, (c) our result, (d) Kmeans result, (e) DTWGAK result, (f) NeuralGas result.

Show All

The extracted subsequence sample comparisons on GestureMidAir dataset are shown in Fig. 14. For motif-based methods, while they are efficient and successful in extracting interesting motifs, they produce some subsequences not existing in the ground-truth patterns in Fig. 14d, 14e, 14f, 14g, and 14h. Our method produces the satisfying subsequence samples to represent the two different patterns in Fig. 14c.

Fig. 14. - 
Extracted subsequence sample comparisons on GestureMidAir dataset. (a) Input time series, (b) subsequence samples ground truth, (c) our result, (d) Kmotif result, (e) MDLKmotif result, (f) VLmotif result, (g) VALMOD result, (h) EXTRACT result.
Fig. 14.
Extracted subsequence sample comparisons on GestureMidAir dataset. (a) Input time series, (b) subsequence samples ground truth, (c) our result, (d) Kmotif result, (e) MDLKmotif result, (f) VLmotif result, (g) VALMOD result, (h) EXTRACT result.

Show All

The subsequence segmentation comparisons on GestureMidAir dataset are shown in Fig. 15. All the comparison methods segment the original gesture patterns into different parts in Fig. 15d, 15e, 15f, 15g, and 15h. Our method produces almost the ideal segmentation in Fig. 15c. This demonstrates the importance of simultaneously estimating subsequence clusters and segmentation in our model that only accurate segmentation can produce satisfying clustering results.

Fig. 15. - 
Subsequence segmentation comparisons on GestureMidAir dataset. (a) Input time series, (b) subsequence segmentation ground truth, (c) our result, (d) Kmeans result, (e) DTWGAK result, (f) NeuralGas result, (g) TICC result, (h) CASC result.
Fig. 15.
Subsequence segmentation comparisons on GestureMidAir dataset. (a) Input time series, (b) subsequence segmentation ground truth, (c) our result, (d) Kmeans result, (e) DTWGAK result, (f) NeuralGas result, (g) TICC result, (h) CASC result.

Show All

With the ground-truth cluster labels in Fig. 12b, we compute the cluster assignment error on GestureMidAir dataset and the results are listed in Table 2. We obtain the minimal cluster assignment error.

TABLE 2 Cluster Assignment Error Comparisons on GestureMidAir Dataset (%)
Table 2- 
Cluster Assignment Error Comparisons on GestureMidAir Dataset (%)
We then compare the runtime for different methods (CPU: Intel(R) Xeon(R) E5-2620 v4 @2.1 GHz, Memory size: 32 GB). We generate multiple input time series by repeating the time series in Fig. 12 by different times. The runtime results are shown in Fig. 16. There are some loop iterations in Algorithm 2, thus our method is slower than some other methods to obtain suitable subsequence clusters to explain the whole time series. However, our runtime is almost linear to time series length as shown in Fig. 16, which is also indicated by time complexity analysis of our method.

Fig. 16. - 
Runtime comparisons for different methods.
Fig. 16.
Runtime comparisons for different methods.

Show All

We also test on a real ECG time series with regular patterns from PhysioBank ATM [24] in Fig. 17. By setting the cluster number to 3, and the minimal and maximal subsequence lengths to 30 and 100, our method accurately estimates the subsequence cluster centers and segmentation, and extracts similar subsequences for each cluster in Fig. 17.

Fig. 17. - 
Adaptive subsequence clustering on ECG time series. (a) Input time series, (b) extracted subsequence clusters, (c) extracted subsequence segmentation, (d) extracted subsequence samples.
Fig. 17.
Adaptive subsequence clustering on ECG time series. (a) Input time series, (b) extracted subsequence clusters, (c) extracted subsequence segmentation, (d) extracted subsequence samples.

Show All

We then test on real temperature time series [18] with highly irregular and complex patterns in Fig. 18. The parameters of (cluster number, minimal subsequence length, maximal subsequence length) in Fig. 18a, 18b, 18c, and 18d are (3, 10, 60), (4, 10, 40), (4, 10, 40) and (4, 10, 40) respectively. For all the time series in Fig. 18a, there exist various irregular and complex subsequence patterns with different lengths. By only setting the minimal and maximal subsequence lengths and cluster number, our method automatically extracts the subsequence patterns in the time series. For example, in the first row of Fig. 18, the extracted blue subsequence cluster corresponds to temperature fluctuations, the green subsequence cluster corresponds to some regular temperature sudden change, and the blue subsequence cluster corresponds to some long-range temperature pattern. Our method can also be used to extract special interesting patterns in quite irregular time series. In Fig. 18d, the blue pattern in the first row, the cyan pattern in the second row, the cyan and blue patterns in the third and fourth rows are all extracted. These extracted patterns can be quite useful for time series data mining.

Fig. 18. - 
Adaptive subsequence clustering on temperature time series. (a) Input time series, (b) extracted cluster centers, (c) the clustering results, (d) extracted subsequences from each cluster.
Fig. 18.
Adaptive subsequence clustering on temperature time series. (a) Input time series, (b) extracted cluster centers, (c) the clustering results, (d) extracted subsequences from each cluster.

Show All

4.4 Parameter Robustness
In our model, there are three parameters to be set, i.e., the minimal and maximal subsequence lengths and the cluster number. Fig. 19 illustrates the robustness of minimal and maximal subsequence lengths on the real GestureMidAir time series. Fixing the cluster number to two, we vary the minimal and maximal subsequence lengths in Fig. 19c, 19d, 19e, 19f, 19g, 19h, and 19i. In Fig. 19d, 19e, 19f, 19g, and 19h, all the clustering results are almost identical to the ground truth. Too small or large minimal and maximal subsequence lengths may result in local optimization in our model, thus produce some unsatisfying results in Fig. 19c and 19i. Generally, we can set the minimal and maximal subsequence lengths not much smaller or larger than the minimal and maximal lengths of subsequence patterns, then we can obtain robust and satisfying clustering results.

Fig. 19. - 
Robustness of minimal and maximal subsequence lengths in our model. (a) Input time series, (b) subsequence clustering ground truth, (c)-(i) subsequence clustering results for different(minimal length, maximal length) pairs of (5, 20), (10, 30), (10, 40), (10, 60), (15, 30), (15, 50) and (30, 60) respectively.
Fig. 19.
Robustness of minimal and maximal subsequence lengths in our model. (a) Input time series, (b) subsequence clustering ground truth, (c)-(i) subsequence clustering results for different(minimal length, maximal length) pairs of (5, 20), (10, 30), (10, 40), (10, 60), (15, 30), (15, 50) and (30, 60) respectively.

Show All

As in many clustering methods [25], the cluster number in our method is considered as a hyperparameter. In Fig. 20, we examine the influences of cluster number in our model by fixing the minimal and maximal subsequence lengths 10 and 40 respectively. When the cluster number is set to the ground truth, we obtain satisfying results in Fig. 20b. When the cluster number increases to three, one cluster in Fig. 20b is split into two clusters with short lengths in Fig. 20c. When the cluster number further increases to four, one cluster in Fig. 20b is divided into two sub-group clusters and another cluster in Fig. 20b is split into two shorter clusters in Fig. 20d. That is, when the cluster number increases, some original clusters may split into shorter clusters or divided into subgroup clusters. Otherwise, when the cluster number decreases, neighboring shorter clusters may combined into a larger cluster or some similar clusters may group into a new larger cluster. Nevertheless, the cluster number in our model is considered as a hyperparameter and is determined in practical use.

Fig. 20. - 
Adaptive subsequence clustering for different cluster numbers. (a) Input time series with two clusters ground truth, (b)-(d) subsequence clustering results for different cluster numbers of 2, 3 and 4 respectively.
Fig. 20.
Adaptive subsequence clustering for different cluster numbers. (a) Input time series with two clusters ground truth, (b)-(d) subsequence clustering results for different cluster numbers of 2, 3 and 4 respectively.

Show All

4.5 Discussion and Extension
In [9], the authors point that accurate time series segmentation is critical for many data mining tasks and is usually assumed to be known in advance. However, in subsequence clustering, it is hard to known either the segmented subsequence lengths or the segmentation positions in advance. Different from many traditional subsequence clustering methods where the subsequence lengths given in advance [7], our method can automatically estimate the subsequence pattern lengths and optimize the subsequence segmentation to produce satisfying subsequence clustering.

As explained in [8], if the subsequences are extracted by sliding window, traditional clustering methods such as k-means and hierarchical clustering will produce meaningless results due to the trivial similar subsequence matches. Our method can naturally extract non-trivial subsequence matches because we only require each point of time series existing in one of the extracted subsequences. This makes our method suitable to subsequence clustering and some other tasks which require non-trivial subsequence matches.

Our model can be extended to cluster subsequence patterns which have different amplitudes and biases. The new objective function becomes
minC,Zf(C,Z)=∑i=1k∑x∈Ci||T(x)−T(μi)||2,s.t.∪x=X,lmin≤|μi|≤lmax,(9)
View SourceRight-click on figure for MathML and additional features.where T(x) and T(μi) transform x and μi by z-normalization to make their means to 0 and variances to 1. Then we define a new distance metrics between a subsequence x and a cluster μi in Algorithm 1 by d(x,μi)=||T(x)−T(μi)||2. Then our Algorithms 1 and 2 with the new distance metric can extract subsequence clusters of similar patterns with different amplitudes and biases in a time series. One result is illustrated in Fig. 21 where the time series is constructed by randomly scaling the similar patterns in Fig. 12 and then concentrating them together. In Fig. 21, we cluster the similar patterns with different amplitudes and biases together and obtain almost ideal results.

Fig. 21. - 
Subsequence clustering on similar patterns with different amplitudes and biases. (a) Input time series, (b) subsequence clustering ground truth, (c) extracted cluster centers, (d) our clustering result.
Fig. 21.
Subsequence clustering on similar patterns with different amplitudes and biases. (a) Input time series, (b) subsequence clustering ground truth, (c) extracted cluster centers, (d) our clustering result.

Show All

SECTION 5Conclusion
In this paper, we have proposed a novel method to cluster unknown variable-length subsequence patterns by simultaneously optimizing subsequence clusters and segmentation. To accurately estimate subsequence cluster lengths and representations, we progressively refine subsequence clusters by cluster splitting, combination and removing. The only parameters in our model, i.e., the cluster number, the minimal and maximal cluster lengths, are naturally interpretable and easy to set which makes our method ready to use in practical applications. We have produced satisfying results on various synthetic and real time series.

In the future, we will extend our model by employing other subsequence similarity metrics to deal with length variations in the same pattern and relaxing time series cover constraint to deal with some non-pattern subsequences.

