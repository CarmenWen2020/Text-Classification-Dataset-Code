We address the problem of biclustering on heterogeneous data, that is, data of various types (binary, numeric, symbolic, temporal). We propose a new method, HBC-t (Heterogeneous BiClustering for temporal data), designed to extract biclusters from heterogeneous, temporal, large-scale, sparse data matrices. HBC-t is based on HBC, using similar mechanisms but adding support for temporal data. The goal of this method is to handle Electronic Health Records (EHR) data gathered by hospitals on patients, stays, acts, diagnoses, prescriptions, etc.; and to provide valuable insights on this data. Temporal data accounts for a majority of the data available for this study, and in EHR in general where medical events are timestamped. Therefore, it is crucial to have an algorithm that supports this type of data. The proposed algorithm takes advantage of the data sparsity and uses a constructive greedy heuristic to build a large number of possibly overlapping biclusters. HBC-t is successfully compared with several other biclustering algorithms on numeric and temporal data. Experiments on full-scale real-life data sets further assert its scalability and efficiency.
SECTION 1Introduction
Biclustering methods produce a set of biclusters, each corresponding to a subset of rows and a subset of columns of the data matrix. The goal of such methods is to group rows which are similar according to a subset of columns, and vice-versa. Biclustering detects correlations among both rows and columns, giving useful insight on the studied data. We consider here, without loss of generality, rows as instances of the data, and columns as attributes describing each instance. Therefore, a bicluster composed of rows R and columns C can be interpreted as “the instances in R are similar on all attributes in C”. Biclustering was developed within the field of biological data analysis, and more specifically gene expression analysis. It still has most of its applications in this field, but has also been used to tackle various data mining problems. In this study, we use biclustering for the analysis of medical data extracted from hospitals’ information systems. This data includes various types of information to describe each patient or each hospital stay: personal information (age, gender, address, etc.), biological measures, acts, diagnoses, and others. Biclustering is usually applied to homogeneous data, i.e., data where all attributes have the same type. Most biclustering applications focus on either binary data, or numeric data with normalized values. However, the most important aspect of medical data lies in its heterogeneity, including attributes of various types. A second distinctive characteristic is its size: hundreds of thousands of records (instances, i.e., hospital stays) with thousands of possible acts or diagnoses (attributes). This high dimensionality is coupled with a high sparsity in the data matrix, as only a small fraction of all possible medical events happen in a given hospital stay. This fact brings the twofold challenge of handling large data matrices with a lot of missing values, while allowing the desired method to exploit the high sparsity in its design. Another significant difficulty lies in the intrinsic temporal aspect of medical data. Acts and diagnoses are performed at specific times within a hospital stay, and these timestamps carry valuable information that should be used in addition to the non- temporal binary information (e.g., “has this act been performed?”). The goal of this article is to present a biclustering method able to handle all of these specificities to extract knowledge from Electronic Health Records (EHR) data.

The remainder of this article is organized as follows:

Section 2 defines the problem in detail and presents relevant existing works on the subject.

Section 3 presents a summary of a previous work on a biclustering method relevant to our goals: HBC (Heterogeneous BiClustering) [1].

Section 4 presents HBC-t, an extension of HBC to temporal attributes (called temporal events in the rest of this study), including a data model, a modified constructive heuristic for biclustering and an adequate quality measure for the biclusters thus formed.

Section 5 details postprocessing procedures that may be used in combination with HBC-t, including the coupling with other optimization algorithms to improve the results’ quality, and a method for sampling the set of results to improve their usability in real-life scenarios.

Section 6 presents experimental results and comparisons with other algorithms on both synthetic and real data sets.

Section 7 summarizes this work and discusses possible improvements or evolutions.

SECTION 2Problem Presentation
2.1 Definition and Related Work
In this work, by heterogeneous data, we mean that we wish to handle simultaneously (i.e., in a same data file) the following data types:

numeric: either integer or real values. Examples: age or blood pressure.

ordered symbolic: symbolic values with an ordering relation. Example: severity ∈{low,medium,high}.

unordered symbolic: other symbolic values. Example: blood type ∈{A,B,AB,O}.

binary: a special case of unordered symbolic, but separated for more flexibility. Example: high blood pressure ∈{true, false}.

temporal events: binary events associated with the time at which the event occurs. Example: biopsy was performed at time t.

Many biclustering methods have been proposed since the seminal work of Cheng and Church on the subject [2]. However, the overwhelming majority of those focus on biological data analysis: detection of gene expression [2], protein interaction [3], microarray data analysis [4], etc. Gene expression was the original field of study from which originated the first biclustering methods, and these methods have led to a better understanding of the studied biological systems. Most biclustering algorithms are designed to handle only one type of data, either numeric or binary [5]. Indeed, these are the usual types of data available from biological applications, representing for example the activation of a gene (binary) or its expression level (numeric). One algorithm, SAMBA (Statistical Algorithmic Method for Bicluster Analysis) [6], deals with data from different sources, but remains focused on numeric data and does not handle other data types. Biclustering has also been applied to other data mining problems [7], such as text mining [8] or recommendation systems [9]. As far as we know, none of the existing applications to data mining tasks deal with the data available from clinical EHR, which gather heterogeneous data types with a temporal aspect for some of them. Focusing on homogeneous data means that biclustering methods can be designed to extract biclusters describing several types of correlations on rows and columns. As explained in Section 2.2, all of these correlations are not applicable on heterogeneous data, which may partly explain the lack of studies on this topic. A recent study proposed a multi-objective local search-based metaheuristic for biclustering on heterogeneous data, LSBC [10]. This method achieves good results on the quality and size of the biclusters generated, but suffers from a lack of diversity that prevents it from finding well-separated biclusters, and it does not handle temporal data.

The other main characteristic of EHR data is high dimensionality and data sparsity. Most biclustering methods work well on data matrices with up to hundreds or thousands of rows and columns. Scaling to larger data remains an open problem, with new approaches involving parallel computing to provide suitable speedup [11]. We focus here on data matrices with around 100,000 rows and 10,000 columns, which have also high sparsity. Sparsity is another seldom explored characteristic in biclustering. A notable work on this topic presents a method for data with around 20 percent of non-missing values [12]. In this study, we consider very sparse data matrices, with less than 1 percent non-missing values. This type of data has not been well studied in the biclustering literature, although one recent work focuses on recommendation systems for data with similar volume and sparsity [13]. However, this work did not deal with heterogeneous or temporal data.

Application of biclustering to temporal data has not attracted much attention either. Since most data in gene expression analysis has an underlying temporal aspect, dedicated methods such as OPSM (Order-Preserving Sub-Matrix) [14] have been developed to specifically focus on the detection of similar evolutions in the values. These methods are only able to deal with temporal data, and are thus not applicable for heterogeneous data. However, OPSM was a notable inspiration for the solution proposed in this study to handle temporal data. Another recent study considers the time aspect andapplies a biclustering algorithm to extract temporal patterns from the time series in EEG data. It then uses these patterns as features for a classification algorithm to detect epileptic seizure incidents [15]. However, time series are different from the temporal events we focus on in the present study, which require adequate representation and handling methods (see Section 4.1).

Biclustering should not be confused with clustering, which is an entirely different class of methods with different goals. Some biclustering methods are based on the same principles as clustering methods, such as Spectral Biclustering [4] which is based on spectral clustering, a type of graph-based clustering technique. Other biclustering methods use clustering algorithms as components, such as Coupled Two-Way Clustering [16], which can use any clustering algorithm on the rows and columns separately, and use the results to build biclusters.

The method presented in the present article has been developed in the context of a national research project, which proposed statistical and algorithmic models to understand the “patient pathway” in hospitals. The main article resulting from this project briefly explains the logic behind the use of biclustering in this context and gives a few examples of resulting biclusters [17]. In the present study, we detail the data model developed for heterogeneous and temporal data, the HBC-t biclustering algorithm; and we present the experimentations to support our claims of the efficiency of the proposed method in this context.

2.2 Bicluster Types
The design of a biclustering algorithm depends on the type of biclusters it should capture. Several types of biclusters exist [5], and most algorithms only focus on a subset of these, depending on the application. Fig. 1 gives an overview of the existing bicluster types. Type (a) biclusters contain similar values on all rows and columns; type (b) contain similar values on each row; type (c) contain similar values on each column. In types (d) and (e), each value ei,j may be obtained by applying a linear transformation to the value in the previous row (ei−1,j) or column (ei,j−1). Type (d) has a shifting pattern, where ei,j=ei,j−1+sj, where sj is the shifting value for column j. Type (e) has a scaling pattern, where ei,j=ei,j−1×s′j, where s′j is the scaling value for column j. We focus on type (c), constant columns biclusters. In this work, we use medical data and try to detect sets of patients or hospital stays sharing the same characteristics for some of the attributes. Therefore, we want a method able to detect biclusters with constant (or similar) values on columns. Because the data is heterogeneous, detecting biclusters with constant values on rows, or coherent values on rows and/or columns, does not make sense. For example, one column may contain numeric values in the [−1.0,1.0] range, another numeric values in the [10,500] range, and another binary values. In this context, it is impossible to form biclusters with constant or coherent values in rows, because there exists no linear function (shifting or scaling pattern) able to describe the values in one row.

Fig. 1. - 
Types of biclusters - constant and coherent values on rows and/or columns.
Fig. 1.
Types of biclusters - constant and coherent values on rows and/or columns.

Show All

Biclustering methods are further divided depending on how they answer the following questions about overlapping:

Can the biclusters have row-overlapping? That is, can two (or more) biclusters include the same row(s)?

Can the biclusters have column-overlapping? That is, can two (or more) biclusters include the same column(s)?

Do we want to cover the whole data space with biclusters? This question does not directly pertain to overlapping, but is also used to design the algorithm. Indeed, some biclustering methods aim to cover the data space in what could be seen as a checkerboard structure. Others extract interesting clusters and may leave regions of the data space unused because deemed less interesting.

Fig. 2 shows biclusters with various types of overlapping and data space coverage. Type h) corresponds to the more general case, where overlapping is allowed on rows and columns, and the biclusters do not need to cover the whole data. Note that biclusters can have “full overlapping” without points in the data matrix belonging to more than one bicluster (see types f) and g) in Fig. 2). “Full overlapping” means that overlapping on both rows and columns is allowed.

Fig. 2. - 
Biclusters - overlapping and data space coverage.
Fig. 2.
Biclusters - overlapping and data space coverage.

Show All

We consider in this study the case illustrated by type h). This choice is made because we suppose that one patient can belong to several interesting groups, and one attribute can be used to describe several such groups. About space covering, it is common for medical applications to focus on the positive cases only, which may be a small part of the whole data. In this regard, it makes sense to extract the interesting sub-parts of the data and to ignore the rest, which corresponds to the biclusters in type h).

SECTION 3Summary of Existing Method - HBC
The work presented in this article builds upon HBC, a biclustering algorithm suited for heterogeneous, large, sparse data, which was explained in detail in a previous article [1]. In the present section, we give a brief outline of this algorithm, necessary to understand the contributions exposed in Sections 4 and 5.

Figure 3 gives an overview of the HBC algorithm, whose main steps are:

Simplification: the data undergoes a preprocessing phase where it is simplified in order to facilitate the biclustering process.

Biclustering: the main phase, in which a constructive heuristic is used to form a set of biclusters.

Translation: biclusters built in the previous phase are transposed back to the original data (reverse action from the simplification).

Postprocessing: once translated back to original data, biclusters have their quality computed and redundant ones are removed.

Fig. 3. - 
Overview of the HBC biclustering algorithm.
Fig. 3.
Overview of the HBC biclustering algorithm.

Show All

The biclustering process uses a greedy constructive heuristic. This heuristic works by creating an initial bicluster with one column and all the rows containing the same value for this column. It then adds, at each step, the column with the largest number of identical values for the rows in the current bicluster, and removes the rows for which the values are different. This process is iterated, starting from each possible value of each column. This general principle induces the need for a finite number of possible values for each attribute, as this makes the counting of identical values much easier than on real-valued numbers for example.

In HBC, numeric attributes are discretized during the simplification phase, which makes the computation easier since discretized attributes can only take a finite number of possible values. Discretization has also been shown to improve performance in various data mining tasks [18]. Since biclustering itself is an unsupervised process and we do not have a unique label for each instance, only unsupervised discretization methods can be used [19]. HBC uses a standard equal-width binning method, which separates the data range in intervals (or bins) of equal size, with 10 bins.

After the translation phase, biclusters are transposed back to the original data and each bicluster's quality is computed using the formula shown in Equation (1).
quality(bc)=1|A|×[∑j∈Anumσ2(dIbcj)σ2(dIj)+∑j∉Anum(1−|most frequent value in dIbcj||Ibc|)],(1)
View SourceRight-click on figure for MathML and additional features.

where A are the attributes (or columns) in the bicluster bc, Anum is the set of numeric attributes in bc, σ2 is the variance of a set of values, Ibc are the rows in bc, and dij is the value of the data matrix at row i and column j. It should be noted that we seek to minimize this measure of quality; biclusters with a quality close to 0 are those with minimal internal variance, which is the desired outcome. This quality measure requires the variance of each column (i.e., over all values of the column), which are computed beforehand. HBC's quality measure is similar to the one that was used in LSBC [10]. Variance is a standard measure of quality for biclusters with numeric values [20], but there is no consensus on the most adequate quality measure for categoric values.

SECTION 4Biclustering for Heterogeneous and Temporal Data - HBC-t
In the previous section, we have presented an overview of the original HBC algorithm for non-temporal data. In this section, we propose HBC-t, an extension of HBC to temporal events. We first define a data model suited for these data, then an adequate quality measure for biclusters with temporal events. Lastly, we propose a heuristic for biclustering on these data.

4.1 Data Model
Let us remind first that by temporal events, we mean events associated with a timestamp. That is, instead of only knowing that the event occurred, we know at which time it occurred. This type of attribute should not be treated as a simple binary attribute, since this would mean not making use of the additional information carried by the timestamps. Moreover, despite the timestamp being a numeric value, it should not be treated as a numeric attribute either, for several reasons. The main reason is that we do not expect the timestamp associated to a single event to bring enough information to infer anything useful. For example, the fact that an event E occurs on one day for a patient A, and on another day for a patient B, does not mean anything when this fact alone is considered. An alternative method would consist in considering the time elapsed since the first event for this patient, for example, and not the absolute timestamp. However, this relative timestamp in itself does not provide enough information and may lead to the omission of biclusters of interest if used as numeric value for the data model of temporal events. Indeed, an event (or sequence of events) may occur at the beginning of one patient's stay at a hospital, and at the end for another patient, and have the same meaning or importance for both. For example, a patient A may be admitted for a stroke and directed to the intensive care unit (ICU), while a patient B may have a stroke as a complication of a surgical intervention and also be redirected to the ICU. These patients A and B share a common sequence of events (stroke then ICU) that should be used to group them in a bicluster. However, such cases are not handled when using the events’ timestamps, despite being frequent in real life given the high variance (in duration or complexity) of hospital stays.

Instead, we choose another approach, in which we focus on the relative order of events within a stay instead of their times of occurrence. In a biclustering context, this means that we consider two patients to be similar if they share the same order for a subset of their events. We expect this second approach to be more flexible than the first one, since it aims to detect sequences of events occurring at different times for different patients. In this work, we focus on the sequentiality of the events, for various reasons. A first conceptual reason is that this approach makes heavier use of correlations between attributes, which is very much in line with the core concept of biclustering itself. Another more practical reason is that such sequences of events, as distinctive features of sub-groups of patients, are in most cases easier to interpret. The idea of looking at the order of the attributes was inspired by the Order-Preserving SubMatrix (OPSM) problem [14].

4.2 Quality Measure
As stated, we focus on the relative ordering of the temporal events in a sequence. Therefore, the approach adopted previously in HBC, where a quality (or homogeneity) measure is computed for each attribute, does not seem relevant in this case. Instead, we use a quality measure proposed in an extension of the OPSM algorithm. This study defines a quality measure for temporal data “a score for the degree of order preservation”[21]. The main idea is to compute the mean-squared residue score [2], using the respective rank (or order) of the events as values. An example of extraction of a bicluster according to the order of the temporal values is shown in Fig. 4.

Fig. 4. - 
Extraction of a bicluster amongst temporal data (evt1 to evt5) and translation to respective order of the events.
Fig. 4.
Extraction of a bicluster amongst temporal data (evt1 to evt5) and translation to respective order of the events.

Show All

The quality measure on temporal data is defined as:
g(R,C)=1|R|×|C|∑i∈R,j∈C(eij−eiC−eRj+eRC)2,(2)
View SourceRight-click on figure for MathML and additional features.

with:
eiC=1|C|∑j∈Ceij,eRj=1|R|∑i∈Reij(3)
View SourceRight-click on figure for MathML and additional features.
eRC=1|R|×|C|∑i∈R,j∈Ceij,(4)
View SourceRight-click on figure for MathML and additional features.

R and C are the rows and columns of the data matrix, respectively. eij is the element of the data matrix located at row i and column j. eRC is the mean value of all the data matrix elements. This had been shown to be a coherent measure for evaluating the degree of order preservation [21]. Notably, this measure is optimal (i.e., equals 0) when the order is perfect, which is in line with what we expect. Since the biclustering process only generates biclusters with perfect order, the quality measure for the temporal events included in a bicluster is by definition 0. A slight difference in the integration of this measure to the global composite quality measure of a bicluster is that it needs to be weighted according to the number of columns (or events) included. Indeed, since we only have one measure for j columns corresponding to j temporal events, the quality measure g for these columns will be weighted as g′=g×j.

4.3 Modifications of the Bicluster Construction Process
We have seen that some data types require a preprocessing phase, such as discretization, to reduce them to a finite set of possible values. This is not required for temporal data, where it is more efficient to slightly alter the bicluster construction process. For the other attributes types, the algorithm searches for the value that occurs the most frequently among the rows of the bicluster being built. For temporal data, we do not consider the value itself, but rather the sequence of temporal events. For these attributes, the algorithm computes for each row the order of events after adding the new attribute. The rows that are kept are those with the most frequent sequence of events. Doing so implies that we modify the general algorithm, but it avoids preprocessing and postprocessing steps that would lose some information contained in this data. We want to consider the position of an event in a sequence, which depends on the values of the other temporal attributes. This means that we cannot reduce the values for a temporal attribute to a finite set of possible values, since these values would vary depending on which other attributes are present in the bicluster. It should also be noted that computing the events sequence order is less costly than ordering a list of values. Indeed, computing the position at which the new event should be inserted is enough, since all the sequences in the bicluster have the same order. This operation is done in linear time on the number of temporal attributes in the bicluster.

Algorithm 1. Function to add a column to a bicluster during the construction process in HBC-t
input: a bicluster B under construction

input: a column C from the data where C ∉ B

output: the bicluster B with added column C

r ← number of rows in B;

c ← number of columns in B;

map ← ();

for i←1 to r do

L ← 0;

for j←1 to c do

if B[i][j] < C[i] then

L ← L+1;

else if B[i][j] == C[i] then

L ← −j;

break;

end

map[L] ← map[L] ∪ i;

end

select the largest set of row indexes S in map;

remove rows R ∉ S from B;

add column C to bicluster B;

return B;

The original HBC algorithm builds biclusters by iteratively adding columns and removing rows where the simplified values are different. A simplification phase is done in preprocessing, that reduces the set of possible values to a finite set if needed (e.g., discretization of numeric attributes), in order to easily determine which values are different and should be removed. This comparison between values is only done amongst the values of the column being added to the bicluster. However, in HBC-t, we consider temporal events and their respective order within a bicluster. Therefore, the order of the new event (from the column being added) depends on the values of the other events in the current bicluster. Thus, we alter the process for adding a column to a bicluster when this column contains temporal events. Since the values for temporal events are the timestamps, the order of a new event amongst the current events of a row is simply the number of events that occur before this one. The pseudocode for this part of HBC-t is shown in Algorithm 1. An example is provided in Fig. 5. The second condition in the loop is added to handle the case where the new event occurs at the same time as another.

Fig. 5. - 
Addition of the column evt6 to a bicluster with three rows and three columns. The position pos of each value of evt6 is computed with regard to the existing sequence in the row, and the most frequent order (here, 3, meaning that the new event occurs after the first 3 events in the current bicluster) is picked.
Fig. 5.
Addition of the column evt6 to a bicluster with three rows and three columns. The position pos of each value of evt6 is computed with regard to the existing sequence in the row, and the most frequent order (here, 3, meaning that the new event occurs after the first 3 events in the current bicluster) is picked.

Show All

SECTION 5Use of the Biclustering Results
In this section, we address two questions that pertain to the use of the raw results HBC-t, that is, the set of all biclusters generated by the algorithm. First, we explain how HBC-t may be coupled with optimization algorithms to improve the biclusters’ quality. Then, we propose a method to extract a representative sample of biclusters amongst the (often large) set of all biclusters generated.

5.1 Coupling With Optimization Methods
In Section 4.2, we have proposed a quality measure for the temporal part of the biclusters generated by HBC-t. This measure is used alongside the one defined in HBC for non-temporal data. However, neither of these measures are used during the formation of the biclusters, but are only computed once, during postprocessing. While the HBC-t heuristic does not use the quality measure to build biclusters, this quality measure can still be used to further improve the biclusters. Indeed, it is possible to plug in, at the end of HBC-t, an optimization algorithm using this quality measure as optimization criterion.

There exist numerous optimization algorithms, notably metaheuristics, which have proven to be efficient for biclustering [22], [23]. Any one of these algorithms may be used at the end of HBC-t. We propose here a rather simple example of such a postprocessing procedure. This procedure adds rows to each bicluster while this does not deteriorate the bicluster's ratio of quality over size. It is described more precisely in Algorithm 2.

We choose as stopping criterion the ratio of the quality (to minimize) over the size of the bicluster (to maximize) for several reasons. The first one is that these two measures are often used within multi-objective biclustering algorithms [10]. Quality and size are inherently concurrent: larger biclusters tend to have poorer quality, and vice versa. Using the ratio computed for the initial bicluster avoids the inclusion of a parameter to control the allowed quality degradation. Note that in this procedure, the rows are sorted only once, at the beginning. This means that the rows are included in an order depending on the quality variation each one brings to the initial bicluster. As the bicluster includes new rows, the qualities change and the ideal order may become different from the original one. Therefore, in order to guarantee optimality, we would have to recompute all the qualities for the remaining rows and sort them again, after each inclusion. Doing so is much more costly in terms of computation time. Experiments revealed that the order tends not to change much when new rows are included. Therefore, we choose to only compute the order once at the beginning of the procedure. This procedure could also be applied to columns instead of rows. However, there is no conceptual reason to do so, since the biclustering heuristic already proceeds by progressively adding columns.

Algorithm 2. An example of optimization procedure: add_rows
foreach bicluster B do

foreach row L not included in B do

BL←B∪L;

qBL←quality(BL);

end

sort the rows Li by increasing quality;

r←quality(B)size(B);

repeat

B←B∪Li;

r′←quality(B)size(B);

until r′≤r

end

5.2 Results Sampling and Selection
One of the features of HBC-t is that the algorithm outputs a variable, and usually large, number of biclusters. Since the heuristic iterates over all possible values for all columns of the data matrix, it may output thousands of biclusters, or even more depending on the data considered. In theory, this is not a problem, since all of these biclusters are unique and coherent, and therefore represent potentially valuable knowledge. However, this makes the qualitative evaluation and interpretation of the results hard to perform if one has to read through that many biclusters. This is a problem when working on real-life data, for which HBC-t was designed.

We propose to answer this problem by designing a method to extract a “representative” sample amongst the biclusters generated by HBC-t. We choose to define a “representative” set as one in which the biclusters have the least possible overlapping between them. Allowing the biclusters to overlap with one another is an important feature of HBC-t, but for sampling purposes, we find more interesting to have a set of biclusters that represent very different (i.e., with low overlapping) regions of the data space. In the context of medical data for example, it means that such a sample would represent very different patients profiles. It should be noted that this method does not aim to identify the most relevant biclusters, but to provide a sample of well-separated biclusters over the whole data space. We have found such a sample to be useful to initiate discussions with domain experts, by showing them some examples of what the method can find.

To these ends, we use a pairwise dissimilarity measure E between two biclusters B1 and B2, defined in Equation (5):
E(B1,B2)=|rows(B1)|−|rows(B1)∩rows(B2)||rows(B1)|×|cols(B1)|−|cols(B1)∩cols(B2)||cols(B1)|,(5)
View SourceRight-click on figure for MathML and additional features.

where cols stands for columns. This is a standard dissimilarity measure [24], which evaluates the proportion of bicluster B1 that does not belong to B2. Note that this measure is not symmetric, and E(B1,B2)≠E(B2,B1). Since we need the measure to be symmetric (see below), we use the agregate dissimilarity instead, defined as:
d(B1,B2)=E(B1,B2)+E(B2,B1)2.(6)
View SourceRight-click on figure for MathML and additional features.

In order to obtain a set of biclusters with minimal overlapping, we propose to use the k-medoids clustering algorithm [25] with the standard Partitioning Around Medoids implementation. This algorithm produces a set of k clusters of objects (here, biclusters), each cluster centered around a medoid (or centroid), that is, an object from which the other objects in the cluster are the closest. We use k-medoids instead of other clustering algorithms because this one allows us to easily cluster arbitrarily complex objects, as long as we have a symmetric pairwise dissimilarity measure between these objects. We developed the d measure presented above so that it fits these criteria, in order to use it in the clustering algorithm. Another advantage of the k-medoids algorithm, compared to the k-means algorithm for example, is that the centroids of the clusters are objects of the set and not points in the domain space. This means that we can take the set of medoids at the end of the clustering process and use these as a representative sample of the biclustering results. Indeed, the clustering guarantees that these objects (biclusters) are well-separated, i.e., have a high dissimilarity value d between them, which means these biclusters have as little overlapping as possible. The k value that determines the number of biclusters should be chosen according to the application needs. Using k-medoids clustering produces a representative sample of results in reasonable time: on the largest data sets (synthetic and real) presented in the Experiments section, this postprocessing step is completed in less than two hours.

SECTION 6Experiments
In this section, we first compare HBC with existing biclustering algorithms on small-scale numeric data. In a second phase, we compare HBC-t to OPSM on temporal data. We then assess HBC-t's scalability to real-life medical data sets and show that the method is able to extract coherent biclusters from these datasets.

6.1 Comparison on Numeric Data
6.1.1 Experimental Protocol
First we compare HBC with several existing biclustering algorithms, in order to assess its validity and efficiency. The first one is the well-known CC algorithm [2], which was one of the first biclustering methods to be developed, and still remains a standard algorithm in most biclustering tools or softwares. It also has the interesting property of being an iterative constructive heuristic, i.e., it has the same underlying algorithmic structure as HBC. We used the implementation available in the ‘biclust’ R package [26]. We also use two biclustering algorithms implemented in the Scikit-learn Python package for machine learning [27]: SpectralCoclustering [8] and SpectralBiclustering [4]. Lastly, we use LSBC, a local-search based multi-objective metaheuristic which is also designed for heterogeneous data [10]. All the algorithms used for comparison are executed with the default parameters in the available implementation, which are the parameters recommended by their respective authors. Part of these experiments, more precisely the comparison between HBC and CC, was already presented in the HBC article [1]. We add here a comparison with three other methods in order to further assess the validity of the proposed algorithm on numeric data.

The efficiency of a biclustering method can be evaluated in several ways. A first way is to compare the results (i.e., a set of biclusters) produced by an algorithm, using measures such as quality and size. A second way is to inject into the data one (or several) bicluster(s), and see if the algorithm is able to find these biclusters among the generated solutions. These two ways complement each other nicely, since they focus on different properties of the biclustering methods.

We use synthetic numeric data with 500 rows and 100 columns, as such dimensions are standard in the biclustering literature. Values for each cell of the data matrix are generated randomly using a uniform distribution on [0,100]. Injected biclusters have a fixed size and are composed of identical values on every cell. For each dataset, a number of variants are generated, using different parameters for biclusters creation and insertion, as described below:

Number of injected biclusters. Chosen values are {1,3,5}.

Bicluster size: small biclusters are usually more difficult to find than larger ones, since many biclustering methods attempt to maximize the size of the biclusters discovered. Chosen values are { 20 x 5, 50 x 10, 100 x 15} (written as “number of rows x number of columns”).

Noise leve: real data usually has noisy or missing values. In order to take this into account, bicluster values are noised, by adding a random value drawn from a gaussian distribution with zero mean. Standard deviation of this distribution determines the noise level. Chosen values are {0,2,5,10}.

Overlapping: detecting partly overlapping biclusters may be more difficult for some methods. We define the overlapping rate as the percentage of data of a bicluster that also belongs to another bicluster. Chosen values are {0%,20%}.

Hence, for each data set, 72 variants are generated, with every possible parameter configuration. 20 data sets are generated, each with 72 variants. All the algorithms considered for comparison are deterministic, except LSBC. Therefore, for LSBC, 20 runs are performed for each configuration and data set, and the average value is computed over these 20 runs. For the other algorithms, one run is enough for each configuration and data set. All experiments are performed on the same computer, running Ubuntu 14.0.1, with four cores at 3.30 GHz and 4 GB of RAM. HBC implementation is done in C++, version 4.8.2.

6.1.2 Step 1 - Biclusters Quality
We focus here on the overall quality of the set of biclusters discovered by each method. A notable difference here is that CC produces a fixed number of biclusters (default value is 20 in the R implementation), while HBC and LSBC generate an undefined number of biclusters. Because of this and to ensure fairness in the comparison, we only consider the 20 biclusters of best quality for HBC and LSBC. SpectralCoclustering and SpectralBiclustering also generate a fixed number of biclusters. However, the default value for both is 3 biclusters. This number does not allow the algorithms to correctly handle the cases where 5 biclusters are injected in the data. We choose to set this parameter to 20 biclusters generated, just as CC. Two measures are used in the comparison: quality (to minimize) and size (number of rows times number of columns) (to maximize). Tables 1 and 2 present the average and standard deviation for quality and size on the 20 data sets for each parameter configuration over noise and bicluster size. In the tables, the notation nXsY refers to a dataset where the biclusters have noise level X and size Y (s0 is 20 x 5, s1 is 50 x 10, s2 is 100 x 15). Due to space constraints, only the table for 5 inserted biclusters with overlapping, which is the most difficult case, is presented here. The algorithms’ behavior is similar on the cases that are not shown here.

TABLE 1 Average Bicluster Quality (to Minimize)
Table 1- 
Average Bicluster Quality (to Minimize)
TABLE 2 Average Bicluster Size (to Maximize)
Table 2- 
Average Bicluster Size (to Maximize)
There are several interesting observations to make from these results. First, the quality and size of the biclusters generated by HBC and CC depend largely on the size of the injected biclusters: larger injected biclusters lead to results larger and of better quality. This observation should however be confirmed in the next phase. On the other hand, LSBC, SpectralCo and SpectralBi produce results of similar quality and size regardless of the data set. LSBC generates the best biclusters (larger, and better quality) by a large margin. However, a significant drawback of this method, highlighted in the original paper [10] is its lack of diversity. Biclusters are all variants of one another, with a few rows or columns added or removed. On the metrics of quality and size, the proposed method, HBC, ranks second on all data sets. As stated before, we need a second phase of experimentations in order to understand the behavior of each method and asses their efficiency.

6.1.3 Step 2 - Finding Inserted Biclusters
We focus here on retrieving the inserted biclusters among the solutions found by the algorithms. This phase is very important, as it simulates the desired behavior for the algorithm on real data. Like previously, we only consider the 20 biclusters of best quality for the proposed method. To evaluate how efficient each algorithm is at discovering the hidden biclusters, we use the dissimilarity measure E mentioned in Section 5.2, as well as the measure U which was introduced in the same study [24]. U evaluates the part of the hidden bicluster B1 present in the bicluster B2 found by the algorithm. This value is a ratio in the [0,1] interval, with values closer to 1 being more desirable. For example, U=0.5 means that half of the inserted bicluster is found in the bicluster discovered by the algorithm.
U(B1,B2)=|rows(B1)∩rows(B2)||rows(B1)|×|cols(B1)∩cols(B2)||cols(B1)|.(7)
View SourceRight-click on figure for MathML and additional features.

When several biclusters are inserted, the best-matching bicluster for each hidden one is selected, and the averages of U and E are computed. For each inserted bicluster, we only consider, among the biclusters found by the algorithm, the one with the highest U for this hidden bicluster. Tables 3 and 4 present the average and standard deviation for U and E on the 20 data sets for each parameter configuration over noise and bicluster size, for 5 inserted biclusters with overlapping.

TABLE 3 Bicluster Recovery Measure U - Part of the Injected Biclusters Recovered by the Algorithms (to Maximize)
Table 3- 
Bicluster Recovery Measure U - Part of the Injected Biclusters Recovered by the Algorithms (to Maximize)
TABLE 4 Bicluster Recovery Measure E - Part of the Recovered Biclusters Not in the Injected Biclusters (to Minimize)
Table 4- 
Bicluster Recovery Measure E - Part of the Recovered Biclusters Not in the Injected Biclusters (to Minimize)
The results in this second phase paint a different picture of the algorithms’ performance. A first observation on the results for U is that HBC finds (at least part of) the hidden biclusters on all the variants, except the n10s0 (highest noise, smallest biclusters), which is the hardest case. Results for the E measure show that few rows and columns external to the hidden biclusters are added. On the other hand, CC only finds large parts of the hidden biclusters on the simplest cases (low or no noise, largest biclusters). LSBC, SpectralCo and SpectralBi do not discover any significant part of the biclusters and always have poor results on U and E. This confirms the intuition from the first phase: these algorithms do not discover the injected biclusters, thus their results are similar in quality and size regardless of the type of the injected biclusters. Overall, HBC is able to find, in most cases, a significant part of the inserted biclusters (high U values), and to clearly isolate these among the other data (low E values). Runtimes are similar for HBC, CC, SpectralCo and SpectralBi, with each algorithm finishing in about 10-20 seconds. On the other hand, LSBC takes several minutes to converge on these data files.

A Wilcoxon statistical test indicates that HBC performs significantly better than the other methods on U and E, and better than all methods except LSBC on quality and size, with p−values<0.05.

6.2 Comparison on Temporal Data
So far, we have assessed the efficiency of HBC on small, numeric, homogeneous data. This first validation phase was done to allow the comparison with other biclustering algorithms, and show the HBC (upon which HBC-t is built) is an efficient biclustering method. In a second phase, we evaluate HBC-t's performance on temporal data, which is the main focus of this work. We compare the proposed method with OPSM, the standard biclustering algorithm for temporal data, using the OPSM implementation available in BicAT [28]. The experimental process is similar to the first experimentations phase. We generate synthetic data with 500 rows, 100 columns, 100 percent temporal attributes and 0 percent missing values (since OPSM does not support those). Temporal data are generated in a similar way as numeric attributes, with a range of [0, 10000] instead of [0,1 00]. These values are noised in the same way as numeric data, adding a random value drawn from a normal distribution with zero mean and a variance corresponding to the noise level chosen. Tables 5 and 6 show the performances of HBC-t and OPSM, for the quality and size of the generated biclusters, and the ability to recover injected biclusters (U and E measures), respectively.

TABLE 5 Temporal Data - Quality and Size of the Biclusters
Table 5- 
Temporal Data - Quality and Size of the Biclusters
TABLE 6 Temporal Data - Recovery of the Injected Biclusters
Table 6- 
Temporal Data - Recovery of the Injected Biclusters
Table 5 shows that HBC-t and OPSM both produce very homogeneous biclusters (quality close to zero). HBC-t also tends to generate larger biclusters than OPSM, on all data sets. Table 6 shows that HBC-t is more efficient to recover injected biclusters. We observe that the performance (on U and E) for both algorithms quickly drops as the noise level increases. This may be explained by the fact that the injected biclusters for temporal data correspond to a sequence [t,t+1,t+2,…,t+m−1], where m is the number of columns of the bicluster. Therefore, any slight alteration of these values can easily change the events order and prevent these parts of the biclusters from being recovered. We do not expect to encounter such extreme cases in real applications, but it is interesting to observe the algorithm's behavior in these situations. Overall, HBC-t produces biclusters of similar quality compared to OPSM, but larger. HBC-t is also able to recover the injected biclusters where OPSM is not. Runtimes between HBC-t and OPSM are similar, around 30 seconds for each data file. Therefore, we conclude that HBC-t seems better suited to handle the kind of temporal data we are interested in. We now want to assess its efficiency on real-scale data.

6.3 Real-Scale Data
6.3.1 Synthetic Data
We generate several synthetic data sets that exhibit the properties expected in real data sets: large scale, sparsity, heterogeneity. We run the HBC-t algorithm on these data sets in order to assess its ability to retrieve biclusters in data sets similar to real life ones (i.e., the U and E measures). We use 20 data sets with 100,000 rows, 10,000 columns and 0.5 percent of non-missing values. 20 percent of the columns are numeric attributes, 20 percent are binary attributes, 20 percent are symbolic attributes and 40 percent are temporal events (the same repartition is used for the inserted biclusters). We then use these base data sets to generate several variants, inserting biclusters according to various parameters, as in Section 6.1. The other parameters take mostly the same values as in Section 6.1. However, we replace the overlapping parameter with a noise type parameter. Since allowing overlapping did not have any notable effect on the results in the previous section, we now use a fixed 20 percent overlapping rate. The noise type refers to the way of noising temporal events. A first option is to delete each event with probability k (as we do for binary attributes), a second option is to alter the timestamp value by adding noise (as we do for numeric attributes). This parameter did not have any noticeable effect on the results either. We also extended the range of chosen values for the noise level parameter to {0,2,5,10,15,20} to further evaluate the algorithm's resilience to noise. Table 7 presents the results for quality and size of the biclusters produced, and recovery rate for the injected biclusters. As in Section 6.1, we only present the mean and standard deviation (over the 20 data sets with the same parameters) for the data sets with 5 inserted biclusters, since the others exhibit the same tendencies.

TABLE 7 Bicluster Quality and Size, And Recovery Measures for the Injected Biclusters
Table 7- 
Bicluster Quality and Size, And Recovery Measures for the Injected Biclusters
The average quality of the generated biclusters tends to remain good (i.e., values close to 0), except for the cases n10s0, n15s0 and n20s0 (high noise, smallest biclusters). The biclusters’ size decreases as the noise level increases, which makes sense since adding noise shrinks the size of the perfectly homogeneous inserted biclusters. The recovery rate U also decreases as the noise increases, which is coherent with the previous observation. A surprising observation is that the recovery rates are higher for the smaller biclusters (the nXs0 cases), even though we expected these to be the hardest cases. The proportion of external values E remains at 0 for all of the experiments, which exhibits the algorithm's ability to correctly distinguish between the data. As a summary, we can say that HBC-t produces a set of high-quality biclusters in most cases, and is able to identify significant parts of the inserted biclusters even with heavy noise and with biclusters much smaller than the data set.

6.3.2 Real Data
We run the biclustering process on private real-life medical data sets extracted from a hospital's information system. These data sets form a good sample of the data we wish to be able to deal with, exhibiting heterogeneity, high sparsity and large size. Dataset MRB comes from a case study for Methicillin-Resistant Bacteria, while PSMI and PMSI_2013 are more generic and gather information on hospital stays (acts, diagnoses, prescriptions, medical units, etc.). Note that datasets PMSI and PMSI_2013 both contain a large majority of temporal attributes (medical events), and only age and gender as non-temporal. On the other hand, MRB includes a few dozens numeric and unordered symbolic attributes, describing additional data on antibiotics. Table 8 presents the main characteristics of the three studied data sets, table 9 presents the average quality and size of the biclusters found.

TABLE 8 Real Data Sets Characteristics
Table 8- 
Real Data Sets Characteristics
TABLE 9 Biclustering Results on Real Data Sets
Table 9- 
Biclustering Results on Real Data Sets
6.3.3 Case Study - Finding Coding Anomalies Using Biclustering Results
We have used the results of the biclustering process on the PMSI dataset to find anomalies in the patients trajectories across the different Medical Units (MU) inside the hospital. We expect patients witha similar profile (identical diagnoses and procedures) to follow a similar trajectory. Discrepancies could indicate unequal access to healthcare, unexpected events, or coding errors; all of which are of interest for the Department of Medical Information (DMI) at the hospital. For each bicluster found in PMSI, we find in the data the sequence of MU each patient went through, and look at the cases where different sequences occur in the bicluster. The cases were shown to the DMI in order to understand these differences. We present here a representative example, concerning a group of 583 patients admitted for cataract surgery. The bicluster includes the following features:

a diagnosis code (ICD) for cataract due to aging

a set of procedure codes for surgical operation of the cataract: anesthesia, intraocular fluid injection, and surgical removal

no event of death

ambulatory surgery

Among these patients, 581 were coded in the MU corresponding to Ophtalmology, ambulatory surgery, but two were coded in Stomatology, ambulatory surgery and Gastroenterology, ambulatory surgery, respectively. These MU do not seem adequate for cataract surgery. However, these three MU have very similar codes in the system (1746, 1745, and 1764, respectively), which means these two cases were coding errors. We found several other cases of miscodings and brought them to the attention of the DMI, who corrected the data. Apart from miscodings, other cases concerned patients temporarily hosted in a different service due to a lack of beds in the primary MU, diagnosis codes used in the wrong situation by care providers, and unexpected complications (e.g., a stroke) during a patient stay. Overall, these findings show that the biclusters are able to identify complex realities in the healthcare process, and can help with data quality control.

SECTION 7Conclusion
In this study, we define the problem of biclustering on heterogeneous and temporal data. We present HBC-t, a new biclustering method designed to find biclusters with constant values on columns amongst these data. This method is also able to deal with the high dimensionality and sparsity that are common in real data, and especially in the medical data that are the topic of this work. The proposed method is an extension of HBC, which uses a constructive greedy heuristic to build biclusters by iteratively adding columns and removing rows. HBC-t adds the elements required to handle the temporal events ubiquitous in clinical data: data model, quality measure and modified biclustering algorithm. Postprocessing tools are proposed to improve the quality and usability of the biclusters in real-life scenarios. Experimental results on synthetic data show that HBC-t outperforms several other biclustering methods, and is the best suited to generate solutions of good quality and to recover biclusters hidden in the data. Comparison with OPSM, the standard method for temporal data, shows that HBC-t performs consistently better. Experiments on heterogeneous real-life data, with both temporal and non-temporal data, show that the method scales well on large sparse sets, exhibiting good performance and reasonable runtime. The biclusters found on the real data correspond to real groups of patients, sharing a set (or sequence) of common events associated to a pathology.

Like most unsupervised methods, the main limitation of this algorithm lies in the difficulty to use the results. However, as shown in the case study, asking the right questions can help leverage the results to find valuable insights. A visualization tool for the biclusters would be helpful, but the complex nature of the data raises many challenges to solve before such a tool can be developed. Another limitation is that biclustering often produces results which are obvious from a domain expert's viewpoint, and can benefit from an iterative process of manual feature selection to improve the relevance of the biclusters. This issue seems inevitable for an unsupervised algorithm applied to massive data, but could be reduced by using domain ontologies to capture features representing the same information, or by building a more interactive tool to facilitate the feature selection depending on the user's needs.

An interesting characteristic is that HBC-t can be easily extended to other data types, requiring only a quality metric and a simplification process (if needed). A first interesting development would consist in extending HBC-t to handle times series, which are common in hospital data. Also, a distributed version of HBC-t could make use of the parallel nature of the algorithm, where each bicluster is built separately, in order to speed up the biclustering process.