Temporal point process (TPP) is an expressive tool for modeling the temporal pattern of event sequences. However, discovering temporal patterns for event sequences clustering is rarely studied in TPP modeling. To solve this problem, we take a reinforcement learning view whereby the observed sequences are assumed to be generated from a mixture of latent policies. The purpose is to cluster the sequences with different temporal patterns into the underlying policies while learning each of the policy model. The flexibility of our model lies in: i) all the components are networks including the policy network for modeling the temporal point process; ii) to handle varying-length event sequences, we resort to inverse reinforcement learning by decomposing the observed sequence into states (RNN hidden embedding of history) and actions (time interval to next event) in order to learn a reward function, it helps to achieve better performance or increasing efficiency compared to existing methods using rewards over the entire sequence such as log-likelihood or Wasserstein distance. We adopt an Expectation-Maximization algorithm, in E-step estimating the cluster labels for each sequence, in M-step aiming to learn the respective policy. Extensive experiments on synthetic and real-world datasets show the efficacy of our method against the state-of-the-arts.
SECTION 1Introduction
Event sequences with time stamps in continuous domain are ubiquitous across different areas and applications. In e-commerce, online purchase records over time can form event sequences. In health informatics, a series of treatments taken by patient can be tracked as an event sequence. In seismology, a sequence of earthquakes can be recorded. In social media like Twitter, every time a user posts, transmits or likes a tweet, it corresponds to a new event adding to the user behavior sequence. In these various domains, recognizing and understanding the structure in the event sequences is of vital importance for downstream applications. Lots of literatures have been proposed for modeling these event sequences. For example, in health informatics, [1] introduces models describing clonal evolution in tumour development and determining the temporal sequence of mutational events, [61] presents a method learning temporal relationships between sequential medications for predicting the next medication likely to be prescribed for the patient, and in [66] the patient flow between intensive care units (ICU) departments is predicted via mutually correcting point processes. In seismology, sequences of earth earthquakes are recorded and studied in [36], [38], [44], [45]. In social media and networks, [73] and [32] learns social infectivity of the user given observed recurrent and time-stamped events occurring at the individuals involved in the social network, [31] models the interactions and infers the influence between pairs of actors through Hawkes processes considering the dependency among dyadic events, and in [19] the problem of how to optimize multi-stage campaigning process over social networks is considered.

For modeling event sequences, temporal point processes (TPP) [8] are useful tools whereby one key concept is to model the event occurrence rate over time using the conditional intensity function. Lots of literatures have been proposed for both data-mining task and prediction task. For example, [74] uses the so-called multi-dimensional Hawkes processes to model the sequential user actions in a social network, and the learned infectivity matrix is useful for uncovering the mutual influences between users. Mixtures of Hawkes processes [32] are modeled for inferring missing event attributes from the behavioral observation by considering the dependency among dyadic events. Temporal point processes are also widely used for event prediction. In [69], a water pipe failure prediction system is designed for effective replacement and rehabilitation, in which the water pipe failure sequence is formulated as a self-exciting stochastic process. In [66], a mutually-correcting point process is proposed for patients flow prediction in hospital. It can predict the destination Care Unit (CU) transition and duration of each CU occupancy, such that the critical care resource can be better allocated.

In this paper, we are devoted to another important but relatively less studied scenario: event sequences clustering in the continuous time domain, which can be more challenging than the traditional (aggregated and discrete) time series clustering. Event sequences clustering can find its utility in many real-world applications. Given a number of event sequences, it is important to discover and learn the underlying clustering structure robustly. For example, the purchase records can help cluster e-commerce users into different groups to benefit a recommender system; clustering patients according to their treatment logs helps hospitals optimize medication resources.

Moreover, the learned model, often in the form of generative models, can be used for group-specific event sequence generation, which is useful when the raw data is sensitive for sharing. For instance, one aims to divide the whole population into several groups and simulate their behaviors using the deployed model without accessing the raw data. This may also be useful for developing a simulated stock exchange system with multiple AI players.

However, despite the extensive existing works on event sequence modeling and prediction as mentioned above, event sequence clustering and especially learning mixture model of event sequences have rarely been addressed, except in [67] to our best knowledge. In [67], a parametric likelihood based latent Dirichlet allocation model is proposed for event sequences clustering using Hawkes processes. In this paper, we propose a deep reinforcement learning (RL) based EM algorithm for event sequence clustering using likelihood-free temporal point processes learning, and the purpose is to discover and learn these underlying policies of experts which can be shared over similar sequences.

The rationale of using RL is that as event sequences can be viewed as the actions taken by experts,1 whereby the cost can timely reflect the fitting deviation in contrast to traditional likelihood based models [8], [67], [74] which require the observation of entire sequence. Moreover, we believe such cost need be learned by inverse reinforcement learning (IRL), especially considering the difficulty of defining the distance between TPP or event sequences of unfixed length over time [63]. We also show in experiments that by decomposing the observed sequences into states and actions from the IRL view to learn reward (i.e., cost) function, our algorithm achieves much more efficiency than that in [63] which computes the cost over the entire sequence.

Inspired by model-based clustering methods like gaussian mixture model [55] (GMM) and GAN mixture model [72] (GANMM), we propose a novel policy mixture model (PMM) to solve the problem of event sequences clustering. A main difference between our work and existing ones is that, the components in GMM are gaussian distributions and in GANMM they are generative adversarial networks, while in the proposed PMM model the components are policy networks with an end-to-end EM learning method.

The concept of policy network can be found in the area of deep reinforcement learning (DRL) [34], where a neural network is used to approximate the policy function. In recent years, deep learning find its prosperity in various domains including reinforcement learning. DRL combines deep learning and reinforcement learning principles and creates efficient algorithms that can be applied in areas like robotics, video games, finance and healthcare [21]. In this paper, recurrent neural networks (RNN) is used as policy network for approximating the policy function. Similar utilization of RNN as policy network can be found in [33] for modeling temporal point processes and [71] for sequence generation.

The mostly related work to our method is the Dirichlet mixture model of Hawkes processes (DMMHP) in [67] as it deals with a similar problem setting: grouping event sequences into different clusters and learn the respective TPP model parameters for each cluster. However, the technical approaches are completely different. First the parametric model [67] is tailored to Hawkes process while our network based model is more general; Second, the work [67] is under the Bayesian probabilistic framework while our method is likelihood-free and incorporates both adversarial learning and inverse reinforcement learning [42] for more effective objective design beyond MLE; We show in the experiments that our method significantly outperforms [67] on real-world data.

As shown in Fig. 1, this paper takes a reinforcement learning perspective on the modeling and clustering of temporal point processes for its dynamic sequence nature. Though there exist works [18], [33], [58] on (deep) RL and intervention of TPP, while little effort ([67] does not involve deep model nor RL) has been paid on TPP clustering which calls additional careful treatment on disentangling the mixture of policies. Using the language of RL, suppose there is a number of event sequences generated (with noise) by N underlying expert policies, which can be reflected in the form of N clusters. In this sense, we formulate the event sequence clustering task as a reinforcement learning problem whereby the purpose is to discover the unknown event generation policies, and meanwhile the learning cost function for fitting event sequences is also automatically learned from data using Inverse Reinforcement Learning.

Fig. 1. - 
The proposed EM algorithm for TPP clustering using deep RL. Dataset $D$D is supposed to be generated by a mixture of $N$N latent policies $\lbrace \pi _n\rbrace _{n=1}^N${πn}n=1N. In E-step, fix the learned $\lbrace \pi _n\rbrace _{n=1}^N${πn}n=1N and update the $N$N-class classifier $h_q$hq by using $N$N sets of event sequences $\lbrace S_n\rbrace _{n=1}^N${Sn}n=1N generated from $\pi _n$πn with cluster label $n$n. In M-step, fix classifier $h_q$hq and update each of $\lbrace \pi _n\rbrace _{n=1}^N${πn}n=1N by subset $D_i$Di classified by $h_q$hq.
Fig. 1.
The proposed EM algorithm for TPP clustering using deep RL. Dataset D is supposed to be generated by a mixture of N latent policies {πn}Nn=1. In E-step, fix the learned {πn}Nn=1 and update the N-class classifier hq by using N sets of event sequences {Sn}Nn=1 generated from πn with cluster label n. In M-step, fix classifier hq and update each of {πn}Nn=1 by subset Di classified by hq.

Show All

In summary, the contributions of this paper include:

We take an RL view to the TPP clustering problem, introducing the policy mixture model to the family of model-based clustering methods. Different from existing mixture models like GMM [55], GANMM [72] or DMMHP [67] whose components are gaussian distributions, generative adversarial networks or conventional Hawkes processes, the components in our model are policy networks which can handle varying-length sequences with less parameters using reinforcement learning. Each cluster corresponds to one of the latent expert policies that generates the observed sequences in the cluster.

We present an end-to-end EM learning algorithm for the PMM model. Though similar algorithm is used for image clustering using GAN mixture models in [72], we adapt the method for event sequence clustering using RL to discover underlying temporal patterns. Differing from previous work using parametric clustering models [67], in the EM algorithm, clustering of the entire dataset and model fitting of each cluster are jointly performed rather than separated in two steps.

For each component policy network, i.e., each TPP model learning the temporal pattern from the view of RL, we implement generative adversarial imitation learning [27] (GAIL) as an efficient IRL embodiment. Instead of conventional MLE loss, an adversarial loss is learned by the IRL procedure. Our method can be seen as a meta learning approach by adopting IRL to learn policy's reward function.

We empirically show that our method exceeds peer models including state-of-the-art ones [67], [72]. Notably to make a fair comparison with existing Hawkes Mixture Model DMMHP [67], experiments are conducted on both Hawkes and non-Hawkes synthetic data to recover the underlying temporal patterns of the generated sequences. We also compare with the recent mixture GAN based (image) clustering model [72] and adapt it to temporal event sequences by using the Wasserstein distance between point processes according to [63] for a fair comparison. The results show that our model outperforms significantly regarding training efficiency (one order faster), with similar performance.

SECTION 2Related Work
In general, our work is a model-based clustering method for clustering event sequences into different time patterns, with each time pattern modeled by a temporal point process model using reinforcement learning. Two domains of knowledge is related to our work, including Temporal Point Processes and Model-based Clustering. In addition, another different but similar topic of Event clustering in sequential data is also included in this section. Though they aim at clustering different entities, one is clustering independent single event and our work is clustering the time-series in event sequence, discussing the differences between our work event sequence clustering and some previous literatures about event clustering in sequential data can help to better explain the objective of this paper.

2.1 Temporal Point Processes
We review TPP methods in literature in two aspects: i) modeling of the intensity function; ii) learning objectives and algorithms. The relevance lies in that both the intensity function and learning objective relate to latent policy discovering and learning.

2.1.1 Temporal Point Process Intensity Modeling
Traditional TPP models are mostly developed around the design of the intensity function λ(t) which measures the instantaneous event occurrence rate at time t, like Reinforced Poisson processes [47], Self-exciting process (Hawkes process) [26], Reactive point process [17], etc. An obvious limitation of these traditional models is that they all assume all the samples obey a single parametric form which is too idealistic for real-world data. This also suggests the need for learning clustered behaviors beyond single model based methods. Moreover, tailored algorithms are often needed for learning specific models.

By contrast, in neural point process [13], [39], [65], recurrent neural network and its variants e.g., long-short term memory (LSTM) are used for modeling the conditional intensity function over time. As such, the learning can be generally fulfilled by gradient descent and no restriction on the form of the intensity function is imposed. More recently attention mechanisms are introduced to improve the interpretability of the neural model [60]. However, in all these models, the event sequences are all fed into the model without any discrimination to the groups they may differently belong to. In fact, the behind mechanisms for generating each event sequence can be very different, thus it may be a better idea to learn one model for each cluster of similar samples for more accurate modeling.

2.1.2 Temporal Point Process Learning
There are alternative objectives for TPP learning for both parametric models and neural models. Traditional methods mostly follow the maximum likelihood estimation (MLE) procedure under the probabilistic framework [46]. While the MLE objective may not be the only choice. This is because we are often given only a limited number of sequences which may further contain arbitrary noises. Recent efforts have been made for devising adversarial learning based objective inspired by generative adversarial networks (GAN) [23] and especially Wasserstein GAN [2]. In [68], adversarial objective is developed in addition with MLE loss by approximating the continuous domain predictions using a discrete time series. In [63], Wasserstein distance over temporal event sequences is explicitly defined to learn a deep generative point process model for temporal events generation. It is further developed into a prediction model [64] via conditional GAN.

Another line of works consider the challenge for learning high-dimensional TPP models, whereby the so-called infectivity matrix to be learned can be of squared size of the dimensionality. One popular technique is imposing low-rank regularizer [73] or factorization model [62] on the infectivity matrix. However, they do not explicitly deal with the sequence clustering problem. In fact, the observed dimension marker does not correspond to the underlying cluster.

2.2 Model-Based Clustering
Model-based clustering techniques have been widely used and have shown promising results in many applications involving complex data. Clustering or segmentation of data is a fundamental data analysis step that has been widely studied across multiple disciplines for over 40 years [14], [25], [28], [54].

For model-based clustering approaches, the model type is often specified a priori, such as Gaussian or hidden Markov models (HMMs). The model structure (e.g., the number of hidden states in an HMM) can be determined by model selection techniques and parameters estimated using maximum likelihood algorithms, e.g., the EM algorithm [12]. Probabilistic model-based clustering techniques have shown promising results in a corpus of applications. Gaussian mixture models are the most popular models used for vector data [4], [20], [22], [55], [70]; multinomial models have been shown to be effective for high dimensional text clustering [40], [59]. By deriving a bijection between Bregman divergences and the exponential family of distributions, [3] shows that clustering based on a mixture of components from any member of this vast family can be done in an efficient manner.

Existing works on model-based clustering largely concentrate on a specific model or application. A notable exception is the work of [7] who proposed an EM framework for partitional clustering with a mixture of probabilistic models. Essentially, this work is EM clustering with an emphasis on clustering of non-vector data such as variable-length sequences. The majority of model-based clustering methods are based on the maximum likelihood formulation [4], [22], [55]. Early work focused on different types of normal distributions. For example, [4] discussed clustering with a mixture of constrained Gaussian models, and [20] described efficient hierarchical algorithms for special cases of Gaussian models.

2.3 Event Clustering in Sequential Data
While this paper aims at clustering event sequences in continuous time domain, it is easy to confuse our work with another different but similar topic of event clustering, especially with works about identifying and clustering events in sequential data. For example, in [10] a semantics-driven event clustering algorithm is proposed to detect events in sequential Twitter feeds data, the semantic informations together with temporal, geographic and community features are used to achieve the task. In [11], a systematic framework is presented for correlating, predicting and clustering the dynamic behaviors or traces for event logs, mainly based on predefined criteria. In its clustering part, each trace is associated with one single event and each cluster groups trances that record executions of process instances with similar outcomes. Most recently in [50], a spatio-temporal event detection and clustering algorithm is developed for identifying and indexing events in sensor data from wearable devices, one can identify and cluster similar events that occur at about the same time interval in consecutive days e.g., cluster regular visits to the gym.

In these literatures, the objects to be clustered are separated events and the temporal information serve as a event-specific feature. While in this paper, the objects to be clustered are event sequences as a whole, and the temporal information describing the occurrence of the events serve as sequence-specific features, which defines a problem which has been rarely studied in previous literature, and we solve it from the view of reinforcement learning in this paper.

SECTION 3Proposed Model
We present our approach under the expectation-maximization (EM) algorithm–a natural way for disentangling the clustering and fitting of each cluster of event sequences which are assumed to be generated by a latent expert policy. In expectation step, each sequence is assigned with a cluster label corresponding to its latent policy that generates this sequence. The latent policy of each cluster is learned in maximization step. The model is illustrated in in Fig. 1. The complete learning algorithm is shown in Algorithm 1 which calls an IRL based subfunction GAIL-TPP described in Algorithm 2.

SECTION Algorithm 1.Reinforcement Learning for Policy Mixture Model (RLPMM) for TPP
Require: dataset D, number of policies N; training set size m=256 for classifier hq; learning rate α=1e−4,β=1e−4; k=0.

randomly initialize classifier hq, N policies’ parameters θ(0)i, discriminators’ parameters w(0)i for i=1,2,…,N;

randomly divide D into {D(0)1,D(0)2,…,D(0)N};

{θ(1)i,w(1)i}Ni=1←GAIL−TPP(θ(0)i,w(0)i,D(0)i);

//E-step: L6, L7, L8; M-step: L9

while πθ not converged do

sample a training set S={xj,yj}mj=1 from {πθ(k)i}Ni=1 with probability |D(k−1)i||D|;

train classifier hq using S;

compute policy yi=hq(xi) to label D into {Di}Ni=1;

{θ(k+1)i,w(k+1)i}Ni=1←GAIL−TPP(θ(k)i,w(k)i,D(k)i);

k=k+1;

end while

Ensure: learned N latent polices π∗θ={πθ(k)i}Ni=1

3.1 EM Learning for Policy Mixture Model
Given a temporal event set X with M observed event sequences: X={x1,x2,…,xM} and the discrete latents i.e., cluster labels Y={y1,y2,…,yM} for yi∈{1,2,…,N}, we suppose that X are generated by a mixture of N experts with a latent policy for each expert, parameterized by θ as a whole. The log likelihood is
L(θ;X,Y)=logp(X,Y|θ),
View Sourcewhere p(X,Y|θ) is the conditional probability of observing X,Y given parameter θ, and θ is determined by maximizing the marginal log likelihood of observed X
θ∗=argmaxθL(θ;X)=argmaxθlogp(X|θ),
View Sourcewhere p(X|θ)=∫p(X,Y|θ)dY is the marginal probability.

Suppose the latents Y are sampled from an arbitrary valid probability distribution q(Y), then a lower bound F(q,θ) of the marginal log likelihood L(θ;X) can be obtained by Jensen's inequality
L(θ;X)=≥==log∫p(X,Y|θ)dY=log∫q(Y)p(X,Y|θ)q(Y)dY∫q(Y)logp(Y|X,θ)p(X|θ)q(Y)dY∫q(Y)logp(X|θ)dY+∫q(Y)logp(Y|X,θ)q(Y)dYL(θ;X)−DKL(q||p),(1)
View SourceRight-click on figure for MathML and additional features.where DKL(q||p) is the KL-divergence between q(Y) and p(Y|X,θ), and we have the lower bound
F(q,θ)=L(θ;X)−DKL(q||p).(2)
View SourceRight-click on figure for MathML and additional features.Equation (1) gives the relation between hidden variable distribution q(Y) and the likelihood function of the observed data L(θ;X): ∀q(Y):L(θ;X)≥F(q,θ) and L(θ;X)=F(q,θ) if and only if q(Y)=p(Y|X,θ) so that DKL(q||p)=0.

Based on Eq. (1), given randomly initialized parameter θ(0) and arbitrary distribution q(0), we have the following EM procedure that iteratively updates q(k) and parameter θ(k):

E-Step. given model parameter θ(k), update q(k) to q(k+1) by matching q to posterior p(Y|X,θ(k)), so that L(θ(k))=F(q(k+1),θ(k)).

M-Step. given q(k+1), update θ: θ(k+1)←θ(k)+∇F(q(k+1),θ) by maximizing F(q(k+1),θ), so that we have: F(q(k+1),θ(k+1))>F(q(k+1),θ(k)).

By Eq. (1), we have: L(θ(k+1))≥F(q(k+1),θ(k+1))>F(q(k+1),θ(k))=L(θ(k)), which suggests that for each iteration with E-step and M-step, L(θ;X) converges to its maximum. We iteratively perform E-step and M-step until θ∗, and the distribution of the hidden variable Y is: q∗(Y)=p(Y|X,θ∗).

Specifically, the computational components in E-step and M-step are all implemented by neural networks as the Reinforcement Learning with Policy Mixture Model (RLPMM), including the E-step for policy clustering and M-step for policy learning.

Algorithm 2. Generative Adversarial Imitation Learning [27] for TPP: GAIL-TPP (θ(k)i,w(k)i,Di)
Require: set Di, discriminator parameter w(k)i, policy net θ(k)i

sample sequence xi∼πθi

IRL: update discriminator parameters from w(k)i to w(k+1)i with gradient computed by Eq. (10)

RL: update policy parameters from θ(k)i to θ(k+1)i with gradient computed by Eq. (11)

Ensure: parameter of policy networks θ(k+1)i, and discriminators w(k+1)i

3.2 Expectation Step for Policy Clustering
As mentioned above, in E-step, we match the hidden variable distribution q to posterior distribution p(Y|X,θ(k)), and fill in values of latent variables Y for samples in observed data X according q(Y), so that we can re-recompute the expectation of X given θ(k), i.e., the likelihood function L(θ(k)).

We compute the hidden variable distribution q as
q(k)=argminq∈HkKL(q||p(k)),(3)
View Sourcewhere we restrict the distribution of hidden variable q(k) is in a bounded hypothesis space Hk.

For mixture of policies, given parameter θ(k) and observed data X, the posterior distribution p(k) is
p(yij|xj,θ(k))=p(xj,yij|θ(k))p(xj|θ(k)),(4)
View Sourcewhere yij=1 if and only if xj is generated by the ith policy.

Inspired by Eq. (4), to find q(k) in Eq. (3), we train a classifier to fit the current guess of the discrete hidden variable distribution q(k) to p(k), i.e., holding the policies parameter θ(k) fixed, train a classifier hq by data generated by learned policies. Therefore, the E-step involves line 6, 7, 8 in Algorithm 1.

In practical application, hq is a 3-layer classifier including sequence embedding layer, RNN layer and classification layer as used in [13], [65]. In addition, an implementation trick dealing with imbalanced classification at the beginning of the procedure is used. As we employed the E-step by training a classifier samples generated by the learned policies, the classifier is easy to assign clusters with imbalanced instances, particularly at the beginning of the procedure. The imbalance could get reinforced through the EM procedure. As a result, some policy models receive more and more data and the remaining get fewer and fewer. Eventually the RLPMM model would converge to an imbalanced solution.

To fix this issue, at the beginning of the EM procedure, we allow a policy model to explore new data by augmenting their training data. After the assignment of clusters D={D1,…,DN}, we augment each cluster data set Di by adding an amount of instances from the complement set Dci={x∈D|x∉Di} with the highest posterior probability of belonging to the ith cluster according to the output of the classifier. The amount of the augmented data is reducing along the procedure for the convergence.

3.3 Maximization Step for Policy Learning
Given the hidden variable Y estimated by the classifier in E-step, each event sequence x in the training dataset D is classified to a specific policy with discrete hidden variable yi. In M-step, dataset D is divided into N clusters {D1,D2,…,DN} to train each policy model.

Now we present an IRL based policy learning method. Differing from previous works imposing a specific form for sequence learning e.g., Wasserstein distance [63], we assume the policy reward (cost) is unknown which need be learned via IRL. The rationale is that it is nontrivial to define the temporal event sequence fitting error with varying length in contrast to the vector-like data. To make the learning scalable to real-world data, the IRL procedure is efficiently fulfilled by a generative adversarial imitation learning scheme [27].

3.3.1 Policy Network for Sequence Generation
The policy function πθ should have the capacity to capture the complex sequential dependency pattern and stochastic nature in the point process. We adopt RNN with stochastic neurons [6] as the policy network. Here action refers to the time to next event from current event timestamp and state refers to the hidden embedding of RNN for the history. Note action a is sampled from distribution π(a|θ(hi−1)) as
ai∼π(a|θ(hi−1)),(5)
View Sourcewhere hi=ψ(Vai+Whi−1), ai=ti−ti−1∈R+ is the ith inter-event time (t0=0), hi∈Rd is hidden state of RNN encoding history before ti, θ is nonlinear mapping from R to policy's parameter space, V∈Rd and W∈Rd×d are coefficients, ψ is nonlinear activation function, e.g., ψ(z)=ez−e−zez+e−z as used in this paper.

There are alternatives for parameterizing the policy function, i.e., the probability density function π, as long as they satisfy the constraint that the random variable sampled from π is positive since a>0, such as exponential distribution: π(a|θ(hi−1))=θ(h)e−θ(h)a and Rayleigh distribution: θ(h)ae−θ(h)a2/2 as used in this paper.

So far the RNN policy network with stochastic neurons is able to mimic the event generating mechanism of stochastic temporal point process by Eq. (5). Given a sequence of past events st={ti}ti<t, the next event time is generated as ti+1=ti+a, with the inter-event time a sampled from stochastic policy πθ(a|st) as the action.

When the cost function c(s,a) is given, then the optimal policy π∗θ can be computed directly by
π∗θ=argminπθ∈GE[c(s,a)],(6)
View SourceRight-click on figure for MathML and additional features.where G is the family of stationary stochastic policies taking actions in action space A given states in state space S, Eπ[c(s,a)]≜E[∑∞t=0c(st,at)] is the expectation to the sequence it generates.

While for varying-length temporal event sequences, its fitting cost function c(s,a) can be difficult to define beforehand. It is attractive to first use IRL (inverse reinforcement learning) to compute the optimal function c∗, then compute π∗θ using RL in each iteration.

Specifically, we adopt the maximum causal entropy IRL for policy learning [76]. The optimal cost function is given by the inverse reinforcement learning procedure IRL(πE)
c∗=argmaxc∈C(minπθ∈G(−H(π)+Eπ[c(s,a)])−EπE[c(s,a)]),(7)
View Sourcewhere H(π)≜Eπ[−logπ(a|s)] is the causal entropy [75] regularizer of policy π to find high-entropy policies robustly. The optimal policy is updated by reinforcement learning procedure RL(c)
π∗θ=argminπθ∈G(−H(π)+Eπ[c∗(s,a)]).(8)
View SourceRight-click on figure for MathML and additional features.

The final optimal policy can be obtained by iteratively executing IRL(πE) and RL(c), expressed as
π∗θ=RL∘IRL(πE).(9)
View Source

Solving Eq. (9) can be quite tedious, as in each iteration, IRL(πE) in Eq. (7) requires to solve an inner loop RL problem. To conquer this problem, we use the Generative Adversarial Imitation Learning framework [27] for reinforcement learning. Instead of using Eq. (7) to compute the cost function c(s,a), a discriminator network Dw:S×A→(0,1) with parameter w is trained to discriminate between expert's true policy actions (s,a)πE and learned policy actions (s,a)πθ. Here we only explain the general mechanism of using GAIL, and detailed derivations can be found in [27].

As shown in Fig. 2, we use the GAIL framework to learn the cost function for reinforcement learning, in which the IRL procedure is substituted by training a Discriminator Dw, with the gradient of discriminator parameter w given by
Exθ[∇wlog(Dw(s,a))]+ExE(∇wlog(1−Dw(s,a)),(10)
View SourceRight-click on figure for MathML and additional features.where xθ∼πθ is the sequences sampled from learned policy πθ and xE∼πE is the sequences sampled from expert's true policy. Actually the discriminator offers a local cost function that provides learning signal to the policy. In each iteration, we take a policy step by policy gradient to decrease the expected cost with respect to the cost function c(s,a)=logD(s,a), so that the learned policy moves toward the expert-like regions of the state-action space in each iteration.

Fig. 2. - 
Generative Adversarial Imitation Learning for TPP employed in M-step (see policy $\pi _i$πi in green block in Fig. 1): Given subset $D_i$Di for policy $\pi _i$πi, for reward (cost) function learning in IRL-step, discriminator is trained by Eq. (10) using generated fake sequences and real sequences. In RL-step, policy network is trained by policy gradient using Eq. (11), using the learned cost. Back to Fig. 1, sequences $\lbrace S_i\rbrace${Si} generated by policy network are used to train classifier $h_q$hq in E-step.
Fig. 2.
Generative Adversarial Imitation Learning for TPP employed in M-step (see policy πi in green block in Fig. 1): Given subset Di for policy πi, for reward (cost) function learning in IRL-step, discriminator is trained by Eq. (10) using generated fake sequences and real sequences. In RL-step, policy network is trained by policy gradient using Eq. (11), using the learned cost. Back to Fig. 1, sequences {Si} generated by policy network are used to train classifier hq in E-step.

Show All

Given the cost function logDw(s,a), the RL procedure is implemented by the policy gradient descent with gradient of policy network parameter θ given by
Exθ[∇θlogπθ(a|s)Q(s,a)]−λ∇θH(πθ),(11)
View SourceRight-click on figure for MathML and additional features.where Q(s¯,a¯)=Exθ[log(Dw(s,a))|s0=s¯,a0=a¯]. And the gradient of the causal entropy regularizer is given by
∇θH(θ)=∇θEπθ[−logπθ(a|s)]=Eπθ[∇θlogπθ(a|s)Qlog(s,a)],(12)
View SourceRight-click on figure for MathML and additional features.where Qlog(s¯,a¯)=Eπθ[−logπθ(a|s)|s0=s¯,a0=a¯]

In essence, by iteratively updating w using IRL in Eq. (10), and updating θ using RL in Eq. (11), the GAIL algorithm finds a saddle point (π∗θ,D∗w) of the expression
Eπθ[log(D(s,a))]+EπE[log(1−D(s,a))]−λH(π),(13)
View SourceRight-click on figure for MathML and additional features.which is equivalent to find the optimal policy π∗θ.

SECTION 4Experiments
Existing work [67] is specially designed for clustering event sequences assumed to be generated from parametric Hawkes processes, while our work is for clustering event sequences from any point process models not limited to Hawkes processes. For this reason, only experimenting on clustering event sequences generated by non-Hawkes point processes is unfair for previous work [67] and not sufficient to prove the efficiency of the proposed model. In order to make a fair comparison with existing parametric event sequence clustering in [67] which is based on Hawkes processes, we experiment on two synthetic datasets, one dataset generate by by Hawkes processes and one generated by non-Hawkes processes respectively, as shown in Table 2. We also experiment on a real-world dataset MemeTracker [29] for discovering the temporal patterns of the meme diffusion process, as shown in Table 3 and Fig. 4.

TABLE 1 Comparison Including WGANMM From [72] Which is Adapted to TPP Data in This Paper
Table 1- 
Comparison Including WGANMM From [72] Which is Adapted to TPP Data in This Paper
TABLE 2 Mean and Standard Deviation (SD, in Bracket) of Metrics by Clustering (CP, RI) and Policy Fitting (EID) on Synthetic Data Generated by Non-Hawkes and Hawkes Process (Both Up to Four Intensities)
Table 2- 
Mean and Standard Deviation (SD, in Bracket) of Metrics by Clustering (CP, RI) and Policy Fitting (EID) on Synthetic Data Generated by Non-Hawkes and Hawkes Process (Both Up to Four Intensities)
TABLE 3 Clustering Consistency on MemeTracker (in Average Over Trials–See Clustering Consistency's Definition)
Table 3- 
Clustering Consistency on MemeTracker (in Average Over Trials–See Clustering Consistency's Definition)
Fig. 3. - 
Ground truth intensity and estimated ones on synthetic data generated by $K=4$K=4 policies.
Fig. 3.
Ground truth intensity and estimated ones on synthetic data generated by K=4 policies.

Show All


Fig. 4.
Estimated intensity functions on MemeTracker: a) one intensity for all data; b) K=4 intensities by clustering.

Show All

In the following subsections, we present the compared baseline methods, our adaption to [72] for temporal point processes, the evaluation metrics used and the experimental results.

4.1 Compared Methods
We evaluate the performance of our Reinforcement Learning for Policy Mixture Model on both synthetic and real-world data. To demonstrate the effectiveness and efficiency of our model, we compare with state-of-the-art methods for event sequence clustering.

As summarized in Table 1, peer methods include: 1) Gaussian Mixture Model includes 3 Two-step pipeline models that first extract features from sequential events using Vector Auto-Regression (VAR) [24], Ordinary Differential Equation (ODE) [30] or Least Squares (LS) [16], the use the GMM to cluster the event sequences; 2) Dirichlet Mixture Model of Hawkes Processes [67] which is the most related work to our knowledge; 3) Wasserstein Generative Adversarial Network Mixture Model (WGANMM) that we adapt from [72].

For the convenience of reproducibility, we present the technical details of the baselines used in the paper as follows:

1) Gaussian Mixture Model. To tackle the sequential data clustering problem, traditional methods usually implement aggregated time series clustering with discrete time-lagged variable [35], [37]. These methods use a probabilistic mixture model to perform sequence clustering with two procedures: first extracting features from sequential data, then identifying clusters via GMM [49], [56]. We use three methods to extract features from sequential events for the Gaussian Mixture Model (GMM), including:

Vector Auto-Regression. The VAR [24] model discretizes event sequences to time series, and learns a transition matrix as features for clustering.

Ordinary Differential Equation. We also use a nonparametric Hawkes process model [30] based on Ordinary Differential Equation.

Least Squares. We also test another nonparametric Hawkes model based on contrast function in [16] relating to the Least Square problem.

Both ODE and LS learn a Hawkes process for each sequence. In line with the protocol in [67], we use its parameter θ=[μ,α] as feature for each sequence, and employ GMM for clustering.

2) Dirichlet Mixture Model of Hawkes Processes. Learning mixture of policies for TPP has been rarely considered in literature, the most related work to our best knowledge is DMMHP [67]. It generates event sequences with different clusters from Hawkes processes of different parameters, and uses a Dirichlet distribution as clusters’ prior.

Both our model and DMMHP are model-based methods that can accomplish temporal processes clustering. The differences lie in that DMMHP use conventional parameterized Hawkes process and cluster with Latent Dirichlet Allocation (LDA), we propose a novel generative adversarial imitation learning point process model.

3) Wasserstein Generative Adversarial Network Mixture Model. Similar to Gaussian mixture model, a GAN mixture model in [72] has been used for image clustering with fixed sized matrix-like data. In [72] N GAN models are trained to capture the each cluster's distribution respectively. To adapt its processing domain from image to event sequences in continuous domain (TPP), we modify vanilla WGANMM by replacing CNN with RNN and introduce the Wassertein distance between point processes as proposed by [63] for adversarial learning, which is presented in the next subsection.

4.2 Adapting [72] to WGANMM for TPP
For notation clearness, we slightly abuse the notations by assuming the used notations in this subsection is separated to the rest of the paper. Suppose for the ith cluster, training data Di={x1,x2,…} is generated by the oracle TPP r and Si={s1,s2,…} is generated by the learned TPP g, then the Wasserstein distance between the distributions of the two point processes is given by
W(Pr,Pg)=infϕ∈Φ(Pr,Pg)E(x,s)∼ϕ[||x−s||⋆],(14)
View SourceRight-click on figure for MathML and additional features.where Φ(Pr,Pg) denotes the set of all joint distributions ϕ(x,s) whose marginals are Pr and Pg, g is learned by minimizing W(Pr,Pg).

As Eq. (14) is computationally intractable, hence the dual form is used [2] to compute W(Pr,Pg) as
maxw∈W,||fw||L≤1Ex∼Pr[fw(x)]−Es∼Pg[fw(s)],(15)
View Sourcewhere fw is the Lipschitz function with parameter w∈W, that assign a value to a event sequence satisfying 1-Lipschitz constraint fw(x)−fw(s)|≤||x−s||⋆ for all x and s. As we have supposed that event sequence s is generated by gθ with parameter θ using noisy input ζ∼Pz, therefore the objective is to learn a generative model gθ by minimize W(Pr,Pg) as
minθmaxw∈W,||fw||L≤1Ex∼Pr[fw(x)]−Eζ∼Pz[fw(gθ(ζ))],(16)
View SourceRight-click on figure for MathML and additional features.where fw is discriminator and gθ is generator. Similar to [41], fw and gθ are fulfilled by RNNs.

The generator gθ transforms a noise input sequence ζ={z1,…zn} to generated sequence s={t1,…,tn} as gθ(ζ)=s using RNN with n LSTM cells
hi=ϕhg(Ahgzi+Bhghi−1+bhg),ti=ϕtg(Btghi+btg),
View Sourcewhere hi is history embedding vector, ϕhg and ϕtg are activation functions, and parameters θ={Ahg,Bhg,bhg,Btg,btg}. Similarly, the discriminator assigns a scalar value fw(ρ)=∑ni=1ai to sequence ρ={t1,…,tn} (ρ can be real data x or generated one s), also by RNN with n cells: hi=ϕhf(Ahfzi+Bhfhi−1+bhf),ai=ϕtf(Btfhi+btf), where the parameters of the discriminator w={Ahf,Bhf,bhf,Baf,baf}.

In general, the adapted WGANMM and the proposed RLPMM use a similar EM clustering algorithm. The major differences for our method to WGANMM lie in that WGANMM learns TPP by using Wasserstein distance between event sequences as cost, which may not be optimum for the clustering task at hand. While RLPMM is more like a meta-learning method which adopts IRL – and the adversarial imitation technique to learn the cost function. Also, RL is used for learning policy of each cluster by RLPMM but WGANMM involves no RL. Moreover, instead of using RNN with multiple cells to learn the discriminators and generators in WGANMM for each cluster, we use RL to learn the latent policy for each cluster. We only need to learn a LSTM cell for the each latent policy, with another LSTM cell to learn the corresponding cost function by IRL. The empirical results in the paper show that our method runs one order faster against WGANMM.

4.3 Evaluation Metrics
Metrics used to measure clustering performance are 1) Clustering Purity (CP) [53]; 2) Rand Index (RI) [48]; 3) Empirical Intensity Deviation (EID) [63]; 4) Clustering Consistency (CC) [57]. All the metrics except for clustering consistency (as CC itself already involves random trials) are computed by the average of 10 trials on the whole dataset by random initialization for clustering.

The detailed definitions of the metrics used in paper include:

Clustering Purity: Purity is the average of portion of true positive class in each cluster [53]
Purity=1M∑k=1Kmaxi∈{1,…,K′}|Wk∩Ci|,
View SourceRight-click on figure for MathML and additional features.where Wk is the learned index set belonging to cluster k, Ci is the real index set of sequences belonging to class i, M is the total number of sequences. Purity lies in between 0 and 1. Higher purity indicates more concentration in each cluster.

Rand Index: By treating the labels as a clustering ground truth, RI can be used as clustering accuracy (the higher the better), measuring the similarity between the learned sequence clustering and real labels, as given by [48]: RI=n11+n00n∈[0,1], where n11 is the number of sequence pairs that are in the same cluster with the same label, and n00 is the number of pairs that are in different clusters with different labels.

Empirical Intensity Deviation: To measure the learned latent policy for each cluster, we follow the protocol in [63] to compute the deviation of empirical intensity function (accumulated absolute error over time for a windowed period) between the real event sequences and sequences generated by the learned policy, for which the lower the better. The empirical intensity is given by: λ′(t)=E(N(t+δt))/δt, where N(t) is the count process for λ(t), and the expectation E(N(t+δt)) is computed by sufficient number of generated sequences through counting the average number of events during [t,t+δt]. Note that it can be only applied to synthetic dataset where the ground-truth cluster label is known.

Clustering Consistency: Purity, RI and EID can be used to measure clustering performance when the cluster labels are known as for synthetic sequences. For real-world sequences without labels, we measure the clustering performance by clustering consistency via cross-validation as in [57].

It is worth mentioning that some intrinsic clustering evaluation such as Davies-Bouldin index [9] or Dunn index [15] is not practical for evaluating the clustering performance in our event sequences clustering problem. The key problem with these clustering evaluations is that it is impractical to define or compute the distance between two event sequence samples. Actually if we can define the distance between stochastic event sequences, it would be easy to directly use distance based clustering method like k-means, instead of designing the whole EM clustering algorithm, of which each component using policy networks(the proposed RLPMM algorithm), or generative networks(the baseline WGANMM). Unfortunately it is actually impractical to compute or define the distance between two stochastic sequences, and we use clustering consistency as evaluation when there is no ground truth for real-world dataset.

To compute Clustering Consistency, we test each clustering method with J=100 trails. In trial j, all sequences are randomly divided into training fold and testing fold. After learning the model from the training fold, we apply the model to the corresponding testing fold. We enumerate all pairs of sequences within a same cluster in the jth trial and count the pairs preserved in all other trials. The clustering consistency is the minimum proportion of preserved pairs over all trials computed by
Consistency=minj∈{1,…,J}∑j′≠j∑(m,m′)∈Mj1{kj′m=kj′m′}(J−1)|Sj|,
View Sourcewhere Sj={(m,m′)|kjm=kjm′} is the sequence pair set within the same cluster in trial j, kjm is the cluster index of for sequence m.

4.4 Experiments on Synthetic Data
We experiment on several synthetic datasets with different K (number of clusters). We generate four kinds of synthetic event sequences in a time interval [0,T](T=100) using the simulation method in [43] for TPP. We experiment with different K from K=2 to K=4, and the results are given by the average of 10 trails as also shown in Table 2. The ratio of each cluster size is the same. To make a fair comparison with the Hawkes process based models, we experiment on synthetic datasets generated by both non-Hawkes processes and Hawkes processes.

Specifically, for non-Hawkes dataset, we have sequences generated by a mixture of K=2 clusters from Sine intensity and Negative-sine intensity, then we add sequences generated by Constant intensity for K=3, followed by the cluster from Bimdoal intensity for K=4. We also list the formulas of the intensity and plot the curve of the ground-truth intensity and learned intensity.

In the experiments with non-Hawkes datasets, the sequences are generated by 4 kinds of intensity functions. The formulas of these functions are:

Sine-like: λ(t)=sin(πt50)10+0.1,t∈(0,T).

Negative-sine-like: λ(t)=−sin(πt50)10+0.1,t∈(0,T).

Constant: λ(t)=0.1,t∈(0,T).

Bimodal-like: λ(t)=⎧⎩⎨⎪⎪⎪⎪⎪⎪0.15exp(−t−(T4)22∗(T8)),0.15exp(−t−(3T4)22∗(T8)),t∈(0,T2].t∈(T2,T).

We also plot the intensity of the ground-truth, and the estimated empirical intensity of RLPMM model and baseline models as shown in Fig. 3.

For Hawkes processes, we adopt the conventional Hawkes process as: λ(t)=γ0+α∑t∈τe−w(t−t′). We also experiment with different K from K=2 to K=4. In line with [67], for each trial, the parameters of the intensity function for cluster k: {γk0,αk} are sampled randomly from [0, 1] by keeping w=1. In line with [67], for each trial, the parameters of the intensity function for cluster k: {γk0,αk} are sampled randomly from [0, 1] by keeping w=1.

4.5 Experiments on MemeTracker Data
Though plenty real-world datasets are available nowadays e.g., various time-series datasets are employed in [51], stochastic time-stamped events, especially abundant event sequences with adequate temporal patterns are relatively rare. In this paper, we are dedicated to discovering the occurrence temporal pattern of stochastic events from the recorded event sequences, instead of modeling of synchronous time series data. Though the raw data in [51] is not suitable for experiment, it is worth mentioning to explain the differences between the stochastic time-stamped event sequences e.g., earthquakes and their aftershocks, or information diffusion process of continually transmitting, and these time-series data e.g., Electricity Usage and Inside Temperature in SmartHome dataset [5], or Ambient light and Average Number of Steps in Wearable dataset [52] used in [51].

For the real-world dataset experiments, we collect real-world event sequences from public MemeTracker [29] as widely used in TPP works [39], [63], [73]. It tracks meme diffusion over public media, containing more than 172 million news articles or blog posts. The memes are sentences, such as ideas, proverbs, and the time is recorded when it spreads to certain websites. We randomly sample 35,000 cascades from MemeTracker to study the diffusion process of the meme since its creation. The memes are supposed generated from different latent policies for discovery. Note one can only use clustering consistency as metrics as there is no ground-truth cluster labels. The results are shown in Table 3.

4.6 Comparison With WGANMM
Particularly, we compare the efficiency of our model with WGANMM on both synthetic non-Hawkes data and real-world MemeTracker data in Table 4. Both WGAMM and RLPMM are neural network based mixture model with adversarial loss. The main differences are that, WGANMM is a mixture of GAN models with dynamic RNN of multiple LSTM cells and using Wasserstein distance between event sequences as cost. While RLPMM models the event sequence as a series of states and actions from the view of RL using one single LSTM cell for each latent policy, and the adversarial loss i.e., the reward/cost function is gained by an adversarial imitation technique.

TABLE 4 General Number of Iterations (in Order) for Training to Convergence and Time Cost per Iteration
Table 4- 
General Number of Iterations (in Order) for Training to Convergence and Time Cost per Iteration
Though WGANMM achieve comparable results in Tables 2 and 3 for clustering and modeling performance, the RLPMM model is much more efficient considering number of iterations and runtime as shown in Table 4. For an empirical verification of the convergence procedure, we also plot the convergence of the evaluation metric Clustering Purity and Rand Index on synthetic dataset in Fig. 5.

Fig. 5. - 
Empirical convergence of RLPMM and WGANMM on synthetic data generated by $K=4$K=4 policies. We compute the clustering purity and rand index every $1\times 10^3$1×103 iterations for RLPMM and $2.5\times 10^3$2.5×103 iterations for WGANMM.
Fig. 5.
Empirical convergence of RLPMM and WGANMM on synthetic data generated by K=4 policies. We compute the clustering purity and rand index every 1×103 iterations for RLPMM and 2.5×103 iterations for WGANMM.

Show All

While it is worth mentioning that we do not use the curve of the training/validation loss to show the convergence process, as we use generative adversarial imitation learning as the IRL method, which is one approach in generative adversarial learning. The same as general adversarial learning algorithm in which the generator and discriminator are against each other, in the proposed RLPMM model as in Fig. 2, the discriminator network and the policy network compete with each other by GAIL algorithm. As in Fig. 2, by iteratively implementing IRL to learn the cost function and RL to learn the policy, though the learned policy is getting better, the cost function is also becoming more and more “discriminative”, so accordingly the loss function do not go all the way down to a convergent value. It is the same with the baseline method WGANMM. Based on this reason, we show the convergence of the algorithm by recording the clustering metrics as Fig. 5.

4.7 Findings and Discussion
We present interpretations to the results as follows.

1) Parametric point process versus neural point process versus discretized time series. The two neural network based TPP models: RLPMM using reinforcement imitation learning and WGANMM using generative adversarial learning, in general outperform the (implicit) parametric intensity models:2 ODE+GMM, LS+GMM and DMMHP using explicit intensity functions, on both clustering performance and modeling capability. While the performance of time series based method VAR+GMM is the worst. These results show the superiority of the neural model compared with parametric TPP which assume a predefined form with limited model capacity.

On the other hand, as shown in Table 2, when the data is exactly generated from the predefined point process – Hawkes processes, the model DMMHP which is based on Hawkes processes model can benefit significantly and even (slightly) outperforms the network based methods including RLPMM and WGANMM. This also suggests the parametric model still have their value when the distribution can be (exactly) known to allow for model specification. The standard deviation in Table 2 also shows the higher stability of neural TPP methods against parametric ones. On the other hand, for neural TPP methods, though not specially designed for Hawkes processes clustering and modeling, the proposed method achieves comparable results with regard to the specially designed one.

2) Two-step pipeline versus joint model. On both the synthetic and the real-world datasets, joint modeling methods DMMHP, WGANMM and RLPMM outperform two-step models VAR/ODE/LS + GMM which run clustering first followed by learning within each cluster. This shows the utility of an elegant joint learning algorithm.

3) WGANMM versus RLPMM. It is shown that WGANMM and RLPMM perform relatively close to each other (though often RLPMM outperforms WGANMM) regarding with clustering performance on all metrics, for both synthetic data and real-world dataset. However, we find that the proposed RLPMM shows superior efficiency to WGANMM. In fact, WGANMM adopts the adversarial training framework based on Wasserstein divergence, where both the generator and the discriminator are modeled as dynamic RNN of multiple LSTM cells. In contrast, RLPMM only models the policy as a single LSTM cell, so for the discriminator with an extra Logistic regression layer. As such, RLPMM requires less parameters and computations. Moreover, we find the real-world MemeTracker need notably fewer iterations to converge than synthetic data, which shows the potential of our method for practical tasks.

We can also see an influence of cluster number K to the convergence of WGANMM and RLPMM in Table 4. As K increase from K=2 to K=4, the WGANMM needs more training iterations to converge than RLPMM, which also testifies that the reinforcement learning based model requires much less parameter and is easier to train than the GAN based model.

4) Average response time. We compare the average response time of training to convergence for proposed algorithm and baselines, as shown in Table 5. Overall, the parametric methods VAR/ODE/LS + GMM and DMMHP requires less time to converge than neural network based methods WGANMM and RLPMM as they requires less parameters, while the performance is relatively low. In particular, for neural network based methods, the proposed RLPMM model is much more efficient than WGANMM with better performance. The Experiments are conducted under CentOS system, with intel E5-2650V4 CPU, 64G RAM and 6 GeForce GTX 1080 GPUs.

5) Learning temporal patterns from synthetic data generated by different numbers of policies. Clustering purity, RI and EID all degenerate as the number of policies used for generating the testing data (i.e., intensity functions) grows from K=2 to K=4 (see the protocol in Section 4.4). This is no surprise as it becomes more challenging when the sequence set becomes more mixed. While the impact is lessened on the joint modeling methods DMMHP, WGANMM and RLPMM than the two-step models VAR/ODE/LS + GMM. This also holds for the comparison between RLPMM and WGANMM whereby RLPMM need much less additional iterations to converge than WGANMM when cluster number increases from K=2 to K=4.

6) Discovering temporal patterns for MemeTracker data. For real-world MemeTracker data, the clustering performance decreases as the number of clusters K increase. The unified models are more robust to increased K than two-step models. Particularly, we plot the intensity functions of the clustered real event sequences given K=4. As we can see in Fig. 4, we discover 4 temporal patterns of meme diffusions.

Instead of using one less informative TPP model for the whole set of sequences as shown in Fig. 4a, we set K=4 and plot the empirical intensity functions as discovered by the proposed RLPMM method in Fig. 4b. The memes patterns are marked as C1 – C4, and we show the text statistics of these patterns which reveals a potential for joint modeling with topic model.

For the discovered memes diffusion patterns marked as C1 – C4, their statistics of average meme length and word frequency are listed in Table 6. We make the following interpretations which we believe can at least be partially supported by our results:

C1: memes have a wide spread as soon as it is generated, and quickly disappear in around 30 days. These memes are mostly catchword, e.g., peace, child, being usually short and clear.

C2: the diffusion intensity decays in 20 days and then holds in subsequent days. These memes are mostly hot topics like economy, health care, job opportunity, etc.

C3: the diffusion intensity gradually decay for around 40 days. The memes are mostly long and complete statements and opinions.

C4: the diffusion suspends for short around 20 days, then begins to diffuse. Though the diffusion processes of C3 and 4 are different, the average length and top frequent words of C4 are similar to C3 as in Table 6. It suggests that the diffusion of the long statements and opinions contains two patterns that are quite different from each other.

TABLE 5 The Average Response Time (in seconds) of Training to Convergence for Proposed Algorithm RLPMM, State-of-the-Art Algorithms DMMHP and WGANMM, and the Two-Step Pipeline Baselines
Table 5- 
The Average Response Time (in seconds) of Training to Convergence for Proposed Algorithm RLPMM, State-of-the-Art Algorithms DMMHP and WGANMM, and the Two-Step Pipeline Baselines
TABLE 6 Average Length (in words) and the Top 15 Most Frequent Nouns of Memes Diffusion Cascades, From the Discovered Four Clusters (top to bottom: C1–C4) on MemeTracker
Table 6- 
Average Length (in words) and the Top 15 Most Frequent Nouns of Memes Diffusion Cascades, From the Discovered Four Clusters (top to bottom: C1–C4) on MemeTracker
Particularly, as in Table 6, we list the top 20 most frequent nouns and the average length to further explore the characteristic of the meme diffusion patterns. Such results also show the potential and need for comprehensive modeling by combing RLPMM with topic models on meme content.

SECTION 5Conclusion
Clustering of event sequences by discovering their temporal patterns in continuous time domain is challenging and has vast application for real-world problems. It is also useful for building event-driven simulators. We study this problem from the reinforcement learning perspective for learning policy mixture model. Our approach involves an end-to-end neural network based EM algorithm, and an adversarial imitation inverse reinforcement learning to learn reward for policy rather than resort to ad-hoc cost for varying-length event sequences. Experiments on both synthetic and real-world dataset show the efficacy of our model.

In future work, cross-domain works could be encouraged, as temporal pattern discovery in event sequences in most cases require expert domain knowledge in related area. For example, discovering temporal patterns of the re-occurrence of earthquake and its aftershocks may help to discover new knowledge in geology, but it requires related expert domain knowledge in geology to associate the temporal patterns with current geology literatures. There are also possible extensions: i) effective handling with multi-typed event sequences especially noticing the fact that the used generative adversarial imitation learning technique is directly applicable to multi-dimensional TPP. Note the RL method for multi-type TPP in [58] does not involve IRL technique; ii) joint learning for TPP and topic model for text data associated with events, which is useful in practice such as for MemeTracker; iii) exploring the way of model sharing among clusters for more effective policy learning.