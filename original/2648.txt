Due to their high retrieval efficiency and low storage cost, cross-modal hashing methods have attracted considerable attention. Generally, compared with shallow cross-modal hashing methods, deep cross-modal hashing methods can achieve a more satisfactory performance by integrating feature learning and hash codes optimizing into a same framework. However, most existing deep cross-modal hashing methods either cannot learn a unified hash code for the two correlated data-points of different modalities in a database instance or cannot guide the learning of unified hash codes by the feedback of hashing function learning procedure, to enhance the retrieval accuracy. To address the issues above, in this paper, we propose a novel end-to-end Deep Cross-Modal Hashing with Hashing Functions and Unified Hash Codes Jointly Learning (DCHUC). Specifically, by an iterative optimization algorithm, DCHUC jointly learns unified hash codes for image-text pairs in a database and a pair of hash functions for unseen query image-text pairs. With the iterative optimization algorithm, the learned unified hash codes can be used to guide the hashing function learning procedure; Meanwhile, the learned hashing functions can feedback to guide the unified hash codes optimizing procedure. Extensive experiments on three public datasets demonstrate that the proposed method outperforms the state-of-the-art cross-modal hashing methods.
SECTION 1Introduction
With a tremendous amount of multimedia data being generated on the Internet everyday such as texts, images and so on, it has led to the surge of research activities in large scale multimedia search [1], [2], [3]. One fundamental research problem is nearest neighbor search (ANN). Among the ANN technics, similarity-preserving hashing methods [4], [5], [6], [7], [8], [9] have been extensively studied due to their high retrieval efficiency and low storage cost. Because the corresponding data of different modalities may have semantic correlations, it is essential to support cross-modal retrieval that returns relevant results of one modality when querying another modality, e.g., retrieving images with text queries. Hence, cross-modal hashing methods [10], [11], [12], [13], [14], [15] get more and more attention.

Roughly speaking, cross-modal hashing methods can be divided into shallow cross-modal hashing methods [5], [10], [16], [17], [18], [19] and deep cross-modal hashing methods [11], [14], [20], [21], [22], [23]. Shallow cross-modal hashing methods mainly use hand-crafted features to learn projections for mapping each example into a binary code. The feature extraction procedure in shallow cross-modal hashing methods is independent of the hash codes learning procedure. It means that the shallow cross-modal hashing methods may not achieve satisfactory performance in real applications, because the hand-crafted features might not be optimally suitable for hash codes optimizing procedure. Compared with shallow cross-modal hashing methods, deep cross-modal hashing methods can integrate feature learning and hash codes learning into a same framework, and capture non-linear correlations among cross-modal instances more effectively to get better performance, where each instance contains two correlated data-points of different modalities like image-text pairs.

However, the existing deep cross-modal hashing methods either cannot learn a unified hash code for the two correlated data-points of different modalities in a database instance or cannot guide the learning of unified hash codes by the feedback of hashing function learning procedure, to enhance the retrieval accuracy. First, most deep cross-modal hashing methods assume that there are different hash codes for the two correlated data-points of different modalities in a database instance, and then try to decrease the gap between two hash codes through optimizing certain pre-defined loss functions. Thus, they just learn the similar hash codes for two correlated data-points of different modalities in a same instance, and cannot obtain unified hash codes. However, the unified hash code schema has been proved that it can enhance the retrieval accuracy [24], [25], [26]. Second, as far as we know, until now there is only one deep cross-modal hashing method that can learn unified hash codes [11]. The method is a two step framework. It first learns unified hash codes for instances in a database, and then utilizes the learned unified hash codes to learn modal-specific hashing function. It means the deep hashing method cannot guide the learning of unified hash codes by the feedback of hashing function learning procedure.

To address the issues above, in this paper, we propose a novel Deep Cross-Modal Hashing with Hashing Functions and Unified Hash Codes Jointly Learning, called DCHUC. DCHUC can learn unified hash codes for database instances, and simultaneously learn modal-specific hashing functions for unseen query points in an end-to-end framework. More specifically, by minimising the objective function, DCHUC uses a four-step iterative scheme to optimize the unified hash codes of the database instances and the hash codes of query data-points generated by the learned hashing networks. With the iterative optimization algorithm, the learned unified hash codes can guide the hashing functions learning procedure; Meanwhile, the learned hashing function can feedback to guide the unified hash codes optimizing procedure. Moreover, the objective function consists of a hashing loss and a classification loss. The hashing loss is used to make the learned hash codes can preserve both inter-modal and intra-modal similarity, and the classification loss can be used to make the learned hashing codes preserve more discriminative semantic information.

In addition, because the training phase of deep models is typically time-consuming, so it is hard to use all instances in a large-scale database to train hashing model. Inspired by ADSH [27], we use an asymmetric scheme to reduce the training time complexity to O(mn). Specially, we samples m anchors instances from n database instances (m≪n) to approximate query datasets, and constructs an asymmetric affinity to supervise hashing functions learning for unseen query instances and unified hash codes optimizing for instances in a database. By using this way, in training phase, DCHUC can map the whole set of database instances into unified hash codes efficiently even if the size of a database is large.

To summarize, the main contributions of DCHUC are outlined as follows:

To the best of our knowledge, DCHUC is the first deep method that can jointly learn unified hash codes for database instances and hashing functions for unseen query points in an end-to-end framework. By using the end-to-end framework, our method can get the high-quality hash codes to improve the retrieval accuracy.

By treating the query instances and database instances in an asymmetric way, DCHUC can use the whole set of database instances in training phase to generate higher-quality hash codes even if the size of a database is large.

Experiments on three large-scale datasets show that DCHUC can outperform the state-of-the-art cross-modal hashing baselines in real applications.

SECTION 2Related Work
In this section, we briefly review the related works of cross-modal hashing methods, including shallow cross-modal hashing methods and deep cross-modal hashing methods.

2.1 Shallow Cross-Modal Hashing Methods
Shallow cross-modal hashing methods [6], [18], [19], [25], [28], [29] mainly use hand-crafted features to learn a single pair of linear or non-linear projections to map each example into a binary vector. The representative methods in this category include Cross Modality Similarity Sensitive Hashing (CMSSH) [6], Semantic Correlation Maximization (SCM) [28], Inter-media Hashing (IMH) [30], Cross View Hashing (CVH) [29],Latent Semantic Sparse Hashing (LSSH) [25], Collective Matrix Factorization Hashing (CMFH) [26], Semantics Preserving Hashing (SePH) [31], Supervised Discrete Manifold-embedded Cross-Modal Hashing (SDMCH) [18], Discrete Latent Factor hashing (DLFH) [19] and Discrete Cross-modal Hashing (DCH) [32]. CMSSH is a supervised hashing methods, which designs a cross-modal hashing method by preserving the intra-class similarity via eigen-decomposition and boosting. SCM utilizes label information to learn a modality-specific transformation, and preserves the maximal correlation between modalities. IMH is an unsupervised hashing methods which encodes data to achieve the inter-modal consistency and intra-modal consistency. CVH presents an unsupervised cross-modal spectral hashing method so that the cross-modality similarity is also preserved in the learned hash functions. LSSH utilizes sparse coding and matrix factorization in the common space to obtain a unified binary by a latent space learning method. CMFH learns a unified binary hash code by performing matrix factorization with latent factor model in the training stage. SePH generates a unified binary hash code by constructing an affinity matrix in a probability distribution while at the same time minimizing the Kullback-Leibler divergence. SDMCH generates binary hash codes by exploiting the non-linear manifold structure of data and constructing the correlations among heterogeneous multiple modalities with semantic information. DLFH directly learns the binary hash codes without continuous relaxation by a discrete latent factor model. DCH jointly learns the unified binary codes and the modality-specific hash functions under the classification framework with discrete optimization algorithm.

Despite of significant progress in this category has been achieved, the performance of hand-crafted feature based methods are still unsatisfactory in many real-world applications. Because the feature extraction procedure is independent of the hash-code learning procedure in hand-crafted feature based methods, which means that the hand-crafted features might not be optimally suitable for the hash codes optimizing procedure.

2.2 Deep Cross-Modal Hashing Methods
Recently, deep cross-modal hashing methods [11], [14], [20], [21], [22], [33] have been proposed to achieve promising performance due to the powerful arbitrary non-linear representation of deep neural network. For example, deep visual-semantic hashing (DVSH) [21] learns a visual semantic fusion network with cosine hinge loss to generate the binary codes and learns modality-specific deep networks to obtain hashing functions. However, DVSH can only be used for some special cross-modal cases where one of the modalities have to be temporal dynamics. Deep cross-modal hashing (DCMH) [33] utilizes a negative log-likelihood loss to generate cross-modal similarity preserving hash codes by an end-to-end deep learning framework. Correlation Autoencoder Hashing (CAH) [20] learns hashing functions by designing an auto-encoder architecture to jointly maximize the feature and semantic correlation between different modalities. Adversarial cross-modal retrieval (ACMR) [14] utilizes a classification manner with adversarial learning approach to discriminate between different modalities and generate binary hash codes. Self-supervised adversarial hashing (SSAH) [22] generates binary hash codes by utilizing two adversarial networks to jointly model different modalities and capture their semantic relevance under the supervision of the learned semantic feature. Cross-modal deep variational hashing (CMDVH) [11] uses a two step framework. In the first step the method learns unified hash code for image-text pair in a database, and utilize the learned unified hash codes to learn hashing functions in the second step. Thus, for CMDVH, the learned hashing function in the second stage cannot give feedback to guide unified hash codes optimizing.

Typically, deep cross-modal hashing methods can outperform shallow hashing methods in terms of retrieval accuracy. However, most of existing deep cross-modal hashing methods cannot bridge the modality gap well to generate unified hash codes for image-text pairs in a database. Although CMDVH can generates unified binary codes for points of modalities, its hashing function learning procedure cannot feedback to guide the unified hash codes optimizing. Hence, CMDVH cannot get the optimal unified hash codes to bridge the modality gap well. Furthermore, please note that, although DCH can jointly learn unified hash codes for instances in a database and hashing functions for query instances, it is a shallow hashing method. Its feature extraction procedure is independent of the hash codes learning procedure, and DCH need use all the database instances to lean hashing functions which means it is hard to reconstruct DCH to a deep architecture. Thus, we propose a novel deep hashing method that can learn the unified hash codes for instances in a database and hashing functions for query instances in an end-to-end framework.

SECTION 3Methodology
3.1 Problem Definition
Assume that we have n training instances in a database, and each instance has two modal data points. Without loss of generality, we use image-text databases for illustration in this paper, which means that each instance in the database has both a data point of text modality and a data point of image modality. We use O={oi}ni=1 to denote a cross-modal dataset with n instances, and oi=(xi,yi,li), where xi and yi denote the original image and text points in the ith instance oi, respectively. li=[li1,li2,…,lic]T is the label annotation assigned to oi, where c is the class number. If oi belongs to the jth class lij=1, otherwise lij=0. Furthermore, a pairwise similarity matrix S∈{−1,+1}n×n is used to describe the semantic similarities between two instances. If Sij=1, it means that oi is semantically similar to oj, otherwise Sij=−1. Specifically, if two instances oi and oj are annotated by multiple labels, we define Sij=1 when oi and oj share as least one label, otherwise Sij=−1.

Given the above database O and similarity information S, the goal of DCHUC is to learn the similarity-preserving hash codes B={bi}ni=1∈{−1,+1}n×k for instances in the database, where k is the length of each binary code and bi denotes the learned hash code for the instance oi, i.e., a unified hash code for the image-text pair xi and yi. Meanwhile, the Hamming distance between bi and bj should be as small as possible when Sij=1. Otherwise, the Hamming distance should be as large as possible. Moreover, in order to generate a binary code for any unseen image modal query point xq or text modal query point yq, DCHUC should learn two modal-specific hashing functions bxq=F(xq)∈{−1,+1}k and byq=P(yq)∈{−1,+1}k, respectively. In order to learn the two hash functions, we sample a subset from the database O as the pseudo query set Q=OΦ for training, where OΦ denotes the pseudo query instances indexed by Φ from the database O. Moreover, we use Υ={1,2,…,n} to denote the indices of all the database instances and Φ={i1,i2,…,im}⊆Υ to denote the indices of the m sampled pseudo query instances, and XΦ and YΦ denote image modal points and text modal points in the pseudo query set Q, respectively. Correspondingly, the similarity between query instances and database instances can be denoted as SΦ∈{−1,+1}m×n, which is formed by the rows of S indexed by Φ. In addition, in this paper, sgn(⋅) is an element-wise sign function which returns 1 if the element is positive and returns −1 otherwise.

3.2 Deep Cross-Modal Hashing with Hashing Functions and Unified Hash Codes Jointly Learning
The model architecture for DCHUC is shown in Fig. 1, which contains three parts: image modal hashing network, text modal hashing network and hash codes optimizing.


Fig. 1.
The DCHUC learning framework. It contains three parts: image modal hashing network, text modal hashing network and hash code optimizing. The hash codes of image modal query data-points can be generated by the image modal hashing network with an element-wise function sgn(⋅), and the hash codes of text modal data-points can be generated text modal hashing network with an element-wise function sgn(⋅). In the hash codes optimizing part, a four-step iterative scheme is used to optimize hash codes for both database instances and query instances by minimising the hashing loss and the classification loss.

Show All

For the image modal hashing network part, it contains a convolutional neural network (CNN) which is adapted from Alexnet [34]. The CNN component contains eight layers. The first seven layers are the same as those in Alexnet [34]. The eighth layer is a fully-connected layer with the output being the learned image features, which is named as hashing layer. The hashing layer contains k units where k is the length of hash codes. An activation function tanh(⋅) is used to make the output features close to ”−1” or ”+1”. We use vi=F(xi;Θ)∈Rk to denote the final output features of the image modal hashing network.

For the text modal hashing network part, a neural network containing two fully-connected layers is used to learn text modal features. We represent each text point yi as a bag-of-words (BoW) vector, and use the BoW as the input of the two-fully-connected neural network. The first fully-connected layer has 10,240 hidden units, and the activation function for the first fully-connect layer is RELU [34]. The second fully-connected layer is also named as hashing layer with k nodes. Similar to the image feature learning part, a tanh(⋅) function is used as an activation function to make the output features close to ”−1” or ”+1”. We use ti=P(yi;Ψ)∈Rk to denote the final output features of the text modal hashing network.

For the hash codes optimizing part, it will optimize hash codes for both database instances and query instances the objective function whose details will be introduced in Section 3.3. More specially, with a four-step iterative scheme, the unified hash codes B for database instances will be learned directly and the modal-specific hashing functions can be learned by back-propagation algorithm which will be introduced in Section 3.4 in detail. Furthermore, the hash codes for query instances are generated by the final output features of modal-specific hashing network with an element-wise function sgn(⋅). Specifically, for an image modal query point xi, we can get its binary hash codes hi=sgn(vi); for a text modal query point yi, its binary hash codes can be generated by gi=sgn(ti).

Furthermore, please note that in the training stage, the query instances used to train the two modal-specific hashing networks are selected from the database, i.e., they are pseudo query instances.

3.3 Objective Function
The goal of DCHUC is to map instances in the database and the unseen query data-points into a semantic similarity-preserving Hamming space where the hash codes of data-points from the same categories should be similar no mater which modalities they belong to, and the hash codes of data-points from different categories should be dissimilar. In the following, we present more details about the objective function of our DCHUC.

In order to bridge the gap across different modalities well, we first assume the image point xi and text point yi for any instance oi in a database share the same hash code bi, i.e., learn a unified hash code bi for an image-text pair xi and yi. Thus, the hash code bi can preserve the image modal information and text modal information at the same time. Moreover, in order to make the learned hash codes of instances in the database and the hash codes of query data-points generated by the learned hashing functions can preserve the semantic similarity, for any two hash codes bi and bj, if the similarity Sij=1, i.e., they are similar, the Hamming distance dh(bi,bj)=0.5(k−bTibj) between bi and bj should be the minimal value 0, which means bTibj should equal to k; otherwise, the Hamming distance dh(bi,bj) should be the maximum value k, which means bTibj should equal to 0. Therefore, the hashing loss can be defined as the Frobenius norm loss between the semantic similarities multiplied by k and the inner product of binary code pairs. Therefore, the hashing loss can be defined as follow:
minB,H,GLhs.t.   BHG=∥∥HBT−kSΦ∥∥2F+∥∥GBT−kSΦ∥∥2F    +μ∥∥HGT−kSΦΦ∥∥2F=[b1,b2,…,bn]T∈{−1,+1}n×k,=sgn(V)∈{−1,+1}m×k,=sgn(T)∈{−1,+1}m×k,(1)
View SourceRight-click on figure for MathML and additional features.where μ is a hype-parameter, B∈{−1,+1}n×k denotes the unified binary hash codes for n database instances; SΦΦ denotes the columns of SΦ indexed by Φ; H∈{−1,+1}m×k denotes the binary hash codes for m images modal query data-points, and G∈{−1,+1}m×k denotes the binary hash codes for m text modal query data-points; V=[vi1,vi2,…,vim]T is the output of images modal hashing network for image query set XΦ, and T=[ti1,ti2,…,tim]T is the output of text modal hashing network for text query set YΦ.

Furthermore, in order to make the learned hashing codes preserve more discriminative semantic information, we expect the learned hashing codes can be ideal for classification too. Then the classification loss function can be defined as follow:
minB,H,G,WLcs.t.  BHG=α(∥∥HW−LΦ∥∥2F+∥∥GW−LΦ∥∥2F)+β∥BW−L∥2F+η∥W∥2F=[b1,b2,…,bn]T∈{−1,+1}n×k,=sgn(V)∈{−1,+1}m×k,=sgn(T)∈{−1,+1}m×k.(2)
View SourceRight-click on figure for MathML and additional features.where L=[l1,l2,…,ln]T∈{0,1}n×c is the label matrix of instances in the database O, and LΦ∈0,1m×c denotes the label matrix of query instances indexed by Φ from the label matrix L. W=[w1,w2,…,wc]∈Rk×c and wj∈Rk×1 is the classification projected vector of the class j.

Thus, our objective hashing function can be defined as follow:
minB,H,G,WLs.t.  BHG=Lh+Lc=[b1,b2,…,bn]T∈{−1,+1}n×k,=sgn(V)∈{−1,+1}m×k,=sgn(T)∈{−1,+1}m×k.(3)
View SourceRight-click on figure for MathML and additional features.

However, the sgn(⋅) function is in-differentiable at zero and the derivation of it will be zeros for a non-zero input. It means that the parameters of the modal-specific hashing networks will not be updated with the back-propagation algorithm when minimizing the formula (3). Thus, we directly discard the sgn(⋅) function to ensure the parameters of our hashing model can be updated, and add a quantization loss to make each element of V and T can be close to “+1” or “-1”. Moreover, because, in the training stage, the query set is sampled from the database. Thus, when constructing the quantization loss, the hash codes generated by the learned hashing function should be the same with the directly learned hash codes as much as possible, i.e., if an instance oi in the database is sampled as query instance, then the hash code hi for image modality data-point and gi for text modality data-point in oi should be the same with bi. Thus, we can further reformulate Formula (3) as
minB,V,TL=∥∥VBT−kSΦ∥∥2F+∥∥TBT−kSΦ∥∥2F+μ∥∥VTT−kSΦΦ∥∥2F+β∥BW−L∥2F+α(∥∥VW−LΦ∥∥2F+∥∥TW−LΦ∥∥2F)+η∥W∥2F+γ∥∥BΦ−0.5(V+T)∥∥2Fs.t.   B∈{−1,+1}n×k.(4)
View SourceRight-click on figure for MathML and additional features.where α,β,η,γ,μ are hyper-parameters, BΦ∈{−1,+1}m×k is formed by the rows of B indexed by Φ, and ∥∥BΦ−0.5(V+T)∥∥2F is the quantization loss.

3.4 Optimization
In order to optimize Formula (4), we propose a four-step iterative scheme as shown below. More specifically, in each iteration we sample a query set from the database and then carry out our learning algorithm based on both the query set and database. The whole four-step learning algorithm for DCHUC is briefly outlined in Algorithm 1, and the detailed derivation steps will be introduced in the following content of this subsection.

3.4.1 Learn Θ With Ψ, B and W Fixed
When Ψ, B and L are fixed, we update the parameter Θ of image hashing network by using a mini-batch stochastic gradient descent with back-propagation (BP) algorithm. More specifically, for each sampled image point xi in XΦ, we first compute the following gradient:
∂L∂vi=2∑j=1n[(vTibj−kSΦij)bj]+2μ∑j=1m[(vTitj−kSΦΦij)tj]    +2α∑j=1c[(vTiwj−LΦij)wj]+γ(vi+ti−2bi).(5)
View SourceRight-click on figure for MathML and additional features.Then we can compute ∂L∂Θ based on ∂L∂vi by using chain rule, and use BP to update the parameter Θ.

Algorithm 1. Learning Algorithm for DCHUC
Require: Database instances O={X,Y,L}, the length of hash codes k.

Ensure: Database instances codes B, image modal hashing network parameters Θ and text modal hashing network parameters Ψ.

Initialize parameters: B, Θ, Ψ, α, η, γ. learning rate: lr, iteration number: tout, tin, the size of mini-batch z=64 (see Implementation Details).

Utilize label L to generate similarity matrix S.

repeat

Randomly generate index set Φ and sample m instances OΦ={XΦ,YΦ,LΦ} from database O as query set. Select SΦ from S

for iter=1,2,…,tin do

for iter_batch=1,2,…,m/z do

Randomly sample z image points from XΦ as a mini-batch

Update parameter Θ based on Formula (5)

end for

for iter_batch=1,2,…,m/z do

Randomly sample z image points from YΦ as a mini-batch

Update parameter Ψ based on Formula (6)

end for

end for

for iter_bit=1,2,…,k do

Update B∗iter_bit based on Formula (12)

end for

Update W based on Formula (14)

until Up to tout

3.4.2 Learn Ψ With Θ, B and L Fixed
When Θ and B are fixed, we also update the parameter Ψ of text hashing network by using a mini-batch stochastic gradient descent with BP algorithm. More specifically, for each sampled text point yi in YΦ, we first compute the following gradient:
∂L∂ti=2∑j=1n[(tTibj−kSΦij)bj]+2μ∑j=1m[(tTivj−kSΦΦij)vj]    +2α∑j=1c[(tTiwj−LΦij)wj]+γ(vi+ti−2bi).(6)
View SourceRight-click on figure for MathML and additional features.Then we can compute ∂L∂Ψ based on ∂L∂ti by using chain rule, and use BP to update the parameter Ψ.

3.4.3 Learn B With Θ, Ψ and W Fixed
When Θ, Ψ and W are fixed, we can reformulate Formula (4) as follows:
minBL=∥∥VBT−kSΦ∥∥2F+∥∥TBT−kSΦ∥∥2F    +β∥BW−L∥2F+γ∥∥BΦ−0.5(V+T)∥∥2F=∥∥VBT∥∥2F−2ktr(BVTSΦ)+∥∥TBT∥∥2F    −2ktr(BTTSΦ)+β∥BW∥2F−2βtr(BWLT)    −γtr(BΦ(VT+TT))+consts.t.  B∈{−1,+1}n×k,(7)
View SourceRight-click on figure for MathML and additional features.where const is a constant independent of B and tr(⋅) is the trace norm. For convenience of calculations, we can further reformulate Formula (7) as follows:
minBL=∥∥VBT∥∥2F+∥∥TBT∥∥2F+β∥BW∥2F−tr(B(γV˚T    +γT˚T+2kVTSΦ+2kTTSΦ+2βWLT))    +const=∥∥VBT∥∥2F+∥∥TBT∥∥2F+β∥BW∥2F    −tr(BD)+consts.t.  B∈{−1,+1}n×k,(8)
View Sourcewhere D=γV˚T+γT˚T+2kVTSΦ+2kTTSΦ+2βWLT;  V˚=[v˚1,v˚2,…,v˚n]T;  T˚=[t˚1,t˚2,…,t˚n]T, and v˚i, t˚i are respectively defined as follows:
v˚i={vi,0,if  i∈Φ,if  i∉Φ.(9)
View Source
t˚i={ti,0,if  i∈Φ,if  i∉Φ.(10)
View SourceRight-click on figure for MathML and additional features.

The above Formula (8) is NP hard. Inspired by SDH [35], the binary codes B can be learned by the discrete cyclic coordinate descent (DCC) method. It means that we directly learn hash codes B bit by bit. Specifically, we update one column of B with the other column fixed. We let B∗i denotes the ith column of B, and Bˆi denotes the matrix of B without the column B∗i; Let V∗i denotes the ith column of V, and Vˆi denotes the matrix of V without the column V∗i; Let T∗i denotes the ith column of T, and Tˆi denotes the matrix of T without the column T∗i; Let Wi∗ denotes the ith row of W, and W˜i denotes the matrix of W without the row Wi∗; Let Di∗ denotes the ith row of D, and D˜i denotes the matrix of D without the row Di∗. Then we can optimize B∗i by the following function:
minB∗iL=∥∥VBT∥∥2F+∥∥TBT∥∥2F+β∥BW∥2F    −tr(BD)+const=tr(B∗i(2VT∗iVˆiBˆTi+2TT∗iTˆiBˆTi+2βWi∗W˜TiBˆTi    −D˜i∗))+consts.t.   B∗i∈{−1,+1}n×k.(11)
View SourceFinally, we can get the optimal solution of Formula (11)
B∗i=−sgn(2BˆiVˆTiV∗i+2BˆiTˆTiT∗i+2βBˆiW˜iWTi∗−D˜Ti∗),(12)
View SourceRight-click on figure for MathML and additional features.then we can use Formula (12) to update B bit by bit.

3.4.4 Learn W With Θ, Ψ and B Fixed
When Θ, Ψ and B are fixed, we can reformulate Formula (4) as follows:
minWL=α(∥∥VW−LΦ∥∥2F+∥∥TW−LΦ∥∥2F)+β∥BW−L∥2F+η∥W∥2F.(13)
View SourceRight-click on figure for MathML and additional features.For Formula (13), it is easy to solve W by the regularized least squares problem, which has a closed-form solution
W=(αVTV+αTTT+βBTB+ηI)−1(αV˚+αT˚+βB)TL.(14)
View Source

3.5 Out-of-Sample Extension
For any instance which is not in the retrieval set, we can obtain the hash code of its two modalities. In particular, given the image modality xq in an instance oq, we can adopt forward propagation to generate the hash code as follows:
hq=sgn(F(xi;Θ)).(15)
View SourceRight-click on figure for MathML and additional features.Similarly, we can also use the text hashing network to generate the hash code of the instance oq with only textual modality yq
gq=sgn(P(yi;Ψ)).(16)
View SourceRight-click on figure for MathML and additional features.

SECTION 4Experiments
To evaluate the performance of DCHUC, we will carry out extensive experiments on three image-text datasets and compared it with seven state-of-the-art cross-modal hashing methods.

4.1 Datasets
Three datasets are used for evaluation, i.e., MIRFLICKR-25K [36], IAPR TC-12 [37] and NUS-WIDE [38], which are described below.

The MIRFLICKR-25K dataset [36] contains 25,000 instances collected from Flickr website. Each instance contains an image and a text. Here, we follow the experimental protocols given in DCMH [33]. In total, 20,015 data instances whose text points have at least 20 textual tags have been selected for our experiment. The text modality for each instance is represented as a 1,386-dimensional bag-of-words vector. Furthermore each instance is manually labeled with at least one of the 24 unique labels. For this dataset, we randomly sampled 2,000 instances as the test set, and the remaining as the database (retrieval set). Furthermore, the training phase of the existing deep cross-modal hashing methods are typically time-consuming, which makes them cannot efficiently work on large-scale datasets. Therefore, for deep methods, we randomly sample 10,000 instances from the retrieval set as the training set.

The IAPR TC-12 [37] consists of 20,000 instances which are annotated using 255 labels. After pruning the instance that is without any text information, a subset of 19,999 image-text pairs are select for our experiment. The text modality for each instance is represented as a 2,000-dimensional BoW vector. For this dataset, we randomly sampled 2,000 instances as test set, with the rest of the instances as retrieval set. We randomly select 10,000 instances from retrieval set for training deep cross-modal baselines.

The NUS-WIDE dataset [38] contains 269,648 instances crawled from Flickr. Each instance is annotated with one or multiple labels from 81 concept labels,. Only 195,834 image-text pairs that belong to the 21 most frequent concepts are selected for our experiment. The text modality for each instance is represented as a 1,000-dimensional BoW vector. For this dataset, we randomly sampled 2,100 instance as test set, with the rest of the instances as retrieval set. Because the deep hashing baselines are very time-consuming for training, we randomly select 10,500 instances from database for training deep cross-modal baselines.

For all the shallow cross-modal baselines, all the database are used for training. For all datasets, the image xi and text yj will be defined as a similar pair if xi and yj share at least one common label. Otherwise, they will be defined as a dissimilar pair.

4.2 Baselines and Implementation Details
We compare our DCHUC with seven state-of-the-art methods, including four shallow cross-modal hashing methods, i.e., DLFH [19], SCM [28], CCA-ITQ [39] and DCH [32], and three deep cross-modal hashing methods, i.e., DCMH [33], CMDVH [11] and SSAH [22]. The source codes of all baselines but CMDVH and DCH are kindly provided by the authors. We carefully tuned their parameters according to the scheme suggested by the authors. For CMDVH and DCH, we implement it carefully by ourselves. In order to make a fair comparison, we utilize Alexnet [34], which has been pretrained on the ImageNet dataset [40] to extract deep features as the image inputs of all shallow cross-modal baselines, and the input for image modality hashing network of each deep cross-modal baseline is the 224×224 raw pixels.

For the proposed method, we initialize the first seven layers neural network in image feature learning part with the pre-trained Alexnet [34] model on ImageNet [40]. All the parameters of the text modal hashing network and the hashing layer of image hashing network are initialized by Xavier initialization [41]. The unified binary code B is initialized randomly and zero-centered. The image input is the 224×224 raw pixels, and the text inputs are the BoW vectors. The hyper-parameters α,γ,β,μ,η in DCHUC are empirically set as 50, 200, 1, 50, 50, respectively, and they will be discussed in Section 4.7. We set tout=30, tin=3, |Φ|=2000 by using a validation strategy for all datasets. We adopt SGD with a mini-batch size of 64 as our optimization algorithm. The learning rate is initialized as 0.0001 for image hashing network and 0.004 for text modal hashing network. To avoid effect caused by class-imbalance problem between positive and negative similarity information, we empirically set the weight of the element ”−1” in S as the ratio between the number of element ”1” and the number of element ”−1” in S.

The source codes of CMDVH, DCH and our proposed method will be available at: https://github.com/Academic-Hammer.

4.3 Evaluation Protocol
For hashing-based cross-modal retrieval task, Hamming ranking and hash lookup are two widely used retrieval protocols to evaluate the performance of hashing methods. In our experiments, we use three evaluation criterions: the mean average precision (MAP), the precision at n (P@n) and the precision-recall (PR) curve. MAP is the widely used metric to measure the accuracy of the Hamming ranking protocol, which is defined as the mean of average precision for all queries. PR curve is used to evaluate the accuracy of the hash lookup protocol, and P@n is used to evaluate precision by considering only the number of top returned points.

4.4 Experimental Results
All experiments are run 3 times to reduce randomness, then the average accuracy is reported.

4.4.1 Hamming Ranking Task
Tables 1 and 2 present the MAP and Precision@1000 on MIRFLICKR-25K, IAPR TC-12 and NUS-WIDE datasets, respectively. ”I→T” denotes retrieving texts with image queries, and ”T→I” denotes retrieving images with text queries. In general, from Tables 1 and 2, we have three observations: (i) Our proposed method can outperforms the other cross-modal hashing methods for different length of hash code. For example, on MIRFLICKR-25K, comparing with the best competitor SSAH on 16-bits, the results of DCHUC for ”I→T” have a relative increase of 12.7 percent on MAP and 9.2 percent on Precision@1000; the results of DCHUC for ”T→I” have a relative increase of 8.6 percent on MAP and 8.7 percent on Precision@1000. On IAPR TC-12, comparing with the competitor SSAH on 64-bits, the results of DCHUC for ”I→T” have a relative increase of 19.4 percent on MAP and 15.5 percent on Precision@1000; the results of DCHUC for ”T→I” have a relative increase of 18.3 percent on MAP and 15.5 percent on Precision@1000. On NUS-WIDE, comparing with the best competitor DCH on 64-bits, the results of DCHUC for ”I→T” have a relative increase of 12.3 percent on MAP and 7.8 percent on Precision@1000; (ii) Integrating the feature learning of data-points and hashing function learning into an end-to-end network can get the better performance. For example, our proposed method can get a better performance than DCH which also can jointly learning unified hashing codes for instances in the database and modal-specific hashing functions for unseen data-points but the feature extraction procedure is independent of the hash codes learning procedure. (iii) Jointly learning unified hashing codes for database instances and modality-specific hashing functions for unseen data-points can greatly increase the retrieval performance. For instance, although DCH is a shallow hashing method, its retrieval performances on MIRFLICKR-25K and IAPR TC-12 datasets are similar to the best deep baseline SSAH, and its retrieval performances on NUS-WIDE dataset is batter than SSAH. Moreover, DCHUC can get better performance on MAP and Precision@1000 over three benchmark datasets than CMDVH. Note that, the results of CMDVH are different from the results of the original article, and there are two reasons: 1) the metric used in our paper is different from the one in the original paper of CMDVH that the metric of our paper is MAP and the one of CMDVH is MAP5000; 2) we used more classes of label to carry out our experimental than the setting in CMDVH, which makes it harder to train the svm used in CMDVH.

TABLE 1 MAP
Table 1- 
MAP
TABLE 2 Precision@1000
Table 2- 
Precision@1000
Furthermore, to fair comparison, we further compare our method with CMDVH and CM-DVStH [42] which is a journal version of CMDVH in the same experiment setting of CM-DVStH and the results are shown in Table 3. It is easy to find that our method still performs better than the two baselines.

TABLE 3 MAP Comparison of CMDVH and CM-DVStH

4.4.2 Hash Lookup Task
When considering the lookup protocol, we compute the precision and recall curve for the returned points given any Hamming radius. The PR curve can be obtained by varying the Hamming radius from 0 to k with a step-size of 1. Figs. 2, 3 and 4 show the precision-recall curve on MIRFLICKR-25K, IAPR TC-12 and NUS-WIDE datasets, respectively. It is easy to find that DCHUC can dramatically outperform the state-of-the-art baselines, which means our DCHUC generates hash codes for similar points in a small Hamming radius. For example, compared with baselines, the precision value of DCHUC decreases more slowly with the recall value increasing, and DCHUC can get a high precision value even though the recall value increasing to 0.9 on MIRFLICKR-25K and NUS-WIDE datasets.

Fig. 2. - 
Precision-recall curve on MIRFLICKR-25K dataset.


Show All

Fig. 3. - 
Precision-recall curve on IAPR TC-12 dataset.


Show All

Fig. 4. - 
Precision-recall curves on NUS-WIDE dataset.


Show All

4.5 Convergence Analysis
To verify the convergence property of DCHUC, we conduct an experiment over NUS-WIDE dataset with the code length being 64. Fig. 5 shows the convergence of objective function value and MAP. As shown in Fig. 5a, the objective function value can convergence after only 10 iterations. In Fig. 5b, ”I→T” denotes retrieving texts with image queries, and ”T→I” denotes retrieving images with text queries. We can find the MAP values of both the two retrieval task can convergence. Furthermore, combining the two subfigure Figs. 5a and 5b, we can find both the two map values can increase with the objective function value decrease and eventually converge.

Fig. 5. - 
Objective function value and MAP of DCHUC over NUS-WIDE on 64 bits.
Fig. 5.
Objective function value and MAP of DCHUC over NUS-WIDE on 64 bits.

Show All

4.6 Training Efficiency
To evaluate the training speed of DCHUC, we conduct experiments between the deep cross-modal baselines except CMDVH on three datasets. Fig. 6 shows the variation between MAP and training time on the three datasets for DCHUC, SSAH and DCMH. It can be find that DCHUC can not only training faster than the two deep cross-modal baselines, but also get a better performance on retrieval tasks than them. For the CMDVH baseline, it is a two step method. Then it is unfair to compare MAP-Time curve. In here, we calculate the the whole training time of CMDVH. The cost times of training phase on IAPR TC-12, MIRFLICKR-25K and NUS-WIDE datasets with 32-bits are 16.3s, 21.2s and 39.2s for CMDVH, and are 11.8s, 12.9s and 28.2s for DCHUC, respectively. We can find that DCHUC is also the faster one.

Fig. 6. - 
Training efficiency of DCHUC, SSAH and DCMH on three datasets.


Show All

4.7 Sensitivity to Parameters
We study the influence of the hyper-parameters α,γ,β,η and μ on IAPR TC-12, MIRFLICKR-25K and NUS-WIDE datasets with the code length being 64-bits. More specially, Fig. 7a, 7f and 7k show the affect of the hyper-parameter α over the three datasets with the value between 1 and 600. Fig. 7b, 7g and 7i show the affect of the hyper-parameter γ over the three datasets with the value between 1 and 600. Fig. 7c, 7h and 7m show the affect of the hyper-parameter β over the three datasets with the value between 10−3 and 10. Fig. 7d, 7i and 7n show the affect of the hyper-parameter η over the three datasets with the value between 1 and 600. Fig. 7e, 7j and 7o show the affect of the hyper-parameter μ over the three datasets with the value between 1 and 600. It can be found that DCHUC is not sensitive to α,γ,β and η, and DCHUC is not sensitive to μ in the range [10,300]. For instance, DCHUC can achieve good performance on all the three datasets in the range of 1 to 600 for the hyper-parameters α,γ and η, and also can achieve good performance on all the three datasets with 1≤β≤300. Furthermore, DCHUC can get the high MAP values with different β from the range of 10−3 to 10.

Fig. 7. - 
MAP values with different parameters on three datasets.
Fig. 7.
MAP values with different parameters on three datasets.

Show All

SECTION 5Conclusion
In this paper, we have proposed a novel cross-modal deep hashing method for cross-modal data, called DCHUC. To the best of our knowledge, DCHUC is the first deep method to jointly learn unified hash codes for database instances and hashing functions for unseen query points in an end-to-end framework. By treating the query instances and database instances in an asymmetric way, DCHUC can use the whole set of database instances in training phase to generate higher-quality hash codes even if the size of a database is large. Extensive experiments on three real-world public datasets have shown that the proposed DCHUC method outperforms the state-of-the-art cross-modal hashing methods.