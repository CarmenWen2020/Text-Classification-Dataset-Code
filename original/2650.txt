In recent years, matrix completion has become one of the main concepts in data science. Truncated nuclear norm regularization (TNNR) approximation of the rank function is an example of the favorite approaches in matrix completion that performs better than other approximations like the nuclear norm. In all TNNR based methods, the observed data is considered to be exact, so they do not give an appropriate solution for inexact observed data. But, in real applications, in addition to missing data, observed data may be inaccurate. In this paper, we proposed a novel method based on TNNR that can deal with missing data and inexact observed data by adding some terms in the objective and constraint of the matrix completion problem. Experimental results confirm the quality of our proposed method in comparison with the other state of the art matrix completion and low-rank approximation techniques.
SECTION 1Introduction
In many applications in data science, the data matrix usually contains dependent components and so could be substituted with a low-rank representation. For this, in recent years low-rank matrix minimization problem becomes a favorite concept in data processing and machine learning [1], [2]. This problem widely has been used in many distinct application such as face recognition and reconstruction [3], [4], [5], recommender system [6], [7], video segmentation [8], matrix completion [9], [10], [11], [12], [13], and image denoising [14], [15], [16]. Matrix completion is a particular and important case of the low-rank approximation, which has the following mathematical form:
minXrank(X)     s.t.Xij=Mij,(i,j)∈Ω,(1)
View Sourcewhere the set Ω⊂m×n contains the locations that the incomplete data matrix M is observed. Here X denotes an approximation of M in all locations, with a minimum rank that is expected to recover missed values of M. The matrix completion is an important problem in many applications such as image processing [17], [18], recommender system [19], [20], time series [21], [22], and so on [23]. Due to existence of the rank function, the minimization problem (1), is NP-hard [24], [25], [26], [27] and so some approximations of rank function have been proposed to deal with this issue. For example, in [28] it has been shown that the nuclear norm
∥X∥∗=∑i=1min(m,n)σi(X),
View Sourcewhere σi are singular values of matrix X∈Rm×n, is an optimal convex approximation of the rank function. By this relaxation, the problem in Eq. (1) could be substituted with the following convex problem
minX∥X∥∗     s.t.Xij=Mij,(i,j)∈Ω.(2)
View SourceBy defining orthogonal projector PΩ as
(PΩ(X))ij={Xij,if(i,j)∈Ω0,otherwise,
View Sourcethe minimization problem (2) could be rewritten as following
minX∥X∥∗     s.t.PΩ(X)=PΩ(M).(3)
View SourceIn [28] this problem is reformulated to a semidefinite problem (SDP) and solved by the interior-point method. The computational cost of this interior-point method is high and only could be used on small problems [29], [30]. In recent years some other efficient methods such as the singular value thresholding method (SVT) [31], accelerated proximal gradient optimization method (APG) [32] have been introduced to solve (3). Although these methods perform better than the interior-point method, they still do not have enough speed and accuracy for real-world problems [30]. In general, these nuclear norm-based methods converge to sub-optimal solutions of (1) in the real-world applications and require a large number of iteration for convergence [30]. Recently, in [29] a non-convex approximation of rank function, named truncated nuclear norm
∥X∥r=∑i=r+1min(m,n)σi(X),(4)
View Sourcehas been substituted in the objective function of the matrix completion problem. The matrix completion problem based on this new norm is known as truncated nuclear norm regularization (TNNR) and experimental results confirm the quality of this non-convex relaxation over convex nuclear norm relaxation [29], [30]. Usually, this non-convex problem is solved by the alternating scheme and at each iteration, this problem is substituted with two convex optimization sub-problems. Due to used method in solving the second subproblem of the TNNR method, different versions named TNNR-ADMM, TNNR-APGL and TNNR-ADMMAP have been introduced [29]. To improve the convergence rate of TNNR, in [30] two other variants of TNNR Method based on weighted residual error named (TNNR-WRE) and ETNNR-WRE have been proposed. In recent years, methods other than approximating the rank of the data matrix have been proposed for matrix completion. For example, the authors in [47] used a deep Autoencoder network for matrix completion which has shown appropriate results in the reported experiments.

In the most real-world applications, in addition to the missing entries, the input data in Ω may be inexact. However, the mentioned nuclear norm and TNNR based methods do not have a term to control the propagation of the errors of the inexact input data.

Although based on our knowledge there is no TNNR based matrix completion method to deal with this issue, there are some methods from other approaches that tried to solve this problem. For example, in [33] this phenomenon is modeled as mixture noise and the missed part of the image is considered as impulse noise. So this method could not give a proper solution if there is a lot of missing data. Also in [34], a robust matrix completion method based on the nuclear norm for inexact observed data has been proposed. They supposed that a few entries of the observed data corrupted by a small amount of Gaussian noise and replaced the constraint of model (3) by ∥PΩ(X)−PΩ(M)∥≤δ for an appropriate δ, where M is the inexact input data. To solve the obtained model they used the FPCA [35] method. The experiments in [34] only contain synthetics data without applying on real images. In our experimental, we found that although this method (we called that MCN) often gives better results in comparison with mentioned methods based on the model (1), it still contains some error effects.

In this paper, we propose a novel TNNR based model named truncated nuclear norm regularization for inexact data (TNNR-ID), that can control the propagation of the errors of the input data M in the reconstruction process. We split the input matrix into two parts corresponding to exact and noise terms of the input data, and apply it in the construction of the objective and constraint of the matrix completion problem. By this model, we obtain the exact data and error term simultaneously. The main advantages of this method could be summarized as follows: unlike other TNNR based methods, the proposed model can give an appropriate solution for inexact observed data. Also, this model has the following advantages over the MCN method: unlike MCN, our method uses the TNNR method in the objective function, which we demonstrated its superiority over the nuclear norm in this section. On the other hand, we consider the error term explicitly in both the objective and constraint of the model and estimate them simultaneously, which increases the degree of freedom of the proposed model over methods like MCN. Plus, to improve the accuracy of the proposed method for controlling noise in the recovered image, we used the discrete first-order derivative operator [36], [37] of the solution as a regularization term in the model. To solve the proposed model we designed an alternating schema that uses the accelerated proximal gradient line search (APGL) -as well as called Fast Iterative Shrinkage-Thresholding Algorithm (FISTA)- method to solve its second subproblem [38]. We applied our proposed method on some well-known test problems with different corruptions; the experimental results confirm the quality of the proposed methods in comparison with the other state of the art methods.

The rest of this paper is organized as follows: Section 2 contains related methods. In section 3, we present our proposed methods. In Section 4 some experimental results and comparisons with other matrix completion such as deep learning matrix completion method are presented.

SECTION 2TNNR Based Matrix Completion Methods
Due to existence of rank(X), the matrix completion problem (1) is NP-hard. Nuclear norm and truncated nuclear norm regularization are two known convex and nonconvex approximations of rank(X), respectively, that a lot of matrix completion methods are designed based on these approximations. The optimal convex approximation of rank function proposed by M. Fazel in [28] is nuclear norm that is still in use [39], [40]. Recently, Hu et al. [29] shown that the nuclear norm does not give good results in real applications and introduced the truncated nuclear norm ∥X∥r as a nonconvex approximation of rank function. Based on this approximation the truncated nuclear norm regularization form of the matrix completion could be presented as following nonconvex problem [29]:
minX∥X∥r     s.t.PΩ(X)=PΩ(M).(5)
View SourceBy using Von Neumann's trace inequality [41] this equation could be written as
minX{∥X∥∗−maxAAT=BBT=ITr(AXBT)},s.t.PΩ(X)=PΩ(M),(6)
View SourceRight-click on figure for MathML and additional features.where A∈Rr×m and B∈Rr×n [29]. To solve Eq. (6) an alternative scheme according to variables X,A, and B with initial X1=PΩ(M) has been used in [29]. In the lth iteration of this schema, for a fixed X=Xl the optimal ATl and BTl become the first r left and right singular vectors of Xl, respectively [29]. In the second step, for fixed Al and Bl, the new update Xl+1 of X is obtained by solving the following convex sub-problem
minX∥X∥∗−Tr(AlXBTl)   s.t.PΩ(X)=PΩ(M).(7)
View SourceDifferent methods have been used to solve this problem and in a result different variants of TNNR are investigated which the following methods are the most knowns:

1) TNNR-ADMM: This method uses alternating direction method of multipliers (ADMM) to solve subproblem (7) [42]. Here by adding an auxiliary variable the sub-problem (7) is rewritten as follows
minX,H∥X∥∗−Tr(AlHBTl)  s.t.X=H,PΩ(H)=PΩ(M).(8)
View SourceRight-click on figure for MathML and additional features.Then, its augmented Lagrange function [42], [43] as follows
minX,H,YLβ(X,H,Y)=∥X∥∗−Tr(AlHBTl)+β2∥X−H∥2F+⟨Y,X−H⟩,(9)
View SourceRight-click on figure for MathML and additional features.is used to obtain optimal solutions. Here Y and β>0 are the Lagrangian variable and the penalty parameter, respectively. Also, ⟨.,.⟩ denotes the inner product of matrices.

2) TNNR-APGL: In this approach, the sub-problem (7) is reformulated as follows [29]:
minX∥X∥∗−Tr(AlXBTl)+γ2∥PΩ(X)−PΩ(M)∥2F.(10)
View SourceRight-click on figure for MathML and additional features.By defining
f(X)=−Tr(AlXBTl)+γ2∥PΩ(X)−PΩ(M)∥2F,(11)
View SourceRight-click on figure for MathML and additional features.and
g(X)=∥X∥∗,(12)
View Sourcethe objective function (10) has the following form
minXf(X)+g(X),(13)
View Sourcewhere f is a convex and differentiable function and g is a convex and closed function. Beck and Teboulle [38] proposed the accelerated proximal gradient line search (APGL) -as well as called FISTA- method to solve convex problems of the form (13). This is an iterative method and at step k for known parameters Yk,tk, the new approximation Xk+1 of the solution is obtained from the following approximated problem
Xk+1=argminXf˜(X)+g(X),(14)
View Sourcewhere
f˜(X)=f(Yk)+⟨X−Yk,∇f(Yk)⟩+12tk∥X−Yk∥2F,(15)
View Sourceis a regularization of the linearized part of f(X) around the given point Yk. For convergence the parameters tk and Yk could be updated as follows [38]:
tk+1= 1+1+4t2k−−−−−−√2,(16)
View SourceRight-click on figure for MathML and additional features.
Yk+1= Xk+1+tk−1tk+1(Xk+1−Xk).(17)
View SourceRight-click on figure for MathML and additional features.In TNNR-APGL method, by using (11) and simple mathematical operations the Equation (14) could be reformulated as follows:
Xk+1=argminX∥X∥∗+12tk∥X−(Yk−tk∇f(Yk))∥2F.(18)
View Source

3) TNNR-ADMMAP: This method uses ADMM with adaptive penalty (ADMMAP) to solve (7). This method merges two constraints of (8) and uses adaptive parameter in augmented Lagrangian function to speed up the convergence [29], [44]. Due to the existence of nuclear norm in sub-problem (7), all of the mentioned approaches have to use singular value thresholding operator at each iteration that has high computational cost. For eliminating SVT operator, Liu et al. [30] employed Von Neumann's trace inequality again to approximate the nuclear norm in (6), which gives the following problem
minX⎧⎩⎨⎪⎪⎪⎪⎪⎪⎪⎪maxCCT=DDT=ITr(CXDT)−maxAAT=BBT=ITr(AXBT)⎫⎭⎬⎪⎪⎪⎪⎪⎪⎪⎪s.t.PΩ(X)=PΩ(M).(19)
View SourceRight-click on figure for MathML and additional features.This problem has been solved by an alternating schema [30]. In iteration l with X=Xl the optimal values of variables A,B,C, and D of optimization problem (19) have closed forms as follows
Al=(u1,…,ur)T,Bl=(v1,…,vr)T,Cl=UT,Dl=VT,
View SourceRight-click on figure for MathML and additional features.where U,V are left and right singular vectors of Xl, respectively. After solving the first sub-problem for fixed Al,Bl,Cl,Dl, we should solve the following problem:
minXTr(ClXDTl)−Tr(AlXBTl)   s.t.PΩ(X)=PΩ(M).(20)
View SourceEquation (20) could be solved by ADMM according to variable X, to find new update Xl+1, without needing to employ SVT operator. A weighted versions of this method named TNNR-WRE has been proposed in [30] as
minXTr(ClWXDTl)−Tr(AlWXBTl)   s.t.PΩ(X)=PΩ(M),(21)
View SourceRight-click on figure for MathML and additional features.which has better convergence rate and performance in comparison with (20). Here W is a diagonal weight matrix. Also, an extension of this method named ETNNR-WRE has been proposed in [30] which uses different diagonal weight matrices on the first and second terms of the objective function of (21). The experimental results show the improvement in convergence rate of this method. Although these TNNR based methods have good performance in recovering missed values of an exact, they do not have appropriate solutions for inexact input data in Ω domain. In real applications, the input data may be contaminated by some errors. In this paper, we try to deal with this situation and propose a variant of TNNR model that able to filter the propagation of the error of the input data in the reconstruction process of the missed values.

SECTION 3The Proposed Method
Generally, in addition to missing entries in Ωc, the input data in Ω may be contaminated by some errors. As mentioned all TNNR based methods do not consider this situation in the model problem and so do not give appropriate solutions for inexact input data. In this paper, we try to deal with this problem by adding some terms in objective and constraint of TNNR model. In the constraint of the matrix completion problem (1), usually the input matrix in domain Ω is considered to be exact, but in the application, this is not a suitable constraint for inexact data M in domain Ω. So, if M,M¯¯¯¯¯ denote the inexact and exact versions of input data in Ω, respectively, instead of constraint Xij=Mij,(i,j)∈Ω, the constraint Xij=M¯¯¯¯¯ij,(i,j)∈Ω should be used. Since the exact version M¯¯¯¯¯ is not available, the constraint in the matrix completion problem could be written as follows
Xij=Mij−Eij,(i,j)∈Ω(22)
View Sourcewhere E=M−M¯¯¯¯¯ denotes the unknown error term. Therefore, we have to estimate the matrix X and error term E, simultaneously. Since the noisy part of M is considered to be small in comparison with the signal part of M, its magnitude should be controlled in matrix completion process. This could be achieved for example by controlling the Frobenius norm of the error term in the objective function. Therefore, a TNNR based method in the presence of the noise could be written as follows
minX,E∥X∥r+λ2 ∥E∥2F   s.t.Xij=Mij−Eij,(i,j)∈Ω,(23)
View Sourcewhere λ is the regularization parameter that controls a trade-off between two terms of the objective function. It should be mentioned that instead of Frobenius norm other penalty terms could be used. For example, in this paper, we show that for sparse noise, the ℓ1 norm of the error term could be used in the objective function. We call this method as TNNR for inexact data. Since the propagation of the noise of input data causes increasing the oscillation of the recovered matrix X, we introduce an extension of our TNNR-ID method by adding another term to control this oscillation as follows:
minX,E∥X∥r+τ2∥D1XD2∥2F+λ2∥E∥2Fs.t.Xij=Mij−Eij,(i,j)∈Ω,(24)
View Sourcewhere Di∈Rsi×si, i=1,2 with s1=m,s2=n, is a discrete first-order derivative operator as follows:
Di=⎛⎝⎜⎜⎜⎜⎜⎜−11−1⋱⋱1−1⎞⎠⎟⎟⎟⎟⎟⎟.
View SourceRight-click on figure for MathML and additional features.Here, large value of η(X)=∥D1XD2∥2F denotes the more oscillation [37]. As an example the images of Fig. 1 show the exact image and noisy versions of a test image. The values of η(.) for exact image, image with Gaussian noise and image with Sparse noise are 2.7581×107, 1.3538×109 and 4.2359×109, respectively, which confirms that the value of this function is higher for noisy images. So, by controlling the value of η(X) for the recovered image in model (24) we could control the oscillation and so the propagation of noise in the solution.

Fig. 1. - 
Sample image. (a) Original image, (b) Noisy image by Gaussian Noise with mean equal to 0 and variance equal to 0.02, (c) Noisy image by Sparse Laplacian Noise, (d) Values of rows 150 to 153 red channels of images.
Fig. 1.
Sample image. (a) Original image, (b) Noisy image by Gaussian Noise with mean equal to 0 and variance equal to 0.02, (c) Noisy image by Sparse Laplacian Noise, (d) Values of rows 150 to 153 red channels of images.

Show All

Since the TNNR-ID model in (23) becomes the special case of TNNR-IDD in (24) for τ=0, it's enough to present the minimization algorithm of the TNNR-IDD model. Here, we have to estimate the values of the reconstructed X and error term E, simultaneously. At start by using Von Neumann's trace inequality problem (24) becomes as follows:
minX,Es.t.∥X∥∗−maxAAT=BBT=ITr(AXBT)+τ2∥D1XD2∥2F+λ2∥E∥2FPΩ(X)=PΩ(M−E),(25)
View Sourcewhere A∈Rr×m, B∈Rr×n. At first, the constraint is moved to objective function as follows
minX,E∥X∥∗−maxAAT=BBT=ITr(AXBT)+τ2∥D1XD2∥2F+λ2∥E∥2F +γ2∥PΩ(X)−PΩ(M−E)∥2F.(26)
View SourceRight-click on figure for MathML and additional features.Now, as other TNNR methods, alternating scheme to solve this problem is employed. As initialization we set X0=PΩ(M) and E0=0. The lth iteration of this alternating scheme has the following two steps:

For fixed X=Xl−1 and E=El−1 the Equation (26) should be solved according to A,B. The optimal solutions ATl and BTl of this problem are the first r left and right singular vectors of Xl−1, respectively.

In this step for known Al,Bl new updates Xl,El could be obtained by solving the following sub-problem

minX,E∥X∥∗−Tr(AlXBTl)+τ2∥D1XD2∥2F+λ2∥E∥2F+γ2∥PΩ(X)−PΩ(M−E)∥2F.(27)
View Source
By denoting Z:=(X,E),
g(Z)=∥X∥∗,(28)
View SourceRight-click on figure for MathML and additional features.and
fl(Z)=−Tr(AlXBTl)+τ2∥D1XD2∥2F        +λ2∥E∥2F+γ2∥PΩ(X)−PΩ(M−E)∥2F,(29)
View Sourcethe optimization problem (27) could be represented as follows
minZfl(Z)+g(Z),(30)
View Sourcewhere fl and g are differentiable convex and closed convex functions, respectively and so iterative APGL method could be used to solve it. If Zlk+1 denotes the solution of iteration k of APGL method on (30), similar to problem (13) the steps of APGL for (30) could be rewritten as follows:
Zlk+1=argminZg(Z)+12tk∥Z−(Υk−tk∇fl(Υk))∥2F,(31)
View Sourceand
tk+1= 1+1+4t2k−−−−−−√2,(32)
View SourceRight-click on figure for MathML and additional features.
Υk+1= Zk+1+tk−1tk+1(Zk+1−Zk).(33)
View SourceRight-click on figure for MathML and additional features.As initial point we set Zl1=(Xl,El). If Υk:=(Y1k,Y2k), the gradient ∇Zfl(Υk)=[∇Xfl(Υk)∇Efl(Υk)] has the following components
∇Xfl(Υk)=−ATlBl+τDT1D1Y1kDT2D2(34)
View Source
∇Efl(Υk)=+γ(PΩ(Y1k)−PΩ(M−Y2k)),λY2k+γ(PΩ(Y1k)−PΩ(M−Y2k)).(35)
View SourceBy these explicit forms, according to Eqs. (31), (32), (33), (34), and (35) we have
Zlk+1==argminX,E∥X∥∗+12tk∥∥∥∥[XE]−([Y1kY2k]−tk[∇Xfl(Υk)∇Efl(Υk)])∥∥∥∥2FargminX,E∥X∥∗+12tk∥∥X−(Y1k−tk∇Xfl(Υk))∥∥2F+12tk∥∥E−(Y2k−tk∇Efl(Υk))∥∥2F.(36)
View SourceRight-click on figure for MathML and additional features.Therefore
Xlk+1=argminX∥X∥∗+12tk∥∥X−(Y1k−tk∇Xfl(Υk))∥∥2F,(37)
View SourceRight-click on figure for MathML and additional features.and
Elk+1===argminE12tk∥∥E−(Y2k−tk∇Efl(Υk))∥∥2FY2k−tk∇Efl(Υk)(1−tkλ)Y2k−tkγ(PΩ(Y1k)−PΩ(M−Y2k)).(38)
View SourceRight-click on figure for MathML and additional features.The problem (37) could be solved by SVT and the solution will be
Xlk+1=Udiag(max{σi−tk,0})VT,(39)
View Sourcewhere σi are the singular values and U,V are the left and right singular vectors of the following matrix
Y1k−tk∇Xfl(Υk)=Y1k+tkATlBl−tkτDT1D1Y1kDT2D2−tkγ(PΩ(Y1k)−PΩ(M−Y2k)),(40)
View Source

respectively. In the end of inner loop the approximated solutions Xl+1 and El+1 will be obtained. This algorithm summarized in Algorithm 1. For getting solutions of (23), just put τ=0 in (39).

You can see the proof of decreasing process and complexity of Algorithm 1 in Appendices A and B, which can be found on the Computer Society Digital Library at http://doi.ieeecomputersociety.org.ezproxy.auckland.ac.nz/10.1109/TKDE.2020.2983708, respectively.

SECTION Algorithm 1.The Main Steps of TNNR-IDD
Input: M: an incomplete original image; Ω: the set of observed entries position; λ, γ, and τ, the regularization parameters.

Output: Xrec: the recovered image.

initial: l=1, X0=PΩ(M), Itmaxin, Itmaxout; r, the number of subtracted singular values; and two tolerance ϵin, ϵout.

repeat

Step 1.

[U,Σ,V]=svd(Xl−1); Al=(u1,…,ur)T; Bl=(v1,…,vr)T;

Step 2. k=1, t1=1, Y11=Xl1=Xl−1, Y21=El1=0;

repeat

Compute Elk+1 and Xlk+1 by Eqs. (38) and (39), respectively ;

Update tk+1 and Υk+1:=(Y1k+1,Y2k+1) by Eqs. (32) and (33), respectively.

compute F(Zlk+1)=fl(Zlk+1)+g(Zlk+1) by Eqs. (28), (29);

k=k+1;

until −(F(Zlk)−F(Zlk−1))≥ϵin & k≤Itmaxin

Xl=Xlk+1; El=Elk+1;

l=l+1;

until ∥Zl−Zl−1∥F∥M∥F≥ϵout & l≤Itmaxout

return Xrec=Xl;

3.1 Proposed Method for Sparse Noise
In situations that the inexactness of the observed domain is sparse, it's better to use the penalty term ∥E∥1=∑i,jEij instead of 12∥E∥2F of in models (23) and (24) to control the error term. Like as before, APGL method could be used to solve these new versions. For example, due to the mentioned replacement in the sparse version of TNNR-IDD, the functions g and fl in (30) will be
g(Z)=∥X∥∗+λ∥E∥1,(41)
View Source
fl(Z)=−Tr(AlXBTl)+τ2∥D1XD2∥2F        +γ2∥PΩ(X)−PΩ(M−E)∥2F.(42)
View SourceRight-click on figure for MathML and additional features.Therefore the all steps of the APGL on this new version become according to Algorithm 1, except the ∇Efl and Elk+1 that will be
∇Efl(Υk)=γ(PΩ(Y1k)−PΩ(M−Y2k)),(43)
View SourceRight-click on figure for MathML and additional features.
Elk+1==argminE12tk∥∥E−(Y2k−tk∇Efl(Υk))∥∥2F+λ∥E∥1Sλtk(Y2k−tk∇Efl(Υk)),(44)
View Sourcewhere Sε is the soft thresholding operator defined as follows:
(Sε(F))ij=sign(Fij)max{|Fij|−ε,0},
View Sourcefor any matrix F∈Rm×n [45].

SECTION 4Experimental Results
In this section, we present several experiments on different color images, including 12 well-known images (Fig. 2) and BSD68 database (68 images). Also, we use 40 grayscale images that choose one image per person from ORL 32×32 data set. The proposed method is compared with some type of methods:

Fig. 2. - 
Image Data Set. First Row, left to right: Autumn, Wheat, Sunrise, Forest, Sea, Apartment. Second row, left to right: Veranda, Village, Starfish, Desert, Butterfly, Boat.
Fig. 2.
Image Data Set. First Row, left to right: Autumn, Wheat, Sunrise, Forest, Sea, Apartment. Second row, left to right: Veranda, Village, Starfish, Desert, Butterfly, Boat.

Show All

Matrix completion methods based on nuclear norm like IRNN-Lp and IRNN-SCAD [46],

Matrix completion methods based on TNNR, like TNNR-ADMM1 [29], TNNR-WRE and ETNNR-WRE [30],

Completion for inexact data with a nuclear norm based robust matrix completion method proposed by Cande`s et al. [34] and in this paper is denoted by MCN,

Deep learning matrix completion proposed in [47], which we denote it as DLMC.2

Also, we applied the mentioned robust method in [33], and two deep learning denoising methods -named DnCNN [48] and FFDNet [49]- but since these methods did not give appropriate solutions, we did not report results of them. To evaluate the quality of the recovered images we used peak signal-to-noise ratio (PSNR). In Algorithm 1 we set its parameters as Itmaxout=10,Itmaxin=200 and ϵout=0.01,ϵin=0.1 for all experiments. Although, in this section most focus on whit Gaussian noise for observed data, we provided two types of inexact observed data with withe Gaussian and sparse noise.

4.1 White Gaussian Noise for Inexact Observed Data
In this section, we do experiments on twelve well-known color images that could be seen in Fig. 2, BSD68 dataset that some images of this database could be seen in Fig. 3, and 40 images of ORL 32×32 dataset (one image per person). Here, inexactness of the input data is created by white Gaussian noise with mean equal to 0 and variance equal to 0.003 that level of noise for different images approximately between 0.08 to 0.73. Also, each image is corrupted with three Text, Random and Block missing methods. In fact, for each image, we construct three test problems with different corruption methods and apply nine algorithms for each test. Since the color images have three channels, we apply algorithms on each channel separately and then combine the results to form the final result. In the following, we investigate the results separately for different corruptions.

Fig. 3. - 
Some images from BSD68 dataset.
Fig. 3.
Some images from BSD68 dataset.

Show All

1) Text Missing : Here the positions of a text in the images are considered as missing. Fig. 4a shows an example of text missing. We applied different completion algorithms on the test images that corrupted by the mentioned text missing method. Table 1 shows the results of each method on every test images, mean, median, and standard deviation of three datasets and total mean and total median of all test images. According to this table, it can be observed that the PSNR provided by our proposed TNNR-ID and TNNR-IDD methods are higher than the other methods. As it is clear, based on Table 1, the proposed methods give better results in comparison with robust MCN model and other nuclear norm based methods. Even this method works better than deep DLMC method. For more details, Fig. 4 shows the corrupted versions of the Starfish test image and the recovered image by all methods could be seen in this figure. It can be seen that the IRNN-Lp method which is a nuclear norm based method does not give a suitable recovery and still we can see effects of text missing. Although the IRNN-SCAD which is an another nuclear norm based method, TNNR-ADMM, TNNR-WRE and ETNNR-WRE methods could detect text missing and complete it, they could not remove the Gaussian noise in the image. Also, the DLMC method that is a deep learning matrix completion method recover the text missing better than other completion methods, but this method also could not remove the Gaussian noise. The MCN method almost able to denoise the image, but the accuracy of this method is less than our proposed methods. However, the TNNR-ID and TNNR-IDD are able to recover and denoise the image simultaneously.

TABLE 1 PSNR of Recovered Images by Different Methods for all Test Images With Gaussian Noise and Text Missing


Fig. 4.
Experiment results on Starfish image with Gaussian noise and Text missing. (a) Noisy Starfish image with a text missing, (b) IRNN-Lp PSNR = 22.8532, (c) IRNN-Scad PSNR = 24.3446, (d) TNNR-ADMM PSNR = 24.4026, (e) TNNR-WRE PSNR = 24.4214, (f) ETNNR-WRE PSNR = 24.4284, (g) MCN PSNR = 24.3311, (h) DLMC PSNR = 24.4479, (i) TNNR-ID PSNR = 26.4475, (j) TNNR-IDD PSNR = 27.7889.

Show All

2) Random Missing: This method generates a random matrix with elements one and zero. Then this random matrix is multiplied pointwise to each image. For all images by this missing method, the 60 percent of the elements are considered to be missed. Fig. 5 shows an example of such corruption on Desert test image. The result of comparing TNNR-ID and TNNR-IDD with recent matrix completion methods are presented in Table 2. We can see that the performance of our methods are better than the others. For more details we present the corrupted and recovered images of Desert test image by all completion algorithms in Fig. 5. The results confirm the quality of the proposed methods.

TABLE 2 PSNR of Recovered Image of Different Methods for all Test Images With Gaussian Noise and 60 percent Random Missing
Table 2- 
PSNR of Recovered Image of Different Methods for all Test Images With Gaussian Noise and 60 percent Random Missing
Fig. 5. - 
Experiment results on Desert image with Gaussian noise and Random missing. (a) Noisy Desert image with a Random missing, (b) IRNN-$L_p$Lp PSNR = 13.5545, (c) IRNN-Scad PSNR = 26.6701, (d) TNNR-ADMM PSNR = 26.9422, (e) TNNR-WRE PSNR = 27.5109, (f) ETNNR-WRE PSNR = 27.5315, (g) MCN PSNR = 28.8033, (h) DLMC PSNR = 28.0485, (i) TNNR-ID PSNR = 30.7281, (j) TNNR-IDD PSNR = 32.1953.
Fig. 5.
Experiment results on Desert image with Gaussian noise and Random missing. (a) Noisy Desert image with a Random missing, (b) IRNN-Lp PSNR = 13.5545, (c) IRNN-Scad PSNR = 26.6701, (d) TNNR-ADMM PSNR = 26.9422, (e) TNNR-WRE PSNR = 27.5109, (f) ETNNR-WRE PSNR = 27.5315, (g) MCN PSNR = 28.8033, (h) DLMC PSNR = 28.0485, (i) TNNR-ID PSNR = 30.7281, (j) TNNR-IDD PSNR = 32.1953.

Show All

3) Block Missing: In this experiment, we added 9 blocks as positions of missing values of the images. Since size of images in ORL 32×32 is too small for apply 9 blocks, we just consider one block of size 10×10 for this dataset. The PSNRs of recovered images are reported in Table 3. Similar to previous experiments, the accuracy of TNNR-ID and TNNR-IDD methods are most better than other methods even in comparison with robust MCN method. Also, the corrupted and the recovery of all methods for Boat test image are shown in Fig. 6. Fig. 7 shows the performance of different methods on one of the ORL test images. Here any image is considered as a two-variable function and horizontal and vertical axes of each plot indicate the indices of pixels and the grayscale values, respectively. Here it is clear that the behavior of the reconstructed image (surface) by the proposed methods are closer than the other to the original image. Also, the oscillation of the obtained images by the proposed approach is less than the others. This behavior illustrates the effect of the error term E on our modeling.

TABLE 3 PSNR of Recovered Images by Different Methods for all Test Images With Gaussian Noise and Block Missing
Table 3- 
PSNR of Recovered Images by Different Methods for all Test Images With Gaussian Noise and Block Missing
Fig. 6. - 
Experiment results on Boat image with Gaussian noise and Blocks missing. (a) Noisy Boat image with a blocks missing, (b) IRNN-$L_p$Lp PSNR = 15.4140, (c) IRNN-Scad PSNR = 22.2718, (d) TNNR-ADMM PSNR = 22.3233, (e) TNNR-WRE PSNR = 22.3590, (f) ETNNR-WRE PSNR = 22.3464, (g) MCN PSNR = 22.1047, (h) DLMC PSNR = 21.3717, (i) TNNR-ID PSNR = 23.2803, (j) TNNR-IDD PSNR = 23.6865.
Fig. 6.
Experiment results on Boat image with Gaussian noise and Blocks missing. (a) Noisy Boat image with a blocks missing, (b) IRNN-Lp PSNR = 15.4140, (c) IRNN-Scad PSNR = 22.2718, (d) TNNR-ADMM PSNR = 22.3233, (e) TNNR-WRE PSNR = 22.3590, (f) ETNNR-WRE PSNR = 22.3464, (g) MCN PSNR = 22.1047, (h) DLMC PSNR = 21.3717, (i) TNNR-ID PSNR = 23.2803, (j) TNNR-IDD PSNR = 23.6865.

Show All

Fig. 7. - 
The surface of all methods on one image of ORL dataset at block missing experimental. (a) Original, (b) Noisy, (c) Noisy-Missing, (d) IRNN-$ L_p$Lp, (e) IRNN-SCAD, (f) TNNR-ADMM, (g) TNNR-WRE, (h) ETNNR-WRE, (i) MCN, (j) DLMC, (k) TNNR-ID, (l) TNNR-IDD.
Fig. 7.
The surface of all methods on one image of ORL dataset at block missing experimental. (a) Original, (b) Noisy, (c) Noisy-Missing, (d) IRNN-Lp, (e) IRNN-SCAD, (f) TNNR-ADMM, (g) TNNR-WRE, (h) ETNNR-WRE, (i) MCN, (j) DLMC, (k) TNNR-ID, (l) TNNR-IDD.

Show All

For better perception, we report the box plot of PSNR for all methods on whole images in Fig. 8. As shown in this figure, the best and worst PSNRs of our methods are better than the corresponding values of other methods. Also, it's clear from these box plots that in all experiments, the PSNR of the first, the second (median), and the third quartiles of our methods strictly are better than the others. Also, for all datasets Table 4 demonstrates the percent that TNNR-ID and TNNR-IDD methods work better than the other ones according to different missings for all data sets. The results show the superiority of the proposed methods over others.

TABLE 4 The Superiority Percentage of Proposed Methods to Other Methods in Different Experiments

Fig. 8. - 
The PSNRs of all methods on whole images. (a) Text missing experimental, (b) 60 percent random missing experimental, (c) Block missing experimental.
Fig. 8.
The PSNRs of all methods on whole images. (a) Text missing experimental, (b) 60 percent random missing experimental, (c) Block missing experimental.

Show All

Overall, the reported results by these figures and tables and different statistical quantities confirm the quality of the proposed method over others. Also the standard deviation of all methods almost in the same range.

In all experiments, the parameter r of TNNR methods was changed to achieve the best accuracy of each method for each image. In experiments, we observed that the proposed methods usually with smaller r give the better results in comparison with the other methods even with larger values of r. This is another evidence for the quality of the proposed methods. For example Figs. 9a and 9b show the PSNR of recovered images of all algorithms by different values of r applied to Apartment image by text missing and Veranda test image by random missing, from left to right. Here we see that in both test problems the recovered images of our proposed methods even by r=1 give the better PSNR in comparison with the other TNNR based methods, even with large r.


Fig. 9.
Variations of PSNR according to different r for all TNNR based methods applied on (a) Apartment image by Text missing (b) Veranda image by Random missing.

Show All

By these experiments, we found that our methods with small values of r could give the most qualified solutions compared with the others. This small value of r causes a sensible reduction in computational costs of the recovery process.

4.2 Sparse Noise for Inexact Observed Data
In the end, we consider a situation that the observed data is contaminated by a sparse noise. Here the sparse versions of our proposed methods introduced in Section 3.1 are used to recover the incomplete image with inexact observed domain. For example, we consider a version of Forest and Butterfly images that is contaminated by a sparse Laplacian noise and then those some parts corrupted by a text missing method. The corrupted images can be seen in Figs. 10 and 11. We applied all mentioned completion methods to reconstruct this images and their best results are reported in Figs. 10 and 11 with their corresponding PSNR.

Fig. 10. - 
Experiment results on Forest image with a sparse Laplacian noise and a Text missing. (a) Noisy Butterfly image with a text missing, (b) IRNN-$L_p$Lp PSNR = 23.4893, (c) IRNN-Scad PSNR = 24.0915, (d) TNNR-ADMM PSNR = 24.2187, (e) TNNR-WRE PSNR = 24.2534, (f) ETNNR-WRE PSNR = 24.3070, (g) MCN PSNR = 24.8653, (h) DLMC PSNR = 24.4878, (i) TNNR-ID PSNR = 26.0828, (j) TNNR-IDD PSNR = 26.9850.
Fig. 10.
Experiment results on Forest image with a sparse Laplacian noise and a Text missing. (a) Noisy Butterfly image with a text missing, (b) IRNN-Lp PSNR = 23.4893, (c) IRNN-Scad PSNR = 24.0915, (d) TNNR-ADMM PSNR = 24.2187, (e) TNNR-WRE PSNR = 24.2534, (f) ETNNR-WRE PSNR = 24.3070, (g) MCN PSNR = 24.8653, (h) DLMC PSNR = 24.4878, (i) TNNR-ID PSNR = 26.0828, (j) TNNR-IDD PSNR = 26.9850.

Show All


Fig. 11.
Experiment results on Butterfly image with a sparse Laplacian noise and a Text missing. (a) Noisy Butterfly image with a text missing, (b) IRNN-Lp PSNR = 27.0463, (c) IRNN-Scad PSNR = 27.6458, (d) TNNR-ADMM PSNR = 27.6768, (e) TNNR-WRE PSNR = 27.6844, (f) ETNNR-WRE PSNR = 27.7292, (g) MCN PSNR = 26.8185, (h) DLMC PSNR = 27.4919, (i) TNNR-ID PSNR = 29.5561, (j) TNNR-IDD PSNR = 31.005.

Show All

Here the results confirm the quality of the proposed matrix completion method on incomplete images with inexact observed data.

SECTION 5Conclusion
In this paper, a novel approach for matrix completion problem on inexact observed data is proposed. Two variants of this approach, named TNNR-ID and TNNR-IDD has been introduced. Experimental results on different test problems with distinct corruptions show the high quality of the proposed approach in comparison with the other TNNR based methods, other methods based on nuclear norm, other deep learning method and other robust completion methods like MCN. Also, our proposed methods with a small value of rank r give the better results in comparison with the other TNNR methods even with large values of r.

