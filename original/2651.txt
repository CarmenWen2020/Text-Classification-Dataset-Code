With the prevalence of online social platforms, social recommendation has emerged as a promising direction that leverages the social network among users to enhance recommendation performance. However, the available social relations among users are usually extremely sparse and noisy, which may lead to inferior recommendation performance. To alleviate this problem, this paper novelly exploits the implicit higher-order social influence and dependencies among users to enhance social recommendation. In this paper, we propose a novel embedding method for general bipartite graphs, which defines inter-class message passing between explicit relations and intra-class message passing between implicit higher-order relations via a novel sequential modelling paradigm. Inspired by recent advances in self-attention-based sequential modelling, the proposed model features a self-attentive representation learning mechanism for implicit user-user relations. Moreover, this paper also explores the inductive embedding learning for social recommendation problems to improve the recommendation performance in cold-start settings. The proposed inductive learning paradigm for social recommendation enables embedding inference for those cold-start users and items (unseen during training) as long as they are linked to existing nodes in the original network. Extensive experiments on real-world datasets demonstrate the superiority of our method and suggest that higher-order implicit relationship among users is beneficial to improving social recommendation.
SECTION 1Introduction
Rcent years have witnessed an increasing amount of research attention on social recommendation, which harnesses social relations to boost the performance of recommender systems [1], [2], [3], [4], [5]. The most impactful theories behind social recommendation are homophily assumption [6] and social influence [7]. In the recommendation context, homophily assumption supposes that similar users share similar preferences, which will lead to similar purchase decisions. In addition to homophily, social influence is another important aspect as it implies users’ purchases are influenced by people having close social ties to them, such as friends, relatives, colleagues, etc. On this basis, social recommendation methods are widely applied as an enhancement upon traditional recommender systems with proven effectiveness in recommendation tasks [3], [4].

For mining user preferences, most recommender systems treat user-item interactions as an indispensable information source, which can be viewed as bipartite graphs. From the user-item bipartite graphs, vectorized representations (a.k.a. embeddings) of both the users and items can be learned via observed interactions, and future user-item interactions can be easily inferred via the learned embeddings [8], [9]. To enrich the semantics within the learned user embeddings, existing social recommender systems primarily adopt the user-user social relationships to thoroughly model user preferences [2], [3], [4]. Specifically, these methods learn two embeddings for each user from the social domain and item domain, and then fuse both embeddings to obtain a comprehensive user representation used for estimating user-item interactions.

However, in recommendation tasks, the sparsity issue is protuberant when utilizing social relations to enrich the information in a user-item bipartite graph. Even for online social platforms, the observable connections between users are also sparse. For example, Yelp1 is an online review platform where users can rate and review a business (e.g., a restaurant) they have visited. Besides, Yelp users are also able to make friends and join different communities. Whilst the platform is gaining extensive popularity over time, most users only use it as a tool for business searching, regardless of its social functions. Consequently, statistics of Yelp's user network (see Section 4) show that, though there are more than 70 million possible user-user links in Edinburgh, only less than 0.04 percent are actually established, contributing to an extreme sparsity of 99.96 percent. Data from Wisconsin shows even more scattered user friendships on Yelp with a 99.99 percent sparsity. Therefore, due to the sparsity issue, the existing social recommendation may consistently fail as the limited social relations would not provide enough auxiliary information to improve the recommendation performance. A similar investigation from IBM reveals that Facebook delivered just 0.68 percent referral traffic on Black Friday, while the percentage was 0 on Twitter [10]. Furthermore, previous research on recommendation [1], [11] points out that the performance gain from introducing social data is marginal due to the limited amount of social relations in the context of recommender systems.

On top of the sparsity issue, active users tend to interact with different items more frequently [12], and traditional recommender systems can easily identify the preferences of active users and recommend the right items to them. In contrast, inactive users are less likely to have abundant interactions with items, hence social recommenders intend to additionally incorporate the social interactions of users to maintain the recommendation quality on these inactive users. However, the performance of social recommenders is hugely constrained by the characteristics of users’ social data. On the one hand, inactive users are also reluctant to establish social connections with other users [13], so it is impractical to assume the sufficiency of social data for such users. On the other hand, as active users usually have a wide variety of friends, the casually formed friendships and inactive social friends are not fully filtered out by existing social recommenders, thus taking excessive noises when modelling each user's preferences from her/his social ties. Confronted with the sparse and noisy social data, the capability of mining high-quality latent semantics from user-user and user-item interactions becomes rather important for social recommender systems. Unfortunately, while the explicitly observable user friendships can be easily modelled as first-order relations for social recommendation, the rich semantic information within indirect higher-order relationships are largely neglected by all the aforementioned social recommenders. For instance, two users who reviewed the same business or purchased the same item should possess an implicit social relationship where similar preferences can be implied. This inevitably sets an obstacle for obtaining satisfying recommendation results, particularly for inactive or new users whose interaction records are typically sparse.

To this end, in this paper, we seek solutions to remedying the deficiencies of existing social recommender systems. Specifically, we propose a general bipartite embedding method FBNE (short for Folded Bipartite Network Embedding) for social recommendation. FBNE tackles the aforementioned challenges of social recommendation by exploring the higher-order relations among users and items in user-item bipartite graphs and learning user/item embeddings via a novel sequence-aware paradigm. We briefly introduce these two key aspects of FBNE below.

Exploring Higher-Order Relations. In a user-item bipartite graph, we innovatively split the relations among user/item nodes into two types, namely inter-class relations and intra-class relations. Inter-class relations are observed links between users and items, while intra-class relations are implicitly inferred from the original bipartite graph for vertices of the same types (i.e., either users or items). To discover the hidden intra-class relations, FBNE employs a folding process that folds a bipartite graph into two homogenous networks for each vertex type. For example, in a user-item bipartite graph, the folding process will generate a user-user graph and an item-item graph. In the folded graphs, a user-user link represents the implicit higher-order links between two users that have purchased the same items, while an item-item link can be identified between two items that are commonly purchased together. Therefore, by leveraging the higher-order intra-class relations, we are able to augment the sparse user-user social graph data and generate more informative embeddings of both users and items.

Sequential Influence Modelling. Compared with the inter-class graphs, learning distinctive node representations from the folded intra-class graphs is non-trivial because the relationships are implicitly established. Recently, graph neural network-based (GNN-based) methods [14], [15], [16] start to show favorable effectiveness in social recommendation tasks [3], [4], where each node's representation is obtained by aggregating the information from its local neighbors. Nevertheless, their complex non-linear forms prevent them from modelling higher-order relationships, and the modelling of only first-order or second-order relationships is adopted for a trade-off between efficiency and performance. Hence, in FBNE, we consider sequence-based methods that learn node representations via node sequences sampled from graphs. As traditional sequence-based methods like node2vec [17], deepwalk [18], and metapath2vec [19] are computationally expensive when modelling long sequences [20], we propose a novel self-attention scheme to learn node representations via sequential influence modelling. Self-attention mechanism [21] is highly efficient and capable of mining the sequential and semantic patterns across the sampled node sequences, and the influence from noisy social data can be effectively mitigated by assigning larger attentive weights on influential nodes.

Apart from the effectiveness evaluation under the traditional social recommendation setting, to fully demonstrate our model's capability of utilizing limited user-item interactions and user-user social relationships for recommendation, we further study a novel problem of inductive inference for social recommendation. With the novel folding mechanism and self-attention based node embedding scheme, FBNE is fully able to learn representations of cold-start users and items. Meanwhile, we propose a joint training procedure that links different bipartite graphs to account for multiple information sources. As such, FBNE supports inductive representation learning that gathers information from observed user/item nodes and generates embeddings for those unseen users/items. Therefore, FBNE achieves superior performance on generic social recommendation problems, and can be effectively generalized to cold-start recommendation scenarios.

The primary contributions of our research are summarized as follows:

To cope with the sparse and noisy social data in social recommendation tasks, we propose FBNE, a general yet effective bipartite graph embedding model. FBNE investigates higher-order relationships between nodes by folding the user-item bipartite graphs and learn representative user/item embeddings. The self-attention-based node embedding scheme enables FBNE to adaptively distinguish important nodes from noisy ones.

The proposed FBNE further supports inductive learning for new users/items, thus allowing for social recommendation under the cold-start setting. Based on the learned embeddings of partially observable nodes, FBNE can effectively generate the embedding for an inductive user/item node by aggregating the information from related nodes within multiple bipartite graphs.

We conduct extensive experiments to evaluate the performance of FBNE on two real large-scale e-commerce datasets. In comparison with state-of-the-art baselines, the advantageous performance of FBNE fully demonstrates its superiority in social recommendation tasks.

SECTION 2Preliminaries and Problem Definitions
In this section, we introduce the necessary definitions and notations used in this paper.

2Definition 1 (User-User Graph).
A User-User graph, denoted as GUU=(VU,EUU), captures the social relationships among users, where VU is a set of users and EUU is a set of edges between users. Given two users u and u′, if they are friends, there is an edge euu′ between them.

2Definition 2 (User-Business Bipartite Graph).
A User-Business graph GUB=(VU,VB,EUB) is a bipartite graph where VU denotes the user set and VB denotes the business set (i.e., item set), while EUB⊆VU×VB is the set of edges between users and businesses. If a user u has visited a business b, there is an edge eub between them. If user ratings are available, the edge weight wub is defined as the actual rating; otherwise, the weight wub=1.

Analogously, various flexible bipartite graphs that are associated with business nodes (items) can be constructed. For example, GBC=(VB,VC,EBC) and GBA=(VB,VA,EBA) for Business-Category (Attributes) nodes in the example in Fig. 1. Finally, we formally define the problems investigated in our work. We aim to solve the following problems:

Fig. 1. - 
A Yelp network that consists of multiple bipartite graphs and a social graph.
Fig. 1.
A Yelp network that consists of multiple bipartite graphs and a social graph.

Show All

2Problem 1 (Bipartite Network Embedding).
Given a heterogenous bipartite graph for a social network G=(V,E) (a joint graph of multiple bipartite graphs), we aim to learn low-dimensional vector representations for each vertex in V, such that in the embedding space, both the inter-class explicit interactions between vertices of different type and the implicit higher-order interactions between vertices of same type can be preserved. To keep the notations simple, we use bold lower-case letter vi to denote the embedding vector for a vertex vi, and use bold upper-case letter V to represent a corresponding embedding matrix.

2Problem 2 (Social Boosted Recommendation).
Given a heterogenous bipartite graph for a social network G, we consider the following two recommendation tasks: 1). (General Social Recommendation) Given a user u∈VU, the goal is to recommend top-n items (businesses) b∈VB that u is interested in. 2). (Cold-Start Social Recommendation). For a cold-start user u∉VU (u is not seen during the training phrase, but has links to VU), our goal is to recommend top-n items (businesses) b∈VB that u is interested in.

SECTION 3Folded Bipartite Network Embedding for Social Recommendation
In what follows, we detail our proposed FBNE which performs social recommendation by embedding the bipartite networks. The purpose of a bipartite graph embedding model is to preserve the linkage information between two disjoint sets of nodes (i.e., users and items) with learned low-dimensional node embedding vectors. These node embeddings are expected to be able to infer and reconstruct the original pairwise user-item links. To achieve this goal, FBNE models the bipartite graph from two perspectives: (1) message passing from observed links between two sets of nodes; and (2) higher-order message passing between same typed nodes from unobserved but transitive links.

Bipartite graphs are special cases of general graphs, where links are formed between two disjoint sets of nodes, and these two sets of nodes are usually different in terms of their types, carrying different semantics. Compared with modelling homogenous graphs, additional information can be mined from bipartite graphs. First, the main structure of a bipartite graph can be directly modelled through the observed links between two sets of nodes (e.g., user-item interactions in a user-item bipartite graph), which reveals the information flow exchanged between two different types of nodes. We term such links the inter-class relations. Besides, for each specific node set, there exists unobserved higher-order message passing among the same typed nodes through transitive links (e.g., co-purchase relationship exists between two users if they have purchased one same item), which is referred to as intra-class relations. Therefore, learning embeddings for a bipartite graph by using traditional graph embedding methods is insufficient and will eventually lead to information loss, and a sensible embedding approach for bipartite graphs should be able to preserve both important properties.

3.1 Modelling Inter-Class Message Passing
First, to model the inter-class similarities between two sets of nodes, taking the user-item bipartite graph as an example, we adopt the idea of weight sharing in GCNs [14], [22], and create message passing channels for each user and item through their interactions in a user-item bipartite graph. The graph convolution mimics the message passing schema, and the representation of each node is computed by aggregating the information passed from its direct heterogeneous neighbours. We assume each node has at least one inter-class links. Therefore, for each link in a user-item bipartite graph, we model the message passing as an information transformation from one end of the link to the other. For example, the message passing from item bj to user ui is the following form:
μji=1cijWubxbj,(1)
View SourceRight-click on figure for MathML and additional features.where cij is a normalization constant, which we choose to be either |N(ui)| or |N(ui)||N(bj)|−−−−−−−−−−−√ (symmetric normalization), with N(ui) denoting the set of neighbors of user node i, and N(bj) denoting the set of neighbors of item node j. Wub is the parameter matrix and xbj is the feature vector of item node j.

After the message passing step, we aggregate incoming messages from every node by using an aggregation function Agg(⋅) that aggregates the neighbors N(ui) connected by links between user i and her/his interacted items. Mathematically, we define the aggregation as follows:
hUi=σ(W⋅Agg({μji,j∈N(ui)})),(2)
View SourceRight-click on figure for MathML and additional features.where σ(⋅) is an element-wise activation function such as ReLU(⋅)=max(0,⋅). W is a learnable parameter matrix. We will refer to Eq. (2) as a graph convolution layer [14], [22]. Analogously, at the other side of a given bipartite graph (e.g., all items in a user-item graph), the item embedding hBi is computed with the same parameter matrix W by performing graph convolutions according to Eq. (2). The graph convolution steps will capture the messages exchanged between two sets of nodes, and therefore preserve the main structure of a bipartite graph. Next, we discuss our design of the aggregation function Agg(⋅).

To aggregate the message passed from neighbours, the original graph convolutional network (GCN) has to obtain the entire graph in order to run the graph convolutional kernel, which is inefficient and inflexible when handling large-scale bipartite graphs. To alleviate this problem, GraphSAGE [15] recently makes an attempt that allows GCN to sample and aggregate a fixed number of neighbours. However, for each node, GraphSAGE uniformly samples a number of neighbours to form a receptive field, which treats all neighbours equally and neglects the varied importance of different neighbourhoods. In fact, different neighbour nodes are not equally influential in terms of passing information to the target node. As a result, a naive neighbourhood sampling scheme can neither highlight influential neighbours nor filter out the noise from irrelevant neighbours.

In light of this, we follow our previous work [23] to sample the neighbours of a node based on their centrality measurements. To be specific, we investigate various centrality indices and perform biased sampling according to their centrality information. Fig. 2 shows the adopted neighbourhood sampling strategy, which first ranks the neighbors of a target node according to a specific centrality measurement (e.g., degree, closeness, betweenness, etc.), and then forms a node sequence by picking a fixed amount of top-ranked nodes. The sampling approach guarantees that the computation of a node is based on its most influential neighbours. Furthermore, a node defines its own computation of embeddings and no longer needs the presence of the entire topology of a graph (it only requires each node to know its immediate adjacent neighbours).

Fig. 2. - 
Centrality based sampling.
Fig. 2.
Centrality based sampling.

Show All

After the sampling, the model has to aggregate the information passed from the sampled nodes. According to existing works on GCNs, popular aggregation functions are Mean(⋅), Max(⋅), LSTM(⋅), Pooling(⋅), etc. We adopt the mean aggregator where it simply takes the element-wise mean of the vectors in {μji,j∈N(ui)}. We do not compare different aggregators as this has been done by previous works [14], [15], [24], [25]. However, despite the significance of distinguishing the varied influences of different neighbour nodes, the mean aggregator assumes all neighbours contribute equally to represent a node, which can hardly generate optimal embeddings. Inspired by [2], [16] and attention mechanism [26], [27], we allow neighbours to contribute differently to a target node's latent factor by assigning each neighbour node a different weight. The graph convolutional layer is now written as:
hUi=σ(W⋅Mean({αjiμji,j∈N(ui)})),(3)
View SourceRight-click on figure for MathML and additional features.where αji is the attentive weight for message μji passed from neighbour (an item node in this case) bj,j∈N(ui) to the target node (a user node). Specifically, we parameterize the attention weight αji on item j with a two-layer attention network. The input of the attention network is the message representation μji through the interaction and the target user ui's embedding ui. Formally, the attention network is,
wji=WT2⋅σ(W1⋅[μji⊕ui]+b1)+b2.(4)
View SourceRight-click on figure for MathML and additional features.The final attention weights are obtained by normalizing the above attentive scores using Softmax function:
αji=exp(wji)∑j′∈N(ui)exp(wj′i).(5)
View SourceRight-click on figure for MathML and additional features.

3.2 Modelling Intra-Class Message Passing
In addition to inter-class relationship modelling, it is also crucial to model the intra-class message passing between two nodes of the same type. Compared with general homogeneous graphs, a special property of a bipartite graph is that links in a bipartite graph can be only formed between two disjoint sets of nodes, and there is no direct links between same typed nodes. For example, in Fig. 3, there are two different sets of nodes U and V, and links between U and V construct the bipartite graph while there are no links among the nodes in the same set (i.e., u∈U and v∈V). It is insufficient for a bipartite graph embedding model to only consider the direct links between two different sets of nodes, because the implicit relationships between the same typed nodes also carry substantial semantic information.

Fig. 3. - 
Folding a bipartite graph.
Fig. 3.
Folding a bipartite graph.

Show All

Although no explicit links can be directly observed between same typed nodes, there always exists higher-order relationships among the nodes in the same set. If two nodes from set U are commonly connected by a node from set V, we regard these two nodes from U possess a higher-oder connectivity (second-order proximity in this case). For example, in Fig. 3, node 1 and 2 are both connected to node A, which implies node 1 and 2 are also connected through the path 1-A-2. We extend this simple observation to a more general claim that in a bipartite graph, for two vertices of the same type, if there exists a path between them, there should be implicit intra-class relationships between them.

Similar to the inter-class relationship modelling, we also consider the message passing among the nodes that are in the same set, and we name it the intra-class message passing in this paper. Intuitively, for two vertices of the same type, if there exists a path between them in a bipartite graph, there should be certain message passing between them, and the number of the paths and the distances between two nodes in a path indicate the strength of the information passed between them. For example, starting from node 7, the intra-class message passing exists between node 7 and all the remaining nodes {1,2,3,4,5,6} through different paths (e.g., paths from node 7 to node 6 and 5 through node D; from node 7 to node 4 through the path {D,5,C}). Obviously, the strength of the message passings starting from node 7 to node 5 and 4 are different, the message from node 7 to node 5 is stronger than that from node 7 to node 4 as the distance between node 7 and 5 (distance of 2) is smaller than the distance between node 7 and node 4 (distance of 4).

Unfortunately, counting the paths and distances between two vertices has a rather high complexity of an exponential order, which is infeasible for implementation on large networks. To encode such higher-order message passing among vertices in a bipartite network, we exploit the folded bipartite graph derived from the original bipartite graph, and generate node sequences from the folded graph. By modelling the generated node sequences, the message passing schema and the signal strength between any two same typed nodes can be captured. Specifically, the bipartite graph is first converted to two folded homogenous graphs, as illustrated in Fig. 3. In the middle of Fig. 3, there is a bipartite graph consisting of two different types of nodes (U and V), the folding process folds the original bipartite graph into two homogenous graphs (graph of U on the left and graph of V on the right) by linking the same typed vertices via transitive links through the other type of nodes. Mathematically, the folding process can be expressed via the self-multiplication of the adjacency matrix:
Afolded=A×AT,(6)
View SourceRight-click on figure for MathML and additional features.where A∈Rn×n is the adjacency matrix of the original bipartite graph, Afolded is the folded graph adjacency matrix, from which two folded homogenous networks can be obtained.

Then, to model the message passing and the signal strength between two intra-class nodes, for each folded homogenous graph, we sample two corpora of vertex sequences with random walks, then the embeddings are learned from the corpora which encodes high-order message passing among vertices. In what follows, we first elaborate how to generate two quality corpora for a bipartite network. After that, we introduce a novel embedding method to work directly on the generated node sequences to learn representative node embeddings.

Starting from each target node in a folded bipartite graph, we regulate a random walker to walk over the graph to generate a fixed length node sequence. Mathematically, for each target node u, we will obtain a node sequence sampled from the folded bipartite graphs, denoted as Su={u1,u2,…,ul}, where l is a scalar parameter, indicating the length of the sequence. We will explore the effects of different sizes of l in the experimental discussions. As such, the random walker will respectively generate |U| and |V| node sequences for nodes in U and V. Fig. 4 is an example of generated node sequences. The left part is a folded graph that only contains user nodes, while the right part shows 7 sequences (with length of 5) starting from each node in the folded graph generated by the random walker. All the sampled node sequences will form the corpora used for learning the node embeddings.

Fig. 4. - 
Generated homogenous node sequences.
Fig. 4.
Generated homogenous node sequences.

Show All

Previous sequence-based embedding methods such as DeepWalk [18] and Node2Vec [17] sample node sequences through truncated random walks and feeds them into a Skip-Gram based on Word2Vec [28] model to learn node representations. However, these methods cannot be directly applied in our problem as Word2Vec-based methods only uses a fixed window size to sample a small number of nearby nodes as contexts. Different from Node2Vec and DeepWalk, in this paper, we expand the existing sequence-based approaches with a flexible sequential modelling scheme, which is also able to differentiate the strength of message passing posted by different nodes in a sequence. In other words, our assumption is that, in a node sequence, the starting node exchanges message to every other nodes, and the amount and the strength of the message passing is highly related to the importance of each node in the sequence. Therefore, the embedding method should be able to be aware of the varying dependencies between the staring node and other remaining nodes.

To serve this need, inspired by recent advances of Transformer model [21] and sequential modelling paradigm [29], [30], [31], we borrow the idea of Transformer to model the node sequences generated from each of the folded bipartite graphs. The Transformer model relies heavily on the proposed a self-attention modules to capture complex structures in sentences, and to retrieve relevant words (in the source language) for generating the next word (in the target language). Inspired by Transformer, we seek to build a new sequential embedding model based upon the self-attention approach, though the problem of sequential embedding is quite different from machine translation, and requires specially designed models.

We start with the self-attention module, a.k.a., the scaled dot-product attention, which is defined as:
H∘=Attention(Q,K,V)=softmax(QKTd−−√)V,(7)
View SourceRight-click on figure for MathML and additional features.where H∘∈Rn×d is the latent representation for all nodes after the self-attention. d−−√ is the scaling factor to smooth the row-wise SoftMax output and avoid extremely large values of the inner product, especially when the dimensionality is high. Q,K,V∈Rn×d respectively represent the queries, keys and values obtained using linear projection:
Q=EWQ,K=EWK,V=EWV,(8)
View SourceRight-click on figure for MathML and additional features.and WQ,WK,WV∈Rd×d are corresponding trainable projection weight matrices for queries, keys and values. To be concise, we reformulate the self-attention module in Eq. (7) and Eq. (8) as the following:
H∘=softmax(EWQ⋅(EWK)Td−−√)EWV,(9)
View SourceRight-click on figure for MathML and additional features.and each row h∘i∈H∘ corresponds to the latent representation of the ith node after the self-attention. Intuitively, for the ith node feature, the self-attention encodes its interactions with all other nodes’ features in a sequence into hi by calculating a weighted sum (i.e., bit-wise interactions) of all other nodes’ features. The embedding matrix E∈Rn×d here carry the original embeddings of all nodes in a graph (i.e., ei=hi with hi acquired from Eq. (3)).

The Rationale of Self-Attention. The self-attention described above takes the sequences (e.g., the sequences obtained from Fig. 4) as the input and produce an attentive embedding for each node in a sequence, which is a weighted sum of all node features in a sequence and therefore captures the relevance between two nodes in that sequence. Link this back to the defined intra-class message passing, the strength of higher-order message passed between each pair of same typed nodes in a bipartite graph can be revealed and captured by the learned attentive weights. More importantly, the self-attention mechanism is computationally-efficient compared with existing sequence-based approaches.

Time Complexity of FBNE. Excluding the inter-class message passing operation that is standard in all GraphSAGE-based models, the computational cost of our model is mainly exerted by the intra-class self-attention units. Hence, for each training sample, the overall time complexity of these two components is O(l2d)+O(∏x1Ni), where l,N are user-specified constants denoting the sequence length and neighbourhood sampling size. x denotes the number of GCN layers, in our experiments, we found that only one layer is enough for bipartite graphs. Therefore the additional time complexity is acceptable and FBNE has linear time complexity w.r.t. the scale of the data.

3.3 Joint Multiple Bipartite Graph Embedding Learning
In this section, we jointly train the introduced bipartite embedding method on multiple bipartite graphs simultaneously to learn embeddings of multiple nodes in a more complex network. With out loss of generality, we use the graph described in Fig. 1 as a typical social recommendation example, which contains multiple bipartite graphs that can be easily observed from e-commerce data.

For the user-item bipartite graph, the embeddings of user node i can be computed as the following:
ui=σ(W(hui⊕h∘i)+b),(10)
View SourceRight-click on figure for MathML and additional features.where hui denotes the user latent representation from Eq. (3), and h∘i denotes the user latent representation obtained from Eq. (9), which is the latent user representation learned by modelling the sequences of folded user graph. Eq. (10) first concatenates these two user representations from inter- and intra-class domains, and then embeds the user's information into a projection space.

On the other side of a user-item bipartite graph, the item embedding bi is computed analogously with the same weight matrix W. To learn the weight matrix and embeddings of users/items, we formulate a link reconstruction problem on the original user-item bipartite graph. Specifically, we model the existence of a link eij∈EUB by the following:
p(eij=1)=11+exp(−ui⋅bTj).(11)
View SourceRight-click on figure for MathML and additional features.The loss function can be defined by minimizing the following negative log likelihood of the existence of a link between a user node and an item node.
LUB=−∑eij∈EUBlog(p(eij=1))−∑eik∉EUBlog(1−p(eik=1)).(12)
View SourceRight-click on figure for MathML and additional features.

Intuitively, minimizing this loss function will make two connected vertices in the bipartite graph also close with each other in the embedding space, which preserves the inter- and intra-class similarities of a bipartite graph as desired. Finally, based on the three bipartite graphs we have defined, i.e., user-item, item-attribute and item-category graphs, we embed the four bipartite graphs by minimizing the sum of all objective functions as follows (note that the item embeddings are shared across in different graphs):
O=λLUB+βLBA+γLBC,(13)
View SourceRight-click on figure for MathML and additional features.where λ,β,γ are wights for each bipartite graph, and
LBA=−∑eij∈EBAlog(p(eij=1))−∑eik∉EBAlog(1−p(eik=1))(14)
View SourceRight-click on figure for MathML and additional features.
LBC=−∑eij∈EBClog(p(eij=1))−∑eik∉EBClog(1−p(eik=1)).(15)
View SourceRight-click on figure for MathML and additional features.

The objective Eq. (13) can be optimized by training all types of graphs simultaneously by merging all the edges in the four sets EUB,EUB,EUB together, and then deploy edge sampling, which samples an edge for model updating in each step, with the sampling probability proportional to its weight. However, the graphs are heterogeneous in our model, the weights of the edges between different graphs are not comparable to each other. A more reasonable solution is to alternatively sample from the four sets of edges respectively which is called joint training.

3.4 Inductive Social Embedding
So far, we have learned the embeddings of users, items, attributes and categories. We now introduce how to represent social relationship in the shared latent space in an inductive manner. An embedding method is proposed to incorporates users social relationships into users’ embeddings acquired from item perspective. More importantly, it supports inductive learning that learns from observed samples and generates embeddings for those unseen samples during training. By introducing the inductive learning paradigm, the proposed inductive social embedding method is therefore able to seamlessly solve the general cold-start problem in recommendation systems.

Similar to the introduced GCN embedding method for a bipartite graph in Section 3, we also adopt weight sharing in GCNs and define the embedding computation of a user based on his/her direct neighbour users. The idea of weight sharing in GCNs enables this local operation of convolving neighbours’ representations to represent a target node to be easily shared and applied across all locations in the user to user graph. Same as the modelling process of user-item bipartite graph, we assume there exists a message passing through every edge between users. For each edge in the user-user graph, we model the following message passing from user ui to uj by the following form:
ηji=1cijWuuxuj,(16)
View SourceRight-click on figure for MathML and additional features.where cij is a normalization constant, which we choose to either be |N(ui)| or |N(ui)||N(uj)|−−−−−−−−−−−−√ (symmetric normalization), with N(ui) denoting the set of neighbors of user node i, and N(uj) denoting the set of neighbors of user node j. Wuu is the parameter matrix and xuj is the feature vector of user node j.

After the message passing step, we combine the incoming messages for node ui by using an aggregation function Agg(⋅) that aggregates the neighbors N(ui) connected by edges between user i and the neighbours. Mathematically, we define the aggregation as follows:
hSi=σ(W′⋅Agg({ηji,j∈N(ui)})),(17)
View SourceRight-click on figure for MathML and additional features.where σ(⋅) is an element-wise activation function such as ReLU(⋅)=max(0,⋅). W′ is a learnable parameter matrix. Then, the embeddings of user node i can be represented as the following in the user-user space.
uSi=σ(WShSi).(18)
View SourceRight-click on figure for MathML and additional features.

We model each social relationship between two users in the social graph. Specifically, we model the existence of a link eij∈EUU by the following:
p(eij=1)=11+exp(−uSi⋅(uSj)T).(19)
View SourceRight-click on figure for MathML and additional features.To optimize the user embedding in the user-user space, we use the following graph based loss:
LUU=−∑eij∈EUUlog(p(eij=1))−∑eik∉EUUlog(1−p(eik=1)).(20)
View SourceRight-click on figure for MathML and additional features.With above, we have been able to learn a cold-start users’ embeddings merely from its direct neighbours. That is to say, by only looking up one's neighbour users, we can inductively infer the embedding of a user.

3.5 Social Recommendation With FBNE
Once we have learned the embeddings of users and items, given a user u, we select top-k items with the highest scores that u has not visited before. More specifically, given a user u, we compute its ranking score as in Eq. (21), and select the k ones with the highest ranking scores as recommendations.
S(u)=u⋅bT,(21)
View SourceRight-click on figure for MathML and additional features.where u is the representation of u's preferences, b is the representation of an item, both u and b can be obtained by optimizing the objective in Eq. (13). The learned representations also automatically capture the semantic content information of item b through the item-attribute and item-category graphs, as our FBNE model jointly learns embeddings of multiple networks in the same latent space.

Cold-Start Social Recommendation: As for cold-start users and items, our FBNE model can still learn their representations in the latent space based on the user-user graph, item-attributes and item-category graphs. Thus, both cold-start and normal users and items can be recommended together by the same ranking function, which distinguishes from other existing recommender models that use different functions to compute the scores for cold-start and normal items, separately.

SECTION 4Experimental Settings
We conduct our experiments on two publicly available large-scale and real-life datasets, which are provided by Yelp Challenge2 published in 2016 and MoiveLens.3 Specifically, the Yelp dataset includes information about local business, user information, interactions between user and business (ratings, reviews), as well as friendship network among users. The original dataset contains the information in five states in the U.S, and we processed and extracted six (five individual state and one complete) large-scale heterogeneous social networks. Each network contains four different sub-networks, which are user-user, user-business, business-attribute, and business-category networks. Table 1 shows the detailed statistics of the extracted datasets.

TABLE 1 Datasets Statistics
Table 1- 
Datasets Statistics
4.1 Baseline Methods
We briefly introduce the baseline methods for comparison below.

PMF [32]: Probabilistic Matrix Factorization utilizes user-item rating matrix only and models latent factors of users and items by Gaussian distributions.

SoRec [33]: Social Recommendation performs collaborative factorization on the user-item rating matrix and user-user social relation matrix.

GCMC [22]: This model defines message passing and performs graph convolutions on user-item bipartite graphs for recommendation, and achieved state-of-the-art results.

GraphSAGE [15]: GraphSAGE is a representative variant of GCN-based models for graph representation learning. This versatile method has achieved state-of-the-art results on various graph-based tasks.

GraphRec [3]: GraphRec is built on GraphSAGE, but it simultaneously aggregates both neighbourhood information in a user-item bipartite graph and social information in a user-user social network.

BiNE [34]: BiNE is a state-of-the-art model dedicated for bipartite network embedding.

4.2 Evaluation Metrics
Evaluation of Recommendation Performance. To evaluate the ranking performance, we adopt the widely applied Hits Ratio at Rank k (Hits@k), Mean Reciprocal Rank (MRR) and Normalized Discounted Cumulative Gain at Rank k (NDCG@k) which are commonly used in information retrieval and recommender systems [35], [36], [37], [38]. Specifically, we perform this task on Yelp dataset, and we preprocess this dataset by filtering out nodes whose degree is less than 5. Then, we use 80th percentile as the cut-off point so that the network linkage records before this point are used for training. In the training dataset, we choose the last 10 percent records as the validation data to tune the model parameters, including the dimension of latent feature vectors, margin, learning rate and the number of gradient steps. According to the above dividing strategies, we split the dataset D+ into D+training, D+validate and D+test. We summarise the detailed statistics of this dataset in Table 2. For each linkage information (a triple consists of two vertices connected by a link) i.e., eijr∈D+test:

We randomly choose 1,000 items with which vertex vi has been never connected by link type r to replace vj and form 1,000 negative examples.

We compute a score for eijr as well as the 1000 negative examples by calculating their inner product.

We form a ranked list by ordering these 1001 examples according to their scores to vi. Let rank(eijr) denote the position of eijr in the ranking list.

We form a top-k prediction list by picking the k top ranked examples from the list. If rank(eijr)<=k, we have a hit. Otherwise, we have a miss.

TABLE 2 Recommendation (ranking) Results
Table 2- 
Recommendation (ranking) Results
The computation of Hits@k proceeds as follows. We define hit@k for a single test case as either 1, if the positive example eijr appears in the top-k results, or 0, if otherwise. The overall Hits@k is defined by averaging over all test cases:
Hits@k=#hit@k|D+test|,
View SourceRight-click on figure for MathML and additional features.where #hit@k denotes the number of hits in the test set, and |D+test| is the number of all test cases. A good predictor should achieve higher #hit@k. We also use NDCG@k to further evaluate whether if the model can rank the ground truth as highly as possible:
NDCG@k=∑e∈D+test∑krank=1rele,ranklog2(rank+1)|D+test|.
View SourceRight-click on figure for MathML and additional features.

For the computation of NDCG@k, we set rele,rank=1 if the item ranked at r is the ground truth, otherwise rele,rank=0. For k in both Hits@k and NDCG@k, we adopt the popular setting of 5, 10, 20 for presentation.

Evaluation of Rating Prediction. Because each pair of user-item embeddings can be used to generate a score, we further test our FBNE model in an additional regression task, i.e., rating prediction which is a classic problem in recommendation. In order to evaluate the capability of the proposed model in terms of rating prediction, we evaluate the regression performance with two popular metrics to evaluate the predictive accuracy, namely Mean Absolute Error (MAE) and Root Mean Square Error (RMSE), which are popular among relevant research communities [39], [40]. Smaller values of MAE and RMSE indicate better predictive accuracy. Note that small improvement in RMSE or MAE terms can have a significant impact on the quality of the top-few recommendations [8].

4.3 Parameter Settings
There are several general parameters in the proposed model. We use the grid search algorithm to obtain the optimal model hyper-parameters setup on the validation dataset. For learning rate, we set it to 0.001. The weights {α,β,γ} for different bipartite graphs in Eq. (13) are set to {0.8,0.1,0.1}. We also set default embedding size at 64, batch size at 128.

SECTION 5Experimental Results and Analysis
Following the settings in Section 4, we conduct experiments to showcase the advantage of FBNE in terms of both effectiveness and efficiency. In particular, we aim to answer the following research questions (RQs):

RQ1: How effectively our proposed approach can perform recommendation compared with state-of-the-art methods.

RQ2: How effectively our proposed approach can perform inductive cold-start social recommendation.

RQ3: How the hyper-parameters affect the performance of our approach in different prediction tasks.

RQ4: How our proposed approach benefits from each component of the proposed model structure.

5.1 Recommendation Performance (RQ1)
The results of the recommendation task are reported in Table 2. Note that higher Hits@k and NDCG@k values imply better prediction performance. We have the following observations. First of all, obviously, on both EDH and WI datasets, FBNE significantly and consistently outperforms all existing social recommendation models and network embedding models with K∈{5,10,20}. This validates the effectiveness of our FBNE solution, especially our proposed novel techniques that exploits and integrates the user-user higher-order interaction data and the user-item bipartite network data to overcome the sparsity issue of the user-item interactions when learning user embeddings for recommendation. Another observation is that all graph convolutional network based models (i.e., GraphSAGE, GCMC, GraphRec) outperform the traditional social recommendation models, which shows that graph convolutional operations can learn better representations on graph structured data than matrix factorization based model on the user-item data. Moreover, the network embedding method for bipartite graph (BiNE) did not perform as good as dedicated methods for social recommendation and GCN-based methods. This is because BiNE is exclusively designed for general bipartite network embedding and puts a significant focus on preserving network distribution when learning node representations in a user-item bipartite graph. Nevertheless, BiNE still performs better the traditional matrix factorization model PMF.

5.2 Rating Prediction Performance (RQ1)
Table 3 reveals all models’ performance achieved in the regression task (rating prediction) on MoiveLens and Yelp datasets. For both MAE and RMSE metrics, the lower the better. As we can see from the results, FBNE yields significant improvements on the regression accuracy over all baselines, which shows that FBNE is effective and can be generalized for rating prediction tasks by only modelling the user-item interactions. Furthermore, though showing competitive regression results, the baseline GraphRec falls short in higher-order node dependency modelling. Compared to GraphRec, our proposed FBNE further incorporates higher-order dependencies among users into node representation learning, which boosts the performance of the rating regression task. Apart from that, we notice that GCN-based methods consistently outperform other methods, which shows that graph convolutions are effective for learning graph structured data. Moreover, there is no obvious winner among GCN-based methods, and some of them work well on one dataset but perform poorly on the other dataset due to unique characteristics of the datasets. Last but not least, we cannot see obvious advantages of social recommendation methods as there is no social relations involved in the MoiveLens dataset; however, our proposed FNBE can still achieve the best performance compared against other methods, showing that the higher-order proximity captured from user-user implicit relations are beneficial for user-item rating regression.

TABLE 3 Rating Prediction (Regression) Results
Table 3- 
Rating Prediction (Regression) Results
5.3 Cold-Start Recommendation Performance (RQ2)
Our proposed approach is capable of inductive learning. Inductive learning requires our proposed approach to first learn embeddings for a portion of nodes, and then infer the embeddings for nodes that are not seen during the training phase. Specifically, for a user, we can infer its embedding merely from the embeddings of its direct neighbours; for an item, we can infer its embedding from its directly connected attributes and/or categories. We report the both the top-k recommendation and rating prediction performance of our proposed approach on cold-start users and items.

Table 4 reveals the cold-start recommendation and rating prediction results of our proposed method on the full Yelp dataset. The results are reported by averaging the results of running experiments for 10 times. From the table, we can see that when we continuously adding more training data, the performance of both rating prediction (revealed by MAE, RMSE) and recommendation (Hits@10, NDCG@10) are improving consistently, showing that our proposed FBNE is effective in inductive node representation learning.

TABLE 4 Inductive Prediction Performance for Cold-Start Users and Items on Yelp (all) Dataset
Table 4- 
Inductive Prediction Performance for Cold-Start Users and Items on Yelp (all) Dataset
5.4 Impact of Hyper-Parameters (RQ3)
We investigate how the performance of our proposed approach varies w.r.t. different key hyper-parameter setups, including the latent dimension d, the maximum intra-class node sequence length l, and the number of negative samples m. For each test, based on the standard setting d = 64, l = 5, m = 5, we vary the value of one hyper-parameter while keeping the others unchanged, and record the new recommendation result achieved. To show the performance differences, we demonstrate Hits@10 for recommendation, and RMSE for rating prediction. Fig. 5 lays out the results with different parameter settings.

Fig. 5. - 
Hyper-parameter sensitivity analysis w.r.t $d,l,m$d,l,m.
Fig. 5.
Hyper-parameter sensitivity analysis w.r.t d,l,m.

Show All

Impact of Embedding Dimension d. The value of the latent dimension d is examined in {8, 16, 32, 64, 128, 256, 512}. As an important hyper-parameter for learning embeddings, the latent dimension is apparently associated with the model's expressiveness. In general, FBNE benefits from a relatively larger d for all types of tasks, but the performance improvement tend to become less significant when d reaches a certain scale (64 in our case). It is worth mentioning that with d = 16, FBNE still outperforms nearly all the baselines in the rating regression tasks, which further proves the effectiveness of our proposed model.

Impact of Node Sequence Length l. As one of the main components of FBNE is the intra-class node sequential modelling, we examine how different node sequences length affects the performance. From Fig. 5, we can conclude that FBNE behaves consistently on different datasets when the sequence length l is adjusted in {2,5,10,15,20,25}. When node sequence is short (i.e., l=2), a significant performance decrease is witnessed. While when increasing the length, it becomes better than other methods in both recommendation and rating prediction. However, when continuously increasing it, the prediction results become stable, and there is no further obvious performance gain. More interestingly, we found that Moivelens dataset requires longer node sequences (l=10) to achieve stable rating prediction results, which is because the number of users in moivelens dataset is larger than that in Yelp dataset, therefore it needs longer node sequences to model the dependencies between users.

Impact of Negative Samples m. We study the performance of FBNE w.r.t. the number of negative examples m∈{1,2,5,10,15,20}. As can be concluded from Fig. 5, we observe that when the number of negative examples is larger than 5, the performance becomes very stable. Besides, when dealing with a larger number of negative samples, the training will take longer time. Therefore, for each positive example, we do not need to sample excessive negative examples and just need to sample a few (5 in our case), which ensures the training efficiency of our FBNE model.

5.5 Importance of Key Components (RQ4)
To better understand the performance gain from the major components of our proposed method, we conduct ablation test on different degraded versions of FBNE. Each variant removes one key component from the model, and the corresponding results of three tasks are reported. Table 5 summarizes the prediction outcomes in different tasks. Similar to previous experiments, Hits@10 and MAE are used. In what follows, we introduce the variants and analyze their effect respectively.

TABLE 5 Ablation Test on Model Components
Table 5- 
Ablation Test on Model Components
Remove Centrality-Based Sampling (Remove CS). The centrality-based sampling strategy allows graph convolutions sample neighbours according to an importance measure. After removing it, a subtle performance drop is consistently observed in both recommendation and rating prediction tasks. Although the performance drop is not quite significant, the experiment shows that centrality-based sampling is beneficial.

Remove Inter-Class Message Passing (Remove Intra). As a core contribution of our proposed approach, the Intra-class message passing is able to capture the higher-order dependencies between same type of nodes by using a self-attention sequential modelling paradigm. After removing it, a noticeable performance drop has been observed in all tasks. The results verify that this sequential modelling in same types of nodes play a significant role in improving the expressiveness of learned embeddings.

Remove Business-Category Graph (Remove LBC). We also consider to remove the loss term of business-category graph to investigate whether this additional bipartite graph can have a positive impact on the performance. After removing LBC from Eq. (13), we can see that there is an obvious performance drop in recommendation task and no obvious drop in rating prediction task. The results suggest that modelling additional business-category bipartite network is helpful in leaning embeddings of a business. Moreover, the main reason of increased MAE in rating prediction is that rating prediction task puts more weights on user-business interactions, and the category information probably will not affect users’ opinion on specific businesses. For example, a user likes Japanese food does not mean that she/he will give a high rating for every Japanese restaurant.

Remove Business-Attribute Graph (Remove LBA). Similar to removing LBC, we also test how LBA is related to the model performance. Interestingly, after removing LBA, we observe that there is a subtle performance drop in recommendation task on EDH dataset, while for WI dataset, the performance change is not obvious. Moreover, the MAE score for rating prediction on EDH dataset drops marginally. We analyse the results and found that a reasonable explanation is that, different from category information, attribute information of a business will affect users’ impression significantly. For example, if a restaurant has enough parking spaces, people usually will like it with a high possibility.

SECTION 6Related Work
6.1 Bipartite Network Embedding
Network embedding (NE) is a recent hot topic with the focus of learning low dimensional latent vector representation for vertices in a network (usually for homogenous networks) [15], [17], [18], [19]. As a special and ubiquitous graph data structure, bipartite networks consist of two different sets of vertices and have been mined for many applications such as recommendations [41]. However, general NE models may not suitable for bipartite graphs embedding because they only consider one type of vertices. Although some heterogeneous network embedding methods [19], [42] can be applied to a bipartite network which can be seen as a special type of heterogeneous networks, they are not tailored for learning on bipartite networks. To build a dedicated embedding model for bipartite graphs, latent factor model (LFM), which has been widely investigated in the field of recommender systems and semantic analysis, is the most representative model. And a typical implementation of LFM is based on matrix factorization [43], [44]. Recent advances utilize deep learning methods to learn vertex embeddings on the user-item network for recommendation [45]. In addition to the above mentioned non-deep embedding techniques, GCN based deep learning models have shown their superiority over most of existing methods in terms of downstream tasks, becoming the state-of-the-art methods. More recently, graph convolutions on matrix completion (GCMC [22]) were proposed, which learns users and items embeddings through defining differentiable message passing on user-item bipartite graphs. Nevertheless, these methods only model the explicit relations in the bipartite network, which can be improved by incorporating implicit relations as shown in [9], [34].

6.2 Social Recommendation
With the prevalence of online social platforms, exploiting social relations for boosting recommendations has been attracting significant attention in recent years. The research line develops from the study of social influences [7] and homophily assumption [6]. Based on these studies, an earlier attempt is SoRec [33], which proposed a co-factorization model that uses a shared latent user representation factorized from the user-item rating matrix and social relations. By treating the social neighbors preferences as the auxiliary implicit feedbacks of an active user, TrustSVD is proposed to incorporate the trust influence from social neighbors on top of SVD++ [46]. Similar related work also includes SoDimRec [12] and TrustMF [47]. A comprehensive review of previous work on social recommendation can be found in [1].

Recently, with the fast development of deep neural networks, several deep models have been proposed to enhance social recommender systems, such as DLMF [48] and DeepSoR [2]. DLMF utilizes deep auto-encoder to learn representation for initializing an existing matrix factorization. DeepSoR utilizes deep neural networks to capture non-linear user representations in social relations and integrate them into a probabilistic matrix factorization model for prediction. GraphRec [3] proposed a GCN-like framework that aggregates both user-item interactions and social relationships to learning representations of users and items.

In summary, all these social recommendation based models have shown superior performance with the social network modelling. Nevertheless, current models were based on leveraging social network structure (e.g., social regularization or combining social neighbors’ preferences as auxiliary feedbacks), which is far from sufficient as the available social relationships are usually noisy and sparse. Instead of only considering the social information, our work differs from these works that we model users'latent preferences with additional higher-order implicit message passing among users through a sequential modelling paradigm, which is capable of encoding the preference influences and dependencies between users.

SECTION 7Conclusion
In complementary to the limited available social relations in the context of social recommendation, this paper further explores the implicit higher-order user-user relations through folding a user-item bipartite graph to boost the performance of current social recommendation. Technically, this paper also proposes a novel embedding method FBNE for general bipartite graphs. By jointly modelling several extra related bipartite graphs, this paper also improves the recommendation performance in the cold-start setting. The main findings of this paper suggests that apart from implicit social relations in social recommendation, mining higher-order implicit social relations also yields significant improvements for recommendation. A promising direction of future work is the exploration of the higher-order sequences generation and modelling (e.g., regulating meta-paths to generate node sequences).