Feature selection is one of the most important techniques to deal with the high-dimensional data for a variety of machine learning and data mining tasks, such clustering, classification, and retrieval, etc. Fuzziness is a widespread nature of data in nature human society. However, most existing feature selection methods ignore the existence of fuzziness in the data, resulting in sub-optimal feature subsets. To address the problem, we propose a novel unsupervised feature selection method, called Unsupervised Discriminative Projection for Feature Selection (UDPFS) to select discriminative features by conducting fuzziness learning and sparse learning, simultaneously. Specifically, we use projection matrix transform data as its low-dimensional representation, which are partitioned into clusters by using membership matrix with sparse constraint. In addition, ℓ2,1 -norm regularization is applied to the projection matrix. Then, a discriminative projection matrix with row sparse is obtained by perform fuzziness learning and sparse learning, simultaneously. An effective alternative optimization algorithm is proposed to solve the objective function. Evaluate experimental results on several real-world datasets show the effectiveness and superiority of the proposed unsupervised feature selection method.
SECTION 1Introduction
With the rapid growth of high dimensional data, feature selection plays an increasingly important role in many field, such as text mining [1], image search [2], person re-identification [3], visual classification [4], bioinformatics [5], etc. It is a very important and challenging task that mine valuable information in complex high-dimensional data. Due to there are many redundant features, noise and outliers in complex high-dimensional data [6], [7], high-dimensional data demands more computing costs and storage requirements in the process of data processing [8], [9], [10]. High-dimensional data will adversely affect the performance of subsequent data processing tasks such as clustering, classification due to curse of dimensionality [11], [12], [13]. Feature selection is a process that obtain relevant low-dimensional feature subsets from original high-dimensional feature set and remove redundant feature and outliers to improve performance of subsequent data processing tasks [14]. Feature selection is an effective solution to deal with high-dimensional data and feature selection methods have received more and more attention in recent years.

Over the last decades, a variety of feature selection applications have been proposed, and according to the use of label information in feature selection, which can be divided into three categories, supervised, semi-supervised and unsupervised feature selection. In the supervised feature selection, discriminative features can be obtained according to the feature relevance according to the label information of data, such as Correlation Feature Selection (CFS) [15], Fisher Score (FS) [16], minimal Redundancy Maximal Relevance (mRMR) [17], Robust Feature Selection (RFS) [18], Discriminative Least Squares Regression (DLSR) [19], Correntropy induced Robust Feature Selection (CRFS) [20], Self-weighted Supervised Discriminative Feature Selection (SSD-FS) [21] and Multi-Source Causal Feature Selection (MSCFS) [22]. With the explosion of technology and the development of the Internet, the number of lack labels high-dimensional data has increased dramatically. Since labeling data is time-consuming and laborious, it is a more practical choice to use unlabeled data for feature selection. However, unsupervised feature selection is more challenging and important due to the lack of label information. Various unsupervised methods of feature selection have been proposed, which perform unsupervised feature selection by utilizing global or local structural information of data with no label information, i.e., Laplacian Score (LapScore) [23], Multi-Cluster Feature Selection (MCFS) [24], Unsupervised Discriminative Feature Selection (UDFS) [25], Structured Optimal Graph Feature Selection (SOGFS) [26], Reconstruction-based Embedded Unsupervised Feature Selection (REFS) [8] and Dependence Guided Unsupervised Feature Selection (DGUFS) [27], etc. Semi-supervised mixed use labeled and unlabeled data, such as sSelect [28] and Rescaled Linear Square Regression (RLSR) [29], etc. Because unsupervised learning is more challenging and important for feature selection, we focus on the unsupervised feature selection.

It is known that fuzziness exists widely in nature and human society. In the past few decades, In the past few decades, many fuzzy-based feature selection methods have been proposed by researchers [30], [31], [32], [33], which combine fuzzy learning and feature selection to show the advantage of fuzzy learning in feature selection.

Inspired by them, we proposed a novel unsupervised feature selection methods that use projection matrix transform data as its low-dimensional representation, which are partitioned into clusters by using membership matrix with sparse constraint to obtain pseudo labels to guide learning discriminative features. Inspired by [18], we learning a projection matrix with structural sparse regularization to perform feature selection, i.e., ℓ2,1-norm regularization. All of the above are aimed to learn a discriminative projection to improve the performance of unsupervised feature selection. The major contributions of this work are as follows:

A novel unsupervised feature selection method is proposed, which introduce fuzziness by fully utilizing membership with sparse constraint to feature selection to adaptively learn sparse membership to obtain a discriminative projection matrix in low-dimensional space.

We apply the ℓ2,1-norm regularization to projection matrix to select features across all data instances with structural sparsity. An alternative iterative optimization algorithm is proposed to solve objection function. And an effective algorithm is proposed to solve the ℓ2,1-norm regularization problem with theoretical analysis of convergence.

Evaluate experiments on the various datasets demonstrate the performance of the proposed UDPFS method better than several state-of-the-art unsupervised feature selection methods with no fuzziness.

SECTION 2Related Work
In this section, we first summarize some notations and defined to be used in this paper. Then, we will brief review several classical unsupervised features selection applications. Last, we will introduce some fuzzy clustering techniques.

2.1 Notations and Definitions
We summarize some notations and defined to be used in this paper. Throughout the paper, we use uppercase bold letter such as A denotes matrix, the vectors are denoted as lowercase bold letter, such as v and we denote scalar as light letter b. For matrix A∈Rn×m, aij, ai are denote the (i,j)th element and the transpose of ith column of matrix A, respectively. AT, rank(A), A−1 denote the transpose, rank and inverse of matrix A. The trace of matrix A is defined Tr(A)=∑ni=1aii, where aii is the ith diagonal element of A. 1m denotes m dimensional vector with all elements 1. For a vector v∈Rn, its ℓp-norm is ∥v∥p=(∑ni=1|vi|p)1p, such as the ℓ0-norm of vector v is denoted as ∥v∥0=∑ni=1|ai|0, the ℓ1-norm is ∥v∥1=∑ni=1|vi|, where |⋅| denotes absolution, the ℓ2-norm is ∥v∥2=∑ni=1v2i−−−−−−√.

The Frobenius-norm of matrix A is defined as
∥A∥F=∑i=1n∑j=1ma2ij−−−−−−−−⎷,(1)
View SourceRight-click on figure for MathML and additional features.the ℓ2,0-norm of A is defined as
∥A∥2,0=∑i=1n|∑j=1ma2ij|0,(2)
View Source

the ℓ2,1-norm [34] of A is defined as
∥A∥2,1=∑i=1n∑j=1ma2ij−−−−−⎷=∑i=1n∥ai∥2.(3)
View Source

Let X=[x1,x2,…,xn]∈Rd×n denote data matrix, where d and n are the number of features and samples, respectively. Each column of X represents a sample, and xi∈Rd is the transpose of ith column of data matrix X.

2.2 Unsupervised Feature Selection
In this subsection, we brief review several classical unsupervised feature selection methods, which can be roughly classified into three categories, namely filtering methods, wrapper methods and embedded methods.

The filtering methods, in short, evaluated the discriminability of each feature in data, and then filtering the unimportant features and retain discriminative features, such as LapScore [23] evaluate the discriminability of each feature by its locality preserving power, i.e., Laplacian score. SPEC [35] rank importance of each feature by spectral regression for feature selection.

The wrapper methods, which are designed to be a component of the learner directly, such as UFSACO [36] seeks to find the optimal features by several iterations with no use any learning algorithms. However, wrapper methods must repeatedly train learning algorithm in the process of feature selection, therefore, wrapper methods have higher time complexity than filtering methods and embedded methods.

For embedded methods, which combine the feature selection with learning machine model, and integrated them into a single objection function to optimal. Generally speaking, existing embedded methods to learn intrinsic structure by various methods for selecting discriminative features. In machine learning, sparse learning is widely studied and applied [37], [38], [39], most of embedded feature selection methods select features by sparse learning. MCFS [24], which best preserve the multi-cluster manifold structure of the data for feature selection. UDFS [25] select discriminative features by incorporating discriminative analysis and ℓ2,1-norm minimization into a joint framework. NDFS [40] explicitly impose a nonnegative constraint to the class indicators learned by spectral clustering to learn intrinsic manifold structure for unsupervised feature selection. JELSR [41] jointly performs embedded learning and sparse regression for feature selection. EUFS [42] directly embedded feature selection into a clustering process via sparse learning without the transformation. SOGFS [26] performs local structure learning and feature learning simultaneously for feature selection. However, obviously, above all the unsupervised feature selection methods ignore such a fact that fuzziness widely found in nature and human society. These problem result in the performance of existing methods is limited, and ultimately, it will lead to sub-optimal results.

2.3 Fuzzy K-Means Clustering With Sparse Learning
In the past of years, numerous of fuzzy clustering methods extensions proposed, in which the most popular application is fuzzy K-Means clustering [43], [44].

Suppose that we want to cluster X to c clusters the standard form of the objective function for fuzzy K-Means clustering is as follow:
minmj,yijs.t.∑i=1n∑j=1c∥xi−mj∥22yτij∑j=1cyij=1,0≤yij≤1,(4)
View Sourcewhere c is the number of clusters, yij is the grade of membership of the ith sample in the jth cluster, τ is the level of cluster fuzziness, and the larger τ, the greater cluster fuzziness. mj is the centroid of jth cluster.

In order to deal with some issue, such as the number of clusters and initial cluster centers, etc. Based on problem (4), [45] proposed a novel fuzzy K-Means clustering methods which introduce a regularization term into the FKM objective function (4), which minimize the follow objective function:
minmj,yijs.t.∑i=1n∑j=1c∥xi−mj∥22yij+λ∑i=1n∑j=1cyijlogyij∑j=1cyij=1,0≤yij≤1,(5)
View Sourcewhere the first term is the cost function, and the second is the regularization term, which can minimize the within clusters dispersion and maximize the negative weight entropy to determine the membership value to the association of object.

Based on problem (4), [46] proposed a novel fuzzy K-Means clustering, whose objective function as follow:
minmj,yijs.t.∑i=1n∑j=1c∥xi−mj∥22yij+α∥Y∥2FyTi1=1,0≤yij≤1,(6)
View SourceRight-click on figure for MathML and additional features.where Y is an n-by-c membership matrix, yi is the jth row vector of membership matrix Y, 1 is a vector with all the elements 1. During the clustering, the problem (6) consider sparsity of membership of each sample being assigned to different clusters [46]. α is regularization parameter to control sparseness of matrix Y.

SECTION 3Unsupervised Discriminative Projection Feature Selection Model
In this section, we introduce the proposed method Unsupervised Discriminative Projection Feature Selection (UDPFS). Feature selection aim at select discriminative feature subsets from original feature set. In this paper, feature selection matrix is denoted as W∈Rd×m, where d represents dimension of data matrix, m is the number of select features.

Inspired by problem (6), we embedded FKM with sparse learing into unsupervised feature selection. To obtain discriminative feature subsets, a sparse constraint is imposed on membership matrix in the objective function. As in problem (5), we achieve formula of obtain discriminative features as follow:
minmj,Y,Ws.t.∑i=1n∑j=1c∥WTxi−mj∥22yij+α∥Y∥2FyTi1=1,0≤yij≤1,WTW=I,(7)
View SourceRight-click on figure for MathML and additional features.where mj is jth cluster centroid of projected data, It is natural to impose orthogonal constraint on projection matrix, which is the trivial solution of all zeros and avoids arbitrary scaling [25]. And we add row sparse constraint into membership matrix, which can help us to achieve discriminative features, and mj denotes the centroid of jth cluster in projection space.

To perform feature selection, we apply a row sparse constraint ∥W∥2,0=k to problem (7), the problem becomes
minmj,Y,W∑i=1n∑j=1c∥WTxi−mj∥22yij+α∥Y∥2Fs.t.yTi1=1,0≤yij≤1,∥W∥2,0=k,WTW=I.(8)
View Source

However, solving ℓ2,0-norm constraint on W is nonconvex and non-smooth, that is to say, the problem (8) is a NP-hard problem, which is difficult to solve. Therefore, to solve the problem (8), we relax it as a row sparse regularization into objective function as follow:
minmj,Y,W∑i=1n∑j=1c∥WTxi−mj∥22yij+α∥Y∥2F+γR(W)s.t.yTi1=1,0≤yij≤1,WTW=I.(9)
View SourceRight-click on figure for MathML and additional features.

Inspire by [18], the ℓ2,1-norm regularization can achieve a result of row sparse to as a relax of ℓ2,0-norm constraint, i.e., let R(W)=∥W∥2,1
minmj,Y,W∑i=1n∑j=1c∥WTxi−mj∥22yij+α∥Y∥2F+γ∥W∥2,1s.t.yTi1=1,0≤yij≤1,WTW=I,(10)
View SourceRight-click on figure for MathML and additional features.where first term is loss function and second is regularization term to learn pseudo soft labels of data, which instruct to learn discriminative features in projection space, and third term perform feature selection by calculation ∥wi∥2 and select first k most important features by sorted ∥wi∥2, thus k is the number of features are selected. γ is used of control row sparsity of W, and the larger γ, the more sparse row of W is.

SECTION 4Optimization Algorithm
In this section, due to the characteristics of the constraints is ℓ2,1-norm regularization and multivariate in the objective function, we design an alternative iterative optimization algorithm to solve the proposed model and obtain the optimal solution of the problem (10).

4.1 Fix W and mj update Y
With fixed W and mj, the problem (10) is transformed into
minyi∑i=1n∑j=1c∥WTxi−mj∥22yij+α∥Y∥2Fs.t.yTi1=1,0≤Y≤1,(11)
View SourceRight-click on figure for MathML and additional features.problem (11) equivalent to problem (10)
minyi∑i=1n∑j=1c(∥WTxi−mj∥22yij+αy2ij)s.t.yTi1=1,0≤yij≤1.(12)
View Source

Since the similarity of each sample xi is independent, we can separate problem (12) into n sub-problems as follows:
minyi∑j=1c(dijyij+αy2ij)s.t.yTi1=1,0≤yij≤1,(13)
View Sourcewhere yi is the transpose of ith row of membership matrix Y and dij=∥WTxi−mj∥22 is an element of distance matrix D. After simplified, we can rewrite problem (13) as follows:
minyi∥yi+di2α∥22s.t.yTi1=1,0≤yij≤1,(14)
View Sourcewhere yi is a variable that needs to be optimized. Note that, the problem (14) can be solved by using the optimization method in [47].

Algorithm 1. Algorithm for Solving Problem (19)
Input: The original data matrix X∈Rd×n, with-in class scatter matrix Sw, regularization parameter γ.

Output: The projection matrix W.

Initialization: Initialize W as identity matrix I.

while not converged do

Pt=Sw+γDt.

Wt=[p1,p2,…,pc], where p1,p2,…,pc are the eigenvectors of Pt corresponding to the first c smallest eigenvalues.

update diagonal matrix Dt+1 with ith diagonal element as dii=1(wi)Twi+ε√.

t=t+1.

end while

4.2 Fix Y and W Update mj
With fixed Y and W, the problem (10) becomes as follows:
minmj∑i=1n∑j=1c∥WTxi−mj∥22yij(15)
View SourceRight-click on figure for MathML and additional features.for each mj, problem (15) can be decomposed into c independent sub-problems as follows:
minmj∑i=1n∥WTxi−mj∥22yij(16)
View SourceBecause of the objective function is a convex function, we can utilize Lagrange multiplier method to solve problem (16). And the global optimum of problem (16) can be found by taking the derivatives and setting the derivatives to zeros. Thus, we have
mj=∑ni=1yijWTxi∑ni=1yij.(17)
View Source

4.3 Fix Y and mj Update W
With fixed Y and mj, the problem (10) can be rewritten as
minW∑i=1n∑j=1c∥WTxi−mj∥22yij+γ∥W∥2,1s.t.WTW=I,(18)
View SourceRight-click on figure for MathML and additional features.we substitute problem (17) into problem (18), after being simplified, we have
minWTr(WTSwW)+γ∥W∥2,1s.t.WTW=I,(19)
View SourceRight-click on figure for MathML and additional features.where
Sw=∑i=1n∑j=1c((xi−μj)(xi−μj)T)yij.(20)
View Source

Let μj=∑ni=1yijxi∑ni=1yij denote the mean vector of the jth cluster in the original data, and Sw called with-in class scatter matrix, which characterizes the degree of discretization of each cluster of data. The smaller Sw, the higher degree of aggregation for each cluster of data. To the convenience of computation, then we will export the matrix operation form of Sw later.

Algorithm 2. Algorithm for Solving Problem (10)
Input:The data matrix X∈Rd×n, the projection dimension c, regularization parameter α, γ.

Output:The projection matrix W.

Initialization: Initialize W at identity matrix I.

while not converged do

Fix W and mj, update yij by solving problem (11).

Fix W and yij, update mj by problem (17).

Calculate Sw by problem (28).

Fix yij and mj, update W by Algorithm 1.

end while

Sort d features of X according to calculate all ∥wi∥2(i=1,2,…,d) in descending order, and select top k ranked features.

Replace ∥W∥2,1 with ∑ni=1∥wi∥2, we can reweite problem (19) as
minWs.t.Tr(WTSwW)+∑i=1n∥wi∥2WTW=I.(21)
View SourceRight-click on figure for MathML and additional features.

However, ∥wi∥2 can be zero in practice. It will result in problem (21) non-differentiable consequences. To avoid that, we utilize (wi)Twi−−−−−−−√ instead of ∥wi∥2, and regularize (wi)Twi−−−−−−−√ as (wi)Twi+ε−−−−−−−−−−√, where ε is an any small constant. Thus, we have
minWs.t.Tr(WTSwW)+γ∑i=1n(wi)Twi+ε−−−−−−−−−−√WTW=I.(22)
View Source

Obviously, which is equal to problem (19) when ε infinite approximation to 0. The Lagrangian function of problem (22) as follow:
L(W,Q)=Tr(WTSwW)+γ∑i=1n(wi)Twi+ε−−−−−−−−−−√+Tr(Q(WTW−I)),(23)
View Sourcewhere P is Largangian multiplier. Taking the derivative of L(W,Q) w. r. t W and seting it to zero, we have
∂L(W,Q)∂W=2SwW+2γDW+2WQ=0,(24)
View Sourcewhere D∈Rd×d is a diagonal matrix which ith diagonal element is
dii=1(wi)Twi+ε−−−−−−−−−−√.(25)
View Source

Obviously, D is unknow and it depend on W. With fixed D, problem (23) can be regard as the derivative of problem (26)
minWs.t.Tr(WTSwW)+Tr(WTγDW)WTW=I.(26)
View SourceRight-click on figure for MathML and additional features.We can get W by solving problem (26), and it is easier to solve than problem (19). Therefore, we optimize problem (26) and the detail of algorithm are detailed in Algorithm 1.

In summary, we obtain a effective algorithm to solve problem (10), which alternative iterating three variables Y, mj and W, which details are shown in Algorithm 2.

In this paper, our optimization goals is to obtain matrix W, in order to obtain discriminating features. And better Y and better W can promote each other to get optimal Y and W in each iteration of Algorithm 2. In the process of alternating optimization, the optimized variables can promote each other to obtain better variables in the next iteration. Therefore, the optimal solution can be guaranteed when the algorithm converges.

SECTION 5Analysis and Discussion
5.1 The Matrix Form of Sw
It is a waste of time that obtain with-in class scatter matrix Sw by problem (20). Therefore, we give the matrix operation form of Sw in this part. To obtain it, we first give the following lemma:

Lemma 1.
for a diagonal matrix
P=⎡⎣⎢⎢⎢⎢p1p2…pn⎤⎦⎥⎥⎥⎥,
View Sourcewhere ∑ni=1pi=1, that
∑i=1npi(xi−x¯¯¯)(xi−x¯¯¯)T=X(I−P11T)P(X(I−P11T))T=X(I−P11T)P(I−P11T)TXT=X(I−P11T)P1/2(X(I−P11T)P1/2)T=X(P−P11TP)XT.(27)
View SourceRight-click on figure for MathML and additional features.For with-in class scatter matrix Sw in problem (20), we let
μj=1∑ni=1yij∑i=1nyijxi=∑i=1nyij∑ni=1yijxi=∑i=1npijxi.
View SourceTherefore, we have
Sw=∑i=1n∑j=1c((xi−μj)(xi−μj))Tyij=∑j=1c(∑i=1nyij)∑i=1n1∑ni=1yij(xi−μj)(xi−μj)T=∑j=1c(∑i=1nyij)∑i=1npij(xi−μj)(xi−μj)T=∑j=1c(∑i=1nyij)X(Pj−Pj11TPj)XT=X(∑j=1c(∑i=1nyij)(Pj−Pj11TPj))XT=X(B−YDYT)XT,(28)
View Sourcewhere B and D are diagnoal matrix, diagnoal elements are ∑ni=1yij and 1/∑ni=1yij, respectively. In this paper, we calculating Sw by problem (28) instead of problem (20) to reduce the run time.

5.2 Convergence Analysis of Algorithm 1
Next, we will prove convergence of Algorithm 1. In Algorithm 1, the value of objective function (19) monotonically decreasing. To prove it, we need the following lemma:

Lemma 2.
For any two non-zero constants p and q, the following inequalty holds [18].
p–√−p2q√<q√−q2q√.(29)
View SourceThe detail and proof of lemma 2 has been given in [18].

Next, we show the convergence of Algorithm 1 by the following theorem:

Theorem 1.
In each iteration, Algorithm 1 will monotonically decrease the objective function value of problem (19), until convergence.

Proof.
In Algorithm 1, according to the definition of W, and suppose the update Wt is Wt+1, we can see the following inequality holds:
⩽Tr(WTtSwWt)+γTr(WTtDt+1Wt)Tr(WTt+1SwWt+1)+γTr(WTt+1Dt+1Wt+1).(30)
View SourceRight-click on figure for MathML and additional features.

We add γ∑ni=1εwTiwi+ε√ on both sides of the inequality (30), and instead of definition of D in problem (25), then inequality (30) can be rewritten as follow:
⩽Tr(WTtSwWTt)+γ∑i⎛⎝⎜wTitwit+ε2wTitwit+ε−−−−−−−−√⎞⎠⎟Tr(WTt+1SwWTt+1)+γ∑i⎛⎝⎜wTit+1wit+1+ε2wTit+1wit+1+ε−−−−−−−−−−−−√⎞⎠⎟.(31)
View Source

By the lemma 2, we have
⩽γ∑i(wTitwit+ε−−−−−−−−√)−γ∑i⎛⎝⎜wTitwit+ε2wTitwit+ε−−−−−−−−√⎞⎠⎟γ∑i(wTit+1wit+1+ε−−−−−−−−−−−−√)−γ∑i⎛⎝⎜wTit+1wit+1+ε2wTit+1wit+1+ε−−−−−−−−−−−−√⎞⎠⎟,(32)
View Sourcethen, we add inequatily (30) and inequatily (32), we have
⩽Tr(WTtSwWTt)+γ∑i(wTitwit+ε−−−−−−−−√)Tr(WTt+1SwWTt+1)+γ∑i(wTit+1wit+1+ε−−−−−−−−−−−−√),(33)
View SourceRight-click on figure for MathML and additional features.this completes the proof of Theorem 1. The value of objective function of problem (19) will monotonically decrease in each iteration.

SECTION 6Experimental Results and Analysis
In this section, we conduct experiments to evaluate and demonstrate the performance of the proposed methods UDPFS on stand public datasets. First, to validate the important of fuzzy learning, we implement feature selection on a synthetic dataset with noise. Second, to validate the effectiveness of the proposed method UDPFS, we apply our UDPFS method to find interest pixels points on Fashion MNIST dataset, and compare it with several other popular unsupervised feature selection methods. Third, to validate the superiority of the proposed method, we evaluate the performance of the proposed method in terms of clustering, and compare it with the state-of-the-art unsupervised feature selection methods. Last, we study the parameter sensitivity and convergence.

6.1 Toy Experiments
In this subsection, we adopt a synthetic dataset to measure the feature selection ability of the proposed method UDPFS. The synthetic dataset is a randomly generated three-clusters data. In this section, there are five dimension, among which the data in the first dimensions are distributed in a three-clusters shape while the data in the other three dimensions are noises.

Since only the first two dimensions contains discriminative information, it would be important if the UDPFS method can find the first two dimensional features. We compare our method UDPFS with Max Variance (MaxVar) method. Moreover, in order to show the important of fuzziness in feature selection, we set α as 0 make membership matrix Y become hard label matrix (only contain 0 and 1). From Fig. 1 we can see that the feature selection results of there three methods. Obviously, MaxVar and UDPFS (α→0) have no enough power to find the best two dimension features. In addition, the proposed method UDPFS can successfully find the best two dimension features. Because fuzziness is taken into account, compared with the MaxVar method and UDPFS (α→0) without fuzzy learning, discriminative features can be selected by using fuzzy learning in feature selection, and if fuzziness is ignored, it will lead to a suboptimal solution as shown in Fig. 1c, so fuzzy learning in feature selection is important.

Fig. 1. - 
Experimental results on three-clusters dataset. (a) the first two dimension features of original dataset. (b) the feature selection result of maximize variance method. (c) the feature selection result of the proposed UDPFS method with no fuzzy learning. (d) the feature selection result of the proposed UDPFS method with fuzzy learning.
Fig. 1.
Experimental results on three-clusters dataset. (a) the first two dimension features of original dataset. (b) the feature selection result of maximize variance method. (c) the feature selection result of the proposed UDPFS method with no fuzzy learning. (d) the feature selection result of the proposed UDPFS method with fuzzy learning.

Show All


Fig. 2.
Features selected by the UDPFS method on the Fashion-MNIST sub-dataset (From 50 to 600 features, and 50 pixels are added at each step).

Show All


Fig. 3.
Features selected by the different methods on the Fashion-MNIST sub-dataset (From 50 to 600 features, and 50 pixels are added at each step). From first to five row are the results of baseline, Laplacian Score, UDFS, SOGFS and UDPFS, respectively.

Show All

6.2 Qualitative Experiments
In this experiments, in order to qualitatively evaluate the effectiveness of our method UDPFS, we conduct experiments of selecting interest features (pixels) over Fashion-MNIST dataset [48], which is a dataset of containing of 28×28 grayscale fashion images 60, 000 train images and 10, 000 test images from 10 categories. For convenience, in our experiments, we randomly select 2, 000 images, where 200 images per category, from test set images. Our two qualitative experiments are as follows:

First, to qualitatively evaluate the feasibility of the proposed method, we rank images features (pixels) according to the importance by calculating all ∥wi∥2(i=1,2,…,d) in descending order for fashion products of different categories, then selecting top k features from 50 to 600, and 50 pixels are added at each step. Fig. 2 shows the results of selecting interest features (pixels) by using the proposed method UDPFS for different categories. From Fig. 2 we have the following observations: (1): The proposed method can select discriminative features (pixels) at first. (2): With the increase of pixels, more and more discriminative features are selected. (3): When the number of pixels increases to 600, all relevant features are selected for different five categories. Therefore, UDPFS method can obtain discriminative features whether the number of pixels is large or small.

Second, to qualitatively evaluate the effectiveness of the proposed UDPFS method, we compared it with several popular unsupervised feature selection method include Laplacian Score, UDFS, SOGFS, and we adopt randomly select features as baseline. For Laplacian Score, UDFS and SOGFS, we set the size of neighborhood is 5, and all the regularization parameters are set as 100 for all method to make experiments fair enough. The results of qualitatively comparision experiments is demonstrated in the Fig. 3. From the Fig. 3, we can see that the performance of UDPFS obviously better than other methods. When the number of pixels is increased about 500, almost of the discriminative features are selected by UDPFS, while other methods cannot achieve ideal results like UDPFS.

Through the observations of above all the experiments, we can qualitatively draw the following conclusions: Our method is effective for the performance of feature selection results of different categories of objects of Fashion-MNIST sub-dataset, that is, the ability to select the discriminative features, no matter how many features are selected.

This is because we learn a discriminative projection to project original data to a low-dimensional representation with fuzziness.

Next we use more accurate quantitative experiments to verify the superiority of our approach.

6.3 Quantitative Experiments
6.3.1 Datasets
In our experiments, we conduct experiments on 11 stand public datasets, include two object datasets COIL20 [49] and COIL100 [50], one handwritten digit images dataset BinAlpha [51], one sound dataset ISOLET [52], two face datasets ORL [53], Yale [54], three bioinformatics datasets GLIOMA [55], Prostate-GE [56] and SRBCT [57]. The detail of these datasets are summarized in Table 1.

TABLE 1 Datasets Description

6.3.2 Comparision Methods
To validate the superiority of the proposed method, we compared it with several state-of-the-art methods of unsupervised feature selection, the brief introductions of these comparision methods are as follows:

AllFea: All Features which adopts all original features as baseline method for clustering in experiments.

MaxVar: Maximum Variances which select the features according to the maximum variances. The variance of feature is larger, the method thinks the feature is more important, which is sensitive to the variance of features.

LapScore [23]: Laplacian Score which select the features according to the power of locality preserving, or, Laplacian Score. LapScore seeks those features that respect local geometric structure.

MCFS [24]: Multi-Cluster Feature Selection which select the features according to the power of the multi-cluster structure of the data can be best preserved by using spectral regression with ℓ1-norm regularization.

UDFS [25]: Unsupervised Discriminative Feature Selection which is able to select the discriminative features by joint discriminative analysis and ℓ2,1-norm minimization.

NDFS [40]: Nonnegative Discriminative Feature Selection which is able to select the discriminative features by joint nonnegative spectral analysis and ℓ2,1-norm minimization.

SOGFS [26]: Structured Optimal Graph Feature Selection which performs feature selection and local structure learning simultaneously, the similarity matrix thus can be determined adaptively.

DGUFS [27]: Dependence Guided Unsupervised Feature Selection which enhances the interdependence among original data, cluster labels, and selected features.

6.3.3 Parameters Setting
To ensure the experiments fair enough, we adopt same strategy to set parameters for all unsupervised feature selection methods. For LapScore, MCFS, UDFS, NDFS, EUFS, SOGFS, we set the number of neighborhood as 5 for all datasets. And we tune all parameters by ”grid search” strategy from {10−9,10−6,10−3,100,103,106,109}, we set α from 10−1 to 101 with 0.5 as interval . For each dataset, how to determine the optimal number of selected features still is a open problem, thus we set the number of selected features as {10,20,30,40,50,60,70,80,90,100}, then perform K-Means clustering for selected features to evaluate the performance of those unsupervised feature selection methods. Best results of K-Means clustering with the optimal parameters. Because the results of K-Means clustering depends on initialization, we report the average results of repeating 20 times kmeans clustering for all methods.

6.3.4 Evaluation Metrics
We utilize two widely used evaluation metrics clustering accuracy (ACC) and normalized mutual information (NMI) to evaluate the performance of all of the above unsupervised feature selection methods. Clustering accuracy (ACC) [25] is defined as
ACC=∑ni=1δ(yi,map(y^i))n,(34)
View Sourcewhere yi is ground true label, and y^i is clustering label, δ(yi,map(y^i))=1 if yi=map(y^i); δ(yi,map(y^i))=0, otherwise, and map(y^i)) denotes the best mapping function which us the Kuhn-Munkres algorithm [58] to permutes cluster label to match the ground true label. The normalized mutual information (NMI) [59] is defined as
NMI=MI(yi,y^i)max{Ω(yi),Ω(y^i)},(35)
View SourceRight-click on figure for MathML and additional features.

where MI(yi,y^i) denotes mutual information metric, and Ω(yi) and Ω(y^i) denote entropies of yi and y^i, respectively. The larger ACC and NMI are, the better performance is.

6.3.5 Clustering Results Analysis With Selected Features
Figs. 4 and 5 demonstrate the best average clustering accuracy (ACC) and best average normalized mutual information (NMI) over nine different datasets, respectively. From the Figs. 4 and 5, we can draw the following conclusions:

Feature selection is effective. Comparing baseline with feature selection methods, feature selection not only can improve the ACC and NMI for almost all the datasets, but reduce the computation cost. This is because feature selection remove the redundancy and noisy features while remain the discriminative features. In particular, the UDPFS method, the best ACC and NMI accuracy in each dimension on many datasets is about 5 to 10 percent higher than the baseline. Because we introduce fuzziness to the feature selection to learn a discriminative projection to select discriminative features.


Fig. 4.
Clustering results (ACC) of different unsupervised feature selection algorithms.

Show All

Fig. 5. - 
Clustering results ($NMI$NMI) of different unsupervised feature selection algorithms.
Fig. 5.
Clustering results (NMI) of different unsupervised feature selection algorithms.

Show All

Generally speaking, with the number of selected feature increased, the performance of feature selection methods demonstrates the trend with first increase then decrease. Because real-world datasets always contains many redundancy features and few discriminative features, with the number of selected features increase, many redundancy features are selected to make the performance of feature selection methods is decreased, include ACC and NMI.

Generally speaking, the performance of the proposed method UDPFS exceed other methods on both of ACC and NMI. In particular, UDPFS has 2 to 10 percent improvement compared to the second best method MCFS on object and handwritten digit datasets (i.e., COIL20, COIL100 and MNIST). And more than 5 percent improvement on voice and face datasets (i.e., ISOLET, ORL, Yale) compared to UDFS, the second best method on voice and face datasets. Bioinformatics datasets are a special type of datasets with small samples and high dimensional features. For bioinforamtics datasets, UDPFS has more than 5-10 percent improvement compared to the second best method UDFS.

The proposed UDPFS utilize the soft clustering with fuzziness to adaptively learn a discriminative projection, which project original data to low-dimensional space to obtain low-dimensional representation of data, this is a better method than hard clustering. Therefore, the proposed UDPFS method can obtain better results.

6.4 Parameter Sensitivity and Convergence Study
6.4.1 Parameter Sensitivity Study
We investigate the impact of parameters on UDPFS. The projection dimension m has a slightly impact for results, and we set m as d3 to 2d3 in the experiments. Therefore, er major focus on the impact of parameters α and γ. The parameters α and γ are used for control the sparsity of membership matrix Y and row sparsity of projection matrix W, respectively. As the value of regularization parameter α and γ and number of selected features k is tuned, the clustering accuracy is demonstrated in Figs. 7 and 6, respectively. For limit the space we only show the results on four datasets (i.e., COIL100, ORL, GLIOMA, Prostate-GE). The results showcase that the proposed method UDPFS is not sensitivity to parameters α and γ with wide range, that is to say, our method can be used in practice.


Fig. 6.
Clustering accuracy with different γ and h (α=5.1) on COIL100, ORL, GLIOMA and Prostate-GE datasets.

Show All

Fig. 7. - 
Clustering accuracy with different $\alpha$α and $h$h ($\gamma = 1$γ=1) on COIL100, ORL, GLIOMA and Prostate-GE datasets.
Fig. 7.
Clustering accuracy with different α and h (γ=1) on COIL100, ORL, GLIOMA and Prostate-GE datasets.

Show All

6.4.2 Convergence Study
To solve the objective function, we proposed Algorithm 1 to iteratively solve it. We have already proven the convergence in Section 5.2. In this section, we study the speed of its convergence experimentally. The convergence curves of the objective value are demostrated in Fig. 8. For simplicity, we only show results on four datasets (i.e., COIL100, ORL, GLIOMA, Prostate-GE). From the Fig. 8, we can see that the speed of convergence of Algorithm 1 is very fast, and within 10 iterations. The fast convergence of Algorithm 1 ensures the speed and efficiency of the proposed whole algorithm UDPFS.


Fig. 8.
Convergence curve of UDPFS on COIL100, ORL, GLIOMA and Prostate-GE datasets.

Show All

SECTION 7Conclusion
In this paper, we proposed a novel method for unsupervised feature selection from a new perspective by sparse learning and introducing fuzziness into sub-space learning to learn a discriminative projection for feature selection. Here, original data is transformed to low-dimensional space to obtain its low-dimensional representation, and learning sparse membership adaptively to assign soft cluster pseudo indicator for each low-dimensional representation to learn a discriminative projection. An effective alternative iterative optimization algorithm is proposed to address the objective function. We also analyze the convergence of the proposed algorithm theoretically. Various evaluate experimental results on different real-world datasets demonstrate the effectiveness and superiority of the proposed unsupervised feature selection method.