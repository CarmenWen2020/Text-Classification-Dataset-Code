The imbalanced data classification problem widely exists in many real-world applications. Data resampling is a promising technique to deal with imbalanced data through either oversampling or undersampling. However, the traditional data resampling approaches simply take into account the local neighbor information to generate new instances in linear ways, leading to the generation of incorrect and unnecessary instances. In this study, we propose a new data resampling technique, namely, Gaussian Distribution based Oversampling (GDO), to handle the imbalanced data for classification. In GDO, anchor instances are selected from the minority class instances in a probabilistic way by taking into account the density and distance information carried by the minority instances. Then new minority instances are generated following a Gaussian distribution model. The proposed method is validated in experimental study by comparing with seven imbalanced learning approaches on 40 data sets from the KEEL repository and 10 large data sets from the UCI repository. Experimental results show that our method outperforms the other compared methods in terms of AUC, G-mean and memory usage with an increase in running time. We also apply GDO to deal with two real imbalanced data classification problems: Internet video traffic identification and metastasis detection of esophageal cancer. The experimental results once again validate the effectiveness of our approach.
SECTION 1Introduction
Imbalanced data classification has received extensive attention in recent years. Generally speaking, in an imbalanced classification problem, the instances of one class significantly outnumber the instances of the other class(es), leading to the imbalanced data distribution. Binary classification is usually the default case since a multi-class data set can be converted into binary data sets using techniques such as one-vs-all (OVA) and one-vs-one (OVO). In imbalanced binary classification, the class with the majority of instances is called the majority class and the other class is called the minority class. In real-word applications, imbalanced data classification problems are ubiquitous, such as disease detection [1], fraud detection [2], text classification [3], network intrusion detection [4] and radar image monitoring [5]. In such real-world tasks, we usually focus on identifying the minority class because of its high misclassification cost. For example, the loss caused by a severe misdiagnosis is immeasurable. Therefore, it is particularly important to improve the accuracy of identifying the minority class in practical classification applications.

The traditional machine learning based classification methods suffer from drawbacks in dealing with imbalanced data, since they all assume balanced data distribution and aim at achieving the highest overall accuracy. Hence, the minority class instances can be easily misclassified to the majority class or ignored, especially for highly imbalanced cases, resulting in the deterioration of classifier performance. Therefore, the standard classifiers such as artificial neural networks, support vector machine, and decision trees, usually do not perform well directly on imbalanced data. Recently, a large number of methodologies have been developed to address the imbalanced learning problem, which can be divided into the following two categories: (1) Data-level approaches [6], [7] attempt to balance the data distribution by oversampling the minority class instances or undersampling the majority class instances. Hence, this type of approaches is commonly combined with other learning strategies such as ensemble learning to solve the imbalanced classification problems. (2) Algorithmic-level approaches try to modify the standard classifiers to effectively identify the minority class instances. In particular, cost-sensitive learning schemes [8], [9] have been proposed to guide model training, in which a cost matrix is introduced with more weight assigned to the misclassified instances and the model is trained to minimize the overall cost.

Data-level approaches are convenient to use due to its independence from classifiers, which have attracted significant research interest. Synthetic Minority Oversampling TEchnique (SMOTE), the most popular algorithm at data level, synthesizes new instances along the lines from a minority class instance to its neighbors [10]. Although SMOTE has been demonstrated to be effective, it still has drawbacks because of blind oversampling. Therefore, numerous improved variants of SMOTE have been proposed, such as SMOTE-ENN [11], SMOTE-TomekLinks [11], borderline-SMOTE [12], ADASYN [13] and safe-level-SMOTE [14]. These improved versions are able to alleviate the problems in SMOTE to some extent, but some essential drawbacks still remain. First, these methods generate new minority instances along the line between two minority instances. Such a linear oversampling strategy is not able to produce a new balanced data set that fits the distribution of the original data set. Second, these methods generate new instances without considering the differences among the existing minority instances. In fact, the position and distance of the existing minority instances can significantly impact the quality of the generated instances. As illustrated in Fig. 1a, the circles represent the majority class instances and the squares denote the minority class instances. A and B are two minority instances. When generating new instances, it is reasonable to place the new instances closer to A instead of B because A is a ‘safe’ instance far from the majority instances while B is an ‘edge’ instance near the majority class area. This can be achieved by assigning different weights to A and B to distinguish their importance in generating new minority instances. Fig. 1b shows the drawback of the linear oversampling strategy. In this case, new instances are generated linearly along the lines from a minority instance to its neighbors, which may result in the generation of incorrect and unnecessary instances such as the new instance E in the figure. Such improper instances will decrease the performance of underlying learning models.

Fig. 1. - 
Example of imbalanced data set distribution.
Fig. 1.
Example of imbalanced data set distribution.

Show All

To address these problems, we make the following contributions to imbalanced data classification in this paper.

We propose a new oversampling method, namely Gaussian Distribution based Oversampling (GDO). Specifically, each minority class instance is weighted by considering the density and distance factors. The original minority class instances are selected probabilitically based on their weights. New instances are generated with Gaussian distribution around the selected minority class instances.

We evaluate GDO on 40 KEEL data sets and 10 large data sets by comparing with seven imbalanced methods, including five data-level methods, one cost-sensitive method, and one ensemble methods. The experimental results show that our proposal achieves promising performance regarding AUC, G-mean, and memory usage. The statistical hypothesis test results illustrate that our method is significantly superior to the compared methods.

We apply the proposed method to two real imbalanced problems: Internet video traffic identification and metastasis detection of esophageal cancer. The experimental results further validate the effectiveness of our method.

The remainder of the paper is organized as follows. Section 2 reviews the related work. Section 3 introduces the details of our proposed method. The experimental results are presented and analyzed in Section 4. Section 5 concludes this paper and gives the future work.

SECTION 2Related Work
In recent years, a large number of techniques have been proposed to deal with the imbalanced learning problem [15]. This section reviews the state-of-the-art techniques on imbalanced data classification in three categories: data-level, cost-sensitive and ensemble learning approaches.

2.1 Data Level Approaches
Data-level approaches use resampling to balance the skewed data distribution. Hence, they are also called resampling techniques, which can be categorized into two groups: oversampling [16], [17] and undersampling [18], [19]. Oversampling, the most popular methods of dealing with imbalanced data, reduces the imbalance degree of the data sets by increasing the number of minority class instances. Undersampling balances the number of instances in each class by reducing the majority class instances. SMOTE is the most popular oversampling method in which new minority class instances are generated along the lines of a minority class instance to its neighbors [10]. However, SMOTE synthesizes the same number of instances for each minority class instance without considering the distribution characteristics of its neighboring instances. To this end, numerous improved variants of SMOTE have been proposed. He et al. proposed ADASYN in which the weight of each minority class instance is calculated based on the number of majority class instances in its k-nearest neighbor and different number of instances are generated for different minority class instances [13]. Han et al. proposed Borderline-SMOTE which resamples minority class instances on the boundary [12]. Ramentol et al. introduced SMOTE-RSB* where instances synthesized by SMOTE were cleaned through editing techniques based on the Rough Set Theory (RST) and the lower approximation of a subset [20]. Majority Weighted Minority Oversampling TEchnique (MWMOTE) was proposed by Barua et al. to handle the imbalanced data [21]. In MWMOTE, minority class instances are first weighted by their euclidean distances to the nearest majority class instances. Then the weighted informative minority class instances are resampled by a clustering approach. Lin et al. proposed MOKAS that was used to model complex data distribution of the minority class while generating synthetic instances using a kernel based adaptive subspace self-organizing map [22]. Chan et al. created data subsets with the appropriate class distribution from the majority class instances and combined each data subset with minority class instances to train the learning model [3]. A new undersampling approach was put forward in 2013 based on ant colony optimization and was successfully applied to the classification of DNA microarray data [23]. Liu et al. proposed two algorithms, EasyEnsemble and BalanceCascade, to deal with imbalanced learning problems [24]. In EasyEnsemble, several subsets are sampled in the majority class to train learners and the outputs of those learners are combined. In BalanceCascade, learners are trained sequentially, and the majority class instances that are correctly classified are removed in each step. Liu et al. proposed a method in which both majority classes and minority classes are considered to learn feature embeddings and utilizes appropriate loss functions to make feature embedding as discriminative as possible. An innovative approach was presented by Koknar et al. to augment the minority class by adding synthetic points in distance spaces to improve the classification rate of the rare events [25]. Cao et al. proposed a novel Integrated Oversampling (INOS) method that can handle highly imbalanced time-series classification [26].

2.2 Cost-Sensitive Approaches
Cost-sensitive approaches [27], [28] are also effective methods to deal with imbalanced data. Cost-sensitive learning is based on the fact that correct identification of the minority class instances is more valuable than that of the majority class instances. Thus the costs of different class instances that are misclassified are different, aiming to improve the accuracy of correctly classifying the minority class instances. In cost-sensitive learning, a cost matrix is established in which the elements indicate the penalty strength for instances that are misclassified. The global goal is to generate a classification model with the minimum cost. Zadrozny et al. proposed a method based on cost-proportionate weighting of the training instances to rebalance data [29]. Zhou et al. put forward a cost-sensitive neural network model to deal with the classification problems of imbalanced data [30]. Correspondingly, C4.5 and SVM have been improved based on cost-sensitive learning in [31] and [32], respectively. A new adaptive method was proposed by Qiu et al. based on differential evolution for class-imbalanced cost-sensitive learning [33]. Liu et al. proposed a cost-sensitive sparse representation based classification method for imbalanced problems using probabilistic modeling [34]. An example-dependent cost-sensitive decision tree algorithm was presented by Bahnsen et al. by incorporating different example-dependent costs into a new cost-based impurity measure and a new cost-based pruning criteria [35].

2.3 Ensemble Learning Approaches
Ensemble learning can achieve great performance on generalization by resembling multiple base classifiers, and has become a promising solution to imbalanced classification problems. Therefore, hybrid models that combine data-level approaches with ensemble learning have been constructed for handling imbalanced learning. Chawla et al. presented the SMOTEBoost algorithm by combining SMOTE and the boost learning model [36]. Seiffert et al. proposed a simpler and more effective algorithm named RUSBoost by improving SMOTEBoost [37]. Lim et al. put forward a novel ensemble method that combines a novel cluster-based synthetic data generating method with an evolutionary algorithm [38]. Liu et al. generated new instances using a fuzzy-based oversampling method and built classification models for each rebalanced data set [39]. A majority voting scheme was introduced to output the final results. Dez-Pastor et al. summarized various ensemble-based methods and presented the experimental study in [40]. A resampling ensemble algorithm was proposed by Qian et al. where the minority class instances are oversampled and the majority class instances are undersampled to construct multiple machine learning models [41]. Fernandes et al. presented a novel adaptive approach named E-MOSAIC in which samples are selected from the training data set and the best combinations of instances are used to construct ensemble models for imbalanced classification [42].

SECTION 3Gaussian Distribution Based Oersampling
The key idea of GDO is twofold. The first is to select the minority class instances for oversampling. The selected instances are called anchor instances. The selecting process is probabilistic based on the importance of the minority class instances. The second is to generate new minority class instances around the selected anchor instances with Gaussian distributions instead of in a linear way as SMOTE. Gaussian distribution, a universal distribution model, can be used to model the distributions of data in the real world. Furthermore, Gaussian distribution has simple parameters, which makes it easier for computation than other complex multi-parameter distributions. Therefore, we use Gaussian distribution rather than other types of distributions to generate data. GDO consists of the following three major components: (1) a weighting strategy to assign weight to each minority class instance based on its importance; (2) a probabilistic method for selecting anchor instances; (3) an instance generating method based on Gaussian distribution.

3.1 Minority Instance Weighting
Let T be the given training set composed of Tmin and Tmaj, where Tmin={X1,X2,…,Xn} is the set of instances belonging to the minority class and Tmaj={Y1,Y2,…,Ym} is the set of instances belonging to the majority class. Let Ni=Nmini⋃Nmaji be the set of K-nearest neighbors of Xi, where Nmini and Nmaji are the sets of minority and majority class instances in Ni, respectively.

Since the information carried by different minority instances can be quite different, these differences should be taken into account for oversampling. In our method, we distinguish the differences among different minority instances using two factors: density factor and distance factor. For each Xi∈Tmin, its density factor, denoted by C(Xi), is defined as follows:
C(Xi)=|Nmaji|K,(1)
View Sourcewhere |Nmaji| is the number of majority class instances in Ni. C(Xi) represents the proportion of the majority class instances in the K-nearest neighbors of Xi. Apart from the density factor, the distance factor, denoted by D(xi), is defined as
D(Xi)=ΣXjϵNmajidist(Xi,Xj)ΣXjϵNmajidist(Xi,Xj)+ΣXjϵNminidist(Xi,Xj),(2)
View SourceRight-click on figure for MathML and additional features.where
dist(Xi,Xj)=∑k=1l(xik−xjk)2−−−−−−−−−−−−⎷.(3)
View Sourcedist(Xi,Xj) is the euclidean distance between Xi and Xj and l is the number of features of the given data set T. D(Xi) represents the proportion of the distance from Xi to instances of Nmaji in the distance from Xi to instances of Ni. Note that the farther the distance from the minority class to the majority class instance on the boundary region, the larger D(Xi) is. Once we get C(Xi) and D(Xi), we add them together and get a new combined factor called information weight I(Xi).
I(Xi)=C(Xi)+D(Xi),(4)
View SourceRight-click on figure for MathML and additional features.I(Xi) is the measure of the importance for Xi. To use Xi for anchor instance selection, we normalize it as follows:
I^(Xi)=I(Xi)∑|Tmin|i=1I(Xi),(5)
View Sourcewhere |Tmin| is the number of instances in the minority class, and
∑i=1|Tmin|I^(Xi)=1.(6)
View SourceRight-click on figure for MathML and additional features.Then, Ii^ is defined as the probability for instance Xi to be selected in the anchor instance selection process.

3.2 Probabilistic Anchor Instance Selection
Once the anchor selection probability for each minority class instance is determined, the anchor instances are selected and new instances are generated using the following iterative strategy: each time a minority instance is selected as the anchor instance according to its probability, a new minority instance is generated around the anchor instance following a Gaussian distribution model that will be introduced in Section 3.3. This procedure is repeated until the number of minority instances is equal to the number of majority instances, that is, the number of synthetic instances to be generated is
G=|Tmaj|−|Tmin|.(7)
View SourceRight-click on figure for MathML and additional features.

In each iteration, an anchor instance is selected using the roulette algorithm based on the anchor selection probabilities for the minority instances. A random number λ is generated with uniform distribution and compared with each cumulative probabilities in the roulette selection algorithm. Table 1 shows the selection probability and cumulative probability for each minority instance. An minority instance Xk is selected if λ satisfies the following formula.
∑i=1k−1Ii^<λ<∑i=1kIi^,1≤k≤n.(8)
View Source

TABLE 1 Roulette Selection for the Minority Instances
Table 1- 
Roulette Selection for the Minority Instances
It should be noted that a minority instance may be selected as anchor instance for more than one time, since anchor instances are selected probabilistically from the minority set. Let N(Xi) be the number of times that Xi is selected as anchor instance. The expectation of N(Xi) can be calculated as
E(N(Xi))=(|Tmaj|−|Tmin|)⋅I^(Xi).(9)
View SourceRight-click on figure for MathML and additional features.Obviously, N(Xi) is proportional to I^(Xi). This shows the advantage of our selection scheme since the minority instances carrying more information will be sampled more times than those carrying less information, thereby increasing the quality of the new generated minority instances.

3.3 New Instance Generation
The next issue after anchor instance selection is how to generate new minority class instances according to the selected anchor instances. In GDO, we design a Gaussian distribution model for new instance generation. In this model, new minority instances are generated around the selected anchor instances with a Gaussian distribution. We use Gaussian distribution for new instance generation because most real data follows Gaussian distribution. The main idea of new instance generation is twofold. The first is to randomly select a direction vector originating from the anchor instance, along which the new instance will be generated. The second is to determine the distance between the new instance and the anchor instance based on a Gaussian distribution.

For any anchor instance Xi, we define a Gaussian distribution N(μi,ασi) where the mean μi is zero, the standard deviation σi is the euclidean distance between Xi and its nearest minority instance X¯¯¯¯i, and α is a scaling coefficient.
μi=0(10)
View Source
σi=∑k=1l(xik−x¯¯¯ik)2−−−−−−−−−−−−⎷.(11)
View SourceRight-click on figure for MathML and additional features.Based on this Gaussian distribution, a new minority instance is generated with the following steps.

Randomly select a direction originating from the anchor minority instance Xi as the direction of the new instance X′. The end point of the direction vector is denoted as V={v1,v2,…,vl}. As illustrated in Fig. 2, Xi, V and X′ are in the same line. The direction vector XiV−→− is
XiV−→−=OV−→−−OXi−→−,(12)
View Sourcewhere O is the origin of the coordinate system, and OV−→− and OXi−→− are position vector of V and Xi, respectively.

Determine the length of XiX′−→−−. It is a random number generated based on the Gaussian distribution N(μi,ασi).
|XiX′−→−−|=di∼N(μi,ασi).(13)
View SourceRight-click on figure for MathML and additional features.

Calculate the ratio between the length of XiX′−→−− and the length of XiV−→−, that is,
r=|XiX′−→−−||XiV−→−|=did0.(14)
View SourceRight-click on figure for MathML and additional features.where
d0=|XiV−→−|=Σlk=1(vk−xk)2−−−−−−−−−−−−√.(15)
View Source

The position vector of the new instance X′ can then be obtained as follows, as illustrated in Fig. 2.
OX′−→−=OXi−→−+r⋅XiV−→−.(16)
View SourceRight-click on figure for MathML and additional features.Finally, the new instance can be calculated as X′={x1+r(v1−x1),x2+r(v2−x2),…,xl+r(vl−xl)}.

Fig. 2. - 
Illustration of the generation of synthetic instance.
Fig. 2.
Illustration of the generation of synthetic instance.

Show All

Algorithm 1 gives the pseudocode for generating new minority instance.

Algorithm 1. Generation of New Minority Instance
Input: The selected minority class instance Xi;

Output: Synthetic instance X′;

Initialize V={v1,v2,…,vl}, viϵ[−1,1];

Calculate XiV−→− using Eq. (12);

Obtain |XiX′−→−−| according to Eq.(13);

Calculate r using Eq.(14);

Calculate OX′−→− using Eq.(16)

Obtain synthetic instance X′

3.4 Analysis on Oversampling Probability
Theoretically we can derive the oversampling probability of any position in the data space. That means the oversampling probability in the whole data space can be visualized. For any instance X in the data space, its oversampling probability can be calculated as follows:
P(X)=∑i=1|Tmin|I^(Xi)12π−−√σie−d2i2σ2i.(17)
View SourceRight-click on figure for MathML and additional features.The second part of Eq. (17) is the Gaussian probability density function and di is the distance between X and Xi.
di=∑k=1l(xik−xk)2−−−−−−−−−−−⎷.(18)
View SourceRight-click on figure for MathML and additional features.

Using Eq. (17), we can obtain the resampling probability contour map and resampling probability distribution map. Fig. 3 shows an example of these maps. The red asterisks are the monority class instances, and the blue dots are the generated instances. Colors from blue to yellow indicate the gradual increase in probability. As can be seen from the left subfigure, the probability of generating new instance around instance X is determined by two factors. The first one is the distance between X and the minority class instances. The second one is the density and the selection probabilities of the minority instances in the neighbor area. The probability of generating new instance at X is high when it is close to the minority class instances or its surrounding minority instances are dense, as shown in the yellow regions. Since the distance between the generated samples in the green region and the minority class instances is larger than those in the yellow region, their probabilities decrease. The generated samples in the blue region has the lowest probability because it is far away from the minority class instances.

Fig. 3. - 
Resampling probability contour (l) and distribution (r) maps.
Fig. 3.
Resampling probability contour (l) and distribution (r) maps.

Show All

3.5 Algorithm Pseudocode
Algorithm 2 shows the pseudocode of GDO, in which the training set T is the input data and the output data is the resampled data set T′. To calculate D(Xi) defined in Eq. (2), the distance between the given instance Xi and every instance in the data set needs to be calculated. Hence, the time complexity of the minority class instance weighting procedure (lines 3 - 8) is O(|Tmin||T|). In the data generating procedure (lines 10 - 14), the minority class instances are resampled G times and the time complexity is O(G). Since G is smaller than T, the time complexity of Algorithm 2 is O(|Tmin||T|).

Algorithm 2. GDO
Input: Training set T and K;

Output: Resampled data set T′;

Initialize T′=∅, and k=1;

Calculate G according to Eq. (7)

for each Xi in Tmin do

Find the K nearest neighbors of Xi;

Calculate C(Xi) using Eq. (1);

Calculate D(Xi) using Eq. (2);

Calculate I(Xi) using Eq. (4);

end for

Normalize I(Xi) using Eq. (5);

while k≤G do

Select anchor instance Xi according to Table 1;

Generate instance X′i according to Eqs.(12)-(16) and add X′i to T′;

k=k+1;

end while

3.6 A Case Study
To show the difference between our method and other oversamping methods, we give a case study on an artificial imbalanced binary data set, which is shown in Fig. 4. The data set includes 200 majority class instances and 33 minority class instances. Fig. 5 visualizes the oversampling results using different imbalanced methods. Red squares represent the majority class instances and blue circles represent the minority class instances. We have the following three key observations. First, it can be seen that the synthetic instances generated by our scheme form a shape that fits well to the distribution shape of the original data. Second, the weights of the minority class instances in the boundary region are greater than those in the safe region. Hence, more instances are generated in the boundary region. Third, for the minority class instances in the boundary region, the farther they are from the majority class, the more instances are generated. In contrast, SMOTE blindly generates the minority class instances without considering the data distribution, which results in some instances being generated in the majority class region. Borderline1-SMOTE only weights the minority class instances in the boundary region and also generates some noisy instances. SMOTE-ENN and SMOTE-TomekLinks generate the same number of instances for each minority class instance even though there is a clear boundary region. ADASYN also generates a large number of instances in the majority class region.


Fig. 4.
The original artificial data set.

Show All

Fig. 5. - 
Visual results of different oversampling methods.
Fig. 5.
Visual results of different oversampling methods.

Show All

SECTION 4Experimental Study
In this section, we evaluate the performance of our approach through a set of empirical studies. First, the performance metrics used in the study are presented in Section 4.1. Then, we give the details of the 40 selected imbalanced KEEL data sets including 10 large data sets and two imbalanced data sets of real applications in Section 4.2. The parameter configurations of the compared algorithms selected for this study are described in Section 4.3. The experimental results are presented in Section 4.4. We apply hypothesis tests to compare the results obtained with different learning algorithms in Section 4.5. The parameter settings of our algorithm are studied in Section 4.6. We compare and analyse the running time of all compared algorithms on the selected imbalanced KEEL data sets and the selected large data sets in Section 4.7. Finally, the results of our method in two real imbalanced problems are presented in Section 4.8.

4.1 Performance Measures
Accuracy, a metric for evaluating the performance of a classifier, reflects the overall classification performance of a classifier for a data set. However, it is possible to get high accuracy without identifying any minority class instance because of the high imbalance rate of imbalanced data sets. To better and more meaningfully measure the performance on classifying imbalanced data, the confusion matrix is constructed to determine the corresponding evaluation metrics [43]. Fig. 6 is a typical two-class confusion matrix.

Fig. 6. - 
Confusion matrix.
Fig. 6.
Confusion matrix.

Show All

TP (true positive) is the number of correctly classified positive instances, FN (false negative) is the number of the positive instances that were incorrectly classified as negative. FP (false positive) is the number of the negative instances that were incorrectly classified as positive, and TN (true negative) is the number of the correctly classified negative instances. The following metrics are calculated by the confusion matrix.
TPR=TPTP+FN(19)
View SourceRight-click on figure for MathML and additional features.
FPR=FPFP+TN.(20)
View SourceRight-click on figure for MathML and additional features.TPR (Recall) is the proportion of correctly classified instances among all positive instances. FPR is the proportion of misclassified instances among all negative instances. We use G-mean and Area Under the Curve(AUC) as the performance measures in our study by combing the above metrics.

G-mean contains two submetrics: sensitivity(Sens) and specificity(Spec), which measure the identification accuracy of the minority and majority class instances, respectively. They are defined as follows:
Sens=TPTP+FN(21)
View Source
Spec=TNTN+FP(22)
View SourceRight-click on figure for MathML and additional features.
G−mean=Sens⋅Spec−−−−−−−−−√.(23)
View SourceRight-click on figure for MathML and additional features.

AUC, another important metric derived from Receiver Operating Characteristic (ROC) curve [44], can be also defined using Sens and Spec as follows:
AUC=Sens+Spec2.(24)
View SourceRight-click on figure for MathML and additional features.

It can be seen that both AUC and G-mean are trade-off metrics between the correctly classified positive and negative instances. So they are usually used to measure the performance of imbalanced learning models.

4.2 Data Sets
Three types of imbalanced data sets are used to validate the performance of our proposal. 40 public data sets from the KEEL data repository [45], and 10 large data sets from the UCI machine learning repository (https://archive.ics.uci.edu/ml/index.php) are used. Two real application data sets collected by ourselves are also used for the experiments.

4.2.1 KEEL Data Sets
Table 2 gives the detailed properties of the 40 selected KEEL data sets. It shows the number of instances (# instances), the number of attributes (# attributes) and the Imbalanced Ratio (IR) for each data set. IR is the ratio between the number of majority and minority class instances. IR is the most widely used measurement for the imbalance degree of imbalanced data, which is defined as follows:
IR=nmajnmin,(25)
View Sourcewhere nmaj and nmin are the number of majority and minority class instances, respectively. There are 12 low imbalanced data sets with IR<9 and 28 high imbalanced data sets with IR≥9 in the selected data sets. Ten fold cross validation is applied to each data set in our experiments. For each fold, we train the learning models with training set and test with test set. The results are integrated to obtain AUC and G-mean.

TABLE 2 Description of the KEEL Data Sets
Table 2- 
Description of the KEEL Data Sets
4.2.2 Large Data Sets
Ten large data sets are also selected from the UCI machine learning repository to test the performance of our proposal. All the data sets contain more than 4000 instances, and two of them are high-dimensional data sets that have more than 160 features. The details of the selected large data sets are shown in Table 3. Some of these data sets are multi-class data sets. For these cases, the class with the smallest instance number is considered as the minority class, and the remaining classes are merged as the majority class.

TABLE 3 Details of the Large Data Sets

4.2.3 Real Application Data Sets
The two imbalanced data sets of real applications are the Internet video traffic data set and the esophageal cancer patient data set. For the Internet video traffic data set, we collect different types of video traffic from five video sites using Wireshark between October 2017 to March 2018. Specifically, all video traffic instances are collected for 15 minutes after skipping the advertisement. Other network applications are banned to avoid generating non-video traffic when videos are played. Then, the collected raw video traffic are preprocessed and features are extracted. We filter noise data and converge the TCP flows, because most video data are transmitted using the TCP protocol. Then, Binary-Coded Decimal (BCD) features of the preprocessed data are extracted, which show the distributions of the 256 byte codes in the video flows and are calculated by the formula below. For more details of BCD features, please refer [46].
fj=∑ni=1cij∑256k=1∑ni=1cik, j=1,2,…,256.(26)
View SourceRight-click on figure for MathML and additional features.where c represents 256 counts for each packets, n is the number of packets in the flow. In this way, we get a 256-dimensional feature vector F={f1,f2,…,f256} and perform feature selection using the CfsSubsetEval algorithm in Weka to form the feature data set. The details of Internet video traffic data are shown in Table 4. For the identification of each type of video traffic in the captured video traffic, we select 90 negative instances and 900 positive instances.

TABLE 4 Details of Video Traffic Data
Table 4- 
Details of Video Traffic Data
For the esophageal cancer patient data set, we collect 2861 clinical diagnostic data of patients with esophageal cancer, including 2076 from patients with cancer cell metastasis and 785 from patients whose cancer cells haven't transferred. For each text data, we get a 750-dimensional vector after natural language processing. The goal of this data set is to pick out the patients whose cancer cells have been transferred.

4.3 Algorithms Selected for the Study
To evaluate the performance of our method, we compare it with seven imbalanced learning algorithms, including five oversampling algorithms, one cost-sensitive algorithm, and one ensemble algorithm. The compared algorithms and the specific parameter configurations of these algorithms are presented in Table 5. All the oversampling algorithms are implemented by imblearn [47], and the cost-sensitive algorithm and the ensemble algorithm are run using the KEEL software [45] 1. SVM is used as the base learning model in this experiments.

TABLE 5 Parameter Settings of the Compared Algorithms
Table 5- 
Parameter Settings of the Compared Algorithms
4.4 Evaluation Results on AUC and G-Mean
The AUC results for the 40 KEEL data sets of all the compared methods are presented in Table 6. Table 7 shows the G-mean results of experimental study. To save space, we use ’BC’ to represent BalanceCascade in these tables. The best result of each data set is highlighted in bold style. The results of the two tables are sorted in ascending order by IR. We count the number of data sets for which an algorithm achieves the highest AUC or G-mean among the compared algorithms, and name this count number as the “winning times” for this algorithm. This can roughly show the performance of each algorithm for the first sight of the two tables. Fig. 7 shows the wining times of each algorithm on AUC and G-mean.

TABLE 6 AUC Results on KEEL Data Sets
Table 6- 
AUC Results on KEEL Data Sets
TABLE 7 G-Mean Results on KEEL Data Sets
Table 7- 
G-Mean Results on KEEL Data Sets
Fig. 7. - 
Wining time comparisons.
Fig. 7.
Wining time comparisons.

Show All

From these results, we have three observations. First, our method outputs more highest AUC and G-mean values than any other compared algorithm. In other word, our method hits the highest “winning times” for both AUC and G-mean. Specifically, our proposal performs best for 14 of the 40 data sets with regard to AUC, and 16 of the 40 for G-mean. Second, our method significantly outperforms the other methods on certain data sets, such as “glass-0-1-6_vs_2”, “vowel0” and “winequality-white-3_vs_7”. Third, GDO is observed to be more effective for high imbalanced cases. It only hits 2 of 12 highest AUC values, and 3 of 12 highest G-mean values for the low imbalanced data sets, losing the competition with BalanceCascade which performs best. However, when considering the high imbalanced data sets, we can see that GDO hits 12 of 28 highest AUC values, and 13 of 28 highest G-mean values, defeating all the other compared methods with significant improvement. Therefore, our method shows high performance on the 40 KEEL imbalanced data sets, especially for the high imbalanced data sets.

The AUC and G-mean results on the ten large data sets are given in Tables 8 and 9. GDO performs impressively well for these large data sets. It gains 7 highest values for the 10 large data sets for both AUC and G-mean. Some of its results are significantly better than those of the other algorithms, such as the results of the “Chesscking-Rook vs. King”, “sat”, “page-block0”, and “parkinsons telemonitoring” data sets. SVM-CS also performs quite well for some data sets. It gets the highest AUC and G-mean values for the “isolet” and “waveform” data sets. However, SVM-CS does not perform stably, as it outputs four 0.5 AUC values and four 0.0 G-mean values, which indicates that SVM-CS acts as a pure random classifier for these cases.

TABLE 8 AUC Results of the Large Data Sets
Table 8- 
AUC Results of the Large Data Sets
TABLE 9 G-Mean Results on Large Data Sets

4.5 Statistical Testing Results
We apply the non-parametric Wilcoxon signed rank hypothesis test [50], [51] to test the significance difference between our method and the compared methods due to the unknown overall distribution of the experimental results. The test is performed on the results of the 40 selected KEEL data sets, as the number of data sets is large enough to support the Wilcoxon signed rank hypothesis test. On the contrary, we do not perform the test on the results of the large data sets, because there are only 10 large data sets. Moreover, our method has shown significant superiorities with respect to the large data sets. According to the Wilcoxon signed rank test, the differences between the experimental results of any two compared methods are first calculated and ranked. Then, the positive and negative differences are summed separately to obtain R+ and R−. The null hypothesis that there is no significant difference between the two methods will be rejected if the difference between R+ and R− is sufficiently large. The obtained p−values are also compared with the given significance level α to determine whether to reject the null hypothesis. Table 10 shows the Wilcoxon test results of AUC and G-mean.

TABLE 10 Wilcoxon Test Results of GDO versus the Compared Methods
Table 10- 
Wilcoxon Test Results of GDO versus the Compared Methods
From Table 10, all the R+ values are greater than 900, whereas all the R− values are lower than 350. The large differences between the R+ and R− values suggest the significant differences between the compared results. Finally, all the p−values are far lower than the standard significance level α=0.05, which results that the null hypothesis should be rejected. Therefore, our method is demonstrated to be significantly better than the compared algorithms on the selected KEEL data sets according to the hypothesis test results.

4.6 Empirical Study on the Impact of Scaling Factor (α)
In our method, the distance between the selected anchor instance Xi and the new instance X′i is randomly generated based a Gaussian distribution N(μi,ασi) where σi is the Euclidian distance between Xi and its nearest minority neighbour and α is a scaling factor. Since a fixed ‘width’ of the sampling normal distribution cannot guarantee to produce the best performance, varying the scale factor α can change the sampling density around the anchor instance, leading to different oversampling effects. In this subsection, we conduct a set of experiments to investigate the impact of α on the performance of GDO. In this set of experiment, α is set between 0.2 and 2, and is cumulatively increased with a step value of 0.2. To save space, five of the KEEL data sets are selected for this case study, instead of using all the 40 data sets. We use the abbreviation of the data set names in the legend in order to save space. For each α value, we perform a complete GDO learning process on the selected data sets. The results are given in Fig. 8.

Fig. 8. - 
AUC and G-mean results with varying $\alpha$α.
Fig. 8.
AUC and G-mean results with varying α.

Show All

The results show two different patterns. For ‘yeast-0-5-6-7-9_vs_4’, ‘car-good’ and ‘glass-0-1-6_vs_2’, the performance of GDO is stable in spite of the variation of α. In other words, its performances is not sensitive to the parameter α for these data sets. Both AUC and G-mean change in a small range with the variation of α. However, for the ‘new-thyroid1’ and ‘winequality-white-3_vs_7’ data sets, the behavior pattern of GDO is quite different. The significant hops can be observed when α steps from 0.2 to 0.6. For the ‘new-thyroid1’ data set, both AUC and G-mean still climb with the increase of α from 0.6 to 1.0. The algorithm steps to a stable status when α>1.0 for both data sets.

These observations indicate that the value of σ can directly affect the performance of GDO for some cases. As mentioned before, α is the scaling factor of σ, and σ is the the standard deviation of the Gaussian distribution. When α starts from a small value (say less than 0.8), the Gaussian distribution has a small width. That will lead to the generated instances to be located in a small neighbor area around the anchor instances with large probabilities. Such concentrated oversampled instances are not able to fit the real distributions well, resulting low performance. On the contrary, if α is too large, e.g., greater than 1.2, the Gaussian distribution will has a large standard deviation. The ‘wide’ sampling Gaussian distribution may lead to generated instances locating in the majority class region, which consequently results in the misclassification of minority instances. In a word, a proper α is necessary to control the width of the sampling Gaussian distribution to get a proper range for generating new instances. In this way, the generated instances can well-fit the distributions of the original data with good sampling diversities.

4.7 Comparisons on Running Time and Memory Usage
Beside AUC and G-mean, we also compare the runtime and memory usage between our method and the other algorithms for both the 40 KEEL data sets and the 10 large data sets. Tables 11 and 12 show the runtime of all the compared algorithms on the KEEL data sets and the large data sets, respectively. It should be noted that the runtime values of SVM-CS are rounded down to one decimal place, because they are exported from the KEEL software. From a global view, the runtime of GDO is not as good as those of the compared algorithms. It spends 10 to 100 times more than the compared algorithms for most of the KEEL data sets. This is because that both of the information weighting and new instance generation processes are time consuming, resulting in high runtime values. In contrast to GDO, the runtime of SMOTE is the lowest as the sampling mechanism of SMOTE is also the simplest. In a word, our proposal achieves high accuracy with relatively high runtime cost.

TABLE 11 Runtime of GDO and Compared Algorithms on the KEEL Data Sets
Table 11- 
Runtime of GDO and Compared Algorithms on the KEEL Data Sets
TABLE 12 Runtime of GDO and Compared Algorithms on the Large Data Sets
Table 12- 
Runtime of GDO and Compared Algorithms on the Large Data Sets
The memory usage of all compared algorithms on the 40 KEEL data sets and the 10 large data sets is presented in the Tables 13 and 14, respectively. The results are measured in megabytes. Different from the runtime usage, our method consumes the least memory for all the data sets, including the large data sets. In most cases, GDO saves nearly 4 MB than the other algorithms. It can be seen from Section 3, our proposal does not need large temporary space to store temporary results, resulting in low memory usage. The results on runtime and memory usage also justify the spatial-temporal contradiction to some extend.

TABLE 13 Memory Consuming of GDO and Compared Algorithms
Table 13- 
Memory Consuming of GDO and Compared Algorithms
TABLE 14 Memory Consuming of GDO and Compared Algorithms on the Large Data Sets
Table 14- 
Memory Consuming of GDO and Compared Algorithms on the Large Data Sets
4.8 Application Results on Real Imbalanced Problems
To test the performance of our approach for real imbalanced problems, we apply GDO for two real imbalanced data sets collected by ourselves, including an Internet video traffic data set and a cancer metastasis detection data set of esophageal cancer patients. We use histograms to show the results. Fig. 9 show the identification results of the four video traffic types in the Internet video traffic data set. GDO hits the highest AUC and G-mean values for all the video traffic types, implying the high performance of GDO in the video traffic identification problem. For the metastasis detection of esophageal cancer, the detection results are shown in Fig. 10. GDO is again observed to get the highest AUC and G-mean values among all the compared algorithms. The results suggest that our method can more accurately detect the metastasis of cancer cells in patients with esophageal cancer than the other algorithms.

Fig. 9. - 
AUC and G-mean results on Internet video traffic data sets.
Fig. 9.
AUC and G-mean results on Internet video traffic data sets.

Show All

Fig. 10. - 
AUC and G-mean results on the esophageal cancer data sets.
Fig. 10.
AUC and G-mean results on the esophageal cancer data sets.

Show All

SECTION 5Conclusion
We proposed a new oversampling method called GDO to deal with the imbalanced data classification problem. The features of GDO can be summarized as two points. First, GDO is a probability-based model as it selects anchor instances and generates new instances probabilistically based on the information carried by different minority instances. Second, the sampled data are generated in a way to fit the real distribution of the original data. Our method was compared with seven other imbalanced approaches on 40 imbalanced KEEL data sets and 10 large imbalanced data sets. All the experimental results suggest that our method can obtain higher AUC and G-mean with low memory consumption than the compared algorithms. The empirical studies on two real imbalanced problems, Internet video traffic identification and cancer metastasis detection of esophageal cancer patients, also validate this. However, our method suffers from high time complexity, as shown in the empirical studies. Therefore, how to reduce the time complexity of GDO, that is to design a “fast and effective” GDO algorithm, is our future work.