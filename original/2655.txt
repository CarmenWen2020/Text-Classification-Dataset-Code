Community detection is one of the fundamental tasks in graph mining, which aims to identify group assignment of nodes in a complex network. Recently, network embedding techniques have demonstrated their strong power in advancing the community detection task and achieve better performance than various traditional methods. Despite their empirical success, most of the existing algorithms directly leverage the observed coarse network structure for community detection. Therefore, they often lead to suboptimal performance as the observed connections fail to capture the essential tie strength information among nodes precisely and account for the impact of noisy links. In this paper, an optimal network structure for community detection is introduced to characterize the fine-grained tie strength information between connected nodes and alleviate the adverse effects of noisy links. To obtain an expressive node representation for community detection, we learn the optimal network structure and network embeddings in a joint framework, instead of using a two-stage approach to derive the node embeddings from the coarse network topology. In particular, we formulate the joint framework as an optimization problem and an alternating optimization algorithm is exploited to solve the proposed optimization problem. Additionally, theoretical analyses regarding the computational complexity and the convergence of the optimization algorithm are also provided. Extensive experiments on both synthetic and real-world networks demonstrate the effectiveness and superiority of the proposed framework.
SECTION 1Introduction
Community structure, a densely connected set of nodes with similar characteristics [1], [2], is a phenomenon that is ubiquitous in a massive amount of real-world networks such as social media networks, biological networks, and citation networks [3]. To gain a deeper understanding of their internal composition, uncovering the community structure hidden in the network is a crucial step before further data analysis. More formally, community detection [2], [4] aims to discover groups of nodes with dense within group connections and sparse between groups connections. It plays an essential role in probing the underlying structure of complex networks, with significant implications in various practical domains. For example, in social media networks, identifying communities of users with common interests enable us to make more effective personalized recommendations [5]; in protein-protein interaction networks, clusters of proteins can be employed to discover protein complexes and functional modules [6], [7]; and in citation networks, groups of publications help researchers find relevant literature to advance their research [8].

In fact, detecting communities on networks can be viewed as a problem of dividing the network into groups of nodes with better internal connectivity than external connectivity. A traditional solution involves the optimization of various measures to score every possible division of the network, such as modularity or its variants [9], [10], to find the best one. Unfortunately, since these measures are often predefined, they are not flexible enough to adapt to various types of networks with distinct topology characteristics, and designing these hand-engineered measures to extract community structure requires a lot of human efforts and can be expensive.

Another prevalent way to solve the problem is first to map nodes to real-valued vectors (a.k.a. embeddings) which encode the community structure information, and then apply any conventional clustering algorithms (e.g., K-means) to divide nodes into different communities [11], [12], [13]. This family of methods generalize across different networks as they are mainly data-driven and do not rely on any predefined measures. However, most, if not all, of the current network embedding techniques [11], [14] are independent of the specific downstream tasks, making the objective function of network embedding not aligned with the downstream task of community detection. Additionally, traditional approaches overwhelmingly focus on encoding the structural information from the observed coarse network topology, which inevitably yield poor community detection results because of the following two reasons. On the one hand at the local level, most of existing methods treat observed links equally for embedding representation learning, which overlooks the fact that nodes may belong to different communities. For example, as shown in Fig. 1a, nodes {v4,v7,v9} are the neighbors of node v8, where {v7,v9,v8} belong to the same community while {v4,v8} do not. However, in most of the existing methods, {v4,v7,v9} contribute equally when we attempt to learn v8's embedding representation, even though the fact that v7 and v9 are obviously more important than v4 to bring out community structure, which hinders us to identify the true community assignment of node v8. On the other hand at the global level, there often exist a number of noisy links (e.g., link farm—a group of nodes all linked each other) blurring the boundaries of different communities, while most of existing methods are not well equipped to handle the noisy links for community detection. To be more specific, as shown in Fig. 1a, the black edges are responsible for densely connecting the nodes within the community, but even so, the two communities are still not very obvious to be distinguished as three red edges, which can be a result of noisy links, could obscure the boundary of two different communities.

Fig. 1. - 
Differences between the original network structure and the optimal network structure for community detection. Top: A subset of network topology on the boundary of two communities; Bottom: (weighted) adjacency matrix of the above (optimal) network.
Fig. 1.
Differences between the original network structure and the optimal network structure for community detection. Top: A subset of network topology on the boundary of two communities; Bottom: (weighted) adjacency matrix of the above (optimal) network.

Show All

To tackle the above mentioned issues, in this paper, we propose a novel joint modeling approach to learn optimal network structure and network representation simultaneously for the community detection task, called LookCom, where the optimal network structure is able to capture the inherent link information among nodes at a finer granularity. Methodologically, we first attempt to choose the optimal number of neighbors and the corresponding weight, then we impose a rank constraint on the Laplacian matrix of the optimal network structure to ensure the number of communities equals the connected components. In such a way, neighbors around each node could be adaptively adjusted and weighted based on the inherent communities they belong to; at the same time, weights of edges between different communities could be reduced or even removed to make the community structure more clear. Give a simple example of the optimal network structure illustrated in Fig. 1b, the tie strength of edges are reweighed (marked with lines of different thickness levels), while the noisy edges (v6,v7) and (v4,v8) are eliminated. The main contributions of our work are as follows:

We introduce a new idea that learns an optimal network structure specialized for community detection from the observed original network topology.

We propose a novel joint community detection framework LookCom to generate node representations based on the optimal network for community detection.

We develop an effective alternating optimization algorithm for the proposed framework LookCom.

We empirically evaluate the effectiveness of the proposed LookCom model on both synthetic and real-world networks of different types.

The rest of this paper is organized as follows. In Section 2 we introduce the proposed LookCom model in detail, and derive the optimization problem. In Section 3 we present the learning algorithm with theoretical analyses on computational complexity and convergence. Experimental evaluations on both synthetic and real-world datasets are shown in Section 4 with discussions. Section 5 briefly reviews related work. Finally, Section 6 concludes the whole paper and visions the future work.

SECTION 2The Proposed Methodology
Throughout this paper, all the sets, matrices, vectors, and scalars are written as calligraphic alphabets (e.g., M), bold uppercase (e.g., M), bold lowercase (e.g., m), and lowercase (i.e., m), respectively. Given a matrix M∈Rp×q, its ith row is denoted by mi(i=1,2,…,p). The trace and rank of matrix M are represented by Tr(M) and rank(M), respectively. The Frobenius norm of M is defined as ∥M∥F=Tr(MM⊤)−−−−−−−−−√. The ℓ2-norm of m=[m1,m2,…,mn]⊤∈Rn is defined as ∥m∥2=m⊤m−−−−−√. Besides, we denote the identity matrix of size n as In∈Rn×n, 1 as a column vector whose entries are all 1. The main symbols used in this paper are summarized in Table 1.

TABLE 1 Symbols
Table 1- 
Symbols
2.1 Characterizing the Optimal Network Structure
Let G=(V,E) be an observed network, where V={v1,…,vn} is the set of n nodes; E⊆V×V collects the edges between the nodes. We use the adjacency matrix A∈{0,1}n×n to denote the connections among nodes.1 In this paper, community detection on a network G aims to divide the node set into c groups2 V1,V2,…,Vc such that:

V1∪V2∪⋯∪Vc=V and  Vi∩Vj=∅ (∀i≠j).

Nodes within groups are densely connected, while sparsely connected between groups.

Conventionally, the embedding based methods first attempt to learn embedding representations of nodes, and then identify communities via clustering as post-hoc processing, such as with K-means. As such, the learned generic node representations may not be optimal for the particular task [16], [17], i.e., the community detection task in this work. Also, as these methods treat all observed edges equally and fail accounting for the adverse effects of noisy links, the detected community structure may be suboptimal for practical demands. To address these issues, we embark on the definition of the optimal network structure, which enables a fine-grained representation of tie strength information among connected nodes for community detection.
Definition 1 (Optimal Network Structure for Community Detection).
Given a network G=(V,E) with c predefined communities, the optimal network structure is encoded in a weighted matrix W∈[0,1]n×n with the following constraints:

For each node vi∈V, ∑vj∈N(vi)wij=1, where wij≥0 for all vj∈N(vi) (in which N(vi) collects neighbors of vi), otherwise wij=0;

For the Laplacian matrix LW associated with weighted matrix W,rank(LW)=n−c, where LW=DW−(W⊤+W)/2 with diagonal matrix DW=diag(dw11, dw22,…,dwnn)∈Rn×n and dwii=∑nj=1(wij+wji)/2 for i=1,2,…,n.

Note that in Definition 1, the first constraint implies that we attempt to adaptively quantify the tie strength information between each node vi and its neighbors, enforcing wij=0 if the nodes vi and vj are not connected. Also, we ensure the tie strength vector around each node is normalized and sum up to one. With this constraint, it is possible that we can find the optimal number of neighbors and the corresponding weights around each node for the community detection task. The second constraint follows an important property in graph theory:

Theorem 1.
[18] Let G be a given undirected graph with all the edge weights non-negative. The algebraic multiplicity of the eigenvalue 0 of the corresponding Laplacian matrix equals to the number of connected components in G.

It indicates that if rank(LW)=n−c, then the communities in the network should stand out clearly based on the optimal network structure W, rather than unclear community structure depicted in Fig. 1a. In summary, the optimal network structure W defined above provides us convenience in finding the inherent community structure embedded in the network. Later, we will offer a coherent and principled approach to learn this optimal network structure.

It also should be noted that the similar idea of finding optimal neighborhood structure has been used in the previous studies of graph-based clustering [19] (e.g., spectral clustering [20]). However, it is different from our studied problem in the following two aspects: (1) We focus on solving the community detection problem where the optimal graph can be obtained from the observed coarse network structure while [19] targets at the clustering problem on conventional i.i.d. data and the optimal neighborhood is derived from the input features of data samples. (2) Even though [19] provides an elegant way to specify different weights to nearest neighbors for graph-based clustering, the optimal number of neighbors around each node is fixed while we provide a more flexible definition by allowing different numbers of optimal neighbors for different nodes.

2.2 Learning the Optimal Network Structure
Based on the above definition, in this subsection, we will introduce the joint modeling framework which learns the optimal network structure and node embeddings simultaneously such that the node embeddings will be customized to the optimal network structure through which the inherent community structure can be well revealed.

The widely observed assortative mixing patterns [21] (i.e., homophily) of real-world networks imply that the properties of nodes should be similar with each other if they are within close proximity in the network (e.g., belong to the same community). As node embeddings encode the inherent properties of nodes, we need to ensure that the obtained node embeddings in the euclidean space well preserve the inherent community structure that can be easily obtained from the optimal network structure. In essence, node pairs with smaller distance in embedding space should be assigned larger weights, where the weights correspond to the tie strength of edge in the optimal network. Suppose Z=[z1;…;zn]∈Rn×m denotes the node embeddings, then the node embeddings and the weighted matrix W of the optimal network can be obtained by solving the following optimization problem:
minZ,Ws.t.∑i,j=1nwij∥zi−zj∥22+α∥W−A∥2FZ⊤Z=Im,W∈PcG,(1)
View SourceRight-click on figure for MathML and additional features.where the first term is for the aforementioned weights assignment, and the second term minimize the difference between the observed network and the learned optimal network. Parameter α is introduced to balance the contributions of these two parts. The constraint Z⊤Z=Im ensures that the learned representations for each node are measured at the same scale. The optimal network structure of G with c predefined communities are collected into set PcG={W∈[0,1]n×n:∑vj∈N(vi)wij=1 for all vi∈V,wij≥0 for all vj∈N(vi),wij=0 for all vj∉N(vi),rank(LW)=n−c} for convenience.

The optimization problem in Eq. (1) is difficult to solve due to the low rank constraint. To solve the problem, motivated by [19], we denote the ith smallest eigenvalue of matrix LW as σi(LW). Note that σi(LW)≥0 since the Laplacian matrix LW is positive semidefinite. In this sense, the optimization problem in Eq. (1) is equivalent to the following problem when λ is specified to a very large value:
minZ,Ws.t.∑i,j=1nwij∥zi−zj∥22+α∥W−A∥2F+2λ∑i=1cσi(LW)Z⊤Z=Im,W∈QG,
View SourceRight-click on figure for MathML and additional features.where QG={W∈[0,1]n×n:∑vj∈N(vi)wij=1 for all vi∈V,wij≥0 for all vj∈N(vi),wij=0 for all vj∉N(vi)}. Further, using Ky Fan's Theorem [22], we have
∑i=1cσi(LW)=minF∈Rn×c,F⊤F=IcTr(F⊤LWF),(2)
View SourceRight-click on figure for MathML and additional features.where F is an intermediate optimization variable that introduced to make the problem much easier to solve. As a result, the optimization problem in Eq. (1) can be reformulated as the following problem:
minZ,W,Fs.t.∑i,j=1nwij∥zi−zj∥22+α∥W−A∥2F+λTr(F⊤LWF)Z⊤Z=Im,F⊤F=Ic,W∈QG.(3)
View SourceRight-click on figure for MathML and additional features.By optimizing Z and W simultaneously, this optimization problem is capable of encoding structural information in the optimal network structure into the representations of the nodes. As such, the learned embeddings will be beneficial to the downstream community identifying task.

SECTION 3The Optimization Algorithm
The objective function of the optimization problem in Eq. (3) is not jointly convex w.r.t. all the variables, but it is convex with respect to each variable when the other variables are fixed. Therefore, we exploit an alternating update algorithm to solve this problem.

Update F. With fixed variables Z and W, we obtain the optimal F by solving the following convex problem:
minF⊤F=IcTr(F⊤LWF).(4)
View SourceRight-click on figure for MathML and additional features.Let the Lagrangian function be
L(F)=Tr(F⊤LWF)−Tr(Λ(F⊤F−Ic)),
View SourceRight-click on figure for MathML and additional features.where diagonal matrix Λ denotes the Lagrangian multipliers. We take the derivative of L(F) and set it to zero
∂L(F)∂F=LWF−FΛ=0n×c.
View SourceIt implies that the optimal F is the top c eigenvectors of the Laplacian matrix LW corresponding to the c smallest eigenvalues.

Update Z. With fixed variables W and F, we learn the optimal Z via solving the following optimization problem:
minZ⊤Z=Im∑i,j=1nwij∥zi−zj∥22=2Tr(Z⊤LWZ).(5)
View SourceRight-click on figure for MathML and additional features.It is evident that this problem is the same as the problem in Eq. (4). Thus, the optimal Z can be obtained by taking the top m eigenvectors of the Laplacian matrix LW corresponding to the m smallest eigenvalues.

Update W. With the fixed variables Z and F, we learn the optimal W through the following optimization problem:
minW∈QG∑i,j=1nwij∥zi−zj∥22+α∥W−A∥2F+λTr(F⊤LWF).(6)
View SourceRight-click on figure for MathML and additional features.It is evident that the optimization problem above is independent between different i=1,2,…,n. Thus, we reformulate it as the following n convex optimization problems:
minwi∈Qviαw⊤iwi+(di−2αai)⊤wi,(7)
View SourceRight-click on figure for MathML and additional features.for i=1,2,…,n, where di=[di1,di2,…,din]⊤∈Rn with dij=dzij+λdfij, dzij=∥zi−zj∥22 and dfij=∥fi−fj∥22 for i,j=1,2,…,n; Qvi={wi∈[0,1]n:∑vj∈N(vi)wij=1,wij≥0 for all vj∈N(vi),wij=0 for all vj∉N(vi)}.

SECTION Algorithm 1.Update wi for Node vi
Input: adjacency matrix A∈{0,1}n×n, ordered vector di∈Rn with the jth element as dij=∥zi−zj∥22+λ∥fi−fj∥22, parameter α.

Output: tie strength vector wi∈Rn for node vi.

Let k=0 and θ=di1−2αai1+1.

while 2αai,k+1+θ>di,k+1 and k<|N(vi)| do

Update k=k+1.

Update θ=2αk−2αk∑kj=1aij+1k∑kj=1dij.

end while

calculate wi according to Eq. (9).

Return wi.

Note that each convex optimization problem as Eq. (7) can be solved by off-the-shelf convex optimization approaches proposed in [23]. Concretely, these methods will find an ϵ-optimal solution with a given accuracy ϵ>0. Motivated by [24], we present a novel greedy algorithm to obtain the exact solution of the problem in Eq. (7) with lower computational cost. First, we denote the Lagrangian function of optimization problem in Eq. (7) as
L(wi,θ,βi)=αw⊤iwi+(di−2αai)⊤wi−θ(1⊤wi−1)−β⊤iwi,(8)
View SourceRight-click on figure for MathML and additional features.where θ∈R and βi∈Rn are the multipliers of the equality constraint and inequality constraints, respectively. By getting the derivative the Lagrangian function with respect to wi and setting it to zero, we get
wij=12α(2αaij+θ+βij−dij).(9)
View SourceRight-click on figure for MathML and additional features.for any j=1,2,…,n. Denoting w∗ij be the optimal solution of the problem in Eq. (7). According to the complementary slackness in the KKT conditions [23], we have w∗ijβij=0. Then, for any w∗ij>0 it follows βij=0. As a result, we arrive at
w∗ij=12α(2αaij+θ−dij)>0.(10)
View SourceBy summing w∗ij over all nonzero entries, we obtain
1=∑w∗ij>0w∗ij=∑w∗ij>0aij+12α∑w∗ij>0θ−12α∑w∗ij>0dij=∑j=1k∗iaij+12α∑j=1k∗iθ−12α∑j=1k∗idij,(11)
View Sourcewhere k∗i nonzero weights {w∗ij}k∗ij=1 correspond to the k∗i smallest values of dij, k∗i≤|N(vi)|. Intuitively, k∗i is the optimal number of neighbors for vi we should choose. Solving for θ, we get
θ=2αk∗i−2αk∗i∑j=1k∗iaij+1k∗i∑j=1k∗idij.(12)
View SourceRight-click on figure for MathML and additional features.When k∗i is obtained, the optimal solution w∗i for problem (7) can be achieved according to Eq. (12) and Eq. (9). Therefore, we greedily add neighbors according to the ordered values of dij until the constraint in Eq. (10) is no longer satisfied. The details of this algorithm for each node are illustrated in Algorithm 1. Also, the detailed algorithm for problem in Eq. (3) is summarized in Algorithm 2.

SECTION Algorithm 2.Algorithm to Solve the Problem in Eq. (3)
Input: the adjacency matrix A∈{0,1}n×n, the number of predefined communities c, dimension of representations m, parameters (α, a large enough λ).

Output: node representations Z∈Rn×m for community detection task.

Initialize W by adjacency matrix A, t=0.

while not converge do

Calculate L(t)W=D(t)W−(W(t))⊤+W(t)2 where D(t)W is a diagonal matrix with dwii=∑nj=1w(t)ij+w(t)ji2 for i=1,2,…,n.

Update F(t+1), which is formed by the top c eigenvectors of L(t+1)W corresponding to the c smallest eigenvalues.

Update Z(t+1), which is formed by the top m eigenvectors of L(t+1)W corresponding to the m smallest eigenvalues.

Update W(t+1) by Algorithm 1.

Update t=t+1.

end while

Return node representations Z.

Computational Complexity Analysis. We theoretically analyze the computational complexity of our proposed algorithm as the following theorem:

3Theorem 2.
The time complexity of Algorithm 2 in each epoch t is O(m|V|+m2|E|)+∑ni=1O(kti+|N(vi)|log|N(vi)|), where kti is the optimal number of neighbors for vi in epoch t.

3Proof.
In each epoch, to update F and Z by computing the top c and m eigenvectors3 of Laplacian matrix LW, it requires O(c|V|+c2|E|) and O(m|V|+m2|E|), respectively [25]. Since c<m in our study, the running time of updating F and Z can be simplified to O(m|V|+m2|E|). For the tth epoch, to update wi by Algorithm 1, it requires an O(|N(vi)|log|N(vi)|) cost for sorting the entries of di, as well as an additional running time of O(kti) to greedily add neighbors (see Eq. (11)) Therefore, the overall computational complexity in the tth epoch of Algorithm 2 is O(m|V|+m2|E|)+∑ni=1O(kti+|N(vi)|log|N(vi)|).

Converge Analysis. Here we provide a theoretical analysis on the convergence of the proposed alternating optimization algorithm depicted in Algorithm 2, which is summarized in the following theorems:

3Theorem 3.
The Algorithm 1 will monotonically decrease the objective of the problem in Eq. (7) in each iteration, and converge to the global optimum of the problem.

3Proof.
Let φk be the objective function value of problem in Eq. (7), which is calculated by Algorithm 1 at iteration k. Then for any k<k∗i the following holds:
φk=minwi∈Q(k)viαw⊤iwi+(di−2αai)⊤wi,(13)
View SourceRight-click on figure for MathML and additional features.where Q(k)vi={wi∈Qvi:wij=0 for all j>k}. By definition of Q(k)vi, this implies that Q(k)vi⊂Q(k+1)vi for any k<|N(vi)|. Therefore, as minimizing the same objective function with stricter constraints yields a higher optimal value, Eq. (13) implies that
φk+1≤φk,
View Sourcefor any k<k∗i. Thus the value of objective function in Eq. (7) will monotonically decrease using the updating rules in Algorithm 1. Since the problem in Eq. (7) is a convex optimization problem, Algorithm 1 will converge to the global optimum [23].

3Theorem 4.
The Algorithm 2 will monotonically decrease the objective function of problem in Eq. (3) in each iteration, and converge to a local optimum of the problem.

3Proof.
In Algorithm 2 at iteration t, F is updated in the step 4 by solving the problem
F(t+1)=argminF⊤F=IcTr(F⊤L(t)WF),
View SourceRight-click on figure for MathML and additional features.with fixed W(t). Thus we obtain the following inequality:
Tr((F(t+1))⊤L(t)WF(t+1))≤Tr((F(t))⊤L(t)WF(t)).(14)
View SourceRight-click on figure for MathML and additional features.Meanwhile, in step 5 of Algorithm 2, we update Z as same as variable F because of the equation in Eq. (5). Thus we have
∑i,j=1nw(t)ij∥z(t+1)i−z(t+1)j∥22≤∑i,j=1nw(t)ij∥z(t)i−z(t)j∥22.(15)
View SourceRight-click on figure for MathML and additional features.Moreover, in step 6 of Algorithm 2, each row of W is updated by the Algorithm 1. Based on Theorem 3 and the inequalities in Eqs. (14) and (15), we have
≤≤∑i,j=1nw(t+1)ij∥z(t+1)i−z(t+1)j∥22+α∥W(t+1)−A∥2F+λTr((F(t+1))⊤L(t+1)WF(t+1))∑i,j=1nw(t)ij∥z(t+1)i−z(t+1)j∥22+α∥W(t)−A∥2F+λTr((F(t+1))⊤L(t)WF(t+1))∑i,j=1nw(t)ij∥z(t)i−z(t)j∥22+α∥W(t)−A∥2F+λTr((F(t))⊤L(t)WF(t)).(16)
View SourceRight-click on figure for MathML and additional features.As a result, the alternating algorithm in Eq. (3) will monotonically decrease in each iteration until it converges. Furthermore, we denote the Lagrangian function of optimization problem in Eq. (3) as follows:
LΛ(X)=∑i,j=1nwij∥zi−zj∥22+α∥W−A∥2F+λTr(F⊤LWF)+Φ(X,Λ),
View Sourcewhere X={X=(Z,F,W):Z⊤Z=Im,F⊤F=Ic,W∈QcG} is the feasible solution set of the problem, and Φ(X,Λ) is the formalized term derived from X with the Lagrangian multiplier Λ. Setting the derivative of LΛ(X) w.r.t. W to zero, we have
∂LΛ(X)∂W=∂∑ni,j=1wij∥zi−zj∥22∂W+α∂∥W−A∥2F∂W+λ∂Tr(F⊤LWF)∂W+∂Φ(X,Λ)∂W=0.(17)
View SourceNote that the inequality in Eq. (16) holds the convergence. Thus, W(t+1) satisfies the equality in Eq. (17), i.e., the KKT condition of optimization problem in Eq. (3). As a result, the Algorithm 2 will converge to the local optimum of the optimization problem in Eq. (3).

SECTION 4Experiments
In this section, we will present experiments on both synthetic and real-world networks to evaluate the performance of our proposed model LookCom. In particular, we attempt to answer the following three research questions: (1) Can LookCom learn an optimal network structure for community detection? (2) Can embedding based methods improve community detection performance compared with traditional approaches? (3) Can LookCom show evident improvement when compared with other embedding based methods? We start by answering the first question according with experiments on synthetic networks.

4.1 Experiments on Synthetic Networks
To verify the capability of our proposed algorithm to learn the optimal network structure for community detection, we perform a controlled test for our model over several synthetic networks. These synthetic networks are generated by the LFR model [26], which is widely used to create networks with power-law of degree and community size similar to real-world networks. To build the community structure, this model introduces a parameter μ∈[0,1] for each node to measure the fraction of its neighbors in other communities, and 1−μ is the fraction in its own community. When we set μ<0.5 we get a community structure that edges are more likely within communities than between them, and the difficulty of the community detection task on this network structure is also controlled by this value. In this experiment, we randomly generate three networks with 200 nodes using the LFR model, in which the power-law exponents of degree distribution and community size distribution are set to 3.0 and 1.5 respectively, as suggested by [26]. At the same time, the parameter μ is set as 0.15, 0.25 and 0.35 respectively on these three networks to have different levels of community detection difficulty.

Using these networks, we test our algorithm and then compare the original network structure A (i.e., adjacency matrix) and the learned optimal network structure W by their corresponding heatmap. The results are shown in Fig. 2. Zooming in the figures will give better visualization. We can notice that whereas the community structure is hard to detect in the original network structure, the communities in the learned optimal network structure stand out more clearly with block diagonal structure. As a result, the optimal network structure that our algorithm learned is more suitable than the original coarse network structure in advancing the downstream community detection task.


Fig. 2.
Visualization of the learned optimal network structure for community detection on synthetic networks. The block diagonal structure is generated by reordering the node indexes based on the ground truth community structure, which is performed after the learning process.

Show All

4.2 Experiments on Real-World Networks
4.2.1 Experimental Settings
Datasets. In this set of experiments, we collect four real-world network datasets Wiki,4 BlogCatalog,5 DBLP,6 PubMed,7 and Amazon8 for the community detection task. These networks are of various types, ranging from webpage networks, social network, to academic citation networks.

Wiki: Wiki is a webpage network which contains real-world webpages and hyperlinks between them. In this network, nodes are webpages, and edges represent hyperlinks. Communities are formed by different topics.

BlogCatalog: BlogCatalog is a social network from a social blogging website. The nodes here represent users and the edges are social relations. All nodes are divided by user interests into different communities.

DBLP: DBLP is a citation network which is collected from DBLP repository. The papers and citation links are as nodes and edges in this network. These papers are classified by research areas into different classes.

PubMed: PubMed is a citation network of scientific publications about diabetes. These publications are classified by diabetes type into different communities.

Amazon: Amazon is a product co-purchasing network, where nodes are products and edges link commonly co-purchased products. All products are divided by product category into communities.

The detailed statistics of the networks outlined above are summarized in Table 2.
TABLE 2 Statistics of the Used Datasets
Table 2- 
Statistics of the Used Datasets
Baselines. To demonstrate the superiority of our approach, we compare LookCom9 with two classes of community detection methods, traditional algorithms and embedding based algorithms. For traditional approaches, we compare against several successful algorithms that are based on modularity maximization, spectral clustering, and label propagation. For embedding based approaches, we compare against state-of-the-art network embedding algorithms that are based on matrix factorization, random walk, and deep neural network. Following the basic scheme for community detection based on network embedding, we obtain node embedding results from these baselines and then apply the K-means10 algorithm to detect communities.

The traditional methods are listed as follows:

CNM: CNM greedy modularity maximization [27] is one of the earliest community detection algorithms devised to maximize the predefined modularity measure.

Louvain: Louvain method [28] is also a greedy optimization algorithm of modularity, which is widely used for many applications because of its speed.

SC: Spectral method [29] is based on spectral properties of graph, which leverages the top eigenvectors of the graph Laplacian matrix for community detection.

FluidC: Fluid Communities algorithm [30] is an approach based on the propagation methodology which allows us to specify the number of communities.

The network embedding approaches are introduced as below:

LE: Laplacian eigenmap [31] is one of the earliest network embedding approaches, which generates node representations by factorizing the Laplacian matrix.

GF: Graph factorization [32] is also a matrix-factorization approach for network embedding. It applies matrix factorization to the adjacency matrix.

LINE: Large-scale Information Network Embedding [33] learns node representations by minimizing the loss functions to preserve the first-order and the second-order node proximity separately.

SDNE: Structural Deep Network Embedding [34] uses autoencoders to generate node representations by reconstructing the adjacency matrix to preserve the second-order proximity.

HOPE: High-order Proximity Preserved Embedding [35] uses matrix factorization technique to learn two representations for each node to preserve asymmetric high-order proximity.

GraRep: GraRep [36] employs Singular Value Decomposition (SVD) to capture the high-order node proximity in networks.

M-NMF: Modularized Nonnegative Matrix Factorization [13] is a community-oriented network embedding model that uses matrix factorization technique with a modularity constraint to capture community structure.

DeepWalk: DeepWalk [37] utilizes truncated random walks and the idea of Skip-Gram model to learn representations for each node.

node2vec: node2vec [38] follows the Skip-Gram architecture and uses the biased random walks strategy to learn node embedding representations.

ComE: ComE [39] is a Gaussian mixture based framework for community detection and node embedding, which jointly optimize two tasks to reinforce each other.

Parameter Settings. For all the netowork embedding approaches, the dimensionality of the learned node embeddings is set to 64. To obtain a fair comparison, we run each algorithm for 20 times and report the best results. In our model, there are two parameters: α and λ. We tune these parameters by a “grid-search” strategy from {10−3,10−2,…,103,104} and report the best results. At the same time, the parameters for baselines are tuned to be optimal as well. For DeepWalk and node2vec, we set the number of random walks, walk length and window size of skip-gram as 10, 80, 10, respectively. Specifically, we consider parameter p and q of node2vec in {0.25,0.5,1,2,4}. For LINE, we concatenate 1st-order and 2nd-order representations as the final node embeddings to obtain better results, and the negative sampling ratio is set as 5. For SDNE, we consider values of the parameter α in {0,0.1,0.2,0.3,0.4} and β in {1,5,10,20,30}. The other parameters of the baselines are set according to the authors’ preferred values.

Evaluation Metrics. Following a common way to assess the community detection results, we evaluate the performance of different algorithms by three widely used metrics: Normalized Mutual Information (NMI), Clustering Accuracy (ACC), and Adjusted Rand Index (ARI) [40]. The higher these metric values are, the better the community detection performance is. Note that two traditional methods, CNM and Louvain, cannot specify the number of desired communities. Thus, the results of them cannot be evaluated by ACC due to mismatching of the number of communities.

4.2.2 Experimental Results
The experimental results in terms of NMI, ARI, and ACC values are summarized in Tables 3, 4, and 5 respectively.11 By comparing the performance of LookCom and other methods, we can make the following observations from the tables:

Generally, our proposed framework LookCom outperforms all the compared baselines across different evaluation metrics except for NMI on BlogCatalog. In particular, LookCom improves the best baseline method by relatively 1.2 to 4.5 percent in NMI scores, 1.7 to 25.7 percent in ARI scores and 2.0 to 5.3 percent in ACC scores. The reason is that in real-world networks, the importance of edges often vary remarkably over the full spectrum while existing methods fail to characterize the tie strength information for community detection. Additionally, the existence of noisy links may further jeopardize finding meaning community structure. In contrast, our model LookCom provides a general way to adaptively estimate the edge strength between nodes, and alleviate the adverse effects of noisy edges to make the boundary between different communities more clear.

The recent proposed embedding based approaches, such as DeepWalk, node2vec, and GraRep, achieve better community detection performance than the traditional methods in most cases, especially on large datasets (e.g., DBLP and PubMed). The result can be attributed to the fact that the embedding learning process can better preserve the node proximity for community detection while traditional methods heavily rely on predefined measures and these measures are not flexible enough to be generalized to different kinds of networks.

In addition, we can notice that, in BlogCatalog, node2vec is slightly better than our model LookCom under the NMI score. A possible reason is that the number of edges in BlogCatalog is much larger than that of other networks, leading to a high average node degree. In this case, it is hard for our model, under the constraint of ∑vj∈N(vi)wij=1, to find the difference between different edges on a node. However, despite this situation, our method is still comparable to the best baseline method on the NMI score and outperforms it on the ACC and ARI scores.

TABLE 3 Summary of Results in Terms of NMI
Table 3- 
Summary of Results in Terms of NMI
TABLE 4 Summary of Results in Terms of ARI
Table 4- 
Summary of Results in Terms of ARI
TABLE 5 Summary of Results in Terms of ACC
Table 5- 
Summary of Results in Terms of ACC
In order to show the community structure information within the learned representations, we map the embedding vectors of the nodes to a two-dimensional space with visualization tool t-SNE [41]. In Fig. 3, we show representative visualization results on the DBLP network. As can be observed, the result of SDNE has no clear community structures because nodes belonging to different communities are mixed together, and GF and M-NMF have similar results.12 The results of LINE and ComE are a little better, but they are still not satisfactory. For GraRep, it fails to cluster the red class (NLP) together, which is similar to the result of HOPE. The result of DeepWalk is much better, which is similar to node2vec and LE. However, the boundaries of each community are not very clear. Obviously, the LookCom performs the best which has the right community partition and clear boundaries.

Fig. 3. - 
Visualization of the learned representations on DBLP (better viewed in color). Each point indicates one paper. Different point colors indicate different communities: red is NLP, orange is Data Mining, cyan is Database, blue is Networking and purple is Computer Vision.
Fig. 3.
Visualization of the learned representations on DBLP (better viewed in color). Each point indicates one paper. Different point colors indicate different communities: red is NLP, orange is Data Mining, cyan is Database, blue is Networking and purple is Computer Vision.

Show All

4.2.3 Parameter Sensitivity Study
We show how the dimensions of the learned representation vector affect the performance in Fig. 4. In each figure, we tune the dimensions within the set of {32,64,128,256,512}, and test on four datasets. It is evident that the performance first improved in terms of both ACC, NMI, and ARI, as the number of dimensions increase. However, when the number of dimensions further increases, the performance then degrades. The possible reason of this phenomenon is that large embedding dimensions will encode more noisy information, which in turn affect the community detection results. Empirically, the community detection performs the best when the dimension is set as around 64.


Fig. 4.
Sensitivity w.r.t.dimensions.

Show All

4.2.4 Efficiency Study
To analyze the efficiency, we perform LookCom with default parameter values (α=1,λ=10,m=64) on LFR benchmarks having increasing sizes from 4,000 to 10,000 nodes and a constant average degree of 20. In Fig. 5, we show how the average runtime of the first five epochs scales with the number of nodes of the networks. The curve shows a linear relation between the runtime and the node number. Therefore, LookCom is efficient and can process large networks in a reasonable time.

Fig. 5. - 
Efficiency of LookCom on LFR benchmarks with an average degree of 20.
Fig. 5.
Efficiency of LookCom on LFR benchmarks with an average degree of 20.

Show All

SECTION 5Related Work
5.1 Community Detection
The goal of community detection is to divide nodes in a network into different groups. Early methods for community detection on networks largely focused on modularity maximization, which are originally introduced by Girvan and Newman [9], [42]. These approaches are based on the definition of quality function modularity, which is by far the most used measure to estimate the quality of a partition of the network. However, it has been proved that modularity optimization is an NP-hard problem [43], and thus a number of variants have been proposed to solve the modularity optimization problem within a reasonable amount of time. For example, Clauset-Newman-Moore (CNM) [27] greedy modularity maximization algorithm considers the sparsity of the adjacency matrix and the change in the modularity to accelerate the update process of modularity optimization, which allows performing community analysis for large-scale networks. Louvain [28] is a heuristic algorithm to find fairly good approximations of the modularity maximum, which is popular in practice because it is fast enough to analyze very large-scale networks (over 108 nodes).

In addition to the modularity based approaches discussed above, many other types of algorithms have been proposed recently. For example, one category is spectral algorithms that are based on spectral analysis of the graph [44], [45]. Most spectral approaches are closely related to spectral clustering techniques, which use the eigenvectors of the adjacency matrix [46] and the Laplacian matrix [47] for graph partitions. Another relatively new category of community detection methods is based on label propagation [48]. The basic idea behind these approaches is to propagate labels throughout the network and form communities following this process of label propagation. Recently, Parés et al. [30] introduced a novel propagation based algorithm, called FluidC, which enables efficient community detection with the idea of fluids interacting in an environment.

So far all of the community detection methods the above mentioned only focus on the network structure. However, a real-world network sometimes contains node attributes (i.e., attributed network) or different types of nodes and edges (i.e., heterogeneous network), and a number of works have introduced strategies to cope with these networks. In the case of attributed networks, a general strategy is to turn the node attribute information into link information, and then classical community detection methods can be applied [49], [50]. And in the case of heterogeneous networks, current methods are often developed to integrate multiple relational information to a single view for finding out the common structure by homogeneous network community detection methods [51].

However, most of the methods outlined above often rely on various measures of the community structure, such as modularity or its variants, which are limited because these measures are inflexible and cannot be generalized to different types of networks with distinct properties. Therefore, a data-driven approach is often more desired in practice. Fortunately, network embedding offers a new paradigm to address this issue, and we will show a brief review of these methods as below.

5.2 Network Embedding
Recently, with the prevalence of network embedding in many applications, it presents us as a potent tool for community detection [12], [52], [53]. Compared with traditional community detection approaches, methods based on network embedding are more flexible to encode the community structure information from the network through the node embedding representation learning. Furthermore, as network structural information can be mapped into node real-valued vector representations, researchers can easily combine structural information with node attribute information in attributed networks, or integrate multiple relational information in heterogeneous networks.

The pioneer of network embedding can be dated back to the early 2000s when many network embedding techniques were proposed for dimensionality reduction [31], [54], [55]. These methods aim to learn first-order proximity preserving node representations in low-dimensional space by matrix factorization techniques. One of the earliest, Laplacian Eigenmaps (LE) [31] is a well-known instance, which uses eigenvectors of the graph Laplacian matrix as the node representations. Based on these earlier works, there is an important stream of work that learn the node embeddings by matrix factorization techniques. For example, GF [32] proposes to factorize the adjacency matrix of the network, which is optimized by stochastic gradient descent. GreRep [36] considers high-order proximity using different powers of the adjacency matrix. HOPE [35] is capable of preserving high-order proximity by factorizing high-order similarity matrix and capturing the asymmetric transitivity for directed network embedding.

Another category of work is built upon random walk statistics, inspired by word representation learning [56], [57]. These approaches aim to learn node embeddings such that the frequently co-occurred nodes on short random walks tend to be represented closely in the embedding space. Among them, Deepwalk [37] and node2vec [38] are highly successful, which have superior performance in a number of settings [58]. Deepwalk preservers high-order proximity between nodes by learning node embeddings with Skip-Gram [57] model from the random walks. Node2vec is similar to Deepwalk with the exception that it explores a flexible method to generate random walks that provide a trade-off between breadth-first search (BFS) and depth-first search (DFS). In addition to the above mentioned algorithms, some other approaches carefully design an objective function for the embedding learning task. For example, LINE [33] optimizes two loss functions derived from the Kullback-Leibler (KL) divergence to preserve first-order and second-order proximity respectively. SDNE [34] employs a deep neural network architecture to better capture network structural information and generate node embeddings by non-linear functions.

However, these methods may not perform well on community detection benchmarks, since they are independent of the community detection task during the representation learning process. Recently, there have been some community-oriented embedding methods, such as M-NMF [13] and ComE [39], but they still focus on the observed coarse network topology, which inevitably returns poor community detection results. While our LookCom generates node representations using an optimal network structure which is specially designed for community detection, which allows encoding more information tailored for the task contributes to improving the task performance.

SECTION 6Conclusion
In this study, a novel joint learning framework LookCom is proposed for embedding based community detection. In particular, the joint framework learns the expressive node embeddings and optimal network structure from the coarse original network, in a way the obtained node embeddings will be more customized to unravel the community structure embedded in the network. By seeking for the optimal network, the proposed algorithm can better characterize the fine-grained tie strength information among connected nodes and alleviate the negative impacts of noisy links for community detection. The joint modeling framework is formulated as an optimization problem and an effective alternating optimization algorithm is developed to facilitate the optimization. Experimental results on both synthetic networks and various types of real-world networks show that the proposed LookCom framework outperforms the traditional community detection methods and the state-of-the-art embedding based approaches. Future work can be focused on extending the LookCom to perform overlapping community detection.