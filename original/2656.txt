Cross-database micro-expression recognition (CDMER) is one of recently emerging and interesting problem in micro-expression analysis. CDMER is more challenging than the conventional micro-expression recognition (MER), because the training and testing samples in CDMER come from different micro-expression databases, resulting in inconsistency of the feature distributions between the training and testing sets. In this paper, we contribute to this topic from three aspects. First, we establish a CDMER experimental evaluation protocol aiming to allow the researchers to conveniently work on this topic and evaluate their proposed methods under the same standard. Second, we conduct benchmark experiments by using NINE state-of-the-art domain adaptation (DA) methods and SIX popular spatiotemporal descriptors for investigating CDMER problem from two different perspectives. Third, we propose a novel DA method called region selective transfer regression (RSTR) to deal with the CDMER task. The overall superior performance of RSTR over the state-of-the-art DA methods demonstrates that taking into consideration the facial local region information used in RSTR contributes to developing effective DA methods for dealing with CDMER problem.
SECTION 1Introduction
Micro-expression is one involuntary facial expression whose duration is usually within 0.5 second [1]. In 1966, Haggard and Isaacs [2] first discovered micro-expressions during their research of ego mechanisms in psychotherapy. Subsequently, Ekman et al. [1] found this type of facial expressions again when they observed a video of a psychotic patient and formally named them micro-expressions. Different from ordinary facial expressions, micro-expressions happen as a result of conscious suppression or unconscious repression. They can be viewed as a “leakage” often occurring on someone’s face when that person tries to conceal a genuine emotion. In other words, people without highly professional training cannot hide micro-expressions and thus micro-expression can usually expose people’s true emotions [3]. For this reason, understanding micro-expressions has great values in lots of practical applications, e.g., criminal investigation [4], human-robot interaction [5], [6], and clinical diagnosis [7]. An interrogator who is good at recognizing micro-expressions is able to spot the discrepancies between what he/she hear and what he/she see from the criminal suspect.

Unfortunately, without proper training most people cannot recognize micro-expressions in real time. In order to lower this barrier, researchers from computer vision and affective computing community have been focusing on automatic micro-expression recognition (MER) techniques and proposed various methods [8], [9], [10], [11], [12], [13], [14], [15], [16]. For example, Pfister et al. [8] adopted the local binary pattern from three orthogonal planes (LBP-TOP) [17] to describe micro-expressions and demonstrated the effectiveness of spatiotemporal descriptors in MER. Since then, lots of excellent spatiotemporal descriptors have been designed for describing micro-expressions, e.g., LBP with six intersection points (LBP-SIP) [9], facial dynamics map (FDM) [14], and fuzzy histogram of optical flow orientation (FHOFO) [15]. Besides the study of spatiotemporal descriptors, some important cues of micro-expression samples have been explored for developing effective MER methods. In the work of [10], Wang et al. investigated whether the color information is beneficial for MER by proposing a tensor independent color space (TICS) method to decompose micro-expression samples into different color channels. Inspired by the work of facial action coding system (FACS)1 [18], Wang et al. [10] and Liu et al. [11] respectively designed a set of Region-of-Interests (ROIs) which covers one or more micro-expression aware action unit (AU) regions and leveraged these AU region information to deal with MER problem. Recently, deep learning techniques are also used for handling MER. For instance, Kim et al. [12] made use of well-performing convolutional neural networks (CNNs) [19] and long short-term memory (LSTM) recurrent neural networks [20] to design a straightforward feature learning network for MER tasks.

Although the above MER methods have achieved promising performance, it should be pointed out that the above methods are still far from the requirements of a high-quality MER system. One major reason is that existing MER methods are mostly designed and evaluated without the consideration of the complex scenarios encountered by the MER system in practice. For example, the training and testing micro-expression samples provided for MER system may be recorded by different cameras (e.g., high-speed camera versus near-infrared camera) or under different environments (e.g., normal illumination versus weak illumination). In this scenario, the performance of the above MER methods may sharply drop due to the largely different feature distributions existing between the training and testing micro-expression samples caused by the heterogeneous video qualities. It thus brings us a new topic in micro-expression analysis, i.e., cross-database micro-expression recognition (CDMER), in which the training and testing samples come from two different micro-expression databases collected by different cameras or under different environments. CDMER offers a good way to mimic the scenarios the MER system would encounter in reality. Therefore, it is worthy to deeply investigate this challenging topic.

Prior to the research of MER, the cross-database emotion recognition problems have been extensively studied in many other modalities such as speech emotion recognition (SER) [21], [22], [23], [24], facial expression recognition (FER) [25], [26], image emotion recognition [27], [28], EEG emotion recognition [29], [30] and sentiment analysis [31]. However, there are several limitations existing in current cross-database emotion recognition research. First, there is a lack of standard evaluation protocols in cross-database emotion recognition like Office-10 [32] in domain adaptation (DA) [33], [34], [35]. Researchers often choose their preferred experiment materials including emotion databases, emotion features, classifiers, and evaluation metrics to set up their own experimental evaluation protocol, which can clearly be seen from the above representative works, e.g., [21], [22], [23], [24]. It thus raises the barriers of entry to this topic for new researchers because it would cost a lot of time to prepare evaluation protocol. Second, it can be found that cross-database emotion recognition was purely viewed as a DA task in most of the existing works. However, it should be noted that cross-database emotion recognition including CDMER is not a simple DA problem which aims at making use of machine learning methods to alleviate the feature distribution difference between the training and testing sets. Designing robust (database-invariant) features used for describing emotions is also a feasible way to cope with cross-database emotion recognition problem, which guide us to make effort to address CDMER in a different direction.

Based on the above considerations, in this paper we investigate the CDMER problem to break through the above two limitations widely existing in cross-database emotion recognition research. To this end, we made three contributions in this paper as follows:

We build a CDMER experimental evaluation protocol and design a set of CDMER experiments based on two public available micro-expression databases, which can be served as a standard platform for evaluating the CDMER methods from not only the perspective of DA but also the micro-expression features.

We use NINE representative DA methods and SIX spatiotemporal descriptors used for describing micro-expressions to conduct extensive benchmark evaluation experiments under the designed protocol and deeply discuss the experimental results.

To deal with CDMER problem, we also propose a novel DA method called region selective transfer regression (RSTR), which takes the facial local region information of micro-expressions into full consideration.

It should be pointed out that this work is the extended version of our previous conference paper [36]. The major motivation of this work is to attract and encourage more researchers to join this challenging but interesting topic. For this reason, all the data and codes involving CDMER in this paper can be downloaded from our project website: http://aip.seu.edu.cn/cdmer. We hope that our work can help researchers to start CDMER research and conveniently test the performance of their proposed DA or spatiotemporal descriptors in dealing with CDEMR.

The rest of this paper is organized as follows: Section 2 describes the details of the CDMER benchmark including the designed experimental evaluation protocol and the evaluated methods. Section 3 introduces the proposed RSTR. The experimental results are reported and discussed in Section 4. Finally, the paper is concluded in Section 5.

SECTION 2Benchmark Detail
As described previously, most existing cross-database emotion recognition problems including CDMER are often viewed as a DA task and solved by DA methods. In this way, followed by feature extraction, DA technique is used to relieve the feature distribution mismatch between the source and target micro-expression samples. Then, we are able to learn a classifier based on the labeled source micro-expression database to predict the micro-expression categories of samples from target database. A picture is drawn to illustrate the detailed process of using DA methods to deal with the CDMER problem, which is shown following the sequence of the orange dash arrow line in Fig. 1. It should be pointed out that besides DA solution, designing robust (database invariant) micro-expression features is also an effective way to solve CDMER problem. By resorting to well-designed robust micro-expression features, CDMER can be actually solved as a typical pattern recognition task which only needs two major steps including feature extraction and classifier learning. We draw its detail in what the green dash arrow directs in Fig. 1. Following the ideas of these two solutions for CDMER, we would like to design a CDMER evaluation protocol which can be served as a standard platform used for evaluating both DA methods and micro-expression features, respectively.

Fig. 1. - 
An illustration of how to solve the CDMER problem from the perspectives of DA methods and micro-expression features, respectively. The DA solution for CDMER problem is demonstrated as the direction of orange dash line. What the green dash line directs shows the solution from the micro-expression feature perspective. Micro-expression feature solution targets at designing the robust features that are less sensitive to the database variance for describing micro-expressions. In this case, CDMER consists of two major steps, i.e., feature extraction and classifier learning. Apart from feature extraction and classifier learning, the DA solution has an additional step between the steps of feature extraction and classifier learning, i.e., leveraging the DA techniques to narrow the feature distribution gap between the source and target data in the original feature space.
Fig. 1.
An illustration of how to solve the CDMER problem from the perspectives of DA methods and micro-expression features, respectively. The DA solution for CDMER problem is demonstrated as the direction of orange dash line. What the green dash line directs shows the solution from the micro-expression feature perspective. Micro-expression feature solution targets at designing the robust features that are less sensitive to the database variance for describing micro-expressions. In this case, CDMER consists of two major steps, i.e., feature extraction and classifier learning. Apart from feature extraction and classifier learning, the DA solution has an additional step between the steps of feature extraction and classifier learning, i.e., leveraging the DA techniques to narrow the feature distribution gap between the source and target data in the original feature space.

Show All

2.1 A Standard Evaluation Protocol for CDMER
2.1.1 Data Preparation
Two publicly available spontaneous micro-expression databases are adopted for building the benchmark evaluation experiments, i.e., CASME II [37] and SMIC [38]. CASME II was built by Yan et al. from Institute of Psychology, Chinese Academy of Sciences. It consists of 257 micro-expression samples from 26 subjects. Among these 257 samples, the micro-expression label of one sample is not provided. Each of the other 256 samples is assigned one of seven micro-expression labels including Happy, Disgust, Repression, Surprise, Sad, Fear, and Others. Different from CASME II, Li et al. from University of Oulu, Finland considered the image quality diversity of micro-expression samples and hence employed three cameras, i.e., a high-speed (HS) camera, a normal visual (VIS) camera, and a near-infrared (NIR) camera, to collect three subsets to obtain the SMIC (HS, VIS, and NIR) database. The HS subset has 164 samples belonging to 16 subjects and 71 samples of eight subjects from these 16 subjects compose VIS and NIR subsets. All the samples in SMIC are categorized into three types of micro-expressions, i.e., Positive, Negative, and Surprise. To make CASME II and SMIC have the same micro-expression labeling, we select the samples of Happy, Disgust, Surprise, Sad, and Fear from CASME II and relabel them according to the labeling rule in SMIC, where Happy samples are given the Positive labels, Disgust, Sad, and Fear samples are relabeled with Negative micro-expression, and the labels of Surprise sample keep unchanged. The sample constitution with respect to consistent categories of the selected CASME II and SMIC databases is shown in Table 1.

TABLE 1 The Sample Constitutions of the Selected CASME II and SMIC Databases With the Same Micro-Expression Labels for CDMER

2.1.2 CDMER Tasks
Following our preliminary works of [39], [40], we design two kinds of CDMER tasks based on the selected CASME II and SMIC databases for our CDMER protocol. The first type of tasks denoted by TYPE-I is the one between either two datasets of SMIC (HS, VIS, and NIR). The second type of tasks chooses the selected CASME II and one dataset of SMIC (HS, VIS, and NIR) to serve as source and target micro-expression databases, alternatively, which is denoted by TYPE-II. This leads to totally 12 CDMER experiments and one CDMER task is denoted by Exp.i:S→T, where Exp.i is the index number of this experiment and S and T are the source and target micro-expression databases, respectively. We summarize all the CDMER experiments in the designed protocol in Table 2.

TABLE 2 The Detailed Information of Two Types of CDMER Tasks in the Designed Evaluation Protocol
Table 2- 
The Detailed Information of Two Types of CDMER Tasks in the Designed Evaluation Protocol
2.1.3 Performance Metrics
In our previous works of [39], [40], weighted average recall (WAR) and unweighted average recall (UAR) are employed to serve as the performance metrics, where WAR is the normal recognition Accuracy while UAR is the mean accuracy of each class divided by the number of the classes without the consideration of sample number of each class. The main reason of introducing UAR is due to the class imbalanced problem which widely exists in CASME II and SMIC databases. As shown in Table 1, the number of Negative samples in the selected CASME II is 73, which is significantly larger than the numbers of the remaining two types of micro-expression samples (32 for Positive and 25 for Surprise).

Mean F1-score is another recommended metric, which has been widely used to avoid the bias in performance measurement caused by the class imbalanced problem in MER literatures [16], [41], [42], [43], [44]. For this reason, in our benchmark, we adopt the combination of mean F1-score and Accuracy to serve as the metrics, where mean F1-score is the main metric and recognition accuracy as the secondary one. The mean F1-score is calculated according to mean F1-score=1c∑ci=12pi×ripi+ri, where pi and ri mean the precision and recall of the ith micro-expression, respectively, and c is the number of micro-expressions. The Accuracy is calculated by Accuracy=TN×100, where T and N are the number of correct predictions and the number of target micro-expression samples.

2.1.4 Preprocessing and Feature Extraction
Before feature extraction, preprocessing operations, e.g., face alignment and face cropping, are performed on the micro-expression samples. For convenience, we directly adopted the image sequence data preprocessed by the collectors of CASME II and SMIC for the benchmark evaluation experiments. Then, we employ the temporal interpolation model (TIM) [45] to normalize the frame number of all the micro-expression video clips to 16 and resize each frame image to 112×112, which allows us to extract spatiotemporal descriptor with specific parameter settings for micro-expression samples.2 Furthermore, we compute the multi-scale spatiotemporal descriptors using four types of spatial grids (1×1,2×2,4×4,8×8) shown in Fig. 2 to serve as the micro-expression features. This is expected to extensively cover micro-expression related facial local regions and increase the discriminative power of the extracted spatiotemporal descriptor [16]. As Fig. 2 shows, given a micro-expression sample M, the spatiotemporal descriptor corresponding to each facial block denoted by xi (i=1,…,K), where the facial block number K=85, is first extracted one by one and then compose the final micro-expression feature vectors, which can be formulated as x=[x1,…,xK]T.

Fig. 2. - 
Multi-scale grid based spatial division scheme for micro-expression feature extraction used in the benchmark.
Fig. 2.
Multi-scale grid based spatial division scheme for micro-expression feature extraction used in the benchmark.

Show All

As described previously, we attempt to investigate CDMER problem from two different perspectives including domain adaptation (DA) and micro-expression feature extraction. By using effective DA methods and micro-expression features, the large feature distribution difference between the source and target micro-expression databases in CDMER would be relieved. Hence, in our benchmark, we will respectively evaluate the performance of state-of-the-art DA methods and spatiotemporal descriptors used for describing micro-expressions in all the above 12 designed CDMER experiments. Note that in the experiments of DA methods, we suggest to employ a baseline spatiotemporal descriptor, uniform LBP-TOP [17] with fixed parameters (neighboring radius R and number of the neighboring points P for LBP operator on three orthogonal planes are fixed at 3 and 8, respectively), and spatial division shown in Fig. 2 to serve as the micro-expression feature vector xv. In the case of using various features, we extract different xv corresponding to different types of spatiotemporal descriptors used for describing micro-expression and then conduct CDMER experiments.

2.1.5 Classifier
To offer a fair comparison, the linear support vector machine (SVM) is suggested for serving as the classifier in our benchmark. Without specific description, LibSVM [46] is used in the implementation of SVM for both micro-expression features and DA evaluation experiments.

2.2 Evaluated Methods
2.2.1 DA Methods
For the evaluation of methods for dealing with CDMER from the perspective of DA, the following baseline and state-of-the-art unsupervised DA methods are employed.

Baseline (SVM Without DA) [46]. A SVM without any DA is served as the baseline method. In the evaluation experiments, we directly learn the linear SVM on the source micro-expression database and then use it to predict the micro-expression labels of samples from the target database.

IW-SVM [47]. Importance-weighted SVM (IW-SVM) was proposed by Hassan et al. to deal with cross-database speech emotion recognition tasks. In this method, a transfer learning method, e.g., kernel mean matching (KMM) [48], unconstrained least-squares importance fitting (uLSIF) [49], and Kullback-Leibler importance estimation procedure [50], is first used to learn a group of importance weights for source samples and then these weights are incorporated into the SVM classifier to eliminate the feature distribution difference between the source and target samples.

TCA [51]. Transfer component analysis (TCA) was proposed by Pan et al., which is to seek some transfer components across domains in a reproducing kernel Hilbert space (RKHS). By using these transfer components, a subspace can be spanned in which the sample distributions from different domains would be close to each other.

GFK [52]. Geodesic flow kernel (GFK) was proposed by Gong et al. and has been a widely-used baseline comparison method in DA research. GFK aims to bridge two domains and narrow their gaps with a well-designed geodesic flow kernel on a Grassmann manifold.

SA [53]. Subspace alignment (SA) is another popular unsupervised DA method and has been served as the baseline comparison method in lots of DA literatures. The target of SA method is to seek a mapping function which can align the subspace the source samples lie in with respect to the target ones.

STM [54], [55]. Selective transfer machine (STM) was originally proposed to cope with personalized facial action unit (AU) detection problem. STM makes use of an instance-wise weighted SVM to model the relationship between the training samples and its AU information and meanwhile KMM to eliminate the difference between the AU samples from testing subject and training subjects.

TKL [56]. Long et al. proposed a novel unsupervised DA method called transfer kernel learning (TKL), which aims to learn a domain invariant kernel for eliminating the feature distribution mismatch between the source and target domains.

TSRG [39]. TSRG is short for target sample re-generator and was proposed to deal with CDMER problem. The aim of TSRG is to learn a sample regenerator for the target micro-expression samples and the feature distribution gap between the source and target micro-expression databases would be narrowed after the regeneration operation.

DRFS-T [40]. TSRG is further extended to a generalized framework called domain regeneration (DR), which inherits the basic idea of TSRG. DRFS-T is one new designed sample regenerator under the DR framework which means domain regeneration in the original feature space with unchanged target samples and it shares the similar idea with TSRG. Their only difference is that DRFS-T keeps the target samples unchanged and regenerates the source samples to have the same or similar feature distributions of target samples, while TSRG is opposite.

DRLS [40]. DRLS is another newly designed sample regenerator based on the DR framework. Different from TSRG and DRFS-T, the subspace used in performing regeneration in DRLS is the label space spanned by the label information provided in the source micro-expression database instead of original feature space.

2.2.2 Micro-Expression Features
For exploring the performance of different existing micro-expression features in coping with CDMER problem, we collect following FIVE representative handcrafted spatiotemporal descriptors and ONE deep spatiotemporal descriptor to conduct the benchmark evaluation experiments:

LBP-TOP [17]. LBP-TOP is a spatiotemporal extension of LBP by performing LBP coding on three orthogonal planes. It is originally proposed to deal with dynamic texture recognition tasks and recently has been widely used for describing micro-expressions [8], [10], [11], [16].

LBP-SIP [9]. LBP-SIP is short for LBP with six intersection points. Wang et al. designed it in order to reduce the redundancy in LBP-TOP patterns and provided a more compact and lightweight representation. Compared with LBP-TOP, LBP-SIP uses six intersection points in the intersection lines surrounding the center points for LBP coding and hence its computational complexity is significantly reduced.

LPQ-TOP [57]. Following the manner of LBP-TOP, local phase quantization (LPQ) [58], which quantifies the Fourier transform phase in local neighborhoods, is also extended to the spatiotemporal version called LPQ from three orthogonal planes (LPQ-TOP).

HOG-TOP [59]. Histograms of oriented gradients (HOG) [60] is earliest proposed for human detection and subsequently applied on lots of vision tasks. In the work of [59], Li et al. extends HOG to a 3D version called HOG-TOP, which borrows the basic idea of LBP-TOP.

HIGO-TOP [59]. Histogram of image gradient orientation (HIGO) was proposed in the work of Li et al. [59] by degenerating HOG. Compared with HOG, HIGO simply uses vote rather than weighted vote in counting the responses of the histogram bins. As the name suggests, HIGO-TOP is the 3D extension of HIGO by using the manner of LBP-TOP.

C3D [61]. Recently, the research of spatiotemporal deep feature learning models have made great progress. Three-dimensional convolutional neural network (C3D) is one of excellent representatives and has gained promising performance in video based action recognition tasks. C3D can be actually viewed as a 3D extension of VGG network [62], which replaces 2D convolution and pooling operations with 3D ones.

Besides the above state-of-the-art DA methods and spatiotemporal descriptors, in this paper we also propose a novel DA method called region selective transfer regression (RSTR) for dealing with CDMER problem and evaluate its performance. The major advantage of the proposed RSTR is that we further consider the different contributions of the facial local regions in the design of RSTR. The detailed information of RSTR including the formulation and optimization is given in Section 3.

SECTION 3Region Selective Transfer Regression
3.1 Formulation
Let Xs=[XsT1,…,XsTK]T∈RKd×Ns and Xt=[XtT1,…,XtTK]T∈RKd×Nt be the micro-expression feature matrices of the samples belonging to source and target micro-expression databases, where each column in Xs and Xt is the feature vector like x shown in Section 2.1.4, K is the number of the divided facial blocks, Ns and Nt are the source and target sample numbers, and d is the dimension of the spatiotemporal descriptor vector extracted from each facial blocks, respectively. According to the problem setting of CDMER, it is known that the label information of the source micro-expression database is provided while the target micro-expression samples are entirely unlabeled. We hence assign source micro-expression database with a label matrix Ls whose ith column lsi reveals the micro-expression category of its corresponding source micro-expression sample in Xs. lsi is a binary vector and the cth element is 1 if and only if it belongs to the cth micro-expression category.

Intuitively, we are able to use a simple linear regression model to build the relationship between the source features and their corresponding label information, which is formulated as follows:
minC∥Ls−CTXs∥2F,(1)
View Sourcewhere C is the regression coefficient matrix. It is clear that the learned regression coefficient matrix of the above problem can only suits the micro-expression recognition problem based on source database and cannot be applicable to the target micro-expression samples.

To overcome this shortcoming of the conventional regression model, we would like to extend it to a transfer regression model such that the learned regression coefficient matrix can also be suitable for the target micro-expression samples. To this end, we introduce the maximum mean discrepancy (MMD) [63], which measures the feature distribution distance between two data sets, for building our transfer regression model. MMD is defined as the distance between the centers of two different data sets in a kernel space and can be formulated as
MMD(Xs,Xt)=∥1NsΦ(Xs)1s−1NsΦ(Xt)1t∥H,(2)
View SourceRight-click on figure for MathML and additional features.where Φ is a kernel mapping operator. It had been proved that Φ(Xs) and Φ(Xt) would have the same or similar feature distributions if their MMD is close to 0 after arising dimension using Φ. By using MMD as the regularization term, we are able to arrive at our transfer regression model which has the following formulation:
minC,Φ∥Ls−Φ(C)TΦ(Xs)∥2F+γMMD(Xs,Xt),(3)
View SourceRight-click on figure for MathML and additional features.where γ is the trade-off parameter controlling the balance between the loss function and the MMD regularization term.

However, the optimization problem of Eq. (3) is a hard task because we cannot directly optimize the kernel mapping operator Φ. To solve this problem, we relax the MMD distance to the following one, which is designed in our previous work of [40]
fG=∥∥∥Φ(C)T[1NsΦ(Xs)1s−1NtΦ(Xt)1t]∥∥∥22.(4)
View SourceRight-click on figure for MathML and additional features.By resorting to fG, the optimization of kernel mapping operator Φ(⋅) in minimizing MMD is then changed to the optimization problem with respect to Φ(C). Thus, our transfer regression model in Eq. (4) can be formulated as
minΦ(C)∥∥Ls−Φ(C)TΦ(Xs)∥∥2F  +γ∥∥∥Φ(C)T[1NsΦ(Xs)1s−1NtΦ(Xt)1t]∥∥∥22.(5)
View SourceRight-click on figure for MathML and additional features.

Note that the transfer regression model in Eq. (5) is still a general DA method rather than an expert CDMER one because this model can be used to deal with any unsupervised DA problem. It can be also seen that any background knowledge of micro-expressions is not considered during its construction. In fact, in recent years various cues in micro-expression samples, e.g., the color information [10], facial local region information [16], and dynamic sparsity [43], have been explored to demonstrate their effectiveness in distinguishing different micro-expressions. Therefore, it would be beneficial to take these positive cues of micro-expressions into consideration when we design CDMER methods. To this end, we would like to leverage the idea of the facial local region selection in our previous work of [16] to enhance our transfer regression model in dealing with CDMER problem.

Specifically, let ϕ(⋅) be another kernel mapping operator that has the relationship with Φ(⋅), i.e., Φ(X)=[ϕ(X1)T,…,ϕ(XK)T]T. Then, the regression coefficient matrix Φ(C) can be rewritten as Φ(C)=[ϕ(C1)T,…,ϕ(CK)T]T and hence we are able to reformulate the optimization problem of Eq. (5) as follows:
minϕ(Ci)∥∥∥∥Ls−∑i=1Kϕ(Ci)Tϕ(Xsi)∥∥∥∥2F+γ∥∥∥∥∑i=1Kϕ(Ci)T[1Nsϕ(Xsi)1s−1Ntϕ(Xti)1t]∥∥∥∥22.(6)
View Source

In order to pick out the micro-expression related local facial regions and leverage them to enhance our transfer regression model in Eq. (6), we endow a non-negative weighted parameter wi to the ith facial block Xsi and Xti to measure its specific contributions to distinguish different micro-expresisons. To achieve this goal, we use a L1-norm with respect to w=[w1,…,wK]T to serve as the regularization term for our transfer regression model and then we arrive at the following new formulation:
minϕ(Ci),w∥∥∥∥Ls−∑i=1Kwiϕ(Ci)Tϕ(Xsi)∥∥∥∥2F+λ∥w∥1  +γ∥∥∥∥∑i=1Kwiϕ(Ci)T[1Nsϕ(Xsi)1s−1Ntϕ(Xti)1t]∥∥∥∥22, s.t. w⪰0.(7)
View SourceRight-click on figure for MathML and additional features.

To differentiate this new transfer regression model with traditional ones, we call it Region Selective Transfer Regression (RSTR). Via the kernel trick, the explicit mapping ϕ(⋅) in RSTR can be effectively avoided and further optimized. According to the kernel representation theory [64], we obtain that Φ(C) can be expressed as Φ(C)=[Φ(Xs),Φ(Xt)]P resulting in ϕ(Ci)=[ϕ(Xsi),ϕ(Xti)]P, where P is a coefficient matrix. We can then arrive at the final proposed RSTR, which has the formulation as follows:
minP,w∥∥∥∥Ls−PT∑i=1KwiKsi∥∥∥∥2F+λ∥w∥1+μ∥P∥1+γ∥∥∥∥∑i=1KwiPT(1NsKsi1s−1NtKti1t)∥∥∥∥22, s.t. w⪰0,(8)
View Sourcewhere λ, μ, and γ are the trade-off parameters which control the balance between the loss function of RSTR and the regularization terms, and Ksi=[ϕ(Xsi)Tϕ(Xti)T]ϕ(Xsi) and Kti=[ϕ(Xsi)Tϕ(Xti)T]ϕ(Xti) and can be computed by using the preset kernel functions, e.g., linear, polynomial, and gaussian kernels. Note that following [16], we also add a L1-norm of P, i.e., ∥P∥1=∑ci=1∥pi∥1, where pi is the ith column of P, for Eq. (8) as the regularization term such that the regression coefficient matrix ϕ(Ci) can be sparsely reconstructed by [ϕ(Xsi),ϕ(Xti)] and the overfitting during the optimization can be avoided.

3.2 Optimization
Before optimization of RSTR, we should choose a kernel function, e.g., ker(x,y)=xTy (linear kernel) and ker(x,y)=(axTy+b)c (polynomial kernel), where x and y denote feature vectors and a, b, and c are kernel parameters, to compute the source and target kernel matrices Ksi and Kti in Eq. (8). Then, RSTR can easily be solved by the alternated direction method (ADM) [65], i.e., fixing one parameter and updating the other one until convergence. More specifically, repeat the following three steps:

3.2.1 Fix w and Update P
In this step, the optimization problem with respect to P can be written as
minP∥Ls−PTK~s∥2F+μ∥P∥1+γ∥PTk~st∥22,(9)
View Sourcewhere K~s=∑Ki=1wiKsi and k~st=∑Ki=1wi(1NsKsi1s−1NtKti1t). In this paper, we adopt the inexact augmented Lagrangian multiplier (IALM) approach [66] to learn the optimal P in Eq. (9). More specifically, we introduce an auxiliary variable Q that equals P to convert the unconstrained optimization problem in Eq. (9) to a constrained one as follows:
minP,Q∥Ls−QTK~s∥2F+μ∥P∥1+γ∥QTk~st∥22,  s.t. P=Q.(10)
View SourceRight-click on figure for MathML and additional features.Subsequently, we can obtain the Lagrangian function of Eq. (10), which has the following formulation:
L(P,Q,T,κ)=∥Ls−QTK~s∥2F+μ∥P∥1+γ∥QTk~st∥22+tr(TT(P−Q))+κ2∥P−Q∥2F,(11)
View SourceRight-click on figure for MathML and additional features.where κ is a trade-off parameter and T is the Lagrangian multiplier matrix. We only need to iteratively minimize the Lagrangian function in Eq. (11) with respect to its different variables and then the optimal P can be learned. We summarize the complete updating procedures in Algorithm 1.

3.2.2 Fix P and Update w
minw∥zs.t. w⪰0.−Aw∥22+γ∥Bw∥22+λ∥w∥1,  (12)
View SourceRight-click on figure for MathML and additional features.

Herein, z is a vector composed by concatenating the columns of Ls one by one, and the ith columns of A and B are a vector by vectorizing their corresponding matrices PTKsi, and PT(1NsKsi1s−1NtKti1t), respectively. Eq. (12) can be further rewritten as the following standard Lasso problem
minw∥y−Dw∥22+λ∥w∥1, s.t. w⪰0,(13)
View SourceRight-click on figure for MathML and additional features.where y=[z0] and D=[AB]. In our optimization, we use the SLEP package [67] to learn the optimal w.

3.2.3 Check Convergence
Check whether the preset maximal iteration steps are reached or the value of the objective function in Eq. (8) is less than the preset threshold.

Algorithm 1. Complete Updating Rule for Learning the Optimal P in Eq. (9)
Repeating Steps 1) to 4) until obtaining convergence:

1) Fix P, T, and κ and update Q:

minQ∥Ls−QTK~s∥2F+γ∥QTk~st∥22+tr(TT(P−Q))  +κ2∥P−Q∥2F,
View SourceRight-click on figure for MathML and additional features.

which has the close-form solution as follows:

Q=(K~sK~s+γ−−√k~stk~Tst+κ2I)−1(K~sLsT+T+κP2),
View Source

where I is an identity matrix.

2) Fix Q, T and κ and update P:
minPμκ∥P∥1+12∥∥∥P−(Q−Tκ)∥∥∥2F.
View Source

The solution of the above optimization problem is Sμκ[Q−Tκ], where Sζ[A] is the soft thresholding operator and is defined as:

Sζ[A]=⎧⎩⎨⎪⎪A−ζ,   if A>ζ;A+ζ,   if A<−ζ;0,         otherwise.
View Source

3) Update T and κ:

T=T+κ(P−Q),κ=min(ρκ,κmax),
View SourceRight-click on figure for MathML and additional features.

where κmax is a preset maximal value for κ and ρ is a scaled parameter and can be set as a value greater than 1.

4) Check convergence:

∥P−Q∥∞<ϵ,
View SourceRight-click on figure for MathML and additional features.

where ∞ norm means the maximal value of the element in a matrix and ϵ is the machine epsilon.



Algorithm 
Algorithm 

Algorithm 
3.3 RSTR for Solving CDMER Problem
Once the optimal parameters of RSTR are learned based on the labeled source and unlabeled target micro-expression samples using the optimization method given in Section 3.2, we can easily predict the micro-expression categories of the testing samples from target database. More specifically, suppose the learned optimal parameters of RSTR are P^ and w^. Then, given a testing target micro-expression sample xte we are able to estimate its micro-expression label vector by solving the following optimization problem:
minlte∥∥∥∥lte−P^T∑i=1Mω^iktei∥∥∥∥22,  s.t. lte⪰0, 1Tlte=1,(14)
View SourceRight-click on figure for MathML and additional features.where ω^i is the ith element of w^ and ktei=[ϕ(Xsi),ϕ(Xti)]Tϕ(xtei) is the kernel vector and can be computed by the kernel function chosen in training stage. Finally, the micro-expression category of this testing sample is assigned as the one whose corresponding value in lte is maximum.

SECTION 4Evaluation Results
4.1 Implementation Details
We conduct the benchmark CDMER experiments under the designed protocol described in Section 2. For the evaluated methods, we implement them using the original source codes provided by the authors (if available) or the codes written by ourselves. To offer a fair comparison, for the experiments of different micro-expression features, the parameters of each spatiotemporal descriptors are fixed throughout the experiments, while for the experiments of DA, we follow the strategy used in current mainstream unsupervised DA evaluation experiments that is reporting the best result (in term of mean F1-score in our CDMER benchmark experiments) corresponding to the optimal parameter setting for a DA method [39], [56], [68], [69], [70]. Subsequently, we show the detailed parameter settings of both micro-expression features and DA methods used for our benchmark experiments.

4.1.1 Parameter Setting for DA Methods
In this section, we give the parameter searching space for different DA methods in the CDMER evaluation experiments.

SVM [46]. To offer a fair comparison, we use linear kernel for SVM throughout the evaluation experiments. As for the penalized coefficient C, we fix it at C=1. Note that for the experiments of all the DA methods, we use the linear SVM with C=1 to serve as the classifier (if needed).

IW-SVM [47]. In our experiments, we choose uLSIF [49], which has shown its excellent performance in CDMER [39], [40], to learn the importance weights for IW-SVM. Following the suggestion of [39], [40], we search the trade-off parameter λ for uLSIF from a parameter space [1:1:100]×t (t = 1, 10, 100, 1000, 10000, 100000).

TCA [51], GFK [52], and SA [53]. Among the experiments of these three methods, principal component analysis (PCA) [71] is used to construct the subspace for GFK and SA. For all of them, we search the optimal dimension k (the number of eigenvectors for composing the projection matrix) by trying all possible dimensions, i.e., searching k∈[1,2,…,kmax].

STM [54], [55]. STM is originally a binary classification model. In our experiments, we extend it to a multi-class version by using one-against-rest strategy. Similar with SVM used in the benchmark evaluation, its penalized coefficient is set as C=1. As for its second trade-off parameter λ, which is used to balance the KMM regularization term with the SVM objective function, the searching space is set as [0.001:0.001:0.009,0.01:0.01:0.09,0.1:0.1:1,2:1:100,1000,10000].

TKL [56]. According to the work of [56], TKL has one important parameter called the eigenspectrum damping factor ζ. In the evaluation experiments, we determine its optimal value by searching from the parameter space [0.1:0.1:5].

TSRG [39], DRFS-T [40], and DRLS [40]. TSRG, DRFS-T, and DRLS have two important trade-off parameters, i.e., λ and μ. Following the works of [39], [40], the optimal values of these two parameters are determined by searching from [0.001,0.01,0.1,1,10,100,1000] for λ and [0.001:0.001:0.009,0.01:0.01:0.09,0.1:0.1:1,2:1:10] for μ.

RSTR. The proposed RSTR has three trade-off parameters including λ, μ, and τ which control the balance between the loss function and the regularized terms. We search their optimal values from the preset parameter spaces, i.e., λ∈[0.1,1,10,100,1000,10000], μ∈[0.1:0.1:5], and γ∈[0.01:0.01:0.1].

4.1.2 Parameter Setting for Micro-Expression Features
We set the parameters for the evaluated spatiotemporal descriptors as follows:

LBP-TOP [17]. LBP-TOP has two important parameters. One is the neighboring radius R and the other is the number of the neighboring points P. In the evaluation experiments, we set the values of these two parameters as the ones of R∈{1,3} and P∈{4,8}, respectively, and hence we report FOUR experimental results of LBP-TOP with different parameter setting, i.e., LBP-TOP(R1P4), LBP-TOP(R1P8), LBP-TOP(R3P4), and LBP-TOP(R3P8). In addition, the uniform pattern is used for LBP coding. We use Hong et al.’s fast LBP-TOP [72] source code to implement the LBP-TOP feature extraction.

LBP-SIP [9]. Only one parameter needs to be set for LBP-SIP, i.e., the neighboring radius R. Similar to LBP-TOP, we set R as 1 and 3 for LBP-SIP, respectively, and report the experimental results of LBP-SIP(R1) and LBP-SIP(R3). In the experiments, LBP-SIP is implemented by ourselves.

LPQ-TOP [57]. For LPQ-TOP, we set its parameters following the suggestion of [57]. Specifically, the size of the local window in each dimension is set as the default one ([5, 5, 5]). The parameters [ρs,ρt] for a correlation model used in LPQ-TOP are set as [0.1, 0.1] and [0, 0] (without correlation), respectively. The results of LPQ-TOPdecorr=0.1 and LPQ-TOPdecorr=0 are reported in the evaluation experiments. The source code of LPQ-TOP is publicly available at http://www.cse.oulu.fi/Downloads/LPQMatlab/.

HOG-TOP and HIGO-TOP [59]. HOG-TOP and HIGO-TOP both have one important parameter, i.e., the number of bins p to be set, which controls the dimensional histogram of image gradient orientations for three orthogonal planes. In the evaluation experiments, we fix p at 4 and 8 for HOG-TOP and HIGO-TOP and they are denoted by HOG-TOPp=4, HOG-TOPp=8, HIGO-TOPp=4 and HIGO-TOPp=8, respectively.

C3D [61]. In the evaluation experiments, we choose the publicly released C3D pretrained on Sports-1M [73] and UCF101 [74] as the feature extractor and use the last two fully connected layers to serve as the micro-expression features, which are denoted by C3D-FC (Sports-1M), C3D-FC2 (Sports-1M), C3D-FC1 (UCF101), and C3D-FC2 (UCF101), respectively.

4.2 Results and Discussions
4.2.1 Results at a Glance
In this section, we report the benchmark results of the evaluated methods including various DA methods and micro-expression features. All the experimental results are depicted in Tables 3, 4, 5, and 6. Before deeply comparing and analyzing the results obtained by different DA methods and micro-expressions, we would like to probe into the CDMER tasks based on the obtained results. We calculate the average results of all the methods in each experiment and all the experiments for each method, which are given in the last line and last column of these tables. From the comparison between them, we are able to reach the following conclusions.

TABLE 3 Experimental Results (Mean F1-Score / Accuracy) of Various Domain Adaptation Methods for CDMER, Where the Source and Target Databases are two Subsets of SMIC (HS, VIS, and NIR)
Table 3- 
Experimental Results (Mean F1-Score / Accuracy) of Various Domain Adaptation Methods for CDMER, Where the Source and Target Databases are two Subsets of SMIC (HS, VIS, and NIR)
TABLE 4 Experimental Results (Mean F1-Score / Accuracy) of Various Domain Adaptation Methods for CDMER, Where the Source and Target Databases are CASME II or One Subset of SMIC (HS, VIS, and NIR)
Table 4- 
Experimental Results (Mean F1-Score / Accuracy) of Various Domain Adaptation Methods for CDMER, Where the Source and Target Databases are CASME II or One Subset of SMIC (HS, VIS, and NIR)
TABLE 5 Experimental Results (Mean F1-Score / Accuracy) of Various Spatiotemporal Descriptors for CDMER, Where the Source and Target Databases are Two Subsets of SMIC (HS, VIS, and NIR)
Table 5- 
Experimental Results (Mean F1-Score / Accuracy) of Various Spatiotemporal Descriptors for CDMER, Where the Source and Target Databases are Two Subsets of SMIC (HS, VIS, and NIR)
TABLE 6 Experimental Results (Mean F1-Score / Accuracy) of Various Spatiotemporal Descriptors for CDMER, Where the Source and Target Databases are CASME II or one Subset Subset of SMIC (HS, VIS, and NIR)
Table 6- 
Experimental Results (Mean F1-Score / Accuracy) of Various Spatiotemporal Descriptors for CDMER, Where the Source and Target Databases are CASME II or one Subset Subset of SMIC (HS, VIS, and NIR)
First, we observe that for our designed benchmark, the second type of experiments (TYPE-II) are significantly more difficult than TYPE-I, which can be clearly revealed by the remarkable differences between the average results of each method in these two types of experiments. For example, the baseline method, GFK (a well-performing representative of DA methods), and LPQ-TOPdecorr=0.1 (a well-performing representative of micro-expression features) achieve the average mean F1-score / Accuracy of 0.6003 / 61.62, 0.7223 / 72.44, and 0.6157 / 63.79 percent in the first type of experiments, which are much higher than their achieved results (0.4112 / 45.55 percent, 0.5161 / 54.31 percent, and 0.3236 / 38.51) in the second type of experiments. In fact, the differences in difficulty between the TYPE-I and TYPE-II experiments stand to reason. The three datasets (HS, VIS, and NIR) used in TYPE-I involve the same subjects, stimulus materials, and recording environments and just different cameras, which results in the relatively small dataset difference. However, another dataset used in TYPE-II experiments, CASME II, corresponds to substantially different subjects, stimulus material, and recording environments compared with SMIC (HS, VIS, and NIR).

Second, it should be pointed out that the class imbalanced problem existing in the source or target database remarkably degrades the performance of either DA method or micro-expression features in dealing with the CDMER tasks. For example, in the cases with SMIC (NIR) as the target database, i.e., Exp.11, Exp.3, and Exp.5, we can observe that the average performance in terms of mean F1-score and Accuracy of all the DA methods can reach 0.6881 and 70.42 percent in Exp.5 whose the source database, SMIC (VIS), is relatively class-balanced. These two metrics drop to 0.6782 and 67.86 percent in Exp.3 and 0.5011 and 51.39 percent in Exp.11, where the source databases of Exp.3 and Exp.11 are SMIC (HS) and CASME II, respectively and very class-imbalanced. Similarly, the performance of all the DA methods is also affected by the class-imbalanced target database. As shown in Exp.6, Exp.3, and Exp.12 whose source database is fixed, i.e., SMIC (NIR), it can be seen that with class-imbalanced database as target one, the average mean F1-score / Accuracy decrease from the level of 0.7272 / 73.88 percent (Exp.6: class-balanced) to 0.5668 / 57.26 percent (Exp.4: class-imbalanced) and 0.3928 / 41.96 percent (Exp.4: class-imbalanced), respectively.

Third, we can also observe that the heterogeneous problem existing between source and target databases raises the level of difficulty of the CDMER tasks. It is known that the samples in SMIC (NIR) are recorded by a near-infrared camera, whose image-quality is considerably different from the samples recorded by high-speed camera (used in CASME II and SMIC (HS)) and visual camera (used in SMIC (VIS)). Therefore, it is intuitive that the CDMER tasks involving SMIC (NIR) would be more difficult than others. In order to check this point, we first fix the source database as SMIC (HS) and observe two experiments, i.e., Exp.1 and Exp.3, where the target database in Exp.1 is SMIC (VIS) and Exp.3 corresponds to SMIC (NIR). It can be seen that the average results among all the DA methods and micro-expression features (0.8405 / 84.12 and 0.6459 / 67.52 percent) in Exp.1 (homogeneous) are significantly higher than the results (0.6782 / 67.86 and 0.4420 / 49.65 percent) in Exp.3 (heterogeneous). We further check the opposite case if the heterogeneous image-quality samples exist in the source database. We observe Exp.2 and Exp.4, whose target databases are the same, i.e., SMIC (HS) and source databases are different, i.e., SMIC (VIS) versus SMIC (NIR). From the results, we notice the performance difference between the Exp.2 (homogeneous case) and Exp.4 (heterogeneous case). Specifically, the average performance achieved by DA methods and micro-expressions are 0.5764 /57.60 and 0.4318 / 45.35 percent in Exp.2 (homogeneous case) versus 0.5668 / 57.26 and 0.3613 / 39.18 percent in Exp.4 (heterogeneous case).

4.2.2 Results for CDMER Experiments of Using DA Methods
Tables 3 and 4 show the experimental results of different DA methods corresponding to the TYPE-I and TYPE-II experiments, respectively. From these two tables, it can be seen that in both two types of designed CDMER experiments, nearly all the DA methods can achieve promisingly better results in terms of both mean F1-score and Accuracy than the baseline method (SVM without any DA). More importantly, some well-performing DA methods, e.g., GFK [52], DRFS-T [40], and the proposed RSTR, have significant improvements of at least 0.1000 (average mean F1-score) and 10 percent (average Accuracy) compared with the baseline results in either TYPE-I or TYPE-II experiments. Based on the above observations, we are able to reach the conclusion that considering CDMER as an DA problem is no doubt an effective solution for the CDMER problem. It is a good choice to develop excellent DA methods to relieve the feature distribution mismatch between the samples from different micro-expression databases.

We also observe that the proposed RSTR achieves the best average results among all the methods in the CDMER experiments. Specifically, RSTR achieves the average mean F1-score / Accuracy of 0.7381 / 73.98 percent in TYPE-I experiments and 0.5587 / 57.74 percent for TYPE-II experiments, which are significantly higher than most of DA methods. Moreover, the proposed RSTR is better at coping with class-imbalanced and heterogeneous CDMER tasks. It can be from Table 4 seen that RSTR obtains two best results in terms of both mean F1-score and Accuracy (Exps. 7 and 10) among four highly class-imbalanced cases (Exps. 7, 8, 9 and 10). As for heterogeneous CDMER (Exps. 3, 4, 5, 6, 11 and 12), RSTR outperforms all the rest DA methods in four cases (best mean F1-score and Accuracy in Exps. 6 and 12 and best Accuracy in Exps. 3 and 5). The superior performance of RSTR may attribute to the consideration of the different contributions of facial local regions in the design of RSTR, which had been proven to benifit distinguishing different micro-expressions [16]. Here we give an example of the visualization of the weighted parameters learned by RSTR in Fig. 3. From the example, it is clear to see that different facial local regions have different contributions in coping with CDMER tasks. Consequently, it is more convincing that taking into consideration the positive cues associated with distinguishing micro-expressions, e.g., facial local region information, offers us a new insight to develop more effective DA methods.

Fig. 3. - 
A visual example for the weighted parameters $\omega _i$ωi learned by RSTR, where the CDMER experiment is Exp. 7. From left to right, (a), (b), (c) and (d) show the contributions of different facial local regions yielded by the spatial grid with sizes of $1\times 1$1×1, $2\times 2$2×2, $4\times 4$4×4, and $8\times 8$8×8, respectively.
Fig. 3.
A visual example for the weighted parameters ωi learned by RSTR, where the CDMER experiment is Exp. 7. From left to right, (a), (b), (c) and (d) show the contributions of different facial local regions yielded by the spatial grid with sizes of 1×1, 2×2, 4×4, and 8×8, respectively.

Show All

Finally, we would also like to discuss the limitation of the proposed RSTR method. Although RSTR achieves best results in most CDMER tasks, it is clear to see that a large gap exists between RSTR and some state-of-the-art DA methods in several challenging cases. For example, TCA and GFK outperform RSTR with a sizeable lead in Exp. 9. Therefore, it may not be enough for design well-performing DA method to merely leverage the facial local region information. We also draw the confusion matrices of RSTR and three comparison methods including the baseline one (SVM without any DA) and two DA ones (TCA and TKL) in two typical cases (Exp.7 and Exp.12) to further analyze the limitation of RSTR. The results are shown in Fig. 4. From these confusion matrices, it is clear to see that in both cases the baseline method was seriously affected by the class-imbalanced problem and hence most of target samples were predicted as the same one micro-expression. While RSTR and other two DA methods can promisingly alleviate this extremely wrong prediction, it can be found that in Exp.12 nearly 60 percent of surprise samples are wrongly predicted as the positive micro-expression by all three DA methods. Consequently, DA methods including the proposed RSTR for dealing with CDMER still have plenty of room for improvement.

Fig. 4. - 
Confusion matrices of the baseline method (SVM without any DA), TCA, TKL and RSTR in Exp.7 and Exp.12.
Fig. 4.
Confusion matrices of the baseline method (SVM without any DA), TCA, TKL and RSTR in Exp.7 and Exp.12.

Show All

4.2.3 Results of CDMER Experiments by Using Micro-Expression Features
The detailed mean F1-score/Accuracy achieved by various micro-expression features are depicted in Tables 5 and 6. From these two tables, it is clear to see that LPQ-TOP with decorr=0 and LBP-SIP with R=3 achieve the best average performance in terms of mean F1-score and Accuracy in TYPE-I and TYPE-II CDMER experiments, respectively. More importantly, we notice that LPQ-TOP(decorr=0) obtains the average mean F1-score/Accuracy of 0.6325/64.05 percent in the first type of experiments, which are even competitive among the results of DA methods. By referring Tables 3 and 5, we can find that LPQ-TOP(decorr=0) outperforms TCA (0.6238/64.01 percent) and is very competitive against STM (0.6440/65.78 percent) and DRLS (0.6552/66.60 percent). In addition, we also observe that several micro-expression features achieve very promising results. For example, LPQ-TOP(decorr=0.1) achieves the mean F1-score of 0.9455 and Accuracy of 94.37 percent in Exp.1, which are significantly better than all the DA methods. In Exp.9 and Exp.10, LBP-SIP(R=3) is a very good competitor against DA methods. Based on the above observations, it is believed that developing database-invariant spatiotemporal descriptors for robustly describing micro-expressions also provide a promising and feasible way to solve the CDMER problem.

However, we have to admit that most of the existing spatiotemporal descriptors including the above well-performing ones are still not satisfactory and cannot completely meet the requirement in CDMER problem. More specifically, several limitations still exist in current micro-expression feature research. First, the performance of most spatiotemporal descriptors is not stable. In other words, one spatiotemporal descriptor performs well in Task A but very poor in Task B. An example is LPQ-TOP. It can be seen that LPQ-TOP with two different parameters both achieve satisfactory results beating the baseline method in TYPE I experiments. But the performance of both two methods decrease sharply in TYPE II ones and even under the level of baseline method. Second, it is clear to see that the performance of most of handcrafted spatiotemporal descriptors is strongly affected by its parameters. As the results of LBP-TOP showed, its performance varies very sensitively in nearly all the experiments of our CDMER benchmark with the changes of its parameter R and P. Therefore, in the future study of spatiotemporal descriptors used for describing micro-expressions, we should also consider to reduce the sensitiveness of its parameters such that it will be simpler and more convenient to use in dealing with the CDMER problem. Finally, it should be pointed out that at present it is not enough for solving CDMER problem to simply use existing spatiotemporal descriptors as micro-expression features because its average performance is still far from the DA methods. From another point of view, it is believed that there is still very large development space in this direction, i.e., developing robust (database-invariant) micro-expression features.

Lastly, we discuss the deep features evaluated in the benchmark. From the results of deep features extracted by C3D pretrained on Sports1M and UCF101, we can observe that these four deep features perform poorly in nearly all the CDMER experiments and cannot reach the level of most handcrafted features. In fact, this is foreseeable because the Sports1M and UCF101 served for C3D pretraining are both action databases, whose samples are quite different from the micro-expression ones. In fact, to enable the C3D to gain the micro-expression information, we can finetune the pretrained models based on the source database and then use the finetuned model to serve as feature extractor. To this end, we finetune the C3D pretrained on Sports1M based on SMIC (HS) and then extract the deep features to conduct the experiments of using SMIC (HS) as source database including Exp.1, Exp.3, and Exp.8. Note that for finetuning, we augment the samples in SMIC (HS) to 3,3503 and divide the samples into a training set whose sample number is 2,550 and a validation set containing 800 samples. The experimental results are shown in Table 7, where we also list the results of C3D pretrained on Sports1M, the baseline method (LBP-TOP with R=3,P=8), and TCA (a representative DA method). From Table 7, it is clear that by resorting to finetuning strategy, the performance of C3D features can be improved significantly compared with the original model. It is also interesting to see that in Exp.8, the deep features extracted by finetuned C3D even outperform the baseline method (LBP-TOP with R=3,P=8) in term of mean F1-score. However, compared with TCA (a representative DA method) and the baseline method, the performance of deep features is still very poor and not satisfactory. We think there may be two possible reasons. First, it may attribute to the problem of lacking enough good data for finetuning the C3D models. It is clear that the training or finetuning of deep learning models requires large numbers of samples while the sample numbers of the CASME II and SMIC are too small. Although we can use some methods to augment the samples, the augmented micro-expression samples are not satisfactory. Therefore, more micro-expression samples need be collected such that a better-performing C3D can be finetuned to deal with CDMER problem. The second reason may be that it seems not enough for learning the database-invariant deep features to simply finetune the deep model based on the source database. Leveraging the idea of domain adaptation methods, i.e., reducing the difference between source and target domains, to finetune deep models may offer a feasible solution to improve the performance of deep features.

TABLE 7 Experimental Results (Mean F1-Score / Accuracy) of Finetuned C3D Features

SECTION 5Conclusion
In this paper, we have investigated the cross-database micro-expression recognition (CDMER) problem by conducting a standard benchmark evaluation from two different perspectives including domain adaptation (DA) and spatiotemporal features used for describing micro-expressions. First, under a well-designed evaluation protocol, we make use of two widely-used micro-expression databases, i.e., CASME II and SMIC, to set up two types of CDMER experiments. Then, we collect NINE state-of-the-art DA methods and SIX excellent spatiotemporal descriptors to perform benchmark evaluation based on our designed CDMER protocol. Moreover, we also propose a novel DA method called region selective transfer regression (RSTR), which fully considers the different contributions of the facial local regions in recognizing micro-expressions. Finally, comprehensive discussions for the benchmark evaluation results are provided. More importantly, we reach a useful conclusion from the comparison between the proposed RSTR and other state-of-the-art DA methods that it is beneficial for dealing with the CDMER problem if we take full advantage of important cues associated with micro-expressions like facial local region information considered in the design of our RSTR. In addition, all the collected data and codes involving the benchmark evaluation in our paper will be released as soon as possible. We hope our work can advance the micro-expression analysis research by inspiring more researchers to focus on the CDMER problem. In the future, we will insist to collect more well-performing DA methods and micro-expression features for updating the evaluation results in our project page.