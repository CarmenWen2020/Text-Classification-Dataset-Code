Recently, many real-world applications exploit multi-view data, which is collected from diverse domains or obtained from various feature extractors and reflect different properties or distributions of the data. In this work, a novel unsupervised multi-view framework is proposed to cluster such data. The proposed method, called Multi-View clustering with Adaptive Sparse Memberships and Weight Allocation (MVASM), pays more attention to constructing a common membership matrix with proper sparseness over different views and learns the centroid matrix and its corresponding weight of each view. Concretely, MVASM method attempts to learn a common and flexible sparse membership matrix to indicate the clustering, which explores the underlying consensus information of multiple views, and solves the multiple centroid matrices and weights to utilize the view-specific information and further modifies the above-mentioned membership matrix. In addition, the theoretical analysis, including the determination of the power exponent parameter, convergence analysis, and complexity analysis are also presented. Compared to the state-of-the-art methods, the proposed method improves the performance of clustering on different public datasets and demonstrates its reasonability and superiority.
SECTION 1Introduction
Machine learning techniques have been widely applied to many real-world applications, such as engineering, astronomy, biology, remote sensing, and economics. As a fundamental technique of machine learning, the data clustering (i.e. K-Means clustering [1] and graph-based clustering [2]) partitions a set of instances into different clusters so that the instances in the same cluster have high similarity but are dissimilar if they belong to different clusters. Data clustering has received extensive research interests during the past decades.

Considering the above data clustering methods, we first review the graph-based clustering approaches and summarize their drawbacks. The graph-based clustering approaches mostly optimize the objectives based on a given data graph associated with a similarity matrix. For instance, the work [3] used a “local” scale to compute the affinity between each pair of points, which better clusters the multi-scale data with the irregular background clutter, and exploits the structure of the eigenvectors to automatically infer the number of groups. The work [4] investigated the consistency of the spectral clustering algorithms, which clusters the data with the help of eigenvectors of the graph Laplacian matrices. The work [5] follows the graph-based paradigm and proposes a graph-based genetic algorithm for clustering, which is flexible to use various kernels. Also, the work [6] presented a spectral clustering technique which integrates the “cannot-link” and the “must-link” constraints on a weighted adjacency matrix of a graph. In addition, [7] proposed a method by approximating the eigenvectors of the Laplacian matrix using a randomized subspace iteration process to alleviate the large-scale data costs. To uncover the clustering structure, [8] proposed a novel convex model to learn the structured doubly stochastic matrix by imposing a low-rank constraint on the graph Laplacian matrix.

Note that such methods conducted a series of procedures based on the constructed similarity/affinity matrix from the original data but rarely modified it. Real-world datasets always contain the noise and outliers that result in the unreliable similarity matrix and may reduce the final performance. Besides, the cluster structures of such methods were not explicit and a post-processing step, such as K-Means clustering, was needed to uncover the clustering indicator. To achieve the reliable similarity/affinity matrix and a proper neighbor assignment, the recent works [9], [10], [11] proposed a school of algorithms which are imposed on the constrained Laplacian Rank. Specifically, [9] first imposed a rank constraint to the Laplacian matrix of the data similarity matrix, which learns the data similarity matrix by assigning the adaptive and optimal the neighbors for each data point and makes the number of connected components in the similarity matrix equal to the number of the clusters. Then, [10] developed two graph-based clustering objectives based upon the ℓ1-norm and ℓ2-norm with the constrained Laplacian rank. The work [11] proposed an unsupervised feature selection method based on the constrained Laplacian rank. The work [12] proposed an unsupervised single-view feature extraction method that simultaneously learns the transformation matrix and the structured graph containing the clustering information.

Although such modified methods introduce the Laplacian rank constraint to improve the performance, how to construct an optimal similarity graph and how to apply to the multi-view data are still two key problems. For the first problem, apart from building the reliable similarity/affinity matrix with a proper neighbor assignment, other important factors also need to be considered, such as the choice of the similarity function and similarity graph, which greatly affects the final clustering performance.

For the second problem, the multi-view learning has shown the promising potential in the machine learning applications, such as, visual recognition [13], public sentiments and activities [14], road traffic speed prediction [15], learning social circles [16], micro-video popularity prediction [17], and so on. In the multi-view scenario, the more the number of views, the more complex constructing the similarity graphs will be. Some improved methods, such as [12], [18], [19], usually need to perform the eigen decomposition for the Laplacian matrix to solve the cluster indicator matrix, i.e. minF⊤F=ITr(F⊤LSF), where F∈Rn×c is the cluster indicator matrix and LS is the corresponding Laplacian matrix. When we apply these methods to the multi-view data, we need to perform the multiple eigen decompositions for different similarity graphs, which has a high computational complexity. Besides, how to fuse different similarity graphs is another key point. If we only use one similarity graph to represent multiple similarity graphs, this strategy may ignore the complementary and diverse information of different views, which certainly loses the advantages of the multi-view data. If we simply concatenate all the views into a high-dimensional vector and directly apply the above single-view clustering methods, it will cause the over-fitting and reduce the interpretability of the information of different views. Thus, it is necessary to explore a novel clustering method for the multi-view data.

In recent years, a variety of spectral clustering methods have been proposed for the multi-view data. For example, based on the bipartite graph, [20] used the local manifold fusion to integrate the heterogeneous features. In [21], authors designed a unified graph-based model with pairwise constrains to perform multi-view clustering. The work [22] used the multi-view information to propose a pairwise sparse spectral clustering with the sparse regularization on a low-dimensional embedding. What is more, some works exploited several novel ways of the fusing information from multiple graph sources, such as Linked Matrix Factorization [23], Co-Regularization [24], Co-Training [25] and Low-Rank [26], [27].

Although these multi-view spectral clustering algorithms can achieve the encouraging performance, the spectral clustering itself is not good at clustering the multi-view data investigated in [28]. In our work, the idea of similarity matrix is integrated with a proper neighbor assignment into the K-Means clustering, and a novel unsupervised multi-view framework, called MVASM, is proposed and used to deal with above two problems. Comparing with the previous works, MVASM utilizes a common membership matrix with adaptive sparseness over different views to indicate the clustering, which explores the underlying consensus information of multiple views. Meanwhile, MVASM alternately learns the centroid matrix and the weight of each view to revise the common membership matrix, which makes good use of the view-specific information and maintains the relative independence of each view. One highlight of the proposed work is transforming the similarity matrix with a proper neighbor assignment into the clustering membership matrix and relaxing the original binary membership values to probabilities summed up to 1. If so, through a balanced parameter, MVASM can control the sparseness degree of the membership vectors to obtain more accurate results. Another highlight is to develop a multi-view clustering model with this adaptive sparse membership matrix, which unifies the clustering indicator information across different views by sharing the common membership matrix and maintains the view-specific information by solving the centroid matrix and the weight of each view alternately.

A Robust and Sparse Fuzzy K-Means (RSFKM) clustering method is proposed in our recent work [29]. It is worth noting that the proposed MVASM method significantly improves the RSFKM method. First, RSFKM is a single-view model which considers the robustness of data residual term (using ℓ2,1-norm [30] and capped ℓ1-norm [31]) as well as a proper sparse membership matrix but only for the single-view data. In contrast, the proposed MVASM method develops a multi-view model, which focuses on multiple views and explores the underlying consensus information by optimizing a common cluster membership matrix across different views. Second, the MVASM method introduces multiple centroid matrices and weights to discover the complementary information across different views. The above-mentioned points can be demonstrated in comparison experiments shown in Section 5.4.

The contributions of the MVASM method can be summarized as the following four aspects:

MVASM is a novel method which introduces a common membership matrix with the proper sparseness to indicate the clustering on each view, which maintains the consistency of the underlying clustering pattern across different views. MVASM is bridged with an easier problem, which can be solved by the Newton's method and returns the optimal solution.

MVASM alternately learns the centroid matrix and the weight of each view to modify the common membership matrix, which makes good use of view-specific information to maintain the relative independence of each view and adjusts the proportion of different views to capture the complementary information during merging the multi-view information.

Theoretical analyses, including the determination of power exponent parameter, convergence analysis, and complexity analysis, are also presented.

Extensive evaluations on four public datasets and the experimental comparisons with several state-of-the-art multi-view clustering methods show the effectiveness and superiority of the proposed MVASM method.

The rest of this paper is organized as follows. Section 2 introduces the related work. Section 3 formulates the framework of the MVASM method and provides an alternating iterative optimization method. Some theoretical analyses of MVASM, which shows how to determine the power exponent parameter and demonstrates an optimal solution that can be obtained with a few iterations, are presented in Section 4. The experimental comparison results followed with some theoretical and empirical analyses on four different datasets are provided in Section 5. Section 6 shows the conclusions.

SECTION 2Related Work
2.1 Clustering With Adaptive Neighbors (CAN)
In the work [9], it considered the ideal neighbor assignment that the connected components in the data are exact c. Given a data matrix X∈Rd×n, for the i-th data point xi, xj can be defined as a neighbor to xi with the probability sij∈R. Thus, all the data points x1,…,xn can be connected to xi with the probabilities sij|nj=1 and usually a smaller distance ∥xi−xj∥22 corresponds to a large probability sij, where S∈Rn×n is the similarity matrix which is composed of the probabilities sij|ni,j=1 and can be seen as the neighbor assignment.

To obtain the ideal neighbor assignment, [9] constrained the probabilities sij|nj=1 of the i-sample as s⊤i1=1,0≤si≤1 and the Laplacian matrix LS as rank(LS)=n−c to make the similarity matrix S with a clear clustering structure. Therefore, the clustering model becomes:
min∀i,s⊤i1=1,0≤si≤1rank(LS)=n−c∑i=1n∑j=1n(∥xi−xj∥22sij+γs2ij),(1)
View SourceRight-click on figure for MathML and additional features.where LS=DS−S⊤+S2 and the degree matrix DS∈Rn×n is a diagonal matrix where the i-th diagonal element is ∑j(sij+sji)2.

However, the above method is only designed for the single-view data. For the multi-view data, we need to calculate different proper similarity graphs for different views. It is more complex to construct the similarity graphs for more different views, which further results in high computational complexity. If we only use one similarity graph to represent multiple similarity graphs, this strategy may ignore the complementary and diverse information of different views, which certainly loses the advantages of the multi-view data.

2.2 Robust and Sparse Fuzzy K-Means Clustering (RSFKM)
Recently, in the work [29], the authors proposed to directly use the membership matrix U∈Rn×c in the problem (2) instead of the similarity matrix S∈Rn×n in the problem (1), which not only uncovers the clustering indicator explicitly but also reduces the computational complexity significantly.

Thus, the objective function can be written as:
minU1=1,U≥0∑i=1n∑k=1cuikd~ik+γ∥U∥2F,(2)
View SourceRight-click on figure for MathML and additional features.where U∈Rn×c is the membership matrix and d~ik is a measurement that can be flexibly defined as various norms to measure the similarity between the sample and the clustering centroid vector.

Nevertheless, the above method is only designed for the single-view data. If all the views are simply concatenated into a high-dimensional vector and then the above single-view clustering methods are directly applied, it will cause the over-fitting and reduce the interpretability of the information of different views.

SECTION 3Multi-view K-Means Clustering With Adaptive Sparse Memberships and Weight Allocation
3.1 Formulation
Inspired by the CAN and RSFKM methods, we relax the original binary membership values to probabilities summed up to 1 and design a multi-view clustering model with the adaptive sparse membership matrix. The proposed MVASN method defines the objective function as follows:
minU,V,α∑i=1n∑k=1cuik∑p=1mαqp∥x(p)i−v(p)k∥22+γ∥U∥2Fs.t.U1=1,U≥0,α⊤1=1,α≥0,(3)
View SourceRight-click on figure for MathML and additional features.where X(p)=[x(p)1,…,x(p)n] and V(p)=[v(p)1,…,v(p)c]. X(p)∈Rdp×n is the data matrix with dp-dimension and n samples in the pth view, and its i-th data point is denoted as x(p)i∈Rdp. V(p)∈Rdp×c is the centroid matrix in the p-th view and its the k-th centroid vector is v(p)k∈Rdp. U∈Rn×c is the common membership matrix over different views and αp is the weight for the p-th view. q is the power exponent of each weight and γ is the regularization coefficient.

3.2 Property
Traditional multi-view K-Means clustering is rigorously established where the membership matrix U is binary such that each row of U contains one 1 and c−1 0. The sum of each column of U indicates the number of samples per cluster. Differed from the multi-view K-Means clustering, the MVASM method relaxes each row of U to probabilities whose sum is 1.

During the clustering, the MVASM method considers the sparsity of the membership matrix U. The importance of two terms in the problem (3) is controlled by the parameter γ. It can be seen that if the parameter γ=0, the membership vector of each data point is binary (only one element is 1 and all other elements are zeros), which is equivalent to K-Means clustering method in the multi-view scenario. If γ>0, the membership vector of each data point is no longer sparse. The sparseness of the membership matrix progressively changes when we tune γ from 0 to ∞. If γ is large enough, the membership matrix is non-sparse. Actually, the parameter γ controls the sparseness degree of the membership matrix.

In addition, since multiple views have their own attributed information, the MVASM method can make the intra-view information and the common information fit well. The intra-view information is derived from a set of V(p) and the relationship between different views is tuned by the weight distribution, and the common information across different views comes from the membership matrix U. This helps us to obtain more better performance of clustering.

3.3 Optimization
In this section, we decompose the problem (3) into three subproblems and solve them via an alternating optimization iteration method.

3.3.1 Updating U When V and α are Fixed
We calculate the cluster membership matrix U via solving a perfect square problem with an equality and an inequality constraints across different views. Thus, the problem (3) can be rewritten as:
⇔⇔minU1=1,U≥0∑i=1n∑k=1cuik∑p=1mαqp∥x(p)i−v(p)k∥22+γ∥U∥2FminU1=1,U≥0∑i=1n∑k=1c(uikhik+γu2ik)minU1=1,U≥0∑i=1n∑k=1cγ(uik+hik2γ)2.(4)
View SourceRight-click on figure for MathML and additional features.Note that the problem (4) is independent of the data point i, so its vector form is:
minui1=1,ui≥0∥ui+hi2γ∥22(5)
View SourceRight-click on figure for MathML and additional features.where hi=[hi1,…,hic]∈R1×c and hik∈R is defined as:
hik=∑p=1mαqp∥x(p)i−v(p)k∥22(6)
View SourceRight-click on figure for MathML and additional features.Here, we will introduce an efficient method to solve the problem (6) in a separate subsection D referring to [32].

3.3.2 Updating V When U and α are Fixed
We update the cluster centroid matrix V by solving an unconstrained optimization problem. Omitting the regularization term which is irrelevant to V, there is:
⇔minV∑i=1n∑k=1cuik∑p=1mαqp∥x(p)i−v(p)k∥22+γ∥U∥2FminV∑k=1c∑p=1m∑i=1nuikαqp∥x(p)i−v(p)k∥22.(7)
View SourceWe use J to represent the objective function of the problem (7), there is:
J=∑k=1c∑p=1m∑i=1n[uikαqp(x(p)i)⊤x(p)i+uikαqp(v(p)k)⊤v(p)k−2uikαqp(x(p)i)⊤v(p)k].(8)
View SourceRight-click on figure for MathML and additional features.Due to solving the minimum of the problem (8), we take the derivative of J with respect to v(p)k and set it as zero:
∂J(v(p)k)∂v(p)k=∑i=1n(2uikaqpv(p)k−2uikaqpx(p)i)=0.(9)
View SourceFurthermore, we can obtain:
v(p)k=∑ni=1uikx(p)i∑ni=1uik.(10)
View SourceRight-click on figure for MathML and additional features.

3.3.3 Updating α When U and V are Fixed
We update the weight of each view by solving an optimization problem with two constraints. By ignoring the regularization term which is irrelevant to α, we have:
⇔⇔minα⊤1=1,α≥0∑i=1n∑k=1cuik∑p=1mαqp∥x(p)i−v(p)k∥22+γ∥U∥2Fminα⊤1=1,α≥0∑p=1mαqp(∑i=1n∑k=1cuik∥x(p)i−v(p)k∥22)minα⊤1=1,α≥0∑p=1mαqpAp,(11)
View SourceRight-click on figure for MathML and additional features.where Ap=∑ni=1∑ck=1uik∥x(p)i−v(p)k∥22∈R.

The Lagrangian function of the problem (11) is:
L(α,λ,β)=∑p=1mαqpAp−λ(α⊤1−1)−β⊤α,(12)
View Sourcewhere λ and β≥0 are the Lagrangian multipliers.

Taking the derivative of Eq. (12) w.r.t αp and setting as zero, there is:
αq−1p=λqAp+βpqAp.(13)
View SourceNote that the KKT conditions of the problem (11) are:
⎧⎩⎨⎪⎪β≥0(14)α≥0(15)α⊤β=0(16).
View Source

Combining the Eqs. (14), (15), (16), it is obvious that βpαp=0. Multiplying the Eq. (13) by αp, we have:
αp=(λqAp)1q−1.(17)
View SourceRight-click on figure for MathML and additional features.In addition, considering the equality constraint α⊤1=1, we can achieve:
∑p=1m(λqAp)1q−1=1⇒(λq)1q−1=1∑mp=1A11−qp.(18)
View SourceTherefore, integrating the Eq. (17) with Eq. (18), we obtain the following equation:
αp=A11−qp∑ms=1A11−qs.(19)
View Source

To sum up, in Algorithm 1, we can obtain a common membership matrix U via step 1, which indicates the clustering across different views. Update V(p) via step 2, which obtains multiple clustering centroid matrices for different views. Updating αp via step 3 can learn the weights for different views, which realizes the intercoordination across views. This process is repeated until the objective function value converged.

Algorithm 1. Algorithm to solve problem (3)
Input:

Data for m views {X(p)|p=1,2,…,m}, X(p)∈Rdp×n. The number of clusters c. Parameters γ and q.

Output:

Membership matrix U∈Rn×c, clustering centroid matrix V(p)∈Rdp×c and weight αp for the p-th view.

Initialization:

Set t=0. Initialize U and V(p) by the K-Means clustering method. Initialize αp=1/m for the p-th view.

While not converge do

For each data point i, calculate the i-th row of U by solving the problem (5), where hi∈R1×c is a vector with the c elements hik|ck=1.

Update the k-th clustering centroid vector v(p)k of the p-th view by using the Eq. (10).

Update the weight αp according to the Eq. (19)

End While, return U, V(p) and αp

3.4 Optimization Algorithm to the Problem (5)
Introducing the Lagrangian multipliers η∈R and ξ=[ξ1,…,ξc]⊤∈Rc×1, the corresponding Lagrangian function is:
L(ui,η,ξ)=∥ui+hi2γ∥22−η(ui1−1)−uiξ.(20)
View SourceIn terms of the KKT conditions, we have the following equations:
⎧⎩⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪2ui+hiγ−η1⊤−ξ⊤=0(21)ui≥0(22)ξ≥0(23)uiξ=0(24)ui1=1(25).
View SourceRight-click on figure for MathML and additional features.According to the Eq. (21) and the Eq. (25), we have:
η=2c+hi1γc−ξ⊤1c.(26)
View SourceRight-click on figure for MathML and additional features.Substituting η into the Eq. (21), there is:
ui=z+ξ⊤2−ξ¯1⊤2.(27)
View SourceRight-click on figure for MathML and additional features.where z=−hi2γ+1⊤c+hi11⊤2γc∈R1×c and ξ¯=ξ⊤1c∈R. For any k, we have:
uik=zk+12ξk−12ξ¯(28)
View SourceRight-click on figure for MathML and additional features.
ξk=2uik+ξ¯−2zk.(29)
View SourceFrom the Eqs. (22), (23), (24), we know that uikξk=0. Combining this equality with the Eq. (29), we obtain:
uik(2uik+ξ¯−2zk)=0⇒uik=(zk−12ξ¯)+,(30)
View Sourcewhere w+=max(w,0). It is obvious that the optimal solution ui can be obtained when ξ¯ is known.

Furthermore, combining uikξk=0 with the Eq. (28), there is:
(zk+12ξk−12ξ¯)ξk=0⇒ξk=(ξ¯−2zk)+.(31)
View SourceSubstituting ξk into the definition of ξ¯, we have:
ξ¯=1c∑k=1cξk=1c∑k=1c(ξ¯−2zk)+.(32)
View SourceRight-click on figure for MathML and additional features.Now, we define a function as:
f(ξ¯)=1c∑k=1c(ξ¯−2zk)+−ξ¯,(33)
View SourceRight-click on figure for MathML and additional features.thus f(ξ¯)=0 and ξ¯ can be calculated by solving the root.

Because ξ≥0 and f′(ξ¯)≤0 is a piecewise linear and convex function, the Newton's method is utilized to find the root of f(ξ¯)=0:
ξ¯t+1=ξ¯t−f(ξ¯t)f′(ξ¯t).(34)
View Source

SECTION 4Theoretical Analysis
4.1 Determine Power Exponent q
In this work, we use a parameter q to tune the weight distribution. According to Eq. (19) and the characteristic of the function 11−q (referring to the Fig. 1), there are two extreme cases. When q→∞, our method can obtain the equal weights 1c. When q→1+, supposing Ao is the smallest among {A1,…,Ao,…,Am} and substituting Ao into the Eq. (19), we can obtain its weight:
limq→1+αo=limq→1+11+∑s≠o(AsAo)11−q=1.(35)
View SourceRight-click on figure for MathML and additional features.It can be seen that MVASM assigns the weight 1 to the view whose Ao value is the smallest and assigns the weights 0 to other views. Through this strategy, we can assure that MVASM has no trivial solution when q>1.

Fig. 1. - 
The function curve of $\frac{1}{1-q}$11-q.
Fig. 1.
The function curve of 11−q.

Show All

4.2 Convergence Analysis
Theorem 1.
In each iteration, the objective function value of the problem (3) decreases until the algorithm converges.

Proof.
Supposed that after the tth iteration, we have obtained U(t), V(t) and α(t). In the t+1th iteration, we first fix V and α as V(t) and α(t), respectively, and then solve U(t+1) among different views. According to the problem (5), U(t+1) can be solved by the following equation:
ui(t+1)=argminui1=1,ui≥0∥ui(t)+hi(t)2γ∥22,(36)
View SourceRight-click on figure for MathML and additional features.where the above objective function and the constraints are convex in the domain of ui. Thus, the problem (36) will converge to a global optimal solution.

In the same way, we fix U and α as U(t) and α(t), respectively, and solve V(t+1) referring to the Eq. (10) in the t+1th iteration. Similarly, we also obtain α(t+1) by the Eq. (19) when we fix U and V as U(t) and V(t).

It is obvious that the problem (3) can be divided into three subproblems and each of them is a convex optimization. Thus, the above proof can verify the convergence of the Algorithm 1.

4.3 Complexity Analysis
Updating U for each iteration requires computing the square root in the Eq. (5) using the Newton's method, which takes O(nmd2M(c)), where M(c) is the cost of calculating f(ξ¯)f′(ξ¯), with c-digit precision. Updating V by the Eq. (10) for each iteration requires the matrix multiplication, which takes O(nmd2c). Updating α according to the Eq. (19), this step takes O(nmd2c). Therefore, denoting t as the number of outer iterations of Algorithm 1, the total computation complexity is O(nmd2(M(c)+2c)) which is linear to the number of data samples n.

SECTION 5Experiments
In this section, the performance of MVASM is evaluated on four public benchmarks according to four widely used clustering evaluation metrics, i.e. ACC [33], NMI [33], Jaccard [34], and Purity [34]. Before performing all the experiments, we normalize all the values of datasets.

5.1 Datasets
In our experiments, following the previous work [29], we adopt four public datasets including COIL20 1, COIL100 2, MNIST 3, and YALE 4 for evaluating MVASM. All the datasets are described as follows.

5.1.1 COIL20
This dataset [35] contains 1440 size normalized images of 20 objects, which corresponds to 72 images per object. Each image is represented by three heterogeneous feature sets: 30 isometric projection (ISO), 19 linear discriminant analysis (LDA), and 30 neighborhood preserving embedding (NPE).

5.1.2 COIL100
The dataset [36] contains 7200 size normalized images of 100 objects, which corresponds to 72 images per object. Each image is represented by three heterogeneous feature sets: 30 ISO, 99 LDA, and 30 NPE.

5.1.3 MNIST
This dataset [37] collects a set of 10000 handwritten digits (0 ∼ 9) which have been size-normalized and centered in a fixed-size image. These digits are described by using three heterogeneous feature sets: 30 ISO, 9 LDA, and 30 NPE.

5.1.4 YALE
The dataset [38] consists of 165 images from 15 subjects, which has 11 images per subject and corresponds to different facial expressions or configurations. Each image is expressed by three heterogeneous feature sets: 30 ISO, 14 LDA, and 30 NPE.

Three heterogeneous feature descriptors are ISO, LDA, and NPE, where ISO visually represents the three-dimensional objects in two dimensions, LDA finds a linear combination of features that can separate two or more classes of objects, and NPE can preserve the local neighborhood structure on the data manifold. Besides, we intuitively show some example images from the different datasets in Fig. 2.

Fig. 2. - 
Some example images from (a) COIL20, (b) COIL100, (c) MNIST and (d) YALE datasets.
Fig. 2.
Some example images from (a) COIL20, (b) COIL100, (c) MNIST and (d) YALE datasets.

Show All

5.2 Evaluation Metric
According to the previous works, we use four evaluation metrics to measure the clustering performance: ACC [33], NMI [33], Jaccard [34], and Purity [34]. For these metrics, the higher value indicates better clustering performance. We report the above metrics in Tables 1, 2, 3, and 4 to obtain a comprehensive evaluation.

TABLE 1 Comparison of MVASM and Several State-of-the-Art Multi-View cluzstering Methods on COIL20 and COIL100 Datasets

TABLE 2 The Comparisons of MVASM and Several State-of-the-Art Multi-View Clustering Methods on COIL100 Dataset

TABLE 3 Comparison of MVASM and Several State-of-the-Art Multi-View clusterzing Methods on MNIST Dataset
Table 3- 
Comparison of MVASM and Several State-of-the-Art Multi-View clusterzing Methods on MNIST Dataset
TABLE 4 Comparison of MVASM and Several sztate-of-the-Art Multi-View Clustering Methods on YALE Dataset
Table 4- 
Comparison of MVASM and Several sztate-of-the-Art Multi-View Clustering Methods on YALE Dataset
5.2.1 ACC
ACC is defined as:
ACC=∑ni=1δ(si,map(ri))n,(37)
View SourceRight-click on figure for MathML and additional features.where ri is the clustering result and si is the ground-truth label. map(⋅) is the best mapping function which permutes ri to match si.

5.2.2 NMI
NMI is defined as:
NMI(C,C′)=∑ck∈C,c′k′∈C′p(ck,c′k′)logp(ck,c′k′)p(ck)p(c′k′)max(H(C),H(C′)),(38)
View SourceRight-click on figure for MathML and additional features.where C and C′ denote the sets of clusters obtained from the ground-truth and MVASM, respectively. p(ck) and p(c′k′) are the probabilities that the sample belongs to the clusters ck and c′k′, respectively. p(ck,c′k′) is the joint probability that the sample belongs to both the clusters ck and c′k′. H(C) and H(C′) are the entropies of C and C′, respectively.

5.2.3 Jaccard and Purity
Jaccard measures similarity between finite sample sets N1 and N2, which can be defined as:
Jaccard=|N1∩N2||N1∪N2|=TPTP+FP+FN,(39)
View Sourcewhere TP is the number of true positives, FP is the number of false positives, and FN is the number of false negatives.

Purity is the percent of the total number of the samples which are classified correctly, and is defined as:
Purity=1n∑k′=1cmaxk|ck∩c′k′|,(40)
View Sourcewhere |ck∩c′k′| denotes the number of samples in both the kth cluster of the ground-truth and k′th cluster of the clustering results.

5.3 Experimental Setup
5.3.1 Compared Methods
First, we compare the performance of MVASM with KmASM (K-Means clustering with Adaptive Sparse Memberships) and RSFKM [29] methods, which demonstrates that MVASM embodies the advantage of the multi-view method over the single-view method. Second, we compare MVASM with other types of clustering algorithms, i.e., Hierarchical Clustering (denoted as HC) and DBSCAN methods, which verifies the superiority of the K-Means based methods. Third, we compare MVASM with several state-of-the-art methods, including NMVKM [28], RMVKM [28], DEKM [39], and RDEKM [40], to show the advantages of learning a common membership matrix with the adaptive sparseness across different views. In addition, to emphasize the importance of intercoordination across different views, we compare MVASM with AVASM (which concatenates all the views and performs the K-Means clustering with adaptive sparse memberships). It is worth noting that, for the sake of fairness, we initialize the membership matrix or the cluster indicator matrix by the litekmeans method [41] in all experiments of this paper.

5.3.2 Parameters Setting
In our experiments, for two comparison single-view methods (KmASM and RSFKM) on different views, they contain a regularization parameter γ. According to [29], we tune the parameter γ of RSFKM from 0.1 to 10 with 0.1 step and search for the optimal parameter corresponding to the highest accuracy. For the above-mentioned methods, the different parameters γ of three views on COIL20, COIL100, MNIST and YALE datasets are set as (0.5, 0.5, 0.5), (0.5, 0.5, 0.5), (0.5, 0.5, 0.5), and (1.0, 1.0, 5.5), respectively. Similarly, we set the parameter γ of KmASM for three views as (1.5, 2.1, 0.6), (0.1, 0.6, 0.2), (0.3, 1.3, 0.2), and (1.6, 2.1, 1.7), respectively. For two comparison multi-view methods (RMVKM and DEKM), they have the power exponent parameter q and search log10q in the range of (0,1] with step 0.1 to obtain the best parameter values. Here, referring to [28], [39], we set log10q as 0.7, 0.9, 0.1, and 0.5 on the above four datasets for the RMVKM method, and set log10q as 0.1, 0.5, 0.7, and 0.9 for the DEKM method. In the proposed MVASM method, there are two parameters γ and q in Algorithm 1. The parameter γ adjusts the sparseness of the common membership matrix, and the parameter q controls the distribution of the weights on different views. They all play important roles in the MVASM method and we show some results (such as ACC and NMI in Figs. 3 and 4) to describe the variation of the different parameter values. Concretely, we tune q according to the Eq. (19) in the range of [1,6] with step 0.01 and show γ from 0 to 6 with step 0.1. Here, we set (γ,q) on COIL20, COIL100, MNIST, and YALE datasets as follows: (0.4, 1.96), (0.4, 2.19), (0.2, 1.62), and (0.9, 1.11).

Fig. 3. - 
Parameter variations of MVASM on COIL20, COIL100, MNIST and YALE datasets, respectively.
Fig. 3.
Parameter variations of MVASM on COIL20, COIL100, MNIST and YALE datasets, respectively.

Show All

Fig. 4. - 
Parameter variations of MVASM on COIL20, COIL100, MNIST and YALE datasets, respectively.
Fig. 4.
Parameter variations of MVASM on COIL20, COIL100, MNIST and YALE datasets, respectively.

Show All


Fig. 5.
The weight of each view on (a) COIL20, (b) COIL100, (c) MNIST and (d) YALE datasets.

Show All

Fig. 6. - 
The common membership matrix across the multiple views on (a) COIL20, (b) COIL100, (c) MNIST and (d) YALE datasets.
Fig. 6.
The common membership matrix across the multiple views on (a) COIL20, (b) COIL100, (c) MNIST and (d) YALE datasets.

Show All

5.4 Experimental Results
5.4.1 Compared With Single-View Methods
From the results reported in Tables 1, 2, 3, and 4, we have the following observations. First, taking the COIL20 dataset as an example, in Table 1, the results of the multi-view methods (including NMVKM, RMVKM, DEKM, RDEKM, and MVASM) are better than that of the single-view method (i.e. KmASM and RSFKM). Concretely, compared with KmASM(1)∼(3), the proposed MVASM method achieves 21.49, 18.07 and 29.48 percent average improvements (i.e., the improvement averaged over four clustering metrics), respectively. Similarly, compared with KmASM(1)∼(3), other five comparison multi-view methods also achieve the same conclusion based on the experimental results in Table 1.

Furthermore, compared with RSFKM(1)∼(3), the proposed MVASM method gains significant average improvements. For instance, on COIL20 dataset, the proposed MVASM method achieves 16.80, 20.04 and 48.99 percent average improvements, respectively. Other five multi-view methods also obtain different average improvements compared to RSFKM(1)∼(3). Besides, making comparisons of HC (1) ∼ HC(3), DBSCAN(1)∼DBSCAN (3) and the proposed MVASM method, it is obvious that MVASM still can improve the clustering performance in different degrees.

It demonstrates the superiority and effectiveness of multi-view information. In the multi-view setting, one view may contain some valuable information that other views do not have. Therefore, MVASM performs significantly better than the above single-view methods on each view.

5.4.2 Compared With Multi-View Methods
Taking the COIL20 dataset in Table 1 as an example, the proposed MVASM method performs best among the NMVKM, RMVKM, DEKM, and RDEKM methods. Specifically, it is obvious that the MVASM method can achieves 5.93, 4.60, 1.86 and 14.86 percent average improvements over the NMVKM, RMVKM, DEKM, and RDEKM methods, respectively. Because the clustering indicator matrix with the binary values in four comparison methods easily leads to incorrect partitions when some classes are ambiguous. On the contrary, the clustering membership matrix with an adaptive sparseness introduces the fuzzy concept to better tackle such problem.

What is more, it can be found that the AVASM method is not as good as that of the RMVKM, DEKM, RDEKM, and MVASM methods. That is mainly because concatenating all the views not only leads to the overfitting but also confuses various attributed information of different views to reduce the interpretability of multi-view information as well as ignoring the intercoordinations across views. Sometimes, the AVASM method cannot cope with the multi-view data better, and degrades the performance compared with some single-view methods, like the result of KmASM (2) on the MNIST dataset shown in Table 3.

Finally, in order to describe visually, we show the weight of each view and the common membership matrix across the multiple views, which is learned by the proposed MVASM method on different datasets in Figs. 5 and 6. We show the convergence curves of MVASM on four datasets in Fig. 7, which can be seen that the proposed MVASM method converges with a few iterations.

Fig. 7. - 
The convergence curves of (a) COIL20, (b) COIL100, (c) MNIST and (d) YALE datasets.
Fig. 7.
The convergence curves of (a) COIL20, (b) COIL100, (c) MNIST and (d) YALE datasets.

Show All

SECTION 6Conclusion
In this paper, we propose a novel unsupervised clustering method, which introduces a common membership matrix with the adaptive sparseness across different views to tackle the problem of ambiguous classes, and controls the interaction information among the multiple views by learning the multiple centroid matrices and weights adaptively. It not only deeply explores the underlying consensus and complementary information, but also provides an efficient way to utilize view-specific information and further modify the membership matrix. Furthermore, we show the optimization algorithm and its convergence behavior. Extensive evaluations of four widely used datasets have demonstrated that the MVASM method is effective for clustering the multi-view data.