Quality of information (QoI) and quality of experience (QoE) are related notions that capture the general idea that a network’s finite resources should be allocated in a manner that optimizes the goals of its users. Importantly, these goals depend not merely on low-level network statistics (like bit rate) but also the semantics and context of the information, as well as the subjective preferences of its users. The difficulty of accurately modeling human-relevant subjectivity and semantics has complicated the realization of QoE/QoI concepts in a rigorous network optimization algorithm. However, modern networks are increasingly comprised of a large number of non-human actors including intelligent and adaptive machine learning (ML) agents. These agents aim to optimize some complex learning or inference objective. In contrast to human actors, the relevant utility functions that such ML agents seek to maximize are in fact readily quantified and naturally depend on the semantic content and context of the data being communicated. Capitalizing on this insight, in this work we argue that for networks whose data producers and consumers are ML agents, QoI can in fact be operationalized within a network utility maximization (NUM) framework. NUM is a classical framework for constrained network utility optimization over a resource-constrained network, but has traditionally been applied only in settings where the utility functions are hand-designed and semantically agnostic, i.e., functions of low-level network metrics alone, such as latency or bit-rate. We show that when the users of the network are ML agents, the NUM framework can be easily extended to be semantically aware, which in turn enables resource allocations which outperform their semantically agnostic counterparts. We illustrate the value of our approach with experiments with object detection and localization in streaming video over a software-defined network (SDN).

Previous
Keywords
Quality of information

Network utility maximization

Machine learning

Resource allocation

Software-defined networking

1. Introduction
The expanding prevalence of data-intensive services and proliferation of networked devices has pushed the current state of networking near its limit (Dat et al., 2016). The volume and demand for data-rich content such as video has put increasing strains on available network resources in wireless networks. There is an increasing need to optimize scarce network resources with the need to optimize user utility or satisfaction. However, existing algorithms for network optimization, which are based on network utility maximization (NUM) (Kelly et al., 1998), rely only on low-level network metrics (like bit-rate); such metrics are often a poor proxy for what the network truly aims to optimize, which is the utility of an end-user.

The idea that a network should optimize for more than just low-level network measures (like bit-rate) is not a new one, and many attempts have been made to formalize it. We discuss a few prominent attempts here. The most well-known is Quality of Service (QoS). QoS involves prioritizing traffic classes and reserving resources (Vegesna, 2001) and seeks to guarantee a pre-specified level of performance to the data flow of a particular class. However, quality in terms of the end-user’s satisfaction or inference goals is again only indirectly optimized for; once again, network statistics (e.g., signal-to-noise ratio or packet loss) serve as a proxy for quality (e.g., how useful sensor data is to a downstream inference task) (Weng et al., 2011). A more modern notion of utility that is focused on the end-user of a network is “quality of experience” (QoE), which refocuses the notion of “quality” that a network delivers as perceived by a human being. QoE has been defined as “the degree of delight or annoyance of the user of an application or service” which is a function of “the fulfillment of his or her expectations with respect to the utility of the application or service in the light of the user’s personality and current state”. Brunnström et al. (2013). The challenge is that QoE is influenced by a huge variety of factors, including the (time-varying) cognitive ability of the human, gender, culture, network statistics, etc. which makes it difficult to model (Baraković and Skorin-Kapov, 2013). Attempts towards this end have been made (Reichl et al., 2013, Hajiesmaili et al., 2012), but such models of quality are necessarily over-simplified and model QoE, a truly multi-dimensional quantity, instead as a scalar depending on one or a few easily measured network parameters (Hoßfeld et al., 2016). A related notion to QoE is “quality of information” (QoI) which tries to quantify how much the data traversing the network engenders situational awareness and intelligent decision-making (Bar-Noy et al., 2011). QoE and QoI are related concepts, with the former focusing more on the data consumption process while the latter focuses more on the actionable insights generated by this data. QoI is a composite, multi-dimensional function  of the raw data and meta-data including the source of information, its credibility, accuracy (how reflective of ground truth it is), and freshness (how much time has elapsed regarding the phenomenon of interest). Considerable work exists in working out the dimensions of QoI and their inter-relationships (Bisdikian et al., 2013, Bisdikian et al., 2009); however, similar to QoE, a general mathematical modeling framework for its practical realization remains elusive. In summary, QoS leads to realizable network algorithms, but does not fully capture end-user utility; QoE and QoI in principle fully capture end-user utility, but do not lead to realizable networking algorithms.

The aforementioned challenge in defining a mathematically tractable framework for network optimization of quality arises mainly due to the difficulty of creating accurate models of a human’s perception of quality. However, increasingly, networks are being used by artificially intelligent agents to carry out complex distributed learning and inference tasks. For example, the emergence of complex deep learning models and advanced edge devices capable of generating large volumes of data has led to the emergence of distributed machine learning. In the increasingly common case of federated-learning, a common model is learned across multiple, decentralized edge devices which each store local data samples, but where no data is explicitly exchanged between edge nodes. Konečnỳ et al. (2016). Another example is in distributed machine inference, in which data generated by some devices is “offloaded” over the network to others running a machine learning (ML) agent, for example, speech detection (Anon, 0000). Such offloading may be imperative in situations where no single sensor has a complete “view” of the environment and ML must be run on data fused from multiple sources. Indeed, in the emerging Internet-of-Things paradigm, the ubiquitous prevalence of a huge number of primarily low-cost devices (Atzori et al., 2010) implies that complex ML processing will be done via (partial) offloading to more capable compute nodes.

In this paper, we argue that the notions of QoE/QoI can be straightforwardly operationalized when the “users” of the network are machine learning agents. This is because the usefulness of the data perceived by an ML agent can be easily measured by the ML algorithm itself in real-time (e.g., whether the data contains an object of interest or not); this makes it easy to evaluate semantically aware utility functions that directly reflect information quality. This, in turn, allows us to use the rigorous mathematical framework of NUM for network resource allocation.

Our work is in contrast to the large and growing literature on data and computation offloading from edge devices to more capable compute nodes (Wang et al., 2020). The central problem studied therein instead is: how to optimally share the burden of computation among edge nodes (which are data-rich but compute-constrained) and compute nodes (which are compute-rich but data-poor and accessible only through a potentially high-latency or low-throughput network)? Computations may be done entirely at the edge, entirely in the cloud, or via some partial cloud offloading of intermediate computations, depending on factors such as network, battery, memory, and timeliness considerations. Our work here is complementary; whereas the literature focuses on where a given computation should be executed, we focus on how useful that computation is (and further, whether the computation is worth doing), thus adding another important dimension that the resource allocator may optimize over.

The rest of the paper is organized as follows. In Section 2, we highlight classical resource allocation framework of NUM (Kelly et al., 1998) as well as its shortcomings for semantically aware applications. In Section 3, we derive several utility functions which can be used in lieu of the classical ones in standard NUM. These utility functions are semantically-aware and useful for typical ML inference tasks; in addition, they are easily integrated into the NUM framework and lead to true optimization of the QoI of ML pipelines. In Section 4, we introduce the concept of software-defined-networking (SDN) and its usefulness in enabling more flexible networking solutions than available in existing devices. In Section 5, we present experimental results from an implementation of our approach on an SDN testbed illustrating the practical feasibility and effectiveness of our approach on a real-world task of ML-based object detection and tracking. We conclude in Section 6.

2. Classical resource allocation: methods and shortcomings
2.1. Motivation
As a concrete example of why a semantically aware resource allocation scheme could greatly outperform a semantically agnostic one, we present a motivating example which we will refer back to throughout this paper. Consider a networked node (call it “M”) running a machine learning algorithm for object localization in video. M ingests raw input data (e.g. video frames) generated by multiple sensor nodes distributed through a scene of interest. These sensor nodes operate over a common contested wireless channel whose total capacity must be shared among them. M is specifically tasked to locate and track a target of interest (e.g., a ground vehicle). M’s success in tracking the object is clearly improved by sensors containing the object of interest and where the object is in close proximity; data from sensors excluding the object, or where the object is very far from the sensor, convey less value to M. For example, M may be unable to confidently infer the location of a ground vehicle which is distant from a sensor. Suppose there are  deployed sensors wherein sensor  is presently sensing the ground vehicle while sensors  and  currently sense only an empty road or field. Then, the data from sensors  and  are irrelevant to M, regardless of the resolution, freshness, or accuracy of the data or the rate, jitter, or other network statistics of the data packets. Classical resource allocation paradigms such as NUM, which are semantically agnostic, are unable to prioritize the useful information of sensor  because they do not consider the meaning of the data. Indeed, such classical schemes, based on low-level network metrics like signal-to-noise ratio, may in fact end up allocating more resources to sensors  and , at the risk of poorer quality tracking of the ground vehicle. Contrast this to a semantically aware scheme in which M conveys to the network that it is sensing the target from sensor ’s data stream (but not from the other sensors); the network should clearly use this information to prioritize its resources towards sensor . In this paper, we seek to develop a rigorous algorithmic framework for such a semantically aware scheme.

2.2. Network utility maximization
Since our approach builds semantic awareness into NUM, we first review the NUM framework. NUM defines a per-node utility function and provides an algorithm for resource allocation (Kelly et al., 1998) which maximizes the aggregate utility across the network subject to resource constraints. However, because the classical utility functions optimized by NUM are functions purely of low-level network parameters (e.g., the bit-rate), such functions generally fail to truly optimize for the objectives of modern networks, such as enabling complex decision-making or maximizing end-user satisfaction. In other words, low-level network metrics are a poor proxy for what the network should truly be optimizing for. In such a formulation, the utility functions are pre-specified known functions of the resources; for example, if maximizing the sum-rate across the network, the utility function of a node is often taken as some known concave function of the achieved rate.

Concretely, the classical formulation of NUM (Kelly et al., 1998) finds  (the vector of source transmission rates) so that (1)
where  is a matrix of size , where L is the number of links and  is the number of flows. A flow corresponds to a unique (source, destination) tuple. Its  entry is  if and only if data link  is being used to support flow ; else the entry is .  is a concave-increasing function (referred to as a “utility” function) of its argument and  is the vector of link capacities.

A widely used family of utility functions is known as  fair (Srikant, 2004) and is of the form (2)
 
in which the analytical expression for 
 is explicitly known and is designed to encourage some notion of fairness across all data sources in the network. Such functions are appropriate only when utility is purely a function of the raw quantity of data communicated.

2.3. Classical solution technique to NUM
The classical approach to solve (1) is to form the Lagrangian and then solve for the primal variables () and dual variables () (Srikant, 2004). This technique is known as dual decomposition. We first form the Lagrangian (3)
where . In order to decompose the Lagrangian over flows, first denote 
 where 
 denotes the th element of . (4)

First, for fixed , we solve for all , (5)
and then (6)
where 
 denotes the maximum of  and 0. The maximization in (5) is a univariate concave maximization problem and hence easily solved at each network node separately.

2.4. Shortcomings of conventional NUM approach
There are two principal shortcomings of NUM. One, as argued in the Introduction, (1) is purely a function of rate; it is a poor choice for capturing richer notions of utility such as the QoI of a human or intelligent machine agent. The solution to this appears straightforward; simply introduce data-dependence into ; we explore different strategies for this in Section 3. Two, this dependence occurs in a potentially complex and time-varying manner; e.g., this functional dependence of the utility on the data may not be expressible as a tractable closed-form expression (i.e., the utility can only be evaluated point-wise). Also, each source node does not in general know 
, since it does not know how M is exploiting the data for its assigned purpose. Finally, M may be exploiting synergies within the data across multiple sensors so that the total utility does not simply decouple across sensors but is instead a potentially complicated inter-dependent function of the data across multiple sensors. For all these reasons, this means that (5) cannot be analytically solved in general by each source independently.

Because of these factors, (5) is an impractical optimization technique in our setting. We make two modifications. First, since a given sensor may not be able to compute its data utility, the computation in (5) will be replaced by one done by the ML agent M. M can determine the per-sensor utility after processing the data across sensors. Second, we pursue an iterative optimization procedure requiring only point-wise evaluations of utility for the primal (rate) variables instead of a global optimization which requires an analytical expression for the utility. Note that since the semantic content (and hence downstream utility) of a sensor network is constantly changing (in response to a changing environment),  is implicitly an unknown function of time. Iterative optimization is more appropriate in such a non-stationary setting. We next describe the mathematical framework for iterative optimization in NUM based on point-wise evaluations of the utility and its (sub)gradients.

2.5. Incremental algorithm for saddle point optimization
Motivated by (4), we are generally interested in solving the convex–concave saddle point problem  over a domain  where the domain of  and  are closed convex sets. A saddle point of  is a point 
 for which (7)

We adopt the approach in Arrow et al. (1958), which iteratively proceeds as 
(8)
 where 
 denotes a projection onto the set , and 
 is a step-size. The notation 
 denotes a (sub)gradient of  with respect to . Note that this only requires the ability to point-wise evaluate  and its (sub)gradient.

A variety of conditions exist to ensure convergence of these equations; sufficient conditions are given in Nesterov (2009) and we assume those hold here. In particular, we assume that the iterates produced are bounded and that the primal (dual) problem is strictly concave (convex). Then, the deviation  - 
 converges like  to a function of  which monotonically decreases to  as  is decreased to . (A diminishing step-size schedule can be used if the goal is to converge arbitrarily close to the saddle point.) In practical terms, this means one can reach close to the minimizer by choosing a small value of . Since in most real-world settings, the utility of information changes with time and hence the problem setting is non-stationary, we will not be concerned with decaying step-sizes. In this paper, we take .

3. Semantically aware utility functions
Before proceeding, we define some further notation. For conciseness, we suppress explicit dependence of time-varying quantities (such as 
) on time. Denote the data outputted by source (or sensor)  by 
. When we want to make the dependence of 
 on 
 explicit, we will write it as 
, interpreted as the actual data (e.g., video frame) sent by source  across the network when using a rate 
. For clarity of focus on the main ideas, we also ignore other physical-layer factors such as channel noise, channel gain, power budgets, etc. though nothing in our formulation precludes modeling of these factors.

For concreteness, we return to our running example of an ML classifier M tasked to detect objects in some (possibly time-varying) subset of interest, e.g. {soldier, truck, UAV}, drawn from the set of all possible classes that M is trained on. Denote by  the vector whose dimensionality is equal to the total number of classes under consideration.  contains a  entry in position  if class  is of interest to M and  otherwise. In other words,  is an indicator vector capturing the subset of objects of interest to M at time . We also denote by 
 the binary-valued function which takes value  (0) if one or more objects of interest is (not) present in 
.

We define 
 as a non-negative differentiable function capturing the utility the ML classifier derives from processing 
.  must be strictly concave (in 
) in order to ensure convergence. Informally,  captures the usefulness of the semantic content contained in 
 to M’s inference objective.  is implicitly a function of  and is chosen so that it is larger if 
 than if not. Thus, in contrast to (2),  now depends on the data semantics.

3.1. Preventing blind spots in evaluation of utility
If the optimization procedure in (8) is applied to a utility function defined purely in terms of ML algorithm performance, then undesirable phenomena may occur, as we discuss here. Consider first a situation in which a particular sensor’s data conveys no utility to the ML agent (while other sensors do convey positive utility). In a stationary environment, it would be appropriate for M to effectively set such a sensor’s rate to  (i.e., ignore the sensor). However, in reality, in non-stationary regimes, situations can evolve such that a sensor carrying no utility to M’s task presently can in fact carry large utility in the future. If this sensor is allocated no network resources, it will never send data to M and hence M will never discover the positive utility of future data arising from that sensor.

Consider next a situation in which M decreases a sensor’s rate sufficiently so that its inference accuracy over that sensor’s data drops significantly. (In general, M will not know this because it is not given real-time feedback about ground truth.) For example, a low rate allocated to a given sensor may correspond to a low “quality” data stream. This could correspond to low-resolution video frames, for example. Further suppose the object of interest is in fact present in the video stream. However, because of the low quality of the data, M may errantly conclude the data does not contain the object. This would then lead M to further reduce the network resources allocated to the corresponding sensor, resulting in an undesirable “downward spiral” of resource allocation.

Both of the above issues can be side-stepped by ensuring that resource allocations remain above some minimum threshold. Prior knowledge about the environment and the machine learning algorithm inform the choice of this threshold, (e.g., how fast objects are expected to move in the scene of interest, or the minimum tolerable data quality that M requires to make inferences of reasonable accuracy). The essential goal is to shield M from “blind spots” in its assessment of data utility. This information is generally easily obtainable with simple offline experiments before system deployment.

Since (8) requires strict concavity for convergence (and the semantically-aware parts of the overall utility function may not be strictly concave), in our utility functions we always include a strictly concave function 
. For simplicity, in our experiments we choose  to be the log function (which can be shifted/scaled appropriately based on prior knowledge). For example, in an object detection scenario, there may be a range of rates (and hence compression levels) for which the object is detected with reasonable accuracy. The utility would then be nearly constant over this range of rates. The main purpose of  is to encourage (strict) concavity in the overall utility function.

3.2. Types of utility functions
We next present several example utility functions which are semantically aware, divided into two categories, direct and indirect. Direct utility functions seek to directly optimize for the machine learning algorithm’s inference goal, such as minimizing the model’s empirical loss or maximizing the model’s internal confidence. However, there are practical challenges associated with such utility functions, as we will see. This motivates the use of utility functions which indirectly optimize for the machine learning algorithm’s inference goal. For example, in a speech recognition example, a direct utility function might have positive utility for every word correctly recognized. However, such a utility function requires an oracle to provide real-time feedback, which is impractical. An indirect utility function might assign higher utility to words spoken loudly than faintly, or to utterances with less background noise than with more background noise, since in general the former scenarios should lead to more accurate speech recognition than the latter ones.

3.3. Direct utility functions
3.3.1. Oracle-based
It is useful to first consider an ideal (but non-practical) oracle-based utility function. For example, assuming M is tasked to conduct a particular inference, the utility function could be (9)
𝟙
where 𝟙 is  if its input argument is true and  otherwise.  is a set of all data points which are semantically relevant to the task; 
 is the true ground-truth label, and 
 is the label assigned by the agent. In an object tracking example,  would be the ground truth bounding boxes of objects of interest, M the machine learner’s outputted bounding boxes, and  the set of all images containing objects of interest. Thus the first clause in the expression captures correctness of the ML algorithm while the second clause captures semantic relevance of the data to the inference objective. Ignoring the potential non-concavity of the utility function, it is unrealizable because it requires an oracle to provide ground-truth labels  as well as an indication of whether 
.

3.3.2. Confidence-based
Perhaps the closest realizable version of the oracle utility function is one based on the machine learner’s internal “confidence” of its output. The idea is to incentivize network resource allocations which make M more certain about its performance. Let 
 be the vector of class probabilities outputted by the ML agent (e.g. via a final softmax layer). Then, we define (10)

The term 
 captures the idea that M derives large utility from data in which it is highly confident (large corresponding value of 
) and which is relevant (the corresponding entries in  are ).

Consider the rate-derivative of the utility function, which is (11)
 
 
 
If M employs a fully differentiable model (common for modern ML models), then 
 is computable by backpropagating from the softmax layer to the input. 
 
 depends on the data modality and compression scheme being employed; for example, for an image, it captures how any pixel would change for a small change in the rate, and hence amount of compression applied to the image. Assuming M knows the compression scheme used, it can form e.g. a finite-difference based estimate by manually degrading 
 (e.g., increasing slightly the image compression factor).

Note that the assumption that 
 is concave in 
 implies, e.g., that the higher the resolution of an image that is generated by a source containing an object of interest, the higher the resulting confidence, but that the marginal gains to confidence diminish with increasing resolution. This is an intuitive property (one that human perception would be plausibly compatible with) but unfortunately in our experiments we found this to not hold in practice for modern ML models. In other words, it is often the case that increasing the data quality (e.g, resolution) decreases the confidence of an ML model, and vice-versa. This apparently paradoxical behavior is a result of the fact that modern ML methods are poorly calibrated; that is, their probability estimates are poor representatives of the ground truth probability (Guo et al., 2017). Although several, often dataset-specific heuristics to remedy this issue have been proposed, no general-purpose robust solution presently exists. For example, there has been an explosion of proposed uncertainty quantification ideas based on Bayesian inference ideas applied to neural networks, but these do not appear to generally yield meaningful probability estimates in deep learning settings (Yao et al., 2019).

This fact motivates us to consider indirect utility functions, i.e., those which (i) optimize some property that is strongly related to the machine learning algorithm’s goal and hence whose optimization should in general lead to improvements in ML algorithm performance, and (ii) have, or are more likely to have, the (strict) concavity property.

3.4. Indirect utility functions
3.4.1. Volume-based
This notion of utility captures the idea that a machine learning system performs better the more prominent the object of interest is within the data. For example, in many cases, M’s inferences are more accurate the closer its proximity to a target of interest. For example, objects (words) nearer to a camera (microphone) are more likely to be accurately detected (transcribed). While proximity is often not directly measurable, a useful surrogate is how much “volume” the specific target’s data occupies within the overall data 
. The specific measure employed will depend on the data modality; for audio, this could be captured by the sound (decibel) level of a specific target signature; for video, it could be the fraction of the total field-of-view occupied by the target. A possible utility function in this case is (12)
where  is the vector of volumes (e.g. areas of bounding boxes) of targets. This utility function will lead to allocation of resources to sources which contain data in which the target is prevalent (large corresponding value of 
) and which is relevant (the corresponding entry in  is ).

As mentioned in Section 3.1, we assume that the system operates at or above a minimum-rate regime in which M can perform its inference goal with an acceptable accuracy level. For example, if M classifies objects in video, the system designer will know beforehand the minimum resolution for M to perform reasonably well. However, if resources are available, M may still wish to operate above this minimum level, due to marginal improvements in accuracy, confidence, or perception of data by a human analyst accompanying M or studying its decision-making performance after the fact.

We can compute the derivative of (12) as (13)
 
 
 

As long as we are operating above some minimum-rate threshold, then the second term in (13) is well-approximated by , since changing the rate above this threshold will typically not alter the ability of M to detect a target and its associated volume. (Note, however, that if we are below the minimum-rate regime, then M may fail to detect a target which is in fact present, or detect this target with large bounding box errors.) As long as 
 is constant above the minimum-rate regime, then (12) inherits strictly concavity from .

3.4.2. Change-based
This type of utility captures general situations in which detection of “changes” in the environment are desired. There is a large body of literature on the exploitation of sensors for the purposes of change detection (Veeravalli and Banerjee, 2014). However, that literature does not typically consider network constraints in the problem formulation.

For example, M may be interested in detecting situations in which the probability model for the current data point 
 is small, i.e. in which the current observation is unlikely given the history. In such a situation, M may wish to prioritize sensors whose information is anomalous with respect to the data model.

As mentioned in Section 3.3.2, the poor probabilistic calibration of modern ML models suggests we may not be able rely on a good probabilistic model . Instead, we can heuristically set the utility to be (14)
where the  subscript indicates time,  is some feature representation of the input (e.g., one of the intermediate layers in a deep neural network), and  denotes a norm. In other words, this utility function prioritizes sensors whose detected changes in feature representations of data from consecutive time instants is large. Of course, this is just an example and many other possibilities exist to capture relevant notions of change detection. As an example, in a distributed camera setup, the utility function would prioritize data from those cameras capturing video with high activity (such as a busy street crossing) over those capturing data from a region of low activity (such as an empty street). As before, assuming the feature representations are minimally impacted above a minimum-rate threshold, the utility function is strictly concave due to .

4. Software-defined networking
The notion of endowing networks with a capability to optimize resource allocation that is semantically aware requires networking paradigms that are richer than those which conventional networks currently provide. In this work, we have taken the relatively simple example of machine-agent-controlled rate allocation. More generally, however, in order to realize the full potential of optimizing QoI, the network must provide a flexible framework enabling fine-grained agile control over network traffic routing, service load balancing, security policies, bandwidth allocation, and responses to node and link failures. For example, consider a scenario in which M is aiming to do near-real-time object detection as part of its goal to accurately track a high value target. Therefore a reasonable utility function for M might specify larger utility from lower latency data sources (and which also carry semantically relevant data) than higher latency ones. Thus M may elect to allocate not only higher rate to such semantically-relevant sensors but also route their data over lower latency links.

Conventional networks built largely on rigid devices with fixed application-specific integrated circuits (ASIC) were never designed to enable intelligent network adaptation and lack the speed, reasoning, and agility required to support this goal. These networks are limited by distributed control planes, each with a local view of the network, and require manual (re-) configuration of each physical networked device in order to effect changes in networking behavior (Kreutz et al., 2015). In addition, classical networking devices expose very few application programming interfaces (APIs) for controlling system-wide network behavior in response to semantic/contextual content in network flows.

Consider an example in which the ML agent M is consuming data from several sensors for an inference objective, and a subset of these sensors are providing it with large utility. M may wish to perform complex link-scheduling operations which prioritize the links along the paths from each of those sensors to M to ensure that at least one of those paths delivers its sensor feed to M in a timely manner. Such operation requires coordination across multiple networked devices and conventional networks do not expose easy mechanisms to achieve such coordination nor implement easily the resulting desired changes to network parameters. Deploying new networking features (e.g., routing algorithms) in conventional networks is very difficult, in many cases requiring modifications to the control plane of all network devices through the installation of new firmware and possibly new hardware middleboxes (Kreutz et al., 2015).

In response to increased demands for network performance and flexibility, software-defined networking (SDN) has emerged as an alternative modern solution that aims to mitigate some of the limitations in conventional networks. The SDN paradigm uses a holistic approach for managing networks by separating the network control plane and the data planes. This allows centralized software-based controllers (e.g., such as a controller under M’s control), to remotely configure the low-level behavior of distributed network devices via APIs, such as OpenFlow (McKeown et al., 2008). Consequently, SDN’s programmable APIs become an attractive interface to introduce ML into communication networks (Xu et al., 2017). The APIs give M direct access to the network allowing M to rapidly and intelligently orchestrate complex network-wide behavior. This orchestration can be a result of an optimization procedure (like NUM) or a set of a priori chosen heuristics.

In this paper, we view SDN as an enabler for QoI based network optimization; indeed, previous work (Karakus and Durresi, 2017) has identified SDN as an important paradigm for QoS/QoE optimization. Machine learning enables situational awareness via rapid mapping of raw data to semantics (e.g. activity recognition), while SDN provides a flexible and simple framework to enable complex networking behaviors. The combination of the two enables near-real-time semantically-aware network re-configuration, wherein a machine learning algorithm can easily and proactively shape network behavior. In the context of our motivating example, M can easily instruct an SDN controller to deploy flow rules that allocate appropriate bandwidth resources to each sensor based on that sensor’s provided utility.


Download : Download high-res image (429KB)
Download : Download full-size image
Fig. 1. Schematic of example scenario that forms the basis for experiments in this paper. Three video sensors operating over a resource-constrained network send their data (via a common relay node) to a node running a machine learning algorithm. The ML node performs an inference task over the video streams; some streams carry more utility to its inference goal than others. It uses NUM to compute optimal utility-aware resource allocations and uses SDN to realize these allocations. More details are in main text.

5. Experiments
Fig. 1 provides a schematic depicting our experimental setup. We adopt a scenario wherein  video sensors operating over a resource-constrained network are sending their video streams to a relay node, which communicates the data to a node running a machine learning algorithm (YOLO Redmon et al., 2016) which does object detection and localization. The  video sensors share a common channel with a maximum capacity of  MBps. The ML node is tasked with tracking a high-value target (HVT), which in this case is a vehicle. Each sensor provides a distinct view of the environment based on its location. The ML node determines the utility provided by each sensor’s data and uses the NUM-based algorithm presented in this paper to control the SDN to re-allocate resources (which we take to be bit-rate in this paper) accordingly. In our scenario, the HVT is taken to be a red truck which is driving back and forth along a stretch of road.

To test our ideas, we manually construct a simple video stream capturing a single red truck driving back and forth across a road. It is important to note that our method can be applied to arbitrary complex datasets with multiple objects of interest moving in arbitrary ways; we choose a simple scenario here (single object, linear motion) because it allows us to analytically derive ground-truth optimal rate allocations and compare our algorithm’s solutions to ground-truth. The top left, right, and bottom left subpanels of Fig. 2(a) show some representative images of this dataset. Seven images per second are sent by each sensor (effectively,  frames per second of video). There are also available  (JPEG) distinct compression levels for any given image. (Note that to maintain focus on the core ideas in this paper, we do not employ popular video encoding frameworks for video transmission, instead modeling each video frame as a distinct image.) Correspondingly, our SDN setup exposes one of  selectable rates for M to choose. Thus, higher rates map to higher quality (lower compression) video frames. In all experiments, the underlying rate variable is continuous but is quantized (to the closest quantized rate smaller than or equal to it) to one of these  values when selecting an SDN rate.

We model our experimental setup within a tactical SDN experimentation testbed. This testbed emulates SDNs deployed in scenarios featuring realistic wireless communication channels and networked devices with mobility patterns inherent to military operations (Marcus et al., 2018). The testbed runs the Extendable Mobile Ad-hoc Network Emulator (EMANE) (AdjacentLink, 2020), which provides wireless radio emulation capabilities. The testbed uses a distributed EMANE model, where each wireless node (which runs within a virtual machine, or VM) is mapped to a single emulated EMANE platform equipped with one or more radios. Three VM nodes, representing the 3 video sensors, run the EMANE application stack, which is responsible for modeling the node’s mobility (although in this paper we take static sensors for simplicity of analysis) and network link-state/connectivity. Each VM node also runs Open vSwitch (Anon, 2020), which allows its network traffic to be dynamically managed by SDN controllers via flow rules and QoS policies. Finally, each VM node runs a streaming video application (capturing video frames of the scene of interest). The VM node representing our ML agent M runs a similar application stack as the sensors but also runs the YOLO object detection software as well as our NUM-based utility optimization algorithm. The ML-based object detection YOLO v2 algorithm (Redmon et al., 2016) provides, for each frame of a video stream, bounding boxes around all detected objects as well as the associated confidence of that detection.

Fig. 2(a) illustrates a single snapshot in time of our scenario. The bottom right subpanel of Fig. 2(a) illustrates schematically the HVT (red circle) and the 3 static camera sensors (colored top to bottom as green, black, and blue). The sensors are linearly orientated with respect to each other along the road; also, the HVT moves up and down along the road. The remaining  subpanels of Fig. 2(a) show the respective views (video frames) of the HVT from the  sensors for the particular time snapshot shown in the bottom right subpanel of Fig. 2(a). The color correspondence establishes the relation between sensors and views (e.g., the title of the bottom left subpanel of Fig. 2(a) for sensor  is colored blue, which corresponds to the bottom most dot in the bottom right subpanel). The fact that the truck appears largest in Sensor ’s field of view also corresponds with the fact that in the bottom right subpanel, the red point (HVT) is closest to the green point (Sensor 1).

5.1. Object detection
We begin with an object detection example in which the goal of M is to determine whether the HVT is present in the scene or not. A good (indirect) semantically aware utility function for this desired goal is the volume-based utility described in Section 3.4.1, which we study first.

5.1.1. Volume-based
Recall that the volume-based utility function is appropriate when we generally expect better ML performance the greater the proximity of the object of interest to a sensor. We begin with the utility function defined in (12). For simplicity, we assume there is a single HVT (i.e.,  contains a single non-zero entry corresponding to the “truck” category).

We apply the update equations (15)
 
where 
 returns  if  and  otherwise. This enforces dual feasibility for  and a minimum acceptable rate for . We computed the minimum threshold for  (which in our scenario is  kBps) based on experiments of how much compression the YOLO algorithm can handle and still exhibit reasonable accuracy. We note that since typical bounding boxes in images are on the order of 100 × 100, we normalize  by dividing it by 
. This results in more stable updates.


Download : Download high-res image (671KB)
Download : Download full-size image
Fig. 2. (a) View of one snapshot in time for a target-tracking example involving  camera sensors and a high value target (red truck). The bottom right subpanel shows the relative position of the HVT with respect to the sensors, while the other subpanels show the view of each sensor. (b) The optimal rate allocations vs. time assuming perfect knowledge of the sensor and HVT locations. (c) The rate allocations produced by the iterative algorithm (15) vs. time. (d) The rate allocations of the algorithm running over the SDN testbed vs. time.

5.1.2. Closed form solution
For this simple setup, it is possible to derive a closed form solution to the optimal rate allocations for an oracle who knows the exact location of the HVT and all the sensors. We adopt the utility formulation as in (12).

Denote by 
 the location of the HVT at time  and 
 the location of sensor s at time . It is a well-known fact that the size of an object in a camera is well-approximated as being inversely proportional to the distance of that object from the camera. Hence, our optimization problem can be written (16) 
 
 

Here for mathematical brevity we have assumed each sensor contains the object of interest; the solution is easily modified otherwise by setting the non-interesting sensors’ rates to the minimum allowable value. The solution to (16) is easily derived by making the following observations. First, at the optimum, the sum of rates must be exactly  since the log function is strictly increasing. Also, no rate can be  since the log is  there. Thus, the problem’s Lagrangian is (17)
 

Because strong duality holds for (16), the KKT conditions are necessary and sufficient; equating the derivative of (17) with respect to 
 to  and solving for 
 gives (18)
 
 
 

5.1.3. Results
Fig. 2(b) plots the oracle solution (18) to problem (16) as a function of the time index. Fig. 2(c) plots the solution obtained by our iterative algorithm (15) in a simulated setup when run locally (i.e., not over an actual network). It should be noted that the HVT begins at the center (black point in the bottom right subpanel of Fig. 2(a)) before moving up (towards sensor 1, green point in bottom right subpanel), then down (towards sensor 2, blue) and then back towards sensor 1. The experiment is shown over  time steps (where each time step is 0.1 s), enough for the HVT to move along the road along the path (sensor 0, sensor 1, sensor 0, sensor 2, sensor 0, sensor 1). The oracle solution shows the optimal rate allocation by an oracle who knows the location of the HVT at all times. The iterative algorithm’s solution qualitatively matches the oracle’s solution; note that unlike the oracle, the iterative algorithm knows nothing about ground-truth (i.e. the true locations of the HVT or the sensors). The differences between the two arise from two primary contributors. One, our algorithm uses rate quantization, since we only allow a finite set of selectable rates instead of a continuum. Two, a close examination shows that our algorithm’s results slightly lag the oracle’s; this is because of the iterative algorithmic overhead of (15) and the presence of memory in the rate. Nonetheless, the discrepancy is very small, indicating this overhead is minimal.

Fig. 2(d) shows the rate allocations as a function of time for the same experiment conducted over an actual network with SDN. “Real-world” effects such as latency (due to network control), packet loss, etc. all contribute to the very slight discrepancies between subfigures (c) and (d). Again, the fact that discrepancies are so small indicate that this overhead is ignorable. Note that the actual bandwidth usage, i.e. the sum of allocated rates across all sensors, at any given time, is very close to  (the maximum link capacity), as it should be.

5.1.4. Comparison with other utility functions
We next compare the volume-based utility function with two others. First we consider a semantically agnostic (and often classically chosen) utility function 
. We also consider the confidence-based utility function given in (10).

Fig. 3 plots the results of the allocated rate as a function of time for both approaches. For the classical log-utility function in panel (a), the behavior is as expected; roughly equal resources are given to all  sensors. For the confidence-based utility in panel (b), the rate allocations seem to switch somewhat arbitrarily, which is also expected given the probabilistic calibration issues of neural networks discussed earlier (and hence the resulting non-concavity of the confidence-based utility function). The frequent changes in rate for the confidence-based utility are potentially problematic, since there is presumably a cost for a sensor to be frequently changing its data generation rate. Frequent rate changes also generate unnecessary control traffic. Over the duration of our experiment, the confidence-based utility function required a total of  rate changes (and hence the same number of control messages to be broadcast by SDN) while the volume-based utility required a total of .

In Fig. 4, we show boxplots of the ML algorithm’s confidences over its inference about the location of the HVT for all  utility functions considered. In more detail, for each timestep, the median confidence across the video frames from all  sensors is computed. The distribution of these median values is shown in the boxplots. For the log-based utility function, the distribution of confidences has the largest spread, while it is tightest for the volume-based utility function. Thus the volume-based utility function achieves the tightest confidence distribution of all the utility functions tested, and with fewer SDN rate changes than the confidence-based utility function. Note that the volume-based utility outperforms the confidence-based utility in terms of achieved confidence even though the latter utility is explicitly aiming to optimizing confidence. This is an indication that well-designed utility functions (e.g., based on prior knowledge) can improve M’s performance while utilizing network resources more judiciously.


Download : Download high-res image (268KB)
Download : Download full-size image
Fig. 3. Rate allocations vs. time for (a) log and (b) confidence-based utility functions. The log-based utility assigns equal resources to all sensors, while the confidence-based utility exhibits frequent and sometimes sporadic rate changes.

5.2. Object localization
We next consider the problem of object localization for the same scenario. Using the inverse relationship between the area of a bounding box and true distance from a camera, given sensor locations 
 we can compute a localization estimate for the HVT 
 using the set of equations (19)
 
Here, 
 is the (ML algorithm computed) bounding box area of the HVT, and  is an unknown constant of proportionality. The sensor locations 
 are known. For each time step independently, we solve (19) for 
 using the Gauss–Newton nonlinear optimization algorithm. More sophisticated algorithms, including those that make use of the dependence of the HVT location across time, are possible but those are outside the scope of this paper.

In contrast to Section 5.1, where the implicit goal was to find at least one sensor containing the HVT, here all three sensors must capture the HVT, otherwise (19) does not yield a unique localization estimate. In other words, in some sense, in this scenario M’s inference is only as strong as the data from the “weakest” sensor. Therefore, for the confidence-based utility function, it is now sensible to allocate more resources to the sensor containing the HVT with the lowest confidence. Similarly, for the volume-based utility function, we allocate resources to the sensor containing the HVT with the  volume. The intuition is that higher-quality data is necessary from sensors for which the HVT is further away, since it will be harder for the ML algorithm to estimate the occupied HVT volume in such cases; and inaccurate volume estimates in even a single sensor’s data will lead to poor object localization accuracy.

As before, we compare three utility functions: i) the classical log-based utility which is agnostic to semantic content, ii) the inverse confidence based utility function 
 
, and (iii) the inverse volume based utility function 
 
. We choose ; its main functions is to prevent division by  (in cases where the HVT is not detected). The inverse-confidence and inverse-volume utility functions are the analogs of the confidence-based and volume-based utility functions introduced in Eqs. (10), (12), respectively; they are essentially inverted. These latter types of utility functions are useful in situations where the system performance is bottlenecked by the “weakest” component of the system (and hence we wish to allocate more resources to this component in the hopes of strengthening it).

Fig. 5 plots the fraction of all time steps for which we are able to obtain an accurate (within  m) localization estimate of the HVT, for the three different utility functions. The (inverse) confidence-based utility functions performs poorest, because there is often no concrete relationship between the computed confidence and the true difficulty of computing the HVT bounding box. The (inverse) volume-based utility functions performs the best, since it typically allocates resources to the sensor from which the HVT is furthest. Also, the inverse-confidence-based utility function requires  total SDN rate adjustments, while the inverse-volume-based utility function requires only . The overall localization success rate is not higher because for this experiment, even in the highest-quality (least compressed) images, the ML algorithm is often not able to detect the HVT in multiple sensor views due to its small size. In other words, the problem is intrinsically hard; in real-life scenarios, one would remedy this by using more geographically dispersed sensors.

6. Conclusion
In this paper, we have presented a paradigm for intelligent dynamic resource allocation over a resource-constrained network which is generating and consuming traffic in the ultimate service of a machine learning algorithm M. We have argued that since M’s learning or inference performance is objectively quantifiable, it becomes possible to adapt the notion of “quality of information”, originally conceived of for a subjective human-centric network actor, to machine learning agents. We have defined several plausible utility functions (i.e., useful measures of M’s QoI) and showed how they can be integrated naturally into the framework of network utility maximization to yield a network adaptation control law. This framework naturally enables network adaptation which is sensitive to M’s performance as a function of the semantic content of information, and not merely to the raw throughput or other classical measures of network performance. Our experiments show that the network is adapted in a semantically-aware manner, prioritizing sensors carrying data of interest to the machine learning algorithm.

Several directions warrant future exploration. We have seen that selecting the appropriate utility function is critical to good performance. This choice may not always be obvious and some offline experiments may be necessary. When such functions are derived exclusively from optimizing quantities internal to the ML agent (like confidence), poor performance may result since these quantities are not always well-calibrated with respect to the end goal. The volume-based utility function includes prior knowledge, and in general the choice of utility function is a powerful mechanism by which prior knowledge can be injected into the resource allocation process. Good utility functions not only encourage resource allocations which improve ML performance, but also help avoid the overhead of rapid and oscillatory changes to resources.

Also, other ML inference scenarios (besides object tracking) can be studied and with different data modalities. Understanding and exploiting synergy across sensors (e.g., sensors which bring complementary views of the environment), in which scene understanding is only possible by combining data across sensors, brings with it interesting opportunities to design utility functions. A simple example might be where a person’s speech in a noisy environment can be accurately decoded only by reading her lips and decoding her audio jointly. This requires multi-modal machine learning models combining data across multiple sensors. Which sensors to prioritize in a resource-constrained environment is then a potentially complex function of the input modalities, with some modalities (like audio) requiring far less resources than others (like video). In such cases, bringing in ideas from the emerging field of explainable machine learning (Gunning, 2017), which seeks to understand the reasons behind a particular inference (“what features in the data are most important to aiding M in doing speech recognition?”), will become important drivers for accurate utility assessment and network adaptation in complex machine learning problems (“how can we adapt the network to prioritize acquisition of those features?”). Finally, expanding our work to include other aspects of network adaptation besides rate control, including routing, power control, and resilience to link errors, is of great interest.