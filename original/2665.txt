Multi-hop reading comprehension focuses on one type of factoid question, where a system needs to properly integrate multiple pieces of evidence to correctly answer a question. Previous work approximates global evidence with local coreference information, encoding coreference chains with DAG-styled GRU layers within a gated-attention reader. However, coreference is limited in providing information for rich inference. We introduce a new method for better connecting global evidence, which forms more complex graphs compared to DAGs. To perform evidence integration on our graphs, we investigate two recent graph neural networks, namely graph convolutional network (GCN) and graph recurrent network (GRN). Experiments on two standard datasets show that richer global information leads to better answers. Our approach shows highly competitive performances on these datasets without deep language models (such as ELMo).
SECTION 1Introduction
Recent years have witnessed a growing interest in the task of machine reading comprehension. Most existing work [1], [2], [3], [4], [5], [6], [7], [8] focuses on a factoid scenario where the questions can be answered by simply considering very local information, such as one or two sentences. For example, to correctly answer a question “What causes precipitation to fall?”, a QA system only needs to refer to one sentence in a passage: “... In meteorology, precipitation is any product of the condensation of atmospheric water vapor that falls under gravity. ...”, and the final answer “gravity” is indicated by the key words of “precipitation” and “falls”.

A more challenging yet practical extension is multi-hop reading comprehension (MHRC) [9], where a system needs to properly integrate multiple pieces of evidence to correctly answer a question. Fig. 1 shows an example, which contains three associated passages, a question and several candidate choices. In order to correctly answer the question, a system has to integrate the facts “The Hanging Gardens are in Mumbai” and “Mumbai is a city in India”. There are also some irrelevant facts, such as “The Hanging Gardens provide sunset views over the Arabian Sea” and “The Arabian Sea is bounded by Pakistan and Iran”, which make the task more challenging, as an MHRC model has to distinguish the relevant facts from the irrelevant ones.

Fig. 1. - 
An example from WikiHop [9], where some relevant entity mentions and their anaphoric pronouns are highlighted.
Fig. 1.
An example from WikiHop [9], where some relevant entity mentions and their anaphoric pronouns are highlighted.

Show All

As a practical task, so far MHRC has received increasing research attention. One notable method, Coref-GRU [10], uses coreference information to gather richer context for each candidate. However, one main disadvantage of Coref-GRU is that the coreferences it considers are usually local to a sentence, neglecting other useful global information. In addition, the resulting DAGs are usually very sparse, thus few new facts can be inferred. The top part of Fig. 2 shows a directed acyclic graph (DAG) with only coreference edges. In particular, the two coreference edges indicate the two facts: “The Hanging Gardens provide views over the Arabian Sea” and “Mumbai is a city in India”, from which we cannot indicate the ultimate fact, “The Hanging Gardens are in India”, for correctly answering this instance.

Fig. 2. - 
A DAG generated by Dhingra et al. [10] (top) and a graph by considering all three types of edges (bottom) on the example in Fig. 1.
Fig. 2.
A DAG generated by Dhingra et al. [10] (top) and a graph by considering all three types of edges (bottom) on the example in Fig. 1.

Show All

We propose a general graph scheme for evidence integration, which allows information exchange beyond co-reference nodes, by allowing arbitrary degrees of the connectivity of the reference graphs. In general, we want the resulting graphs to be more densely connected so that more useful facts can be inferred. For example each edge can connect two related entity mentions, while unrelated mentions, such as “the Arabian Sea” and “India”, may not be connected. In this paper, we consider three types of relations as shown in the bottom part of Fig. 2.

The first type of edges connect the mentions of the same entity appearing across passages or further apart in the same passage. Shown in Fig. 2, one instance connects the two “Mumbai” across the two passages. Intuitively, same-typed edges help to integrate global evidence related to the same entity, which are not covered by pronouns. The second type of edges connect two mentions of different entities within a context window. They help to pass useful evidence further across entities. For example, in the bottom graph of Fig. 2, both window-typed edges of ① and ⑥ help to pass evidence from “The Hanging Gardens” to “India”, the answer of this instance. Besides, window-typed edges enhance the relations between local mentions that can be missed by the sequential encoding baseline. Finally, coreference-typed edges, each of which connects a pronoun and the corresponding mention, are further complimentary to the previous two types, and thus we also include them.

Since our generated graphs are complex and can have cycles, making it difficult to directly apply a DAG network (e.g. the structure of Coref-GRU), we adopt graph neural networks [11], which can encode arbitrary graphs. In particular, we choose graph convolutional network (GCN) and graph recurrent network (GRN), as they have been shown successful on encoding semantic graphs [12], dependency graphs [13], [14], [15], [16], raw texts [17] and other complex data structures [18], [19], [20].

Given an instance containing several passages and a list of candidates, we first use NER and coreference resolution tools to obtain entity mentions, and then create a graph out of the mentions and relevant pronouns. As the next step, evidence integration is executed on the graph by adopting a graph neural network on top of a sequential layer. The sequential layer learns local representation for each mention, while the graph network learns a global representation. The answer is decided by matching the representations of the mentions against the question representation.

Experiments on WikiHop [9] show that our created graphs are highly useful for MHRC. On the hold-out testset, it achieves a highly-competitive accuracy of 65.4 percent. In addition, our experiments show that the questions and answers are dramatically better connected on our graphs than on the coreference DAGs, if we map the questions on graphs using the question subject. Our experiments also show a positive relation between graph connectivity and end-to-end accuracy.

On the testset of ComplexWebQuestions [21], our method also achieves better results than all published numbers. To our knowledge, we are among the first to investigate graph neural networks on reading comprehension tasks. Our code is available at https://github.com/freesunshine0316/MHQA.

SECTION 2Related Work
Question Answering With Multi-Hop Reasoning. Multi-hop reasoning is an important ability for dealing with difficult cases in question answering [22], [23]. Most existing work on multi-hop QA focuses on hopping over knowledge bases or tables [24], [25], [26], thus the problem is reduced to deduction on a readily-defined structure with known relations. In contrast, we study multi-hop QA on textual data and we introduce an effective approach for creating evidence integration graph structures over the textual input for solving our problems. Previous work [7], [27] studying multi-hop QA on text does not create reference structures. In addition, they only evaluate their models on a simple task [28] with a very limited vocabulary and passage length. Our work is fundamentally different from theirs by modeling structures over the input, and we evaluate our models on more challenging tasks.

Recent work starts to exploit ways for creating structures from inputs. Talmor and Berant [21] build a two-level computation tree over each question, where the first-level nodes are sub-questions and the second-level node is a composition operation. The answers for the sub-questions are first generated, and then combined with the composition operation. They predefine two composition operations, which makes it not general enough for other QA problems. Dhingra et al. [10] create DAGs over passages with coreference. The DAGs are then encoded using a DAG recurrent network. Our work follows the second direction by creating reasoning graphs on the passage side. However, we consider more types of relations than coreference, making a thorough study on evidence integration. Besides, we also investigate recent graph neural networks (namely GCN and GRN) on this problem.

Being released in the same period and similar to our work, Cao et al. [29] also proposed a graph-based model using a GCN for multi-hop reasoning. In addition to the graph neural network being used, there are also differences on graph construction: (1) they only consider the question subject and the candidates as graph nodes, while we consider all named entities (including both the question subject and the candidates), and (2) they further distinguish inter- and intra-paragraph edges with the same relation, while we consider them as the same type. There are also later efforts that follow this path by explicitly creating graphs for reasoning. Tu et al. [30] further propose a heterogeneous graph network to separately model different types of graph nodes. Cao et al. [31] enrich a graph-based model with rich features of POS and NER tags, and they also add a bi-directional attention module to their model.

Question Answering Over Multiple Passages. Recent efforts in open-domain QA start to generate answers from multiple passages instead of a single passage. However, most existing work on multi-passage QA selects the most relevant passage for answering the given question, thus reducing the problem to single-passage reading comprehension [32], [33], [34], [35], [36]. Our method is fundamentally different by truly leveraging multiple passages.

A few multi-passage QA approaches merge evidence from multiple passages before selecting an answer [37], [38], [39]. Similar to our work, they combine evidences from multiple passages, thus fully utilizing input passages. The key difference is that their approaches focus on how the contexts of a single answer candidate from different passages could cover different aspects of a complex question, while our approach studies how to properly integrate the related evidence of an answer candidate, some of which come from the contexts of different entity mentions. This increases the difficulty, since those contexts do not co-occur with the candidate answer nor the question. When a piece of evidence does not co-occur with the answer candidate, it is usually difficult for these methods to integrate the evidence. This is also demonstrated by our empirical comparison, where our approach shows much better performance than combining only the evidence of the same entity mentions.

SECTION 3Baseline
As shown in Fig. 3, we introduce two baselines, which are inspired by Dhingra et al. [10]. The first baseline, Local, uses a standard BiLSTM layer (shown in the green dotted box), where inputs are first encoded with a BiLSTM layer, and then the representation vectors for the mentions in the passages are extracted, before being matched against the question for selecting an answer. The second baseline, Coref LSTM, differs from Local by replacing the BiLSTM layer with a DAG LSTM layer (shown in the orange dotted box) for encoding additional coreference information, as proposed by Dhingra et al. [10].

Fig. 3. - 
Baselines. The upper dotted box is a DAG LSTM layer with addition coreference links, while the bottom one is a typical BiLSTM layer. Either layer is used.
Fig. 3.
Baselines. The upper dotted box is a DAG LSTM layer with addition coreference links, while the bottom one is a typical BiLSTM layer. Either layer is used.

Show All

3.1 Local: BiLSTM Encoding
Given a list of relevant passages, we first concatenate them into one large passage p1,p2…pN, where each pi is a passage word and xpi is the embedding of it. It adopts a Bi-LSTM to encode the passage:
h←iph→ip=LSTM(h←i+1p,xpi)=LSTM(h→i−1p,xpi).
View SourceRight-click on figure for MathML and additional features.Each hidden state contains the information of its local context. Similarly, the question words q1,q2…qM are first converted into embeddings xq1,xq2…xqM before being encoded by another BiLSTM:
h←jqh→jq=LSTM(h←j+1q,xqj)=LSTM(h→j−1q,xqj).
View SourceRight-click on figure for MathML and additional features.

3.2 Coref LSTM: DAG LSTM Encoding With Conference Knowledge
Taking the passage word embeddings xp1,…xpN and coreference information as the input, the DAG LSTM layer encodes each input word embedding (such as xpj) with the following gated operations:1
ijojfi,jujc→jph→jp=σ⎛⎝Wixpj+Ui∑i∈N(j)h→ip+bi⎞⎠=σ⎛⎝Woxpj+Uo∑i∈N(j)h→ip+bo⎞⎠=σ(Wfxpj+Ufh→ip+bf)=σ⎛⎝Wuxpj+Uu∑i∈N(j)h→ip+bu⎞⎠=ij⊙uj+∑i∈N(j)fi,j⊙c→ip=oj⊙tanh(c→jp),
View SourceRight-click on figure for MathML and additional features.Nj represents all preceding words of pj in the DAG, ij, oj and fi,j are the input, output and forget gates, respectively. Wx, Ux and bx (x∈{i,o,f,u}) are model parameters.

3.3 Representation Extraction
After encoding both the passage and the question, we obtain a representation vector for each entity mention ϵk, spanning from ki to kj, by concatenating the hidden states of its start and end positions, before they are correlated with a fully connected layer:
hkϵ=W1[h←kip;h→kip;h←kjp;h→kjp]+b1,(1)
View SourceRight-click on figure for MathML and additional features.where W1 and b1 are model parameters for compressing the concatenated vector. Note that the current multi-hop reading comprehension datasets all focus on the situation where the answer is a named entity. Similarly, the representation vector for the question is generated by concatenating the hidden states of its first and last positions:
hq=W2[h←1q;h→1q;h←Mq;h→Mq]+b2,(2)
View SourceRight-click on figure for MathML and additional features.where W2 and b2 are also model parameters.

3.4 Attention-Based Matching
Given the representation vectors for the question and the entity mentions in the passages, an additive attention model [40] is adopted by treating all entity mention representations and the question representation as the memory and the query, respectively. In particular, the probability for a candidate cφ being the answer given input X is calculated by summing up all the occurrences of cφ across the input passages:
p(cφ|X)=∑k∈Ncφαk∑k′∈Ncαk′,(3)
View SourceRight-click on figure for MathML and additional features.where Ncφ and Nc represent all occurrences of the candidate cφ and all occurrences of all candidates, respectively. Previous work [35] shows that summing the probabilities over all occurrences of the same entity mention is important for the multi-passage scenario. αk is the attention score for the entity mention ϵk, calculated by an additive attention model shown below:
ek0=vTαtanh(Wαhkϵ+Uαhq+bα)(4)
View SourceRight-click on figure for MathML and additional features.
αk=exp(ek0)∑k′∈Nexp(ek′0),(5)
View SourceRight-click on figure for MathML and additional features.where vα, Wα, Uα and bα are model parameters, and N represents all occurrences of all entities.

3.5 Comparison With Dhingra et al. [10]
The Coref-GRU model [10] is based on the gated-attention reader (GA reader) [6], which is designed for the cloze-style reading comprehension task [1], where one token is selected from the input passages as the answer for each instance. To adapt their model for the WikiHop benchmark, where an answer candidate can contain multiple tokens, they first generate a probability distribution over the passage tokens with GA reader, and then compute the probability for each candidate c by aggregating the probabilities of all passage tokens that appear in c and renormalizing over the candidates.

In addition to using LSTM instead of GRU, the main difference between our two baselines and Dhingra et al. [10] is that our baselines consider each candidate as a whole unit no matter whether it contains multiple tokens or not. This makes our models more effective on the datasets containing phrasal answer candidates.

SECTION 4Evidence Integration With Graph Neural Networks
Over the representation vectors for a question and the corresponding entity mentions, we build an evidence integration graph of the entity mentions by connecting relevant mentions with edges, and then integrating relevant information for each graph node (entity mention) with a graph recurrent network (GRN) [12], [17] or a graph convolutional network (GCN) [41].2 Fig. 4 shows the overall procedure of our approach.

Fig. 4. - 
Model framework.
Fig. 4.
Model framework.

Show All

4.1 Graph Construction
As a first step, we create a graph from a list of input passages. The entity mentions within the passages are taken as the graph nodes. They are automatically generated by NER and coreference annotators, so that each graph node is either an entity mention or a pronoun representing an entity. We then create a graph by ensuring that edges between two nodes follow the situations below:

They are occurrences of the same entity mention across passages or with a distance larger than a threshold τL when being in the same passage.

One is an entity mention and the other is its coreference pronoun. This information is automatically generated by a coreference resolution toolkit.

Between two mentions of different entities in the same passage within a window threshold of τS.

Between every two entities that satisfy the situations above, we make two edges in opposite directions. As a result, each generated graph can also be considered as an undirected graph.

4.2 Evidence Integration With Graph Encoding
Tackling multi-hop reading comprehension requires inferring on global context. As the next step, we merge related information through the three types of edges just created. We investigate two recent graph networks: GRN and GCN.

Graph Recurrent Network (GRN). GRN models a graph as a single state, performing recurrent information exchange between graph nodes through graph state transitions. Formally, given a graph G=(V,E), a hidden state vector sk is created to represent each entity mention ϵk∈V. The state of the graph can thus be represented as:
g={sk}|ϵk∈V,
View SourceIn order to integrate non-local evidence among nodes, information exchange between neighborhooding nodes is performed through recurrent state transitions, leading to a sequence of graph states g0,g1,…,gT, where gT={skT}|ϵk∈V and T is a hyperparameter representing the number of graph state transition decided by a development experiment. For initial state g0={sk0}|ϵk∈V, we initialize each sk0 by:
sk0=W3[hkϵ;hq]+b3,(6)
View SourceRight-click on figure for MathML and additional features.where hkϵ is the corresponding representation vector of entity mention ϵk, calculated by Equation 1. hq is the question representation. W3 and b3 are model parameters.

A gated recurrent neural network is used to model the state transition process. In particular, the transition from gt−1 to gt consists of a hidden state transition for each node. At each step t, direct information exchange is conducted between a node and all its neighbors via the following LSTM [43] operations:
iktoktfktuktcktskt=σ(Wimkt+bi)=σ(Womkt+bo)=σ(Wfmkt+bf)=σ(Wumkt+bu)=fkt⊙ckt−1+ikt⊙ukt=okt⊙tanh(ckt),(7)
View SourceRight-click on figure for MathML and additional features.where ckt is the cell vector to record memory for skt, and ikt, okt and fkt are the input, output and forget gates, respectively. Wx and bx (x∈{i,o,f,u}) are model parameters. mkt is the sum of the neighborhood hidden states for the node ϵk:3
mkt=∑i∈N(k)sit−1,(8)
View SourceRight-click on figure for MathML and additional features.N(k) represents the set of all neighbors of ϵk.

Graph Convolutional Network (GCN). GCN is a convolution-based alternative to GRN for encoding graphs. Similar with GRN, encoding with a GCN model consists of two main steps: state initialization and state update. For state initialization, GCN adopts the same approach as with GRN by initializing from the representations vectors of entity mentions, as shown in Equation (6). The main difference between GCN and GRN is the way for updating node states. GRN adopts gated operations (shown in Equation (7)), while GCN uses linear transportation with ReLU as the activation function:
skt=ReLU(Wgmkt+bg),(9)
View Sourcewhere mkt is also the sum of the neighborhood hidden states defined in Equation (8). Wg and bg are model parameters.

4.3 Matching and Combination
After evidence integration, we match the hidden states at each graph encoding step with the question representation using the same additive attention mechanism introduced in the Baseline section. In particular, for each entity ϵk, the matching results for the baseline and each graph encoding step t are first generated, before being combined using a weighted sum to obtain the overall matching result:
ekt=vTattanh(sktWat+hqUat+bat)(10)
View SourceRight-click on figure for MathML and additional features.
ek=wc⊙[ek0,ek1,…,ekT]+bc,(11)
View SourceRight-click on figure for MathML and additional features.where ek0 is the baseline matching result for ϵk, ekt is the matching results after t steps, and T is the total number of graph encoding steps. Wat, Uat, vat, bat, wc and bc are model parameters. In addition, a probability distribution is calculated from the overall matching results using softmax, similar to Equation (5). Finally, probabilities that belong to the same entity mention are merged to obtain the final distribution, as shown in Equation (3).

SECTION 5Training
We train both the baseline and our models using the cross-entropy loss:
l=−logp(c∗φ|X;θ),(12)
View SourceRight-click on figure for MathML and additional features.where c∗φ is ground-truth answer, X and θ are the input and model parameters, respectively. Adam [44] with a learning rate of 0.001 is used as the optimizer. Dropout with rate 0.1 and a l2 normalization weight of 10−8 are used during training.

SECTION 6Experiments on WikiHop
In this section, we study the effectiveness of rich types of edges and the graph encoders using the WikiHop [9] dataset.

6.1 Data
The dataset contains around 51K instances, including 44 K for training, 5K for development and 2.5K for held-out testing. Each instance consists of a question, a list of associated passages, a list of candidate answers and a correct answer. One example is shown in Fig. 1. We use Stanford CoreNLP [45] to obtain coreference and NER annotations. Then the entity mentions, pronoun coreferences and the provided candidates are taken as graph nodes to create an evidence graph. The distance thresholds (τL and τS, in Section 4.1) for making same and window typed edges are set to 200 and 20, respectively.

6.2 Settings
We study the model behavior on the WikiHop devset, choosing the best hyperparameters for online system evaluation on the final holdout testset. Our word embeddings are initialized from the 300-dimensional pretrained Glove word embeddings [46] on Common Crawl, and are not updated during training.

For model hyperparameters, we set the graph state transition number as 3 according to development experiments. Each node takes information from at most 200 neighbors, where same and coref typed neighbors are kept first. The hidden vector sizes for both bidirectional LSTM and GRN layers are set to 300.

6.3 Development Experiments
Fig. 5 shows the devset performances of our model using GRN or GCN with different transition steps. It shows the baseline performances when transition step is 0. The performances go up for both models when increasing the transition step to 3. Further increasing the transition step leads to a slight decrease in performance. One reason can be that executing more transition steps may also introduce more noise through richly connected edges. We set the transition step to 3 for all remaining experiments. GRN shows slightly better performances than GCN with the increase of transition steps. However, the gap is not very significant, with the main reason being the small number of steps (up to 4).


Fig. 5.
Dev performances of different transition steps.

Show All

6.4 Main Results
Table 1 shows the main comparison results with existing work. GA w/ GRU and GA w/ Coref-GRU correspond to Dhingra et al. [10], and their reported numbers are copied. The former is their baseline, a gated-attention reader [6], and the latter is their proposed method.

TABLE 1 Main Results on WikiHop, Where Systems With †† Indicates Using a Deep Language Model or a Gigantic Model Architecture
Table 1- 
Main Results on WikiHop, Where Systems With $\dagger$† Indicates Using a Deep Language Model or a Gigantic Model Architecture
For our baselines, Local and Local-2L encode passages with a BiLSTM and a 2-layer BiLSTM, respectively, both only capture local information for each mention. We introduce Local-2L for better comparison, as our models have more parameters than Local. Coref LSTM is another baseline, encoding passages with coreference annotations by a DAG LSTM (Section 3.2). This is a reimplementation of Dhingra et al. [10] based on our framework. Coref GRN is another baseline that encodes coreferences with GRN. It is for contrasting coreference DAGs with our evidence integration graphs. MHQA-GCN and MHQA-GRN correspond to our evidence integration approaches via graph encoding, adopting GCN and GRN for graph encoding, respectively.

First, even our Local show a much higher accuracy compared with GA w/ GRU and GA w/ Coref-GRU. This is because our models are more compatible with the evaluated dataset. In particular, GA w/ GRU and GA w/ Coref-GRU calculate the probability for each candidate by summing up the probabilities of all tokens within the candidate. As a result, they cannot handle phrasal candidates very well, especially for the overlapping candidates, such as “New York” and “New York City”. On the other hand, we consider each candidate answer as a single unit, and does not suffer from this issue. As a reimplementation of their idea, Coref LSTM only shows 0.4 points gains over Local, a stronger baseline than GA w/ GRU. On the other hand, MHQA-GCN and MHQA-GRN are 1.6 and 1.8 points more accurate than Local, respectively. Our Local baseline achieves a decent number without the help of explicit multi-hop operations. The reason, as mentioned recently [49], is that many easy instances can be inferred from the entity types of candidates. But, our model still largely improves the performance by tackling more difficult instances.

The comparisons below help to further pinpoint the advantage of our approach: MHQA-GRN is 1.4 points better than Coref GRN, while Coref GRN gives a comparable performance with Coref LSTM. Both comparisons show that our evidence graphs are the main reason for achieving the 1.8-points improvement, and it is mainly because our evidence graphs are better connected than coreference DAGs and are more suitable for integrating relevant evidence. Local-2L is not significantly better than Local, meaning that simply introducing more parameters does not help.

In addition to the systems above, we introduce Fully-Connect-GRN for demonstrating the effectiveness of our evidence graph creating approach. Fully-Connect-GRN creates fully connected graphs out of the entity mentions, before encoding them with GRN. Within each fully connected graph, the question is directly connected with the answer. However, fully connected graphs are brute-force connections, and are not representative for integrating related evidence. MHQA-GRN is 1.5 points better than Fully-Connect-GRN, while questions and answers are more directly connected (with distance 1 for all cases) by Fully-Connect-GRN. The main reason can be that our evidence graphs only connect related entity mentions, making our models easier to learn how to integrate evidence. On the other hand, there are barely learnable patterns within fully connected graphs. More analyses on the relation between graph connectivity and end-to-end performance will be shown in later paragraphs.

We observe some recent papers showing better results on the leaderboard.4 The last group of Table 1 shows the top two published non-ensemble models, where Reasoning Chain uses a BERT [50] model, and DynSAN adopts a very deep model architecture that needs 4 12GB GPUs for training. Our main contribution is studying an evidence integration approach, which is orthogonal to the contribution of BERT and other deep model-framework design. We will investigate these in a future version.

6.5 Analysis
Effectiveness of Edge Types. Table 2 shows the ablation study of different types of edges that we introduce for evidence integration. The first group shows the situations where one type of edges are removed. In general, there is a large performance drop by removing any type of edges. The reason can be that the connectivity of the resulting graphs is reduced, thus fewer facts can be inferred. Among all these types, removing window-typed edges causes the least performance drop. One possible reason is that some information captured by them has been well captured by sequential encoding. However, window-typed edges are still useful, as they can help passing evidence through to further nodes. Take Fig. 2 as an example, two window-typed edges help to pass information from “The Hanging Gardens” to “India”. The other two types of edges are slightly more important than window-typed edges. Intuitively, they help to gather more global information than window-typed edges, thus learn better representations for entities by integrating contexts from their occurrences and co-references.

TABLE 2 Ablation Study on Different Types of Edges Using GRN as the Graph Encoder
Table 2- 
Ablation Study on Different Types of Edges Using GRN as the Graph Encoder
The second group of Table 2 shows the model performances when only one type of edges are used. None of the performances with single-typed edges are significantly better than the Local baseline, whereas the combination of all types of edges achieves a much better accuracy (1.8 points) than Local. This indicates the importance of evidence integration over better connected graphs. We show more detailed quantitative analyses later on. The numbers generally demonstrate the same patterns as the first group. In addition, only same is slightly better than only coref. It is likely because some coreference information can also be captured by sequential encoding.

Distance. Fig. 6 shows the percentage distribution of distances between a question and its closest answer when either all types of edges are adopted or only coreference edges are used. The subject of each question5 is used to locate the question on the corresponding graph.

Fig. 6. - 
Distribution of distances between a question and an answer on the Devset.
Fig. 6.
Distribution of distances between a question and an answer on the Devset.

Show All

When all types of edges are adopted, the questions and the answers for more than 90 percent of the development instances are connected, and the question-and-answer distances for more than 70 percent are within 3. On the other hand, the instances with distances longer than 4 only count for 10 percent. This can be the reason why performances do not increase when more than 3 transition steps are performed in our model. The advantage of our approach can be shown by contrasting the distance distributions over graphs generated either by the baseline or by our approach.

We further evaluate both approaches on a subset of the development instances, where the answer-and-question distance is at most 3 in our graph. The accuracies of Coref LSTM and MHQA-GRN on this subset are 61.1 and 63.8, respectively. Comparing with the performances on the whole devset (61.4 vs 62.8), the performance gap on this subset is increased by 1.3 points. This indicates that our approach can better handle these “relatively easy” reasoning tasks. However, as shown in Fig. 5, instances that require large reasoning steps are still challenging to our approach.

SECTION 7Experiments on ComplexWebQuestions
In this section, we conduct experiments on the newly released ComplexWebQuestions version 1.1 [21] for better evaluating our approach. Compared with WikiHop, where the complexity is implicitly specified in the passages, the complexity of this dataset is explicitly specified on the question side. One example question is “What city is the birthplace of the author of ‘Without end”’. A two-step reasoning is involved, with the first step being “the author of ‘Without end”’ and the second being “the birthplace of x”. x is the answer of the first step.

In this dataset, web snippets (instead of passages as in WikiHop) are used for extracting answers. The baseline of Talmor and Berant [21] (SimpQA) only uses a full question to query the web for obtaining relevant snippets, while their model (SplitQA) obtains snippets for both the full question and its sub-questions. With all the snippets, SplitQA models the QA process based on a computation tree6 of the full question. In particular, they first obtain the answers for the sub-questions, and then integrate those answers based on the computation tree. In contrast, our approach creates a graph from all the snippets, thus the succeeding evidence integration process can join all associated evidence.

Main Results. As shown in Table 3, similar to the observations in WikiHop, both MHQA-GRN and MHQA-GCN achieve large improvements over Local, and MHQA-GRN gives slightly better accuracy. Both the baselines and our models use all web snippets, but MHQA-GRN and MHQA-GCN further consider the structural relations among entity mentions. SplitQA achieves 0.5 percent improvement over SimpQA.7 Our Local baseline is comparable with SplitQA and our graph-based models contribute a further 2 percent improvement over Local. This indicates that considering structural information on passages is important for the dataset.

TABLE 3 Results on the ComplexWebQuestions Dataset
Table 3- 
Results on the ComplexWebQuestions Dataset
Analysis. To deal with complex questions that require evidence from multiple passages to answer, previous work [37], [38], [39] collect evidence from occurrences of an entity in different passages. The above methods correspond to a special case of our method, i.e. MHQA with only the same-typed edges. From Table 3, our method gives 1 point increase over MHQA-GRN w/ only same, and it gives more increase in WikiHop (comparing all types with only same in Table 2). Both results indicate that our method could capture more useful information for multi-hop QA tasks, compared to the methods developed for previous multi-passage QA tasks. This is likely because our method integrates not only evidences for an entity but also these for other related entities.

The leaderboard reports SplitQA with additional sub-question annotations and gold answers for sub-questions. These pairs of sub-questions and answers are used as additional data for training SplitQA. The above approach relies on annotations of ground-truth answers for sub-questions and semantic parses, thus is not practically useful in general. However, the results have additional value since it can be viewed as an upper bound of SplitQA. Note that the gap between this upper bound and our MHQA-GRN is small, which further proves that larger improvement can be achieved by introducing structural connections on the passage side to facilitate evidence integration.

SECTION 8Conclusion
We have introduced a new approach for tackling multi-hop reading comprehension (MHRC), with a graph-based evidence integration process. Given a question and a list of passages, we first connect related evidence in reference passages into a graph, and then adopt recent graph neural networks to encode resulted graphs for performing evidence integration. Results show that the three types of edges are useful on combining global evidence and that the graph neural networks are effective on encoding complex graphs resulted by the first step. Our approach shows highly competitive performances on two standard MHRC datasets.

