A common assumption in semi-supervised learning is that the class label function has a slow variation on the data graph, while in many problems, the label function may vary abruptly in certain graph regions, resulting in high-frequency components. Although semi-supervised learning is an ill-posed problem, it is often possible to find a source graph on which the label function has similar frequency content to the target graph where the actual classification problem is defined. In this paper, we propose a method for domain adaptation on graphs based on learning the spectrum of the label function in a source graph with many labels, and transferring the spectrum information to the target graph. When transferring the frequency content, it is not easy to share graph Fourier coefficients directly between the two independently constructed graphs, since no match exists between their Fourier bases. We solve this by learning a transformation between the Fourier bases of the two graphs that flexibly “aligns” them. The unknown class label function on the target graph is then reconstructed from the learnt spectrum while retaining consistency with the available labels. Comparative experiments suggest that the proposed algorithm often outperforms recent domain adaptation methods in various data classification applications.
SECTION 1Introduction
Most classification algorithms rely on the assumption that the labeled and unlabeled data samples at hand are drawn from the same distribution. However, in many practical data classification problems, the labeled training samples and the unlabeled test samples may have different statistics [1]. Domain adaptation methods make use of the class labels sufficiently available in a source domain in order to infer the label information in a target domain where labeled data are much more scarce. In order to be able to “transfer” the information from one domain to another, some inherent relation must exist between the two domains. In this work, we focus on a setting where the source and the target data are represented with a graph in each domain. We consider that the source and the target graphs are related in such a way that the spectra of the source and the target class label functions on the two graphs share similar characteristics. We then propose a method that makes use of this relation in order to estimate the missing labels in the target domain based on the sufficiently available label information in the source domain.

The domain adaptation problem has attracted much attention in the recent years. Each domain adaptation solution is based on a certain assumption about how the source and the target domains are related. Some methods assume that the data samples from different domains can be aligned via projections and transformations [2], [3], while some try to establish a relation between their distributions [4], [5], or learn joint feature representations [6]. Meanwhile, what is common between all these methods is that they are strictly based on the assumption that the data samples reside in an ambient space such as an euclidean domain, hence they have physical coordinates. Although this may be true in various settings, there are also many data classification problems where the source and the target data are defined or described solely through the pairwise affinities or the relations between data samples. Some examples are social networks [7], where no physical coordinates are associated with a user but relations or links between different users define the network; or sensor networks [8], where the pairwise similarities between different sensors are identifiable via their geographical or other kinds of proximities. Graph models provide very convenient tools for such problems. For instance, in a social network each user can be represented as a graph node and relationships between users can be captured with edges. One can then consider an inference problem on the graph, e.g., whether a user is likely to be interested in a product or not. Similarly, in a sensor network one may infer the missing data at a broken sensor based on the data obtained from the other sensors.

In this work, we propose a new domain adaptation method that uses a source graph and a target graph representing the source and the target data. We consider the problem of estimating a label function on the target graph where very few labels are available. Depending on the application, the label function we consider can be any function defined on a graph domain, whose missing values are to be inferred from the available values. In particular, in a data classification problem, which is the main application area of our work, the values of the label function are class labels. Our assumption about the relation between the source and the target domains is that the spectrum, i.e., the frequency content of the label function has similar characteristics over the source and the target graphs. Given the observations of the label function on the source graph, we estimate the label function on the target graph under the prior that its frequency spectrum resembles that on the source graph.

Harmonic analysis on graph domains, which permits the definition of the Fourier transform on graphs, has been an active and popular research topic of the recent years [9], [10]. However, the notion of smoothness or smoothly-varying functions on graphs has actually been essential to many dimensionality reduction and semi-supervised learning methods since a long time [11], [12], [13]. Graph-based semi-supervised learning algorithms in a single domain typically rely on the assumption that the label function has a smooth variation on the graph [14], [15]. Meanwhile, the validity of the smoothness assumption is questionable in the general sense. For instance, in Fig. 1, a generic face manifold is illustrated, where the face images of different individuals may get arbitrarily close to each other due to extreme lighting conditions. Consequently, the label function has fast variation along certain directions on the data graph and its spectrum contains some non-negligible high-frequency content. Although the assumption that the label function should vary slowly on the graph is reasonable especially in a single domain where no information about its spectral content is available, the spectrum can actually be learnt in a setting with more than one domain. Our method is then based on the idea of learning the spectral content of the label function from the source graph, and transferring it to the target graph as illustrated in Fig. 2.

Fig. 1. - 
Illustration of a generic face manifold. Face images [16] of three different individuals are indicated with different colors. While the class label function varies slowly along the blue direction, it has a relatively fast variation along the red direction.
Fig. 1.
Illustration of a generic face manifold. Face images [16] of three different individuals are indicated with different colors. While the class label function varies slowly along the blue direction, it has a relatively fast variation along the red direction.

Show All

Fig. 2. - 
Illustration of the graph domain adaptation problem studied in this work. Given that the source label function has slow and fast variations along the indicated directions, we would like to transfer this label spectrum information to the target graph in order to estimate the target label function more accurately.
Fig. 2.
Illustration of the graph domain adaptation problem studied in this work. Given that the source label function has slow and fast variations along the indicated directions, we would like to transfer this label spectrum information to the target graph in order to estimate the target label function more accurately.

Show All

Given a source and a target graph that are independently constructed, we propose to learn a pair of “aligned” bases on the two graphs through which information can be transferred or shared. In particular, the “aligned” source and target bases are such that the coefficients of the source and target label functions are similar when represented in the corresponding bases. We formulate the basis learning problem as the learning of a linear transformation between the source and the target graph Fourier bases so that each source Fourier basis vector is mapped to a new basis vector in the target graph obtained as a linear combination of the target Fourier basis vectors. The learning of this transformation then becomes a key problem of the proposed scheme. In particular, the linear transformation to be learnt must be sufficiently flexible to indeed “align” the two graphs even if they are independently constructed, while retaining the capability of transferring the spectral content of the label function between the two graph bases. In order to achieve this, we impose suitable priors on the linear transformation, and then learn the transformation matrix jointly with the source and the target label functions under the constraint that the source and the target label functions must have similar coefficients over the learnt bases. The resulting objective function is not jointly convex in the coefficients and the transformation matrix; nevertheless, it is separately convex in one when the other is fixed. We thus minimize the objective function with an alternating optimization procedure. The output of the algorithm is the estimated label function on the target graph, which provides the class labels of the initially unlabeled data samples. To the best of our knowledge, our treatment is the first to study the domain adaptation problem based on explicitly analyzing the spectrum of label functions in a pure graph setting. Our proposed method is applicable not only to data analysis problems defined purely on graphs, but also to data embedded in an ambient space via the construction of graphs with respect to, e.g., nearest-neighborhoods. We demonstrate the usage of the algorithm in several data classification and regression applications. Classification results on synthetic data, face and object images, and social network data as well as regression results for the prediction of product ratings of users show that the proposed algorithm often outperforms traditional classifiers and reference domain adaptation methods in comparison.

The paper is organized as follows. In Section 2, we overview the related literature. In Section 3, we present a brief introduction to frequency analysis on graphs and formulate the problem of graph domain adaptation. In Section 4, we describe the proposed algorithm for domain adaptation on graphs via spectral graph alignment. In Section 5, we evaluate the performance of the method with comparative experiments. Finally, we conclude in Section 6.

SECTION 2Related Work
The domain adaptation problem has been treated in several settings and under different assumptions so far [1]. Some works focus on a problem where the source and the target distributions are defined on the same data space [17], [18], [19]. In the case that the conditional distributions of labels remain unchanged and only the marginal distributions of data coordinates vary between the source and target domains, the domain adaptation problem is referred to as covariate shift or sample selection bias, where solutions based on sample reweighting are applicable [17], [20]. Daumé III et al. and Duan et al. have proposed to map the source and the target features to a higher dimensional domain via feature augmentation, where a common classifier can be learnt [21], [22], [23], [24]. In settings with multiple sources domains, a common approach is to learn the target hypothesis based on a weighted combination of the source hypotheses [25], [26].

Another domain adaptation solution consists of learning a transformation or a projection that aligns the source and the target data [2], [3], [27], [28], [29], [30], [31]. In fact, the idea of aligning the source and the target domains by mapping them to an intermediate space through a transformation has been at the core of many domain adaptation algorithms, some of which can also be applied to problems where the source and the target samples reside in different ambient spaces [32]. Several authors have proposed to reduce the distance between the samples from different domains by learning a transformation [33], [34], [35], where the maximum mean discrepancy is a common choice as a distribution distance [36], [37], [38], [39] or scatter measure [40]. The approaches in [4], [5], [41] rely on matching the densities or the second-order statistics of the source and the target domains via copula functions or transformations. A metric adapted to the domain adaptation problem is learnt in [42], [43]. In some works, a classifier is learnt in a joint manner with the mapping [44], [45], or directly in the original data domain based on a self-training principle [46].

Deep networks have also gained popularity in domain adaptation applications in the recent years. These methods are typically based on the extraction of domain-invariant features that are shared between [6], [47] or adapted specifically [48] to the source and the target domains. Domain classifier layers aiming to reduce the distribution discrepancy are often learnt along with the label predictors in an adversarial manner [48], [49].

While all of the above domain adaptation methods rely strictly on the availability of a representation of the data in an ambient space, in thus study, we focus on a setting where the data does not need to have a physical embedding and the problem may be directly defined over an abstract data graph. Frequency analysis on graph domains is now a well-established framework, thanks to the recent advances in the field of graph signal processing. The convergence of the graph Laplacian operator to the continuous Laplace-Beltrami operator on manifolds has been studied in several works [50], [51], which provides a foundation for graph signal processing. Characterizing the Fourier basis vectors as the eigenvectors of the Laplacian operator, the Fourier transform and Fourier bases can be extended to graph domains via the eigenvalue decomposition of the graph Laplacian matrix [9], [10], [52].

The idea of matching graph bases with transformations or pairwise correspondences has been explored before in the previous works [53], [54], [55]; however, in different settings related to unsupervised clustering or 3D shape analysis problems. Note that, several previous methods have already incorporated manifold models or graph models in domain adaptation. The algorithm in [32] employs a manifold model and learns projections by preserving the topology of the data set while achieving discrimination between different classes. The works in [45], [56], [57] similarly impose priors on the smoothness of the label function over the data graph. The K-NN graphs used in [57] are iteratively refined with the aid of a supervised metric learner. A pair of source and target graphs are constructed in [58], which is followed by a graph matching stage to map source classes to target clusters for multispectral image classification.

Finally, a preliminary version of our work has been presented in [59], where the idea of transferring the label spectrum between a source and a target graph has been explored for the first time. However, a major limitation of the algorithm in [59] is that it relies on a one-to-one match between the graph Fourier basis vectors. This restricts its applicability to settings where the source and the target graphs are highly similar so as to admit a direct match between the two graph Fourier bases. This limitation is circumvented in the current study by learning a transformation between the two Fourier bases.

SECTION 3Domain Adaptation on Graphs
In this section, we first give an overview of the extension of classical frequency analysis techniques to graph domains [10]. Then, we propose a problem formulation for domain adaptation on graphs. In the following, matrices are represented with uppercase letters, and vectors are denoted with lowercase or Greek letters. Vectors are considered as column vectors unless stated otherwise. Aij stands for the (i,j)th entry of a matrix A, and |⋅| denotes the cardinality of a set.

3.1 Overview of Frequency Analysis on Graphs
In graph-based methods, a data set with N data samples is typically represented with a graph with N vertices, such that each vertex corresponds to a data sample. Let G=(V,E,W) be a weighted graph with N vertices (nodes), where V={xi}Ni=1 is the set of vertices, E is the set of edges, and W∈RN×N is the weight matrix. If there is an edge between the nodes xi and xj, then Wij consists of the weight of this edge. If the nodes xi and xj are not connected with an edge, then Wij=0.

A graph signal is a function f:V→R taking a real value on each graph vertex, which can equivalently be represented as an N-dimensional vector f∈RN. A set {vk}Nk=1⊂RN of linearly independent graph signals form a graph basis, so that any graph signal f can be represented as
f=∑k=1Nαkvk,(1)
View SourceRight-click on figure for MathML and additional features.in terms of the graph basis vectors vk with coefficients αk. Representing the basis as a matrix V=[v1…vN]∈RN×N and the coefficient vector as α=[α1…αN]T∈RN, the graph signal can be expressed as f=Vα.

The graph Laplacian matrix L∈RN×N is defined as L=D−W, where D∈RN×N is the diagonal degree matrix given by Dii=∑jWij. The graph Laplacian is an essential element in spectral graph theory, since its application to a graph signal f as an operator via the matrix multiplication
(Lf)(xi)=∑j=1NWij(f(xi)−f(xj)),(2)
View SourceRight-click on figure for MathML and additional features.is the graph equivalent of applying the Laplacian operator to a signal in classical signal processing [10], [50], [51]. This analogy allows the extension of classical Fourier analysis to graph domains as follows. First recall that for one-dimensional signals, the complex exponentials ejΩt defining the Fourier transform are given by the eigenfunctions of the Laplacian operator Δ
−Δ(ejΩt)=Ω2ejΩt.(3)
View SourceRight-click on figure for MathML and additional features.The eigenvalue Ω2 of the Laplacian operator increases with the frequency of the complex exponential ejΩt. Characterizing the Fourier transform via the eigenfunctions of the Laplacian operator, the graph counterparts of complex exponentials are then the eigenvectors of the graph Laplacian given by
Luk=λkuk.(4)
View SourceRight-click on figure for MathML and additional features.The set of eigenvectors {uk}Nk=1⊂RN of the graph Laplacian corresponding to the eigenvalues λ1=0≤λ2≤⋯≤λN thus defines a graph Fourier basis. In analogy with (3), the eigenvalues λk bear a notion of frequency in a graph. The eigenvectors uk for increasing values of k indeed have an increasing speed of variation over the graph when regarded as graph signals [10]. In particular, a common measure for the speed of variation of a graph signal f over the graph is
fTLf=12∑i,j=1NWij(f(xi)−f(xj))2,(5)
View Sourcewhich takes larger values if the function f varies more abruptly between neighboring graph nodes. The above term becomes the corresponding eigenvalue λk of the graph Laplacian when the graph signal is taken as a Fourier basis vector f=uk
uTkLuk=λk.(6)
View SourceRight-click on figure for MathML and additional features.Once the Fourier basis {uk}Nk=1 of a graph is computed, the graph Fourier transform f^(λk) of a graph signal f is simply given by its inner product with the basis vectors
f^(λk)=⟨f,uk⟩=∑i=1Nf(xi)uk(xi).(7)
View SourceRight-click on figure for MathML and additional features.This can be equivalently written as f^=UTf∈RN in matrix notation, where f^=[f^(λ1)…f^(λN)]T and U=[u1…uN]∈RN×N. Here f^(λk) is the kth Fourier coefficient of f corresponding to the basis vector uk with frequency λk. The inverse Fourier transform is then obtained as the reconstruction of the signal from its representation over the Fourier basis as
f=∑k=1Nf^(λk)uk=Uf^.(8)
View SourceRight-click on figure for MathML and additional features.

3.2 Problem Formulation for Graph Domain Adaptation
We now propose our problem formulation for domain adaptation in graph settings. We consider a source graph Gs=(Vs,Es,Ws) that consists of Ns vertices Vs={xsi}Nsi=1 and edges Es, and a target graph Gt=(Vt,Et,Wt) with Nt vertices Vt={xti}Nti=1 and edges Et. The weighted edges of the source and the target graphs are respectively represented in the weight matrices Ws, Wt. Let Us∈RNs×Ns and Ut∈RNt×Nt denote the matrices containing the Fourier basis vectors, respectively on the source and the target graphs. These are computed using the eigenvalue decompositions of the respective graph Laplacians Ls∈RNs×Ns and Lt∈RNt×Nt as explained in (4).

Consider a label function fs∈RNs on the source graph and a label function ft∈RNt on the target graph. We assume that the labels of some of the nodes are available. We denote the known labels as ysi=fs(xsi) on the source graph (for labeled xsi), and as yti=ft(xti) on the target graph (for labeled xti). The sets containing the indices of the labeled data samples are denoted as Is⊂{1,…,Ns} and It⊂{1,…,Nt} in the source and the target domains. The label functions fs and ft take discrete values in a classification problem and continuous values in a regression problem. For instance, in a classification problem with two classes, one can set yti as equal to 1 if the labeled data sample xti belongs to the first class and as −1 if it is from the second class. The problem is then to compute the labels of all unlabeled data samples, which is done by estimating the label vector ft. Domain adaptation methods often assume a setting with many labeled samples in the source domain and much fewer labeled samples in the target domain, i.e., |It|≪|Is|.

Let Vs∈RNs×Ns and Vt∈RNt×Nt denote a pair of bases for the functions respectively on the source and the target graphs. We can then decompose the label functions fs and ft to be predicted in the source and target graphs over the bases Vs and Vt as
fs=∑k=1Nsαskvsk=Vsαs,ft=∑k=1Ntαtkvtk=Vtαt.(9)
View SourceRight-click on figure for MathML and additional features.Here Vs and Vt contain respectively the basis vectors {vsk} and {vtk} in their columns; and αs∈RNs and αt∈RNt are coefficient vectors.

Domain adaptation methods assume the presence of a relationship between the source and the target domains and aim to transfer the knowledge in the source domain to the target domain in order to better predict the target label function. In the following, we consider a domain adaptation setting where a relationship can be established between the source and the target domains via a “coherent” pair of bases Vs, Vt for the space of functions on the source and the target graphs. In particular, if Vs and Vt are a “coherent” pair of bases, then one can transfer the label information from the source graph to the target graph based on the representations of the label functions on these bases. We can then formulate the following problem:

Problem 1.
minαs,αt∥SsVsαs−ys∥2+∥StVtαt−yt∥2+μ∥α¯¯¯s−α¯¯¯t∥2.(10)
View SourceRight-click on figure for MathML and additional features.

Here ys∈RKs and yt∈RKt are vectors consisting respectively of the available labels {ysi} and {yti} in the source and the target domains where Ks=|Is| and Kt=|It| are the number of known labels. The matrices Ss∈RKs×Ns and St∈RKt×Nt are binary selection mask matrices consisting of 0's and 1's, which enforce the label prediction functions fs, ft to match the available labels ys, yt over the subsets Is, It of labeled data; and μ>0 is a weight parameter. The coefficients αs and αt of the source and target label functions must be found such that the resulting estimation of the label predictions correspond to the given labels, while αs and αt (or their appropriately restricted versions α¯¯¯s, α¯¯¯t in the case that the graph sizes are different Ns≠Nt) are close over the source and the target graphs.

Then, an important question is what properties a “coherent” pair of bases Vs and Vt should have, and how such bases can be found in practice. If a one-to-one match exists between the source and the target graphs, e.g., as in a problem where each source node has a known corresponding target node, then one can simply select the bases as the Fourier bases Vs=Us, Vt=Ut, so that the spectra of the source and the target label functions can be directly matched by solving the problem in (10). However, in a realistic setting such a one-to-one match often does not exist. For instance, the experiments reported in Section 5.1, Fig. 8 study the frequency content of the label function on the source and the target graphs. The results in Fig. 8 indicate that the general shape (envelope) of the spectrum resembles between the two graphs; however, corresponding Fourier coefficients across the two graphs are not always the same. This suggests that although it would be too restrictive to transmit the exact Fourier coefficients, it is possible to exploit the similarity between the shapes of the source and the target spectra. Based on these observations, we propose to learn Vs, Vt relying on the available observations of the label function, in a manner that allows the transfer of the spectral content between the graphs. In particular, we propose to choose
Vs=Us,Vt=UtT,(11)
View SourceRight-click on figure for MathML and additional features.where Us and Ut are the Fourier bases, and the matrix T∈RNt×Nt represents a transformation between Ut and Vt. From Problem 1, one can observe that T matches the source basis vector vsi=usi to the target basis vector
vti=∑j=1NtTjiutj,(12)
View SourceRight-click on figure for MathML and additional features.obtained as a linear combination of the Fourier vectors utj.


Fig. 3.
Illustration of the transformation between similar frequencies in the proposed method. The figure illustrates a case where the second Fourier signal us2 in the source graph and the second and third Fourier signals ut2 and ut3 in the target graphs oscillate mainly along the indicated directions. Due to the differences between the graph topologies, us2 can be successfully matched to neither ut2 nor ut3. Nevertheless, us2 might possibly be matched to some signal vt2 in the target graph that can be written as a linear combination of ut2 and ut3.

Show All

Fig. 4. - 
Synthetic data sets with two classes.
Fig. 4.
Synthetic data sets with two classes.

Show All

Fig. 5. - 
Sample images from the MIT-CBCL face data set for three different subjects [62]. Leftmost two, middle two, and rightmost two images are rendered respectively under poses 1, 5, and 9 for various illumination conditions.
Fig. 5.
Sample images from the MIT-CBCL face data set for three different subjects [62]. Leftmost two, middle two, and rightmost two images are rendered respectively under poses 1, 5, and 9 for various illumination conditions.

Show All


Fig. 6.
Sample images from the COIL-20 data set. The upper and lower rows show the objects respectively in the source domain and the target domain. Each source domain object is matched to the target domain object right below it. Matched object pairs are considered to have the same class label in the experiments.

Show All


Fig. 7.
Source and target community graphs for the Facebook data.

Show All

Fig. 8. - 
Source and target label spectra on the MIT-CBCL data set.
Fig. 8.
Source and target label spectra on the MIT-CBCL data set.

Show All

When learning the transformation T, our purpose is to learn a representation that is flexible enough to properly “align” the two individually constructed graphs, while also preserving the spectral relation between the two graphs. The rate of variation of the ith source Fourier vector vsi=usi is proportional to the ith eigenvalue λsi of the source graph Laplacian Ls. In order to preserve the spectral relation between the graphs, the corresponding target vector vti in (12) must have a similar rate of variation on the target graph, so that slowly (or rapidly) varying source label functions are matched to slowly (or rapidly) varying target label functions. For this reason, we propose to learn T such that the weight Tji of the jth target Fourier vector utj in the representation of vti is encouraged to be higher for j values close to i, and to decay as j deviates from i. In this way, the source Fourier vector usi=vsi is mapped to a target vector that is mainly composed of the target Fourier vectors utj having frequencies close to that of usi, as illustrated in Fig. 3. This can be achieved by penalizing high magnitudes for the entries of T distant from the diagonal, by including a term ∥M⊙T∥2 in the overall objective, where M∈RNt×Nt is a symmetric weight matrix of the form
Mij=exp((i−j)2σ2),(13)
View Sourcethe scale parameter σ adjusts the width of the window of matched frequencies, and ⊙ denotes the Hadamard (element-wise) product between two matrices. The overall objective function to minimize then becomes the following:

Problem 2.
minαs,αt,T∥SsUsαs−ys∥2+∥StUtTαt−yt∥2+μ1∥α¯¯¯s−α¯¯¯t∥2+μ2∥M⊙T∥2Fsubject to∑i=1NtT2ij=1, for  j=1,…,Nt.(14)
View SourceRight-click on figure for MathML and additional features.

Here μ1>0, μ2>0 are weight parameters, and ∥⋅∥F denotes the Frobenius norm of a matrix. The equality constraints ensure that the columns of the transformation matrix T have unit norm, in order not to approach the trivial solution T=0.

While Problem 2 aims to learn a pair of complete bases on the two graphs, it is often not necessary to use all basis vectors for obtaining a good reconstruction of the label function: Fourier basis vectors usi, uti with very high frequencies (eigenvalues) λsi, λti, have a quite rapid variation over the graph, and discarding some of these not only reduces the complexity of the problem, but also serves the important purpose of regularization. For these reasons, we select a subset of the basis vectors {usi}Ri=1, {uti}Ri=1, corresponding to the R smallest frequencies in both domains, where R<Ns and R<Nt. Let U¯¯¯¯s∈RNs×R, U¯¯¯¯t∈RNt×R denote the reduced source and target Fourier bases consisting of the first R basis vectors. When label functions are reconstructed with the reduced bases, Problem 2 can be reformulated as

Problem 3.
minα¯¯¯¯s,α¯¯¯¯t,T¯¯¯¯∥SsU¯¯¯¯sα¯¯¯s−ys∥2+∥StU¯¯¯¯tT¯¯¯¯α¯¯¯t−yt∥2+μ1∥α¯¯¯s−α¯¯¯t∥2+μ2∥M¯¯¯¯¯⊙T¯¯¯¯∥2Fsubject to∑i=1RT¯¯¯¯2ij=1, for  j=1, …,R.(15)
View SourceRight-click on figure for MathML and additional features.

Here, the matrix T¯¯¯¯∈RR×R is the submatrix of T consisting of its first R rows and columns, which match the source vectors {usi}Ri=1 to linear combinations of {uti}Ri=1. The reduced weight matrix M¯¯¯¯¯∈RR×R has entries as defined in (13). The vectors α¯¯¯s, α¯¯¯t consist of the projections of the label functions onto the Fourier vectors in the reduced bases U¯¯¯¯s, U¯¯¯¯t such that the source and the target label functions fs and ft are reconstructed as
fs=U¯¯¯¯sα¯¯¯s,ft=U¯¯¯¯tT¯¯¯¯α¯¯¯t,(16)
View SourceRight-click on figure for MathML and additional features.once Problem 3 is solved. Note that, although the main focus in domain adaptation is to estimate the target labels, the above formulation also allows the estimation of the missing source labels in case of interest.

Estimating the label functions by solving Problem 3, one may then wonder how well the variations of the source and target label functions on the two graphs agree. In the following, we provide an upper bound on the difference between the rates of change of the source and the target label functions fs and ft. Let 0=λs1≤λs2≤⋯≤λsR and 0=λt1≤λt2≤⋯≤λtR respectively denote the smallest R eigenvalues of the source and the target graph Laplacians Ls and Lt. Let the similarity of the source and the target graph topologies be so that the deviation between the corresponding eigenvalues of the two graph Laplacians are bounded as |λsi−λti|≤δ, for all i=1,…,R. Let us define λR=max(λsR,λtR), which indicates a spectral upper bound (bandwidth) for the frequencies of the first R source and target Fourier basis vectors. Assume that the difference between the source and the target coefficients is bounded as ∥α¯¯¯s−α¯¯¯t∥≤Δα, and the deviation between T¯¯¯¯ and the R×R identity matrix I is bounded as ∥T¯¯¯¯−I∥≤ΔT, with ∥⋅∥ denoting the operator norm for matrices. Finally let C be a bound for the norms of the computed coefficients with ∥α¯¯¯s∥,∥α¯¯¯t∥≤C. We then have the following result.

Proposition 1.
Assume that the constants λR>0, δ≥0, ΔT≥0, Δα≥0, and C>0 are such that the above conditions hold for the solution α¯¯¯s, α¯¯¯t, T¯¯¯¯ of Problem 3. Then, the difference between the rates of variation of the estimated source and target label functions fs, ft on the source and target graphs is bounded as
|(fs)TLsfs−(ft)TLtft|≤C2δ+2CλRΔα+C2λR(2ΔT+Δ2T).
View SourceRight-click on figure for MathML and additional features.

The proof of Proposition 1 is given in the accompanying technical report [60, Appendix A]. In the light of this theoretical bound, the formulation proposed in Problem 3 can be interpreted as follows. In the considered setting, due to the assumption of the similarity of their spectra, the source and target label functions must have similar rates of variation over the two graphs. The bound in Proposition 1 shows that the source and target label functions have similar rates of variation if the constants δ, λR, Δα, ΔT are sufficiently small. The constant δ depends on the topological similarity between the two graphs and cannot be controlled by the learning algorithm. Meanwhile, the constant λR in the above bound suggests that preventing λR from taking very large values should have a positive effect on the learning. This is in line with the choice of representing the label functions with a relatively small number R of basis vectors in Problem 3, in contrast to Problem 2. Then, another objective of Problem 3 is to minimize the difference between the coefficient vectors α¯¯¯s and α¯¯¯t, which reduces Δα. Finally, the term ∥M¯¯¯¯¯⊙T¯¯¯¯∥2F in the learning objective aiming to discourage large off-diagonal entries will eventually help reduce the constant ΔT in the above bound. Note, however, that we deliberately avoid imposing T¯¯¯¯≈I in Problem 3, which would restrict the flexibility of the learnt bases in aligning the two graphs to account for the differences in the graph topologies. This is discussed in more detail in Section 4.3.

SECTION 4Proposed Method: Domain Adaptation via Spectral Graph Alignment
In this section, we present the proposed domain adaptation method, which we call Domain Adaptation via Spectral Graph Alignment (DASGA). Our algorithm aims to learn a pair of “aligned” bases on the source and target graphs based on Problem 3.

The problem in (15) is not jointly convex in all optimization variables α¯¯¯s, α¯¯¯t, T¯¯¯¯. Nevertheless, it is convex separately in the overall coefficient vector α¯¯¯=[(α¯¯¯s)T(α¯¯¯t)T]T, and the transformation matrix T¯¯¯¯. Hence, we propose to minimize the objective (15) with an iterative and alternating optimization approach, by first fixing T¯¯¯¯ and optimizing α¯¯¯s, α¯¯¯t; and then fixing the coefficient vectors α¯¯¯s, α¯¯¯t and optimizing T¯¯¯¯ in each iteration. We describe these two optimization steps in the sequel.

4.1 Optimization of the Coefficient Vectors
In the first step of an iteration, the transformation matrix T¯¯¯¯ is fixed, and the coefficient vectors α¯¯¯s and α¯¯¯t are optimized. Fixing T¯¯¯¯, the optimization problem in (15) becomes the following unconstrained problem in α¯¯¯s and α¯¯¯t
minα¯¯¯¯s,α¯¯¯¯tG(α¯¯¯s,α¯¯¯t)=minα¯¯¯¯s,α¯¯¯¯t  ∥SsU¯¯¯¯sα¯¯¯s−ys∥2+∥StU¯¯¯¯tT¯¯¯¯α¯¯¯t−yt∥2+μ1∥α¯¯¯s−α¯¯¯t∥2.(17)
View SourceRight-click on figure for MathML and additional features.

The function G(α¯¯¯s,α¯¯¯t) is convex in the coefficients α¯¯¯s and α¯¯¯t and its global minimum can be found by setting its derivatives to 0
∂G(α¯¯¯s,α¯¯¯t)∂α¯¯¯s∂G(α¯¯¯s,α¯¯¯t)∂α¯¯¯t=2Asα¯¯¯s−2Bsys+2μ1α¯¯¯s−2μ1α¯¯¯t=0=2Atα¯¯¯t−2Btyt+2μ1α¯¯¯t−2μ1α¯¯¯s=0,(18)
View SourceRight-click on figure for MathML and additional features.where
AsAt=(U¯¯¯¯s)T(Ss)TSsU¯¯¯¯s,Bs=(U¯¯¯¯s)T(Ss)T=(U¯¯¯¯tT¯¯¯¯)T(St)TStU¯¯¯¯tT¯¯¯¯,Bt=(U¯¯¯¯tT¯¯¯¯)T(St)T.(19)
View SourceRight-click on figure for MathML and additional features.This gives the coefficient vectors as
α¯¯¯sα¯¯¯t=(μ−11AtAs+At+As)−1(μ−11AtBsys+Bsys+Btyt)=(μ−11Asα¯¯¯s+α¯¯¯s−μ−11Bsys).(20)
View Source

4.2 Optimization of the Transformation Matrix
In the second step of an iteration, the coefficient vectors α¯¯¯s and α¯¯¯t are fixed and the transformation matrix T¯¯¯¯ is optimized. Then the minimization of the objective in (15) becomes equivalent to the following problem
minT¯¯¯¯H(T¯¯¯¯)=minT¯¯¯¯∥StU¯¯¯¯tT¯¯¯¯α¯¯¯t−yt∥2+μ2∥M¯¯¯¯¯⊙T¯¯¯¯∥2Fsubject to∑i=1RT¯¯¯¯2ij=1, for  j=1,…,R.(21)
View Source

The above problem involves the minimization of a quadratic convex function H(T¯¯¯¯) in T¯¯¯¯ subject to R equality constraints that are also quadratic and convex in T¯¯¯¯. We solve the problem in (21) using the Sequential Quadratic Programming (SQP) algorithm [61], which is a method to numerically solve constrained nonlinear optimization problems. The SQP algorithm is based on iteratively approximating the original problem with a Quadratic Programming problem, where the objective function is replaced with its local quadratic approximation, and the equality and inequality constraints are replaced with their local affine approximations. In our problem (21), the objective function H(T¯¯¯¯) is already a quadratic function of T¯¯¯¯ and we only have equality constraints.

The first and second order derivatives to be used in the solution of (21) are found as follows. Let t¯∈RR2 denote the column-wise vectorized form of the matrix T¯¯¯¯, such that its kth entry is given by t¯k=Tij, with k=(j−1)R+i, for i,j=1,…,R. We denote by h(t¯)=H(T¯¯¯¯) the objective in (21) when considered as a function of t¯. The objective function h(t¯)=H(T¯¯¯¯) can then be rewritten in terms of t¯ as
h(t¯)=∥At¯−yt∥2+μ2∥Ft¯∥2.(22)
View SourceRight-click on figure for MathML and additional features.Here A∈RLt×R2 is a matrix with entries given by Alk=(StU¯¯¯¯t)liα¯¯¯tj and F∈RR2×R2 is a diagonal matrix with entries given by Fkk=M¯¯¯¯¯ij, where l=1,…,Lt and k=R(j−1)+i, for i,j=1,…,R. The variable Lt here is the number of labeled target samples. Next, the jth equality constraint of the problem (21) can be written in terms of t¯ as
gj(t¯)=∑i=1RT¯¯¯¯2ij−1=0,(23)
View SourceRight-click on figure for MathML and additional features.for j=1,…,R.

The problem (21) is then solved by forming the Lagrangian function
/L(t¯,η)=h(t¯)−g(t¯,η),(24)
View Sourcewhere
g(t¯,η)=∑j=1Rηjgj(t¯),(25)
View SourceRight-click on figure for MathML and additional features.ηj>0 are the Lagrange multipliers, and η=[η1…ηR]T. From (22), we obtain the gradient of the objective h(t¯) as
∇t¯h=2(ATA+μ2FTF)t¯,(26)
View SourceRight-click on figure for MathML and additional features.and its Hessian as
∇2t¯t¯h(t¯)=2(ATA+μ2FTF).(27)
View SourceNext, from (23), the kth entry of the gradient of gj(t¯) is found as
(∇t¯gj)k={2t¯k,0,if (j−1)R+1≤k≤jRotherwise,(28)
View SourceRight-click on figure for MathML and additional features.for k=1,…,R2. From (28), the Hessian ∇2t¯t¯g(t¯,η) of the second term g(t¯,η) of the Lagrangian in (25) is obtained as a diagonal matrix with entries given by
[∇2t¯t¯g(t¯,η)]kk=2ηj,(29)
View SourceRight-click on figure for MathML and additional features.for R(j−1)+1≤k≤Rj. Putting (27) and (29) together, we get the Hessian of the Lagrangian as
∇2t¯t¯/L(t¯,η)=∇2t¯t¯h(t¯)−∇2t¯t¯g(t¯,η).(30)
View SourceRight-click on figure for MathML and additional features.

The SQP algorithm optimizes objectives with equality constraints by iteratively updating the solution (t¯,η), where a linear system representing the approximate solution of the KKT conditions with the Newton's method is solved in each iteration [61, Algorithm 18.1]. The linear system is constructed from the objective h(t¯), the constraints gj(t¯), their gradients, and the Hessian of the Lagrangian.

Remark.
Although the SQP algorithm often converges to a solution in practice, it is not easy to establish a general theoretical convergence guarantee. For our problem, the convergence can be theoretically guaranteed under certain conditions: Let the algorithm parameter μ2 be chosen such that there exists a local solution (t¯∗,η∗) to the Lagrangian function /L(t¯,η) of the constrained problem (21) such that μ2>η∗j for all j=1,…,R. Then, if the initialization (t¯,η) of the SQP algorithm is sufficiently close to (t¯∗,η∗), the algorithm converges to (t¯∗,η∗). The details of this convergence analysis are provided in [60, Appendix B].

4.3 Overall Optimization Procedure
We now overview the overall optimization procedure employed in the proposed DASGA method. First, the optimization variables T¯¯¯¯, α¯¯¯s, and α¯¯¯t are initialized as follows. Since the objective in Problem 3 aims to find a transformation that aligns the source and target Fourier bases, a natural choice would be to initialize T¯¯¯¯ as the identity matrix, so that each source vector usi is mapped to the target vector uti. However, even in a simple scenario where the source and target graphs are very similar, as the eigenvalue decomposition determines eigenvectors up to a sign, mapping each usi to uti might in fact constitute a bad initialization; e.g., consider the very simple case where the source and target graphs are identical but uti=−usi. An unfavorable initialization of the transformation matrix may consequently influence the estimates of the coefficient vectors α¯¯¯s, α¯¯¯t and affect the overall solution of the alternating optimization procedure.

In order to obtain a more favorable initialization, we propose to set the initial T¯¯¯¯ matrix with a strategy that corrects the sign of each target vector according to its best match among the source basis vectors. This strategy is based on the method presented in our previous work [59], where the best match of a target vector uti among the source vectors is determined by finding
maxj|⟨u~sj,u~ti⟩|.(31)
View SourceRight-click on figure for MathML and additional features.

Here u~sj, u~ti are subvectors of the basis vectors usj, uti obtained by restricting them to a subset of their entries indexed by some {si}Ki=1 and {ti}Ki=1. It is difficult to directly compare the vectors usj, uti as the nodes of the source and target graphs are ordered arbitrarily and independently of each other. If a set of corresponding source and target node pairs N={(xssi,xtti)}Ki=1 is known, then this set can be used for the restriction of the basis vectors to a subset of their entries in the problem (31), so that the vectors usj, uti can be compared throughout their chosen entries. However, in our method we do not rely on the availability of a set of corresponding node pairs and propose to form the set N={(xssi,xtti)}Ki=1 based on the class labels, such that each pair of matched nodes (xssi,xtti) is formed randomly among the source and target nodes having the same class labels. We then compare the vectors usj, uti over their entries u~sj, u~ti corresponding to these nodes. Although very few labeled target nodes are typically available in a domain adaptation application, we have observed that only a few pairs is often sufficient to determine the correct signs for initializating T¯¯¯¯, which is next done as follows
T¯¯¯¯ii=sgn(⟨u~sJi,u~ti⟩),Ji=argmaxj|⟨u~sj,u~ti⟩|.(32)
View SourceRight-click on figure for MathML and additional features.Here sgn denotes the sign function and T¯¯¯¯ is initialized as a diagonal matrix with −1's or 1's on the diagonals that matches the sign of each target vector uti to the source vector usj best corresponding to it. This initialization respects the normalization constraint (15) on the entries of T¯¯¯¯.

Once the transformation matrix T¯¯¯¯ is initialized in this way, the alternating optimization procedure starts, where the coefficient vectors α¯¯¯s and α¯¯¯t are computed by fixing T¯¯¯¯ first, and then T¯¯¯¯ is optimized by fixing α¯¯¯s and α¯¯¯t in each iteration, as described in Sections 4.1 and 4.2. In each iteration, both the updates on α¯¯¯s and α¯¯¯t, and the update on T¯¯¯¯ either reduce or retain the value of the objective function in (15). Since the objective function is nonnegative and thus bounded from below, it converges throughout the proposed iterative alternating optimization process. We continue the iterations until the convergence of the objective function. The proposed Domain Adaptation via Spectral Graph Alignment algorithm is summarized in Algorithm 1.

4.4 Complexity Analysis
The overall complexity of the proposed method is mainly determined by the complexity of Steps 4 and 5 of Algorithm 1 executed iteratively until convergence. Let Ls and Lt denote the number of labeled samples respectively in the source and the target domains. The overall complexity of the DASGA algorithm is then obtained as O(R6+(Ls+Nt)R2+LtNtR), the details of which are presented in [60, Section 4.4].

Algorithm 1. Domain Adaptation via Spectral Graph Alignment (DASGA)
Input:

Ws, Wt: Source and target graph weight matrices

ys, yt: Available source and target labels

Initialization:

Set the transformation matrix T¯¯¯¯ as in (32).

repeat

Update coefficients α¯¯¯s, α¯¯¯t by solving (17).

Update transformation matrix T¯¯¯¯ by solving (21).

until the objective function (15) converges

Output:

ft=U¯¯¯¯tT¯¯¯¯α¯¯¯t: Estimated target label function

fs=U¯¯¯¯sα¯¯¯s: Estimated source label function

SECTION 5Experimental Results
In the following, we first introduce the datasets and then evaluate the performance of the proposed method with comparative experiments. Next, we study the behavior of the algorithm throughout the iterative optimization procedure and examine its sensitivity to the choice of the algorithm parameters.

5.1 Data Sets
The following data sets are used in the experiments.

Synthetic Data Sets. The two synthetic data sets shown in Figs. 4a and 4b are generated by drawing 100 samples for each class from a normal distribution in R3, with different means for the two classes. The means of the source classes and the corresponding target classes are symmetric along the y-direction. The variance of the distribution is higher in Synthetic dataset-2 compared to Synthetic dataset-1; hence the difficulty level of the classification task is higher. The source and the target graphs are constructed by connecting each data sample to their 25 nearest neighbors. The edge weights are computed with a Gaussian kernel as wsij=exp(−∥xsi−xsj∥2/σ2) in the source graph and similarly in the target graph, where xsi and xsj are the data sample coordinates and the scale parameter σ is chosen proportionally to the typical distance between neighboring samples.

MIT-CBCL Face Image Data Set. The MIT-CBCL face recognition database [62] consists of a total of 3,240 face images rendered from the 3D head models of 10 participants under varying illumination and poses. The images of each participant are rendered under 9 different poses varying from the frontal view (Pose 1) to a nearly profile view (Pose 9), and 36 illumination conditions at each pose. Some sample images are shown in Fig. 5. We downsample the images to a resolution of 100×100 pixels and consider the images taken under each pose as samples from a different domain. Raw features consisting of pixel intensities are used in the experiments. Two settings are considered, where the source domain is taken as Pose 1 in both settings. The target domain is taken as Pose 5 in the first setting, and as Pose 9 in the second setting. Source and target data graphs are constructed independently in the source and the target domains, by connecting each image to its nearest 38 neighbors with respect to the euclidean distance. The weight matrices Ws, Wt are constructed with a Gaussian kernel as in the synthetic data sets.

COIL Object Image Data Set. The COIL-20 object database [63] consists of a total of 1,440 images of 20 objects. Each object has 72 images taken from different viewpoints rotating around it. We downsample the images to a resolution of 32×32 pixels. We consider a transfer learning setting by dividing the 20 objects in the data set into two groups and matching each object in the first group to another object in the second group with respect to their similarity computed via pairwise distances. The experiments are then done by considering each group of 10 objects as a different domain, and regarding the images of the matched objects across the two domains as having the same class label. The two groups and the matched object pairs are shown in Fig. 6. The source and the target graphs are constructed by connecting each sample to its 3 nearest neighbors and the weights are set with a Gaussian kernel. A small number of neighbors is chosen deliberately to be coherent with the small intrinsic dimension of the data set as the images are formed by rotating the camera around each object in only one direction.

Amazon Product Ratings Data Set. The Amazon data set [64] is used in the task of predicting user ratings on books. The data set contains scores from users who purchased a book from Amazon, where the scores are integers in the range [1,5]. The experiment is conducted on the first 150000 ratings in the data set. The users who rated less than three books are excluded from the experiment. In each repetition of the experiment, two bestsellers are chosen from the book catalogue of Amazon. The source graph consists of the users who read the first bestseller, and the target graph consists of the users who read the second bestseller. Each graph node represents a user, and the scores that the users gave to the first and the second bestsellers are regarded as signals (label functions), respectively on the source and the target graphs. The source and the target graphs are constructed with respect to the similarities between the users, where two users are considered similar if their past reading records agree. Thus, if two users have read books in common, they are connected with an edge in the graphs. The edge weights are determined as inversely proportional to the average difference of the scores the users assigned to the same books, in order to capture the similarity of their literary preferences. Given the scores on the source bestseller, and the available scores on the target bestseller, we consider the task of predicting the unavailable scores on the target bestseller.

Facebook Data Set. The Facebook data set [65] consists of various communities (friend circles) extracted from the Facebook network. Graph nodes and edges respectively represent Facebook users and their friendship relations. In our experiments two different communities are chosen as the source graph and the target graph. Isolated users or user cliques are removed and the weights of all edges are set to the constant value 1. The gender of the Facebook users is taken as the binary label function to be predicted. The source and the target graphs, consisting respectively of 157 and 40 users, are shown in Fig. 7, where the values of the label function are represented with two different colors.

In the following, we first verify the validity of the main assumption of the proposed method that the frequency content of the label function is similar on the source and the target graphs. The Fourier coefficients of the source and the target label functions are plotted in Fig. 8 for the MIT-CBCL data set. In Fig. 8a, the source and the target Fourier coefficients at the same frequency have quite similar magnitudes due to the high similarity between the source and the target images captured under nearby camera angles. On the other hand, in Fig. 8b where the two domains bear smaller resemblance, the source and the target Fourier coefficients at the same frequency do not always have similar magnitudes. Nevertheless, the shape of the spectrum is similar between the source and the target graphs, with similar amplitudes at nearby frequencies. The spectrum of the label function is studied also for the other data sets in [60, Section 5.1]. The results lead to the common conclusion that the assumption that the label function has similar frequency content on the source and the target graphs is realistic in practice.

5.2 Evaluation of the Algorithm Performance
The performance of the proposed DASGA method is compared to the domain adaptation methods Heterogeneous Domain Adaptation using Manifold Alignment (DAMA) [32], Easy Adapt++ (EA++) [23], Subspace Alignment (SA) [2], Geodesic Flow Kernel for Unsupervised Domain Adaptation (GFK) [3], Scatter Component Analysis (SCA) [40], LDA-Inspired Domain Adaptation (LDADA) [46], Joint Geometrical and Statistical Alignment (JGSA) [31]; as well as the baseline classifiers Support Vector Machine (SVM), Nearest-Neighbor classification (NN), and the graph-based Semi-Supervised Learning with Gaussian fields (SSL) algorithm [13]. The baseline classifiers SVM and NN are evaluated under the “source+target” setting using the labeled samples from both the source and the target domains for training, and the SSL algorithm is used in the “target only” setting, which give the best results. When testing the SA and GFK algorithms, once the source and the target domains are aligned in an unsupervised way as proposed in [2] and [3], the known source and target labels are both used in the final classification. In all experiments the source labels are assumed to be known and the ratio of known target labels is varied gradually. The class labels of the unlabeled target samples are then estimated with the tested methods and the classification performances are compared.

5.2.1 Experiments on Synthetic Data Sets
The proposed DASGA algorithm is used with the parameters μ1=0.1, μ2=1, R=9 in the experiments with synthetic data sets. In Fig. 9, the misclassification rates of unlabeled target samples in percentage are plotted with respect to the ratio of labeled target samples in percentage. The results are averaged over 50 repetitions of the experiment with random selections of the labeled samples. As expected, the misclassification rates of the algorithms tend to decrease as the ratio of known target labels increases. The proposed DASGA algorithm is observed to outperform the compared methods in both data sets. The performance gap between DASGA and the other methods is larger in Synthetic dataset-2, which is a more challenging data set due to the relatively high distribution variance. Among the domain adaptation methods, DAMA [32] and LDADA [46] give the closest performance to the proposed DASGA method. The approach in both of these methods is to learn supervised projections, which is relatively successful in this synthetic data set consisting of normally distributed data. On the other hand, the proposed DASGA method relies on a pure graph representation of data, therefore its performance is less affected by the ambient space properties of the data. This feature is seen to provide some robustness against the challenges such as large variance and poor separation between the classes.

Fig. 9. - 
Misclassification rates of target samples for synthetic data sets.
Fig. 9.
Misclassification rates of target samples for synthetic data sets.

Show All

5.2.2 Experiments on Image Data Sets
We next evaluate the performance of the proposed algorithm on the image data sets. In the experiments with the MIT-CBCL face image data set, the parameters of the proposed DASGA method are set as μ1=0.1, μ2=0.85, and R=9. The experiment is repeated over 10 realizations with random selections of the labeled samples and the results are averaged. The misclassification rates of the unlabeled target images are plotted with respect to the ratio of labeled target images in Figs. 10a and 10b, where the target domain is respectively taken as Pose 5 and Pose 9. The misclassification errors of the algorithms are seen to be larger in Fig. 10b compared to Fig. 10a, as the source and the target poses differ more significantly and the similarity between the two domains is weaker. In Fig. 10a, where the source and the target domains are relatively similar, the proposed DASGA method is seen to be outperformed by the domain adaptation methods EA++ and JGSA, as well as the SA, GFK and LDADA methods which yield almost zero error. Capturing the face images of the same participants from nearby poses in a clean and controlled environment, the two domains in this experiment are quite convenient to align with methods using projections and geometric transformations, which explains the success of these algorithms. On the other hand, the proposed graph-based DASGA algorithm does not use the pixel intensity values of image data samples once the source and target graphs are constructed, hence, it does not employ the same type of information as these methods. Nevertheless, in Fig. 10b, where the source and target images differ more significantly, the performance of DASGA catches up with the other methods when the ratio of known target labels reaches 7 percent.

Fig. 10. - 
Misclassification rates of target samples for the MIT-CBCL data.
Fig. 10.
Misclassification rates of target samples for the MIT-CBCL data.

Show All

In the experiments with the COIL-20 image data set, the parameters of the proposed method are set as μ1=1, μ2=1 and R=10. The misclassification rates of the algorithms are plotted with respect to the ratio of known target labels in Fig. 11. The proposed DASGA method is observed to often yield the best classification performance. The misclassification rate of the proposed algorithm falls to zero when about 7 percent of the samples are labeled in the target domain. The graph-based semi-supervised learning algorithm SSL also performs well in this experiment. The regular sampling of the images on the image manifold in this data set allows the construction of well-organized graphs, which can be successfully exploited by graph-based learning methods. The performances of the domain adaptation methods SA, DAMA, and LDADA fall behind that of the simple NN classifier in this experiment. Relying on the alignment of the source and the target domains via transformations, these methods fail in the transfer learning problem considered in this experiment. The source and the target images belong to different objects; hence, they are difficult to align via linear projections or transformations. It is also interesting to note that the relatively more sophisticated SCA method based on nonlinear kernel transformations, is more successful in this challenging data set compared to the previous data sets of simpler structure.

Fig. 11. - 
Misclassification rates of target samples for the COIL-20 data.
Fig. 11.
Misclassification rates of target samples for the COIL-20 data.

Show All

5.2.3 Experiments on the Amazon Book Ratings Data Set
In the experiments with the Amazon book ratings data, the parameters of DASGA are set as μ1=0.001, μ2=0.8, and R=10, which are selected by trials on a test setup with two arbitrarily chosen bestsellers that are not used in the experiments. Being a purely graph-based method, the proposed DASGA algorithm requires only the source and the target user graphs and the available ratings. Meanwhile, the other algorithms in comparison require the coordinates of the data samples; thus, need an embedding of the data in an ambient space. Unlike the image data and the synthetic data used in the previous experiments, the data samples do not have a physical embedding in this experiment. One could possibly regard the user ratings given to previously read books as feature vectors. However, due to the very large number of books in the Amazon catalogue and the small number of books users typically read, such feature vectors are very sparse in a very high-dimensional ambient space. This increases the complexity and impairs the performance and feasibility of most of the compared methods. Another solution could be to represent graph nodes using graph-theoretic features as in [66], [67]; however, such features should be selected and used carefully. In order to test the compared methods, we follow an alternative approach and embed the source and the target graphs into an euclidean domain of optimal dimension using the Multidimensional Scaling (MDS) algorithm [68]. The coordinates learnt for each user with MDS are then used as training features by the compared algorithms.

The experiment is conducted over 10 different pairs of source and target bestsellers, with 10 repetitions of the experiment for each bestseller pair by randomly selecting the labeled nodes. The average misclassification rates of the score predictions (considering each score from 1 to 5 as a different class label) are plotted in Figs. 12a, and 12b shows the root mean square (RMS) error of the predictions. The results in Fig. 12a show that most of the methods including DASGA yield similar misclassification errors. Although DASGA does not provide smaller misclassification error than the other methods, Fig. 12b shows that it clearly outperforms the other methods in terms of the RMS prediction error. The ensemble of the results in Fig. 12 suggests that the proposed DASGA algorithm is well-fit to the regression problem inherent to this setting as it relies on the analysis of the rate of variation of the user rating functions over the graphs.

Fig. 12. - 
RMS errors and misclassification rates of target user score predictions for Amazon book ratings.
Fig. 12.
RMS errors and misclassification rates of target user score predictions for Amazon book ratings.

Show All

5.2.4 Experiments on the Facebook Data Set
As the Facebook data set involves a pure graph environment, the two graphs are embedded into an euclidean domain via the MDS algorithm as in the Amazon data set in order to provide feature representations for the other algorithms than DASGA and SSL. The parameters of DASGA are set as μ1=1, μ2=1, and R=8. The misclassification rates of the compared methods are presented in Fig. 13. The classification errors of all methods are relatively high in this experiment, which can be explained by observing the challenging structure of the data set in Fig. 7. The proposed DASGA method is seen to generally outperform the other methods. It is interesting to compare DASGA to the reference graph-based SSL method. When the ratio of available target labels is relatively small, DASGA performs better than SSL thanks to the information of the label spectrum transmitted from the source graph. Meanwhile, when the ratio of available target labels exceeds 50 percent, the SSL method has sufficient information to diffuse in the target graph and it can guess the label function more accurately than DASGA. This is coherent with the principle of domain adaptation: learning the label spectrum from an exemplar source graph improves the performance in the target graph when the label information is restricted in the target graph, which is typically the case in a domain adaptation problem.

Fig. 13. - 
Misclassification rates of target samples for the Facebook data.
Fig. 13.
Misclassification rates of target samples for the Facebook data.

Show All

5.3 Stabilization and Sensitivity Analysis of the Proposed Algorithm
We first study the behavior of the proposed DASGA algorithm throughout the iterative optimization procedure. We examine the variations of the objective function and the misclassification rate of target samples during the iterations. The value of the objective function (15) is evaluated in each iteration of the alternating optimization procedure, as well as the misclassification rate given by the solution computed in each iteration. The evolutions of the objective function and the misclassification rate are shown for the COIL-20 and the MIT-CBCL data sets in Fig. 14. The results confirm that the objective function decreases monotonically throughout the iterations and converges as discussed in Section 4.3. The misclassification rate also has the general tendency to decrease during the iterations. The rate of decrease of the misclassification error follows closely that of the objective function in both data sets. This suggests that the objective function (15) underlying the proposed method captures well the actual performance of classification.

Fig. 14. - 
Evolution of the objective function and the misclassification rate throughout the iterations.
Fig. 14.
Evolution of the objective function and the misclassification rate throughout the iterations.

Show All

Next, we discuss the sensitivity of the DASGA method to the choice of the algorithm parameters.

Sensitivity to the Weight Parameters μ1 and μ2. In order to determine how the choice of the weight parameters affects the algorithm performance, the target misclassification rate of the algorithm is examined in all data sets for various (μ1,μ2) pairs in a region of interest in the accompanying technical report [60, Section 5.3]. The results indicate that although the optimal values of the μ1 and μ2 parameters may vary among different data sets, the (μ1,μ2) pair yielding the smallest misclassification error is often found within the region μ1∈[0.1,1], μ2∈[0.5,1.5]. In addition, the misclassification error does not vary dramatically within this region, with the relative difference in the error changing between 1-4 percent for all data sets. These findings suggest that it is safe to choose the weight parameters within the intervals μ1∈[0.1,1] and μ2∈[0.5,1.5], where the algorithm performs sufficiently well.

Sensitivity to the Number of Neighbors K. Next, we study the effect of the choice of the number of nearest neighbors K when constructing the source and the target graphs. The variation of the target misclassification rate with K is examined on the synthetic, MIT-CBCL, and the COIL-20 data sets, where the source and the target graphs need to be constructed from data. Detailed results are presented in [60, Section 5.3]. While the misclassification rate has been seen to be stable over a relatively wide range of K values for the synthetic data sets, the performance has been found to be more sensitive to the choice of K for the COIL-20 data set. In particular, the optimal value of K is quite small (around 3−4) for COIL-20. This result is in line with the intrinsic geometric properties of this data set: As the images of the objects are taken by rotating the camera around the object by varying a single camera angle parameter, the intrinsic dimension of COIL-20 is quite low; hence, K should be chosen small accordingly. By comparison, generated under a larger set of illumination parameters, the intrinsic dimension of the MIT-CBCL data set is higher than that of COIL-20. As a result, the optimal value of the K parameter has been found to be larger (around 35) for MIT-CBCL.

Sensitivity to the Number of Eigenvectors R. Finally, we investigate how the choice of the number of graph basis vectors R used in the objective (15) affects the algorithm performance. An analysis of the variation of the target misclassification rate with the number of basis vectors R is presented in [60, Section 5.3] for all data sets. The results suggest that the variation of the misclassification rate with R has similar characteristics among different data sets. At small R values, the classification performance improves as R increases, since the label function can be approximated more accurately when more basis vectors are used. The optimal value of R is often around 9-12, and the performance tends to degrade when R is increased beyond these values. This is because increasing R too much results in poor regularization and increases the misclassification error, which is also consistent with the theoretical bound in Proposition 1.

SECTION 6Conclusion
We have considered the problem of domain adaptation on graphs. Given a source graph with sufficiently many labeled nodes and a target graph, we have proposed a graph-based domain adaptation algorithm that estimates a label function on the target graph, relying on the assumption that the frequency content of the source and target label functions have similar characteristics. Our method is based on the idea of learning a pair of coherent bases on the source and the target graphs. The learnt bases not only resemble in terms of their spectral content, but also “align” the two graphs such that the label functions on the two graphs can be reconstructed with similar coefficients. The proposed domain adaptation algorithm is completely graph-based and is particularly applicable in learning problems defined purely on graph domains where no physical embedding of data samples is available. The performance of the proposed method is demonstrated mainly in data classification applications; however, it can potentially be applied to a wide range of machine learning problems concerning the inference of the unknown values of a graph function from available values. The exploration of information transfer based on more elaborate graph kernels than the graph Fourier basis, or the extension of the method to explicitly employ data embeddings in addition to graph models in order to improve its performance on data sets with available ambient space representations remain as future directions.