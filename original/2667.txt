Real graphs often contain edge and node weights, representing, for instance, penalty, distance or uncertainty. We study the problem of keyword search over weighted node-labeled graphs, in which a query consists of a set of keywords and an answer is a subgraph whose nodes contain the keywords. We evaluate answers using three ranking strategies: optimizing edge weights, optimizing node weights, and a bi-objective combination of both node and edge weights. We prove that optimizing node weights and the bi-objective function are NP-hard. We propose an algorithm that optimizes edge weights and has an approximation ratio of two for the unique node enumeration paradigm. To optimize node weights and the bi-objective function, we propose transformations that distribute node weights onto the edges. We then prove that our transformations allow our algorithm to also optimize node weights and the bi-objective function with the same approximation ratio of two. Notably, the proposed transformations are compatible with existing algorithms that only optimize edge weights. We empirically show that in many natural examples, incorporating node weights (both keyword holders and middle nodes) produces more relevant answers than ranking methods based only on edge weights. Extensive experiments over real-life datasets verify the effectiveness and efficiency of our solution.
SECTION 1Introduction
Massive amounts of graph data are generated every day from a variety of sources including social networks, e-commerce, Semantic Web (RDF data) and biochemical networks [1], [2]. Even relational databases can be modeled as graphs in which nodes correspond to tuples and edges correspond to primary key – foreign key relationships. As a result, query processing over graphs has become an important problem.

Keyword search in graphs, in particular, is a popular and intuitive approach that does not require mastery of a new query language such as SPARQL or knowledge of the graph database schema; in fact, some graph datasets do not have a well-defined schema and therefore cannot be explored using structured query languages [3]. Given a node-labeled graph in which nodes contain a set of keywords/terms (e.g., names of people, products or chemical compounds), and a query consisting of a set of input keywords, keyword search over graphs aims to find relevant subgraphs whose nodes contain the query keywords.

An answer to a keyword query is a subgraph, some of whose nodes, referred to as content nodes or keyword holders, contain the input keywords, and other nodes, referred to as middle nodes, connect the content nodes. We study the problem of ranking answers over weighted graphs, where weights may correspond to cost, penalty, distance, heterogeneity/homogeneity or uncertainty. We study the optimization (specifically, minimization) of three objectives: edge weights, node weights, and a bi-objective combination of both edge and node weights. To measure the proximity of the query keywords in an answer subgraph, we use the sum of distances among keyword holders, as it provides a number of benefits compared to other metrics such as the diameter of the subgraph (details in Section 2). To motivate our approach, we present examples from two domains: keyword search over the TPC-E benchmark data, and biological protein-protein interaction (PPI) networks with the associated genes.

Example: Keyword Search Over TPC-E Benchmark Data. Suppose we want to find relationships between tuples in a relational database. One way to do this is to convert the relational database into a graph (as was done in BANKS [5]). Tuples are converted to nodes and foreign keys become the edges of the graph connecting the associated nodes. Our running example is based on the TPC-E1 benchmark, which models and stores the on-line transaction processing (OLTP) data of a brokerage firm. The database contains 33 tables with financial information such as fees of brokers, customer accounts, and traded companies.

Consider a small portion of the TPC-E database in Fig. 1, containing nodes from six tables. For example, node “Aetna Inc.” comes from the table Company. For each node in the graph, we also show its cost, which is inversely proportional to the importance of its table in the database. To find the importance of each table, we use a method proposed in [4] that defines the importance of tables in a relational schema. The definition is based on statistical models and includes the content of the table and the relationships of the table in the entire schema. For example, in the TPC-E database, table Customer is significantly more important than table Status_Type. This is expected as Status_Type has only five rows and simply states the status of different entities. On the other hand, table Customer stores different features of thousands of customers and is a vital part of the database. Therefore, the cost of the Status_Type node in Fig. 1 is significantly higher than the cost of the Customer node. In general, different applications and scenarios might have different node weight or cost. For example, the node weight could be the social credibility/incredibility of people in a social network, the authority of a professional in an expert network, or compound interaction uncertainty in biological networks.

Fig. 1. - 
A portion of the TPC-E graph (converted from the TPC-E database) and three answers to the query “Rolanda Aetna”. Associated table names in the TPC-E database are shown in bold. The value of each tuple (node) is shown under the table name. Node costs are also shown in the graph, and were obtained from the associated table importance values calculated by the method in [4].
Fig. 1.
A portion of the TPC-E graph (converted from the TPC-E database) and three answers to the query “Rolanda Aetna”. Associated table names in the TPC-E database are shown in bold. The value of each tuple (node) is shown under the table name. Node costs are also shown in the graph, and were obtained from the associated table importance values calculated by the method in [4].

Show All

Consider the query “Rolanda Aetna” over the TPC-E database. In Fig. 1, Rolanda is the name of a customer and Aetna is the name of a company. Thus, the user wishes to find the relationship between customer Rolanda and company Aetna. Three answers are shown on the right side of Fig. 1. Answer (a) shows that both customer Rolanda and company Aetna have “Active” status (i.e., connected through the Status_Type node). However, looking at the TPC-E schema description, this is not a strong relationship between a customer and a company. Status_Type is a dimension table, is connected to six other tables, and stores the status values (e.g., active or cancelled). Looking at the TPC-E database, it turns out that all companies and all customers have active status. Thus, any company and any customer are connected through the active status. This indicates that answer (a) is not relevant. Answer (b) also does not present a strong relationship between Rolanda and Aetna. It indicates that the security traded on Aetna Inc. has the same status as the customer Rolanda and is similar to answer (a). However, answer (c) indicates that the stock of Aetna Inc. was traded by Rolanda in January 2018. This is clearly an interesting and strong relationship between a company and a customer. Also, few companies and customers might have this particular relation as each customer only trades the stock of a few companies. Using the weight and cost of the nodes of the graph, answer (c) is ranked ahead of answers (a) and (b). This motivates the need to consider node weights.

Example: Protein-Protein Interaction Network. Proteins are one of the most important components of every cell in the body. Their interactions control various mechanisms [6]. These interactions form a protein-protein interaction (PPI) network (i.e., graph). Fig. 2 shows a fragment of a PPI network, in which each node is a protein. If protein p1 interacts with protein p2, there is an edge between them in the PPI network. An interaction between p1 and p2 means that if they come in physical contact with each other, a bond or reaction occurs between them. Different proteins interact with each other with different probabilities. This biological behaviour is modeled by assigning uncertainty weights to the edges of a PPI network [7]. For example, in Fig. 2, p2 and p3 have the highest chance of interaction since the edge between them has the lowest uncertainty value. Furthermore, different proteins have different chances of formation [7]. Protein formation is directly related to its length and structure [7], and we use this to assign a cost to each protein. For example, in the PPI network illustrated in Fig. 2, p1 has the highest chance of formation since it has the lowest cost. In biology, proteins that have a higher chance of formation are more interesting as they are considered more effective in drug discovery [7]. Finally, different proteins are encoded by different genes. Thus, genes can be considered as attributes (i.e., keywords) of the proteins. In Fig. 2, p1 encodes two genes: G1 and G2.

Fig. 2. - 
A small portion of a PPI network.
Fig. 2.
A small portion of a PPI network.

Show All

Some genes are known to be associated with certain diseases [7]. In Fig. 2, gene G2 is associated with Lung Carcinoma. Given a set of genes, each responsible for a particular disease, we want to find a set of interconnected proteins that are encoded by these genes. The goal is to see the effect of silencing these proteins on the progress of the disease [8]. The genes are keywords to our search system, and the output is a subgraph of the PPI network. We prefer to silence a group of relevant proteins that interact with high confidence (i.e., smaller edge weights). We also want proteins with higher expression (i.e., smaller node cost), as they have a higher chance of formation, and silencing them is more likely to control the disease.

Fig. 3 presents two subgraphs from a PPI network, which are answers to a query consisting of three genes: {ARF1, KATNA1, CCT8}. Each gene is associated with a different type of Carcinoma, which is a type of cancer. Node costs and edge weights are also shown. Edge-based methods choose the left subgraph while node-based and combined methods choose the right subgraph. Both answers are almost equally certain/uncertain (the left answer is slightly better). However, the answer on the right has a higher expression and its proteins appear in more organs than those of the left answer. This is another indicator of a higher chance of formation. Without taking node weights into account, the answer on the right would not be chosen.

Fig. 3. - 
Two subgraphs as answers to a query with three genes (ARF1, KATNA1, CCT8). Subgraphs are labelled with edge and node weights as well as the number of organs that each protein is expressed in.
Fig. 3.
Two subgraphs as answers to a query with three genes (ARF1, KATNA1, CCT8). Subgraphs are labelled with edge and node weights as well as the number of organs that each protein is expressed in.

Show All

Contributions. We study the problem of finding relevant answers to keyword search over weighted graphs. We consider three ranking functions based on the sum of distances among keyword holders, which optimize: edge weights, node weights, and a bi-objective combination of the two.2 Optimizing edge weights was previously shown to be NP-hard. Our contributions are as follows.

We prove that optimizing node weights and the bi-objective functions are NP-hard by reduction from 3-SAT.

We propose an approximation algorithm with a approximation ratio of two to optimize the edge weight function for the unique node enumeration paradigm. To optimize the node weight and the bi-objective functions, we first propose transformations that distribute node weights onto the edges. We then prove that our algorithm for optimizing edge weights, together with the transformations, can also optimize the other objectives with the same approximation ratio of two. Our node weight transformation strategy is general and can be used by prior work that only optimizes edge weights.

For efficiency on large graphs, we propose a pruning rule that limits the set of nodes used as starting points for a potential answer subgraph. We show that our optimization significantly improves performance and still guarantees an approximation ratio of two. We also implement a 2-hop cover index that returns the shortest path distance between any two nodes. We propose an optimization (only indexing content nodes that are reasonably close to each other) to the 2-hop cover index that significantly improves performance and scalability without compromising the precision of answers. Since users are often interested in a ranked list of top-k answers, we also develop an algorithm to find top-k answers in polynomial time.

We perform a comprehensive evaluation using real weighted graphs to demonstrate the viability of our methods and to compare against existing methods. Our tested datasets include: IMDb, TPC-E, DBLP, and PPI. We empirically show that incorporating the weights of all nodes (keyword holders and middle nodes) produces more relevant answers.

SECTION 2Problem Statement
Given a weighted node-labeled graph and a query consisting of a set of keywords, we want to find and rank the answers. Following prior work in this area, each answer is a subgraph3 that contains all the query keywords [9], [10]. We assume that the input graph is undirected but our approach can work with directed graphs. Table 1 summarizes the important symbols used in this paper.

TABLE 1 Symbols Used in This Paper
Table 1- 
Symbols Used in This Paper
Each node and edge in the input graph has an application-supplied positive weight.4 Let di(nv,nw) be the edge distance between neighbor nodes nv and nw. If there is no edge between nv and nw then di(nv,nw)=∞. A path P is a sequence of edges that connect a sequence of nodes. The nodes and edges on a path are distinct and we do not allow loops. Let cost(ni) be the cost (weight) of node ni. Optimizing edge and node weights (i.e., edge distances and node costs) are minimization problems.

2Definition 1 (Answer).
Given a graph G and a list of t keywords Q={k1,k2,…,kt}, an answer to Q is a subgraph composed of a list {n1,n2,…,nt} of content nodes (not necessarily distinct) and a set of middle nodes that connect content nodes in G. An answer must satisfy the following properties:

Coverage: ∀i,1≤i≤t, content node ni contains keyword ki.

Connectivity: There is a path between each pair of content nodes in the answer.

Minimality: Each middle node (i.e., a node that does not contain an input keyword) must be on a weighted shortest path between at least one pair of content nodes.

Our definition of coverage ensures that each keyword is present in at least one node in the answer. Furthermore, minimality eliminates “dangling” middle nodes (that do not contain any input keywords).

Each answer A has a cumulative edge distance and node cost. To define these, we first introduce the concepts of minimum edge distance path and minimum node cost path between any two nodes in G. As we mentioned before, a path P is a sequence of edges that connect a sequence of nodes. The edge distance of a path P between two nodes ns and nd is defined as the sum of the distances of the edges along that path and is denoted as edGP(ns,nd). The node cost of a path P between two nodes ns and nd is defined as the sum of the cost of the nodes along that path, including those of ns and nd themselves, and is denoted as ncGP(ns,nd). The minimum edge distance path (edGmin) and minimum node cost path (ncGmin) are defined below and will be used in Definitions 4 and 5, respectively.

2Definition 2 (Minimum Edge Distance Path).
Given a graph G and two nodes ns and nd, a minimum edge distance path between nodes ns and nd is a path whose edge distance is smallest among all paths between ns and nd. The sum of the distances of the edges in this path is denoted as edGmin(ns,nd). If ns and nd are not connected in G, edGmin(ns,nd)=∞.

2Definition 3 (Minimum Node Cost Path).
Given a graph G and two nodes ns and nd, a minimum node cost path between nodes ns and nd is a path whose node cost is smallest among all paths between ns and nd. The sum of the cost of the nodes in this path is denoted as ncGmin(ns,nd). If ns and nd are not connected in G, ncGmin(ns,nd)=∞.

A minimum edge distance path (i.e., weighted shortest path) can be found using Dijkstra's algorithm in graphs with positive edge weights. In addition, in graphs with positive edge weights, the shortest path function (i.e., distance function) is a metric and satisfies the triangle inequality. As we show later, by distributing node costs onto the edges, we can also use Dijkstra's algorithm to find minimum node cost paths. Therefore, the minimum edge distance and minimum node cost functions are metrics and satisfy the triangle inequality. We will use the triangle inequality property of these metrics to derive the approximation ratio of our algorithms.

The minimum edge distance between a node ni and a subset of nodes5 N′ is defined as edGmin(ni,N′)=min[edGmin(ni,nj) ∀ nj∈N′]. The node that has the smallest minimum edge distance path to ni among the nodes in N′ is defined as noGmin(ni,N′)=nx | ∀ nj∈N′ edGmin(ni,nx)≤edGmin(ni,nj). If more than one node have the smallest minimum edge distance path to ni, one of them is chosen randomly as noGmin(ni,N′). If N′=∅, we define edGmin(ni,N′)=∞, and noGmin(ni,N′)=∅. Now, we are ready to define the edge distance and node cost of an answer A.

2Definition 4 (Edge Distance of an Answer (ED)).
Let {n1,n2, …,nt} be a list of content nodes in an answer A to query Q={k1,k2,…,kt}. The edge distance (ED) of A is defined as ED(A) =∑t−1i=1∑tj=i+1edGmin(ni,nj).

2Definition 5 (Node Cost of an Answer (NC)).
Let {n1, n2,…,nt} be a list of content nodes in an answer A to query Q={k1,k2,…,kt}. The node cost (NC) of A is defined as NC(A)=∑t−1i=1∑tj=i+1ncGmin(ni,nj).

Note that ED and NC are computed by summing up the minimum edge distances or node costs between all pairs of content nodes (also called sum of distances function). Previous work has shown that considering all content nodes during ranking produces meaningful answers [11], [12], giving each query keyword and therefore each content node the same priority, whereas other distance functions such as the diameter of the subgraph are biased towards a subset of the content nodes.6 Another useful property of ED and NC is that a content node's contribution to the score of an answer is proportional to the number of keywords it covers. This has semantic benefits in some applications such as team formation.7 For example, if a team member is responsible for more than one task, he or she should contribute more to the score of an answer. The same benefit applies to edges. If an edge connects more than one pair of content nodes, it contributes more to the score [3], [14]. Now, based on the above ED and NC objectives, we define the following two problems.

2Problem 1.
Given a graph G and a query Q, find an answer A to Q in G with the minimum edge distance ED(A).

Problem 1 is NP-hard and has been studied in previous work on graph keyword search [3], [11], [15]. However, it only considers path lengths between content nodes. We introduce a new problem that takes the cost of nodes into account.

2Problem 2.
Given a graph G and a query Q, find an answer A to Q in G with the minimum node cost NC(A).

2Theorem 1.
Problem 2 is NP-hard.

2Proof.
By reduction from 3-satisfiability (3-SAT), we prove that the decision version of the problem is NP-hard: does there exist an answer A with node cost NC(A) at most w (w>0), for some constant w?

Consider a set of m clauses Dk=xk∨yk∨zk (k=1,…,m) and {xk,yk,zk}⊂{u1,u¯¯¯1,…,un,u¯¯¯n}. We define an instance of the above problem as follows. First, we create a graph G. For each pair of variables ui and u¯¯¯i, two nodes are created in G. Thus, we have 2×n nodes. For each pair of variables ui and u¯¯¯i, we create one keyword ki (i=1,…,n). Thus, ui and u¯¯¯i have keyword ki and the only holders of ki are ui and u¯¯¯i. In addition, for every clause Dk, we create one keyword kn+k (k=1,…,m) such that the holders of keyword kn+k are the triplet of nodes associated with those of xk, yk and zk. Therefore, the number of keywords is n+m. We set the value of the minimum node cost path between each variable and its negation (i.e., ui and u¯¯¯i) to 2×w (i.e., ncGmin(ui,u¯¯¯i)=2×w). The value of the minimum node cost path between other variables is set to w(n+m2). Note that the values of the minimum node cost paths are given as part of the input to the problem and will not be re-calculated.

A feasible answer A to the above problem with node cost NC(A) at most w is any set of nodes such that from each pair of nodes corresponding to ui and u¯¯¯i, exactly one is selected and from each triplet of nodes corresponding to xk, yk and zk, one is selected. Thus, if there exists a subset of nodes A with NC(A) at most w, then there exists a satisfying assignment for D1∧D2∧⋯∧Dm. On the other hand, a satisfying assignment determines a feasible set of nodes A with NC(A) at most w. This completes the proof.

Additionally, we are interested in the bi-criteria optimization problem of minimizing both the edge distance and the node cost objectives. One way to solve a bi-criteria optimization problem is to convert it into a single objective problem. To do this, we define the minimum combined path between any two nodes in the graph based on a tradeoff parameter λ. Then, we define a new problem that minimizes both node cost and edge distance of an answer. If node cost and edge distance values have different scales, their values should first be normalized.

2Definition 6 (Minimum Combined Path).
Given a graph G, two nodes ns and nd, and a tradeoff parameter λ (0≤λ≤1), a minimum combined path between nodes ns and nd is a path that minimizes the following quantity:
coGmin(ns,nd)=min[λ.ncGP(ns,nd)+(1−λ).edGP(ns,nd)].
View SourceRight-click on figure for MathML and additional features.

If ns and nd are not connected in G, coGmin(ns,nd)=∞.

The parameter λ varies from 0 to 1 and determines the tradeoff between edge distances and the node cost. Since λ is application-dependent, we leverage user and domain expert feedback to set and update it over time. Incorporating user feedback helps achieve high search precision (as will be illustrated in our experimental evaluation).

2Definition 7 (Combined Objective of an Answer (CO)).
Let {n1,n2,…,nt} be a list of content nodes of an answer A to query Q={k1,k2,…,kt}. The combined objective (CO) of A is defined as CO(A)=∑t−1i=1∑tj=i+1coGmin(ni,nj).

2Problem 3.
Given a graph G, a query Q and a tradeoff parameter λ, find an answer A to Q with the minimum combined objective CO(A).

2Theorem 2.
Problem 3 is NP-hard.

2Proof.
Similar to Theorem 1, this can be proven by reduction from 3-SAT; details omitted for brevity.

2.1 Bi-Objective Optimization Discussion
We now discuss why we use the combined objective CO(A) instead of directly merging Definitions 4 and 5. In other words, we discuss why we are not minimizing λ.NC(A)+(1−λ)ED(A). The problem is that the same set of content nodes could be connected through different paths, some having lower NC and others having lower ED. Recall the expert team formation application. Suppose we want to form a team of experts with two skills: DB and IR. Two hypothetical teams that cover these two skills, A1 and A2, are shown in Fig. 4, along with the edge distance and node cost. To minimize the edge distance objective of A1, we should use the dotted edges and we get 2; to minimize its node cost, we should use the solid edges and we get 3. The edge distance of A2 is 4 and its node cost is 6, both using the dashed edges. Let us set the tradeoff parameter λ to 0.5. If we simply combine the two objectives, A1 is the best answer because the combined objective of A1 is (0.5×2)+(0.5×3)=2.5 and the combined objective of A2 is (0.5×4)+(0.5×6)=5. However, how can we communicate A1 to the user? The content nodes should be connected differently depending on the objective. We could show both answers embedded in A1 but it is not clear if Jack and Sarah should collaborate through David or Colleen. On the other hand, CO(A1) using the solid lines is (0.5×20)+(0.5×3)=11.5 and using the dotted lines is (0.5×2)+(0.5×12)=7. For A2, it is (0.5×4)+(0.5×6)=5. Therefore, A2 would be ranked higher than the two individual answers in A1.

Fig. 4. - 
Two answers with different edge distance, node cost and combined objectives.
Fig. 4.
Two answers with different edge distance, node cost and combined objectives.

Show All

2.2 Answer Presentation
Recall that an answer to a graph keyword query is a subgraph composed of a set of content nodes. To reveal the relationships between content nodes, we include the weighted shortest path between every pair of content nodes in the answer. If content nodes are not directly connected, these paths include middle nodes. Following previous work [3], [5], [10], we use weighted shortest paths between content nodes to capture relationship strength. For example, in a PPI network, where edge weights represent the certainty/uncertainty of interactions and node weights represent the level of expression, we prefer a weighted shortest path indicating certain interactions among proteins with a high level of expression. For example, Fig. 5 (left) shows two paths that connect P1 and P2. The top path has three edges, while the bottom path has two edges. However, the top path has smaller edge weights (less uncertainty) than the bottom path. If the goal is to observe a series of interactions from P1 to P2, the top path has a higher chance even though it has more proteins on it. In Fig. 5 (right), the same scenario happens with node weights. The goal is to observe interaction between P3 and P4. The top answers is a better candidate since the connection nodes (i.e., x and y) are both well expressed (i.e., smaller node weights), while the bottom path is composed of a connection node (i.e., z) with low expression level and may not eventually trigger the interaction between P3 and P4.

Fig. 5. - 
Different scenarios in a PPI network in which a path with smaller weights (but longer) is preferred over a path with higher weights (but shorter).
Fig. 5.
Different scenarios in a PPI network in which a path with smaller weights (but longer) is preferred over a path with higher weights (but shorter).

Show All

2.3 Dealing With Probabilities in Noisy Graphs
We conclude this section by explaining how to modify our approach to deal with noisy graphs in which node and edge weights correspond to probabilities of existence. In these cases, it may be more appropriate to maximize the product of node and edge probabilities rather than minimizing their sums [16]. The required modification is simple: we observe that maximizing a product of probabilities is equivalent to minimizing the sum of their negative logarithms and we transform the weights accordingly.

SECTION 3Approximation Algorithms
In this section, we first present an algorithm to optimize the edge distance objective over an input graph G with an approximation ratio of two. This optimization objective has been studied before, but our algorithm is the first to guarantee an approximation ratio for the unique node enumeration paradigm.8 In this approach, one potential answer is constructed around each node in the input graph. This approach is suitable for large graphs as the maximum number of answers is bounded by the number of nodes in the graph (versus Steiner trees with a potentially exponential number of answers) [3]. Then, we build two graphs, G′ and G′′, from G by distributing node cost onto the edges. We show that our original algorithm optimizes the node cost and combined objectives, respectively, over G′ and G′′ and preserves the approximation ratio of two. These two objectives are new.

3.1 Minimizing the Edge Distance Objective
Algorithm 1 solves Problem 1 of minimizing the edge distance objective. The algorithm takes as input a graph G with N nodes and a list of query keywords Q={k1,k2,…,kt}. For each keyword ki, the set of nodes in G having ki is also provided as C(ki). This set can be obtained from an inverted index. The algorithm returns an answer A whose edge distance objective ED(A) is at most twice that of an optimal answer. Lines 1 and 2 initialize the best answer so far (to the empty set) and its edge distance (to infinity). In lines 3-16, we consider each node ni∈G as a connection node that connects all content nodes and attempt to build an answer around it. In lines 7-12, for each keyword ki, we find the closest content node to the connection node. In line 8, we find the value of the shortest path from the connection node to all the content nodes that include ki. The content node itself is found in line 9. If the algorithm finds such a node (line 10), the current distance and answer are updated in lines 11-12. If the current answer covers all the query keywords, and its distance is smaller than the current best distance (lines 13-14), the best approximate answer and its distance are updated in lines 15-16. The best approximate answer is returned in line 17.

Theorem 3.
Assuming that a shortest path can be computed in constant time (which can be done using the 2-hop cover index described later in this paper), the time complexity of Algorithm 1 is O(N.t.|Cmax|), where t is the number of query keywords, N is the number of nodes in the graph, and |Cmax| is the maximum size of the content node sets C(ki) for 1≤i≤t.

Algorithm 1. Finding Best Approximate Answer Minimizing ED
Input: graph G with N nodes; query keywords {k1,k2,…,kt}; the set of content nodes for each keyword ki, C(ki), for 1≤i≤t

Output: best approximate answer

bestA←∅

leastD←+∞

for i←1 to |N| do

connection←ni

dist←0

answer←∅

for j←1 to t do

minValuej←edGmin(connection,C(kj))

contentj←noGmin(connection,C(kj))

if contentj≠∅ then

dist←dist+minValuej

answer. add(⟨kj, contentj⟩)

if size(answer) = t then

if dist<leastD then

leastD←dist

bestA←answer

return bestA

Theorem 4.
Algorithm 1 finds an answer A in G that minimizes the edge distance objective from Definition 4 with an approximation ratio of two.

Proof.
Consider two answers, an optimal answer and an answer produced by Algorithm 1, denoted here as best answer. Let t be the number of keywords and r be the connection node of the best answer. Suppose the minimum edge distance (i.e., weighted shortest distance) from r to each of the content nodes in the best answer is d1,d2,…,dt. Thus, the sum of weighted shortest distances from r to all of the content nodes in the answer is ∑ti=1di. Call this distance connectionDist. Based on Algorithm 1, the connection node r has the smallest connectionDist among all the nodes in graph G, including the content nodes of the optimal answer, because Algorithm 1 minimizes this sum of distances (i.e., connectionDist) (recall that all nodes, including the content nodes of the optimal answer, could be connection nodes).

Now, consider the node with the keyword kj in an optimal answer. Suppose the weighted shortest distance from kj to other content nodes in an optimal answer is o1j,o2j,…, o(j−1)j,oj(j+1),…,ojt, respectively. Based on Algorithm 1, the following holds for each j (where j=1,…,t)
∑i=1j−1oij+∑i=j+1toji≥∑i=1tdi.(1)
View SourceRight-click on figure for MathML and additional features.

If we write the above equation for all t nodes of an optimal answer and sum up both sides of the inequalities, we get: 2×∑ti=1∑tj=i+1oij≥t×∑ti=1di. The left side of this equation is twice the edge distance of an optimal answer. Thus, we have
2×(optimal distance)≥t×∑i=1tdi.(2)
View Source

Now, suppose the shortest distance between content nodes of the best answer (produced by our algorithm) covering keywords ki and kj is bij. The edge distance of the best answer is: best distance=∑ti=1∑tj=i+1bij. Since the shortest distances satisfy the triangle inequality (i.e., bij≤di+dj,i≠j), we have: ∑ti=1∑tj=i+1bij≤∑ti=1∑tj=i+1(di+dj). In the right side of the above equation, each distance di appears exactly t−1 times. Thus, we have: ∑ti=1∑tj=i+1(di+dj)=(t−1)×∑ti=1di. As a result, we have
best distance≤(t−1)×∑i=1tdi.(3)
View SourceRight-click on figure for MathML and additional features.

Based on Equations (2) and (3), we have
2×(t−1)t×(optimal distance)≥best distance.
View Source

This completes the proof that the edge distance of the best answer is at most twice that of an optimal answer.

3.2 Minimizing the Node Cost Objective
We now show that Algorithm 1 can solve Problem 2 with the same approximation ratio using a transformation of the input graph G into a graph G′ with node costs transferred onto the edges. The new graph G′ has the same sets of nodes and edges as the original graph G. If there exists an edge between nv and nw in G (i.e., w(nv,nw)≠∞), the edge distance between two nodes nv and nw in G′ is defined as follows:
di′(nv,nw)=cost(nv)+cost(nw)2.(4)
View Source

Let the weighted shortest distance between any pair of nodes ns and nd in G′ be edG′min(ns,nd). This can be obtained by running Dijkstra's algorithm on G′. Finding the shortest path between nodes ns and nd in G′ is equivalent to finding the minimum node cost path (i.e., ncGmin(ns,nd)) between ns and nd in G.

Lemma 1.
For any pair of nodes ns and nd, the following holds:
edG′min(ns,nd)+cost(ns)+cost(nd)2=ncGmin(ns,nd).
View SourceRight-click on figure for MathML and additional features.

Proof.
First, for any path (not necessarily shortest path) P={ns,n1,…,nq,nd} with length q+1 between two connected nodes ns and nd in G′, the following holds: edG′P(ns,nd)=di′(ns,n1)+∑q−1i=1di′(ni,ni+1)+di′(nq,nd).

Then, using Equation (4), we have: edG′P(ns,nd)=cost(ns)+cost(nd)2+∑qi=1cost(ni). We can rewrite this equation as: edG′P(ns,nd)+cost(ns)+cost(nd)2=ncGP(ns,nd). If we take the minimum of each side of the above equation, the right side of it is similar to Definition 3
min[edG′P(ns,nd)+cost(ns)+cost(nd)2]=min[ncGP(ns,nd)]=ncGmin(ns,nd).
View SourceRight-click on figure for MathML and additional features.

For any two fixed nodes ns and nd, cost(ns) and cost(nd) are constants. Thus, we only need to minimize edG′P(ns,nd) which is equivalent to finding the shortest path between nodes ns and nd in G′, and the proof is complete.

In light of Lemma 1, we define a new distance function d′ between any pair of nodes ns and nd in G′ as follows:
d′(ns,nd)=cost(ns)+cost(nd)2+edG′min(ns,nd).(5)
View Source

Adding the cost of the source and destination nodes (i.e., ns and nd) to edG′min guarantees the correct value of ncGmin for any pair of nodes in G. Below we show that the node cost of an answer A in G is the same as the edge distance of an answer A in the transformed graph G′ and using the new distance function d′.

Lemma 2.
For any answer A from graph G to the query Q={k1,k2,…,kt}, the following holds: NCG(A)=EDG′(A), where G′ is transformed from G by distributing the node cost in G onto the edges of G′ (Equation (4)) and the distance between any pair of nodes in G′ is calculated using Equation (5).

Proof.
Let answer A={n1,n2,…,nt}. Based on Definition 4, EDG′(A)=∑ti=1∑tj=i+1d′(ni,nj). According to Equations (4) and (5) and Lemma 1, we have
EDG′(A)=   ∑i=1t∑j=i+1t[cost(ni)+cost(nj)2+edG′min(ni,nj)]=∑i=1t∑j=i+1tncGmin(ni,nj) = NCG(A),
View SourceRight-click on figure for MathML and additional features.and this completes the proof.

Based on the above lemma, finding an answer over graph G that minimizes the node cost objective function (Definition 5) is equivalent to finding an answer over graph G′ that minimizes the edge distance objective function (Definition 4) using the distance function d′. As we discussed before, finding an answer that minimizes the edge distance objective is NP-hard [11]. In the previous section, we proposed an approximation algorithm (Algorithm 1) that finds an answer with the approximation ratio of two if the pairwise distance function satisfies the triangle inequality. Since the node cost values are positive, the edge distances in G′ are also positive. Furthermore, since the function ewG′min is a shortest path function in G′, it satisfies the triangle inequality. The distance function d′ is linearly related to ewG′min (note that cost(ns) and cost(nd) are constants for given nodes ns and nd). Thus, d′ in G′ also satisfies the triangle inequality. As a result, we can use Algorithm 1 with an approximation ratio of two to find an answer A in G′ that minimizes the edge distance function, which also optimizes the node cost objective in the original input graph G.

Theorem 5.
An answer A can be found in G that minimizes the node cost objective with 2-approximation.

Proof.
Based on Lemma 2, minimizing the node cost objective over G is equivalent to minimizing the sum of the edge distances over G′ using the distance function d′. Since d′ satisfies the triangle inequality, we can use Algorithm 1 to obtain the same approximation ratio of two.

3.3 Minimizing the Combined Objective
Algorithm 1 can also solve Problem 3 with the same approximation ratio of two. As before, the idea is to transform G. Call the transformed graph G′′. It has the same sets of nodes and edges as the original graph G. If there exists an edge between nv and nw in G (i.e., w(nv,nw)≠∞), the edge weight between two nodes nv and nw in G′′ is defined as follows:
di′′(nv,nw)= λ.cost(nv)+cost(nw)2+(1−λ).di(nv,nw).(6)
View Source

Recall that λ is the tradeoff parameter between the edge distance and the node cost objectives. Let the shortest distance between any pair of nodes ns and nd in G′′ be edG′′min(ns,nd). This can be obtained by running Dijkstra's algorithm on G′′. Below, we show that finding the shortest path between nodes ns and nd in G′′ is equivalent to find the minimum combined path coGmin(ns,nd) between ns and nd in G.

Lemma 3.
For any pair of nodes ns and nd, the following holds:
edG′′min(ns,nd)+λ.cost(ns)+cost(nd)2=coGmin(ns,nd).
View SourceRight-click on figure for MathML and additional features.

Proof.
First, for any path (not necessarily shortest path) P={ns,n1,…,nq,ns} with length q+1 between two connected nodes ns and nd in G′′, the following holds: ewG′′P(ns,nd)=w′′(ns,n1)+∑q−1i=1w′′(ni,ni+1)+w′′(nq,nd). Then, using Equation (6), we have:
edG′′P(ns,nd)= λ.cost(ns)+cost(nd)2+λ.∑i=1qcost(ni) +(1−λ).d(ns,n1)+(1−λ).d(nq,nd) +(1−λ).∑i=1q−1d(ni,ni+1).
View Source

We can rewrite this equation as
edG′′P(ns,nd)+λ.cost(ns)+cost(nd)2= λ.ncGP(ns,nd)+(1−λ).edGP(ns,nd).
View Source

If we take the minimum of each side of the above equation, the right side of it is similar to the right side of Definition 6 (similar to Lemma 1). For any two fixed nodes ns and nd, cost(ns) and cost(nd) are constants. Thus, we need to minimize edG′′P(ns,nd) which is equivalent to finding the shortest path between ns and nd in G′′ and the proof is complete.

Using Lemma 3, we define a new distance function d′′ between any pair of nodes ns and nd in G′′ as follows:
d′′(ns,nd)=λ.cost(ns)+cost(nd)2+edG′′min(ns,nd).(7)
View SourceRight-click on figure for MathML and additional features.

Adding the (λ-weighted average of the) cost of the source and destination nodes (i.e., ns and nd) to edG′′min guarantees the correct value of coGmin for any pair of nodes in G. Below we show that the combined cost of an answer A over G is the same as the edge distance of an answer A over the transformed graph G′′ and using the new distance function d′′.

Lemma 4.
For any answer A over G to the query Q={k1,k2, …,kt}, the following holds: COG(A)=EDG′′(A). In this equation, G′′ is transformed from G by distributing the node cost in G onto the edges of G′′ (Equation (6)) and the distance between any pair of nodes in G′′ is calculated using Equation (7).

Proof.
Let answer A={n1,n2,…,nt}. Based on Definition 4, EDG′′(A)=∑ti=1∑tj=i+1d′′(ni,nj). According to Equations (6) and (7) and Lemma 3, we have
EDG′′(A)=  ∑i=1t∑j=i+1t[λ.cost(ni)+cost(nj)2+edG′′min(ni,nj)]= ∑i=1t∑j=i+1tcoGmin(ni,nj) = COG(A),
View Sourceand this completes the proof.

Based on the above lemma, finding an answer over G that minimizes the combined objective function (Definition 7) is equivalent to finding an answer over G′′ that minimizes the edge distance objective function (Definition 4) using the distance function d′′. As before, we use Algorithm 1 to find an approximate answer in G′′ using d′′. We can do this because edge distances and the node cost values are positive and therefore d′′ is a metric and satisfies the triangle inequality.

Theorem 6.
An answer A in G can be found that minimizes the combined objective with 2-approximation.

Proof.
Based on Lemma 4, minimizing the combined objective over G is equivalent to minimizing the sum of the edge distances over G′′ using the distance function d′′. Since d′′ satisfies the triangle inequality, we can use Algorithm 1 to obtain the same approximation ratio of two.

3.4 Finding Top-k Answers
Users are often interested in a sorted list of top-k answers where k could be 10, 20 or 100. Algorithm 1 can be extended to efficiently produce top-k answers with unique sets of content nodes, as shown in Algorithm 2. Before the main for-loop, we initialize a list of size k for the output; the list will be updated after each iteration of the loop, which ensures that the answer currently being constructed covers all the query keywords. If the distance of the current answer is lower than the largest distance currently in the output list, the current answer is added to the list. Lines 13-16 ensure that we do not produce two answers with different connection nodes but the same content nodes.9 For each answer, we create an ID of its content nodes. This can be done by converting the list of content nodes to a set and sorting the set based on the node IDs. The concatenation of the sorted unique node IDs is the ID of the content nodes and the associated answer. Then, before inserting a new answer to the list of top-k answers, we check whether this ID (which is equivalent to a set of content nodes) already exists in the list. If not, the new answer is inserted in the list. Otherwise, the new answer is inserted only if its distance is smaller than that of the existing answer with the same ID.

Algorithm 2. Extensions to Find Top-k Answers
Input: graph G with N nodes; query keywords {k1,k2,…,kt}; the set of content nodes for each keyword ki, C(ki), for 1≤i≤t; the value of k

Output: the list of top-k answers

initialize list with k elements

for i←1 to |N| do

connection←ni

answer←∅

dist←0

for j←1 to t do

minValuej←edGmin(connection,C(kj))

contentj←noGmin(connection,C(kj))

if contentj≠∅ then

dist←dist+minValuej

answer. add(⟨kj, contentj⟩)

if size(answer) = t then

set the answer ID based on its content nodes

if dist<max{ distance of answers in list} then

if list contains answer.ID then

cDist← distance of answer in list with same ID

if dist<cDist then

replace answer, its distance and ID in list

else

remove the answer with largest distance from list

insert answer, its distance and ID in list

return list

Compared to Algorithm 1, we need an extra pass through the list of content nodes of the answer (i.e., t.log(t) operations to create an answer ID by sorting the list of content nodes) and another pass through the top-k answer list (i.e., k operations). The time complexity changes from O(N.t.|Cmax|) to O(N.t.|Cmax|+N.t.log(t)+N.k) (recall that |Cmax| is the maximum size of any content node list C(ki) for 1≤i≤t and t is the number of query keywords). However, since t.|Cmax|≫t.log(t) and t.|Cmax|≫k in practice, the runtime complexity stays the same.

Theorem 7.
The time complexity of Algorithm 2 is O(N.t.|Cmax|) (the same as the complexity of Algorithm 1), where t is the number of query keywords, N is the number of nodes in the graph, and |Cmax| is the maximum size of the content node sets C(ki) for 1≤i≤t.

3.5 Speeding Up the Algorithm
Even though the runtimes of Algorithms 1 and 2 are linear in terms of the number of nodes in the input graph, they consider every node in the input graph as a connection node. We now propose a modification that limits the number of potential connection nodes while keeping the same approximation ratio: only content nodes can be connection nodes. The intuition is that most of the time, connection nodes are also content nodes (i.e., keyword holders), and we verify this in the experiments.

At the beginning of the modified algorithms, we create a of set of potential connection nodes by putting all content nodes of all keywords in one set: ⋃i=1tC(ki). Recall that a set of content nodes can be obtained from an inverted index. The main for loop of both algorithms iterates through this new set rather than all nodes in G. The number of connection nodes is O(t.|Cmax|), where t is the number of query keywords and |Cmax| is the maximum size of the content node sets C(ki) for 1≤i≤t. In practice, this number is orders of magnitude smaller than total number of nodes in G (N≫t.|Cmax|). This is because only a small portion of the nodes are likely to contain the input keywords. Below, we show that the new set of connection nodes does not change the approximation ratio of Algorithm 1.

Theorem 8.
When the potential connection nodes in Algorithm 1 are restricted to the set ⋃i=1tC(ki), Algorithm 1 finds an answer A in G that minimizes the edge weight as in Definition 4 with an approximation ratio of two.

Proof.
The proof is similar to the proof of Theorem 4. In Theorem 4, and using Equation (1), we form a relation between the nodes in the best approximate answer and the optimal answer. A similar relation can be formed between the best answer returned by the modified algorithm and the optimal answer. The reason is that the edge weight of an answer (Definition 4) only depends on the sum of the distances between every pair of content nodes. As long as the set of connection nodes contains all content nodes (from all keywords), Equation (1) still holds. From Equation (1), the rest of the proof is similar to the proof of Theorem 4.

3.6 Speeding Up Distance Queries
A distance query returns the shortest distance between two nodes in the input graph G. We frequently need to find the distance between a connection node and the content nodes. However, computing distance on-the-fly is too slow to be interactive, while pre-computing the distances between all pairs of nodes takes too much space (O(N2) for a graph with N nodes; thus, for big graphs, it quickly runs out of memory). We use distance labeling, or 2-hop cover [17], a practical middle ground between the two extremes mentioned above (see [1], [17] for details of the index).

To build the index, we use a modification of the algorithm in [1] to improve efficiency. We are only interested in answers in which the content nodes are reasonably close to each other. Therefore, we only keep the distances up to a threshold Dmax. In other words, during index creation, if d(ns,nt)>Dmax, we assume ns and nt are disconnected and we do not compute any further shortest paths. Dmax determines the maximum distance between any pair of nodes and it depends on the application. As we will show, the modified 2-hop cover index controlled by the Dmax parameter scales well for large graphs and does not impact the quality of answers.

SECTION 4Experiments
In this section, we use Algorithm 2 over the original graph G or the transformed graphs to test three ranking strategies: 1) optimizing edge distances (ED), 2) optimizing node costs (NC), and 3) combined ranking (CO) with λ=0.5 (unless otherwise specified). We also compare the results of our algorithms with community [10], BLINKS [3], and r-clique [11]. All of these methods rank answers based on edge weights. BLINKS returns trees as answers while community and r-clique return subgraphs that might not be trees. We also compare our results with BANKS [5], which also has a tradeoff parameter (i.e., λ) for controlling the contributions of edge weights versus node weights. As in our CO algorithm, we set λ=0.5. Note that BANKS does not consider middle node weights (details in Section 5). Our method of distributing node weights onto edges is general and can be used by other algorithms that only consider edge weights. To show the results of other algorithms using our new edge weights, we run BLINKS with the new edge weights (community and r-clique show similar trends and their results are omitted for brevity). The results of BLINKS using node weights and combined weights are called BLINKS-NC and BLINKS-CO, respectively. To be consistent and fair, we use the same 2-hop cover index with all algorithms. We implemented our algorithms in Java10 and executed them on an Intel Core i7 2.8 GHz computer with 12 GB of RAM. We demonstrate the following:

Effectiveness of our approaches (NC and CO) using real datasets. This includes precision and quality of answers, comparison with exhaustive search, and utility of the proposed algorithms in real world scenarios.

Sensitivity and Scalability of our methods: sensitivity of the tradeoff parameter λ in CO, performance of the 2-hop cover index for finding shortest paths, effect of our proposed refinement of 2-hop cover on the precision of answers, scalability (and runtime) of our algorithms, and the effect of using only content nodes as potential connection nodes.

4.1 Datasets and Settings
We create graphs from DBLP,11 IMDb,12 IntAct PPI,13 and TPC-E.14 As in [13], [14], we use top-tier database, data mining, artificial intelligence, and theory publications to create the DBLP graph.15 This produces a network of high quality researchers in these areas. The DBLP graph contains 200 K nodes and 615 K edges, with maximum node degree of 428 and average node degree of 6.2. Our IMDb16 graph is the same as in other keyword search papers such as [18]. It has 1M nodes and 3M edges, with maximum node degree of 752 and average node degree of 6.1. The PPI network has 70k nodes and 425k edges, with maximum node degree of 4,019 and average node degree of 12.1.

For DBLP, we use ArnetMiner [19] to compute h-indices and use their inverse values as node costs. For potential content nodes, we take junior researchers with fewer than ten papers—in practice, junior team members typically carry out the main tasks and senior researchers act as mentors. We label content nodes with keywords that occur in at least two of their paper titles. This gives us the areas of expertise. For IMDb, node costs are equal to the inverse importance of the table from which the node (i.e., tuple) came and are computed using the method from [20]. For the PPI network, the expression level of each protein (node cost) is calculated based on the length and structure of the protein which is obtained from UniProt.17 The associations of genes and diseases are obtained from DisGeNET.18 The method in [4] calculates the importance of tables in a relational database. We use the inverse importance values of the associated tables as the node costs in the TPC-E dataset. We assume that all edges have the same weight.

For DBLP and IMDb, we test three ways of assigning edge distances: 1) equal weights, 2) logarithmic weights, in which the weight of an edge between adjacent nodes ni and nj is (log2(1+degni)+log2(1+degnj))/2, where degn is the degree of node n, and 3) application-dependent semantic weights. Semantic weights correspond to the inverse of the number of co-authored publications in DBLP and the inverse of foreign key importance in IMDb computed as in [20]. Note that BLINKS-CO uses semantic weights. Edge distances and node cost values are normalized to lie between 0 and 1.

4.2 Queries
For the first and second experiments (Exp-1 and Exp-2), we use fixed queries. For other experiments, we use randomly generated queries. For DBLP, we use the following queries (projects) to form teams of experts: P1 = [“Embedded” “Stream”], P2 = [“Optimization” “Transaction” “Concurrency”], P3 = [“Provenance” “Temporal” “Spatial”] and P4 = [“Filtering” “Probabilistic” “Integration” “Heterogeneous”]. We use the following queries over movie and actor names in the IMDb dataset: Q1 = [“Morgan Freeman” “Tim Robbins”], Q2 = [“Morgan Freeman” “Tim Robbins” “Keanu Reeves”], Q3 = [“The Matrix” “The Shawshank Redemption”] and Q4 = [“The Matrix” “The Shawshank Redemption” “Morgan Freeman” “Keanu Reeves”].

The IMDb answers could be ranked using content-based metrics (i.e., IR metrics) that compare the query keywords to the keywords found in the content nodes of the answer [9]. However, since our queries use exact actor and movie names, as found in the content nodes, the content-based metrics would be the same for every answer that contains all the query keywords. Similarly, using content-based metrics in our DBLP example is not meaningful because our queries use precise keywords found in paper titles.

TPC provides a set of transactions along with the TPC-E database. These transactions model the usage of the database and thus they are equivalent to a query log for the TPC-E benchmark19 [21]. These transactions provide independent quality measures for each table and join in the database. We parsed the transaction pseudo-code and recorded the number of times a table or a join (i.e., foreign key connection) appears. Table a is more important than table b if it appears more in the transactions. The same applies for joins: a join that appears in more transactions is more important. The intuition is that more informative and meaningful tables and joins appear in more queries [21]. We normalize the values of all tables. For any given answer subgraph from the TPC-E dataset, we use the average value of these normalized importance values as an indicator of the quality and interestingness of the answer.

4.3 Effectiveness
Exp-1: Precision. A common metric used in information retrieval is top-k precision, which is the percentage of answers in the top-k answers that are relevant to the query. To compare the precision of different algorithms, we conducted a user study using the DBLP and IMDb queries listed in Section 4.1. We follow the same methodology as EASE [9]. For DBLP, we asked 12 Computer Science graduate students to judge the relevance of top-5 answers. For IMDb, we asked 15 users interested in movies, each with different educational background, to judge the relevance of top-5 answers. Each user received the output of each algorithm at the same time and was asked to assign a relevance score to each answer. The scores range from zero (completely irrelevant to the query) to 100 (completely relevant). For each query and each algorithm, we report the top-5 precision, which is the average relevance score of the top-5 answers across all users.

We compare the following methods: community, BLINKS, r-clique, BANKS (λ=0.5), ED with equal weights, ED with logarithmic weights, ED with semantic weights, NC, CO with equal weights, CO with logarithmic weights, and CO with semantic weights. We use equal weights for community, BLINKS and r-clique. For brevity, we omit logarithmic and semantic weights for these algorithms as these produce similar results to ED with logarithmic and semantic weights. We also test two versions of BLINKS: BLINKS-NC (just node weights) and BLINKS-CO (using our method to distribute node weights onto the edges).

Fig. 6 shows the top-5 precision of each tested method on each query. NC and CO, the two new methods proposed in this paper, outperform community, BLINKS, r-clique, and EW on both datasets and all queries. Furthermore, NC outperforms CO for most queries. As for the different ways of assigning edge weights, semantic weights give better precision than equal weights. In DBLP, logarithmic weights are worse than equal weights. This is because edges between nodes with small degrees have small logarithmic weights, and therefore, small distances; however, this means that authors with fewer co-authors are closer together and are more likely to be included in expert teams. We also experimented with different values of λ for CO between 0.25 and 0.75. We found that larger values of λ led to slightly higher precision (because they put more emphasis on node weights, and, as mentioned above, NC generally had higher precision). We note that BANKS produces results with higher quality than ED with equal weights, community, BLINKS and r-clique. This is because BANKS takes into account the cost of content nodes and the root node. However, since it does not take into account the cost of middle nodes, its precision is usually lower than NC and CO. Also, the precision of BLINKS increases significantly using our methods to distribute node costs onto the edges (with both CO and NC), verifying that our methods are also effective with other algorithms. Since BLINKS uses a different function to rank answers, its precision is slightly lower than our CO and NC.


Fig. 6.
Precision of top-5 answers.

Show All

Exp-2: Quality of Expert Teams. Next, we check if the top-10 teams of researchers returned by each algorithm were successful in real life. For this, we used the Microsoft Academic conference ranking to rate the publication venues in which the teams published joint papers. We used the DBLP dataset until 2015 to identify teams, and we verify papers they published within those teams in 2016 and 2017. For CO, we set λ to 0.5. We use the same projects described in Section 4.1. From the teams that actually co-authored papers in 2016 and 2017, we found that 76 percent of the time the teams found by NC published in more highly-rated venues than those found by ED. This number is 69 percent for CO versus ED.

Exp-3: Experiment on the TPC-E Benchmark. We compare the results of different algorithms over the TPC-E dataset. We randomly generate 100 queries with two, three and four keywords from different entities (e.g., customer, company, and broker). Fig. 7 shows the average node and edge scores that are obtained from the TPC-E transaction logs (see Section 4.1 for details). A higher score means that on average, the nodes (or edges) appear more in the transactions. NC obtained the highest score. Note that we calculate the node and edge scores from an independent resource (i.e., TPC-E transactions) and the results indicate that incorporating node costs produces more relevant results.

Fig. 7. - 
Average node and edge scores for each method obtained from the TPC-E transaction logs.
Fig. 7.
Average node and edge scores for each method obtained from the TPC-E transaction logs.

Show All

Exp-4: Experiment on PPI Network. We compare average edge distances and node costs of the ED, NC, and CO algorithms over the PPI network. We randomly generate 100 queries with three and four genes responsible for different diseases. Fig. 8 shows the average edge distances and node costs of all nodes in each answer and for each algorithm. Recall that in a PPI, edge distance represents the uncertainty of the interaction and node cost is associated with the expression of the protein. As expected, ED has the lowest edge distance and NC has the lowest node cost. CO lies in between. The average edge distance of ED is 34 percent lower than NC, but its average node cost is 76 percent higher than NC's node cost. Although the output of NC has a higher interaction uncertainty, it has a much higher expression value. This result indicates that we do not lose much interaction certainty and gain significant expression using NC and CO.

Fig. 8. - 
Average edge distances and node costs over PPI network.
Fig. 8.
Average edge distances and node costs over PPI network.

Show All

Exp-5: Comparison With Exhaustive Search. In this experiment, we compare the ED, NC and CO scores of our approximation algorithms with optimal solutions obtained by exhaustive search (in practice, exhaustive search is too expensive as it takes more than one hour to run). For each number of keywords, we randomly generate 100 queries. Note that exhaustive search did not terminate after 5 hours for most queries with five keywords. Therefore, we only report results for queries with two, three, and four keywords. Results for IMDb using equal weights are shown in Fig. 9; results for DBLP and PPI show similar trends and are omitted for brevity. Our approximate results are only about 25 percent worse than optimal, which outperforms the worst case theoretical approximation ratio of two.


Fig. 9.
Comparison with exhaustive search (top-5 answers).

Show All

Exp-6: Comparison With Search by SPARQL. We now show that graph search can save time even for expert users well-versed in SPARQL. For ten experts, we measure the time to perform search manually and automatically using a Web interface to our algorithm (see [22] for screen shots of our system). To simulate a real world environment for manual search, experts were allowed to access the tools that they routinely use when they query graph databases. When we measured the time for automatic search with our system, we included the time for specifying parameters as well as preforming the actual search. When we measured the time for manual search, we included the time for writing the SPARQL query and executing it. We observed that our tools reduce time by a factor of ten (15 seconds versus 160 seconds). As expected, manual search is prone to human errors that introduce delays, such as syntax mistakes or misunderstanding the schema/structure. This experiment shows the benefits of our framework compared to manual search.

4.4 Sensitivity and Scalability
Exp-7: Sensitivity of λ. The impact of λ (the tradeoff parameter between node cost and edge distance) using the DBLP dataset and equal edge weights is shown in Fig. 10. It shows the sensitivity of the average h-index of experts. We examine the effect of λ on the top-10 teams produced by CO using 100 randomly generated projects with three and four skills. The results suggest that answers change slowly as λ increases: changing the value of λ by less than 0.1 does not significantly affect the answers and the quality of the teams stays the same.


Fig. 10.
Sensitivity of the tradeoff parameter λ on DBLP. The red and blue lines correspond to 3 and 4 skills respectively.

Show All

Exp-8: 2-Hop Cover Index. Fig. 11 shows the performance of the 2-hop cover index on the IMDb graph using ED, CO and NC. We omit results on the smaller DBLP and PPI graphs. Query time denotes the average time for 1,000,000 random distance queries. The Dmax value for NC is set to 0.1 because the distribution of edge weights in NI is different than the other two methods. We observe that distance queries are answered almost instantly. Furthermore, while changing Dmax does not have much of an effect in ED and CO, index size quickly increases with Dmax for NC. We also show the average precision of our four queries for each value of Dmax in Fig. 11. For the IMDb dataset, we can save space without losing precision by setting Dmax to seven times the minimum edge weight. Higher values of Dmax increases the index size without affecting the precision of answers. In future work, we plan to test other datasets to verify if this is a robust guideline. Note that answering distance queries on the fly (without an index) using Dijkstra's algorithm takes on average 800 ms for each pair of nodes in the IMDb graph.


Fig. 11.
Performance of the 2-hop cover index on the IMDb graph using different values of Dmax.

Show All

Exp-9: Scalability and Runtime. Figs. 12 and 13 show the scalability of our algorithm for finding top-50 answers. For the DBLP dataset, we randomly generate 100 projects with different numbers of required skills. For the IMDb dataset, we randomly generate 100 queries composed of different numbers of keywords. ED, NC and CO have similar runtime since they use the same basic algorithm and only differ in the graph transformations. In Fig. 12, we also present the runtime of community [10], BLINKS [3], and r-clique [11]. To be fair, all of these algorithms also use the fast 2-hop cover index. Our algorithm and BLINKS are slightly faster since they both use the unique node enumeration paradigm (it is called unique root semantics in BLINKS since answers are trees), while community and r-clique use the so-called polynomial delay approach, in which answers are computed one-by-one, each in polynomial time. Note that the main objective of this work is to find meaningful answers over weighted graphs rather than finding answers faster. Using the updated 2-hop cover index, we also find answers efficiently.


Fig. 12.
Scalability of our proposed algorithm (top-50 answers) when the number of input keywords varies.

Show All

Fig. 13. - 
Scalability of our proposed algorithm (top-50 answers) for different sizes of the input graph.
Fig. 13.
Scalability of our proposed algorithm (top-50 answers) for different sizes of the input graph.

Show All

Exp-10: Effect of Limiting Potential Connection Nodes. As we discussed in Section 3.5, we can speed up our algorithms by only considering content nodes as potential connection nodes. In this experiment, we compare the output of the original and modified versions of our algorithms. We compare the set of content nodes of top-k answers and report the percentage of answers that overlap (over 100 randomly generated queries). Results are shown in Fig. 14 for 100 randomly generated queries and for top-10, top-50 and top-100 answers. For top-10 answers, the results are identical. For top-100, there is 95 percent overlap between the two sets. These results indicate that most of the top-k answers can be constructed when only content nodes can be connection nodes on the tested data.


Fig. 14.
Percentage of answers that have the same set of content nodes when only content nodes can be connection nodes versus when all nodes are considered as connection nodes.

Show All

SECTION 5Related Work
Keyword Search Over Graphs. Graph keyword search takes a set of keywords (terms) as input and returns a subgraph that contains the input keywords. Solutions are categorized based on the types of answers they produce: tree-based and graph-based. Tree-based methods can be further divided into Steiner trees and distinct root trees. For finding trees with distinct roots, a bidirectional search algorithm was proposed in [15] and improved in BLINKS with a different indexing structure [3]. For graph based methods, [9] finds r-radius Steiner graphs as answers and [10] searches for multi-centered subgraphs called communities. Each community contains some center nodes. The authors proposed an algorithm that produces all communities in arbitrary order and another algorithm that produces top-k communities. In [11], the authors find r-cliques as answers to keyword search over graphs in which all content nodes are guaranteed to be close to each other.

The above methods use structural metrics such as distance between query keywords or answer sizes, or content-based metrics. The node cost and combined ranking objectives (based on the sum of distances) introduced in this work are suitable for graphs with weighted nodes and edges. Note that some content-based (e.g., IR-based) metrics incorporate node scores, but they are based on relevance to query keywords. Thus, the score of content nodes depends entirely on the query keywords, whereas our techniques accommodate node weights that model semantic importance and are query-independent. These methods also ignore the relevance/cost of middle nodes that, by definition, do not contain keywords. Therefore, prior work uses different indexing and exploration algorithms and cannot solve our problem.

One of the early works on graph keyword search (BANKS) uses node weights (called node prestige) in the ranking function [5]. BANKS works based on the strategy of backward searching to form the answers. BANKS chooses not to incorporate the weights of middle nodes in the ranking function and instead only uses the weights of content nodes and the root node. The reason is stated as “to favor meaningful root nodes, and to reduce the effect of intermediate nodes” [5]. However, there are many natural examples where the weight of intermediate nodes affects the quality of results (as pointed out in our motivating examples and in our experiments). Thus, we explicitly take the weights of middle nodes into account. To the best of our knowledge, subsequent work after BANKS also do not use the weights of middle nodes to rank answers.

In [22], we presented a system demonstration of our search engine for node-labeled heterogeneous graphs which included simple greedy algorithms that took node cost into account. In this paper, we propose approximation algorithms with theoretical guarantees for ranking objectives that include node cost.

Keyword Search Over RDF Data. Keyword search is a useful mechanism for exploring Resource Description Framework (RDF) data. The work on keyword search over RDF data falls into three categories. The first category converts RDF data into a graph and directly uses graph keyword search techniques (like ours) on the converted graph [23]. The second approach is a class based summarization technique [24]. First, the hierarchy of RDF class nodes is used to generate a summary graph. Then, graph algorithms can be used on the summary graph to find relevant answers. The third category has recently been introduced by Han et al. [25]. Based on the input keywords, the authors first obtain elementary query graph building blocks (e.g., entity/class nodes and predicate edges). Then, they formulate keyword search over RDF data as a query graph assembly problem. In this work, we focus on general weighted graphs. Our algorithms can be applied to converted (and weighted) RDF graphs.

Group Steiner Tree Problem. While our problem is different from classical Steiner tree problem, it is similar to the classical group Steiner tree problem. In this problem, we receive groups (i.e., subsets) of nodes as input and the goal is to find a tree with minimum weight that contains at least one node from each group. Each group of nodes can be considered as a set of nodes that contain a specific keyword in this work. Similar to the Steiner tree problem, there exist approximation algorithms for the group Steiner tree problem [26]. However, they have a high runtime that makes them impractical for large graphs. Additionally, the objective function of minimizing the weight of the tree does not involve all nodes equally. Recall that our objective function (i.e., the sum of distances) is fair towards all nodes.

Similarity Search in Heterogeneous Information Networks. Heterogeneous information networks (HIN) were introduced by Sun et al. [27] and extended by Wang et al. [28]. A HIN is a directed graph in which nodes and edges have multiple types. Various data mining problems have been studied on HINs including similarity search (such as finding meta-paths, meta-structures, and meta-graphs [27], [29]). For example, over the DBLP dataset, by receiving one author as input, similarity search can find similar types of authors in terms of publication venues or titles. Compared to search in HINs, our problem has a different input and produces a different output.

SECTION 6Conclusions and Future Work
In this paper, we studied the problem of finding relevant answers to keyword search over weighted graphs. We considered ranking objectives that take edge distance and node costs into account. We proved that optimizing these objectives is NP-hard and proposed approximation algorithms with provable approximation ratios. Experiments on real datasets showed that users found the answers produced by our NC and CO methods more relevant than those computed using existing methods.

Another way to jointly optimize the edge and node weight objectives is to find a set of Pareto-optimal answers. In future work, we plan to develop algorithms to find such answers, rank them based on relevant measures of interestingness. While we presented answers as subgraphs, answers can also be represented as trees. This can be done by using the connection node as the root of the tree or by using the algorithm in [30] to form a Steiner tree among the content nodes. In future work, we will explore these different ways of selecting middle nodes and presenting answers.