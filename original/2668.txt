Accuracy is a popular measure for evaluating the performance of classification algorithms tested on ordinary data sets. When a data set is imbalanced, F -measure will be a better choice than accuracy for this purpose. Since F -measure is calculated as the harmonic mean of recall and precision, it is difficult to find the sampling distribution of F -measure for evaluating classification algorithms. Since the values of recall and precision are dependent, their joint distribution is assumed to follow a bivariate normal distribution in this study. When the evaluation method is k -fold cross validation, a linear approximation approach is proposed to derive the sampling distribution of F -measure. This approach is used to design methods for comparing the performance of two classification algorithms tested on single or multiple imbalanced data sets. The methods are tested on ten imbalanced data sets to demonstrate their effectiveness. The weight of recall provided by this linear approximation approach can help us to analyze the characteristics of classification algorithms.
SECTION 1Introduction
It is very popular to evaluate the performance of classification algorithms by k-fold cross validation. When the numbers of instances for various class values in a data set are not largely different, accuracy is generally used for performance evaluation. Several statistical methods, including parametric and nonparametric ones, have been proposed to compare the performance of classification algorithms when they are tested on multiple data sets [1], [2], [3]. A parametric method is applicable for an evaluation measure only when the assumptions about the method are met. The nonparametric methods introduced in [1] can be applied for the evaluation measures other than accuracy, while they are less sensitive and less efficient than their parametric counterparts when the assumptions of the parametric methods are satisfied [4]. In general, an imbalanced data set has two class values, and analysts pay their attention on the minor one that has few instances. For instance, only few persons have a certain disease, and how to determine whether a person has such a disease is critical. Another case is that managers want to know the causes of the defective items produced by a manufacturing system.

Label the minor and major class values in an imbalanced data set as ‘Yes’ and ‘No’, respectively, and let ni be the number of instances with class value i for i = yes, no. The classification results of an imbalanced data sets can be summarized in a confusion matrix as shown in Table 1. FP+TN=nno is far larger than TP+FN=nyes, and accuracy is calculated as (TP+TN)/(TP+FP+FN+TN). When the predicted class value of every instance is ‘No’, we will have TP=FP=0,FN=nyes, and TN=nno. In this case, accuracy =nno/(nno+nyes) is close to one. Almost every classification algorithm will have a high prediction accuracy when it assigns the major class value to all instances. Accuracy is therefore not an appropriate measure to determine whether a classification algorithm is superior to another in an imbalanced data set.

TABLE 1 The Confusion Matrix for an Imbalanced Data Set
Table 1- 
The Confusion Matrix for an Imbalanced Data Set
The performance measures proposed for imbalanced cases include recall, precision, specificity, and false positive rate. Since a high achievement on one of these measures generally indicates a poor achievement on another, some of them are aggregated for performance evaluation. For instance, Fβ is a combination of recall and precision [5], and G-Mean is the geometric mean of recall and specificity [6]. The two axes of a receiver operating characteristic (ROC) curve are recall and false positive rate [7], and the area under the ROC curve is called AUC. Branco et al. [8] and Japkowicz [9] presented and discussed the characteristics of the performance measures used for imbalanced domains. This study focuses on proposing the parametric methods for F-measure that is a special case of Fβ with β=1.

Recall and precision are necessary for computing F-measure, and their calculation are expressed as recall =TP/(TP+FN) and precision =TP/(TP+FP). The enumerators of both recall and precision focus on the number of correct predictions on positive instances, the instances with class value ‘Yes’. Lopez et al. [10] proposed a method to perform stratified k-fold cross validation for reducing variation of performance measure. Guo et al. [11] made a survey on the methods and applications involved in processing imbalanced data in years 2006 through 2016. Wang and Li [12] introduced several ways to calculate confidence intervals for recall and precision. If only one of recall and precision is considered in performance evaluation, then the methods proposed by Wong [3] can be considered for comparing the performance of two classification algorithms on multiple imbalanced data sets.

A classification algorithm can achieve high recall by sacrificing precision, or vice versa [13]. Recall and precision can be combined to calculate F-measure for performance comparison. Wang et al. [14] proposed a way to calculate the confidence interval of F-measure based on the recall and precision obtained from blocked 3×2 cross validation, while this interval cannot be directly used for statistical comparison because it is nonsymmetrical. Since it is difficult to derive the sampling distribution of F-measure, no parametric statistical methods have been proposed to compare this measure for evaluating classification algorithms. This paper will introduce a linear approximation approach for aggregating recall and precision to calculate F-measure such that the sampling distribution of F-measure can be derived. Statistical methods based on the linear approximation of F-measure is then proposed to compare the performance of two classification algorithms on single and multiple imbalanced data sets.

The remainder of this paper is organized as follows. Section 2 introduces the sampling distributions of recall and precision, and presents some properties of bivariate normal distribution for describing the joint probability of recall and precision. The linear approximation approach for aggregating recall and precision to derive the sampling distribution of F-measure is proposed in Section 3. This approximation approach for F-measure is used in Section 4 and Section 5 to design parametric methods for comparing the performance of two classification algorithms on single and multiple data sets, respectively. The experimental results on 10 imbalanced data sets are shown in Section 6 to demonstrate the effectiveness of our methods. Conclusions and directions for future works are addressed in Section 7.

SECTION 2Sampling and Bivariate Normal Distributions
Sampling distribution is essential in determining the statistic for hypothesis test. This section first briefly introduces the sampling distribution of a point estimator in k-fold cross validation. Then the properties of bivariate normal distribution that will be used as the joint distribution of recall and precision will be presented. In the remainder of this paper, recall, precision, and F-measure are statistics calculated from samples, and actual recall, actual precision, and actual F-measure are parameters corresponding to populations.

2.1 Sampling Distributions
Assume without loss of generality that the n instances in a data set D are collected from an infinite population by the simple random sampling. Let xi represent the prediction of instance i in data set D by classification algorithm L, and xi = 1 if the prediction is correct, and xi = 0 otherwise. Then p¯=Σni=1xi/n is an estimate of the actual accuracy p of L on the population corresponding to data set D. Since xi is a Bernoulli test with success probability p, Σni=1xi will have a binomial distribution with parameters n and p. By the normal approximation, Σni=1xi can be assumed to follow a normal distribution with mean np and variance np(1-p) when the large-sample conditions np ≥ 5 and n(1-p) ≥ 5 hold [15]. Since p is unknown, the large-sample conditions are revised as np¯≥5 and n(1−p¯)≥5. The large-sample conditions are therefore said to hold when the numbers of correct and wrong predictions are both larger than or equal to five.

Recall represents the proportion of positive instances that are predicted correctly. The large-sample conditions for recall are TP≥5 and FN≥5; i.e., the numbers of correct and wrong predictions on positive instances are both larger than or equal to five. Precision specifies the proportion of the correct predictions on the instances that are classified as positives. The large sample conditions for precision are TP≥5 and FP≥5; i.e., the numbers of correct and wrong predictions on the instances that are classified as positives are both larger than or equal to five. Let the true recall and precision of algorithm L on the population corresponding to a data set be R and Q, respectively. Then when the classification results satisfy conditions TP≥5,FN≥5, and FP≥5, recall can be assumed to follow a normal distribution with mean R and variance R(1−R)/(TP+FN), and it is also appropriate to assume that precision follows a normal distribution with mean Q and variance Q(1−Q)/(TP+FP).

2.2 Bivariate Normal Distributions
Definition 1.
The joint distribution of random variables X and Y is a bivariate normal distribution with parameters μx, μy, σ2x, σ2y, and ρxy if it has density
f(x,y)=12πσxσy1−ρ2xy−−−−−−√e−12(1−ρxy)2[(x−μxσx)2−2ρxy(x−μxσx)(y−μyσy)+(y−μyσy)2],
View Sourcewhere x and y are possible values of X and Y, respectively. This distribution will be denoted N(μx,μy,σ2x,σ2y,ρxy).

When random variables X and Y follow N(μx,μy,σ2x,σ2y,ρxy), then the marginal distributions of X and Y are N(μx,σ2x) and N(μy,σ2y), respectively. The correlation coefficient ρxy specifies the strength of the linear relationship between these two random variables. The linear combination of these two random variables will also follow a normal distribution [16], as presented in the following property.

Property 1.
Let random vector (X, Y) follow N(μx,μy,σ2x,σ2y,ρxy). Then aX+bY will follow N(aμx+bμy,a2σ2x+b2σ2y+2abρxyσxσy) for any a, b ∈ R and a2 + b2 ≠ 0.

Let the sample collected for estimating ρxy be {(x1,y1),(x2,y2),…,(xk,yk)}. The sample correlation coefficient is calculated as
Σj=1k(xj−x¯)(yj−y¯)Σj=1k(xj−x¯)2Σj=1k(yj−y¯)2−−−−−−−−−−−−−−−−−−−√,(1)
View SourceRight-click on figure for MathML and additional features.where x¯=Σkj=1xj/k and y¯=Σkj=1yj/k.

SECTION 3Aggregation of Recall and Precision
A classification algorithm may have a larger recall but a smaller precision than another on an imbalanced data set. It is therefore desirable to aggregate recall and precision to determine which algorithm has a better performance on the data set. One of the popular ways for this purpose is the F-measure that equals the harmonic mean of recall and precision calculated as
F−measure=11recall+1precision2=2×recall×precisionrecall+precision.
View Source

The harmonic mean of two numbers that are largely different will be close to the smaller one. Aggregating recall and precision to obtain F-measure can avoid favoring an algorithm that sacrifices recall to achieve high precision, or vice versa.

When k-fold cross validation is used for performance evaluation, the number of folds is generally less than 30 for the sake of computational efficiency. Since the number of positive instances in an imbalance data set is few, it is unlikely that the experimental results of every fold can satisfy the large-sample conditions when the number of folds is not small. Several studies have addressed that it is inappropriate to perform k-fold cross validation on a data set more than once to obtained observations, because the values of performance measure calculated from different replications are dependent [3], [17]. The central limit theorem therefore cannot be applied to derive the sampling distribution of F-measure for comparing the performance of two classification algorithms.

When the joint distribution of recall and precision are assumed to follow a bivariate normal distribution, by Property 1, their linear combination will also follow a normal distribution. We will therefore use a linear combination of recall and precision to approximate the F-measure in this study. Let r¯ and q¯ be the recall and precision obtained by applying k-fold cross validation on a data set. Then the F-measure can be calculated as f¯=2r¯q¯/(r¯+q¯). Let w¯ represent the weight of recall such that w¯r¯+(1−w¯)q¯=f¯. Then when r¯≠q¯, this weight can be calculated as
w¯(r¯−q¯)+q¯=f¯⇒w¯=f¯−q¯r¯−q¯.(2)
View Source

For the sake of continuity, weight w¯ is set to be 1/2 when r¯=q¯.

F-measure is also called F1 score that is a special case of Fβ calculated as
Fβ=(1+β2)×recall×precisionrecall+β2×precision(3)
View Source

Coefficient β is used to set the relative importance of recall with respect to precision [8]. In calculating F-measure, recall and precision have the same level of importance. The weight of recall w¯ introduced in this study represents the contribution level of recall on F-measure. It does not mean that the importance levels of recall and precision will be different when w¯ ≠0.5.

SECTION 4Single Data Set
As address by Wong [3], the aggregation level of performance measure can be set at either data set or fold when evaluation method is k-fold cross validation. The aggregation level can be set at fold only when the observations of performance measure obtained from all folds for a classification algorithm follow the same distribution. This section first argues that the aggregation level for F-measure can be set only at data set. Then a statistical method for comparing the F-measure of two algorithms on single data set is presented.

Let the n instances in an imbalanced data set D be randomly and independently divided into k and l disjoint folds for classification algorithms 1 and 2, respectively. Then every fold is in turn used to test the model induced from the others. Let TPij, FNij, and FPij be the true positives, false negatives, and false positives of fold j for classification algorithm i, respectively. Our intention is to determine whether algorithms 1 and 2 have significantly different F-measure on this imbalanced data set. Let Fi be the actual F-measure of algorithm i for i = 1, 2, and hence the null hypothesis for testing is H0:F1−F2=0. The actual F-measure will be represented as a linear combination of actual recall Ri and actual precision Qi; i.e., Fi=wiRi+(1−wi)Qi for some wi∈[0,1].

Let STP1=Σkj=1TP1j, SFN1=Σkj=1FN1j, SFP1=Σkj=1FP1j, STP2=Σlj=1TP2j, SFN2=Σlj=1FN2j, and SFP2=Σlj=1FP2j. For algorithm i, the recall and precision of fold j are calculated as rij=TPij/(TPij+FNij) and qij=TPij/(TPij+FPij), respectively. If the experimental results of every fold satisfy the large-sample conditions, then we can assume that rij∼N(Ri,Ri(1−Ri)/(TPij+FNij)) and qij∼N(Qi,Qi(1−Qi)/(TPij+FPij)). Note that TPij+FNij represents the number of positive instances in fold j for algorithm i. In dividing instances into folds, it is possible to control that every fold have approximately the same number of positive instances. This implies that the rij for algorithm i can be assumed to follow the same normal distribution. However, the denominator for precision is the number of instances that are predicted to be positive. It is inappropriate to assume that an algorithm will classify approximately the same number of instances to be positive in every fold. This means that the qij for algorithm i generally have normal distributions with the same mean but different variances. The observations of F-measure for all folds should not be assumed to follow the same probability distribution, and hence they cannot be a sample for estimating actual F-measure. It is therefore inappropriate to set the aggregation level at fold. Since the aggregation level for imbalanced data sets can be set only at data set, the number of folds for various algorithms need not be the same.

When the aggregation level is set at data set, the recall and precision for algorithm i are r¯i=STPi/(STPi+SFNi) and q¯i=STPi/(STPi+SFPi), respectively. If the experimental results satisfy the large-sample conditions STPi≥5,SFNi≥5, and SFPi≥5, then we can assume that r¯i∼N(Ri,σ2ri) and q¯i∼N(Qi,σ2qi), where σ2ri=Ri(1−Ri)/(STPi+SFNi) and σ2qi=Qi(1−Qi)/(STPi+SFPi). Let ρi represent the correlation coefficient between r¯i and q¯i, and let Fi=2RiQi/(Ri+Qi) for i=1,2. Then there exists a value wi∈[0,1] such that Fi=wiRi+(1−wi)Qi, and wi is therefore called the actual weight of recall for algorithm i. Since r¯i and q¯i are unbiased point estimators of Ri and Qi, respectively, f¯i=wir¯i+(1−wi)q¯i will be an unbiased point estimator of Fi. The following theorem gives the sampling distribution of f¯1−f¯2 for testing hypothesis H0:F1−F2=0 when the aggregation level is set at data set.

4Theorem 1.
When the aggregation level is set at data set, classification algorithms 1 and 2 are independent, and the experimental results satisfy the large-sample conditions, point estimator f¯1−f¯2 can be assumed to follow a normal distribution with mean F1–F2 and variance σ21−σ22, where σ2i=w2iσ2ri+(1−wi)2σ2qi+2wi(1−wi)ρiσriσqi for i = 1, 2.

4Proof.
Since both r¯i and q¯i are normally distributed and dependent, they can be assumed to follow a bivariate normal distribution with parameters Ri,Qi,σ2ri, σ2qi, and ρi. For any wi∈[0,1], by Property 1, f¯i=wir¯i+(1−wi)q¯i will follow a normal distribution with mean wiRi+(1-wi)Qi and variance w2iσ2ri+(1−wi)2σ2qi+2wi(1−wi)ρiσriσqi. When the two classification algorithms are independent, point estimator f¯1−f¯2 can be assumed to follow a normal distribution, and its mean and variance are E(f¯1−f¯2)=E(f¯1)−E(f¯2)=F1−F2 and Var(f¯1−f¯2)=Var(f¯1)+Var(f¯2) =Σ2i=1[w2iσ2ri+(1−wi)2σ2qi+2wi(1−wi)ρiσriσqi], respectively.

Since both Ri and Qi are unknown, σ2ri and σ2qi are estimated as s2ri=r¯i(1−r¯i)/(STPi+SFNi) and s2qi=q¯i(1−q¯i)/(STPi+SFPi), respectively. We still need the estimates of wi and ρi to obtain an estimate of Var(f¯1−f¯2). The value of wi can be solved from equation f¯i=wir¯i+(1−wi)q¯i. We can calculate f¯i=2r¯iq¯i/(r¯i+q¯i) to estimate actual weight wi as w¯i=(f¯i−q¯i)/(r¯i−q¯i). Every fold is a random sample of the population corresponding to the data set, and all folds for a classification algorithm have approximately the same size. Let rij=TPij/(TPij+FNij) and qij=TPij/(TPij+FPij). Then the linear relationship between r¯1 and q¯1 will exhibit in pairs (r11,q11),(r12,q12),…,(r1k,q1k), and similarly for the pairs for r¯2 and q¯2. By formula (1), those pairs can be used to estimate correlation coefficients as
ρ¯1=Σkj=1(r1j−x¯1)(q1j−y¯1)Σkj=1(r1j−x¯1)2−−−−−−−−−−−−√Σkj=1(q1j−y¯1)2−−−−−−−−−−−−√(4)
View SourceRight-click on figure for MathML and additional features.and
ρ¯2=Σlj=1(r2j−x¯2)(q2j−y¯2)Σlj=1(r2j−x¯2)2−−−−−−−−−−−−√Σlj=1(q2j−y¯2)2−−−−−−−−−−−−√,(5)
View SourceRight-click on figure for MathML and additional features.where x¯1=Σkj=1r1j/k, y¯1=Σkj=1q1j/k, x¯2=Σlj=1r2j/l, and y¯2=Σlj=1q2j/l. Since s2ri and s2qi are respective estimates of σ2ri and σ2qi resulting from population proportions Ri and Qi, the statistic for testing hypothesis H0:F1−F2=0 is therefore z value instead of t value. This test statistic is calculated as
z=f¯1−f¯2s21+s22−−−−−−√,(6)
View SourceRight-click on figure for MathML and additional features.where s2i=w¯2is2ri+(1−w¯i)2s2qi+2w¯i(1−w¯i)ρ¯isrisqi for i = 1, 2. When the significance level is α, the two classification methods will not have significantly different performance on imbalanced data set D if z ∈ [-zα/2, zα/2]. The whole procedure for comparing the F-measure of two classification algorithms is summarized as follows:

Step 1. Perform k-fold cross validation for two algorithms to collect TPij, FNij, and FPij for each possible i and j.

Step 2. Calculate STPi, SFNi, SFPi for i = 1, 2. If any one of them is less than five, then stop. Otherwise, proceed to step 3.

Step 3. Calculate rij and qij for each possible i and j to compute ρ¯i for i = 1, 2 by formulas (4) and (5).

Step 4. Calculate r¯i and q¯i to compute f¯i, w¯i, s2ri, and s2qi for i = 1, 2

Step 5. Calculate s2i for i = 1, 2 to compute statistic z by formula (6) for testing hypothesis H0:F1−F2=0.

4Example 1.
Suppose that two classification algorithms are tested on an imbalanced data set D to determine whether they have significantly different performance. The procedure for this test is executed as follows:

Step 1. Perform k-fold cross validation for the two algorithms, and the experimental results are shown in Table 2.

TABLE 2 The Experimental Results for Example 1
Table 2- 
The Experimental Results for Example 1
Step 2. After the calculation of STPi, SFNi, SFPi for i = 1, 2, the values given in the two rows for sum indicate that the experimental results of both algorithms satisfy the large-sample conditions, and hence proceed to step 3.

Step 3. The values of recall and precision for each fold are used to estimate their correlation coefficients ρ¯1=0.2897 and ρ¯2=0.7333.

Step 4. By the two rows for sum, we have r¯1=0.3553, q¯1=0.5094, r¯2=0.5528, and q¯2=0.3350. These values are used to compute f¯1=0.4186, f¯2=0.4172, w¯1=0.5891, w¯2=0.3773, s2r1=0.003014, s2q1=0.004715, s2r2=0.002010, and s2q2=0.001097.

Step 5. Since s21=0.002371 and s21=0.001223, the statistic for testing hypothesis H0:F1−F2=0 is calculated as z=0.4186−0.41720.002371+0.001223√=0.0238, and the p-value corresponding to this z value is 0.9810. The two algorithms do not have significantly different F-measure on this imbalanced data set.

SECTION 5Multiple Data Sets
As addressed by Dietterich [18], performance evaluation can be categorized as for single data set or for multiple data sets. Classification algorithms are generally tested on multiple data sets to determine which one has the best performance. This section presents a way to use multiple imbalanced data sets for comparing the performance of two classification algorithms.

Let Rim and Qim be the actual recall and precision of algorithm i on data set m, respectively. Then the actual F-measure of algorithm i on data set m is calculated as Fim=2RimQim/(Rim+Qim). This actual F-measure can also be expressed as Fim=wimRim+(1−wim)Qim=Fim for some wim∈[0,1], where wim is called the actual weight of recall for algorithm i on data set m. The actual mean F-measure for algorithm i on M data sets equals μi=ΣMm=1Fim/M. The null hypothesis for identifying whether algorithms 1 and 2 have significantly different performance is expressed as H0: μ1–μ2 = 0.

When the aggregation level is set at data set, the number of folds for various algorithms has no impact in deriving the sampling distribution of F-measure. For the sake of simplicity, the number of folds for two algorithms are assumed to be the same. Let TPijm,FNijm,andFPijm be the true positives, false negatives, and false positives, respectively for algorithm i on fold j of data set m for i = 1, 2, j = 1, 2, …, k, and m = 1, 2,…, M. Similarly, let STPim=Σkj=1TPijm, SFNim=Σkj=1FNijm, and SFPim=Σkj=1FPijm for i = 1, 2 and m=1,2,…,M. Then the recall and precision for algorithm i on data set m are calculated as r¯im=STPim/(STPim+SFNim) and q¯im=STPim/(STPim+SFPim), respectively. Suppose that the experimental results of every data set satisfy the large-sample conditions; i.e., STPim ≥ 5, SFNim ≥ 5, and SFPim ≥ 5 for i = 1, 2 and m = 1, 2, …, M. Then we can assume that r¯im follows N(Rim,σ2rim) and that q¯im follows N(Qim,σ2qim), where σ2rim=Rim(1−Rim)/(STPim+SFNim) and σ2qim=Qim(1−Qim)/(STPim+SFPim). Since r¯im and q¯im are correlated and normally distributed, they can be assumed to follow bivariate normal distribution N(Rim,Qim,σ2rim, σ2qim,rim), where ρim is the correlation coefficient between r¯im and q¯im. The linear combination of r¯im and q¯im will also follow a normal distribution, and hence f¯im=wimr¯im+(1−wim)q¯im is assumed to be normally distributed. Then sample mean F-measure x¯i=ΣMm=1f¯im/M is an unbiased point estimator of μi. This procedure is summarized in Table 3. The following theorem gives the sampling distribution of x¯1−x¯2 for testing hypothesis H0:μ1−μ2=0.

TABLE 3 The Procedure for Aggregating the F-Measure of Multiple Data Sets
Table 3- 
The Procedure for Aggregating the F-Measure of Multiple Data Sets
5Theorem 2.
When the aggregation level is set at data set, classification algorithms 1 and 2 are independent, and the experimental results of every data set satisfy the large-sample conditions, point estimator x¯1−x¯2 can be assumed to follow a normal distribution with mean μ1–μ2 and variance Σ2i=1ΣMm=1σ2im/M2, where σ2im=w2imσ2rim+(1−wim)2σ2qim+2wim(1−wim)ρimσrimσqim.

5Proof.
Since the testing results of every data set satisfy the large-sample conditions, r¯im=STPim/(STPim+SFNim) and q¯im=STPim/(STPim+SFPim) can be assumed to follow N(Rim, σ2rim) and N(Qim, σ2qim), respectively for i = 1, 2 and m=1,2,…,M. Let ρim be the correlation coefficient between r¯im and q¯im. Then the joint distribution of r¯im and q¯im can be assumed to follow N(Rim,Qim,σ2rim,σ2qim,ρim). By Property 1, f¯im=wimr¯im+(1−wim)q¯im will follow a normal distribution with mean wimRim + (1-wim)Qim and variance σ2im, where
σ2im=w2imσ2rim+(1−wim)2σ2qim+2wim(1−wim)ρimσrimσqim.
View Source

Since the instances in various data sets are collected independently, the f¯im for m=1,2,…,M are independent. We therefore have x¯i=ΣMm=1f¯im/M that will follow a normal distribution with mean
E(x¯i)=E(ΣMm=1f¯im/M)=ΣMm=1E(f¯im)/M=ΣMm=1Fim/M=μi
View Sourceand variance
Var(x¯i)=Var(ΣMm=1f¯im/M)=ΣMm=1Var(f¯im)/M2=ΣMm=1σ2im/M2.
View SourceRight-click on figure for MathML and additional features.

When two classification algorithms are independent, f¯1u and f¯2v are independent for all possible u and v. Variable x¯1−x¯2 therefore follows a normal distribution with mean
E(x¯1−x¯2)=E(x¯1)−E(x¯2)=μ1−μ2
View SourceRight-click on figure for MathML and additional features.and variance
Var(x¯1−x¯2)=Var(x¯1)+Var(x¯2)=Σ2i=1ΣMm=1σ2im/M2.
View Source

Since x¯1−x¯2 can also be expressed as ΣMm=1(f¯1m−f¯2m)/M, there is an alternative to prove Theorem 2. Let dm=f¯1m−f¯2m be the difference of the two F-measures for data set m. When the two classification algorithms are independent, and the experimental results for every data set satisfy the large-sample conditions, dm will follow a normal distribution with mean F1m–F2m and variance σ21m+σ22m. Since data sets are collected independently, ΣMm=1(f¯1m−f¯2m)/M follows a normal distribution with mean ΣMm=1(F1m−F2m)/M=μ1−μ2 and variance ΣMm=1(σ21m+σ22m)/M2, the same as the one given in Theorem 2. The way to aggregate the F-measures of data sets for two algorithms therefore have no impact on the sampling distribution for performance comparison.

In identifying whether classification algorithms 1 and 2 have significantly different performance on M imbalanced data sets, the null hypothesis is H0: μ1–μ2 = 0. The values of wi, σ2rim, and σ2qim are estimated as w¯i=(f¯i−r¯i)/(q¯i−r¯i), s2rim=r¯im(1−r¯im)/(STPim+SFNim), and s2qim=q¯im(1−q¯im)/(STPim+SFPim), respectively for i = 1, 2 and m = 1, 2, …, M. Then the pairs of fold recall and precision for each data set are used to estimate their correlation coefficient ρ¯im. Variance σ2im is therefore estimated as s2im=w¯2is2rim+(1−w¯i)2s2qim+2w¯i(1−w¯i)ρ¯imsrimsqim. The statistic for testing the hypothesis is calculated as
z=x¯1−x¯2Σ2i=1ΣMm=1s2im/M−−−−−−−−−−−−−√(7)
View SourceRight-click on figure for MathML and additional features.

For any given significance level α, the two algorithms will not have significantly different performance on the M imbalanced data sets if z ∈ [-zα/2, zα/2].

SECTION 6Experimental Study
The characteristics of the 10 data sets chosen from the UCI data repository [19] for evaluating the performance of classification algorithms are summarized in Table 4. They are not proper imbalanced data sets, and hence some of the class values in each data set are chosen to be the positive one, as shown in the fourth column of Table 4. The last column of Table 4 shows the imbalanced ratio (IR) of a data set, that is defined to be the number of negative instances over the number of positive instances. Every data set has an IR larger than 9.0 to indicate that the proportion of positive instances is less than 10 percent. The number of folds is set to be five.

TABLE 4 The Characteristics of the 10 Imbalanced Data Sets for This Study
Table 4- 
The Characteristics of the 10 Imbalanced Data Sets for This Study
Four classification algorithms k-nearest neighbors with k = 1, naïve Bayesian classifier, random forest, and ripper algorithm provided by Weka with default settings are chosen to process these 10 data sets, and the algorithms are abbreviated as 1NN, NBC, RF, and RIP, respectively. The experimental results of the four algorithms are shown in Table 5. Every algorithm has some of the TPs, FNs, and FPs in a fold to be less than five. This indicates that the aggregation level for imbalanced data sets should not be set at fold.

TABLE 5 The Classification Results of (A) K-Nearest Neighbors With k = 1, (B) Naïve Bayesian Classifier, (C) Random Forest, and (D) Ripper Algorithm
Table 5- 
The Classification Results of (A) K-Nearest Neighbors With k = 1, (B) Naïve Bayesian Classifier, (C) Random Forest, and (D) Ripper Algorithm

6.1 Single Data Set
The testing results given in Table 5 can be used to calculate F-measure for comparing the performance of a pair of classification algorithms on an imbalanced data set. Consider the testing results of data set ‘Abalone’ for 1NN and NBC denoted as algorithms 1 and 2, respectively. Since the sums of TPs, FNs, and FPs for 1NN are 107, 284, and 306, respectively, we have r¯1=107/(107+284)=0.2737 and q¯1=107/(107+306)=0.2591. The F-measure is calculated as f¯1=2×0.2737×0.25910.2737+0.2591=0.2662, and hence the weight of recall is w¯1=(0.2662−0.2591)/(0.2737−0.2591)=0.4863. The variance estimates for recall and precision are s2r1=0.2737×(1−0.2737)/(107+284)=0.000508 and s2q1=0.2591×(1−0.2591)/(107+306)=0.000465, respectively. By formula (4), the correlation coefficient between recall and precision computed from their five pairs (0.3014, 0.2651), (0.3590, 0.2828), (0.2639, 0.2184), (0.2222, 0.2821), and (0.2319, 0.2424) is ρ¯1=0.3417, and hence the estimate of the variance of f¯1 is
s21=0.48632×0.000508+(1−0.4863)2×0.000465+2× 0.4863×(1−0.4863)×0.3417×0.000508−−−−−−−√×0.000465−−−−−−−√=0.000326.
View Source

By the same argument, we will have r¯2= 0.8491, q¯2= 0.2294, f¯2= 0.3613, w¯2= 0.2127, s2r2= 0.000328, s2q2= 0.000122, ρ¯2= −0.1275, and s22= 0.000082. By formula (6), the statistic for testing H0: F1 – F2 = 0 is z=0.2662−0.36130.000326+0.000082√=−4.7082, and the corresponding p-value is 2.50×10-6. When the significance level α is 0.05, the null hypothesis is rejected to indicate that 1NN and NBC have significant different performance on data set ‘Abalone’. The statistics for the four classification algorithms are summarized in Table 6. The results of hypothesis testing by employing the statistics in Table 6 are shown in Table 7, where a bold value indicates that the test statistic is less than -z0.025 = -1.96 or larger than z0.025 = 1.96.

TABLE 6 The Statistics for Classification Algorithms (A) K-Nearest Neighbors With k = 1, (B) Naïve Bayesian Classifier, (C) Random Forest, and (D) Ripper Algorithm
Table 6- 
The Statistics for Classification Algorithms (A) K-Nearest Neighbors With k = 1, (B) Naïve Bayesian Classifier, (C) Random Forest, and (D) Ripper Algorithm
TABLE 7 The z Value of the Hypothesis Testing for the Six Possible Pairs of the Four Classification Algorithms on Single Data Set

In the original expression, F-measure is the harmonic mean of recall and precision. The value of F-measure will be close to the smaller one. The linear approximation of F-measure proposed by this study calculates F-measure by expression f¯=w¯r¯+(1−w¯)q¯, and hence the value of w¯ is the weight of recall. When w¯ is close to zero, F-measure will be close to precision. This means that recall is far larger than precision. On the contrary, when w¯ is close to one, F-measure is primarily determined by recall. In this case, recall is far smaller than precision. The weight of recall will be close to 0.5 only when recall and precision are approximately equal.

The last row for each algorithm in Table 6 shows the range of the weight of recall. This range for 1NN indicates that the weights for all data sets are close to 0.5. This means that 1NN has approximately the same recall and precision on every data set. This algorithm is likely to achieve a balanced performance between recall and precision. However, NBC has a wide range of weight [0.2127, 0.7244]. It is difficult to say that the F-measure resulting from naïve Bayesian classifier will mainly depend on recall, precision, or both. The ranges of weight for random forest and ripper algorithm are similar. Their ranges indicate that the recall resulting from these two algorithms is generally smaller than the precision. The weight of recall provided by the linear approximation approach can help us to study the characteristics of classification algorithms on imbalanced data sets.

Only nine of the 60 z values given in Table 7 are in non-rejecting region [-1.96, 1.96]. Since F-measure is a combination of recall and precision, two classification algorithms is easy to have significantly different performance on an imbalanced data set. This suggests that an imbalanced data set should be analyzed by several algorithms to determine the best one. The last two rows of Table 7 show the numbers of significant wins and losses of the first algorithm in a column versus the second one in the same column. For example, 1NN is significantly better than NBC in seven data sets while significantly lower in two data sets in column ‘1NN versus NBC’. Columns two through four of Table 7 suggest that 1NN should be the best among the four algorithms. Columns five and six indicate that NBC is generally inferior to RF and RIP, and RF should be usually better than RIP by the last column.

Though NBC seems to be the one that generally has the lowest performance on an imbalanced data set, it is significantly better than the other three algorithms in data sets ‘Abalone’ and ‘Seismic’. Note that 1NN, RF, and RIP all have low F-measure on these two data sets. When the other classification algorithms cannot achieve a good F-measure on an imbalanced data set, naïve Bayesian classifier may be able to provide an acceptable result.

6.2 Multiple Data Sets
As analyzed in Section 6.1, even naïve Bayesian classifier is inferior to the other three classification algorithms in several data sets, it can still achieve the highest F-measure in two data sets. This section will use the method proposed in Section 5 to compare the performance of the four algorithms on the 10 imbalanced data sets. The results can help us to draw more concrete conclusions on the four algorithms. Nonparametric method Wilcoxon signed-rank test will be the benchmark for comparison.

Let 1NN and NBC be algorithms 1 and 2, respectively again. Then by the columns for F-measure f¯ and variance s2 in (A) and (B) of Table 6, we have
x¯1=(0.2662+0.8607+⋯+0.3804)/10=0.5952,s21=0.000326+0.000205+⋯+0.002376=0.006004,x¯2=(0.3613+0.5959+⋯+0.4632)/10=0.4078,
View SourceRight-click on figure for MathML and additional features.and
s22=0.000082+0.000310+⋯+0.001680=0.004400.
View Source

Let μ1 and μ2 be the actual mean F-measure of 1NN and NBC on the 10 data sets, respectively. Then by formula (7), the statistic for testing hypothesis H0: μ1–μ2 = 0 is calculated as
z=0.5952−0.4078(0.006004+0.004400)/10−−−−−−−−−−−−−−−−−−−−√=5.8099,
View SourceRight-click on figure for MathML and additional features.and its corresponding p-value is 6.25×10-9. When the significance level α = 0.05, the null hypothesis is rejected, and hence the two algorithms have significantly different mean F-measure on the 10 imbalanced data sets. Since x¯1>x¯2, we conclude that 1NN significantly outperforms NBC. The testing results for the six possible pairs of the four classification algorithms are listed in Table 8, where a bold value indicates that the p-value is less than or equal to 0.05.

TABLE 8 The Testing Results for the Six Possible Pairs of the Four Classification Algorithms on Multiple Data Sets
Table 8- 
The Testing Results for the Six Possible Pairs of the Four Classification Algorithms on Multiple Data Sets
In Table 8, columns 2, 5, and 6 show that naïve Bayesian classifier is significantly inferior to the other three algorithms, and 1NN does not have significantly different performance with respect to RF and RIP by columns 3 and 4. The last column in Table 8 indicates that RF is a better choice than RIP. The testing results of the 10 imbalanced data sets suggest that 1NN or RF should be the first choice among the four algorithms, followed by RIP.

When algorithms 1 and 2 represent 1NN and NBC, respectively for Wilcoxon signed-rank test, the 10 differences of the F-measure are computed, and the absolute values of the differences are ranked. In this case, sample size M = 10 and the sum of positive signed ranks is T+=49. The sampling distribution of T+ can be assumed to follow a normal distribution with mean M(M+1)/4 and variance M(M+1)(2M+1)/24 when sample size M is larger than or equal to 10 [15]. The test statistic is calculated as
z=49−10×11/410×11×21/24−−−−−−−−−−−−−√=2.1915,
View SourceRight-click on figure for MathML and additional features.and the corresponding p-value is 0.0284. When the significance level α = 0.05, the null hypothesis H0: μ1–μ2 = 0 is rejected to indicate that 1NN and NBC have significant different performance on the 10 data sets. The testing results for the six possible pairs of the four classification algorithms are listed in Table 9, where a bold value indicates that the p-value is less than or equal to 0.05.

TABLE 9 The Testing Results for the Six Possible Pairs of the Four Classification Algorithms for Wilcoxon Signed-Rank Test

The two significant cases in Table 9 are ‘1NN versus NBC’ and ‘RF versus RIP’ that are also significant in Table 8. By Wilcoxon signed-rank test, both RF and RIP are not significantly better than naïve Bayesian classifier. Since the mean F-measure of 1NN is less than that of RF, the z value for ‘1NN versus RF’ in Table 8 is negative. However, the z value for ‘1NN versus RF’ in Table 9 becomes positive, because all differences of F-measure are transformed into ranks. These suggest that the method proposed in this study is more powerful in identifying which classification algorithm can have a higher mean F-measure on multiple imbalanced data sets.

6.3 Discussion
Ripper algorithm finds the classification rules for minor class value first, and hence it can be a proper tool for imbalanced data sets. Naïve Bayesian classifier calculates the classification probability for each class value and assigns the one with the largest probability to be the predicted class value. In calculating classification probabilities, there is a factor that represents the proportion of a class value in the training set. When a data set is imbalanced, the ratio of the proportions for negative and positive class values will equal the imbalanced ratio. This suggests that naïve Bayesian classifier is not a classification algorithm designed for processing imbalanced data sets. It is therefore not surprising that naïve Bayesian classifier has the lowest performance among the four algorithms.

The sample variance of the F-measure for a data set is calculated as
s2=w¯2s2r+(1−w¯)2s2q+2w¯(1−w¯)ρ¯srsq,
View Sourcewhere ρ¯ is the sample correlation coefficient calculated from recall-precision pairs (rj, qj) for j = 1, 2, …, k. Those pairs can also be used to calculate sample covariance as
srq=Σkj=1(rj−x¯)(qj−y¯)k−1,
View Sourcewhere x¯=Σkj=1rj/k and y¯=Σkj=1qj/k. However, since s2r and s2q are not estimated from the recall-precision pairs, srq cannot be used to replace ρ¯srsq in calculating the sample variance of F-measure.

As mentioned by Casella and Berger [20], two normally distributed random variables do not imply that their joint distribution will be a bivariate normal distribution. Let U and V be two independent normal random variables, and let a, b, c, and d be constants. Then the joint distribution of X = aU + bV and Y = cU + dV will follow a bivariate normal distribution. Let an imbalance data set D be divided into two disjoint subsets D1 and D2. Then the true positives of D can be expressed as TP = TP1+TP2, where TPj is the true positives in Dj for j = 1, 2. The recall and precision for this data set can be rewritten as
recall=TPTP+FN=|D1|TP+FN×TP1|D1|+|D2|TP+FN×TP2|D2|=aU+bV
View SourceRight-click on figure for MathML and additional features.and
precision=TPTP+FP=|D1|TP+FP×TP1|D1|+|D2|TP+FP×TP2|D2|=cU+dV,
View SourceRight-click on figure for MathML and additional features.respectively, where U = TP1/|D1|, V = TP2/|D2|, a = |D1|/(TP+FN), b = |D2|/(TP+FN), c = |D1|/(TP+FP), d = |D2|/(TP+FP), and |Dj| is the number of instances in Dj for j = 1, 2. For any given bipartition on data set D, |D1|, |D2|, and TP+FN are constants, and hence a and b are constants. TP+FP is the number of instances in D predicted to be positive, and it should be appropriate to say that TP+FP is a constant for any given classification algorithm. When a, b, c, and d are all considered to be constants, if U and V are independent random variables and normally distributed, then recall and precision will follow a bivariate normal distribution.

TPj/|Dj| represents the proportion of true positives in Dj. As introduced in Section 2, this sample proportion can be assumed to be normally distributed if TPj ≥ 5 and |Dj|–TPj ≥ 5. Since U and V are sample proportions obtained from disjoint sets, and instances are collected independently, they are independent random variables. This implies that if TP1+TP2 = TP ≥ 10 and |D|–TP ≥ 10, then the joint distribution of recall and precision can be assumed to follow a bivariate normal distribution. These conditions are more restrictive than the large-sample conditions employed in Sections 4 and 5 for deriving the sampling distributions of point estimators. In an imbalanced data set D, condition |D|–TP ≥ 10 generally holds. To be conservative in comparing the performance of two classification algorithms on imbalanced data sets, large-sample condition TP ≥ 5 should be replaced by TP ≥ 10.

F-measure will generally not be used for evaluating the performance of classification algorithms on ordinary data sets. The nonsymmetrical confidence intervals derived from blocked 3×2 cross validation can be employed to compare the performance of two classification algorithms on single data set. However, the proportion of positive instances in the two data sets tested by [12] both equals 0.5. They did not show that the nonsymmetrical intervals will have better degree of confidence on imbalanced data sets. The parameter of priors and the number of training instances have to be set for deriving confidence intervals. It is unlikely that the confidence intervals for multiple data sets can be aggregated for performance comparison.

Since the point estimator for each data set is normally distributed, the linear approximation approach proposed in this study can be applied to compare the performance of two classification algorithms on any number of imbalanced data sets. The sampling distributions of the F-measure for various classification algorithms are unlikely to have the same variance, and hence this linear approximation approach cannot be extended to compare the performance of more than two algorithms.

As mentioned in Section 3, F-measure is equivalent to Fβ with β = 1. The value of β can be set to reflect the importance level of recall with respect to that of precision. Since Fβ is a value between zero and one, the linear approximation approach proposed in this study is also applicable for Fβ when β ≠ 1.

SECTION 7Conclusions
Accuracy is a proper measure for evaluating the performance of classification algorithms on ordinary data sets. When the class distribution in a data set is highly skewed, all algorithms are likely to have close accuracies. F-measure that is the harmonic mean of recall and precision is therefore more appropriate than accuracy in analyzing the performance of classification algorithms on imbalanced data sets. This study first introduces a linear approximation approach to derive the sampling distribution of F-measure. Then the parametric statistical methods based on this linear approximation approach are proposed to compare the performance of two classification algorithms on single and multiple imbalanced data sets.

Ten imbalanced data sets and four classification algorithms, k-nearest neighbors with k = 1, naïve Bayesian classifier, random forest, and ripper algorithm, are chosen to test the statistical methods. The experimental results show that the proposed methods can effectively compare any pair of classification algorithms on imbalanced data sets. The weight of recall provided by the linear approximation approach can help us to analyze the characteristics of classification algorithms. In an imbalanced data set, the recall resulting from random forest or ripper algorithm is generally smaller the precision, and k-nearest neighbors with k = 1 has close recall and precision.

Though naïve Bayesian classifier is not an algorithm designed for processing imbalanced data, it can achieve significantly higher F-measure on some sets in which the other algorithms have relatively low performance. The weight of recall calculated from the linear approximation approach can thus provide directions for improving the performance of classification algorithms on imbalanced data sets.