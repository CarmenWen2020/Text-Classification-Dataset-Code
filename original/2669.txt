Network embedding algorithms learn a mapping from the discrete representation of nodes to continuous vector spaces that preserve the proximities of nodes. The techniques have a wide range of applications in various downstream tasks such as node classification, link prediction, and network alignment. Despite recent efforts to the design of novel models, little attention has been paid to understanding the instability of network embedding. In this paper, we fill this gap by investigating several state-of-the-art network embedding methods. Node embeddings form a geometric shape in the latent space. Characterizing the geometry is critical to figure out the variance of network embedding. Hence, we define two metrics to characterize the geometric properties and find that node embeddings tremble in different instantiations of an embedding space. Then, we formally define the stability of node embeddings as the invariance of the nearest neighbors of nodes. Experimental results show that existing embedding approaches have significant amounts of instability. We explore the influence factors that affect the stability of different methods and find that both network structure and algorithm models affect the stability of node embeddings significantly. Finally, we examine the implications of embedding instability for downstream tasks and find remarkable impacts on the performance.
SECTION 1Introduction
Graphs are essential for the representation of networked systems such as social, biological, and communication networks [1], [2], [3]. The discrete nature of graphs has significantly contributed to the analytics of many complex systems. However, most existing discrete algorithms suffer from a high computational cost. Besides, it is usually challenging to design and implement parallelized and distributed alternatives [4], [5]. Moreover, graphs also face a dilemma in generalizing many continuous feature-based machine learning algorithms. To address these issues, researchers have developed many useful embedding algorithms to learn vector representations of nodes which preserve node proximities [6]. It has been verified that embedding methods such as DeepWalk [7], Node2vec [8], and Struc2vec [9] are useful in various downstream tasks such as node classification [10], link prediction [11], and network alignment [12]. In this paper, we use the terms “network” and “graph”, “node” and “vertex”, and “edge” and “link”, interchangeably.

Network embedding algorithms commonly comprise two primary steps, including node proximity estimation and low-dimensional representation learning. Since it is extremely difficult or impossible to compute node proximities accurately, many existing algorithms exploit sampling schemes such as random walks and negative sampling to improve effectiveness and efficiency [7], [8], [9], [13]. Some deep-neural-network-based methods employ stochastic optimization such as stochastic gradient descent schemes to improve computational efficiency [14], [15]. All these methods unavoidably introduce some variances in the learned vectors; that is, we obtain discrepant results in different instantiations of an embedding method. An instantiation means that we apply an algorithm to a specific network and obtain the embedding vectors that are ready for downstream tasks.

Many applications of network embedding revolve around the calculation of nearest neighbors in the latent space [16]. Experimental results show that the variances of node embeddings cause significant fluctuations of nearest neighbors, which may affect the performance of network embedding in downstream tasks. This observation indicates that these methods have significant amounts of instability. Revealing the characteristics and influence factors of the instability is critical to the design, interpretation, and evaluation of network embedding algorithms. For example, understanding the causes of embedding variance could contribute to the interpretability of the models, such as the Skip-Gram model with Negative Samples (SGNS) [7], [8], [13], multiple-layered neural networks [14], [17], and deep auto-encoder networks [15]. Besides, the interpretability of these models is essential to broaden their applications in many confidence-critical scenarios such as medical treatment, autonomous vehicles, and financial security. Despite recent efforts to the design of novel models, little attention has been paid to the understanding of network embedding instability. We acknowledge that it is critical to design an efficient embedding model to overcome the instability. However, in this paper, we focus on exploring the characteristics and causes of network embedding instability as well as its implications on downstream tasks. The possibilities of stable network embedding and possible research directions of theoretical analysis on embedding instability are discussed in Section 7.

There are three challenges in the understanding of network embedding instability. First, node embeddings form a geometric shape in the latent space. We must design proper metrics to characterize the geometry of node embeddings, which is critical to the understanding of embedding instability. Second, we must define a proper metric of embedding stability to reflect the utilization in downstream tasks in order to make the study meaningful. Third, many factors may affect the stability of network embedding, and different models may have different influence factors. Hence, we must propose a suitable methodology to explore the influence factors for different models.

In this paper, we develop a measurement framework to address the above issues and perform a systemic study on network embedding instability. Fig. 1 presents the road map of this study. First, we investigate the geometric characteristics of the embedding space for different method-network pairs. To describe how node embeddings distribute in the latent space, we define two metrics, including magnitude and concentricity, to characterize the geometry of node embeddings. We also examine how embedding variance affects the geometry. It is shown that each node point trembles in a small region in the latent space, and different nodes have different amounts of variance. Besides, different methods have different variance distributions. These observations indicate that different embedding models have different geometric characteristics, and the variances of node embeddings have different impacts on the geometry.

Fig. 1. - 
The road map of this study.
Fig. 1.
The road map of this study.

Show All

Second, we present the formal definition of network embedding stability. The geometric variances of node embeddings cannot be used as a metric for instability measurement because they do not reflect the utilization in downstream tasks. To address this issue, we quantify the instability of network embedding as the variance of the nearest neighbors of nodes in different embedding spaces. We measure the instability characteristics of different network embedding algorithms based on both real-world and artificial networks. It is found that different network algorithms have different stability distributions on different networks, indicating that both network structure and algorithm models affect the stability of network embedding.

Finally, we investigate the influence factors that affect embedding stability and the implications of embedding instability for downstream tasks. In this paper, we consider two kinds of inherent factors, including network structure and algorithm models. The results demonstrate that network properties such as the number of nodes and edges, average node degree, network density, and assortativity have significant impacts on the instability. We also evaluate the implications of embedding instability for the performance of several downstream tasks, including node classification, link prediction, and network alignment. It is revead that instability has remarkable impacts on the performance.

We clarify that the purpose of this work is to understand the instability of network embedding and its implications for downstream tasks, rather than comparing the performance of different algorithms or seeking methods to minimize embedding instability.

SECTION 2Related Work
Part of this study is motivated by the recent findings of instability in word embeddings in the field of Natural Language Processing (NLP). Word embeddings are mappings of words to points in a low-dimensional vector space [18], [19], [20]. The technique has been used for a variety of NLP tasks [21], [22], [23]. Recent studies have shown that the nearest neighbors of words in different embedding spaces are not consistent, indicating that embeddings of words have large amounts of instability [24], [25], [26]. Antoniak et al. [24] found that the distances of words in the embedding spaces are highly sensitive to small changes in the training corpus for a variety of word embedding algorithms. Pierrejean and Tanguy [25] studied the nearest neighbors’ variations of word embeddings trained with different parameters. They found that words in different semantic spaces have varying nearest neighbors in different embedding spaces. Wendlandt et al. [26] investigated the factors influencing the instability of word embeddings, finding that domains and part-of-speech are two critical factors. The study also reveals that word embedding instability has significant impacts on the performance in downstream tasks.

Compared with word embeddings, network embedding algorithms have distinct causes of instability. For example, DeepWalk employs a random walk model to explore the relational information of nodes [7]. The model introduces inherent variance in the generated contexts of nodes, which further causes the instability of node embeddings. We roughly classify existing methods into three categories, including matrix factorization-based approaches, structural context inference models, and deep neural network-based methods [27], [28]. Different approaches and their variants have different causes of variances in the embedding space.

2.1 Matrix Factorization
The matrix factorization (MF)-based approaches such as HOPE [29] and GraRep [30] employ conventional matrix decomposition methods such as Singular Value Decomposition (SVD) to find a low-dimensional representation of a network. For example, HOPE [29] preserves high-order relational information of large-scale networks by capturing the asymmetric transitivity between nodes. GraRep [30] considers the powers of adjacency matrices in order to capture the relational information amongst nodes. Given the same network, these MF-based methods always produce identical embedding results due to the deterministic nature of matrix factorization. It means that these methods are stable for static networks. However, these methods exhibit significant embedding instability versus the changes of network structure. Preliminary experiments show that the stability of the MF-based algorithms is more sensitive to the change of network structure than other stochastic models (See the supplementary material S1, which can be found on the Computer Society Digital Library at http://doi.ieeecomputersociety.org.ezproxy.auckland.ac.nz/10.1109/TKDE.2020.2989512). In this paper, we focus on the network embedding instability problem in static networks.

2.2 Structural Context Inference
Structural context inference based methods adopt the Skip-gram model to train deliberately generated structural contexts of nodes [27]. DeepWalk [7] utilizes truncated random walks to learn the structural contexts of nodes. The co-occurrence rates of nodes in the generated random paths encode the structural information, such as the neighborhood relationships of nodes. Different from DeepWalk, Node2vec [8] also employs biased random walks guided by two additional parameters p and q to generate structural contexts of nodes. The two additional parameters allow the search procedure to interpolate between Breadth-First-Sampling (BFS) and Depth-First-Sampling (DFS). Struc2vec [9] constructs a multilayer graph to encode structural similarity at different scales. Random walks are performed based on the constructed multilayer graph to generate structural contexts of nodes. All these three methods exploit the Skip-Gram model for latent representation learning. LINE [13] optimizes a carefully designed loss function based on the first and second-order relational information amongst nodes to preserve both local and global network structures. It adopts an edge-sampling scheme to improve inference effectiveness and computational efficiency. The causes of instability for these methods are twofold. First, the context generation models, such as random walks and edge sampling, bring uncertainty in the inferred node proximities. Second, the negative sampling scheme adopted by the Skip-gram model introduces variances into the embedding space. These factors together result in the instability of node embeddings.

2.3 Deep Neural Network
Deep neural network-based methods utilize the nonlinear characteristics of neural networks to transform the original discrete space into a low-dimensional continuous vector space [27]. SDNE (Structural Deep Network Embedding) [14] adopts a semi-supervised deep model with multiple layers of non-linear functions to capture the hidden information of network structure. The method exploits the first and second-order neighborhoods jointly to preserve both local and global information of network structure. DNGR (Deep Neural networks for Graph Representations) [15] adopts a random surfing model to capture structural information directly from a graph. It introduces a stacked de-noising auto-encoder to extract sophisticated features and model the non-linearities of network embedding. SiNE [17] is a deep neural network learning framework for signed networks, which optimizes an objective function guided by graph theories. The variances of these methods are jointly caused by the adoption of stochastic gradient descent schemes, the randomness of neural network initialization, and the possibility of local optimization.

The above analyses indicate that node embeddings have some amounts of instability in the latent space. However, little attention has been paid to the understanding of the characteristics and causes of embedding instability. In this paper, we take the first step to measure the stability of the state-of-the-art network embedding methods.

SECTION 3Geometry of Network Embedding
Network embedding intrinsically transforms geometric data from the non-euclidean space to the euclidean space, which keep the affinity of nodes approximately [31]. Investigating the geometry of node embeddings is critical to the understanding of embedding instability. Throughout this paper, we write matrices in boldface capital letters and denote vectors as boldface lowercase letters. A network is denoted as G=(V,E), where V is the set of nodes, and E is the set of edges. An embedding method is a mapping M(G):V→Xn×d from the node set V to a d-dimensional vector space X, where X is an n×d matrix, and n=|V| is the number of nodes. The ith row xi of X is the embedding vector of node vi. Node embeddings form a point set in the multidimensional space. Fig. 2 illustrates the geometry of node embeddings in a 3-dimensional space. In this paper, we design two metrics, including magnitude and concentricity, to characterize the geometry of node embeddings. Both metrics reflect the arrangement and positions of node vectors in the embedding space. In this study, we measure the characteristics of the two metrics from both macroscopic and microscopic perspectives.

Fig. 2. - 
Geometric properties of network embedding.
Fig. 2.
Geometric properties of network embedding.

Show All

We study six embedding algorithms including DeepWalk [7], Node2vec [8], Struc2vec [9], LINE [13], SDNE [14], and DNGR [15]. The taxonomy of the six methods and their default parameter values are presented in Table 1. The default dimension of all methods is 64. All experiments are conducted based on the default parameter settings unless it is pointed out. We employ eight real-world networks to conduct the experiments. A statistical summary of these networks is shown in Table 2. Bio-DM-LC, Bio-DM-HT, Bio-grid-worm, and Bio-celegans are all biological networks [32]. US-power-grid is a network that contains information about the power grid of the Western States of the United States of America [33]. Advogato is an online community platform for developers of free software launched in 1999 [33]. The nodes are Advogato users, and the edges represent their trust relationships. Email-Eu-core represents the “core” of the email-EuAll network, which contains correspondence links between members of the institution and people outside the institution [34]. Wiki-Vote is a network that contains all the Wikipedia voting data from the inception of Wikipedia to January 2008 [34].

TABLE 1 A Taxonomy of Network Embedding Methods and Their Default Parameter Settings
Table 1- 
A Taxonomy of Network Embedding Methods and Their Default Parameter Settings
TABLE 2 A Statistical Summary of the Networks
Table 2- 
A Statistical Summary of the Networks
3.1 Magnitude
From a macroscopic perspective, we calculate the mean vector of all nodes as:
x¯=1n∑i=1nxi.(1)
View SourceRight-click on figure for MathML and additional features.where x¯ is the central point of node embeddings. We employ the magnitude of the central point x¯ as the magnitude of the whole point set, which is calculated as follows:
R=∥x¯∥2=∑k=1dx¯2k−−−−−⎷.(2)
View Source

From a microscopic perspective, we define the magnitude of a node vector xi as the euclidean distance of the point from the origin:
Ri=∑k=1dx2ik−−−−−−⎷.(3)
View SourceRight-click on figure for MathML and additional features.We use the max-min normalized root mean squared error (NRMSE) to measure the variance of embedding magnitudes, which is defined as follows:
σRi=1maxt(R(t)i)−mint(R(t)i)1T∑t=1T(R(t)i−Ri¯)2−−−−−−−−−−−−−−−⎷,(4)
View SourceRight-click on figure for MathML and additional features.where R(t)i is the magnitude of node vi in tth instantiation, and R¯i=1T∑Tt=1R(t)i is the average magnitude of node vi.

We measure the distribution of R to evaluate the magnitude variance of node embeddings from a macroscopic perspective. We generate T instantiations by running an embedding algorithm for T times with fixed parameters on the same network. We set T=300 to guarantee the statistical significance of the results. Fig. 3 presents the empirical distributions of R based on the Bio-DM-HT network. Similar results based on other networks are omitted due to the limited space. It is found that magnitudes of the whole point sets follow normal distributions for all algorithms. However, these methods exhibit very different characteristics. This further confirms the finding that embedding models could significantly affect the geometry of node embeddings. The magnitudes for all methods except DNGR only vary in a small range. This indicates that the randomness of these embedding models does not cause systematic fluctuations of magnitudes of the whole point sets. However, the magnitude of node embeddings of DNGR varies in a broad range, which may lead to significant amounts of instability.

Fig. 3. - 
Distributions of $R$R.
Fig. 3.
Distributions of R.

Show All

Fig. 4 presents the empirical distributions of NRMSEs of node magnitudes for different methods based on the Bio-DM-HT network. We find that node magnitudes vary in different instantiations for all embedding algorithms, revealing that node embeddings obtained by these methods have significant amounts of variances. For a specific model, different nodes have different NRMSEs, indicating that the local structure of nodes also affects the geometry of node embeddings. Besides, different algorithms have different NRMSE distributions, suggesting that the embedding models also affect the variance of node magnitudes. Particularly, DNGR has the largest average NRMSE, demonstrating that DNGR has more significant magnitude variances compared to other methods. Node embeddings obtained by SDNE have the smallest average NRMSE, indicating that embeddings generated by SDNE have more stable magnitudes than those generated by other methods, that is, the vector points exhibit fewer lengthwise fluctuations in the latent space. It is worth noting that DeepWalk and Node2vec have similar results, illustrating that random-walk strategies have limited effects on the variance of embedding magnitudes.

Fig. 4. - 
Distributions of $\sigma _{R_i}$σRi.
Fig. 4.
Distributions of σRi.

Show All

Remarks. The above results demonstrate that individual node embeddings tremble in regions with different volumes, and the magnitudes of the whole point set are relatively stable. Therefore, the variance of network embedding could cause significant fluctuations of nearest neighbors of nodes in the embedding space if the trembling regions of different node embeddings intersect.

3.2 Concentricity
The concentricity measures the relative distance of a specific point to the central point. Explicitly, we define the concentricity of xi as the Cosine similarity between xi and x¯:
αi=xi⋅x¯∥xi∥2∥x¯∥2=∑dk=1xikx¯k∑dk=1x2ik−−−−−−−√∑dk=1x¯2k−−−−−−−√,(5)
View SourceRight-click on figure for MathML and additional features.where x¯ is the central point of node embeddings. The higher the value of αi, the closer the point vector of node vi to the center. We define the average concentricity of all nodes in an instantiation as:
φ=1n∑i=1nαi.(6)
View SourceRight-click on figure for MathML and additional features.The higher the φ, the denser the point set in the embedding space. From a macroscopic perspective, the metric φ characterizes the concentricity of the whole point set.

Similarly, we use the max-min NRMSE to measure the concentricity variance of an individual node vi from a microscopic perspective, which is defined as follows:
σαi=1maxt(α(t)i)−mint(α(t)i)1T∑t=1T(α(t)i−αi¯)2−−−−−−−−−−−−−−⎷,(7)
View SourceRight-click on figure for MathML and additional features.where α(t)i is the concentricity of node vi in tth instantiation, and α¯i=1T∑Tt=1α(t)i is the average concentricity.

Fig. 5 displays the distributions of φ for different methods based on the Bio-DM-HT network. The results show that φ approximately follows a normal distribution. DeepWalk and Node2vec have very similar distributions of φ. Simultaneously, they have the lowest mean of φ, indicating that node embeddings concentrate in a small spatial domain. This finding reveals that random-walk policies have little effects on the distributions of φ. SDNE achieves the highest mean of φ, indicating that node points generated by this method have a large spread in the embedding space. LINE has a much wider distribution of φ compared to other methods. The result reveals that point sets generated by LINE have fluctuating extents in the embedding spaces. It confirms that LINE is more variable in terms of concentricity, although it has a stable magnitude.

Fig. 5. - 
Distributions of $\varphi$φ.
Fig. 5.
Distributions of φ.

Show All

Fig. 6 shows the empirical distributions of NRMSEs of node concentricity for different methods based on the Bio-DM-HT network. The results demonstrate that nodes have variable cosine distances to the central points in different instantiations for all methods. Besides, different nodes have different concentricity variances, indicating that concentricity is relevant to the local network structure of nodes. The different empirical NRMSE distributions for different methods exhibit that the base models also affect the variances of node concentricity. Specifically, LINE has the most substantial NRMSEs of node concentricity, followed by Struc2vec and DNGR. These findings reveal that node embeddings generated by LINE, Struc2vec, and DNGR have much more crosswise fluctuations; that is, these methods exhibit more amounts of instability compared to other methods. Moreover, DeepWalk and Node2vec have slightly different NRMSE distributions, suggesting that random-walk policies also affect the variances of node concentricity.

Fig. 6. - 
Distributions of $\sigma _{\alpha _{i}}$σαi.
Fig. 6.
Distributions of σαi.

Show All

Remarks. The measurement results of node concentricity indicate that, for most algorithms except LINE, the whole point sets have a relatively stable extent in the latent space, while individual embedding points tremble in a small range of different ranges. These findings further confirm that the variances of node embeddings could cause significant fluctuations of nearest neighbors.

SECTION 4Network Embedding Stability
In this section, we first formally define the stability of network embedding based on its utilization in downstream tasks. Then, we characterize the stability distributions of different method-network pairs.

4.1 Definition of Embedding Stability
Many network embedding applications such as link prediction, node classification, personalized graph search, and fraud detection directly revolve around the computation of the most “similar” nodes to a query node [16]. Other graph mining problems such as node clustering and network alignment also implicitly rely on the calculation of nearest neighbors in the embedding spaces [35]. Hence, we define the stability of network embedding as the invariance of nearest neighbors of nodes in different instantiations, which assesses the possible impacts of embedding variance on the performance in downstream tasks.

To quantify the stability of an embedding method M, we apply M to a network G for T times with identical parameter settings. This yields a set of embedding spaces, which is denoted by Ω={X1,X2,…,XT}. Downstream tasks usually adopt two distance metrics, including the Cosine similarity and the euclidean distance, to retrieve nearest neighbors of nodes [36]. In this paper, we report the results based on the Cosine similarity. Higher similarity scores indicate closer distances in the space. We obtain similar results based on the euclidean distance and omit them for space concerns.

We denote Nt as the set of the top K nearest neighbors of a node in the latent space Xt. For each embedding space pair (Xs,Xt), we first calculate Ns and Nt, respectively. Then, we measure the invariance of the nearest neighbors from two aspects, including the overlap ratio of Ns and Nt and the ranking invariance of shared nodes in Ns and Nt. We quantify the first through the Jaccard overlap ratio [37]:
Ji(Ns,Nt)=|Ns⋂Nt||Ns⋃Nt|=|Ns⋂Nt||Ns|+|Nt|−|Ns⋂Nt|.(8)
View SourceRight-click on figure for MathML and additional features.If Ji(Ns,Nt)=0, we set the embedding stability of node vi to 0 immediately. Otherwise, let C=Ns∩Nt denote the set of the overlapping nearest neighbors. We calculate the ranking invariance of Nt and Ns as follows:
Hi(Ns,Nt)=1Hmax∑vj∈C2(1+|r(s)j−r(t)j|)(r(s)j+r(t)j),(9)
View SourceRight-click on figure for MathML and additional features.where r(s)j and r(t)j are the ranks of node vj in Ns and Nt, sorted by the distances to node vi in ascending order. We normalize Hi with its maximum value Hmax=∑|C|h=11h that is obtained when Ns≡Nt, i.e., Ns and Nt are exactly the same. Each node in C has twofold contributions to the calculation of ranking invariance. First, the rank difference of a shared node in Nt and Ns reflects the order invariance of the two sets, which is captured by the left part of the denominator in Eqn. (9). Second, nodes with higher ranks are more critical and thus contribute more to the calculation of stability than those with lower similarity, which is reflected by the right part of the denominator in Eqn. (9). When the nearest neighbor set is large, nodes in lower places have few impacts on the calculation of stability. It is worth mentioning that there are other metrics, such as Kendall and Spearman coefficients to measure the correlation of two ranked lists. However, none of them consider the biased contributions of nodes to the calculation of stability.

Both the Jaccard overlap ratio J and the ranking invariance H reflect the invariance of nearest neighbor sets. The Jaccard overlap ratio of J measures the invariance from a macroscopic perspective. It concerns more about the element invariance of the nearest neighbor sets but ignores the order invariance of nodes in the sets. However, the ranking invariance H well captures the order invariance. Hence, we use the production of the two metrics to assess the invariance of the nearest neighbor sets synthetically. Finally, we calculate the embedding stability of node vi by averaging the results of all embedding space pairs:
Si=2T(T−1)∑1≤s<t≤TJi(Ns,Nt)×Hi(Ns,Nt).(10)
View SourceRight-click on figure for MathML and additional features.We define the embedding stability of a network as the average stability of all nodes:
SG=1n∑vi∈VSi,(11)
View SourceRight-click on figure for MathML and additional features.which is the average stability of all nodes.

Fig. 7 presents an example based on the Karate network. Nearest neighbors are calculated based on the euclidean distance for clear visualization. N1 and N2 are the top 10 nearest neighbors of node 34 in X1 and X2. We get J34(N1,N2)=0.667, and H34(N1,N2)=0.158. Thus, the stability of the node 34 is 0.106. Although there is a considerable overlap of the nearest neighbors, the ranks of nodes fluctuate vastly, which eventually results in the low embedding stability of node 34.

Fig. 7. - 
An example of embedding stability.
Fig. 7.
An example of embedding stability.

Show All

The definition of embedding stability has two important parameters, namely the number T of generated embedding spaces and the number K of nearest neighbors for a node. In this paper, we concern more about the stability distribution of node embeddings. In order to examine the impacts of T and K, we adjust one of the parameters and keep the other with its default value. We evaluate their impacts on SG based on three network datasets, namely Bio-celegans, Bio-DM-LC, and Email-Eu-core. Fig. 8a shows a bar plot of SG and its variance for different networks versus T. When T≥2, we obtain stable embedding stability for all evaluated networks because SG is the average embedding stability of all nodes in the measured network, leading to the resultant stability almost constant. Therefore, the parameter T has little effect on the stability distribution. Fig. 8b shows the embedding stability of 100 randomly selected nodes from each network. It should be noted that K increases in proportion to the total number of nodes. The results show that small values of K will lead to significant fluctuations in the calculated embedding stability. However, embedding stability decreases rapidly and becomes stable with the increase of K. Besides, larger values of T will lead to higher computational intensity. Hence, we set T=3 and K=0.3×|V| as their default values to balance computational efficiency and performance.

Fig. 8. - 
Stability of network embedding versus $T$T and $K$K.
Fig. 8.
Stability of network embedding versus T and K.

Show All

4.2 Characterization of Embedding Stability
In this part, we measure the stability distributions of different method-network pairs. According to the definition, the embedding stability si of a node vi falls in the interval [0,1]. Hence, we regard the stability as the probability that a node has the same nearest neighbors in different instantiations. The Beta distribution represents a family of probabilities and is a versatile way to represent outcomes for percentages or proportions. Thus, the Beta distribution is a feasible tool to represent the distributions of node embedding stability. The empirical results also show that the Beta distribution well captures the characteristics of embedding stability of different models:
f(s)=sα−1(1−s)β−1B(α,β),0≤s≤1,(12)
View SourceRight-click on figure for MathML and additional features.where α and β are shape parameters that jointly determine the shape of the distribution, and B(α,β) is the Beta function. In this paper, we employ the method of moments to estimate the shape parameters [38]:
α^=s¯(s¯(1−s¯)σ¯−1), if σ¯<s¯(1−s¯),(13)
View SourceRight-click on figure for MathML and additional features.and
β^=(1−s¯)(s¯(1−s¯)σ¯−1), if σ¯<s¯(1−s¯),(14)
View SourceRight-click on figure for MathML and additional features.where s¯=1n∑ni=1si is the average stability, σ¯=1n−1∑ni=1(si−s¯)2 is the sample variance of node embedding stability, and n is number of nodes in the test graph G. Fig. 9 present an example of an empirical stability distribution and its fitting of a Beta distribution.


Fig. 9.
An empirical embedding stability distribution and its fitting of a Beta distribution.

Show All

In addition to the real network datasets, we also examine two artificial network models, including the Erdös-Rényi (ER) random graph model and Barabási-Albert (BA) preferential attachment graph model. Fig. 10 shows the fitted stability distributions of node embeddings for different method-network pairs. The embedding stability in all experiments is lower than 0.5 because the definition of embedding stability is strict with the consideration of the ranking invariance of neighbors. In a single method-network experiment, as shown in Fig. 10, some nodes have higher embedding stability than others, indicating that the local network structure of nodes affects embedding stability. When we compare the results of the same method amongst different networks, it is found that different networks have different stability distributions. The results demonstrate that the global network structure also affects embedding stability. When we compare the results of different methods on the same network, we find that DeepWalk and Node2vec have similar stability distributions in all networks except Advogato. It suggests that random walk policies have limited effects on embedding stability. This result is consistent with the finding that random walk policies have limited effects on the variance of embedding geometry. Moreover, different embedding models have very different stability distributions, suggesting that embedding models also affect the characteristics of embedding stability. In summary, both network structure and embedding models significantly affect the stability of network embedding.

Fig. 10. - 
Comparison of stability distribution of different network under the same embedding method.
Fig. 10.
Comparison of stability distribution of different network under the same embedding method.

Show All

SECTION 5Influence Factors Analysis
In this section, we present the methodology to explore and analyze the factors affecting embedding stability.

5.1 Possible Influence Factors
We explore two kinds of factors, including algorithm properties and network properties. For algorithm properties, we focus on the basic parameters of embedding models, which highly depend on the base models used in different algorithms. It is worth noting that some parameters are training data-relevant. For example, the number of training sampling depends on the networks for embedding. Incorporating these parameters may result in misleading results because the networks used for experiments have very different scales (See Table 2). Hence, we exclude these data-relevant parameters and fix them with sufficiently large values for different networks. Network properties include global and local ones. Global properties refer to network attributes such as the number of nodes and edges. Local properties are descriptive attributes such as degrees and clustering coefficients of individual nodes.

5.1.1 Algorithm Properties
The random walk-based methods have two common parameters, including walk length and number of walks. The walk length represents the number of nodes in a context sequence, and the number of walks determines how many times that a random walk starts from a node. Node2vec has two additional parameters p and q to smoothly interpolate between Breadth-first Sampling (BFS) and Depth-first Sampling (DFS). The parameter p controls the likelihood of immediately revisiting a node in the walk, and the parameter q allows the walk to differentiate between “inward” and “outward” nodes. Struc2vec has a parameter k to specify the number of layers of the constructed multilayer graph. All three methods exploit the Skip-gram model to learn meaningful representations based on generated contexts of nodes. The Skip-gram model also has parameters such as the window size and sampling rate. Previous work has explored the influence of these parameters [26], and we do not consider them in this paper. We set these parameters to their common default values.

The LINE method has a parameter order to specify the tendency to preserve local and global information of network structure. When order=1 or order=2, LINE only uses the first-order or second-order relational information (neighborhood relationship or neighbors of neighbors) of nodes for embedding. When order=3, LINE combines the first and second-order relational information of nodes by concatenating the two embeddings.

The SDNE method has two parameters, namely number of layers and number of neurons in each layer, to specify the structure of a neural network. The DNGR has two parameters, including k-step and number of layers. The number of neurons in DNGR depends on the number of layers.

All these methods have a common parameter dimensions to specify the dimensionality of embedding spaces.

5.1.2 Network Properties
We explore the network properties from two different levels, namely 1) a macroscopic level of global properties, and 2) a microscopic level of local properties.

1) Global Properties: The impacts of network scales are evaluated by the number of nodes (denoted as n) and edges (denoted as m). We also evaluate the impacts of network connectivity. We examine the impacts of network sparsity by the average node degree d¯ and network density D. We also consider the average clustering coefficient of all nodes. Degree assortativity is the similarity of connections concerning node degrees in a network [39]. This metric assesses the joint probability distribution of node degrees. The assortativity may affect the stability of network embedding because it affects the mixing time of random walks [40], [41]. Transitivity measures the fraction of triangles present in a network. This metric counts the possibility that a random walk circulates in a local triangle.

2) Local Properties: Several attributes capture the local structure of a node. Node degree is the number of neighbors of a node. The clustering coefficient measures the proportion of links amongst the neighbors of a node divided by the number of links that could exist between them [42]. The centrality metrics measure the importance of nodes in a network. Closeness centrality is measured by the reciprocal of the sum of the lengths of the shortest paths between a node and all other nodes [43]. The higher the closeness of a node, the closer the node to the “center” of the network. Betweenness centrality of a node is the sum of the fractions of all node-pairwise shortest paths that pass through the node [44]. All of these properties could affect the generated structural contexts of nodes.

5.2 Factor Analysis
Given an embedding model, plenty of training instances are generated by observing the stability of nodes with Monte Carlo simulations, which randomly adjust the algorithm parameters and network structure of the eight real-world networks. Explicitly, given a node vi, we encode both the algorithm and network properties into a feature vector xi. Then we calculate the stability Si of node vi based on the algorithm-network pair. A training instance is composed of the property feature vector xi and the calculated stability Si. We treat the feature vector xi as independent factors and regard the stability as a dependent variable. For each of the six algorithms, we create a training dataset and establish a separate regression model to investigate the significant influence factors. A training instance is generated as follows:

Step 1. We randomly select an algorithm from the six methods, and then randomly select a parameter set within proper ranges for the algorithm. The number of walks and walk length are selected in the range from 10 to 80, with an interval of 5. The parameters p and q for Node2vec are within the range from 1 to 3, with an interval of 0.1. The parameter k for Struc2vec ranges from 2 to 20. The parameter order for Line has three possible values 1, 2, and 3. The number of layers for SDNE and DNGR ranges from 2 to 5. The number of neurons is in the range from 100 to 600, with an interval of 50. The parameter k-step for DNGR ranges from 2 to 5. The dimension is selected from 10 to 300, with an interval of 10.

Step 2. We randomly select a network from the list of Table 2. We randomly sample the network with an edge sampling rate ranging from 0.5 to 1.0, with an interval of 0.1. This step aims to diversify network properties.

Step 3. We extract the global network properties of the sampled network and apply the algorithm to the network for T times.

Step 4. We randomly select 100 nodes from the network and extract the local properties of these nodes.

Step 5. We calculate the stability of the selected nodes. We construct the training instances by including both network and algorithm properties as features and the stability of nodes as a dependent variable.

We repeat the above procedure until we have a sufficiently large number of training instances. When the number of instances is greater than 20,000, the regression model outputs stable weights of influence factors as we add more training instances.

We employ the ridge regression model to fit the relationships between factors and stability [45]. A similar method has been adopted to examine the influence factors of word embedding stability [26]. Explicitly, assume that each instance has f features, and we have N training instances forming a feature vector matrix H∈RN×f. We standardize H with the Z-score method to eliminate the collinearity of different influence factors:
zij=hij−h¯jSj,(15)
View SourceRight-click on figure for MathML and additional features.where h¯j and Sj are the mean and standard deviation of the jth factor, respectively. Let y∈RN×1 be the vector of stability observations. The ridge regression fits a linear model with coefficients w∈Rf×1 to minimize a penalized residual sum of squares:
minw∥Zw−y∥22+α∥w∥22,(16)
View SourceRight-click on figure for MathML and additional features.where α≥0 is a complexity parameter that controls the amount of shrinkage: the larger the value of α, the greater the amount of shrinkage, and thus the coefficients become more robust to collinearity. The experimental results show that the selection of α has negligible impacts on the results. We report the results of α=0.5 for all algorithms. The coefficient of determination R2 score estimates the fitness of the regression model. The larger the value of R2, the higher the interpretation of the dependent variable by the independent variables. The absolute values of weights represent the effects of factors.

5.3 Results and Analysis
Table 3 shows the averages and variances of stability in the training datasets. The results demonstrate that DeepWalk and Node2vec have the highest average stability among the six approaches. Both algorithms exploit the random walk model to generate structural contexts of nodes. Struc2vec adopts a different random walk model to estimate the relational information amongst nodes and achieves moderate average stability. LINE has the lowest average stability amongst all methods. One possible reason is that LINE only utilizes the first and second-order relations of nodes, resulting in losing more useful structural information. For deep neural network-based algorithms, SDNE has higher average stability than DNGR.

TABLE 3 A Summary of the Training Datasets
Table 3- 
A Summary of the Training Datasets
All methods except LINE have a R2 score higher than 0.3, indicating that the regression has learned a linear model that can reasonably fit the training data. In other words, there are significant correlations between the explored factors and stability. DeepWalk, Node2vec, and Struc2vec have much higher R2 scores than other approaches. We investigate the relative effects of factors by normalizing the weights with the maximum one and ordering the influence factors by their absolute values of weights [26].

Since the training datasets are generated based on a limited number of networks, we also verify the generalization of the above conclusions. We remove the instances generated by a specific network (e.g., Wiki-Vote) alternately and repeat the experiments. We obtain similar orders of top influence factors with slight fluctuations.

5.3.1 Impacts of Network Properties
Fig. 11 shows the orders of influence factors for different algorithms. To make the analysis simple, we first focus on the top factors of these methods. Overall, it is shown that different kinds of methods have common significant influence factors. The top influence factors are mainly global network properties such as the number of nodes and edges, network density, average node degree, average clustering coefficient, assortativity, and transitivity. It is worth noting that some of these network properties correlate with one another. For example, for networks with similar numbers of nodes, a higher average node degree means a higher network density. It indicates that the entire network structure significantly contributes to embedding instability. One possible interpretation of the effects of network structure is that network embedding methods sacrifice some relational information in the mapping from discrete representations of nodes to a low-dimensional vector space. For example, the structural contexts of nodes generated by random walks may lose some relational information of the network. Random walks on different network structures have various characteristics [40], leading to different losses of relational information. Our findings strongly suggest that it is vital to consider the effects of global network structure in the design and evaluation of embedding methods.

Fig. 11. - 
Orders of influence factors for different network embedding methods.
Fig. 11.
Orders of influence factors for different network embedding methods.

Show All

Some local network properties, such as the closeness of nodes, are also amongst the top influence factors. It indicates that the “positions” of nodes in the network structure also affect its embedding stability. These factors result in the diversity of node embeddings in the same network. However, local network properties such as node degree, betweenness centrality, and clustering coefficients have limited effects on embedding stability. This finding coincides with the observations that the same methods have similar stability distributions on artificial networks generated by different network models.

5.3.2 Impacts of Algorithm Properties
It is demonstrated in Fig. 11 that algorithm properties such as dimensions, number of walks, and walk length have moderate significant impacts on the stability of embeddings. This finding coincides with the claims of most researches that the performance of algorithms is robust to the small change of parameters [8].

Figs. 11a and 11b illustrate that DeepWalk and Node2vec have different orders of influence factors. However, we obtain the same orders of influence factors as presented in Fig. 11a when we fix the parameters p=q=1.0 for Node2vec (which is equivalent to DeepWalk) and generate another dataset of training instances. It indicates that the parameters p and q affect the orders of other influence factors. Fig. 12a shows the effects of the parameters p and q in Node2vec based on the Bio-celegans network. The results demonstrate that the stability of Node2vec decreases with the decrease of p and the increase of q. Node2vec would keep the walk “local” close to the starting nodes when p is small and q is large, that is, Node2vec leads to more substantial amounts of instability when it focuses more on local structures. Therefore, focusing more on local structure leads to more significant losses of relational information.

Fig. 12. - 
Effects of parameters on embedding stability.
Fig. 12.
Effects of parameters on embedding stability.

Show All

We also examine the impacts of the number of walks and walk length. Fig. 12b displays that both parameters have similar impacts on embedding stability. Specifically, small values of number of walks and walk length lead to significant amounts of instability. The stability reaches a constant approximately when the number of walks and walk length are greater than 40, suggesting that we should select proper values for parameters to obtain high embedding stability.

SECTION 6Implications for Downstream Tasks
Network embedding has a wide range of applications in various downstream tasks such as node classification, link prediction, and network alignment. In this section, we examine whether the instability of embedding algorithms affects the performance in downstream tasks. It is worth noting that the purpose of this work is to understand the instability of network embedding and its implications for downstream tasks, rather than comparing the performance of different algorithms.

6.1 Node Classification
Node classification refers to the problem of classifying nodes into two or more classes by the features of nodes. To solve this problem, we first obtain the feature vectors of nodes with network embedding methods. Then, we adopt the Support Vector Machine (SVM) method for node classification.

We conduct experiments based on the Wiki dataset [46], which contains 2,405 documents and 17,981 links between them. The dataset has 19 ground-truth classes of documents. We use 80 percent nodes as the training set and 20 percent nodes as the test set. We divide the embedding stability of nodes into six intervals and examine the percentage of nodes that are correctly classified. We repeat the experiments for 50 times and report the average accuracy to alleviate the performance fluctuations caused by embedding instability. Fig. 13 reveals that the average classification accuracy increases as the embedding stability rises, indicating that the higher the embedding stability, the higher the accuracy of the classification model. We also draw a bar-plot with an additional axis to show the percentages of nodes in each stability interval. Different colors in the bars illustrate the percentages of success and failure nodes in the downstream tasks. The overall classification accuracy rate is 0.657. Similar results are obtained for other methods and are omitted due to the limited space. We also conduct some case studies to show how the stability metric guides the parameter settings of existing models (See the supplementary material S2, available online).

Fig. 13. - 
Node classification accuracy versus stability
Fig. 13.
Node classification accuracy versus stability

Show All

6.2 Link Prediction
Link prediction refers to inferring the existence probability of an edge between two nodes that have not yet been connected, utilizing information such as known connected nodes and network structure. Given a network G, we randomly remove 20 percent links of G as the test set Etest and train an embedding model on the remaining 80 percent links. Then we calculate the Cosine similarity between the two nodes of each link in Etest [47]. We transform the similarity to existence probability of an edge as:
pe=1+cos(xi,xj)2,(17)
View SourceRight-click on figure for MathML and additional features.where e=(i,j), and xi, xj are the feature vectors of nodes i and j, respectively. The closer the probability approximates to 1, the higher the prediction accuracy is. Then we calculate the prediction error of each edge in the test set Etest as follows:
erre=|1−pe|,e∈Etest.(18)
View SourceRight-click on figure for MathML and additional features.

We conduct experiments based on the Jazz network [48] and calculate the stability of the nodes in Etest. Fig. 14 shows the results obtained by Node2vec. The x-axis and y-axis are the stabilities of the two nodes in a test link, respectively. The node with higher stability is associated with the y-axis (S2>S1). Each point is the average prediction error of test links falling in that grid. The results demonstrate that the average prediction errors decrease as the stability increases, indicating that the higher stability of network embedding, the lower the prediction errors. Similar results are observed for other methods.

Fig. 14. - 
Absolute prediction errors versus stability
Fig. 14.
Absolute prediction errors versus stability

Show All

6.3 Network Alignment
Structural network alignment is aimed to find a bijective mapping between the nodes of two networks based on their topological structures [12]. Several methods exploit graph embedding techniques for network alignment [12], [41]. Wang et al. [12] proposed a scheme named DeepMatching. The method first obtains the vector representations of nodes in the two matching graphs by network embedding algorithms and then maps the node vectors in the two embedding spaces algebraically. We refer readers to [12] for the detailed methods. We conduct experiments to evaluate the impacts of instability on the performance of network alignment. We use the Cit-HepPh network [34] for experiments. We first extract two networks G1 and G2 from the original network by sampling the edges with a fixed rate of 0.8 as that in [12]. Then, we apply an embedding method to G1 and G2 to generate two embedding spaces M1 and M2, respectively. DeepMatching is used to align the two networks based on M1 and M2. Finally, we calculate the embedding stability of each node in G1 and check whether the node is correctly matched.

We divide the embedding stability of nodes into six intervals and calculate the accuracy in each interval. We repeat the experiments for 50 times and report the average accuracy. Fig. 15 presents the experimental results based on the DeepWalk method [12]. The proportions of mismatched nodes are high when the stability is low. The proportions of correctly matched nodes increase as the stability of node embeddings increases; that is, the alignment accuracy is improved relative to the embedding stability. The results illustrate that the instability of network embedding remarkably impacts the performance of network alignment. We have similar results with the Node2vec method.

Fig. 15. - 
Network alignment accuracy versus stability
Fig. 15.
Network alignment accuracy versus stability

Show All

SECTION 7Discussion
7.1 Towards Stable Network Embedding
Instability is fundamentally introduced by the two phases of network embedding, including node proximity estimation and dimensionality reduction. An appropriate node proximity estimation model is expected to include as much structural and attributional information as possible. It is usually challenging to accurately compute high-order structural proximities because the computational cost rapidly increases versus the orders of node proximity. Therefore, many feasible methods are proposed to estimate structural proximity with sampling schemes, such as random walks [7], [8] and random surfing methods [14], which introduce some variances in the embedding space. Therefore, a principal research direction is to design an efficient node proximity computation method without introducing stochastic processes.

Recently, Qiu et al. [49] found that network embedding models such as DeepWalk, LINE, PTE, and node2vec are essentially performing implicit matrix factorizations. Based on this observation, they proposed a matrix factorization framework NetMF which avoids the random operations such as negative sampling and random initializations. Thus, NetMF achieves stable embeddings with improved performance. However, the algorithm is prohibitively expensive in terms of both time and space, making it not scalable for large-scale networks. To address this issue, NetSMF is proposed to improve the computational efficiency with sparse matrix factorization [50]. NetSMF adopts the randomized singular value decomposition (SVD) for efficient sparse matrix factorization, which sacrifices some accuracy performance in downstream tasks and introduces some variances [50]. Zhang et al. [51] proposed an arbitrary-order proximity preserved embedding method based on an SVD-based framework. They presented a scalable decomposition solution to derive the embedding vectors and shift them between proximities of arbitrary orders. The proposed method has a linear time complexity concerning network size. The above analyses indicate that SVD-based approaches are feasible ways to generate stable embeddings. However, new schemes must be designed to tackle the efficiency problem.

Another cause of embedding instability is the optimization schemes such as negative sampling used in the Skip-Gram model and the stochastic gradient descent methods used in deep neural networks in the phase of dimensionality reduction. These methods improve computational efficiency by adopting stochastic processes, which result in significant variances in the latent space. It is the tradeoff between computational efficiency and embedding effectiveness that leads to the instability of network embedding in this phase. Some researchers have proposed to improve the performance by feeding the models with high-quality negative samples [52], [53], [54]. However, these approaches still suffer from embedding instability because they do not change the base models.

It is worth noting that some recent works suggest using Gaussian distributions as the hidden representations of nodes to enhance the robustness of node embeddings [55], [56]. The mean vectors represent the position of nodes in the embedding space, and the variance terms reflect the uncertainty of nodes. This model captures the uncertainty of data, such as the possible contradiction between neighboring nodes and uncertain edge generation due to the multi-faceted human behavior [56]. The idea is also adopted to defend graph convolutional networks against adversarial attacks [57]. Different from these works, we focus on the inherent instability of the embedding model itself, i.e., one gets discrepant results even if the same model is applied to the same networks multiple times. We also examine the stability of these models and find that the Gaussian embedding models also exhibit significant amounts of instability.

An arising issue is the adversarial attacks to network embedding models based on the perturbations of network structure [58], [59], [60]. Zügner et al. [58] have shown that adversarial perturbations of the connections of nodes would significantly reduce the accuracy of neural network-based methods in the semi-supervised task of node classification. Some researchers have proposed to use adversarial learning to improve the robustness of network embedding [61]. Compared to these works, the embedding instability problem is studied in the scenarios of static networks, which is stricter than the adversarial attack problem. A good understanding of network embedding instability provides insights into assessing the vulnerability and improving the robustness of network embedding methods.

7.2 Directions of Theoretical Analysis
Recent studies have found that network embedding models are essentially factorizing a proximity matrix [62]:
M≈XX′,(19)
View SourceRight-click on figure for MathML and additional features.where M∈Rn×n is some matrix containing the proximity information of nodes estimated by context inference models such as random walks; X∈Rn×d and X′∈Rd×n are embedding matrix and context matrix, respectively; n is the number of nodes, and d is the embedding dimensionality. This observation provides us a feasible way to analyze the instability of network embedding theoretically. Formally, we regard the proximity matrix of random-walk based network embedding models as a random variable due to the randomness of random walks. Denote M¯ as the expected proximity matrix, then
M=M¯+Δ,(20)
View Sourcewhere Δ represents the perturbations caused by random walks. Evaluating the influence of Δ on the eigenvalues and eigenvectors is a promising way to analyze the instability of node embeddings. This theoretical tool has been used to analyze the impacts of dimensionality on the quality of embeddings [63]. Moreover, the perturbed eigenpairs can be efficiently approximated by the first-order of the perturbation matrix Δ [64], [65], which provides a powerful tool for the theoretical analysis of embedding stability.

Qiu et al. [49] have derived the closed forms of the proximity matrices that unify DeepWalk, LINE, PTE, and node2vec into the matrix factorization framework. For example, it has been shown that DeepWalk is implicitly factorizing a random matrix that converges in probability to a shifted pointwise mutual information (PMI) matrix [49]. Therefore, we can formally formulate the expected proximity matrix M¯. Unfortunately, it is still an open problem about how random walks affect the perturbation matrix Δ, which is an important research direction for theoretical analysis of network embedding instability.

An important side-effect of the matrix factorization implication is that embeddings are arbitrarily rotated by an orthogonal matrix [64], without impacting the objective:
XQTQXc=XXc≈M,(21)
View SourceRight-click on figure for MathML and additional features.where Q∈Rd×d is an arbitrary orthogonal matrix, i.e., QTQ=I. Since the embeddings are unconstrained, and the only error signal comes from the orthogonally-invariant objective, the entire embedding space is free to rotate arbitrarily during training. However, the rotation variance is not problematic because it does not affect the relative positions of nodes in the embedding space.

SECTION 8Conclusion
In this paper, we propose a framework to investigate the instability of network embedding. We first measure the geometric characteristics with two metrics and find that node embeddings have significant variance in the embedding space. Then, we formally define the stability of network embedding as the invariance of the nearest neighbors of nodes in different embedding spaces. We find that network embeddings have significant amounts of instability and employ the Beta distribution to characterize the stability distributions of node embeddings. It is revealed that both network structure and model parameters affect the stability of network embedding. Besides, we explore the influence factors of embedding instability. Compared with algorithm properties, network properties have more significant effects on the stability of network embedding. Global network properties such as the number of nodes and edges, average node degree, and network density are crucial factors affecting the instability of network embedding. Moreover, instability has remarkable impacts on the performance of network embedding in downstream applications. These findings strongly suggest that researchers should fully consider the impacts of network structure in the design of novel methods. An important research direction is to theoretically analyze the instability of network embedding based on the matrix perturbation theory.