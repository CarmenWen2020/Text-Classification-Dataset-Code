Multi-view data are becoming common in real-world applications and many multi-view clustering algorithms have thus been proposed. The existing algorithms usually focus on the cooperation of different visible views in the original space but neglect the influence of the hidden information among these visible views, or they only consider the hidden information among the views. The algorithms are therefore not efficient since the available information is not fully exploited, particularly the otherness information in different views and the consistency information among them. In practice, the otherness and consistency information in multi-view data are both very useful for effective clustering analyses. In this study, a Multi-View clustering algorithm with the Cooperation of Visible and Hidden views, i.e., MV-Co-VH, is proposed. The MV-Co-VH algorithm first projects the multiple views from different visible spaces to the common hidden space by using non-negative matrix factorization to obtain the common hidden view data. Collaborative learning is then implemented in the clustering procedure based on the visible views and the shared hidden view. The experimental results of extensive experiments on UCI multi-view datasets and real-world image multi-view datasets show that the clustering performance of the proposed algorithm is competitive with or even better than that of the existing algorithms.
SECTION 1Introduction
Clustering is a process of dividing data samples into different groups. As a kind of important data processing technique, clustering has a broad range of applications, including data mining, image processing and pattern recognition. Many traditional clustering algorithms have been established and widely used, such as k-means [1], [2], [3], fuzzy c-means [4], [5], [6], DBSCAN [7], spectral clustering [9], [10] and collaborative clustering [37], [38]. However, the data collected for clustering analysis are becoming more and more complex with the advancement of media technology, which poses great challenges to the traditional clustering methods. A typical issue is that datasets can be described by different sets of attributes, i.e., multi-view datasets. Multi-view datasets are indeed common in real-world applications. For example, a document can be translated into two languages; an image can be represented by different feature descriptors. Traditional clustering methods mainly deal with the single-view data. Multi-view data are usually concatenated as a series of single-view data along the feature dimension when traditional clustering methods are used to process the data [11], [12], where the connections between different views are ignored under such strategies, thereby exhibiting poor clustering performance.

To meet with above challenges, multi-view clustering technologies specially designed for multi-view data are required. Different from the traditional clustering algorithms, multi-view clustering methods can utilize the connections among different views to improve the performance. Based on the literature, there are several criteria that can be used for classifying the existing multi-view clustering algorithms [16], [39]. In this paper, multi-view clustering methods are categorized into two classes according to the nature of the information used to achieve the final clustering results, i.e., the visible-view-based methods and hidden-view-based methods. The visible-view-based methods usually cluster multi-view data with a process that contains one stage only and the results are obtained from the visible views. The hidden-view-based methods are usually a two-stage process and the results are obtained from the derived hidden view, i.e., the data in the common feature space extracted from the multiple views.

The visible-view-based methods refer to the methods that improve the k-means, fuzzy c-means (FCM), or spectral clustering of the corresponding multi-view versions. In these methods, parameters or terms are often introduced to the objective function to control the interactions among different views and the results can be obtained by optimizing the objective function directly. The clustering process is illustrated in Fig. 1. In [37], Pedrycz proposed collaborative clustering based on FCM (CoFC) for multi-view data by introducing a family of collaborative fuzzy matrices, where the interaction coefficients in the collaborative matrices describe the intensity of interaction between the views. Similarly, Cleuziou et al. [16] developed the multi-view collaborative fuzzy clustering algorithm (CoFKM). Unlike CoFC where each view has a separate objective function, CoFKM is derived from the multi-view co-training algorithm Co-EM and the objective function is coupled for all views. To reward more crisp membership degree for the samples, an improved penalty term originated from the fuzzy c-means clustering algorithm with improved fuzzy partitions (GIFP-FCM) is proposed and the corresponding multi-view version Co-FCM [17] is developed based on the idea of CoFC. Co-FCM implicitly assumes that each view is treated equivalently. Based on the fact that the importance of each view is different in the process of clustering, view weights are introduced into Co-FCM to derive the multiple weighted view Co-FCM algorithm (WV-Co-FCM) [17]. Variable weights are also introduced into multi-view clustering method two-level variable weighting clustering (TW-k-means) [13]. Besides, minimax optimization is also introduced into FCM based multi-view clustering and the algorithm MinimaxFCM [18] is derived. The algorithms aforementioned are all based on k-means or FCM which are primarily used for spherical shaped clustering. Another widely used method is spectral clustering which is applicable for arbitrary shaped clustering. Many multi-view versions of spectral clustering have also been developed. Inspired by co-training strategy in semi-supervised learning, Kumar et al. [40] used the eigenvectors of the Laplacian matrices in spectral clustering to search for consistent clusters across the views. Co-regularization technique is also used in multi-view spectral clustering to minimize the disagreement between the views [26]. To cope with data nonlinearity, Houthuys et al. introduced weighted kernel into multi-view clustering and developed the multi-view kernel spectral clustering algorithm (MVKSC) [29]. These algorithms use various techniques to capture the interactions between different views and the clustering results can be obtained by optimizing the objective function directly under the constraints of the interactions.

Fig. 1. - 
The framework of visible-view-based multi-view clustering methods. The dotted lines and blocks indicate the interactions between the multiple views.
Fig. 1.
The framework of visible-view-based multi-view clustering methods. The dotted lines and blocks indicate the interactions between the multiple views.

Show All

The hidden-view-based methods adopt a completely different idea to implement multi-view clustering. These methods decompose the multi-view clustering process into two stages. The first stage is to find a hidden feature space shared by the views and the second stage is to cluster the extracted common features in the hidden space. The clustering process is illustrated in Fig. 2. Several feature transformation techniques are used to find the hidden feature space for the multiple views, including sparse subspace clustering, nonnegative matrix factorization (NMF) and canonical correlation analysis (CCA). In [41], each view was represented by its subspace coefficient matrix. To get a consistent clustering result from multiple views, Yin et al. [42] proposed to share a common coefficient matrix by enforcing the coefficient matrices for each pair of views to be as similar as possible. The graph Laplacian can be constructed from the common coefficient matrix and then spectral clustering is applied on the graph Laplacian to get the final clustering results. Similar to [43] where each view is decomposed into a basis matrix and a coefficient matrix, NMF is another widely used technique of this kind. Liu et al. [44] proposed to learn a common representation matrix shared by all views and different coefficient matrices for each view based on NMF, where the common representation matrix was enforced to be equal to each coefficient matrix. Then, traditional clustering method was applied to the common representation matrix to get the final result. Different from the above decomposition techniques, feature projection can also be used to obtain the shared hidden features. Rasiwasia et al. [45] proposed to project the data of two views to a shared feature space based on CCA and then the clustering was performed on the projected data.

Fig. 2. - 
The framework of hidden-view-based multi-view clustering methods. The dotted block indicates the extracted features shared by multiple views.
Fig. 2.
The framework of hidden-view-based multi-view clustering methods. The dotted block indicates the extracted features shared by multiple views.

Show All

The two categories of algorithms discussed above have provided some feasible solutions for multi-view clustering, but many of them (e.g., [13], [14], [15], [16], [17], [18]) only focus on the information provided by the visible views, and ignore the shared hidden information between the views. Meanwhile, other multi-view clustering algorithms [19], [20], [21], [22], [23] only focus on mining the shared hidden information of different views. In fact, both the individual information of different visible views and the shared hidden information play an important role in the clustering process. How to effectively combine the individual information and the shared information is a challenging topic for multi-view clustering. To this end, a multi-view clustering algorithm developed by the cooperation of visible and hidden views is proposed in this study. The clustering process of the proposed algorithm MV-Co-VH is illustrated in Fig. 3. The main contributions of this paper are as follows:

A method that extracts the shared hidden view from multi-view data is proposed by using non-negative matrix factorization.

A multi-view clustering algorithm integrating both the visible and hidden views is proposed.

The performance of the proposed algorithm is evaluated on both UCI multi-view datasets and real-world multi-view datasets.

Fig. 3. - 
The framework of the proposed multi-view clustering with the cooperation of visible and hidden views.
Fig. 3.
The framework of the proposed multi-view clustering with the cooperation of visible and hidden views.

Show All

The rest of this paper is organized as follows. Section 2 briefly reviews K-means algorithm and non-negative matrix factorization. Section 3 first proposes a shared hidden view data extraction method for multi-view data, followed by the novel multi-view clustering algorithm MV-Co-VH with the cooperation of visible and hidden views. Section 4 evaluates the performance of the proposed algorithm by conducting extensive experiments on multi-view datasets and comparing it with many existing clustering algorithms. Section 5 gives the conclusions.

SECTION 2Related Work
Most of visible-view-based multi-view clustering methods are developed based on k-means whereas hidden view extraction methods are usually based on NMF. Therefore, to achieve the cooperation of visible and hidden views in multi-view clustering, k-means and NMF are adopted as the basic algorithms of the proposed method. They are introduced briefly in the following sections.

2.1 K-Means
K-means [1], [2], [3] is a classical clustering algorithm. Attributed to its simplicity and strong adaptive ability, k-means has been applied in various fields.

Given a dataset of N samples, the corresponding matrix can be expressed as X=[x1, x2,…,xN], where xj∈Rd,j=1,2,…,N. Taking Euclidean distance as the similarity measure, data samples are clustered into C (2≤C≤N) clusters. The cluster centers are represented by matrix Z=[z1, z2,…,zC] with zi∈Rd,i=1,2,…,C and the partitions are represented by matrix U=[uij]∈RC×N, where the value of uij is either zero or one, with uij=1 indicating that the sample j is clustered into cluster i.

The objective function of the classical k-means algorithm is defined as
P(U,Z)=∑i=1C∑j=1Nuij∥xj−zi∥2s.t.∑i=1Cuij=1, uij∈{0,1}, j=1,2,…,N.(1)
View SourceRight-click on figure for MathML and additional features.

By solving the above objective function, the iterative equations of the partition and the cluster center are given as follows. The fuzzy membership can be calculated as
uij={1, Di≤Dj0, i≠j,(2)
View SourceRight-click on figure for MathML and additional features.where Di=||xj−zi||2 and the cluster centers can be calculated as
zi=∑j=1Nuijxj/∑j=1Nuij.(3)
View Source

2.2 Non-Negative Matrix Factorization
Non-negative matrix factorization [24], [25] is a dimension reduction technology that has been widely used in different fields in the past decades, e.g., pattern recognition and image engineering. The dimension of non-negative dataset can be reduced through NMF. Given a non-negative data matrix X=[x1,x2,…,xN] with m dimensional features and N samples, NMF aims to obtain two non-negative matrix factors W∈Rm×r+ and H∈Rr×N+, so that X≈WH, i.e., their product can approximatively represent X, where r denotes the reduced dimension, W and H denote the basis matrix and coefficient matrix respectively. Therefore, NMF can be transformed to the optimization problem below,
minW,H∥X−WH∥2F s.t. W≥0,H≥0,(4)
View SourceRight-click on figure for MathML and additional features.where ∥⋅∥F represents the Frobenius norm.

Eq. (4) can be solved by the strategy in [26], and the update rules are as follows.
Hi,j=Hi,j(WTX)i,j/(WTWH)i,j(5a)
View Source
Wi,j=Wi,j(XHT)i,j/(WHHT)i,j(5b)
View SourceRight-click on figure for MathML and additional features.

NMF is commonly used in clustering analyses in recent years. For example, a document clustering method was proposed based on NMF to obtain the terminology document matrix [26]. A graph regularization non-negative matrix factorization algorithm called GNMF was proposed in [27], the geometric information of the data was encoded by constructing an affinity graph to perform NMF with respect to the graph structure.

SECTION 3Multi-View Clustering With the Cooperation of Visible and Hidden Views
There are two key issues in multi-view learning are: 1) how to make full use of the otherness information between the different views of the multi-view datasets, and 2) how to completely discover the consistency information between the different views. It can be seen from the last two sections that the existing multi-view learning methods usually only focus on the information of the visible views in the multi-view data, i.e., the otherness information is exploited more than the consistency information; or they only use the shared information for clustering, i.e., only the consistency information is used and the otherness information between different views is ignored.

SECTION Algorithm 1:SHD-NMF
Input: A multi-view dataset X={X1,X2,…,XK} with K visible views, the kth visible view Xk=[xk1,xk2,…,xkN]∈Rmk×N, the desired reduced dimension r, parameter λ.

Output: mapping matrices {W1,W2,…,WK} and the shared hidden view data H.

Procedure

Normalize each view Xk (1≤k≤K)

Initialize Wk, H, and qk (1≤k≤K) randomly

Repeat

Update Wk by (8)

Update H by (10)

Update qk by (12)

Until (6) converges

In this section, a multi-view clustering algorithm with the cooperation of visible and hidden views, called MV-Co-VH, is proposed. The algorithm not only makes full use of the information of the visible views, but also the hidden information shared among different visible views. The framework of the MV-Co-VH is shown in Fig. 3. The algorithm not only achieves collaborative learning between visible views, but also collaborative learning between visible and hidden views. That is, the proposed MV-Co-VH algorithm utilizes the otherness and consistency information of the multi-view data simultaneously.

3.1 Shared Hidden View Extraction From Multi-View Data
For multi-view data, it is reasonable to assume that there is a hidden space shared by different visible views, and that the data of the different visible views can be generated from the shared hidden space data. NMF can decompose the data of each view into a basis matrix and a coefficient matrix, where the coefficient matrix can be viewed as low-rank representations of the data. Therefore, the coefficient matrices decomposed from multiple views can be enforced to be consistent. Then the consistent coefficient matrix extracted from the multiple views can be treated as the shared hidden view. In practical applications, the importance of each view is usually different. Weights are thus introduced to control the importance of different views, which should be adjusted adaptively for different views. Based on the above analyses, we present a method to show how to extract the shared hidden view data based on NMF and maximum entropy mechanism in this subsection.

Given a multi-view dataset X={X1,X2,…,XK}, the kth visible view is represented as the matrix Xk=[xk1,xk2,…,xkN]∈Rmk×N, where N is the number of samples, K is the number of visible views, and mk is the number of features of the kth visible view. Since the elements in Xk may be negative, to perform NMF, we first normalize Xk so that the elements in Xk are non-negative. Let H=[h1,h2,…,hN]∈Rr×N be the enforced consistent coefficient matrix extracted from the multi-view data in the shared hidden space and Wk∈Rmk×r be a basis matrix that can map the data H in the shared hidden space to the kth view. Then the shared hidden view extraction from the multi-view data can be formulated as follows,
minO(Wk,H,qk)=∑k=1Kqk∥∥Xk−WkH∥∥2F+λ∑k=1Kqklnqks.t.∑k=1Kqk=1, qk,Wk,H≥0,(6)
View SourceRight-click on figure for MathML and additional features.where the vector q=[q1,q2,…,qK] is a set of weights, qk is the weight of the kth visible view, and λ≥0 is a regularization coefficient.

The first term in (6) indicates the weighting empirical loss of NMF for the multi-view data. The second term corresponds to the regularization term of the adaptive weights of the multiple views based on the maximum entropy mechanism. From the definition of maximum entropy, the following conclusion can be obtained: when λ→∞, the weights of the multiple views tend to be equal; when λ→0, the weights associated with the more important views become larger while the weights associated with the less important views become smaller. Thus, the weights can be adaptively adjusted by setting an appropriate value of λ with some strategies, such as cross validation.

Eq. (6) can be optimized by iterative strategy. The solution process can be divided into three steps: (i) optimizing Wk by fixing H and q, (ii) optimizing H by fixing Wk and q, and (iii) optimizing q by fixing H and Wk. The details of the optimization are as follows.

Optimizing Wk by fixing H and q

When H=H^ and q=q^, the optimization problem is given by
minWk∑k=1Kqk∥∥Xk−WkH∥∥2F+λ∑k=1Kqklnqk.(7)
View SourceRight-click on figure for MathML and additional features.

Hence, Wk can be updated using the strategy in [25] and the update rule is as follows.
(Wk)i,j=[(XkHT)i,j/(WkHHT)i,j](Wk)i,j.(8)
View Source

Optimizing H by fixing Wk and q

When Wk=W^k and q=q^, the optimization problem is given by
minH∑k=1Kqk∥∥Xk−WkH∥∥2F+λ∑k=1Kqklnqks.t.H≥0.(9)
View SourceRight-click on figure for MathML and additional features.

H can be updated using the strategy in [25] and the update rule is as follows.
Hi,j=Hi,j⋅∑k=1Kqk((Wk)TXk)i,j∑k=1Kqk((Wk)TWkH)i,j.(10)
View SourceRight-click on figure for MathML and additional features.

Optimizing q by fixing H and Wk

When H=H^ and Wk=W^k, the optimization problem is given by
minq∑k=1Kqk∥∥Xk−WkH∥∥2F+λ∑k=1Kqklnqk.(11)
View SourceRight-click on figure for MathML and additional features.

The update rule can be obtained using the Lagrange multiplier method as follows.
qk=exp(−1/λ∥∥Xk−WkH∥∥2F)∑h=1Kexp(−1/λ∥∥Xh−WhH∥∥2F).(12)
View SourceRight-click on figure for MathML and additional features.

Based on the above analyses, the algorithm for extracting the shared hidden space data from the multi-view data by using NMF, called SHD-NMF, is presented in Algorithm 1.

3.2 Objective Function of MV-Co-VH
Given a multi-view dataset X={X1,X2,…,XK}, the shared hidden view data of the visible views can be obtained by the SHD-NMF algorithm. Hence, the proposed MV-Co-VH algorithm can be developed based on the cooperation of the visible and hidden views obtained. The objective function of MV-Co-VH is defined as follows.
minJ(U,V,V~,w)=λ∑i=1C∑j=1Nuij∥hj−v~i∥2 +(1−λ)∑k=1Kwk∑i=1C∑j=1Nuij∥∥xkj−vki∥∥2 +η∑k=1Kwklnwks.t.⎧⎩⎨⎪⎪⎪⎪⎪⎪⎪⎪∑i=1Cuij=1, uij∈{0,1}, 1≤j≤N∑k=1Kwk=1, 0≤wk≤1, H≥0,(13)
View SourceRight-click on figure for MathML and additional features.where U is the partition matrix with size C×N, whose elements uij is a binary value with uij=1 indicating that the sample j is clustered in cluster i. V={V1,V2,…,VK} is the clustering centers of K visible views, where Vk=[vk1,vk2,…,vkC] is the clustering center matrix of the kth visible view and vki is the center of cluster i of the kth visible view. V~=[v~1,v~2,…,v~C] is the clustering center matrix of the hidden view and v~i is the center of cluster i of the hidden view. w=[w1,w2,…,wK] is a set of weights of the visible views and wk is the weight assigned to the kth visible view. H=[h1,h2,…,hN]∈Rr×N is the shared hidden view of K visible views. The terms in (13) are further explained as follows.

The first and the second term are the sum of the within cluster dispersions of the hidden view and the sum of the within cluster dispersions of the visible views respectively. The parameter 0≤β≤1 is a collaborative learning coefficient used to control the effectiveness of the two terms. Through collaborative learning of these two terms, the final partition matrix can be obtained.

Algorithm 2: MV-Co-VH
Input: A multi-view dataset X={X1,X2,…,XK} with K visible views and the kth visible view Xk=[xk1,xk2,…,xkN]∈Rmk×N, reduced dimension r, the number of clusters C(2≤C≤N), convergence threshold ε, the number of current iterations t, maximum number of iterations T, parameters β, η.

Output: Final partition matrix U, cluster centers of different visible views vki, cluster centers of hidden view v~i, weights of visible views w=[w1,w2,…,wK].

Procedure

: Obtain the shared hidden view data H=[h1,h2,…,hN]∈Rr×N of the multi-view data by using the SHD-NMF algorithm.

Select the cluster centers of visible views vki(1≤i≤C,1≤k≤K), the cluster centers of hidden view v~i(1≤i≤C) and the random weights of visible views w=[w1,w2,…,wK].

while t≤T do

Update uij by (2) and (14)

Update cluster centers of different visible views vki by (15)

Update cluster centers of hidden view v~i by (16)

Update weights of visible views wk by (17)

Until ∥Jt+1−Jt∥<ε

In order to adaptively adjust the weight of each visible view, Eq. (13) introduces the Shannon entropy regularization term. Let ∑Kk=1wk=1 and wk≥0, by considering the weights of all visible views as a probability distribution, the Shannon entropy is expressed as −∑Kk=1wklnwk. The consistency of the weight of each view can then be obtained by minimizing the negative Shannon entropy ∑Kk=1wklnwk. The regularization parameter η is used to control the influence of the entropy.

3.3 Optimization for Objective Function
We can minimize (13) by solving the four subproblems below iteratively:

Problem P1: Set V=V^, V~=V~^ and w=w^, and solve the subproblem J(U,V^,V~^,w^);

Problem P2: Set U=U^, V~=V~^ and w=w^, and solve the subproblem J(U^,V,V~^,w^);

Problem P3: Set U=U^, V=V^ and w=w^, and solve the subproblem J(U^,V^,V~,w^);

Problem P4: Set U=U^, V=V^ and V~=V~^, and solve the subproblem J(U^,V^,V~^,w).

The problem P1 can be solved with (2) while Di has a different form which is given by.
Di=λ∥hj−v~i∥2+(1−λ)∑k=1Kwk∥∥xkj−vki∥∥2,1≤i≤C.(14)
View SourceRight-click on figure for MathML and additional features.

The problems P2 and P3 can be solved as follows.
vki=∑j=1Nuijxkj/∑j=1Nuij(15)
View SourceRight-click on figure for MathML and additional features.
v~i=∑j=1Nuijhj/∑j=1Nuij.(16)
View Source

The solution to problem P4 can be obtained based on Theorem 1 below.

Theorem 1.
Assume U=U^, V=V^ and V~=V~^, the necessary conditions for minimizing J(U^,V^,V~^,w) is as follows.
wk=exp{−(1−λ)∗Dk/η}∑Kh=1exp{−(1−λ)∗Dh/η}.(17)
View SourceRight-click on figure for MathML and additional features.

Proof.
Refer to the objective function in (13), under the constraint ∑Kk=1wk=1, the following Lagrange function can be established.
L(wk,γ)=J(U^,V^,V~^,w)+γ(∑k=1Kwk−1).(18)
View SourceRight-click on figure for MathML and additional features.

By taking derivatives with respect to wk, and setting the derivatives to zero, the following equation can be obtained.
(1−λ)∑i=1C∑j=1Nuij∥∥xkj−vki∥∥2+η(lnwk+1)+γ=0.(19)
View Source

Furthermore, under the constraint that ∑Kk=1wk=1, (17) can be obtained and Dk can be represented as follows.
Dk=∑i=1C∑j=1Nuij∥∥xkj−vki∥∥2.(20)
View SourceRight-click on figure for MathML and additional features.

Based on the above analyses and discussions, the process of the proposed MV-Co-VH method is summarized Algorithm 2.

SECTION 4Experimental Study
The proposed method is evaluated using the synthetic datasets and real-world datasets in Section 4.2 and 4.3 respectively. To further verify the usefulness of the proposed method, the clustering results are used in the multi-view TSK-FS classifier in Section 4.7. The sensitivity of the parameters and the convergence of the proposed method, and the effectiveness of the hidden information are analyzed in the Section 4.4 and 4.5. The statistical significance of the performance improvement achieved with the proposed method is examined in Section 4.6.

4.1 Experiments Settings
4.1.1 Comparison Algorithms and Parameter Settings
In the experiments, the proposed MV-Co-VH algorithm was compared with six related clustering algorithms, including the baseline algorithm FCM [4], four visible-view-based methods, i.e., WV-Co-FCM [17], Co-FKM [16], CombKM [28] and MVKSC [29], and the hidden-view-based method MultiNMF [21]. For single-view clustering algorithms, i.e., FCM and CombKM, single-view datasets were constructed directly by combining the features from all the visible views.

The setting of the parameters in the algorithms are as follows. The optimal fuzzy index in all the FCM-based methods was searched from {1.05,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2.0}. The optimal collaborative learning parameter among multiple views in all the collaboration-based methods was searched from the grid [0,(K−1)/K] with a step size of 0.05. All the regularization parameters in the methods were optimally searched from the grid {2−6,2−5,⋯,25,26}, whereas the optimal kernel width in the MVKSC was also searched from the same grid. The dimensionality of the hidden view r in the proposed method was searched from the grid [0.1d,0.2d,⋯,d], where d is the minimum dimensionality of all visible views. The range of r was searched from the grid [1,d] with a step size of 1.

4.1.2 Performance Measures
The performance of the aforementioned clustering algorithms was evaluated using three performance indices, i.e., normalized mutual information (NMI) [2], [30], rand index (RI) [30], [31] and precision [32], which are defined as follow.

Normalized Mutual Information (NMI)
NMI=∑i=1C∑j=1Cni,jlogN×ni,jni×nj(∑i=1CnilogniN)×(∑j=1CnjlognjN−−−−−−−−−−−−−−−−−−−−−−−√)(21)
View SourceRight-click on figure for MathML and additional features.

Rand Index (RI)
RI=f00+f11N(N−1)/2(22)
View SourceRight-click on figure for MathML and additional features.

Precision
Precision=f11f00+f11(23)
View Source

Here, ni,j is the number of data points belonging to class i and cluster j, ni is the number of data points in class i, nj is the number of data points in cluster j, f00 denotes the number of any two data points belonging to two different clusters, f11 denotes the number of any two data points belonging to the same cluster, and N is the total number of samples in dataset. The values of these three indices were all within the interval [0, 1]. The higher the value, the better the clustering performance.

In the experiments, the appropriate settings of the parameters in the algorithms were determined by grid search strategy as discussed above. With the optimal parameters, the algorithms were executed ten times, and the performance was reported in terms of the mean and standard deviation (SD) of NMI, RI and Precision. In addition to the above three performance indices, the computing overhead of all the algorithms is also reported with the form of runtime.

4.2 Synthetic Data
Experiments on synthetic data were conducted to examine the effectiveness of the proposed method in improving the clustering performance by exploiting both visible and hidden information. For all hidden-view-based methods, it was assumed that data from multiple views were generated from a common representation. Furthermore, the common representation was assumed to be generated from two Gaussian distributions, where the positive samples were generated from the two-dimensional Gaussian distribution with mean [6,7] and covariance [1,0;0,1], whereas the negative samples were generated from the two-dimensional Gaussian distribution with mean [12,9] and covariance [1,0;0,1]. The generated data are illustrated in Fig. 4 and were treated as the hidden view. It can be seen that the samples in Fig. 4 are highly separable. The measures of NMI and RI of clustering algorithm FCM on the samples were both equal to 1, which also verified the above statement experimentally.

Fig. 4. - 
The data of the hidden view generated from two Gaussian distributions
Fig. 4.
The data of the hidden view generated from two Gaussian distributions

Show All

To construct a multi-view dataset from the hidden view data in Fig. 4, two transformation matrices were constructed. The transformation matrix for the first view is W1=[0.1,4;0.3,2] and that for the second view is W2=[1,2;1,0.1]. Denote the hidden view data as H∈R2×n. The transformed multi-view data can then be represented as X1=W1H and X2=W2H. The transformed multi-view data are illustrated in Fig. 5. It can be seen from Fig. 5 that the separability of the first view is not as good as that of the second view. The proposed method SHD-NMF was adopted to extract the hidden view data from the multi-view dataset [X1,X2]. FCM was then used to cluster the extracted hidden view data H^ directly and the performance NMI and RI were both equal to 1 as well, which means that the hidden view data were perfectly reconstructed with sufficient information for clustering partition from the multi-view data.

Fig. 5. - 
The multi-view data transformed from the hidden space.
Fig. 5.
The multi-view data transformed from the hidden space.

Show All

However, perfect reconstruction was not possible in some situations. Unlike the constructed data in Fig. 5, some shifts were added to the hidden view data H before transformation. The shifts N1 and N2 were randomly generated from two Gaussian distributions. Then two views constructed can be expressed as follows.
X1=W1(H+N1)(24a)
View SourceRight-click on figure for MathML and additional features.
X2=W2(H+N2).(24b)
View SourceRight-click on figure for MathML and additional features.

The constructed noisy multi-view data are illustrated in Fig. 6. It can be seen that the separability of the data is significantly worse than that in Fig. 5. Since noisy shift was added into the data, the hidden view H^ could not be reconstructed perfectly as that in Fig. 5, i.e., the hidden view data did not suffice for obtaining ideal clustering result. In this situation, some valid information in the visible views were helpful for clustering, and the cooperation of hidden and visible views was necessary in such situations.

Fig. 6. - 
The multi-view data transformed from the hidden space with random shift.
Fig. 6.
The multi-view data transformed from the hidden space with random shift.

Show All

Table 1 shows the results on the data in Fig. 6 under three experimental settings, where ‘Visible Views’ denotes that FCM was directly used to cluster the data of each view and the best result was reported; ‘Hidden View’ denotes that FCM was directly used to cluster the hidden view data extracted by the SHD-NMF in Algorithm 1. Because of the added shift, the separability was degraded so that the clustering performance was much worse than that on the data in Fig. 5; and ‘Visible and Hidden Views’ denotes that the proposed algorithm was applied on the multi-view data in Fig. 6, and the results were obviously better among the three experimental settings.

TABLE 1 Experimental Results on Synthetic Data in Fig. 6
Table 1- 
Experimental Results on Synthetic Data in Fig. 6
In summary, the results show that the cooperation of visible and hidden information is very effective to enhance multi-view clustering performance when the hidden view cannot be perfectly reconstructed with the limited information from the visible views. In fact, many real-world datasets fall into the same situation in Fig. 6.

4.3 Real-World Data
The clustering performance of the proposed algorithm was evaluated experimentally on eight multi-view real-world datasets. Five datasets were obtained from the UCI repository, i.e., multiple features (MF), Statlog image segmentation (IS), Dermatology, Forest Type and water treatment plant (WTP). The other three datasets were obtained from the Caltech image multi-view dataset [26], the Corel image multi-view dataset [27] and the Reuters dataset. The Caltech dataset is an image dataset containing 101 classes, from which we selected three classes, i.e., Airplanes, Faces and Motorbikes, and preprocessed them into two-view data. The Corel dataset contains 34 classes of images, each class containing 100 pictures. We selected 10 classes with more prominent foreground and the images were represented with two views. The descriptions of these multi-view datasets are given in Table 2.

TABLE 2 Description of Eight Multi-View Datasets

The performance of the proposed algorithm was evaluated on the eight multi-view datasets and the results are shown in Tables 3, 4, and 5 in terms of the mean and SD of NMI, RI and Precision. The runtime of the algorithms is shown in Table 6. The following conclusions can be drawn from the experimental results.

The proposed MV-Co-VH algorithm exhibits the best performance on all the eight multi-view datasets.

The performance of the proposed MV-Co-VH algorithm is superior to that of the two single-view clustering algorithms FCM and CombKM algorithms. The results demonstrate that when the data samples are constructed by directly combining different views, the performance of these single-view clustering algorithms are not satisfactory.

The performance of the proposed MV-Co-VH algorithm is better than that of Co-FKM, WV-Co-FCM, MVKSC, and MinimaxFCM on the multi-view datasets. While Co-FKM, WV-Co-FCM, MVKSC and MinimaxFCM only use visible views for clustering, MV-Co-VH further considers the hidden space information and the performance is improved. Compared with MultiNMF that only uses hidden information, MV-Co-VH algorithm achieves better clustering performance by exploiting both visible and hidden information at the same time.

The computing overhead of the proposed method is superior to FCM, co-FKM, WV-co-FCM and multi-NMF, which inferior to CombKM and MVKSC. The most time-consuming algorithm is WV-co-FCM.

TABLE 3 Mean NMI of Algorithms on Eight DATASETS (SD in Bracket)
Table 3- 
Mean NMI of Algorithms on Eight DATASETS (SD in Bracket)
TABLE 4 Mean RI of Algorithms on Eight DATASETS (SD in Bracket)

TABLE 5 Mean Precision of Algorithms on Eight DATASETS (SD in Bracket)

TABLE 6 Mean Runtime (Second) of Algorithms on Eight Datasets
Table 6- 
Mean Runtime (Second) of Algorithms on Eight Datasets
The advantages of the proposed method on the dataset Reuters is not particularly significant when compared to that on the other datasets. This can be explained with reference to the analyses on the synthetic datasets as discussed above. Since the two views of the dataset Reuters, English Documents and French Documents, are translated from the same text content, the discrepancy of the two views is little and the information contained in them are very similar. This means that useful hidden information can already be reconstructed relatively completely from the visible views. Hence, the effectiveness of the collaboration between the visible and hidden views is not significant. For the other datasets, the information contained in different views is so different that useful hidden information cannot be reconstructed perfectly from the visible views, where the collaboration of the visible and hidden views is helpful.

4.4 Parameter and Convergence Analysis
In this section, the parameters of the proposed MV-Co-VH and the convergence of the algorithm is analyzed.

4.4.1 Parameter Analysis
The proposed MV-Co-VH algorithm was investigated to study the influence of the hidden view on the clustering performance on multi-view data. The investigation was conducted by analyzing the effect of the collaborative learning parameter β which can be adjusted to control the proportion of the visible and hidden views exploited in the clustering process. When β=0, MV-Co-VH only uses the information of the visible views; when β=1, MV-Co-VH only uses the information of hidden view; when 0<β<1, MV-Co-VH utilizes both visible and hidden information to different extent. To investigate the effect of β on the clustering performance, we conducted experiments by fixing all the other variables and setting β to 11 different values respectively, i.e., 0,0.1,0.2,…,1. It is found that the performance trends, in terms of NMI, RI, and Precision, are similar for all the datasets. Hence, as an illustration, we present the analysis in terms of NMI only, and on three of the datasets, i.e., Multiple Features, Forest Type and Caltech.

Fig. 7 shows the variation of NMI with β on three datasets. It can be seen that the NMI values obtained by using only the visible views are not optimal. When the hidden view information is introduced, the clustering performance of the proposed MV-Co-VH algorithm become better than that using the visible views information only, and the performance can be significantly improved with an appropriate β. This indicates that the cooperation of visible and hidden information can enhance clustering performance effectively with an optimal β. However, the determination of the optimal value of β is still an open problem which deserves further study. In this paper, we used the grid search strategy to optimize this parameter.


Fig. 7.
Variation of the performance of MV-Co-VH with β.

Show All

4.4.2 Convergence Analysis
In the proposed MV-Co-VH algorithm, we use an alternate iteration strategy to minimize the objective function. Based on the Zangwill convergence theorem [33], [34], the value of the objective function is guaranteed to be monotonically decreasing. Therefore, theoretically speaking, the algorithm can converge under a convergence threshold, i.e., it converges when the change in the value of the objective function is smaller than a given threshold during the optimization process. Fig. 8 shows the variation of the value of the objective function of the proposed algorithm with the number of iterations on three datasets. For all cases, the algorithm can reach convergence after a certain number of iterations.

Fig. 8. - 
Convergence of MV-Co-VH.
Fig. 8.
Convergence of MV-Co-VH.

Show All

4.5 Effect of the Hidden Information
To verify whether the addition of hidden information can improve the performance of multi-view clustering, the clustering results of the proposed MV-Co-VH algorithm with and without the hidden information were compared. The results in terms of NMI are shown in Table 7, which indicate improvement in clustering performance on most datasets with the introduction of hidden information.

TABLE 7 Mean NMI of MV-Co-VH With and Without the Hidden Information

4.6 Statistical Analysis
To evaluate whether the performance improvement observed for the proposed MV-Co-VH algorithm is statistically significant, the Friedman test [35], [36], a non-parametric statistical analysis method, is used to analyze the experimental results of the seven algorithms for all the datasets. The significance level α was set to 0.05. If the p-value was less than α, the null hypothesis that all algorithms having the same performance was rejected, i.e., the performance of the algorithms was significantly different. In that case, the Holm's post-hoc test was then used to further evaluate whether the performance difference between the best algorithm and the other algorithms was significant. Since the performance trends in terms of NMI, RI and Precision were similar, the statistical analysis with NMI is only presented as an example. The results are shown in Table 8 and Table 9.

Table 8 shows the Friedman test results based on NMI. It is evident that there is significant difference in performance among the seven algorithms. Furthermore, it can be seen from the ranking, the lower the better, that the proposed MV-Co-VH algorithm is ranked first. The Holm's post-hoc tests were then conducted between MV-Co-VH and each of the other six algorithms, respectively, to further determine whether the performance improvement observed in MV-Co-VH (the best algorithm) is significantly better. The results shown in Table 9 indicate that there is significant difference in performance between MV-Co-VH and the other six algorithms.

TABLE 8 Friedman Test Analysis Based on NMI
Table 8- 
Friedman Test Analysis Based on NMI
TABLE 9 Holm's Post-Hoc Test Analysis Based on NMI
Table 9- 
Holm's Post-Hoc Test Analysis Based on NMI
4.7 Application in Multi-View Classification
The proposed method has shown great advantages over other multi-view clustering algorithms. In fact, it can also be used for other tasks, such as multi-view classification. To illustrate this superiority of the proposed method, the clustering results are used in the multi-view classification algorithm based on TSK-FS (MV-TSK-FS) [46] which has good interpretability and the ability to deal with multi-view datasets. The MV-TSK-FS algorithm trains multiple TSK fuzzy systems and each TSK-FS corresponds to a distinct view of the multi-view data. In MV-TKS-FS, the antecedent parts of each fuzzy system are obtained using the clustering results of the single view clustering algorithm FCM on the data of the corresponding view, while the consequent parts are calculated based on the data of all the views. To improve the quality of antecedent parts of fuzzy systems, the proposed method MV-Co-VH can be used to replace FCM in MV-TSK-FS, which yields a variant of MV-TSK-FS that is named MV-TSK-FS-VV in this study.

Hence, to evaluate the effectiveness of the proposed in classification, experiments were conducted to compare the performance of MV-TSK-FS and MV-TSK-FS-VV on two multi-view datasets. We used 10 percent data to train the multi-view classifier and the remaining data were used for test. The classification accuracies on test sets are illustrated in Table 10. It can be seen that the proposed MV-Co-VH can effectively improve the classification performance of MV-TSK-FS.

TABLE 10 The Classification Accuracies of Multi-View Classifier Using Different Clustering Algorithms

SECTION 5Conclusion and Future Work
In this paper, we propose the algorithm MV-Co-VH with the cooperation of visible and hidden views. We first develop a method to extract the shared hidden view using NMF. By introducing the hidden view into the clustering process, collaborative learning between the visible and hidden views is then implemented. Thus, the proposed algorithm not only utilizes the otherness information between different views, but also the consistency information among them. The experimental results on various multi-view datasets show that the proposed algorithm has better clustering performance than that of the existing algorithms. Further research will be conducted on two aspects: how to determine the optimal collaborative learning parameter, and how to optimize the hyper-parameters more efficiently.

