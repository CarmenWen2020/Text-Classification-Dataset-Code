Nonnegative matrix factorization (NMF) has been successfully applied in several data mining tasks. Recently, there is an increasing interest in the acceleration of NMF, due to its high cost on large matrices. On the other hand, the privacy issue of NMF over federated data is worthy of attention, since NMF is prevalently applied in image and text analysis which may involve leveraging privacy data (e.g, medical image and record) across several parties (e.g., hospitals). In this paper, we study the acceleration and security problems of distributed NMF. First, we propose a distributed sketched alternating nonnegative least squares (DSANLS) framework for NMF, which utilizes a matrix sketching technique to reduce the size of nonnegative least squares subproblems with a convergence guarantee. For the second problem, we show that DSANLS with modification can be adapted to the security setting, but only for one or limited iterations . Consequently, we propose four efficient distributed NMF methods in both synchronous and asynchronous settings with a security guarantee. We conduct extensive experiments on several real datasets to show the superiority of our proposed methods. The implementation of our methods is available at https://github.com/qianyuqiu79/DSANLS .
SECTION 1Introduction
Nonnegative matrix factorization (NMF) is a technique for discovering nonnegative latent factors and/or performing dimensionality reduction. Unlike general matrix factorization (MF), NMF restricts the two output matrix factors to be nonnegative. Specifically, the goal of NMF is to decompose a huge matrix M∈Rm×n+ into the product of two matrices U∈Rm×k+ and V∈Rn×k+ such that M≈UV⊤. Rm×n+ denotes the set of m×n matrices with nonnegative real values, and k is a user-specified dimensionality, where typically k≪m,n. Nonnegativity is inherent in the feature space of many real-world applications, where the resulting factors of NMF can have a natural interpretation. Therefore, NMF has been widely used in a branch of fields including text mining [1], image/video processing [2], recommendation [3], and analysis of social networks [4].

Modern data analysis tasks apply on big matrix data with increasing scale and dimensionality. Examples [5] include community detection in a billion-node social network, background separation on a 4K video in which every frame has approximately 27 million rows, and text mining on a bag-of-words matrix with millions of words. The volume of data is anticipated to increase in the ‘big data’ era, making it impossible to store the whole matrix in the main memory throughout NMF. Therefore, there is a need for high-performance and scalable distributed NMF algorithms. On the other hand, there is a surge of works on privacy-preserving data mining over federated data [6], [7] in recent years. In contrast to traditional research about privacy which emphasizes protecting individual information from single institution, federated data mining deals with multiple parties. Each party possesses its own confidential dataset(s) and the union of data from all parties is utilized for achieving better performance in the target task. Due to the prevalent use of NMF in image and text analysis which may involve leveraging privacy data (e.g, medical image and record) across several parties (e.g., hospitals), the privacy issue of NMF over federated data is worthy of attention. To address aforementioned challenges of NMF (i.e., high performance and privacy), we study the acceleration and security problems of distributed NMF in this paper.

First of all, we propose the distributed sketched alternating nonnegative least squares (DSANLS) for accelerating NMF. The state-of-the-art distributed NMF is MPI-FAUN [8], a general framework that iteratively solves the nonnegative least squares (NLS) subproblems for U and V. The main idea behind MPI-FAUN is to exploit the independence of local updates for rows of U and V, in order to minimize the communication requirements of matrix multiplication operations within the NMF algorithms. Unlike MPI-FAUN, our idea is to speed up distributed NMF in a new, orthogonal direction: by reducing the problem size of each NLS subproblem within NMF, which in turn decreases the overall computation cost. In a nutshell, we reduce the size of each NLS subproblem, by employing a matrix sketching technique: the involved matrices in the subproblem are multiplied by a specially designed random matrix at each iteration, which greatly reduces their dimensionality. As a result, the computational cost of each subproblem significantly drops.

However, applying matrix sketching comes with several issues. First, although the size of each subproblem is significantly reduced, sketching involves matrix multiplication which brings computational overhead. Second, unlike in a single machine setting, data is distributed to different nodes in distributed environment. Nodes may have to communicate extensively in a poorly designed solution. In particular, each node only retains part of both the input matrix and the generated approximate matrices, causing difficulties due to data dependencies in the computation process. Besides, the generated random matrices should be the same for all nodes in every iteration, while broadcasting the random matrix to all nodes brings severe communication overhead and can become the bottleneck of distributed NMF. Furthermore, after reducing each original subproblem to a sketched random new subproblem, it is not clear whether the algorithm still converges and whether it converges to stationary points of the original NMF problem.

Our DSANLS overcomes these problems. First, the extra computation cost due to sketching is reduced with a proper choice of the random matrices. Then, the same random matrices used for sketching are generated independently at each node, thus there is no need for transferring them among nodes during distributed NMF. Having the complete random matrix at each node, an NMF iteration can be done locally with the help of a matrix multiplication rule with proper data partitioning. Therefore, our matrix sketching approach reduces not only the computational overhead, but also the communication cost. Moreover, due to the fact that sketching also shifts the optimal solution of each original NMF subproblem, we propose subproblem solvers paired with theoretical guarantees of their convergence to a stationary point of the original subproblems.

To provide solutions to the problem of secure distributed NMF over federated data, we first show that DSANLS with modification can be adapted to this security setting, but only for one or limited iterations. Therefore, we design new methods called Syn-SD and Syn-SSD in synchronous setting. They are later extended to Asyn-SD and Asyn-SSD in asynchronous setting (i.e., client/server), respectively. Syn-SSD improves the convergence rate of Syn-SD, without incurring much extra communication cost. It also reduces computational overhead by sketching. All proposed algorithms are secure with a guarantee. Secure distributed NMF problem is hard in nature. All parties involved should not be able to infer the confidential information during the process. To the best of our knowledge, we are the first to study NMF over federated data.

In summary, our contributions are as follows:

DSANLS is the first distributed NMF algorithm that leverages matrix sketching to reduce the problem size of each NLS subproblem and can be applied to both dense and sparse input matrices with a convergence guarantee.

We propose a novel and specially designed subproblem solver (proximal coordinate descent), which helps DSANLS converge faster. We also discuss the use of projected gradient descent as subproblem solver, showing that it is equivalent to stochastic gradient descent (SGD) on the original (non-sketched) NLS subproblem.

For the problem of secure distributed NMF, we propose efficient methods, Syn-SD and Syn-SSD, in synchronous setting and later extend them to asynchronous setting. Through sketching, their computation cost is significantly reduced. They are the first secure distributed NMF methods for federated data.

We conduct extensive experiments using several (dense and sparse) real datasets, which demonstrates the efficiency and scalability of our proposals.

The remainder of the paper is organized as follows. Section 2 provides the background and discusses the related work. Our DSANLS algorithm with detailed theoretical analysis is presented in Section 3. Our proposed algorithms for secure distributed NMF problem in both synchronous and asynchronous settings are presented in Section 4. Section 5 evaluates all algorithms. Finally, Section 6 concludes the paper.

SECTION Algorithm 1.Two-Block Coordinate Descent: Framework of Most NMF Algorithms
Input: M

Parameter: Iteration number T

initialize U0≥0, V0≥0

for t=0 to T−1 do

Ut+1← update(M, Ut, Vt)

Vt+1← update(M, Ut+1, Vt)

return UT and VT

Notations. For a matrix A, we use Ai:j to denote the entry at the ith row and j-th column of A. Besides, either i or j can be omitted to denote a column or a row, i.e., Ai: is the ith row of A, and A:j is its jth column. Furthermore, i or j can be replaced by a subset of indices. For example, if I⊂{1,2,…,m}, AI: denotes the sub-matrix of A formed by all rows in I, whereas A:J is the sub-matrix of A formed by all columns in a subset J⊂{1,2,…,n}.

SECTION 2Background and Related Work
In Section 2.1, we first illustrate NMF and its security problem in a distributed environment. Then we elaborate on previous works which are related to this paper in Section 2.2.

2.1 Preliminary
2.1.1 NMF Algorithms
Generally, NMF can be defined as an optimization problem [9] as follows:
minU∈Rm×k+,V∈Rn×k+∥∥M−UV⊤∥∥F,(1)
View Sourcewhere ||X||F=(∑ijx2ij)1/2 is the Frobenius norm of X. Problem (1) is hard to solve directly because it is non-convex. Therefore, almost all NMF algorithms leverage two-block coordinate descent schemes (shown in Algorithm 1): they optimize over one of the two factors, U or V, while keeping the other fixed [10]. By fixing V, we can optimize U by solving a nonnegative least squares (NLS) subproblem:
minU∈Rm×k+∥∥M−UV⊤∥∥F.(2)
View SourceSimilarly, if we fix U, the problem becomes:
minV∈Rn×k+∥∥M⊤−VU⊤∥∥F.(3)
View SourceRight-click on figure for MathML and additional features.

Within two-block coordinate descent schemes (exact or inexact), different subproblem solvers are proposed. The first widely used update rule is Multiplicative Updates (MU) [9], [11]. MU is based on the majorization-minimization framework and its application guarantees that the objective function monotonically decreases [9], [11]. Another extensively studied method is alternating nonnegative least squares (ANLS), which represents a class of methods where the subproblems for U and V are solved exactly following the framework described in Algorithm 1. ANLS is guaranteed to converge to a stationary point [12] and has been shown to perform very well in practice with active set [13], [14], projected gradient [15], quasi-Newton [16], or accelerated gradient [17] methods as the subproblem solver. Therefore, we focus on ANLS in this paper.

2.1.2 Secure Distributed NMF
Secure distributed NMF problem is meaningful with practical applications. Suppose two hospitals A and B have different clinical records, M1 and M2 (i.e., matrices), for same set of phenotypes. For legal or commercial concerns, it is required that none of the hospitals can reveal personal records to another directly. For the purpose of phenotype classification, NMF task can be applied independently (i.e., M1≈U1V⊤1 and M2≈U2V⊤2). However, since M1 and M2 have the same schema for phenotypes, the concatenated matrix M=[M1,M2] can be taken as input for NMF and results in better user (i.e., patients) latent representations V1 and V2 by sharing the same item (i.e., phenotypes) latent representation U:
M=[M1M2]≈[UV⊤1UV⊤2]=U⋅[V⊤1V⊤2].(4)
View Source

Throughout the factorization process, a secure distributed NMF should guarantee that party A only has access to M1, U and V1 and party B only has access to M2, U and V2. It is worth noting that the above problem of distributed NMF with two parties can be straightforwardly extended to N parties. The requirement of all parties over federated data in secure distributed NMF is actual the so-called t-private protocol (shown in Definition 1 with t=N−1) which derives from secure function evaluation [18]. In this paper, we will use it to assess whether a distributed NMF is secure.

Definition 1 (t-private protocol).
All N parties follow the protocol honestly, but they are also curious about inferring other party's private information based on their own data (i.e., honest-but-curious). A protocol is t-private if any t parties who collude at the end of the protocol learn nothing beyond their own outputs.

Note that a single matrix transpose operation transforms a column-concatenated matrix to a row-concatenated matrix. Without loss of generality, we only consider the scenario that matrices are concatenated along rows in this paper.

Secure distributed NMF problem is hard in nature. First, party A needs to solve the NMF problem to get U and V1 together with party B. At the same time, party A should not be able to infer V2 or M2 during the whole process. Such secure requirement makes it totally different from traditional distributed NMF problem, whose data partition already incurs secure violation.

2.2 Related Work
In the sequel, we briefly review three research areas which are related to this paper.

2.2.1 Accelerating NMF
Parallel NMF algorithms are well studied in the literature [19], [20]. However, different from a parallel and single machine setting, data sharing and communication have considerable cost in a distributed setting. Therefore, we need specialized NMF algorithms for massive scale data handling in a distributed environment. The first method in this direction [21] is based on the MU algorithm. It mainly focuses on sparse matrices and applies a careful partitioning of the data in order to maximize data locality and parallelism. Later, CloudNMF [22], a MapReduce-based NMF algorithm similar to [21], was implemented and tested on large-scale biological datasets. Another distributed NMF algorithm [23] leverages block-wise updates for local aggregation and parallelism. It also performs frequent updates using whenever possible the most recently updated data, which is more efficient than traditional concurrent counterparts. Apart from MapReduce implementations, Spark is also attracting attention for its advantage in iterative algorithms, e.g., using MLlib [24]. Finally, there are implementations using X10 [25] and on GPU [26].

The most recent and related work in this direction is MPI-FAUN [5], [8], which is the first implementation of NMF using MPI for interprocessor communication. MPI-FAUN is flexible and can be utilized for a broad class of NMF algorithms that iteratively solve NLS subproblems including MU, HALS, and ANLS/BPP. MPI-FAUN exploits the independence of local update computation for rows of U and V to apply communication-optimal matrix multiplication. In a nutshell, the full matrix M is split across a two-dimensional grid of processors and multiple copies of both U and V are kept at different nodes, in order to reduce the communication between nodes during the iterations of NMF algorithms.

2.2.2 Matrix Sketching
Matrix sketching is a technique that has been previously used in numerical linear algebra [27], statistics [28] and optimization [29]. Its basic idea is described as follows. Suppose we need to find a solution x to the equation: Ax=b,(A∈Rm×n, b∈Rm). Instead of solving this equation directly, in each iteration of matrix sketching, a random matrix S∈Rd×m (d≪m) is generated, and we instead solve the following problem: (SA)x=Sb. Obviously, the solution to the first equation is also a solution to the second equation, but not vice versa. However, the problem size has now decreased from m×n to d×n. With a properly generated random matrix S and an appropriate method to solve subproblem in the second equation, it can be guaranteed that we will progressively approach the solution to the first equation by iteratively applying this sketching technique.

To the best of our knowledge, there is only one piece of previous work [30] which incorporates dual random projection into the NMF problem, in a centralized environment, sharing similar ideas as SANLS, the centralized version of our DSANLS algorithm. However, Wang et al. [30] did not provide an efficient subproblem solver, and their method was less effective than non-sketched methods in practical experiments. Besides, data sparsity was not taken into consideration in their work. Furthermore, no theoretical guarantee was provided for NMF with dual random projection. In short, SANLS is not same as [30] and DSANLS is much more than a distributed version of [30]. The methods that we propose in this paper are efficient in practice and have strong theoretical guarantees.

2.2.3 Secure Matrix Computation on Federated Data
In federated data mining, parties collaborate to perform data processing task on the union of their unencrypted data, without leaking their private data to other participants [31]. A surge of work in the literature studies federated matrix computation algorithms, such as privacy-preserving gradient descent [32], [33], eigenvector computation [34], singular value decomposition [35], [36], k-means clustering [37], and spectral clustering [38] over partitioned data on different parties. Secure multi-party computation (MPC) are applied to preserve the privacy of the parties involved (e.g., secure addition, secure multiplication and secure dot product) [39], [37]. These Secure MPC protocols compute arbitrary function among n parties and tolerate up to t<(1/2)n corrupted parties, at a cost Ω(n) per bit [40], [41]. These protocols are too generic when it comes to a specific task like secure NMF. Our proposed protocol does not incorporate costly MPC multiplication protocols while tolerates up to n-1 corrupted (static, honest but curious) parties. Recently, Kim et al. [6] proposed a federated method to learn phenotypes across multiple hospitals with alternating direction method of multipliers (ADMM) tensor factorization; and Feng et al. [7] developed a privacy-preserving tensor decomposition framework for processing encrypted data in a federated cloud setting.

SECTION 3DSANLS: Distributed Sketched ANLS
In this section, we illustrate our DSANLS method for accelerating NMF in general distributed environment.

3.1 Data Partitioning
Assume there are N computing nodes in the cluster. We partition the row indices {1,2,…,m} of the input matrix M into N disjoint sets I1,I2,…,IN, where Ir⊂{1,2,…,m} is the subset of rows assigned to node r, as in [21]. Similarly, we partition the column indices {1,2,…,n} into disjoint sets J1,J2,…,JN and assign column set Jr to node r. The number of rows and columns in each node are near the same in order to achieve load balancing, i.e., |Ir|≈m/N and |Jr|≈n/N for each node r. The factor matrices U and V are also assigned to nodes accordingly, i.e., node r stores and updates UIr: and VJr: as shown in Fig. 1a.

Fig. 1. - 
Partitioning data to $N$N nodes with node $r$r's data shaded.
Fig. 1.
Partitioning data to N nodes with node r's data shaded.

Show All

Data partitioning in distributed NMF differs from that in parallel NMF. Previous works on parallel NMF [19], [20] choose to partition U and V along the long dimension, but we adopt the row-partitioning of U and V as in [21]. To see why, take the U-subproblem (2) as an example and observe that it is row-independent in nature, i.e., the r-th row block of its solution UIr: is given by:
UIr:=argminUIr:∈R|Ir|×k+∥∥MIr:−UIr:V⊤∥∥2F,(5)
View SourceRight-click on figure for MathML and additional features.and thus can be solved independently without referring to any other row blocks of U. The same holds for the V-subproblem. In addition, no communication is needed concerning M when solving (5) because MIr: is already present in node r.

On the other hand, solving (5) requires the entire V of size n×k, meaning that every node needs to gather V from all other nodes. This process can easily be the bottleneck of a naive distributed ANLS implementation. As we will explain shortly, our DSALNS algorithm alleviates this problem, since we use a sketched matrix of reduced size instead of the original complete matrix V.

3.2 SANLS: Sketched ANLS
To better understand DSANLS, we first introduce the Sketched ANLS (SANLS), i.e., a centralized version of our algorithm. Recall that in Section 2.1.1, at each step of ANLS, either U or V is fixed and we solve a nonnegative least square problem (2) or (3) over the other variable. Intuitively, it is unnecessary to solve this subproblem with high accuracy, because we may not have reached the optimal solution for the fixed variable so far. Hence, when the fixed variable changes in the next step, this accurate solution from the previous step will not be optimal anymore and will have to be re-computed. Our idea is to apply matrix sketching for each subproblem, in order to obtain an approximate solution for it at a much lower computational and communication cost.

Specifically, suppose we are at the t-th iteration of ANLS, and our current estimations for U and V are Ut and Vt respectively. We must solve subproblem (2) in order to update Ut to a new matrix Ut+1. We apply matrix sketching to the residual term of subproblem (2). The subproblem now becomes:
minU∈Rm×k+∥∥MSt−U(Vt⊤St)∥∥2F,(6)
View Sourcewhere St∈Rn×d is a randomly-generated matrix. Hence, the problem size decreases from n×k to d×k. d is chosen to be much smaller than n, in order to sufficiently reduce the computational cost.1 Similarly, we transform the V- subproblem into:
minV∈Rn×k+∥∥M⊤S′t−V(Ut⊤S′t)∥∥2F,(7)
View SourceRight-click on figure for MathML and additional features.where S′t∈Rm×d′ is also a random matrix with d′≪m.

3.3 DSANLS: Distributed SANLS
Now, we come to our proposal: the distributed version of SANLS called DSANLS. Since the U-subproblem (6) is the same as the V-subproblem (7) in nature, here we restrict our attention to the U-subproblem. The first observation about subproblem (6) is that it is still row-independent, thus node r only needs to solve:
minUIr:∈R|Ir|×k+∥∥(MSt)Ir:−UIr:(Vt⊤St)∥∥2F.(8)
View SourceFor simplicity, we denote:
Atr≜(MSt)Ir:and Bt≜Vt⊤St,(9)
View Sourceand the subproblem (8) can be written as:
minUIr:∈R|Ir|×k+∥∥Atr−UIr:Bt∥∥2F.(10)
View SourceRight-click on figure for MathML and additional features.Thus, node r needs to know matrices Atr and Bt in order to solve the subproblem.

For Atr, by applying matrix multiplication rules, we get Atr=(MSt)Ir:=MIr:St. Therefore, if St is stored at node r, Atr can be computed without any communication. On the other hand, computing Bt=(Vt⊤St) requires communication across the whole cluster, since the rows of Vt are distributed across different nodes. Fortunately, if we assume that St is stored at all nodes again, we can compute Bt in a much cheaper way. Following block matrix multiplication rules, we can rewrite Bt as:
Bt=Vt⊤St=[(VtJ1:)⊤…(VtJN:)⊤]⎡⎣⎢⎢⎢StJ1:⋮StJN:⎤⎦⎥⎥⎥=∑r=1N(VtJr:)⊤StJr:.(11)
View SourceNote that the summand B¯tr≜(VtJr:)⊤StJr: is a matrix of size k×d and can be computed locally. As a result, communication is only needed for summing up the matrices B¯tr of size k×d by using MPI all-reduce operation, which is much cheaper than transmitting the whole Vt of size n×k.

Now, the only remaining problem is the transmission of St. Since St can be dense, even larger than Vt, broadcasting it across the whole cluster can be quite expensive. However, it turns out that we can avoid this. Recall that St is a randomly-generated matrix; each node can generate exactly the same matrix, if we use the same pseudo-random generator and the same seed. Therefore, we only need to broadcast the random seed, which is just an integer, at the beginning of the whole program. This ensures that each node generates exactly the same random number sequence and hence the same random matrices St at each iteration.

In short, the communication cost of each node is reduced from O(nk) to O(dk) by adopting our sketching technique for the U-subproblem. Likewise, the communication cost of each V-subproblem is decreased from O(mk) to O(d′k). The general framework of our DSANLS algorithm is listed in Algorithm 2.

Algorithm 2. Distributed SANLS on Node r
Input: MIr: and M:Jr

Parameter: Iteration number T

Initialize U0Ir:≥0, V0Jr:≥0

Broadcast the random seed

for t=0 to T−1 do

Generate random matrix St∈Rn×d

Compute Atr←MIr:St

Compute B¯tr←(VtJr:)⊤StJr:

All-Reduce: Bt←∑Ni=1B¯ti

Update Ut+1Ir: by solving minUIr:∥Atr−UIr:Bt∥

Generate random matrix S′t∈Rm×d′

Compute A′tr←(M:Jr)⊤S′t

Compute B¯′tr←(UtIr:⊤S′tIr:

All-Reduce: B′t←∑Ni=1B¯′ti

Update Vt+1Jr: by solving minVJr:∥A′tr−VJr:B′t∥

return UTIr: and VTJr:

3.4 Generation of Random Matrices
A key problem in Algorithm 2 is how to generate random matrices St∈Rn×d and S′t∈Rm×d′. Here we focus on generating a random St∈Rd×n satisfying Assumption 1. The reason for choosing such a random matrix is that the corresponding sketched problem would be equivalent to the original problem on expectation; we will prove this in Section 3.5.

Assumption 1.
Assume the random matrices are normalized and have bounded variance, i.e., there exists a constant σ2 such that E[StSt⊤]=IandV[StSt⊤]≤σ2 for all t, where I is the identity matrix.

Different options exist for such matrices, which have different computation costs in forming sketched matrices Atr=MIr:St and B¯tr=(VtJr:)⊤StJr:. Since MIr: is much larger than VtJr: and thus computing Atr is more expensive, we only consider the cost of constructing Atr here.

The most classical choice for a random matrix is one with i.i.d. Gaussian entries having mean 0 and variance 1/d. It is easy to show that E[StSt⊤]=I. Besides, Gaussian random matrix has bounded variance because Gaussian distribution has finite fourth-order moment. However, since each entry of such a matrix is totally random and thus no special structure exists in St, matrix multiplication will be expensive. That is, when given MIr: of size |Ir|×n, computing its sketched matrix Atr=MIr:St requires O(|Ir|nd) basic operations.

A seemingly better choice for St would be a subsampling random matrix. Each column of such random matrix is uniformly sampled from {e1,e2,…,en} without replacement, where ei∈Rn is the ith canonical basis vector (i.e., a vector having its ith element 1 and all others 0). We can easily show that such an St also satisfies E[StSt⊤]=I and the variance V[StSt⊤] is bounded, but this time constructing the sketched matrix Atr=MIr:St only requires O(|Ir|d). Besides, subsampling random matrix can preserve the sparsity of original matrix. Hence, a subsampling random matrix would be favored over a Gaussian random matrix by most applications, especially for very large-scale or sparse problems. On the other hand, we observed in our experiments that a Gaussian random matrix can result in a faster per-iteration convergence rate, because each column of the sketched matrix Atr contains entries from multiple columns of the original matrix and thus is more informative. Hence, it would be better to use a Gaussian matrix when the sketch size d is small and thus a O(|Ir|nd) complexity is acceptable, or when the network speed of the cluster is poor, hence we should trade more local computation cost for less communication cost.

Although we only test two representative types of random matrices (i.e., Gaussian and subsampling random matrices), our framework is readily applicable for other choices, such as subsampled randomized Hadamard transform (SRHT) [42], [43] and count sketch [44], [45]. The choice of random matrices is not the focus of this paper and left for future investigation.

3.5 Solving Subproblems
Before describing how to solve subproblem (10), let us make an important observation. As discussed in Section 2.2.2, the sketching technique has been applied in solving linear systems before. However, the situation is different in matrix factorization. Note that for the distributed matrix factorization problem we usually have:
minUIr:∈R|Ir|×k+∥∥MIr:−UIr:Vt⊤∥∥2F≠0.(12)
View SourceRight-click on figure for MathML and additional features.So, for the sketched subproblem (10), which can be equivalently written as:
minUIr:∈R|Ir|×k+∥∥(MIr:−UIr:Vt⊤)St∥∥2F,(13)
View SourceRight-click on figure for MathML and additional features.where the non-zero entries of the residual matrix (MIr:−UIr:Vt⊤) will be scaled by the matrix St at different levels. As a consequence, the optimal solution will be shifted because of sketching. This fact alerts us that for SANLS, we need to update Ut+1 by exploiting the sketched subproblem (10) to step towards the true optimal solution and avoid convergence to the solution of the sketched subproblem.

3.5.1 Projected Gradient Descent
A natural method is to use one step2 of projected gradient descent for the sketched subproblem:
Ut+1Ir:=max{UtIr:−ηt∇UIr:∥∥Atr−UIr:Bt∥∥2F∣∣UIr:=UtIr:, 0}=max{UtIr:−2ηt[UtIr:BtBt⊤−AtrBt⊤], 0},(14)
View Sourcewhere ηt>0 is the step size and max{⋅,⋅} denotes the entry-wise maximum operation. In the gradient descent step (14), the computational cost mainly comes from two matrix multiplications: BtBt⊤ and At,rBt⊤. Note that Atr and Bt are of sizes |Ir|×d and k×d respectively, thus the gradient descent step takes O(kd(|Ir|+k)) in total.

To exploit the nature of this algorithm, we further expand the gradient:
∇UIr:∥∥Atr−UIr:Bt∥∥2F=2[UIr:BtBt⊤−AtrBt⊤]=(9)2[UIr:(Vt⊤St)(Vt⊤St)⊤−(MIr:St)(Vt⊤St)⊤]=2[UIr:Vt⊤(StSt⊤)Vt−MIr:(StSt⊤)Vt].(15)
View SourceRight-click on figure for MathML and additional features.By taking the expectation of the above equation, and using the fact E[StSt⊤]=I, we have:
=E[∇UIr:∥∥Atr−UIr:Bt∥∥2F]=2[UIr:Vt⊤Vt−MIr:Vt]∇UIr:∥∥MIr:−UIr:Vt⊤∥∥2F,(16)
View SourceRight-click on figure for MathML and additional features.which means that the gradient of the sketched subproblem is equivalent to the gradient of the original problem on expectation. Therefore, such a step of gradient descent can be interpreted as a (generalized) stochastic gradient descent (SGD) [46] method on the original subproblem. Thus, according to the theory of SGD, we naturally require the step sizes {ηt} to be diminishing, i.e., ηt→0 as t increases.

3.5.2 Proximal Coordinate Descent
However, it is well known that the gradient descent method converges slowly, while the coordinate descent method, namely the HALS method for NMF, is quite efficient [10]. Still, because of its very fast convergence, HALS should not be applied to the sketched subproblem directly because it shifts the solution away from the true optimal solution. Therefore, we would like to develop a method which resembles HALS but will not converge towards the solutions of the sketched subproblems.

To achieve this, we add a regularization term to the sketched subproblem (10). The new subproblem becomes:
minUIr:∈R|Ir|×k+∥∥Atr−UIr:Bt∥∥2F+μt∥∥UIr:−UtIr:∥∥2F,(17)
View Sourcewhere μt>0 is a parameter. Such regularization is reminiscent to the proximal point method [47] and parameter μt controls the step size as 1/ηt in projected gradient descent. We therefore require μt→+∞ to enforce the convergence of the algorithm, e.g., μt=t.

At each step of proximal coordinate descent, only one column of UIr:, say UIr,j where j∈{1,2,…,k}, is updated:
minUIr:j∈R|Ir|+∥∥∥Atr−UIr:jBtj:−∑l≠jUIr:lBtl:∥∥∥2F+μt∥∥UIr:j−UtIr:j∥∥22.(18)
View SourceRight-click on figure for MathML and additional features.It is not hard to see that the above problem is still row-independent, which means that each entry of the row vector UIr:j can be solved independently at each node. For example, for any i∈Ir, the solution of Ut+1i:j is given by:
Ut+1i:j==argminUi:j≥0∥∥∥(Atr)i:−Ui:jBtj:−∑l≠jUi:lBtl:∥∥∥22+μt∥∥Ui:j−Uti:j∥∥22max{μtUti:j+(Atr)i:Bt⊤j:−∑l≠jUi:lBtl:Bt⊤j:Btj:Bt⊤j:+μt,0}.(19)
View Source

At each step of coordinate descent, we choose the column j from {1,2,…,k} successively. When updating column j at iteration t, the columns l<j have already been updated and thus UIr:l=Ut+1Ir:l, while the columns l>j are old so UIr:l=UtIr:l.

The complete proximal coordinate descent algorithm for the U-subproblem is summarized in Algorithm 3. When updating column j, computing the matrix-vector multiplication AtrBt⊤j: takes O(d|Ir|). The whole inner loop takes O(k(d+|Ir|)) because one vector dot product of length d is required for computing each summand and the summation itself needs O(k|Ir|). Considering that there are k columns in total, the overall complexity of coordinate descent is O(k((k+d)|Ir|+kd)). Typically, we choose d>k, so the complexity can be simplified to O(kd(|Ir|+k)), which is the same as that of gradient descent.

Since proximal coordinate descent is much more efficient than projected gradient descent, we adopt it as the default subproblem solver within DSANLS.

Algorithm 3. Proximal Coordinate Descent for Local Subproblem (10) on Node r
Parameter: μt>0

for j=1 to k do

T←μtUtIr:j+AtrBt⊤j:

for l=1 to j−1 do

T←T−(Btl:Bt⊤j:)Ut+1Ir:l

for l=j+1 to k do

T←T−(Btl:Bt⊤j:)UtIr:l

Ut+1Ir:j←max{T/(Btj:Bt⊤j:+μt),0}

return Ut+1Ir:

3.6 Theoretical Analysis
3.6.1 Complexity Analysis
We now analyze the computational and communication costs of our DSANLS algorithm, when using subsampling random sketch matrices. The computational complexity at each node is:
O(dgenerating St+|Ir|dconstructing Atr and Bt+kd(|Ir|+k)solving subproblem)=O(kd(|Ir|+k))≈O(kd(mN+k)).(20)
View SourceMoreover, as we have shown in Section 3.3, the communication cost of DSANLS is O(kd).

On the other hand, for a classical implementation of distributed HALS [48], the computational cost is:
O(kn(|Ir|+k))≈O(kn(mN+k)),(21)
View Sourceand the communication cost is O(kn) due to the all-gathering of Vt's.

Comparing the above quantities, we observe an n/d≫1 speedup of our DSANLS algorithm over HALS in both computation and communication. However, we empirically observed that DSANLS has a slower per-iteration convergence rate (i.e., it needs more iterations to converge). Still, as we will show in the next section, in practice, DSANLS is superior to alternative distributed NMF algorithms, after taking all factors into account.

3.6.2 Convergence Analysis
Here we provide theoretical convergence guarantees for the proposed SANLS and DSANLS algorithms. We show that SANLS and DSANLS converge to a stationary point. To establish convergence result, Assumption 2 is needed first.

Assumption 2.
Assume all the iterates Ut and Vt have uniformly bounded norms, which means that there exists a constant R such that ∥Ut∥F≤R and ∥Vt∥F≤R for all t.

We experimentally observed that this assumption holds in practice, as long as the step sizes used are not too large. Besides, Assumption 2 can also be enforced by imposing additional constraints, such as:
Ui:l≤2∥M∥F−−−−−−√andVj:l≤2∥M∥F−−−−−−√∀i,j,l,(22)
View SourceRight-click on figure for MathML and additional features.with which we have R=max{m,n}k2∥M∥F−−−−−−√. Such constraints can be very easily handled by both of our projected gradient descent and regularized coordinate descent solvers. Lemma 1 shows that imposing such extra constraints does not prevent us from finding the global optimal solution.

Lemma 1.
If the optimal solution to the original problem (1) exists, there is at least one global optimal solution in the domain (22).

Based on Assumptions 1 (see Section 3.4) and Assumption 2, we now can formally show our main convergence result:

Theorem 1.
Under Assumptions 1 and 2, if the step sizes satisfy ∑∞t=1ηt=∞ and ∑∞t=1η2t<∞, for projected gradient descent, or ∑∞t=11/μt=∞ and ∑∞t=11/μ2t<∞, for regularized coordinate descent, then SANLS and DSANLS with either sub-problem solver will converge to a stationary point of problem (1) with probability 1.

The proofs of Lemma 1 and Theorem 1 can be found in the supplementary material, which can be found on the Computer Society Digital Library at http://doi.ieeecomputersociety.org.ezproxy.auckland.ac.nz/10.1109/TKDE.2020.2985964.

SECTION 4Secure Distributed NMF
In this section, we provide our solutions to the problem of secure distributed NMF over federated data.

4.1 Extend DSANLS to Secure Setting
DSANLS and all lines of works discussed in Section 2.2.1 store copies of M across two-dimensional (shown in Fig. 1a), and exploit the independence of local update computation for rows of U and V to apply communication-optimal matrix multiplication. They cannot be applied directly to secure distributed NMF setting. The reason is that, in secure distributed NMF setting (shown in Fig. 1b), only one column copy is stored in each node, while the others cannot be disclosed.

Nevertheless, DSANLS can be adapted to this secure setting with modification, but only for one or limited iterations. The reason is illustrated in Theorem 2. In modified DSANLS algorithm, each node still takes charge of updating UIr: and VJr: as before, but only one copy M:Jr of M=[M1,M2,…,MN] will be stored in node r. Thus, V-subproblem is exactly the same as in DSANLS. Differently, we need to use MPI-AllReduce function to gather M:JrSt from all nodes before each iteration of U-subproblem, so that each node has access to fully sketched matrix MSt to solve sketched U-subproblem. Note that here random matrix St not only helps reduce the communication cost from O(mn) to O(md) with a smaller NLS problem, but also conceals the full matrix M in each iteration.

Theorem 2.
M cannot be recovered only using information about MS (or SM) and S.

Proof.
Assume S is a square matrix. Given MS (or SM) and S, we are able to get M by M=MSS−1 (or M=S−1 SM). However, the numbers of row and column are highly imbalanced in S and it is not a square matrix. Therefore M cannot be recovered only using information about MS (or SM) and S.

However, NMF is an iterative algorithm (shown in Algorithm 1). Secure computation in limited iterations cannot guarantee an acceptable accuracy for practical use due to the following reason:

Theorem 3.
M can be recovered after enough iterations.

Proof.
If we view M⋅S=MS as a system of linear equations with a variable matrix M and constant matrices S and MS. Each row of M can be solved by a standard Gaussian Elimination solver, given a sufficient number of (S, MS) pairs.

Theorem 3 suggests that DSANLS algorithm suffers from the dilemma of choosing between information disclosure and unacceptable accuracy, making it impractical to real applications. Therefore, we need to propose new practical solutions to secure distributed NMF.

4.2 Synchronous Framework
A straightforward solution to secure distributed NMF is that each node solves a local NMF problem with a local copy of U (denoted as U(r) for node r). Periodically, nodes communicate with each other, and update local copy of U to the aggregation of all local copies U(j),j∈{1,…,N} by All-Reduce operation. We name this method as Syn-SD under synchronous setting. The detailed algorithm is shown in Algorithm 4. Within inner iterations, every node maintains its own copy of U (i.e., U(r)) by solving the regular NMF problem. Every T2 rounds, different local copies of U will be averaged through nodes by using ∑Nj=1U(j)/N. Note that, U(r) is one copy of the whole matrix U stored locally in node r, while VJr: is the corresponding part of the matrix V=[VJ1:,VJ2:,…,VJN:] stored in node r.

In Syn-SD, the local copy U(r) in node r will be updated to a uniform aggregation of local copies from all nodes periodically. Small number of inner iteration T2 incurs large communication cost caused by All-Reduce. Larger T2 may lead to slow convergence, since each node does not share any information of its local copy U(r) inside the inner iterations.

To improve the efficiency of data exchange, we incorporate matrix sketching to Syn-SD, and propose an improved version called Syn-SSD. In Syn-SSD, information of local copies is shared across cluster nodes more frequently, with communication overhead roughly the same as Syn-SD. As shown in Algorithm 5, the sketched version StU(r) of the local copy U(r) is exchanged within each inner iteration. There are two advantages of applying matrix sketching: (1) Since the sketched matrix has a much smaller size, All-Reduce operation causes much less communication cost, making it affordable with higher frequency. (2) Solving a sketched NLS problem can also reduce the computation cost due to a reduced problem size of solving U(r) and VJr: for each node. It is worth noting that St1 is exactly the same for each node by using the same seed and generator. The same for St2. But St1 and St2 are not necessarily equivalent. With such a constraint, the algorithm is equivalent to NMF in single-machine environment and the convergence can be guaranteed.

Algorithm 4. Syn-SD: Secure Distributed NMF on node r
Input: M:Jr

Parameter: Iteration numbers T1,T2

initialize U0(r)≥0, V0Jr:≥0

for t1=0 to T1−1 do

for t2=1 to T2 do

t←t1×T2+t2

Ut(r)← update(M:Jr, Ut−1(r), Vt−1Jr:)

VtJr:← update(M:Jr, Ut(i), Vt−1Jr:)

All-Reduce: Ut(r)←∑Nj=1Ut(j)N

return Ut(r) and VtJr:

It is straightforward to see that Syn-SD and Syn-SSD satisfy Definition 1 and they are (N−1)-private protocols, since VJr: and M:Jr are only seen by node r.

4.3 Asynchronous Framework
In Syn-SD and Syn-SSD, each node must stall until all participating nodes reach the synchronization barrier before the All-Reduce operation. However, highly imbalanced data in real scenario of federated data mining may cause severe workload imbalance problem. The synchronization barrier will force nodes with low workload to halt, making synchronous algorithms less efficient. In this section, we study secure distributed NMF in an asynchronous (i.e., server/client architecture) setting and propose corresponding asynchronous algorithms.

Algorithm 5. Syn-SSD: Secure Sketched Distributed NMF on node r
Input: M:Jr

Parameter: Iteration numbers T1,T2

initialize U0(i)≥0, V0Jr:≥0

for t1=0 to T1−1 do

for t2=1 to T2 do

t←t1×T2+t2

Generate random matrix St1

Ut(r)← update(M:JrSt1, Ut−1(r), Vt−1Jr:St1)

Generate random matrix St2

All-Reduce: SU¯¯¯¯¯¯¯t←∑Nj=1St2Ut(j)N

VtJr:← update(St2M:Jr, SU¯¯¯¯¯¯¯t, Vt−1Jr:)

All-Reduce: Ut(r)←∑Nj=1Ut(j)N

return Ut(r) and VtJr:

First of all, we extend the idea of Syn-SD to asynchronous setting and name the new method Asyn-SD. In Asyn-SD, the server (in Algorithm 6) takes full charge of updating and broadcasting Ut. Once received Ut(r) from the client node r, the server would update Ut locally, and return the latest version of Ut back to the client node r for further computing. Note that the server may receive local copies of Ut from clients in an arbitrary order. Consequently, we cannot use the same operation of All-Reduce as Syn-SD any more. Instead, Ut in server side is updated by the weighted sum of current Ut and newly received local copy Ut(r) from client node r. Here the relaxation weight ωt asymptotically converges to 0. Thus a converged Ut is guaranteed on server side. Our experiments in Section 5 suggest that this relaxation has no harm to factorization convergence.

Algorithm 6. Asyn-SD, Asyn-SSD: Server part
Parameter: Relaxation parameter ρ

initialize U0≥0

t←0 ▹ t is the update counter.

while not stopping do

Receive Ut(r) from client node r

ωt←ρρ+t ▹ ωt is the relaxation weight.

Ut←(1−ωt)Ut+ωtUt(r)

Send Ut back to client node r

t←t+1

return Ut

On the other hand, client nodes of Asyn-SD (in Algorithm 7) behave similarly as nodes in Syn-SD. Clients locally solve the standard NMF problem for T iterations, and then update local Ut(r) by communicating only with the server node. Unlike Syn-SD, Asyn-SD does not have a global synchronization barrier. Client nodes in Asyn-SD independently exchange their local copy Ut(r) with the server without an All-Reduce operation.

Algorithm 7. Asyn-SD, Asyn-SSD: Client part of node r
Input: M:Jr

Parameter: Iteration number T

initialize V0Jr:≥0

while Server not stopping do

Receive U from server

U0(r)←U

for t=1 to T do

VtJr:← update(M:Jr, Ut−1(r), Vt−1Jr:)

Ut(r)← update(M:Jr, Ut−1(r), VtJr:) ▹ For Asyn-SSD, replace it with Lines 5-6 of Algorithm 5.

Send UT(r) to server

return VTJr:

Similarly, Syn-SSD can be extended to its asynchronous version Asyn-SSD. However, the algorithm for clients is more constrained and conservative in sketching. Note that the random sketching matrices S1 and S2 (in Algorithm 5) should be the same across the nodes in the same summation in order to have a meaningful summation of sketched matrices. However, enforcing the same St2 for updating sketched U will result in a synchronous All-Reduce operation. Therefore, U cannot be sketched in asynchronous algorithms and we only consider sketching VJr: in Asyn-SSD (Line 7 in Algorithm 7). The server part of Asyn-SSD is the same as Asyn-SD in Algorithm 6.

Similar to synchronous versions, Asyn-SD and Asyn-SSD satisfy Definition 1 and they are (N−1)-private protocols, since VJr: and M:Jr are only seen by node r.

SECTION 5Experimental Evaluation
This section includes an experimental evaluation of our algorithms on both dense and sparse real data matrices. The implementation of our methods is available at https://github.com/qianyuqiu79/DSANLS.

5.1 Setup
We use several (dense and sparse) real datasets as Qian et al. [49] for evaluation. They corresponds to different NMF tasks, including video analysis, image processing, text mining and community detection. Their statistics are summarized in Table 1.

TABLE 1 Statistics of Datasets

We conduct our experiments on a Linux cluster with 16 nodes. Each node contains 8-core Intel Core i7-3770 CPU @ 1.60 GHz cores and 16 GB of memory. Our algorithms are implemented in C++ using the Intel Math Kernel Library (MKL) and Message Passing Interface (MPI). By default, we use 10 nodes and set the factorization rank k to 100. We also report the impact of different node number (2-16) and k (20-500). We use μt=α+βt [50], do the grid search for α and β in the range of {0.1, 1, 10} for each dataset and report the best results. Because the use of Gaussian random matrices is too slow on large datasets RCV1 and DBLP, we only use subsampling random matrices for them.

For the general acceleration of NMF, we assess DSANLS with subsampling and Gaussian random matrices, denoted by DSANLS/S and DSANLS/G, respectively, using proximal coordinate descent as the default subproblem solver. As mentioned in [5], [8], it is unfair to compare with a Hadoop implementation. We only compare DSANLS with MPI-FAUN3 (MPI-FAUN-MU, MPI-FAUN-HALS, and MPI-FAUN-ABPP implementations), which is the first and the state-of-the-art C++/MPI implementation with MKL and Armadillo. For parameters pc and pr in MPI-FAUN, we use the optimal values for each dataset, according to the recommendations in [5], [8].

For the problem of secure distributed NMF, we evaluate all proposed methods: Syn-SD, Syn-SSD with sketch on U (denoted as Syn-SSD-U), Syn-SSD with sketching on V (denoted as Syn-SSD-V), Syn-SSD with sketching on both U and V (denoted as Syn-SSD-UV), Asyn-SD, Asyn-SSD with sketching on V (denoted as Asyn-SSD-V), using proximal coordinate descent as the default subproblem solver. We do not list secure building block methods as baselines, since communication overhead is heavy in these multi-round handshake protocols and it is unfair to compare them with MPI based methods. For example, a matrix sum described by Duan and Canny [39] results in 5X communication overhead compared to a MPI a all-reduce operation.

We use the relative error of the low rank approximation compared to the original matrix to measure the effectiveness of different NMF approaches. This error measure has been widely used in previous work [5], [8], [51] and is formally defined as ∥∥M−UV⊤∥∥F/∥M∥F.

5.2 Evaluation on Accelerating General NMF
5.2.1 Performance Comparison
Since the time for each iteration is significantly reduced by our proposed DSANLS compared to MPI-FAUN, in Fig. 2, we show the relative error over time for DSANLS and MPI-FAUN implementations of MU, HALS, and ANLS/BPP on the 6 real public datasets. Observe that DSANLS/S performs best in all 6 datasets, although DSANLS/G has faster per-iteration convergence rate. MU converges relatively slowly and usually has a bad convergence result; on the other hand HALS may oscillate in the early rounds4, but converges quite fast and to a good solution. Surprisingly, although ANLS/BPP is considered to be the state-of-art NMF algorithm, it does not perform well in all 6 datasets. As we will see, this is due to its high per-iteration cost.

Fig. 2. - 
Relative error over time for general distributed NMF
Fig. 2.
Relative error over time for general distributed NMF

Show All

5.2.2 Scalability Comparison
We vary the number of nodes used in the cluster from 2 to 16 and record the average time for 100 iterations of each algorithm. Fig. 3 shows the reciprocal of per-iteration time as a function of the number of nodes used. All algorithms exhibit good scalability for all datasets (nearly a straight line), except for FACE (i.e., Fig. 3a). FACE is the smallest dataset, whose number of columns is 300, while k is set to 100 by default. When n/N is smaller than k, the complexity is dominated by k, hence, increasing the number of nodes does not reduce the computational cost, but may increase the communication overhead. In general, we can observe that DSANLS/Subsampling has the lowest per-iteration cost compared to all other algorithms, and DSANLS/Gaussian has similar cost to MU and HALS. ANLS/BPP has the highest per-iteration cost, explaining the bad performance of ANLS/BPP in Fig. 2.


Fig. 3.
Reciprocal of per-iteration time as a function of cluster size for general distributed NMF

Show All

5.2.3 Performance Varying the Value of k
Although tuning the factorization rank k is outside the scope of this paper, we compare the performance of DSANLS with MPI-FAUN varying the value of k from 20 to 500 on RCV1. Observe from Figs. 2 and 4 (k=100) that DSANLS outperforms the state-of-art algorithms for all values of k. Naturally, the relative error of all algorithms decreases with the increase of k, but they also take longer to converge.


Fig. 4.
Relative error over time for general distributed NMF, varying k value

Show All

5.2.4 Comparison With Projected Gradient Descent
In Section 3.5, we claimed that our proximal coordinate descent approach (denoted as DSANLS-RCD) is faster than projected gradient descent (also presented in the same section, denoted as DSANLS-PGD). Fig. 5 confirms the difference in the convergence rate of the two approaches regardless of the random matrix generation approach.


Fig. 5.
Relative error per-iteration of different subproblem solvers for general distributed NMF

Show All

5.3 Evaluation on Secure Distributed NMF
5.3.1 Performance Comparison for Uniform Workload
In Fig. 6, we show the relative error over time for secure distributed NMF algorithms on the 4 real public datasets, with a uniformly partition of columns. Syn-SSD-UV performs best in BOAT, FACE and GISETTE. As we will see in the next section, this is due to the fact that per-iteration cost of Syn-SSD-UV is significantly reduced by sketching. On MNIST, Syn-SSD-U and Syn-SSD-V has a better convergence in terms of relative error. Syn-SD and Asyn-SD converge relatively slowly and usually have a bad convergence result; on the other hand Asyn-SSD-V converges slowly but consistently generates better results than Syn-SD and Asyn-SD.

Fig. 6. - 
Relative error over time for uniform workload in secure distributed NMF
Fig. 6.
Relative error over time for uniform workload in secure distributed NMF

Show All

5.3.2 Performance Comparison for Imbalanced Workload
To evaluate the performance of different methods when the workload is imbalanced, we conduct experiments on skewed partition of input matrix. Among 10 worker nodes, node 0 is assigned with 50 percent of the columns, while other nodes have a uniform partition of the rest of columns. The measure for error is the same as the case of uniform workload.

It can be observed that in imbalanced workload, asynchronous algorithms generally outperform synchronous algorithms. Asyn-SSD-V gives the best result in terms of relative error over time, except dataset FACE. In FACE, Asyn-SD slowly converges to the best result. Unlike the case of uniform workload in Fig. 6, the sketching method Syn-SSD-UV does not perform well in imbalanced workload. Syn-SD are basically inapplicable in BOATS, MNIST and GISETTE datasets due to its slow speed. In sparse datasets MNIST and GISETTE, Syn-SSD-V and Syn-SSD-U can converge to a good result, but they do not generate satisfactory results on dense dataset BOATS and FACE.

5.3.3 Scalability Comparison
We vary the number of nodes used in the cluster from 2 to 16 and record the average time for 100 iterations of each algorithm. Fig. 8 shows the reciprocal of per-iteration time as a function of the number of nodes for uniform workload. All algorithms exhibit good scalability for all datasets (nearly a straight line), except for FACE (i.e., Fig. 3a). FACE is the smallest dataset, whose number of columns is 361 and number of row is 2,429. When n/N is smaller than k=100, the time consumed by subproblem solvers is dominated by the communication overhead. Hence, increasing the number of nodes is does not reduce per-iteration time. In general, we can observe that Syn-SSD-UV has the lowest per-iteration time compared to all other algorithms, and also has the best scalability as we can see from the steepest slope. Synchronous averaging has the highest per-iteration cost, explaining the bad performance in uniform workload experiments in Fig. 6.

Fig. 7. - 
Relative error over time for imbalanced workload in secure distributed NMF
Fig. 7.
Relative error over time for imbalanced workload in secure distributed NMF

Show All

Fig. 8. - 
Reciprocal of per-iteration time for uniform workload in secure distributed NMF
Fig. 8.
Reciprocal of per-iteration time for uniform workload in secure distributed NMF

Show All

In imbalanced workload settings, it is not surprising that asynchronous algorithms outperform synchronous algorithms with respect to scalability, as shown in Fig. 9. Synchronization barriers before All-Reduce operations severely affect the scalability of synchronous algorithms, resulting in a nearly flat curve for per-iteration time. The per-iteration time of Syn-SSD-UV is satisfactory when cluster size is small. However, it does not get significant improvements when more nodes are deployed. On the other hand, asynchronous algorithms demonstrate decent scalability as number of nodes grows. The short average iteration time of Asyn-SD and Asyn-SSD-V, shown in Fig. 9, also explains their superior performance over their synchronous counterparts in Fig. 7.


Fig. 9.
Reciprocal of per-iteration time for imbalanced workload in secure distributed NMF

Show All

In conclusion, with an overall evaluation of convergence and scalability, Syn-SSD-UV should be adopted for secure distributed NMF under uniform workload, while Asyn-SSD-V is a more reasonable choice for secure distributed NMF under imbalanced workload.

SECTION 6Conclusion
In this paper, we studied the acceleration and security problems for distributed NMF. First, we presented a novel distributed NMF algorithm DSANLS that can be used for scalable analytics of high dimensional matrix data. Our approach follows the general framework of ANLS, but utilizes matrix sketching to reduce the problem size of each NLS subproblem. We discussed and compared two different approaches for generating random matrices (i.e., Gaussian and subsampling random matrices). We presented two subproblem solvers for our general framework, and theoretically proved that our algorithm is convergent. We analyzed the per-iteration computational and communication cost of our approach and its convergence, showing its superiority compared to the state-of-the-art. Second, we designed four efficient distributed NMF methods in both synchronous and asynchronous settings with a security guarantee. They are the first distributed NMF methods over federated data, where data from all parties are utilized together in NMF for better performances and the data of each party remains confidential without leaking any individual information to other parties during the process. Finally, we conducted extensive experiments on several real datasets to show the superiority of our proposed methods. In the future, we plan to study the applications of DSANLS in dense or sparse tensors and consider more practical designs of asynchronous algorithm for secure distributed NMF.