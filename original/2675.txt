The low-rank matrix completion problem is fundamental in both machine learning and computer vision fields with many important applications, such as recommendation system, motion capture, face recognition, and image inpainting. In order to avoid solving the rank minimization problem which is NP-hard, several surrogate functions of the rank have been proposed in the literature. However, the matrix restored from the optimization problem based on the existing surrogate functions seriously deviates from the original one. In this paper, we first design a new non-convex Schatten capped p norm which generalizes several existing non-convex matrix norms and balances between the rank and the nuclear norm of the matrix. Then, a matrix completion method based on the Schatten capped p norm is proposed by exploiting the framework of the alternating direction method of multipliers. Meanwhile, the Schatten capped p norm regularized least squares subproblem is analyzed in detail and is solved explicitly. Finally, we evaluate the performance of the proposed matrix completion method based on extensive experiments in the field of image inpainting. All the experimental results demonstrate that the proposed method can indeed improve the accuracy of matrix completion compared with the existing methods.
SECTION 1Introduction
Recovering a low-rank matrix from incomplete observation of its entries arises increasingly in both machine learning and computer vision fields, including recommendation systems [1], motion capture [2], video surveillance [3], and image inpainting [4], etc. For most of these applications, the data is usually organized into a matrix X∈Rm×n, in which only a small number of entries are known. Without prior knowledge of the underlying problem, the matrix completion task is highly ill-posed. Fortunately, the data matrix X often exhibits a low-rank or an approximately low-rank structure in real-world applications, which enables the recovery of the missing entries in X [5].

It is well known that most information and energy of the matrix are concentrated on its dominated singular values [6]. Therefore, it is natural to assume that the underlying matrix is of low-rankness. Then, the missing entries of the matrix can be recovered based on the low-rank prior knowledge [7]. The incomplete image in image inpainting [8], the users’ rating matrix in recommendation systems [9], and the human motion matrix in motion capture [10] are all low-rank matrices in which the effective factors are far less than their rows and columns.

Specifically, the matrix completion problem is usually formulated as follows:
minXs.t.rank(X)XΩ=DΩ,(1)
View Sourcewhere D∈Rm×n denotes the observed incomplete matrix, Ω denotes the set of locations corresponding to the observed entries, and XΩ={Xij|(i,j)∈Ω}, DΩ={Dij|(i,j)∈Ω}. However, the rank minimization problem has been proven to be NP-hard due to the non-convexity and the discontinuity of the rank function. Thus, the exponential time in the matrix dimension is required to recover the original matrix in both theory and practice [11]. Nowadays, many approximations for the rank function have been proposed in the literature to study the rank minimization problem approximately.

As the tightest lower convex bound of the rank function, the nuclear norm has been widely adopted to surrogate the non-convex rank function. It has been shown that the low-rank matrix can be recovered exactly based on the nuclear norm minimization as long as the entries of the matrix are sampled uniformly at random and the matrix satisfies certain incoherence property. Meanwhile, a number of heuristic algorithms, such as the singular value thresholding (SVT) algorithm [12], the nuclear norm regularized least squares (NNLS) algorithm [13], and the robust principal component analysis (RPCA) algorithm [14], have been proposed to solve the nuclear norm minimization problem. However, the nuclear norm minimization is sub-optimal and often leads to the over-shrinkage of the rank component [15]. Specifically, the nonzero singular values are added together in the nuclear norm, instead of being counted as in the rank function. Thus, the nonzero singular values have different contributions to the penalty function and larger singular values are preferred to be punished.

Recently, several non-convex surrogate functions have been proposed in the literature to approximate the rank function more accurately. The representatives include the truncated nuclear norm [16], the capped norm [17], the Schatten p norm [18], the smoothly clipped absolute deviation (SCAD) [19], the minimax concave penalty (MCP) [20], and the logarithm penalty [21]. Although the non-convex surrogate function can generate more accurate recovery results than that of its convex counterpart, it remains a big challenge to solve the non-convex and non-smooth optimization problem. Moreover, the value of the objective function in the non-convex optimization problem usually decreases slowly due to the loose bound between the surrogate function and its approximation [22].

Through carefully examining the existing matrix completion methods in the literature, we identify the following two technical challenges that are still posing in the matrix completion problem.

The first challenge is that the underlying matrix is usually approximately low-rank instead of exactly low-rank in practical applications. Thus, the matrix with the least rank may not be the optimal solution we desired [23]. A suitable optimization model that can balance the rank and the nuclear norm of the underlying matrix should be re-designed to replace the rank minimization problem.

The other challenge is that the larger singular values and the smaller singular values of the matrix usually represent the significant information we want to preserve and the error or noise we want to remove, respectively [24]. Thus, an appropriate measurement function that can promote the preservation of larger singular values and punish the smaller singular values to zero should be re-designed to improve the recovery accuracy of the matrix completion operation.

In this paper, we design a new non-convex measurement function, called the Schatten capped p (SCp) norm, to focus on the smaller singular values in the matrix. By minimizing the Schatten capped p norm in each iteration of the matrix completion, the missing data restoration problem can be formulated as a non-convex optimization problem. Then, we solve it by following the framework of the alternating direction method of multipliers (ADMM). Meanwhile, we also solve the SCp norm regularized least squares subproblem analytically. The main contributions of this paper are summarized as follows:

We propose a non-convex Schatten capped p norm which generalizes several existing matrix norms in this paper. By adjusting its parameters, the Schatten capped p norm can degenerate to the Schatten p norm, the nuclear norm, and the capped norm, respectively.

We present the SCp norm based matrix completion method based on the newly designed Schatten capped p norm. Thereinto, the SCp norm regularized least squares subproblem is also analyzed in detail and is solved explicitly.

We adopt the SCp norm based matrix completion method to deal with the image inpainting problem. The images masked by the random noise, blocks, and texts are recovered by our proposed method and are compared with the results obtained from other baseline methods in detail.

The rest of this paper is structured as follows. In Section 2, we summarize the existing matrix completion methods. In Section 3, we introduce the Schatten capped p norm and present the Schatten capped p norm based matrix completion method. In Section 4, we report the experimental results in comparison with other state-of-the-art matrix completion methods. Finally, in Section 5, we conclude this paper in which we also discuss our future research.

SECTION 2Related Work
Matrix completion has been extensively studied in both theory and practice due to its wide application in numerous fields, such as recommendation system, motion capture [25], face recognition [26], [27], subspace clustering [28] and image processing [29]. In general, the existing matrix completion methods can be classified into two categories: the spectral regularization based methods and the matrix factorization based methods.

2.1 The Spectral Regularization Based Methods
In the spectral regularization based methods, the singular values or their variations are utilized to recover the missing entries in the matrix. To avoid solving the rank minimization problem, the nuclear norm ∥X∥∗=∑min(m,n)i=1σi is usually chosen to surrogate the rank function, where σi denotes the ith largest singular value of X. Thus, the rank minimization problem can be reformulated to the following nuclear norm minimization problem:
minXs.t.∥X∥∗XΩ=DΩ.(2)
View SourceRight-click on figure for MathML and additional features.The singular value thresholding (SVT) method and the inexact augmented Lagrange multiplier (IALM) method are two representative methods designed to solve the nuclear norm minimization problem. In order to reduce the computational and storage expense of the singular value decomposition (SVD), the soft-impute algorithm was proposed in [30] by computing Xk+1=Sλ(PΩ(D)+P⊥Ω(Xk)), where Sλ() denotes the singular value shrinkage operator, and P⊥Ω denotes the complementary of the projection operator PΩ. Specifically, the singular value shrinkage operation on matrix X is defined as Sλ(X)=Udiag(max{σi−λ,0})VT, where U and V are the left and right singular matrices of X, respectively.

However, the nuclear norm has been proven to be a loose approximation of the rank function. All the singular values of X shrink simultaneously in the solving process of the problem (2), which may result in a sub-optimal solution. To approximate the rank function more accurately, a number of non-convex surrogate functions have been proposed in the literature. Specifically, Hu et al. proposed the truncated nuclear norm, denoted as ∥X∥r=∑min(m,n)i=r+1σi, to just punish the smallest min(m,n)−r singular values [16]. They formulated the following truncated nuclear norm regularization (TNNR) problem
minXs.t.∥X∥rXΩ=DΩ,(3)
View SourceRight-click on figure for MathML and additional features.with two newly designed methods, i.e., the truncated nuclear norm regularization based on the alternating direction method of multipliers (TNNR-ADMM) and the truncated nuclear norm regularization based on accelerated proximal gradient line search (TNNR-APGL).

Similar to the truncated nuclear norm, the capped norm ∥X∥θ=∑min(m,n)i=1min(1,σi/θ) was also designed to minimize the insignificant singular values which are smaller than θ [17]. Essentially, both the truncated nuclear norm and the capped norm aim to suppress the significant singular values and punish the insignificant singular values. The relationship between the truncated nuclear norm and the capped norm has been deduced in Ref. [7]. Moreover, the Schatten p norm ∥X∥Sp=(∑min{m,n}i=1σpi)1p is another well-studied non-convex surrogate function [31].1 It has been proven that the matrix completion result based on the Schatten p norm (to the power p) is superior to that of the methods based on the nuclear norm when p∈(0,1) [32]. To solve the Schatten p norm minimization problem, Lai et al. proposed the improved iteratively reweighted least squares method for the unconstrainted smooth lq minimization (LRucLq) in Ref. [33] and proved that the limit point of the iterative sequence is the stationary point of the objective function. The weighted nuclear norm (WNN) minimization algorithm was also proposed in Ref. [34] to solve a series of low-level computer vision tasks. Apart from the above-mentioned norms, other non-convex norms, such as the logarithm norm, the Geman norm, and the Laplace norm, have also been proposed in the literature. Lu et al. summarized the existing non-convex norms and proposed the iterative reweighted nuclear norm (IRNN) algorithm in Ref. [22] to solve the following general non-convex optimization problem
minX∑g(σi(X))+f(X),(4)
View SourceRight-click on figure for MathML and additional features.where g denotes a general non-convex penalty function and f denotes the loss function. However, the convergence speed of the IRNN algorithm is slow due to the minimization of a linear surrogate function of the loss function and the regularization part simultaneously.

2.2 The Matrix Factorization Based Methods
In the matrix factorization based methods, the original matrix is approximated by the product of two or more small-sized matrices. Compared with the spectral regularization based methods, this type of methods is usually computationally cheap and less memory-consuming. However, the rank of the matrix should be known in advance, which is generally unavailable in certain applications.

As one of the representative method in this type, the maximum margin factorization (MMF) method have been proposed in Ref. [35]. Empirical observations show that only sub-optimal solutions can be obtained in the MMF method and its extensions. Wen et al. proposed the low-rank matrix fitting (LMaFit) method in Ref. [36] by constructing a nonlinear successive over-relaxation algorithm which requires solving a series of linear least squares problems. To tackle the large-scale nonnegative matrix factorization problem, a new form of update functions have been designed in Ref. [37] to implement the parallelism in a distributed environment.

Recently, several matrix factorization methods based on the deep-structured neural network have been proposed in the literature to solve the nonlinear matrix completion problem. Fan et al. proposed the deep matrix factorization (DMF) method in Ref. [38] based on a nonlinear latent variable model. Moreover, they proposed an AutoEncoder-based matrix completion (AEMC) method to learn the latent variable structure in the form of AutoEncoder structure [39].

SECTION 3The SCp Norm Based Matrix Completion Method
3.1 The Schatten Capped p Norm
To make a balance between the rank and the nuclear norm of the matrix, we design a Schatten capped p norm in this paper.

Definition 1.
Give a matrix X∈Rm×n, the Schatten capped p quasi norm is
∥X∥Sp,τ=(∑i=1min{m,n}min(σi,τ)p)1p,(5)
View SourceRight-click on figure for MathML and additional features.

where τ≥0 denotes the predefined threshold used to filter the singular values, and p∈(0,1].

For convenience, we omit the quasi term in the Schatten capped p quasi norm and use the Schatten capped p norm to the power p in the proposed matrix completion method, which is defined as follows:
∥X∥pSp,τ=∑i=1min{m,n}min(σi,τ)p.(6)
View SourceIn Eqs. (5) and (6), when τ is large enough (e.g., τ is larger than all the singular values), the Scp norm (to the power p) degenerates to the Schatten p norm (to the power p). Furthermore, we can obtain the nuclear norm by setting p=1. On the contrary, we can also obtain the rank of a matrix by letting τ to be a small positive number (e.g., τ is smaller than all the nonzero singular values) and p=0 in Eq. (6).2 Therefore, the Schatten capped p norm can be utilized in the matrix completion problem to preserve the significant information (in corresponding to the large singular values) in the matrix and filter out the insignificant noise (in corresponding to the small singular values) by adjusting τ and p.

3.2 The Matrix Completion Method
By substituting the objective function in the rank minimization problem with the the Schatten capped p norm to the power p, we can define the following matrix completion problem based on the SCp norm:
minXs.t.∥X∥pSp,τXΩ=DΩ.(7)
View SourceRight-click on figure for MathML and additional features.According to the primal dual theory, the above problem can be transformed into the following unconstrained optimization problem:
minX∥XΩ−DΩ∥2F+γ∥X∥pSp,τ,(8)
View SourceRight-click on figure for MathML and additional features.where γ denotes the regularization parameter. By introducing EΩ=XΩ−DΩ and W=X, the above problem can be rewritten as follows:
minX,EΩ,Ws.t.    ∥EΩ∥2F+γ∥W∥pSp,τ  EΩ=XΩ−DΩW=X.(9)
View SourceRight-click on figure for MathML and additional features.

Thus, we can obtain the augmented Lagrangian function L(X,EΩ,W,Y,Z,μ) for the problem (9), i.e.,
L(X,EΩ,W,Y,Z,μ)=∥EΩ∥2F+γ∥W∥pSp,τ+μ2∥EΩ−(XΩ−DΩ)∥2F+μ2∥X−W∥2F+tr(YTΩ(EΩ−(XΩ−DΩ)))+tr(ZT(X−W)),(10)
View SourceRight-click on figure for MathML and additional features.where Y and Z are the Lagrangian multipliers, μ is the regularization parameters, and tr(ATB)=⟨A,B⟩ is the inner product between the matrix A and the matrix B. By following the framework of the alternating direction method of multipliers, we can solve the original problem through optimizing its subproblems iteratively.

When fixing EΩ and W, the problem (9) is simplified to the following subproblem:
minXμ2∥XΩ−EΩ−DΩ∥2F+μ2∥X−W∥2F−tr(YTΩ(XΩ−EΩ−DΩ))+tr(ZT(X−W)).(11)
View SourceRight-click on figure for MathML and additional features.After combining the similar terms, X can be computed as follows:
X=argminX∥XΩ−KΩ∥2F+∥X−N∥2F,(12)
View Sourcewhere KΩ=EΩ+DΩ+1μYΩ and N=W−1μZ. Thus, the optimal solution of X is
X=NΩc+KΩ+NΩ2,(13)
View Sourcewhere the first part is XΩc and the second part is XΩ [24].

When fixing X and W, the problem (9) is simplified to the following subproblem:
minEΩ∥EΩ∥2F+μ2∥EΩ−XΩ+DΩ∥2F+tr(YTΩ(EΩ−XΩ+DΩ)).(14)
View SourceRight-click on figure for MathML and additional features.Then, EΩ can be computed as follows:
EΩ=argminEΩ∥EΩ∥2F+μ2∥EΩ−HΩ∥2F=μμ+2HΩ,(15)
View SourceRight-click on figure for MathML and additional features.where HΩ=XΩ−DΩ−1μYΩ.

Finally, when fixing X and EΩ, the problem (9) is simplified to the following subproblem:
minWμ2∥W−X∥2F−tr(ZT(W−X))+γ∥W∥pSp,τ.(16)
View SourceAfter combing the similar terms, it can be rewritten as follows:
minW12∥W−G∥2F+λ∥W∥pSp,τ,(17)
View SourceRight-click on figure for MathML and additional features.where G=X+1μZ and λ=γμ. However, the SCp norm regularized least squares subproblem can not be solved analytically for the non-convex term ∥W∥pSp,τ. We will discuss it in the next subsection.

3.3 The SCp Norm Regularized Least Squares Subproblem
To solve the subproblem (17), we first introduce the following von Neumann's trace inequality theorem.

Theorem 1.
For any two matrices A,B∈Rm×n, tr(ATB)≤tr(σ(A)Tσ(B)), where σ(A) and σ(B) are the ordered singular value matrices of A and B with the same order.

Next, we present the following theorem to solve the subproblem (17).

Theorem 2.
The optimal solution W of the subproblem (17) is QΣRT, where Q and R are the left and right singular vector matrices of G, respectively. Σ is a diagonal matrix with the ith diagonal element σi solved by the following problem:
minσi≥012(σi−δi)2+λmin(σi,τ)p,(18)
View SourceRight-click on figure for MathML and additional features.

where δi denotes the ith singular value of G.

The proof of Theorem 2 can be found in Appendix A, which can be found on the Computer Society Digital Library at http://doi.ieeecomputersociety.org.ezproxy.auckland.ac.nz/10.1109/TKDE.2020.2978465. In order to solve the subproblem (18), we define
h(σ)={12(σ−δ)2+λσp,12(σ−δ)2+λτp,if σ∈[0,τ]if σ∈(τ,∞).(19)
View SourceRight-click on figure for MathML and additional features.Note that h(σ) is piecewise differentiable in the range of [0,∞). Let x∗ be the minimal solution of h(σ) in the range of [0,τ], i.e., x∗=argminσ∈[0,τ]h(σ). Moreover, let v′ be the threshold of δ to decide the minimum between h(0) and h(x∗). In Fig. 1, we show the plots of h(σ) with five typical δ values when p and λ are set to 0.5 and 1, respectively. We can see that there exists a specif threshold τ∗ and three potential minimal solutions of h(σ), i.e., 0, x∗ and δ (the minimum of h(σ) in the range of (τ,+∞)). If τ≤τ∗, δ is the minimum solution of h(σ); otherwise, 0 and x∗ can be the minimum solutions. In the later case, we could differentiate them by further computing the first-order derivative of h(σ). Therefore, we focus on the following two issues to solve the subproblem (18): (1) the finding of the threshold τ∗ and the minimal solution x∗, and (2) the searching of the threshold v′.

Fig. 1. - 
Plots of the function $h(\sigma)$h(σ) with different values of $\delta$δ where $p=0.5$p=0.5 and $\lambda =1$λ=1.
Fig. 1.
Plots of the function h(σ) with different values of δ where p=0.5 and λ=1.

Show All

According to the fundamental theorem of calculus, the shape of h(σ) is controlled by its derivatives. We compute the first-order, second-order and third-order derivatives of h(σ) as follows:
h′(σ)={(σ−δ)+λpσp−1,σ−δ,if σ∈[0,τ]if σ∈(τ,∞)(20)
View SourceRight-click on figure for MathML and additional features.
h′′(σ)={1+λp(p−1)σp−2,1,if σ∈[0,τ]if σ∈(τ,∞)(21)
View Source
h′′′(σ)={λp(1−p)(2−p)σp−3,0,if σ∈[0,τ]if σ∈(τ,∞).(22)
View Source

Next, we introduce Lemma 1 and Theorem 3 with the goal of restricting the range of regularization parameter λ and computing the optimal solution of h(σ), respectively. It should be noted that λ is usually far less than the upper bound deduced in Lemma 1 in practice.

Lemma 1.
For given parameters p∈(0,1], h′′(τ)>0 if and only if λ∈(0,τ2−pp(1−p)).3

Theorem 3.
For given parameters p∈(0,1] and λ∈(0,τ2−pp(1−p)), the minimum solution of (19) is
σ∗={δ,x′,if τ∈[0,τ∗]if τ∈(τ∗,∞),(23)
View SourceRight-click on figure for MathML and additional features.where τ∗=(12λ(x′−δ)2+(x′)p)1p,
x′={0,x∗,if δ∈[0,v′]if δ∈(v′,∞),(24)
View Sourcev′=v+λpvp−1, and v=(2λ(1−p))12−p.

The proof of Lemma 1 and Theorem 3 can be found in Appendix B, available in the online supplemental material, and Appendix C, available in the online supplemental material, respectively. To solve x∗ when δ>v′ (which is required in Theorem 3), we can utilize the classical gradient descent method. The pseudo code of the x∗ solving algorithm is shown in Algorithm 1. Empirically, a satisfactory result can be obtained with a small number of iterations It.

Algorithm 1. The x∗ Solving Algorithm
Input:δ>0,λ>0,0<p≤1,It.

Output: x∗.

initialize x(0) with δ.

for i=0:It do

x(i+1)=δ−λp(x(i))p−1.

end for

According to Theorem 2 and Theorem 3, the solution of subproblem (17) can be constructed as follows:
W=Qdiag(σ∗1,σ∗2,…,σ∗r,0,…,0)RT,(25)
View SourceRight-click on figure for MathML and additional features.where r denotes the rank of matrix G, and diag() denotes the diagonal function which generates a diagonal matrix by setting the given parameters as the diagonal elements.

Finally, we present the pseudo code of the proposed matrix completion method in Algorithm 2. Note that the method will terminate when the number of iterations k exceeds the predefined maximum number of iterations Iter. Alternatively, it will also terminate when the reconstructed error ∥X−X+∥2F∥X∥2F is smaller than the predefined error tolerance parameter, where X and X+ are two consecutive matrices computed before and after a certain iteration.

Algorithm 2. The Schatten Capped p Norm Based Matrix Completion Method
Input:DΩ,XΩ,W,Y,Z,0<p≤1,μ>0,1<ρ<2,τ>0,λ>0,k=1,Iter.

Output: X∗.

while k<Iter and not converge do

update X by Eq. (13).

update EΩ by Eq. (15).

update W by Eq. (25).

YΩ=YΩ−μ(XΩ−EΩ−DΩ).

Z=Z−β(W−X).

μ=ρμ.

k=k+1

end while

SECTION 4Performance Evaluations
In this section, we conduct three types of experiments based on the real-world image data set to evaluate the performance of the proposed method, and to compare it with some existing matrix completion methods. Specifically, the following six matrix completion methods are to be evaluated:

SCp ADMM: It is our proposed method which utilizes the SCp norm ∥X∥Sp,τ and the ADMM method to solve the matrix completion problem.

TNNR APGL: The truncated nuclear norm regularized matrix completion method [16] utilizes the truncated nuclear norm ∥X∥r and the APGL method to solve the matrix completion problem.

Sp ADMM: The Schatten p norm based matrix completion method [31] utilizes the Schatten p norm ∥X∥Sp and the ADMM method to solve the matrix completion problem.

Log ADMM: The logarithm penalty regularized matrix completion method [21] utilizes the logarithm penalty λlog(γ+1)∑min{m,n}i=1log(γσi+1) and the ADMM method to solve the matrix completion problem.

Log IRNN: The reweighted logarithm penalty regularized matrix completion method [40] utilizes the logarithm penalty λlog(γ+1)∑min{m,n}i=1log(γσi+1) and the IRNN method to solve the matrix completion problem.

Cap l1 IRNN: The capped l1 norm based matrix completion method [40] utilizes the capped l1 norm ∥X∥τ=∑min{m,n}i=1min(σi,τ) and the IRNN method to solve the matrix completion problem.

All the experiments were carried out on a desktop computer with i5-6500 CPU and 4G memory. The source code of the proposed method is public available.4

4.1 Experiment Setup
In our experiments, the test image set is obtained from the released code of the TNNR method.5 Specifically, we choose the following four types of images: trees, Lena, fisher, and scenery. In our proposed SCp norm based method, we set ρ=1.5, μ=1, and tune λ and p in the range of [0.1,10] and (0,1], respectively. The missing rate η=1−num(XΩ)num(X), where num() counts the number of entries in the parameter matrix. As to the other five baseline methods, the parameters are tuned according to the authors’ suggestions. The maximal iteration times in all six methods are uniformly set to 100.

To measure the data completion performance, we utilize the PSNR (Peak Signal-to-Noise Ratio), the SNR (Signal-to-Noise Ratio) and the RE (Relative Error) in the experiments. They are widely adopted indicators used to reflect the quality of image restoration. After computing the mean square error MSE=1mn∥X−X∗∥2F, the PSNR and the SNR can be calculated as follows:
PSNR=10⋅log10(2552MSE)(26)
View SourceRight-click on figure for MathML and additional features.
SNR=10⋅log10(∥X∥2Fmn⋅MSE).(27)
View SourceRight-click on figure for MathML and additional features.Intuitively, they represent the ratio of the signal power to the error power of the data restoration. Therefore, the larger the PSNR and the SNR are, the higher the accuracy that the method can achieve. On the contrary, the RE represents the ratio of the power of the data restoration error to that of the original data. It can be calculated as follows:
RE=∥X−X∗∥2F∥X∥2F.(28)
View SourceThus, the smaller the RE is, the higher the accuracy that the method can achieve.

4.2 Random Mask Experiments
We conduct a set of random mask experiments by stochastically discarding a number of entries in the test image. Specifically, the row and column indexes of the missing entries in each channel of the image are randomly chosen and the values of the corresponding pixels are set to 0. Furthermore, the missing rate η is also tuned from 90 to 10 percent to simulate the matrix completion tasks with different levels of difficulty. Apparently, the larger the missing rate η, the harder the matrix completion task.

We show the incomplete images of ’Lena’ with different missing rate η and their corresponding restored results by our proposed method in the first and second rows of Fig. 2, respectively. It can be seen from Figs. 2a and 2b that the rough sketch of the original image can be restored even when a large proportion of the entries in the image are missing. Meanwhile, the restored image gets clearer and sharper along with the decreasing of the missing rate η, which can be seen from the rest sub-figures of Fig. 2.

Fig. 2. - 
The incomplete images and their restored results in the random mask experiments.
Fig. 2.
The incomplete images and their restored results in the random mask experiments.

Show All

In Table 1, we compare the performance of six matrix completion methods via the metrics of PSNR, SNR, and RE in detail. It can be seen that our proposed method can obtain the highest accuracy in almost all cases (except in one case when η=80%). This is because the SCp norm can provide a more accurate approximation to the objective function than that of the other proposed matrix norms for the matrix completion problem. Furthermore, we can also see from Table 1 that the accuracy of the matrix completion increases as the missing rate η goes down in all the matrix completion methods. This is due to the fact that the smaller missing rate implies the easier matrix completion task. Thus, the PSNR and the SNR increase, and the RE decreases, correspondingly.

TABLE 1 The Performance of Matrix Completion Methods in the Random Mask Experiments
Table 1- 
The Performance of Matrix Completion Methods in the Random Mask Experiments
4.3 Block Mask Experiments
In the block mask experiments, we add the following three types of block masks to the test images: the block column mask, the block square mask, and the block triangle mask. In the block column mask, four increasing-sized blocks are arranged diagonally in the image. In the block square mask, seven same-sized blocks are arranged centrally in the image. In the block triangle mask, a large equilateral triangle is located in the center of the image. The missing rates of these block masks are 5.76, 6.62, and 35.36 percent, respectively. We show the original images (original img.) and the block masked images (masked img.) in Figs. 3a and 3b, respectively. Apparently, the restoration of the block masked image is much harder than that of the random masked image for the continuous and extensive missing entries.


Fig. 3.
The incomplete images and their restored results in the block mask experiments.

Show All

Figs. 3c, 3d, 3e, 3f, 3g and 3h show the recovered images of six matrix completion methods in parallel. Meanwhile, the accuracy of six matrix completion methods in terms of the PSNR, SNR, and RE are also reported in Table 2. It is clear that our proposed method presents the best performance of matrix completion. Not only the numerical indicators of matrix completion in our proposed method are the highest, but also the restored images exhibit the best visual quality. In the trees and Lena instances, the restored missing entries of our proposed method are less distinctive than that of baseline methods. In the fisher instance, all the matrix completion methods can not recover the missing entries perfectly because the percentage of the missing part is too high. However, the inconsistent part between the original image and the restored image of our proposed method is less obvious than that of baseline methods. Thus, our proposed method is superior to its competitors in the block mask experiments.

TABLE 2 The Performance of Matrix Completion Methods in the Block Mask Experiments
Table 2- 
The Performance of Matrix Completion Methods in the Block Mask Experiments
4.4 Text Mask Experiments
In the text mask experiments, we mask the test images by introducing two English sentences and two Chinese sentences with different font sizes. The length of the English sentence is shorter than that of the Chinese sentence. We show the original images (original img.) and the text masked images (masked img.) in Figs. 4a and 4b, respectively. The missing rates in this set of experiments are 7.26, 15.12, 13.90, and 28.82 percent, respectively. The intrinsic structure in the English words and Chinese words makes the restoration of the text masked image easier than that of the block masked image. However, it is still harder than the restoration of the random masked image for the continuous missing entries masked by the strokes in the words.


Fig. 4.
The incomplete images and their restored results in the text mask experiments.

Show All

In Figs. 4c, 4d, 4e, 4f, 4g, and 4h, we show the recovered images of six matrix completion methods in parallel. The performance of these methods in terms of the PSNR, SNR, and RE are listed in Table 3. Once again, it is seen that our proposed method can obtain the best performance among all the methods. The images recovered by our proposed method is much clear than that of baseline methods. For example, the images recovered by the Schatten p ADMM method are obscured with the shadow of the text masks. Furthermore, by comparing the first (third) row with the second (fourth) row of Fig. 4, we can see the recovered images masked by the smaller-sized words are much clear than those masked by the larger-sized words. Apparently, the less the missing entries in the image, the easier the matrix completion. Finally, we can also see that the recovered images masked by Chinese words are less clear than those masked by English words with the same font size. This is obvious since the structure of Chinese words is much complex than that of English words. Thus, more missing entries reside in the images masked by Chinese words.

TABLE 3 The Performance of Matrix Completion Methods in the Text Mask Experiments
Table 3- 
The Performance of Matrix Completion Methods in the Text Mask Experiments
4.5 Parameters Related Experiments
In the parameters related experiments, we test the influence of parameters p and τ on the accuracy of the matrix completion. The parameter p is an important parameter in the SCp norm based method which controls the non-convexity of the objective function in the problem (7). For convenience, we generate the strictly low-rank matrix with rank k by only keeping the largest k singular values in the original matrix and then multiplying it with the left and right singular value matrices [8]. In the following experiment, we randomly discarding 50 percent of the entries in the original Lena image and its strictly low-rank versions with k equals 30, 20 and 10, respectively. The accuracy of our proposed method in terms of PSNR and RE at different p and k is shown in Fig. 5. It can be seen that the accuracy of our proposed method first increases with the increase of the parameter p and then decreases gradually. Obviously, the recovered matrix with the lowest rank is not the optimal solution we desired, which verifies the first challenge we proposed in Section 1. Meanwhile, the best parameter p increases along with the rank k. In other words, the less complex the incomplete matrix, the smaller the best parameter p.

Fig. 5. - 
The accuracy of the SC$p$p norm based method at different $p$p and $k$k.
Fig. 5.
The accuracy of the SCp norm based method at different p and k.

Show All

As to our proposed method, τ is another important parameter that needs to be analyzed. As shown in the definition of the Schatten capped p norm, the parameter τ controls the minimization of insignificant singular values. In the following experiment, we set p=0.5 and randomly discard 50 percent of the entries in the strictly low-rank version of the Lena image with k=20. The PSNR of our proposed method at different τ is shown in Fig. 6. It can be seen that the accuracy of our proposed method first increases rapidly, and then decreases slightly with the parameter τ. Finally, it arrives at a stable status. When τ is small, the principal information that is in correspondence with the significant singular values is filtered out. Thus, the accuracy of our proposed method is low. With the increase of τ, significant information is gradually preserved in the objective function of the optimization problem (7). Thus, the accuracy of our proposed method increases correspondingly. When τ is greater than the largest singular value σ1, the increase of τ can not affect the accuracy since all the singular values are preserved for the optimization problem (7). Therefore, the idea of treating the larger singular values and the smaller singular values differently can improve the accuracy of matrix completion, which verifies the second challenge we proposed in Section 1.

Fig. 6. - 
The accuracy of the SC$p$p norm based method at different $\tau$τ.
Fig. 6.
The accuracy of the SCp norm based method at different τ.

Show All

SECTION 5Summary and Future Work
In this paper, we designed a new Schatten capped p norm and proposed a matrix completion method based on the Schatten capped p norm for solving the low-rank matrix completion problem. The non-convex Schatten capped p norm can balance between the rank and the nuclear norm of the matrix. It can also generalize the rank and several existing matrix norms. To solve the SCp norm minimization problem, we proposed the matrix completion method based on the Schatten capped p norm by following the framework of the alternating direction method of multipliers. Meanwhile, the SCp norm regularized least squares subproblem was analyzed in detail and solved explicitly. The image inpainting experiments, including the random mask experiments, the block mask experiments, and the text mask experiments, were performed based on the real-world image data set. The experimental results demonstrate that our proposed method can improve the accuracy of the matrix completion much better than that of the existing methods.

There are still some issues that deserve our further investigation. On one hand, we will plan to conduct the theoretical analysis on the convergence of the matrix completion method based on the proposed SCp norm. On the other hand, a high-dimensional tensor can maintain the spatial and multi-channel structure within the original data at the same time. Therefore, we will also plan to extend the matrix completion problem to the tensor completion problem to deal with the high-dimensional structured data.
