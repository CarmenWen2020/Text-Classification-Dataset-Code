Heterogeneous information network (HIN) embedding aims to learn the low-dimensional representations of nodes while preserving structures and semantics in HINs. Although most existing methods consider heterogeneous relations and achieve promising performance, they usually employ one single model for all relations without distinction, which inevitably restricts the capability of HIN embedding. In this paper, we argue that heterogeneous relations have different structural characteristics, and propose a novel Relation structure-aware HIN Embedding model, called RHINE. By exploring four real-world networks with thorough analysis, we present two structure-related measures which consistently distinguish heterogeneous relations into two categories: Affiliation Relations (ARs) and Interaction Relations (IRs). To respect the distinctive structural characteristics of relations, in RHINE, we propose different models specifically tailored to handle ARs and IRs, which can better capture the structures in HINs. Finally, we combine and optimize these models in a unified manner. Furthermore, considering that nodes connected via heterogeneous relations may have multi-aspect semantics and each relation focuses on one aspect, we introduce relation-specific projection matrices to learn node and relation embeddings in separate spaces rather than a common space, which can better preserve the semantics in HINs, referring to a new model RHINE-M. Experiments on four real-world datasets demonstrate that our models significantly outperform the state-of-the-art methods in four tasks.
SECTION 1Introduction
Network embedding aims to embed nodes into a low-dimensional space, while preserving the network structures and properties, which has shed a light on the analysis of networks and become a focal point of study interests in both academic and industrial domains [1], [2], [3]. Recent works on network embedding have achieved promising performance in many data mining tasks, such as classification [4], [5], [6], [7], recommendation [8], [9] and link prediction [10], [11], [12], [13], etc. However, most of these methods generally focus on homogeneous networks which only contain one single type of nodes and edges.

In reality, many real-life information networks consist of multiple types of nodes and edges, widely known as heterogeneous information networks (HINs) [14], [15]. Due to its flexibility in modeling heterogeneous data, HIN has been proposed as a powerful information modeling method. As shown in Fig. 1a, we present an example of HIN, i.e., the DBLP network. We can see that the DBLP network contains four types of nodes: Author (A), Paper (P), Conference (C) and Term (T), and multiple types of relations: writing/written relations, and publish/published relations, etc. In addition, there are composite relations represented by meta-paths [14], [16] such as APA (co-author relation) and APC (authors write papers published in conferences), which are widely used to exploit rich semantics in HINs. Thus, compared to homogeneous networks, HINs fuse more information and contain richer semantics.

Fig. 1. - 
The illustration of an HIN and the comparison between conventional methods and our method (non-differentiated relations versus differentiated relations).
Fig. 1.
The illustration of an HIN and the comparison between conventional methods and our method (non-differentiated relations versus differentiated relations).

Show All

HIN embedding, which provides a new perspective for heterogeneous data analysis, has thus attracted considerable research attention. A natural and straightforward idea to embed an HIN is to directly apply homogeneous network embedding models for heterogeneous data, but this will inevitably lead to reduced performance in downstream tasks.

Some researchers have been working on exploring methods for HIN embedding [17], [18], [19], [20], [21], [22], and have demonstrated the effectiveness of HIN embedding in HIN analysis applications. Roughly speaking, some methods employ meta-path based random walk to generate node sequences for optimizing the similarity between nodes [21], [23], [24]. Some methods decompose the HIN into simple networks and then optimize the proximity between nodes in each sub-network [17], [22], [25]. There are also some neural network based methods that learn non-linear mapping functions for HIN embedding [19], [20], [26], [27]. Also, some methods are designed for specific tasks, such as link prediction [28] and recommendation [22]. Although these methods are designed for heterogeneous networks and have achieved performance improvement to some extent, they usually have an assumption that one single model can handle all relations and nodes, through keeping the representations of two nodes close to each other, as illustrated in Fig. 1b.

However, various heterogeneous relations in an HIN have significantly different structural characteristics, which should be carefully handled with different models. Let's see a toy example in Fig. 1a. The relations in the network include atomic relations (e.g., AP and PC) and composite relations (e.g., APA and APC). Intuitively, AP relation and PC relation reveal rather different characteristics in structure. That is, some authors write some papers in the AP relation, which shows a peer-to-peer structure. While that many papers are published in one conference in the PC relation reveals the structure of one-centered-by-another. Similarly, APA and APC indicate peer-to-peer and one-centered-by-another structures respectively. The intuitive examples clearly illustrate that relations in an HIN indeed have different structural characteristics.

It is non-trivial to consider different structural characteristics of relations for HIN embedding, due to the following challenges: (1) How to distinguish the structural characteristics of relations in an HIN? Various relations (atomic relations or meta-paths) with different structures are involved in an HIN. Quantitative and explainable criteria are desired to explore the structural characteristics of relations and distinguish them. (2) How to capture the distinctive structural characteristics of different categories of relations? Since the various relations have different structures, modeling them with one single model may lead to some loss of information. We need to specifically design appropriate models which are able to capture their distinctive characteristics. (3) The different models for the differentiated relations should be easily and smoothly combined to ensure simple optimization in a unified manner.

In this paper, we first present a Relation structure-aware HIN Embedding method, called RHINE, which is capable of capturing the distinctive structural characteristics of relations in HINs. In specific, we first explore the structural characteristics of heterogeneous relations in HINs with intuitive observations and thorough quantitative analysis. Intuitively, we find a huge difference between the degree distributions of nodes w.r.t. various relations, which indicates the distinctive structural characteristics of relations in HINs. From four real datasets, we systematically analyze the structural characteristics of relations in HINs, and present two structure-related measures which can consistently distinguish the various relations into two categories: Affiliation Relations (ARs) with one-centered-by-another structures and Interaction Relations (IRs) with peer-to-peer structures. In order to capture the distinctive structural characteristics of the relations, we then propose two specifically designed models. For ARs where the nodes share similar properties [29], [30], we calculate euclidean distance as the proximity between nodes, so as to make the nodes directly close in the low-dimensional space. On the other hand, for IRs which bridge two compatible nodes, we model them as translations between the nodes and calculate translation-based distance as the proximity between nodes. Since the euclidean distance and translation-based distance are consistent in terms of mathematical form, they can be optimized in a unified and elegant way. Furthermore, considering that nodes connected via heterogeneous relations may have multi-aspect semantics and each relation focuses on one aspect, we introduce relation-specific projection matrices into the above two models to learn node and relation embeddings in separate spaces rather than a common space. We denote the extended model as RHINE-M. With the simultaneous projection and united training, RHINE-M ensures the unified measure of two proximities between nodes (i.e., the euclidean distance and translation-based distance) in the specific space.

It is worthwhile to highlight our contributions as follows:

To the best of our knowledge, we make the first attempt to explore the different structural characteristics of relations in HINs and present two structure-related criteria which can consistently distinguish heterogeneous relations into ARs and IRs.

We propose a novel relation structure-aware HIN embedding model (RHINE) and its extended version RHINE-M, which fully respects the distinctive structural characteristics of ARs and IRs by exploiting appropriate models and combining them in a unified and elegant manner.

We conduct comprehensive experiments to evaluate the performance of our models. Experimental results demonstrate that our models significantly outperform state-of-the-art network embedding models in various tasks.

Please notice that the preliminary work has been accepted for oral presentation at the Thirty-Third AAAI Conference on Artificial Intelligence (AAAI-19) [31]. Based on the conference paper, we substantially extend the original work from the following four aspects: (1) Considering that nodes connected via heterogeneous relations may have multifaceted semantics and various relations focus on different aspects, we extend the original RHINE into a novel RHINE-M model with relation-specific projection matrices. RHINE-M learns node embeddings with respect to different relations in separate latent spaces, rather than a common space. Besides, in five data mining tasks, we demonstrate the superior performance of RHINE-M on four datasets. (2) In order to more intuitively illustrate the difference in various relations, we further analyze the distinctive structural characteristics of heterogeneous relations in HINs and conduct qualitative analysis from the perspective of the node degree distribution on four real-world HINs. (3) To further verify the effectiveness of the proposed models, we significantly enrich experiments with two new baselines (i.e., HERec and JUST), a new dataset (i.e., Amazon) and a new tasks (i.e., node recommendation). Concretely, we evaluate our proposed RHINE and RHINE-M on the new dataset, and our methods consistently achieve outstanding performances in five tasks. Compared with two new baselines (i.e., HERec and JUST), our methods also perform better than them. (4) A comprehensive survey of related work is provided in our paper, ranging from general network embedding to HIN embedding. Besides, we carefully polish our paper and improve the language quality.

The rest of the paper is organized as follows. In Section 2, we summarize and compare the related works. Section 3 describes notations used in the paper and presents some preliminary knowledge. Then, we describe four real-world HINs and analyze the structural characteristics of relations in Section 4. Section 5 presents our proposed models, i.e., RHINE and RHINE-M in detail. Extensive experiments are done to validate the proposed models in Section 6. Finally, we conclude the paper in Section 7.

SECTION 2Related Work
In this section, we first introduce the related methods of general network embedding, and then discuss the recent works on HIN embedding.

2.1 Network Embedding
The goal of network embedding is to project a network into a low-dimensional vector space under the principle of preserving the original structural information and properties in networks [1], [2], [3]. Hence, network embedding can be traced back to the dimensionality reduction techniques. Traditional dimensionality reduction techniques typically learn the latent low-dimensional vectors for nodes or edges by decomposing a network [32], [33]. Graph factorization [34] represents a graph as a matrix where matrix elements correspond to edges between nodes, and then learns a low-dimensional representation of a graph through matrix factorization. However, decomposition-based models are inflexible and not scalable. They suffer from the computational cost of decomposing a large-scale matrix, making them neither practical nor effective for addressing data mining tasks in large networks.

Recently, rapid development on deep learning has shed a light on network representation learning and significant effort has been devoted to designing neural network-based representation learning models [4], [5], [6], [7], [10], [11], [13], [35], [36]. Inspired by word2vec [37], [38], Perozzi et al. propose DeepWalk [4] based on truncated random walks and Skip-gram model. DeepWalk views node sequences generated from random walks as “sentences”, and nodes as “words”, and then maximizes the co-occurrence probability among nodes. Based on DeepWalk, Grover et al. propose node2vec [10] which obtains nodes’ neighbors by employing breadth-first and depth-first sampling on homogeneous networks. In addition, to learn representation of large-scale network, Tang et al. present LINE [5] which captures both the first-order and second-order proximities in networks. M2DNE [39] is proposed to learn node embeddings for temporal network, which incorporates micro and macro-dynamics into temporal network embedding. However, these methods only focus on learning the representation of nodes in homogeneous information networks, leading to the fact that they could not be directly applied for HIN embedding.

2.2 HIN Embedding
As a newly emerging network model, HINs can naturally model complex objects and their rich relations. HIN embedding, which aims to embed multiple types of nodes into a low-dimensional space, has received growing attention. Considerable researches have been done on representation learning for HINs [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28]. Broadly speaking, the existing works on HIN embedding can be categorized into four types: random walk based methods [21], [23], [24], decomposition based methods [17], [22], [25], deep neural network based methods [19], [20], [26], [27] and task-specific methods [18], [20], [22], [27], [28].

2.2.1 Random Walk Based Methods
Random walk based methods are inspired by word2vec [37], [38], where a node vector should be able to reconstruct the vectors of its neighborhood nodes which are defined by co-occurrence rate. Typically, metapath2vec [24] formalizes meta-path based random walks to construct heterogeneous neighborhoods of a node and leverages Skip-gram of word2vec [38] to learn the representation of networks. HIN2Vec [21] conducts random walk and learns latent vectors of nodes and meta-paths by conducting multiple prediction training tasks jointly. Shang et al. propose ESim [23] to perform random walks based on user-defined meta-paths on HINs, and learn the vector representation of nodes appearing in the instance by maximizing the probability of meta-path instances.

2.2.2 Decomposition Based Methods
Decomposition based methods separate an HIN into multiple simple homogeneous networks, and respectively embed these networks into low-dimensional spaces. As an extension of LINE, PTE [17] is proposed to suit HIN embedding. It decomposes a HIN to a set of edgewise bipartite networks and then performs network embedding individually by using LINE. EOE [25] decomposes the complex academic heterogeneous network into a word co-occurrence network and an author cooperative network, and simultaneously performs representation learning on node pairs in sub-networks.

2.2.3 Deep Neural Network Based Methods
Deep neural network based methods benefit from the powerful modeling capabilities of deep models, which employ different deep neural models, such as MLP, CNN and Autoencoder, etc. to model heterogeneous data. For instance, HNE [19] utilizes CNN and MLP to extract the features of text and image data respectively, and then projects different types of data into the same space through the transfer matrix to overcome the challenges of modeling heterogeneous data. SHINE [20] uses the autoencoder to encode and decode the heterogeneous information in the social network, the emotional network and the portrait network respectively to obtain the feature representation, and then fuses these representations through an aggregate function to obtain the final node embeddings.

2.2.4 Task-Specific Methods
Task-specific methods mainly focus on solving a specific task (e.g., link prediction or recommendation) with representation learning on HINs. In order to predict links between nodes with different types in HINs, PME [28] projects different types of nodes into the same relation space and conducts heterogeneous link prediction. For recommendation in e-commerce, HERec [22] integrates matrix factorization with HIN embedding and predicts ratings for items. Fan et al. [40] proposes a embedding model metagraph2vec, where both the structures and semantics are maximally preserved for malware detection.

To sum up, all the above mentioned models deal with various relations without distinguishing their different properties and handle them with one single model. In this paper, we explore and distinguish the structural characteristics of relations with thorough qualitative and quantitative analysis. For relations with distinct structural and semantic characteristics, we propose to handle them with specifically designed models.

SECTION 3Preliminaries
In this section, we introduce some basic concepts and formalize the problem of HIN embedding.

3Definition 1 Heterogeneous Information Network (HIN) [15].
An HIN is defined as a graph G=(V,E,T,ϕ,φ), in which V and E are the sets of nodes and edges, respectively. Each node v and edge e are associated with their type mapping functions ϕ:V→TV and φ:E→TE, respectively. TV and TE denote the sets of node and edge types, where |TV|+|TE|>2, and T=TV∪TE.

In HINs, two objects can be connected via different semantic paths, called meta-paths, which describe composite relations between objects. We define the concept of meta-paths and node-relation triples as follows.

3Definition 2 Meta-path [14].
Given an HIN G=(V,E,T,ϕ,φ), a meta-path m∈M is defined as a sequence of node types tvi or edge types tej in the form of tv1⟶te1tv2...⟶teltvl+1 (abbreviated as tv1tv2...tvl+1), which describes a composite relation between v1 and vl+1.

3Definition 3 Node-Relation Triple.
In an HIN G, relations R include atomic relations (e.g., links) and composite relations (e.g., meta-paths). A node-relation triple ⟨u,r,v⟩∈P, describes that two nodes u and v are connected by a relation r∈R. Here P represents the set of all node-relation triples.

3Example 1.
As shown in Fig. 1a, a meta-path A⟶writeP ⟶PublishedC (abbreviated as APC) describes a composite relation between authors and conferences, which indicates that ‘authors write papers published in conferences’. In Fig. 1a, both ⟨a1,AP,p1⟩ and ⟨a2,APC,c2⟩ are node-relation triples of DBLP.

3Definition 4 Heterogeneous Information Network Embedding.
Given an HIN G=(V, E, T, ϕ, φ), the goal of HIN embedding is to project nodes into a latent low-dimensional representation space while preserving the network structure and properties. Formally, we aim to develop a mapping function f:V→Rd that projects each node v∈V to a low-dimensional vector in Rd, where d≪|V|.

SECTION 4Structural Characteristics Analysis of Relations
In this section, we first describe four real-world HINs and analyze the structural characteristics of relations in HINs. Then we present two structure-related measures which can consistently distinguish various relations quantitatively.

4.1 Dataset Description
Before analyzing the structural characteristics of relations, we first briefly introduce four datasets used in this paper, including DBLP,1 Yelp,2 AMiner3 [41] and Amazon.4 The detailed statistics of these datasets are illustrated in Table 1.

TABLE 1 Statistics of the Four Datasets
Table 1- 
Statistics of the Four Datasets
DBLP is an academic network, which contains four types of nodes: author (A), paper (P), conference (C) and term (T). We extract node-relation triples based on the set of relations {AP, PC, PT, APC, APT}. Yelp is a social network, which contains five types of nodes: user (U), business (B), reservation (R), service (S) and star level (L). We consider the relations {BR, BS, BL, UB, BUB}. AMiner is also an academic network, which contains four types of nodes, including author (A), paper (P), conference (C) and reference (R). We consider the relations {AP, PC, PR, APC, APR}. Amazon is a product purchasing dataset, from which we extract a subset that contains four types of nodes, including user (U), item (I), brand (B) and tag (T). We consider the relations {UI, IB, IT, UIB, UIT}. Notice that we can actually analyze all the relations based on meta-paths. However, not all meta-paths have a positive effect on embeddings [42], [43] and the selection of meta-path is still an open question [44]. In addition, our work focuses on the exploration of structural characteristics of various relations in HINs, rather than the selection of meta-paths. Hence, following previous works [23], [24], we choose the popular and meaningful meta-paths in this work.

4.2 Affiliation Relations and Interaction Relations
In order to explore the structural characteristics of relations, we present mathematical analysis on the above datasets.

4.2.1 Data Observation
Since the degree of nodes can well reflect the structures of networks [45], we first conduct some data observation with respect to node degree distributions on four datasets. To be specific, given a relation r, we have the node types at both ends of the relation r, denoted as tu and tv. Then, we calculate two distributions of node degree under the relation r, denoted as Dis(tu|r) and Dis(tv|r). Here, we plot the degree distribution of some typical relations on four datasets due to similar trends of other relations. For DBLP dataset, we show the degree distributions of nodes with type A and type C with respect to APC in Fig. 2a, and the degree distributions of nodes with type A and type T with respect to APT in Fig. 2d. For Yelp and AMiner, we show the degree distributions of nodes in Figs. 2b and 2e, and Figs. 2c and 2f, respectively.

Fig. 2. - 
Degree distribution of nodes connected via different relations on four datasets.
Fig. 2.
Degree distribution of nodes connected via different relations on four datasets.

Show All

Observing the degree distribution of nodes connected via different relations on the same dataset (i.e., compare the up and down figures in Fig. 2), we can find that different relations have obviously distinct structural characteristics. Taking the DBLP dataset as an example, we can observe from Figs. 2a and 2d that the degree distributions of nodes with respect to relation APC and APT are quite different. In terms of the relation APC (i.e. Fig. 2a), we find that the degree distribution of nodes with type A is significantly different from that of nodes with type C. This fact implies that two types of nodes connected via relation APC are extremely unbalanced, and such relation means an inequivalent structure. On the other hand, the two degree distributions of nodes with respect to relation APT are almost identical, which shows that the similar and compatible structural roles of two types of nodes connected via relation APT. With the above analysis in terms of the degree distribution of nodes, we find that different relations in an HIN have quite distinctive structural characteristics, which should be carefully analyzed and considered for HIN embedding.

4.2.2 Quantitative Analysis
Data observation only gives us an intuitive understanding, and more accurate and reasonable quantitative analysis is a must. Hence, we define a degree-based measure D(r) to explore the distinction of various relations in an HIN. Specifically, we compare the average degrees of two types of nodes connected with the relation r, via dividing the larger one by the smaller one (D(r)≥1). Formally, given a relation r with nodes u and v (i.e., node relation triple ⟨u,r,v⟩), tu and tv are the node types of u and v, we define D(r) as follows:
D(r)=max[d¯tu,d¯tv]min[d¯tu,d¯tv],(1)
View SourceRight-click on figure for MathML and additional features.where d¯tu and d¯tv are the average degrees of nodes of the types tu and tv respectively.

A large value of D(r) indicates quite inequivalent structural roles of two types of nodes connected via the relation r (one-centered-by-another), while a small value of D(r) means compatible structural roles (peer-to-peer). In other words, relations with a large value of D(r) show much stronger affiliation relationships. Nodes connected via such relations share much more similar properties [29]. While relations with a small value of D(r) implicate much stronger interaction relationships. Therefore, we call the two categories of relations as Affiliation Relations (ARs) and Interaction Relations (IRs), respectively.

In order to better understand the structural difference between various relations, we take the DBLP network as an example. As shown in Table 1, for the relation PC with D(PC)=718.8, the average degree of nodes with type P is 1.0 while that of nodes with type C is 718.8. It shows that papers and conferences are structurally inequivalent. Papers are centered by conferences. While D(AP)=1.1 indicates that authors and papers are compatible and peer-to-peer in structure. This is consistent with our common sense. Semantically, the relation PC means that ‘papers are published in conferences’, indicating an affiliation relationship. Differently, AP means that ‘authors write papers’, which explicitly describes an interaction relationship.

In fact, we can also define some other measures to capture the structural difference. For example, we compare the relations in terms of sparsity, which can be defined as:
S(r)=NrNtu×Ntv,(2)
View SourceRight-click on figure for MathML and additional features.where Nr represents the number of relation instances following r. Ntu and Ntv mean the number of nodes with type tu and tv, respectively. The measure can also consistently distinguish the relations into two categories: ARs and IRs. The detailed statistics of all the relations in the four HINs are shown in Table 1.

Evidently, Affiliation Relations and Interaction Relations exhibit rather distinct characteristics: (1) ARs indicate one-centered-by-another structures, where the average degrees of the types of end nodes are extremely different. They imply an affiliation relationship between nodes. (2) IRs describe peer-to-peer structures, where the average degrees of the types of end nodes are compatible. They suggest an interaction relationship between nodes.

SECTION 5Relation Structure-Aware HIN Embedding
In this section, we present a novel Relation structure-aware HIN Embedding model (RHINE), which individually handles two categories of relations (ARs and IRs) with different models in order to preserve their distinct structural characteristics, as illustrated in Fig. 3.

Fig. 3. - 
The overall architecture of the proposed models. (a) An HIN can be abstracted as a network schema describing various relations in the network. (b) Analysis of structural characteristics clearly divides heterogeneous relations into ARs and IRs in an HIN. (c) The proposed RHINE individually handles ARs and IRs with two carefully designed models. (d) The proposed RHINE-M extends RHINE with relation-specific projection matrices. (e) RHINE and RHINE-M derive node embeddings, respectively.
Fig. 3.
The overall architecture of the proposed models. (a) An HIN can be abstracted as a network schema describing various relations in the network. (b) Analysis of structural characteristics clearly divides heterogeneous relations into ARs and IRs in an HIN. (c) The proposed RHINE individually handles ARs and IRs with two carefully designed models. (d) The proposed RHINE-M extends RHINE with relation-specific projection matrices. (e) RHINE and RHINE-M derive node embeddings, respectively.

Show All

5.1 Basic Idea
Through our exploration with thorough mathematical analysis, we find that the heterogeneous relations can be typically divided into ARs and IRs with different structural characteristics. In order to respect their distinct characteristics, we need to specifically design different while appropriate models for the different categories of relations.

For ARs, we propose to take euclidean distance as a metric to measure the proximity of the connected nodes in the low-dimensional space. There are two motivations behind this: (1) First of all, ARs show affiliation structures between nodes, which indicate that nodes connected via such relations share similar properties [29], [30]. Hence, nodes connected via ARs could be directly close to each other in the vector space, which is also consistent with the optimization of euclidean distance [46]. (2) Additionally, one goal of HIN embedding is to preserve the high-order proximity. Euclidean distance can ensure that both first-order and second-order proximities are preserved as it meets the condition of the triangle inequality [47].

Different from ARs, IRs indicate strong interaction relationships between compatible nodes, which themselves contain important structural information of two nodes. Thus, we propose to explicitly model an IR as a translation between nodes in the low-dimensional vector space. Additionally, the translation based distance is consistent with the euclidean distance in the mathematical form [48]. Therefore, they can be smoothly combined in a unified and elegant manner.

5.2 Different Models for ARs and IRs
In this subsection, we introduce two different models exploited in RHINE for ARs and IRs, as shown in Fig. 3c.

5.2.1 Euclidean Distance for Affiliation Relations
Nodes connected via ARs share similar properties [29], therefore nodes could be directly close to each other in the vector space. We take the euclidean distance as the proximity measure of two nodes connected by an AR.

Formally, given an affiliation node-relation triple ⟨p,s,q⟩∈PAR where s∈RAR is the relation between p and q with weight wpq, the distance between p and q in the latent vector space is calculated as follows:
f(p,q)=wpq||xp−xq||22,(3)
View SourceRight-click on figure for MathML and additional features.in which xp∈Rd and xq∈Rd are the embedding vectors of p and q, respectively. As f(p,q) quantifies the distance between p and q in the low-dimensional vector space, we aim to minimize f(p,q) to ensure that nodes connected by an AR should be close to each other. Hence, we define the margin-based loss [48] function as follows:
LEuAR=∑s∈RAR∑⟨p,s,q⟩∈PAR∑⟨p′,s,q′⟩∈P′ARmax[0,γ+f(p,q)−f(p′,q′)],(4)
View SourceRight-click on figure for MathML and additional features.where γ>0 is a margin hyperparameter. PAR is the set of positive affiliation node-relation triples, while P′AR is the set of negative affiliation node-relation triples.

5.2.2 Translation-Based Distance for Interaction Relations
Interaction Relations demonstrate strong interactions between nodes with compatible structural roles. Thus, different from ARs, we explicitly model IRs as translations between nodes.

Algorithm 1. The Optimization Model Algorithm
Input: An HIN G=(V,E,T,ϕ,φ), AR set RAR, IR set RIR, interaction triple batch size batch_sizeIR, affiliation triple batch size batch_sizeAR, margin γ, embedding dimension d, negative samples k, epochs I

Output: Node embedding matrix X∈R|V|×d and relation embedding matrix Y|R|×d.

Initialize

X∈R|V|×d and Y|R|×d with uniform distribution;

Interaction node-relation triples PIR ←∅;

Affiliation node-relation triples PAR ←∅.

for each interaction relation r in RIR do

PIR.add(GenerateTriples(r))

end

for each affiliation relation s in RAR do

PAR.add(GenerateTriples(s))

end

X, Y = RHINE(PIR, PAR)

return X, Y;

RHINE(PIR, PAR)

iter← 0

repeat

Sample(PIR, batch_sizeIR);

Compute gradients;

Update node embeddings X and relation embeddings Y;

Sample(PAR, batch_sizeIR);

Compute gradients;

Update node embeddings X;

iter←iter+1;

until iteration>=I

return X, Y;

Formally, given an interaction node-relation triple ⟨u,r,v⟩ where r∈RIR with weight wuv, we define the score function as:
g(u,v)=wuv||xu+yr−xv||,(5)
View SourceRight-click on figure for MathML and additional features.where xu and xv are the node embeddings of u and v respectively, and yr is the embedding of the relation r. Intuitively, this score function penalizes deviation of (xu+yr) from the vector xv.

For each interaction node-relation triple ⟨u,r,v⟩∈PIR, we define the margin-based loss function as follows:
LTrIR=∑r∈RIR∑⟨u,r,v⟩∈PIR∑⟨u′,r,v′⟩∈P′IRmax[0,γ+g(u,v)−g(u′,v′)],(6)
View SourceRight-click on figure for MathML and additional features.where PIR is the set of positive interaction node-relation triples, while P′IR is the set of negative interaction node-relation triples.

5.3 A Unified Model for HIN Embedding
Finally, we smoothly combine the two models for different categories of relations by minimizing the following loss function:
L=LEuAR+LTrIR(7)
View SourceRight-click on figure for MathML and additional features.
=∑s∈RAR∑⟨p,s,q⟩∈PAR∑⟨p′,s,q′⟩∈P′ARmax[0,γ+f(p,q)−f(p′,q′)]+∑r∈RIR∑⟨u,r,v⟩∈PIR∑⟨u′,r,v′⟩∈P′IRmax[0,γ+g(u,v)−g(u′,v′)].
View SourceRight-click on figure for MathML and additional features.

5.3.1 Sampling Strategy
As shown in Table 1, the distributions of ARs and IRs are quite unbalanced. Additionally, the proportion of relations are unbalanced within ARs and IRs. Traditional edge sampling may suffer from under-sampling for relations with a small amount or over-sampling for relations with a large amount. To address the problems, we draw positive samples according to their probability distributions. As for negative samples, we follow previous work [48], [49] to construct a set of negative node-relation triples P′(u,r,v)={(u′,r,v)|u′∈V}∪{(u,r,v′)|v′∈V} for the positive node-relation triple (u,r,v), where either the head or tail is replaced by a random node, but not both at the same time.

5.3.2 Optimization Algorithm
With the objective function (i.e., Eq. (7)), we adopt stochastic gradient descent algorithm for optimization. We optimize euclidean Distance based model and Translation-based distance model, step by step. To be specific, we first initialize node embedding matrix X and relation embedding matrix Y with uniform distribution. Then we sample positive and negative interaction node-relation triples, and update X and Y correspondingly. In the same way, we sample positive and negative affiliation node-relation triples, and only update X. Finally, the node embedding matrix X and relation embedding matrix Y are returned. The detail of the optimization algorithm is illustrated in Algorithm 1. The code of our model is publicly available at website,5

5.4 The Extended Model RHINE-M
The proposed RHINE takes into account the structural characteristics of heterogeneous relations for HIN embedding, which embeds various nodes and relations in a common low-dimension latent space. However, nodes of the same type connected via heterogeneous relations may have multi-aspect semantics and each relation focus on one aspect. For instance, an author (i.e., a node of type A) may indicate a writer or participant, depending on whether he/she is in the AP relation or APC relation. Hence, we need to project node embeddings into a relation-specific representation space. Following this idea, we extend the original RHINE to RHINE-M with relation-specific projection matrices, as shown in Fig. 3(d).

In specific, we introduce a relation-specific projection matrix Mr∈Rd×d to project nodes from the node representation space to corresponding relation-specific spaces. The projection matrix makes it much more effective to measure proximities between nodes in the relation-specific space, thus RHINE-M is capable of capturing structures and semantics in an HIN at a finer-grained manner. Hence, we redefine Eq. (3) as follows:
f(p,q)=wpq||Msxp−Msxq||22,(8)
View SourceRight-click on figure for MathML and additional features.where Ms is the relation s-specific projection matrix. With the above definition, we can directly enable nodes connected via ARs closer and push nodes without ARs farther in the certain relation space.

Similarly, we leverage projection matrices to enhance the embeddings learned with translation-based distance. Thus, we reformulate Eq. (5) as follows:
g(u,v)=wuv||Mrxu+yr−Mrxv||,(9)
View SourceRight-click on figure for MathML and additional features.where Mr is also a projection matrix corresponding to the relation r.

Notice that here we apply the relation-specific projection matrix for both the euclidean distance and the translation-based distance, which not only enables node proximities to be measured in the unified space, but also ensures that our RHINE-M can be jointly trained. Assuming that we only apply the transformation matrix to the euclidean distance, the node proximity measured in our model will not be in the same space. That is, the node proximity based on the euclidean distance is measured in the node space, while the node proximity of the translation-based distances is measured in the relation-specific space, which is inconsistent with our motivation.

With the above definition, we can project nodes from node space to relation-specific space, which makes nodes that actually hold the relation close with each other, otherwise far away. By replacing Eqs. (3) and (5) with Eqs. (8) and (9) in Eq. (7), we initialize RHINE-M with node embeddings learned using RHINE and optimize RHINE-M in the same way as RHINE. At last, we can learn the node embeddings for downstream tasks.

5.5 Discussion of RHINE and RHINE-M
By taking the structural characteristics of heterogeneous relations into consideration, the proposed RHINE specifically tailors the euclidean distance and translation-based distance to respect the distinctive structural characteristics of relations in HINs. With the euclidean distance for ARs, nodes connected via affiliation relations, i.e., one centered by another, are directly pushed close in the latent low-dimensional space. On the other hand, translation-based distance bridges two compatible nodes with relation representations. Jointly training the euclidean distance and translation-based distance ensures that our RHINE can capture the structural characteristics of heterogeneous relations.

RHINE assumes embeddings of nodes and relations being in the same latent low-dimensional space. However, nodes connected via heterogeneous relations may have multifaceted semantics and various relations focus on different aspects. Hence, it is intuitive that heterogeneous nodes and relations in HINs should be embedded in distinct spaces. Inspired by TransH [50] and TransR [49], we introduce a relation-specific projection matrix to map nodes from the node space to the corresponding relation-specific space. Different from TransR modeling knowledge graph with translation mechanism, we not only apply relation-specific matrix for translation-based distance but also the euclidean distance. As mentioned before, with the simultaneous projection and united training, RHINE-M ensures the unified measure of two proximities between nodes (i.e., the euclidean distance and translation-based distance) in the specific space.

SECTION 6Experiments
In this section, we conduct extensive experiments to demonstrate the effectiveness of RHINE. Code and datasets are available at https://github.com/rootlu/RHINE.

6.1 Datasets
As described in Section 4.1, we conduct experiments on four datasets, including DBLP, Yelp, AMiner and Amazon. The statistics of them are summarized in Table 1.

DBLP is an academic dataset in computer science, consisting of 14,456 authors, 14,375 papers, 8,811 terms and 20 conferences. In DBLP, 200 papers are labeled with their research areas such as data mining. We conduct clustering and classification on papers.

Yelp is a social media dataset provided by Yelp Challenge. We extract information related to restaurants of three sub-categories: “American (New) Food”, “Fast Food” and “Sushi Bars” [43], and construct a HIN, which includes 1,286 users, 2,614 businesses, 9 star levels, 2 service types and 2 reservation types. We conduct clustering and classification on businesses.

AMiner is also an academic network in computer science, which is much larger than DBLP, including 164,472 authors, 127,623 papers, 147,251 references and 101 conferences. We take 10 domains related to computer science as labels of papers, and perform clustering and classification on papers.

Amazon is a product purchasing dataset. We extract information and construct a HIN, which contains 15,619, users, 8,493 items, 22,140 tags and 22 brands. We take 9 categories as labels of items (e.g., sports, electronics, etc), and perform clustering and classification on items.

6.2 Baseline Methods
We compare our proposed model with seven state-of-the-art network embedding methods, the first two of which are designed for homogeneous networks and the rest are capable of modeling heterogeneous information networks. We use codes of the baseline methods provided by their authors.

DeepWalk [4]6 performs a random walk on networks and then learns low-dimensional node vectors via the skip-gram model.

LINE [5]7 considers first-order and second-order proximities in networks. We denote the model that only uses first-order or second-order proximity as LINE-1st or LINE-2nd, respectively.

PTE [17]8 decomposes an HIN to a set of bipartite networks and then learns the low-dimensional representation of the network.

ESim [23]9 takes a given set of meta-paths as input to learn a low-dimensional vector space. For a fair comparison, we use the same meta-paths with equal weights in Esim and our model RHINE.

HIN2Vec [21]10 learns the latent vectors of nodes and meta-paths in an HIN by conducting multiple prediction training tasks jointly.

Metapath2vec [24]11 leverages meta-path based random walks and skip-gram model to perform node embedding. We leverage the meta-paths APCPA, UBSBU and APCPA in DBLP, Yelp and AMiner respectively, which perform best in the evaluations.

HERec [22]12 designs a type constraint strategy to filter the node sequence and utilizes Skip-gram to embed the heterogeneous information network.

JUST [51]13 is a heterogeneous graph embedding technique using random walks with jump and stay strategies to learn node embeddings in an more efficient manner.

6.2.1 Parameter Settings
For a fair comparison, we set the embedding dimension d=100 and the size of negative samples k=3 for all models. For DeepWalk, HIN2Vec, metapath2vec and JUST based on random walk, we set the number of walks per node w=10, the walk length l=100 and the window size τ=5. For our models RHINE and RHINE-M, margin γ is set to 1 and learning rate α=0.005. We set the batch size and the number of epochs to 128 and 400, respectively.

6.3 Node Clustering
We conduct node clustering experiments to illustrate how the latent representations learned by embedding methods can benefit the node clustering task in HINs [52], [53].

6.3.1 Experimental Setting
In this task, we learn the representations of nodes using network embedding methods mentioned above. Based on the learned node embeddings, we leverage the K-means algorithm to cluster the nodes and evaluate the clustering results in terms of normalized mutual information (NMI) [54]. All clustering experiments are conducted 10 times. We report the average performance in Table 2.

TABLE 2 Performance Evaluation of Node Clustering
Table 2- 
Performance Evaluation of Node Clustering
6.3.2 Results
As shown in Table 2, our model RHINE significantly outperforms all the compared methods. (1) Compared with the best competitors, the clustering performance of our model RHINE improves by 18.79, 6.15, 7.84, 6.12 percent on DBLP, Yelp, AMiner and Amazon, respectively. It demonstrates the effectiveness of our model RHINE by distinguishing the various relations with different structural characteristics in an HIN. In addition, it also validates that we utilize appropriate models for different categories of relations. (2) In all baseline methods, homogeneous network embedding models achieve the lowest performance, because they ignore the heterogeneity of relations and nodes. (3) RHINE and RHINE-M significantly outperform existing HIN embedding models (i.e., ESim, HIN2Vec, metapath2vec, HERec and JUST) on all datasets. We believe the reason is that our proposed methods with appropriate models for different categories of relations can better capture the structural and semantic information of HINs. Furthermore, RHINE-M performs slightly better than RHINE due to the separate relation-specific representation spaces.

6.4 Link Prediction
Link prediction aims to estimate the likelihood of the existence of a link between two nodes in a network. Formally, given a node pair ⟨u,v⟩, we aim to predict whether there exists a relation r between them in the network [55].

6.4.1 Experimental Setting
We model the link prediction problem as a binary classification problem that aims to predict whether a link exists. In this task, we conduct co-author (A-A) and author-conference (A-C) link prediction for DBLP and AMiner. For Yelp and Amazon, we predict user-business (U-B) and user-item (U-I) links which indicate whether a user reviews a business/item. We first randomly separate the original network into training network and testing network, where the training network contains 80 percent relations to be predicted (i.e., A-A, A-C, U-B and U-I) and the testing network contains the rest. Then, we train the embedding vectors on the training network and evaluate the prediction performance on the testing network.

6.4.2 Results
The results of link prediction task are reported in Table 3 with respect to AUC and F1 score. It is clear that both RHINE and RHINE-M perform better than all baseline methods on four datasets. The reason behind the improvement is that our models based on euclidean distance modeling relations can capture both the first-order and second-order proximities. In addition, our models distinguishes multiple types of relations into two categories in terms of their structural characteristics, and thus can learn better embeddings of nodes, which are beneficial for predicting complex relationships between two nodes. Since RHINE-M learn node embeddings in relation-specific spaces, it can captures much more semantics and structural information.

TABLE 3 Performance Evaluation of Link Prediction
Table 3- 
Performance Evaluation of Link Prediction
6.5 Multi-Class Classification
Multi-class classification is a common task to evaluate the performance of representation learning on networks. In this task, we use the labeled data to train a classifier and evaluate the performance on test set [56], [57].

6.5.1 Experimental Setting
In this task, we employ the same labeled data used in the clustering task. After learning the node vectors, we train a logistic classifier with 40, 60 and 80 percent of the labeled nodes and test with the remaining data. We use Micro-F1 and Macro-F1 score as the metrics for evaluation [24].

6.5.2 Results
We summarize the results of classification in Table 4. As we can observe, (1) RHINE and RHINE-M achieve better performance than all baseline methods on all datasets except Aminer. It improves the performance of node classification by about 4 percent on both DBLP and Yelp averagely. On Amazon dataset, our models RHINE and RHINE-M continue to perform well in most cases. In terms of AMiner, our models perform slightly worse than ESim, HIN2vec and metapath2vec. This may be caused by over-capturing the information of relations PR and APR (R represents references). Since an author may write a paper referring to various fields, these relations may introduce some noise. (2) Although ESim, HIN2Vec and JUST can model multiple types of relations in HINs, they fail to perform well in most cases. Our model RHINE and RHINE-M achieve good performance due to the respect of distinct characteristics of various relations. (3) The stable performances of our methods against different training ratio indicates the robustness of our learned node embeddings when served as features for node classification.

TABLE 4 Performance Evaluation of Multi-Class Classification
Table 4- 
Performance Evaluation of Multi-Class Classification
6.6 Node Recommendation
Node recommendation is a wide application of network embedding, such as business or item recommendation in social network (e.g., Yelp and Amazon). Note that the node recommendation is different from link prediction in several ways. Specifically, node recommendation can be regarded as a ranking task while link prediction can be taken as a binary classification. Additionally, node recommendation evaluates the performance of network embedding methods from the node view, which is totally different from link prediction in terms of edges [8], [22].

6.6.1 Experimental Setting
In this task, we aim to recommend conferences to an author in DBLP and AMiner networks (i.e., A-C), and we also perform business and item recommendation for a user in Yelp network and Amazon network (i.e., U-B and U-I). In specific, we first divide the original data set into a training set and test set, as in the link prediction task, and learn node embeddings on training set. Then, for each node vi in test set, we calculate the similarity between the embedding of node vi and those of other nodes in the network, referring as the ranking score between two nodes. After that, we sort the ranking scores of node vi against other nodes and derive the top-k nodes with the highest scores as candidates. At last, we compare the ground truth with the candidates and use Hit@k to evaluate the quality of recommendation.

6.6.2 Results
Fig. 4 reports the results of node recommendation on four datasets. We can observe that our proposed RHINE and RHINE-M continuously perform better than the compared methods in terms of Hit@k. We believe that the significant improvement is due to that our models preserve the higher order proximities in an HIN. What's more, the proposed RHINE and RHINE-M take the distinctive structural characteristics and semantic information into consideration, which effectively guarantees the embedding accuracy of various types of nodes connected via heterogeneous relations.

Fig. 4. - 
Performance evaluation of node recommendation.
Fig. 4.
Performance evaluation of node recommendation.

Show All

6.7 Comparison of Variant Models
In order to verify the effectiveness of distinguishing the structural characteristics of relations, we design three variant models based on RHINE as follows:

RHINEEu which does not distinguish multiple relations in HINs, and only leverages euclidean distance to embed heterogeneous information networks.

RHINETr models all nodes and relations in HINs with translation-based distance, regardless of the different categories of relations, which is just like TransE [48].

RHINERe leverages euclidean distance to model Interaction Relations while translation-based distance for Affiliation Relations.

We set the parameters of variant models as the same as those of our proposed model RHINE. The results of the three tasks are shown in Fig. 5. It is evident that our model outperforms RHINE Eu and RHINETr, indicating that it is beneficial for learning the representations of nodes by distinguishing the heterogeneous relations. Besides, we find that RHINETr achieves better performance than RHINEEu. This is due to the fact that there are generally more peer-to-peer relationships (i.e., IRs) in the networks. Directly making all nodes close to each other leads to much loss of information. Compared with the reverse model RHINERe, RHINE also achieves better performance on all tasks, which implies that two models for ARs and IRs are well designed to capture their distinctive characteristics.

Fig. 5. - 
Performance evaluation of variant models.
Fig. 5.
Performance evaluation of variant models.

Show All

6.8 Visualization
In order to understand the representation of the networks intuitively, we visualize the vectors of nodes (.e., papers) in DBLP. To be specific, we utilize t-SNE [58] to layout a vector in a 2-dimensional space, and report the results of DeepWalk, metapath2vec and our RHINE in Fig. 6.

Fig. 6. - 
Visualization of node embeddings.
Fig. 6.
Visualization of node embeddings.

Show All

It is evident that our model RHINE can separate nodes in different datasets clearly with obvious borders. As we can see, papers in the latent space are geographically clustered into four groups, which is consistent with four domains of papers. Moreover, each group is well separated from others, which demonstrates that our model learns superior node embeddings by distinguishing the heterogeneous relations in HINs. In contrast, DeepWalk and LINE barely split papers into different groups. Since ESim, HIN2Vec and Metapath2vec are designed for heterogeneous information network, they perform better than DeepWalk and LINE, but the boundary is blurry.

6.9 Parameter Analysis
In order to evaluate the influence of different parameters in our model, we investigate the sensitivity of them on node clustering task. Specifically, we investigate the sensitivity of two parameters, including the number of negative samples and the number of embedding dimension. We vary the number of embedding dimensions as 10, 50, 100, 200 and 300, and the number of negative samples as 1, 2, 3, 5 and 7. The results of node clustering are reported in Fig. 7.

Fig. 7. - 
Parameter analysis.
Fig. 7.
Parameter analysis.

Show All

As shown in Fig. 7a, the performance of our model improves with the increase of the number of embedding dimensions, and then tends to be stable once the dimension of the embeddings reaches around 100. It is evident that our model are capable to capture rich information of various relations in HINs using a low-dimensional representation. Similarly, Fig. 7b shows that as the number of negative examples increases, the performance of our model first grows and then becomes stable when the number reaches 3. Overall, the change trend is smooth, indicating that the proposed model is not very sensitive to the two parameters.

SECTION 7Conclusion
In this paper, we make the first attempt to explore and distinguish the structural characteristics of relations for HIN embedding. We present two structure-related measures which can consistently distinguish heterogeneous relations into two categories: Affiliation Relations and Interaction Relations. To respect the distinctive structures of relations, we propose a novel relation structure-aware HIN embedding model (RHINE), which individually handles these two categories of relations. Considering the heterogeneity of nodes and relations, we further introduce relation-specific projection matrices to learn node and relation embeddings in separate spaces (i.e., RHINE-M), rather than a common space. Experimental results demonstrate that RHINE and RHINE-M outperform state-of-the-art baselines in various tasks. In the future, we will explore other possible measures to differentiate relations so that we can better capture the structural information of HINs. In addition, we will exploit deep neural network based models for different relations.