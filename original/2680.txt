Hierarchical Dirichlet Process (HDP) has attracted much attention in the research community of natural language processing. Given a corpus, HDP is able to determine the number of topics automatically, possessing an important feature dubbed nonparametric that overcomes the challenging issue of manually specifying a suitable topic number in parametric topic models, such as Latent Dirichlet Allocation (LDA). Nevertheless, HDP requires a much higher computational cost than LDA for parameter estimation. By taking the advantage of multi-threading, a parallel Gibbs sampling algorithm is proposed to estimate parameters for HDP based on the equivalence between HDP and Gamma-Gamma Poisson Process (G2PP) in terms of the generative process. Unfortunately, the above parallel Gibbs sampling algorithm requires to apply the finite approximation on the number of topics manually (i.e., predefine the topic number), thus can not retain the nonparametric feature of HDP. Another drawback of the above models is the lack of capturing the semantic dependencies between words, because the topic assignment of words is independent with each other. Although some works have been done in phrase-based topic modelling, these existing methods are still limited by either enforcing the entire phrase to share a common topic or requiring much complex and time-consuming phrase mining methods. In this paper, we aim to develop a copula guided parallel Gibbs sampling algorithm for HDP which can adjust the number of topics dynamically and capture the latent semantic dependencies between words that compose a coherent segment. Extensive experiments on real-world datasets indicate that our method achieves low perplexities and high topic coherence scores with a small time cost. In addition, we validate the effectiveness of our method on the modelling of word semantic dependencies by comparing the extracted topical phrases with those learned by state-of-the-art phrase-based baselines.
SECTION 1Introduction
Topic models aim to discover latent topics embedded in textual or other types of data. For textual data, each word in a document is generated by introducing a topic layer. Particularly, such models regard topics and words as discrete random variables, which assume that topics are distributed over documents (i.e., the document-topic distribution), and words are distributed over topics (i.e., the topic-word distribution). The main purpose of topic models is to learn these two distributions through inference methods, and then use them to respectively get a semantic representation of each document and tackle the ambiguous issue of words. For instance, Latent Dirichlet Allocation (LDA) [1] is one of the most widely used parametric topic models. LDA assumes that both document-topic and topic-word distributions are multinomial distributions, and both of their prior distribution to be the Dirichlet distribution. This assumption benefits the computation of posterior probabilities since the Dirichlet distribution is the conjugate prior of the multinomial distribution. However, LDA leaves the number of topics as a manually specified parameter, which is usually intractable to find the best option. To address this, a nonparametric Bayesian model named Hierarchical Dirichlet Process (HDP) has been proposed [2]. HDP is a three-layer Bayesian network, in which, the top two layers of Dirichlet processes are used to automatically adjust the number of topics from the data, and the bottom layer of multinomial distribution is used to generate words.

A major problem of HDP is that it requires massive computational resources for parameter inference, resulting in the difficulty of HDP to be applied to large-scale data. In the past a few years, some studies have been conducted to speed up HDP, mostly by parallelizing the parameter inference algorithm. For instance, based on the equivalence between HDP and Gamma-Gamma-Poisson Process (G2PP) in terms of the generative process [3], Cheng and Liu [4] proposed a parallel Gibbs sampling algorithm for HDP. Unfortunately, the algorithm requires a predefined topic number, as each thread associates with one topic, and the number of threads must be fixed. From this perspective, the above method loses the nonparametric feature of HDP (i.e., learning the number of topics from the data automatically).

The other frequently mentioned limitation of both LDA and HDP lies in the bag-of-words assumption, i.e., words are assumed to be independently generated and their semantic meanings are isolated. Therefore, these models neglect the latent topical dependencies between grammatically related words, e.g., the words that compose a phrase. Topical phrases have been shown to be essential in various tasks [5], [6], where mining topical phases facilitates readers to better grasp the central meanings of a corpus. To this end, researchers have proposed a series of phrase-based topic models to capture the topical dependencies of words. These topic models mainly have two stages, namely the phrase mining stage and topic modelling stage. Nevertheless, the preliminary existing methods enforce the whole phrase to share a common topic [7], [8], which is proven inappropriate by recent studies [9], [10]. On the other hand, these recent methods require much complex and time-consuming phrase mining methods at the above first stage.

In light of these considerations, we aim to develop a new parallel Gibbs sampling algorithm for nonparametric and coherent topic discovery. The main contributions of this paper can be summarized as follows: First, we propose a parallel nonparametric topic model, dubbed Topic Self-Adaptive Model (TSAM), which can reserve the nonparametric feature of HDP by automatically adding new topics and pruning marginal ones based on the topic assignment. In the above, a marginal topic refers to the topic that only a few words are assigned to, and such topics are unimportant in most cases [11]. Specifically, each topic is organized as a stack that stores all words being assigned, analogous to the “restaurant” in Chinese Restaurant Franchise [2]). Then, a mechanism that is sensitive to both of the topic capacity (i.e., the number of words assigned to it) and the quantity of unassigned words is developed to balance the processes of adding and removing topics. Second, we extend our TSAM to TSAMcop by using the apparatus of copulas, so as to model the local topical dependencies within phrases and relax the independence assumption of bag-of-words models. Recent works have proven that copulas are useful for variable dependency modelling. For instance, the vine copula and the Gaussian copula are used to reduce the bias of estimated posterior in variational inference by modelling the dependencies between latent variables (e.g., topics) [12], [13]. In this study, we refer to the autoregressive models (e.g., Recurrent Neural Networks [14]) and assume the local topical dependencies to be nested. Particularly, for a phrase of length L, we model it as a set of L−1 Markov chains ranging from orders 1 to L−1, where each node corresponds to a topic. Hence, its topical dependencies compose a “nested” structure, which can be naturally modeled by nested copulas. Although the local topical dependency modelling within phrases is similar to the global (i.e., corpus-level) dependency modelling of topics in Correlated Topic Model [15], the former is fine-grained with the help of copulas while the latter is coarse-grained by placing a logistic normal prior over the topic distributions.

For evaluation, we first validate the effectiveness of our methods on topic discovery by perplexity and normalised point-wise mutual information (NPMI). In the above, the perplexity measures the probability of the corpus being generated, and the NPMI is widely-used in evaluating the topic coherence. Second, the quality of topics generated by different methods is compared in text classification by taking the document-topic distribution as the input of classifiers. Third, TSAMcop is evaluated by recovering topical phrases from topical words [16] and comparing the results with state-of-the-art phrase-based topic models. Finally, the convergence time of various methods is presented.

SECTION 2Background
For the convenience of describing our models, we here introduce some background studies.

2.1 Hierarchical Dirichlet Process
When clustering a dataset, parametric Bayesian models assume that the number of groups is a priori, i.e., the number of clusters is predetermined. For instance, conventional topic models like LDA require an expert to predetermine a suitable number of topics. On the other hand, nonparametric Bayesian models leave the group number as a data-driven parameter, that is, the number of clusters is determined by the feature distribution of the data, instead of predefining manually. From this perspective, nonparametric Bayesian models are suitable to fit the complex data, particularly when the number of groups is a latent variable which is intractable to be observed.

Hierarchical Dirichlet process is a typical nonparametric Bayesian model for topic discovery with a data-driven topic number. The most intuitive representation of HDP is the Chinese Restaurant Process (CRP), which indicates that the number of topics increases over time, and the probability of adding a topic is controlled by a concentration parameter α of the global Dirichlet process G0∼DP(γ,H). Given the dth document Xd=(xd1,…,xdn), the generative process of HDP can be illustrated as follows:
 G0|γ,H∼DP(γ,H),Gd|α,G0∼DP(α,G0),ϕdk|Gd∼Gd, xdk|ϕdk∼p(xdk|ϕdk).
View SourceRight-click on figure for MathML and additional features.

In the above, H is the mixture component (topic) space, Gd is the topic distribution of the dth document under the global prior G0, each ϕk is a mixture component selected from G0 (hence, some ϕk may carry a same parameter set, which is well illustrated in [17]), and xdk is the word generated by the kth topic in the dth document. Each component carries a set of parameters, where the prior distribution G0 is used to ensure that each Gd shares the same set of mixture components. This is similar to LDA, where G0 corresponds to the Dirichlet prior that generates the document-topic distribution. G0 is defined by G0=∑∞k=1γkδθk, where θk∈H denotes the set of parameters carried by the kth component, i.e., the parameter set of the topic-word distribution. δθk is a dirac function of component θk, and γk is the associated mixture weight (i.e., the weight of the parameter set of the kth topic). Similarly, Gd can be represented by Gd=∑∞k=1αdkδϕk, where αdk is the weight of the kth topic zk in the dth document Xd. There are infinite components in Gd, but they strictly satisfy ∑∞k=1αdkδϕk=1. The uncertainty of composing various Gd from G0 and the infinity of mixture components in G0 enables HDP to automatically learn topic numbers from the data.

However, when assigning a word to a topic, HDP will select an existing topic with the probability proportional to the number of words that have already been assigned to it, or select a new topic otherwise. Therefore in HDP, the topics that contain more words have higher weights (the riches get richer), while the new topics will be marginalized. This can be easily illustrated by Sethuraman's stick-breaking construction of HDP [18], where the components with higher weights in G0 are more likely to be selected to compose Gd [17]. Also, there is always a chance for HDP to generate a new topic regardless the number of existing topics, which may result in redundant marginal topics and an “overlarge” topic number. Besides, similar to LDA, HDP treats each word independently, which leads to its incapability of discovering latent semantic information carried by coherent text segments, i.e., the phrases. Finally, HDP is expensive to infer through Gibbs sampling. Although many works [17], [19], [20] have been done to enhance the efficiency of HDP by variational inference, they are still limited to the non-extendable issue of serial computing.

2.2 Gamma-Gamma-Poisson Process
Developing a parallel inference algorithm for HDP directly boosts up the efficiency, but it is intractable to run HDP in parallel because the model's parameters are dependent with each other. Considering this, Cheng and Liu [4] proposed a parallel Gibbs sampling algorithm for HDP based on the equivalence of the generation process between HDP and Gamma-Gamma-Poisson Process (G2PP) [3]. To begin with, we introduce the following generative process of the Gamma-Poisson Process:
Gd|H∼GaP(H),Πd|m,Gd∼PoisP(mGd),xdk|Πd∼p(xdk|Πd),
View Sourcewhere GaP refers to the Gamma process, and PoisP refers to the Poisson process. G2PP is an extended Gamma-Poisson process, by replacing the basic measure H with another random measure G0 which is generate by another Dirichlet process on the top layer. The generative process of G2PP is given below:
G0|α,H∼GaP(αH),Gd|G0∼GaP(G0),Πd|m,Gd∼PoisP(mGd),xdk|Πd∼p(xdk|Πd).
View SourceRight-click on figure for MathML and additional features.

Similar to HDP, G0 serves as a prior to ensure that each Gd shares the same set of mixture components.

The generative process can be explained as follows. Same as HDP, we know that Gd=∑∞k=1πdkδθk is the unnormalized document-topic distribution for the dth document with a shared prior G0=∑∞k=1αkδθk, where πdk is the weight of the parameter set of the kth topic zk in the dth document Xd, i.e., πdk=p(Z=zk|Xd). The Poisson process generates Πd=∑∞k=1ndkδξk, where ndk is the number of words assigned to zk in Xd. At last, each document's words can be generated by ndk and the conditional probability p(xdk|Πd). Further proof of the equivalence between the generative process of HDP and G2PP is done in [3].

2.3 Introduction to Copulas
The copulas are useful tools to model the dependency structure of a group of random variables. Given the marginal distributions of multiple variables, a copula is able to model their multivariate joint distribution. In topic models, we can easily access the document-topic distribution, which can be regarded as the marginal distribution of each topic, thus the copula is able to use this information to generate the corresponding joint distribution. Previous works by Amoualian et al. [21], [22] have demonstrated the effectiveness of copulas in topic models.

SECTION 3Topic Self-Adaptive Model
In this section, we describe our basic method named Topic Self-Adaptive Model (TSAM) for parallel topic discovery. The model is developed to achieve the following two goals: (i) to prune marginal topics that achieve low prior weights and contain a few assigned words, and (ii) to add new topics that are uncovered by the current topic space. Generally, we adopt the Reversible Jump MCMC method [23] as the basis of parallelization, and propose a topic number adjustment method to automatically increase or decrease the number of topics without sacrificing model parallelization.

3.1 Parallelization of Topic Assignments
Although G2PP [4] is equivalent to HDP in terms of the generative process, these two models behave differently. Unlike HDP whose parameters are dependent to each other, G2PP was developed to update parameters independently. However, a major problem of G2PP is, given the dth document Xd with Nd words, the number of words assigned to the kth topic ndk is restricted to ∑Ki=1ndk=Nd, where K is the topic number. In HDP, Nd is an observed constant, which makes sense since we can always observe the word number of a document. While in G2PP, Ndk is unknown until each ndk is generated, according to the restriction ∑Ki=1ndk=Nd. This indicates the dependency between ndk and Nd, any explicit assumption on Nd will break the independence between topics. A straightforward solution is to “Unobserve” Nd, which helps break the dependence between ndk, since Nd is no longer a constant but a variable s.t. ∑Kk=1ndk=Nd. This enables the model to update ndk independently. To achieve this, X′d, a “copy” of Xd with a flexible length is constructed in [4]. Furthermore, a word stack Sd is built to store X′d, which is initialized by randomly sampling words from the original document set.

By introducing X′d, all words assigned to the same topic zk will be grouped in X′dk. When asynchronously updating X′dk, there is an equal chance of adding words to it or removing words from it. Once X′dk needs to add a new word, the thread visits the word on the top of Sd and accept it with a certain acceptance probability. For the deletion of existing words, the thread randomly chooses a word assigned to X′dk with another acceptance probability. We will introduce these acceptance probabilities in Section 4. According to [4], a buffer Bdk is set up to restore “abandoned words”. Once X′dk rejects to add a word or accepts to remove a word, this “abandoned” word will be pushed into Bdk. The above study exploited the Reversible Jump MCMC method for updating ndk, which adapts to a time-variant length of X′dk, i.e., the number of words assigned to topic zk.

3.2 Adjustment of Topic Numbers
In terms of the generative process, G2PP will assign imbalanced prior weights to topics under a fixed prior. Thus, the sizes of all X′dk are not guaranteed to be balanced after each iteration. This feature of G2PP reflects the nature of topics: Within a corpus, some topics are highly concerned by many documents, while some other topics only exist in a few documents. If a topic is seldom concerned, it naturally achieves a low prior weight and contains a few assigned words. Such a marginal topic is redundant (or unimportant) and should be removed to save the time cost [11]. However, G2PP lacks the pruning mechanism of marginal topics. Furthermore, for the Jump Reversible MCMC method, some words will be left in Sd without any topic assignment. These words are likely to be assigned to some new topics, while in G2PP, it is intractable, because this model applied the finite approximation [24] on the topic number K to avoid the unlimited growth of topic numbers (i.e., K is predefined and fixed). It is also why the algorithm can not retain the nonparametric feature of HDP.

To take the full advantages of nonparametric Bayesian models, how to automatically adjust the number of topics is requisite for our TSAM. Similar to [4], we use multiple threads, with each thread being associated with a unique topic. Particularly, the kth thread tk maintains the set of words assigned to topic zk. During each iteration, there is an equal chance to add or remove a word from thread tk, which means that after certain times of iteration, some topics (i.e., marginal topics) may contain no words or only a few words. If we keep running the threads associated with the marginal topics, the computational resources will be wasted because these threads are seldom updated. Therefore, these marginal topics should be deleted to save the cost of both time and space. In practice, we use a threshold p to identify marginal topics. The value of p is determined as p=k%∗N, where k is a hyper parameter and N is the total number of words in the document set, i.e., N=∑dNd. To balance workload and avoid busy-waiting, we evenly divide the dataset into K subsets, where K is the current number of topics. The K subsets are arranged in a circle, with each thread conducting the topic assignment at a distinct position and eventually travels all subsets.

Another point is that the word distribution of a topic may be unstable within only a few epochs, so we set up a threshold ϵ, that is, wait for a predetermined number of epochs before deleting a marginal topic. This is to ensure that we only delete the topics with persistent low proportions instead of “temporary marginal” ones. Pruning marginal topics is equal to destruct associated threads, thus it requires to modify the local topic distribution Gd concurrently, and we need to guarantee that Gd should be updated shortly after the destruction to avoid memory leak (e.g., visiting an element that is already removed from Gd). We propose a monitor-executor mechanism using a hierarchical structure of threads to manage the global topic information. First, we use one thread as the monitor, which is responsible for collecting information from all executor threads (each is associated with a topic). A reporting thread is also used to act as the bridge between the monitor and executor threads, in which, the index of marginal topics and the remaining word number of each stack are recorded. Second, each executor thread conducts the topic assignment independently to determine the signal of which topics are marginal. The reporting thread will receive these signals and report to the main thread. By setting a monitor thread, we construct a two-level hierarchical structure that enables us to modify topic numbers and adjust thread numbers without thread conflicts. Correspondingly, after every epoch, if all Sd are not empty, it indicates that some words are not assigned to any topics. It is reasonable to assume that these words belong to some new topics that are uncovered by the current topic space, therefore the topic number should be increased. Since adding new topics will change the global distribution (e.g., a newly-added topic takes the place of a newly-removed topic, thus the old index points to a different topic with a different proportion now), the above monitor-executor mechanism is also necessary from this perspective to ensure that the topic information is correct. Besides, even if the memory leak will not take place when adding topics, we conservatively add only one new topic at a time to avoid the sharp increase of workload.

Algorithm 1. Topic Number Adjustment With the Monitor-Executor Mechanism
Input: Word stacks Sd, total word number N, current topic number K, burn-in threshold ϵ, threshold k for identifying marginal topics.

Output: Adjusted topic number K.

Initialize global monitor thread tM;

for each topic thread tk asynchronously in parallel do

Calculate current topic word number n(0)dk;

if n(0)dk<k%∗N then

Wait for ϵ iteration(s);

Update total topic word number n(ϵ)dk;

if n(ϵ)dk<k%∗N then

Report to tM;

tM tags tk to be deleted and update the counting variable del_cnt.

for all word stacks Sd do

Calculate current size of Sd as N′d;

if N′d>0 then

Report to tM;

Update the counting variable add_cnt.

tM modifies K according to del_cnt and add_cnt.

if add_cnt>del_cnt then

Initialize and update new thread(s).

return New topic number K.

Now we propose the adjustment method of topic numbers, detailed in Algorithm 1. Inspired by [25], we accept the adjustment of topic numbers only if the change contributes to the decline of perplexity—a classical measure in evaluating the performance of topic models, which will be introduced later in Section 6.3.

SECTION 4Extended Topic Self-Adaptive Model With Copulas
To model the local topical dependencies within phrases, the copula is used to extend our TSAM. The main motivations are as follows: First, the inputs of TSAM are organized as sorted arrays of word indexes, rendering no extra semantic structures such as phrases being preserved. Second, it may be inappropriate to initialize each word stack by stochastically sampling words from the original documents, because rare words that help complete the whole context of a topic are unlikely to be sampled into the word stacks.

In TSAM, we update the topic assignment of each word by employing the Reversible Jump MCMC method exploited in [4]. Particularly, two acceptance probabilities derived from the likelihood are respectively set for adding and removing words from a topic:
A++ndk=min(1,mπdkpx∗(k)ndk+1),(1)
View SourceRight-click on figure for MathML and additional features.
A−−ndk=min(1,ndkmπdkpx∗(k)),(2)
View Sourcewhere x∗ is the chosen word to be added or removed, px∗(k) is the normalized likelihood of topic assignment with respect to word x∗ and topic k. As we can see, a larger ndk results in a lower acceptance probability of assigning new words to the kth topic in the dth document. If a word w frequently occurred in the corpus, it is more possible to be sampled and pushed to the word stack for many times, which restrains the selection of rare words.

Although some frequent words do provide significant information, rare words usually help complete the whole context and supply extra semantic meanings to frequent words. This is also supported by [26]. For example, if there is a phrase “support vector machine”, where “support” is rare, “vector” and “machine” are frequent in the context, topic models will learn that “vector” and “machine” have higher frequencies within the topic, while the topical probability of “support” is too low to make it a representative topical word. Nonetheless, by combining “support”, “vector” and “machine”, we know that the context probably does not focus on “vector” or “machine” (which can be interpreted in many ways), but “support vector machine” does. In general, when given a phrase, if one of its component word (e.g., “vector”) is assigned to a topic, then the other word(s) is/are quite likely to be assigned to the same topic. Therefore, we need to guarantee the words that belong to the same phrase will not be pushed into different word stacks. To achieve this, we build a global phrase vocabulary V. Once a word w is sampled from the original documents, we find phrases begin with w by looking up V. For a certain phrase with N words P = {w(1),…,w(N)}, if w(i) is sampled, then the rest components w(j) (j=1,2,…,i−1,i+1,…,N) will be pushed into Sd along with w.

4.1 Properties of Copulas
The existing LDA, HDP, G2PP, and our TSAM are bag-of-words models, which assume that words are independently generated. These models do not take the topical dependencies of words into account, thus scatters the words that compose a phrase. To address this, a suitable choice is exploiting the apparatus of copulas, and we will explain the reasons by introducing the corresponding properties.

A simplified definition of a copula is “a joint distribution function that results in all marginal distributions being uniformly distributed on (0, 1)”. In the bivariate case, assume that we need to find an appropriate joint cumulative distribution function (CDF) denoted as H(X,Y) to describe the dependency between random variables X and Y (e.g., topics), whose marginal CDFs are F(X) and G(Y), respectively. Normally, it is difficult to define the analytical form of H(X,Y), but we can indirectly derive it by constructing a function that models the dependency between F(X) and G(Y), according to the following equivalence:
H(x,y)=P(X≤x,Y≤y)=P(F(X)≤F(x),G(Y)≤G(y))=CXY(F(x),G(y)),(3)
View Sourcewhere x and y are the observed values (e.g., topic indexes). CXY is a copula of X and Y, which is also a parameterized joint distribution of F(X) and G(Y) controlled by copula parameters. Hence, we can follow the analytical construction process of a copula to derive H(X,Y). Particularly, once given a vector (u1=F(x), u2=G(y)) sampled from the copula CXY, H(x,y) can be derived by
CXY(u1,u2)=P(F(X)≤u1,G(Y)≤u2)=P(X≤F−1(u1),Y≤G−1(u2))=H(F−1(u1),G−1(u2))=H(x,y).(4)
View SourceRight-click on figure for MathML and additional features.

Note that the above equivalence is identical to Equation (3) when replacing u1 and u2 by F(x) and G(y). Then, x and y are determined by
x=F−1(u1),y=G−1(u2).(5)
View SourceRight-click on figure for MathML and additional features.

In this way, we can model the topical dependency within a phrase by sampling from a copula. Since the sampled u1 and u2 are jointly distributed, x and y are also dependent.

In the multivariate case, we refer to the Sklar's theorem of copulas [27], as follows:

Theorem 4.1.
Given a p-dimensional joint CDF H(z1,…,zp) with univariate marginal CDFs F1(z1), F2(z2) ,…,Fp(zp), there always exists a copula C such that for all (z1, z2 ,…,zp) ∈ Rp, H(z1,…,zp)=C(F1(z1),…,Fp(zp)).

Given a series of random variables (z1, z2 ,…,zp), the marginal probability of zi (i = 1 ,…,p) is Fi(zi)=P(Z≤zi)=ui. According to the proof in [28], we have a corollary of the Sklar's theorem, as follows:
C(u1,u2,…,up)=H(F−11(u1),…,F−1p(up)).(6)
View Source

Given a vector (u1,u2,…,up)∼C(F1(z1),…,Fp(zp)) (i.e., a sample of copula C), the elements of (u1,u2,…,up) are the values of marginal CDFs, thus we can obtain the desired values of variables z1,z2,…,zp by
zi=F−1i(ui),i=1,…,p,(7)
View Sourcewhich compose a sample of the joint distribution of z1,z2,…,zp. From Equations (6) and (7), we can derive samples of multiple variables’ joint distribution, if given their marginal CDFs and a carefully chosen copula. This corresponds to the bivariate case described by Equation (5), and explains why copula is a good choice for modeling the local topical dependencies within phrases. Some existing phrase-based topic models like [7] enforces words within a phrase to inherit the same topic, which is inappropriate in some cases such as long noun phrases [29]. In contrast, our model permits a phrase to have more than one topic, depending on the topics’ joint probability.

4.2 Copula Setup for Modeling the Local Topical Dependencies Within Phrases
Following Equations (6) and (7), we use the Archimedean copula family to model the joint distribution function of Fi(zi). Archimedean copulas take the form as follows:
C(u1,u2,…,up)=ψ−1(ψ(u1)+⋯+ψ(up)).(8)
View SourceRight-click on figure for MathML and additional features.

Particularly, we choose the Frank copula, a special case of Archimedean copulas by setting ψ(u)=−1λlog(1−(1−e−λ)e−u). When λ→0, it approaches an independent copula that the joint probability equals ∏i=1pui, which corresponds to the independence assumption of bag-of-words models such as LDA. When λ→∞, the joint probability becomes min(u1,u2,…,up), which means that the variables are completely and positively correlated. In practice, we can vary parameter λ from 0 to ∞ to model different strengths of local topical dependencies within phrases.

As mentioned in Section 1, the local topical dependencies are assumed to be nested by following autoregressive models which regard documents as sequences and strongly emphasize the sequential reliance between words. In autoregressive models, the occurrence of each word is conditioned on all previous words, corresponding to a Markov chain of a certain order. Given a phrase with L words in our scenario, the first two words form a Markov chain of order 1, and the last word obtains a Markov chain of order L−1 which involves all previous L−1 words. To model this structure, we construct a nested Frank copula by decomposing a multivariate one into a set of bivariate copulas recursively, as follows:
C2(u1,u2,u3)=C2(C1(u1,u2),u3)C3(u1,u2,u3,u4)=C3(C2(C1(u1,u2),u3),u4)…Cp−1(u1,u2,…,up)=Cp−1(Cp−2(Cp−3…),up),(9)
View SourceRight-click on figure for MathML and additional features.and the sampling method will be described in Section 5.

Deriving from Equations (1) and (2), we can write the original conditional probabilities of the ith word being assigned to, or removed from the kth topic in the dth document as p(zi=k|m,πdk,pxi(k),ndk)=A++ndk and p(zi≠k|m,πdk,pxi(k),ndk)=A−−ndk. After introducing the nested Frank copula, the probabilities can be rewritten as
p(zi=k|zi−1,…,z1,λ)=1+(1−1)∗A++ndk,p(zi≠k|zi−1,…,z1,λ)=1+(1−1)∗A−−ndk,(10)
View SourceRight-click on figure for MathML and additional features.where 1 is the identity function defined as
1={1,0,ICDF(U∼C(λ))i=ICDF(U∼C(λ))1,otherwise. (11)
View Source

In the above, zi is the computed topic index of the ith word in the phrase, λ is the shared parameter of the components of the nested Frank copula, U is the sample of the nested Frank copula, and the subscript i indicates its ith element. The conditional variables zi−1,…,z1 are implicitly involved by the nested Frank copula. Note that the first word's conditional probabilities remain as p(z1=k|m,πdk,px1(k),ndk)=A++ndk and p(z1≠k|m,πdk,px1(k),ndk)=A−−ndk. The topic index of the first word computed through copula samples and the inverse CDF (ICDF), i.e., a quantile of the multinomial distribution, does not need to be identical to k, since copula is used as an auxiliary topic co-assignment indicator to “switch” the conditional probability.

4.3 Copula Guided Topic Self-Adaptive Model
Now we propose an extended version of TSAM, namely TSAMcop, to discover coherent topics by integrating copulas with TSAM. Again, we know that the stochastic initialization of Sd actually causes the lost of phrase-level information. Inversely, when generating X′d, we also need to consider the influence of phrases, as component words of a phrase are semantically related to each other so that they are more likely to be assigned to the same group X′dk and should be push to Sd simultaneously. Therefore, we need to model the latent dependency before assigning all words in a phrase into the same group. This is where the copula is involved. The overall framework of our TSAMcop is shown in Fig. 1, where the inference procedure inside thread m (m<K) is illustrated, and it is identical for other threads.


Fig. 1.
The overall framework of TSAMcop.

Show All

In the preprocessing stage, we use the Stanford Parser1 to extract all phrases. Particularly, a grammar tree is constructed based on the probabilistic context-free grammar, which is further used to obtain phrases by post-order travelling. Afterwards, we build up a phrase vocabulary V for all phrases in the whole dataset. Note that words and phrases are stored in different vocabularies, and the range of phrases’ and words’ indexes are different. Each time when X′dk decides to add or remove word w, while the index of w is outside the vocabulary of words, we look up V to find phrases of length L corresponding to the index of w. Then, we sample an L-dimensional vector U=(u1,…,uL) from copula C, and calculate F−1i(ui),i∈{1,…,L}.

It is important to note that a topic is represented by its index instead of a probability value. Thus, we transform the copula samples U=(u1,…,uL) into a vector Z=(z1,z2,…,zL), where zi is the topic co-assignment indicator of the ith word in a phrase. However, the problem is, how do we know the topic distribution of the dth document? By analysing the mathematical definition of G2PP, we notice that Gd=∑∞k=1πdkδθk can be normalized into a discrete distribution generated by the upper level Gamma process. In the above, k is the index of topic. Recall that dirac function δθk=1 when Z=zk and δθk=0 else where, and the associated weight πdk equals the probability of zk in the dth document, i.e., P(Z=zk)=πdkδθk, since topics are discrete and assumed to be independent. Considering the independence of topic assignment for each word, the copula is built by admitting the CDF of the document's topic distribution Gd as the marginal CDF, i.e., Fi(zi)=∑zij=1πdjδθj. The sample of the copula is an L-dimensional vector with each element ranging from 0 to 1. Particularly, each element is exactly a CDF value of Gd, which can be recovered to a multinomial sample by using the inverse CDF. In this way, the copula is able to model the “local” joint distribution of topics for words within each phrase.

Now we need to derive F−1. Following the work by Balikas et al. [30], we transform the copula sample U into a multinomial sample using the quantile. A quantile is defined as follows:

For a given probability density function f(x), if there exist xi that satisfies
P(X≤xi)=F(xi)=∫xi−∞f(x)dx=α,(12)
View SourceRight-click on figure for MathML and additional features.then xi is the α-quantile of f(x).

Unfortunately, this formula only holds for continuous variables, but we are handling the sample transformation involving discrete distributions. To adapt the quantile for our purpose, we revisit the process of sampling from a p-dimensional multinomial distribution by using the inverse CDF. First, we divide the x-axis of a unit length (i.e., the total length is 1, so as to meet the condition of a probability distribution) into p intervals, each has a length of the weight of the ith component pi. Then, we draw a number from the uniform distribution (admitted as the CDF value), and find its location at the same axis. The associated component of the interval where the number lies is a valid sample drawn from the given distribution. In our models, the sample is the topic index, and the above sampling process can be rewritten as follows:
z=inf{z|ui≤∑j=1zpj},(13)
View Sourcewhere inf denotes the infimum that satisfies the condition, ui is the aforementioned CDF value, pj is the weight of the jth topic, and z is the topic index. This is similar to finding a quantile of the multinomial distribution. Therefore, given a copula sample U, we can transform each of its elements into a sample of a multinomial distribution, i.e., the document-topic distribution. In this way, deriving F−1i(ui) equals to find a specific topic index z that satisfies
∑j=1z−1pdj<ui≤∑j=1z−1pdj+pdz.(14)
View SourceRight-click on figure for MathML and additional features.

Based on the above inequation, we can derive the corresponding topic index z associated with each ui.

Particularly, since Gd is composed of discrete mixture components for TSAMcop, we can similarly derive z by solving
∑j=1z−1πdjδθj<ui≤∑j=1z−1πdjδθj+πdzδθz,(15)
View SourceRight-click on figure for MathML and additional features.where j and z are the indexes of a mixture component corresponding to specific topics. Recall that πjz is the weight of topic z in the jth document. Thus we transform U=(u1,…,uL) into Z=(z1,…,zL), where zi, i∈{1,…,L} is the topic co-assignment indicator of the ith word in a phrase. Once zi=zn,i,n∈{1,…,L},i≠n, then we simultaneously push the ith and the nth words into, or remove them from, X′dk if they co-occurred in Sd or X′dk.

4.4 Copula Guided Topic Assignment in Infinite Latent Topic Space
We have already discussed the characteristic of latent topic space Gd in Section 2. For G2PP and TSAM, the mixture components are required to be consecutively sampled from Gd to generate Πd. The process of updating Gd is equal to updating the topic assignment for the dth document, this is where new topics will emerge. Note that unlike stick-breaking, the components in Gd are unique. On a large-scaled corpus, the topic number grows gradually and may be very large. G2PP solved this problem by applying the finite approximation to constrain the range of mixture component space, which is equal to limit the maximum topic number based on the prior knowledge. This is another motivation for us to propose the topic number adjustment method: to let the maximum topic number be a data-driven variable. Although our models run on an infinite latent topic space, a balance of adding and removing topics will be reached under certain parameter settings, so that the maximum topic number is controlled.

For computational consideration, an alternative way is to apply the dynamic finite approximation on the topic space, which requires to construct a reasonable document-topic distribution G′d=∑kπ′dkδθk with normalization, where π′dk is the weight of the kth component in the dth document, normalized by ∑kπdk. The upper bound of k is the current topic number K, thus the topic space is limited to K mixture components at a point of time. We preserve the chance for expanding or compressing the topic space by our topic number adjustment method. The process of expanding the topic space can be interpreted as reallocating the topic proportions by normalization, and the process of compressing is equal to averaging the weights of the outcast components and allocating to the remainders. Thus, we can derive the following modification of Inequation (15):
∑k=1T−1π′dkδθk≤ui<∑k=1T−1π′dkδθk+π′dTδθT(T≤K),(16)
View SourceRight-click on figure for MathML and additional features.where T is the infimum, corresponding to z in Inequation (15). Note that the above approximation still allows the size of the global (corpus-level) topic space to be infinite. In our experiment, we use the above inequation to determine the topic indexes.

Algorithm 2. Copula Guided Method for Coherent Topic Assignment
Input: The original corpus Xd and the document-topic distribution G′d of the dth subset.

Output: The modified Sd and the reconstructed X′dk.

Construct word vocabulary W, phrase vocabulary V, word stack Sd, and buffer Bd;

Push all indexes of words and phrases into Sd;

for each topic/thread tk asynchronously in parallel do

if X′dk requires to add new words then

Pop wtop from the top of Sd;

Accept wtop by acceptance probability A++ndk;

if Accepts to add wtop then

if wtop not in W then

Search V for phrases P={w1,…,wL} corresponding to the index of wtop;

Sample vector U=(u1,…,uL) from copula C by admitting G′d as the marginal distribution;

Transform U into Z=(z1,…,zL) by solving Inequation (16);

Push w1 into X′dk;

for all zi=z1,i∈{2,…,L} do

Push wi into X′dk.

Push wtop into X′dk.

else

Push wtop into Bdk.

if X′dk requires to remove words then

Randomly choose a word wrmv in X′dk;

Remove wrmv by acceptance probability A−−ndk;

if Accepts to remove wrmv then

if wrmv not in W then

Search V for phrases P={w1,…,wL} corresponding to the index of wrmv;

Infer Z=(z1,…,zL) for P;

Move w1 to Bdk;

for all zi=z1,i∈{2,…,L} do

Move wi to Bdk.

Move wrmv to Bdk.

else

Reserve wrmv.

return Sd and X′dk.

Discussions above guarantee the rationality of the proposed copula guided method for coherent topic assignment as shown in Algorithm 2 (one iteration on Xd). For the dth subset, the data flows of adding and removing words in TSAMcop are illustrated in Fig. 2, where each X′di and Bdk is maintained by one topic/thread tk, k∈{1,…,K}. In practice, the words within each phrase are tagged and their statistics are stored in a matrix to avoid duplicate counting, because they may be assigned to different topics or scattered in Sd or Bdk.

Fig. 2. - 
The data flows of (a) adding and (b) removing words in TSAMcop.
Fig. 2.
The data flows of (a) adding and (b) removing words in TSAMcop.

Show All

SECTION 5Time Complexity Analysis
One of the major drawbacks of HDP is the high time complexity. Consider applying the Gibbs sampling for the Chinese Restaurant Process (an equivalent representation of HDP) [2], the approximate time complexity is O(Niter∗T∗D∗K∗V¯), where Niter is the number of iterations, T is the number of tables, D is the number of documents, V¯ is the average length of documents, and K is the non-decreasing topic number. The time complexity increases with the increase of topic numbers K. Moreover, when there is a new topic, consecutive operations such as allocating top level proportion and normalizing topic matrix are required. For G2PP, there are four parameters ndk, πdk, θk, and αk which can be updated in parallel. Thus, the time complexity of G2PP for each thread is O(Niter∗K∗D∗K∗Vd¯), where the first K results from updating the global topic proportion, the second K comes from updating the document-level topic proportion, and Vd¯ is the average length of the documents in Xd. TSAM shares the same time complexity with G2PP, but the topic number K is not increased continuously, thus the complexity will drop when the model accepts to remove a bunch of marginal topics. For TSAMcop, either the construction or the sampling operation of nested Frank copulas is a time-consuming process. The construction can be done in O(1), while it requires exponential time for rejection sampling. Here we apply the sampling method for nested Archimedean copulas proposed by Hofert [31], which reduced the complexity from exponential to logarithmic, and the approximate time complexity is O(Niter∗K∗D∗(K∗Nword+log(L¯)∗Nphrase)), where Nword, Nphrase, and L¯ denote the number of single words, the number of phrases, and the average length of phrases, respectively. The multi-threading also helps alleviate the extra computational cost, hence TSAMcop will not suffer from unaffordable loss of efficiency.

SECTION 6Experimental Study
In this section, we compared the performance of TSAM and TSAMcop with other baseline models on real-world datasets in terms of perplexity, topic coherence, text classification, topic interpretability, and convergence time.

6.1 Datasets
To validate the effectiveness of our models on topic discovery, we employed the following real-world datasets:

NIPS.2 It contains 1,500 full papers accepted by NIPS from 2001 to 2017. For each paper, the main research problems and contributions were provided.

NIPS17. It contains 678 full papers accepted by NIPS 2017,3 where each paper's references are excluded. Mining phrase-level topical information from this corpus allows us to extract frequent terminologies that indicate the most concerned problems and the methods applied to solve them.

20NewsGroup.4 This corpus is officially divided into 11,314 training and 7,531 testing documents. Each document was manually divided into 20 categories, and we used these categories as labels to demonstrate the effectiveness of our models on text classification.

NYTimes.5 As a widely-used large scale corpus, it contains 300,000 articles from New York Times since 1851. This dataset is mainly used to evaluate the performance of parallel topic models. Considering that each article in this corpus is represented as sorted arrays of word indexes, we assumed that the consecutive words with identical indexes compose a coherent segment analogous to phrase.

The statistics of these datasets are shown in Table 1. For NIPS and NYTimes, we randomly selected 70, 20, and 10 percent samples as the training set, the validation set, and the testing set, respectively. Different combinations of model parameters were first tested on the validation set. Then, the parameter sets performed the best were used on all testing samples. The parameter tuning on 20NewsGroup was performed by following [25].

TABLE 1 Statistics of Datasets
Table 1- 
Statistics of Datasets
6.2 Baseline Models
In the quality evaluation of topical words and document-topic distributions learnt by our TSAM and TSAMcop, we used two groups of baselines for comparison. The first group is Bayesian models, as follows:

HDP [2]. HDP was a classical nonparametric Bayesian network in statistics and machine learning areas. According to [17], we adopted the gensim implementation of the classical HDP as a baseline model.

G2PP-x [4]. G2PP was equivalent to HDP in terms of the generative process, where a parallel algorithm was proposed by associating each topic with one thread to accelerate parameter inference. Since G2PP requires a predefined topic number, we followed [4] by running 50 threads (i.e., 50 topics) on 4 CPU cores (x = 4) and 8 CPU cores (x = 8). The number of cores only affects the efficiency and will not have an impact on the quantitative performance.

CopulaLDA [30]. CopulaLDA was an extended LDA model that utilized copulas to model the topical dependency of coherent text-spans. We used the open source implementation provided by the authors on the Github.6

The second group of baseline models is developed based on neural networks:

NTM [32]. NTM was a multi-layer neural network aiming to learn document-topic and topic-n-gram distributions and generate meaningful representations for documents. We also employed its supervised extension (i.e., sNTM) for the evaluation of text classification.

GSM [25]. GSM was a neural topic model with Gaussian Softmax, extended from NVDM [33]. Under the framework of variational auto-encoder, GSM explicitly modeled the document-topic distribution by a mean field Gaussian, so as to reduce the ambiguity of topic vectors in NVDM.

TMN [34]. TMN was a topic memory network for text classification by encoding latent topic representations via memory networks. It utilized topic vectors and pre-trained word embeddings to compute document representations, which was proven to be effective in document classification.

Unless otherwise specified, the number of topics is set to 50 for all parametric topic models.

6.3 Performance on Perplexity
Perplexity gives a measure of the probability of the corpus being generated, and it holds the form as follows:
Perplexity=exp(−∑Dd=1∑Ndi=1log(p(wdi))∑Dd=1Nd),
View Sourcewhere D is the total number of documents, Nd is the length of the dth document, and p(wdi) is the probability of the ith word in the dth document. The smaller the perplexity value is, the closer the inferred distribution to the real one is. We present perplexity values and topic numbers of different topic models over NIPS, 20NewsGroup, and NYTimes in Table 2, where each perplexity value is averaged from 5 runs, and the number of topics over each dataset is inferred automatically for nonparametric topic models. Following [4], subsampling on the original dataset is used for G2PP and our models. As the result indicates, TSAMcop and CopulaLDA achieve better values than TSAM and other baseline models. This validates the effectiveness of copula guided models in boosting the generative probabilities of these corpora.

TABLE 2 Perplexity and Topic Numbers of Different Topic Models, Where the Best Result for Each Dataset is Highlighted in Boldface, and Final Topics in HDP are Selected by the Global Stick Proportions
Table 2- 
Perplexity and Topic Numbers of Different Topic Models, Where the Best Result for Each Dataset is Highlighted in Boldface, and Final Topics in HDP are Selected by the Global Stick Proportions
6.4 Performance on Topic Coherence (NPMI)
In this part, we evaluated our models by the normalised pointwise mutual information (NPMI) [35]—a widely-used kind of topic coherence scores [36]. This metric was also employed in the evaluation of neural network-based models, e.g., GSM [25]. A model that generates more coherent topics will achieve a higher NPMI score. Table 3 presents the NPMI score of each model, where all scores are computed on the testing sets, and the value is averaged by using 5 and 10 top words for all topics. The experimental result indicates that TSAMcop outperforms TSAM and other baselines in topic coherence, which further indicates that our copula guided model generates more coherent topics.

TABLE 3 NPMI Scores of Different Topic Models, Where the Best Result for Each Dataset is Highlighted in Boldface, and the Value Reported in the Original Paper is Underlined

6.5 Topic Evaluation by Text Classification
To further evaluate the effectiveness of the topics discovered by different topic models on downstream tasks, we employed the 20NewsGroup for text classification. Particularly, the normalized document-topic distribution generated by each topic model was used as the feature representation of a document and input to a logistic regression classifier with L2 penalty term. The widely-used precision, recall, and F1 were adopted as the evaluation metrics. The results in Table 4 validated that our methods can generate better document-topic distributions than Bayesian baseline models for text classification. We can also observe that sNTM and TMN performed better than our models, because pre-trained word embeddings and document labels were exploited by these baselines to enhance classification performance. However, the major issue of sNTM is that it does not generate interpretable topic-word distributions which should follow probabilistic characteristics. As for TMN which learns the topic vectors by a neural topic model similar to GSM, it neglects the latent topical dependencies between grammatically related words. This limits the performance of TMN on topic discovery, as already shown in Tables 2 and 3. Besides, although RNNs can be used as encoder and decoder to model the sequential dependencies between words, it may lead to posterior collapse [37] when combining with variational auto-encoder based models like GSM.

TABLE 4 Text Classification Performance on 20NewsGroup, Where the Best Result is Highlighted in Boldface
Table 4- 
Text Classification Performance on 20NewsGroup, Where the Best Result is Highlighted in Boldface
6.6 Topical Word Evaluation
Although TSAMcop achieved a higher topic coherence, it is also necessary to validate the interpretability of topics. We listed the top words from 5 of the most representative topics and the topical words learnt by aforementioned baselines over the NIPS17 dataset. Following [10], we label and align these topics by human experts according to their top 10 words. As shown in Tables 5 and 6, benefiting from the topical dependency modelling of phrases, the representative topics in TSAMcop and CopulaLDA are generally more interpretable than others by focusing on specific contents instead of frequently mentioned terminologies.

TABLE 5 Topical Words Learned by TSAMcop on NIPS17NIPS17, Where ML, NN, DM, TM, and CV Denote “Machine Learning”, “Neural Network”, “Data Mining”, “Training Method”, and “Computer Vision”, Respectively
Table 5- 
Topical Words Learned by TSAMcop on $NIPS_{17}$NIPS17, Where ML, NN, DM, TM, and CV Denote “Machine Learning”, “Neural Network”, “Data Mining”, “Training Method”, and “Computer Vision”, Respectively
TABLE 6 Topical Words Learned by TSAM and Baseline Models on NIPS17NIPS17, Where ML, NN, DM, TM, and CV Denote “Machine Learning”, “Neural Network”, “Data Mining”, “Training Method”, and “Computer Vision”, Respectively
Table 6- 
Topical Words Learned by TSAM and Baseline Models on $NIPS_{17}$NIPS17, Where ML, NN, DM, TM, and CV Denote “Machine Learning”, “Neural Network”, “Data Mining”, “Training Method”, and “Computer Vision”, Respectively
6.7 Topical Phrase Evaluation
To further demonstrate that TSAMcop can generate coherent topics, we compared the topical phrases recovered from the topical words generated by TSAMcop with those learned by the existing phrase-based topic models.

6.7.1 Phrase Reassemble
Although phrases and copulas are used to guide the topic assignment in TSAMcop, it outputs topics that contain a series of words. Therefore, we need to recover topical phrases from the topical words before evaluating the coherence and reliance between them. Instead of assembling words to form phrases simply, we turned to recover more natural and meaningful phrases. A previous study [38] suggested that a phrase can be evaluated by frequency, “phraseness”, and completeness. The terminology “phraseness” describes the hierarchical structure of phrases: If adjacent phrases or words co-occurred more frequently than expected, than they should be merged into a longer phrase. For example, the word “knowledge” and the word “bases” can be merged into a longer phrase “knowledge bases” because they co-occurred more frequently than expected [10]. The completeness of phrases guarantees that the model will not allow partial phrases, e.g., “convolutional neural network” is more coherent and precise than “convolutional network” and “neural network”. In our model, we reassembled topical phrases by listing all possible combinations of topical words, filtering phrases of low qualities by phraseness, and ranking the remainders by frequencies. This approach is similar to KERT—a topical key phrase generation framework [16], where similar criteria are used to filter and rank the candidate topical phrases. However, KERT performs frequent pattern mining on each topic as a postprocessing step, which may have the collocation problem where related words in different topics cannot be aggregated to form a meaningful phrase. In TSAMcop, this drawback is overcame by the copula guided topic assignment.

6.7.2 Baseline Models
To evaluate the quality of generated topical phrases, we used the following phrase-based topic models as baselines:

ToPMine [7]. ToPMine regarded every document as a “bag-of-phrases”, and used undirected cliques to model the correlations of words in a same phrase. Unlike our models, ToPMine used N-gram phrases and enforced the words in the same phrase to have identical topic assignments.

TPM [8]. Based on the Pitman-Yor process, TPM was a phrase-based topic model specializing in clinical applications. By extracting clinical phrases via MedTagger, TPM was efficient to perform topic modelling on clinical corpora and generated high quality topical phrases. For consistency, the Stanford Parser was applied to extract phrases in TPM.

PhraseCTM [9]. PhraseCTM was a two-stage method that generated the correlated topics at the phrase level by extending the CTM [15]. However, we only focus on the first stage, where PhraseCTM linked the phrases and the component words within a Markov Random Field by using NPMI to determine whether the phrases share the same topic with their component words (e.g., the phrase Boston Globe and its component words), and updated the variational parameters by Markov Random Field for variational inference. The second stage of PhraseCTM was to generate the correlation of topics, which is not the focus of our study.

CPhrLDA [10]. CPhrLDA was an iterative phrase-mining framework where an efficient method named CQMine was proposed and utilized to support phrase-based topic modelling. It evaluated the topical frequency of all phrases and ranked them by proposed criteria, namely coverage, purity, phraseness, and completeness, to obtain quality phrases, and to discover the phrase-based topical representation of the textual data.

There were several other phrase-based topic models, e.g., the Labeled Phrase LDA [39] manually segmented documents into phrases according to frequency, and used multi-label corpora to perform topic modelling, which makes it incomparable to our unsupervised models. PTSM_IHDP [40] was a phrase-based incremental HDP model for discovering the evolutionary trend of topic-based sentiments from online reviews. We did not take it as a baseline since the objective task is different.

6.7.3 Result Analysis
The results of manually aligned topical phrases learned by different models on NIPS17 are given in Table 7. We can observe that TopMine tends to group phrases that share common words into the same topic (e.g., most of the topical phrases in the topic “Machine Learning” share a common word “algorithm”) due to its basis of frequent pattern mining [8]. The other models can detect topics comprising a diverse range of words, but still focus on specific domains. The state-of-the-art methods such as PhraseCTM and CPhrLDA can extract coherent and meaningful topical phrases, and our method achieved similar results with the help of KERT-like methods. Although the topical phrases learned by CopulaLDA are also close to TSAMcop, these two models are quite different in the application of copulas. Particularly, CopulaLDA suffers from the extremely low acceptance probability during Gibbs sampling, because the joint probability derived from copula serves as a quite small multiplier in this model. For our TSAMcop, we used copula to model the dependence of words, and unlike traditional Gibbs sampling, all threads “select” the words in the word stacks according to the acceptance probability based on the Reversible Jump MCMC technique [4]. As aforementioned, we further divided the sampling process into two parts to avoid multiplying small probabilities which may dramatically shrink the acceptance probability. These factors boost the quality of topical phrases learned by our method.

TABLE 7 Topical Phrases Learned by Different Models on NIPS17NIPS17, Where ML, NN, DM, TM, and CV Denote “Machine Learning”, “Neural Network”, “Data Mining”, “Training Method”, and “Computer Vision”, Respectively
Table 7- 
Topical Phrases Learned by Different Models on $NIPS_{17}$NIPS17, Where ML, NN, DM, TM, and CV Denote “Machine Learning”, “Neural Network”, “Data Mining”, “Training Method”, and “Computer Vision”, Respectively
To evaluate the performance of different models quantitatively, we further calculated the NPMI score based on the discovered phrases. This metric was also used in the evaluation of PhraseCTM [9]. For fair comparison, we used the English Wikipedia corpus as reference for 20NewsGroup by following [9]. Besides, the training set was used as reference for NIPS since the testing set is too small to cover enough phrases. Table 8 presents the NPMI score of each phrase-based topic model over NIPS and 20NewsGroup, indicating that our model achieved similar results with the top-performing baseline of PhraseCTM. Furthermore, these baselines either rely on preprocessing (e.g., segmenting documents by delicate manually-designed rules for TopMine and CPhrLDA, and utilizing an existing manually constructed phrase vocabulary for TPM) or postprocessing via phrase reassemble (i.e., recovering phrases from topical words for CopulaLDA). Our method combines the preprocessing and postprocessing processes, by using the automatically segmented phrases as “references” to determine the semantic relations between words, and recovering phrases from the semantically related words in the inferred topics.

TABLE 8 NPMI Scores of Phrase-Based Topic Models, Where the Best Result for Each Dataset is Highlighted in Boldface, and the Value Reported in [9] is Underlined
Table 8- 
NPMI Scores of Phrase-Based Topic Models, Where the Best Result for Each Dataset is Highlighted in Boldface, and the Value Reported in [9] is Underlined
6.8 Impact of Parameters
In this part, we evaluated the impact of our model parameters on the performance over the NIPS dataset.

Parameter ϵ. There is a dilemma when determining the order of adding new topics and pruning marginal topics. If we prune the marginal topics first, their topical words will be pushed to the buffer and later to the word stacks, thus a new topic will be generated to collect them in the next iteration. Reversely, if we start with adding a topic, the new topic may collects only a few words in the next iteration, so that it will be immediately deleted, unless ϵ is larger than 0. Hence, we need to add a topic before pruning the marginal ones, with a proper value of ϵ to relax the condition of deleting topics. Table 9 presents the performance of our models under different settings of ϵ, from which we can observe that both models perform the best when ϵ equals 1. The model performance decreases as the increase of ϵ, because it is hard for our models to delete any marginal topics. In the case of ϵ=0, the model performance is only acceptable since some temporary marginal topics which are important may be deleted.

TABLE 9 Performance Under Different Settings of ϵε (λλ = 10)
Table 9- 
Performance Under Different Settings of $\epsilon$ε ($\lambda$λ = 10)
Parameter λ. Now we study the influence of λ on our TSAMcop. As mentioned earlier, λ controls the strength of dependencies between copula samples. We do not expect a complete correlation, which is inappropriate as the same as the complete independence assumption. We set λ to different values, and the results in Table 10 indicate that the performance decays as λ decreases. For λ=0 (e.g., applying independent copulas), the model degenerates into TSAM.

TABLE 10 Performance Under Different Settings of λλ (ϵε = 1)
Table 10- 
Performance Under Different Settings of $\lambda$λ ($\epsilon$ε = 1)
6.9 Comparison of Convergence Time
In this part, we compared the convergence time of different models over the large-scaled NYTimes dataset. The results are presented in Table 11.

TABLE 11 The Convergence Time of Bayesian Models
Table 11- 
The Convergence Time of Bayesian Models
The experiments of G2PP-8, TSAM, and TSAMcop were run on 2 Intel Xeon W-2123 CPUs, each has 4 cores and 8 threads @3.6 GHz. For G2PP-4 and other models, 4 cores and 1 core were respectively used. Note that the number of occupied threads was reported. Although TSAMcop took an extra time cost for the copula sampling process, we can observe that its convergence time was competitive when compared with TSAM and G2PP-8. On the other hand, the baselines of HDP and CopulaLDA did not reach convergence over 48 hours. As for neural network-based models, we ran them on a GPU of GeForce GTX 1080 Ti. The GPU contains 6 major cores and 1 minor core, with memory clock rate being 1.582 GHz. Unfortunately, these models did not reach convergence over 48 hours either. This is because for the large-scaled NYTimes dataset, neural networks need to maintain millions of weights and update them by batch-wise back propagation under the limited GPU memory, which can be quite time-consuming. For instance, it took 2,344 batches in average to finish one epoch for GSM when the size of each batch was set to 128.

SECTION 7Related Work
Latent Dirichlet Allocation (LDA) [1] was a conventional topic model, which regarded each document as a distribution of topics, and each word was generated from the topic-word distribution. LDA assumed that both document-topic and topic-word distributions are multinomial distributions, and employed Dirichlet distributions as the prior. Taking the advantage of the property of conjugate prior, the collapsed Gibbs sampling can be conveniently used for parameter inference. One of the most widely discussed drawback of LDA is how to determine the topic number [41], [42], [43]. Particularly, a manually specified topic number is lack of interpretability and it may be unsuitable for the new data. Hierarchical Dirichlet Process (HDP) [2] provided a nonparametric approach to discover the word-level topical information. Unlike finite mixture models, such as the Gaussian mixture model, the number of topics in HDP is automatically inferred from the data. However, HDP is much more time-consuming than LDA, hence many works focus on the parallel inference algorithm for HDP [4], [44]. In a preliminary study, Zhou et al. [3] proposed the Gamma-Negative Binomial (Gamma-NB) process. Gamma-NB process is a completely random measure that can be augmented into a Gamma-Gamma-Poisson construction, of which the normalization is HDP. One of the representation of Gamma-NB is the Gamma-Gamma-Poisson Process (G2PP) [4], in which, the Gibbs sampling algorithm for HDP was parallelized based on the equivalence between HDP and G2PP in terms of the generative process. However, all these models are limited to the lack of capturing the semantic dependencies between words.

One of the early topic models beyond the bag-of-words assumption was the Bigram Topic Model (BTM) [45], in which, the words were generated from the distribution over words, given the context defined by a latent topic and the previous word. BTM was extended by Wang et al. [29], where a switch variable was introduced to identify either the end of a previous defined n-gram or the start of a new n-gram. The words within an n-gram do not definitely share the same latent topic, thus it required the postprocessing to take the topic of the words as the final topic of the entire n-gram. Lindsey et al. [46] proposed a phrase-discovering topic model (PDLDA), which used the hierarchical Pitman-Yor process (HPYP) priors for the topic-word matrix and modeled the topic distribution over phrases instead of n-grams, since phrases are more coherent and meaningful segments. El-Kishky et al. [7] proposed TopMine, which associated frequent pattern mining with LDA to discover topical phrases. Nonetheless, TopMine enforced the component words of a phrase to share the same latent topic, which is regarded as a drawback and overcame by the Topical Phrase Model (TPM) proposed by He [8]. TPM utilized an off-the-shelf medical concept mention extraction tool to extract high quality clinical phrases and applied a hierarchy of Pitman-Yor Processes (PYPs) as priors. To explore the possibility of modeling more general topical dependencies, Balikas et al. [30] integrated copula with LDA to discover the topical dependency of text-spans. Although CopulaLDA learns more interpretable topics, it still suffers from the problem of selecting topic numbers. Recently, Huang et al. [9] proposed a 2-stage phrase-based correlated topic model, where the model linked the phrases and component words within Markov Random Fields, and thereafter generated the correlation of topics. The other recent research on phrase-based topic modelling is done by Li et al. [10], which enhanced the efficiency of phrase mining by proposing a method called CQMine.

SECTION 8Conclusion
This study presented a parallel Gibbs sampling algorithm for topic discovery that can automatically determine the number of topics, in addition to generate coherent and interpretable topics by exploiting latent dependencies of words via copulas. In the topic number adjustment part, we focused on the problem of modeling the emergence and decay of topics. We proposed a topic number adjustment method that automatically generated new topics and deleted marginal topics from the data. In the latent semantic dependency exploitation part, we used the nested Frank copula to model the local topical dependencies within phrases. Experiments demonstrated that our models achieved higher interpretability and efficiency than state-of-the-art baselines. In the future, we plan to apply our models to the streaming data, such as documents on social media platforms which are continuously posted.