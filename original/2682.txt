In open set recognition (OSR), almost all existing methods are designed specially for recognizing individual instances, even these instances are collectively coming in batch. Recognizers in decision either reject or categorize them to some known class using empirically-set threshold. Thus the decision threshold plays a key role. However, the selection for it usually depends on the knowledge of known classes, inevitably incurring risks due to lacking available information from unknown classes. On the other hand, a more realistic OSR system should NOT just rest on a reject decision but should go further, especially for discovering the hidden unknown classes among the reject instances, whereas existing OSR methods do not pay special attention. In this paper, we introduce a novel collective/batch decision strategy with an aim to extend existing OSR for new class discovery while considering correlations among the testing instances. Specifically, a collective decision-based OSR framework (CD-OSR) is proposed by slightly modifying the Hierarchical Dirichlet process (HDP). Thanks to HDP, our CD-OSR does not need to define the decision threshold and can implement the open set recognition and new class discovery simultaneously. Finally, extensive experiments on benchmark datasets indicate the validity of CD-OSR.
SECTION 1Introduction
In real-world recognition/classification tasks, limited by various objective factors, it is usually difficult to collect training instances exhaustively of all classes when training a classifier. A more realistic scenario is open set recognition (OSR) [1], where incomplete knowledge of the world exists at training time, and unknown classes can be submitted to an algorithm during testing. This requires the classifiers to not only accurately classify the seen known classes but also effectively deal with the unknown ones.

The main challenge for OSR is that the traditional classifiers usually trained under closed set scenario divide over-occupied space for known classes, thus resulting in misclassifying the instances of unknown classes unseen in training as the known classes. To meet this challenge, related studies have been conducted under a number of frameworks, assumptions and names [2], [3], [4], [5], [6], [7]. For example, Phillips et al. [2] proposed a typical framework for open set identity recognition in a study on evaluation methods for face recognition, while Li and Wechsler [3] again viewed open set face recognition from an evaluation perspective and proposed the Open Set TCM-kNN algorithm. It is Scheirer et al. [1] that first formalized the open set recognition problem and proposed a preliminary solution—1-vs-Set machine, which incorporates an open space risk term in modeling to account for the space beyond the reasonable support of known classes. Although 1-vs-Set machine decreases the region of known class for each binary support vector machine (SVM), the space occupied by each known class remains unbounded. Therefore, the open space risk still exists. As shown in Fig. 1, the 1-vs-Set machine will make misclassifications if the instances of unknown classes ?2,?3 appear in testing. To overcome this problem, researchers have further made many efforts.

Fig. 1. - 
Only known classes 1-4 are available in training, while unknown classes ?1-?6 appear during testing. ‘A’ and ‘B’ are the decision boundaries of class 1 obtained by 1-vs-Set machine, while ‘C’ is the decision boundary of class 1 obtained by OSNN.
Fig. 1.
Only known classes 1-4 are available in training, while unknown classes ?1-?6 appear during testing. ‘A’ and ‘B’ are the decision boundaries of class 1 obtained by 1-vs-Set machine, while ‘C’ is the decision boundary of class 1 obtained by OSNN.

Show All

Scheirer et al. [8] incorporated non-linear kernels into a solution that further limited the open space risk by positively labeling only set with finite measure. Specifically, they proposed a novel Weibull-calibrated SVM (W-SVM), which combines the statistical extreme value theory (EVT) for score calibration with one-class and binary SVMs. Intuitively, we can reject the large set of unknown classes (even under an assumption of incomplete class knowledge) if the positive data for any known classes is accurately modeled without overfitting. Based on this intuition, Jain et al. [9] invoked EVT to model the positive training data at the decision boundary and proposed the PI-SVM algorithm. Note that both W-SVM and PI-SVM adopt the threshold-based classification scheme, thus the thresholds play a key role. However, the thresholds in those models are usually assumed to be equal for all known classes, which is not reasonable since the distributions of known classes in feature space are unknown. On the other hand, the authors in [8], [9] recommended setting these thresholds according to the problem openness. But unfortunately the openness of the corresponding problem usually is also unknown. To overcome these deficiencies, Scherreik et al. [10] proposed the probabilistic open set SVM classifier (POS-SVM), where the unique reject threshold for each known class is empirically determined.

Recently, Ju´nior et al. [11] extended the Nearest-Neighbor classifier to OSR scenario and proposed the OSNN classifier. Zhang et al. [12] proposed the SROSR algorithm based on sparse representation, where they modeled the tails of the matched and sum of non-matched reconstruction error distributions using EVT. Taking distributional information into account when learning recognition functions, Rudd et al. [13] formulated a theoretically sound classifier—the Extreme Value Machine (EVM) which is further developed in [14]. Besides, researchers also explored open set recognition based on deep neural networks [15], [16], [17], [18], [19], [20], [21], [22], [23].

In summary, all current existing OSR algorithms are designed specially for recognizing individual instances, even these instances are all arriving collectively in batch like image-set recognition [24]. Only one decision that so-designed recognizer can make is to either reject or categorize them to some known class instance by instance using some empirically-set threshold. Thus the decision threshold plays a key role. However, the selection for it is usually based on the knowledge of known classes. This inevitably incurs risks due to no available information from unknown classes. As shown in Fig. 1, the decision boundary1 ‘C’ obtained by OSNN for known class 1 can reject the large set of unknown classes, whereas it still makes a misclassification when unknown class ?4 appears in testing.

On the other hand, a more realistic or desired OSR system should NOT just rest on a reject decision but should go further, especially for discovering the hidden unknown classes among the reject instances. Unfortunately, existing OSR methods do not pay special attention to this point. Although Bendale and Boult [25] introduced the open world recognition framework which can collect and label (e.g., by humans) the reject instances to further use for updating the OSR model, it is actually a post-event strategy needing human intervention. Meanwhile, the authors in [21] transferred the knowledge of the similarity and difference in known classes for new class rediscovery among the already-rejected instances. Obviously, this is still a post-event approach. In fact, such a two-step manner [21], [25] easily incurs a suboptimal solution. Therefore, it is necessary to specifically design a model so that both open set recognition and new class discovery can be proceeded simultaneously.

Towards this goal, in this paper, we attempt to introduce a novel collective/batch decision strategy for OSR. As a Bayesian nonparametric modeling method, hierarchical Dirichlet process (HDP) does not overly depend on the training data and can achieve adaptive change as the data changes [26]. In particular, for the new coming instance, HDP has the ability to assign this instance an existing subclass or a new subclass drawn from the base distribution (the details c.f. Section 3.1). Such a property makes HDP automatically reserve space for unknown classes in testing, naturally leading to a new class discovery function. Therefore, with slight modification to HDP, we propose a collective decision-based OSR framework (CD-OSR) as an initial solution towards the open set recognition of collective decision. Note that the HDP can also be replaced by other Bayesian nonparametric techniques [26] like the hierarchical beta process [27], but beyond our focus here. Thanks to the properties of HDP, CD-OSR does not need to define the decision threshold and can implement the open set recognition and new class discovery simultaneously. Additionally, treating the testing instances in batch makes CD-OSR take into account correlations among the instances obviously ignored by existing methods. Note that CD-OSR actually can handle both batch and individual instances. Specifically, the contributions and details of our CD-OSR can be highlighted as follows:

A novel collective/batch decision strategy is first introduced for open set recognition, which can address the instances in batch, even individual instances. Specifically, a collective decision-based OSR framework (CD-OSR) is proposed, which can address both the open set recognition and new class discovery simultaneously.

CD-OSR does not need to define the decision threshold and can automatically reserve space for unknown classes in testing, naturally leading to the new class discovery function.

Treating the testing instances in batch makes CD-OSR consider correlations among the instances obviously ignored by the other existing OSR motheds.

A thorough empirical evaluation of CD-OSR is reported, showing the significant improvement in classification performance and the new class discovery function.

The remainder of this paper is organized as follows. Section 2 gives the related work in open set recognition. Section 3 introduces a novel collective decision strategy for OSR, where a collective decision-based OSR framework is specifically given. Experimental evaluation is reported in Section 4. Finally, Section 5 gives a conclusion.

SECTION 2Related Work
With the formalization of OSR developed in [1], the openness of a particular problem or data universe is defined by considering the number of training, target, and testing classes:
openness=1−2×|training classes||testing classes|+|target classes|−−−−−−−−−−−−−−−−−−−−−−−−−−√.(1)
View SourceRight-click on figure for MathML and additional features.Larger openness corresponds to more open problems, while the problem is completely closed when openness=0. Furthermore, the OSR problem can be defined as follows: given a set of training data U, an open space risk RO, and an empirical risk Rε, the goal of OSR is to find a measurable recognition function f∈H defined by minimizing the following open set risk
f=argminf^∈H{RO(f^)+λrRε(f^(U))},(2)
View SourceRight-click on figure for MathML and additional features.where λr is a regularization constant. Thanks to the guidance of this definition, a large number of OSR algorithms have been proposed. Next, we will briefly review the relevant representative approaches.

2.1 The Existing OSR Methods
2.1.1 The 1-vs-Set Machine
Using the definition of OSR, an SVM-based OSR method called the 1-vs-Set machine [1] is proposed. In 1-vs-Set, the open space risk RO is considered to be the ratio of the Lebesgue measure of positively labeled open space compared to the overall measure of positively labeled space. Concretely, a hyperplane ’B’ (shown in Fig. 1) parallelling the separating hyperplane ’A’ obtained by the SVM is derived in score space, leading to a slab in feature space. Thus, the open space risk for a linear kernel slab model is defined as follows:
RO=ζB−ζAζ++ζ+ζB−ζA+pAωA+pBωB,(3)
View SourceRight-click on figure for MathML and additional features.where ζA and ζB denote the marginal distances of the corresponding hyperplanes, and ζ+ is the separation needed to account for all positive data. Additionally, user-specified parameters pA and pB are given to weight the importance between the margin spaces ωA and ωB.

After training the 1-vs-Set machine, a testing instance that appears between the two hyperplanes would be labeled with the appropriate class. Otherwise, it is considered as non-target class or rejected, depending on which side of the slab it resides. As discussed in Section 1, the 1-vs-Set machine reduces the open space risk to some extent. However, it still occupies the infinite space, meaning the open space risk still exists.

2.1.2 The W-SVM Model
To further reduce the open space risk, Scheirer et al. [8] incorporated non-linear kernels into a solution that further limited open space risk by positively labeling only sets with finite measure. They formulated a compact abating probability (CAP) model, where probability of class membership abates as points move from known data to open space. Specifically, a Weibull-calibrated SVM (W-SVM) model was proposed, which combined the EVT for score calibration with two separated SVMs. The first is a one-class SVM CAP model used as a conditioner: if the posterior estimate PO(y|x) of an input instance x predicted by one-class SVM is less than a threshold ρτ, the instance will be rejected outright. Otherwise, it will be passed to the second SVM. The second one is a binary SVM CAP model via a fitted Weibull cumulative distribution function, yielding the posterior estimate Pη(y|x) for the corresponding positive class. Furthermore, it also obtains the posterior estimate Pψ(y|x) for the corresponding negative classes by a reverse Weibull fitting. Defined an indicator variable: ιy=1 if PO(y|x)>ρτ and ιy=0 otherwise, then the W-SVM model for OSR is defined as follows
y∗=argmaxy∈YPη(y|x)×Pψ(y|x)×ιysubject to  Pη(y∗|x)×Pψ(y∗|x)≥ρR,(4)
View Sourcewhere Y denotes all the known classes, and ρR is the threshold of the second SVM CAP model.

Additionally, the thresholds ρτ and ρR are set empirically, e.g., ρτ is fixed to 0.001 as specified by the authors, while ρR is recommend to set depending on the openness of the specific problem by
ρR=0.5×openness.(5)
View SourceRight-click on figure for MathML and additional features.The W-SVM effectively limits the open space risk by the threshold-based classification schemes. However, such a threshold setting, especially for ρR, is difficult since we usually have no prior knowledge about unknown classes.

2.1.3 The OSNN Model
Adapting the traditional closed-set Nearest Neighbor classifier to the OSR scenario, Ju´nior et al. [11] proposed the OSNN classifier. Let ϑ(x)∈L={ℓ1,ℓ2,…,ℓn} represent the class of the corresponding instance x and L be the set of training labels (known classes). The OSNN first finds the nearest neighbor u1 and u2 of testing instance s, where ϑ(u1)≠ϑ(u2). Then one can calculate the ratio υ=d(s,u1)/d(s,u2), in which d(x,x′) is the euclidean distance between instances x and x′ in the feature space. If υ is less than or equal to the predefined threshold σ (0<σ<1), s is classified as the same label of u1. Otherwise, it is considered as unknown, i.e.,
ϑ(s)={ϑ(u1)if υ≤σ′′unknown′′if υ>σ.(6)
View Source

Note that applying a threshold on the ratio of similarity scores seems better than on the similarity scores themselves as reported in [11]. However, the selection of such a threshold is still an empirical setting, inevitably incurring risks due to lacking available information from unknown classes. As described in Section 1, the OSNN will make a misclassification when unknown class ?4 appears in testing. In addition, just selecting two reference instances from different classes for comparison makes the OSNN model vulnerable for outliers.

2.2 Unseen Class Discovery in existing OSR
In fact, there are also some researchers paying attention to the unknown class discovery in OSR. To further extend open set recognition, the authors in [25] formalized the open world recognition problem: a recognition system should perform four tasks including detecting unknown classes (open set recognition), choosing which instances to label for addition to the model, labelling those instances, and then updating the classifier. Ideally, all of these tasks should be automated. But in [25], the authors just presumed supervised learning with labels obtained by human labeling. In addition, the unknown class discovery in [25] is a post-event strategy, meaning that people must first obtain the rejected instances before proceeding to the new class's discovery. Actually, such a two-step manner easily incurs a suboptimal solution.

Besides, Shu et al. [21] mainly focused on discovering the unknown classes hiding among the reject instances by transferring the knowledge of the similarity and difference in known classes. Correspondingly, a joint open classification framework was proposed with four components: an Open Classification Network (OCN) used for open set recognition, a Pairwise Classification Network (PCN) for classifying whether two input instances are from the same class or not, an auto-encoder for learning representation from unlabeled instances, and a hierarchical clustering model for clustering the reject instances. Similar to [25], this approach adopts a two-step manner as well. Furthermore, the use of knowledge in known classes is risky when the transferred knowledge in known and unknown classes differs.

SECTION 3Collective Decision for Open Set Recognition
As discussed previously, the current existing OSR methods are designed specially for recognizing individual instances, even these instances are all arriving collectively in batch. Hence recognizers in decision either reject or categorize them to some known class instance by instance using empirically-set threshold, where such a decision threshold plays a key role. However, its selection is usually based on the knowledge of known classes, inevitably incurring risks due to no available information from unknown classes. On the other hand, a more realistic OSR system should NOT just rest on a reject decision but should go further, especially for discovering the hidden unknown classes in the reject instances. Regrettably, existing OSR methods do not pay much attention. Although [21], [25] have made some efforts, they are just a two-step strategy.

To overcome these limitations mentioned above, we introduce a novel collective decision strategy for OSR problem with an aim to extend existing open set recognition for new class discovery. Specifically, a collective decision-based OSR framework (CD-OSR) is proposed by slightly modifying HDP. Thanks to the properties of HDP, Our CD-OSR does not need to define the decision threshold and can automatically reserve space for unknown classes in testing, naturally leading to the new class discovery function. Interestingly, this also makes it able to handle OSR and new class discovery at the same time. Moreover, treating the testing instances in batch makes CD-OSR consider correlations among the instances obviously ignored by existing methods. Note that CD-OSR actually can handle both batch and individual instances.

Next, we first give a brief review of HDP [28] widely used for co-clustering multiple groups of data by sharing mixture components among the groups. In HDP, the commonly used terms are ‘group’ or ‘component’. However, we here adapt HDP with slight modification to the OSR problem. Under the OSR scenario, ‘class’ actually corresponds to ‘group’, while ‘subclass’ corresponds to ‘component’. Therefore, in order to avoid confusion, we unify these terms (’class’ ↔ ‘group’, ‘subclass’ ↔ ‘component’) throughout this paper.

3.1 Hierarchical Dirichlet Process
The Dirichlet process (DP) [29], [30] considered as a distribution over distributions is a stochastic process, which is mainly used in clustering and density estimation problems as a nonparametric prior defined over the number of mixture components. As a hierarchical extension to DP, the Hierarchical Dirichlet Process [28] is proposed, modeling each group of data in the form of a Dirichlet process mixture model (DPM). Under this hierarchical structure, an elegant way of sharing parameters is provided, allowing the DPM models across different groups to be connected together through a higher level DP.

Let xji∈Rd, i={1,…,nj}, j={1,…,J} denote the instance i in the group j where nj denotes the number of instances in group j, J is the total number of groups, and θji represents the parameters of the mixture component associated with xji. Then the HDP framework is completed as follows:
G0|γ,H∼DP(γ,H)Gj|α0,G0∼DP(α0,G0) for each jθji|Gj∼Gj  for each j, ixji|θji∼F(θji)  for each j, i,(7)
View SourceRight-click on figure for MathML and additional features.where G0 as a global distribution is distributed as a Dirichlet process with concentration parameter γ and base distribution H. Gj for each group is distributed according to the DP with concentration parameter α0 and base distribution G0. Moreover, as α0 increases, the number of components (or clusters) used to represent each group data increases. Note that although increasing γ can add the clusters used to represent the data of all groups, the degree to which these clusters are shared between groups will decrease at the same time [31]. Besides, xji can also been viewed as a draw from a distribution F(θji).

An intuitive understanding of the generative process defined by a HDP model can be through an analogy to the Chinese Restaurant Franchise (CRF). CRF actually extends upon the Chinese restaurant process (CRP), allowing multiple restaurants to share a set of dishes. In the CRF metaphor, customer i in restaurant j is associated with θji and sits at table tji. The table t is associated with one of the K random draws from H, i.e., ψjt∈{ϕ1,…,ϕK} denoting the global menu of dishes. Moreover, a dish from the global menu served at table t in restaurant j is represented by the indicator variable kjt. In addition, the concentration parameter γ controls the prior probability of serving a new dish at a new table [32].

In this framework, the restaurants correspond to groups, the tables in each restaurant correspond to the mixture components in the DP mixture model, and the dishes in the global menu correspond to the unique set of parameters shared among the restaurants.

The conditional distributions for θji and ψjt can be obtained by integrating out Gj and G0, respectively.
θji|θj1,…,θj,i−1,α0,G0∼∑t=1mj⋅njt⋅i−1+α0δψjt+α0i−1+α0G0,(8)
View SourceRight-click on figure for MathML and additional features.where mj⋅ represents the number of tables in restaurant j, njt⋅ is the number of customers in restaurant j at table t, and δψjt denotes the Dirac measure2 at ψjt. According to (8), the conditional θji is assigned to one of the existing ψjt with probability njt⋅i−1+α0 or ψj,mj⋅+1, i.e., a new table drawn from G0 with probability α0i−1+α0. Marginal counts are represented with dots. Similarly,
ψjt|ψ11,ψ12,…,ψ21,…,ψjt−1,γ,H∼∑k=1Km⋅km⋅⋅+γδϕk+γm⋅⋅+γH,(9)
View SourceRight-click on figure for MathML and additional features.where m⋅k represents the number of tables across all restaurants serving dish ϕk, m⋅⋅ denotes the total number of tables occupied by all restaurants, and δϕk denotes the Dirac measure at ϕk. According to (9), the conditional distribution ψjt inherits one of the existing ϕk with probability m⋅km⋅⋅+γ or ϕK+1, i.e., a new dish drawn from H with probability γm⋅⋅+γ.

The inference of CRF can be performed by using a Gibbs sampling scheme [28], [33]. In order to simplify the derivation without losing the general applicability, the base distribution H here is conjugate to the data distribution F. Furthermore, for ease of understanding, the notations used here are the same as those in [28]. Let zji=kjtji represent xji's index variable. Since tji and kjt are the respective index variables of θji and ψjt (θji=ψjtji, ψjt=ϕkjt, the prior of ϕkjt is H), we actually sample tji and kjt instead of dealing with θji's and ψjt's directly. Let x=(xji:all j,i), xjt=(xji:all i with tji=t), t=(tji:all j,i), k=(kjt:all j,t), z=(zji:all j,i), m=(mjk:all j,k) and ϕ=(ϕ1,…,ϕK) be described like those in [28]. Let t−ji, k−jt or n−jijt, m−jt⋅k respectively denote the corresponding superscripts removed from the sets or from the calculation of the counts. Let f(⋅|θ) and h(⋅) respectively denote the densities F(θ) and H. Integrating out the mixture component parameters ϕ, we can obtain xji's conditional density under mixture component k given all data items except xji as
f−xjik(xji)=∫f(xji|ϕk)∏j′i′≠ji,zj′i′=kf(xj′i′|ϕk)h(ϕk)dϕk∫∏j′i′≠ji,zj′i′=kf(xj′i′|ϕk)h(ϕk)dϕk.
View SourceSimilarly, the conditional density of xjt, i.e., f−xjtk(xjt), can be obtained given all data items associated with mixture component k leaving out xjt. Note that the conditional distributions here have omitted the conditions such as concentration parameters, e.g., f−xjik(xji)=f−xjik(xji|x−ji,α0,γ). Then the conditional distribution of tji is
p(tji=t|t−ji,k)∝{n−jijt⋅f−xjikjt(xji),α0p(xji|t−ji,tji=tnew,k),iftpreviously used,ift=tnew,(10)
View SourceRight-click on figure for MathML and additional features.where
p(xji|t−ji,tji=tnew,k)=∑k=1Km⋅km⋅⋅+γf−xjik(xji)+γm⋅⋅+γf−xjiknew(xji),
View Sourcewhere f−xjiknew(xji)=∫f(xji|ϕ)h(ϕ)dϕ is simply the prior density of xji. Furthermore, the conditional distribution of kjt is
p(kji=k|k−jt,t)∝{m−jt⋅kf−xjtk(xjt),γf−xjtknew(xjt),ifkpreviously used,ifk=knew.(11)
View Source

3.2 CD-Based Open Set Recognition
Since the properties of hierarchical Dirichlet process (HDP) described above fits our problem, we here adapt HDP with slight modification to addressing the OSR problem. Thus a collective decision-based OSR framework (CD-OSR) is proposed as an initial solution towards open set recognition of collective decision. Concretely, the CD-OSR works as follows.

(1) Training Phase: In our CD-OSR framework, we first divide the training set into a fitting set F and a validation set V (the details are given in Section 4.1.1). Next, we model each known class data in F as a group of HDP using a Gaussian mixture model (GMM) with an unknown number of components. Simultaneously, the whole validation set V as one batch is treated in the same way. Then all of the groups are co-clustered under the HDP framework. Unlike HDP, we also append a parameter ϱ denoting the proportion of the corresponding subclass in its class in CD-OSR. If the ϱ is below some constant ε after co-clustering, the corresponding subclass intuitively should be omitted for avoiding the overfitting. Note that the role of ε should not be confused with the thresholds of the existing OSR methods which are used to determine the boundary between known and unknown classes. Then repeat this process several times to preform a grid search operation on the corresponding candidate parameter set, and obtain the appropriate initialization parameter values for CD-OSR. Note that these parameters actually do not overly depend on training data due to the properties of HDP.

Algorithm 1. CD-OSR
Initializing

Let Xtr and Xts respectively denote the Training Set and Testing

Set obtained by the experimental protocol in Section 4.1.1.

According to the class labels, Divide the Training Set

Xtr=[Xtr1;Xtr2;...] sequentially class by class.

Initializing parameters: μ0,Σ0,β,ν,ς,α0,γ,H,ε,T, InS described in the paper.

Co-clustering under the HDP framework

Results = HDP([Xtr;Xts], α0, γ, H, InS, T)

Determining the subclasses

After co-clustering, let ϱ denote the proportion of the corresponding subclass in its class: the subclass will be removed, if ϱ<ε.

Predicting

For a new coming instance, it will be labeled according to the following rule:

a corresponding known class, if the subclass assigned to it belongs to some known class.

an unknown class, if the subclass assigned to it comes from a new draw from H.

The HDP here is the software package from [28] used to implement the Hierarchical Dirichlet Process

(2) Testing Phase: Fixing the appropriate initialization parameters achieved in training, we will obtain our CD-OSR recognition framework. Similar to the training phase, we model each known class data in training set as a group of the CD-OSR, while the whole testing set as one collective/batch3 is treated in the same way. Then all of the groups are co-clustered under CD-OSR. After the co-clustering process, each class will obtain one or many subclasses. Note that the key to classification is whether the subclass assigned to the testing instance is included in the corresponding known class or not: if yes, the instance is labeled as the appropriate known class; otherwise it will be recognized as unknown class, as shown in Fig. 2. Furthermore, Algorithm 1 also shows the workflow of CD-OSR.

Fig. 2. - 
Each known class (here is class 1-4), as a group in CD-OSR, is modeled by a Dirichlet Process while the testing set (including unknown classes or not) as a whole is treated in the same way. Then all of the groups are co-clustered under the CD-OSR framework. For a testing instance, it would be labeled as the appropriate known class or unknown class, depending on whether the subclass this sample is assigned associates with the corresponding known class or not. The number in the circle indicates the corresponding subclass.
Fig. 2.
Each known class (here is class 1-4), as a group in CD-OSR, is modeled by a Dirichlet Process while the testing set (including unknown classes or not) as a whole is treated in the same way. Then all of the groups are co-clustered under the CD-OSR framework. For a testing instance, it would be labeled as the appropriate known class or unknown class, depending on whether the subclass this sample is assigned associates with the corresponding known class or not. The number in the circle indicates the corresponding subclass.

Show All

Note that the testing phase is nothing but a co-clustering process, which seems to have the flavor of lazy learning to some extent. Furthermore, the collective/batch operation for the testing set makes our CD-OSR can address the instances in batch, even individual instances. Unlike existing OSR methods which infer unknown classes depending on the empirically-set decision threshold, our CD-OSR does not need to define such a threshold and can provide explicit modeling for the unknown classes appearing in testing. This naturally endows it new class discovery capability which will be detailed in Section 4.3. Such a capability intuitively makes our CD-OSR have zero open space risk under ideal conditions where all classes including known and unknown classes are mutually exclusive. Moreover, under the CD-OSR framework, each new/unknown class will inherently have only one subclass as we have no available knowledge from unknown classes. Note that unlike the two-step manner in [21], [25], CD-OSR actually is a jointly solving manner due to the co-clustering of HDP. In addition, the collective operation in CD-OSR also makes our framework consider the correlations among the testing instances obviously ignored by other existing OSR methods.

Besides, the key to accurate prediction of our CD-OSR is the sharing of subclasses between the testing set's group and the groups of the training set. However, the known classes of the training data may also share the same subclasses between themselves, resulting in an unidentifiable problem. Therefore, we usually set a lager γ to decrease the degree to which the subclasses are shared between those classes. Intuitively, if all classes (including known and unknown classes) are mutually exclusive, the subclasses associated with the different classes would be different, making the input instances identifiable. Furthermore, we state the following proposition.

Proposition 1.
Assume the set of potential classes, i.e., known and unknown, are mutually exclusive. Let m⋅k, m⋅⋅, γ, ϕk, and H be described as above, and L denote the number of subclasses associated with the corresponding known classes. Then our CD-OSR framework can model the subclasses of the new coming instances associated with the corresponding known classes with probability ∑Lk=1m⋅km⋅⋅+γδϕk or unknown classes with probability γm⋅⋅+γ, whilst it would have zero open space risk.

Proof.
This proposition can be obviously obtained from the generative process of HDP.

3.2.1 Computational Complexity Analysis
Using the Gaussian-Wishart distribution (the details c.f. Section 4.1.2) as the base distribution H, CD-OSR adopts the Gibbs sampling scheme developed by [28] to implement the inference process. In this scheme, the only computations needed are marginal likelihood (ML) computations, and posterior (PO) computations for the parameters in the Gaussian-Wishart distribution.4 Thus its computational complexity is mainly determined by two parts. One part needs no repeated updates, which contains the ratio of Gamma terms with O(dN) (N is the total number of the instances) in ML and the prior of covariance matrix with O(d3) in both ML and PO, the other is the posterior update of the covariance matrix with O(d2T) (T is the number of iterations) in both ML and PO. Further considering the number of mixture component K, the number of groups/classes J, the total computational complexity of CD-OSR is roughly about O(d3+dN+d2TKJ). Note that K is changing in each iteration.

SECTION 4Experimental Evaluation
To verify the effectiveness of our CD-OSR framework, we carry out several experiments on the benchmark datasets commonly used in OSR scenario, including LETTER [34], USPS [35], PENDIGITS [36], COIL20 [37], and Extended Yale B [38]. As an initial solution towards collective decision for open set recognition, we compare our CD-OSR with the mainstream OSR methods, including the 1-vs-Set machine, W-OSVM,5 W-SVM, PI-SVM and OSNN. Note that the W-SVM and PI-SVM are the currently popular algorithms.

Here we mainly focus on the comparisons of the F-measure among these methods since it better emphasizes the distinction between correct positive and negative classifications [8]. The F-measure is defined as a harmonic mean of Precision and Recall
F-measure=2⋅Precision⋅RecallPrecision+Recall,
View SourceRight-click on figure for MathML and additional features.where
Precision=TPTP+FP,
View SourceRight-click on figure for MathML and additional features.and
Recall=TPTP+FN.
View SourceRight-click on figure for MathML and additional features.TP, FN and FP respectively represent true positive, false negative and false positive of known classes. Note that although the computations of Precision and Recall are only for available known classes, the FN and FP actually also consider the false unknown classes and false known classes by taking the false negative and the false positive into account [11]. Concretely, we use the micro-F-measure [11] as an evaluation metric. The higher the micro-F-measure, the better the performance of an OSR algorithm. For comparison, we also give the recognition accuracy for these algorithms in the Supplementary Material, which can be found on the Computer Society Digital Library at http://doi.ieeecomputersociety.org.ezproxy.auckland.ac.nz/10.1109/TKDE.2020.2978199.

In addition, the experimental setups including the experimental protocol and the parameter setting are given in Section 4.1. Section 4.2 presents the main experimental results, while the new class discovery function is reported in Section 4.3.

4.1 Experimental Setup
4.1.1 Experimental Protocol
As described in Section 1, the selection of suitable thresholds for the corresponding OSR methods is difficult and risky due to lacking available information from unknown classes. To mitigate this challenge, similar to [11], a parameter optimization phase adapted to the OSR scenario is performed to find the better parameters for all methods in this paper. Note that the optimal parameter values are selected based on the tradeoff on F-measure between the simulations of ‘Closed-Set’ and ‘Open-Set’ scenarios built in the validation set.

As shown in Fig. 3, the dataset is first divided into training set owning known classes and testing set including known and unknown classes, respectively. Among the classes occurring in training set, half are chosen to act as “known” classes in the simulation, the other half as “unknown” in the simulation. Thus the training set is divided into a fitting set F just containing the “known” classes and a validation set V including a ‘Closed-Set’ simulation and an ‘Open-Set’ simulation. The ‘Closed-Set’ simulation only owns the “known” classes, while the ‘Open-Set’ simulation contains all the classes appearing in the training set. Note that in the training phase, all the methods are trained with F and evaluated on V. Additionally, we give the following experimental protocol. For each experiment in this paper, we

Fig. 3. - 
Data partitioning. The dataset is first divided into training and testing sets, then the training set is further divided into a fitting set and a validation set containing a ‘closed set’ simulation and a ‘open set’ simulation.
Fig. 3.
Data partitioning. The dataset is first divided into training and testing sets, then the training set is further divided into a fitting set and a validation set containing a ‘closed set’ simulation and a ‘open set’ simulation.

Show All

randomly select Ω available classes as known classes for training from the dataset;

randomly choose 60 percent of the instances in each of the Ω selected classes as training set;

select the remaining 40 percent of the instances from step 2 and the instances from other classes excluding the Ω classes as testing set;

randomly select [(Ω/2+0.5)] classes as “known” classes for fitting from the training set, while the remaining classes as “unknown” classes for evaluating;

randomly choose 60 percent of instances from each “known” classes of the training set as fitting instances in F;

select the remaining 40 percent of the instances from step 5 as the ‘Closed-Set’ simulation, while the remaining 40 percent of the instances from step 5 and the ones from “unknown” classes in training set as the ‘Open-Set’ simulation;

train all the models with F and evaluate them on V, then find the suitable parameters;

evaluate the performance for all the methods with 10 randomized training and testing sets after the parameters of corresponding models are determined.

Remark.
While several different randomness in the experiments (e.g., the Gibbs sampling during the inference process, the random division for the dataset and so on), the experimental results in this paper are from the repetition of multiple evaluations based on the corresponding random division for the dataset.

4.1.2 Parameter Setting
This part details the parameter setting for all of the methods used in this paper. For the 1-vs-Set machine, we use the default setting in the code provided by the authors. For W-OSVM and W-SVM adopting one-vs-rest approach, we fix the threshold ρτ for the one-class SVM CAP model in 0.001 as specified by the authors, while a grid search in {10−7,10−6,…,10−1} is performed for threshold ρR. Similar to W-SVM, PI-SVM also uses the one-vs-rest approach. Accordingly, a grid search in {10−7,10−6,…,10−1} is performed for threshold ρ in PI-SVM. As for the related SVM parameters including the W-OSVM, W-SVM and PI-SVM, we perform grid search for C∈{2−5,2−4,…,25} and g (gamma)∈{2−8,2−7,…,23}. Furthermore, the implementation codes including 1-vs-Set machine, W-OSVM, W-SVM and PI-SVM can be found at https://github.com/ljain2/libsvm-openset. For OSNN, only the threshold σ needs to be optimized, and we adopt the same strategy described in [11]. Please note that in this paper, once the thresholds of these methods are determined in training, their values will no longer change in testing, since we usually know nothing about unknown classes.

For CD-OSR, we have two learning phase. In the training phase, our goal is to get the the appropriate initialization parameters. Towards this goal, we model each known class in the fitting set F using the Gaussian mixture model. Each component in the mixture model is associated with a Gaussian distribution with the mean vector μjt and covariance matrix Σjt, i.e., ψjt={μjt,Σjt}. For the base distribution H, we define a conjugate prior, i.e., Gaussian-Wishart distribution
H=p(μ,Σ|μ0,β,Σ0,ν)=N(μ|μ0,(βΣ)−1)W(Σ|Σ0,ν),(12)
View Sourcewhere μ0 is the prior mean, β is a scaling constant controlling the deviation of the mean vectors of mixture components from the prior mean. Σ0 denotes the prior covariance matrix, and ν is the number of degrees of freedom of the distribution. In order to confirm the validity of our learning framework, we do not take an overly complicated means to select the initialization parameters in the CD-OSR. In contrast, we here let μ0 simply equal the mean of all the instances in F,6 β equal 1, and ν be selected by performing a grid search from the set {d,d+1,…,d+20}. Furthermore, Σ0 is set as follows
Σ0=ς×∑J−1j=1(nj−1)Σjn−(J−1),(13)
View SourceRight-click on figure for MathML and additional features.where the ς is a scaling constant and also obtained by performing a grid search from the set {0.00001,0.0001,0.001,0.01,0.1,0.2,…,1}. J−1 represents the number of known classes in F.7 n is the total number of the instances in F. The second term on the right side of (13) denotes the common pooled covariance matrix of the known classes [39]. Moreover, for the base distributions H and G, the concentration parameters are given by the vague gamma priors [40]. Specifically, we set γ∼Gamma(100,1) and α0∼Gamma(10,1) to ensure enough subclasses used to represent each known class, while reducing the sharing of subclasses between the different known classes. The maximum number of iterations of CD-OSR (T) is set to 30, while the initial number of mixture components (InS) is set to 30. Additionally, the ε is empirically set to 0.01, where Section 4.2.3 details the reason of this setting.

After the training phase, we will obtain the appropriate values of initialization parameters for CD-OSR. Fixing these parameters’ values, we only need to respectively replace fitting set F and validation V with the training set and testing set, then repeat 10 rounds of the co-clustering process to obtain the final experimental evaluation.

4.2 Performance Evaluation
This subsection mainly contains three parts. Section 4.2.1 reports the F-measure comparisons among our CD-OSR with the 1-vs-Set, W-OSVM, W-SVM, PI-SVM, and OSNN. Section 4.2.2 shows the influence of the batch size of the testing data on performance, while the influence on parameter ε is discussed in Section 4.2.3.

4.2.1 Comparisons on F-Measure
Results on LETTER. The LETTER dataset has a total of 20000 instances from 26 classes, where every instance owns 16 features. To recast Letter dataset as a dataset for open set problem, we randomly select 10 available classes as known classes for training, and vary openness by adding a subset of the remaining 16 classes. Fig. 4 shows the average F-measure results on this dataset. With the openness less than about 12 percent, the performance of our CD-OSR is comparable to the W-SVM and PI-SVM. However, it is almost significantly higher than the other five methods used in this paper when the openness is larger than about 12 percent. Furthermore, the changing trend of F-measure in CD-OSR is also relatively stable when varying the openness.

Fig. 4. - 
F-measure for multi-class open set recognition on LETTER dataset. Error bars reflect the standard deviation.
Fig. 4.
F-measure for multi-class open set recognition on LETTER dataset. Error bars reflect the standard deviation.

Show All

Results on USPS. The USPS dataset has a total of 7291 instances from 10 classes, where every instance owns 256 features. In this paper, principal component analysis (PCA) is used to project instance space into 39 dimensional subspace, retaining 95 percent of the instances’ information. Similar to the operation of LETTER dataset, we randomly select 5 available classes as known classes for training, and vary openness by adding a subset of the remaining 5 classes. Fig. 5 shows the average F-measure results on this dataset. As can be seen from Fig. 5, our CD-OSR obtains much higher performance than the 1-vs-Set, W-SVM and PI-SVM with increasing the openness. Although the OSNN obtains the higher performance than the CD-OSR when the openness is larger than about 12 percent, its performance is much lower than our method when openness is less than 8 percent, especially when the openness equals zero. Furthermore, compared to the other methods, the changing trend of F-measure in OSNN is most stable, followed by our CD-OSR, W-SVM and PI-SVM. Note that the performance of W-OSVM is not shown in Fig. 5 due to its poor F-measure.

Fig. 5. - 
F-measure for multi-class open set recognition on USPS dataset, and the performance of W-OSVM is not shown due to its poor F-measure. Error bars reflect the standard deviation.
Fig. 5.
F-measure for multi-class open set recognition on USPS dataset, and the performance of W-OSVM is not shown due to its poor F-measure. Error bars reflect the standard deviation.

Show All

Results on PENDIGITS. The PENDIGITS dataset has a total of 10992 instances from 10 classes, where every instance owns 16 features. Similar to the operation of USPS dataset, we randomly select 5 available classes as known classes for training, and vary openness by adding a subset of the remaining 5 classes. Fig. 6 shows the average F-measure results on this dataset. As can be seen from Fig. 6, our CD-OSR obtains much higher performance than the other methods as the openness increases. Simultaneously, the performance of our CD-OSR is almost unchanged when varying the openness.


Fig. 6.
F-measure for multi-class open set recognition on PENDIGITS dataset. Error bars reflect the standard deviation.

Show All

Results on COIL20. The COIL20 dataset has a total of 1440 gray images from 20 objects [48]. Each image is down-sampled to 16×16, i.e., the input dimension is 256. We further reduce it to 55 dimensions (retaining 95 percent of the instances’ information) using PCA technique. Then we randomly select 10 objects as known classes, and vary openness by adding a subset of the remaining 10 objects. As shown in Fig. 7, our CD-OSR is significantly better than the other five algorithms: CD-OSR not only achieves the best F-measure performance but also has the better stability with increasing openness.

Fig. 7. - 
F-measure for multi-class open set recognition on COIL20 dataset. Error bars reflect the standard deviation.
Fig. 7.
F-measure for multi-class open set recognition on COIL20 dataset. Error bars reflect the standard deviation.

Show All

Results on Extended Yale B. The Extended Yale B (YALEB) dataset has a total of 2414 frontal-face images from 38 individuals. Each class has around 64 images. The images are cropped and normalized to 32×32. Similar to COIL20, we reduce the input dimension to 69 using PCA. Fig. 8 shows the F-measure performance on this dataset. As can been seen from Fig. 8, both our CD-OSR and PI-SVM gain significant advantages over other methods. Furthermore, with the openness increasing (about openness >15%), CD-OSR starts to be slightly better than PI-SVM. Besides, though W-SVM and OSNN also have good stability with increasing openness, their F-measure performances are not satisfactory.


Fig. 8.
F-measure for multi-class open set recognition on Extended Yale B dataset. Error bars reflect the standard deviation.

Show All

Remark.
From the experimental results reported above, we can find that the classification performance of our CD-OSR is significantly improved compared to other existing OSR algorithms. However, what we still want to emphasize is that the CD-OSR currently does not make full use of the information from the known class labels. More precisely, it just uses this kind of information to assign the training data to different groups, while the discriminative information from these labels actually is not fully utilized. Nevertheless, CD-OSR still achieves at least comparable classification performance than other existing OSR methods making full use of label information like W-SVM, PI-SVM, and so forth. Additionally, the above experimental results are also shown in tables of the Supplementary Material, available online, to further demonstrate the superiority of our CD-OSR.

4.2.2 The Influence of the Batch Size on Performance
Since our CD-OSR adopts the collective/batch decision strategy, meaning it can address the data in batch. Then a natural problem is that whether the size of batch for testing data has an influence on CD-OSR's performance. To explore this problem, we conduct the following experiments.

For each dataset in our experiments, we choose a medium openness: 18.35 percent for LETTER (10 unknown classes), 12.29 percent for USPS (3 unknown classes), 12.29 percent for PENDIGITS (3 unknown classes), 12.29 percent for COIL20 (6 unknown classes), and 18.35 percent for YALEB (10 unknown classes) then vary the size of the batch by changing the number of testing instances. Specifically, we randomly select 20, 40, 60, 80, 100 percent of the whole testing set for each dataset, then repeat 10 times of the co-clustering process to obtain the final experimental evaluation. Figs. 9, 10, 11, 12, and 13 show the performance of F-measure on these datasets. The (a) in these figures denotes the boxplot graph for the different number of testing instances, while (b) represents the corresponding errorbar graph, where error bars reflect the standard deviation. From these experimental results, we can find that the batch size of the testing instances has almost no significant influence on the performance of CD-OSR. Therefore, we can flexibly set the batch size according to the needs of the tasks.

Fig. 9. - 
The F-measure on LETTER dataset when openness = 18.35 percent. (a) denotes the boxplot graph for the different number of testing instances, while (b) represents the corresponding errorbar graph, where error bars reflect the standard deviation.
Fig. 9.
The F-measure on LETTER dataset when openness = 18.35 percent. (a) denotes the boxplot graph for the different number of testing instances, while (b) represents the corresponding errorbar graph, where error bars reflect the standard deviation.

Show All

Fig. 10. - 
The F-measure on USPS dataset when openness = 12.29 percent. (a) denotes the boxplot graph for the different number of testing instances, while (b) represents the corresponding errorbar graph, where error bars reflect the standard deviation.
Fig. 10.
The F-measure on USPS dataset when openness = 12.29 percent. (a) denotes the boxplot graph for the different number of testing instances, while (b) represents the corresponding errorbar graph, where error bars reflect the standard deviation.

Show All

Fig. 11. - 
The F-measure on PENDIGITS dataset when openness = 12.29 percent. (a) denotes the boxplot graph for the different number of testing instances, while (b) represents the corresponding errorbar graph, where error bars reflect the standard deviation.
Fig. 11.
The F-measure on PENDIGITS dataset when openness = 12.29 percent. (a) denotes the boxplot graph for the different number of testing instances, while (b) represents the corresponding errorbar graph, where error bars reflect the standard deviation.

Show All


Fig. 12.
The F-measure on COIL20 dataset when openness = 12.29 percent. (a) denotes the boxplot graph for the different number of testing instances, while (b) represents the corresponding errorbar graph, where error bars reflect the standard deviation.

Show All

Fig. 13. - 
The F-measure on Extended Yale B dataset when openness = 18.35 percent. (a) denotes the boxplot graph for the different number of testing instances, while (b) represents the corresponding errorbar graph, where error bars reflect the standard deviation.
Fig. 13.
The F-measure on Extended Yale B dataset when openness = 18.35 percent. (a) denotes the boxplot graph for the different number of testing instances, while (b) represents the corresponding errorbar graph, where error bars reflect the standard deviation.

Show All

4.2.3 The Influence of Parameter ε on Performance
Different from the thresholds in existing OSR methods (such as W-SVM, PI-SVM and OSNN, etc.) where they are essentially used to determine the decision boundary between known and unknown classes, the parameter ε here determines whether a subclass in certain class should be removed or not for avoiding the overfitting. As discussed in Section 1, existing OSR methods obtain such thresholds only based on the known class knowledge. However, this is actually very risky since it is usually agnostic where an instance of an unknown class appears. In contrast, the selection of ε is relatively easier due to the intuition that both known and unknown classes get equal-treatment. In other words, no matter it is a known class or an unknown class, if any of its subclasses contains just very few instances, the subclass intuitively should be removed to avoid overfitting.

In order to verify such an intuition, we perform sensitivity experiments on ε in both closed and open set scenarios. For the open set scenario, we adopt the same openness setting in Section 4.2.2, i.e., 18.35 percent for LETTER, 12.29 percent for USPS, 12.29 percent for PENDIGITS, 12.29 percent for COIL20, and 18.35 percent for YALEB. The candidate set of ε is {0, 0.00001, 0.0001, 0.001, 0.01, 0.1}. As shown in Figs. 14a and 14b, the small ε (about ≤0.01) usually leads to satisfactory classification performance for most datasets (4/5), while the change in ε value does not generally have a dramatic influence on performance. Of course there are also exceptions: the USPS's performance curve has a sharp drop when ε<0.01. Even though so, we can still observe that the trends of performance curves on all datasets are nearly consistent in both closed set and open set scenarios as the ε value varies, corroborating our intuition. More interestingly, our CD-OSR obtains better performances at ε=0.01 on all datasets, which experimentally indicates the generality for the ε's selection to great extent. Therefore, ε can reasonably be set to 0.01 in all experiments of our paper, as described in Section 4.1.2.

Fig. 14. - 
Parameter $\varepsilon$ɛ sensitivity experiment. (a) shows the influence of $\varepsilon$ɛ on classification performance in closed set scenario, while (b) shows this influence in open set scenario.
Fig. 14.
Parameter ε sensitivity experiment. (a) shows the influence of ε on classification performance in closed set scenario, while (b) shows this influence in open set scenario.

Show All

4.3 New Class Discovery
In this subsection, we show the new class discovery function under our CD-OSR framework. Unlike the existing methods infer the unknown classes depending on accurately modeling for the known classes, the CD-OSR is able to provide explicit modeling for unknown classes appearing in testing. Thus it can discover new classes. Furthermore, instead of the two-step manner in [21], [25], CD-OSR adopts a jointly solving process to handles both open set recognition and new class discovery. As mentioned above, the true labels of unknown classes are unknown, making it impossible to further aggregate the newly generated subclasses. Therefore, each new class will inherently have only one subclass. In other words, these newly discovered classes are just at subclass level. Fortunately, we can still roughly estimate the number of real unknown classes based on the number of subclasses of known classes. This can be used as a prior for the other clustering algorithms (such as K-means, etc.) to further discover the real classes among the reject instances. Concretely, we have
Δ=[|Sunknown||Sknown/(J−1)|+0.5],(14)
View SourceRight-click on figure for MathML and additional features.where |Sunknown| denotes the number of subclasses corresponding to unknown classes, |Sknown| denotes the number of subclasses of known classes, and J−1 here represents the number of known classes. Note that this is just a relatively rough estimate. Actually, a more realistic operation is that we can construct a candidate set according to (14) for other clustering algorithms to quickly determine a more accurate estimate.

Furthermore, Tables 1 and 2 respectively report the new class discovery function for USPS and PENDIGITS datasets under CD-OSR framework. Each table has three columns, where the first column denotes the corresponding group data (known classes and testing set), the second one indicates the number of the subclasses of the corresponding group, and the third one represents the proportions of the corresponding subclasses in their group.

TABLE 1 New Class Discovery on USPS Dataset

TABLE 2 New Class Discovery on Pendigits Dataset

For USPS shown in Table 1, we randomly select 5 classes (the real classes in brackets) as the known classes for training, while the testing set has all of the classes (5 known classes and 5 unknown classes). According to (14), we can obtain the rough estimate
Δ=[|Sunknown||Sknown|/(J−1)+0.5]=[1419/5+0.5]=4,(15)
View SourceRight-click on figure for MathML and additional features.where the estimated number of unknown classes Δ approaches the true number of unknown classes. Actually, we may obtain more accurate estimate if the number of subclasses for the corresponding classes are relatively uniform.

Similar to the operation of USPS, we also randomly choose 5 classes as the known classes for training, while testing set owns all the classes (5 known classes and 5 unknown classes). Table 2 reports the specific results, where we can obtain the similar conclusion described above. Moreover, we can discover the internal distribution corresponding to each known class at the subclass level, which can be seen as a by-product of our approach. For example, the distribution of instances corresponding to class 1 (‘2’) is very concentrated, where almost all the instances are clustered in one subclass S13. In contrast, the instances’ distribution of class 5 (’3’) is relatively scattered, where most of the instances are scattered in 7 subclasses, as shown in Table 1.

SECTION 5Conclusion
The main contribution of this paper is to present a collective/batch decision strategy for open set recognition with an aim to extend existing open set recognition for new class discovery while considering correlations among the testing instances. To achieve this goal, we adapt HDP with slight modification to addressing the OSR problem, leading an initial solution towards collective decision in OSR. What needs to be highlighted is that our CD-OSR does not overly depend on the training data and can achieve adaptive change as the data changes. More precisely, CD-OSR can provide explicit modeling for unknown classes appearing in testing. This naturally leads to the new class discovery function, even though it is just at the subclass level. Furthermore, unlike the existing methods dealing with the OSR problem from the discriminative model perspective, the CD-OSR actually addresses this problem from the generative model perspective due to the use of HDP. Finally, the experimental results on a set of benchmark datasets indicate the validity of our learning framework.

Besides, it should be noted that modeling unknown classes only performs in the testing phase of our CD-OSR, whilst no available knowledge from unknown classes is utilized during the training phase. This seems to have the flavor of lazy learning to some extent. Thus the co-clustering process (testing process) will be repeated when other batch testing data arrives, resulting in higher computational overhead. Therefore, overcoming this limitation will be a promising research direction in the future. Furthermore, since the CD-OSR currently does not make full use of the discriminative information from the known class labels, embedding this kind of information more effectively will be also worth further exploring. In addition, replacing the Gibbs sampler with scalable deterministic inference techniques is a promising direction as well in the future work. In conclusion, the CD-OSR is just as a conceptual proof for open set recognition towards collective decision at present. Therefore, the more effective collective decision methods for OSR are worth further exploring in the future work.