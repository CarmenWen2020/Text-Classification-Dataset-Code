We suggest a new optimization technique for minimizing the sum ∑ni=1gi(x) of n non-convex real functions that satisfy a property that we call piecewise log-Lipschitz. This is by forging links between techniques in computational geometry, combinatorics and convex optimization. As an example application, we provide the first constant-factor approximation algorithms whose running-times are polynomial in n for the fundamental problem of Points-to-Lines alignment : Given n points p1,…,pn and n lines ℓ1,…,ℓn on the plane and z>0 , compute the matching π:[n]→[n] and alignment (rotation matrix R and translation vector t ) that minimize the sum of euclidean distances ∑ni=1dist(Rpi−t,ℓπ(i))z between each point to its corresponding line. This problem is non-trivial even if z=1 and the matching π is given. If π is given, our algorithms run in O(n3) time, and even near-linear in n using core-sets that support: streaming, dynamic, and distributed parallel computations in poly-logarithmic update time. Generalizations for handling e.g., outliers or pseudo-distances such as M -estimators for the problem are also provided. Experimental results and open source code show that our algorithms improve existing heuristics also in practice. A companion demonstration video in the context of Augmented Reality shows how such algorithms may be used in real-time systems.
SECTION 1Introduction
We define below the general problem of minimizing sum of piecewise log-Lipschitz functions, and then suggest an example application.

Minimizing Sum of Piecewise Log-Lipschitz Functions. We consider the problem of minimizing the sum ∑g∈Gg(x) over x∈Rd of a set G of n real non-negative functions that may not be convex but satisfy a property called the piecewise log-Lipschitz property as in Definition 2.

More generally, we wish to minimize the cost function f(g1(x),…,gn(x)) where f:Rn→[0,∞) is a log-Lipschitz function as in Definition 1, and {g1,…,gn} is the set of piecewise log-Lipschitz functions in G.

As an application, we reduce the following problem to minimizing such a set G of functions.

Points-to-Lines alignment is a fundamental problem in computational geometry, and is a direct generalization of the widely known problem of point set registration, where one is required to align to sets of points; see [1] and references therein.

Formally, the input to the points-to-lines alignment problem in 2D space is an ordered set P={p1,…,pn} of n points and a corresponding ordered set L={ℓ1,…,ℓn} of n lines, both in R2. The output is a pair (R,t) of rotation matrix R∈R2×2 and translation vector t∈R2 that minimizes the sum of euclidean distances over each point (column vector) and its corresponding line, i.e.,
min(R,t)∑i=1ndist(Rpi−t,ℓi),(1)
View Sourcewhere the minimum is over every rotation matrix R∈R2×2 and translation vector t∈R2. Here, dist(x,y)=∥x−y∥2 is the euclidean distance but in practice we may wish to use non-euclidean distances.

The generalization of the problem in (1) for 3D space where the lines intersect the origin yields the well known Perspective-n-Point (PnP) problem from computer vision, which was first introduced in [2], and aims to compute the position of an object (formally, a rigid body) based on its position as detected in a 2D-camera. Here, each 2D pixel corresponds to a line in 3D, and we assume that the structure of the object (its 3D model) is known. This problem is equivalent to the problem of estimating the position of a moving camera, based on a captured 2D image of a known object, which is strongly related to the very common procedure of “camera calibration” that is used to estimate the external parameters of a camera using a chessboard. More example applications include object tracking and localization, localization using sky patterns and many more; see Figs. 1, 2 and 3


Fig. 1.
Alignment of points to lines. Finding the optimal alignment (R,t) that aligns the set of points P (blue circles) to the set L of lines (red infinite lines). The set P′ is the result of applying the alignment (R,t) to the initial set P.

Show All


Fig. 2.
(Top left) A potential AR application. A user observing a scene through AR glasses. The goal is to present virtual objects to the user by placing them on top of the currently observed frame. Object tracking and localization for the AR glasses can be done by aligning the points in the observed image (top right) to the predefined lines of the object (bottom left). The aligned sets are presented in the bottom right image. The images were taken from our companion supplementary material video [3].

Show All


Fig. 3.
Localization using sky patterns.

Show All

While dozens of heuristics were suggested over the recent decades, to our knowledge, this problem is open even when the points and lines are on the plane, as in this paper handles, e.g., when we wish to align a set of GPS points to a map of lines (say, highways).

We hope the results of this paper will lead to future research of the points-to-lines alignment problem (1) in higher dimensions.

1.1 Generalizations
We consider more generalizations of the above problem as follows.

Unknown matching is the case where the matching π:[n]→[n] between the ith point pi to its line ℓi in the PnP problem (1) is not given for every i∈{1,…,n}=[n]. E.g., in the PnP problem there are usually no labels in the observed images. Instead, the minimization is over every pair of rotation and translation (R,t) and matching (bijective function) π:[n]→[n]
min(R,t),π∑i=1ndist(Rpi−t,ℓπ(i)),(2)
View SourceRight-click on figure for MathML and additional features.where π(i) is the index of the line corresponding to the ith point pi.

Non-distance functions where for an error vector v=(v1,…,vn) that corresponds to the n input point-line pairs given some candidate solution, the cost that we wish to minimize over all these solutions is f(v) where f:Rn→[0,∞) is a log-Lipschitz function as in Definition 1. Special cases include f(v)=∥v∥∞ for “worst case” error, f(v)=∥v∥1/2 which is more robust to noisy data, or f(v)=∥v∥2 of sum of squared errors for maximizing likelihood in the case of Gaussian noise. As in the latter two cases, the function f may not even satisfy the properties of a distance function such as the triangle inequality. Another example may be f(v)=∥v∥1,n−k for some integer k≥1, where ∥v∥1,n−k is the ℓ1 norm of the smallest n−k entries in the vector v, i.e., suppressing the k largest entries of v. Such f is a 1-log-Lipschitz function since ∥c⋅v∥1,n−k=∥c⋅smallest(v,n−k)∥1=c⋅∥smallest(v,n−k)∥1=c⋅∥v∥1,n−k, where smallest(v,n−k)∈Rn−k is a vector containing the smallest n−k entries of v. This function f may be used to ignore k outliers in the input.

Robustness to outliers may be obtained by defining f to be a function that ignores or at least suppresses the effect of very large distances, maybe based on some given threshold. An M-estimator can be used as such a function f, and is usually a non-convex function. M-estimators are in general robust statistics functions, such as the well known Tukey loss function; see [4]. Another example may be the special norm ∥⋅∥1,n−k defined in the previous paragraph.

Coreset in this paper refers to a small representation of the paired input point-line sets P and L by a weighted (scaled) subset. The approximation is (1+ε) multiplicative factor, with respect to the cost of any item (query). E.g., in (1) the item (query) is a pair (R,t) of rotation matrix R and translation vector t. Composable coresets for this problem have the property that they can be merged and re-reduced; see e.g., [5], [6].

Our main motivation for using existings coresets is (i) to reduce the running time of our algorithms from polynomial to near-linear in n, and (ii) handle big data computation models as follows, which is straightforward using composable coresets.

Handling big data in our paper refers to the following computation models: (i) Streaming support for a single pass over possibly unbounded stream of items in A using memory and update time that is sub-linear (usually poly-logarithmic) in n. (ii) Parallel computations on distributed data that is streamed to M machines where the running time is reduced by M and the communication between the machines to the server should be also sub-linear in its input size n/M. (iii) Dynamic data which includes deletion of pairs. Here O(n) memory is necessary, but we still desire sub-linear update time.

1.2 Our Contribution
The contribution of this paper is summarized as follows.

Optimization of Piecewise Log-Lipschitz Functions. Given x∗∈R and piecewise O(1)-log-Lipschitz functions g1,…,gn, we prove that one of their minima x′∈R approximates their value gi(x∗) simultaneously (for every i∈[n]), up to a constant factor. See Theorem 3. This yields a finite set of candidate solutions that contains an approximated solution, without knowing x∗.

Generic framework for defining a cost function cost(A,q) for any finite input subset A={a1,…,an} of a set X called ground set, and an item q (called query) from a (usually infinite) set Q. We show that this framework enables handling the generalizations in Section 1.1 such as outliers, M-estimators and non-distance functions. Formally, we define
cost(A,q):=f(lip(D(a1,q)),…,lip(D(an,q))),(3)
View SourceRight-click on figure for MathML and additional features.where f:[0,∞)n→[0,∞) and lip:[0,∞)→[0,∞) are O(1)-log-Lipschitz functions, and D:X×Q→[0,∞); See Definition 4.

We use this framework along with the first result to compute q′∈Q that approximates D(ai,q∗) simultaneously (for every i∈[n]), where q∗ is the query that minimizes cost(A,q) over every q∈Q; see Observation 5.

Simultaneous optimization and matching may be required for the special case that A={(ai,bi)}ni=1 is a set of pairs, and we wish to compute the permutation π∗ and query q∗ that minimize cost(Aπ,q) over every π∗:[n]→[n] and q∗∈Q, where Aπ={(ai,bπ(i))}ni=1 is the corresponding pairs after permuting bi. We provide constant factor approximations for the case f=∥⋅∥1 in (3); See Theorem 11.

Approximated Points-to-Lines alignment as defined in Section 1 is the main example application for our framework, where A={(pi,ℓi)}ni=1 is a set of point-line pairs (pi,ℓi), both on the plane, Q=Alignments is the set of all pairs (R,t) where R is a rotation matrix and t is a translation vector, D((pi,ℓi),(R,t))=minx∈ℓi∥Rpi−t−x∥ for every (R,t)∈Q and i∈[n], and where ∥⋅∥=∥⋅∥2. We provide the first constant factor approximation for the optimal solution min(R,t)∈Qcost(A,(R,t)) that takes time polynomial in n; see Theorem 9. Using existing coresets we reduce this running time to near linear in n; see Theorem 12. Such a solution can be computed for every cost function as defined in (3). We also solve this problem for the simultaneous optimization and matching scenario for f=∥⋅∥1 in time polynomial in n; See Theorem 11.

Streaming and distributed computation for the problem of aligning points-to-lines is easy by applying our algorithm on existing coresets, as suggested in Corollary 13.

Experimental Results show that our algorithms perform better also in practice, compared to both existing heuristics and provable algorithms for related problems. A system for head tracking in the context of augmented reality shows that our algorithms can be applied in real-time using sampling and coresets. Existing solutions are either too slow or provide unstable images, as is demonstrated in the companion video [3].

SECTION 2Related Work
For the easier case of summing convex functions, a similar framework was suggested in [9], [10]. However, for the case of summing non-convex functions as in this paper, each with more than one local minima, these techniques do not hold. This is why we had to use more involved algorithms in Section 4. Moreover, our generic framework such as handling outliers and matching can be applied also for the works in [9], [10].

Summing non-convex but polynomial or rational functions was suggested in [11]. This is by using tools from algebraic geometry such as semi-algebraic sets and their decompositions. While there are many techniques and solvers that can solve sets of polynomials of constant rational degree z [12], [13], [14], in this paper we show how to handle e.g., the case z=∞ (max distance to the model), and non-polynomial error functions that are robust to outliers such as M-estimators and a wide range of functions, as defined in (3). Our experimental results show that even for the case of constant z, our algorithm is much faster due to the large constants that are hidden in the O notation of those techniques compared to our running time. For high degree polynomials, the known solvers may be used to compute the minima in Theorem 3. In this sense, piecewise log-Lipschitz functions can be considered as generalizations of such functions, and our framework may be used to extend them for the generalizations in Section 1.1 (outliers, matching, etc.).

Aligning Points to Lines. The problem for aligning a set of points to a set of lines in the plane is natural e.g., in the context of GPS points [15], [16], localizing and pose refinement based on sky patterns [17] as in Fig. 3. Moreover, object tracking and localization can be solved by aligning pixels in a 2D image to an object that is pre-defined by linear segments [18], [19], as in augmented reality applications [20].

The only known solutions are for the case of sum of squared distances and d=2 dimensions, with no outliers, and when the matching between the points and lines is given. In this case, the Lagrange multipliers method can be applied in order to get a set of second order polynomials. If the matching is unknown, an Iterative Closest Point (ICP) method can be applied. The ICP is a known technique based on greedy nearest neighbours; see references in [8]. For d=3, the problem is called Perspective-n-Points (PnP), where one wishes to compute the rotation and translation of a calibrated camera given 3D points and their corresponding 2D projection, which can be interpreted as 3D lines intersecting the origin. While many solutions were suggested for this problem over the years [21], [22], [23], [24], [25], those methods either: 1) do not have provable convergence guarantees, 2) do not provide guarantees for optimality, 3) minimize some algebraic error, which is easy to solve and somehow related to the original problem, but whose solution does not prove a bounded approximation to the original problem, or 4) assume zero reprojection/fitting error. To handle outliers RANSAC [2], [7] is heuristically used.

While this natural problem was intensively studied for the special case of d=3 where the set of lines intercept the origin, to the best of our knowledge, there are no known results so far which have successfully and provably tackled this problem either in higher dimensions, in its generalized form where the lines do not necessarily intersect, with non-convex minimization functions such as M-estimators, or when the matching between the two sets is unknown.

Coresets have many different definitions. In this paper we use the simplest one that is based on a weighted subset of the input, which preserve properties such as sparsity of the input and numerical stability. We refer the reader to recent coreset surveys [26], [27], [28], [29], [30] for further details.

Coresets for ℓp regression were suggested in [31] using well-conditioned matrices that we cite in Theorem 12. Similar coresets of size dcp for ℓp regression were later suggested in [32], with a smaller constant c>1. Both [31] and [32] could have been used for coreset constructed in this paper. We reduce the points-to-lines aligning problem to constrained ℓ1 optimization in the proof of Theorem 12, which allows us to apply our algorithms on these coresets for the case of sum over point-line distances.

In most coresets papers, the main challenge is to compute the coreset. However, as we use existing coresets in this paper, the harder challenge was to extract the desired constrained solution from the coreset which approximate every possible alignment, with or without the constraints.

SECTION 3Optimization Framework
In what follows, for every pair of vectors v=(v1,…,vn) and u=(u1,…,un) in Rn we denote v≤u if vi≤ui for every i∈[n]. Similarly, f:Rn→[0,∞) is non-decreasing if f(v)≤f(u) for every v≤u∈Rd. For a set I⊆R and c∈R, we denote Ic={ac∣a∈I}.

The following definition is a generalization of Definition 2.1 in [33] from n=1 to n>1 dimensions, and from R to I⊆Rn. If a function h is a log-Lipschitz function, it roughly means that the functions slope does not change significantly if its input only changes slightly.

3Definition 1 (Log-Lipschitz function).
Let r>0 and n≥1 be an integer. Let I be a subset of Rn, and h:I→[0,∞) be a non-decreasing function. Then h(x) is r-log-Lipschitz over x∈I, if for every c≥1 and x∈I∩Ic, we have h(cx)≤crh(x). The parameter r is called the log-Lipschitz constant.

Unlike previous papers, the loss fitting (“distance”) function that we want to minimize in this paper is not a log-Lipschitz function. However, it does satisfy the following property, which implies that we can partition the range of the function g:Rd→[0,∞) into m subsets of Rd (sub-domains), such that g satisfies the log-Lipschitz condition on each of these sub-domains; see Fig. 4. That is, g has a single minimum in this sub-domain, and increases in a bounded ratio around its local minimum. Note that g might not be convex even in this sub-domain. Formally, if the distance x from the minimum in a sub-domain is multiplied by c>0, then the value of the function increases by a factor of at most cr for some small (usually constant) r>0. For example, if we double the distance from the local minimum, then the value of the function increases by at most a constant factor of 2r.


Fig. 4.
An example of a piecewise log-Lipschitz function: A function g(x)=min{2⋅|x−3|,5⋅|x−6|} (blue graph) over the set X=R. X can be partitioned into 4 subsets X1,…,X4, where each subset has a unique infimum x1=3,x2=3,x3=6 and x4=6 respectively (green stars). There exist 4 1-log-Lipschitz functions h1(x)=h2(x)=2x and h3(x)=h4(x)=5x, such that g(x)=hi(|xi−x|) for every x∈Xi.

Show All

3Definition 2 (Piecewise log-Lipschitz).
Let g:X→[0,∞) be a continuous function whose domain is X, and let (X,dist) be a metric space. Let r≥0. The function g is piecewise r-log-Lipschitz if there is a partition of X into m subsets X1,…,Xm such that for every j∈[m]:

g has a unique infimum xj in Xj, i.e., {xj}=arginfx∈Xjg(x).

hj:[0,supx∈Xjdist(x,xj)]→[0,∞) is an r-log-Lipschitz function; see Definition 1.

g(x)=hj(dist(xj,x)) for every x∈Xj.

The union of minima is denoted by M(g)={x1,…,xm}.

Description of Theorem 3. Suppose that we have a set of piecewise r-log-Lipschitz functions, and consider the union ⋃M(g) over every function g in this set. The following lemma states that, for every x∈R, this union contains a value x′ such that g(x′) approximates g(x) up to a multiplicative factor that depends on r. This theorem can be of independent interest and can be applied in a wide range of optimization problems. In this paper, we utilize this theorem to find an approximate solution for the points-to-lines alignment problem discussed above; see proof of Lemma 6.

3Theorem 3 (simultaneous approximation).
Let g1,…,gn be n functions, where gi:R→[0,∞) is a piecewise r-log-Lipschitz function for every i∈[n], and let M(gi) denote the minima of gi as in Definition 2. Let x∈R. Then there is x′∈⋃i∈[n]M(gi) such that for every i∈[n]
gi(x′)≤2rgi(x).(4)
View Source

3Proof.
Let x′∈⋃i∈[n]M(gi) be the closest item to x, i.e., that minimizes dist(x′,x) (Ties are broken arbitrarily). Put i∈[n]. We have that gi is a piecewise r-log-Lipschitz function. Identify M(gi)={x1,…,xm}, let X1,…,Xm and h1,…,hm be a partition of R and a set of functions that satisfy Properties (i)–(iii) in Definition 2 for gi. Let j∈[m] such that x∈Xj. The rest of the proof holds by the following case analysis: (i) x′∈Xj, (ii) x′∈Xj+1 and j≤m−1, and (iii) x′∈Xj−1 and j≥2. There are no other cases since if x′∈Xj−2 or x′∈Xj+2, it will imply that dist(xj−2,x)<dist(x′,x) or dist(xj+2,x)<dist(x′,x) respectively, which contradicts the definition of x′.

Case (i) x′∈Xj. We first prove that (4) holds for every y∈Xj that satisfies dist(x,y)≤dist(x,xj). If we do so, then (4) trivially holds for Case (i) by substituting y=x′ since dist(x,x′)≤dist(x,xj).

Let y∈Xj such that dist(x,y)≤dist(x,xj). Then we have
dist(xj,y)≤dist(xj,x)+dist(x,y)≤2dist(xj,x),(5)
View Sourceby the definition of y and the triangle inequality. This proves (4) as
gi(y)=hj(dist(xj,y))≤hj(2dist(xj,x))≤2rhj(dist(xj,x))=2rgi(x),
View Sourcewhere the first and last equalities hold by the definition of hj, the first inequality holds by combining (5) with the fact that hj satisfies the log-Lipschitz property and hence is a monotonic non-decreasing function, and the second inequality holds since hj is r-log-Lipschitz in Xj by Property (ii) of Definition 1.

Case (ii) x′∈Xj+1 and j≤m−1. In this case x′≤xj+1 by its definition and the fact that x∈Xj. Hence, x′∈(xj,xj+1]. Let y∈(xj,xj+1] such that gi(y)=supy′∈(xj,xj+1]gi(y′). Combining the definition of y with the assumption that gi is continuous and non-decreasing in (xj,xj+1]∩Xj, and is non-increasing in (xj,xj+1]∩Xj+1, we have that
limz→y+gi(z)=limz→y−gi(z).(6)
View SourceRight-click on figure for MathML and additional features.

Hence
gi(x′)≤limz→y+gi(z)=limz→y−gi(z)≤2rgi(x),
View SourceRight-click on figure for MathML and additional features.where the first derivation holds by combining that gi is non increasing in (xj,xj+1]∩Xj+1 and the definition of y, the second derivation is by (6). Since dist(x,z)≤dist(x,x′)≤dist(x,xj) the last derivation holds by substituting y=z in Case (i).

Case (iii) x′∈Xj−1 and j≥2. The proof for this case is symmetric to Case (ii).

Hence, in any case, there is x′∈⋃i∈[n]M(gi) such that for every i∈[n],gi(x′)≤2rgi(x).

In what follows we define a framework for approximating general sum of functions. The goal is to minimize some loss function cost(A,q) over a query set Q. This function is a function f of n pseudo distances, e.g., sum, max or sum of squares. A pseudo distance between an input point a∈A and a query q∈Q is a Lipschitz function lip of their distance D(a,q).

In the rest of the paper, for every minimization problem that is presented, we shall define such a cost function by specifying the functions D, lip and f, and show that we can minimize it up to some constant factor.

For example, in Section 4 we present the problem of aligning points to lines in 2 dimensional space, where the input set A is a set of n point-line pairs. The goal is to compute a 2D rotation matrix R∈R2×2 and a 2D translation vector t∈R2 that, when applied to the set of points, will minimize some cost function, for example, the sum of distances to the power of 3, between each point-line pair. Here, the query set Q is the set of all pairs of 2D rotation matrices and translation vectors (R,t), the function D((p,ℓ),(R,t)) is the euclidean distance between a given point-line pair (p,ℓ) after applying the transformation (R,t), i.e., D((p,ℓ),(R,t))=minq∈ℓ∥Rp−t−q∥, the function lip is lip(x)=x3, and the function f simply sums those distances over all the input point-line pairs.

3Definition 4 (Optimization framework).
Let X be a set called ground set, A={a1,…,an}⊂X be a finite input set and let Q be a set called a query set. Let D:X×Q→[0,∞) be a function. Let lip:[0,∞)→[0,∞) be an r-log-Lipschitz function and f:[0,∞)n→[0,∞) be an s-log-Lipschitz function. For every q∈Q we define
cost(A,q)=f(lip(D(a1,q)),…,lip(D(an,q))).
View Source

Description of Observation 5. The following observation states that if a query q′∈Q approximates, up to a constant factor, the function D for every input element for some other query q∗∈Q, then q′ also approximates up to a constant, the cost of a more involved function cost, relative to q∗, where cost is as defined in Definition 4.

3Observation 5.
Let
cost(A,q)=f(lip(D(a1,q)),…,lip(D(an,q))),
View Sourcebe defined as in Definition 4. Let q∗,q′∈Q and let c≥1. If D(ai,q′)≤c⋅D(ai,q∗) for every i∈[n], then
cost(A,q′)≤crs⋅cost(A,q∗).
View SourceRight-click on figure for MathML and additional features.

3Proof.
We have that
cost(A,q′)=f(lip(D(a1,q′)),…,lip(D(an,q′)))(7)
View SourceRight-click on figure for MathML and additional features.
≤f(lip(c⋅D(a1,q∗)),…,lip(c⋅D(an,q∗)))(8)
View Source
≤f(cr⋅lip(D(a1,q∗)),…,cr⋅lip(D(an,q∗)))(9)
View Source
≤crs⋅f(lip(D(a1,q∗)),…,lip(D(an,q∗)))(10)
View Source
=crs⋅cost(A,q∗),(11)
View Sourcewhere (7) holds by the definition of cost, (8) holds by the assumption of Observation 5 which imply that both lip and f are non-decreasing functions (since they satisfy the log-Lipschitz property), (9) holds since lip is r-log-Lipschitz, (10) holds since f is s-log-Lipschitz, and (11) holds by the definition of cost.

Observation 5 implies that in order to approximate a non-trivial cost function as in Definition 4, it suffices to approximate a much simpler function, namely the function D, for every input element.

SECTION 4Algorithms for Aligning Points to Lines
In this section, we introduce our notations, describe our algorithms, and give both an overview and intuition for each algorithm. See Table 1 for example cost functions that our algorithms provably approximate. See Sections 4.1.2, 4.2.2 and 4.3.2 for an intuition of the algorithms presented in this section.

TABLE 1 Main Results of This Paper for the Problem of Aligning Points-to-Lines
Table 1- 
Main Results of This Paper for the Problem of Aligning Points-to-Lines
Notation for the Rest of the Paper. Let Rn×d be the set of all n×d real matrices. We denote by ∥p∥=∥p∥2=p21+⋯+p2d−−−−−−−−−−√ the length of a point p=(p1,…,pd)∈Rd, by dist(p,ℓ)=minx∈ℓ∥p−x∥2 the euclidean distance from p to a line ℓ in Rd, by proj(p,X) its projection on a set X, i.e, proj(p,X)∈arginfx∈Xdist(p,x), and by sp{p}={kp∣k∈R} we denote the linear span of p. For a matrix V∈Rd×m whose columns are mutually orthonormal, i.e., VTV=I, we denote by V⊥∈Rd×(d−m) an arbitrary matrix whose columns are mutually orthogonal unit vectors, and also orthogonal to every vector in V. Hence, [V∣V⊥] is an orthogonal matrix. If p∈R2, we define p⊥=(p⊥1,p⊥2)∈R2 to be the point such hat pTp⊥=0 and p⊥1≥0. We denote [n]={1,…,n} for every integer n≥1.

For the rest of this paper, every vector is a column vector, unless stated otherwise. A matrix R∈R2×2 is called a rotation matrix if it is orthogonal and its determinant is 1, i.e., RTR=I and det(R)=1. For t∈R2 that is called a translation vector, the pair (R,t) is called an alignment. We define Alignments to be the union of all possible alignments in 2-dimensional space. The x-axis and y-axis are defined as the sets {(z,0)∣z∈R} and {(0,z)∣z∈R} respectively.

For a bijection (permutation) function π:[n]→[n] and a set A={(a1,b1),…,(an,bn)} of n pairs of elements, Aπ is defined as Aπ={(a1,bπ(1)),…,(an,bπ(n))}.

We use the original definition of O(⋅) as a set of functions and write e.g., t∈O(n) and not t=O(n) to avoid dis-ambiguity as in 1+1=O(1)=3; see discussion in [34].

SECTION Algorithm 1.Z−Configs(v,p,q,z)
Input: A unit vector v=(vx,vy)T such that

vy≠0, and p,q,z∈R2 such that p≠q.

Output: A tuple of matrices P,Q,Z∈R2×2

that satisfy Lemma 7.

Set r1←∥p−q∥,r2←∥p−z∥,r3←∥q−z∥

Set d1←r21+r22−r232r1; d2←|r22−d21|−−−−−−−√

Set R←(01−10) // A rotation matrix that rotates the coordinates system by π/2 radians counter clockwise around the origin.

Set P←r1(vxvy010)

Set Q←PT

Set b←{1,0,(z−p)T(q−p)⊥>0otherwise

// b=1 if z is in the halfplane to the right of the vector q−p, and b=0 otherwise.

Set Z←P+d1r1(Q−P)+b⋅d2r1R(Q−P)

return (P,Q,Z)

Algorithms. We now present algorithms that compute a constant factor approximation for the problem of aligning points to lines, when the matching is either known or unknown. Algorithm 2 handles the case where the matching between the points and lines is given, i.e., given an ordered set P={p1,…,pn} of n points, and a corresponding ordered set L={ℓ1,…,ℓn} of n lines, both in R2, we wish to find an alignment that minimizes, for example, the sum of distances between each point in P and its corresponding line in L.

SECTION Algorithm 2.Align (A)
Input: A set A={(p1,ℓ1),…,(pn,ℓn)} of n pairs, where for every i∈[n], we have that pi is a point and ℓi is a line, both on the plane.

Output: A set C⊂Alignments of alignments that satisfies Lemma 8.

Set C←∅.

Set vi∈R2 and bi≥0 such that ℓi={q∈R2∣vTiq=bi} for every i∈[n].

for every j,k,l∈[n] such that j≠k do

Set C1,C2←∅.

if |vTjvk|≠1 then

/* ℓj and ℓk are not parallel. */

Set v⊥j← a unit vector in R2 that is orthogonal to vj.

Set Rvj←(−vTj−−v⊥jT−).

// Rvj aligns vj with the x-axis.

Set (P′,Q′,Z′)←Z−Configs(Rvjvk,pj,pk,pl).

// See Algorithm 1

Set P←RTvjP′,Q←RTvjQ′,Z←RTvjZ′.

Set s←ℓj∩ℓk.

// ℓj∩ℓk contains one point since ℓj and ℓk are not parallel.

Set cl←dist(s,ℓl).

Set X←argminx∈R2:∥x∥=1|vTlZx−cl|. // The set of unit vectors that minimize the distance between pl and ℓl while maintaining pj∈ℓj and pk∈ℓk.

Set C1←{(R,t)∈Alignments∣Rpj−t=Px and Rpk−t=Qx and Rpl−t=Zx for every x∈X}. / * The set of alignments that align the points (pj,pk,pl) with points (Px,Qx,Zx) for every x∈X. */

else

Set C2←{(R,t)} such that (R,t)∈Alignments, Rpj−t∈ℓj and (R,t)∈argmindist(Rpk−t,ℓk).

Set C←C∪C1∪C2.

return C

SECTION Algorithm 3.Align+Match(A,cost)
Input: A set A={(p1,ℓ1),…,(pn,ℓn)} and a cost function as in Theorem 11.

Output: An element (R~,t~,π~) that satisfies Theorem 11.

Set X←∅.

for every i1,i2,i3,j1,j2,j3∈[n] do

Set X′←Align({(pi1,ℓj1),(pi2,ℓj2),(pi3,ℓj3)}) .

// See Algorithm 2

Set X←X∪X′.

Set S←{(R,t,π^(A,(R,t),cost))∣(R,t)∈X}.

/* see Definition 10 and Section 4.3.2 for details.*/

Set (R~,t~,π~)∈argmin(R′,t′,π′)∈Scost(Aπ′,(R′,t′)).

return (R~,t~,π~)

Formally, let A={(pi,ℓi)}ni=1 be a set of n≥3 point-line pairs, z>0, and Dz:A×Alignments→[0,∞) such that Dz((p,ℓ),(R,t))=minq∈ℓ∥Rp−t−q∥z is the ℓz distance between Rp−t and ℓ for every (p,ℓ)∈A and (R,t)∈Alignments. Let cost,s,r be as defined in Definition 4 for D=Dz. Then Algorithm 2 outputs a set of alignments that is guaranteed to contain an alignment which approximates min(R,t)∈Alignmentscost(A,(R,t)) up to a constant factor that depends on s,r and z; See Theorem 9.

Algorithm 3 handles the case when the matching is unknown, i.e., given unordered sets P and L consisting of n points and n lines respectively, we wish to find a matching function π:[n]→[n] and an alignment (R,t) that minimize, for example, the sum of distances between each point pi∈P and its corresponding line ℓπ(i)∈L.

Formally, let cost be as defined above but with f=∥⋅∥1. Then Algorithm 3 outputs a set of alignments that is guaranteed to contain an alignment which approximates min(R,t,π)cost(Aπ,(R,t)) up to a constant factor, where the minimum is over every alignment (R,t) and matching function π; See Theorem 11.

4.1 Algorithm 1: Z-Configs
In this section we present a sub-routine called Z-Configs that is called from our main algorithm; see Algorithm 1 and our main algorithm in Algorithm 2.

4.1.1 Overview of Algorithm 1
The algorithm takes as input a triangle, and a line ℓ that intersects the origin. The triangle is defined by its three vertices p,q,z∈R2 and denoted by Δ(p,q,z). The line is defined by its direction (unit vector) v.

The usage of this algorithm in the main algorithm (Algorithm 2) is to compute the union over every feasible configuration Δ(p′,q′,z′), which is a rotation and a translation of Δ(p,q,z) such that p′ is on the x-axis, and q′ is on the input line (simultaneously).

To this end, the output of the Algorithm 1 is a tuple of three 2×2 matrices P,Q and Z such that the union of (Px,Qx,Zx) over every unit vector x is the desired set. That is, for every feasible configuration Δ(p′,q′,z′) there is a unit vector x∈R2 such that (p′,q′,z′)=(Px,Qx,Zx), and vice versa. See illustration in Fig. 5 and further details in Lemma 7 and the description above it.

Fig. 5. - 
Illustration for Algorithm 1. A triangle $\Delta (p,q,z)$Δ(p,q,z) whose vertex $p$p intersects the $x$x-axis, and its vertex $q$q is on the line $\mathrm{sp}\left\lbrace v\right\rbrace$ sp v. There are infinitely many such triangles, and the union over every possible solution $z$z are the points on the green ellipse. If $(P,Q,Z)$(P,Q,Z) is the output of Algorithm 1, then $(p,q,z)=(Px,Qx,Zx)$(p,q,z)=(Px,Qx,Zx) for some unit vector $x$x.
Fig. 5.
Illustration for Algorithm 1. A triangle Δ(p,q,z) whose vertex p intersects the x-axis, and its vertex q is on the line sp{v}. There are infinitely many such triangles, and the union over every possible solution z are the points on the green ellipse. If (P,Q,Z) is the output of Algorithm 1, then (p,q,z)=(Px,Qx,Zx) for some unit vector x.

Show All

4.1.2 Intuition Behind Algorithm 1
For every matrix Z∈R2×2, the set {Zx∣x∈R2,∥x∥=1} defines the boundary of an ellipse in R2. Hence, the shape formed by all possible locations of vertex z in R2, assuming p is on the x-axis and q∈ℓ, is an ellipse. Furthermore, this ellipse is centered around the intersection point of the x-axis and ℓ (the origin, in this case). See Fig. 5 and the description above Lemma 7.

4.2 Algorithm 2: Align
In this section we present our main algorithm, called Align; See Algorithm 2.

4.2.1 Overview of Algorithm 2
The input for Algorithm 2 is a set of n pairs, each consists of a point and a line on the plane. The algorithm runs exhaustive search on all the possible tuples and outputs a set C of O(n3) alignments, each consisting of a rotation matrix R and a translation vector t. Theorem 8 proves that one of these alignments yields the desired approximation.

Line 2 identifies each line ℓi by its direction (unit vector) vi and distance bi>0 from the origin. Lines 3–16 iterates over every triple (j,k,l) of input pairs such that j≠k, and turns it into a constant number of alignments C1. In Lines 6–13 we handle the case where the lines ℓj and ℓk are not parallel. In Line 15 we handle the case where ℓj and ℓk are parallel.

The Case Where ℓj and ℓk are not Parallel. Lines 6–7 compute a rotation matrix Rvj that rotates vj to the x-axis. Line 8 calls the sub-procedure Algorithm 1 for computing three matrices P′,Q′ and Z′. In Line 9 we revert the effect of the rotation matrix Rvj. Lines 10–11 compute the distance between ℓl and the intersection between ℓj and ℓk since we assumed this intersection point is the origin in Algorithm 1. The matrix Z and the line ℓl are used to compute a set X of O(1) unit vectors in Line 12. Every x∈X defines a possible positioning for the triplet. In Line 13 we define an alignment (R,t) for each x∈X. The union of the alignments in C1 is then added to the output set C in Line 16.

The Case Where ℓj and ℓk are Parallel. In this case, we place pj∈ℓj, and place pk as close as possible to ℓk. If there are more than one alignment that satisfies these conditions, then we pick an arbitrary one. This is done in Line 15.

4.2.2 Intuition Behind Algorithm 2
The idea behind the algorithm consists of three steps. At each step we reduce the set of feasible alignments by adding another constraint. Each constraint typically increases our approximation factor by another constant.

Consider any alignment of the input points to lines, and suppose that (p,ℓ1) is the closest pair after applying this alignment. By the triangle inequality, translating the set P of points so that p intersects ℓ1 will increase the distance between every other pair by a factor of at most 2. Hence, minimizing (1) under the constraint that p∈ℓ1 would yield a 2-approximation to the original (non-constrained) problem.

Similarly, we can then translate P in the direction of ℓ1 (while maintaining p∈ℓ1) until the closest pair, say (q,ℓ2), intersects. The result is a 4-approximation to the initial alignment by considering all the possible alignments of P such that p∈ℓ1 and q∈ℓ2. There are still infinite such alignments which satisfy the last constraint.

Hence, we add a third step. Let (z,ℓ3) be the pair that requires the minimal rotation of the vector q−p in order to minimize dist(z,ℓ3) under the constraints that p∈ℓ1 and q∈ℓ2. We now rotate the vector q−p and translate the system to maintain the previous constraints until dist(z,ℓ3) is minimized.

The result is a 16-approximation to (1) by considering all the possible alignments of P such that: z is closest to ℓ3 among all alignments that satisfy p∈ℓ1 and q∈ℓ2. Unlike the previous steps, there are only a finite number of such alignments, namely |C|∈O(n3).

4.3 Algorithm 3: Alignment and Matching
4.3.1 Overview of Algorithm 3
Algorithm 3 takes as input a set of n paired points and lines in R2, and a cost function as defined in Theorem 11. The algorithm computes an alignment (R^,t^)∈Alignments and a matching function π^ (which rearranges the given pairing of the pairs) that approximate the minimal value of the given cost function; See Theorem 11.

In Line 2 we iterate over every i1,i2,i3,j1,j2,j3∈[n]. In Lines 3- 4 we match pi1 to ℓj1,pi2 to ℓj2 and pi3 to ℓj3, compute their corresponding set of alignments X′ by a call to Algorithm 2, and then add X′ to the set X. Finally, in Lines 5- 6 we compute the optimal matching for every alignment in X, and pick the alignment and corresponding matching that minimize the given cost function.

4.3.2 Intuition Behind Algorithm 3
While solving for both the optimal alignment and optimal matching is a difficult task, solving only for the optimal matching function given the optimal (or near optimal) alignment is a much simpler task; If this optimal alignment is given, i.e., the two sets of points are now static, then one can compute the optimal matching between them using standard matching techniques. For example, given a set P of n points and a set L of n lines, one can construct an n×n matrix whose (i,j)th entry contains the distance between a point pi∈P and a line ℓj∈L according to the given distance function. Now, one can apply e.g., the Hungarian method [35] to solve for the optimal matching between P and L in O(n3) time.

To this end, Algorithm 3 aims to decouple the alignment problem and the matching problem by first constructing a set of alignments, which is guaranteed to contain a near optimal alignment. Then, it simply computes the optimal matching, as described above, for each of those (potentially almost optimal) alignments. Finally, it outputs the alignment and corresponding matching function which yield the smallest cost. The set of candidate alignments is constructed by iterating over every triplet of points and triplet of lines from the input set A. Each such tuple of 3 points and 3 lines define a set of O(1) alignments using Algorithm 2.

SECTION 5Statements of Main Results
Description of Lemma 6. The following lemma is one of the main technical results of this paper, and lies in the heart of the proof of Lemma 8. It proves that for every two paired sets a1,…,an⊆R2 and b1,…,bn≥0 and unit vector x∈R2, there exists x′∈argmin∥y∥=1|aTky−bk| for some k∈[n] that approximates |aTix−bi| for every i∈[n].

5Lemma 6.
Let a1,…,an⊆R2 and b1,…,bn≥0. Then there is a set C of |C|∈O(n) unit vectors that can be computed in O(n) time such that (i) and (ii) hold as follows:

For every unit vector x∈R2 there is a vector x′∈C such that for every i∈[n]
|aTix′−bi|≤4⋅|aTix−bi|.(12)
View Source

There is k∈[n] such that x′∈argmin∥y∥=1|aTky−bk|

5Proof.
See proof of Lemma 15 in the appendix, which can be found on the Computer Society Digital Library at http://doi.ieeecomputersociety.org.ezproxy.auckland.ac.nz/10.1109/TKDE.2020.2980836.

Lemma 6 suggests a result of independent interest. Consider the constrained linear regression problem of computing a unit vector x∗∈R2 that minimizes
x∗∈argminx∈R2:∥x∥=1∥Ax−b∥1=argminx∈R2:∥x∥=1∑i=1n|aTix−bi|,
View SourceRight-click on figure for MathML and additional features.for a given matrix A=(a1∣…∣an)T∈Rn×2 and a given vector b=(b1,…,bn)∈R2. Lemma 6 implies that in linear time, we can compute a candidate set of unit vectors C, which contains a unit vector x′ that approximates each of the n “distances” |aTix∗−bi| up to a multiplicative factor of 4. Hence, x′ also approximates the optimal (unknown) value of the constrained ℓ1 linear regression ∥Ax∗−b∥1, up to a multiplicative factor of 4. It is easy to see that a similar argument holds for the constrained ℓz regression ∥Ax∗−b∥z, for z>0, up to a constant factor that depends only on z.

Description of Lemma 7. The following lemma proves the correctness of Algorithm 1. It proves that the matrices P,Q and Z computed in Algorithm 1 satisfy a set of properties, which will be useful in the proof of Lemma 8. Given a triangle Δ(p,q,z) and a line ℓ intersecting the origin, the first property states that any feasible alignment of the vertices p and q such that p lies on the x-axis and q lies on ℓ can be represented as p=Px and q=Qx respectively, for some unit vector x. In other words, every feasible position of the triangle vertices can be represented by the output matrices and some unit vector x. The second property simply states that if the first two vertices are located at the (feasible) locations p=Px and q=Qx, then the third vertex is positioned at z=Zx. However the opposite direction of the second property does not necessarily hold.

5Lemma 7.
Let v be a unit vector and ℓ=sp{v} be the line in this direction. Let p,q,z∈R2 be the vertices of a triangle such that ∥p−q∥>0. Let P,Q,Z∈R2×2 be the output of a call to Z-Configs(v,p,q,z); see Algorithm 1. Then the following hold:

p∈x-axis and q∈ℓ iff there is a unit vector x∈R2 such that p=Px and q=Qx.

For every unit vector x∈R2, we have that z=Zx if p=Px and q=Qx.

5Proof.
See proof of Lemma 18 in the appendix, available in the online supplemental material.

5.1 Aligning Points-to-Lines
Table 1 summarizes the main results and cost functions in the context of aligning points-to-lines, which our algorithms support.

Description of Lemma 8. Lemma 8 is the main technical lemma for the correctness of Algorithm 2. It proves that for every (possibly optimal) alignment (R∗,t∗), the output set C of Algorithm 2 must contain an alignment (R′,t′) that approximates each of the distances dist(R∗pi−t∗,ℓi) for every i∈[n], up to some multiplicative constant factor. Furthermore, the set C is computed in time polynomial in n.

Lemma 8.
Let A={(p1,ℓ1),…,(pn,ℓn)} be set of n≥3 pairs, where for every i∈[n], we have that pi is a point and ℓi is a line, both on the plane. Let C⊆Alignments be an output of a call to Align(A); see Algorithm 2. Then for every alignment (R∗,t∗)∈Alignments there exists an alignment (R,t)∈C such that for every i∈[n]
dist(Rpi−t,ℓi)≤16⋅dist(R∗pi−t∗,ℓi).(13)
View SourceRight-click on figure for MathML and additional features.Moreover, |C|∈O(n3) and can be computed in O(n3) time.

Proof.
See proof of Lemma 20 in the appendix, available in the online supplemental material.

5.1.1 Generalization
Observation 5 states that in order to approximate a complicated and non-convex cost function cost as in Definition 4, all needs to be done is to compute a query (alignment) that approximates the simpler (distance) function D=dist that lies in the heart of the function cost. Lemma 8 implies that in polynomial time we can indeed compute a query (alignment) that approximates the distance function dist. Combining Observation 5 with Lemma 8 yields that in polynomial time we can compute a candidate set of queries (alignments) which contain a query that approximates, up to a constant factor, a complex cost function, as follows.

Theorem 9.
Let A={(p1,ℓ1),…,(pn,ℓn)} be set of n≥3 pairs, where for every i∈[n], we have that pi is a point and ℓi is a line, both on the plane. Let z>0, w=2∣∣1z−12∣∣, and let Dz:A×Alignments→[0,∞) such that Dz((p,ℓ),(R,t))=minq∈ℓ∥Rp−t−q∥z is the ℓz distance between Rp−t and ℓ. Let cost,s,r be as defined in Definition 4 for D=Dz. Let C be the output of a call to Align(A); see Algorithm 2. Then there exists (R′,t′)∈C such that
cost(A,(R′,t′))≤(w⋅16)rs⋅min(R,t)∈Alignmentscost(A,(R,t)).
View SourceFurthermore, (R′,t′) can be computed in O(n4) time.

Proof.
See proof of Theorem 24 in the appendix, available in the online supplemental material.

Since C is guaranteed to contain an alignment (R′,t′) that approximates the optimal value of cost, up to some multiplicative constant factor, simple exhaustive search in O(n4) time on C can find an alignment (R^,t^)∈C with cost smaller or equal to the cost of (R′,t′).

5.1.2 Unknown Matching
Given an input set of n unmatched pairs A=(p1,ℓ1),…,(pn,ℓn), an alignment (query) (R,t) and some cost function, we seek a matching function π, such that after rearranging the input pairs using π, the rearranged (rematched) set Aπ=(p1,ℓπ(1)),…,(pn,ℓπ(n)) minimizes the given cost function for the given query (R,t). A formal definition is given as follows.

Definition 10 (Optimal matching).
Let n≥1 be an integer and Perms(n) denote the set of all permutation (bijection) functions π:[n]→[n]. Let A={a1,…,an} be an input set, where ai=(pi,ℓi) is a pair of elements for every i∈[n], and let Q be a set of queries. Consider a function cost as defined in Definition 4 for f(v)=∥v∥1. Let q∈Q. A permutation π^ is called an optimal matching for (A,q,cost) if it satisfies that
π^(A,q,cost)∈argminπ∈Perms(n)cost(Aπ,q).
View Source

Description of Theorem 11. Theorem 11 proves that in polynomial time, Algorithm 3 outputs an alignment and a matching function that approximate, up to a constant factor, the optimal alignment and matching function for a given cost function.

Recall that for π:[n]→[n] and a set A={(p1,ℓ1),…,(pn,ℓn)} of n pairs of elements, Aπ is defined as Aπ={(p1,ℓπ(1)),…,(pn,ℓπ(n))}.

Theorem 11.
Let A={(p1,ℓ1),…,(pn,ℓn)} be a set of n≥3 pairs, where for every i∈[n] we have that pi is a point and ℓi is a line, both on the plane. Let z>0, w=2∣∣1z−12∣∣, and define
Dz((p,ℓ),(R,t))=minq∈ℓ∥Rp−t−q∥z,
View Sourcefor every point p and line ℓ on the plane and alignment (R,t). Consider cost and r to be as defined in Definition 4 for D=Dz and f(v)=∥v∥1. Let (R~,t~,π~) be the output alignment (R~,t~) and permutation π~ of a call to Align+Match(A,cost); see Algorithm 3. Then
cost(Aπ~,(R~,t~))≤(w⋅16)r⋅min(R,t,π)cost(Aπ,(R,t)),(14)
View SourceRight-click on figure for MathML and additional features.where the minimum is over every alignment (R,t) and permutation π:[n]→[n]. Moreover, (R~,t~,π~) can be computed in O(n9) time.

Proof.
See proof of Theorem 25 in the appendix, available in the online supplemental material.

Observe that the function f in Theorem 11 is restricted to f(v)=∥v∥1. As we show in the proof of Theorem 11, computing the matching (bijection) function reduces to solving an optimal matching problem (multiple times). Handing a wider range of functions f(v), such as f(v)=∥v∥1,m which sums only the smallest m≤n values in v (see Section 1.1) requires solving a hard variant of the optimal matching problem, where we need to match only an (unknown) subset of m elements and ignore n−m elements. We do not know of a polynomial time algorithm that solves the optimal matching problem that arises for the most general f.

5.2 Coresets for Big Data
Description of Theorem 12. Theorem 12 presents a data reduction technique, called coreset, for the points-to-lines alignment problem, based on [31]. This reduction will enable a near-linear implementation of our algorithm by running it on this small coreset while obtaining an almost similar approximation.

Theorem 12 (coreset for points-to-lines alignment).
Let d≥2 be an integer. Let A={(p1,ℓ1),…,(pn,ℓn)} be set of n pairs, where for every i∈[n], pi is a point and ℓi is a line, both in Rd, and let w=(w1,…,wn)∈[0,∞)n. Let ε,δ∈(0,1). Then in O(nd11log(nd)) time we can compute a weights vector u=(u1,…,un)∈[0,∞)n that satisfies the following pair of properties.

With probability at least 1−δ, for every (R,t)∈Alignments it holds that
(1−ε)⋅∑i∈[n]wi⋅dist(Rpi−t,ℓi)≤∑i∈[n]ui⋅dist(Rpi−t,ℓi)≤(1+ε)⋅∑i∈[n]wi⋅dist(Rpi−t,ℓi).
View SourceRight-click on figure for MathML and additional features.

The weights vector u has O(d9.5logdε2log1δ) non-zero entries.

Proof.
See proof of Theorem 31 in the appendix, available in the online supplemental material.

Instead of running the approximation algorithms proposed in Section 4 on the entire input data, Theorem 12 implies that we can first compress the input 2D point-line pairs in O(nlogn) time, and then apply the approximation algorithms only on the compressed data, which will take time independent of n. Hence, the total running time will be dominated by the coreset computation time. However, the approximation factor in this case will increase by an additional multiplicative factor of (1+ϵ).

Description of Corollary 13. The following corollary gives the main results for handling streaming, distributed, and dynamic data, which is made possible using the coreset presented above.

Corollary 13 (streaming, distributed, dynamic data).
Let A={(p1,ℓ1),(p2,ℓ2,),…} be a (possibly infinite) stream of pairs, where for every i∈[n], pi is a point and ℓi is a line, both in the plane. Let δ∈(0,1). Then, for every integer n>1 we can compute with probability at least 1−δ an alignment (R∗,t∗) that satisfies
∑i=1ndist(R∗pi−t∗,ℓi)∈O(1)⋅min(R,t)∑i=1ndist(Rpi−t,ℓi),
View Sourcefor the n points seen so far in the stream, using log(n/δ)O(1) memory and update time per a new pair. Using M machines the average update time per point can be reduced by an order of M.

Proof.
See proof of Corollary 32 in the appendix, available in the online supplemental material.

In the previous corollary, we assume that the main server partitions the original stream of n points into consecutive subsets of size M. Each of the M points in this subset is sent to a different machine in parallel. Hence, the average insertion time per point over a batch of M (or n>M) points is reduced by order of M.

SECTION 6Conclusion and Open Problems
We described a general framework for approximating functions under different constraints, and used it for obtaining generic algorithms for minimizing a finite set of piecewise log-Lipschitz functions. We apply this framework to the points-to-lines alignment problem. Coresets for these problems enabled us to turn our polynomial time algorithms in some cases into near linear time, and support streaming and distributed versions for Big Data.

Open problems include generalization of our results to higher dimensions, as some of our suggested coresets. We generalize our algorithms for the case when no matching permutation π is given between the points and lines. However, the results are less general than our results for the known matching case, and we do not have coresets for these cases, which we leave for future research.