Multi-view semi-supervised learning (SSL) has attracted great attention due to its effectiveness in information utilization of multiple views and labeled and unlabeled data to solve practical problems. However, most existing methods exhibit high computational complexity. Effective integration of the information on different views to achieve enhanced performance remains a challenging task. In this study, we combine an anchor-based approach with multi-view semi-supervised learning to address these problems. A novel multi-view SSL method called fast multi-view SSL (FMSSL) based on learned graph is proposed. Starting from the affinity graphs constructed by using an anchor-based strategy, FMSSL learns an optimal multi-view consensus graph by using feature and label information. The learned graph can jointly consider the relation of multiple views to approximate the manifold structure. The learned graph is then introduced into the SSL model as the weight matrix of a bipartite graph to simultaneously perform separate classification on the original samples and anchors. Accordingly, multi-view SSL can be efficiently performed, and the computational complexity can be significantly reduced. We propose an effective algorithm to optimize the objective function. Extensive experimental results on different real-world datasets demonstrate the effectiveness and efficiency of the proposed algorithm.
SECTION 1Introduction
In multi-view data, each sample is collected from different measurement methods or represented with diverse features [1]. For instance, images and videos can be characterized by color and texture features, and human genes can be measured by various techniques, such as gene expression, methylation and single-nucleotide polymorphism. Different views describe specific aspects of independent or complementary heterogeneous feature spaces. Many studies have shown that if the information contained in multiple views is properly integrated, then the final performance of many learning tasks, such as dimensionality reduction, classification and clustering, can be significantly improved [2], [3], [4], [5], [6].

With the vigorous development of data mining and expansion of cyberspace, the scarcity of labeled data and the enrichment of unlabeled data are characteristics of multi-view learning [7], [8], [9]. In general, labeling samples is time-consuming and laborious. Although labeling sufficient samples for any specific purpose is possible, this approach is not always practical and realistic. Semi-supervised learning (SSL) can learn useful information from a small amount of labeled data and a large amount of unlabeled data. The study of multi-view SSL is very meaningful in real-world scenarios. Therefore, we focus on multi-view SSL.

Graph-based method [10] is a widely used method in SSL. In a graph, nodes encompass labeled and unlabeled samples, and edges reflect the similarity between the corresponding sample pairs [11], [12]. Given a dataset that is partially labeled, unlabeled data can be labeled by learning the similarity between pairs in the graph. Many state-of-the-art semi-supervised algorithms based on graphs are available [10], [13], [14]. Argyriou et al. [15] proposed a method that optimally combines differently constructed graphs for SSL. In graph-based multi-view learning, many applications, such as image classification [4], [5], [16], gene expression data classification [17], and web page classification [18], in pattern recognition have achieved good performance. Constructing an informative and accurate affinity graph plays an important role in the success of a graph-based method. Therefore, this study aims to derive an effective method of graph construction in multi-view SSL.

Many existing graph-based multi-view SSL methods use kernel-based neighbor assignment strategy [17], [18], [19], [20]. For instance, Gaussian kernel brings a bandwidth parameter that needs to be tuned for enhanced classification performance. The tuning of the bandwidth is empirical [21] but time-consuming. Nie et al. [22] introduced a parameter-free but effective neighbor assignment strategy to avoid this issue. In traditional graph-based methods, k-nearest neighbors (K-NN) is always adopted to construct an almost full-rank similarity graph. However, constructing a normal K-NN graph of n vertexes takes O(n2d) [19], [20], [23] at least, where n is the number of samples, and d is the feature dimensionality. This condition is unbearable for the dataset that contains hundreds of thousands of samples. Such an approach makes graph construction and subsequent analysis time-consuming. An efficient framework is needed for multi-view SSL as the amount of data increases. Anchor-based graph (AG) has been adopted in graph-based learning, such as anchor graph regularization [24], efficient anchor graph regularization [25], and hierarchical anchor graph regularization [26] in SSL, unsupervised large graph embedding [27], and multi-view large-scale spectral clustering [28], to reduce high computational cost and achieve enhanced performance. The number of anchors m is significantly smaller than the number of original samples n, and the construction of an affinity graph with original data and anchors needs to consider O(mn) distances. This finding indicates that the graph construction and subsequent learning procedure are faster than the traditional graph-based algorithms. The number of anchors should depend on the size of the original samples. The anchors should not be particularly sparse; otherwise, obtaining an effective adjacent graph for accurate performance would be difficult. Accordingly, the computational cost will increase with the increase of data size. In this study, we focus on fast multi-view SSL (FMSSL) with AG.

In graph-based multi-view learning, constructing a multi-view consensus graph that is more accurate and informative than any single one for subsequent processing is also challenging. Expressing the relationship between all features and achieving consistency on different views are important problems. Many approaches that can reduce the inconsistency of multiple graphs are found in the literature. Two commonly used approaches are optimal linear combination and co-regularization. For instance, similarity matrices of different views are merged with an optimal linear combination in multi-view semi-supervised feature selection [29] and classification [18], [19], [20], [30], [31], [32]. Pairwise differences between view-specific spectral learnings are penalized with co-regularization in multi-view face recognition [33] and multimedia retrieval [34]. Recently, Sun et al. [6] proposed some PAC-Bayes bounds for co-regularization style methods for both supervised and semi-supervised multi-view learning. These approaches show different strengths and weaknesses in their applications. However, most of these graphs are constructed with feature information only. The independent and complementary relationships among multiple features and label information learned in the learning process are meaningful for the construction of multi-view consensus graphs. Constructing an optimal multi-view consensus graph on the combination of feature and label information is necessary.

Existing multi-view SSL approaches rarely consider noisy samples. These approaches deal with noisy samples by training the raw data. In our SSL model, a soft error term is introduced to release the limitation that the predicted labels are strictly equal to the given hard labels.

In this study, we combine an anchor-based approach with multi-view semi-supervised learning and propose a new multi-view SSL approach called FMSSL with learned graph to address the aforementioned problems. Starting with affinity graphs constructed by using an anchor-based strategy, FMSSL constructs an optimal multi-view consensus graph that can accurately encode the relationships between data samples by combining feature and label information. Subsequently, FMSSL propagates class labels from labeled samples to unlabeled ones by incorporating the learned multi-view consensus graph as the weight matrix of a bipartite graph with the SSL model. The proposed FMSSL greatly benefits from the AG and optimal consensus multi-view graph. The graph construction and learning process becomes more efficient than traditional algorithms because the number of anchors can be much smaller than that of the original samples.

The main contributions of our work are summarized as follows:

We combine an anchor-based strategy with multi-view data to improve the efficiency of multi-view semi-supervised learning. It studies an advanced multi-view semi-supervised paradigm and provides a new semi-supervised solution for multi-view data.

The proposed algorithm focuses not only on the proper integration of multiple features but also on the label information that changes in the learning process. FMSSL automatically weighs each view and constructs an optimal multi-view consensus graph that can accurately encode the relationships between samples to approximate the manifold structure by using feature and label information.

This algorithm comes up with the idea of joint classification of the original multi-view samples and anchors, which points to a duality between the original samples and anchors. The learned consensus graph is introduced into the SSL model as the weight matrix of a bipartite graph. Exploring the special structure of the bipartite graph makes the classification highly efficient.

Comprehensive experiments are conducted on different real-world datasets to demonstrate the effectiveness and efficiency of the proposed algorithm. Accordingly, the algorithm not only obtains satisfactory classification accuracy but also saves a large amount of time.

The rest of this paper is organized as follows. In Section 2, we provide an overview of the background. The proposed approach FMSSL is described in Section 3. In Section 4, an efficient algorithm is presented to optimize this approach. We perform experiments compared with other state-of-the-art methods on several real-world datasets in Section 5. The conclusions are provided in Section 6.

SECTION 2Background
In this section, we introduce related graph-based methods. We first introduce several basic notations and definitions used in this study. Matrices and vectors are written in uppercase and lowercase boldface letters, respectively. For matrix M∈Rn×d, let mi and mij denote the ith row and (ij)th element of M, respectively. An identity matrix is denoted by I. The transpose of matrix M is MT. The trace of matrix M is Tr(M). The inverse of matrix M is M−1. We define ∥v∥2 as the ℓ2-norm of v and ∥M∥F as Frobenius norm of M. 1 is a column vector with all the elements as one. 0 is a matrix with all the elements as zero.

2.1 Graph Construction With Anchor-Based Strategy
Many existing graph-based learning methods are challenging because of their cubic computational complexity. The AG has been widely adopted to speed up the procedure of graph-based methods and has achieved good performance [24], [25], [26], [27], [28]. The core idea is to use a small set of representative samples to capture the manifold structure.

The graph construction with anchor-based strategy consists of two steps. The first step involves seeking m anchors from n original samples, where m≪n. The second involves constructing an affinity graph B∈Rn×m(bij≥0) that measures the adjacency between the anchors and original samples. Generating anchors is crucial to ultimate performance. In multi-view learning, if we independently select anchors on each view, different views will generate different anchors, thereby making feature fusion unreasonable. Therefore, we concatenate all features to generate anchors and then separate the result points into different views. In this way, uniform anchors can be generated for different views, to simplify the classification process.

The two commonly used methods of anchor generation are random selection and k-means generation [27], [35]. The random selection method randomly sampled m samples from n original samples as anchors. This method has strong randomness and cannot guarantee that the selected m anchors are suitable or the result is steady. The k-means generation method clusters n original samples into m clusters and selects the m clustering centers as anchors. This method can generate representative anchors but has high computational complexity. Two simple strategies, namely, stopping the iteration early [35] and performing downsampling, can be adopted to speed up the procedure of k-means generation. Both strategies cannot guarantee the quality of the generated anchors.

In this study, we use an efficient algorithm balanced k-means-based hierarchical k-means (BKHK)) [36] to generate representative anchors. The BKHK designs a balanced k-means algorithm that can separate the data into two clusters with the same number of samples and hierarchically performs the balanced k-means algorithm on the data to obtain the representative anchors. The computational complexity of this algorithm is O(ndlog(m)), which has great advantage compared with the k-means method with computational complexity of O(ndm).

For a multi-view dataset, suppose that we have V features denoted as X(1),…,X(V) and U(1),…,U(V) as the original samples and generated anchors of each feature, respectively. X(v)=[x(v)1;x(v)2;…;x(v)n]∈Rn×dv, U(v)=[u(v)1;u(v)2;…;u(v)m]∈Rm×dv, v∈1,…,V, and dv is the feature dimensionality of the vth view. The similarity matrix between m anchors and n original samples in the vth view is defined as Bv∈Rn×m. bvij measures the similarity between the ith sample and jth anchor of the vth view. We adopt a parameter-free but effective neighbor assignment strategy [22] to avoid bringing extra parameters into the graph construction. The neighbor assignment for the ith sample of the vth view x(v)i can be established by solving the minimization problem in the following equation:
minbTi1=1,bi≥0∑j=1m∥∥x(v)i−u(v)j∥∥22bvij+γ∑j=1m(bvij)2.(1)
View SourceRight-click on figure for MathML and additional features.

Nie et al. [22] reported that bvi is sparse and has k nonzero values. Thus, the learned Bv is sparse, and the computational burden of subsequent processing can be largely alleviated. Defining the square of euclidean distance between the ith sample and its jth nearest anchor of the vth view in Eq. (1) as h(x(v)i,u(v)j)=∥x(v)i−u(v)j∥22, γ can be set as γ=k2h(i,k+1)−12∑kj=1h(i,j). The solution to Eq. (1) is as follows:
bvij=h(x(v)i,u(v)k+1)−h(x(v)i,u(v)j)∑kj′=1(h(x(v)i,u(v)k+1)−h(x(v)i,u(v)j′)).(2)
View SourceRight-click on figure for MathML and additional features.A detail derivation can be found in [22].

Constructing Bv takes O(ndvm+nmlog(m)) time. By contrast, constructing a normal K-NN graph takes O(n2dv) time. An anchor-based strategy has less computational costs compared with conventional graph-based methods. However, anchors must be sufficiently dense for effective adjacency relationships to obtain reasonable accuracy. Therefore, the computational cost varies with the scale of the dataset.

2.2 Bipartite Graph
The bipartite graph model used in classical bipartite spectral graph partitioning (BSGP) [37] is an effective model for co-clustering words and documents in a document collection. This graph can be regarded as an undirected weighted graph G={N,A}, where N is the node set with n1+n2 nodes, and A∈R(n1+n2)×(n1+n2) represents the affinity matrix
A=[0WTW0].(3)
View SourceRight-click on figure for MathML and additional features.W∈Rn1×n2 is viewed as the weight matrix of the bipartite graph, and wij indicates the edge-weight to connect the ith left-side node and the jth right-side node of W.

The BSGP is proposed to essentially perform spectral clustering with normalized cut on the graph G for co-clustering words and documents in a document collection, and it effectively works. Nie et al. [38] proposed a novel co-clustering method to learn a bipartite graph with exactly k connected components, where k is the number of clusters. The new bipartite graph learned in [38] approximates the original graph but maintains an explicit cluster structure. A common aim between these two algorithms is to cluster rows on the basis of their column distributions. By contrast, column clustering is determined by co-occurrence in rows.

In this study, we come up with the idea of simultaneous classification of the original samples and anchors, which points to a duality between original samples and anchors, based on the use of a bipartite graph in [37], [38]. In our model, an undirected bipartite graph is a triple G={X,U,S}, where X={x1,…,xn} and U={u1,…,um} denote the original and anchor datasets, respectively; and S is the set of edges {{xi,uj}:xi∈X,uj∈U,1≤i≤n,1≤j≤m}. The edge {xi,uj} is the similarity between the ith original data sample and jth anchor. We view the multi-view consensus similarity matrix P, which measures the adjacency between the original samples and the anchors as the weight matrix of the bipartite graph
S=[0PTP0],(4)
View Sourcewhere P∈Rn×m and S∈R(n+m)×(n+m). In the proposed FMSSL, we incorporate the bipartite graph in our SSL model. The class labels of the original data and anchors are simultaneously propagated from the labeled to the unlabeled samples. The SSL can be efficiently performed by using the bipartite graph structure, and the computational complexity can be significantly reduced.

SECTION 3FMSSL With Learned Graph
In this section, we introduce the proposed method FMSSL, which consists of three phases (Fig. 1). First, we construct the similarity graphs B1,B2,…,BV by using an anchor-based strategy on the representation of all features. Second, a multi-view consensus graph P is learned on the integration graph ∑Vv=1dvBv and currently available label matrix [F;G] to encode the relationships among all data samples. The currently available label information is the label matrix [F;G] learned in the current iteration process. Then, we update the weight coefficient dv and label matrix [F;G] on the basis of P. Finally, graph P and label matrix [F;G] are iteratively optimized in the SSL loop.

Fig. 1. - 
Overview of FMSSL.
Fig. 1.
Overview of FMSSL.

Show All

In graph-based multi-view learning, each feature can construct a similarity graph and maximize the performance quality on its own. FMSSL starts with the idea of merging multiple graphs in an optimal linear combination. We learn weight coefficients dv for each graph when label information changes. This task is initiated to properly integrate feature information from multiple views and achieve enhanced performance, with similarity graphs B1,B2,…,BV(Bv∈Rn×m, v=1,2,…,V) constructed by using an anchor-based strategy. Subsequently, the integration graph ∑vdvBv is obtained. Consider that this linear combination relation may be overstrict to fit the relationship in multiple views, we relax this hard relation by proposing a flexible regression residue (i.e., ∥P0∥2F, P0∈Rn×m). We assume the multi-view consensus affinity graph P=∑vdvBv+P0. The regression residue P0=P−∑vdvBv is obtained to model the mismatch between P and ∑vdvBv
minP∈Rn×m,P1=1,P≥0,dT1=1,d≥0∥∥∥P−∑vdvBv∥∥∥2F,(5)
View Sourcewhere d=[d1;d2;…;dV]∈RV×1 is the weight vector.

Based on the idea of classification on a bipartite graph by simultaneously implementing a separate classification on original data and anchors, P is viewed as the weight matrix of a bipartite graph with n+m nodes, where pij is the weight to connect the ith original data and jth anchor. P should be further constrained with P≥0 and P1=1 to achieve the ideal neighbor assignment.

Expression yi∈Rc×1 is defined as the label for the ith(1≤i≤n) sample and c is the number of classes. Variable yi is one-hot, and element yij=1 means that the ith sample belongs to the jth class; otherwise, yij=0. We let Y=[Yn;Ym]∈R(n+m)×c. Expressions Yn∈Rn×c and Ym=0∈Rm×c are denoted as the initial label assignment matrix of n original data points and m anchors. We suppose that the number of original samples n=l+u, where l and u are the number of labeled and unlabeled samples, respectively. We rearrange the initial label assignment matrix Yn into blocks as Yn=[Yl;Yu] without loss of generality. The first l original samples xi(1≤i≤l) are labeled as Yl=[y1,…,yl]T∈Rl×c. The remaining u original samples xi(l+1≤i≤n) are unlabeled as Yu=0∈Ru×c. Thereafter, F=[Fl;Fu]=[f1,…,fn]T∈Rn×c and G=[g1,…,gm]T∈Rm×c are defined as the class indicator matrices of n original samples and m anchors, where fi∈Rc×1(1≤i≤n) and gj∈Rc×1(1≤j≤m). Subsequently, the data matrix is split into X(v)=[X(v)l;X(v)u]. This study aims to predict the class indicator matrix F.

The traditional graph-based SSL solves the following problem:
minF,Fl=Yl∑ij∥fi−fj∥22sij,(6)
View SourceRight-click on figure for MathML and additional features.where fi and fj are the labels for the ith and jth samples, and S∈Rn×n is the similarity matrix with each entry sij indicating the similarity between the ith and jth samples.

Our anchor-graph-based multi-view SSL aims to solve the following problem:
minF,G∑ij∥∥fi−gj∥∥22pij.(7)
View SourceRight-click on figure for MathML and additional features.Eq. (7) can be rewritten according to the property of Laplacian matrix
minF,GTr([FG]TLP[FG]),(8)
View SourceRight-click on figure for MathML and additional features.where LP=DP−S is the Laplacian matrix associated with S, S=[0PTP0], and DP∈R(n+m)×(n+m) is the diagonal degree matrix whose ith diagonal element is ∑jsij.

An optimal consensus graph P should be smooth not only on all features but also on learned label information. Therefore, the multi-view consensus graph learned on feature and label information can be formulated as follows:
minF,G,P∈Rn×m,P1=1,P≥0,dT1=1,d≥0∥∥∥P−∑vdvBv∥∥∥2F+αTr([FG]TLP[FG]),(9)
View SourceRight-click on figure for MathML and additional features.where α>0 is the balancing parameter.

In our FMSSL, we aim to learn the optimal multi-view consensus affinity graph P to obtain the optimal label matrix F. Specifically, we should simultaneously find the optimal P and F in a unified objective function. Thus, the proposed FMSSL learns the label information on the basis of the following objective function:
minF,G,P∈Rn×m,P1=1,P≥0,dT1=1,d≥0∥∥∥P−∑vdvBv∥∥∥2F+αTr([FG?]TLP[FG])Tr(([FG]−Y)TU ([FG]−Y))(10)
View Sourcewhere α and U are balancing parameters. U∈R(n+m)×(n+m) is a diagonal matrix. In general, if sample xi is labeled uii=∞; otherwise, uii=0. The first term is the regression residue to relax the hard relation that the optimal linear combination relation may be overstrict to fit the relationship in multiple views. The second term is the penalty function, which measures the smoothness of multi-view consensus graph P on the learned label information [F;G]. Thus, the close labels have high similarity, and the far ones have low similarity. The third term is a soft error term introduced to release the limitation that the predicted labels Fl are strictly equal to the given hard labels Yl because the provided ones may be noisy. Variable dv determines the importance of each feature. We iteratively learn the optimal P when [F;G] changes and optimize [F;G] with the learned P, which is viewed as the weight matrix of the bipartite graph S by simultaneously implementing a separate classification on the original samples and anchors. In the next section, we propose an efficient algorithm to solve this problem.

SECTION 4Optimization
4.1 Optimization Algorithm
We use the alternative optimization approach to solve the challenging objective function in Eq. (10). Variables are updated one by one in the iteration procedure. The specific parameter updated in the last step could be regarded as a constant during the current step. First, we initialize dv=1/V with each Bv being constructed by using an anchor-based strategy and randomly initialize F and G. After the needed parameters are initialized, in each iteration, we first update P by using F, G, and d and then F and G by fixing P and d. We update d by fixing F, G, and P. These iteration steps are described in the following.

4.1.1 Fixing F, G and d and Updating P
When F, G and d are fixed, we can obtain P by optimizing Eq. (10). The third term of Eq. (10) now can be regarded as a constant. We denote B=∑vdvBv, and then the objective function is equivalent to the following function:
minP∈Rn×m,P1=1,P≥0∥P−B∥2F+αTr([FG]TLP[FG]).(11)
View SourceEq. (11) can be further rewritten as follows:
minpTi1=1,pi≥0∑i=1n∑j=1m(pij−bij)2+α∑i=1n∑j=1mpij∥∥fi−gj∥∥22.(12)
View Source

Problem (12) is independent between different i, so we can individually solve the following problem for each i. For convenience, ei is denoted as a vector with the jth element equal to eij=∥∥fi−gj∥∥22. Thus, this problem can be reformulated in vector form as follows:
minpTi1=1,pi≥0∥∥pi−(bi−α2ei)∥∥22.(13)
View SourceRight-click on figure for MathML and additional features.This proximal problem can be solved with a closed-form solution. The Lagrangian function of problem (13) is
L(pi,ζ,βi)=12∥∥pi−(bi−α2ei)∥∥22−ζ(pTi1−1)−βTipi,(14)
View SourceRight-click on figure for MathML and additional features.where ζ and βi≥0 are the Lagrange multipliers.

The optimal solution pi should satisfy that the derivative of Eq. (14) w.r.t. pi is equal to zero, so we have
p^i−(bi−α2ei)−ζ1−βi=0.(15)
View SourceThen, for the jth element of p^i, we have
p^ij−(bij−α2eij)−ζ−βij=0.(16)
View SourceNote that pijβij=0 according to the KKT condition [39]. Then, we obtain
p^ij=(bij−α2eij+ζ)+.(17)
View SourceEach pi can then be solved and we can update P.

4.1.2 Fixing P and d and Updating F and G
When P, d are fixed, we can obtain F and G by optimizing Eq. (10). The first term of Eq. (10) now can be regarded as a constant. It is equivalent to optimize the following objective function:
minF,GαTr(([FG]TLP[FG])+Tr([FG]−Y)TU([FG]−Y)).(18)
View SourceBy taking the derivative of Eq. (18) w.r.t [F;G] and setting it to zero, we obtain
α(DP−[0PTP0])[FG]+U([FG]−Y)=0.(19)
View SourceRight-click on figure for MathML and additional features.The solution can be derived as follows:
[FG]=(α(DP−[0PTP0])+U)−1UY=([αDn−αPT−αPαDm]+[Un000])−1UY=[D~n−αPT−αPαDm]−1UY,(20)
View SourceRight-click on figure for MathML and additional features.where Dn∈Rn×n is the diagonal degree matrix whose ith diagonal element is ∑jpij, Dm∈Rm×m is the diagonal degree matrix whose ith diagonal element is ∑jpji, Un∈Rn×n is the upper left block diagonal matrix of U, and D~n=αDn+Un.

We use the block matrix inversion formula to quickly solve the large matrix inverse of Eq. (20). By the use of C1=D~n−αPD−1mPT and C2=αDm−α2PTD~−1nP, we have
[D~n−αPT−αPαDm]−1=[C−11αC−12PTD~−1nαD~−1nPC−12C−12].(21)
View SourceRight-click on figure for MathML and additional features.

According to the Woodbury matrix identity
(A~−U~C~V~)−1=A~−1+A~−1U~(C~−1−V~A~−1U~)−1V~A~−1,(22)
View SourceRight-click on figure for MathML and additional features.we can solve the inverse of C1 as
C−11=(D~n−αPD−1mPT)−1=D~−1n+αD~−1nP(Dm−αPTD~−1nP)−1PTD~−1n.(23)
View SourceRight-click on figure for MathML and additional features.Consider that Y=[Yn;Ym]=[Yl;Yu;Ym], Un=[Ul,0;0,Uu], Yu=0, Ym=0 and Uu=0, combined with Eqs. (20) and (21), we can solve F and G by
F=C−11UnYn=C−11[UlYl0],(24)
View SourceRight-click on figure for MathML and additional features.
G==αC−12PTD~−1nUnYnαC−12PTD~−1n[UlYl0].(25)
View SourceAfter all iterations, the final single-class label could be assigned to unlabeled data by the following decision function:
fi=argmaxjFij,∀i=l+1,…,n.(26)
View Source

4.1.3 Fixing P, F and G and Updating d
When P, F and G are fixed, we can obtain d by optimizing Eq. (10). The second and third terms of Eq. (10) now can be regarded as constants and it is equivalent to optimize the following objective function:
mindT1=1,d≥0∥∥∥P−∑vdvBv∥∥∥2F.(27)
View SourceThe manner of expanding P and Bv into large vectors p∈R(n×m)×1 and vec(Bv)∈R(n×m)×1 can be reformulated as follows:
=mindT1=1,d≥0∥p−Md∥2FmindT1=1,d≥0Tr(dTMTMd−2dTMTp+pTp),(28)
View Sourcewhere M=[vec(B1),vec(B2),…,vec(BV)]∈R(n×m)×V. Based on the assumption that Q=MTM and b=2MTp, Eq. (28) is equivalent to
mindT1=1,d≥0dTQd−dTb.(29)
View SourceRight-click on figure for MathML and additional features.

The Augmented Lagrangian Multiplier (ALM) method[40], [41], [42] is introduced to solve the constrained optimization problem
minh(X)=0f(X).(30)
View SourceRight-click on figure for MathML and additional features.The augmented Lagrangian function can be defined as
L(X,μ,Λ)=f(X)+Tr(Λh(X))+μ2∥h(X)∥2F,(31)
View Sourcewhere Λ is the Lagrangian multiplier, and μ is a positive scalar called quadratic penalty parameter. Eq. (30) can be solved through the ALM method described in Algorithm 1. This Algorithm has been proved to converge to the optimal solution under general conditions [42].

Algorithm 1. ALM Method
Initialization: Initialize μ>0, Λ, 1<ρ<2.

repeat

Update X by minXf(X)+Tr(Λh(X))+μ2∥h(X)∥2F.

Update Λ=Λ+μh(X).

Update μ=ρμ.

until converge

Output: X.

In order to solve problem (29) using the ALM method, we introduce a slack variable x and equivalently rewritten this problem as
mindT1=1,d≥0,d=xdTQx−dTb.(32)
View SourceThe corresponding augmented Lagrangian function of Eq. (32) is
mindT1=1,d≥0,xdTQx−dTb+μ2∥∥∥d−x+1μη∥∥∥22,(33)
View SourceRight-click on figure for MathML and additional features.where μ is the quadratic penalty parameter and η is the Lagrangian multiplier.

We use an efficient algorithm based on the ALM method to solve problem (33) alternatively and iteratively. At the beginning of iteration, μ should be relatively small (e.g., 0.001). Variables d and x move closer as μ iteratively increases with step ρ. When d and x are sufficiently close to each other, the function converges. We obtain the optimal d. Specifically, we optimize the problem with respect to one variable when fixing another one, thereby resulting in the following processes:

The first step is fixing x and solving d. Subsequently, the subproblem becomes
mindT1=1,d≥0dTd−2μdT(μx−η−Qx+b).(34)
View SourceBased on the assumption that r=1μ(μx−η−Qx+b), Eq. (34) can be written as follows:
mindT1=1,d≥0∥d−r∥22.(35)
View SourceRight-click on figure for MathML and additional features.This problem can be solved in the same way as Eq. (13).

The second step is fixing d and solving x. Then, we need to solve the following subproblem:
minxdTQx−dTb+μ2∥∥∥d−x+1μη∥∥∥22.(36)
View SourceRight-click on figure for MathML and additional features.For this problem, by taking the derivative of Eq. (36) w.r.t. x and setting it to zero, we obtain
QTd−μ(d−x+1μη)=0.(37)
View SourceThe solution can then be derived as follows:
x=d+1μ(η−QTd).(38)
View SourceRight-click on figure for MathML and additional features.We iteratively update d and x according to the aforementioned two steps. Subsequently, we summarize the algorithm in Algorithm 2.

Algorithm 2. Algorithm to Solve Problem (29)
Initialization: Initialize x, η, μ, 1<ρ<2.

repeat

Update d by the optimal solution to problem (35).

Update x by Eq. (38).

Update η=η+μ(d−x).

Update μ=ρμ.

until converge

Output: the weight matrix d.

The final F in the objective function Eq. (10) can be obtained by iteratively updating P, F, G and d according to the aforementioned three steps. The detailed algorithm to solve Eq. (10) is summarized in Algorithm 3.

Algorithm 3. Algorithm to Solve Problem (10)
Input: Similarity matrix Bv∈Rn×m, label matrix Yl, number of classes c, α and U.

Output: Predicted label matrix F∈Rn×c for all data samples.

Initialization: Randomly initialize F∈Rn×c and G∈Rm×c. Initialize the weight coefficient d(v)=1/V for Bv.

repeat

For each i, update the ith row of P by solving problem (13),

where the jth element of ei is eij=∥∥fi−gj∥∥22.

Update F and G by solving problems (24)-(25).

Update d by Algorithm 2.

until converge

Assign the single-class label to the unlabeled point by Eq. (26).

4.2 Theoretical Analysis of Convergence and Complexity
To illustrate the efficiency of Algorithm 3, we theoretically analyze the convergence and computational complexity in this section.

4.2.1 Convergence Analysis
We adopt Algorithm 3 to optimize the problem. We only optimize one variable at a time when the others are fixed. So, the problem is divided into three sets of convex subproblems. Finding the optimal solution to each subproblem alternatively and iteratively, the objective function will converge to a local solution. The convergence of Algorithm 3 is given through Theorem 1.

Theorem 1.
The alternate updating rules in Algorithm 3 monotonically decrease the objective value of problem (10) in each iteration until convergence.

Proof.
Suppose after the tth iteration, we have obtained Pt, Ft, Gt and dt. In the next iteration, when fixing F, G and d as Ft, Gt and dt, we obtain the global optimal Pt+1 by solving minimization problem
Pt+1=argminP∈Rn×m,P1=1,P≥0∥∥∥P−∑v(dv)tB∥∥∥2F+αTr([FtGt]TLP[FtGt]).(39)
View SourceThus, we have
∥∥∥Pt+1−∑v(dv)tB∥∥∥2F+αTr([FtGt]TLPt+1[FtGt])≤∥∥∥Pt−∑v(dv)tB∥∥∥2F+αTr([FtGt]TLPt[FtGt]).(40)
View SourceWith the currently calculated Pt+1, we update Ft+1 by solving problem
[Ft+1Gt+1]=argminF,GαTr([FG]TLPt+1[FG])+Tr⎛⎝([FG]−Y)TU([FG]−Y)⎞⎠.(41)
View SourceThen, we obtain
αTr([Ft+1Gt+1]TLPt+1[Ft+1Gt+1])+Tr(([Ft+1Gt+1]−Y)TU([Ft+1Gt+1]−Y))≤αTr([FtGt]TLPt+1[FtGt])+Tr(([FtGt]−Y)TU([FtGt]−Y)).(42)
View SourceIn Algorithm 2, an efficient ALM method is exploited to solve the optimal dt+1. Note that the ALM method converges fast [42], so the following inequality holds
∥∥∥Pt+1−∑v(dv)t+1Bv∥∥∥2F≤∥∥∥Pt+1−∑v(dv)tBv∥∥∥2F.(43)
View SourceRight-click on figure for MathML and additional features.Based on the preceding inequalities, we arrive at
∥∥∥Pt+1−∑v(dv)t+1B∥∥∥2F+αTr([Ft+1Gt+1]TLPt+1[Ft+1Gt+1])+Tr(([Ft+1Gt+1]−Y)TU([Ft+1Gt+1]−Y))≤∥∥∥Pt+1−∑v(dv)tB∥∥∥2F+αTr([FtGt]TLPt+1[FtGt])+Tr(([FtGt]−Y)TU([FtGt]−Y))≤∥∥∥Pt−∑v(dv)tB∥∥∥2F+αTr([FtGt]TLPt[FtGt])+Tr(([FtGt]−Y)TU([FtGt]−Y)).(44)
View SourceThe equal sign holds only when all subproblems find the global optimal solution. Otherwise, the less than sign holds. Thus, Algorithm 3 will monotonically decrease the objective value of problem (10) in each iteration. In addition, the objective function in (10) is lower bounded by 0, so the objective value will convergence. The proof is completed.

4.2.2 Complexity Analysis
We analyze the computational complexity of FMSSL. We only consider the multiplications of the method because multiplication requires a large amount of time than operating addition. A dataset X(v)∈Rn×dv is obtained, where v∈1,…,V, V is the number of views, and d(v) is the feature dimensionality of the vth view. m is the number of anchors. The computational complexity of FMSSL can be divided into the following parts:

The computational cost of generating m anchors from n original data points by BKHK algorithm is O(ndlog(m)t), where d=∑Vv=1dv and t is the iterative number of balanced k-means.

Constructing a similarity graph by anchor-based strategy costs O(ndm+nmlog(m)).

The complexity of obtaining F and G by optimizing the algorithm is O(m3+nm2+nlc+mlc), where c is the number of classes and l is the number of labeled data.

Consider that m≪n, c≪m, d≪n and l≪n, the computational complexity can be approximated as O(ndm+nm2). FMSSL has a significant computational advantage on large-scale datasets compared with the conventional graph-based methods that need O(n2d) at least.

SECTION 5Experiments
In this section, we conduct several experiments to evaluate the effectiveness and efficiency of the proposed FMSSL on several real-world multi-view datasets. We first introduce the datasets and then present the evaluation results. Our experiments are implemented on a Windows 10 desktop computer with a 3.2 GHz Intel Core i7-8700 CPU, 32 GB RAM and Matlab R2018b (64 bit).

5.1 Datasets
Here, we select the following public datasets with scales ranging from 1,000 to 10,000: WebKB, MNIST, Handwritten, Caltech101 and NUS-WIDE. NUS-WIDE and MNIST have large scales, and WebKB exhibits the highest dimensionality. Certain methods encounter a memory overflow problem when experimenting on large-scale datasets. We select 10 categories of images from NUS-WIDE to test the running time of all these methods on NUS-WIDE and avoid memory overflow. Subsequently, two different scale datasets, namely, MNIST1 and MNIST2, from MNIST are used for comparative experiments. The explicit information of these datasets is summarized in Table 1. The brief descriptions of all datasets are shown in the following.

TABLE 1 Overview of Adopted Datasets

WebKB [43] consists of 1051 two-view web pages collected from computer science department web sites at four universities: Cornell University, University of Washington, University of Wisconsin, and University of Texas. A total of 230 course pages and 821 non-course ones are available.

Caltech101 [44] consists of 101 categories of images. We follow [45] and select a large set that contains 1,474 images of 7 classes including Face, Motorbikes, Dolla-Bill, Garfield, Snoopy, Stop-Sign, and Windsor-Chair. Six features are extracted (i.e., 48 Gabor, 40 wavelet moments, 254 CENTRIST, 1984 HOG, 512 GIST and 928 LBP).

Handwritten (HW) [46] is a dataset of handwritten digits of 0 to 9 from the UCI machine learning repository. It contains 2,000 data points. We use the 6 published features: 76 Fourier coefficients of the character shapes, 216 profile correlations, 64 Karhunen-Loéve coefficients, 240 pixel averages in 2×3 windows (Pix), 47 Zernike moments (ZER) and 6 morphological features.

NUS-WIDE [47] is a real-world dataset contains 269,648 images of 81 concepts described in 6 features. We select the 10 concepts with a number of 15,883 from all 81 categories in alphabetical order (bear, bird, boat, book, bridge, building, car, castle, cityscape, cloud, computer and coral) for testing. Five low-level features are extracted to represent each image: 64 color histograms, 144 color correlograms, 73 edge direction histograms and 128 wavelet textures.

MNIST [48] contains 70,000 samples of handwritten digits from 0 to 9. Each class contains approximately 7,000 samples, which are images centered in a 28×28 field by computing the center of mass of the pixels. We directly extract three features to represent each image without considering the manner of designing a more efficient feature: 256 LBP, 256 gray correlograms and 576 HOG. We select 1,000 and 2,000 samples from each class to make up MNIST1 of size 10,000 and MNIST2 of size 20,000, respectively.

5.2 Comparison Methods
We first compare our method FMSSL with the classical graph-based single-view approach label propagation (LP) [10] conducted on each single view of all multi-view datasets. Then, we compare our method with several state-of-the-art graph-based multi-view learning methods. These methods can be briefly introduced as follows:

Multi-feature learning via hierarchical regression (MLHR) [49]: One approach is proposed for multimedia content analysis. It constructs local linear regression models for each feature to learn view-based graphs.

Sparse multiple graph integration (SMGI) [18]: One algorithm uses sparse weights to linearly combine different graphs for implementing label propagation.

Auto-weighted multiple graph learning (AMGL) [31]: One parameter-free multi-view learning method based on the spectral clustering can be extended to a semi-supervised classification task.

Multi-view learning with adaptive neighbors (MLAN) [32]: One model performs clustering/semi-supervised classification and local structure learning simultaneously.

Multi-view SSL (MSSL) is FMSSL without anchors. MSSL directly uses raw data points to construct similarity matrix for label information learning. We use MSSL as a baseline to verify the effectiveness of FMSSL.

Mean-weight MSSL (MMSSL) method is used to demonstrate the importance of weight coefficients learning in FMSSL.

We implemented three versions of different anchor selection methods for the proposed FMSSL:

FMSSL-K: FMSSL using k-means for anchor selection.

FMSSL-R: FMSSL using random sampling for anchor selection.

FMSSL: FMSSL using BKHK for anchor selection

5.3 Experimental Setup and Evaluation Metrics
To ensure fair comparison, we downloaded the source codes of the compared methods from the authors’ websites, and we followed the experimental settings and parameter tuning steps in each paper in performing the experiments. We utilized the seven-nearest neighbors approach to construct graphs in the experiments. In SMGI, we optimized the hyperparameters λ1 and λ2 by tenfold cross-validation and set them to the optimal value 0.1. We selected σ from {10−1σ0,100σ0,101σ0} by cross-validation, where σ0 was computed by the mean value of the distance of K-NN. The four parameters to be determined in MLHR include the tradeoff parameters μ1 and μ2 and regularization parameters γ and λ. As reported in [49], we fixed λ as one. We tune γ, μ1 and μ2 from −6 to 6 with step 3 by cross-validation.

In our method, we searched parameter α starting from 1e−4 to 1e2 and growing at a tenfold rate. The number of anchors m was set according to the number of samples in different datasets, and the anchors had to be sufficiently dense for effective adjacency relations. In the following experiments, m was set to 256 on WebKB, Caltech101 and HW, and 1024 on NUS-WIDE, MNIST2 and MNIST1. The proposed algorithm was not terminated until the difference of objective was less than 1e−8. The class proportions were unchanged for each dataset. We hold the training and testing samples as a whole to simulate the real situations in a semi-supervised scenario where l≪u and randomly selected 10, 15, 20, 25 and 30 percent samples as labeled samples. The rest of the samples remained as unlabeled data. All experiments were repeated tenfold, and the average results and computational time were recorded.

In the following experimental result presentations, we used the proportion of the correct-classified data samples, classification accuracy (ACC), to evaluate the classification performance. Based on a data sample xi, the obtained classification label was defined. The label provided by the datasets are ri and si. The ACC is defined as follows:
ACC(s,r)=∑ni=1δ(si,ri)n,(45)
View SourceRight-click on figure for MathML and additional features.where n is the total number of data samples, and δ(si,ri)=1, if si=ri; otherwise, δ(si,ri)=0.

5.4 Classification Results
In this subsection, we first evaluate the capability of FMSSL and compare it with the representative single-view method LP. We then compare FMSSL with MLHR, SMGI, AMGL, MLAN and MMSSL. The best results are marked in bold face and the second-best results are underlined. In each table, the ratio denotes the proportions of the labeled data.

Fig. 2 shows the classification performance of the proposed FMSSL and LP on each view of different datasets. For all datasets, we can observe that the method FMSSL achieves better performance than the state-of-the-art single-view method. The combining feature information from different views can strengthen the classification performance. This result validates the effectiveness of FMSSL, verifying that the combining information from different views can greatly improve the classification result.


Fig. 2.
Comparison between LP and proposed FMSSL on different datasets: (a) NUS-WIDE, (b) MNIST1, (c) MNIST2, (d) HW, (e) WebKB, and (f) Caltech101.

Show All

The classification results of all the compared methods on six datasets with different percentages of labeled samples are summarized in Table 2. The comparison between FMSSL and other methods can be observed in Fig. 3. The results show that the accuracy of all approaches increased in most cases with the increased proportion of labeled samples. The more label information is available, the better is the accuracy. Obviously, MLAN has achieved a good accuracy, but it takes a lot of time and memory. Compared with the result of MSSL, FMSSL maintains the performance. FMSSL selects anchors to represent the manifold structures and approximate the similarity graphs using bipartite graphs, which enables us to handle outliers. So, the performance of FMSSL is comparable or even better than the model without anchors. The classification accuracy is unstable because of the FMSSL-R randomness. The comparison of MMSSL and FMSSL shows that FMSSL constantly outperforms the mean weight methods, which neglect the importance of weight learning. This finding illustrates that the learning performance can be enhanced if the multiple features are properly integrated. Multi-view consensus graph learning is important in graph-based multi-view learning. After analyzing the results, FMSSL can obtain relatively ideal results with the best stability.

TABLE 2 Classification Accuracy Comparison of Different Methods With Various Percentages of Labeled Data on All Datasets


Fig. 3.
Classification accuracy comparison of different methods with various percentages of labeled data on all datasets: (a) NUS-WIDE, (b) MNIST2, (c) MNIST1, (d) HW, (e) WebKB, (f) Caltech101.

Show All

The running time of the multi-view classification methods on six real-world datasets are shown in Table 3. The table shows that anchor-based graphs FMSSL-K, FMSSL-R and FMSSL can greatly reduce the computational cost and need less computational resources. The other methods spend sufficient time and need additional computational resources because they have square time complexity with respect to the data size. The gap in the MNIST dataset is large. For the MNIST2 dataset, MSSL, MLAN, AMGL and SMGI are out of memory on our device, while FMSSL needs 885.8376 s, which is almost approximately fivefold faster than MLHR. As the dimensionality of WebKB in each view is much larger than its data size, MLHR consumes most time on it. SMGI uses sparse weights to linearly combine different graphs for propagating labels that make it efficient on small-size datasets but also with poor performance on certain large datasets. Table 3 shows that compared with other methods, FMSSL achieves fairly high performance with minimal associated time cost and less associated computational resources.

TABLE 3 Running Time of Each Method on All Datasets seconds,ratio=20%seconds,ratio=20%
Table 3- 
Running Time of Each Method on All Datasets $seconds, ratio=20\%$seconds,ratio=20%
5.5 Parameter Analysis
The number of anchors m and parameter α must be tuned to obtain the efficient performance of FMSSL. For simplicity, the WebKB dataset is used to study the influence of parameters on running time and classification accuracy.

First, we verify the performance of FMSSL, FMSSL-K and FMSSL-R with respect to parameter α. As mentioned, the number of anchors in this work is set to 256 on WebKB, Caltech101 and HW, and 1024 on NUS-WIDE, MNIST2 and MNIST1. We vary α=[1e−4,1e2] with a step size of 10 on the WebKB dataset. The experimental result is shown in Fig. 4. As indicated in Fig. 4a, FMSSL is comparatively less sensitive to α even though α has an effect on the classification results. The classification accuracy of FMSSL steadily changes when α changes in the grid. As shown in Fig. 4b, the overall running time on FMSSL, FMSSL-K and FMSSL-R increases as α increases. However, the running time of FMSSL slightly changes with the increase of α compared with FMSSL-K and FMSSL-R because they require numerous iterations to converge in the cases of α values 1, 1e1 and 1e2. Therefore, α needs to be appropriately set to a small value and cannot be extremely large, thereby adjusting to the role of the three parts of the algorithm and accelerating the convergence procedure.

Fig. 4. - 
Accuracy and running time (seconds, ratio = 20%) obtained by FMSSL, FMSSL-K and FMSSL-R versus $\alpha$α.
Fig. 4.
Accuracy and running time (seconds, ratio = 20%) obtained by FMSSL, FMSSL-K and FMSSL-R versus α.

Show All

The performance of FMSSL, FMSSL-K, and FMSSL-R is expected to improve as m increases. Therefore, we suggest increasing m in real-world applications. Thus, we conduct experiments by varying this important parameter. The number of anchors is set to 2k, so we vary k={3,4,5,6,7,8,9} and set α to 1e−2 in these experiments. Fig. 5 shows the experiment performance. As shown in Fig. 5a, a large anchor number m improves the classification accuracy, especially for FMSSL and FMSSL-K. The classification accuracy of FMSSL-R is unstable because of its randomness. As shown in Fig. 5b, the time spent on FMSSL-K significantly increases with the anchor number. The time spent on FMSSL and FMSSL-R slightly increase as the anchor number increases compared with FMSSL-K. The preceding analysis exhibits the appropriate anchor number m to obtain enhanced classification accuracy and acceptable running time.


Fig. 5.
Accuracy and running time (seconds, ratio = 20%) obtained by FMSSL, FMSSL-K and FMSSL-R versus number of anchors.

Show All

SECTION 6Conclusion
In this study, we introduce a FMSSL method. Starting with the similarity matrix constructed by an anchor-based strategy, FMSSL learns an optimal multi-view consistency affinity graph on the basis of feature and label information. We simultaneously perform separate classification on the original samples and anchors by regarding the learned graph as the weight matrix of a bipartite graph. FMSSL balances the contributions of different features and makes its performance much robust in the existence of low-quality features. The graph construction and iterations become much faster than those in the traditional graph-based approaches as the number of anchors is much less than that of the original data, and we use an efficient optimization algorithm. FMSSL can effectively perform the classification task with minimal computational cost and less computational resources with the learned multi-view consensus graph. The overall computational complexity is O(ndm+nm2), which is a significant improvement compared with the conventional methods O(n2m). Lastly, the proposed method is evaluated on several real-world datasets. The results show that the method can achieve superior performance in most cases and demonstrate its high efficiency and practicality.

