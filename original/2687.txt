Semi-supervised learning (SSL) that utilizes plenty of unlabeled examples to boost the performance of learning from limited labeled examples is a powerful learning paradigm with widely real-world applications such as information retrieval and document clustering. Label propagation (LP) is a popular SSL method which propagates labels through the dataset along high density areas defined by unlabeled examples, but it is fragile to bridge examples. Semi-supervised K-Means uses labeled examples to initialize clustering centers to separate different examples, however, semi-supervised K-Means fails in the situation of imbalanced issues, that is, the example size of each class varies significantly. This paper proposes a novel label propagated nonnegative matrix factorization method (LPNMF) to handle clean labeled but biased data and its extension LPNMF-E to handle noisy labeled data based on the framework of NMF. LPNMF decomposes the whole dataset into the product of a basis matrix and a coefficient matrix. To propagate labels to unlabeled examples, LPNMF regards the class indicators of labeled examples as their coefficients and iteratively updates both basis matrix and coefficients of unlabeled examples. LPNMF absorbs the merits from both semi-supervised K-Means and label propagation to handle their respective shortages. Specifically, on the one hand, LPNMF learns representative clustering centers based on the distribution of the dataset, similar to semi-supervised K-means, and thus is robust to the bridge examples. On the other hand, LPNMF pushes labels according to the affinity between examples, similar to label propagation, and thus relieves the biased problem. Moreover, we introduce a LPNMF extension to handle the noisy label case. LPNMF-E relaxes the constraint of labeled examples. Since the label of each labeled example also obtains label information from the global distribution of the whole dataset and local manifold of its neighbors, LPNMF-E outputs reliable class indicators even if a portion of examples are incorrectly labeled. Theoretical analyses for the generalization ability of our proposed models are also provided. Experimental results on both clean and noisy labeled datasets confirm the effectiveness of LPNMF and LPNMF-E compared with both LP and the representative semi-supervised K-Means algorithms.
SECTION 1Introduction
Semi-supervised learning (SSL) attracts significant amount of attention, which learns from both labeled examples and unlabeled examples [1], [2]. SSL is superior because labeled examples are mostly hard and expensive to obtain, especially in the era of big data, while unlabeled examples are available in large quantity and relatively easy to collect. Traditional supervised learning methods train models requiring that all examples are correctly labeled which may be expensive, time-consuming, and sometimes even infeasible. Unsupervised learning avoids the frustrating label procedure and inevitably sacrifices the performance. SSL has been widely studied and extended to various methods, among them, graph based semi-supervised learning method has shown its effectiveness in theory and practice. Graph-based method constructs a graph to measure the similarity of examples, where the vertices of the graph denote examples and the edges reflect the similarity of different examples. Label propagation (LP) [3] is an effective graph based semi-supervised learning method. It propagates the labels under the assumption that the closely connected examples should share the same label, thus it is reasonable to use their distances to construct the graph and propagate labels. Linear Neighborhood Propagation (LNP), however, assumes each example can be linearly represented by its neighbors and reconstructed the graph. It propagates labels using the reconstructed weight of neighbor examples. Since the two label propagation methods focus only on local manifold, they fail to handle the bridge points which connect different classes and easily misleads label to the wrong direction. In noisy cases, as the incorrect labeled examples can be viewed as the extreme case of bridge points, the label propagation based semi-supervised methods are very sensitive to the noisy labeled examples.

Semi-supervised K-means [4], [5], [6] originates from the most widely used unsupervised K-means algorithm [7], [8]. It is robust to the bridge point comparing to label propagation, as semi-supervised K-means separate examples according to cluster centers. The performance of semi-supervised K-means heavily depends on the initialization of centers. Randomly initialized centers may lead to poor local solutions. Wrong labels in imbalanced dataset will further degenerate the performance of Semi-supervised K-means, since K-means tends to balance each cluster, and the wrong labeled examples in large cluster easily deviate the center of small cluster from its position.

In the clustering, non-negative matrix factorization (NMF) [9] is inherently related to kernel K-means, theoretically [10]. Many experiments empirically show NMF has clear clustering effects in spite of the authors of NMF emphasize the differences between NMF and K-means [11], [12]. Many previous works [10], [13], [14], [15] have proved that NMF could provide a more flexible framework than K-means for the task of clustering.

Based on the discussion above, we note the performance of the label propagation is significantly effected by the bridge examples or noisy labeled examples. The semi-supervised K-means methods, to some extent, relieve this defect, however lose the ability to handle the imbalanced dataset. We also note that, NMF provides a more flexible framework than K-means in clustering. We thus in this paper propose label propagated non-negative matrix factorization (LPNMF) and its extension LPNMF-E to handle the biased and noisy data in clustering. LPNMF combines label propagation with NMF to extract effective features. LPNMF explains the label propagation process in an innovative way. Specifically, LPNMF consists of two steps in each iteration round. In the first step, LPNMF learns the basis matrix according to label vectors of labeled examples and unlabeled examples, which corresponds to the W update of NMF. In the second step, LPNMF propagates the label to the unlabeled examples according to their local structures and the global distribution of the whole examples, which corresponds to the H update. LPNMF remains the labels of labeled examples unchanged throughout the iteration rounds, which makes the obtained basis matrix more representative, thus propagates the labels smoothly. LPNMF pushes labels effectively using few clean labeled examples but fails in the situation where parts of examples are labeled incorrectly. LPNMF-E is proposed to handle noisy labeled examples. It never fixes the labels of labeled examples, the label matrix of labeled examples is also updated iteratively in the H step. Each labeled example derives its label from three sources, i.e., its initially assigned label, the distribution information of the whole dataset and the local structure of its neighbors, thus, the wrong labels could be rectified by the global distribution or local manifold.

The contributions can be summarized as:

We propose a novel label propagated non-negative matrix factorization (LPNMF) for clustering, which combines the merits of label propagation and non-negative matrix factorization and thus overcomes their respective shortcomings. We unify them in a framework and provide a optimization algorithm, and the convergence analysis is given as well.

We extend LPNMF to LPNMF-E to handle the noisy labeled data. Specifically, the label vector of each labeled example is no longer fixed, that is, in spite of having its initially assigned label, each example could change its label information according to the global distribution of whole data and local manifold of neighboring examples.

We provide a deep theoretical analysis to study the generalization ability of the proposed LPNMF, and interestingly observe that the generalization error converges to zero as the labeled examples increase in size, which guarantees the good performance of LPNMF on the test data. We also disclose the relations between the error bound and the hyperparameters.

We conduct clustering experiments on the toy datasets, the real image datasets and text datasets to validate the proposed LPNMF and its extension of LPNMF-E. To make a comprehensive evaluation, we also test the robustness of the proposed model on the noisy and imbalance scenes.

The remainder of this paper is organized as follows: Sections 2 and 3 introduces the related works and the theoretical preliminaries respectively. LPNMF and LPNMF-E are introduced in Section 4. We detail the algorithm in Section 5, and make the theoretical analyses in Section 6. Section 7 evaluates the clustering performance of LPNMF and analyzes the results, and we make a conclusion in Section 8.

SECTION 2Related Work
Our work mostly relates to label propagation and non-negative matrix factorization. Many interesting ideas are proposed recently and achieve promising performances on their respective fields. Gong et al. [16] proposed multi-label teaching-to-learn and learning-to-teach (ML-TLLT) to use the teach and learn strategy to guide the propagation from simple examples to difficult ones. Wagner et al. [17] introduced a temporal label propagation to quickly label examples on fast-moving data streams. Li et al. [18] proposed a novel label propagation for the task of saliency detection of natural images. In their work, the saliency object regions could be estimated by propagating the labels. Zhang et al. [19] used the pairwise constraints to construct graph and proposed a novel label propagation. The core idea of their work focuses on enriching the pairwise constraints by softening and propagating the labels. Regardless of the great advancements in both applications [18], [19] and speed [20], graph based label propagation faces the common challenge of label misleading in the border.

NMF decomposes a given non-negative matrix into two low-rank non-negative matrices, namely, the basis matrix and coefficient matrix. The basis matrix can be treated as the ensembles of different clustering centers, while the coefficient matrix can be understood as the probability distributions over these centers of examples. The solution of this factorization yields a natural parts-based representation for data and shares a similar psychological and physiological intuition of combining parts to form a whole object in human's brain [9]. NMF has a very close relation with K-means, many previous works attempt to reveal their similarities. Ding et al. [10] proved that constrained NMF equals to K-means theoretically, the objective function of K-means similar to NMF when imposing the orthogonal constraint to rows of coefficient matrix [10], [13]. K-means has also been proved to equal symmetric NMF which decomposes data matrix into two completely same low rank matrices [14], [15]. Generally, comparing to K-means, NMF presents a more flexible framework to absorb constraints to guide the decomposition.

Recently, many regularized or constrained NMF methods are proposed to utilize the priors to obtain a better representation [21], [22], [23] or achieve a robust clustering [24]. Moreover, the output probability distribution over different clustering centers draw a better interpretation to clustering [25], [26], [27]. Guan et al. [28] proposed manifold regularized discriminative NMF (MD-NMF) to preserve more effective margin-based discriminative information in NMF subspace. Zaferiou et al. [29] proposed discriminant NMF (DNMF) to incorporate the Fisher's criterion in NMF. Guan et al. [30] proposed a truncated cauchyNMF to handle the outliers by designing the truncated cauchyNMF loss. Li et al. [21] proposed a structured NMF to deal with the noise and outliers by introducing the l2,p-norm loss function. Huang et al. [31] proposed a graph regularized NMF to alleviate the impact of noises and outliers by using the l1-norm function. All the mentioned methods show that NMF provides a flexible way to incorporate prior information to enhance the performance. We, inspired by these ideas, explore the semi-supervised NMF in the view of label propagation.

SECTION 3Theoretical Preliminaries
In this section, we introduce the theoretical preliminaries of the studied baselines, including the classical label propagation, semi-supervised K-means, non-negative matrix factorization and also their respective variants.

3.1 Label Propagation
Label propagation is a simple method to predict the labels of unlabeled examples, which assumes that closer examples tend to have more similar labels and example's labels propagate to neighboring examples according to their proximity [3], [32]. Suppose we obtain the affinity matrix S for the whole examples. For simple algebra deduction, the labels of the unlabeled examples Yu can be evaluated as follows:
Yu=(I−Suu)−1SulYl,(1)
View SourceRight-click on figure for MathML and additional features.where u is the number of unlabeled examples, Suu is part of S refer to the edge weights among unlabeled examples, the component Sul refers to the edge weights between unlabeled examples and labeled examples. The learned Yu denotes the probability distribution of unlabeled examples over different classes, and the index of maximum of Yu predicts the example's label.

Recently, Wang and Zhang [33] proposed linear neighborhood propagation (LNP), LNP propagates label to unlabeled examples with sufficient smoothness; Unlike LP, LNP assumes that every examples can be reconstructed using a linear combination of its neighbors, and the reconstructed coefficients compose the new affinity matrix. Likes LP, the label matrix updates can be accomplished using one step
Y←(1−α)(I−W)−1Y0,(2)
View SourceRight-click on figure for MathML and additional features.where Y0 is the initial label matrix, Y is the predicted label matrix, α is a hyperparameter, and W is the affinity matrix. LNP is more suitable to noisy labeled case than LP, since LNP has the oppotunity rectify the wrongly labeled examples according to its neighbors. LNP shows significant effectiveness and robustness to different datasets. However, similar to LP, it also faces bridge point problem. On the other hand, despite the fact that LNP can recover few incorrectly labeled examples, LNP easily leads to huge prediction error when the wrong labeled examples increases.

3.2 Seeded and Constrained K-Means
K-means is a classic unsupervised clustering algorithm [7], [8], which iteratively relocates centers to optimize clustering results until convergence. For a set of data examples X={x1,x2,…xn},xi∈Rd, K-means partitions X into K clusters {Xm}km=1. If the center of each cluster represent as U={u1,u2,…uk}, K-means can be defined as
min∑i=1n(xi−Uhi)2,(3)
View SourceRight-click on figure for MathML and additional features.where hi∈Rk is the label vector of the example xi, only one element of hi equals 1, others remain 0. The objective of K-means is to find label vectors for all examples.

Generally, K-means initializes clustering centers randomly. Thus it is unavoidable to get stuck in trivially local optima and may achieve poor performances. Seeded K-means and Constrained K-means [4] introduce labeled examples which called seeds to centers initialization and help reduce the chance of obtaining poor local optima. Seeded K-means and Constrained K-means differentiate from the traditional K-means mainly on the initialization stage. Assuming {Sm}km=1 denotes the seeds set, the center of each cluster are initialized as
u0i←1|Si|∑xj∈Sixj,(4)
View SourceRight-click on figure for MathML and additional features.where |Si| means the cardinality of Si. With temporarily predicting labels, the centers is calculated as
ui←1|Xi|∑xj∈Xixj.(5)
View SourceRight-click on figure for MathML and additional features.

After calculating the new centers, Seeded K-means and Constrained K-means reassign the label of each example according to its distance to the obtained centers
l←argminl||xi−ul||2.(6)
View Source

Using (6), the label l can be appointed to xi. Seeded K-means and Constrained K-means share the same centers calculated step. However, they are different in the labels assigned step, Seeded K-means reassigns the labels of all examples including the seeds using (6), while the reassigned process of Constrained K-means solely focuses on unlabeled examples, the labels of seeds keep their initial state throughout.

3.3 Non-Negative Matrix Factorization
NMF is an unsupervised learning method [9], which decomposes an nonnegative data matrix into two low-rank nonnegative matrices. To achieve this goal, NMF minimizes summation of the squared residues between the data matrix V=[v1,v2,…,vn]∈Rm×n+ and the product of the basis matrix W=[w1,w2,…,wc]∈Rm×c+ and the coefficient matrix H=[h1,h2,…,hn]∈Rc×n+, where c denotes a new dimension introduced by NMF. Generally c≪min(m,n). Thus NMF is widely used to dimension reduction
f(W,H)=minW,H||V−WH||2F,s.t.W,H≥0,(7)
View SourceRight-click on figure for MathML and additional features.where ||⋅||2F denotes Frobenius norm.

NMF has been proven to theoretically equal to K-means when imposing constraints of row orthogonality on the coefficient matrix [10]. The objective function of (13) can then be represented as
f(W,H)=min∑i=1n(vi−Whi)2.(8)
View Source

Comparing (14) to (9), when we regard W as cluster centers and h as a label vector, we find that NMF and K-means have the same formation. The only difference between K-means and NMF lies in the definition of the label vector of each example. K-means strictly constrains the value of each label vector, allowing only one non-zero element. However, NMF relaxes this constraint, and just emphasizes its non-negativity of all elements. From this view, we can also see that K-means may easily fall into poor local optima because of the discrete optimization; while NMF can be easily solved by continuously optimizing. NMF shows good performances in clustering. When c equals to the number of classes, NMF can explains the clustering directly [34], [35]. Note that W signifies cluster centers whose columns are the centroids of every cluster, and that H signifies the label matrix whose columns indicate the cluster membership of examples. As each column of H have many non-zero elements, NMF considers the index of maximal value as example's cluster label.

SECTION 4Label Propagated NMF
In this section, we introduce label propagated nonnegative matrix factorization, which introduces label propagation to NMF. LPNMF propagates labels along with the optimization of NMF. First, the coefficient matrix of labeled examples is initialized as the label matrix. This initialization constrains the construction of the basis matrix which can be considered as a propagation matrix. In the following updates, labels can be traveled to the unlabeled part in virtue of the basis matrix. Traditional label propagation method fixes its propagation matrix and travels labels to neighbors according to proximity, whereas, LPNMF updates the propagation matrix iteratively and propagates labels to unlabeled examples according to both their distribution and geometry. It is difficult for the traditional label propagation to deal with bridge examples, as bridge examples connect examples from different classes, and they easily misguide label to examples of other classes. Worst of all, bridge examples usually propagate labels to the wrong direction and lead to large crowd errors. However, LPNMF propagates label to a specific example by both utilizing the label information from its neighbors and the distribution information of all examples, and thus can well handle bridge examples. For each iteration round, LPNMF regulates the basis matrix to makes it more representative. Meanwhile, LPNMF regulates the coefficient matrix to perform label propagation.

4.1 The Framework of LPNMF
Given a data matrix V=[v1,v2,…,vn]∈Rm×n+, it consists of n examples and each has m features. As to semi-supervised learning, without loss of generality, we assume the first l examples are labeled, and the remained u(=n−l) examples are unlabeled, namely Vl and Vu respectively. Similar to label propagation, we construct label matrix with the known label information. For simplicity, supposing there are c clusters, label matrix Hl∈Rc×l can be defined as follows:
(Hl)ij={10ifexamplejislabeledwithi,otherwise.(9)
View SourceRight-click on figure for MathML and additional features.

Here, we introduce a affinity graph of Guu to push labels for unlabeled examples. The construction of Guu is similar to S of label propagation in (5). The differences are, first we focused on the unlabeled examples only, and second we column-normalized the Guu to G¯uu. We note that, the proposed LPNMF does not directly push labels from labeled examples to unlabeled examples as traditional label propagation methods do. The final label of each unlabeled example is determined by two unrelated aspects: its distance to the centers and the labels of its neighbors.

The above label matrix is part of coefficient matrix as to NMF. To achieve the label propagation, we introduce the following objective:
f(W,Hu)=minW≥0,Hu≥012||Vl−WHl||2F+λ2||Vu−WHu||2F+γ2∣∣∣∣Hu−HuG¯uu∣∣∣∣2F+β2∣∣∣∣WT1m−1c∣∣∣∣2F,(10)
View SourceRight-click on figure for MathML and additional features.where λ tradeoffs the labeled part and unlabeled part. We use euclidean distance to measure the residue of NMF. Note that Hl and Hu share the same W. Hu can be regarded as a soft label matrix of unlabeled examples, i.e., each column is supposed to be the probabilities over different labels for an example. The third constrained item travels label information among unlabeled examples through the affinity graph of G¯uu. γ is the parameter to weight this term. The forth term of (20) is to column-normalize W, which makes sense for Hu, and will be explained in the following. β is the balance parameter, 1m∈Rm and 1c∈Rc are vectors whose elements are all equal to 1.

To maintain the label probability interpretation, traditional LP normalizes the label matrix in each iteration round. Favorably, some simple normalization can make LPNMF interpretable to probability distribution. We column-normalize V and G¯uu in advance, this normalization shows favorable effects, as showing in the following propositions:

Proposition 1.
Assuming V=WH, if columns of V and W both normalize to 1, then H is also column-normalized.

Proof 1.
|vk|1=∑lVlk=∑l∑mWlmHmk=∑mHmk∑lWlm=1.(11)
View SourceRight-click on figure for MathML and additional features.
∑lWlm=1.(12)
View Source

Both (21) and (22) are the given prerequisites of the proposition, it is easy to obtain (23)
∑mHmk=1.(13)
View Sourcewe would see that |hk|1=1.

Proposition 2.
If Hu and G¯uu are both column-normalized, then HuG¯uu is also column-normalized.

Proof 2.
∣∣(HuG¯uu)k∣∣1=∑l(HuG¯uu)lk=∑l∑m(Hu)lm(G¯uu)mk=∑m(G¯uu)mk∑l(Hu)lm=1,(14)
View SourceRight-click on figure for MathML and additional features.we would see that ∣∣HuG¯uu∣∣1=1.

The propositions explicitly explains the necessity of the initial normalization of V and G¯uu. Specifically, LPNMF normalizes each column of the data matrix at first, as Hl is defined as (19), the only non-zero element equals 1. It is obvious that the minimization of the first term and forth term of (20) make W approximate column-normalized. And according to the proposition 1, the column-normalized W can be used to normalize Hu. The third term also support the Hu to be column-normalized as G¯uu is column normalization itself according to the Proposition 2. The parameter λ and β control the degree of normalization.

4.2 The Extension of LPNMF
LPNMF travels labels from labeled examples to unlabeled examples according to the distribution of unlabeled examples over the centers initialized by labeled examples. For the unlabeled examples, labels also travel among them according to the affinity of their neighbors. The basis matrix of LPNMF updates iteratively to generate more representative centers for all examples. From (10), we see that these centers are constrained by the label information of labeled examples and the distribution of unlabeled examples. Thus LPNMF works well solely with the help of clean labels. In real-world application, labels are usually assigned incorrectly. Seeded K-means corrects labels of seeds according to the current clusters centers. LNP fractionally absorbs labels of neighbors to correct the wrongly labeled examples. In this section, we propose LPNMF-E, an extension of LPNMF, to deal with the noisy labeled examples. The objective of LPNMF-E is as follows:
f(W,Hl,Hu)=minW≥0,Hu≥012||Vl−W(αHl+(1−α)Ht)||2F+λ2||Vu−WHu||2F+γ2∣∣∣∣Hu−HuG¯uu∣∣∣∣2F+β2∣∣∣∣WT1m−1c∣∣∣∣2F.(15)
View Source

LPNMF-E reshapes the label matrix of labeled examples. It consists of the initial state Hl and transfer state Ht. The final label matrix is determined by the combination of the two states, αHl+(1−α)Ht. We introduce α to balance Hl and Ht. In practice, Ht is initialized as column-normalization, which certainly ensures column-normalization of the label matrix of labeled examples and hence enforces the constraint of column-normalization of W.

LPNMF just handles situation where examples are all correctly labeled, since the label matrix of labeled examples keeps its initial state and never changes. LPNMF-E corrects inaccurately labeled examples using Ht. When the effect of the finally learned Ht surpasses Hl for a labeled example, LPNMF-E reassigns its label. LPNMF-E degrades to LPNMF when we set α=1. LPNMF-E uses four parameters α, λ, γ, and β to shape this model. It is hard work to tune the best parameters of them. However, for a given dataset, LPNMF-E can achieve good enough results based on the following consideration. We note that α should be smaller than 0.5, as larger value of α makes the final label hard to absorb enough information after the updating. α is set to 0.1 for all our experiments. λ and γ measures the importance of unlabeled examples comparing to labeled examples. As the number of examples vary from datasets to datasets, it is impossible to use a absolute value for all datasets to balance them. To make a reasonable selection for these two parameters, we use the ratio of labeled examples’ size to unlabeled examples’ size as a reference and tune the parameters base on them to find the best ones. In experiments, the λ and γ are both set equal to 0.1. We also find, in the parameters tuning, that LPNMF-E is robust to the variable β, we thus set β=1.

SECTION 5Algorithm
There are many methods to solve NMF, e.g., multiplicative update rule (MUR) [36], projected gradient descent (PGD) [37], NeNMF [38]. All these algorithms can be applied to LPNMF with a little modification. MUR is the most popular method among them due to the simple form and effective performance. In this paper, we introduce MUR to solve LPNMF and LPNMF-E. MUR optimizes the basis matrix and label matrix alternatively until converging to local optima. Compare to NMF, LPNMF does some changes to basis and coefficient updating. We present the iterative rules at first. Then the convergence proofs involving optimization theory, auxiliary function are given. For the optimization problem (20), the update rules are
W=WVlHTl+λVuHTu+β1m1TcWHlHTl+λWHuHTu+β1m1TmW.(16)
View SourceRight-click on figure for MathML and additional features.
Hu=HuγHuG¯uu+γHuG¯uuT+λWTVuγHu+γHuG¯uuG¯uuT+λWTWHu.(17)
View SourceRight-click on figure for MathML and additional features.

We first prove the update rule of (26) decrease the (20).

5Proposition 3.
Give H, (26) does not increases the objective value of (20).

5Proof 3.
We introduce an auxiliary function to prove this proposition. To make a clear illustration, we rewrite the (20) as following:
J(W)=trace(−VTlWHl−λVTuWHu−β1TcW1m+12WHlHTlWT+λ2WHuHTuWT+β21TmWWT1m).(18)
View Source

Assuming (26) updates from W′ to W′′, i.e.,
W′′=W′VlHTl+λVuHTu+β1m1TcW′HlHTl+λW′HuHTu+β1m1TmW′.(19)
View SourceRight-click on figure for MathML and additional features.

Our goal is to prove J(W′′)≤J(W′). To this end, we construct an auxiliary function for J(W) as
Z(W,W′)=−∑ik(VlHTl+λVuHTu+β1m1Tc)ikWik+12∑ik(W′HlHTl)ikW2ikW′ik+λ2∑ik(W′HuHTu)ikW2ikW′ik+β2∑ik(1m1TmW′)ikW2ikW′ik(20)
View SourceRight-click on figure for MathML and additional features.

In the following, we will prove that Z(W,W′) is an upper bound of J(W), i.e., J(W)≤Z(W,W′), and the equality is satisfied when W=W′. It is easy to verify that J(W′)=Z(W′,W′). It remains to prove J(W)≤Z(W,W′) for any W. According to [36], we have
∑i=1n∑p=1k(AW′B)ipW2ipW′ip≥trace(AWBWT),(21)
View Sourcewhere both A and B are symmetric matrices. By substituting A=I and B=HlHTl into (31), we have
∑ik(W′HlHTl)ikW2ikW′ik≥trace(WHlHTlWT).(22)
View SourceRight-click on figure for MathML and additional features.

By substituting A=I and B=HuHTu into (31), we have
∑ik(W′HuHTu)ikW2ikW′ik≥trace(WHuHTuWT).(23)
View SourceRight-click on figure for MathML and additional features.

By substituting A=1m1mT and B=I into (31), we have
∑ik(1m1mTW′)ikW2ikW′ik≥trace(1m1mTWWT).(24)
View SourceBased on (32), (33), and (34), it is easy to check that J(W)≤Z(W,W′).

It is evident that Z(W,W′) is a quadratic function with respect to W and W′′ is its minimum. Since J(W)≤Z(W,W′) and J(W′)=Z(W′,W′), we have
J(W′′)≤Z(W′′,W′)≤Z(W′,W′)=J(W′).(25)
View Source

This completes the proof.

The update rule of (27) decrease the (20) share a similar proof.

5Proposition 4.
Give W, (27) does not increases the objective value of (20).

5Proof 4.
Here we also use an auxiliary function to prove this proposition. We rewrite the (20) with the variable Hu as following:
J(Hu)=trace(−γHuG¯uuHTu−λWHuVTu+γ2HuHTu+γ2HuG¯uuG¯uuTHTu+λ2WHuHTuWT).(26)
View SourceRight-click on figure for MathML and additional features.

Assuming (27) updates from H′u to H′′u, i.e.,
H′′u=H′uγH′uG¯uu+γH′uG¯uuT+λWTVuγH′u+γH′uG¯uuG¯uuT+λWTWH′u.(27)
View SourceRight-click on figure for MathML and additional features.To prove J(H′′u)≤J(H′u), we introduce another auxiliary function for J(Hu) as
Z(Hu,H′u)=−∑ik(γH′uG¯uu+γH′uG¯uuT+λWTVu)ik(Hu)ik+γ2∑ik(Hu)2ik+γ2∑ik(H′uG¯uuG¯uuT)ik(Hu)2ik(H′u)ik+λ2∑ik(WTWH′u)ik(Hu)2ik(H′u)ik.(28)
View Source

Next, we prove J(Hu)≤Z(Hu,H′u), and the equality satisfied when Hu=H′u. As J(H′u)=Z(H′u,H′u) can be easily verified, we only need to prove J(H′u)≤Z(Hu,H′u) for any Hu. Similar to the proof of W, we use the (31) to reach the upper bound of J(H′u).

By substituting we have the following inequalities
∑ik(H′u)ik(Hu)2ik(H′u)ik≥trace(HuHTu).(29)
View Source
∑ik(H′uG¯uuG¯uuT)ik(Hu)2ik(H′u)ik≥trace(HuG¯uuG¯uuTHTu).(30)
View Source
∑ik(WTWH′u)ik(Hu)2ik(H′u)ik≥trace(WTWHuHTu).(31)
View SourceBased on (39), (40), and (41), it is easy to check that J(Hu)≤Z(Hu,H′u).

It is evident that Z(Hu,H′u) is a quadratic function with respect to Hu and H′′u is its minimum. Since J(Hu)≤Z(Hu,H′u) and J(H′u)=Z(H′u,H′u), we have
J(H′′u)≤Z(H′′u,H′u)≤Z(H′u,H′u)=J(H′u).(32)
View SourceRight-click on figure for MathML and additional features.

This completes the proof.

The update rules of LPNMF-E similar to LPNMF, the difference lies in that LPNMF-E has to updates Ht in each iteration rounds. The update rule of Ht can be defined as
Ht=HtWTVlWTWHt.(33)
View SourceThe Ht can be understood as the modification coefficients of labeled examples that effected by their neighbors. We note that we use HM=αHl+(1−α)Ht for our final coefficients. Thus if the modification coefficients Ht outweight the initial coefficients Hl in the updating, the involved example will correct its label.

The update rules of W and Hu share the similiar forms with (27) and (26) respectively. (27), (26), and (43) can also be analogously proven to decrease the objective of LPNMF-E using the above proof method.

SECTION 6Theoretical Analysis
In this section, we will study the generalization ability of the proposed algorithm LPNMF. To make the trade-off parameters invariant to training sample size, we rewrite the object function with W as
12l∑i=1l∥vi−Whi∥2+λ2u∑i=1u∥vl+i−Whui∥2+β2∥WT1m−1c∥2,(34)
View SourceRight-click on figure for MathML and additional features.where hui represents the soft label vector for unlabeled data and W is the learned bases for generating soft label vectors. We are interested in analyzing the generalization of the learned bases W. Assume data pairs {(v1,h1),…,(vl,hl)} are i.i.d., the generalization error w.r.t. the bases W is defined as
supW∈W(E(v,h)∥v−Wh∥2−1l∑i=1l∥vi−Whi∥2),(35)
View SourceRight-click on figure for MathML and additional features.where W is the set of possibly learned W by LPNMF. Note that different from the traditional NMF, the bases learned here will be further regularized by labeled data and a normalization term.

6Lemma 1.
For LPNMF, we have
W⊆{W|∥∥WT1m∥∥≤1+λβ−−−−−√+2c}.(36)
View Source

Our main result for generalization error bound is presented in the following theorem.

6Theorem 1.
For LPNMF, any δ>0, with probability at least 1−δ, we have
supW∈W(E(v,h)∥v−Wh∥2−1l∑i=1l∥vi−Whi∥2)≤4(1+λ)/β−−−−−−−−√+(1+λ)/β+2c+4c2l√+(2+21+λβ−−−−−√+4c)log1/δ2l−−−−−−√.(37)
View SourceRight-click on figure for MathML and additional features.

From Theorem 1, we can see that by increasing the labeled training data, the generalization error w.r.t. learned W will converge to zero, which means it will have good performance on test data. This justifies the efficiency of the proposed LPNMF algorithm. The result also shows that error bound will increase by increasing the value of λ while decrease when increasing the value of β. This makes sense because large λ will increase the uncertainty of W while large β will regularize the variance of W. The proof of our Theorem 1 and Lemma 1 can be found in the appendices, which can be found on the Computer Society Digital Library at http://doi.ieeecomputersociety.org.ezproxy.auckland.ac.nz/10.1109/TKDE.2020.2982387.

SECTION 7Experiment
In this section, several experiments are conducted to show the effectiveness of our algorithm on clustering. To validate the effectiveness of the proposed LPNMF in the bias datasets, we compare LPNMF with several classical algorithms in the document corpora of Reuters-21578 and TDT-2 with the clean labels. Next, to explore the robustness of the proposed LPNMF-E, we randomly produce some error labels for each cluster, and evaluate our proposed LPNMF-E in the noisy case. We test LPNMF-E on both the above document corpora and image datasets of COIL20, MNIST and Caltech101. Several popular and classical algorithms are compared to the proposed methods. We also generate a toy dataset to explicitly illustrate the advantages of our method in clustering.

7.1 Evaluation Metric
We use two metrics to measure the clustering performance of LPNMF, i.e., AC and NMI. AC compares the predicted label with the given label of all examples. Specifically, for each example, if the propagated label is identical with its given label, we think the label propagated correctly and then the accuracy number adds 1, otherwise we ignore the incorrect propagation. AC is the ratio of the accuracy number to the number of all examples. Another metric is the normalized mutual information (NMI). In our experiments, we use NMI to measure how similar predicted label set and ground truth label set are. The value of NMI ranges from 0 to 1. The more similar predicted label set and ground truth are, the larger NMI is. Comparing to AC, NMI have a better intuition about evaluating the clustering performances on imbalanced dataset. More details about these two metrics can be find in [34].

7.2 Data Corpora
We conduct our clustering experiments both on the document corpora Reuters-21578, TDT-2 and the image datasets COIL20, MNIST and Caltech101 to demonstrate the effectiveness of our algorithms. Reuters-21578 consists of 21578 documents, and divides into 135 clusters. TDT-2 has 64527 documents, which are grouped into 100 clusters. In our experiments, to meet the requirements of testing, we remove the documents which have multiple labels and leave these ones which have only one label. On the other hand, we also discard these documents whose clusters size is smaller than 10. After this preprocessing, Reuters-21578 leaves 8213 documents grouping into 41 clusters. The maximal and minimal cluster size is 3713 and 10 respectively. TDT-2 remains 10021 documents and can be categorized into 56 clusters. The maximal and minimal size of clusters is 1844 and 10, respectively. There are two obvious characters for those document corpora. First, the number of examples for different clusters is usually bias. And second, one cluster corresponds to one specific topic, the semantics of different topics may not completely independent. For image datasets, COIL20 has 20 different clusters, each corresponds to an object and each object has 72 images. The size of each image is of 32×32 pixels, with 256 grey levels per pixel. In our experiments, we use all the 20 objects for our clustering. MNIST contains 10 types of handwritten digits from 0 to 9. We use the test set of MNIST for our clustering which includes 10000 images, and each digit has around 1000 images. Each image is cropped to the size of 28×28 pixels. Caltech101 has in total 8677 images belonging to 101 clusters, the largest cluster contains 800 images and the smallest cluster contains 31 images, to make a easy comparison, we first crop all objects from the original images and resize all cropped patches to 128×128.

7.3 Performance With Clean Labels
In this section, we analyze the performance of LPNMF with clean labels, which are obtained from the given ground truth label. First, we compare LPNMF with the label propagation algorithm (LP), seeded K-means (S-KMeans), and constrained K-means (C-KMeans) in different cluster size. The number of cluster varies from 2 to 10 in our experiments. These varied clusters are randomly selected from the 41 clusters of Reuters-21578 and from the 56 clusters of TDT-2, respectively. Second, we test the performance of LPNMF along with the increasing labeled examples for each cluster. To validate the clustering results, we repeat the above algorithms 50 times for randomly selected clusters sets and calculate the averages. NMF and K-means are also implemented as the baselines. We note that, for make a fair comparison, the label information is exactly the same for all compared methods.

Figs. 1 and 2 show the performance of LPNMF versus different cluster size on Reuters-21578 and TDT-2 respectively. In experiments, we arbitrarily label two examples for each cluster. It is obvious that LPNMF outperforms other methods both on Reuters-21578 and TDT-2 for different cluster size. The curves of AC and NMI suppose that LPNMF robustly propagates labels to unlabeled examples. LP is inferior to LPNMF and superior to the rest algorithms on Reuters-21578. Seeded K-means and Constrained K-means almost achieve the same result from the picture, which can be well explained since there are no error labels. In this case, Seeded K-means and Constrained K-means share approximately consistent cluster centers.

Fig. 1. - 
$AC$AC (a) and $NMI$NMI (b) versus number of clusters on Reuters-21578.
Fig. 1.
AC (a) and NMI (b) versus number of clusters on Reuters-21578.

Show All

Fig. 2. - 
$AC$AC (a) and $NMI$NMI (b) versus number of clusters on TDT-2.
Fig. 2.
AC (a) and NMI (b) versus number of clusters on TDT-2.

Show All

We vary label size of each cluster to see the ability of LPNMF to utilize label information. For example, label size varies from 1 to 10 means we arbitrarily label 1 to 10 examples for each cluster in the experiments. We fixed the number of clusters to 10. The results are the average of 50 performances with different cluster sets.

From Figs. 3 and 4, we can see that LPNMF shows the best performance comparing to the rest algorithms in different label size on both Reuters-21578 and TDT-2. It is also evident that the performance of LPNMF improved as the label size increased. LPNMF especially excels in utilizing few labels. For example, as label size changes from 1 to 3, the performance of LPNMF experience significantly rising. LP also shows promising clustering result along with the increased label size; while Seeded K-means and Constrained K-means share the same results in small label size. However, Constrained K-means outperforms Seeded K-means as the labels increasing. Both Seeded K-means and Constrained K-means demonstrate their abilities in using label information. However, they are insensitive to the increasing label size comparing to LPNMF. It is reasonable to empirically believe that LPNMF can travel labels to unlabeled examples effectively, even there are few available labels.

Fig. 3. - 
$AC$AC (a) and $NMI$NMI (b) versus label size on Reuters-21578.
Fig. 3.
AC (a) and NMI (b) versus label size on Reuters-21578.

Show All

Fig. 4. - 
$AC$AC (a) and $NMI$NMI (b) versus label size on TDT-2.
Fig. 4.
AC (a) and NMI (b) versus label size on TDT-2.

Show All

7.4 Performance for Noisy Labeled Data
In this section, we explore the performance of LPNMF-E for clustering noisy labeled data. LPNMF shows very promising performance with clean labels. However, it keeps the initial state of labels from begin to end and fails to correct the wrongly labeled data. Based on LPNMF, we propose its extension of LPNMF-E. LPNMF-E only uses the given labels as its initial value of labeled examples. However, during the procedure of updating, more statistical information is absorbed from other examples, the final label of labeled examples may change based on the adjusting clustering center and the labels of their neighbors.

Toy dataset

In this section, we generate a toy dataset to verify the robustness of the proposed LPNMF-E. Fig. 5a presents a toy dataset. As showed, the data set consists of two very close rounded data groups and can be easily separated intuitively. These two groups with different number of points denote two bias clusters (with 600 points and 200 points in the figure, respectively). In the beginning, for each cluster, we randomly assign 15 data points with two different labels to make a noisy scene. In specific, for the small cluster, we set 10 data points from the 15 with the red label and the rest of 5 points take the green label. On the contrary, for the big cluster, we set 10 data points with green label and red label for the left 5 points. We thus can assume that there are 5 wrongly labeled data points for each cluster in the Fig. 5a. Fig. 5b is the clustering result of LPNMF-E, it corrects all wrong labels and almost accurately predicts labels of all unlabeled data points in this noisy case. Fig. 5c is the result of Seeded K-means. The labels of small group affect the large one, because Seeded K-means tends to balance the two groups with the influence of noisy labeled data. Fig. 5d is the result of LNP. We note that LNP lead to prediction errors occur in block, the incorrectly labeled examples propagate their noisy to their neighbors uncontrollably as the geometry is the only used information.

Fig. 5. - 
Label propagation on noisy labeled synthesized data.
Fig. 5.
Label propagation on noisy labeled synthesized data.

Show All

Document corpora

We first test the robustness of LPNMF-E on the document corpora of Reuters-21578 and TDT-2. We arbitrarily select 10 examples for each cluster to label. The error label in Figs. 6 and 7 denotes the number of wrongly labeled examples for each cluster, which varies from 0 to 9. In our setting, every two error labels are different. That is when the number of error label equals 9, it means that all the 10 selected examples are different from each other.

Fig. 6. - 
$AC$AC (a) and $NMI$NMI (b) versus error label on Reuters-21578.
Fig. 6.
AC (a) and NMI (b) versus error label on Reuters-21578.

Show All

Fig. 7. - 
$AC$AC (a) and $NMI$NMI (b) versus error label on TDT-2.
Fig. 7.
AC (a) and NMI (b) versus error label on TDT-2.

Show All

From the above two figures, we can observe that LPNMF-E is robust to the varied error label, and achieve promising performance. LPNMF and LP fails in this noisy situation, especially when the error label increases. Seeded K-means is obviously superior to Constrained K-means on both corpora since it can correct wrong labels in the clustering. LNP is robust than LP in extreme noisy case. However, the poor performance limits its application on both corpus.

Image dataset

We also test the proposed LPNMF and LPNMF-E on three image datasets of COIL20, MNIST and Caltech101. To further validate our proposed LPNMF and LPNMF-E in the noisy image datasets, we divide the noisy into three level as trivial, middle and serious. To be specific, for COIL20, there are 20 different objects. We randomly select 10 images for each object to label. In the trivial noisy case, we only set 2 of the 10 images to have wrong labels. We note that the wrong labels are also different to each other. In the middle noisy case, we set 5 images to have wrong labels; and similarly, in the serious noisy case, we set 8 images to take wrong labels. Table 1 presents the clustering results of different algorithms on COIL20 with different noisy level. In the trivial noisy level, we can observe that our LPNMF obtain the best performance. We can conclude that the proposed LPNMF is robust to handle the clean or trivially pollute datasets. As the noisy level heighten to middle and serious, we can observe that LPNMF-E gets the very promising clustering results. We note that SK-means also obtains a very comparative performance in serious noisy scene. However, it fails in trivial case.

TABLE 1 Clustering Results With Error Label on COIL20
Table 1- 
Clustering Results With Error Label on COIL20
Fig. 8. - 
Performances with varied parameters $\beta,\lambda,\gamma$β,λ,γ and $\alpha$α on Reusters-21578.
Fig. 8.
Performances with varied parameters β,λ,γ and α on Reusters-21578.

Show All

TABLE 2 Clustering Results With Error Label on MNIST
Table 2- 
Clustering Results With Error Label on MNIST
TABLE 3 Clustering Results With Error Label on Caltech101
Table 3- 
Clustering Results With Error Label on Caltech101
We then evaluate the proposed LPNMF and LPNMF-E on the noisy MNIST dataset. We use the same setting as on COIL20. We can also observe in Table 2 that LPNMF gets a very promising clustering performance on the trivial noisy case and LPNMF-E obtains the best AC in the middle and serious noisy case, while S-Kmeans gets the best NMI in the middle and serious noisy case. As NMI addresses more to the balance of the clustering results, a reasonable explanation is that in this very balanced image dataset (each digit have almost the same number of images), K-means shows off its ability of balance and achieve better NMI.

The image numbers in different clusters are vary significantly in Caltech101. We test both LPNMF and LPNMF-E on Caltech101 in Table 3 to observe their performances on the biased and noisy cases. K-means methods keep a relatively stable but disappointing performances on different noisy level. Label propagation methods work only in the trivial noisy situation and when the wrong labels increase the performances deteriorate sharply. In contrast, the proposed LPNMF and LPNMF-E achieve the best performances on this biased and noisy dataset.

7.5 Sensitivity of Trade-Off Parameters
As mentioned, we introduce the parameter λ to trade off the contribution of global distribution to the unlabeled examples, the parameter γ to address the contribution of local manifold in the label propagation. The parameter β regularizes the column-normalization of the basis matrix W. For the proposed LPNMF-E, as the assigned label may not correct, we thus use the parameter α (less than 0.5) for each labeled example to reduce the dependence to its initially assigned label. In this section, we test all the above parameters on the Reuters-21578 to observe the performance changes in Fig. 8. We notice that the regularization parameter β has a robust performance on all considered values ranged from 0.01 to 100. For the λ and γ, we do not use the fixed value as datasets vary significantly. We first calculate the ratio of labeled examples’ size to unlabeled examples’ size and then multiply it by an absolute value from the sets of [0.01, 0.1, 1, 10, 100] to observe the changing performances. We tune β, λ and γ on Reuters-21578 with 10 labeled examples for each cluster. We test the parameter α with seven different values in the case of 8 wrongly labeled examples out of all 10 labeled examples. For α, we can observe that totally relying on the noisy label (α=1) is unwise, while totally discarding the label information (α=0) is inadvisable as well. We also observe an irregular fluctuation for this parameter curve in the test dataset. We guess a suitable value depends on many factors, such as noisy level and informative level of labeled examples, and thus, it is hard to achieve a smooth curve versus the changing parameter. For the β, it has a robust performance on different setting, this is because the column-normalization is easy to obtain and hold. For the λ, we see a sharp performance drop with its value increasing. The trend strongly supports our idea that the proposed LPNMF exploits the label information very efficiently. The large λ overshadows the LPNMF since it excessively reduces the contribution of labeled examples. For the γ, it is easy to validate the significance of exploring local manifold (from the case of γ=0). However, excessively large value will cover up the benefits of exploring global distribution and weaken the overall performance.

SECTION 8Conclusion
We have in this paper introduced a novel semi-supervised non-negative matrix factorization method called label propagated non-negative matrix factorization (LPNMF) for clustering. LPNMF absorbs the merits of NMF and label propagation to overcome their respective shortcomings. To be specific, LPNMF considers the coefficient matrix as the label matrix and learns the basis matrix under the constraints of label information and the distribution information of the whole examples. The learned basis matrix is then regarded as a propagation matrix and thus labels propagate to unlabeled examples rely on not only their local manifold but also the global distribution of the whole dataset. The normalization of LPNMF makes the label matrix probability interpretable and takes favorable effects. To deal with the noisy labels, we extend LPNMF to LPNMF-E, which corrects the wrongly labeled examples, successfully pushes labels to unlabeled examples, and shows significant robustness to noisy labeled data. The experiments on two document corpora and three image datasets have demonstrated the effectiveness of our algorithm. We also take a deep analysis to the generalization ability of the proposed method and justify the efficiency of the proposed method. Moreover, we develop an efficient Multiplicative Update Rule (MUR) for optimizing LPNMF and LPNMF-E, the theoretical convergence of the proposed MUR has also been provided in this paper.