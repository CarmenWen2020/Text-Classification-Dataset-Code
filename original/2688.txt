In this extended abstract, we describe and analyze a lossy compression of MinHash from buckets of size O(logn) to buckets of size O(loglogn) by encoding using floating-point notation. This new compressed sketch, which we call HyperMinHash, as we build off a HyperLogLog scaffold, can be used as a drop-in replacement of MinHash. Unlike comparable Jaccard index fingerprinting algorithms in sub-logarithmic space (such as b-bit MinHash), HyperMinHash retains MinHash's features of streaming updates, unions, and cardinality estimation. For a additive approximation error ϵ on a Jaccard index t , given a random oracle, HyperMinHash needs O(ϵ−2(loglogn+log1ϵ)) space. HyperMinHash allows estimating Jaccard indices of 0.01 for set cardinalities on the order of 1019 with relative error of around 10 percent using 2MiB of memory; MinHash can only estimate Jaccard indices for cardinalities of 1010 with the same memory consumption.
SECTION 1Introduction
Many questions in data science can be rephrased in terms of the number of items in a database that satisfy some Boolean formula, or by the similarity of multiple sets as measured through the relative overlap. For example, “how many participants in a political survey are independent and have a favorable view of the federal government?”, or “how similar are the source IPs used in a DDoS attack today versus last month?” The MinHash sketch, developed by Broder in 1997 [1], is a standard technique used by industry to answer these types of questions [2]. MinHash uses O(ϵ−2logn) space, where n is the number of unique items, allows streaming updates, and directly estimates both the union cardinality and Jaccard index [3]. Further, by coupling together Jaccard index with union cardinality, intersection sizes are also accessible.

The literature provides sub-logarithmic near-optimal probabilistic data structures for approximating the ‘count-distinct’ problem [4], [5], [6], [7], [8]. Because these sketches can be losslessly merged to find the sketch of the union of sets, this enables determining the cardinality of unions of sets. Rephrased as a query on items satisfying a Boolean formula, unions enable OR queries. Similarly, sub-logarithmic near-optimal fingerprints exist for computing Jaccard index [3], [9], [10], [11]. When paired with a count-distinct sketch, this allows AND queries by looking at the intersection cardinalities of sets. Unfortunately, in general, Jaccard index fingerprints (such as b-bit MinHash [10]) cannot be merged to form the Jaccard index of a union of sets. While queries of the form |A∩B| for sets A and B can be performed by combining together a count-distinct sketch and a Jaccard index fingerprint, more complicated queries, such as |A∩(B∪C)| cannot. One of the major advantages of MinHash is that as a merge-able sketch providing both count-distinct and Jaccard index estimators, it can be composed in this fashion.

To our knowledge, there are no practical sub-logarithmic sketches to replace MinHash in that problem space, because existing sub-logarithmic Jaccard index fingerprints cannot be merged. For this reason, much work has gone into improving the performance characteristics of MinHash sketches. These advances include requiring only a single permutation [12] and showing that very minimal independence is actually needed in the random hash function [13], [14], [15]. Here, we build on prior work and use a LogLog counter to give a lossy compression of the minimum hash values that reduces the required storage size for each hash from O(logn) to O(loglogn).

1.1 MinHash
Given two sets A and B, where |A|=n and |B|=m, and n>m, the Jaccard index is defined as
t(A,B)=|A∩B||A∪B|.(1)
View SourceClearly, if paired with a good count-distinct estimator for |A∪B|, this allows us to estimate intersection sizes as well. Though Jaccard originally defined this index to measure ecological diversity in 1902 [3], in more modern times, it has been used as a proxy for the document similarity problem. In 1997, Broder introduced min-wise hashing (colloquially known as ‘MinHash’) [1], a technique for quickly estimating the resemblance of documents by looking at the Jaccard index of ‘shingles’ (collections of phrases) contained within documents.

MinHash Jaccard-index estimation relies on a simple fact: if you apply a random permutation to the universe of elements, the chance that the smallest item under this permutation in sets A and B are the same is precisely the Jaccard index. To see this, consider a random permutation of A∪B. The minimum element will come from one of three disjoint sets: A∖B, B∖A, or A∩B. If the minimum element lies in A∖B, then min(A)∉B, so min(A)≠min(B); the same is of course true by symmetry for B∖A. Conversely, if min(A∪B)∈A∩B, then clearly min(A)=min(B). Because the permutation is random, every element has an equal probability of being the minimum, and thus
P(min(A)=min(B))=|A∩B||A∪B|.(2)
View SourceRight-click on figure for MathML and additional features.

While using a single random permutation produces an unbiased estimator of t(A,B), it is a Bernouli 0/1 random variable with high variance. So, instead of using a single permutation, one can average k trials. The expected fraction of matches is also an unbiased estimator of the Jaccard index, but with variance decreased by a multiplicative factor of 1/k.

Though the theoretical justification is predicated on having a true random permutation, in practice we approximate that by using hash functions instead. A good hash function will specify a nearly total ordering on the universe of items, and provided we use θ(log(n)) bits for the hash function output space, the probability of accidental collision is exponentially small.

Though theoretically easy to analyze, this scheme has a number of drawbacks, chief amongst them the requirement of having k random hash functions, implying a θ(nk) computational complexity for generating the sketch. To address this, several variants of MinHash have been proposed [16]:

k-hash functions. The scheme described above, which has the shortcoming of using θ(nk) computation to generate the sketch.

k-minimum values. A single hash function is used, but instead of storing the single minimum value, we store the smallest k values for each set (also known as the KMV sketch [4]). Sketch generation time is reduced to O(nlogk), but we also incur an O(klogk) sorting penalty when computing the Jaccard index.

k-partition. Another one-permutation MinHash variant, k-partition stochastically averages by first deterministically partitioning a set into k buckets using the first couple bits of the hash value, and then stores the minimum value within each bucket [12]. k-partition has the advantage of O(n) sketch generation time and O(k) Jaccard index computation time, at the cost of some difficult in the analysis.

k-partition one-permutation MinHash [12] is significantly more efficient in both time (as it uses only one hash function, and no sorting is required to compute the Jaccard index) and space (as the bits used to the partition the set into buckets can be stored implicitly). For this reason, it is more commonly used in practice, and whenever we refer to MinHash in this paper, we mean k-partition one-permutation MinHash.

MinHash sketches of A and B can be losslessly combined to form the MinHash sketch of A∪B by taking the minimum values across buckets. Additionally, using order statistics, it is possible to estimate the count-distinct cardinality [4], so we can directly estimate union cardinalities using merged sketches, intersection cardinality by multiplying Jaccard index with union cardinality, and more complex set operations by rewriting the set as an intersection of unions (e.g., |(A∪B)∩C|).

1.2 Count-Distinct Union Cardinality
All of the standard variants of MinHash given in the last section use logarithmic bits per bucket in order to prevent accidental collisions (i.e., we want to ensure that when two hashes match, they came from identical elements). However, in the related problem of cardinality estimation of unique items (the ‘count-distinct’ problem), literature over the last several decades produced several streaming sketches that require less than logarithmic bits per bucket. Indeed, the LogLog, SuperLogLog, and HyperLogLog family of sketches requires only loglog(n) bits per bucket by storing only the position of the first 1 bit of a uniform hash function, and for a multiplicative error ϵ, use a total of O(ϵ−2loglogn) bits [5], [6], [7].

The analyses of these methods all originally required access to a random oracle (i.e., a truly random hash function), and storing such hash functions requires an additional O(logn) bits of space, for a total of O(ϵ−2loglogn+logn) space. Further compacting of the hashes by use of concentration inequalities, and use of k-min-wise independent hash functions allowed both relaxing the requirement of a random oracle, and reduction of the space-complexity to O(ϵ−2+logn) (or without counting the hash function, O(ϵ−2+loglogn) in the setting of shared randomness) [8], resulting in an essentially optimal sketch requiring only constant bits per bucket. In practice though, the double logarithmic HyperLogLog sketch is used under the assumptions of shared randomness and that standard cryptographic hash functions (e.g., SHA-1) behave as random oracles (clearly not true, but empirically good enough)—for real-world data sets, double logarithmic is small enough to essentially be a constant <6.

1.3 Jaccard Index Estimation
First we note that trivially, HyperLogLog union cardinalities can be used to compute intersection cardinalities and thus Jaccard index using the inclusion-exclusion principle. Unfortunately, the relative error is then in the size of the union (as opposed to the size of the Jaccard index for MinHash) and compounds when taking the intersections of multiple sets; for small intersections, the error is often too great to be practically feasible, unless significantly more bits are used. More precisely, in order to achieve the same relative error, the number of bits needed when using inclusion-exclusion scales with the square of the inverse Jaccard index, as opposed to scaling with just the inverse Jaccard index (Table 1 of [17]). Notably, some newer cardinality estimation methods based on maximum-likelihood estimation are able to more directly access intersection sizes in HyperLogLog sketches, which can then be paired with union cardinality to estimate Jaccard index [18], [19]. However, this approach is restricted to the information available in the HyperLogLog sketch itself, and seems empirically to be a constant order (<3x) improvement over conventional inclusion-exclusion.

Alternately, when unions and streaming updates are not necessary, the literature provides many examples of Jaccard index fingerprints in better than log-space [9], [10], [11]. State-of-the-art fingerprinting methods based on either reduction to F2-norm sketching or truncating bits of MinHash (‘b-bit MinHash’) are able to achieve space bounds of O(ϵ−2) in the general problem [9], [10] and O((1−t)2ϵ2log11−t) for highly similar streams when the Jaccard index t is large [11]. For estimating the Jaccard similarity between two sets, these fingerprinting techniques are basically asymptotically optimal [17].

However, b-bit MinHash and F2-norm reductions, while great for Jaccard index, lose many of the benefits of standard MinHash, even just for Jaccard index estimation. Because b-bit MinHash only takes the lowest order b bits of the minimum hash value after finding the minimum, it also requires log(n) bits per bucket during the sketch generation phase, the same as standard MinHash. This also implies a lack of mergeability: the fingerprint of the union of two sets cannot be computed from the fingerprints of the two constituent sets. The same holds true for F2-norm reductions because of double counting shared items.

1.4 Double logarithmic MinHash
We aim to reduce space-complexity for MinHash, while preserving all of its features; we focus on Jaccard index in the unbounded data stream model, as that is the primary feature differentiating MinHash from count-distinct sketches. As a preliminary, we note that as with the count-distinct problem, much theoretical work has focused on reducing the amount of randomness needed by MinHash to only requiring k-min-wise (or k−d-minwise) independent hash functions [13], [14], [15]. In this paper, we will however again assume the setting of shared randomness and the existence of a random oracle, which is standard in industry practice.

We construct a HyperMinHash sketch by compressing a MinHash sketch using a floating-point encoding, based off of HyperLogLog scaffold; using HyperLogLog as a scaffold for other statistics of interest has been previously proposed, but not for MinHash [20]. HyperMinHash as we describe below requires
O(ϵ−2(loglogn+log1ϵ)),
View SourceRight-click on figure for MathML and additional features.space and has all of the standard nice features of MinHash, including streaming updates, the ability to take unions of sketches, and count-distinct cardinality estimation. Though this construction is not space optimal—e.g., we could very likely drop in the KNW sketch [8] instead of HyperLogLog [7] for a space-reduction—it is a practical compromise and easy to implement in software.

SECTION 2Methods
MinHash works under the premise that two sets will have identical minimum value with probability equal to the Jaccard index, because they can only share a minimum value if that minimum value corresponds to a member of the intersection of those two sets. If we have a total ordering on the union of both sets, the fraction of equal buckets is an unbiased estimator for Jaccard index. However, with limited precision hash functions, there is some chance of accidental collision. In order to get close to a true total ordering, the space of potential hashes must be on the order of the size of the union, and hence we must store O(logn) bits.

However, the minimum of a collection of uniform [0,1] random variables X1,…,Xn is much more likely to be a small number than a large one (the insight behind most count-distinct sketches [4]). HyperMinHash operates identically to MinHash, but instead of storing the minimum values with fixed precision, it uses floating-point notation, which increases resolution when the values are smaller by storing an exponent and a mantissa. We can compute the exponent of a binary fraction by taking the location of first nonzero bit in the binary expansion (the HyperLogLog part), and the mantissa is simply some fixed number of bits beyond that (the MinHash part). More precisely, after dividing up the items into k partitions, we store the position of the leading 1 bit in the first 2q bits (and store 2q+1 if there is no such 1 bit) and r bits following that (Fig. 1). We do not need a total ordering so long as the number of accidental collisions in the minimum values is low.

Fig. 1. - 
HyperMinHash generates sketches in the same fashion as one-permutation k-partition MinHash, but stores the hashes in floating-point notation. It begins by hashing each object in the set to a uniform random number between 0 and 1, encoded in binary. Then, the hashed values are partitioned by the first $p$p bits (above, 2 bits, in green), and the minimum value within each partition is taken. For ordinary MinHash, the procedure stops here, and a fixed number of bits (above, 64 bits) after the first $p$p bits are taken as the hash value. For HyperMinHash, each value is further lossily compressed as an exponent (blue) and a mantissa (red); the exponent is the position of the leftmost 1 bit in the next $2^q$2q bits, and $2^q + 1$2q+1 otherwise (above, $q=6$q=6). The mantissa is simply the value of the next $r$r bits in the bit-string (above, $r=4$r=4).
Fig. 1.
HyperMinHash generates sketches in the same fashion as one-permutation k-partition MinHash, but stores the hashes in floating-point notation. It begins by hashing each object in the set to a uniform random number between 0 and 1, encoded in binary. Then, the hashed values are partitioned by the first p bits (above, 2 bits, in green), and the minimum value within each partition is taken. For ordinary MinHash, the procedure stops here, and a fixed number of bits (above, 64 bits) after the first p bits are taken as the hash value. For HyperMinHash, each value is further lossily compressed as an exponent (blue) and a mantissa (red); the exponent is the position of the leftmost 1 bit in the next 2q bits, and 2q+1 otherwise (above, q=6). The mantissa is simply the value of the next r bits in the bit-string (above, r=4).

Show All

To analyze the performance of HyperMinHash compared to random-permutation MinHash (or equivalently 0-collision standard MinHash) it suffices to consider the expected number of accidental collisions. We first describe an intuitive analysis before proving things rigorously. Here, we will also only analyze the simple case of collisions while using only a single bucket, but the same flavor of argument holds for partitioning into multiple buckets. The HyperLogLog part of the sketch results in collisions whenever two items match in order of magnitude (Fig. 2).


Fig. 2.
HyperLogLog sections, used alone, result in collisions whenever the minimum hashes match in order of magnitude.

Show All

By pairing it with an addition r-bit hash, our collision space is narrowed by a factor of about 2r within each bucket (Fig. 3). An explicit exact formula for the expected number of collisions is
EC=∑i=1∞∑j=02r−1[(1−2r+j2i+r)n−(1−2r+j+12i+r)n]⋅[(1−2r+j2i+r)m−(1−2r+j+12i+r)m],(3)
View SourceRight-click on figure for MathML and additional features.though finding a closed formula is rather more difficult.


Fig. 3.
HyperMinHash further subdivides HyperLogLog leading 1-indicator buckets, achieving a much smaller collision space, so long as we precisely store the position of the leading 1.

Show All

Intuitively, suppose that our hash value is (12, 01011101) for partition 01. This implies that the original bit-string of the minimum hash was 0.01000000000001––––––––––––––01011101⋯. Then a uniform random hash in [0,1] collides with this number with probability 2−(2+12–––+8)=2−21. So we expect to need cardinalities on the order of 221 before having many collisions. Luckily, as the cardinalities of A and B increase, so does the expected value of the leading 1 in the bit-string, as analyzed in the construction of HyperLogLog [7]. Thus, the collision probabilities remain roughly constant as cardinalities increase, at least until we reach the precision limit of the LogLog counters.

But of course, we store only a finite number of bits for the leading 1 indicator (often 6 bits). Because it is a LogLog counter, storing 6 bits is sufficient for set cardinalities up to O(226=264). This increases our collision surface though, as we might have collisions in the lower left region near the origin (Fig. 4). We can directly compute the collision probability (and similarly the variance) by summing together the probability mass in these boxes, replacing the infinite sum with a finite sum (Lemma 4). For more sensitive estimations, we can subtract the expected number of collisions to debias the estimation. Later, we will prove bounds on the expectation and variance in the number of collisions.


Fig. 4.
In practice, HyperMinHash has a limited number of bits for the LogLog counters, so there is a final lower left bucket at the precision limit.

Show All

2.1 Implementation Details
Here, we present full algorithms to match a naive implementation of HyperMinHash as described above. A Python implementation by the authors of this paper is available at https://github.com/yunwilliamyu/hyperminhash. Additionally, Go and Java implementations based on this pseudo-code were kindly provided by Seif Lotfy at https://github.com/axiomhq/hyperminhash and Sherif Nada at https://github.com/LiveRamp/HyperMinHash-java.

2.1.1 HyperMinHash Sketch
The procedure for generating a HyperMinHash sketch can be thought of as a standard one-permutation k-partition MinHash sketch [12], but where the minimum hash value is stored in a floating-point encoding. Programmatically though, it is easier to directly split the hash value into three parts: (1) the bucket ID, (2) the negative exponent of the bucket value, and (3) the mantissa of the bucket value.

Let h1,h2,h3:D→[0,1]≡{0,1}∞ be three independent hash functions hashing data from domain D to the binary domain. (In practice, we generally use a single Hash function, e.g., SHA-1, and use different sets of bits for each of the three hashes).

Let ρ(s), for s∈{0,1}∞ be the position of the left-most 1-bit (ρ(0001⋯)=4).

Let σ(s,n) for s∈{0,1}∞ be the left-most n bits of s (σ(01011⋯,5)=01011).

function HyperMinHash A,p,q,r

Let h^1(x)=σ(h1(x),p).

Let h^2(x)=min(ρ(h1(x)),2q).

Let h^3(x)=σ(h3(x),r).

Initialize 2p tuples B1=B2=⋯=B2p=(0,0).

for a∈A do

if Bh1(a)[0]<h^2(a) then

Bh^1(a)←(h^2(a),h^3(a))

else

if Bh^1(a)[0]=h^2(a) and Bh^1(a)[1]>h^3(a) then

Bh^1(a)←(h^2(a),h^3(a))

end if

end if

end for

return B1,…,B2p as B

end function

2.1.2 HyperMinHash Union
Given HyperMinHash sketches of two sets A and B, we can return the HyperMinHash sketch of A∪B by for each bucket, taking the maximum exponent; or if the exponents are the same, taking the minimum mantissa. In the floating-point interpretation of the bucket values, this is simply taking the minimum bucket value.

function Union S,T

assert |S|=|T|

for i∈{1,…,|S|} do

Initialize |S| tuples B1=B2=⋯=B|S|=(0,0).

if Si[0]>Ti[0] then

Bi←Si

else if Si[0]<Ti[0] then

Bi←Ti

else if Si[0]=Ti[0] then

if Si[[1]<Ti[1] then

Bi←Si

else

Bi←Ti

end if

end if

end for

return B1,…,B2p as B

end function

2.1.3 Estimating Cardinality
Note that the left parts of the buckets can be passed directly into a HyperLogLog estimator. We can also use other k-minimum value count-distinct cardinality estimators, which we empirically found useful for large cardinalities.

function EstimateCardinality (S,p,q,r)

Initialize |S| integer registers b1=b2=⋯=b|S|=0.

for i∈{1,…,|S|} do

bi←Si[0]

end for

R←HyperLogLogCardinalityEstimator({bi},q)

if R<1024|S| then

return R

else

Initialize |S| real registers r1,…,r|S|.

for i∈{1,…,|S|} do

ri←2−Si[0]⋅(1+Si[1]2r)

end for

if ∑ri=0 then

return ∞

else

return |S|2/∑ri

end if

end if

end function

2.1.4 Computing Jaccard Index
Given two HyperMinHash sketches A and B, we can compute the Jaccard index t(A,B)=|A∩B|/|A∪B| by counting matching buckets. Note that the correction factor EC is generally not needed, except for really small Jaccard index. Additionally, for most practical purposes, it is safe to substitute ApproxExpectedCollisions for ExpectedCollisions (algorithms to follow).

function JaccardIndex S,T,p,q,r

assert |S|=|T|

C←0, N←0

for i∈{1,…,|S|} do

if Si=Ti and Si≠(0,0) then

C←C+1

end if

if Si≠(0,0) or Ti≠(0,0) then

N←N+1

end if

end for

n←EstimateCardinality(S,q)

m←EstimateCardinality(T,q)

EC←[Approx]ExpectedCollisions(n,m,p,q,r)

return (C−EC)/N

end function

2.1.5 Computing Expected Collisions
The number of expected collisions given two HyperMinHash sketches of particular sizes can be computed from Lemma 4. Note that because of floating point error, BigInts must be used for large n and m. For sensitive estimation of Jaccard index, this value can be used to debias the estimator in Algorithm 2.1.4.

function ExpectedCollisions n,m,p,q,r

x←0

for i∈{1,…,2q} do

for j∈{1,…,2r} do

if i≠2q then

b1←2r+j2p+r+i, b2←2r+j+12p+r+i

else

b1←j2p+r+i−1, b2←j+12p+r+i−1

end if

Prx←(1−b2)n−(1−b1)n

Pry←(1−b2)m−(1−b1)m

x←x+PrxPry

end for

end for

return x⋅2p

end function

2.1.6 Approximating Algorithm 2.1.5
Here we present a fast numerically stable approximation to Algorithm 2.1.5, which generally underestimates collisions. We discuss it in more detail in Section 2.2.

function ApproxExpectedCollisions n,m,p,q,r

if n<m then

SWAP(x,y)

end if

if n>22q+r then

return ERROR: cardinality too large for approximation.

else if n>2p+5 then

ϕ←4n/m(1+n/m)2

return 0.169919487159739093975315012348⋅2p−rϕ

else

return ExpectedCollisions(n,m,p,q,0) ⋅2−r

end if

return x⋅2p

end function

2.2 Empirical Optimizations
We recommend several optimizations for practical implementations of HyperMinHash. First, it is mathematically equivalent to:

Pack the hashed tuple into a single word; this enables Jaccard index computation while using only one comparison per bucket instead of two.

Use the max instead of min of the sub-buckets. This allows us take the union of two sketches while using only one comparison per bucket.

These recommendations should be self-explanatory, and are simply minor engineering optimizations, which we do not use in our prototyping, as they do not affect accuracy.

However, while we can exactly compute the number of expected collisions through Lemma 4, this computation is slow and often results in floating point errors unless BigInts are used because Algorithm 2.1.5 is exponential in r. In practice, two ready solutions present themselves:

We can ignore the bias and simply add it to the error. As the bias and standard deviation of the error are the same order of magnitude, this only doubles the absolute error in the estimation of Jaccard index. For large Jaccard indexes, this does not matter.

We also present a fast, numerically stable, algorithm to approximate the expected number of collisions (Algorithm 2.1.6).

We can however approximate the number of expected collisions using the following procedure, which is empirically asymptotically correct (Algorithm 2.1.6):

For n<2p+5, we approximate by taking the number of expected HyperLogLog collisions and dividing it by 2r. In each HyperLogLog box, we are interested in collisions along 2r boxes along the diagonal 4. For this approximation, we simply assume that the joint probability density function is almost uniform within the box; this is not completely accurate, but pretty close in practice.

For 2p+5<n<22q+p, we noted empirically that the expected number of collisions approached 0.1699⋅2p−r for n=m as n→∞. Furthermore, the number of collisions is dependent on n and m by a factor of 4nm(n+m)(n+m−1) from 7, which for n,m≫1 can be approximated by 4n/m(1+(n/m)2. This approximation is primarily needed because of floating point errors when n→∞.

Unfortunately, around n>22q+p, the number of collisions starts increasing and these approximations fail. However, note that for reasonable values of q=6,p=15, this problem only appears when n>289≈1026.

2.3 Proofs
The main result of this section bounds the expectation and variance of accidental collision, given two HyperMinHash sketches of disjoint sets. First, we rigorously define the full HyperMinHash sketch as described above. Note that in our proofs, we will operate in the unbounded data stream model and assume both a random oracle and shared randomness.

Definition 1.
We will define fp,q,r(A):S→{{1,…,2q}×{0,1}r}2p to be the HyperMinHash sketch constructed from Fig. 1, where A is a set of hashable objects and p,q,r∈N, and let fp,q,r(A)i:S→{1,…,2q}×{0,1}r be the value of the ith bucket in the sketch.

More precisely, let h(x):S→[0,1] be a uniformly random hash function. Let ρq(x)=min(⌊−log2(x)⌋+1,2q), let σr(x)=⌊x2r⌋, and let h^q,r(x)=(ρq(x),σr(x2ρq(x)−1)).

Then, we will define
fp,q,r(A)i=h^q,r⎡⎣⎢mina∈Ai2−p<h(a)<(i+1)2−p(h(a)2p−i)⎤⎦⎥.
View Source

Definition 2.
Let A,B be hashable sets with |A|=n, |B|=m, n>m, and A∩B=∅. Then define an indicator variable for collisions in bucket i of their respective HyperMinHash sketches
Zp,q,r(A,B,i)=11(fp,q,r(A)i=fp,q,r(B)i).(4)
View SourceRight-click on figure for MathML and additional features.

Our main theorems follow:

Theorem 1.
C=∑2p−1i=0Zp,q,r(A,B,i) is the number of collisions between the HyperMinHash sketches of two disjoint sets A and B. Then the expectation
EC≤2p(52r+n2p+2q+r).(5)
View Source

Theorem 2.
Given the same setup as in Theorem 1,
Var(C)≤E[C]2+E[C].
View Source

Theorem 1 allows us to correct for the number of random collisions before computing Jaccard distance, and Theorem 2 tells us that the standard deviation in the number of collisions is approximately the expectation.

We will first start by proving a simpler proposition.

Proposition 3.
Consider a HyperMinHash sketch with only 1 bucket on two disjoint sets A and B. i.e., f0,q,r(A) and f0,q,r(B). Let γ(n,m)∼Z0,q,r(A,B,0). Naturally, as a good hash function results in uniform random variables, γ is only dependent on the cardinalities n and m. We claim that
Eγ(n,m)≤62r+n22q+r.(6)
View SourceRight-click on figure for MathML and additional features.

Proving this will require a few technical lemmas, which we will then use to prove the main theorems.

Lemma 4.
Eγ(n,m)=P(f0,q,r(A)0=f0,q,r(B)0)=∑i=12q−1∑j=02r−1[(1−2r+j2r+i)n−(1−2r+j+12r+i)n]⋅[(1−2r+j2r+i)m−(1−2r+j+12r+i)m]+∑i=2q2q∑j=02r−1[(1−j2r+i−1)n−(1−j+12r+i−1)n]⋅[(1−j2r+i−1)m−(1−j+12r+i−1)m].
View Source

Proof.
Let a1,…,an be random variables corresponding to the hashed values of items in A. Then ai∈[0,1] are uniform r.v. Similarly, b1,…,bm, drawn from hashed values of B are uniform [0,1] r.v. Let x=min{a1,…,an} and y=min{b1,…,bm}. Then we have probability density functions
pdf(x)=n(1−x)n−1, for x∈[0,1],pdf(y)=m(1−y)m−1, for y∈[0,1],
View SourceRight-click on figure for MathML and additional features.and cumulative density functions
cdf(x)=1−(1−x)n, for x∈[0,1],cdf(y)=1−(1−y)m, for y∈[0,1].
View SourceRight-click on figure for MathML and additional features.We are particularly interested in the joint probability density function
pdf(x,y)=n(1−x)n−1m(1−y)m−1, for (x,y)∈[0,1]2.
View SourceThe probability mass enclosed in a square along the diagonal S=[s1,s2]2⊂[0,1]2 is then precisely
μ(S)=∫s2s1∫s2s1n(1−x)n−1m(1−y)m−1dydx=[(1−s2)n−(1−s1)n][(1−s2)m−(1−s1)m].(7)
View SourceRight-click on figure for MathML and additional features.Recall f0,q,r(A)0∈{1,…,2q}×[0,1]r≡{1,…,2q}×{0,…,2r−1}, so given f0,q,r(A)0=(i,j), x=0.000i1j⋯ in the binary expansion, unless i=2q, in which case the binary expansion is x=0.000ij⋯. That in turn gives s1<x<s2, where s1=2r+j2r+i,s2=2r+j+12r+i when i<2q, and s1=j2r+i−1,s2=j+12r+i−1. Collisions happen precisely when s1<x,y<s2.

Finally, using the s1,s2 formulas above, it suffices to sum the probability of collision over the image of f, so
Eγ(n,m)=∑i=12q∑j=02r−1μ([s1,s2]).(8)
View SourceRight-click on figure for MathML and additional features.Substituting in for s1,s2, and μ completes the proof. Note also that this is precisely the sum of the probability mass in the red and purple squares along the diagonal in Fig. 4.

While Lemma 4 allows us to explicitly compute Eγ(m,n), the lack of a closed form solution makes reasoning about it difficult. Here, we will upper bound the expectation by integrating over four regions of the unit square that cover all the collision boxes (Fig. 5). For ease of notation, let τ¯=2r and q¯=22q.

The Top Right box TR=[τ¯τ¯+1,1]2 (in orange in Fig. 5).

Fig. 5. - 
We will upper bound the collision probability of HyperMinHash by dividing it into these four regions of integration: (a) the Top Right orange box, (b), the magenta ray covering intermediate boxes, (c) the black strip covering all but the final purple box, and (d) the final purple sub-bucket by the origin.
Fig. 5.
We will upper bound the collision probability of HyperMinHash by dividing it into these four regions of integration: (a) the Top Right orange box, (b), the magenta ray covering intermediate boxes, (c) the black strip covering all but the final purple box, and (d) the final purple sub-bucket by the origin.

Show All

The magenta triangle from the origin bounded by the lines y=τ¯τ¯+1x and y=τ¯+1τ¯x with 0<x<τ¯τ¯+1, which we will denote RAY.

The black strip near the origin covering all the purple boxes except the one on the origin, bounded by the lines y=x−1τ¯q¯, y=x+1τ¯q¯, and 1τ¯q¯<x<1q¯, which we will denote STRIP.

The Bottom Left purple box BL=[0,1τ¯q¯]2.

Lemma 5.
The probability mass contained in the top right square μ(TR)≤1τ¯.

Proof.
By Equation (7),
μ(TR)=∫1τ¯τ¯+1∫1τ¯τ¯+1n(1−x)n−1m(1−y)m−1dydx=[−(1−x)n]1τ¯τ¯+1[−(1−y)m]1τ¯τ¯+1=1(τ¯+1)n+m≤1τ¯.
View SourceRight-click on figure for MathML and additional features.

Lemma 6.
The probability mass contained in the bottom left square near the origin is μ(BL)≤nτ¯q¯.

Proof.
μ(BL)===∫1τ¯q¯0∫1τ¯q¯0n(1−x)n−1m(1−y)m−1dydx[−(1−x)n]1τ¯q¯0[−(1−y)m]1τ¯q¯0[1−(1−1τ¯q¯)n][1−(1−1τ¯q¯)m].
View SourceRight-click on figure for MathML and additional features.For n,m<τ¯q¯, we note that the linear binomial approximation is actually a strict upper bound (trivially verified through the Taylor expansion), so μ(BL)≤nmτ¯2q¯2≤nτ¯q¯ .

Lemma 7.
The probability mass of the ray from the origin can be bounded μ(RAY)≤3τ¯ .

Proof.
Unfortunately, the ray is not aligned to the axes, so we cannot integrate x and y separately.
μ(RAY)=∫τ¯τ¯+10∫τ¯+1τ¯xτ¯τ¯+1xn(1−x)n−1m(1−y)m−1dydx=∫τ¯τ¯+10n(1−x)n−1[(1−τ¯τ¯+1x)m−(1−τ¯+1τ¯x)m]dx.
View SourceUsing the elementary difference of powers formula, note that for 0≤α≤β≤1,
αm−βm=(α−β)(∑i=1mαm−iβi−1)≤(α−β)mβm−1.
View SourceRight-click on figure for MathML and additional features.With a bit of symbolic manipulation, we can conclude that
μ(RAY)≤∫τ¯τ¯+10n(1−x)n−1[2τ¯+1τ¯(l+1)xm(1−τ¯τ¯+1x)m−1]dx≤2τ¯+1τ¯(l+1)∫τ¯τ¯+10nm(1−τ¯τ¯+1)n+m−2xdx.
View Source

With a straight-forward integration by parts,
≤≤μ(RAY)−2τ¯+1τ¯2⋅nmn+m−1⋅τ¯τ¯+1(1−τ¯2(τ¯+1)2)n+m−1−2τ¯+1τ¯2⋅nmn+m−1⋅τ¯+1τ¯⋅1n+m(1−τ¯2(τ¯+1)2)n+m−1+2τ¯+1τ¯2⋅nmn+m−1⋅τ¯+1τ¯⋅1n+m(2τ¯+1)(τ¯+1)τ¯3⋅nm(n+m)(n+m−1)≤3τ¯.
View SourceRight-click on figure for MathML and additional features.

Lemma 8.
The probability mass of the diagonal strip near the origin is μ(STRIP)≤2τ¯ .

Proof.
Using the same integration procedure and difference of powers formula used in the proof of Lemma 7,
μ(STRIP)=∫1q¯1τ¯q¯∫x+1τ¯q¯x−1τ¯q¯n(1−x)n−1m(1−y)m−1dydx≤2τ¯q¯∫1q¯1τ¯q¯nm(1−x+1τ¯q¯)n+m−2=2τ¯q¯⋅nmn+m−1⋅τ¯−1τ¯q¯≤2τ¯.
View Source

Proof of Proposition 3.
Summing bounds from Lemmas 5, 6, 7, and 8, Eγ(n,m)≤6τ¯+nτ¯q¯=62r+n22q+r.

Proof of Theorem 1.
Let Ai, Bi be the ith partitions of A and B respectively. For ease of notation, let us define p¯=2p. Recall that C=∑p¯−1j=0Zp,q,r(A,B,j). We will first bound EZp,q,r(A,B,j) using the same techniques used in Proposition 3. Notice first that Zp,q,r(A,B,j) effectively rescales the minimum hash values from Z0,q,r(A,B,j)=γ(n,m) down by a factor of 2p; i.e., we scale down both the axes in Fig. 5 by substituting q¯←q¯p¯ in Lemmas 8 and 6. We do not need Lemma 5 because its box is already covered by the Magenta Ray from Lemma 7, which we do not scale. Summing these together, we readily conclude EZp,q,r(A,B,j)≤5τ¯+nτ¯q¯p¯=52r+n2p+2q+r . Then by linearity of expectation, EC≤2p[52r+n2p+2q+r].

Proof of Theorem 2.
By conditioning on the multinomial distribution, we can decompose C into
C=∑α1+⋯αp¯=nβ1+⋯βp¯=m11(∀i,|Ai|=αi∀i,|Bi|=βi)∑i=0p¯−1Zp,q,r(A,B,j∣∣∣|Ai|=αi|Bi|=βi).
View SourceRight-click on figure for MathML and additional features.

For ease of notation in the following, we will use α˚,β˚ to denote the event ∀i,|Ai|=αi and ∀i,|Bi|=βj respectively. Additionally, let Z˚(j)=Zp,q,r(A,B,j). So, C=∑α˚,β˚∑p¯−1j=011α˚,β˚Z˚(j∣∣α˚,β˚).

Then
Var(C)=∑α˚1,β˚1α˚2,β˚2∑j1,j2=0p¯−1Cov(11α˚1,β˚1Z˚(j1∣∣α˚1,β˚1),11α˚2,β˚2Z˚(j2∣∣α˚2,β˚2)).
View Source

But note that for (α˚1,β˚1)≠(α˚2,β˚2), 11α˚1,β˚1=1⇒11α˚2,β˚2=0 and vice versa, because they are disjoint indicator variables. As such, for (α˚1,β˚1)≠(α˚2,β˚2),
Cov(11α˚1,β˚1Z˚(j1∣∣α˚1,β˚1),11α˚2,β˚2Z˚(j2∣∣α˚2,β˚2))≤0,
View SourceRight-click on figure for MathML and additional features.implying that
Var(C)≤∑α˚,β˚∑j1,j2=0p¯−1Cov(11α˚,β˚Z˚(j1∣∣α˚,β˚),11α˚,β˚Z˚(j2∣∣α˚,β˚))=∑α˚,β˚∑j=0p¯−1Var(11α˚,β˚Z˚(j∣∣α˚,β˚))+∑α˚,β˚∑j1≠j20≤j1≤p¯−10≤j2≤p¯−1Cov(11α˚,β˚Z˚(j1∣∣α˚,β˚),11α˚,β˚Z˚(j2∣∣α˚,β˚)).
View SourceRight-click on figure for MathML and additional features.

Note that the first term can be simplified, recalling that Z˚ is a {0,1} Bernouli r.v., so
∑j=0p¯−1Var(11α˚,β˚Z˚(j∣∣α˚,β˚))=∑j=0p¯−1Var(Z˚(j))≤∑j=0p¯−1E[Z˚(j)]=EC.
View SourceRight-click on figure for MathML and additional features.

Moving on, from the covariance formula, for independent random variables X1,X2,Y,
Cov(X1Y,X2Y)=E[X1X2Y2]−E[X1Y]E[X2Y]=E[X1]E[X2](E[Y2]−E[Y]2)=E[X1]E[X2]Var(Y).
View SourceThus the second term of the summation can be bounded as follows:
∑α˚,β˚∑j1≠j20≤j1≤p¯−10≤j2≤p¯−1Cov(11α˚,β˚Z˚(j1∣∣α˚,β˚),11α˚,β˚Z˚(j2∣∣α˚,β˚))=∑α˚,β˚∑j1≠j20≤j1≤p¯−10≤j2≤p¯−1E[Z˚(j1∣∣α˚,β˚)]E[Z˚(j2∣∣α˚,β˚)]Var(11α˚,β˚)≤∑α˚,β˚∑j1=0p¯−1∑j2=0p¯−1E[Z˚(j1∣∣α˚,β˚)]E[Z˚(j2∣∣α˚,β˚)]Var(11α˚,β˚)=∑α˚,β˚E[C|α˚,β˚]2Var(11α˚,β˚)=∑α˚,β˚E[C|α˚,β˚]2P(α˚,β˚)(1−P(α˚,β˚))≤∑α˚,β˚E[C|α˚,β˚]2P(α˚,β˚)=E[C]2
View SourceRight-click on figure for MathML and additional features.

We conclude that Var(C)≤E[C]2+E[C] .

2.4 Space-Complexity Analysis
Note that Theorem 2 implies that the standard deviation of the number of collisions is about the same as the number of collisions itself, as bounded in Theorem 1. For Jaccard index t=J(A,B), the absolute error caused by minimum hash collisions is then approximately
EC2p=52r+n2p+2q+r.
View SourceRight-click on figure for MathML and additional features.So long as n<22q, the second term is bounded by 1/2r. Then, the absolute error in the Jaccard index estimation from collisions is bounded by 6/2r, where the constant 6 is a gross overestimate (empirically, the constant seems closer to 1). For any desired absolute error ϵ, we then need only let
r>log6ϵ.
View SourceRight-click on figure for MathML and additional features.Of course, for error ϵ, this then implies that HyperMinHash needs O(loglogn+log(1/ϵ)) bits per bucket. Additionally, note that ϵ−2 buckets are needed to control the error, completing our proof that HyperMinHash requires O(ϵ−2(loglogn+log1ϵ)) bits of space.

An astute reader will note that in this space-complexity analysis, we have implicitly assumed that there are no jointly empty buckets in the HyperMinHash sketches of the two sets. This assumption allows us to use the simpler Jaccard index estimator
t(A,B)≈[# matches][# buckets].
View SourceRight-click on figure for MathML and additional features.rather than the harder to analyze estimator
t(A,B)≈[# matches][# buckets] - [# jointly empty buckets].
View SourceRight-click on figure for MathML and additional features.In the small set regime where there are empty buckets, although our proofs of the number of additional accidental collisions from using HyperMinHash compression still hold, the relative error in the Jaccard index estimator can be higher because there are fewer filled buckets. However, we note that this does not change the asymptotic results in the large cardinality regime, which we believe of greater interest to practitioners who are considering using HyperMinHash for space-savings (i.e., in the small cardinality regime, double logarithmic scaling is of little importance).

SECTION 3Results
Now that we have presented both detailed algorithms and a theoretical analysis of HyperMinHash, for completeness, we turn to simulated empirical experiments comparing against MinHash. Given two sets of interest, we first generate one-permutation k-partition MinHash sketches of both. As detailed in the introduction, this consists of hashing all elements within a set, partitioning them into k buckets, and storing the minimum valued hash within each bucket. Then, we estimate the Jaccard index by dividing the number of matching buckets by the number of buckets that are not empty in both sketches. Using HyperMinHash proceeds identically, except that we then compress the minimum valued hash by storing the position of the leading 1 indicator and several bits following it; Jaccard index estimation is identical, except of course that the buckets must match in both the leading 1 indicator position and the additional bits (Fig. 1).

In Fig. 6, we allocate 64 bytes for two standard MinHash sketches and a HyperMinHash sketch, and then plot the mean relative error in the Jaccard index for simulated overlapping sets with Jaccard index 0.1, 0.33, 0.5, and 0.9. Note that for the sake of comparability, we turn off the expected error collision correction of HyperMinHash. A Python implementation is available on Github (https://github.com/yunwilliamyu/hyperminhash) for rerunning all experiments and regenerating the figure.

Fig. 6. - 
For a fixed size sketch, HyperMinHash has better accuracy and/or cardinality range than MinHash. We compare Jaccard index estimation for identically sized sets with Jaccard index of 0.1, $1/3$1/3, 0.5, and 0.9, and plot the mean relative errors without estimated collision correction. All datasets represent between 8 and 96 random repetitions, as needed to get reasonable 95 percent confidence interval (shaded bands). (blue) A 64 byte HyperMinHash sketch, with 64 buckets of 8 bits each, 4 bits of which are allocated to the LogLog counter. Jaccard index estimation remains stable until cardinalities around $2^{21}$221. (orange) A 64 byte MinHash sketch with 64 buckets of 8 bits each achieves similar accuracy at low cardinalities, but fails once cardinalities approach $2^{13}$213. (green) A 64 byte MinHash sketch with 32 buckets of 16 bits can access larger cardinalities of around $2^{19}$219, but to do so trades off on low-cardinality accuracy.
Fig. 6.
For a fixed size sketch, HyperMinHash has better accuracy and/or cardinality range than MinHash. We compare Jaccard index estimation for identically sized sets with Jaccard index of 0.1, 1/3, 0.5, and 0.9, and plot the mean relative errors without estimated collision correction. All datasets represent between 8 and 96 random repetitions, as needed to get reasonable 95 percent confidence interval (shaded bands). (blue) A 64 byte HyperMinHash sketch, with 64 buckets of 8 bits each, 4 bits of which are allocated to the LogLog counter. Jaccard index estimation remains stable until cardinalities around 221. (orange) A 64 byte MinHash sketch with 64 buckets of 8 bits each achieves similar accuracy at low cardinalities, but fails once cardinalities approach 213. (green) A 64 byte MinHash sketch with 32 buckets of 16 bits can access larger cardinalities of around 219, but to do so trades off on low-cardinality accuracy.

Show All

While changing the Jaccard index of the sets changes the baseline mean relative error and the maximum possible mean relative error, the errors for all three methods remain basically stable across the cardinality ranges that they can handle without hash collisions. We ran HyperMinHash with 64 buckets of 8 bits, with 4 allocated to the leading 1 indicator. This sketch was able to access cardinalities ranging up to 221. When running MinHash with 64 buckets of 8 bits, it maintains the same error level as HyperMinHash for union cardinalities below 213, but then fails. Using fewer buckets with larger numbers of bits, such as when we ran MinHash with 32 buckets of 16 bits, allows it to access higher cardinalities (around 219), but at the cost of a higher baseline error level. Thus, for fixed sketch size and cardinality range, HyperMinHash is more accurate; or, for fixed sketch size and bucket number, HyperMinHash can access exponentially larger set cardinalities.

SECTION 4Conclusion
We have introduced HyperMinHash, a compressed version of the MinHash sketch in loglog space, and made available a prototype Python implementation at https://github.com/yunwilliamyu/hyperminhash. It can be thought of as a compression scheme for MinHash that reduces the number of bits per bucket to loglog(n) from log(n) by using insights from HyperLogLog and k-partition MinHash. As with the original MinHash, it retains variance on the order of k/t, where k is the number of buckets and t is the Jaccard index between two sets. However, it also introduces 1/l2 variance, where l=2r, because of the increased number of collisions.

Alternately, it can be thought of as an extension for sublogarithmic Jaccard index fingerprinting methods such as b-bit MinHash [10], adding back in the features of streaming updates and union sketching. Should a practitioner desire only to create a one-time Jaccard index fingerprint of a set, we recommend they use b-bit MinHash; however, we believe that HyperMinHash serves as a better all-around drop-in replacement of MinHash because it preserves more of MinHash's compositional features than other more space-efficient Jaccard index fingerprints.

There remain many theoretical improvements to be made, especially in removing the requirement of a random oracle. Notably, the KNW sketch [8] sketch improves HyperLogLog to using constant size buckets and a double logarithmic offset, while also using limited randomness. We envision that their techniques can be extended to HyperMinHash to further reduce space complexity, as the HyperLogLog parts of the buckets are identical to regular HyperLogLog. Similarly, Feigenblat, et al. introduce d-k-min-wise hash functions to reduce the amount of necessary randomness for a bottom-k MinHash sketch [15]. Although we have analyzed HyperMinHash as a k-partition variant, the same floating-point encoding can be applied to bottom-k sketches, with similar error behavior.

Luckily, even without these theoretical improvements, HyperMinHash is already practically applicable. For reasonable parameters of p=15,q=6,r=10, the HyperMinHash sketch will use up 2MiB memory per set, and allow for estimating Jaccard indices of 0.01 for set cardinalities on the order of 1019 with accuracy around 10 percent. HyperMinHash is to our knowledge the first practical streaming summary sketch capable of directly estimating union cardinality, Jaccard index, and intersection cardinality in loglog space, able to be applied to arbitrary Boolean formulas in conjunctive normal form with error rates bounded by the final result size. We hope that HyperMinHash as presented in this manuscript will be of utility for Boolean queries on large databases.