We are witnessing a fast development of Artificial Intelligence (AI), but it becomes dramatically challenging to explain AI models in the past decade. “Explanation” has a flexible philosophical concept of “satisfying the subjective curiosity for causal information”, driving a wide spectrum of methods being invented and/or adapted from many aspects and communities, including machine learning, visual analytics, human-computer interaction and so on. Nevertheless, from the view-point of data and knowledge engineering (DKE), a best explaining practice that is cost-effective in terms of extra intelligence acquisition should exploit the causal information and explaining scenarios which is hidden richly in the data itself. In the past several years, there are plenty of works contributing in this line but there is a lack of a clear taxonomy and systematic review of the current effort. To this end, we propose this survey, reviewing and taxonomizing existing efforts from the view-point of DKE, summarizing their contribution, technical essence and comparative characteristics. Specifically, we categorize methods into data-driven methods where explanation comes from the task-related data, and knowledge-aware methods where extraneous knowledge is incorporated. Furthermore, in the light of practice, we provide survey of state-of-art evaluation metrics and deployed explanation applications in industrial practice.
SECTION 1Introduction
Artificial Intelligence (AI) technology, empowered by machine learning, has shown its revolutionary power in numerous application domains. In computer vision (CV), deep learning (DL) based methods have outperformed traditional methods and can even beat human beings in related open challenges (e.g., ImageNet image-classification challenge [1], COCO object-detection challenge [2]) since the birth of AlexNet [3]. Similar to CV, DL methods have also achieved significant success in various natural language tasks such as visual question answering [4] and machine translation [5]. In strategy game playing such as Go, an artificial intelligent player–AlphaGo [6] defeated the human world champion Lee Sedol in 2016. Since then, deep reinforcement learning (RL) based computer programs have prevailed human champion players in various games including Texas hold'em poker [7] and Dota 2 [8].

However, the aforementioned AI models are mostly built on extremely complex non-linear functions, e.g., deep neural networks (DNNs), and usually contain millions of parameters [3], [9], [10], [11]. The high non-linearity and complexity make these models black-boxes, i.e., it is difficult for humans to understand their internal working mechanisms and decision making process. Such intransparency could raise severe issues and hinder further applications of AI. To begin with, it is difficult to justify the decision making of a black-box model. For example, passengers can feel highly insecure in the self-driving system when the predictions are not self-explanatory, e.g., the vehicle suddenly turns right at an intersection where it normally goes straight without explanation. Second, it is hard to control and avoid unexpected behavior of black-box models. As a center issue of AI safety, DNNs have been shown to be fragile and easily attacked by adversarial perturbation [12]. Even without adversarial attacking, the black-box models may make wrong decisions due to potential biases and prejudices in training data. In critical scenarios, e.g., medical diagnosis, the consequences could be catastrophic or even life-threatening. To tackle these problems, explainability of AI models, i.e., the ability to summarize the reasons for model behavior, or gain insights into their working mechanisms, is highly demanded. Revisiting the example of self-driving, if the vehicle turns right with an explanation such as “There is a car accident 300 m in front of us. Turning right will take us 5 mins more than usual but going straight will take at least 30 mins more.”, the passengers will trust and be satisfied with the automated decision. Recognizing the necessity of XAI, the European Parliament adopted the General Data Protection Regulation (GDPR) which confers a right of explanation for all individuals to obtain ”meaningful explanations of the logic involved” for automated decision making.

As a result, XAI has emerged as an important and popular research direction among academia in the past few years. An exponentially increasing number of scientific works have been conducted to push forward this branch of research [13]. Based on their focus on the phases of life-cycle of XAI, these works can be grouped into three lines: (i) Methodology–developing more superior methods to explain black-box models; (ii) Evaluation–designing the methods or defining metric on evaluating the effectiveness of the explanations; (iii) Application–applying XAI in actual tasks. In line (i), some methods attempt to provide explanations through learning interpretable surrogate models to mimic the input-output relation of the black-box model, e.g., Local Interpretable Model-Agnostic Explanations (LIME) [14] and knowledge distillation [15]. Meanwhile, there are methods aiming to explain the black-box model by attributing the input features or intermediate features, e.g., Gradient-weighted Class Activation Mapping (Grad-CAM) [16], SHapley Additive exPlanations (SHAP) [17]. Works in line (ii) aim to evaluate the effectiveness of explanations generated by these methods. Human evaluation is a typical way to achieve this, through designing questionnaire delicately for users and collecting their answers [18]. As for line (iii), most of the works focus on applying XAI in specific domains, e.g., self-driving [19]. Meanwhile, a few works attempt to discuss the potential general application of XAI such as model debugging [20].

To summarize and systematically review these works, plenty of surveys are conducted during the past few years. Most of these surveys focus on classifying the XAI methods from different possible dimensions [20], [21], [22], [23], [24], [25], [26], [27], [28]. Meanwhile, some of them review the different evaluation metrics [26] and potential applications of XAI [20]. However, there are very few surveys covering the progress of XAI from all the three perspectives including methodology, evaluation and application. In addition, the existing methodology surveys mainly discuss data-driven methods (e.g., LIME, Grad-CAM), i.e., explanations are purely inducted or generated from datasets or their extensions, while the increasingly popular knowledge-aware methods [29], [30], [31] in which external knowledge is also applied to generate explanations are uncovered. To fill in these gaps and update the progress of XAI, we put forward this survey with the following contributions:

The progress of all the three phases in life-circle of XAI is covered–from methodology to evaluation and then to application.

A new hierarchical taxonomy is introduced to review the existing XAI methods based on whether external knowledge is involved.

In particular, a systematic review on the knowledge-aware methods is provided.

This survey is organized as the following: data-driven methods will be first discussed in Section 3. Then knowledge-aware methods will be reviewed in Section 4. The evaluation methods of the explanations will be covered in Section 5, followed by the review on application of XAI method in different domains in Section 6. The contents of this survey are concluded in Sections 7 and in 8 we shall discuss the future of XAI from our perspective.

SECTION 2Related Work
With the explosive growth of XAI in recent years [21], multiple related surveys and tutorials have been conducted to review the research progress in this field. To give a glance at the existing surveys, we compare them from the following perspectives:

Domain. While the majority of the surveys provide a general overview of XAI and a full-scale summarization and comparison of XAI methods [20], [21], [22], [23], [24], [25], [26], [27], [28], a few of them customize the survey on particular aspects, e.g., [24], [32] focus on the methods providing visual explanation. There are also surveys focusing on the methods of specific domains, e.g., CV [21], [28], medical intelligence [33].

Methodology. Doshi et al. [26] introduce factors of interpretability from disparate tasks or methods, which could be taxonomies: explanation scope (global versus local), the nature of user expertise, the explanation form, etc. Following this idea, a majority of surveys review the XAI methods based on one or more of the aforementioned taxonomies, e.g., explanations scope [21], [22], [25], [32], [33], complexity [21], human perspective [33], the stage of training life cycle [32], or applicable fields [28].

Evaluation. Although evaluation on XAI is of vital importance, there are only a few of the surveys review the evaluation methods [21]. [26] categorizes the evaluation methods into three types based on human and tasks, i.e., application-grounded, human-grounded and functionally-grounded evaluation. [28] also proposes some attributes (eg. comprehensibility, succinctness, completeness) that could be important indicators in the evaluation process.

Application. The application of XAI is still in a premature stage, yet, there are several surveys [20], [25], [28], [33] presenting toy examples in different social scenarios, e.g., transportation, sales, finance, human resource, and health. [25] also mention the software that uses one or more XAI methods. Specially, [20] reviews XAI applications in data science field: model validation, model debugging, and knowledge discovery.

SECTION 3Data-Driven eXplainable AI
Data-driven eXplanation AI refers to the methods that generate explanations purely from data, without external information such as prior knowledge. To provide explanations, data-driven methods often start from selecting a dataset (with global or local distribution). Then the selected dataset or its variation is input to the black-box model (in certain cases, e.g., activation maximization in Section 3.1.2, a selected dataset is not necessary to get an insightful output). With certain analyses on the corresponding predictions of black-box model (e.g., taking derivative on the predictions w.r.t. input features), human-friendly explanations are generated. These methods can be further categorized into global or local ones according to the scope of explainability, namely whether they explain global model behavior for all data points or a subset of predictions. The explanations from the data-driven methods could be in various forms, e.g., features importance, decision rules. In particular, instance-based methods provide a special type of explanations–they return data instances directly as explanations. Although from the taxonomy of interpretation scope, instance-based methods could also fit into global methods (prototype and criticism) or local methods (counterfactual), we list them separately to emphasize their special ways to provide explanations. In the rest of this section, we review the progress of data-driven explanation methods in the past decades. Specifically, we discuss (i) Global methods in Section 3.1; (ii) Local methods in Section 3.2 and (iii) Instance-based methods in Section 3.3.

3.1 Global Methods
Global methods aim to provide understandings of the logic of a model and the complete reasoning for all of the predictions, based on a holistic view of its features, learned components and structures, etc. There are several directions to explore global explainability. For the sake of readability, we distinguish them into the following three subclasses: (i) Model Extraction–to extract an interpretable model from the original black-box model; (ii) Feature-based Methods–to estimate the importance or the relevance of a feature; (iii) Transparent Model Design–to modify or redesign a black-box model to improve its interpretability.

3.1.1 Model Extraction
The basic idea of model extraction is to learn an interpretable model (also known as global surrogate model) to mimic the behaviors of black-box model. To generate this surrogate model, one can follow the steps below:

Select a dataset X. This can be the training dataset of the black-box model or a new dataset with the same distribution.

For the selected dataset X, perform inference of black-box model and get the corresponding predictions.

Select an interpretable model type and construct the desired model.

Train the interpretable model on dataset X to fit predictions from the black-box model.

In the following, we will investigate the development of model extraction algorithms during the past decades.

Tree Extraction. Decision tree is one of the most widely used models for model extraction as it is usually considered interpretable and simple. The pioneering work of tree extraction can be traced back to TREPAN [34]. TREPAN induces a decision by querying the neural network and approximating the output of networks by maximizing the gain ratio [59]. With the black-box model, TREPAN can produce a desired amount of instances for each split, so that the node splits near to the bottom of the tree are also realized using a considerable amount of data. This makes it superior to the decision tree directly trained from original training data. However, when the number of nodes or the size of a decision tree gets large, it becomes human-unfriendly to interpret. To generate a more understandable explanation, Krishnan et al. present a method to extract smaller trees [60]. This method first generates prototypes for each target class and then selects the best prototypes to induce the interpretable decision tree. Boz proposes Decision Tree extractor [61], providing four methods to locate the most relevant features during the decision tree construction. Furthermore, this work defines a fidelity-based pruning strategy to reduce tree size. In [62], the authors propose to extract neural networks with evolving decision trees with genetic programming. Very recently, Renard et al. introduce Concept Tree for tree extraction, with the use of concept which can be grouped automatically or with expert knowledge [63]. The key difference between Concept Tree and TREPAN lies in the learning of the decision rule at each node. At a given node, the Concept Tree fits a decision rule for each concept using only the variables related to the concept and selects the one that yields the best information gain. Each node thus splits the sample on a decision rule based on a concept, using related variables only.

We note here that although the algorithms mentioned above are developed based on neural networks, they are model-agnostic as no specific model structure of the black-box model is assumed during the learning process.

Rule Extraction. Another commonly used interpretable and easily understandable model is the set of rules. Back in 1994, Craven et al. present a method to extract rules from neural networks by casting the rule search problem into a learning problem [64]. An empty set of rules is first initialized. The original training data and its randomized extension are then provided to the black-box. If the input x with output y^ is not covered by the set of rules, then a conjunctive rule is formed from {x,y^} considering all the possible antecedents and update the set of rules. The procedure is conducted iteratively until all the target classes have been processed. Not far from the proposal of [34], a survey provides categorizations on the rule extraction based on the various criteria [65]. However, the methods reviewed in [65] are strongly dependent on the black-box model as well as the type of rules and thus are not generalizable. In [66], Johansson et al. exploit a generalizable algorithm named G-REX for rule extraction with genetic programming. In [67], the authors further extend G-REX to handle regression problems by generating regression trees, and classification problems by generating fuzzy rules. In [68], Zhou et al. perform Rule Extraction From Neural network Ensemble (REFNE) based on generated instances. One key feature of REFNE is that it avoids useless discretizations on the continuous attributes through adaptive intervals. In [69], Augasta et al. perform Rule extraction by Reverse Engineering the Neural networks (RxREN) for classification problems. In [35], the authors propose Deep neural network Rule Extraction via Decision tree induction (DeepRED). Based on Continuous/discrete Rule Extractor via Decision tree induction (CRED) [70] which extracts continuous or discrete rules from neural networks with one hidden layer, DeepRED extends it to an arbitrary number of layers by applying CRED layer by layer.

Knowledge Distillation. The works above can be viewed as explainer specific models as the methods are developed based on the extracted model. Rather than developing algorithms for extracting a certain model, knowledge distillation is introduced by Hinton et al. as a unified method for model extraction [15]. The basic idea is to train a simpler student model to mimic the complex teacher model by optimizing the loss function below
L=−∑i=1kgilogqi(T=1)−∑i=1kpi(T=t)logqi(T=t),(1)
View SourceRight-click on figure for MathML and additional features.where gi is the ground truth label for class i, qi(T)=exp(zi/T)∑jexp(zj/T) is the output soft label (when T>1) or hard label (when T=1) of the student model, and pi(T=t>1) is the soft label of the teacher model. Here T is a hyperparameter to soften the distribution of labels. With the distillation loss on the soft label, the ”dark knowledge” can be learned by the student model, which makes the student model superior to the same model learned from scratch.

Although the original knowledge distillation paper [15] does not aim to train an interpretable model, the distillation idea has become an effective method for interpretable model extraction with favorable fidelity. In [36], a DNN is distilled to a soft-decision tree where the splitting of a node is parametrized and thus can be trained by backpropagation. In [71], a DNN is further distilled to a vanilla decision tree by formulating the problem as a multi-output regression problem. Tan et al. apply knowledge distillation to extract transparent iGAMs [72]. In a later work from the same group, knowledge distillation is applied to extract global additive explanation in the form of feature shapes [73].

3.1.2 Feature-Based Methods
Providing global explanations through an extracted model is an elegant and neat solution and has been popular throughout the community. However, such extraction is often based on a simplification of the model complexity and leading to a compromise on the accuracy of the original model. To provide explanations meanwhile maintaining the accuracy, methods focusing on analyzing the influence or the importance of input features are proposed. Here we review two alternative paths of works to measure input feature's relevance: (i) Feature Importance–a measure on the importance of each feature towards the model output; and (ii) Activation Maximization–preferred input generation for the output.

Feature Importance. Feature importance measures the contributions of each feature to the final prediction, either on a local or a global level. In this section, we focus on the works that provide global explanations. The most prominent approach is the permutation feature importance (PFI) originally introduced by Breiman [74] for random forests. The performance, e.g., the mean squared error of each tree is computed once with and once without permuted values of the feature of interest. The average difference between these two performances among the trees is feature importance. If the permutation causes a large drop in performance, the feature is considered to be important. Fisher et al. later extend the idea to a model-agnostic version [37], in which the PFI of features in subset S is defined as
PFIS=E(P(f^(X~S,XC),Y))−E(P(f^(X~),Y)),(2)
View SourceRight-click on figure for MathML and additional features.where X~S refers to a permuted replication of XS, XC is the feature matrix without subset S, and P(f^,Y) is the performance measure on f^. This implies that X~s is a new (multivariate) random variable, which is distributed as XS, but independent of everything else. The more drop in the performance when we permute feature values in S, the larger the value of PFIs is, the more important the feature set S is. The PFI measure ignores features in S by permuting or marginalizing over them, which destroys any correlation and interaction of features in S with features in C. Consequently, the PFI of a feature will also include the unexpected effect of correlated interaction between this feature and other features. To overcome this issue, the authors in [75] propose a permutation-based Shapley feature importance. It considers the marginal contribution of a feature and equally distributes the importance of interactions among the interacting features harnessing the idea of Shapley value calculation.

Activation Maximization. Feature importance focuses on measuring the importance of input variables towards the black-box model. Starting from another perspective, activation maximization tries to shed light on what features the black-box model has learned by searching an input pattern that produces a maximum model response [76]. The formulation of this problem is defined as
x^=argmaxx(hij(x;θ)),(3)
View SourceRight-click on figure for MathML and additional features.where hij represents the activation of the ith node in jth layer, θ is the parameter of the network. However, the images produced from this approach are usually unrealistic and uninterpretable, because the set of images satisfying Eq. (3) is vast that it is possible to produce “fooling” images without resemblance to the natural images from the training set. To overcome this issue, [77], [78] incorporate natural image priors into the objective function. In these works, the image priors such as Gaussian blur [79], α-norm [43], [78], total variation [77], jitter [80], [81], data-driven patch priors [78], are hand-designed. To automatically project image priors into the learning process, Nguyen et al. propose to use learned natural image prior akin to generative model [38]. The authors harness the generator network trained from generative adversarial network (GAN) [82] as images priors. The network is called deep generative network activation maximization (DGN-AM) and is shown in Fig. 1. Provided the image generator model G, the goal is to find a code y such that G(y) is an image that produces high activation of the target neuron in the DNN Φ. Thus the formulation for activation maximization can be posed as finding a code y^l at lth layer such that
y^l=argmaxyl(Φh(Gl(yl))−λ||yl||).(4)
View SourceRight-click on figure for MathML and additional features.Here Gl is a generator network trained to reconstruct images from the lth layer features, λ is the L2 regularization penalty parameter. After optimizing Eq. (4), the synthesis image is obtained through Gl(y^l), as shown in Fig. 2. One can see that the key features for these classes are captured by the network.

Fig. 1. - 
DGN-AM architecture proposed in [38] to synthesize a preferred input for a target neuron $h$h. The hidden code input of a deep image generator network(DGN) is optimized to produce an image that highly activated $h$h.
Fig. 1.
DGN-AM architecture proposed in [38] to synthesize a preferred input for a target neuron h. The hidden code input of a deep image generator network(DGN) is optimized to produce an image that highly activated h.

Show All

Fig. 2. - 
Synthesis images generated from DGN-AM for an AlexNet DNN trained on the MIT Places dataset [38].
Fig. 2.
Synthesis images generated from DGN-AM for an AlexNet DNN trained on the MIT Places dataset [38].

Show All

There are also other global feature-based approaches through visualization. To name a few, Partial Dependence Plots (PDPs) [83] visualize the marginal impact of one or two features have on the prediction. These plots are especially helpful in displaying the non-linearities in the underlying black-box model. However, PDP have a serious problem when there is strong correlation between features. To work around this issue, Accumulated Local Effects (ALE) Plot is proposed [84], which calculate differences in predictions instead of averages based on the conditional distribution of the features.

3.1.3 Transparent Model Design
Beyond passive probing or diagnosing the explainability of the black-box model, there are also works aiming to improve the interpretability by modifying or redesigning the models. In [39], the authors propose to modify the traditional convolutional neural network (CNN) by introducing part templates as masks on top of traditional CNN filters. In addition to the cross-entropy, the minus mutual information between feature maps and the part templates is introduced into the loss function as the following:
Lf=−MI(X;T)=−∑Tμip(Tμi)∑xp(x|Tμi)logp(x|Tμi)p(x),(5)
View SourceRight-click on figure for MathML and additional features.where x∈X is the feature map after rectified linear unit (ReLU) activation and Tμi∈T is the designed part template. Different templates indicate different activation locations in the feature map. Regularized by this loss function, the feature map is expected to fit the part templates distribution. In other words, given a specific part as input, the filter needs to activate a single location on the feature map. Thus each filter in the conv-layer is semantically related to a specific high-level part.

In [40], Wu et al. propose an interpretable region-based convolutional neural network (R-CNN) for object detection by unfolding latent configurations of object parts automatically. The authors use an “And-Or graph (AOG)”, a top-down hierarchical and compositional grammar to model latent configurations. The RoI-Pooling operator used in R-CNN is replaced by an AOG-based parsing operator while maintaining the discrimination power of an R-CNN. During the detection process, a bounding box is interpreted as the best parse tree derived from AOG. In [85], the authors propose to extract visual concepts by clustering after pooling layer in DNNs and then use these visual concepts as building blocks for compositional models. These compositional models thus provide more explicit representations of parts and lead to more robust results toward occlusion.

3.2 Local Methods
While global methods provide a transparent view of black-box models, both constructing faithful approximation of large-scale complex model and extracting the general saliency pattern from the whole input distribution remain challenging in practice. Global explanation also lacks local fidelity when generating explanation for the single observation, since globally important features may not precisely account for the decision of a single instance [14].

Instead of giving a complete description of opaque mechanism inside the black-box model, local methods attempt to justify the model behavior for a single instance or a set of instances. Local behavior of complex models may be correlated to fewer features through simple relation (e.g., linearity), and thus relieve the challenges to explain the black-box models. Simple functions also help to provide faithful explanations within local regions. Based on the procedures to obtain explanations, we categorize local methods into two classes: Local approximation and Propagation-based methods. Section 3.2.1 introduces local approximation which generates comprehensible sub-models to simulate the behaviors of black-box model locally. Methods using propagation to attribute contribution to input features are described in Section 3.2.2.

3.2.1 Local Approximation
While representing the holistic model with an explainable proxy relies on the entire dataset and numerous parameters as well, local approximation makes use of simple proxy models (i.e., linear models and decision trees) to interpret the black-box models within a limited boundary. In other words, local approximation simulates the local behavior of the complex model by training with a sub-dataset related to the given instance. LIME [14] provides a prevalent pattern to generate local approximations. Given an instance and corresponding decision made by the black-box model, a sub-dataset is formed by aggregating perturbations around the instance. With this sub-dataset, the black-box model can be approximated by a proxy model through the following:
ξ(x)=argming∈GL(f,g,πx)+Ω(g),(6)
View SourceRight-click on figure for MathML and additional features.where f is the function of black-box model and g∈G is the approximation function with G as the set of model candidates. L is the loss function measuring the local difference between the approximation and black-box model. Since perturbation points are randomly sampled around the instance, in order to mitigate the impact of sampling noise (samples far from instance and unfaithful to the local mechanism), different perturbations are weighted by πx in term of the distances from the reference. Regularization Ω(g) is applied to restrict the growing complexity of the approximation function. There is no specific assumption for the form of f and the data type in Eq. (6), thus LIME is a model-agnostic and data-type-agnostic explaining method [14]. While LIME is able to generate the explanations for black-box models in a local region, it is challenging to apply LIME to imitate regions involving high non-linearity. Besides, the region with effective fidelity is also unclear [86]. To resolve these problems, Ribeiro et al.[86] propose Anchors which extends the framework of LIME by introducing precise rules. They replace the linear models by several rules to precisely locate the effective region for local explanations.

Inheriting the proposals from LIME and Anchors, Guidotti et al. [41] propose Local Rule-based Explanations (LORE), a pair-rule local explanation including a single rule and several counterfactual rules to describe local behaviors. The pair of rules are extracted from the paths of a decision tree, built from a given instance and its neighbor [41]. In addition to local explanations in LIME and Anchors, they put further attention on the counterfactual rules, indicating the edge situation where the input change could invert the decision. As sparse linear models lack expressive ability to simulate complex models, other tree-based local explaining methods gain more attention to mitigating the problem recently [87], [88].

While the methods above are model-agnostic, the feasibility of these local approximations on Graph neural network (GNN) receives little attention. Ying et al. [89] leverage the mutual information to propose an explanation for GNN through sub-graph with related nodes and edges, which to the best of our knowledge, is the first investigation of local approximation of GNN.

3.2.2 Propagation-Based Methods
Local approximation introduces an additional model to imitate complex model locally, but the behavior in the local region can be complicated and need a vast of parameters for faithfulness, leading to uninterpretable proxy models. Instead of inducing additional approximation, researchers directly locate the relevant features through propagation-based methods which encapsulate backpropagation-based method and forward propagation-based methods. Backpropagation-based method attributes the contribution from the output to the input feature, while forward propagation-based method quantifies the relevance through output difference after perturbing the feature.

Backpropagation-Based. A basic proposal of the backpropagation-based method is to take derivatives of the output w.r.t the input features and generate a map of importance (called saliency map in some studies) with the gradients [42], [43]. Features with higher gradients are more crucial to the model as perturbations on such features can lead to more change in the predictions. However, gradient-based attribution methods suffer from noisy and meaningless results caused by sharp variance of the black-box function [90]. To tackle the problem, Smilkov et al. [90] present SmoothGrad which attains a sharper sensitivity map by getting smoothing from perturbations of inputs.

Explanations are also provided through interpretations of feature maps. Zeiler and Fergus [44] retrieve a projection of the specific activation in the feature map through deconvolution. For each layer in the CNN, reconstruction of the activation is observed by unpooling, ReLU and deconvolution [44]. As an extension of deconvolution, Guided Backpropagation reconstructs sharper visualization by filtering out the negative forward activation and backward gradients [91]. Taking the idea from Class Activation Map (CAM) [92] which attains the localization information by combining the feature maps weighted by global average pooling, Selvaraju et al. [16] generate the localization map from the combination of weighted feature map by considering the derivative w.r.t. last convolution layer as the weights. An interpretable and intuitive feature-importance map is further obtained through Guided Prop [91].

A crucial problem of the methods above is that much attention is put on the local tendency by taking the derivative w.r.t the target point rather than a sensible point digging out the internal contribution of input features. As shown in Fig. 3, the explanation of the instance is misled by local gradient pointing to the local optima instead of the locations on decision boundary. An effective solution to the problem is to select a reference node on the decision boundary and take the difference between the given instance and reference into account, as introduced in Layer-wise Relevance Propagation (LRP) [93]. Extending the gradient-based sensitivity visualization with reference point, Sundararajan et al. [45] obtain the saliency map by integrating the gradients along the path from the reference point to the observation. They argue that Integrated Gradients satisfies both Sensitivity and Implement Invariance which is broken by the aforementioned methods [42], [43], [44], [93]. Following similar idea, Shrikumar et al. [94] introduce Deep Learning Important FeaTures (DeepLIFT) to assign both positive and negative contributions based on the activation differences between the target input and a reference input.

Fig. 3. - 
A prediction function for classification with the dashed line separating the blue points and the green points [93]. The gradient of the squared point $x$x does not point to the nearest location on the decision boundary. Instead, it is misled to the local optima which might be irrelevant to the decision making.
Fig. 3.
A prediction function for classification with the dashed line separating the blue points and the green points [93]. The gradient of the squared point x does not point to the nearest location on the decision boundary. Instead, it is misled to the local optima which might be irrelevant to the decision making.

Show All

As a majority of local explanations pay attention to the presence of important features in the input, Dhurandhar et al. [95] argue the absence of some features also matters. As shown in Fig. 4, pertinent positive (PP) features highlighted in cyan, are necessary for the “3” to be recognized. Meanwhile, the pertinent negative (PN) features highlighted in pink, should be absence since the horizontal line beside “3” supports the alternative prediction of “5”. Ignored by the aforementioned methods, PN features are the complement evidence for the black-box model to confirm their prediction. These absent features also matter when exploring the counterfactual instance for explanations, which will be discussed in Section 3.3.

Fig. 4. - 
Explanations for digits classification through contrastive explanations method [95]. PP features which should be present on the image are highlighted in cyan, while PN features which should be absent for the prediction are highlighted in pink.
Fig. 4.
Explanations for digits classification through contrastive explanations method [95]. PP features which should be present on the image are highlighted in cyan, while PN features which should be absent for the prediction are highlighted in pink.

Show All

Forward Propagation-Based. While back-propagation method obtain saliency map by propagating the attributions from the output to the input image, researchers also leverage perturbations on the input to locate important features [96], [97]. Since the features given in the explanation are sufficient in a given image for classifier to make specific decision, removing or blurring the saliency features affects the decision of the original images. Fong et al. [96] obtain the important features by repeatedly perturbing the images to find the smallest mask accounting for the classification. Dabkowski and Gal [97] also introduce the smallest sufficient region which alone accounts for the specific decision into the objective function to find sharper saliency mask. They leverage trainable mask-generating model to speed up the perturbation process. Unlike back-propagation methods which retrieve the intrinsic model structure to obtain gradient, local methods based on perturbations are model-agnostic as they only depend on the input images and decision made by black-box models.

3.2.3 Other Methods
Some representative methods share both (or none of) characters described above, we give a brief introduction to them. SHAP values [17] take advantage of combination of Shapley values from game theory [98], which intends to fairly attribute the contributions. Unifying the objective function in several local methods (e.g., LIME, DeepLIFT, LRP), SHAP constructs the additive local approximation as the following:
g(z′)=ϕ0+∑i=1Mϕiz′i,(7)
View SourceRight-click on figure for MathML and additional features.where ϕ are considered as SHAP values and z′i is the individual input feature of instance z′. Since the observing SHAP values consume large computation, several approximations are proposed to mitigate this problem. Kernel SHAP leverages LIME to reduce sampling complexity and provide a model-agnostic estimation of SHAP values. Specifying the model structure, approximation efficiency is further improved on linear model, DNN and tree-based models through Linear SHAP, Deep SHAP, Tree SHAP respectively [17], [99]. Goldstein et al. [100] induce Individual Conditional Expectation (ICE) to demonstrate the relationship between one (or two) feature and the prediction of a single instance. While maintaining the other feature, ICE generates variants of the instance by iteratively changing the feature value and make the inference on these variants. With these variants on hand, ICE visualizes the dependence of predictions on the specific feature.

3.3 Instance-Based Methods
Instance-based explanation methods select or generate specific instances of dataset to provide insight into dataset or interpret the model behavior of the black-box model. These methods are especially effective when data items are humanly understandable such as images. An instance-based explanation can help users better understand a machine learning model from the decision boundary and the characteristics of the training data, especially when the training data has complex distributions.

The promising instance-based explainability methods include: i) Prototypes & Criticisms - using real data instances to explain the dataset distribution; ii) Counterfactual explanations - explaining a prediction by searching or generating some instances with different feature values that change the prediction to a predefined output.

3.3.1 Prototypes & Criticisms
A prototype is a representative data instance from the original dataset. A criticism is a data instance that is not well represented by the set of prototypes. Methods of prototypes & Criticisms attempt to provide insight into the black-box model by selecting prototypes or both prototypes and criticisms. In most of these methods, the prototype selection problem is transformed into an optimization problem with constraints. For instance, in [46], the prototype selecting problem is turned into the geometric problem of selectively covering points with a specified set of balls. This geometric problem is then further narrowed down to a set coverage problem. Three basic attributes are defined to identify the required prototype set:

covers as many training points of class L as possible,

covers as few training points of classes other than L as possible,

is sparse.

Property 2) indicates that, in some cases, it may be necessary to conceal certain points of class L. Therefore, a prize-collection set coverage framework can be adopted for this problem. Set coverage frameworks assign each set to a cost and a penalty for each uncovered point, and then find the lowest cost part of the set.

A well-known approximation algorithm for set coverage problem is greedy search. At each step, the algorithm finds a data instance xj of class L in the original dataset X with the greatest incremental improvement on the objective function ΔObj(⋅). In the process, the number of prototypes is determined automatically. The greedy algorithm can be simplified in Algorithm 1:

Algorithm 1. Prototype Selecting
while ΔObj(x∗,L∗)>0): do

Find (X∗,L∗)=argmax(xj,L)ΔObj(xj,L)

Let PL∗:=PL∗∪x∗

end while

Recently, the Bayesian framework has been combined with Case-based Reasoning (CBR) methods to improve the explainability in unsupervised learning [101]. The Bayesian case model learns prototypes by performing joint inference on cluster labels. It provides a new perspective on explainability, especially on prototype selection approaches. Based on the idea of prototype selection and Bayesian model criticism, a novel algorithm call maximum mean discrepancy critic (MMD-critic) [47] is introduced, which selects prototypes and criticisms independently. In this work, good prototypes are defined as data points in high-density regions and from different data clusters. To select good prototypes, the squared MMD measure (MMD2) below is applied as the cost function
MMD2=1m2∑i,j=1mk(zi,zj)−2mn∑i,j=1m,nk(zi,xj)+1n2∑i,j=1nk(xi,xj).(8)
View SourceRight-click on figure for MathML and additional features.Here k is a kernel function used to measure the similarity of two points. m is the number of prototypes z, and n is the number of data points x in the original data set. Through minimizing MMD2, MMD-critic minimizes the difference between the distribution of the data and the distribution of the selected prototypes. The set of prototypes can thus be selected through greed search.

MMD-critic also provides the solution to select criticisms C through the following objective function:
C=argmaxC⊆[n]/ s,∣C∣≤c∗L(C)+r(K,C),(9)
View SourceRight-click on figure for MathML and additional features.where [n]/ s denote all indexes except the prototypes, and c∗ is the number of criticism points desired. The second term in the cost function enhancing the diversity of the criticisms. And L(⋅) is the loss function defined as
L(C)=∑l∈C∣∣∣∣1n∑i∈[n]k(xi,xl)−1m∑j∈Sk(xj,xl)∣∣∣∣.(10)
View SourceRight-click on figure for MathML and additional features.L(C) indicates the deviation of two similarities, similarity between C and the entire dataset versus similarity between C and the prototypes. By maximizing L(C), criticisms are selected. Fig. 5 shows a few examples of the selected prototype and criticisms via MMD-critic. For class “dog”, prototypes are mostly the data instances with only one dog in the middle, showing clear dog ears. And the colors of these pictures are mostly bright and warm. On the contrary, the selected criticisms either contain multiple dogs, or the ears of the dogs in these images are covered, or the images are in dark color.


Fig. 5.
Learned prototypes and criticisms from Imagenet dataset [47].

Show All

MMD-critic is model agnostic and data type agnostic. However, choosing the number of prototypes and criticisms remains a challenge. Moreover, the results can be sensitive to the choice of kernel function, which makes the MMD-critic performance unstable.

Extending from MMD-critic, Karthik et al. present ProtoDash to selects prototypes with non-negative weights indicating their importance [102]. Compared with the original MMD-critic, in ProtoDash, each prototype is associated with a non-negative weight and thus the cost function is modified to
MMDˆ=∑zi,zj∈z1m2∑i,j=1mωiωjk(zi,zj)−2mn∑zi∈Zωi∑i,j=1m,nk(zi,xj)+1n2∑i,j=1nk(xi,xj).(11)
View SourceThe non-negative weights w here indicate the importance of the prototypes and are learned simultaneously while prototypes are selected. The introduction of the prototype importance brings an improvement of MMD-critic and it works for any positive definite kernel function.

Another prototype selection algorithm proposed by Karthik et al. is ProtoStream where the streaming method is applied [103]. It maintains multiple sets of candidate elements in parallel. The sets are updated if adding a new element with incremental gains greater than the threshold. Finally, the set and corresponding weights will be returned if it has the highest value of the scoring function. This algorithm is an innovation of providing sufficient conditions for obtaining a constant factor streaming algorithm for weakly submodular functions.

Besides developing methods for prototypes selection, there are also works exploiting the idea of prototypes to construct intrinsic interpretable models. In [48], the authors embed a special prototype layer in CNN, where each unit stores a weight vector similar to encoded training input. Since the prototypes are learned during the training, the network is intrinsically interpretable. In [104], the authors extend the idea to sequential data and propose prototype sequential network (ProSeNet). ProSeNet combines prototype learning with variants of recurrent neural network (RNN) to achieve both interpretability and high accuracy for sequence modeling. Based on ProSeNet, Yao et al. develop a framework named ProtoSteer [105]. With ProtoSteer, domain experts can steer the model by directly adding, deleting, or revising the prototypes.

3.3.2 Counterfactuals
Counterfactual is one of the basic ways for human beings to understand the world and to understand how human behavior affects the world. In [106], Pearl defined counterfactual as a probabilistic answer to a “what would have happened if” question. Counterfactual explanations take a form similar to the statement: You were denied a loan because your annual income was Â£30,000. If your income had been Â£45,000, you would have been offered a loan [49]. The income here is a hypothetical fact. That is to say, counterfactual is to discover facts that contradict existing facts. The prediction may be related to many features, and one or more feature changes may lead to different results. The counterfactual statement is to find the smallest change of the feature values so that it can change the prediction into the desired output. For models that predict continuous results, a good counterfactual instance should generate the prediction closest to the desired output. Besides, the fewer features that the counterfactual instance change, the better the counterfactual instance is. Another restriction is that a counterfactual instance should have a reasonable value. A temperature of 30 ∘C makes sense, but 100 ∘C should not be a good counterfactual instance.

In [49], the loss function to minimize for selecting counterfactual instances is
L(x,x′,y′,λ)=λ(f^(x′)−y′)2+d(x,x′),(12)
View SourceRight-click on figure for MathML and additional features.where y′ is the label for data point x′. d(x,x′) is a metric measuring the distance between the counterfactual x′ and the original data point x. λ here is to balance the distance in prediction against hte distance in feature values. Here the choice of distance function is especially important. In [49], the distance is measured by the Manhattan distance weighted by the inverse median absolute deviation (MAD)
d(xi,x′)=∑k∈F∣xi,k−x′k∣MADk,(13)
View SourceRight-click on figure for MathML and additional features.where F is the feature set, and MAD for feature k can be computed by
MADk=medianj∈P(∣Xj,k−medianl∈P(Xl,k)∣).(14)
View SourceRight-click on figure for MathML and additional features.Here P refers to the dataset. One advantage of using MAD is that it is robust to outliers. Besides, it can produce sparse solutions, which is human-understandable as only a few of the features are changed and most remain constant.

The general implementation of counterfactual assumes that all features are equally important, but this is not necessarily the case. To address this issue, [107] introduces a weighting strategy to generate more explainable counterfactual instances. Specifically, a weight vector θ is introduced to the distance metric
d2(x,x′)=∑j=1p∣xj−x′j∣MADjθj.(15)
View SourceThe weight vector θ can be derived from global or local feature importance. This method leads to more intelligible explanations as they take the importance of features into account.

In [50], the authors present a generative counterfactual introspection framework as shown in Fig. 6 to produce inherently interpretable counterfactual visual explanations in the form of prototypes and criticisms. Take the picture from celebA dataset [108] as an example, the task is to classify a celebrity face image into young or old. The framework can generate a given person's superficial appearance without altering facial features and identify. As shown in Fig. 7, (c) looks older than original image (a) by adding wrinkles to (a). This method helps users get more meaningful counterfactual explanations for probing the behavior of the given classifier.

Fig. 6. - 
Generative counterfactual introspection concept [50].
Fig. 6.
Generative counterfactual introspection concept [50].

Show All


Fig. 7.
Illustrate different editing scheme for the input image [50].

Show All

Prototype is also applied to guide the counterfactual searching procedure in [109]. Searching for counterfactual using prototypes has been proved effective because adding in a prototype loss term helps to shrink too much hyperparameter tuning.

An alternative way to search counterfactual instances is Growing Sphere, proposed in [110]. It locally explores the input space of the classifier to find its decision boundary without knowing data outside the observation being interpreted. A similar method can be found in [111], which proposes a method called counterfactual local explanations via regression (CLEAR). It generates counterfactual explanations that state the minimum changes necessary to reverse the predictive classification.

Summarizing Section 3, we review the current data-driven methods from three subclasses: global methods, local methods and instance-based methods. While global methods can provide explanations for the whole datasets or can gain a holistic insight into the black-box model, they are difficult to achieve in practice. Local methods address this issue by focusing on only sub-datasets or part of the model. These models provide explanations through surrogate models, or analyzing feature importance or with visualization. Alternatively, instance-based methods provide explanations by selecting or generating data instances. Most of the methods we review here are post-hoc methods, while a few of them are attempting to build intrinsic interpretable models. Table 1 summarizes the characteristic features of a few representative methods we review in this section. Column “Knowledge” refers to the involved external knowledge while generating explanations. For all data-driven methods, this column is left blank as no external knowledge is needed. Column “Intrinsic or Post-hoc” indicates whether the method attempt to provide a post-hoc explanation or build an intrinsically interpretable model. The last three columns indicate the output explanation type, the black-box model type and data-type to input for the method.

TABLE 1 Summarized Features for Selected XAI Methods in Sections 3 and 4
Table 1- 
Summarized Features for Selected XAI Methods in Sections 3 and 4
SECTION 4Knowledge-Aware eXplainable AI
The data-driven explanation methods are able to provide comprehensive explanations from dataset or the output between input and output. Based on that, the external knowledge can be used to enrich the explanation and make it more user-friendly. For example, it may be difficult for laymen in machine learning to understand feature importance directly. Since they are not equipped with specialized background knowledge, the connection between features and the target may be unclear to them. With external domain knowledge, we can generate explanations that not only indicate feature importance, but also describe the reason why certain features are more important than others. As a result, knowledge-aware methods for XAI have attracted more and more attention during the past few years. To give a systematic review on these methods, we hereby divide them into two categories according to the knowledge source: general knowledge methods and knowledge-base (KB) methods. The former takes unstructured data as knowledge source while the latter constructs explanations with the foundation of structured knowledge base.

4.1 General Knowledge Methods
Compared with raw datasets collected from variant scenarios, knowledge is generally regarded as entities or relations that are concluded by humans according to their life experience or strict theoretical reasoning. Generally, knowledge can be in diverse forms. It can be reserved within human's mind or recorded in natural language, audios or rules with strict logic. In this subsection, we focus on discussing XAI methods exploiting general formats of knowledge without rigorous logical requirements.

4.1.1 Human-in-the-Loop
One relative direct way to provide knowledge is through human participation. Actually, along with the explosion of AI research and application, the key role of human in AI systems has slowly become apparent. Such a prospect is called Human-centered AI. Riedl et al. [112] argue that human-centered AI does not only make AI systems understand humans better from a sociocultural view, but it is also able to make AI systems help humans understand themselves. To achieve these targets, several attributes such as interpretability and transparency need to be fulfilled.

Specifically, humans are capable to play a role in AI systems by providing quite a few human-defined concepts. Kim et al. [51] utilize Concept Activation Vector (CAV) to test the importance of a concept in a classification task. CAV is the vector perpendicular to the decision boundary between activations of positive and negative inputs for the concept of interest. Given a DNN for image classification, the pipeline to measure the importance of a concept for a certain class, e.g., “stripes” for “zebra”, is shown in Fig. 8.


Fig. 8.
The pipeline of Testing with CAV (TCAV).

Show All

To compute TCAV score, a “concept sensitivity” representing the importance of a concept at layer l towards the prediction on class k can be first computed as the directional derivative SC,k,l(x)
SC,k,l(x)==limϵ→0hl,k(fl(x)+ϵvlC)−hl,k(fl(x))ϵ∇hl,k(fl(x))⋅vlC,(16)
View SourceRight-click on figure for MathML and additional features.where fl(x) is the activation of input x at layer l, hl,k(⋅) is the logit of class k, and ∇hl,k(⋅) is the derivative of hl,k w.r.t the activation at layer l. vlC is the CAV of concept C that the user aims to explore. The positive (or negative) sensitivity indicates that concept C has a positive (or negative) impact on the activation of an input.

With SC,k,l, TCAV can then be obtained as the ratio of samples from class k with positive SC,k,l's
TCAVQC,k,l=|{x∈Xk:SC,k,l(x)>0}||Xk|.(17)
View SourceCombining with t-distribution hypothesis method, if TCAVQC,k,l is greater than 0.5, it shows that concept C has a significant impact on class k.

Following this idea, abstract human-defined concepts can be encoded into the representation space of original model so that the similarity between them can be measured as impact. As a consequence, such concepts with corresponding importance can construct a meaningful and user-friendly explanation.

The actual applications of TCAV have been further explored in [113]. It aims to deal with the problem of similar medical images retrieval to assist doctors in diagnosing disease. Previous methods only retrieve images based on various mathematical distance measurements. However, it is arduous for these measurements to fulfill diverse expectations from doctors for different kinds of diagnoses.

In [114], the authors propose an interactive method that incorporates humans to manually select semantically correct latent patterns from pre-trained CNN. With the selected patterns, the pre-trained CNN is transferred to a human-interpretable AOG model. Owing to the guidance of human interactions, the method also exhibits superior performance in part location.

4.1.2 Knowledge Corpus Import and User-Friendly Explanation
Although human participation can project external knowledge into XAI system and produce knowledgeable explanations, it is difficult to standardize and automate the procedure. Besides, it is hard for a person to enumerate the related knowledge especially when there are a large number of related concepts. For example, in TCAV, when the concept space is vast, it is over demanding for human to list out all of the concepts. One way to address these issues is to form a knowledge corpus, i.e., a standard, general and as complete as possible corpus that contains the domain knowledge or concepts, and import it into the XAI system.

In [115], Bau et al. assemble Broadly and Densely Labeled Dataset (Broden)–a unifying image dataset including pixel-level segmentation for both low-level concepts (textures, colors, materials) and higher-level concepts (objects, object parts, scenes). With Broden and the proposed framework–network dissection, the semantics of hidden units of CNN can be identified and aligned with human-interpretable concepts. In detail, images from Broden are input to the pre-trained CNN and the corresponding activation maps of the internal hidden convolutional units are collected. Feature map regions of each filter f with activation greater than a threshold (0.005 in [115]) are selected as the valid activation regions. Upsampling the valid activation regions to the scale of input image, the receptive field of f's valid activations w.r.t image I is obtained as SIf. Thus, the consistency between a filter f and a semantic concept c can be quantified through an intersection-over-union (IoU) score
IoUf,c=∑I|SIf∩SIc|∑I|SIf∪SIc|,(18)
View SourceRight-click on figure for MathML and additional features.where SIc is the ground-truth mask of concept c in image I.

Based on Broden, Zhou et al. further propose a method based on CAV and feature vector decomposition as demonstrated in Fig. 9.

Fig. 9. - 
A function display of knowledge corpus import for explanation from [52]. The image on the left is the original image and its prediction. The image next to it is the comprehensive Grad-CAM heatmap. The images on the right are the corresponding decomposition of Grad-CAM heatmap based on a corpus of concepts.
Fig. 9.
A function display of knowledge corpus import for explanation from [52]. The image on the left is the original image and its prediction. The image next to it is the comprehensive Grad-CAM heatmap. The images on the right are the corresponding decomposition of Grad-CAM heatmap based on a corpus of concepts.

Show All

For a black-box classification model such as CNN, GradCAM [16] generates comprehensive heatmap for input images. After that, feature vectors are decomposed based on a set of interpretable bases [52]. Ultimately, for each base, the corresponding sub-heatmap and quantified contribution proportion are generated.

Specifically, following the idea of CAV, a binary classifier hc(a)=sigmoid(wTca+bc) indicating the presence of concept c is first trained with Broden. The input here is a feature vector a at a single location of the activation A. After training is done, an embedding vector qc for concept c can be obtained as qc=(wc−wc¯)/||wc−wc¯||. This vector captures the features relevant to concept c. The black-box model can then be decomposed as the following:

Let a=g(x)∈RD be the activation at the second last layer of original model and hk(x) is the computation score of the last layer for class k. hk(a) is a linear function, which can be written as
hk(a)=wTka+bk,(19)
View Sourcewhere wk represents the weight vector of the last layer and bk is the bias term. This equation can then be decomposed as
wkhk(a)=sc1qc1+…+scnqcn+r=s1qTc1a+…+siqTcia+…+snqTcna+rTa+bk,(20)
View Sourcewhere qci represents the embedding vector of ith concept, while sci is the corresponding weight that needs to be learned. After decomposition, siqTcia can be regarded as the contribution of concept ci.

In [53], Ehsan et al. develop a method to automatically build a corpus of explanations for RL agent behavior by guiding humans to verbalize the motivations for their actions. With the corpus, an encoder-decoder RNN is trained to generate readable natural language explanations for any given action. Specifically, the network learns to translate the state representation as well as action information into an explanatory sequence of words, which is especially suitable to end-users who have no expertise in AI. For example, given the action of moving right in a game, the network can provide explanations such as “I move to the right so I can jump onto the next log”. Following this idea, several similar works are proposed to provide user-friendly explanations for RL agents [116], [117], [118].

4.2 Knowledge-Base Methods
The knowledge-aware methods mentioned in Section 4.1 exploit unstructured knowledge to explain black-box models. In this section, we focus on the explanation methods with structured and systematic knowledge. For comparison, humans can be subjective while KB can be objective. Knowledge from corpus is well suited for specific scenes while that from KB is more general. Therefore, knowledge from KB can be used to generate explanations from a different perspective. In current literature, KB is often modeled as knowledge graph (KG). Researches into applying KG in explanation can be divided into internal and external route of the model.

4.2.1 Internal Route
The internal route considers KG when designing the model, using features like entities, relations, paths and rules in KG to enhance the model performance and explain the model decision. This strategy is mainly applied in recommendation systems [54], [55], [119], [120], [121], since background knowledge like relations between items is beneficial for similarity measurement. Wang et al. [54] combine KG and sequence model to recommend songs for a user, as shown in Fig. 10. Based on KG and the history data including songs that the user has interacted with, different paths between a user-item pair in KG are extracted. Then a Long Short-Term Memory (LSTM) model is used to assign scores to these paths by taking the concatenation of entities, entity types and relation type embeddings in that path as input. Finally, scores of all paths are aggregated to a preference score for the user-item pair by a weighted pooling layer. Items for recommendation are ranked in order of scores. The paths with high scores can also be used to explain the recommendation result. For example, the path [Alice −→−−−Interact Shape of You −→−−−SungBy Ed Sheeran −→−−−Produce÷−→−−−−−−ContainSong I see Fire] in Fig. 10 is an explanation for recommending the song “I see File” for user Alice. Alternatively, Ma et al. [55] propose a joint learning framework integrating a rule learning module and a recommendation module. The inductive rules between items are mined from KG to both enhance and explain the recommendation results.


Fig. 10.
Overview of the model structure. The embedding layer contains 3 individual layers for entity, entity type, and relation type, respectively. The concatenation of the 3 embedding vectors is the input of LSTM for each path [54].

Show All

In addition, KG can also be embedded in the design of objective functions. Zhang et al. [119] propose a method to suggest entities related to the input query based on an objective function, and the goal is to maximize the relation score of suggested entities and the query. At the same time, a concept is extracted from KG as an explanation for the suggested results, which is an intermediate result in the process of computing the objective function. For example, the entities in the query are “China, India, Brazil”, and the suggested entity is “Russia”. An explanation concept for this suggestion is “BRIC”, since all of these entities are members of “BRIC”.

4.2.2 External Route
The external route combines KG and reasoning to generate explanations without changing the model structure. While reasoning is powerful for computing explanations, the explanations can be refined with more knowledge. In practice, the explanations are usually simplified rules mapping the input-output behaviors of black-box models [57], [122] or high-level concepts which are viewed as class expressions [58], [123]. Seneviratne et al. [56] utilized abductive reasoning with the knowledge to find the best explanation for the treatment of a patient with diabetes. In this work, the guideline recommendations are transformed into Resource Description Framework (RDF), and the patients are represented in terms of their attributes, diagnoses and treatment plans. Initially, deductive reasoning is used to find a recommended treatment. If the recommendation is different from the treatment history, abductive reasoning is applied to find an explanation. For example, the inferred recommendation is a Metformin prescription for a diabetic patient, which, however, is different from his administered treatment. The potential reason generated by abductive reasoning is that the patient had a creatine clearance of 58 mL/min, which is below the threshold of 60 mL/min to recommend the use of Metformin.

With the combination of KG and reasoning, simpler and more comprehensive explanations can be generated. Labaf et al. [57] extract rules to fit the input-output behaviors of neural networks, and the rules are finally reduced and simplified based on common knowledge. In addition to rules, Sarker et al. [58] use high-level concepts generated by inductive reasoning on common knowledge to explain the black-box model decision. In this work, the black-box model is applied to classify the input image into two classes: “indoor warehouse” and “outdoor warehouse”. The images are extracted from ADE20K dataset [124], which are annotated with tags of objects like “door”, “window” and “road” identified in the images. To generate explanations, the object annotations in each image and the classification result of the black box model are added to a KB. Then DL-Learner is used to run inductive reasoning on the KB, to further generate high-level concepts of the objects. As shown in [58], one of the generalized concepts is “Transitway”. Since the transitway usually appears outside the warehouse in our common knowledge, it can be used as an appropriate explanation for the model decision.

In summary, knowledge-aware methods provide explanations harnessing different forms of external knowledge. Owing to the extra information, knowledge-aware methods can provide more human-friendly explanations. Compared to data-driven methods, knowledge-aware methods are still in an early stage and might have a large expansion capacity in the future. A few representative works are listed together with methods from Section 3 in Table 1.

To give a clearer picture on these different methods, we now take depression diagnose as a brief example and state the possible results of different methods. Given a trained neural network and the dataset, global methods such as tree extraction or rule extract can generate explanations like “If Age < 50 and Past-Depression=True, Insomnia=True and Melancholy=True and Tiredness=True, then Depression=True”. As local methods focus on a single data point, the explanation in this case can be “The most influential feature for Candidate No.30 to be diagnosed as Depression=True is Melancholy, with importance 0.9”. For instance-based methods, e.g., counterfactual method can provide explanations as “Candidate No.32 is diagnosed as Depression=False, he/she would've been diagnosed as Depression=True if Insomnia=True”. As for knowledge-aware methods, a more suitable scenario is that clinical diagnosis with medical images. Given a medical images, pathologists can crop several regions as important concepts (e.g., some strange tissues). And the explanation on the diagnosis result can be “Candidate No.12 is diagnosed as lung cancer positive, because the CT image contains large amount of strange tissues. ”

SECTION 5Metrics
While approaches from Data-driven and Knowledge-aware XAI provide vast explanations, there are several significant questions left behind: how good are the explanations, whether we can build a faithful understanding of the black-box model, and whether we can trust them? Before the trust and reliance between users and machines are built, users need to justify the effectiveness of the explanations. To answer these questions, researches develop various metrics to evaluate the explanations, e.g., localization capability for saliency map, accuracy for model diagnoses and trust of explainees on discrimination. Even for a single task, we can evaluate the explanations through different aspects to obtain a comprehensive analysis. Revisiting the explanations of LIME for image classification, quantification method is used to measure the fidelity between the proxy and original models, while mental models are leveraged to see whether users could identify the biases inside the black-box model through explanations [14]. Thus, diverse metrics are developed for different purposes, model structures, explanation types, etc.

Pioneers have introduced different taxonomies of interpretability evaluations. Doshi-Velez and Kim [26] separate the evaluations of explanation into application-ground, human-ground and function-ground in terms of justification ground and complexity. Hoffman et al. [18] discuss the popular evaluation metrics in four aspects: Goodness, Satisfaction, Comprehension (i.e., Mental model, curiosity and trust on XAI explanations) and performance measuring. More recently, Mohseni et al. [125] conclude the literature of evaluation methods through target users (e.g., AI laymen, data experts and machine learning experts) as well as measures to verify the “effectiveness” of explanations. Following the taxonomies, we group evaluation methods into two categories: computational metrics and cognitive metrics.

5.1 Computational Metrics
Computational metrics are referred here as intuitive scalar indicators of “good” explanations, especially for AI experts i.e., the experts who design the XAI system [125]. The computational measurements are usually accomplished through reasonably designed formulas involving the data on hand. Thus, computational metrics can serve as guidance for developing interpretation methods without human participating [126]. In the following, we introduce several computational metrics in terms of evaluation targets.

Post-Explanation Performance. One of the prospects of XAI is to further improve the performance of black-box model by adjusting the model for the explanations [44], [127]. Thus, it is intuitive to evaluate the explanations by comparing the model performance (e.g., accuracy or error in classification task) before and after adjustment. Zeiler and Fergus [44] obtained a better model with a lower error rate than the original AlexNet after getting insight into the operation. Reasonable explanations can assist researchers with model diagnosing, but it remains a doubt that greater performance improvement confirms better explanations.

Faithfulness and Fidelity. A significant question in XAI is whether the important features or concepts in explanations refer to the truly relevant features. To gauge faithfulness of the explantions, a common method is to remove or obscure the important features of a certain class according to the feature importance from the explanations and measure the probability drop of that class [128], [129]. If the highly attributed features are truly important for that class, we should observe a significant drop in the probability. Kim et al. [51] use a dataset with noisy captions integrated into the images to measure faithfulness of TCAVs. TCAV scores of caption decrease as noisier caption or no caption presents on the image, which immulates the black-box model dependence on different images. Correspond to faithfulness, we refer fidelity to the percentage on which the surrogate model make the same decision as the original black-box model [34], [35]. High fidelity indicates that the surrogate model mimic the behavior of black-box model. Similarly, Ribeiro et al. [14] obtain local surrogate model on an interpretable model and calculate the recall of significant features highlighted by the surrogate model. Reasonable explanations should emphasize the relevant feature (or concept) used for decision making or reflect the rationale of black-box models. Based on this, one can evaluate explanation using faithfulness of features (or concepts) importance or fidelity of surrogate models.

Robustness. Robustness refers to the capability of interpretation methods to protect their explanations from diverse attack e.g., input perturbation [129], [130], adversarial attack [127], [131] and model manipulation [132]. Robust interpretation methods are supposed to provide similar explanations when the inputs or black-box models sustain such attacks. Kindermans et al. [130] compare the saliency maps of original inputs and perturbations to obtain an intuitive measurement of robustness. Considering saliency map and object segmentation, Zhang et al. [133] use L1 distance and IoU to compute the similarity between benign and adversarial maps. Ghorbani et al. [131] adopt rank-order correlation and Top-k intersection to quantify the similarity of features importance ranking between the images.

Localization. Saliency map has the potential to encode the location information of the objects of a specific class. Thus, saliency map is considered as a weakly supervised reference for object localization. Leveraging the object localization datasets such as COCO, we can evaluate the explanations through localization capability. The initial step is to capture the saliency map of a specific class and transform the numerical map into a binary object mask. Segmentation masks can be generated through binarization of saliency map from CAM and Grad-CAM with a certain threshold [16], [92]. However, saliency map only highlights the most relevant region rather than the whole object in an image, and thus Simonyan et al. [43] diffuse the saliency map through GraphCut [134] to cover entire objects. The weakly supervised segmentation is further compared with the bounding-box ground truth through the error of localization defined in [135]. Similarly, Zhang et al. [136] propose Pointing Game to compute the percentage of hit, i.e., the maximum point of saliency map lies within the bounding-box. Under the assumption that the distance between objects related to a certain pattern and several references will not change significantly among different images, Zhang et al. [127] use deviation of distance to evaluate localization stability of interpretable CNN.

5.2 Cognitive Metrics
Cognitive metrics gauge the explanation with respect to the human. Since a major goal of XAI is to let human understand the rationale and mechanism behind the machines, human-subject evaluation serves as a blunt indicator for explanations. While computational metrics should be carefully adopted to specific tasks or applications, human-subject evaluation can generalize to various scenarios by directly asking participants to discern the better explanation in their perspective. Experiments are also designed to evaluate explanations through measuring mental model, satisfaction and trust, which are introduced in the following.

Mental Model. Mental models refer to the representation of how a person understands the mechanism of a system or an event [18], [125]. Explanations describing the mechanism of black-box models help users establish better mental model as well as promote better human-machine interaction [137], [138], [139], [140]. Experiments of local approximation indicate that explanations help users gain a rough insight into the classifier, knowing how it distinguishes between different categories [14].

Satisfaction. Satisfaction is a context-based evaluation, considered as the degree to which users understand the explanations of the AI system [18]. It is also considered as a prerequisite for trust [141]. Gedikli et al. [141] support a strong correlation between transparency and satisfaction with ten types of explanations as experimental subjects. In the real-world scenario, users are more satisfied when several types of explanation (e.g., why, certainty, control) are provided [142]. On the other hand, complex explanations (i.e., long explanation, new concepts and repetition of input conditions) might make users unsatisfied [143]. Researches usually use questionnaires for qualitative satisfaction evaluation. Early in recommendation system investigations, Tintarev and Masthoff [144] measure satisfaction by rating the explanation through a single sentence (How good do you think this explanation is?”) . More recently, Hoffman et al. [18] justify the scale validation and introduced an Explanation Satisfaction Scale for qualitative measurement of users’ judgments after working with XAI systems. Likert scale on several questions (e.g., ”From the explanation, I understand how the [software, algorithm, tool] works”) is utilized to measure users’ satisfaction. To compare saliency maps from different interpretation methods, participants are also asked to choose the more reasonable and satisfactory heatmaps directly [16], [51], [52]. Better explanations attract explainees through various attributes (e.g., understandability, usefulness and explaining accuracy), and thus statisfaction-based evaluations depend on the certain task, explanation type, type of target explainees, etc.

Trust and Reliance. Trust is significant in automation, especially in the emergencies, whether the user adopts the advice from an intelligent machine depends on his/her trust on the machine. Glass et al. [145] suggest sufficient explanations as guideline to solve several themes which users utilized to build trust on machine. Users trust the intelligent system more (or less) when they are provided explanations for the decisions [14]. Especially in local methods, users enhance (or reduce) their trust if the interpretable region is correctly determined [86]. To evaluate explanations through trust, a fundamental assumption suggests system with better explanations should receive more trust from users. Zhou et al. [52] collect the trust feedback in the experiment where the participants are asked to discern the better model based on the explanations. In this case, better explanations could help users discriminate different models with more confidence.

Limited by the length of this survey, we shall not discuss vast of metrics in reviewed literature. We note that various metrics are usually used together to attain reasonable and practical XAI systems. However, sometimes it is challenging to observe high scores in computational and cognitive metrics simultaneously. A typical example is the trade-off between fidelity and human interpretability, particularly for the surrogate-model explanations. Human-friendly explanation usually demands a low-complexity surrogate model, which leads to low fidelity to the black-box models. Herman [146] also discuss the implicit cognitive bias due to cognitive attributes and human expectations in human-subject evaluations. Thus, evaluation strategy in the whole experiment needs careful design to balance the attention on different metrics and avoid cognitive bias on the results.

Drawing upon [18], [146], interpretable models (or ad-hoc explanations) need initially to be trustworthy through computational metrics used by AI experts. The explanations are then adjusted to human-friendly format targeting greater satisfaction and mental model from AI layman and domain experts. Trust and reliance are measured to ensure the crucial goal of XAI, i.e., to help users understand the mechanism and rationale behind machine and trust in their automated decision. The pipeline provides a suggested instruction of the reviewed metrics. Table 2 further summarizes the taxonomy described in this section with their corresponding explanation type, representative XAI methods (only a subset of methods reviewed in this survey) and target users of XAI systems. We note that the target users here refer to users of XAI systems rather than users of evaluation methods.

TABLE 2 Summary of Taxonomy of Metrics in Section 5 Together With Suitable Explanation Type, Subset of Representative XAI Methods and Target Users

SECTION 6Scenarios & Applications
In the three sections above, we discuss the development of XAI methods and the evaluation metrics for explanations. In this section, we shall review the application of XAI in actual scenarios. Considering the domain criticality, we here choose self-driving and financial AI as the representative scenarios in Sections 6.1 and 6.2, respectively.

6.1 Self-Driving
As mentioned in Section 1, XAI is highly sought after to push forward further application of AI in real life. This is especially the case for self-driving scenario. Currently, most of the advanced self-driving models are based on DL [147]. The black-box nature of DL could cause a series of chaos and severe problems in this scenario. To start, passengers will not trust in the self-driving system as the actions are not explained. Besides, it will be hard for insurance companies to identify the share of the responsibility after an accident. Therefore, explainability of self-driving systems is indispensable. As a result, more and more related researches and discussions have been conducted.

One line of works takes the idea of importing knowledge corpus into black-box self-driving systems to provide user-friendly explanations, which is specifically introduced in Section 4.1.2. In [19], the knowledge corpus was constructed by organizing humans to describe and give textual explanations for a series of actions made by a vehicle in a driving video dataset. At the same time, the core of the self-driving system is made of a visual attention model [148]. Then it aligned attentions with corresponding explanations to make sure the explanations reflect internal decision mechanism rather than just superficial rationalization. Given the state and action of the vehicle, an example of explanation can be “The car heads down the street, because the street is clear”, which consists of descriptions and corresponding explanations. Besides providing explanations, [149] proposes that a knowledge corpus composed of human instructions can further improve the performance of a self-driving controller system, which can be regarded as an agent in reinforcement learning. These instructions include experience and advice that can seldom be obtained from video dataset directly.

Apart from simple end-to-end training methods introduced above, another technical routine is widely taken, which decomposes the self-driving task into several sub-processes such as perception, prediction, motion planning and control. This is because, in practice, it is difficult to collect enough data for end-to-end DL model training to generalize all situations. And such training process cannot provide explanations independently. However, the decomposed training strategy also has shortcomings. The optimization for each subtask is conducted separately. The combination of each optimal is usually not the global optimal. Thus the decomposed training strategy may lead to compromised performance. Zeng et al. [150] bridge the gap between these two routines to build an interpretable motion planner with end-to-end training. It fits the idea of building a global surrogate model, which is specified in Section 3.1.1. The pipeline of motion planner is shown in Fig. 11.

Fig. 11. - 
The pipeline for operation of the end-to-end motion planner from [150]. The motion planner takes High-Definition (HD) map and LiDAR points cloud as input. Through a backbone CNN network, it generates perception results and cost volume for each location the self-driving car can take within its planing range. After that, the motion planner samples a set of trajectories and computes accumulated cost volume respectively. Finally, the self-driving car will choose a trajectory having the smallest cost volume.
Fig. 11.
The pipeline for operation of the end-to-end motion planner from [150]. The motion planner takes High-Definition (HD) map and LiDAR points cloud as input. Through a backbone CNN network, it generates perception results and cost volume for each location the self-driving car can take within its planing range. After that, the motion planner samples a set of trajectories and computes accumulated cost volume respectively. Finally, the self-driving car will choose a trajectory having the smallest cost volume.

Show All

Here the cost volume of each position on a trajectory can be taken to compose explanations for every decision made by motion planner. More importantly, such explanations fully match internal structure of the system.

Besides specific research diving into models of self-driving system, there are other works attempting to lead to interpretable self-driving vehicles via helping humans build trust in machines theoretically. Mittu et al. [151] systematically studied human trust-building in autonomous systems. It gives an example that when a self-driving car suddenly changes lane, passengers may feel more comfortable if the system explains like “an accident occurred up ahead.”. Such a process will gradually cultivate trust. More specifically, focusing on actual application of semi-autonomous vehicles, Petersen et al. [152] try to increase passenger's trust by augmenting their situational awareness. From another point of view, Haspiel et al. [153] investigate the influence of providing explanations at different time frames for trust-building. Cognitive experiments shows that providing an explanation before vehicle taking action benefits trust-building the most. Moreover, to evaluate trust in self-driving systems more quantitatively, Cysneiros et al. [154] cut into the problem from the view of software transparency. Combined with the idea of Non-Functional Requirement (NFR), it aims to establish Vehicle Intelligence Certification System (VICS), which calculates a “Trust Score” for each self-driving system. VICS can be referenced by customers and other parties like insurance companies. Also, Hengstler et al. [155] choose to measure trust based on the Technology Acceptance Model (TAM).

6.2 Financial AI
AI has great potential to assist financial services as well. Recent implementations have made financial decisions more timely and automatic [156], e.g., online-loan is available with automatic credit risk assessment. Since the financial industry is highly regulated, opaque and uncontrollable black-box decision systems could result in severe problems. For example, the financial institutions might be sued if they fail to justify the decisions of an AI system, especially the fairness. Thus, the explanations for decisions are significant for AI deployment in the financial industry.

Bank of England [157] attempts to explain the mortgage defaults model through Shapley values mentioned in Section 3. Besides the feature importance, data scientists and consumers demand far more information on the rationale. In the competition of Fair Isaac Corporation (FICO), Chen et al. [158] propose a globally interpretable model to predict whether applicants will delay within two years, attaching the intrinsic explanations for the predictions. Features are partitioned into subgraphs with smaller models, and the subgroup models are then combined into the global model. Major contributing factors are visualized through interactive tools.

The team from IBM specify three different types of users: data scientist, loan officer and bank customer, and provide different types of explanations according to their needs [159]. For data scientists, the explanation is based on sensitivity analysis, i.e., how the prediction score changes as the variable changes as shown in Fig. 12. Data scientists can see that external risk estimate contributes positively to a person's likelihood to repay the loan. As for loan officers, prototypes are provided as explanation to inform a new loan decision. For denied customers, not only the reason for rejection is provided, the improving direction for them to get accepted is informed through counterfactuals.

Fig. 12. - 
External risk estimate presented in [159]. The $x$x-axis represents the external risk score and the $y$y-axis is the logarithm of a person's likelihood to repay the loan.
Fig. 12.
External risk estimate presented in [159]. The x-axis represents the external risk score and the y-axis is the logarithm of a person's likelihood to repay the loan.

Show All

In [107], Accenture Labs et al. provide counterfactual explanations for both accepted and denied onsumers. For the accepted consumers, the counterfactual acts as a reminder to prevent the client to get rejected in the future. The explanation goes as: “If instead you had the Net Fraction Revolving Burden higher than 55, your application would have been rejected”. For the denied consumers, the counterfactual acts as a suggestion for the consumer to improve his/her current status, where the explanation goes as: “If instead you had the Number of Satisfactory Trades increased by 15, your application would have been accepted.”

XAI has also been applied to detect and prevent financial discrimination in loan lending. In [160], [161], researchers enhance the transparency of financial algorithms, and by analyzing the common reject reasons, they can find whether there exists discrimination. In [162], researchers use Quantitative Input Influence (QII) to identify feature influence, such as by changing the race of the applicant to know whether race changing can change the final result.

Besides loan lending, financial institutions are also making efforts to make decisions of their other businesses more transparent and acceptable. Freddy and Wen [163] introduce an Artificial Intelligence Finance System (AIFS), which combines Semantic Web and machine learning technologies, using data visualization method to predict abnormal expenses. Akur8 [164] applies XAI, specifically the Generalized Linear Models (GLM, also known as Additive Models) in setting insurance prices, which establishes interpretability and automation while keeping the same performance.

SECTION 7Conclusion
In conclusion, we conduct a thorough review on the progress of XAI in the past decades. In detail, we follow three lines based on the phases of life-circle of XAI to review the related works: (i) Methodology; (ii) Evaluation; (iii) Application. For the line of methodology, we put forward a new taxonomy based on whether external knowledge is involved in the explanation generation process. Data-driven methods without external knowledge are reviewed in terms of global, local and instance-based methods. Instance-based method is emphasized for its unique way to provide explanations, e.g., through representative instances. Compared to data-driven methods, knowledge-aware methods make use of external knowledge to provide more human-friendly explanations. To the best of our knowledge, this is the first survey to review these recently popular methods. Based on whether knowledge is structured or unstructured, we categorize the knowledge-aware methods into general knowledge methods and knowledge-base methods. The evaluation metrics are gathered up into two categories: (i) Computational metrics and (ii) Cognitive metrics. Though computational metrics are usually standardized and can be automated without human involving, they are usually task-dependent and explanation-type-specific. Furthermore, knowledge on AI and XAI is required by computational metrics and thus target users for computational metrics are AI experts rather than laymen. On the contrary, as cognitive metrics are usually collected through user-study on cognitive experiments, they are task-agnostic and can be applied to all levels of users. We further review the application of XAI on self-driving, financial AI.

SECTION 8Future and Discussion
As shown in this survey, a great number of works related to XAI have been proposed. To further push forward this direction, we summarize here a few notable prospects:

To begin with, there is still a great expansion capacity for knowledge-aware XAI. Section 4 covers different attempts in this direction. However, there are still many open questions to make use of external knowledge efficiently. One of the questions is, how to obtain or retrieve useful knowledge in such a vast knowledge space. Take Human-in-the-loop in Section 4.1.1 as an example, a person usually has knowledge from various domains, thus the XAI system needs to guide this person to provide the desired knowledge instead of unrelated knowledge.

In addition, a more standard and unified evaluation framework is in great need for deploying XAI systems. Section 5 introduces several evaluation metrics including computational and cognitive metrics. To build a standard and unified evaluation framework, we may need to exploit different metrics at the same time, with one complementing another. As pointing out in Section 7, different metrics may serve for different tasks and users. The unified evaluation framework should have corresponding flexibility.

Last but not least, we believe it will be beneficial to have multi-disciplinary collaborations. The growth of XAI might need not only computer scientists to develop advanced algorithms, but also physicists, biologists and cognitive scientists to unveil the mystery of human cognition, as well as domain experts to project their domain knowledge.