Online monitoring user cardinalities in graph streams is fundamental for many applications such as anomaly detection. These graph streams may contain edge duplicates and have a large number of user-item pairs, which makes it infeasible to exactly compute user cardinalities due to limited computational and memory resources. Existing methods are designed to approximately estimate user cardinalities, but their accuracy highly depends on complex parameters and they cannot provide anytime-available estimation. To address these problems, we develop novel bit/register sharing algorithms, which use a bit/register array to build a compact sketch of all users’ connected items. Our algorithms exploit the dynamic properties of the bit/register arrays (e.g., the fraction of zero bits in the bit array) to significantly improve the estimation accuracy, and have low time complexity O(1) to update the estimations for a new user-item pair. In addition, our algorithms are simple and easy to use, without requirements to tune any parameter. Furthermore, we extend our methods to detect super spreaders with large cardinalities in real-time. We evaluate the performance of our methods on real-world datasets. The experimental results demonstrate that our methods are several times more accurate and faster than state-of-the-art methods using the same amount of memory.
SECTION 1Introduction
Many real-world networks are given in the form of graph streams. Calling network is such an example with nodes representing users and an edge representing a call from one user to another. When web surfing activities are modeled as a bipartite graph stream where users are from one node partition and websites are from the other node partition, a user has edges to the websites that he/she browsed. Monitoring the cardinalities (or degrees) of users in these networks is fundamental for many applications such as network anomaly detection [15], [54], where a user's cardinality is defined to be the number of distinct users/items that the user connects to in the unipartite/bipartite graph stream of interest. Due to the large-size and high-speed nature of these graph streams, it is infeasible to collect the entire graph especially when the computational and memory resources are limited [37], [44]. For example, network routers have fast but very small memories, which leads their traffic monitoring modules incapable to exactly compute the cardinalities of network users. Therefore, it is important to develop fast and memory-efficient algorithms to approximately compute user cardinalities over time.

However, existing methods have the following challenges:

Challenge 1: One can implement Bloom Filters [5] to handle edge duplicates and exploit frequency estimation methods such as Count-Min sketch [9] to approximate user cardinalities, but it requires large memory usage and results in large estimation errors. Compared with the methods such as Count-Min sketch [9] that utilize a variety of counters to record each user's frequency (i.e., the number of times the user occurs) over time, it is essential to build a hash table of distinct occurred edges such as a Bloom Filter [5] to handle edge duplicates in graph streams when computing user cardinalities. Specifically, a Bloom Filter is a one-dimensional array consisting of m bits and z independent hash functions. For each incoming edge, the Bloom Filter randomly selects z bits using z hash functions and sets them to one. Afterwards, to check whether an edge is presented previously, the Bloom Filter uses z hash functions to get z corresponding bits of the edge. An edge is treated as a duplicated edge if all these z associated bits are all set to one and a new edge otherwise. The key characteristic of the Bloom Filter is that it has a false positive but no false negative. The false positive is calculated as p=(1−(1−1/m)zn(t))z, where n(t) is the sum of all user cardinalities at time t. The more edges occur in graph streams, the larger false positive is caused. Therefore, it is necessary to assign a large memory space to the Bloom Filter to achieve the desired false positive. Also, the false positives of wrongly identified edges result in complex estimation errors in Count-Min sketch because the false positives are unknown in advance. Moreover, Count-Min usually provides over-estimations for user cardinalities, and it is easy to mis-classify users with small cardinalities as users with large cardinalities and generate large estimation bias.

Challenge 2: It is difficult to set an optimal value for the size of existing sketch methods. A variety of cardinality estimation methods such as Linear-Time Probabilistic Counting (LPC) [48] and HyperLogLog (HLL) [17] are developed to approximately compute cardinalities. An LPC/HLL sketch consists of m bits/registers, where m is a parameter affecting the estimation accuracy. Since user cardinalities are not known in advance and change over time, one needs to set m large (e.g., thousand) to achieve reasonable accuracy for each user, whose cardinality may vary over a large range. However, this method wastes considerable memory because a large value of m is not necessary for most users, which have small cardinalities. To solve this problem, [46], [49], [53], [54] develop different virtual sketch methods to compress each user's LPC/HLL sketch into a large bit/register array shared by all users. These virtual sketch methods build each user's virtual LPC/HLL sketch using m bits/registers randomly selected from the large bit/register array. This significantly reduces memory usage because each bit/register may be used by more than one user. However, bits/registers in a user's virtual LPC/HLL sketch may be contaminated by other users. We refer to these bits/registers as “noisy” bits/registers. In practice, most users have small cardinalities and their virtual LPC/HLL sketches tend to contain many “noisy” bits/registers, which results in large estimation errors.

Challenge 3: Existing methods are customized to estimate user cardinalities after all the data has been observed and unable to report user cardinalities on the fly. For real-time applications like online anomaly detection, it is important to track user cardinalities in real-time. For example, network monitoring systems are required to detect abnormal IP addresses such as super spreaders (i.e., IP addresses with cardinalities larger than a specified threshold) as soon as possible. Moreover, online monitoring of IP address cardinalities over time also facilitates online detection of stealthy attacks launched from a subclass of IP addresses.

To address the above challenges, we develop two novel streaming algorithms FreeBS and FreeRS to accurately estimate user cardinalities over time. In addition, in many cases such as anomaly detection, we mainly focus on abnormal users with extremely large cardinalities, and there is no need to cost a lot of memory resources to set a counter for each user to keep track of its cardinality. Therefore, we extend both our methods in combination with the Unbiased Space Saving method [43] to detect super spreaders in real-time. We summarize our main contributions as:

Compared with previous bit and register sharing methods, our algorithms FreeBS and FreeRS exploit the dynamic properties of the bit/register array over time to significantly improve the estimation accuracy. The dynamic properties of the bit/register array can be summarized as: 1) The number of bits/registers used by a user dynamically increases as its cardinality increases over time and thus users with larger cardinalities tend to use more bits/registers; 2) The fraction of zeros bits in the bit array and the register values in the register array dynamically change over time. FreeBS and FreeRS incrementally estimate user cardinalities based on the probability of each new arriving user-item pair changing bits/registers in the bit/register array, which decreases over time.

Our algorithms report user cardinality estimations on the fly and allow to track user cardinalities in real-time. The time complexity is reduced from O(m) in state-of-the-art methods CSE [53] and vHLL [49] to O(1) for updating user cardinality estimations each time they observe a new user-item pair.

We extend our algorithms to detect super spreaders whose cardinalities rank top-K among all users in real-time, which is more accurate and faster than existing state-of-the-art methods.

We evaluate the performance of our methods on real-world datasets. Experimental results demonstrate that our methods are orders of magnitude faster and up to 10,000 times more accurate than state-of-the-art methods using the same amount of memory.

The rest of this paper is organized as follows. Section 2 summarizes related work. The problem formulation is presented in Section 3. Section 4 introduces preliminaries. Section 5 presents our algorithms FreeBS and FreeRS. The extension of our methods to detect super spreaders is shown in Section 6. The performance evaluation and testing results are presented in Section 7. Conclusions and future works then follow.

SECTION 2Related Work
Estimating Data Streams’ Cardinalities. In addition to LPC [48] and HLL [17], there are also many methods developed to estimate cardinality for each user. In detail, [7], [15] combine LPC and different sampling methods to enlarge the estimation range of LPC. Flajolet and Martin [18] develop a sketch method FM, which uses a register to estimate the data stream's cardinality bounded by 2w, where w is the number of bits in the register. To further improve the accuracy of [18], sketch methods MinCount [3], LogLog [12], HyperLogLog++ [24], RoughEstimator [28], and HLL-TailCut+ [50] are developed to use a list of m registers and compress the size of each register from 32 bits to 5 or 4 bits. [20], [34] develop several cardinality estimators based on order statistics of observed samples. Ting [42] introduces the concept of an area cutting process to generally model the above sketch methods and provides a martingale based estimator to further improve the accuracy of these methods. Chen et al. [8] extend HLL to estimate the cardinality over sliding windows. Besides these sketch methods, Wegman's adaptive sampling [16] and distinct sampling [19] are also developed for estimating a large stream's cardinality. Furthermore, Harmouch et al. [23] summarize existing cardinality estimation methods and present experiments to evaluate their performance.

Estimating All User Cardinalities. To achieve a desired accuracy, above methods require a large number of bits/registers for each user because user cardinalities are unknown in advance and vary over a large range. This is not memory-efficient because most users have small cardinalities. To solve this problem, [46], [49], [53], [54] develop different virtual sketch methods to compress the LPC/HLL sketches of all users into a large shared bit/register array shared by all users. To reduce “noisy” bits in virtual LPC sketches, [41] builds a small regular LPC sketch for each user, which also provides information for estimating the user cardinality. All these virtual sketch methods have to set a large value for the virtual sketch size to achieve desired accuracy for user cardinalities over a large range. However, it results in high computation costs and large estimation errors, because mosts users have small cardinalities and many bits/registers in their virtual sketches tend to be contaminated by “noisy” bits. In addition, the above virtual sketch methods are customized to non-streaming settings, i.e., estimate host cardinalities at the end of an interval, and are computationally expensive to be extended to streaming settings.

Detecting Heavy Hitters or Super Spreads. Heavy hitter and super spreader are two different but important anomaly user behaviors. Usually heavy hitters are defined as users whose frequencies are larger than a given threshold, while super spreaders are defined as users whose cardinalities exceed a specific threshold. To detect heavy hitters, Graham et al. [10] present both deterministic sample and randomized sketch algorithms to find heavy hitters in graph streams consisting of item insertions and deletions. [11] calculates heavy hitters in multiple dimensions at a high speed with very small memory space. Cristian et al. [13], [14] present two algorithms sample and hold and multistage filters to detect heavy hitters without storing entire small network flows. Also, [26], [30], [33], [52] develop general sketch algorithms to detect a variety of network measurements at the same time. Specially, [52] is faster and more memory efficient because it divides users into a heavy part and a light part according to their frequencies, which is consistent with the basic idea in [51]. Moreover, to detect top-K heavy hitters, Metwally et al. [35] present a Stream-Summary data structure but it is biased for each arriving element. Ting [43] later proposes a probabilistic model to replace the minimum element in the data structure with a varied probability to reduce the estimation bias. The policy of updating the minimum counter in [43] is consistent with [4], which is developed for solving frequent and top-K heavy hitter estimation problems. In addition, Basat et al. [4] further propose a hardware-friendly method by dividing the logical locations into a variety of sets containing a fixed number of elements. Gong et al. [21] introduce a novel strategy count-with-exponential-decay to achieve high precision in finding top-K heavy hitters, but it is not memory-efficient in comparison with [35], [43]. Schweller et al. [38] develop reversible sketch data structure and reverse hashing to monitor users whose frequencies change significantly from one period to another.

When there exist edge duplicates in graph streams, the heavy hitter estimation methods cannot be directly used to detect super spreaders, and one requires to use a hash table to store distinct items. Guan et al. [22] present a connection degree sketch based method and use Chinese Remainder Theorem to monitor and reverse super spreaders. This method is also implemented in [46], [47]. Besides, Liu et al. [32] propose a novel mergeable and reversible data structure for distributed network monitoring system to detect super spreaders. Liu et al. [31] further introduces a Vector Bloom Filter (VBF), which uses Bit-Extraction hash function to improve the cardinality estimation with lower memory usage and less computation time. Although above methods can utilize well-designed hash functions to reverse users without storing them, the computation costs are large because they require to enumerate and combine almost all possible hash values to reconstruct users, which is not practicable to detect super spreaders over time. Also, these methods always return false users not actually occurred in graph streams and introduce large estimation errors. In addition, there are also many sampling based methods to detect super spreaders. Venkataraman et al. [45] first propose flow sampling based streaming algorithms to detect super spreaders and provide proven accuracy and performance bounds. [27] further presents adaptive sampling methods to detect super spreaders in real time. Cao et al. [6] show an online sampling approach to identify super spreaders whose cardinalities exceed a threshold or within specific ranges. Tang et al. [40] propose a reservoir sampling based method but it is biased. Shi et al. [39] combine sampling and streaming algorithms and introduce a “Filter-Tracker-Digester” framework to catch top spreaders and scanners online. All these sampling methods first filter out users with small cardinalities when implemented to detect super spreaders, but they cannot guarantee all super spreaders are sampled and exhibit large errors using their cardinality estimation methods.

SECTION 3Problem Formulation
To formally define our problem, we first introduce some notations. Let Γ=e(1)e(2)⋯ be the graph stream of interest consisting of a sequence of edges. Note that an edge in Γ may appear more than once. In this paper, we focus on bipartite graph streams consisting of edges between users and items. Our methods, however, can be easily extended to unipartite graphs such as computer networks where each edge represents the network communication between a source IP address and a destination IP address, and online social networks where each edge represents the social relation between a user and his/her friend. Let S and D denote the user and item sets respectively. For t=1,2,…, let e(t)=(s(t),d(t)) denote the tth edge of Γ, where s(t)∈S and d(t)∈D are the user and the item of e(t) respectively. Let N(t)s denote the set of distinct items that user s connects to before and including time t. Define n(t)s=|N(t)s| to be the cardinality of user s at time t. Then, n(t)=∑s∈S|N(t)s| is the sum of all user cardinalities at time t. In this paper, we develop fast and accurate methods for estimating user cardinalities at times t=1,2,… using a limited amount of memory. Besides, we aim to monitor top-K super spreaders based on the cardinality estimation results. When no confusion arises, we omit the superscript (t) to ease exposition.

SECTION 4Preliminaries
4.1 Estimating a Single User's Cardinality
4.1.1 Linear-Time Probabilistic Counting
For a user s∈S, Linear-Time Probabilistic Counting [48] builds a sketch Bs to store the set of items that s connects to, i.e., N(t)s. Formally, Bs is defined as a bit array consisting of m bits, which are initialized to 0. Let h(d) be a uniform hash function with range {1,…,m}. When user-item pair (s,d) arrives, the h(d)th bit in Bs is set to one, i.e., Bs[h(d)]=1. For any bit Bs[i], 1≤i≤m, the probability that it remains zero at time t is P(Bs[i]=0)=(1−1m)n(t)s. Denote by U(t)s the number of zero bits in Bs at time t. Then, the expectation of U(t)s is computed as E(U(t)s)=∑mi=1P(Bs[i]=0)≈me−n(t)sm. Based on the above equation, when U(t)s>0, Whang et al. [48] estimate n(t)s as n^(t,LPC)s=−mlnU(t)sm. The weakness of LPC is that it has a limited estimation range and can only estimate user cardinalities in range [0,mlnm].

4.1.2 HyperLogLog
To estimate the cardinality of user s, HyperLogLog [17] is developed based on the Flajolet-Martin (FM) sketch [18] Rs consisting of m registers Rs[1],…,Rs[m]. All m registers are initialized to 0. For 1≤i≤m, let R(t)s[i] be the value of Rs[i] at time t. When a user-item pair (s,d) arrives, HLL maps the item into a pair of random variables h(d) and ρ(d), where h(d) is an integer uniformly selected from {1, …, m} at random and ρ(d) is drawn from a Geometric(1/2) distribution, P(ρ(d)=k)=12k for k=1,2,….1 Then, register Rs[h(d)] is updated as R(t)s[h(d)]←max{R(t−1)s[h(d)],ρ(d)}. At time t, Flajolet et al. [17] estimate n(t)s as n^(t,HLL)s=αmm2∑mi=12−R(t)s[i], where αm=(m∫∞0(log22+x1+x)mdx)−1 is a constant to correct bias. The above formula for αm is complicated. In practice, αm is computed numerically, e.g., α16≈0.673, α32≈0.697, α64≈0.709, and αm≈0.7213/(1+1.079/m) for m≥128.

Since n^(t,HLL)s is severely biased for small cardinalities, HLL treats Rs as an LPC sketch of m bits when αmm2∑mi=12−R(t)s[i]<2.5m. In this case, n(t)s is estimated as −mlnU~(t)sm, where U~(t)s is the number of zero registers among Rs[1], …, Rs[m] at time t. Therefore, we easily find that LPC outperforms HLL for small cardinalities under the same memory usage.

4.1.3 Discussions
To compute all user cardinalities, one can use an LPC or HLL sketch to estimate each user cardinality. Clearly, using a small m, LPC and HLL will exhibit large errors for users with large cardinalities. Most user cardinalities are small and assigning an LPC/HLL sketch with large m to each user in order to accurately estimate large user cardinalities is wasteful as LPC and HLL do not require to set a large m to achieve reasonable estimation accuracy for users with small cardinalities. In practice, the user cardinalities are not known in advance and vary over time. Therefore, it is difficult to set an optimal value of m when using LPC and HLL to estimate all user cardinalities. In the next subsection, we introduce state-of-the-art methods to address this problem, and also discuss their shortcomings.

4.2 Estimating All User Cardinalities
4.2.1 CSE: Compressing LPC Sketches of All Users into a Shared Bit Array
As shown in Fig. 1a, CSE [53] consists of a large bit array A and m independent hash functions f1(s),…,fm(s), each mapping users to {1,…,M}, where M is the length of the one dimensional bit array A. Similar to LPC, CSE builds a virtual LPC sketch for each user and embeds LPC sketches of all users in A. For user s, its virtual LPC sketch B^s consists of m bits selected randomly from A by the group of hash functions f1(s),…,fm(s), i.e., B^s=(A[f1(s)],…,A[fm(s)]). Each bit in A is initially set to zero. When a user-item pair (s,d) arrives, CSE sets the h(d)th bit in B^s to one. Similar to LPC, h(d) is a uniform hash function with range {1,…,m}. Since the h(d)th element in B^s is bit A[fh(d)(s)], CSE only needs to set bit A[fh(d)(s)], i.e., A[fh(d)(s)]←1. Let U^(t)s and U(t) be the number of zero bits in B^s and A at time t respectively. A user's virtual LPC sketch can be viewed as a regular LPC sketch containing “noisy” bits (e.g., the bit in red and bold in Fig. 1a), that are wrongly set from zero to one by items of other users. To remove the estimation error introduced by “noisy” bits, Yoon et al. [53] estimate n(t)s as n^(t,CSE)s=−mlnU^(t)sm+mlnU(t)M. On the right-hand side of the above equation, the first term is the same as the regular LPC, and the second term corrects the error introduced by “noisy” bits.


Fig. 1.
Overview of bit sharing method CSE and register sharing method vHLL. Virtual CSE/vHLL sketches of users may contain “noisy” bits/registers (e.g., the bit and register in red and bold in the figure).

Show All

4.2.2 vHLL: Compressing HLL Sketches of All Users into a Shared Bit Array
Xiao et al. [49] develop a register sharing method, vHLL, which extends the HLL method to estimate all user cardinalities. vHLL consists of a list of M registers R[1],…,R[M], which are initialized to zero. To maintain the virtual HLL sketch R^s of a user s, vHLL uses m independent hash functions f1(s),…,fm(s) to randomly select m registers from all registers R[1],…,R[M], where each function f1(s),…,fm(s) maps users to {1,…,M}. Formally, R^s is defined as R^s=(R[f1(s)],…,R[fm(s)]).

For 1≤i≤M, let R(t)[i] be the value of R[i] at time t. When a user-item pair (s,d) arrives, it maps the item to a pair of two random variables h(d) and ρ(d), where h(d) is an integer uniformly selected from {1, ..., m} at random, and ρ(d) is a random integer drawn from a Geometric(1/2) distribution, which is similar to HLL. We can easily find that the h(d)th element in the virtual HLL sketch of user s is R[fh(d)(s)], therefore, vHLL only needs to update register R[fh(d)(s)] as R(t)[fh(d)(s)]←max{R(t−1)[fh(d)(s)],ρ(d)}. A user's virtual HLL sketch can be viewed as a regular HLL containing “noisy” registers (e.g., the register in red and bold in Fig. 1b), which are wrongly set by items of other users. To remove the estimation error introduced by “noisy” registers, Xiao et al. [49] estimate n(t)s as n^(t,vHLL)s=MM−m(αmm2∑mi=12−R(t)[fi(s)]−mαMM∑Mi=12−R(t)[i]), where αm is the same as that of HLL. For the two terms between the parentheses on the right-hand side of the above equation, the first term is the same as the regular HLL, and the second term corrects the error introduced by “noisy” registers. Similar to the regular HLL, the first term is replaced by −mlnU^(t)sm when αmm2∑mi=12−R(t)[fi(s)]<2.5m where U^(t)s is the number of registers among R[f1(s)] ,…, R[fm(s)] that equal 0 at time t.

4.2.3 Discussions
LPC/HLL builds a single bit/register array for each user and each user cardinality is not affected by others. To achieve the desired estimation accuracy, LPC/HLL requires to set a large size m for each user and it results in a waste of memory space especially for users with small cardinalities. On the contrary, CSE/vHLL is more memory-efficient because it allows each bit/register in the shared bit/register array to be used by other users, but it cannot solve the following challenges:

Challenge 1: It is difficult to set parameter m for both CSE and vHLL. The estimation accuracy of CSE and vHLL highly depends on the value of m. Increasing m introduces more “unused” bits in virtual LPC sketches of occurred users, which can become contaminated with noise. Here “unused” bits refer to the bits in a user's virtual LPC sketch that no user-item pairs of the user are hashed into. However, decreasing m introduces large estimation errors for users with large cardinalities. Similarly, vHLL also confronts the same challenge in determining an optimal m. Later our experimental results will also verify that errors increase with m for users with small cardinalities under CSE and vHLL.

Challenge 2: It is computationally intensive to estimate user cardinalities for all values of t. At each time, both CSE and vHLL require time complexity O(m) to compute the cardinality of a single user. When applied to compute cardinalities for all users in S at all times, CSE and vHLL have to be repeatedly called and will incur a high computational cost, which prohibits their application to high speed streams in an on-line manner.

SECTION 5Our Methods
In this section, we present our streaming algorithms FreeBS and FreeRS for estimating user cardinalities over time, which are designed based on two novel bit sharing and register sharing techniques respectively. It is not necessary to assign more bits/registers for users with small cardinalities because there will not be a significant improvement for their cardinality estimation accuracy. There should be more bits/registers provided for users with large cardinalities to ensure estimation performance. However, existing sharing methods waste bits/registers for users with small cardinalities while limiting the estimation accuracy for users with large cardinalities. Therefore, unlike vHLL/CSE mapping each user's items into m≪M bits/registers, FreeBS/FreeRS randomly maps user-item pairs into a large shared bit/register array. The corresponding bits/registers of a user are sampled according to its cardinality (i.e., the number of connected items). Thus, users with larger cardinalities tend to use more bits/registers. For each user-item pair e(t)=(s(t),d(t)) occurred at time t, we discard it when updating e(t) does not change any bit/register in the bit/register array shared by all users. Otherwise, e(t) is a new user-item pair that does not occur before time t, and we increase the cardinality estimation of user s(t) by 1q(t) [25], where q(t) is defined as the probability that a new user-item pair changes any bit/register in the bit/register array at time t.

5.1 FreeBS: Parameter-Free Bit Sharing
Data Structure. The data structure of FreeBS is shown in Fig. 2. FreeBS consists of a one-dimensional bit array B of length M, where each bit B[j], 1≤j≤M, is initialized to zero. In addition, FreeBS uses a hash function h∗(e) to uniformly and independently map each user-item pair e=(s,d) to an integer in {1,…,M} at random, i.e., P(h∗(e)=i)=1M, and P(h∗(e)=i∧h∗(e′)=i′|e≠e′)=1M2, i,i′∈{1,…,M}. Note that h∗(e) differs from the hash function h(s) used by CSE that maps user s to an integer in {1,…,M} at random.

Fig. 2. - 
Overview of our method FreeBS.
Fig. 2.
Overview of our method FreeBS.

Show All

Update Procedure. The pseudo-code for FreeBS is shown as Algorithm 1. When a user-item pair e=(s,d) arrives at time t, FreeBS first computes a random variable h∗(e), and then sets B[h∗(e)] to one, i.e., B[h∗(e)]←1. Let B(t)0 denote the set of the indices corresponding to zero bits in B at time t. Formally, B(t)0 is defined as B(t)0={i:B(t)[i]=0,1≤i≤M}. Let n^(t,FreeBS)s denote the cardinality estimate for user s at time t. We initialize n^(0,FreeBS)s to 0. Let m(t)0=|B(t)0| denote the number of zero bits in B at time t. Let q(t)B denote the probability of e changing a bit in B from 0 to 1 at time t, and q(t)B is defined as q(t)B=∑i∈B(t−1)0P(h∗(e)=i)=|B(t−1)0|M=m(t−1)0M. Let 1(P) denote the indicator function that equals 1 when predicate P is true and 0 otherwise. Besides setting B[h∗(e)] to 1 at time t with the arrival of user-item pair e=(s,d), we also update the cardinality estimate of user s as
n^(t,FreeBS)s←n^(t−1,FreeBS)s+1(B(t−1)[h∗(e)]=0)q(t)B.
View SourceRight-click on figure for MathML and additional features.For any other user s′∈S∖{s}, we keep its cardinality estimate unchanged, i.e., n^(t,FreeBS)s′←n^(t−1,FreeBS)s′. We easily find that q(t)B can be fast computed incrementally. That is, we initialize q(1)B to 1, and incrementally compute q(t+1)B as q(t+1)B←q(t)B−1(B(t−1)[h∗(e)]=0)M,t≥1. Hence, the time complexity of FreeBS for processing each user-item pair is O(1).

Algorithm 1. The Pseudo-Code for FreeBS
B[1,…,M]←[0,…,0];

n^(FreeBS)s←0, s∈S;m0←M;

foreach e=(s,d) in Γ do

if B[h∗(e)]==0 then

B[h∗(e)]←1;

n^(FreeBS)s←n^(FreeBS)s+Mm0;

m0←m0−1;

end

end

Error Analysis. Let T(t)s denote the set of the first occurrence times of user-item pairs associated with user s in stream Γ, i.e., T(t)s={i:s(i)=s∧e(j)≠e(i),0<j<i≤t}.

Theorem 1.
The expectation and variance of n^(t,FreeBS)s are
E(n^(t,FreeBS)s)=n(t)s,
View Source
Var(n^(t,FreeBS)s)=∑i∈T(t)sE(1q(i)B)−n(t)s,
View Sourcewhere E(1q(i)B)≈en(i)M(1+1M(en(i)M−n(i)M−1)).

Proof.
Let δe denote an indicator that equals 1 when updating a user-item pair e incurs a value change of B[h∗(e)] (i.e., B[h∗(e)] changes from 0 to 1), and 0 otherwise. Let η(i) denote a random variable that represents the update probability of an arriving user-item pair e=(s,d) changing a bit in the bit array from 0 to 1 at time i. Each δe(i) is calculated based on the current bit array (i.e., the probability q(i)B). Then we have
E(δe(i)|η(i)=q(i)B)=q(i)B,
View Source
Var(δe(i)|η(i)=q(i)B)=q(i)B−(q(i)B)2,1≤i≤t.
View Sourcen^(t,FreeBS)s is incrementally calculated based on all random variables δe(i) and their corresponding probabilities q(i)B, i∈T(t)s. n^(t,FreeBS)s=∑i∈T(t)sδe(i)q(i)B. Then, we have
E(n^(t,FreeBS)s|η(1)=q(1)B,…,η(t)=q(t)B)=E⎛⎝⎜∑i∈T(t)sδe(i)q(i)B∣∣∣η(1)=q(1)B,…,η(t)=q(t)B⎞⎠⎟=∑i∈T(t)sE(δe(i)|η(1)=q(1)B,…,η(t)=q(t)B)q(i)B=n(t)s.
View SourceRight-click on figure for MathML and additional features.E(δe(i)|η(1)=q(1)B,…,η(t)=q(t)B)=E(δe(i)|η(i)=q(i)B)=q(i)B. This is because all random variables δe(i), i∈T(t)s, are independent of each other and each of them only has association with the variable η(i) at the current time when given a sequence of update probabilities η(1),…,η(t). Then, we have
E(n^(t,FreeBS)s)=E(E(n^(t,FreeBS)s|η(1)=q(1)B,…,η(t)=q(t)B))=E(n(t)s)=n(t)s.
View SourceThe variance of n^(t,FreeBS)s is computed as
Var(n^(t,FreeBS)s|η(1)=q(1)B,…,η(t)=q(t)B)=Var⎛⎝⎜∑i∈T(t)sδe(i)q(i)B∣∣∣η(1)=q(1)B,…,η(t)=q(t)B⎞⎠⎟=∑i∈T(t)sVar(δe(i)|η(1)=q(1)B,…,η(t)=q(t)B)(q(i)B)2=∑i∈T(t)sq(i)B−(q(i)B)2(q(i)B)2=∑i∈T(t)s1q(i)B−n(t)s.
View SourceSince Var(E(n^(t,FreeBS)s|η(1)=q(1)B,…,η(t)=q(t)B))=0, using the equation Var(X)=Var(E(X|Y))+E(Var(X|Y)), we have
Var(n^(t,FreeBS)s)=E(Var(n^(t,FreeBS)s|η(1)=q(1)B,…,η(t)=q(t)B))=E⎛⎝⎜∑i∈T(t)s1q(i)B⎞⎠⎟−n(t)s=∑i∈T(t)sE(1q(i)B)−n(t)s.
View Source

In what follows we derive the formula for E(1q(i)B). For j specific distinct bits in B, there exist j!τ(n(i),j) ways to map n(i) distinct user-item pairs occurred in stream Γ before and including time i into these bits given that each bit has at least one user-item pair, where τ(n(i),j), the Stirling number of the second kind [2], is computed as Undefined control sequence \mathchoice. In addition, there exist Undefined control sequence \mathchoice ways to select j distinct bits from B, therefore we have Undefined control sequence \mathchoice Then, the expected value of 1q(i)B can be computed as
Undefined control sequence \mathchoice
View SourceRight-click on figure for MathML and additional features.We expand the function 1q(i)B by its Taylor series around E(q(i)B) as
E(1q(i)B)≈E(1E(q(i)B)−q(i)B−E(q(i)B)(E(q(i)B))2+(q(i)B−E(q(i)B))2(E(q(i)B))3)=1E(q(i)B)+Var(q(i)B)(E(q(i)B))3.
View SourceFrom Eqs. (5) and (6) in [48], we easily have E(q(i)B)=e−n(i)M and Var(q(i)B)=1Me−n(i)M(1−(1+n(i)M)e−n(i)M). Then, we obtain E(q(i)B)≈en(i)M(1+1M(en(i)M−n(i)M−1)).

5.2 FreeRS: Parameter-Free Register Sharing
Data Structure. The data structure of FreeRS is shown in Fig. 3. FreeRS consists of M registers R[1] ,…, R[M], which are initialized to zero. In addition, FreeRS also uses a hash function h∗(e) to randomly map each user-item pair e=(s,d) to an integer in {1,…,M} and another function ρ∗(e) that maps e to a random integer in {1,2,…} according to a Geometric(1/2) distribution. Note that h∗(e) and ρ∗(e) differ from hash functions h(s) and ρ(s) used by vHLL, which map user u to a random integer in {1,…,M} and {1,2,…}, respectively.

Algorithm 2. The Pseudo-Code for FreeRS
R[1,…,M]←[0,…,0];

n^(FreeRS)s←0, s∈S;qR←1;

foreach e=(s,d)∈Γ do

if ρ∗(e)>R[h∗(e)] then

n^(FreeRS)s←n^(FreeRS)s+1qR;

qR←qR+2−ρ∗(e)−2−R[h∗(e)]M;

R[h∗(e)]←ρ∗(e);

end

end


Fig. 3.
Overview of our method FreeRS.

Show All

Update Procedure. The pseudo-code for FreeRS is shown as Algorithm 2. When user-item pair e=(s,d) arrives at time t, FreeRS first computes two random variables h∗(e) and ρ∗(e), and then updates R(t)[h∗(e)] as R(t)[h∗(e)]←max{R(t−1)[h∗(e)],ρ∗(e)}. Let q(t)R denote the probability of e changing the value of a register among R[1],…,R[M] at time t. Formally, q(t)R is defined as q(t)R=∑Mj=1P(h∗(e)=j∧R(t)[j]>R(t−1)[j])=∑Mj=12−R(t−1)[j]M.

Let n^(t,FreeRS)s denote the cardinality estimate of user s at time t. When user-item pair e=(s,d) arrives at time t, we update the cardinality estimate of user s as
n^(t,FreeRS)s←n^(t−1,FreeRS)s+1(R(t)[h∗(e)]≠R(t−1)[h∗(e)])q(t)R.
View SourceFor any other user s′∈S∖{s}, we keep its cardinality estimate unchanged, i.e., n^(t,FreeRS)s′←n^(t−1,FreeRS)s′. Similar to q(t)B, we initialize q(1)R=1 and incrementally compute q(t+1)R as q(t+1)R←q(t)R+2−ρ∗(e)−2−R[h∗(e)]m1(R(t)[h∗(e)]≠R(t−1)[h∗(e)]). Hence, the time complexity of FreeRS for processing each user-item pair is also O(1).

Error Analysis. We derive the error of n^(t,FreeRS)s as follows:

Theorem 2.
The expectation and variance of n^(t,FreeRS)s are
E(n^(t,FreeRS)s)=n(t)s,
View SourceRight-click on figure for MathML and additional features.
Var(n^(t,FreeRS)s)=∑i∈T(t)sE(1q(i)R)−n(t)s,
View SourceRight-click on figure for MathML and additional features.where
Undefined control sequence \mathchoice
View Sourcewith
γnj,kj=⎧⎩⎨⎪⎪(1−2−kj)nj−(1−2−kj+1)nj,0,1,nj>0,kj>0nj>0,kj=0nj=0,kj=0.
View SourceE(1q(i)R) is approximately 1.386n(i)M when n(i)>2.5M.

Proof.
The proof of the above theorem is similar with that of Theorem 1. Here we derive the formula for E(1q(i)R). Using h∗(⋅), FreeRS randomly splits stream Γ into M sub-streams Γj, 1≤j≤M. Each R[j] tracks the maximum value of function ρ∗(⋅) for user-item pairs in sub-stream Γj. At time i, assume that nj distinct user-item pairs have occurred in Γj. Then, P(R(i)[j]=kj|nj)=γnj,kj. Therefore,
Undefined control sequence \mathchoice
View SourceAn exact expression for E(1q(i)R) is easily derived. However, it is too complex to analyze. Hence, we introduce a method to approximate E(1q(i)R). From [17], we have E(αMMq(i)R)=αMME(1q(i)R)≈n(i) for n(i)>2.5M. Thus, E(1q(i)R)≈n(i)αMM≈1.386n(i)M.

5.3 Discussions
FreeBS versus CSE. FreeBS outperforms CSE in three aspects: (1) FreeBS can estimate cardinalities up to ∑Mi=1Mi≈MlnM, which is larger than the maximum cardinality mlnm allowed by CSE; (2) FreeBS exhibits a smaller estimation error than CSE. The expectation and variance of CSE are computed in [41] as
E(n^(t,CSE)s)≈n(t)s+12(E(1q(t))en(t)sm−n(t)sm−1),
View Source
Var(n^(t,CSE)s)≈m(E(1q(t))en(t)sm−n(t)sm−1),
View SourceRight-click on figure for MathML and additional features.where q(t)=U(t)M is the fraction of zero bits in the shared bit array at time t. We find that CSE exhibits a large bias when n(t)s≫m, while FreeBS is unbiased. When m is large (e.g., m approaches to M), FreeBS and CSE perform nearly the same bit setting operations but use their different cardinality estimators. Then, we have E(1q(t))≈E(1q(t)B). From Theorem 2, we have
Var(n^(t,CSE)s)⪆m(E(1q(t))(1+n(t)sm)−n(t)sm−1)>n(t)sE(1q(t))−n(t)s⪆Var(n^(t,FreeBS)s);
View Source3) FreeBS has complexity O(1) to update user cardinality estimates each time it observes a new user-item pair, while CSE has complexity O(m).

FreeRS versus vHLL. FreeRS outperforms vHLL in two aspects: 1) FreeRS has complexity O(1) to update user cardinality estimates each time it observes a new user-item pair, while vHLL has complexity O(m); 2) FreeRS exhibits a smaller estimation error in comparison with vHLL. From Theorem 2, we have Var(n^(t,FreeRS)s)≤n(t)s(E(1q(t)R)−1)≈n(t)s(n(t)MαM−1)<1.386n(t)n(t)sM, while the variance of vHLL is
Var(n^(t,vHLL)s)⪆(MM−m)2×1.042m×2n(t)n(t)smM(1−mM)=2.163n(t)n(t)sM−m>2.163n(t)n(t)sM.
View SourceFreeBS versus FreeRS. We observe that 1) FreeBS is faster than FreeRS. For each coming user-item pair e, FreeBS only computes h∗(e) to select and set a bit, but FreeRS needs to compute both h∗(e) and ρ∗(e) to select and update a register; 2) Under the same memory usage, we compare the accuracy of FreeBS using M bits and FreeRS using M/w registers, where w is the number of bits in a register. From Theorems 1 and 2, we have
Var(n^(t,FreeBS)s)=∑i∈T(t)sE(1q(i)B)−n(t)s,
View SourceRight-click on figure for MathML and additional features.
Var(n^(t,FreeRS)s)=∑i∈T(t)sE(1q(i)R)−n(t)s,
View SourceRight-click on figure for MathML and additional features.where E(1q(i)B)≈en(i)M and E(1q(i)R)≈1.386wn(i)M<en(i)M when n(i)M≥0.772w. Therefore, FreeRS is more accurate than FreeBS for users not appearing among the first 0.772wM distinct user-item pairs presented on stream Γ. Flajolet et al. [17] observe that HLL exhibits a large error for estimating small cardinalities. To solve this problem, they view a register of HLL as a bit of LPC and estimate the cardinality based on the fraction of registers that equal 0. When n(i)≪M/w, we easily find that q(i)R is approximately computed as the fraction of registers that equal 0 at time i, and we have E(1q(i)B)<E(1q(i)R) because the number of bits in FreeBS is w times larger than that of registers in FreeRS under the same memory usage. It indicates that FreeBS is more accurate than FreeRS for users whose user-item pairs appear early in stream Γ.

Limitations. The limitation of all above methods to estimate all users’ cardinalities over time is that they require to store users together with their estimated cardinalities. We consider this limitation in the following cases: case 1) When exploiting all these methods to compute user cardinalities in graph streams consisting of a limited number of users of interest (e.g., monitoring the network connection counts of hosts in a campus), the number of users is not large enough. Therefore, storing all users and their cardinalities requires a limited amount of memory space and so it is practical. case 2) When estimating user cardinalities for graph streams consisting of a large number of users of interest (e.g., monitoring the network connection counts of hosts on edge routers between internal and external networks), it is impractical to record all users and cardinalities due to limited memory resources. However, many applications such as anomaly detection only focus on users with extremely large cardinalities (i.e., super spreaders). Therefore, we can implement sketch methods such as Space Saving [35] and Unbiased Space Saving [43] to store only a small number of users and their cardinalities.

SECTION 6Extension: Super Spreader Detection
For estimating cardinalities in graph streams over time, above methods all require to set an extra counter for each user to keep track of its corresponding cardinality. However, many real-world datasets contain millions of or more users, which will cost extremely large memory resources. Meanwhile, real-world datasets are always with skewed distribution (i.e., there are few users with large cardinalities), and in many cases such as anomaly detection, we mainly focus on monitoring abnormal users with large cardinalities rather than all users. As mentioned above, existing frequency estimation methods cannot be directly used for complex super spreader detection due to edge duplicates in graph streams, while existing super spreader detection methods exhibit large estimation errors. Therefore, in this section, we extend both our methods and combine them with the Unbiased Space Saving method [43] to monitor top-K super spreaders in real-time. Unbiased Space Saving almost implements the data structure and update procedure of Space Saving method [35], but replaces the minimum element with a varied probability to make it unbiased. The basic idea of our methods is that we use FreeBS/FreeRS to handle edge duplicates in graph streams and accurately estimate user cardinalities, and then utilize Unbiased Space Saving method to monitor top-K super spreaders with small time complexity.

6.1 FreeBS-SSD: FreeBS for Super Spreader Detection
Data Structure. The pseudo-code for FreeBS-SSD is shown as Algorithm 3. FreeBS-SSD consists of two data structures, a one-dimension bit array B of length M used to record connected items and estimate cardinalities for all users over time, and a Stream-Summary data structure H with l buckets used to store users with large cardinalities. Specifically, the Stream-Summary data structure is a doubly linked list, where each bucket is initialized as ∅ and contains three parameters, user s, counter n^(FreeBS−SSD)s used to record its estimated cardinality, and its maximum over-estimation εs. Note there should be a sufficient number of buckets to record super spreaders, i.e., the number of buckets is larger than that of super spreaders. Meanwhile, all buckets in H are ranked according to values of their counters.

Algorithm 3. The Pseudo-Code for FreeBS-SSD
B[1,…,M]←[0,…,0];H[1,…,l]←[∅,…,∅];

n^(FreeBS−SSD)s←0, s∈S;m0←M;

foreach e=(s,d) in Γ do

if B[h∗(e)]==0 then

B[h∗(e)]←1;

qB←m0M;

if s∈H then

n^(FreeBS−SSD)s←n^(FreeBS−SSD)s+1qB;

rerank buckets in H;

else

if H is full then

pt←1/qBn^(FreeBS−SSD)min+1/qB;

r←Rand(0,1);

if r≤pt then

replace smin with s;

εs←n^(FreeBS−SSD)min;

n^(FreeBS−SSD)s←n^(FreeBS−SSD)min+1qB;

rerank buckets in H;

else

n^(FreeBS−SSD)min←n^(FreeBS−SSD)min+1qB;

rerank buckets in H;

end

else

n^(FreeBS−SSD)s←n^(FreeBS−SSD)s+1qB;

εs←0;

insert s into H and rerank buckets in H;

end

end

m0←m0−1;

end

end

/* n^(FreeBS−SSD)min is the minimum counter, smin is the user in this bucket. */

Update Procedure. When a user-item pair e=(s,d) arrives at time t, FreeBS-SSD first updates the bit array B as described in Algorithm 1, and then updates Stream-Summary H. In detail, when user s is already monitored in H, i.e., the bucket of user s is contained in H, we directly update its corresponding counter as n^(t,FreeBS−SSD)s←n^(t−1,FreeBS−SSD)s+1q(t)B, where q(t)B=m(t−1)0M is the probability of a user-item pair e changing a bit in B from 0 to 1 at time t. Then we rerank buckets in H according to values of their counters. Because all buckets in H are always-sorted, we only need to move the updated bucket forward into a position where its counter is smaller than that of the prior bucket and larger than the next bucket. In other words, we usually need to move the updated bucket forward a bucket to keep H ranked. On the contrary, when the arriving user s is not monitored in H, we consider two different scenarios: scenario 1) H is full and there are no empty buckets left, we choose to update the bucket with the minimum counter. Let n^(t,FreeBS−SSD)min denote the minimum counter in H at time t, and its corresponding user is represented as smin. We use a probability pt=1/q(t)Bn^(t,FreeBS−SSD)min+1/q(t)B and compare it with a random number r in (0,1). If this condition is met, we update the bucket as smin←s,εs←n^(t,FreeBS−SSD)min,n^(t,FreeBS−SSD)s←n^(t,FreeBS−SSD)min+1q(t)B, and then rerank buckets in H according to their counters. Otherwise, we update the minimum counter as n^(t,FreeBS−SSD)min←n^(t,FreeBS−SSD)min+1q(t)B, and rerank buckets in H; scenario 2) There are still empty buckets in H, we initialize the bucket of user s and insert it into a suitable position according to its counter. Specially, the over-estimation of user s is initialized as εs←0. Therefore, the time complexity of FreeBS-SSD to update the buckets in H for each arriving user-item pair is O(1).

Error Analysis. According to the data structure and update procedure of FreeBS-SSD, we have the following theorem.

Theorem 3.
For any user s, FreeBS-SSD gives an unbiased estimation of its cardinality n(t,FreeBS−SSD)s at any time t.

Proof.
Here we simplify n^(t,FreeBS−SSD)s as n^(t)s to ease exposition. There are two scenarios that n^(t)s will be changed by the arriving user: 1) user s is the next arriving user at time t+1, the estimated cardinality is updated as n^(t+1)s←n^(t)s+1q(t+1)B. As mentioned in Theorems 1, this increment is an unbiased estimation; 2) user s is not the next arriving user at time t+1 but its counter n^(t)s is the minimum one in H. In this case, n^(t)s is changed with probability pt=1/q(t)Bn^(t)s+1/q(t)B, i.e., n^(t+1)s=n^(t)s happens with probability 1−pt, and the expected increment to be updated is computed as
E(n^(t+1)s−n^(t)s)=(n^(t)s+1q(t)B)(1−1/q(t)Bn^(t)s+1/q(t)B)−n^(t)s=0.
View SourceTherefore, FreeBS-SSD shows an unbiased estimation of its cardinality n(t,FreeBS−SSD)s at any time t.

6.2 FreeRS-SSD: FreeRS for Super Spreader Detection
Data Structure. The pseudo-code for FreeRS-SSD is shown as Algorithm 4. FreeRS-SSD consists of a register array of M registers R[1],…,R[M] to record connected items and estimate cardinalities for all users, and a Stream-Summary data structure H of l buckets to store users with large cardinalities.

Update Procedure. When a user-item pair e=(s,d) arrives at time t, FreeRS-SSD follows the same procedure of FreeRS to update the register array, and implements a similar procedure as FreeBS-SSD to update buckets in H. Specifically, when user s is contained in H, we directly update its counter as n^(t,FreeRS−SSD)s←n^(t−1,FreeRS−SSD)s+1q(t)R, where q(t)R←q(t−1)R+2−ρ∗(e)−2−R[h∗(e)]M denotes the probability of changing the value of a register among R[1],…,R[M]. Then we move the updated bucket into a position of H according to its counter to keep H ranked. When user s is not monitored in H, we also consider the following scenarios: scenario 1) H is full, then we update the bucket with the minimum counter as smin←s,εs←n^(t,FreeRS−SSD)min,n^(t,FreeRS−SSD)s←n^(t,FreeRS−SSD)min+1q(t)R with the probability pt=1/q(t)Rn^(t,FreeRS−SSD)min+1/q(t)R, and rerank buckets in H according to their counters. Otherwise, we only update the bucket as n^(t,FreeRS−SSD)min←n^(t,FreeRS−SSD)min+1q(t)R, and rerank buckets in H; scenario 2) There are still empty buckets left in H, we initialize the counter of user s, and its over-estimation is also set as εs←0. Then we move the bucket into a suitable position in H according to the value of its counter. Also, the time complexity of updating the buckets in H for each arriving user-item pair is O(1).

Error Analysis. Similar to FreeBS-SSD, we have

Theorem 4.
For any user s, FreeRS-SSD gives an unbiased estimation of its cardinality n(t,FreeRS−SSD)s at any time t.

The proof of the above theorem is similar to that of Theorem 3, and we omit it due to limited pages.

6.3 Super Spreader Detection
Based on FreeBS-SSD and FreeRS-SSD, we detect top-K super spreaders, i.e., the set of users whose cardinalities rank top-K among all users. As mentioned above, each bucket in H contains three parameters user s, counter n^(t,FreeBS−SSD)s for FreeBS-SSD (resp. n^(t,FreeRS−SSD)s for FreeRS-SSD) and maximum over-estimation εs, and all buckets are ranked according to the values of their counters. |n^(t,FreeBS−SSD)s−εs| (resp. |n^(t,FreeRS−SSD)s−εs|) represents the increments after user s is inserted in the Stream-Summary data structure H, which can also be approximately treated as the cardinality of user s. Therefore, we sequentially enumerate all buckets in H, and then report users with top-K largest values of |n^(t,FreeBS−SSD)s−εs| (resp. |n^(t,FreeRS−SSD)s−εs|).

SECTION 7Evaluation
7.1 Datasets
We conduct our experiments on a variety of publicly available real-world datasets, which are summarized in Table 1. Dataset sanjose (resp. chicago) consists of one-hour passive traffic traces collected from the equinix-sanjose (resp. equinix-chicago) data collection monitors during March 20, 2014. Twitter, Flickr, Orkut and LiveJournal are all graph-datasets where each edge represents social relationship between any two users and may occur more than once in these datasets. Fig. 4 shows the CCDFs of user cardinalities for all datasets used in our experiments.

Algorithm 4. The Pseudo-Code for FreeRS-SSD
R[1,…,M]←[0,…,0];H[1,…,l]←[∅,…,∅];

n^(FreeRS−SSD)s←0, s∈S;qR←1;

foreach e=(s,d)∈Γ do

if ρ∗(e)>R[h∗(e)] then

if s∈H then

n^(FreeRS−SSD)s←n^(FreeRS−SSD)s+1qR;

rerank buckets in H;

else

if H is full then

pt←1/qRn^(FreeRS−SSD)min+1/qR;

r←Rand(0,1);

if r≤pt then

replace sm with s;

εs←n^(FreeRS−SSD)min;

n^(FreeRS−SSD)s←n^(FreeRS−SSD)min+1qR;

rerank buckets in H;

else

n^(FreeRS−SSD)min←n^(FreeRS−SSD)min+1qR;

rerank buckets in H;

end

else

n^(FreeRS−SSD)s←n^(FreeRS−SSD)s+1qR;

εs←0;

insert s into H and rerank buckets in H;

end

end

qR←qR+2−ρ∗(e)−2−R[h∗(e)]M;

R[h∗(e)]←ρ∗(e);

end

end


Fig. 4.
CCDFs of user cardinalities.

Show All

TABLE 1 Summary of Datasets used in Our Experiments
Table 1- 
Summary of Datasets used in Our Experiments
7.2 Baselines
In this paper, we compare our methods with other state-of-the-arts under the same memory usage. Therefore, we let bit-sharing algorithms FreeBS and CSE [53] have M bits, whereas register-sharing algorithms FreeRS and vHLL [49] have M/5 5-bit registers. Moreover, CSE and vHLL use virtual sketches of size m to record the cardinality for each user, and we vary different m and select the optimal one according to a given metric. LPC [48] and HyperLogLog++ [24] (short for HLL++) build a sketch for each user s∈S to record items that s connects to. Specially, HLL++ is an optimized method of HyperLogLog, which uses 6 bits for each register, and implements bias correction and sparse representation strategies to improve cardinality estimation performance. In our experiments, we let LPC have M|S| bits and HLL++ have M6|S| 6-bit registers for each user respectively. In addition, we also compare our methods with a combined method BF+CM (i.e., a Bloom Filter [5] and a Count-Min sketch [9]). In detail, we set each counter in Count-Min as 32 bit, the number of hash functions in the Bloom Filter as 3, and use a parameter μ(0<μ<1) to control the memory space of both structures, where μ is the ratio of the memory size of the Bloom Filter to the total memory size. Let w denote the number of counter arrays contained in Count-Min, and then the number of counters in each counter array is M(1−μ)32w. Later in our experiments, we vary parameters μ and w and select optimal ones under the same memory space.

In this paper, we aim to compute the cardinalities of all users. At each time t, enumerating each occurred user and computing its cardinality requires time O(|S(t)|m) for methods CSE, vHLL, LPC, and HLL++, where S(t) is the set of occurred users before and include time t. It is computationally intensive and thus is prohibitive for estimating all users’ cardinalities over time t. To solve this problem, we allocate each occurred user s a counter n^s to keep tracking of its cardinality for CSE, vHLL, LPC, and HLL++. For each edge e(t)=(s(t),d(t)) arriving at time t, we only estimate the cardinality of user s(t) for CSE, vHLL, LPC, and HLL++, and then update its counter n^s(t) and keep the counters of the other occurred users unchanged, which reduces time complexity from O(|S(t)|m) to O(m). To track all users’ cardinalities over time, therefore, all methods FreeRS, FreeBS, CSE, vHLL, LPC, and HLL++ require a counter for each user, and this memory usage of all methods is the same and we do not consider this memory usage in our comparisons.

7.3 Metrics
In our experiments, we use a fine-grained metric relative standard error (RSE) to evaluate the performance of estimating the cardinality for any user with a particular cardinality n at time t. Formally, we define RSE(t)(n)=1n∑s∈S(n^(t)s−n)21(n(t)s=n)∑s∈S1(n(t)s=n)−−−−−−−−−−−−−−−√. In addition, we also use a metric average absolute relative error (AARE) to evaluate the average performance of estimating each user's cardinality with respect to its true value at any time t, and AARE is formally defined as AARE(t)=1|S|∑s∈S|n^(t)s−n(t)s|n(t)s.

7.4 Accuracy of User Cardinality Estimation
Whenever an element e=(s,d) arrives, we implement all methods to estimate the cardinality of user s respectively. We first present the experimental results of methods LPC and CSE in the dataset Orkut in Fig. 5, where the memory size M=5×108 bits and the number of bits in the virtual sketch of CSE m=1024. The experimental results in other datasets are similar. Points close to the diagonal line n^(t)s=n(t)s indicate better estimation accuracy. From Fig. 5, we observe that LPC and CSE have limited estimation ranges and LPC exhibits extremely bad estimation performance especially for larger cardinalities. Under the fixed memory size, each user in the dataset Orkut uses only 167 bits for LPC and its maximum estimation range is much smaller than other methods. Therefore we omit the experimental results of LPC in the later experiments. Meanwhile, we vary the parameters of different methods respectively and use the metric AARE to compare their estimation performance in the dataset sanjose, which is shown in Fig. 6. AAREs of all methods decrease as the total memory usage increases from 1×108 to 5×108 bits (Fig. 6a), and both our methods FreeBS and FreeRS have better performance than other methods. When we use M=1×108 bits to estimate user cardinalities for dataset sanjose that contains 8,387,347 users, each user of methods CSE/FreeBS has about 12 bits and each user of vHLL/HLL++/FreeRS has only about 2 registers on average. In this case, there occur many “noisy” bits/registers contaminated by other users in the sharing methods CSE/vHLL/FreeBS/FreeBS, and the number of each user's registers of HLL++ is so limited that it cannot guarantee the estimation accuracy. In addition, we notice that AAREs of CSE and vHLL first decrease and then increase when m increases, and both methods maintain the best performance when m=1024 (Fig. 6b). This is consistent with our analysis in Section 4.2.3. Increasing m can increase the estimation performance of CSE/vHLL to some extent but it introduces more “unused” bits/registers for users with small cardinalities, which makes it easy for these bits/registers to be contaminated by other users. Furthermore, we observe that BF+CM is more accurate when μ=0.2 and w=2 (Figs. 6c and 6d) but its AARE is larger than other methods except CSE because it costs large memory space to handle edge duplicates in graph streams and introduces complex errors to Count-Min sketch.


Fig. 5.
(Orkut) Estimated cardinalities versus actual cardinalities for LPC and CSE under the same memory size M=5×108 (bits), where the number of bits in the virtual sketch of CSE m=1024.

Show All


Fig. 6.
(sanjose) AAREs of our methods in comparison with CSE, vHLL, HLL++, and BF+CM for different parameters.

Show All

Based on above experiments, we fix the total memory usage as M=5×108 bits, and set m=1024 for CSE/vHLL and μ=0.2,w=2 for BF+CM. We use the metric RSE to compare the estimation errors of our methods with other methods at a fine-grained level, which is shown in Fig. 7. We see that our methods FreeBS and FreeRS are up to 10,000 times more accurate than CSE, vHLL, HLL++ and BF+CM. Bit sharing method FreeBS (resp. CSE) is more accurate than register sharing method FreeRS (resp. vHLL) for characterizing users with small cardinalities. For users with large cardinalities, register sharing method FreeRS (resp. vHLL) outperforms bit sharing method FreeBS (resp. CSE). Specially, the RSE of CSE first decreases and then increases as the actual cardinality increases. This is because CSE has a small estimation range, i.e., mlnm, which is consistent with the original paper [53]. Meanwhile, HLL++ is more accurate than CSE and vHLL for small cardinalities due to its bias correction strategy. Because each user uses fewer registers for HLL++ than vHLL, HLL++ performs larger estimation errors than vHLL for large cardinalities. Also, when using Count-Min to record user cardinalities, users with small cardinalities are more easily affected by other users and thus identified as users with large cardinalities. Therefore, RSE of BF+CM is large for small cardinalities. Besides, Fig. 8a shows AAREs of all methods for estimating user cardinalities over time and Fig. 8b presents the memory usage per user. We notice that the estimation errors of all methods increase when more and more elements arrive in the graph stream while both our methods FreeBS and FreeRS have better performance than other methods. Also, the memory usage per user decreases as the volume of graph streams increases and each user costs about 60 bits at the end of the graph stream.


Fig. 7.
(All datasets) Cardinality estimation accuracy (RSE), where memory size M=5×108 (bits), and m=1024 for CSE/vHLL, and μ=0.2,w=2 for BF+CM.

Show All

Fig. 8. - 
(sanjose) Cardinality estimation accuracy (AARE) and memory usage (bits/user) of cardinality estimation over time, where memory size $M=5 \times 10^{8}$M=5×108 (bits), $m=1024$m=1024 for CSE/vHLL, and $\mu =0.2, w=2$μ=0.2,w=2 for BF+CM.
Fig. 8.
(sanjose) Cardinality estimation accuracy (AARE) and memory usage (bits/user) of cardinality estimation over time, where memory size M=5×108 (bits), m=1024 for CSE/vHLL, and μ=0.2,w=2 for BF+CM.

Show All

7.5 Runtime
For CSE, vHLL, LPC and HLL++, its runtime is mainly determined by the number of bits/registers in each user's (virtual) sketch, because estimating a user's cardinality needs to enumerate m bits/registes in the user's sketch, which requires large time complexity O(m). On the contrary, our methods FreeBS and FreeRS have low time complexity O(1) to process each coming element and update the counter for its user. In our experiments, we vary the number of bits/registers in the sketch m and record the average time required for processing each element and updating the user cardinality of all six methods. The experimental results are shown in Fig. 9. As m increases, the runtime of all four methods CSE, vHLL, LPC and HLL++ increases. Our methods FreeBS and FreeRS are substantially faster than other four methods for different m. Also, we notice that CSE is faster than vHLL and FreeBS is faster than FreeRS, this is because register sharing based methods perform more operations than bit sharing methods for processing each element.


Fig. 9.
Runtime of our methods FreeBS and FreeRS in comparison with CSE, vHLL, LPC and HLL++ for different m (i.e., the number of bits/registers in the (virtual) sketch) of a user.

Show All

7.6 Accuracy of Super Spreader Detection
For detecting super spreaders, we compare our methods with CSE, vHLL HLL++, BF+CM, and a reversible sketch-based method Vector Bloom Filter (VBF) [31] under the same memory size. VBF consists of 5 2-dimensional bit arrays and 6 hash functions, where each bit array contains 4096×m bits initialized to 0. Whenever an element e=(s,d) arrives, VBF uses a 32-bit string to represent user s and divides it into 8 sub-strings without overlapping, i.e., s=s1s2s3s4s5s6s7s8. In this case, the first four hash values are computed as h1(s)=s1s2s3,h2(s)=s3s4s5,h3(s)=s5s6s7,h4(s)=s7s8s1, and the fifth hash function randomly maps user s into an integer in {0,…,4095}. All these hash functions are used to find the row index in each bit array. The sixth hash function is used to generate the column index in all bit arrays for each arriving element. Then the corresponding bit in each bit array is set to 1. After, VBF enumerates all hash values in the first four bit arrays to reconstruct users based on their overlapping parts, and estimates their corresponding cardinalities with all five bit arrays. Therefore, we can easily find top-K super spreaders.

In our experiments, we use two metrics false negative ratio (FNR) and false positive ratio (FPR) to evaluate super spreader detection performance. The memory size of CSE, vHLL, HLL++, and BF+CM consists of the sketch size and user-counter pairs, while the memory size of our methods includes the bit/register array and the Stream-Summary data structure. Each bucket in the Stream-Summary data structure contains three parameters, user, counter and maximum over-estimation, and we assign 32 bits to represent each of them respectively. Therefore, the total memory size is M+64|S| bits and the number of columns in each bit array of VBF is M+64|S|4096×5, where |S| is the number of users. Also, we use a variable λ to control the ratio of the memory size of the Stream-Summary data structure to all memory size. Fig. 10 shows the accuracy of detecting top-100 super spreaders in dataset sanjose, and we omit the similar results in other datasets. We notice that FNRs and FPRs of our methods are much smaller than other methods and increase as λ increases from 16 to 12. This is because that the accuracy of user cardinality estimation of our methods decreases when there is smaller memory size given for the bit/register array. Later in our experiments, we fix λ=16. Fig. 11 shows the accuracy of detecting top-100 super spreaders for all datasets when all elements arrive. We see that FreeBS-SSD and FreeRS-SSD outperform other methods in all datasets. Specially, due to limited estimation range, CSE exhibits large estimation errors in datasets Twitter and Orkut. Although VBF reduces the memory usage of user-counter pairs using reversible sketches, its estimation accuracy is seriously affected by false users reversed but not actually occurred in graph streams. Also, it is computationally expensive to reverse each possible user, which is not practicable to detect super spreaders in real-time. Fig. 12 shows FNRs and FPRs of all methods for detecting top-100 super spreaders over time. We observe that the number of super spreaders not detected increases over time and thus FNRs of all methods increase. The number of users that are misidentified as super spreaders and the number of all users in graph streams increases simultaneously, and thus it is complex to analyze the trend of FPRs of all methods. Meanwhile, we notice that both our methods outperform other methods for detecting super spreaders.


Fig. 10.
(sanjose) Accuracy of detecting top-100 super spreaders for different λ with the same memory size, where M=5×108 (bits), m=1024 for CSE/vHLL, and μ=0.2,w=2 for BF+CM.

Show All


Fig. 11.
(All datasets) Accuracy of detecting top-100 super spreaders, where M=5×108 (bits), λ=16 for our methods, m=1024 for CSE/vHLL, and μ=0.2,w=2 for BF+CM.

Show All

Fig. 12. - 
(sanjose) Accuracy of detecting top-100 super spreaders over time, where $M=5 \times 10^8$M=5×108 (bits), $\lambda =\frac{1}{6}$λ=16 for our methods, $m=1024$m=1024 for CSE/vHLL, and $\mu =0.2, w=2$μ=0.2,w=2 for BF+CM.
Fig. 12.
(sanjose) Accuracy of detecting top-100 super spreaders over time, where M=5×108 (bits), λ=16 for our methods, m=1024 for CSE/vHLL, and μ=0.2,w=2 for BF+CM.

Show All

SECTION 8Conclusion
In this paper, we develop two novel streaming algorithms FreeBS and FreeRS to accurately estimate user cardinalities over time. Compared to existing bit/register sharing methods using (i.e., selecting) only m bits/registers for each user, FreeBS/FreeRS enables that the number of bits/registers used by a user dynamically increases as its cardinality increases over time and each user can use all shared bits/registers. It is therefore capable to estimate user cardinalities over a large range. Besides, both our algorithms exploit dynamic properties of shared bits/registers to significantly improve estimation accuracy. They are simple yet effective, and sharply reduce time complexity of computing all user cardinalities to O(1) each time they observe a new user-item pair. FreeBS is more accurate than FreeRS for users whose user-item pairs appear early in the graph stream, and FreeRS outperforms FreeBS for users whose user-item pairs arrive late. Furthermore, we extend both methods to accurately and fast detect top-K super spreaders. We conduct experiments on real-world datasets and experimental results demonstrate that our methods significantly outperform other methods in terms of accuracy and computational time.