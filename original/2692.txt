To aggregate useful information among diversified sources, a hotspot research topic called truth discovery has emerged in recent years. Existing truth discovery methods attempt to infer the true attribute values for the entities by identifying and trusting reliable data sources. That is, the values provided by reliable sources are more likely to be the true values. However, all these methods neglect the relations among different entities, which play important roles in truth discovery task. When reliable data sources cannot provide sufficient information of entities, the true attribute values of these entities can still be inferred by propagating trustworthy information from related entities. Motivated by this, in this paper, we introduce the constrained truth discovery problem. We incorporate denial constraints, a universally quantified first-order logic formalism which can express a large number of effective and widely existing relations among entities, into the process of truth discovery. We formulate it as a constrained optimization problem and analyze its hardness. To address the problem, we propose algorithms to partition the entities into disjoint groups, and generate arithmetic constraints for each disjoint group separately. Then, the true attribute values of the entities in each disjoint group are derived by minimizing the objective function under the corresponding arithmetic constraints. Experimental results on both real-world and synthetic datasets demonstrate that the proposed approach achieves good performance even with very few constraints and reliable sources.
SECTION 1Introduction
Aggregating the information provided by multiple data sources, which is also known as information integration, plays an important role in data analytics. Since there often exists recording errors, intentional errors, conflicts and outdated data across different data sources, finding the true attribute values of each entity is a fundamental task of crucial importance [17]. The process to fulfill this task is called truth discovery, which has been extensively studied in the literature. A series of methods [8], [11], [14], [16], [18], [26], [31], [33], [35], [36] have been proposed to enhance the effectiveness of the truth discovery process.

Motivations. To find the true attribute values of an entity, almost all of the previous work only considers the properties such as reliability on the data source. That is, we assign each data source a reliability value representing the likelihood that the information provided by this data source is correct. For each entity, its true attribute values are then easily obtained by trusting the data sources with high reliability values. Whereas, the correlations of the attribute values of the entities, which are also important for truth discovery, are neglected in these methods. As the correlations of attribute values widely exist in numerous real-world applications [9], utilizing the relations between attribute values can further improve the quality of the truth discovery process.

We elaborate this by the following motivating example.

1Example 1.
Table 1

TABLE 1 Personal Information Table

shows three tables on personal information extracted from different data sources, which are denoted as X1, X2, and X3. Table 2
TABLE 2 Ground Truth and Truth Discovery Results

shows the ground truth and the truth discovery results generated by the well-known CRH method [15] and our method, respectively.
First we analyze the results generated by the CRH method, which considers the source reliability degrees as source weights. The truth values are inferred by solving an optimization problem which minimizes the overall weighted deviation between the identified truth values and the multi-source data. The incorrect attribute values generated by CRH are labelled by strikethrough text. Suppose that the data source of X1 is more reliable than X2, and X2 is more reliable than X3. As a result, if X1 provides inaccurate or incomplete values, the obtained results will be incorrect. For example, the city of Bob is directly obtained from the incorrect value in X1 to be “LA”; the city information of Kate is only provided by X2 (e.g., “SF”) and X3 (e.g., “BUF”), so CRH falsely treats the information provided by X2 as the true value.

In our method, we consider both data source reliabilities and relations of entities to produce more accurate results. The accurate attribute values exclusively found by our method are labelled in underline text. Suppose that we have three constraints defined on the attribute values of Table 1: 1) each zipcode uniquely corresponds to a city; 2) the zipcode of “NYC” must be “10,002”; and 3) the salary of each person in “NYC” must be no less than “23,660” annually.1 According to Constraint 1, Kate should live in the same city of Mike as they have the same zipcode. Since the city of Mike is essentially “BUF” in all of the three data sources, our method corrects “BUF” to be the city of Kate. According to the Constraint 2, our method corrects Bob's city to be “NYC”. According to the Constraint 3, our method infers Bob's salary to be “23,660”, which is much closer to the ground truth value.

Challenges. Although the information provided by the relations of attribute values can significantly improve the result quality of truth discovery, its effects have been undervalued in the past. Specifically, existing work focuses on utilizing trivial relations of attribute values, such as temporal relationships [18], [20], [33], spatial relationships [20], common sense [21], [32] and etc. To comprehensively and sufficiently enhance the power of such information, it is much better to introduce more general relations of attribute values in truth discovery methods. Naturally, this arises two following questions on problem formulation and computation, respectively:

Q1: How to generally characterize relations of attribute values?

Q2: How to efficiently obtain the results when simultaneously considering the information provided by data sources and relations of attribute values?

Contributions. To answer these two questions, in this paper, we propose a novel truth discovery method with high efficiency and effectiveness by integrating the information of data sources and relations between attribute values. For Q1, we apply the Denial Constraints (DCs) model [6], a universally quantified first-order logic formalism, to represent the diversified relations of attribute values within a single entity (e.g., Constraints 2 and 3 in Example 1) or multiple entities (e.g., Constraints 1 in Example 1). The DCs are well designed to support a variety of operators between attribute values, so it is able to place restrictions on both types of categorical attributes (e.g., Constraints 1 and 2 in Example 1) and continuous attributes (e.g., Constraint 3 in Example 1). Based on DCs, there may exist several ways to assign true values to satisfy DCs. In consideration of the information of data sources, we formulate the truth discovery under this circumstance as an optimization problem. Our goal is to find the optimal true values minimizing the variances with respect to the reliable data sources under the DCs.

For Q2, due to the NP-Hardness of the optimization problem, we propose a heuristic algorithm to obtain approximate results with high quality. To accelerate the computation process, we apply two strategies, namely partition and reduction. The partition strategy divides all entities into a number of disjoint groups according to all DCs such that all entities within the same group are highly correlated with each other under some DCs. After that, for each group, each corresponding DC can be further reduced to an arithmetic formula. The computational cost for processing all arithmetic constraints is then substantially decreased.

By extensive experimental results on both real-world and synthetic datasets, our proposed method showcases its superiority over the state-of-the-art baselines.

Organizations. The rest of the paper is organized as follows. Section 2 formalizes the constrained truth discovery problem. Section 3 presents the detailed truth discovery algorithm. Section 4 reports the experimental results. Section 5 reviews the related work. Section 6 concludes this paper.

SECTION 2Problem Definition
In this section, we describe the constrained truth discovery problem formally. We first define the mathematical notations for the problem.

Consider N entities, each of which is made up of M attributes. The information of these entities are provided by K sources. ei denotes the ith entity. The value of the mth attribute of ei provided by the kth source is denoted as vkim. The information table provided by the kth source is denoted as Xk, where vkim is its imth entry. The true value of the mth attribute of ei is denoted as v∗im. The true values of all the entities on all the attributes are stored in a truth table X∗, whose imth entry is v∗im. Source weights are denoted as W={w1,…,wK}, in which wk is the reliability value of the kth source. A higher wk indicates that the kth source is more reliable and the information provided by this source is more likely to be correct.

2Example 2.
Consider Example 1. The value of Bob's city provided by X1 is v112=“LA”. In the CRH method, as the source reliability values w1>w2>w3 are estimated, the true value of Bob's city is identified as v∗12=“LA”, which is equal to v112.

In this work, we support the subset of integrity constraints identified by denial constraints (DCs)2 over the truth table X∗. Given a set of operators B={=,<,>,≠,≤,≥}, each DC is a first-order formula with the form φ:∀ei,ej,¬(C1∧⋯∧CZ), where each clause Cz is of the form v∗imϕv∗jn or v∗imϕc∗im, c∗im is a constant, ϕ∈B. The truth table X∗ satisfies φ, denoted as X∗⊨φ, if the true values in X∗ meet all the requirements defined in φ. If we have a set D of DCs, X∗⊨D if and only if ∀φ∈D,X∗⊨φ.

2Example 3.
Consider Example 1. We denote the entities Bob, Kate, and Mike as e1, e2, and e3, respectively. With the DC format, three relations (Constraint 1∼3) among e1∼e3 are expressed as follows:
φ1:∀ei,ej,¬(v∗i1=v∗j1∧v∗i2≠v∗j2),
View SourceRight-click on figure for MathML and additional features.
φ2:∀ei,¬(v∗i1=”10002”∧v∗i2≠”NYC”),
View Source
φ3:∀ei,¬(v∗i2=”NYC”∧v∗i3<”23660”).
View SourceRight-click on figure for MathML and additional features.

2Remark.
With the help of DCs, various relations within one entity or multiple entities are formulated into a uniform format, which can be conveniently incorporated as constraints into the process of truth discovery. Benefiting from this, truths are inferred based on not only source reliability estimation, but also the relations formulated as DCs.

Problem Definition. Given the information tables {X1,X2, …,XK} from K sources, and a set D of DCs, the problem is to find the truth table X∗⊨D and the source weights W.

Intuitively, the true values we aim to find should satisfy two requirements: 1) they should be close to the information given by the reliable sources, 2) they should satisfy each DC φ in D. Based on these intuitions, we formalize the constrained truth discovery problem as follows:
minX∗,Wf(X∗,W)=∑i=1N∑m=1M∑k=1Kwkd(v∗im,vkim)(1)
View Source
s.t.X∗⊨D(2)
View Source
∑k=1Kexp(−wk)=1,(3)
View SourceRight-click on figure for MathML and additional features.where d(⋅) is the loss function to measure the distance between the information tables and the estimated truths.

The objective function f(X∗,W) in Eq. (1) captures the intuition that the truths should be close to the information given by the reliable sources. It aims to minimize the variances between the information from different sources (i.e., vkim) and the estimated truths (i.e., v∗im), among which the variances on the information from reliable sources (i.e., wk is high) are weighed higher. It means that higher penalties are given to more reliable sources if their providing values deviates from the corresponding true values. Eq. (2) captures the intuition that the true values in X∗ should satisfy each φ in D (e.g., φ:∀ei,ej,¬(v∗im=v∗jm)). Eq. (3) is used to restrict the range of the source weights and prevent them from going to negative infinity, which is the same setting as [15].

In summary, we aim at solving the following problem:

Problem: Constrained Truth Discovery

Input: The information tables {X1,…,XK} from K sources and a set D of DCs.

Output: The truth table X∗ and the source weights W satisfying Eqs. (1), (2), and (3).

It can be inferred that when K=1, the problem is equivalent to the data repairing problem: given as input a information table X and a set D of DCs, the goal is to compute a repair X∗ of X such that X∗⊨D and the objective function f is minimum. When the loss function d is defined as the squared distance, it has been shown that finding the repair X∗ by minimizing the objective function f is NP-complete even for FDs only [5]. Moreover, minimizing f for DCs on continuous data only is known to be a MaxSNP-hard problem [3].

Given the intractability of the problem, our goal is to compute approximate results with high quality. We rely on two strategies to achieve it: 1) exact algorithm Partition divides all the entities into a number of disjoint groups; and 2) approximation algorithm Reduction simplifies DCs into arithmetic constraints. We detail our solutions in the following sections.

SECTION 3Constrained Truth Discovery
In this section, we propose the constrained truth discovery algorithm CTD. We start by introducing the whole framework in Section 3.1. We then detail the technical solution for component Partition and Reduction in Section 3.2, and propose the overall algorithm CTD in Section 3.3. Finally, we discuss several important issues about implementing CTD in Section 3.4.

3.1 The Framework Overview
An illustration of the whole framework is shown in Fig. 1. Our method takes as input a series of information tables from multiple sources (multi-source noisy data) and a set of denial constraints. For a set of entities, each information table provides a result of their attribute values. The DCs are enforced over these entities, which state that the true attribute values of every single entity or pairwise entities should satisfy some requirements. Intuitively, directly finding the true values based on the DCs may lead to huge calculations. To reduce the unnecessary calculations, a natural way is to determine the entities whose true attribute values may violate the DCs, which is regarded as potentially violated entities. Then, only the true values of the potentially violated entities should be inferred under the DCs.


Fig. 1.
Framework overview.

Show All

To achieve this goal, we first propose Partition, an algorithm to detect the potentially violated entities for the DCs. As the candidate true attribute values for the entities are supported by different sources which can be various, the algorithm detects potential violations among entities by considering every candidate attribute value as the true attribute value. The potentially violated entities are regarded as high correlated and divided into the same group. Benefiting from Partition, the truths of the entities within the same group need to be inferred together under the related DCs. The entities in disjoint groups are independent and can be processed separately. Considering a DC may contain several clauses which involve relationships among multiple attributes, the computation cost of truth discovery under the DCs is still high. To tackle this issue, an algorithm Reduction is designed to generate arithmetic constraints (ACs) specific to each group's potentially violated entity or entity pair based on the DCs in the group. Then, the true values towards disjoint groups are calculated by minimizing the objective function under the corresponding ACs. Depending on the type of the attribute values constrained by ACs, we have two calculation approaches, i.e., Simulated Annealing (SA) for categorical data and Quadratic Programming (QP) for continuous data. To achieve a more accurate result, an iterative process is adopted to simultaneously update the source weights and the true values towards disjoint groups.

3.2 Partition and Reduction
As a DC constrains the attribute values for each entity or every pairwise entities, directly finding the true values based on the DCs may lead to huge calculations when multiple DCs are available. Thus, it is crucial to propose an approach to reduce unnecessary calculations. To tackle this issue, in the following parts, we first propose an algorithm Partition to divide the entities into disjoint groups, which reduces the calculations in the entity-level. Then, we design an algorithm Reduction to generate arithmetic constraints for the single entity or pairwise entities based on the DCs, reducing the calculations in the clause-level.

Partition. Recall that a DC φ:∀ei,ej,¬(C1∧⋯∧CZ) is a first-order logic formalism made up of clauses, where each clause Cz is of the form v∗imϕv∗jn or v∗imϕc∗im, c∗im is a constant, ϕ∈B. The semantics is that for each pairwise entities (ei,ej) (or entity ei), their attribute values should not satisfy all the clauses in the DC. In this way, a DC builds the relations among the entities’ attributes, by filtering out several value combinations of them (i.e., the value combinations satisfy all the clauses in the DC). Thus, given a set of DCs, if we know whether the attribute values of the entities will potentially violate these DCs, we can reduce the calculations under these DCs. That is to say, assuming that each entity belongs to a separate group, the entities whose attribute values will potentially violate at least one DC will be merged into the same group, while the entities whose attribute values do not violate any DC will be maintained in disjoint groups.

Algorithm 1. Partition
Input: Data from K sources {Xk}Kk=1, a set D of DCs

Output: The disjoint groups {It}Tt=1, the corresponding

DC sets {Dt}Tt=1

Initialize the disjoint groups {It}Nt=1 such that each entity belongs to a separate group

Initialize the corresponding DC set Dn←∅ to each disjoint group In

for each φ∈D do

if φ defines on pairwise entities then

for each pairwise entities (ei,ej) may not

potentially satisfy φ do

Merge the disjoint groups and corresponding

DC sets of ei and ej (the new generated

group and DC set are denoted as It and Dt)

Add φ into Dt

else

for each entity ei does not potentially satisfy

φ do

Add φ into the corresponding DC set of ei

return {It}Tt=1, {Dt}Tt=1

To figure out whether the attribute values of the entities will potentially violate a set of DCs, we analyze the scope of the attribute values for each entity. Recall that the attribute values of each entity is provided by K sources, to minimize f in Eq. (1), v∗im should be close to {v1im,…,vKim}. Then, the situations are divided into two cases:

Given a DC φ defined on pairwise entities (ei,ej), if all the clauses in φ cannot be satisfied at the same time, where clause Cz is made up of any value combination {(vkim,vk′jn)|vkim∈{v1im,…,vKim},vk′jn∈{v1jn,…,vKjn}}, (ei,ej) will not violate φ. Then, ei and ej will stay in the disjoint groups. Otherwise, the attribute values of ei and ej are regarded to violate φ potentially, and thus ei and ej will be merged into the same group, and φ will be added into the corresponding DC set.

Given a DC φ defined on the single entity ei, similar with case (1), if all the clauses in φ cannot be satisfied at the same time, where clause Cz is made up of any value combination {(vkim,c∗im)|vkim∈{v1im,…,vKim}}, ei will not violate φ. Otherwise, the attribute values of ei are regarded to violate φ potentially, and thus φ will be added into the DC set that ei corresponding to.

Algorithm 1 shows the process of dividing the entities into disjoint groups. Initially, each entity belongs to a separate group associating with an empty DC set (Lines 1-2). Then, for each DC φ, if φ is defined on pairwise entities, for each pairwise entities that potentially violate φ, we merge their groups and their DC sets. Meanwhile, we add φ to the new generated DC set (Lines 3-8). Otherwise, for each entity that potentially violates φ, we add φ into its corresponding DC set (Lines 9-11).

Example 4.
Consider Example 3. With the DCs φ1, φ2, φ3, we describe how to divide the entities e1, e2, e3 into disjoint groups and generate the corresponding DC sets using Algorithm 1. We initially assign each entity to a separate group associating with an empty DC set. For φ1 defined on pairwise entities, we locate (e2,e3) whose attributes’ truths may not satisfy φ1. Then, we merge their disjoint groups and corresponding DC sets (the new generated group and DC set are denoted as I2 and D2, respectively). Meanwhile, we add φ1 into D2. Similarly, for φ2 and φ3, we identify e1 whose attributes’ truths may not satisfy them potentially. Let's denote the group and DC set of e1 as I1 and D1, respectively. Then, the DCs φ2 and φ3 will be added into D1. In summary, the algorithm generates two disjoint groups and two corresponding DC sets, where I1={e1}, D1={φ2,φ3}; I2={e2,e3}, D2={φ1}.

Time Complexity. The time complexity of the outer loop depends on the number of DCs in D (Lines 3-14). For each DC, the worst case is to confirm all pairwise entities whether they may violate the DC. The time complexity is O(ZφN2K2), where N is the total number of the entities, Zφ is the number of the clauses in DC φ, and K is the number of sources. Thus, the total time complexity of Algorithm 1 is O(LZφN2K2), where L is the number of DCs in D.

Remark.
In order to efficiently process Partition, we use a scheme proposed in [4] to discover the potentially violating single entity and pairwise entities. The schema employs specific data structures (e.g., cluster pairs) to represent pairwise entities, which needs much less memory. Evaluation algorithms are also designed for the operators of the DC clauses. The experiment results in Section 4 indicate that Partition with this efficient schema can scale well on large-size datasets.

Reduction. Through the Partition algorithm, the computation process for the constrained truth discovery is accelerated in entity-level. For each DC, we only need to consider the violations among several entities (i.e., the entities in the corresponding disjoint group). However, as each DC may contain a large number of clauses to be dealt with, it is still tricky to discover the true values under these DCs. Thus, in this part, we aim to simplify the DCs by generating arithmetic constraints specific for different entities in clause-level.

Given a DC φ:¬(C1∧⋯∧CZ), when φ is violated by some entities, these entities’ attribute values should satisfy all the clauses C1,…,CZ at the same time. In other words, if we ensure that one of the clauses Cz in φ cannot be satisfied, then the DC φ will not be violated. Thus, a DC φ can be reduced to an arithmetic constraint (AC) for these entities, i.e., ¬Cz, to simplify the calculation. For example, a DC φ:¬(v∗11=v∗21∧v∗12≠v∗22) can be reduced to ¬(v∗12≠v∗22) for (e1,e2), i.e., v∗12=v∗22.

Given a DC φ, in order to choose a proper ¬Cz for different entities, it is essential to estimate the probability of each clause to be satisfied in a DC. If a clause Cz has a high probability to be satisfied, then generating an AC ¬Cz for it can lead to huge mistakes. We detail the discussion in two cases:

(1) Consider a clause Cz which is in the form v∗imϕc∗im, the probability of Cz to be satisfied depends on the probability of different sources’ values {v1im,…,vKim} regarded as the true value v∗im. As the reliability degree of the kth source is weighted by wk, the probability of vkim being the true value v∗im should be proportional to wk, too. Then, p(Cz), the probability of clause Cz to be satisfied, is calculated by the proportion of the sum of the weights of sources who provide values satisfying the clause
p(Cz)=∑vkimϕc∗imwk∑vkimϕc∗imwk+∑vk′imϕ¯c∗imwk′,(4)
View SourceRight-click on figure for MathML and additional features.where Cz is of form v∗imϕc∗im, wk is the weight of the source whose provide value vkim satisfying Cz. Intuitively, if the values providing by the reliable source (i.e., wk is high) satisfies Cz, p(Cz) is high. Otherwise, p(Cz) is low.

(2) Similarly, when Cz is of the form v∗imϕv∗jn, we focus on the sources whose providing value combinations satisfying the clause. For a value combination (vkim,vljn) which satisfies Cz, the probability of (vkim,vljn) to be the true value combination (v∗im,v∗jn) should be proportional to wkwl. Then, p(Cz) is the proportion of the sum of the product of these pairwise source weights
p(Cz)=∑vkimϕvljnwkwl∑vkimϕvljnwkwl+∑vk′imϕ¯vl′jnwk′wl′,(5)
View SourceRight-click on figure for MathML and additional features.where Cz is of form v∗imϕv∗jn. Through Eq. (5), the clause Cz satisfied by value combinations from both reliable sources will receive high p(Cz). Otherwise, p(Cz) is low.

Example 5.
Consider Example 4. Suppose that an initial estimation of the source weights is W={12,13,14}. For φ2 in D1, the probability of each clause to be satisfied is calculated according to Eq. (4). For the entity e1 in I1, p(v∗11=”10002”)=∑vk11=”10002”wk∑vk11=”10002”wk+∑vk′11≠”10002”wk′=1; p(v∗12≠”NYC”)=1−p(v∗12=”NYC”)=0.46. Similarly, For φ3 in D1, p(v∗12=”NYC”)=0.54; p(v∗13<”23660”)=0.46. For φ1 in D2, the probability of each clause to be satisfied is calculated according to Eq. (5). For the pairwise entities (e2,e3) in I2, p(v∗21=v∗31)=∑vk21=vl31wkwl∑vk21=vl31wkwl+∑vk′21≠vl′31wk′wl′=0.77; p(v∗22≠v∗32)=1−p(v∗22=v∗32)=0.57.

In summary, for each DC φ, the clause Cz to be satisfied with the low probability is supported by few reliable sources. When p(Cz) is low, p(¬Cz) will be relatively high. Thus, for each DC φ, the arithmetic constraint should be generated for ¬Cz whose p(Cz) is the lowest. Benefiting from this, the true attribute values can be further found by trusting the reliable sources as well as satisfying the generated ACs.

Algorithm 2 shows the whole process of generating arithmetic constraints for the disjoint sets. We first initialize each disjoint set with an empty AC set (Lines 1-2). Then, for each DC in each disjoint set, we calculate the probability of each clause in the DC and rank the probability of all the clauses (Lines 3-8). Finally, we generate an arithmetic constraint for the unvisited entities with the lowest probability (Lines 9-13).

Algorithm 2. Reduction
Input: Data from K sources {Xk}Kk=1, source weights W={wk}Kk=1, the disjoint groups {It}Tt=1, the corresponding DC sets {Dt}Tt=1

Output: The corresponding AC sets {At}Tt=1

for t←1 to T do

Initialize the corresponding AC set At←∅ to

each disjoint set It

for φ∈Dt do

Initialize all the entities in It as unvisited

if φ is defined on single entity (or pairwise entities) then

for each ei∈It (or (ei,ej)∈It) do

Calculate the probability of each clause

in φ according to Eq. (4) (or Eq. (5))

Rank the probability of all the clauses in

ascending order

for each clause Cz whose related entity ei (or entitiy pair (ei,ej)) is not visited do

Generate the arithmetic constraint for ¬Cz

Add the arithmetic constraint into At

Mark ei (or ei,ej) as visited

return {At}Tt=1

Example 6.
Consider Example 5. We describe how to generate arithmetic constraints for each disjoint set using Algorithm 2. For I1, in the first round, we calculate the probability of each clause in φ2 and get p(v∗11=”10002”)>p(v∗12≠”NYC”). Thus, we have an arithmetic constraint v∗12=”NYC” for the clause v∗12≠”NYC”, and add it to A1. In the second round, we calculate the probability of each clause in φ3 and get p(v∗12=”NYC”)>p(v∗13<”23660”). Then, we generate an arithmetic constraint v∗13≥”23660” for the clause v∗13<”23660”, and add it to A1. For I2, we calculate the probability of each clause in φ1 and get p(v∗21=v∗31)>p(v∗22≠v∗32). Thus, we generate an arithmetic constraint v∗22=v∗32 for the clause v∗22≠v∗32, and add it to A2.

Time Complexity. The algorithm contains two phrases: calculating the probabilities of the clauses for the entities in the disjoint groups, and ranking these probabilities. We analyze the time complexity in the worst case, i.e., there exists only one group containing all the entities, and all the DCs in D are defined on pairwise entities. In this case, for each DC, the probabilities of the clauses is calculated between pairwise entities. Thus, the time complexity of the first phase is O(K2N2), where K is the number of sources, N is the number of the entities. For the second phase, which needs to rank N2 entities, the time complexity is O(N2log(N)). In summary, the total time complexity is O(LK2N2+LN2logN), where L is the number of DCs in D.

3.3 CTD Algorithm
Given the disjoint groups {I1,…,IT} and the corresponding AC sets {A1,…,AT}, the true attribute values of the entities in the same group need to be jointly inferred under the corresponding ACs. Let's denote I=∪Tt=1It as the set containing all the entities and denote {I∗1,…,I∗T} as the true attribute values of the entities in the disjoint groups accordingly. Given an initial estimate of source weights, based on the disjoint groups and the corresponding AC sets, Eq. (1) can be rewritten as follows:
minf(X∗)=∑It⊆I∑i∈It∑m=1M∑k=1Kwkd(v∗im,vkim)s.t.I∗t⊨At,(6)
View SourceRight-click on figure for MathML and additional features.stating that the true attribute values of the entities in each disjoint set It are calculated by minimizing objective function f(X∗) under the AC set At related to I∗t.

For categorical data, the loss function is defined as the indicator function
d(v∗im,vkim)={10ifv∗im≠vkim;otherwise.(7)
View SourceTo solve Eq. (6), we need to search an enormous number of possible solutions to find the optimal one. In such cases, we consider the simulated annealing approach [12], a probabilistic technique to approximate the global optimum of Eq. (6). To achieve this goal, we first transform Eq. (6) to the following equation.
minh(X∗)=f(X∗)+p(X∗)=∑It⊆I(∑i∈It∑m=1M∑k=1Kwkd(v∗im,vkim)+∑v∗imϕ¯c∗im∈At∑vkimwk+∑v∗imϕ¯v∗jn∈At(∑vkimwk+∑vk′jnwk′)),(8)
View SourceRight-click on figure for MathML and additional features.where p(X∗) is the penalty function. Intuitively, if an attribute value v∗im (or value combination (v∗im,v∗jn)) cannot satisfy the related AC, a high penalty i.e., the sum of the weights of sources who provide v∗im (or the value combination (v∗im,v∗jn)) will be received. Then, the SA approach can be adopted to find a solution by minimizing h(X∗). In the experiments, we show that the SA approach performs well in finding the true values for categorical data.

Remark.
Note that for categorical data, the ACs only consist of = and ≠. For the entities’ attributes associated with operator =, we group them together during the search of the true values, so that the ACs with operator = can be satisfied and the efficiency is further improved. Also, in order to propagate information among related entities’ attributes, the candidate values of each entity's attribute come from both its values provided by different sources and the candidate values of related entities’ attributes.

Example 7.
Consider Example 6. Assume that an initial estimation of source weights are W={12,13,14}. In I2, we want to minimize the objective function f(v∗21,v∗22,v∗31,v∗32) with the related constraint v∗22=v∗32 in A2. The candidate values for v∗22 and v∗32 are {“SF”, “BUF”}, which come from the values provided by three sources for both entities’ city. For v∗21 and v∗31 which are unconstrained, the candidate values for them are {“14221”}, {“14221”, “14226”}, respectively. Through the SA approach, the optimal value combination {v∗21=”14221”,v∗31=”14221”,v∗31=”BUF”,v∗32=”BUF”} is identified as the truths with the objective function being h(v∗21,v∗22,v∗31,v∗32)=14+13=0.58.

For continuous data, the loss function is defined as the squared function
d(v∗im,vkim)=(v∗im−vkim)2.(9)
View SourceWith the squared loss function, we want to minimize the squared objective function. The problem is a constrained quadratic programming problem, where the true values are inferred according to the quadratic program approach [1]. As the objective function given as a quadratic has a positive definite matrix, the quadratic program could be efficiently solvable [13].

Example 8.
Consider Example 6. Assume that an initial estimation of source weights is W={12,13,14}. In I1, we want to minimize the objective function 12(v∗13−21000)2+13(v∗13−24000)2+14(v∗13−25000)2 with the related constraint v∗13≥”23660” in A1. With the quadratic program approach, we obtain the true value of the salary of e1, i.e., v∗13=23660 with the value of objective function being 4.03×106.

Algorithm 3. CTD Algorithm
Input: Data from K sources {Xk}Kk=1, a set D of DCs

Output: Truth table X∗={I∗t}Tt=1

Initialize the source weights W

{It}Tt=1, {Dt}Tt=1 ← Partition({Xk}Kk=1, D)

{At}Tt=1 ← Reduction({Xk}Kk=1, W, {It}Tt=1, {Dt}Tt=1)

for t←1 to T do

Calculate I∗t in each disjoint set It under At

according to the SA/QP approach

return X∗

The whole process of constrained truth discovery is summarized in Algorithm 3. We initialize the source weights in the first step (Line 1). Then, the disjoint sets and AC sets are partitioned and generated according to Partition and Reduction in sequence (Lines 2-3). With the disjoint sets and corresponding AC sets, we determine the true values for different types of data according to the SA/QP approach (Lines 4-6).

Time Complexity. We analyze the time complexity of CTD by analyzing the time complexity of simulated annealing approach. First we confirm the candidate values for each entity's attributes. In the worst case, there exists only one set containing all the entities, and all the ACs are related to pairwise truths. The time complexity is O(K2MN), where K is the number of sources, M is the total number of attributes, and N is the number of entities. In each iteration, we generate a new solution and compute h(X∗). The time complexity for solution generation and the calculation of f(X∗) is both O(1). For p(X∗), we check if all the attribute values satisfy the ACs. The time complexity is O(MN). In summary, the total time complexity is O(K2MN+rMN), where r is the iteration number.

3.4 Discussions and Extensions
Here we discuss several important issues to make the proposed algorithm applicable. We discuss the construction of a high-quality DC set and the application of more complicated DCs, followed by the update of source weights.

High-Quality DC Set Construction. As the proposed algorithm takes advantage of the DCs to boost the performance of truth discovery, it is essential to analyze the availability of DCs as well as the construction of a high-quality DC set. Typically, DCs can be obtained through consultation with domain experts. Information from reliable sources (e.g., official websites) can also be formulated as DCs. For instance, the information of zip, city, and state can be easily obtained and formulated as CFDs [10]. When domain experts are not available, automatic DC discovery methods [4], [7], [22], [23] can be adopted to effectively identify meaningful DCs from the reliable data, which shares the similar attribute schema with the noise multi-source data.

With the aforementioned ways, a set of DCs can be obtained. However, in all likelihood, not all of them are equally useful for the proposed method. In order to construct a high-quality DC set, we discuss two important measures for the DCs, i.e., the correctness degree and the violation degree. The correctness degree of a DC reflects the reliability of its defined relationships, which can be given by experts or directly obtained through the automatic DC discovery methods [4], [7], [22], [23]. Intuitively, a DC with a low correctness degree represents unreliable relationships between attribute values, according to which the proposed method's accuracy may be limited. We denote the truth table obtained by minimizing Eq. (1) with Eq. (3) as X∗^. Then, the violation degree of a DC reflects to what extend does X∗^ violates the DC, which is measured by the proportion of entities whose attribute values violate the DC. Specifically, if the values in X∗^ does not violate the DC at all, the violation degree is 0, indicating that the DC may be less useful.

Based on the above analysis, a high-quality DC set should contain DCs with high correctness degrees and violation degrees. To further show the effectiveness of the proposed method using DCs obtained from experts and an automatic DC discovery method, with different degrees of correctness and violation degrees, we conduct experiments in Section 4.3.

Complicated DCs Application. To simplify the discussions, we focus on the DCs with single entity and pairwise entities above. For the DCs with Q entities (Q≥3), the proposed partition-reduction framework is still functional. Specifically, in the partition phrase, we determine the potential violation within every Q entities. These process can also be implemented by the efficient schema [4], where the cluster pair needs to be extended to cluster set for the representation of multiple entities. In the reduction phrase, as the clauses of the DCs are still defined within two variables, the probability of each clause in the DC can also be calculated using Eqs. (4) and (5) for every Q entities.

Iterative Process. In Algorithm 3, the true values are estimated only once according to the initial source weights, where the initial estimation of source weights can be obtained with existing truth discovery methods, e.g., [8], [14], [16], [26], [31], [35]. If the information is not available, an equal source weights can also be chosen as an initial source weights. To achieve a more accurate result, we can adopt an iterative procedure on both source weights and truth computation. In each iteration, the proposed CTD method improves the accuracy of source weight estimation using the latest true values. In turn, the true values in disjoint sets are updated based on the current weight assignment. We stop the procedure until the termination criterion is met, which can be the maximum number of iterations or a threshold for the similarity between true values from current computation and the true values from the previous computation.

Computing Source Weights W. With an estimation of the truth table X∗, the source weights W are updated by minimizing the objective function with constraints related to W as follows.
W←argminWf(X∗,W)s.t.∑k=1Kexp(−wk)=1.(10)
View SourceRight-click on figure for MathML and additional features.Through Lagrange multipliers, we derive the following equation for the source weight computation.
wk=log(∑Kk=1∑Ni=1∑Mm=1d(v∗im,vkim)∑Ni=1∑Mm=1d(v∗im,vkim)),(11)
View SourceRight-click on figure for MathML and additional features.where d(⋅) is calculated according to Eqs. (7) and (9).

Remark.
Considering the reliability of a source may vary for different attributes, we also propose the method of source weight calculation related to attributes. We denote the source weight wk regarding to the mth attribute as wkm. Then, the objective function Eq. (1) turns to
minX∗,Ws.t.f(X∗,W)=∑i=1N∑m=1M∑k=1Kwkmd(v∗im,vkim)X∗⊨D∑k=1Kexp(−wkm)=1,form=1,…,M.(12)
View SourceRight-click on figure for MathML and additional features.

Similarly, the source weight wkm is derived as follows.
wkm=log(∑Kk=1∑Ni=1d(v∗im,vkim)∑Ni=1d(v∗im,vkim)),(13)
View SourceRight-click on figure for MathML and additional features.where d(⋅) is calculated according to Eqs. (7) and (9).

SECTION 4Experiments
In this section, we evaluate the proposed method on both real-world and synthetic datasets. The experimental results clearly demonstrate the advantages of the proposed method by incorporating DCs into truth discovery. We first discuss the experimental setup in Section 4.1 and then show the experimental results on real-world and synthetic datasets in Sections 4.2 and 4.3, respectively.

4.1 Experimental Setup
Algorithms. For the proposed method CTD, the source weights are initialized as equal, and the iterative procedure is applied. For the simulated annealing approach, the parameters are set as follows. The initial temperature T0=50, the minimal temperature Tmin=0.01, the cooling rate γ=0.9, and the iteration number for each temperature rt=5 without extra mentioning. The baseline methods are listed as follows:

CATD [14]: This approach also models the conflict resolution problem by solving an optimization problem which extends source weights with confidence intervals to account for sparsity in source observations.

CRH [15]: This approach models the conflict resolution problem by solving an optimization problem. The solution consists of a two-step iterative procedure to update the true values and source weights.

GTM [35]: This is a Bayesian probabilistic approach designed for continuous data.

AccuSim [8]: This is also a Bayesian truth discovery method which adopts the usage of the similarity function. We do not consider source copying.

DART [19]: This is a domain-aware approach designed for multi-truth discovery problem, while other methods are designed for single-truth problem. However, as a state-of-the-art method, we still include it in the comparison. We do not consider the domain features, which is not available in our datasets. For each attribute of an entity, we treat the candidate value with the maximum veracity score as its single truth.

Voting: We also include majority voting as a baseline for categorical data. It chooses the values with the highest votes from the sources as the final results.

Mean: It simply takes the mean of all the values provided by sources on each continuous attribute of each entity as the final results.

Median: It calculates the median of all the values providing by sources on each continuous attribute of each entity as the final results.

All the experiments are conducted on a Linux machine with 8G RAM, Intel Core i5 processor.

Evaluation Measures. In this experiment, we evaluate the performance of the proposed method and the baseline methods in terms of effectiveness and efficiency. Note that we focus on both categorical and continuous data. To test effectiveness, we adopt the following measures for these two data types.

Categorical data: We use Error Rate as the performance measure of an approach, which is computed as the percentage of the outputs that are different from the ground truths.

Continuous data: We calculate the following metrics on the outputs by comparing them with the ground truths, Mean of Absolute Error (MAE) and Root of Mean Squared Error (RMSE) [18]. MAE uses L1−norm distance that penalizes more on smaller errors, while RMSE adopts L2−norm distance that gives more penalty on larger errors.

For all these measures, the lower the value, the closer the method's estimation is to the ground truths, and thus the better the effectiveness. Efficiency is measured as the running time of the approaches. Note that for the proposed method, as Partition is the off-line algorithm specific for DCs defined on pairwise entities, without extra mentioning, we report the running time of CTD without Partition.

4.2 Experiments on Real-World Datasets
Datasets. In order to evaluate the proposed method and the baseline methods in real-world applications, we adopt the Restaurant and Flight datasets as the benchmark. The statistics of the datasets are summarized in Table 3

TABLE 3 The Statistics of Real-World Datasets
Table 3- 
The Statistics of Real-World Datasets
. Here we provide more details as follows:
Restaurant: The restaurant data [29], consists of the restaurant information from 5 sources. The ground truths are also available. Restaurant has 5 categorical attributes: RestaurantName(RN), BuildingNumber(BN), StreetName(SN), ZipCode(ZC), and PhoneNumber(PN). The DC set D contains 1 FD: two restaurants with the same SN and BN should have the same ZC. We use this dataset to test how effective CTD is to deal with categorical data.

Flight: The flight data [16], crawled over one-month period starting from December 2011, consists of the time information of 1,200 flights from 37 sources. The gold standard for 100 flights is provided. To obtain 100 percent ground truth for a better comparison, we treated the sources providing correct information for these 100 flights as highly-accurate sources and removed them from the original dataset. We took the majority values provided by these sources as the ground truths. Similar approach is used in existing work [8], [26]. We convert the time information into minutes and treat it as a continuous type. The attributes includes ScheduledDepartureTime(SDT), ActualDepartureTime(ADT), ScheduledArrivalTime(SAT), and ActualArrivalTime(AAT). The DC set D contains 3 DCs: (1) ADT should not be earlier than SDT; (2) AAT should not be earlier than ADT; (3) SAT should not be earlier than SDT. We use this dataset to test how effective CTD is to deal with continuous data.

Performance Comparison. Table 4

TABLE 4 Performance Comparison on Real-World Datasets
Table 4- 
Performance Comparison on Real-World Datasets
summarizes the results for all the methods on the Flight and Restaurant datasets. The method CTD−att refers to the CTD method where the source weights are calculated related to attributes, i.e., Eq. (13). In terms of effectiveness, the proposed methods CTD and CTD−att achieve the best performance on the Restaurant dataset and Flight dataset, respectively. The experimental results indicate that on Flight dataset, the source reliability tends to vary for different attributes. While on Restaurant dataset, it is not the case. Among the baseline methods, DART has the worst performance, as it is designed for multi-truth problem. Vote, Mean, and Median simply aggregate the multi-source information without considering source reliability. Thus, they also perform bad. AccuSim takes categorical data as the input, so it cannot handle continuous data type well, resulting in poor performance on Flight dataset. GTM takes continuous data as the input, and thus cannot deal with Restaurant dataset. In this sense, CATD, CRH are more appropriate for the tasks with various data types. Our proposed methods CTD and CTD−att are based on information propagation among related entities as well as source weight estimation. By incorporating DCs into the process of truth discovery, they achieve the best performance.
In terms of efficiency, Vote, Mean, and Median run the fastest. On Flight dataset, the running time of the proposed method CTD is faster than the most efficient truth discovery baseline method GTM. While on Restaurant dataset, CTD is slower than AccuSim, CRH and DART, as CTD needs more time to infer the true values among related pairwise entities. Considering its improvement in the effectiveness, CTD does not sacrifice too much on its efficiency, and it is still faster than CATD.

To summarize, the proposed method outperforms all the baselines in terms of effectiveness without sacrificing too much on efficiency. Next, we analyze the performance of the proposed method and the baseline methods from two perspectives: (1) the effect of sources’ coverage rates; (2) the effect of the number of sources.

The Effect of Sources’ Coverage Rates. We first compare the effectiveness of different approaches under various coverage rate of the sources. The rate is defined as the percentage of the source number providing information for each entity. The results are shown in Figs. 2a, 2c, and 2e. It can be seen that the proposed method CTD achieves a significantly lower MAE, RMSE, Error Rate in all cases. Moreover, when the coverage rate is small (e.g., 0.2), CTD performs much better than the other baselines. For instance, on Restaurant dataset, compared with the best baseline Voting, the Error Rate of CTD decrease by 25 percent. The results confirm the idea proposed in this paper, i.e., the less reliable information we have for some entities (i.e, reliable sources are not enough), the greater DCs can help to find their true values.

Fig. 2. - 
Performance comparison w.r.t. varying coverage rate (left) and # sources (right).
Fig. 2.
Performance comparison w.r.t. varying coverage rate (left) and # sources (right).

Show All

The Effect of the Number of Sources. To explore the influence of multiple sources to the overall performance, we also study the effectiveness of the proposed method by varying the number of sources. The results are shown in Figs. 2b, 2d, and 2f. We can see that the proposed method CTD performs the best on both Flight and Restaurant datasets. However, the improvement is not obvious compared with other baseline methods. The reason is that when the number of sources is limited (i.e., reliable sources may not available), the effectiveness of discovering the true values for most entities cannot be ensured, which also influences the performance of CTD.

Finally, we report the MAE and RMSE of each source by comparing the attribute values provided by the source with the truths obtained by CTD method and the ground truth. The results are shown in Fig. 3. We can see that the MAE and RMSE computed by CTD is in general consistent with that computed by the ground truth, which indicates the proposed method can also accurately estimate the quality of data sources.

Fig. 3. - 
The performance of CTD on Flight dataset regarding to each source.
Fig. 3.
The performance of CTD on Flight dataset regarding to each source.

Show All

4.3 Experiments on Synthetic Datasets
To further study the performance of the proposed method, we conduct experiments on the synthetic datasets in the environment involving diversified factors: (1) the effectiveness w.r.t. various noise rates; (2) the effectiveness w.r.t the number of DCs; (3) the efficiency w.r.t the number of DCs and entities.

Datasets. The synthetic data sets are generated from SP Stock3 and Tax4 datasets. On SP Stock dataset, 6 DCs are defined by experts, and 563 DCs are discovered automatically using [7]. On Tax dataset, 8 DCs are defined by experts, and 1,527 DCs are discovered automatically using [7]. The original data sets are regarded as the ground truths, and based on each of them, we generate a data set consisting of 5 conflicting sources by injecting different levels of noise into the ground truths as the input to our approach and the baseline methods. To simulate the real-world problem setting, we allow each source to provide a subset of entities. However, for each entity in the original dataset, we also require that at least one source provides its information. Gaussian noise is added to each continuous attribute, and values in categorical attributes are randomly flipped to generate facts that deviate from the ground truths. A parameter β is used to control the reliability degree of each source (i.e., a lower β indicates a lower chance that the ground truths are altered to generate observations). To test the performance of the algorithms under different noise rates, we simulate both datasets under weak noise α = 10% (β = 0.1, 0.1, 1, 1, 1.3) and moderate noise α = 30% (β = 0.1, 0.1, 1.8, 2.3, 2.5). The statistics of the datasets are summarized in Table 6.

TABLE 5 Performance Comparison Under Different Noise Rates
Table 5- 
Performance Comparison Under Different Noise Rates
TABLE 6 The Statistics of Synthetic Datasets
Table 6- 
The Statistics of Synthetic Datasets
The Effect of Noise Rates. We first compare the effectiveness of different approaches under various noise rates. We use 10k entities for both datasets. The results on SP Stock and Tax datasets are shown in Table 5. It can be observed that the proposed method CTD achieves better performance in all cases. The reason is that with the help of DCs, CTD is able to infer the truths from not only the reliable sources, but also the reliable information of related entities. We can also see that the improvement slightly drops with the increase of noise rate. The reason is that when the noise rate is large, the reliable information is not enough for most of the entities. Thus, the power of propagating reliable information among related entities becomes less effective. Additionally, as all of the DCs are involved with categorical data, the improvement of the proposed method for categorical data is more obvious compared with that of continuous data. While for SP Stock dataset, the improvement of the proposed method for continuous data is more obvious than that of categorical data (e.g., compared with the best baseline AccuSim, the proposed method's MAE, RMSE, and Error Rate decrease by 3.99, 16.38, and 4.08 percent respectively, when the noise rate is 10 percent). Among the baseline methods, Voting, Mean, and Median achieve the worst performance in most cases, as they do not consider the reliability of different sources. GTM estimates source reliability only by continuous data which may not have sufficient information, and thus leads to bad performance on both datasets. CRH, CATD, DART, and AccuSim perform well with relatively accurate source weight estimation. Our proposed method can take advantage of the DCs as well as the source weights, and thus achieves better performance.

The Effect of the Number of DCs. To explore the impact of multiple DCs on the overall performance, we study the MAE, RMSE, and Error Rate of the proposed method CTD by varying the number of two types of DCs, respectively. The types of DCs are (1) Gold DCs: the DCs defined by experts; (2) Discovered DCs: the DCs discovered automatically through [7]. Each discovered DC is associated with a confidence score δ that represents its support from the data. To ensure the effectiveness, we integrate the discovered DCs with δ≥0.7 into the proposed method CTD. We randomly select multiple combinations of x DCs 10 times and report the average performance, where x represents the number of DCs. The noise rate is set as 30 percent. The results are shown in Fig. 4. It can be seen that the MAE, RMSE of the proposed method CTD drop fast with the increase of the number of gold DCs and discovered DCs and then reach a stable stage. The results indicate that a few DCs are enough to benefit the process of truth discovery. In contrast, the Error Rate of CTD stay stable with the increase of the number of gold DCs and discovered DCs, as these DCs are mostly defined on continuous attributes. Compared with the results of two types of DCs, similar performance is achieved in most cases, indicating that the discovered DCs are as effective as the gold DCs, and CTD can perform well with the discovered DCs.

The Effect of Iterations in SA Approach. To explore the effectiveness of simulated annealing approach for categorical data, we show the results of h(x) and Error Rate by varying the number of iterations on Tax dataset in Fig. 5. Here h(x) represents h(X∗) defined in Eq. (8) for one group, i.e., h(X1∗). The noise rate is set as α=50%, and the parameters in the SA approach are set as T0=10,Tmin=0.001,γ=0.998,rt=20. According to these parameters, the total number of iterations is calculated as: logTminT0logγrt=9.2∗104. From Fig. 5a we observe that there are significant oscillations in the h(x) at the beginning of iterations and then h(x) becomes stable (iteration 5∗104−9.2∗104). Similar curve trend can be found in Fig. 5b, we can see that the Error Rate is about 0.25 when the iteration process ends. The experimental result indicates that the SA approach is effective to minimize h(x) and obtain a good result within a reasonable amount of iterations.


Fig. 4.
Performance comparison on SP stock dataset w.r.t. # DCs.

Show All

Fig. 5. - 
Results of the simulated annealing approach on Tax Dataset w.r.t # iterations.
Fig. 5.
Results of the simulated annealing approach on Tax Dataset w.r.t # iterations.

Show All

Performance w.r.t. Sample DCs. To explore the effectiveness of CTD using DCs with different correctness degrees and violation degrees, we report the Error Rate on Tax dataset with three sample DCs φ1,φ2,φ3 in Table 7

TABLE 7 Performance w.r.t. Sample DCs on Tax Dataset
Table 7- 
Performance w.r.t. Sample DCs on Tax Dataset
. The noise rate is set as 10 percent. The Error Rate of CTD without using any DC is 0.0356. We can see that the Error Rate of CTD decreases by using these DCs. For φ1, although the violation degree is 0, it is still effective to find more truths. This result indicates that finding the truths without DCs may obtain a consistent result, but the accuracy cannot be ensured. For φ2 and φ3, although φ2 has higher correctness and violation degree, φ3 is more powerful. The result indicates that DCs defined on single entity with constants are more effective, and they can provide more information for CTD to find the truths.
Efficiency Study. We further test the efficiency of the proposed method CTD on Tax dataset by varying the number of entities and DCs, respectively. For the experiments of varying the number of entities, CTD uses 2 DCs defined on pairwise entities. The iteration number is set to 5 for CTD and all the iterative baselines. The total running time for 5 iterations is recorded. The results are shown in Fig. 6


Fig. 6.
Efficiency study on tax dataset w.r.t # entities and # DCs.

Show All

. In Fig. 6a, we observe that the running time of all the approaches rises with the increase of the number of tuples. Among them, as Voting, Mean, and Median do not estimate source reliability, they have the optimal efficiency comparing with truth discovery methods. For the proposed method, it has a comparable efficiency compared with GTM, DART, and AccuSim, which is also faster than CRH and CATD. In Fig. 6b, we observe that the running time of CTD arises with the increase in the number of entities. The reason is that, for each DC defined on pairwise entities, CTD needs to confirm all the pairwise entities whether they may violate the DC in the Partition phrase and generate AC constraints for every pairwise entity in one group in the Reduction phrase.
To further analyze the time cost for CTD in different phrases, we report the runtime distribution for each phrase on both SP Stock and Tax datasets with regard to the number of entities in Fig. 7. We use two pairwise DCs for each dataset. Overall, the truth calculation appears to be the most expensive part, which includes multiple iterations. It dominates especially for datasets with a lot of entities (Tax dataset). The runtime of the partition and reduction grows linearly in the number of entities. As the groups divided by the partition phrase determines the size of the ACs generated by the reduction phrase, we also report the size of disjoint groups in the partition phrase on the Tax dataset in Table 8. It can be inferred that the maximum, mean, and median of the group size rise linearly in the number of entities. Compared with the total entity size 1M, the largest group contains 314 entities, which is acceptable.

TABLE 8 Group Size w.r.t. # Entities on Tax Dataset
Table 8- 
Group Size w.r.t. # Entities on Tax Dataset
Fig. 7. - 
Runtime distribution for the different phrases of CTD w.r.t. # entities.
Fig. 7.
Runtime distribution for the different phrases of CTD w.r.t. # entities.

Show All

SECTION 5Related Work
Truth discovery is an advanced data fusion technique to resolve conflicts among multi-source data [2], [16], [17]. The problem of truth discovery is first formally introduced in [31]. Then, various scenarios have been considered in truth discovery, such as heterogeneous data types [15], long-tailed data [14], multi-truth problem [19], [28], [36], dynamic truth discovery [18], [33] and dependent data sources [8], [24], [25], [34]. However, most of these approaches neglect the correlations among entities and attributes, thus they cannot take advantage of the correlation information for more accurate truth estimation.

There are also several truth discovery methods [18], [20], [21], [32], [33] consider some kind of correlations under specific applications or scenarios. In [21], the authors assume that some common sense is already known to the user, and they propose a framework to incorporate the common sense knowledge into the truth discovery algorithm. However, the common sense may not hold true and needs to be set manually. Differently, our method can not only utilize the DCs defined by the user, but also through the automatic DC discovery approaches, which ensure the availability and high-quality of the DCs. In [32], the authors combine information extraction and truth discovery methods to solve Slot Filling Validation (SFV) task. The dependency relationships among entities (slots) are linguistic correlations acquired from a knowledge graph, and these dependencies are used to initialize the credibility of the extracted information. Thus, the type and usages of relationships are different from our scenario.

In [20], the authors study the correlated entities regarding the temporal and spatial relationships on continuous data in crowd sensing applications. In [33], the authors estimate the evolving truths on the categorical data in social media sensing applications, and physical constraints are pre-defined on the truths of each entity in different timestamps. However, the temporal/spatial relationships in [20] and physical constraints (e.g., order constraints, global path constraints, frequency constraints) in [33] can also be modeled using DCs defined on the corresponding attribute values. So far, no truth discovery method can tackle the complete correlation among entities and attributes with different data types.

In the field of data repairing, there has been several work adopting integrity constraints to detect and resolve the violations in the databases [27], [29], [30]. In [27], the authors study the problem of repairing dirty data under DCs. Nevertheless, they focus on repairing the data under the situation of oversimplified and overrefined constraint inaccuracies. In our work, we concentrate on discovering the truths by applying DCs on multi-source data, which is a different problem setting. In [29], [30], the authors resolve the inconsistencies and conflicts in multi-source data by considering the constraints and source reliability together. The constraints adopted in [29] is functional dependency, which can only express the equality relations. Comparing with the DCs using in our approach, its expressive power is quite limited. Although [30] adopts the DCs for multi-source data repairing, the authors only propose simple solutions to typical DCs, which make this problem far from being solved. In contrast, our method is the first one to provide a detailed solution for applying general DCs for truth discovery.

SECTION 6Conclusion
In this paper, we study the problem of integrating DCs into truth discovery. We formulate the problem as an optimization problem and analyze its hardness. To solve the problem efficiently, we derive a unified algorithm called CTD. In order to reduce the unnecessary calculations, we propose Partition and Generation algorithms. SA/QP approaches are adopted for continuous and categorical data, respectively. Our experimental results with both real-world and synthetic datasets verify the effectiveness and efficiency of the proposed framework under different challenging scenarios. In the future, we plan to extend the framework so that it can automatically discover DCs for truth discovery.