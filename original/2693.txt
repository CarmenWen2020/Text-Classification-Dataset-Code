Model-checking and verification using Kripke structures and computational tree logic* (CTL*) use abstractions from the model/process/application to create the state-transition graphs that verify the model behavior. This scheme of profiling the performance of a process imports that the depth of the process operation correlates with the level abstraction. However, because of state explosion problems, these abstractions tend to restrict the scope to create manageable execution states. Therefore, for context modeling, this procedure does not generate a fine-grained behavioral model as generated states limit the ability of the abstraction to capture the execution time interactions amongst the processes, the hardware, and the kernel. Hence, in this paper, we present an end-to-end framework that comprises auto-encoders and probabilistic models to understand the behavior of system processes and detect deviant behaviors. We test this framework with a publicly available dataset generated from an autonomous aerial vehicle (UAV) application and the results show that by creating a fine-grained model that exploits previously unharnessed properties of the system calls, we can create a dynamic anomaly detection framework that evolves as the threats change.
Acronyms
ALρ	
Anomaly level probability.

Ain	
Sorted system call arguments frequency distribution.

FED	
Frequency of operation of the event-driven model.

FTD	
Frequency of operation of the time-driven model.

IC	
Instruction cycle.

Ji	
Frequency distribution of event type i.

RFD	
Relative frequency distribution.

SRFin	
Input events relative frequncy distribution.

SRFout	
Predicted events relative frequency distribution.

Sout	
A sequence of predicted system call events.

S	
Integer enconded system call events.

Tout	
Sequence of predicted execution cycles.

Win	
Input window.

Wout	
Output window.

Xi→	
Input at time i.

dB	
Execution times prediction error.

dS	
System events prediction error.

dSRF	
Prediction error of RFD of events.

d	
Deviations between predicted and actual value.

Bin	
Execution time window of system call events.

Sin	
A sequence of input system call events.

Ci→	
Recursive context input at time i.

SECTION 1Introduction
Context modeling involves the use of abstractions like Kripke structures and CTL* [1], [2] to describe the process/application behavior by creating state-transition graphs to verify the behavior of the process/system. This model checking and verification schemes rely on different layers of abstraction and that implies that the depth of the model action that can be synthesized correlates with the level of model abstraction. However, because the symbolic model behavior verification like CTL* requires that there exists a finite number of states, deep and broad level of abstraction may result in state explosion problems. Hence, the resort to Kripke structures that have finite and manageable states to represent model behavior. Therefore, for complex processes, this approach does not return a fine-grained behavioral model and does not capture in-depth execution time interactions amongst the processes, the hardware, and the kernel. However, the salient difference between the model we are introducing and the symbolic model checking schemes is that the former uses boolean probabilities to create the state relationships while the latter utilizes non-binary probabilities in capturing the state interactions. The implication of the use of non-binary probabilities is that a previously unseen state can be handled without resort to generating an entirely new model as is obtainable with the use of boolean probabilities. An example system call sequence like open⟶getrlimit⟶mmap⟹close using symbolic model checking methods assumes that the open, getrlimit, mmap have boolean values of 1 and that they all contributed equally to the output state close. Meanwhile, owing to the semi-structured nature of the system events occasioned by the presence of interrupts at the kernel layer, this open, getrlimit, mmap may not have had equal impact in the emergence of close as the next state. For example, in Table 1, with binary probabilities the previous states open, getrlimit, mmap in a linear combination produces the same output irrespective of the order of the system calls in rows 1 and 2 of the table. Therefore, assuming that Φ is a transformative function in (1) and yi is the encoded system call at state i, the use of boolean probabilities cannot distinguish between the previous states in rows 1 and 2 of Table 1. However, with non-binary probabilities, the linear combinations will result in different output values which emphasizes the weight of the order of the previous states in forecasting the next state. Furthermore, using system call sequences as an example, it implies that all the possible states (system calls) of an application or a process must be seen before a model is created using the symbolic model checking schemes but with our model, this is not required
yt=Φ(a×yt−1+b×yt−2+c×yt−3).(1)
View SourceRight-click on figure for MathML and additional features.Also, some process executions are time-constrained (discrete events have a maximum execution time during the general modeling process), and it becomes difficult to model these constraints via abstraction only as some of the factors that affect timeliness or currency are execution dependent. In most instances of clock glitches attack [3], application-layer abstraction is not sufficient to capture such an anomalous injection because of the many machine-to-machine interactions involved at the kernel layer.

TABLE 1 Showing the Import of Non-Binary Probabilities
Table 1- 
Showing the Import of Non-Binary Probabilities
1.1 Time-Driven Versus Event-Driven Execution
We can generate system calls via the regularly scheduled tasks which are as a consequence of events from a given process, and sometimes, these system calls occur as a result of interrupts which are event-driven. Moreover, when it is event-driven, fast and slow profiles can be observed making it difficult to use one of the known types of distribution to model the behavior of the process. Also, for a real-time process, the constraint on response and execution time can best be modeled by observing the timestamp property of the system call and making use of it in creating the model. The fact that some processes may be time-driven, event-driven or a mixture of both makes a one-size-fits-all solution difficult and complicates the use of system call information for profile construction. Hence, in this paper, we introduce a hybrid model that uses RNN to profile the behavior of time-driven and event-driven processes based on system call information. The temporal nature of the system traces drives the use of a deep learning network based on RNN cells in understanding the temporal relationships because the frequency, order and timestamp information of the traces contribute significantly in learning the behavior of the process or application. We use the popular long short-term memory (LSTM) [4] variant of RNN because of its proven performance in time-series data prediction [5] and ability to capture long temporal dependency. Therefore, our contributions in this paper are as follows;

design and development of a unique architecture for profiling system processes or applications.

creation of fine-grained features from system calls to capture the broad representation of process behavior, and increase the scope of the anomalies we can detect in order to match the ever-increasing sophistication of threats.

generation of a public dataset from contemporary anomalous scenarios for benchmarking anomaly detection frameworks.

Based on the contributions stated above, we will be investigating the following; a) the ability of the model to generalize on the typical operating profile of the system. b) the ability of the model to detect deviations from the standard profile and quantify the anomaly based on the level of disturbance it generates on the system profile. c) the residual effects of an anomaly on the system behavior. This work builds on [6] by adding several chapters to provide clarity and depth in the concepts discussed. Also, we have provided extensive experimental results that are not present in [6] to back up our hypothesis. Finally, we present a new, reduced (in terms of parameters), and more efficient (multiple-input, multiple-output model instead of disparate inputs and outputs model) framework which is different from the ensemble design of [6]. The time-driven Model and event-driven Model of Fig. 1 is based on the RNN architecture of [7].

Fig. 1. - 
Architecture of the process-based anomaly detection framework.
Fig. 1.
Architecture of the process-based anomaly detection framework.

Show All

The paper is organized as follows: Section 2 highlights the related anomaly detection work concerning trace analysis. In Section 3, we discuss the threat vectors, the motivations and mention some of the assumptions we made in the paper while Section 4 presents our framework and the technical details of the work. Section 5 discusses our experimental setup and the dataset used for the simulation. Finally, we analyze the results in Section 6 and conclude the work in Section 7 and give further research directions.

SECTION 2Related Work
Detecting deviations or attacks have been primarily classified as an intrusion or anomaly detection [8], [9], [10]. While intrusion detection techniques rely on the use of signatures to detect a misuse behavior by storing signatures of discovered anomalies, anomaly detection models construct contexts using known standard features, and label any deviation from the learned profile as anomalous. The obvious limitation of this signature-based approach is that zero-day vulnerabilities cannot be detected as it only searches for observed signatures, i.e., it emphasizes memorization over generalization. On the other hand, the anomaly-based approaches target both known and unknown anomalies and can detect zero-day vulnerability. Because our work centers on anomaly detection approach, we highlight mostly the research works in this area.

Discrete events anomaly detection is context-based (relies on the construction of contexts or profiles) and these profiles can be constructed using either the features or the volume of the input [11]. The volume-based models employ the use of information-theoretical concepts like entropy to detect deviations based on the size of the input to the anomaly model [12]. However, since entropy values are not unique and only the traffic volume is used as a feature, we will be adopting the feature-based anomaly framework in this paper because it offers the flexibility of taking charge of the granularity of the model derived.

In [13], the authors used kernel events to build an offline anomaly detection model using some vector space model concepts and agglomerative clustering technique. Imputation techniques were also used to increase the scope of their model and reduce the incidents of false positives. While the performance accuracy of [13] is high, the framework can only detect deviations related to quantitative variance in the composition of system call events, and cannot account for the temporal relationships of the system call events. Also, the size of the number of events required as input makes it difficult to use the framework of [13] for online anomaly detection or to detect fast-changing profiles. The use of only the system call event IDs as features while ignoring other properties of the system call events also limits the scope of the anomalies that can be detected with [13]. Reference [14] used deep LSTM models constructed from system logs to create anomaly detection models. The LSTM model is augmented with a workflow model that helps to detect context switches. The work of [14] captures the temporal relationships but ignores the varying contexts occasioned by the change in the quantitative variance of the system call events. Therefore, while threats targeting the difference in the order of the system call events can be detected by [14], others targeting the quantity of the system call events like denial of service attacks or those aiming to inject fictitious contents into the system call arguments like buffer overflow attacks cannot be detected. Authors of [15] used textual information buried in console logs, which are logged as a program is running to create an anomaly detection model using principal component analysis. Like [13], the work of [15] only captures anomalies that target variation in the quantity of the system call events. Also, [9] constructed host-based anomaly detection models using Bayesian Networks that uses system call arguments and variables as its features. The model of [9] uses aggregate weighting to reduce the effect of inconsistent anomaly scores from different models, but it does not consider the temporal relationship amongst the sequence of the system calls. Also, in [7], [16], the hierarchical LSTM network is used to explore the understanding of relationships amongst the kernel event traces of an embedded system, but other features which ordinarily should yield a more representative model like timestamps, CPU cycles and system call arguments are skipped.

Authors of [17] have an anomaly model built using system call frequency distribution as well as clustering techniques to detect when processes deviate from their standard profiles. Furthermore, the authors of [18] used the statistical metric of entropy to implement an anomaly detection model for network logs, but this type of anomaly model is best suited for cases where the volume of logs determine if an anomaly has occurred as obtainable in denial of service attack. Also, the model of [18]’s use of entropy as the discriminating factor makes the result non-specific as entropy values are not unique. In [19], a real-time system anomaly detection model is designed using the principle of inter-arrival curves to detect anomalous traces in a log sequence. Again, this inter-arrival curve-based model ignores other properties of system calls. The authors of [20] used an optimization method of minimum debugging frontier sets to create a model for the detection of errors/faults in software execution. Also, [21] used system call traces without arguments to create an anomalous profile detector using deterministic finite automaton (DFA). The authors assume that anomalous system call sequences have a local profile and that with the use of a locality frame, these anomalies can be detected. Authors of [21] however admitted that when the anomalous sequences are not concentrated in a burst; their algorithm cannot handle such scenarios. Therefore, unlike our framework which targets both local and non-local profiles, [21] targets only bursty profiles. Reference [22] used techniques such as the counting of observed system calls, frequency distribution approaches, a rule-based method called RIPPER and a Hidden Markov Model (HMM) to construct anomaly models to detect valid and irregular behavioral profiles. Finally, in [23], a sliding window approach is used to construct a tree of the possible routes of the system call sequences based on the observed behavior during the learning phase. Again, [22], [23] only consider the temporal ordering of the traces and all other parameters like the timing information and system call arguments are ignored, and their approaches cannot handle a previously unseen system call without reconstructing the whole model.

SECTION 3Threat Model
Our models learn the regular operational profile of a process or the application they monitor by studying the execution cycles of the system calls, the temporal ordering of the system calls, and the system call arguments made by the process. Therefore, the models are process or application specific. To this end, we discuss the motivation of using system calls as a profile feature, the assumptions and definitions of the common terms we use in this paper as well as the threat vector.

In [24], the authors use UML as a tool to model a program behavior. For a well-specified program, this method does capture all the states that the program will take, but these states are based on the program requirements and specifications. Therefore, this builds a profile based on memorization rather than on generalization. Also, because the UML state diagrams rely on information gleaned from the application layer of the program, it hardly reflects the multiple process-to-process interactions that happen at the kernel layer. On the other hand, with process IDs, Node IDs, parent process IDs, thread IDs, and other properties of system calls, we can follow all the machine-to-machine communications that truly reflect the execution sequence of a program, and any model constructed from such information tend to have a more fine-grained representation of the program profile. As demonstrated with Stuxnet worm attack [25], relying on the upper layer alone to model program profile may not be sufficient because the other components can be hijacked at the lower layer and used to deceive the program monitor at the upper layer. Therefore, in this work, we focus on the use of system calls as the driving feature for creating our models.

3.1 Definitions and Assumptions
3.1.1 Definitions
An anomaly refers to a behavior that varies from the profile learned during the regular operation of the process or application. This deviant behavior could be the alteration to the temporal ordering of the system calls, change in the system call arguments or difference in the local distribution of the system calls.

A feature refers to a tuple Fm={f1,f2,f3,…,fT} where m is the index of the system call in the data and |Fm|=T which is the dimension of the feature Fm processed in one sample. f1,f2,…. are explained further in the model design of Section 4.

3.1.2 Assumptions
In this paper, we make the following assumptions:

That the process or an application has a well defined behavioral pattern or states and that we can exhaust all the normal states during learning. Because we validate our model with an embedded application, tasks are mostly repetitious, and this assumption is easily met.

That an error or insertion of malicious code will not result in null events at the kernel. i.e., execution of malicious codes or errors should create system call events in the kernel. This is a necessary condition because if the malicious code or error does not generate the kernel events, then our model cannot detect the anomaly.

During the learning phase, we assume no compromise of the legitimate execution profile nor that of the storage location where we store the execution traces.

3.2 Threat Vector
We illustrate the threat vector of the anomalies we aim to catch via the application which we used to test our model. The test bed is an autopilot application for controlling UAV and the normal profile consists of four states: Takeoff, Cruising, Landing, Landed. In Fig. 2, the blocks Takeoff, Cruising, Landing and Landed depict a complete execution flow of the normal operational profile scenario of the UAV application. The system call events resulting from the states Takeoff ⟶Cruising ⟶ Landing ⟶Landed mark one complete standard operational cycle logs. Therefore, if there is an insertion of a malicious code block Malicious code1 into the sequence of the normal execution as depicted in Fig. 2, one or more of the five possibilities will occur regarding how the malicious code affects the overall profile of the system call events when the tasks complete one cycle of execution. The possible consequences and what our models can do to each issue are illustrated in Fig. 2 and explained next:

Fig. 2. - 
Malicious code insertion and possible consequences.
Fig. 2.
Malicious code insertion and possible consequences.

Show All

No Syscall generated: the Malicious code surreptitiously changes the destination coordinates or the cruise control operation without producing system call events. Our models do not target this type of anomaly.

Seen Syscall (same order): the Malicious code block can cleverly yield the same temporal ordering of the system calls between adjoining task so that transition in the normal profile is imitated. While this kind of attack requires knowing the temporal ordering of the normal profile, it is not an impossibility as demonstrated in [17]. This scenario can be captured by our models.

Seen Syscall (different order): if the inserted block of code creates system call events sequence that is made up of already seen event types but with a markedly different order from the learned profile, our models can detect this type of anomaly.

Seen and Unseen Syscall: also, when there is a mixture of both seen and unseen system events generated as a result of the activity of the Malicious code, our models can detect that.

Seen Syscall (varying execution time): if there is clock manipulation attack or denial of service resulting in a markedly varying execution time, our models can also capture such an anomaly.

Finally, the malicious block of code illustrated in Fig. 2 could be as a result of hardware failures, glitches in software or intentional hijack of the application or process by an intruder but our threat model does not cover how that block of code got injected in the application.

SECTION 4Framework Design
A high-level architecture of our model is depicted in Fig. 1, and it has many blocks which we explain in the Sections 4.1 through 4.3.2. The high-level overview is that we stream system call traces from the instrumented kernel and we feed the streams to the model via the feature processor module. The two models are concurrent but the event-driven model lags the time-driven model because they target different kinds of anomalies in the traces as highlighted in Section 3.2. While the time-driven model targets the temporal ordering of the system calls, denial of service or clock manipulation attacks, and injection attacks like buffer overflow which changes the system call arguments, the event-driven model targets anomalies that tend to cause a burst consisting of seen/unseen system calls in the system. The two pools work on the same principle of sequence replication but while the time-driven model replicates the system calls, and the instruction cycle (IC) count, the event-driven model focuses on regenerating the system call relative frequency distribution of the input within a window of observation. The errors incurred as a result of this replication is fed to the anomaly detector to enable it to set an upper bound for each of the models. The feature processor stores the system call IDs temporarily in a buffer pending onward processing (conversion to SRF) and transmission to the event-driven model. The buffer is necessary because FTD/FED>1 where FTD and FED are the operational frequencies (number of times they are called) of the time-driven and event-driven models respectively. In summary, while time-driven model targets anomalous sequence calls with local temporal profile, event-driven model targets anomalous system call sequences that has long temporal profile.

4.1 Feature Processor
The system call traces contain several properties, but the features of interest to us are timestamp, system call ID and system call arguments. To create a relative deterministic model, we avoided using the CPU clock or wall clock as a timestamp; alternatively, we use the CPU cycle count. We assume that the process is running in a scaled-down operating system with not much-competing processes as typically obtained in embedded applications. Therefore, a single trace is a multivariate variable which undergoes further processing to yield the desired features. Given timestamp as t, system call string name as k and system call arguments as a, we process these properties to yield a multi-input feature space that we feed to our model.

4.1.1 Timestamp and System Call Classes
During the learning phase, we store the training samples in a database, but the model can run online after learning. Given an observation of system call samples {s1,s2,s3,…,sn} in storage where n is the total number of the observed traces, then the function B of (2) defines the time window between system calls. The essence is for the model to capture both the ordering of the system calls and the relative duration within such a relationship. This way, for malicious code which does not alter the ordering of the system calls but creates anomalous execution times or delays which do not match the relative duration defined by B will be detected
B:(ti,ti+1)⟼ti+1−ti;i∈N.(2)
View SourceRight-click on figure for MathML and additional features.In order to get S in Fig. 1, we use the function defined in (3) to encode the system call ID strings into their corresponding Linux tables. There are a total of 330 unique system calls in the X_64 platform
S:k⟼w;wherew={w∈N∣1≤w≤330}.(3)
View SourceRight-click on figure for MathML and additional features.

4.1.2 System Call Arguments
The system call argument is alphanumeric and special character strings. System call arguments are part of the features we process because it has some compelling distribution which we use to discriminate between a valid and incorrect behavior. The motivation is that while there is no consistent frequency distribution in a particular system call, a little reorganization (e.g., sorting) of the relative frequency distribution of the characters yields a thoughtful insight into its usability.

System call arguments are strings, and in order to maintain a relatively few vocabularies, we encode the string characters using ASCII values. This encoding provides us with a range of unique 256 classes. After encoding the string values, we calculate the frequency distribution and relative frequency distribution for the 256 classes in each system call argument. Our aim is not to check how consistent each ASCII value is across the system call but to find the steepness of the distribution of the characters when we sort them. Checking for consistency of each ASCII value detected during training amounts to assuming that the other characters not seen during learning cannot be present in the system call arguments during operation, and this is unrealistic. Therefore, we do not consider the kind of ASCII values present in the argument; we are somewhat concerned about the steepness of the distribution of the characters per system call. Although we do not expect a completely normal distribution, legitimate system calls tend to maintain a fairly regular distribution. Therefore, we posit that this will improve the model accuracy because malicious codes that target the system arguments like buffer overflow usually stuff the system call arguments with more characters or some unprintable characters to achieve their aim. Although tools like autocorrelation or Naive Bayes can be used to understand the relationship amongst features, these tools learn on the assumptions that each sample is independent (thereby lacking temporal relationship), and it focuses on the relationship amongst features while we focus on the distribution of the sorted ASCII values in a sample regardless of the features it contains.

To illustrate our point further, let us assume that a system call argument returned the following frequency distribution x=[34,0,56,78,27,10,34,0,23,0,0,5,0,0,75,6,68,0,90] for 19 ASCII classes monitored. Then we compute the relative frequency distribution (RFDxi) of one class using RFDxi=xi∑|x|j=1xj/where j≠i. Therefore, the RFDx is given as [.06,0,.11,.15,.05,.02,.07,0,.05,0,0,.01,0,0,.15,.01,.13,0,.18]. Now the sorted system call argument relative frequency distribution (A) of Fig. 1 is the sorted version of RFD. To drive home our argument, Fig. 4 is a plot of the sorted A and unsorted A. When we present the unsorted A to a machine learning model, it tries to learn the relationship amongst the features at the index of the x-axis. Therefore, it takes only a structured input whereby the index of the incoming features are determined by the class of the feature. On the other hand, when we present the sorted A to a model, we are forcing it to learn how the steepness of the distribution that connects the observed classes of the ASCII values in a sample without recourse to a fixed index for a particular class. Therefore, we impose the condition of (4) where n has a maximum value of 256
Ai≥Ai+1,…,≥Anwheren∈N.(4)
View SourceRight-click on figure for MathML and additional features.Hence, (5) provides the mapping from the raw string arguments to the A feature highlighted in the architecture of Fig. 1
A:L⟼Mwhere(5)
View SourceRight-click on figure for MathML and additional features.
L={l∈N∣1≤l≤256}(6)
View SourceRight-click on figure for MathML and additional features.
M={m∈R∣0≤m≤1}(7)
View SourceRight-click on figure for MathML and additional features.
∑i=1256Ai=1.(8)
View SourceRight-click on figure for MathML and additional features.Then, for every FTD/FED, the system call relative frequency distribution (SRF) is computed in the event-driven lane and transmitted to the model block of that channel.

Fig. 3. - 
LSTM-based design of the time-driven and event-driven models.
Fig. 3.
LSTM-based design of the time-driven and event-driven models.

Show All

Fig. 4. - 
Sorted vs unsorted relative frequency distribution.
Fig. 4.
Sorted vs unsorted relative frequency distribution.

Show All

4.2 Predictor
4.2.1 Merge Layer
Our hypothesis is based on creating a deep execution context via a recursive input generated from the attention layer. Since the attention layer has a learned weight, it means that its output which is used to create the context C contains information of multiple previous inputs, and feeding it along with the present input either reinforces a standard profile or weakens the prediction accuracy which will indicate the presence of an anomaly. Therefore, (9) describes our approach of merging the recursive input Ci with the present input Xi
v=merge(Xi→,Ci→).(9)
View SourceRight-click on figure for MathML and additional features.

4.2.2 LSTM Layer (Encoder)
Our choice of LSTM cells in this layer stems from the fact that it is designed primarily for time-series data and its recursive nature helps to propagate temporal information across so many timesteps infinitely in theory. However, we recognize that in practice, there is a limit to how far behind it can propagate the errors before the vanishing gradient problem discussed in [26] sets in. Hence, our idea to augment it with a recursive context to improve the learnability over a long span of time. We feed the output of (9) to the LSTM layer. Different kinds of LSTM configuration can be used, but we use the LSTM units described in [4] to create our layer. This layer’s output is captured mathematically in (10). We omit the bias terms for brevity, and Φ is a nonlinear function like an LSTM
hi=Φ(vi,hi−1).(10)
View SourceRight-click on figure for MathML and additional features.

4.2.3 Attention Layer
Attention layers come in broadly two flavors: soft and hard attention. The soft-attention uses weighted outputs of the input to attend while hard-attention randomly selects a subset of the input to attend. Each has its advantage and disadvantages, but we focus on the soft-attention method in this work. We sacrifice the efficiency of computation by using the weighted sum of all source inputs, as this helps the model to learn efficiently using backpropagation with gradient descent during training. Differing from [27] that uses memory to create context, we add a query weight Wq that is learned during training to ensure that each tuple is not attended to solely based on the information in the present input sequence but also based on the knowledge gained during the learning phase. This query performs the similar role as the term-frequency inverse document frequency used to weigh the occurrence of tuples in the vector space model.

Fig. 5 shows the connections of the attention layer block of Fig. 3. The inputs to this layer are the input weights Wi and the LSTM layer output hi of (10). We pass the LSTM layer output via a tanh layer after being scaled by the input weights Wi and the input bias vector bi to generate correlation vectors mi given in (11)
mi=tanh(Wi⋅hi+bi).(11)
View SourceRight-click on figure for MathML and additional features.This correlation vector (11) represents the effect of each input based on the present. Hence, we multiply it with the query vector Wq which has the global knowledge of each input tuple in the present input sequence to provide deep horizontally spanning inputs for the inference process as shown in (12). This vector is then passed through a softmax layer to generate si in (13). This normalized value is scaled by the input vectors hi and summed to generate the attention vector Zi in (14)
ai=Wq⋅mi+bq(12)
View SourceRight-click on figure for MathML and additional features.
si=(eai∑jeaj)i(13)
View SourceRight-click on figure for MathML and additional features.
Zi=∑i=1nsi×hi.(14)
View SourceRight-click on figure for MathML and additional features.

Fig. 5. - 
Context-aware attention network for the anomaly detection model.
Fig. 5.
Context-aware attention network for the anomaly detection model.

Show All

4.2.4 LSTM Layer (Decoder)
This layer performs the function of a decoder while the lower LSTM is responsible for encoding the input. This layer in conjunction with the fully connected layer tries to reconstruct the input sequence. This layer creates an intermediate output hdi using the previous output yi−1, the context vector Z and the previous hidden state hdi−1 of the previous unit di−1. Equation (15) defines this relationship where Ψ is a nonlinear function called LSTM. Again, bias vector is omitted for brevity
hdi=Ψ(hdi−1,yi−1,Z).(15)
View SourceRight-click on figure for MathML and additional features.

4.2.5 Fully Connected (FC) Layer
Our FC layer is a simple dense layer with the same number of units as there are unique features in the input sequences. The output of this layer is given in (16) where hdi, Wz and bz are the decoding LSTM layer output, the layer weight and the bias vector respectively
yi=Wz⋅hdi+bz.(16)
View SourceRight-click on figure for MathML and additional features.The yi is then passed through a softmax layer for each of the outputs. The whole model is implemented using the Keras/TensorFlow deep learning tool [28].

4.3 Detector
4.3.1 Error Estimator
Time-Driven System Call Events
Given the system call events, Sin in Fig. 1, as part of the input stream, part of the objectives of the time-driven model is to detect variations in the order, and composition of the predicted system call events, Sout. Therefore, the target during learning is that given {Sin:S1,S2,S3,…,SWin}, a shifted version of the system call events, {Sout:SWin+1,SWin+2,SWin+3,…,SWin+Wout} is predicted where Win and Wout are 15 and 5 respectively in our model. Hence, when we have f:(Sin,Bin,Ain)⟼(Sout,Tout) given Sin as the ground truths, the deviation, di=|Sini−Souti| at index, i is the difference between the ground truth and the predicted value for the given sequence. Since the system call events are categorical values, the prediction error is the Boolean error between the predicted and the truth value given in (17)
dS=∑i=1WoutSini≠Souti.(17)
View SourceRight-click on figure for MathML and additional features.

Time-Driven Instruction Cycle Count
To monitor and detect deviations which affect the execution cycle of events like denial of service attacks, we compare the predicted execution cycle with the actual instruction cycle of events. While attacks targeting the order of system call events can also affect the execution cycle, and vice-versa, we predict both the order and instruction cycle count of an event with our multiple output design to enable us determine what kind of anomaly is affecting the system, and make an informed decision during the investigation of the abnormality. Given Bin as the ground truths, and Bout as the predicted instruction cycle count, we compute the reconstruction error using (18)
dB=∑Wouti=1|Bini−Bouti|Wout.(18)
View SourceRight-click on figure for MathML and additional features.

Event-Driven Relative System Call Events
The time-driven network of Fig. 1 can capture anomalies that does the following: a) alters the ordering of known system call events. b) changes the sequence of system call events by creating unknown system call events. c) changes the expected execution cycle of observed system call events. However, the time-driven network lacks the ability to detect variations that change the distribution of the system call events as a result of changes in context occasioned by an error or an attack. And this is where the event-driven network of Fig. 1 comes in. While most of the anomalies monitored by the time-driven network have a local profile, the target of event-driven model is to capture anomalies that span over a long profile (like thousands of events). Hence, the differences in the frequency of operation of the two models. Also, [13], [18], [29] frameworks target anomalies due to variations in the distribution of the events but these anomaly frameworks do not take into account the temporal nature of the system call events. Therefore, with our use of recurrent neural networks to capture the temporal variation of the distribution of the events with time, we create a model that takes into account the behavior of the sources of the events into the design consideration. From Fig. 1, the Buffer serves the purpose of temporarily storing the events for the event-driven model until the number of events reaches the computational window. For our framework, we use 10000 as the computational window since that is the window used for engineering the anomalies in Section 5.2. Given SRFin in Fig. 1 as normalized inputs of dimension 330 (maximum number of system call types in a Linux machine) in a 10000 event window, we predict the distribution of events in the next 10000 event window. This way, we eliminate the limitation of clustering-based anomaly schemes which consider each 10000 event window as an independent observation. The predicted distribution of events is the SRFout and has the same 330 dimension as the input. Therefore, the event-driven model error is given in (19) where Ji is the frequency distribution of the system call event type i
dSRF=∑|SRFout|i=1Ji×∑|SRFout|i=1|SRFini−SRFouti||SRFout|.(19)
View SourceRight-click on figure for MathML and additional features.

4.3.2 Anomaly Detector
The deviations dv={d1,d2,d3,…,d|Vj|} from the validation dataset constitute random variables which we have no knowledge of the underlying distribution. However, since they are error values, we are interested in creating an upper bound or threshold to detect anomalies. Hence, the generation of a threshold value for the prediction errors. Because we know the sample μ and variance σ2, we transform the Bienaymé-Chebyshev inequality to compute the threshold. The inequality guarantees that no more than a certain fraction of values can be more that a certain distance from the μ. The inequality is stated mathematically in (20)
P(|X−μ|≥Υ)≤σ2Υ2.(20)
View SourceRight-click on figure for MathML and additional features.Although the inequality computes the absolute bound with the assumption of a symmetric distribution, we use it in our model because we are only interested in the range where the prediction error d−μ>1. Hence, lack of symmetry will not affect the threshold computation. Therefore, we interchange Υ in (20) with |d−μ| to derive (21) which we use to compute the decreasing probability for increasing deviation |d−μ| where d>μ
P(d>μ)=P(|X−μ|≥|d−μ|)=σ2(d−μ)2.(21)
View SourceRight-click on figure for MathML and additional features.One of the advantages that this technique provides is that we do not have to know the other parameters of the underlying probability distribution. Also, instead of creating a binary threshold of True or False values as was the case in [7], this probability gives us an opportunity to quantize the anomaly scores into bands per one complete cycle of operation like the safety integrity level (SIL) provided by safety standards like IEC 61508 [30]. Our quantized levels are called anomaly level probability, (ALρ) and each level depends on the value of the probability from (21). As (21) measures how the error values are clustered around the mean, our framework should ideally create a high fidelity predictions (reconstruction of the normal profile sequence) to ensure that (21) performs optimally and reduces false negatives.

SECTION 5Experiments
In this section, we describe the details of the experiments used for the dataset generation, the profiles of the data, and the format in which the dataset is hosted.

5.1 Experimental Setup
We use the dataset of [31] to verify the performance of the framework. The dataset was generated using a modified UAV control application derived from the West Virginia University (WVU) Atlas project [32]. There are four main blocks in the UAV platform used for generating the dataset: UAV Physics and Controller (written in C/C++), Bochs Virtual Machine (CPU emulator), Instrumentation Script (written in Python) and SYSCALL Data Log. These four blocks are inside one docker container as shown in Fig. 6. The instrumentation script starts and ends the simulation when the FlightBegin and FlightEnd commands are issued respectively. The script communicates with the UAV physics via a serial interface and dumps captured system calls along with their system call arguments in the data log. POSIX IPC creates an interface between the Instrumentation script and the Bochs VM. The physics of the UAV simulates: a) three throttles of the X-axis, the Y-axis, and the Z-axis, b) gravity of 9.81m/s2, and c) aerodynamic drag. The UAV controller application has four states captured in Fig. 7, and these are explained as follows:

Fig. 6. - 
Setup of the experiment for dataset generation.
Fig. 6.
Setup of the experiment for dataset generation.

Show All

Fig. 7. - 
State machine diagram of the UAV controller.
Fig. 7.
State machine diagram of the UAV controller.

Show All

Take Off: Apply the Z-axis throttle until we attain cruising height.

Cruising: Adjust Z-axis throttle to counteract gravity, and set the X-axis and Y-axis throttles to move at a uniform velocity to the destination.

Landing: Turn off X-axis and Y-axis throttles, and lower the Z-axis throttle to ensure a smooth landing of the UAV.

Landed: The UAV reaches the ground and the instrumentation script exits.

The docker container of this experiment is self-contained and can be run on any computer platform. In our case study, we ran the experiment in a multi-core personal computer with 8GB of RAM and Intel(R) Core(TM) i5-6200 CPU @2.3GHz. Since our experiment aims to capture various profiles that cover the modern attack scenarios using system calls as shown in Section 3.2, we highlight the different scenarios present in Section 5.2.

5.2 Dataset Profiles
We stream the data for three different modes to enable us cover every possible outcome shown in Fig. 2. Therefore, we label the datasets Normal, Delay and Pseudo-Random profiles to denote the experimental conditions. To ensure efficiency in our experiment, we use the simple, security-oriented, and lightweight Alpine Linux which enables us to scale up or down the active processes as the need arises. This way, we have a small but powerful kernel which can be packaged in a container and used for modeling the contexts of processes. Details of the operation of the modes are as follows:

Normal Profile: In this mode, the UAV control application followed strictly the states defined in Fig. 7 without any internal or external injection or interruption to the best of our knowledge. This is the baseline experiment.

Delay Profile: We generate the dataset in this mode by occasionally creating computationally expensive operations that force the controller to lag in sensing the parameters of the application. These expensive diversionary computations result in the UAV controller sensors not polled as supposed, thereby creating instability in the UAV as it struggles to adjust the set parameters concerning the destination, cruising, and altitude. These tasks impact the type and order of system calls generated because its execution leads to the generation of known or unknown system calls that may or may not follow the normal profile pattern. This profile is an example of a non-stealth attack as it results in the crashing of the UAV when the normal state cannot be restored. This scenario is less complicated than the Pseudo-Random profile because the non-stealth nature creates high instability, which can be detected. We achieve the attack in this mode by exponentially raising the computational intensity of the fictitious tasks until the UAV crashes.

Pseudo-Random Profile: Finally, to increase the complexity of the attack scenarios and generate an anomaly based on stealth operation, we create a task that leaks the UAV’s controller states and parameters via a UDP socket at pseudo-random intervals. We aim to mimic stealth operation which does not crash the UAV but monitors its activity for other purposes. So, in the process of monitoring the UAV application, it generates some system calls which might be different regarding the order, type, and argument structure of the system calls. Unlike the delay profile, this operates in stealth mode and does not lead to the crash of the UAV controller application. To launch an attack, after every 10000 events, we apply a 1/10 probability that the stealth attack will run in that window. If the stealth attack is to run within a window, we ensure that the scenario is close to the field experiment by allowing the attack to run repeatedly in a sporadic pattern within the window under attack. This way, we lose control of the distribution of events but gain an opportunity to test anomaly models on sophisticated scenarios.

The dataset of [31] in the current form is unsupervised (it has no labels), therefore, unsupervised anomaly frameworks are the target models. Also, anomaly frameworks that aim to perform labeling by extracting information from the current form can benefit from the dataset of [31].

5.3 Dataset Description
Reference [31] contains the records of system call events, timestamp of the system call event as well as the system call arguments as the UAV controller switches from one state to the other until the UAV lands and the controller terminates. We make available both the raw and processed dataset because the raw dataset contains the system call arguments which we recognize as a valuable resource. Also, since different researchers can create different insights from the timestamp information other than the ones shown in [6], and expanded in this work, the raw dataset might appeal to some people more as it gives room for further exploration of the data to suit different anomaly framework requirements. The raw dataset folder contains the following files: 001_NORMAL_Flight.txt, 002_BUSYDELAY_Flight.txt, and 003_SOCKETS_Flight.txt corresponding to the three modes of the experiments. Complete explanation of the meaning of the different entries in the raw dataset folder can be found in http://tiny.cc/x71chz.

The processed dataset folder uses the process_data.py code included in the http://tiny.cc/x71chz repository to process the raw dataset Timestamp, and RAX attributes to generate the processed dataset. In the processed dataset folder, the files are named delay.csv, normal.csv, and random.csv which corresponds to 002_BUSYDELAY_Flight.txt, 001_NORMAL_Flight.txt, and 003_SOCKETS_Flight.txt respectively in the raw dataset folder. The data in the processed dataset folder has headers, and below we explain the headers:

Beta, β=Ti−Ti−1 (the difference between current timestamp and previous timestamp values).

SysCall ID (using the integer equivalent found in http://tiny.cc/sf2chz, we converted the RAX attributes of the raw dataset to their integer equivalent)

In Table 2, we have provided information on the number of stream events which we used to train and test our model. The number of tested events for the normal profile is different from the validation data we employ to compute the mean, μ and variance, σ which are required by (20) to compute the anomaly level probabilities, ALρ.

TABLE 2 Time-Driven Model Performance Metrics
Table 2- 
Time-Driven Model Performance Metrics
SECTION 6Results and Analysis
We have broken down the results analysis into two subsections to reflect the outputs of the time=driven and event-driven models. In each of the categories, the normal profile serves as the baseline since it has no known anomalies.

6.1 Time-Driven Model
The time-driven pool is a MIMO model that predicts both categorical and continuous values. The inputs are (2), (3), and (5). And the two outputs are the upcoming system call categories and the expected IC count of the system calls. Instead of feeding all the inputs as a multivariate input, we created parallel input lines because of the nature of the inputs. The input of (3) are categorical values which necessitated our use of embedding layer to perform vector encoding while that of (5) and (2) are continuous variables. Hence, feeding all of them as a single multivariate input is not a good design option. Also, creating a single multivariate output was not feasible because the outputs are categorical and continuous variables for the system call and timestamp respectively. The prediction error is computed differently for the two outputs. The categorical output prediction accuracy is the summation of the prediction instances where the prediction outcome matches the target category within a prediction window, wout as given in (17). On the other hand, the average of the absolute error between the predicted and the actual value of the number of instruction cycles required by the system call instance is used for the continuous variable output within a wout and this is given in (18). In our experiment, the ratio of the input window, win to the output window, wout is always greater than 1. The error values from the validation dataset of the normal profile is used to compute the sample mean and variance which is used in (21) to calculate the probability of outputs within a window containing an anomaly. According to (21), the lesser the probability, the higher the influence of anomalies within that prediction window. We used a window instead of an instant-to-instant comparison of output because we realize that a single malicious system call in isolation can rarely cause damage in a process behavior but a sustained and closely occurring malicious prediction is an indication of anomaly in the system process.

6.1.1 Baseline
To verify how our model performs concerning our first objective in Section 1.1, we have shown a snapshot of the prediction accuracy of the model on the normal profile test data for both the system call and IC count prediction at Figs. 8a and 8b respectively. The aim of this section is to test the ability of the anomaly framework to learn the normal operating profile of the UAV controller application. The snapshot in Fig. 8 shows how closely our model generalizes to the normal profile operating condition and having satisfied our first objective; this becomes our basis for discriminating between normal and malicious process contexts.

Fig. 8. - 
Normal profile system call vs instruction cycle count prediction accuracy.
Fig. 8.
Normal profile system call vs instruction cycle count prediction accuracy.

Show All

6.1.2 Anomalous Tests
Figs. 9 and 10 are the prediction accuracy for the normal and delay profiles respectively. It should be emphasized that the system call accuracy values are categorical (representing one of the possible system calls in linux), hence, it is a boolean measure and not a deviation measure. Using the information of Figs. 8, 9, and 10, we compute the error, d generated from the prediction for each of the profiles and we plot the results in Figs. 11a and 11b for the system call and IC count prediction error respectively. From Fig. 11, we can observe that the error values for the normal profile, dnormal is always less than or equal to 1 which is an indication that (d−μ)2≤1, and this condition trivializes (21), and proves that most of the normal profile events contain no anomalies. Since the anomalies we inject as explained in Section 5.2 are applied within a brief duration, it makes sense to see that the error values for both the pseudo-random and delay profiles sometimes are within the non anomalous error range. The significant difference in the error values of Fig. 11 satisfies our second objective of Section 1.1. Also, we set out to investigate the propagation effect of these anomalies, and as can be observed in both figures of Fig. 11, it is obvious that a brief injection of an anomaly can have a propagation of error/fault effect which outlives the duration of the actual anomalous action. In our experiment, during the delay profile, we observed that a brief clock signal manipulation affected the scheduler heavily to the extent that the UAV could not recover even when we stop the attack, and this caused the UAV to crash. This propagation of error/fault effect for the pseudo-random and delay profiles are conspicuous in both figures of Fig. 11. Another interesting observation is the relationship between both outputs. In most steps for the snapshots shown in Fig. 11, when the system call output error is high, the IC count error is high too, but they may not be in the same ALρ band.

Fig. 9. - 
Random profile system call vs instruction cycle count prediction accuracy.
Fig. 9.
Random profile system call vs instruction cycle count prediction accuracy.

Show All

Fig. 10. - 
Delay profile system call vs instruction cycle count prediction accuracy.
Fig. 10.
Delay profile system call vs instruction cycle count prediction accuracy.

Show All

Fig. 11. - 
Prediction error for the normal, pseudo-random and delay profiles’ system calls and instruction cycle count.
Fig. 11.
Prediction error for the normal, pseudo-random and delay profiles’ system calls and instruction cycle count.

Show All

Furthermore, we pass the error value from the stream of events to the anomaly detector block of Section 4.3.2, and based on the value of the ALρ returned, we determine if there is an anomaly in that window of the stream. If there is an anomaly present, we assign it an anomaly level to indicate the severity (which is a measure of errors detected within the prediction window). In our model, we have four levels of ALρ, and the higher the value of the ALρ, the lesser the severity of the anomaly as captured by (21). ALρ≥1 is the special case when (d−μ)2≤1, and since probability cannot be greater than 1, this scenario affirms that there is no anomaly and is not assigned any level. Hence, this scenario depicts the standard operating context of the process
ALρ=⎧⎩⎨⎪⎪⎪⎪⎪⎪AL1AL2AL3AL4ρ<0.250.25≤ρ<0.50.5≤ρ<0.750.75≤ρ<1.(22)
View SourceRight-click on figure for MathML and additional features.Therefore, we present the Alρ as a piece-wise function in (22). These ALρ are illustrated in Fig. 12 where Figs. 12a and 12b are the system call and IC count prediction ALρ respectively while ρ is the probability computed by (21).

Fig. 12. - 
Anomaly level probabilities $ \boldsymbol{AL_{\rho }}$ALρ for the pseudo-random and delay profiles’ system calls and instruction cycle count.
Fig. 12.
Anomaly level probabilities ALρ for the pseudo-random and delay profiles’ system calls and instruction cycle count.

Show All

6.1.3 Performance Metrics
In our experiment, we kept track of the window in which an anomaly was injected but we did not keep track of the exact instants within a window that it runs. This is because we understand that effects of anomalies can linger beyond when the attack stopped, and it would have been untenable to assume that the effects of anomalous injection can stop at an instant. Therefore, we cannot give an exact number of anomalies present but we know the windows in which anomalies exist. As seen from Table 2, the time-driven model reported 969 and 706 false positives for the syscall event and instruction cycle anomaly level probability respectively, mostly from system events generated during the take-off and landing instability. The false positives are 11.09 and 8.08 percent for the system call events, and the instruction cycle count ALρ respectively. Also, looking at the ALρ numbers for both the pseudo-random and delay profiles, we observe that the numbers are not the same, and this confirms our hypothesis that the effect of the attacks on either of the output channels depends on the type of attack injected into the system. Also, the variation in the number of anomalies captured by the two output channels confirms our hypothesis in Section 1 that single output frameworks that aims to detect anomalies in the system call event output might miss anomalies aimed at the execution cycle count feature of the system calls and vice-versa. The delay profile attack affects the system call event sequence more because it creates fictitious computationally intensive tasks that generate events that affected the order and type of system calls. In a window where the probability indicated that an anomaly was not run but we observed an ALρ in that window, we label that a false positive. For the pseudo-random profile, there were a total of ≊13% false system call event ALρs and 18.2 percent false instruction cycle count ALρs. Expectedly, the delay profile has a lesser false alarm since the attack was non-stealthy and was increasing exponentially. The exact number of false positives are 9.6 and 7.9 percent for the system call event and the instruction cycle count respectively.

6.2 Event-Driven Model
To detect deviations which has a long profile (spans over thousands of events), we use the event-driven model to monitor the distribution of the system calls events within a window of observation. Each window of observation is called a trace, and each trace has 10000 events corresponding to the same window used for the experiments in Section 5.2. Therefore, it takes as input, the SRFin at time, t and predicts SRFout at time, t+δt where δt=1/FED and FED remains the frequency of operation of the event-driven model which is far less than FTD. This transformation results in 4 normal profile traces for testing, 43 pseudo-random profile traces, and 10 delay profile traces. As seen from Table 3, there is no false positive on the normal profile test traces. In the pseudo-random profile, a total of 8 traces has anomalies, and 87.5 percent of the anomalous traces was flagged. In the delay profile, the anomalies progressively increased in intensity until the UAV crashed. Therefore, all the traces contain anomalies and the event-driven model was able to flag 90 percent of the traces. Without complementing the time-driven model with the event-driven model, anomalies with long profile will be missed since the effect within a short span monitored by the time-driven model may not be significant for it to be captured by the time-driven model.

TABLE 3 Event-Driven Model Performance Metrics
Table 3- 
Event-Driven Model Performance Metrics
6.3 Complexity of the Framework
Running the framework on a multi-core personal computer with 8GB of RAM and Intel(R) Core(TM) i5-6200 CPU @2.3GHz, we use a Python timing library to obtain the inference time of the time and event-driven models in both the CPU and Wall clock times as shown in Table 4. As seen from the table, the event-driven model has a longer inference time because of the dimension of both the input and the output. It also has to process up to 10000 events into a 330 relative frequency feature set and this adds to the inference time. From Table 4, the inference time returned by these models are suitable for any online anomaly detection applications.

TABLE 4 Complexity of the Framework
Table 4- 
Complexity of the Framework
As stated in Section 4.1, FTD/FED<1 represents the frequency of computation of the two models where FTD and FED represents the time-driven and event-driven models respectively. Since the time-driven model targets changing fast-profiles and the event-driven model targets varying slow-profiles, the longer inference time of the event-driven model does not impact the system negatively as its frequency of operation is far lower than that of the time-driven model.

SECTION 7Conclusion and Future Work
In this work, we present a deeper insight into anomaly detection in system processes using system calls and its properties. We argue that detection of anomalies at the kernel level presents a novel view into the more complex machine-to-machine interactions which take place at that layer. To accomplish this, we present the details of how to extract more useful features from a system call to broaden the scope of anomalies that can be detected by the model. Then, a MIMO architecture is developed to increase the scope of the model and respond to the increased scope of threats that are available with more profound model abstraction. Our results confirm our hypothesis that a broadened feature set and a MIMO based model do not just assist us to increase the scope of anomalies we can detect, but also help us to understand the effect of one type of anomaly on the output of another model targeting a different type of anomaly. The differences in the number of anomalies detected by each output channel confirms our design hypothesis that using a one-size-fits-all single output of system call event IDs only may result in missing other anomalies that aim to alter the behavior of other features of the system like clock-glitch attack. Finally, the variation in the type and number of anomalies (fast and slow profiles) targeted by the time-driven and event-driven models respectively has justified our ensemble architecture design. Our next research direction will be exploring a novel offloading scheme that can manage this model in an edge network environment as we hope to deploy this for anomaly detection in both the embedded and non-embedded devices.