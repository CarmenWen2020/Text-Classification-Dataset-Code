Since support vector regression (SVR) is a flexible regression algorithm, its computational complexity does not depend on the dimensionality of the input space, and it has excellent generalization capability. However, a central assumption with SVRs is that all the required data is available at the time of construction, which means these algorithms cannot be used with data streams. Incremental SVR has been offered as a potential solution, but its accuracy suffers with noise and learning speeds are slow. To overcome these two limitations, we propose a novel incremental regression algorithm, called online robust support vector regression (ORSVR). ORSVR solves nonparallel bound functions simultaneously. Hence, the large quadratic programming problem (QPP) in classical v-SVR are decomposed into two smaller QPPs. An incremental learning algorithm then solves each QPP step-by-step. The results of a series of comparative experiments demonstrate that the ORSVR algorithm efficiently solves regression problems in data streams, with or without noise, and speeds up the learning process.
SECTION 1Introduction
As a flexible parametric regression algorithm, support vector regression (SVR) [1], [2], [3] is a popular regression algorithm in many fields – for example, it is used to predict electrical loads [24], stock market prices [25], wind speeds [26], and the weather [27]. SVR trains using a symmetrical loss function, which equally penalizes high and low mis-estimates. In addition, SVR uses Vapnik's ϵ-insensitive approach, so a flexible tube of minimal radius is formed symmetrically around the estimated function, such that the absolute values of errors less than a certain threshold ϵ are ignored both above and below the estimate. In this manner, points outside the tube are penalized, but those within the tube, either above or below the function, receive no penalty [39]. One of the main advantages of SVR is that its computational complexity does not depend on the dimensionality of the input space. Additionally, it has excellent generalization capability with high prediction accuracy. When SVR algorithms were first designed, the assumption was that all data could be obtained at one time. However, the data in many of today's applications are not usually modeled as persistent tables. Rather they take the form of a data stream [4], [5], [6], i.e., an ordered infinite sequence of data instances {(X1,Y1),(X2,Y2),…,(Xt−1,Yt−1),(Xt,Yt),…}. Each data instance (Xt,Yt) is the smaple of this data stream at timestamp t [47], where X ∈ Rd is input varable and Y ∈ R is output variable. Representative applications include dynamic talent flow analysis [28], traffic control [29], web media processing [30], and time-series data [31], [32]. This streaming data poses new challenges for SVR algorithms [33], [34], [35].

For using SVR to handle a data stream, Liu et al. [41] proposed an ensemble SVR. The proposed approach creates new sub-models directly from a basic model and the sub-models represent separately the data stream at different periods. Chen et al. [42] proposed an improved multiple kernel SVR approach. However, this is in fact the learning of these approaches is still needed a small batch of data, so these methods cannot always work in a data stream environment because only one sample can be operated in extreme condition. Based on the reason, online learning, which investigates how to learn in an extreme condition, has been proposed as a possible solution [7], [8]. In online learning, data is processed sequentially, i.e., (X1,Y1),(X2,Y2),…,(Xt,Yt) and, with each new input data, the model is updated (Xt,Yt) as ft←ft−1 [9], [44]. Therefore, researchers proposed online version of SVR for handling data streams. Essentially, online SVR integrates an SVR algorithm with online learning to handle regression problems with data streams [11], [12], [13], [14]. Examples of this approach include Junshui et al. [11] who introduced ϵ-SVR in an online algorithm called accurate online support vector regression (AONSVR). A new adjustment method which based on [10] was proposed to adjust model ft. In this adjustment method, all samples were assigned into three sets: remaining, supporting, error set. Gu et al. [12] then improved this adjustment method to solve the extra constriction in v-SVR and proposed an incremental v-SVR algorithm (INVSVR). In INVSVR, the speed of adjusting model ft was significantly improved. Next, for handling uncertain data streams, Hang et al. [13] decomposed the classical v-SVR into a new model called dual-v-SVR, and use the same adjustment method in INVSVR to adjust the model Omitaomu et al. [14] applied the AONSVR to handle evolving data streams. In his method, the AONSVR with varying rather than fixed parameters and proposed a method to update parameter weight automatically.

Not like other online regression algorithms such as online regression tree [45] and online multiple kernel regression [46], above mentioned online/incremental SVR algorithms do not need any prior knowledge of whole data streams. For example, in online regression tree, the depth of tree need to be user defined, and the number of kernels need to be user defined in online multiple kernel regression. These parameters could be set suitable if some prior knowledge of whole data streams were obtained. However, the performance of online SVR algorithms is still limited by two challenges:

If a data source is very noisy due to, say, electromagnetic interference, temporary failure of sensors, and so on, accuracy will suffer. Some robust SVR algorithms have been explicitly designed to handle noisy data, and these perform better than classical SVRs [15], [16], but performance is still not optimal. For example, Lin et al. [15] incorporated the concept of fuzzy set theory into SVR, while Peng [16] proposed an interval twin SVR algorithm. Other SVR algorithms handle noisy data [17], [19] by replacing the constraints in classical SVR with probability constraints, such as the SVR algorithm in [19] that is robust to bounded noise. However, the quadratic programming problems (QPPs) [40] in these above mentioned SVR algorithms remain too complex for direct translation into incremental SVR.

Incremental SVRs [11], [12] learn slowly. The main idea of the incremental learning algorithm is to adjust the weight wnew of a new sample in an infinite number of discrete steps until it meets the Karush–Kuhn–Tucker (KKT) conditions. Further, the existing samples must continue to satisfy the KKT conditions at each step. However, the KKT conditions in existing incremental SVR algorithms are complex so, in practice, any adjustments to wnew tend to create many conflicts among the KKT conditions of the existing samples. As a result, repeated adjustments are needed for wnew, which drastically slows down the learning speed.

To overcome these two challenges, we propose a novel incremental SVR algorithm called online robust support vector regression (ORSVR). ORSVR is a novel variant of incremental v-SVR algorithm that modified classical v-SVR to handle noisy data and proposed a new incremental learning method. ORSVR seeks two unparallel bound functions to construct an insensitive zone that includes as many samples as possible. A specifically-designed incremental learning algorithm with KKT conditions is then used to solve the QPPs in ORSVR step-by-step based on only one sample per round. Therefore, the two major contributions of this paper are:

To handle noisy data, we modified classical v-SVR as the regression model of ORSVR. In ORSVR, the QQP in a classical v-SVR [21] is transformed into two QPPs. Then, through the new regression model, ORSVR seeks up- and down-bound functions by solving two related OPPs simultaneously. Each new QPP is smaller than the large QPP in classical v-SVR, which means the KKT conditions for each bound are simpler. As a result, ORSVR learns faster than standard incremental v-SVR or incremental ɛ-SVR algorithms. ORSVR is also good at capturing the characteristics of data distributions due to the modification of classical v-SVR, so it is very useful for handling noise. Moreover, the correlation between the box constraints and the size of the training sample set in the classical formulation of v-SVR [21] makes it difficult to design an incremental learning algorithm. However, with ORSVR, the box constraints are independent of the sample set size, which means ORSVR can learn online.

An incremental learning algorithm based on KKT conditions that ensures ORSVR can solve the QPPs in a sequential manner. The basic idea is similar to [11] where the weight αc is changed according to a new sample (Xt,Yt) in an infinite number of discrete steps until it meets the KKT conditions, while ensuring that the existing samples continue to satisfy the KKT conditions at each step. However, ORSVR contains one additional complex equation constraint over ϵ-SVR, so an easy initial adjustment needs to be made to ensure the KKT conditions are still met when no new sample arrives. Using the incremental learning algorithm to seek the upper function and lower functions simultaneously, ORSVR can quickly and effectively handle data streams with noise.

The rest of this paper is organized as follows. Section 2 gives a brief overview of classical v-SVR. Section 3 introduces the modified formulation of v-SVR. An online robust support vector regression algorithm then is introduced in Section 4. The experiments are presented in Section 5. Concluding remarks and future works are given in Section 6.

Notation: To make the notations easier to follow, we give a summary of the notations in the following list.

ϵ	
is the ϵ-insensitive loss function and defined as |Y −f(X)|ϵ = max{0,|Y −f(X)| −ϵ} for a predicted value f(X) and a true output Y, which does not penalize errors below some ϵ > 0, chose a priori. Thus, the region of all samples with {|Y −f(X)|≤ϵ} is called ϵ-tube.

K	
is the kernel function

αi−α∗i	
is the weight of K(Xi,Xj)

∗1i	
is the ith * variable in the upper function

∗2i	
is the ith * variable in the lower function

Δ	
is the amount of the change of each variable.

Q′ij	
is the submatrix of Qij after initial adjustment

∗′1i	
is the ith * variable in the upper function after initial adjustment

∗′2i	
is the ith * variable in the lower function after initial adjustment

Q′ssSS	
is the submatrix of Q’ with the rows and columns indexed by SS

Q′nnSs	
is the submatrix of Q’ with the rows and columns indexed by nn

SECTION 2Preliminary
A regression algorithm learns a model Y=f(X)=<w,X>+b, where X means one or more independent variables and Y means a response variable. w represents the weight of X, b represents the bias, and 〈⋅,⋅〉 denotes inner product in reproducing kernel Hilbert space (RKHS).

v-Support Vector Regression (v-SVR) [21] is a popular regression algorithm, which can automatically adjust the parameter ϵ of an ϵ-insensitive loss function. Given a training sample set T = {(X1,Y1),(X2,Y2),…,(Xi,Yi)} with Xi ∈ Rd and Yi ∈ R, considered the following primal problem:
minω,ϵ,b,ξ(∗)i12s.t.  ∥w∥2+C⋅(vϵ+1l∑i=1l(ξi+ξ∗i))(⟨w,ϕ(Xi)⟩+b)−Yi≤ϵ+ξi,Yi−(⟨w,ϕ(Xi)⟩+b)≤ϵ+ξ∗i, ξ(∗)i≥0, ϵ≥0,i=1,…,l.(1)
View Sourcewhere the training samples Xi are mapped into a high dimensional RKHS using the transformation function Φ. ∥w∥ is a regularization term, which characterizes the complexity of the regression model. C is the regularization constant, and ξ(∗) is the slack variable (‘∗’ is shorthand for the variables both with and without asterisks). v is the introduced proportion parameter with 0 ≤ v ≤ 1, which controls the number of support vectors and errors.

Then, the corresponding dual is:
minα,α∗s.t.12∑i,j=1l(α∗i−αi)(α∗j−αj)K(Xi,Xj)−∑i=1l(α∗i−αi)Yi∑i=1l(α∗i−αi)=0, ∑i=1l(α∗i+αi)≤Cv,0≤α(∗)i≤Cl, i=1,…,l.(2)
View SourceRight-click on figure for MathML and additional features.where K(Xi,Xj) =⟨Φ(Xi),Φ(Xj)⟩.

In v-SVR, the parameter v is used to determine the proportion of the number of support vectors you prefer to keep in your solution with respect to the total number of samples in the dataset, and the parameter ϵ in the optimization problem formulation and it is estimated automatically (optimally) according to v. However, in ϵ-SVR [22], there is no control over how many data vectors will become support vectors. It could be a few; it could be many. But you do have total control over how much error is allowed in the model, and anything beyond the specified ϵ will be penalized in proportion to C, i.e., the regularization parameter.

SECTION 3Formulation of Online Robust Support Vector Regression
Although v-SVR has some advantages over ϵ-SVR [22], v-SVR introduces two complications. First, the box constraints are related to the size of the training sample set. Second, the formulation contains an additional inequality constraint, which makes them more complicated than ϵ-SVR. These complications lead to researchers need to design extra adjustments to solve it. For example, in [12], Gu extends the training samples and adds an extra adjustment, called an initial adjustment, as a preprocessing step. In addition, classical v-SVR cannot robust to noisy data well, so the prediction performance of v-SVR will be sharply decreases if the training dataset with noisy data.

In this paper, we aim to propose a novel type of SVR with simplified formulation so that easily designing an adjustment method to online adjust it. In addition, this novel SVR also robust to noisy data. In [23], Peng et al. proposed twin SVR (TSVR), which is a regressor that determines a pair of up and down-bound functions by solving two related SVM-type problems. Each problem is smaller than that of a classical SVR. Hence, to simplify the formulation of the classical v-SVR, we design a novel SVR which called ORSVR following the spirit of TSVR. Like TSVR, ORSVR transforms the classical v-SVR into a new nonparallel plane regressor, but different with TSVR, ORSVR is robust to noisy data.

ORSVR is based on the assumption that the upper and lower bounds will build an insensitive zone. Then, according to the same concept of an insensitive zone in classical v-SVR, any deviations inside the insensitive zone are discarded, and any deviations outside the insensitive zone are rejected. Further, the insensitive zone should be of minimal size to include as many training samples as possible (Xi,Yi), and as soon as possible, so as to ensure a better fit for the given samples. Consequently, the upper bound function f1(X) should have a minimal w, b, ɛ, and ξ and be moved upward. That way, more samples remain below the upper bound function f1(X). Any error above f1(X) will not be captured in the slack variable ξ1i, which is penalized in the objective function via the regularization parameter C1>0, which is chosen a priori. Similarly, the lower bound function f2(X) of the insensitive zone is moved downward by maximizing w,b,ϵ and ξ in the objective function, again, to ensure that as much training data as possible (Xi,Yi) is kept above the lower bound. The specifics of the upper and lower bound functions, f1(X) and f2(X), are introduced below.

First, (2) is transformed the following pair functions:
minw1,ϵ1,b1,ξ1is.t.12∥w1∥2+C1(v1ϵ1+1N∑i=1Nξ1i)⟨w1⋅Φ(Xi)⟩+b1−Yi≤ϵ1−ξ1iξ1i≥0,ϵ1≥0,   fori= 1, …,N,(3)
View SourceRight-click on figure for MathML and additional features.and
minw2,ϵ2,b2,ξ2is.t.12∥w2∥2+C2(−v2ϵ2+1N∑i=1Nξ2i)⟨w2⋅Φ(Xi)⟩+b2−Yi≤ϵ2+ξ2i ξ1i≥0,ϵ2≥0,  fori= 1, …,N,(4)
View Source

Each of these two equations determines the upper and lower bounds; however, they cannot be solved directly. Therefore, the problem of seeking the upper bound function f1(X)=⟨w1⋅Φ(Xi)+b1⟩ is transformed into solving the following QPPs:
minw1,b1,ξ1is.t.12∥w1∥2+C1(v1(b1−ϵ1)+1N∑i=1Nξ1i)⟨w1⋅Φ(Xi)⟩+(b1−ϵ1)≥Yi−ξ1iandξ1i≥0fori= 1, …,N,(5)
View Sourcewhere the meaning of the parameters is the same as (2). If we set B1=b1−ϵ1, the solution of minimizing w1,B1, and ξ1 can be found by solving the corresponding dual:
max−s.t.12∑i=1N∑j=1Nα1iα1jK⟨Xi⋅Xj⟩+∑i=1Nα1iYi∑i=1Nα1i=C1v1α1i∈[0, C1/N] ,i= 1, …,N,(6)
View SourceRight-click on figure for MathML and additional features.where K(Xi,Xj) =⟨Φ(Xi),Φ(Xj)⟩, 〈⋅, ⋅〉 denotes inner product in RKHS. Similarly, the problem of estimating f2(Xi)=⟨w2⋅Φ(Xi)⟩+b2 is equivalent to the following optimization problem:
minw1,b1,ξ1is.t.12∥w2∥2+C2(−v2B2+1N∑i=1Nξ2i)⟨w2⋅Φ(Xi)⟩+B2≤Yi+ξ2i and ξ2i≥0 fori= 1, …,N,(7)
View SourceRight-click on figure for MathML and additional features.and the solution is found by solving this dual:
max−s.t.12∑i=1N∑j=1Nα2iα2jK⟨Xi⋅Xj⟩−∑i=1Nα1iYi∑i=1Nα2i=C2v2α2i∈[0, C2/N],i= 1, …,N,(8)
View Source

After estimating the upper bound and lower bound functions, the final regression function is constructed as
f(Xi)=12[f1(Xi)+f2(Xi)]=12[∑i=1N(α1i−α2i)K⟨Xi⋅Xj⟩+(B1+B2)].(9)
View Source

Based on (5) and (7), it is clear that the formulation of ORSVR is quite different from that of v-SVR in one fundamental way. ORSVR solves a pair of QPPs, whereas in v-SVR, only a single QPP needs to be solved. Further, in v-SVR, the QPP has two groups of constraints for all data points, but there is only one group of constraints per QPP in ORSVR for all data points. This strategy of solving two smaller sized QPPs, rather than one large QPP, means the formulation of ORSVR simpler than classical v-SVR.

However, the box constraints from (6) and (8) are still correlated to the size of the training samples, which makes it difficult to design an incremental learning algorithm for the upper and down-bound functions. Therefore, to obtain an equivalent formulation with box constraints that are independent of the sample size, the objective function of (3) is multiplied by the size of the training sample set, resulting in the following primal problem:
minw1,b1,ξ1is.t.N2∥w1∥2+C1(v1B1N+∑i=1Nξ1i)⟨w1⋅Φ(Xi)⟩+B1≥Yi−ξ1i and ξ1i≥0fori=1,…,N,(10)
View Source

It is easy to verify that (10) is equivalent to the primal problem (5), and the dual problem for (10) is
max−s.t.12∑i=1N∑j=1Nα1iα1jQij+∑i=1Nα1iYi∑i=1Nα1i=C1v1Nα1i∈[0, C1] ,i= 1, …,N(11)
View SourceRight-click on figure for MathML and additional features.where Q is a positive semidefinite matrix with Qij=(1/N)⋅K(Xi,Xj). The lower bound function f2(X)=⟨w2⋅Φ(Xi)⟩+b2 can be modified in the same way.

After modification, the formulation of the classical v-SVR is transformed into the ORSVR where two smaller sized QPPs need be solved, and the box constraints are independent of the size of the training sample set for each QPP. In addition, the upper bound and lower bound functions of the regression model, as estimated by ORSVR, well capture the characteristics of the data distributions. This allows the conditional mean and the predictive variance to be estimated both automatically and simultaneously. Such a feature could be useful in many cases, especially when the noise is heteroscedastic and depends strongly on the input values. Due to these advantages, ORSVR makes a good choice as the regression model for handling noisy data.

SECTION 4Learning process of Online Robust Support Vector Regression
The previous section introduced the formulation of ORSVR. This section presents the details of the online learning process.

4.1 Karush–Kuhn–Tucker Conditions
According to convex optimization theory, a solution for the minimization problem (11) could be obtained by minimizing the following convex quadratic objective function under constraints:
minW= s.t.12∑i=1N∑j=1Nα1iα1jQij−∑i=1Nα1iYi+∑i=1Nδi(α1i−C1v1N)+∑i=1Nηiα1i−∑i=1N[μi(α1i−C1)]ηi,μi≥0, i=1,…N,(12)
View Source

Then, according to the KKT theorem [20], the first-order derivative of W leads to the following KKT conditions:
∂W∂α1i=12∑i=1Nα1jQij−Yi+δ+ηi−μi=0(13)
View SourceRight-click on figure for MathML and additional features.
∂W∂δi=∑i=1Nα1i−C1v1N=0.(14)
View SourceRight-click on figure for MathML and additional features.

To increase readability, we can replace δ with B1 then, as explained above, this becomes an optimization problem with a convex domain. Following the Kuhn-Tucker Theorem [20], the sufficient conditions for a point to be an optimum are
α1i∈[0, C1], α1i(∑j=1Nα1jQij+B1−Yi+ηi−μi)=0(15)
View Source
ηi≥0, ηiα1i=0(16)
View Source
μi≥0, μi(α1i−C1)=0.(17)
View SourceRight-click on figure for MathML and additional features.

The regression function f(Xi) estimation can be written as
f(Xi)=∑i=1Nα1jQij+B1(18)
View SourceRight-click on figure for MathML and additional features.and margin function is defined as
h(Xi)=f(Xi)−Yi(19)
View Source

Replacing ∑Nj=1α1jQij+B1 with h(Xi) in (15), then the relation of h(Xi) and 0 at the changing of α1i can be found:
⎧⎩⎨⎪⎪h(Xi)≥0 h(Xi)=0 h(Xi)≤0 α1i=0α1i∈[0, C1]α1i=C1.(20)
View Source

Equation (20) is defined as a system of conditions, called Karush-Kuhn-Tucker (KKT) conditions. The training sample set S is then partitioned into three independent sets according to the value of h(Xi) (see Fig. 1):

Ss={i: h(Xi)=0,0<α1i<C1} is the supporting set SS, which includes the training samples strictly on the tube;

SE={i: h(Xi)≤0, α1i=C1} includes the training samples exceeding the tube; and

SR={i: h(Xi)≥0, α1i=0} is the remaining set SR, which includes the training samples covered by the tube.


Fig. 1.
The partitioning of the training samples S into three independent sets by KKT conditions. (a) SS. (b) SE. (c) SR.

Show All

The same procedure is used for finding the lower function f2(Xi), and the KKT condition for the analyses is
⎧⎩⎨⎪⎪h(Xi)≤0 h(Xi)=0 h(Xi)≥0 α2i=0α2i∈[0, C2]α2i=C2.(21)
View SourceRight-click on figure for MathML and additional features.

Similarly, the training sample set S for the lower function f2(Xi) is also be partitioned into three independent sets according to the value of h(Xi).

4.2 Adjustment Methods
In this section, we focus on the step-by-step process for obtaining the optimum solution of the ORSVR model. The main idea this adjustment method follows the same procedure as used in AONSVR and INVSVR. That is, the weight αc is changed according to a new sample Xt in an infinite number of discrete steps until it meets the KKT conditions, while ensuring that the existing samples continue to satisfy the KKT conditions at each step.

However, in AONSVR, when a new sample Xt arrives, the weights αc of the new sample Xt are initially set to 0 because the sum of α1i equals 0. Two adjustment steps are then needed to ensure all existing samples meet the KKT conditions. In INVSVR, the sum of α1i equals CvN. But because the training samples in INVSVR extended to (Xi,Yi,zi), where zi={−1,1} is the label of the training sample (Xi,Yi), and the constraint ∑2Ni=1αi=CvN has a conflict with ∑2Ni=1ziαi=0. Thus, the incremental learning algorithm in [12] must include an extra adjustment step, called the initial adjustment, to preprocess the training samples. Note that two further adjustment steps are still needed to resolve any conflicts.

Hence, we designed a simple adjustment method for upper function to seek the minimization problem (10). Adding a new sample Xc, according to (11), the equation:
∑i=1Nα1i+α1c=C1v1(N+1)(22)
View Sourceneed to be met. However, if we let the weights α1c be the equal of 0, the sum of α1i should equals  C1v1(N+1), i.e., ∑Ni=1α1i=C1v1(N+1).

To ensure all the existing samples continue to satisfy the KKT conditions, weight α1i of each sample {s1,s2,…,ss} is adjusted the through following equation (see Appendix A, which can be found on the Computer Society Digital Library at http://doi.ieeecomputersociety.org.ezproxy.auckland.ac.nz/10.1109/2979967 for the proof):
⎡⎣⎢⎢⎢⎢⎢B′1α′1s1⋮α′1ss⎤⎦⎥⎥⎥⎥⎥=⎡⎣⎢⎢⎢⎢⎢B1α1s1⋮α1ss⎤⎦⎥⎥⎥⎥⎥+⎡⎣⎢⎢⎢⎢⎢01⋮11Q′s1s1  ⋮Q′sss1 …… ⋱…1 Q′s1ss ⋮ Q′ssss⎤⎦⎥⎥⎥⎥⎥−1R⎡⎣⎢⎢⎢⎢C1v1⋮−h′−h′⎤⎦⎥⎥⎥⎥(23)
View SourceRight-click on figure for MathML and additional features.where α′1i represents the new weight of each sample in the supporting set, and B′1 represents the new bias. Thus, Q′ij=(1/(N+1))⋅K(Xi,Xj). Then according to (19), we get
h′(Xi)=f′(Xi)−Yi=∑i=1Nα1jQ′ij+B1−Yi.(24)
View Source

Because ∑Ni=1α′1i=C1v1(N+1), when a new sample Xc arrives, the weight α′1c of the new sample Xc can be set to 0, but it needs to be assigned to a set to satisfy the KKT conditions. If the assignment violates the KKT conditions, α′1c will be adjusted.

The process for making these adjustments is as follows. When adding a new sample (Xc,Yc), the margin function changes to
h′(Xi)=∑j=1NQ′ijα∗1j+Q′ijα∗1c+B∗1−Yi(25)
View SourceRight-click on figure for MathML and additional features.

The variation of the margin can then be easily computed with
Δα1i=α∗1i−α′1i, i=1,…N,c(26)
View SourceRight-click on figure for MathML and additional features.
ΔB1=B∗1−B′1(27)
View Source
Δh′(Xi)=∑j=1NQ′ijΔα1j+Q′icΔα1c+Δb1.(28)
View Source

Because ∑Nj=1α′1j=C1v1(N+1) and α′1c+∑Nj=1α′1j=C1v1(N+1) the following equation can be constructed:
∑j=1NΔα1j+Δα1c=0⇒∑j=1NΔα1j=−Δα1c.(29)
View SourceRight-click on figure for MathML and additional features.

Given the specific properties of each of the three sets, only the samples in supporting set {s1,s2,…,ss} are able to change Δα1j, so only these samples can contribute to the new equation
∑j∈SQ′ijΔα1j+ΔB1=−Q′icΔα1cwherei∈SS∑j∈SΔα1j=−Δα1c.(30)
View Source

Then, the equations above can be rewritten in an equivalent matrix form:
⎡⎣⎢⎢⎢⎢⎢ΔB1Δα1s1⋮Δα1ss⎤⎦⎥⎥⎥⎥⎥=−⎡⎣⎢⎢⎢⎢⎢01⋮11Q′s1s1  ⋮Q′sss1 …… ⋱…1 Q′s1ss ⋮ Q′ssss⎤⎦⎥⎥⎥⎥⎥−1R⎡⎣⎢⎢⎢⎢⎢1Q′s1c⋮Q′ssc⎤⎦⎥⎥⎥⎥⎥Δα1c.(31)
View Source(31) can also be rewritten as follows:
⎡⎣⎢⎢⎢⎢⎢ΔB1Δα1s1⋮Δα1ss⎤⎦⎥⎥⎥⎥⎥=βΔα1c=⎡⎣⎢⎢⎢⎢⎢β1bβ1s1⋮β1ss⎤⎦⎥⎥⎥⎥⎥Δα1c=−R⎡⎣⎢⎢⎢⎢⎢1Q′s1c⋮Q′ssc⎤⎦⎥⎥⎥⎥⎥Δα1c.(32)
View SourceRight-click on figure for MathML and additional features.

According to (32), the values of Δα1i and Δb1 values are updated to compute β.

In terms of the error and the remaining samples, a set N=SE∪SR ={n1, n2,…,nn} is defined. These do not change Δα1j, but they do change h′(Xi). The variations in h′(Xi) are rewritten in matrix notation:
⎡⎣⎢⎢Δh′(Xn1)⋮Δh′(Xnn)⎤⎦⎥⎥=⎡⎣⎢⎢ΔQ′n1c⋮ΔQ′nnc⎤⎦⎥⎥Δα1c+⎡⎣⎢⎢⎢⎢⎢01⋮11Q′n1s1  ⋮Q′nns1 …… ⋱…1 Q′n1ss ⋮ Q′nnss⎤⎦⎥⎥⎥⎥⎥⎡⎣⎢⎢⎢⎢⎢ΔB1Δα1s1⋮Δα1ss⎤⎦⎥⎥⎥⎥⎥.(33)
View Source

Replacing the variations of Δα1i and ΔB1 with the results obtained in (32) gives
⎡⎣⎢⎢Δh′(Xn1)⋮Δh′(Xnn)⎤⎦⎥⎥=⎡⎣⎢⎢ΔQ′n1c⋮ΔQ′nnc⎤⎦⎥⎥Δα1c+⎡⎣⎢⎢⎢⎢⎢01⋮11Q′n1s1  ⋮Q′nns1 …… ⋱…1 Q′n1ss ⋮ Q′nnss⎤⎦⎥⎥⎥⎥⎥βΔα1c.(34)
View SourceRight-click on figure for MathML and additional features.

Then, γ can be defined as
γ=⎡⎣⎢⎢ΔQ′n1c⋮ΔQ′nnc⎤⎦⎥⎥+⎡⎣⎢⎢⎢⎢⎢01⋮11Q′n1s1  ⋮ Q′nns1 ……⋱…1 Q′n1ss ⋮ Q′nnss⎤⎦⎥⎥⎥⎥⎥β.(35)
View SourceRight-click on figure for MathML and additional features.

And (34) is rewritten as
⎡⎣⎢⎢Δh′(Xn1)⋮Δh′(Xnn)⎤⎦⎥⎥=γΔα1c.(36)
View Source

According to (36), the values of Δh′(Xi) can be updated by computing γ.

In summary, if the weight α′1c of the new sample Xc be set to 0 initially, adjusting the value of α1i, b1, and h(Xi) can be followed (32) and (36).

Algorithm 1 summarizes ORSVR in pseudo code. Lines 2–6 perform the first step of the adjustment, after which the sum of α′1j equals C1v1(N+1). Lines 7–18 assign the new samples Xc into one of the three sets SS,SE,SR. Among lines 7-18, line 14 derives the minimal increment Δαmin1c using the method in [11], then line 15 adjusts the weight α′1c of the new sample Xc until the assignment meets the KKT conditions.

However, it may happen that α′1i<0 after the update, which means α′1i<0 conflicts with α′1i∈(0,C1). Therefore, we designed an additional method to overcome this possible conflict. If α′1i<0, we will transform one supporting samples is transformed with a minimal Δα′1i into the remaining set SR, and Δα′1i is obtained with the following equation:
{Δα′1i=α′1i/βi,Δα′1i=−α′1i/βi,whenD⋅βi>0whenD⋅βi<0,(37)
View SourceRight-click on figure for MathML and additional features.where D represents the direction and is decided by the following equation:
D=sign(−h′c).(38)
View SourceRight-click on figure for MathML and additional features.

Calculating all Δα′1i yields the minimal Δα′1i, and the other α′1i and b’ are updated as follows:
⎡⎣⎢⎢⎢⎢⎢b′1α′1s1⋮α′1ss⎤⎦⎥⎥⎥⎥⎥=⎡⎣⎢⎢⎢⎢⎢b′1α′1s1⋮α′1ss⎤⎦⎥⎥⎥⎥⎥+⎡⎣⎢⎢⎢⎢⎢Δb1Δα1s1⋮Δα1ss⎤⎦⎥⎥⎥⎥⎥=⎡⎣⎢⎢⎢⎢⎢b′1α′1s1⋮α′1ss⎤⎦⎥⎥⎥⎥⎥+βΔα1c.(39)
View Source

Once complete, the α′1i of the rest supporting samples will increase, so this operation is repeated until all α′1i∈(0,C1).

Algorithm 1. Online Robust Support Vector Regression (ORSVR)
Read a new sample (Xc,Yc),

Set α1c=0

Compute h′(Xi), i in {s1,s2,…⋅,ss,c}.

if existing h’(Xi)≠ 0:

Adjust α1i through (23) to get the new value α′1i

i in {s1,s2,…⋅,ss,c}

end if

Compute h′(Xc)

if h′(Xc)>0:

Add Xc to the remaining set and Exit

else:

Compute Δh′(Xi). i in {s1,s2,…,ss}

while Xc is not added into a set:

Compute β and γ according to (36) and (39)

Compute the minimal increment Δαmin1c.

Update α′1i, Δh′(Xi),SS,SE and SR.

Update the inverse matrix R.

end while

end if

Algorithm 1 shows that rebuilding the matrix R is very inefficient at each iteration due to the high complexity of the matrix inversion (about O(n2log(n))). To avoid this problem, a further method is needed that is specific to this type of matrix R and reduced the complexity to about O(s2), where s is the number of samples in the supporting set.

The first sample is updated with
R=[−N′N′+1Q′11110](40)
View Sourcewhere N’ = N+1.

The other samples are then added and updated with
Rnew=⎡⎣⎢⎢⎢⎢⎢ 0 −N′N′+1R…00⋮00⎤⎦⎥⎥⎥⎥⎥+1γi[β1][βT1].(41)
View Source

If a new sample is added to the supporting set Ss, γi is defined as:
γi=N′N′+1Q′cc+[1N′N′+1Q′cs1⋯N′N′+1Q′csls]β.(42)
View SourceRight-click on figure for MathML and additional features.

If the new sample is moved from the error set SE or the remaining set SR into the supporting set, β and γi also need to be recomputed as follows:
β=−N′N′+1R⎡⎣⎢⎢⎢⎢⎢1Q′is1⋮Q′isls⎤⎦⎥⎥⎥⎥⎥(43)
View Source
γi=N′N′+1Q′ii+[1N′N′+1Q′is1⋯N′N′+1Q′isls]β,(44)
View SourceRight-click on figure for MathML and additional features.

If the sample is moved from the supporting set Ss to the error SE or remaining set SR, the matrix R is updated as follows:
I=[1⋯(i−1)(i+1)⋯(ls+1)]Rnew=RI,I−RI,iRi,IRi,i,(45)
View SourceRight-click on figure for MathML and additional features.which deletes the row and column of the removed sample and updates the others.

4.3 Complexity
Like every good complexity analysis, we compute the value of the complexity in the worst case (O(f(x))).

We first analyze the time complexity. In Algorithm 1, the time complexity caused by three operation: 1). Step 3, i.e., calculating the margin distance h′(Xi) of each sample. The time complexity of this operation is O(n)O(kernel); 2). Step 5, i.e., initially adjusting the weight of each support vectors. The time complexity of this operation is O(n); 3). From Step 12 to Step 17, i.e., adjusting the weights of each sample O(5n). Therefore, the total time complexity is O(n3)∗O(kernel). As for space complexity, it is very easy to compute in Algorithm 1, because it mainly caused by saving kernel matrix. Therefore, the total space complexity is O(n2).

The complexity of the kernel operations can easily be avoided by saving all the kernel values in a matrix. This reduce time complexity, yet adds to the space complexity the factor O(n2). Although the complexity O(n3) can seem same compared to other online SVR algorithms, but, in practice, this does not happen. In the average case, the algorithm has almost half complexity of other online SVR algorithms, as shown in the experimental section. The speed of learning depends mostly on the number of support vectors, which can influence significantly performances.

SECTION 5Experiments
In this section, we illustrate the effectiveness of our proposed ORSVR through comparing its performance to other online SVR algorithms. The evaluations involved different scenarios using both artificial and real-world datasets. The artificial datasets allowed us to control the relevant parameters and to evaluate empirically the algorithms with specific types of changes. The real-world datasets enabled us to evaluate the merit of the proposed approach in practical scenarios. All experiments were conducted in Python 3.5 on a PC running Windows 7 with an Intel Core i5 processor (2.40 GHz) and 8-GB RAM.

5.1 Artificial Datasets
In our first set of experiments, we compared ORSVR with AONSVR [11] and INVSVR [12] on several artificial datasets. For simplicity, we used the radial basis function (RBF) kernel for all algorithms and set the model parameters C1=C2=C and v1=v2=v for each algorithm to the values listed in Table 1.

TABLE 1 Parameters for Each Compared Incremental SVR
Table 1- 
Parameters for Each Compared Incremental SVR
The first test was to estimate a sinc function, given N examples (Xi,Yi), with Xi drawn uniformly from [−3π, 3π]] and Yi=sinc(Xi)+ei.
sinc(X)={sin(X)/X1ifX≠0ifX=0,(46)
View SourceRight-click on figure for MathML and additional features.where ei is the noise drawn from a uniform distribution on U(−k,k). Here, U(−k,k) represents the uniformly random variable in [−k,k].

Fig. 2 shows the results for the sinc evaluation. The red dots represent the predicted value, and the blue dots represent the true value. When the predicted value is very close to the true value, the blue dot will almost cover the red dot. Therefore, from Fig. 2, we can see the regression model built by AONSVR does not perfectly fit the data, because most of the red dots are not covered. The results with INVSVR were better, but the model still does not perfectly fit the data. By contrast, the regression model built by ORSVR does fit the data well, showing that ORSVR built the model with the highest accuracy.


Fig. 2.
The regression models obtained built by applying different incremental SVR algorithms on the sinc data.

Show All

Fig. 3 shows the results of the same experiment in terms of root-mean-square error (RMSE) [43], training time (in seconds), and the number of support vectors needed to estimate the final regression function for different sample sizes N.

Fig. 3. - 
Comparative results in terms of RMSE, training time, support vectors in sinc dataset.
Fig. 3.
Comparative results in terms of RMSE, training time, support vectors in sinc dataset.

Show All

As shown in Fig. 3a, ORSVR had a smaller RMSE than AONSVR and INVSVR, which shows that ORSVR has better generalization ability. Fig. 4b shows that ORSVR's learning speed was not significantly lower than either AONSVR or INVSVR, but ORSVR was able to include more support vectors (as shown in Fig. 4c). These results shows that although ORSVR has more support vectors, the learning speed of ORSVR not much slower than AONSVR and INVSVR. The reason may be the strategy of converting a single large QPP into two smaller QPPs speeds up the learning process.

Fig. 4. - 
The regression model obtained by applying different incremental SVR algorithms on the sinc dataset with noise.
Fig. 4.
The regression model obtained by applying different incremental SVR algorithms on the sinc dataset with noise.

Show All

To further explore the potential advantages of ORSVR over AONSVR and INVSVR, we compared the regression performance trends on a noisy version of the sinc data and an increased sample size N. The noisy data are randomly sampled from a Uniform distribution Uni(−0.015, 0.015). The noisy sinc data is shown in Fig. 4.

From Fig. 4, we can see that AONSVR and INVSVR suffered serious overfitting problems, and many noisy samples were selected as support vectors, whereas ORSVR was still able to represent the data distribution accurately despite the noise. Further, ORSVR only selected a few samples as support vectors, ignoring a large proportion of the noisy ones. The results demonstrate that ORSVR is not sensitive to the variances brought about by noise. We attribute this to ORSVR's ability to automatically increase the width of the insensitive zone as the amount of noise increases. As such, these results also illustrate one of ORSVR's advantages.

Fig. 5 shows results of this experiment in terms of root-mean-square error (RMSE), training time (in seconds), and the number of support vectors required to estimate the final regression function for different data sizes N.

Fig. 5. - 
Comparative results in terms of RMSE, training time, support vectors in sinc dataset with noise.
Fig. 5.
Comparative results in terms of RMSE, training time, support vectors in sinc dataset with noise.

Show All

As shown in Fig. 5a, ORSVR had a smaller RMSE than AONSVR and INVSVR in this experiment, too, which again shows that ORSVR is more robust to noisy samples. By handling noisy samples effectively with a ‘dynamic‘ insensitive zone, the resulting model was less influenced by the noisy samples. Thus, ORSVR has better generalization ability (i.e., a smaller RMSE). This is because the ORSVR is good at characterizing data distributions, whereas AONSVR and INVSVR had a tendency to over fit the training samples. These algorithms cannot prevent the influence of noisy samples because ɛ-SVR and v-SVR consider the deviations over all training samples to minimize the cost function. Comparing ORSVR with AONSVR and INVSVR with many different settings of data size N, the generalization ability of ORSVR was the best. Fig. 5b highlights that ORSVR learned significantly faster than AONSVR and INVSVR. The strategy of dividing the large QPP into two smaller QPPs is partly responsible, but the sparsity is another contributing factor. ORSVR had the lowest sparsity of the three algorithms. In addition, as shown in Fig. 5c, the number of supporting vectors required to estimate the regression function with AONSVR and INVSVR was almost equal to the number of noisy samples, while the proportion was significantly lower with ORSVR. Hence, ORSVR also showed the best sparsity among all approaches and, as mentioned earlier, the number of support vectors is the main determinant of prediction speed. In summary, we find from the results of this analysis that the ORSVR algorithm provides superior results in the face of noisy data compared while preserving the advantage of faster learning speeds.

5.2 Real-world Datasets
Turning to the real world, we tested ORSVR on nine publicly-available regression datasets. Each dataset represents a different application, with a wide range of data sizes and dimensionalities. We divided the datasets into two groups. The first five being small datasets, and the last four being larger datasets. Table 2 lists the details for each.

TABLE 2 List of Datasets
Table 2- 
List of Datasets
Datasets D1-D5 were sourced from the UCI repository (http://archive.ics.uci.edu/ml/). D6 is available at http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression.html. D7 comes from the Santa Fe Time Series Competition Datasets (www.psych.stanford.edu/∼andreas/Time-Series/SantaFe.html). D8 is a noisy dataset (Friedman, 1991), where the input attributes (x1, …, x10) are generated independently, each of which is uniformly distributed over [0, 1]. The dataset is produced by
y= 10sin(πx1x2)+20(x3−0.5)2+10x4+5x5+δ(0,1),(47)
View Sourcewhere σ(0, 1) is the noise term, which is normally distributed with a mean of 0 and a variance of 1. Note that x1, …, x5 only are used in (34), while x6, …, x10 are noisy irrelevant input attributes. D9 is are available from StaLib.

In next experiments, we used a linear kernel K(x1,x2)=(x1⋅x2), and a Gaussian kernel K(x1, x2) =exp(−∥x1−x2∥2/2σ2) with σ = 0.5 for all experiments. The parameter ɛ, representing the strictness of the restoration adjustments, was fixed at −1. The values for v and C were fixed at 0.1 and 100, respectively, in all experiments.

Fig. 6. compares the regression performance of ORSVR, AONSVR, and INVSVR on the different experimental datasets and different kernels. Over all nine datasets, the Gaussian kernel produced the lowest RMSE with the exception of the Parkinson's dataset (D5). ORSVR and INVSVR performed better than AONSVR on all datasets because both use the parameter v to control the bounds of the proportion of support vectors and errors. The ORSVR algorithm showed markedly better on the Friedman dataset (D8) because it includes noisy data and a heteroscedastic error structure, which offers further support for the advantages of ORSVR over INVSVR. This result also highlights the benefits of allowing an arbitrarily-shaped insensitive zone in ORSVR, rather than the tube shape in v-SVR.

Fig. 6. - 
RMSE obtained by different algorithms in different datasets.
Fig. 6.
RMSE obtained by different algorithms in different datasets.

Show All

Fig. 7 compares the run-time of the three algorithms. In general, as the size of the training samples size N increased, the run-time increased for all three. Further, the Gaussian kernel was faster than the linear kernel and, moreover, the ORSVR's had the fastest training speed over all data sets. These results support the notion that solving two smaller-sized QPP instead of a single larger QPP speeds up the learning process. Like the synthetic experiments, ORSVR required fewer support vectors in proportion to the number of training samples, which further contributed to the faster run-time.


Fig. 7.
Training time obtained by different algorithms in different datasets.

Show All

In many real-world applications, the ability to generalize a model and the speed of learning are important considerations. By successfully combining the advantages of SVR and incremental learning, ORSVR shows promise as an alternative for these situations. It not only has the advantage of faster learning speeds but is also sparser and has a greater capacity for generalization than other incremental SVRs in terms of high prediction speeds and satisfactory test accuracy. In addition, ORSVR has the advantage of handling noisy data, which makes it suitable for real-world regression problems of with data streams.

SECTION 6Conclusion and Further Study
ORSVR is an exact incremental regression algorithm for handling data streams that transforms the classical v-SVR into a dual regression model. ORSVR captures the characteristics of data distributions very well, which makes ORSVR robust to noise. Additionally ORSVR determines the up and down-bound functions by breaking the large QPP associated with SVR into two smaller QPPs and solving each simultaneously. The KKT conditions are met for each new sample and maintained for existing samples, but each bound of the divided QPP is simpler than in classical v-SVR, which results in a faster incremental learning speed. However, the ORSVR requires an additional constraint to be compatible with incremental learning. Therefore, the ORSVR approach incorporates several new methods constitute the incremental learning algorithm for ORSVR. One method introduces a procedure for preparing the initial solution prior to incremental learning. The other is an adjustment step to ensure all the weights of the support vectors are greater than 0. The experimental results demonstrate that ORSVR successfully handles noisy data and is faster than other incremental SVR algorithms.

Theoretically, a decremental learning paradigm for ORSVR could be designed in a similar manner. Further, both the incremental and decremental versions of ORSVR may benefit from leave-one-out cross-validation [36] and/or learning with limited memory [37] in terms of efficiency. However, ORSVR still needs some improvement for use with real-world noisy data streams. From our analysis, we find that variations in the relationship between the dependent and independent variables impacts performance. Addressing this concept drift problem where ft(X)≠ft+1(X) [38], is the next step in our research and should result in an incremental SVR algorithm based on drift learning.

