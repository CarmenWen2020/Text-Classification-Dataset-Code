Density based clustering techniques discover the intrinsic clusters by separating the regions present in the dataset as high- and low-density regions based on their neighborhood information. They are popular and effective because they identify the clusters of arbitrary shapes and automatically detect the number of clusters. However, the distribution patterns of clusters are natural and complex in the datasets generated by different applications. Most of the existing density based clustering algorithms are not suitable to identify the clusters of complex pattern with large variation in density because they use fixed global parameters to compute the density of data points. Minimum spanning tree (MST) of a complete graph easily captures the intrinsic neighborhood information of different characteristic datasets without any user defined parameters. We propose a new Relative Density measure based on MST Neighborhood graph (RDMN) to compute the density of data points. Based on this new density measure, we propose a clustering technique to identify the clusters of complex patterns with varying density. The MST neighborhood graph is partitioned into dense regions based on the density level of data points to retain the shape of clusters. Finally, these regions are merged into actual clusters using MST based clustering technique. To the best of our knowledge, the proposed RDMN is the first MST based density measure for capturing the intrinsic neighborhood without any user defined parameter. Experimental results on synthetic and real datasets demonstrate that the proposed algorithm outperforms other popular clustering techniques in terms of cluster quality, accuracy, and robustness against noise and detecting the outliers.
SECTION 1Introduction
Clustering is a data mining technique of grouping the unlabelled data points into different classes based on their intrinsic characteristics or similarity. Many clustering techniques have been proposed in the literature to effectively cluster the datasets of different characteristics [1], [2], [3], [4], [5], [6], [7], [8].

Though traditional methods like partitional and hierarchical techniques have been used extensively, but they fail to identify the intrinsic clusters in multi-scale datasets, i.e., dataset containing clusters of arbitrary shapes, different sizes and varying densities without any domain knowledge [9], [10], [11]. To improve these limitations, density based clustering techniques were introduced where the clusters are recognized based on the density difference of the regions. They identify the clusters with irregular shapes and sizes and also detect the number of clusters automatically. They can be categorized based on how the density is estimated. Density based spatial clustering of application with noise (DBSCAN) is the most popular clustering technique to detect the clusters based on the density level of points in the neighborhood [3]. Although it identifies the clusters with arbitrary shapes, the clustering result depends on the user defined parameter for computing the points in the local neighborhood. Many improved density based clustering techniques have been proposed in [5], [6], [7]. A novel density based clustering using Fast search-and-find of Density Peak (FDP) was proposed [5] which identifies the cluster centers which are surrounded by lower density points and they are separated from any point of higher density by relatively large distance. Density core based method (Dcore) [6] recognizes the clusters by a smaller number of density cores instead of centroids. Similarly, a density core based clustering algorithm with dynamic scanning radius (DCNaN) [7] improves the existing density estimation by removing the fixed global parameters.

Though density based clustering methods identify the clusters of arbitrary shapes and identify the number of clusters automatically, however they also face number of challenges. First, most of them become ineffective due to their dependency on the user defined parameters such as distance threshold or scanning radius which is required for computing the density of data points. If the dataset contains clusters having varying densities then fixing the scanning radius is a challenge. Also, pre-processing step is required for computing the parameters which increases the computational overhead of density based clustering techniques. Second, most of them fail to detect the local outliers because of the difficulty in selecting the global parameters.

Minimum spanning tree (MST) has been used extensively for collecting the similarity information of data points with their closest neighbors [13], [14], [15], [16]. Therefore, MST based similarity graphs have been used to represent the intrinsic structure of multi-scale dataset. To collect more neighborhood information of data points, similarity graph is computed by merging multiple rounds of MST. Unlike the traditional density based clustering algorithms, the MST based neighborhood graph does not need any global parameters to compute the nearest neighbors of data points. The neighborhood information captured in the similarity graph is a potential candidate for computing the density of data points.

With this motivation, a novel relative density measure based on MST neighborhood (RDMN) is proposed to compute the density of data points for clustering. We compute the density based on the average similarity of data points with its neighbors in the neighborhood graph. The proposed algorithm mainly consists of three steps. In the first step, density (RDMN) of each data points is computed. In the second step, an outlier detection technique is applied on the RDMN to identify the local outliers in the dataset. The dense regions are identified based on the RDMN values which partitions the graph into a disjoint forest. In the last step, we merge the sub-clusters based on the MST neighborhood information. The proposed technique discovers the proper partitions of multi-scale dataset without any user defined parameters.

The experimental analysis performed on synthetic and gene expression datasets illustrates that the proposed technique outperforms the other competing clustering methods in terms of cluster quality, accuracy, robustness against noises and execution time. The experimental analysis also demonstrates that the proposed technique efficiently detects the local outliers in the dataset which improves the cluster quality.

This paper is described as follows. Section 2 illustrates the related work. The proposed relative density measure and our clustering technique is described in Section 3. Section 4 demonstrates the experimental results and analysis. Finally the conclusion of the paper is described in Section 5.

SECTION 2Related Work
Clustering methods are broadly classified into Partitional, Hierarchical and Density-based techniques. Partitioning techniques such as k-means, and k-medoids identify the clusters present in a dataset based on the initial random centroid of clusters [1]. Tough they take linear time but converge of the loss function e.g., sum of the squared error depends on the initial centroids. A distortion function called Bregman divergence was proposed to generalize the loss function of the centroid based clustering techniques like k-means [17]. Hierarchical clustering methods use a similarity matrix to generate a tree structure called dendrogram and it is cut at some appropriate label to find a set of clusters [2]. However, they are computationally expensive and their clustering result is sensitive to the selection of the similarity measure.

Density-based techniques first identify the density distribution of the data points in the data space and then identify the clusters as a region of high density separated by the region of low density [3], [6], [7], [8]. Density based clustering techniques efficiently identify the clusters of arbitrary shape and also automatically detect the number of clusters presents in the dataset. Several density based clustering techniques have been proposed to effectively discover the clusters based on the density variation.

The most popular density based clustering technique is, density based spatial clustering of application with noise (DBSCAN) which effectively detects the clusters of irregular shapes [3]. Density of a point is defined as the number of data points lies within a fixed scanning radius of Ïµ. However, DBSCAN is unsuitable for a dataset with varying densities because the density computation depends on user defined parameters. Subsequently, some improved techniques were introduced to eliminate the limitations of DBSCAN [4], [5], [6], [7], [12], [18], [19], [20], [21] .

OPTICS is another density based clustering algorithm [4] which provides a visual tool to determine the parameters used in DBSCAN as well as to find the structure of clusters. This algorithm identifies the clusters of varying density. Although OPTICS reduces the complexity of parameter estimation, still the obstacle lies in determining the number of scanning radiuses required to find the potential clusters when the dataset is complex [22].

An improved density-based clustering algorithm by finding density peaks (FDP) was proposed by reducing the number of input parameters [5]. In the first phase, it computes the local density of each data points which is defined as the number of data points in its dc neighborhood, and then applies group decision approach to find out the centroid of clusters, called density peaks. In the second phase, remaining data points are assigned to the nearest centroid. FDP effectively identifies the clusters of arbitrary shapes. However, it is challenging to determine the actual number of cluster centers when there are several density peaks exist in a dataset [6], [7].

Density core based method (Dcore) proposed in [6] assumes that each cluster can be represented by a smaller number of density cores instead of centroids and all the outliers, borders and edges lie around the outside of these cores. Each density core consists of a set of points which are loosely connected with relatively high densities in a cluster. Although, Dcore is efficient in identifying the complex patterns, still it requires non-trivial scanning radius and parameters for better results, which is same as selecting dc in FDP. Furthermore, Dcore fails to identify the clusters of extremely large variations in density because it uses fixed global parameters [7].

A density core based clustering algorithm with dynamic scanning radius (DCNaN) was proposed for removing the use of fixed global parameter in existing density based clustering algorithms [7]. It defines the density of a point x as the number of neighbors presents within the average distance with all its natural neighbors. DCNaN still requires a tuning parameter as an input for outlier detection. Also, the complexity of the algorithm increases when it is applied on dense datasets [7].

Another technique to compute the local density using k-nearest neighbors was introduced in [12]. The local density of a data point is defined as the inverse of the average reachability distance from all its k-nearest neighbors. However, it is not efficient to identify the clusters with complex distribution because it requires the parameter k to identify the neighboring data points [22].

Kernel density estimation is a popular alternative to cut-off density estimation techniques. Many kernel based density estimation techniques were proposed [18], [19], [20], [21] for clustering where high density regions are considered as clusters. The DENCLUE clustering algorithm efficiently handles the high dimensional datasets [18]. The density of data points is determined by kernel density using the influence function of neighboring data points [23]. This density measure requires a constant for controlling the scale, influencing the clustering results. This algorithm is less sensitive to outliers [22]. Kernel density function is continuous monotonically decreasing and is less sensitive to parameter choices. However, it may give biased density estimation for the small size clusters, since the density of all points are considered for the estimation [23].

Local density estimation (LDE) has been proposed by Latecki et al., [19] in which variable-width Gaussian kernel density estimation (KDE) along with reachability distance is used to compute the density. Kernel Density Estimation Outlier Score (KDEOS) used kernel density measure and standardized the density using the z-score transformation with respect to the density of the k nearest neighbors [21]. LDE and KDEOS require parameter comparable kernel bandwidth multiplier constant h to tune for optimal result in addition to the neighborhood size k. The impact of choice of k is expected to be different for different applications and methods, e.g., smaller value of k can influence the density estimation and larger values of k can increase the computational complexity of finding the k nearest neighbors. Kernel based approach is also applied in many other areas, mainly in the area of machine learning such as support vector machines (SVM). A clustering method using SVM was introduced in which the data points are mapped by means of gaussian kernel to higher dimensions in which the minimal enclosing sphere is identified [24]. Then this sphere is mapped back to the original space that can be separated into many groups and each group is considered as a cluster. The width of the gaussian kernel and soft margin helps in identifying the outliers and cluster boundary.

SECTION 3Proposed Algorithm
3.1 Relative Density Measure Based on MST Neighborhood
Most of the density measures proposed in the literature consider global density level of the dataset, that are necessary to identify the clusters of different levels of density. In this section, we first describe the limitation of traditional density measures followed by the proposed density measure, RDMN.

Most of the density measure techniques depend on the cut off distance. For example, DBSCAN computes the density based on the number of data points lies within a Ïµ-radius [3]. Its result is sensitive to Ïµ, i.e., a slight change in Ïµ can result in drastic variation in the density computation as shown in Fig. 1a. Number of closest neighbors of a point x is less if we select Ïµ1 as compared to Ïµ2 as a distance threshold. Another traditional density estimation is the kernel density function which is defined as [19], [20], [21] Ïi=âjker(d(i,j)/h), where ker(.) is a monotonically decreasing and continuous function and h is a kernel bandwidth multiplier constant used for scaling [23]. Though, it is less sensitive to the selection of parameter but can give biased estimation for points in small cluster since contribution of all the points even in different density regions of the dataset are considered [23]. Fig. 1b shows the two clusters, one is smaller than the other cluster. As can be seen, the kernel density estimation of the point x depends on the contribution of all other points which are in a different density region. Similarly, another classical density measure defined as : Ïi=âjÏ(d(i,j)âdc) where, Ï(x)=1if(x<0) and dc is also a cut off distance [5]. This computation depends on the value of dc. As depicted in Fig. 1c, the dataset contains two clusters with varying densities. If we select distance threshold dc, the number of nearest neighbors of a point y in low density area is very less. Thus selection of the actual parameters for varying density dataset is a challenging problem.

Fig. 1. - 
Limitation of existing density measures: (a) Sensitivity to $\epsilon$Îµ in DBSCAN. (b) Kernel density estimation. (c) Sensitivity to distance threshold $d_{c}$dc in varying density dataset.
Fig. 1.
Limitation of existing density measures: (a) Sensitivity to Ïµ in DBSCAN. (b) Kernel density estimation. (c) Sensitivity to distance threshold dc in varying density dataset.

Show All

MST of a set of points reflects the similarity of data points with their neighbors [13], [14], [15], [16]. To collect the local neighborhood information of data points more accurately, multiple rounds of MST can be used [13]. This neighborhood information can give an insight on the density of data points. In addition to that, no parameter is required to collect the neighborhood information. Let t be the number of rounds of MSTs. The t-rounds of MST is constructed as follows [13]: Let G=(V,E) be the complete weighted undirected graph of the dataset where vertex set V denotes the points and edge set E is the weighted edges between every pair of points. The weight of an edge (u,v) is the euclidean distance between u and v. The first round of MST of G, say T1 is computed. Then the consecutive MSTs are computed by removing the edges of the MSTs computed in the previous rounds, i.e., MST Ti of the graph G=(V,Eââj=1iâ1Ej)) is constructed where 2â¤iâ¤t and Ej denotes the edges of jth round MST Tj. Then t-round of MST neighborhood graph is defined as Gt=(V,âj=1tEj).

Fig. 2 demonstrates the construction of a 2-round MST neighborhood graph of a dataset. To identify the parameter (number of rounds) t automatically, we use diameter of the graph as a objective function which is defined as the maximum shortest path distance between any two vertices in the graph [13]. The structural similarity between the graphs can be represented by the diameter of graphs [13]. In each iteration, we include the edges of MST. At certain iteration, union of MST edges may not affect the diameter of the neighborhood graph which indicates that the two graphs in the consecutive iterations are structurally similar. With this intuition, we use diameter of two subsequent rounds as the stopping criterion. We empirically observed that, t=3 is sufficient to capture the neighborhood information of data points. Nearest neighbor of a point u is the vertices which are adjacent to u. It is defined as follows:

Fig. 2. - 
MST neighborhood graph: (a) An example dataset (data points=240) with two clusters. (b) First round of MST $T_{1}=(V,\; E_{1})$T1=(V,E1) is computed from complete graph $G=(V,\; E)$G=(V,E). (c) Next round of MST $T_{2}=(V,\; E_{2})$T2=(V,E2) is computed from $G=(V,\; E-E_1)$G=(V,E-E1). (d) MST neighborhood graph is genrated by taking the union of the edges in $T_{1}$T1 and $T_{2}$T2 i.e., $G^{t}=(V, E_{1} \cup E_{2})$Gt=(V,E1âªE2).
Fig. 2.
MST neighborhood graph: (a) An example dataset (data points=240) with two clusters. (b) First round of MST T1=(V,E1) is computed from complete graph G=(V,E). (c) Next round of MST T2=(V,E2) is computed from G=(V,EâE1). (d) MST neighborhood graph is genrated by taking the union of the edges in T1 and T2 i.e., Gt=(V,E1âªE2).

Show All

Definition 1 (Nearest neighbors (NN) of a data point).
The set of nearest neighbor of a data point u, denoted as NN(u), is the set of vertices which are adjacent to u in the MST neighborhood graph (Gt).
NN(u)={vâ£(u,v)is an edge inGt}.(1)
View SourceRight-click on figure for MathML and additional features.

Density of a data point should be positive and also it should be inversely proportional to the total edge weights with its neighbors. With this observations, we define a density measure based on NN as follows:

Definition 2 (Density of a data point).
The density of a data point uâV is defined as:
D(u)=expââââââvâNN(u)d(u,v)ââ /|NN(u)|ââ ,(2)
View SourceRight-click on figure for MathML and additional features.where, |NN(u)| denotes the number of nearest neighbor of a data point u in the neighborhood graph and d(u,v) denotes the edge weight between u and v.

If the average distance of a data point is large, then the point is more dissimilar with its NNs, i.e., the point belongs to a low density region. Similarly, if the average distance is less, the point belongs to a high density region. Density of points acts as an excellent parameter to efficiently distinguish the outliers from the actual data points in a cluster. However, for the clusters of low density, estimation primely based on this parameter might result into mistaking the data points of clusters as an outliers. Here the critical observation is that the density of data points in a low density region is relatively less as compared to density of data points in the high density regions. Therefore the density of data points lie in the vicinity should contribute towards the density computation. The relative density measure based on MST neighborhood is defined as follows:

Definition 3 (Relative Density based on MST neighborhood).
The relative density based on MST neighborhood graph of a data point u, denoted as RDMN(u) is the ratio between the density of u and the minimum density of NNs of u.
RDMN(u)=D(u)min(vâNN(u))D(v),(3)
View SourceRight-click on figure for MathML and additional features.

RDMN measure considers the densities of NNs for computing the density value of a data point which can effectively accentuate both sparse and dense regions, and also can diminish the local outliers as well. As it can be seen clearly, no parameter is required for density computation.

Fig. 3a shows the density and RDMN value of the proposed RDMN density measure on sample dataset (N=11 points with 2 clusters of varying densities and 1 local outlier data point). The value written in blue color indicates the density values whereas the red ones shows RDMN values. The density values of data points in the dense region is high as compared to the points of the low density region because the average MST neighborhood distance of points with their NNs of dense region is less than the low dense region points. The outlier data point has low density value as compared to all other points.

Fig. 3. - 
Illustration of the density and RDMN values of points and identification of the different density regions: (a) The density and RDMN value of each data points are shown, where blue and red colors indicate the density and RDMN values respectively. (b) Every data point is connected with its closest nearest neighbor having higher RDMN value, so two separate regions are identified.
Fig. 3.
Illustration of the density and RDMN values of points and identification of the different density regions: (a) The density and RDMN value of each data points are shown, where blue and red colors indicate the density and RDMN values respectively. (b) Every data point is connected with its closest nearest neighbor having higher RDMN value, so two separate regions are identified.

Show All

3.2 RDMN Based Clustering Algorithm
Now, we are in a state to present the proposed clustering algorithm using RDMN values. In the first step, we detect and remove the outliers. Then the dense regions (sub-clusters) are identified based on RDMN values. In the third step, a MST of the complete graph is constructed considering the centroid of all the dense regions as vertices. In the next step, the inconsistent edges of the MST are removed iteratively to form disjoint components. Finally, the connected components are found out and the sub-clusters having centroids present in each connected components are merged together to form the actual clusters. Each of the steps of the proposed clustering technique is demonstrated in Fig. 4.

Fig. 4. - 
Overview of the proposed algorithm: (a) Construction of the $3-$3- rounds of MST neighborhood graph. (b) Computation of RDMN value of each data points and detection of low density data points as outliers (represented by pink color). (c) The dataset is divided into dense regions (sub-clusters) according to the density value. (d) The MST on the centroids of sub-clusters is computed to identify the similarity between sub-clusters and the maximum standard deviation reduction edge is represented by red color. (e) Removal of the red edge from MST to partition it into two sub-clusters. (f) Merging of the data points of the connected sub-clusters into actual clusters.
Fig. 4.
Overview of the proposed algorithm: (a) Construction of the 3â rounds of MST neighborhood graph. (b) Computation of RDMN value of each data points and detection of low density data points as outliers (represented by pink color). (c) The dataset is divided into dense regions (sub-clusters) according to the density value. (d) The MST on the centroids of sub-clusters is computed to identify the similarity between sub-clusters and the maximum standard deviation reduction edge is represented by red color. (e) Removal of the red edge from MST to partition it into two sub-clusters. (f) Merging of the data points of the connected sub-clusters into actual clusters.

Show All

3.2.1 Outliers Detection Technique
An outlier is an observation that is far away and divergent from other data points. Outliers can affect the performance of any clustering method. Therefore, proper detection of the outliers is an important step in recognizing the clusters of complex patterns accurately. There are several outlier detection techniques available in density based clustering algorithms, however they identify the extreme outliers only and their performance depend on the user defined parameters [25]. To overcome these limitations, we propose an outlier detection technique based on box-plot method using RDMN values [26], [27].

The box-plot method is an approach of displaying the distribution of data points based on the five number summary, namely minimum, first quartile (Q1), median (Q2), third quartile (Q3), and maximum, as shown in Fig. 5 [26], [27]. Here, Box-plot method is applied on RDMN values to identify the outlier data points. First, RDMN values are sorted in ascending order, and then we get a threshold Î± based on the first and interquartile range values. The data points whose RDMN values are less than Î± are considered as outliers. The threshold value Î± is defined as: Î±=Q1â1.5â(IQR), where Q1 is the value of the (1/4)th quartile and IQR is the value difference between (3/4)th and (1/4)th quartile values [26], [27], [28]. The low density value data points are considered as outliers, which slightly fluctuate around the Q1, and the width of (3/4)th and (1/4)th quartile values is far less than the smooth lines. Fig. 6a displays the density level of data points and the threshold value (Î±) of a dataset. The local outliers are marked by circles in Fig. 6b.

Fig. 5. - 
Illustration of the box-plot method for detecting the outliers.
Fig. 5.
Illustration of the box-plot method for detecting the outliers.

Show All

Fig. 6. - 
Illustration of outlier detection technique: (a) The density value representation of each data points on R15 dataset ($N=600$N=600 data points with 15 clusters). (b) The local outliers based on the RDMN value using box-plot method is represented by circle on R15 dataset.
Fig. 6.
Illustration of outlier detection technique: (a) The density value representation of each data points on R15 dataset (N=600 data points with 15 clusters). (b) The local outliers based on the RDMN value using box-plot method is represented by circle on R15 dataset.

Show All

3.2.2 Finding the Dense Regions
We identify the dense regions or sub-graphs in the dataset based on the RDMN values. A group of data points is considered as a dense region if their RDMN values are closer to each other. Two regions are separated based on the variation in RDMN values. To identify the similar density data points in a region, we connect a point u with its closest nearest neighbor v of higher density [28]. Based on this criterion, the data points in a dense region are connected to each other because of similarity in their density. Any boundary data point of a region is likely to be connected with the boundary data points of another region in the neighborhood graph due to which it must have lower density as compared to all other points in its region. Therefore, it is connected with the higher density data points in its own region. Hence, the boundary points of two different regions are disconnected.

We construct a graph Gp=(Vp,Ep) in which vertices Vp are data points excluding the outliers, and each data point uâVp is connected with its closest NN (vâNN(u)) of higher density. The connection between data points is defined as follows.

Definition 4 (Connectivity of a data point).
The connectivity of uâVp, denoted as Ep(u), is defined as follows:
Ep(u)={v|vis the closest neighbor ofuandRDMN(v)>RDMN(u)}.
View SourceRight-click on figure for MathML and additional features.

Each connected components of the graph Gp is considered as a separate region (sub-cluster). Fig. 3 demonstrates that the dense regions is identified based on the RDMN values. The dataset contains two separate density regions with one local outlier as shown in Fig. 3a. The proposed approach connects the points with their closest neighbor of higher RDMN value. Each region has similar density points as shown in Fig. 3b. The process of finding the dense regions of the proposed technique is illustrated in algorithm 2.

Algorithm 1. Computation of the Relative Density Based on MST Neighborhood (Density-Computation)
Input: Dataset X;

Output: List of Nearest neighbors and RDMN of all data points;

Initially, set the nearest neighbor list NN=â, and relative density RDMN =â;

Construct the complete graph G=(V,E), where V=X and E={d(vi,vj)|âvi,vjâVandiâ j}, where d(vi,vj) denote the Euclidian distance between vi and vj;

Compute a MST T1=(V,E1) of G and set the diameter dia1=Diameter(T1);

Set i=2 and change in diameter Îdia=â;

While (Îdiaâ 0) do

Compute MST Ti of G=(V,Eââj=1iâ1Ej);

Compute the iâround of MST neighborhood graph, Gi=(V,âj=1iEj);

Compute the diameter diai of the graph Gi;

Compute Îdia=diaiâ1âdiai;

i=i+1;

End While

For each uâV do

NN(u)={vâ£(u,v)is an edge inGi};

D(u)=exp(â(âvâNN(u)d(u,v))/|NN(u)|);

End For

For each uâV do

RDMN(u)=D(u)min(vâNN(u))D(v);

End For

Return NN and RDMN

Algorithm 2. Finding the Dense Regions Based on RDMN (Partitioning)
Input: Relative density (RDMN) and Nearest neighbor (NN);

Output: Sub-clusters C={C1,C2,.....,Ckâ²};

Initially, set Outliers = â;

Compute the threshold Î±

For each uâV do

If(RDMN(u)<Î±) Outliers=Outliersâª{u};

end For

Let Gp=(Vp,Ep), where, Vp=VâOutliers and Ep=â; // Ep denotes the connectivity of data points with their closest NN of higher density

For each uâVp do
Ep(u)={v|vis the closest neighbor ofuandRDMN(v)>RDMN(u)}
View SourceRight-click on figure for MathML and additional features.

end For

Find all the connected components C={C1,C2â¦,Ckâ²} of Gp using Breadth first search (BFS) algorithm // Each component of C is a dense region (sub-cluster).

Return C

Algorithm 
3.2.3 Merging of the Dense Regions Into Actual Clusters
After partitioning the dataset into dense regions or sub-trees (sub-clusters), merging technique is applied to identify the actual clusters. In this phase, we first compute the centroid of each sub-clusters, and consider them as the representative point of each dense regions. Let C={C1,C2,â¦,Ckâ²} is the set of sub-clusters and SÎ¼={Î¼(C1),Î¼(C2),â¦,Î¼(Ckâ²)} is the set of centroids. Then a complete graph on these centroids is constructed by considering the centroids as vertices and distance between them as edge weights, let it be denoted as GÎ¼=(VÎ¼,EÎ¼). Then we compute MST, TÎ¼, of the graph GÎ¼. Removing an MST edge can partition the tree into two disjoint trees where each tree may contain several sub-clusters. We adopt the technique proposed in [14] for removing an MST edge which reduces the maximum overall standard deviation. This edge removing process is applied iteratively to form disjoint sub-trees, each tree containing several sub-clusters until difference of standard deviation between two consecutive iterations is within a threshold. This process also identifies the actual number of clusters automatically by computing the local minima of standard deviation reduction function.

The MST TÎ¼ is partitioned into set of disjoint sub-trees denoted as Sk={TÎ¼1,TÎ¼2,â¦,TÎ¼k} such that the following conditions are satisfied [14]:
â§â©â¨âªâªLetSkdenotes the set which maximisesÏ(TÎ¼)âÏ1(Sk)|ÎÏ1(Sk)âÎÏ1(Sâ²k)|â¤(0.001.(ÎÏ1(Sk)+1)),
View SourceRight-click on figure for MathML and additional features.where Ï(TÎ¼) denotes the standard deviation of edges in TÎ¼ and Ï1(Sk) denotes the weighted average of the standard deviation of the edges of the disjoint trees TÎ¼jâSk, defined as [14]:
Ï1(Sk)=ââTÎ¼jâSk|TÎ¼j|.(Ï(TÎ¼j))ââTÎ¼jâSk|TÎ¼j|.(4)
View SourceRight-click on figure for MathML and additional features.Here ÎÏ1(Sk) and ÎÏ1(Sâ²k) illustrate the maximum standard deviation reduction that prompts the partition Sk={TÎ¼1,TÎ¼2,â¦,TÎ¼k} and the immediate precedent of Sk, i.e., Sâ²k={TÎ¼1,â¦,TÎ¼kâ1} respectively. The expected number of clusters is identified by applying the polynomial regression on the (ÎÏ1(Sk))i which is computed in each iteration i. The number of clusters is computed by computing the local minima of the regression function. The sub-clusters represented by vertices in each disjoint trees, are the members of a particular cluster. Therefore, the data points of each sub-clusters within a tree are merged into actual clusters. The proposed technique is illustrated in algorithm 3.

Fig. 7 demonstrates the merging process of the proposed technique. The sample dataset (N=300 data points with 3 clusters) is demonstrated in Fig. 7a. The partition of dataset (dense regions) is shown in Fig. 7b. Each partition of the dataset is represented by the centroid of data points and a MST TÎ¼ is constructed on these centroids as shown in Fig. 7c. Next, we select an edge (red color edge as shown in Fig. 7c) which reduces the maximum standard deviation. By removing this edge, TÎ¼ is partitioned into two disjoint subtrees TÎ¼1 and TÎ¼2, i.e., Sk={TÎ¼1,TÎ¼2} as shown in Fig. 7d. Further, the disjoint subtrees of Sk is partitioned into more disjoint trees (sub-clusters) until the standard deviation of two consecutive iterations is within a threshold as shown in Fig. 7e. Finally, Fig. 7f illustrates the actual clusters with outliers after merging the data points of sub-clusters in each disjoint trees.

Fig. 7. - 
Illustration of the merging process: (a) An example dataset (N=300 data points with 3 clusters). (b) Partition of the points into dense regions (sub-clusters) based on the RDMN values. (c) Each sub-clusters is represented by its centroid and we construct a MST $T_{\mu }$TÎ¼ on these centroids. (d) Removing the edge of maximum standard deviation reduction partitions $T_{\mu }$TÎ¼ into two sub-clusters, namely $T_{\mu _1}$TÎ¼1 and $T_{\mu _2}$TÎ¼2. (e) Again, removing the edge forms three sub-clusters. (f) Final clusters are obtained by merging all the data points of connected sub-clusters in each $T_{\mu _j}$TÎ¼j, where $j=1,2,3$j=1,2,3 and outliers are marked as circles.
Fig. 7.
Illustration of the merging process: (a) An example dataset (N=300 data points with 3 clusters). (b) Partition of the points into dense regions (sub-clusters) based on the RDMN values. (c) Each sub-clusters is represented by its centroid and we construct a MST TÎ¼ on these centroids. (d) Removing the edge of maximum standard deviation reduction partitions TÎ¼ into two sub-clusters, namely TÎ¼1 and TÎ¼2. (e) Again, removing the edge forms three sub-clusters. (f) Final clusters are obtained by merging all the data points of connected sub-clusters in each TÎ¼j, where j=1,2,3 and outliers are marked as circles.

Show All

Algorithm 3. A Relative Density Measure Based Clustering Algorithm
Input: Dataset X;

Output: Cluster label CL;

Initially, set cluster level CL = â;

Compute the NNs and RDMN values of data points using Algorithm 1;

Compute Outliers and set of sub-clusters (C) using Algorithm 2;

Compute the set of centroids of each sub-clusters denoted as Î¼(Ci);

Construct a complete graph GÎ¼=(VÎ¼,EÎ¼) where centroids are the vertices;

Compute an MST (TÎ¼) of the complete graph GÎ¼;

Delete the edges from TÎ¼ to partition it into kâ sub-tress, denoted as Sk={TÎ¼1,TÎ¼2,â¦,TÎ¼k};

For each non outlier data point ui

If ui is in sub-cluster Cl with centroid Î¼(Cl) and Î¼(Cl) is a vertex in the tree TÎ¼j then mark CL(ui)=j;

For each (uâOutliers), mark CL(u)=â1;

Return CL

3.2.4 Complexity Analysis
The overall time complexity of the proposed technique depends on the total time it requires to compute the NNs of points, to find the dense regions, and to merge the dense regions into actual clusters. Time required to compute the NNs of the data points depends upon the time to construct the neighborhood graph. In the worst case, the running time required to construct the neighborhood graph using MST is O(N2). Computing the RDMN values of the data points depends on the number of NN of each points. Since each data point has a constant number of NNs, therefore, this step takes O(N) time. Similarly, identification of the dense regions depends on the number of NN of each data points. In the worst case, it takes O(N) time.

Next, the time required for merging approach depends upon the number of dense regions (sub-clusters). To merge the kâ² number of dense regions into k clusters, O(kâ²)2 time is required where (kâ²) is very less as compared to the number of data points. Hence, the total complexity of the proposed clustering technique is O(N2), which is dominated by the time required for constructing the graph. If we exclude the time of graph computation phase, then our proposed algorithm takes only O(N) time.

SECTION 4Experimental Results and Analysis
In this section, the performance of the proposed technique is evaluated on synthetic, real and gene expressions datasets with respect to five other competing clustering methods such as k-means [1], Hierarchical (average linkage) [2], DBSCAN [3], FDP [5], and DCNaN [7]. Our implementations are performed utilizing MATLAB R2013a computing framework on the Desktop running Windows 64bit operating system with Intel Xeon 2.1 GHz processors and 128 GB of RAM.

4.1 Datasets
4.1.1 Synthetic Datasets
Eight synthetic datasets of different characteristics are used to compare the performance of the proposed technique with other competing clustering algorithms. DS2, DS4, and DS5 datasets are available at [29] and DS8 dataset is taken from [30]. Fig. 8 illustrates the datasets with distinct characteristics of clusters that differ based on geometry, dispersion and density level. Table 1 illustrates the detailed description about these datasets.

TABLE 1 The Description of Synthetic Datasets: Number of Data Points (N)(N), Dimension of Dataset (d)(d), and Number of Clusters (k)(k)
Table 1- 
The Description of Synthetic Datasets: Number of Data Points $(N)$(N), Dimension of Dataset $(d)$(d), and Number of Clusters $(k)$(k)
Fig. 8. - 
Description of the datasets: (a) Gaussian distributed (b) Aggregate clusters (c) Varying density (d) Flame (e) R15 (f) Fullmoon (g) Two Half kernel with some added points (h) Mixed dataset with nine clusters.
Fig. 8.
Description of the datasets: (a) Gaussian distributed (b) Aggregate clusters (c) Varying density (d) Flame (e) R15 (f) Fullmoon (g) Two Half kernel with some added points (h) Mixed dataset with nine clusters.

Show All

4.1.2 Real Datasets
The performance of the proposed algorithm is also evaluated on thirteen real datasets (five real + eight gene expression datasets). The detail description of these datasets are given in Table 2. These datasets are taken from various sources [30], [31], [32], [33].

TABLE 2 The Description of Real Datasets: Number of Data Points (N)(N), Dimension (d)(d), Number of Clusters (k)(k)

4.2 Evaluation Measures
Cluster validity measure is used for evaluating the degree of agreement between partitions produced by the clustering algorithm and ground truth partitions. There are two types of cluster validity measures such as internal and external measures. An external validity measure evaluates the performance of clustering based on the predicted class label by true class label, whereas, an internal validity, measures the properties of clusters such as closeness and degree of separateness. External validity measures such as Rand Index (RI) and Adjusted Rand Index (ARI) are used to evaluate the performance of clustering techniques [34]. Rand index is defined as the total number of data points correctly identified over the total pairs [35]. Rand index value of 1 demonstrates appropriate clustering whereas 0 demonstrates inappropriate clustering. ARI is an improved version of the Rand Index [35]. The ARI value varies between -1 and 1, where -1 and 1 illustrate the improper and proper clustering respectively.

4.3 Experimental Results on Synthetic Datasets
In this section, we demonstrate the experimental results on eight synthetic datasets and compare the performance of the proposed technique with other competing algorithms such as k-means, Hierarchical, DBSCAN, FDP, and DCNaN. The clustering results of six algorithms on synthetic datasets are illustrated in Figs. 9, 10, 11, 12, 13, 14, 15, and 16.

Fig. 9. - 
The clustering results of algorithms in DS1: (a) k-means, (b) Hierarchical, (c) DBSCAN, (d) FDP (e) DCNaN, (f) Proposed.
Fig. 9.
The clustering results of algorithms in DS1: (a) k-means, (b) Hierarchical, (c) DBSCAN, (d) FDP (e) DCNaN, (f) Proposed.

Show All

Fig. 10. - 
The clustering results of algorithms in DS2: (a) k-means, (b) Hierarchical, (c) DBSCAN, (d) FDP (e) DCNaN, (f) Proposed.
Fig. 10.
The clustering results of algorithms in DS2: (a) k-means, (b) Hierarchical, (c) DBSCAN, (d) FDP (e) DCNaN, (f) Proposed.

Show All

Fig. 11. - 
The clustering results of algorithms in DS3: (a) k-means, (b) Hierarchical, (c) DBSCAN, (d) FDP (e) DCNaN, (f) Proposed.
Fig. 11.
The clustering results of algorithms in DS3: (a) k-means, (b) Hierarchical, (c) DBSCAN, (d) FDP (e) DCNaN, (f) Proposed.

Show All

Fig. 12. - 
The clustering results of algorithms in DS4: (a) k-means, (b) Hierarchical, (c) DBSCAN, (d) FDP (e) DCNaN, (f) Proposed.
Fig. 12.
The clustering results of algorithms in DS4: (a) k-means, (b) Hierarchical, (c) DBSCAN, (d) FDP (e) DCNaN, (f) Proposed.

Show All

Fig. 13. - 
The clustering results of algorithms in DS5: (a) k-means, (b) Hierarchical, (c) DBSCAN, (d) FDP (e) DCNaN, (f) Proposed.
Fig. 13.
The clustering results of algorithms in DS5: (a) k-means, (b) Hierarchical, (c) DBSCAN, (d) FDP (e) DCNaN, (f) Proposed.

Show All


Fig. 14.
The clustering results of algorithms in DS6: (a) k-means, (b) Hierarchical, (c) DBSCAN, (d) FDP (e) DCNaN, (f) Proposed.

Show All

Fig. 15. - 
The clustering results of algorithms in DS7: (a) k-means, (b) Hierarchical, (c) DBSCAN, (d) FDP (e) DCNaN, (f) Proposed.
Fig. 15.
The clustering results of algorithms in DS7: (a) k-means, (b) Hierarchical, (c) DBSCAN, (d) FDP (e) DCNaN, (f) Proposed.

Show All

Fig. 16. - 
The clustering results of algorithms in DS8: (a) k-means, (b) Hierarchical, (c) DBSCAN, (d) FDP (e) DCNaN, (f) Proposed.
Fig. 16.
The clustering results of algorithms in DS8: (a) k-means, (b) Hierarchical, (c) DBSCAN, (d) FDP (e) DCNaN, (f) Proposed.

Show All

Table 3 illustrates the value of parameters considered for implementation of competing clustering techniques on eight synthetic datasets. In k-means, the k is set as the actual number of clusters and the initial cluster centers are generated randomly. DBSCAN requires two parameters: Ïµ and Minpts. FDP depends on the distance threshold dc and DCNaN on Ï to identify the outliers. These parameters are selected based on the best clustering quality results achieved by the respective algorithms. The proposed technique is not included in the Table 3 because our algorithm does not require any user defined parameter.

TABLE 3 The Best Possible Value of the Parameters of Competing Clustering Techniques in Synthetic Datasets
Table 3- 
The Best Possible Value of the Parameters of Competing Clustering Techniques in Synthetic Datasets
Fig. 9 demonstrates the results of different clustering techniques on gaussian distributed dataset. The proposed algorithm, hierarchical, DCNaN and FDP only identify the expected clusters but the proposed method also detect the local outliers present in the dataset. DBSCAN aggregates the two clusters due to the presence of local outliers. Due to its dependency on selection of initial centroids, k-means fails to identify the proper clusters. Hierarchical clustering also fails to detect the outliers and it considers each of them as a cluster.

The clustering results of aggregate dataset is shown in Fig. 10. DBSCAN does not identify the expected clusters, the aggregated clusters are merged into a single cluster. Proposed algorithm, hierarchical, FDP and DCNaN recognize the proper clusters, but the clustering result of FDP depends on the parameter value dc.

The clustering results of varying density datasets are illustrated in Fig. 11. FDP, DCNaN and the proposed algorithm recognize the desired clusters. k-means, hierarchical, and DBSCAN fail to identify the proper clusters. In fact, FDP and DCNaN work better in single density variation environment than the multi-density variation environments. Because of varying density, DBSCAN fails to identify the actual clusters.

Experimental results of flame dataset are shown in Fig. 12. Only proposed algorithm identifies the actual clusters. FDP does not identify the expected clusters because it selects the improper density peak points. k-means, hierarchical and DCNaN do not discover the expected clusters because the dataset contains a spherical and a half ring clusters with varying dispersion levels.

The clustering results of DS5 are illustrated in Fig. 13. This dataset contains fifteen gaussian distributed clusters that are arranged in two concentric circles. DBSCAN, hierarchical, FDP and proposed algorithm produce the proper partitions whereas k-means and DCNaN do not. Proposed algorithm also identifies the local outliers.

DS6 dataset consists of a half ring and spherical clusters. Fig. 14 illustrates the clustering result of different clustering methods on DS6 dataset. Proposed technique, DCNaN and DBSCAN discover the actual partitions. k-means, hierarchical and FDP failed on this.

The clustering results of two-half kernel dataset with some added data points are demonstrated in Fig. 15. The proposed algorithm, DCNaN and DBSCAN recognize the expected clusters, but DBSCAN and DCNaN do not identify all outliers. Therefore, proposed algorithm shows better performance over DBSCAN and DCNaN on this dataset. k-means, hierarchical and FDP fail to recognize the expected clusters because of complex pattern of clusters with outliers.

Experiment results of DS8 dataset are illustrated in Fig. 16. DS8 is composed of nine clusters of arbitrary shapes and sizes with vertical strip of varying density. In fact, the two half moon and a cluster inside other cluster with varying density make detection of clusters more challenging. As shown in Fig. 16f, proposed algorithm recognizes the expected nine clusters. DBSCAN detects the two right most clusters but fails to recognize the cluster inside cluster and two rectangle clusters. DBSCAN, hierarchical, FDP and DCNaN fail to recognize the expected clusters because of complex patterns with varying density. Proposed algorithm applies MST instead of manual parameters to compute the nearest neighbors of data points for density computation that can easily distinguish from varying density and retain the shape of different complex patterns.

From Figs. 9, 10, 11, 12, 13, 14, 15, and 16, we can see that the proposed algorithm performs better than the other competing clustering algorithms. Proposed algorithm also automatically detects the local outliers through level partitioning of relative density (RDMN). ARI and RI values of synthetic datasets are illustrated in Tables 4 and 5 respectively. It can be examined from the experimental results that the proposed technique shows better performance over all competent methods on synthetic datasets.

TABLE 4 Comparison of Different Algorithms According to ARI on Synthetic Datasets
Table 4- 
Comparison of Different Algorithms According to ARI on Synthetic Datasets
TABLE 5 Comparison of Different Algorithms According to RI oN Synthetic Datasets
Table 5- 
Comparison of Different Algorithms According to RI oN Synthetic Datasets
4.4 Experimental Results on Real Datasets
In this section, we evaluate the performance of k-means, hierarchical, DBSCAN, FDP, DCNaN, and the proposed algorithm in terms of RI and ARI on thirteen real datasets (five real + eight gene expression datasets). Table 6 shows the values of the parameters of each clustering algorithm. In Tables 7 and 8, the RI and ARI values of real datasets are given respectively which demonstrate that the proposed technique performs better than k-means, hierarchical, DBSCAN, FDP, and DCNaN except on DLBCLA, Lukemia and Biotrain datasets. k-means achieves better result in DLBCLA and Lukemia datasets. Performance of FDP is better on Biotrain dataset but the result is parameter sensitive. Furthermore, parameters in DBSCAN, FDP and DCNaN need to be chosen properly. The performances of average linkage and FDP are relatively stable but the values of RI and ARI are generally low. From the above discussion, we can conclude that the proposed technique achieves better performance as compared to all other competing clustering techniques on almost all real datasets.

TABLE 6 The Values of the Parameters of Clustering Techniques in the Real Datasets
Table 6- 
The Values of the Parameters of Clustering Techniques in the Real Datasets
TABLE 7 Comparison of Different Techniques in Terms of RI Values on Real Datasets
Table 7- 
Comparison of Different Techniques in Terms of RI Values on Real Datasets
TABLE 8 Comparison of Different Algorithms in Terms of ARI Values on Real Datasets
Table 8- 
Comparison of Different Algorithms in Terms of ARI Values on Real Datasets
4.5 Robustness Against Noise
The experiments on two synthetic such as two half kernel and aggregate datasets with distinct levels of noise is performed to evaluate the robustness of the proposed algorithm against noise. Fig. 17 demonstrates the addition of distinct levels of noise with the original datasets. The levels âLowâ, âMediumâ and âHighâ denote 25, 50 and 75 percent noises are added to the initial datasets respectively. To add noise in the dataset, we select x% data points randomly from the dataset and apply Gaussian noise with mean 0 and variance Ï2=1 on these points. We run this step 10 times for each dataset and calculate the average ARI score. Experimental results demonstrate that the proposed algorithm achieves the significant improvement over competing clustering methods such as k-means, hierarchical, DBSCAN, FDP, and DCNaN against different noise levels as shown in Fig. 18. The cluster quality of the proposed algorithm is not compromised with increase of the noise level whereas the cluster quality of other competing clustering techniques is degraded. The proposed approach uses MST to compute the NNs of data points, which distinguishes the noise. Thus the result of our algorithm is not affected by the presence of noise in the dataset.

Fig. 17. - 
Synthetic dataset with various levels of noise. First row corresponds to 2- half kernel dataset and second row corresponds to aggregate dataset. Columns correspond to low, medium, and high levels of noise.
Fig. 17.
Synthetic dataset with various levels of noise. First row corresponds to 2- half kernel dataset and second row corresponds to aggregate dataset. Columns correspond to low, medium, and high levels of noise.

Show All

Fig. 18. - 
Comparison of different algorithms with respect to robustness against noise. $x$x-axis and $y$y-axis denote the dataset with different levels of noise and ARI value respectively.
Fig. 18.
Comparison of different algorithms with respect to robustness against noise. x-axis and y-axis denote the dataset with different levels of noise and ARI value respectively.

Show All

4.6 Effectiveness of the Proposed Outlier Detection Technique
In this section, we also compare the proposed outlier detection technique with two other outlier detection methods of DBSCAN and DCNaN. Fig. 19 shows the clustering results with added outliers on the half kernels dataset. The clustering results of DBSCAN (Ïµ=0.9653,Minpts=2) is shown in Fig. 19a which fails to recognize the actual clusters because of the presence of some local outliers that connect the two clusters. Fig. 19b illustrates the clustering results of DCNaN algorithm which identifies the expected clusters as well as the global outliers but fails to detect the local outliers. The proposed algorithm identifies the actual clusters and also detects the global as well as local outliers as shown in Fig. 19c. After removing these outliers identified by the proposed algorithm, we run DBSCAN and it identifies the actual clusters with same parameters as shown in Fig. 19d.

Fig. 19. - 
The illustration of local outliers on sample dataset ($N=1026, d=2, k=2$N=1026,d=2,k=2): (a) The clustering result (outliers are represented by pink color) of DBSCAN on half kernel dataset with added outliers with value of the parameters $(\epsilon = 0.9653,\; Minpts=2)$(Îµ=0.9653,Minpts=2). (b) The outliers detected by the DCNaN. (c) The outliers detected by the proposed outlier detection technique. (d) The clustering results of DBSCAN with same parameter values after removing the outliers detected by the proposed approach.
Fig. 19.
The illustration of local outliers on sample dataset (N=1026,d=2,k=2): (a) The clustering result (outliers are represented by pink color) of DBSCAN on half kernel dataset with added outliers with value of the parameters (Ïµ=0.9653,Minpts=2). (b) The outliers detected by the DCNaN. (c) The outliers detected by the proposed outlier detection technique. (d) The clustering results of DBSCAN with same parameter values after removing the outliers detected by the proposed approach.

Show All

We also apply it on more synthetic and real datasets. The real outlier dataset are taken from [36], [37]. The result is shown in the Table 9. The proposed approach efficiently detects the outliers in all these datasets.

TABLE 9 The Percentage of Outliers Detected by the Proposed Technique (rows) on Synthetic and Real Datasets (columns)
Table 9- 
The Percentage of Outliers Detected by the Proposed Technique (rows) on Synthetic and Real Datasets (columns)
4.7 Estimation of Number of Clusters
In this section, we evaluate the performance of the proposed technique on the grounds of the optimal number of clusters present in the dataset. The following criteria are used to measure the performance [38]:

Number of hits: It is defined as the number of times a clustering technique predicts the actual number of clusters when it is applied over D number of datasets.

Average error: It is the average difference between the number of clusters that are predicted by the technique and the number of clusters that actually exists in the given dataset. The average error over D number of datasets is defined as:
avgerage error=âi=1|D||actualâpredicted|/|D|.(5)
View SourceRight-click on figure for MathML and additional features.

Results of the number of hits and average error of the proposed algorithm on gene expression dataset is given in Table 10. The proposed technique also detects the actual number of cluster on different characteristic synthetic datasets as shown in Figs. 9, 10, 11, 12, 13, 14, 15, and 16. Experimental result demonstrates that our approach achieves satisfying results in terms of the number of clusters recognized and the average error on synthetic and gene expression datasets.

TABLE 10 The Number of Clusters Detected by the Proposed Technique (rows) on Gene Expression Datasets (columns)
Table 10- 
The Number of Clusters Detected by the Proposed Technique (rows) on Gene Expression Datasets (columns)
4.8 Execution Time
In this section, we analyse the execution time (in seconds) of the proposed work. The execution time of various methods on the large size real datasets (DR9 to DR13) (as described in Section 4.1.2) is shown in Fig. 20. We consider two cases to compare the execution time of the proposed technique, one with the similarity graph construction phase and other without considering that. The results illustrate that in practical applications the running time is acceptable.

Fig. 20. - 
Comparison of execution time (in seconds) of different density based clustering techniques on large size real datasets.
Fig. 20.
Comparison of execution time (in seconds) of different density based clustering techniques on large size real datasets.

Show All

4.9 Cluster Analyses With Varying Dimension Datasets
We also compare the proposed algorithm with the popular clustering methods by varying the dimension of the dataset. We consider Gaussian clusters with varying dimensions (G2 set with 20 percent overlapping of clusters (N=2048,k=2)) and DIM-set datasets (N=1024,k=16) from [29]. The dimension is varied from 32 to 1024. The clustering results of different methods are described in Table 11. The results illustrate that the proposed approach preserves the quality of the cluster even when the dimension of the dataset is varied.

TABLE 11 Comparison of Different Methods on Varying Dimension (Dim) Datasets in Terms of ARI Values
Table 11- 
Comparison of Different Methods on Varying Dimension (Dim) Datasets in Terms of ARI Values
SECTION 5Conclusion and Future Work
In this paper, we proposed a relative density measure based clustering technique using minimum spanning tree. The core part of the proposed algorithm is devising a new relative density measure based on MST neighborhood graph without using any user defined parameter. An outlier detection technique using box-plot on values of RDMN was also introduced to detect the local outliers. We demonstrated the performance through experiments on both synthetic and real datasets that the proposed technique has satisfying performance in terms of cluster quality, accuracy, execution time, robustness against noise and detecting the local outliers. In future, we would like to explore the application of the proposed density measure in other real life scientific applications such as image segmentation, network intrusion detection, social network analysis etc.