Linking authors of short-text contents has important usages in many applications, including Named Entity Recognition (NER) and human community detection. However, certain challenges lie ahead. First, the input short-text contents are noisy, ambiguous, and do not follow the grammatical rules. Second, traditional text mining methods fail to effectively extract concepts through words and phrases. Third, the textual contents are temporally skewed, which can affect the semantic understanding by multiple time facets. Finally, using knowledge-bases can make the results biased to the content of the external database and deviate the meaning from the input short text corpus. To overcome these challenges, we devise a neural network-based temporal-textual framework that generates the subgraphs with highly correlated authors from short-text contents. Our approach, on the one hand, computes the relevance score (edge weight) between the authors through considering a portmanteau of contents and concepts, and on the other hand, employs a stack-wise graph cutting algorithm to extract the communities of the related authors. Experimental results show that compared to other knowledge-centered competitors, our multi-aspect vector space model can achieve a higher performance in linking short-text authors. In addition, given the author linking task, the more comprehensive the dataset is, the higher the significance of the extracted concepts will be.
SECTION 1Introduction
Generating the subgraphs with similar vertices finds important applications in numerous domains: (i) In recommender systems [1], [2], the members in a subgraph can enrich the history of other cold-start members [3], [4], [5]. (ii) In community detection, the subgraphs can identify groups of correlated users [6], [7]. (iii) In propagation networks [6], [8], the group-based immunization policies [9], [10] can better control the burst of contagions (i.e., gossips). Nowadays, the social networks record the commonly brief textual contents of the authors. Given a graph G of short-text authors and the query author nq, our aim is to find a subgraph g~q comprising of highly similar authors to nq. Compared to the comprehensive formal documents, individuals usually tend more to compose informal short-text contents, that are generated with a high throughput rate. While the short-text daily contents can better reveal the genuine similarity between social network users, we can utilize such rich contents to exploit real-world social communities. The NP-hard subgraph mining problem can be initiated by the computation of similarity weights between authors [11], [12] and completed by a stack-wise graph-cutting algorithm. However, some challenges abound:

Challenge 1 (Mismatched Author Contents)

Short-text contents include abbreviations, misspellings, and possible errors. For instance, “afternoon” is informally used as “arvo”. As a result, current text mining approaches like topic modeling [13], [14] and heuristics [15], [16] may not gain enough textual cues to match similar authors, resulting in incorrect correlation weights.

Challenge 2 (Context Temporal Alignments)

Vector representation models analyze each word in the whole corpus. GloVe [17] uses word pair co-occurrences and CBOW [18] predicts a word given its context. However, the current models [19], [20] ignore that the word co-occurrence patterns change by time in various facets. To witness the fact, we set up an observation on our Twitter dataset [15].

As Fig. 1

Fig. 1. - 
Co-occurrence probability.
Fig. 1.
Co-occurrence probability.

Show All

demonstrates, while most people talk about going to work between 6 to 11 am, half others drive to work in the evening (Fig. 1a). Also, people tweet about Cold+Drink, Hot+Day, and Hot+Night during the summer and such word pairs co-occur less during the winter (Fig. 1b).
Challenge 3 (Ignoring Conceptual Relevance)

The works [21], [22], [23], [24] in finding author similarities, compute the textual relevance between authors through exact or approximate matching. Though the Skip-gram [17], [20] enriches the contents using word vectors. External knowledge-bases (e.g., WordNet) can also relax the negative effects of the noisy contents [25]. However, since even the enriched contents of authors may turn up irrelevant, we also explore the common concepts they share. As noted in Table 1, where Tweet #1 and #2 are textually different, they have similar concepts. Hence, a new approach must involve both textual and conceptual signals.

TABLE 1 Conceptual Relevance
Table 1- 
Conceptual Relevance
Contributions. While our previous work [25] detects the concepts of a single tweet with the help of an external knowledge base (KB), the proposed framework in this paper identifies concepts through unsupervised clustering in the tweet space itself, thus eliminating the bias and deviation caused by a KB. Furthermore, the concept distribution of all the tweets is aggregated for each author, which is then utilized as an important feature for determining author similarity. We also enrich the short-text inference models with semantic vector space approaches to further understand the contextual (contents+concepts) relationships between short-text authors.

The proposed model in this paper not only infers the temporal dynamics [16] in embedding procedure but in addition to computing inner words similarities, it is capable to compute the correlation weight between the words and concepts. Therefore, our model can benefit many NLP tasks including ontology, named entity recognition, and stemming. Giving an example for stemming, while our model can discover the similarities between word vectors, knowing how the words correlate with exploited concepts can improve accuracy. Our contributions are fourfold:

We develop the first neural network-based temporal-textual embedding approach that can enrich short-text contents using word vectors, that are collectively learned through temporal dimensions.

We analyze both DB-Scan and K-medoids in extracting of the concept clusters from a short-text corpus.

We design a graph-cutting algorithm to extract subgraphs from the authors’ weighted graph.

We propose a temporal-textual framework that can achieve better effectiveness in generating of the highly relevant short-text authors.

The rest of our paper is as follows: in Section 2, we study the literature; in Section 3, we provide the problem and our framework; in Sections 4 and 5 we respectively explain our model and experiments. The paper is concluded in Section 6.

SECTION 2Related Work
As briefed in Table 2, the related work comprises word embedding, user similarity, and semantic understanding.

2.1 Word Embedding
Word embedding [26] associates each word in documents with a meaningful continuous vector. The applications includes Information Retrieval (IR), Natural Language Processing (NLP) [27], [28], [29], and Recommendation Systems (RS) [3], [30], [31], [32]. Traditional LSI [33] approach captures the relevance between terms and concepts through Singular Value Decomposition(SVD), while Bag-of-Words(BOW) [34] disregards the order of words and considers the word frequencies. Expectation-Maximization(EM) can equip the BOW model [35], [36] to reduce ambiguity in machine translation. The continuous BOW (CBOW) [18] ensures that the words with similar meanings will be represented by close embeddings in the latent space. The Word2vec [20] extracts meaningful syntactical and semantical regularities from word pairs and is classified into CBOW and skip-gram. While the Global Vector model (GloVe) [17] consumes word pair co-occurrences to accomplish word embedding, the CBOW model [18] predicts a word given the surrounding context. Based on our Twitter dataset, the CBOW model surpasses the GloVe approach in the standard analogy test (Section 5.2.1). Vector representation has other types: Paragraph2Vec [37], ConceptVector [30], Category2Vec [38], Prod2Vec [31]. Moreover, [13] includes topic models to collectively generate a word from either Dirichlet multinominal or the embedding module. [38] enriches the embedding with Knowledge Graphs to eliminate ambiguity and improve similarity measures. However, the state-of-the-art models [19], [20] ignore that the word co-occurrences changes at different times. Even temporal models [39], [40] rely on a single aspect and omit semantical relations [41]. But our embedding model can employ multiple temporal facets.

2.2 User Similarity
The similarity between users can be computed by their contextual information through a similarity function (e.g., Cosine). The collaborative filtering [47], graph-theoretic model [53], or other classification methods [66], [67] can then be used to group correlated users. Embedding models like User2Vec [50] and Mllda [52] utilize associated vectors to find the order of user relations. node2vec [49] and DeepWalk [51] use skip-gram and treat each user as a node. Author2Vec [11] combines the content and link information of the users to predict the correlation weight between authors. [21], [22], [23], [24] compute the user similarity through exact and approximate textual matching. In contrast, [20] maps users to a latent space. Unlike [55] which uses temporal contents, most of embedding models neglect the time factor. Hence, we use multi-facet time-based clusters to infer the both temporal and textual correlations between authors.

2.3 Semantic Understanding
Textual semantics can be learned through various approaches. Early NER methods [68], [69] employ classification techniques to label the entities in a document. However, NER models cannot function effectively on noisy short-text contents. Topic models [62], [63] exploit latent topics through word distributions. However, since they do not effectively retrieve the statistical cues from short-text contents, the semantic labeling task is left out. Current models like CBOW [18] and ParagraphVEC [37] are proved to be beneficial to the understanding of the textual contents. Expansion models [60] are inspired by query expansion techniques [58], [59], [61] and enrich initial textual contents by complementary relevant contents. More recent Deep Neural Network models such as CNN [57] and RNN [56] facilitate short-text understanding through classification. Some other works [25], [64], [65] exploit global concepts from the corpus to better represent the semantics in each document. Hence, we employ clustering modules besides embedding procedure to take advantage of concepts and contents concurrently.

SECTION 3Problem Statement
In this section, we elucidate preliminary concepts, problem statement for author linking, and our proposed framework.

3.1 Preliminary Concepts
We commence with definitions as formalized below:

Definition 1 (node).
Every Node, denoted by ni ∈ N represents a distinguished author of short-text contents in a social network.

Definition 2 (short-text message).
mj∈M advocates a short-text that is composed and published by an author ni. Each message has an identity (mj), the associated author (i.e., ni), and the time-stamp (mj.t). Accordingly, M (M={M1,M2,…,Mn}) includes all short-text contents, where Mi delineates the set of short-text messages that are owned by the author ni.

Definition 3 (latent temporal facets).
The associated time-stamp can be interpreted by multiple time facets, where each facet represents a latent parameter zk in T={z1,z2,…,zt}.

Definition 4 (temporal slab).
Each latent temporal dimension zx can comprise η splits zx={sx1,sx2,…,sxη}, e.g., 7 splits for day dimension where each split advocates an individual day in the week. Accordingly, the Uni-facet Temporal Slabs are built via the merging of similar splits.

Definition 5 (vector representation).
Given the corpus of textual contents C, for elucidation purposes, the word embedding model conveys each vocabulary vi∈V with a distinguished real-valued vector vi→ which is obtained in conjunction with other word vectors.

Definition 6 (authors weighted graph).
G=(N,L) is the authors’ weighted graph which includes all the authors N with pertinent links L between them. The link lij∈L describes the similarity weight between two authors (ni,nj) and can be computed using various approaches.

Definition 7 (query subgraph).
The query subgraph denoted by gq~⊂G contains the set of authors that are highly correlated with the input query user nq∈N.

We not only use the vectorization for words, but also devise a textual time-aware model to represent the tweets, concepts, and authors by vectors.

3.2 Problem Definition
Problem 1 (extracting hierarchical time-aware slabs).
Given the set of authors N, messages M, and the temporal latent factors T, we aim to extract the set of uni-facet temporal slabs through clustering similar splits in each facet. The child temporal facets are hierarchically affected by the parent dimension(s).

Problem 2 (computing contextual similarities).
Given the set of temporal facets T and the messages of each author (e.g., Mi for ni), we aim to compute the correlation weight between each pair of authors (lij≡Link(ni,nj)|∀ni,nj∈N&lij∈L).

Problem 3 (author linking).
Given the author weighted graph G, we aim to mine the single query subgraph g~q containing the set of authors, where all of them are highly correlated with every query author nq∈N.

3.3 Framework Overview
The problem of author linking (Problem 3) includes two steps: (1) to compute the similarity weights between authors (Problem 2). (2) to employ a stack-wise graph cutting algorithm to optimize the number of exploited subgraphs and maximize the intra-subgraph correlations. Fig. 2 illustrates our framework for linking authors of short-text contents through a multi-aspect temporal embedding model.

Fig. 2. - 
Framework.
Fig. 2.
Framework.

Show All

In the offline part, we use microblog contents to acquire the grids to record similarities between temporal splits in every dimension (e.g., 24 splits for hour dimension). We then build uni-facet time-based slabs by merging similar splits. We construct tweet vectors through merging the word vectors. We build author contents vectors through merging tweet vectors. Moreover, we cluster similar tweets to discover the concepts, where we associate each tweet with a set of concepts. Accordingly, each author can be collectively represented by the content and concept vectors (so-called context), that we use to compute the similarity weight between authors.

In the online part, we aim to discover a subgraph that includes a set of highly correlated authors to the query author. We first generate the contextual vectors of the query author and update the authors’ similarity matrix. Finally, we employ a simple but effective stack-wise graph cutting algorithm to extract the output subgraph, in the form of a maximum spanning tree.

SECTION 4Methodology
4.1 Offline Phase
4.1.1 Constructing Multi-Facet Dynamic Slabs
For the similarity matrix, we measure the Textual likelihood between each pair of splits (e.g., Sunday and Monday in day dimension). To proceed, we merge the textual contents of each temporal split. Accordingly, every temporal facet can be assigned with a vector where each cell can contain the short-text contents for the split. We use a modified TF-IDF algorithm (Eq. (1)) to find the weight of each word in the textual contents of every temporal facet
w^(ti,Slk)=f(ti,Slk)Max(t∈Sl){f(t,Slk)}×LogNN(ti).(1)
View SourceRight-click on figure for MathML and additional features.Here, N designates the total number of the splits and N(ti) is the number of splits at which the term ti has appeared. While Slk is the textual contents of split k in the latent facet l, f(ti,Slk)Max(t∈Sl){f(t,Slk)} normalizes the term frequency. Every split can be presented by a vector S⃗ lk where the cells contain the weights for the terms.

Finally, a similarity measure, like Cosine, can report how correlated each pair of splits are. The number of dimensions, (e.g., binary facets of zh and zd for the hour and day latent factors) can be decided by the sparsity of data and complexity of the solution. Unlike our prior work [16], we noted the effects of the parent(s) on the child temporal facet. For instance, zh⊂zd elucidates that the hour dimension gets affected by the parent temporal dimension (i.e., zd).

As implemented in [16], the bottom-up Hierarchical Agglomerative Clustering (HAC via complete linkage) can merge similar temporal splits in each latent temporal facet to shape the final temporal slabs. The threshold of the HAC model may place irrelevant splits into the same cluster or wrongly place relevant splits into different slabs.

Fig. 3 depicts the similarity grid and the clustering dendrogram for the day dimension. On the one hand, threshold 1.0 will make a cluster from every day split. On the other hand, threshold 0.59 results in more meaningful slabs, reported in Table 3. Since we consider the influence of the parent facets, for the hour dimension, we will need to consider two similarity matrices (one for each daily slabs). Fig. 4 illustrates the hour similarity grids based on which the dendrograms are obtained in Fig. 5. As Table 4 shows, we have two sets of clusters for the hour dimension, one set for each daily slab.

TABLE 2 Literature
Table 2- 
Literature
TABLE 3 Day Slabs, Threshold Set to 0.59
Table 3- 
Day Slabs, Threshold Set to 0.59
TABLE 4 Extracting Hour Slabs Affected by the Day Slabs

Fig. 3. - 
The day latent facet.
Fig. 3.
The day latent facet.

Show All

Fig. 4. - 
Hour similarity grids based on daily temporal slabs.
Fig. 4.
Hour similarity grids based on daily temporal slabs.

Show All


Fig. 5.
Hourly slabs hierarchical dendrograms.

Show All

4.1.2 Word Embedding Models
Informal short-text contents include noise and writing errors. Hence, as discussed in Challenge 1 (Section 1), recent text mining approaches including topic models [13], [14] fail to obtain significant statistical cues to match the textual contents of the similar authors. On the other hand, the correlation weight between a pair of microblog authors (u,v) will be computed incorrectly when we compare the exact textual contents (Ou and Ov). To address this issue, the semantic vector space models [17], [18], [20], [33], [70] retrieve the vector representation of each word. As the first solution to correctly compute the semantic relevance between authors, one can construct a decently ordered list of similar words to each comprising word vi in an author's contents ou, denoted by v⃗ oi. The textual contents of each author will then be represented by a new encyclopedic semantic representation form (O′u), every word vi∈Ou will be replaced by the top ζ most similar words from v⃗ oi. To this end, we can choose four embedding models: Singular Value Decomposition(SVD) [70], Skip-gram [20], CBOW [18], and GloVe [17].

Word embedding algorithms utilize the word usage patterns (e.g., word pair co-occurrence) and represent each word with a real-valued vector. Hence, the embedding models learn the vectors using knowledge from the whole corpus. Therefore, when a sample pair of conceptually similar words, like “perfect” and “excellent”, turn to be textually mismatched (Challenge 1 in Section 1), the word embedding models can easily reveal the high similarity rate between the pair through comparing of their pertinent vectors. SVD computes the word vectors without training and using matrix operations over the co-occurrence matrix. For the other three models, the well-trained vectors are iteratively enumerated through forward and backward propagations.

While the CBOW model estimates the center word of the window by the one-hot vector of the surrounding context (order is important), the skip-gram calculates the co-occurrence probability of the surrounding words with the middle word. Nevertheless, both models return the word vectors that are trained in the hidden layer. GloVe consumes the word co-occurrence matrix, where the model converges toward the optimized values in context and main vectors.

4.1.3 Temporal Word Embedding
As elucidated in Section 1, the word proximity patterns change in various temporal facets. However, current word embedding models [19], [20] ignore this reality. Also, notice that the CBOW algorithm can pass the word analogy test better than other vector space models (Section 2.1). Hence, we devise our novel time-aware embedding model based on CBOW, named as TCBOW, to better track the multi-aspect temporal-textual variations in short-text contents. Note that the time-aware embedding process should predict unforeseen observations through merging the knowledge from all the slabs. Hence, we first devise a TCBOW module which uses the slabs from all temporal dimensions.

Fig. 6 depicts the diagram for the slab-based TCBOW where k is a single slab in dimension l∈T. The input layer contains the number of C one-hot encoded input words {xlk{1},xlk{2},…,xlk{C}} where C is the size of the window and the number of vocabularies is denoted by |Vlk|. The hidden layer hlk is N-dimensional and ylk represents the output word. The one-hot encoded input vectors are connected to the hidden layer via Wlk weight matrix and W′lk associates the hidden layer to the output. We employ [18] to compute both weight matrices of Wlk and W′lk. Given surrounding vocabs, Stochastic Gradient Descent maximizes the conditional probability of the output word.

The hidden layer output is the average of one-hot input vectors that utilize the slab-based weights of Wlk (Eq. (2))
hlk=1C×Wlk.(∑i=1Cxlk{i}).(2)
View SourceRight-click on figure for MathML and additional features.We also employ Eq. (3) to calculate the input from the hidden layer to every node in the output layer. Here V′lk{Wlk{j}} is the jth column of the output matrix W′lk
ulk{j}=V′lk{Wlk{j}}T.hlk.(3)
View SourceFinally, we can apply the soft-max function on ulk{j} to attain the output layer ylk{j} (Eq. (4))
ylk{j}=Exp(ulk{j})∑Vlkj′=1Exp(u′lk{j}).(4)
View SourceGiven slab k in the facet l and relying on embedding weights (Wlk), v⃗ lk{i} can denote the embedded vector for each word i in the hidden layer. The cosine function (Eq. (5)) can determine the slab based similarity between each word pair (i,j)
SCosine(v⃗ lk{i},v⃗ lk{j})=v⃗ lk{i}.v⃗ lk{j}|v⃗ lk{i}|×|v⃗ lk{j}|.(5)
View SourceRight-click on figure for MathML and additional features.

The vocabulary corpus can be denoted by Vlk where v⃗ lk{i} and v⃗ lk{j} are the subsets of slab-based vector V⃗ lk. To continue, we invoke two attributes to better infer the correlation intensity between each pair of words (i,j) in temporal slabs.

Level (Slevel(l,v⃗ i,v⃗ j)) explains how extended each pair of words correlate together in all the temporal slabs of a single latent facet of l.

Fig. 6. - 
CBOW.
Fig. 6.
CBOW.

Show All

Depth (Sdepth(l,v⃗ i,v⃗ j)) infers how the pair of words correlate in each slab, while hierarchically impacted by parent temporal dimension(s).

Eq. (6) formalizes the level-wise similarity between the vectors of the words i and j, where A~lk is the normalized accuracy of the analogy test for slab k in dimension l
Slevel(l,v⃗ i,v⃗ j)=∑k∈lA~lk×SCosine(v⃗ lk{i},v⃗ lk{j}).(6)
View SourceRight-click on figure for MathML and additional features.

Eq. (7) shows the depth similarity between the hidden layer vectors of the pair (i,j). Note that we propose two static methods for the latent facet class. Suppose that the hour facet is directly impacted by the day dimension zh⊂zd, in this case, if the current facet in the loop is the day, l.child() can return the slabs from hour factor
SDepth(l,v⃗ i,v⃗ j)=∑k∈τbl∑q∈τbl.child()A~l.child()q×SCosine(v⃗ l.child()q{i},v⃗ l.child()q{j}).(7)
View SourceHere τbl and τbl.child() are the set of uni-facet temporal slabs that are respectively associated with the current level (l) and its child dimension. Accordingly, A~l.child()q denotes the normalized accuracy of the analogy test for slab q of the child facet for l.

The cosine similarity is computed for the hidden layer vector representation of the words (vi,vj) in two layers of the child (l.child()) influenced by the parent (l). Note that the similarity feature for all the slabs of the child latent factor l.child() (denoted by q), are impacted by corresponding slab k in the parent latent factor (i.e., l). To this end, we generalize Eqs. (7) to (8) to recursively call the depth property toward the leaf nodes (l.child()≠Null)
SDepth(l,v⃗ i,v⃗ j)=⎧⎩⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪∑q∈τblA~lq×SCosine(v⃗ lq{i},v⃗ lq{j})+SDepth(l.child(),v⃗ i,v⃗ j)ifl.child()≠Null∑q∈τblA~lq×SCosine(v⃗ lq{i},v⃗ lq{j})otherwise.(8)
View SourceRight-click on figure for MathML and additional features.Eventually, the correlation intensity ([−1,1]) between each pair of words (i,j) can be collectively evaluated by both the {level and depth}-wise attributes, Eq. (9):
S¯¯¯Cosine(v⃗ i,v⃗ j)=∑l∈T(Slevel(l,v⃗ i,v⃗ j)+SDepth(l,v⃗ i,v⃗ j)).(9)
View SourceRight-click on figure for MathML and additional features.After computing the similarity between each pair of words, we obtain BTCBOW as a |V|×|V| matrix, where each row i is associated with a single vocabulary vi and v⃗ i can represent the similarity between vi and other words. V⃗ TCBOW is also the set of vectors in BTCBOW grid.

Inherently, the dimension of vectors in V⃗ TCBOW equates to the number of words (i.e., |V|) that is much more than |d|, the dimension of vectors in the hidden layer. Due to complexity, such a high dimension can negatively affect efficiency. To address this challenge, we propose V⃗ C which collectively computes the word vectors based on each of the slabs in all dimensions.

To compute these collective word vectors, we include the impact of the word vectors of V⃗ lk in each slab Slk through multiplying the vectors by their normalized accuracy in analogy A~lk. We compute the collective word vector v⃗ Ci for each word vi using two attributes of level and depth. Unlike Vlevel(l,i) which only includes the slabs for the current latent facet (Eq. (10)), the depth property Vdepth(l,i) hierarchically considers the effects from all the parent latent facets
Vlevel(l,i)=∑k∈lA~lk×v⃗ lk{i}.(10)
View SourceRight-click on figure for MathML and additional features.Vlevel(l,i) considers the slabs in the same latent facet. Similarly, Eq. (11) calculates the depth property
VDepth(i)=⎧⎩⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪∑q∈τblA~lq×v⃗ lk{i}+VDepth(l.child(),i)ifl.child()≠Null∑q∈τblA~lq×v⃗ lk{i}otherwise.(11)
View SourceHere l is the current layer and i is the index of words for which the collective vector is computed. Like Eq. (8), the depth property behaves recursively. We can use Eq. (12) to get v⃗ Ci, the final collective word vector for each word vi
v⃗ Ci=∑l∈T(Vlevel(i)+VDepth(i)).(12)
View Source

4.1.4 Generating Tweet Vectors
Given the word vectors that are constructed by the temporal embedding model (Collective), we now need to generate tweet vectors. Summation and Averaging are two simple but effective approaches to combine word vectors in each tweet and obtain the outcome tweet vector. The Tweet vector is computed by merging the vectors of the comprising word (Eq. (13)).

The main idea behind using the summation and average methods was the fact that both methods were simple but effective at the same time. In fact, we aimed to demonstrate that such primary methods with the least time complexities could still appear to be effective on this task. While other methods may even return better results, we did not concentrate on this aspect of our framework. We leave the study of other methods in the computation of tweet vectors as a future task. Indeed, there can be better methods to extend the way we combine the word vectors, therefore we plan to devise a more competent algorithm for this task. While the summation approach generates vectors with bigger values and augments the computation time, the average method places the resulting vector between input vectors, which can better represent the blending
m⃗ Avgi=∑|mi|j=1v⃗ mi[j]|mi|,m⃗ Sumi=∑j=1|mi|v⃗ mi[j].(13)
View SourceHere, |mi| denotes the number of words in each short-text mi and v⃗ mi[j] constitutes the word vector for the jth word in mi. Tweet vectors can represent the comprising word vectors. However, short-text instances might refer to different concepts when context differs. Therefore, understanding of the concept(s) to which the tweets correspond matters in recognition of the authors’ preferences. We utilize two popular clustering methods of DBScan [71] and K-Medoids [72], which differ in nature. Where DB-Scan detects the densely grouped tweets, K-medoids discover the outliers, that have been cast-out by the DB-Scan algorithm. Nevertheless, we employ the well-known euclidean distance to measure the space between cluster points (Eq. (14))
DistanceEuclidean(v⃗ i,v⃗ j)=∑p=1d(v⃗ i{p}−v⃗ j{p})−−−−−−−−−−−−−⎷.(14)
View SourceRight-click on figure for MathML and additional features.Here d denotes the dimension of word vectors and p indicates the index of any word vector (e.g., v⃗ i). Nevertheless, given the list of exploited clusters, we can present each tweet mi with the tweet concept vector. The new vector lists the dissimilarities between each tweet mi and each of the concepts that are the center point of each cluster, Eq. (15)
∀|Cf|j=1m⃗ fi[j]=DistanceEuclidean(m⃗ i,c⃗ fj).(15)
View SourceRight-click on figure for MathML and additional features.Here, |Cf| shows the number of clusters that are extracted using any clustering method of f. Where m⃗ fi shows the tweet concept vector that is computed using f, and c⃗ fj symbolizes the center tweet vector of the jth cluster that is extracted using the same clustering model (i.e., f). Finally, m⃗ fi[j] denotes the jth entry of the m⃗ fi. Furthermore, C as the chosen set of clustering models (f∈C) can collectively include two features from the type of clustering (k-medoid and DB-Scan), and whether the tweet vectors are constituted from summation or average of comprising word vectors. For instance, Sum−DB can specify an f clustering model where the tweet vectors are constructed by the summation of word vectors and the employed clustering model is DB-Scan. It is noteworthy that the Tweet concept vectors tend to impose the smaller grid of R|Cf|.

4.1.5 Generating Author Vectors
Each author can be associated with content and concept vectors. We can apply the summation or average operators on tweet vectors to obtain the author's content vector. Let mj be a tweet from the set of tweets composed by the author ni (mj∈Mi) where m⃗ j denotes the vector for mj. It is easy to see that the sum and average vectors for author ni can be computed using Eq. (16)
n⃗ Content−Sumi=∑m∈Mim⃗ ,n⃗ Content−Avgi=∑m∈Mim⃗ |Mi|,(16)
View Sourcen⃗ Content−Sumi and n⃗ Content−Avgi respectively represent the sum and average author content vectors. Rather than considering operational functions (sum and average), let us consider statistical approaches.

We aim to predict the vector of an unobserved tweet that is deemed to be semantically aligned toward the preferences of the author ni. While topic-modeling [14] seems to be the best solution for document generation, we extend the embedding model of the tweet vectors by the K-Fold statistics. Let d be the dimension of the tweet content vector. Hence, we can conveniently discover the maximum probability for each value to be assigned to the corresponding item in the author's content vector. To this end, we partition the range [-1,+1] into ς number of bins (In here ς=10). For each index in the author vector, we consider the bin which contains the values from the majority of the tweet vectors on the same index. The selected bin will represent the weight with the highest probability for the current index in the author vector.

If the values are evenly distributed between some or all of the bins, we should find a neutral value for the current field in the collective word vector. We need to place the value at the same distance from all the bins. Given three bins of (-0.8,-0.6), (0,0.2) and (0.8,1) that evenly conclude the maximum number of values, we can first compute the centroids as -0.7, 0.1 and 0.9 and second, calculate the average as 0.15. Since the value of 0.15 is the closest measure to the centroid of the bin (0,0.2), we can choose 0.1 as the neural value for the current field.

As instantiated in Fig. 7, the bin [-0.8,-0.6] reports the values from four tweets and demonstrates that with more than 50 percent chance, the index zero of the author vector will be assigned between -0.8 and -0.6. Similar to the author's content vector, we can aggregate the tweet concept vectors of each author to construct the author's concept vector (n⃗ Concepti). The concept vectors are low in dimension and dynamically carry the knowledge of concepts through clustering. However, content vectors come with less complexity and include the information of the tweets which are possibly ignored in DB-Scan or misclassified by K-medoid. The eventual author similarity should include the impacts from both content and concept vectors (Eq. (17)) that is adjusted by α
∀|U|i=1∀|U|j=1,XTotal−αij=α×XConceptij+(1−α)×XContentij.(17)
View SourceBecause of the difference in dimensionality, it is not inherently feasible to consolidate content and concept vectors of the authors. Given U as the set of authors, we should compute the similarity between each pair of authors to build the distinctive correlation matrices using concept and content vectors, respectively denoted by XConcept and XContent. The impact from correspondent matrices can be merged by α to maximize the final performance.

Fig. 7. - 
(Fold).
Fig. 7.
(Fold).

Show All

4.2 Online Phase
In the online phase, we aim to mine a set of authors that are highly correlated to the query author nq. Two duties are undertaken in the online phase: Including query author and extracting stack-wise maximum spanning trees.

4.2.1 Including Query Author
This duty is divided into two tasks of generating query author vectors and computing query author contextual similarities that are quite similar to what we explained in Section 4.1. First, we need to generate the author vectors for the query author nq. This is especially necessary for the cold start authors. Because by posting a single tweet, the author can reveal his/her conceptual alignment.

Algorithm 1. Stack-Wise Max. Spanning Tree (SW-MST)
Input: G

Output: G′,S

N′=∅,L′=∅,N′′=N,L′′=L,S=∅

while L′′≠∅ do

l=Min(L′′)

S.push(l)

L′′.remove(l)

end while

while N′′≠∅ do

l=S.pop()

L′.append(l)

if l[0]∉N′ then

N′.append(l[0])

end if

if l[1]∉N′ then

N′.append(l[1])

end if

if l[0]∈N′′ then

N′′.remove(l[0])

end if

if l[1]∈N′′ then

N′′.remove(l[1])

end if

end while

G′=(N′,L′)

return G′, Avg(L′)

Given the set of current tweets (Mq={m1,m2,…,mr}) belonging to nq, we can generate corresponding tweet vectors M⃗ q={m⃗ 1,m⃗ 2,…,m⃗ r} using precomputed V⃗ C. This step is not time-consuming as the language model is already generated in the offline phase.

What is the usage for Trigger? Trigger follows frequent intervals to continuously rebuild the slabs and subsequently construct the vector representations. This is especially useful to include the tweets of new authors where it can partially affect the embedding results.

Using the tweet vectors we can easily retrieve the nq's content vector n⃗ Contentq. Correspondingly, we need to find the distance between each tweet mi∈Mq and the cluster centroids which results in the set of tweet concept vectors M⃗ fq={m⃗ f1,m⃗ f2,…,m⃗ fr}. Here f denotes the selected clustering approach. Accordingly, the author concept vector n⃗ Conceptq can be computed by averaging of the vectors in M⃗ fq. Given the content n⃗ Contentq and concept n⃗ Conceptq vectors of the query author nq, we can respectively update XContent and XConcept author similarity matrices which are accomplished through measuring the similarity between nq and others. Eventually, graph G=(N,L) can represent the authors weighted graph, where N is the set of nodes (authors) and L denotes the set of undirected edges with similarity weights.

4.2.2 Extracting Query Author Subgraph
We now aim to exploit the subgraphs with highly correlated authors which further comprises the query author nq. In general, we can address the challenge through Lemma 1. Inspired by the Lemma 1, we devise the Stack-Wise Maximum Spanning Tree (SW-MST) approach (Algorithm 1) to calculate the MST for each of the highly correlated subgraphs in G.

As Algorithm 1 shows, we first push the edges into the empty stack S in ascending order, where the links with the lower weight are pushed downward. Correspondingly, we initiate an empty graph G′=(N′,L′) to store the resulting spanning trees. To continue, we iteratively pop the edges from the stack and add them to L′ and append the corresponding nodes to N′. We repeat the process until every ni∈N is added to the N′. The G′ will finally include a set of maximum spanning-trees. Algorithm 1 first extracts distinctive subsets of the graph in the form of maximal cliques and subsequently exploits an MST out of the cliques. Finally, each exploited MST can represent a highly correlated author subgraph.

Lemma 1.
Linking a set of highly correlated authors to the query author nq can be facilitated by the inner-author edge weights.

Proof.
Given a fully connected weighted graph which represents the weight of contextual similarity between author vectors, the subgraph g~q comprising nq will result in the maximum spanning tree with the biggest average edge weight. With this logic, the node nq will be highly correlated to every node in g~q via either of the direct or indirect link(s).

SECTION 5Experiment
We conducted extensive experiments on a real-world twitter dataset [15] to evaluate the performance of our model in short-text author linking. We ran the experiments on a server with 4.20 GHz Intel Core i7-7700K CPU and 64 GB of RAM. The codes are available to download.1

5.1 Data
Our Twitter dataset [25] includes 8 million English tweets in Australia, collected via Spritzer Twitter Feed. The sampling was done at various times of the day for a complete year. We then used Twitter API to select approximately 4K users from streaming tweets and retrieved up to 1,000 records from their Twitter history. Finally, we attained ≈1M geo-tagged tweets which are all composed in Australian territory. The dataset contains 305K vocabs and is made of 65M collocations.

5.1.1 Baselines
The baselines in computing of the similarity weights between authors are listed as follows. Note that the author's similarities can be computed by measuring the similarity between author vectors.

SoulMateConcept: As explained in Section 4.1.5, this method renders the authors with the closeness of their tweets to each of the concepts.

SoulMateContent: this embedding approach [18] obtains the tweet vectors and then combines them to form author content vectors.

SoulMateJoint: this model regulates α to combine the author's similarities through concept and content vectors (Section 4.1.5).

Temporal Collective: this model computes the collective word vector through multi-facet temporal embedding [16], [73] and then enriches the textual contents of each author by replacing each word with its top ζ most similar words. Finally, TF-IDF can measure the textual similarity between authors.

CBOW Enriched: this model uses CBOW [18], [74] to produce the distributed representation of the words. Given the enriched textual contents of the authors, the model employs the Jaccard coefficient to compute the textual similarities.

Document Vector: this model [75] computes the similarities between authors using TF-IDF statistics.

Exact Matching: this straightforward baseline exactly matches the short-text contents of the authors.

5.2 Effectiveness
5.2.1 Basic Comparison of Vector Space Models
In this section, we first apply the Google word analogy task [20] to compare the effectiveness and efficiency of the vector representation models. The vector representation baselines (Section 4.1.2) are four-fold: SVD [70], Skip-gram [20], CBOW [18], and GloVe [17]. SVD-15:15000 limits the word pair co-occurrences between 15 and 15,000. Also, the numerical extension in GloVe-30 highlights the number of training epochs.

The analogy test aims to discover the model that on the one hand, suits best to the short-text noisy contents, and on the other hand, is the best candidate for time-aware embedding. The test includes ≈20K syntactical and semantical questions like “a is to b as c is to ?”, where each competitor suggests a word to alter the question mark.

Fig. 8a reports the accuracy of the analogy task on the twitter dataset (Section 5.1), where the dimension varies. Our dataset contains enough words for only ≈7K questions, resulting in lower numbers. The CBOW model overpasses all rivals and SVD performs the least as it lacks the training phase. Conversely, the CBOW as the most noise-resistant model surpasses skip-gram because it better involves the context of the words in the training procedure. Finally, notice that excessive noise in microblog contents leads to a sparse and oversized co-occurrence matrix which significantly reduces the performance of the GloVe model.

Fig. 8. - 
Performance of the vector space models.
Fig. 8.
Performance of the vector space models.

Show All

Our online author linking framework must handle millions of the short-text contents, where the vector representation module forms the underlying time-aware module. So as illustrated in Fig. 8b, we compare the efficiency of vector space methods. We notice that due to the lack of training procedures, the temporal latency of the SVD model is the least. Furthermore, for the models with training, the CBOW and skip-gram closely gain the highest efficiencies. Because the GloVe models receive the word co-occurrence matrix as the huge size input, they naturally demonstrate the highest latency in training. Therefore, we conclude that CBOW model performs better than other rivals for both parameters of effectiveness and efficiency.

5.2.2 Comparison of the Author Subgraph Mining Methods
In this part, we compare our approaches to linking authors with well-known competitors (Section 5.1.1). As the first step, the author's similarity matrix of each baseline model can establish the author weighted graph. We propose three algorithms to calculate author similarities (SoulMateConcept, SoulMateContent, and SoulMateJoint). Eventually, given the originated weighted graph, each model can employ the SW-MST algorithm (Section 4.2.2) to acquire the final author subgraphs, as Maximum Spanning Trees (MST).

Benchmark. Since the authors within each spanning tree should exceedingly correlate, as Table 5 shows, we evaluate the baselines through assessing the similarity between authors in the same exploited subgraphs. To this end, we first obtain the set of MSTs out of G′ (output of SW-MST) which comprises any of 50 arbitrarily chosen authors. We then pick top 5 MSTs with at least 5 nodes that have the highest average edge weights. Finally, given the top 10 most similar tweets from each pair of authors in the selected MSTs, we consider the votes of 5 local (Australian) experts. The possible votes are defined as follows:

score 0: neither textually or conceptually similar.

TABLE 5 Precision of Author Similarity in Subgraph Mining
Table 5- 
Precision of Author Similarity in Subgraph Mining
score 1: minor textual and conceptual similarity.

score 2: high textual and conceptual similarity.

score 3: minor textual but high conceptual similarity.

Subsequently, we compute the average of the votes given to each pair of tweets and round it to the nearest lower integer. We then count the tweet pairs with the scores of 2 and 3 for each method in computing of the authors’ similarities. We then compute the precision metrics through dividing the counts for the scores of 2 and 3, admitted by the average of experts’ votes, by the total number of selected tweet pairs in subgraphs.

SoulMateConcept is devised to detect the conceptual similarities, where the textual relevance is minor. Moreover, the SoulMateContent can trace the textual relevance. However, since SoulMateJoint combines both modules through parameter adjustment (α=0.6), it gains the highest votes for both conditions. Table 5 shows where the textual similarity is low, SoulMateConcept can still gain an approximate precision of %30 for the conceptually relevant pairs of authors. Conversely, SoulMateContent can track textual similarity up to %43. We notice that where the textual similarity between short-text contents is very low and textual models including Temporal Collective, CBOW, Document Vector and Exact matching perform less than 2 percent, SoulMateConcept can detect the semantic correlation between authors by 30 percent. Therefore, SoulMateJoint which is equipped both with content and concept components performs more accurately than other competitors.

Note that based on our peripheral experiments on Temporal word embedding (Section 4.1.3), where the accuracy of V⃗ TCBOW in generating of the tweet vectors is 0.881, the dimension is quite large (the number of words (|V|)). Hence, we employ the collective manner V⃗ C which offers a lower precision of 0.861 but in contrast, provides a much smaller dimension, the size of hidden layer vectors (|d|).

5.2.3 Effect of Embedding on Author Content Vectors
We here study the impact of several parameters on the effectiveness of author contents vectors. We compare CBOW versus Collective methods that are respectively the best non-temporal and temporal embedding models. To form the tweet vectors, the word vectors can be combined using summation or averaging. The tweet vectors can also form the author's content vectors through various aggregations, such as Average, Summation, and 10 Fold model.

Benchmark. The experts label the top 10 most similar tweets with the mentioned scores that were previously defined in the benchmark part of Section 5.2.2. We also deploy averaging and rounding to the bottom integer method, as we did in the previous section, to take into account the votes of all the experts. We then consider the computed score as the final score for each selected pair of tweets. Inspired by [76], we propose two weighted precision equations of (18) and (19) to compare the effectiveness of the methods: Note that in both equations, the pertinent scores are prefixed by ρ (e.g., the number of items for score 1 is denoted by ρ1).

PConceptual: The weighted precision formulated in Eq. (18) pays more attention to the pairs with high conceptual but low textual similarity which leads to the high numerical coefficient of 3 for ρ3 and the null significance for ρ0. Here, the precision is normalized by multiplying the sum of the score counts by 3 in the denominator
PConceptual=ρ1+ρ2×2+ρ3×33×(ρ0+ρ1+ρ2+ρ3).(18)
View SourcePTextual: As verbalized in Eq. (19), both textual and conceptual similarities gain the same importance in PTextual metric. This enforces the same coefficient of 2 for ρ2 and ρ3
PTextual=ρ1+(ρ2+ρ3)×22×(ρ0+ρ1+ρ2+ρ3).(19)
View SourceRight-click on figure for MathML and additional features.

As Table 6 shows, while among the embedding methods, the time-aware approach (collective embedding) is better than CBOW, the summation works better than average in the aggregation of word vectors. The 10 fold algorithm gains a higher precision for PTextual in the aggregation of tweet vectors. However, since the 10 Fold approach performs low for PConceptual and extracting the conceptual similarity between authors is one of the main advantages of the proposed model, we choose the aggregation algorithms that can jointly support both weighted precisions. Since the normalized vector for the summation method is very similar to the average approach, both precisions come with similar results.

Regarding author content vectors, while summation and averaging methods return similar results, we choose the average operator to obtain lower decimal values and also less computational complexities.

5.2.4 Impact of Short-Text Vector Clustering
To extract the concepts from microblog contents, we need to cluster the tweet vectors. Where the number of concepts (concept vector dimensions) increases, we gain higher effectiveness for conceptual word vectors. Hence, we aim to select those thresholds that can maximize the number of exploited clusters (concepts), and simultaneously maintain the satisfying quality. Hence, in this section, we compare the performance of various clustering models. Note that we use K as the number clusters for K-medoids and ϵ as the radius for DBSCAN. Where the thresholds vary, it is both tedious and time-consuming to test the quality of clusters by human experts. Therefore in this section, we first study the cohesion and separation properties of the clusters using two well known methods of the Silhouette score [77] and the Davies-Bouldin index [78]. Subsequently in Section 5.2.5, we limit the range of thresholds first and then ask the experts to evaluate the quality of clusters.

Fig. 9 illustrates the impact of thresholds on clustering. In general, the lower the Davies-Bouldin index and the higher the Silhouette score, the better the threshold will be. For K-medoids as depicted in Fig. 9a, we select the range [15,30] where the number of clusters is higher and the indicators highlight a good clustering quality. Subsequently, from the selected range and based on the limited variability in the quality slope of the clustering, we select the values 20, 22, 24, and 26 for the number of clusters (i.e., K). This ensures a higher number of clusters with reasonable quality. Similarly, Figs. 9b and 9c study the impact of ϵ on the number and the quality of clusters in the DBSCAN method. Fig. 9b shows the number of clusters where ϵ varies. Here the threshold range of [0.325,0.475] supports the highest number of clusters, which is more than 15. Consequently, Fig. 9c can analyze the clustering scores against various ϵ values, where we aim to find the thresholds in the selected range. We notice that when the value of ϵ grows bigger than 0.4, both the number of concepts and the quality metrics reduce. We then nominate 0.36, 0.38, 0.4, and 0.42 for ϵ to maximize the number of high standard clusters.


Fig. 9.
Impact of thresholds on clustering.

Show All

5.2.5 Selection of Clustering Thresholds
We limited the range of thresholds in Section 5.2.4. In this section, we choose the best final clustering thresholds that are voted by human experts.

Benchmark. To select the best thresholds in K-medoid and DBSCAN, we consider the clusters that have been retrieved by each threshold. We then choose the top 10 most similar pair of tweets from each tweet cluster, where the similarity measure is carried out by the well-known TF-IDF method. Subsequently, the human experts of five determine the similarity of the pairs through majority voting using ρ0,ρ1,ρ2, and ρ3 that are then used to compute PTextual (Section 5.2.3). The best threshold should generate the clusters with the highest weighted precision.

Fig. 10 depicts the weighted precision based on the selected thresholds when the ζ (Section 4.1.2) varies. For DBSCAN (Fig. 10a), the value of 0.36 for ϵ can demonstrate the best performance for all ζ values. However, other thresholds turn up untrusted with many perturbations where ζ varies. Therefore, as Fig. 10b illustrates, none of the thresholds significantly performs better. Nevertheless, for K=22, the k-medoids model gains the highest quality at ζ=10 and at the same time maintains a reasonable precision for different values of ζ.

Fig. 10. - 
The weighted precision by $\zeta$ζ for various thresholds.
Fig. 10.
The weighted precision by ζ for various thresholds.

Show All

5.2.6 Impact of Clustering on Authors Concept Vectors
As elucidated in Section 5.2.3, in this section we evaluate the precision of author concept vectors via two weighted metrics of PConceptual and PTextual. As shown in Table 7, we report the weighted precisions based on three variations: (1) embedding type (CBOW versus Collective), (2) the type of combination (Avg versus Sum) for word vectors in generation of tweet vectors, (3) clustering type (K-Medoids versus DBSCAN) in constructing author concept vectors.

TABLE 6 Weighted Precision of User Content Vectors

TABLE 7 Precision of User Concept Vectors
Table 7- 
Precision of User Concept Vectors
As shown in Table 7, our proposed time-aware collective model can outperform the CBOW model in both weighted precisions, where the overall improvement for PTexutal and PConceptual are approximately 7 and 4 percent. The K-Medoids clustering performs better than DBSCAN. This is because the DBSCAN model can ignore outliers. We notice that the time-aware collective model performs the best @K=22, where the CBOW gains the lowest results. Since the normalized summation vectors resemble the average approach, their corresponding precision results turn the same. We can, therefore, overlook the impact of combination type in tweet generation.

5.2.7 Effect of Vectors on Author Subgraph Mining
Author similarity matrices (denoted by XConcept and XContent), can be combined by α to form the contextual author similarity matrix, denoted by XTotal−α (Section 4.1.5). We study the impact of α on the effectiveness of our approach that is measured by PTextual and PConceptual.

As shown in Fig. 11, XTotal−α provides the best precision in both metrics when α is set to 0.6. It is found that the effectiveness of author subgraph mining stops growing at α=0.6. We notice that the decrease in performance becomes faster when α increases over 0.8. This can be explained by two rationales: First, the number of exploited concepts are limited to the current dataset, and Second, although both XContent and XCocept are influenced by the embedding process, none of them can be sacrificed in favor of the other.

Fig. 11. - 
Impact of $\alpha$α (Concept impact ratio) on effectiveness.
Fig. 11.
Impact of α (Concept impact ratio) on effectiveness.

Show All

SECTION 6Conclusion
In this paper, we propose a novel framework that processes short-text contents (e.g., tweets) to exploit subgraphs with highly correlated authors. To this end, we first need to link authors through computing the similarity weights, which results in the authors’ weighted graph. Primarily, the time-aware word embedding model considers temporal-textual evidence to infer the similarity rate between temporal splits in multiple dimensions (e.g., Monday and Tuesday in day dimension) and collectively computes the word vector representations. Subsequently, we obtain short-text vectors and author content vectors through combining the word vectors. Similarly, author concept vectors represent how every author is relevant to each of the short-text clusters.

We notice that compared to DBSCAN, the k-medoids clustering can better discover the concepts from tweet contents. We then fuse the content-based and conceptual author similarities to calculate the correlation weight between each pair of authors. Consequently, given the authors’ weighted graph, the stack-wise graph-cutting component in our framework can extract the maximum spanning trees, establishing the subgraphs with highly correlated authors. The result of the extensive experiments on a real-world microblog dataset proves the superiority of our proposed temporal-textual framework in short text author linking.

To conclude, the short-texts differ insignificance. Hence, to nominate the concepts from short-text clusters, we should not only consider the relevance between the tweet but also grant higher importance to the concepts of those with higher popularity. We leave this task for future work.