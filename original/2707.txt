Tensor factorization methods have recently gained increased popularity. A key feature that renders tensors attractive is the ability to directly model multi-relational data. In this work, we propose ParaSketch, a parallel tensor factorization algorithm that enables massive parallelism, to deal with large tensors. The idea is to compress the large tensor into multiple small tensors, decompose each small tensor in parallel, and combine the results to reconstruct the desired latent factors. Prior art in this direction entails potentially very high complexity in the (Gaussian) compression and final combining stages. Adopting sketching matrices for compression, the proposed method enjoys a dramatic reduction in compression complexity, and features a much lighter combining step. Moreover, theoretical analysis shows that the compressed tensors inherit latent identifiability under mild conditions, hence establishing correctness of the overall approach. Numerical experiments corroborate the theory and demonstrate the effectiveness of the proposed algorithm.
SECTION 1Introduction
Tensors are a natural generalization of matrices: whereas matrices model binary relations (e.g., user-item), tensors can model multi-relations (e.g., user-item-time). Tensor factorizations have long been invaluable tools for Chemometrics and Psycometrics, where fluorescence of chemical analytes and hidden traits of personalities are examined under the lens of low-rank tensor factorization. Tensors have also attracted considerable (and rapidly growing) attention from the computer science community, most notably in Data Mining (DM) and Machine Learning (ML), due to the emergence of large, multi-relational data sets. Pioneering works in this area include [2], where the authors applied tensor factorization methods to the problem of webpage link analysis, and [3] where tensor factorization methods were used to analyze social network graphs. Subsequently, tensor methods have been applied to recommender systems [4], [5], [6], [7], [8], topic modeling [9], [10], clustering [11], [12], and bioinfomatics [13], to name a few. We refer the interested readers to the overview papers [14], [15] for an in-depth review.

For DM applications, the large data volume poses a major challenge to tensor factorization methods. For canonical polyadic decomposition (CPD), a very popular tensor factorization model, classical applications involve mainly small datasets, where “heavy” algorithms like alternating least squares (ALS) or Gauss-Newton usually suffice. On the other hand, modern datasets in DM are often huge, thus performing CPD is challenging. To tackle this problem, several algorithms have been proposed recently, e.g., [16], [17], [18], [19], [20], [21]

GigaTensor [17] proposes a way to avoid “intermediate data explosion” in using ALS to perform CPD. DFacTo proposes to exploit the inherent parallelism in ALS and gradient descent (GD) iterations. However, these methods operate on the whole tensor in each iterative step, which is still prohibitive when the tensor is very large. As such, these works rely on distributed implementation, where data and (or) variables are stored in different machines – but the networking links can be a bottleneck. PARCUBE [21] puts forth a “divide-and-conquer” approach which offers scalable approximation of sparse data tensors using sparse factors, see Fig. 3 for an illustration. However, it cannot guarantee the recovery of the true underlying factors. That is, while the true latent factors of the original tensor may be identifiable, PARCUBE may fail to identify them, owing to the divide-and-conquer strategy it employs. This gives up one important advantage of tensors over matrices: the ability to recover true factors from the tensor – formally known as the essential uniqueness property [22]. Essential uniqueness means that, under mild conditions, the latent factor matrices of a low-rank tensor can be identified up to column scaling and permutation—which is drastically different from matrix factorization models, where linear transformation ambiguity is usually present.

From the application perspective, model uniqueness / identifiability is necessary for interpretability, as different model parametrizations are associated with different explanations of the data. On the theoretical front, uniqueness of CPD has been tapped to establish model identifiability—a key property in statistical learning—for some well-known models, e.g., the Gaussian mixture model (GMM), hidden Markov model (HMM), and the latent Dirichilet allocation (LDA), see e.g., [9], [23], [24]. In such cases, tensor factorization is employed as an estimation tool for model parameters (e.g., conditional probability mass functions), hence it is paramount that these parameters can be uniquely pinned down from data—which is what the essential uniqueness property of low-rank tensor decomposition offers.

With identifiability in mind, PARACOMP [20] proposes a similar technique as PARCUBE, where the sub-sampling step of PARCUBE is replaced by random compression. With this modification, strong identifiability guarantees for the rank-one tensor were established [20]. Albeit PARACOMP enables parallel computation, there are two issues that limit its application. First, the compression step involves multiplying dense matrices with tensors. Second, the combining step involves solving a large dense linear system of equations. These two issues can become a bottleneck when dealing with large tensors.

This is an undesirable compromise: the strong model identifiability guarantee of PARACOMP comes from using dense random compression matrices, which can become a computation bottleneck; whereas the scalability of PARCUBE comes from random sampling, which lacks theoretical identifiability guarantees. In this paper, we aim at bridging this apparent dichotomy. Specifically, we propose ParaSketch, a parallel algorithm for tensor factorization that enjoys the following properties:

Scalability: The proposed method employs sketching matrices to perform compression, which are much more computationally efficient compared to the Gaussian random matrices in PARACOMP.

Identifiability: With sketching matrices, we characterize conditions under which the compressed small tensors admit unique factorization, thus ensuring that one can correctly recover factors of the large tensor from those of the small tensors.

We emphasize that identifiability of the factorization of the compressed / sampled small tensors is crucial in this “divide-and-conquer” approach, because without it we cannot guarantee correctness of the final result.

By design, the ParaSketch algorithm is naturally parallelizable as the sketched tensors are independently factored. However, the huge computational gains of parallelization come with some degradation in the accuracy of the recovered factors of the original tensor. In order to elucidate this trade-off, we also introduce a new joint factorization approach as a baseline. This method processes all the sketched tensors in a centralized fashion. Numerical experiments confirm that the joint factorization approach provides a more accurate estimation of the latent factors of the original tensor, at the expense of higher computational complexity (and of course loss of parallelization).

A conference version of part of this work was presented at the 2018 SIAM International Conference on Data Mining [1]. Relative to [1], this journal version includes several additional contributions, namely i) extension of the ParaSketch framework from the CPD to the so-called block term decomposition (BTD, [25], [26]), which is well motivated in applications where one of the factor matrices is known to have colinear columns, see [25]; ii) more comprehensive experimental results and insights; and iii) the new joint factorization approach, which serves as a baseline to allow us to gauge the trade-off between accuracy and parallel computation.

We conclude this introduction by noting that sketching [27] has also been utilized to accelerate tensor decompositions in a different way. Specifically, in [28], sketching methods were used to solve the overdetermined linear least squares sub-problems in the ALS algorithm for CPD. Different from our method, the approach in [28] requires centralized processing of the big tensor, thus hindering its application for large-scale datasets. Additionally, while we focus on large scale unconstrained tensor factorization, it is worth mentioning that several recent works [16], [29], [30], [31] have proposed scalable algorithms for constrained tensor factorization.

Notation. The symbols ⊗,⊙,∘ denote the Kronecker product, Khatri-Rao product, and outer product, respectively. We use bold lowercase letters to denote vectors (e.g., x), and bold uppercase for matrices (e.g., A). The vectors are assumed to be column vectors, and (⋅)T denotes transposition. Bold symbols with underscore denote tensors, e.g., X–––. The symbols ∥⋅∥2 and ∥⋅∥F denote the ℓ2-norm and the Frobenius norm, respectively. We use f(t)=O(t) to mean that there exits constants C1 and C2, such that C1t≤f(t)≤C2t, and f(t)=Ω(t) to mean f(t)≥Ct for some constant C. More specialized notation will be introduced where needed in the main text.

SECTION 2Background
2.1 CPD and its Uniqueness Property
The CPD model is defined as
X–––=∑f=1FA(:,f)∘B(:,f)∘C(:,f),(1)
View Sourcewhere F is the smallest integer such that the factorization (1) is possible. This smallest integer is defined to be the rank of X–––. The symbol ∘ denotes outer product. We use A(:,f) to denote the fth column of A, and (:) means all the elements in the corresponding argument. Henceforth, we use the shorthand notation X–––=[[A,B,C]] to denote (1). To facilitate discussion, we will also make use of the matricized and vectorized forms of tensors. For matricization, we stack 1-D vectors of a tensor into a matrix. For instance, consider a 3-D tensor X–––∈RI×J×K, we denote
X1=[vec(X–––(1,:,:))⋯vec(X–––(I,:,:))](2a)
View Source
X2=[vec(X–––(:,1,:))⋯vec(X–––(:,J,:))](2b)
View SourceRight-click on figure for MathML and additional features.
X3=[vec(X–––(:,:,1))⋯vec(X–––(:,:,K))],(2c)
View SourceRight-click on figure for MathML and additional features.

where vec(⋅) is the usual vectorization operator, which stacks columns of a matrix into a long column vector. We call these mode-1, mode-2, and mode-3 matricization. Correspondingly, three different vectorized versions of the same tensor can be obtained as
x1=vec(X1),x2=vec(X2),x3=vec(X3),(3)
View Sourcewhich are permuted versions of one another. The matricization and vectorization operations are illustrated in Fig. 1.

Fig. 1. - 
Tensor matricization and vectorization. The matrix and vector correspond to $\boldsymbol{X}_3$X3 and $\boldsymbol{x}_3$x3.
Fig. 1.
Tensor matricization and vectorization. The matrix and vector correspond to X3 and x3.

Show All

It can be readily checked that if X––– admits a CPD model X–––=[[A,B,C]] of rank F, we have
X1X2X3=(C⊙B)AT=(C⊙A)BT=(B⊙A)CT.
View SourceRight-click on figure for MathML and additional features.The symbol ⊙ denotes the Khatri-Rao product, which is defined as
A⊙B:=[A(:,1)⊗B(:,1),…,A(:,F)⊗B(:,F)],
View Sourcewhere ⊗ denotes the Kronecker product. Consequently, for the vectorized form, we have x1=(A⊙C⊙B)1, x2=(B⊙C⊙A)1, and x3=(C⊙B⊙A)1, where 1 is an all-one vector of compatible size.

One central concept in this work is uniqueness of CPD, which is defined as follows.

Definition 1 (Essential uniqueness).
Given a tensor X––– of rank F, we say that its CPD is essentially unique if the rank-1 terms in the decomposition are unique, i.e., there is no other way to decompose X––– for the given number of terms. Note that there exists inherently unresolvable permutation ambiguity since permutation of rank-1 tensors does not change their sum. If X–––=[[A,B,C]], with A:I×F, B:J×F, and C:K×F, then essential uniqueness means that A, B, and C are unique up to a common permutation and scaling / counter-scaling of columns. In other words, if X–––=[[A~,B~,C~]], for some A~:I×F, B~:J×F, and C~:K×F, then there exists a permutation matrix Π and diagonal scaling matrices Λ1, Λ2, and Λ3 such that
A~=AΠΛ1,B~=BΠΛ2,C~=CΠΛ3,Λ1Λ2Λ3=I.
View Source

To present identifiability results of CPD, we need the definition of Kruskal rank of a matrix.

Definition 2 (Kruskal rank).
The Kruskal rank kA of an I×F matrix A is the largest integer k such that any k columns of A are linearly independent.

Clearly, kA≤rA:=rank(A)≤min(I,F). We next state the following classical result on identifiability of CPD.

Theorem 1 (Identifiability of CPD, [32]).
Given X–––=[[A,B,C]], with A:I×F, B:J×F, and C:K×F, if kA+kB+kC≥2F+2, then rank(X–––)=F and the decomposition of X––– is essentially unique.

Theorem 1 asserts that under some mild conditions, the factors of a tensor are identifiable.

2.2 Block Term Decomposition
A closely related model of CPD is the so-called PARALIND [25], where factors have colinear columns. This model is well motivated for analyzing real-world data, when it is known a priori that linear dependency exists due to, e.g., experimental design, or the underlying physics / chemistry – see [25] for some examples on analyzing fluorescence data of chemical reactions. It is worth pointing out that the PARALIND model is the same as a particular case of the so-called block term decomposition (BTD, [26])—and we adopt the name BTD in this work for conciseness. The rank-(R,R,1) BTD model is defined as
X–––=∑f=1F(AfBTf)∘cf,(4)
View Sourcewhere the matrices Af and Bf have rank R – hence the name “rank-(R,R,1) BTD”. In fact, the BTD can also be written as a special polyadic decomposition (PD) (but not necessarily minimal – hence not canonical), as follows:
X–––=∑f=1F(AfBTf)∘cf(5)
View Source
=∑f=1F(∑r=1RAf(:,r)Bf(:,r)T)∘cf(6)
View SourceRight-click on figure for MathML and additional features.
=∑f=1F∑r=1RAf(:,r)∘Bf(:,r)∘cf(7)
View SourceRight-click on figure for MathML and additional features.
=∑i=1FRAˆi∘Bˆi∘Cˆi,(8)
View Sourcewhere Af(:,r) is the rth column of matrix Af, and we have defined Aˆ=[A1,…,AF], Bˆ=[B1,…,BF] and Cˆ=[c11TR,…,cF1TR]. One can see that indeed BTD can be thought as a special PD, where the third mode has a special structure: each original column in the third mode of BTD is repeated R times in the corresponding PD representation.

Uniqueness of BTD cannot be established via uniqueness of CPD—note in particular that the condition in Theorem 1 always fails to work for BTD, due to the colinearity in the third mode. Instead, uniqueness of BTD was established in [26] using a generalized notion of Kruskal rank, named k′-rank. To introduce this definition, we consider a partitioned matrix A=[A1,…,AF], where each Ai is a matrix.

Definition 3 (k′-rank, [26]).
For a partitioned matrix A, the k′-rank, denoted as k′A (or k′(A)) is the maximal integer r such that any r submatrices of A form a set of linearly independent columns.

Theorem 2 (Theorem 4.4 of [26]).
Let (A,B,C) represent a decomposition of a tensor X––– into F rank-(R,R,1) terms. If
kC=Fandk′A+k′B≥F+2,(9)
View Sourcethen (A,B,C) is essentially unique.

Here essential uniqueness of (A,B,C) means that the F rank-(R,R,1) terms are unique; it is clear that within each of these terms, there is freedom for arbitrary nonsingular linear transformation. For instance, (AfBTf)∘cf=(AfMM−1BTf)∘cf for any nonsingular matrix M. Note that this is analogous to essential uniqueness of CPD, where the F rank-1 terms are unique; but due to the inner dimension being 1, there is only scaling/counter-scaling freedom in that case.

2.3 Compressed Tensor Factorization
We aim at developing “divide-and-conquer” strategies for large scale tensor factorization. PARACOMP [20] serves as the starting point of our work. PARACOMP uses parallel compression stages to create multiple small tensors, performing independent CPD on the small tensors, and combining the results to get CPD factors for the original large tensor. To illustrate the idea, we consider a 3-way tensor X–––∈RI×J×K. Reference [20] propose to compress the 3 modes of the tensor independently, with 3 matrices U∈RL×I, V∈RM×J and W∈RN×K, where L≤I, M≤J, and N≤K. The compression scheme is shown in Fig. 2, where U multiplies the tensor on the first mode, yielding a tensor of size L×J×K. Similar compression is performed on the second and third mode with V and W. Suppose the original tensor X––– admits a CPD model, with latent factors A, B, and C, i.e., x=(C⊙B⊙A)1, where the subscript 3 is omitted for conciseness, the compressed tensor y can be written as [33]
y=(W⊗V⊗U)(C⊙B⊙A)1=((WC)⊙(VB)⊙(UA))1.(10)
View SourceRight-click on figure for MathML and additional features.

Fig. 2. - 
Tensor compression with random projection.
Fig. 2.
Tensor compression with random projection.

Show All

To materialize the compression and factorization scheme, a key issue is under what conditions are the latent factors of the compressed tensor y identifiable. For CPD, the following theorem answers this question.

Theorem 3 (Identifiability of CPD under compression, [20]).
Let x=(C⊙B⊙A)1∈RIJK, where A is I×F, B is J×F, C is K×F, and consider compressing it to y=(W⊗V⊗U)x=((WC)⊙(VB)⊙(UA))1=(C~⊙B~⊙A~)1∈RLMN, where the mode-compression matrices U(L×I,L≤I), V(M×J,M≤J), and W(N×K,N≤K) are independently drawn from an absolutely continuous distribution. If
min(L,kA)+min(M,kB)+min(N,kC)≥2F+2,(11)
View Sourcethen A~, B~, C~ are almost surely identifiable from the compressed data y up to a common column permutation and scaling / counter-scaling.

Remark 1.
One may be wondering, why does uniqueness of factorization of the compressed tensors matter at all? In this “divide-and-conquer” algorithm design context, uniqueness of factorization of the compressed tensors is important mainly due to algorithmic correctness considerations. In short, if the compressed tensors do not admit unique factorization, then there is no principled way of combining results from these compressed tensors, see Section 3.2.

Despite the desirable identifiability guarantees offered by PARACOMP, its compression stage can be expensive since all the compression matrices are dense and unstructured. Overall, for a dense tensor, the computational complexity is O(min(L,M,N)IJK), which can be prohibitive even for moderate tensors, say I=J=K=1000.

It would be interesting then, to seek matrices that can perform compression on the tensor faster than the dense matrix-tensor multiplication as in PARACOMP. Now the question is, how to construct such compression matrices, so that one can still guarantee identifiability of the compressed tensors, with reasonable assumptions?

2.4 Fast Johnson Lindenstrauss Transform
To answer the above question, we introduce the following subsampled randomized Hadamard transform (SRHT), a fast version of the celebrated Johnson-Lindenstrauss transform (JLT, [34]). For an overview on JLT and its variants, please see [27]. The SRHT of a vector x∈Rn is Sx, where S is a specially structured matrix, S=PHD. The P is a random subsampling matrix, i.e., each row of P contains only one nonzero element (equal to 1) whose position is uniformly distributed. The H is a Hadamard matrix, which is recursively defined as
H2=[11−11],H2k=[H2k−1H2k−1−H2k−1H2k−1].(12)
View SourceRight-click on figure for MathML and additional features.Note that H is defined only for dimensions that are a certain power of 2. The D is a diagonal matrix with i.i.d. diagonal entries, each being a Rademacher random variable that takes values in {1,−1} with equal probability.

Proposition 1 (SRHT, [27], [35], [36]).
Let S=1mn√PHnD, where D is an n×n diagonal matrix with i.i.d. diagonal entries Dii=1 with probability 1/2, and Dii=−1 with probability 1/2. Matrix Hn is a Hadamard matrix of size n×n, where n is assumed to be a power of 2. The m×n matrix P samples m coordinates of an n-dimensional vector uniformly at random, where
m=Ω(ϵ−2(lnd)(d−−√+lnn−−−√ )2).
View SourceThen with probability at least 0.99, for any fixed U∈Rn×d with orthonormal columns
∥I−UTSTSU∥2≤ϵ.(13)
View SourceMoreover, for any vector x∈Rn, Sx can be computed in O(nlnm) time.

A proof of this result can be found in [35]. This proposition asserts that, with enough rows in the SRHT matrix S, the matrix SU preserves the dimension of the subspace spanned by the columns of U, as shown in (13).

SECTION 3Proposed Approach: ParaSketch for CPD
Our goal is to design an efficient parallel tensor factorization algorithm, by replacing the Gaussian random matrices in PARACOMP with SRHT matrices. Towards that end, we first address the identifiability issue of the CPD of the compressed tensors in Section 3.1; then, in Section 3.2, we discuss how to exploit the structure of SRHT matrices to lower the computation burden of the combining step. We analyze the complexity of the proposed method in Section 3.3, and compare it with that of PARACOMP. Finally, in Section 3.4, we propose a new baseline method.

3.1 Identifiability of Compressed Tensor CPD With SRHT Matrices
Theorem 4 (main result).
For x=(C⊙B⊙A)1, the CPD model of the sketched tensor y=(Sc⊗Sb⊗Sa)(C⊙B⊙A)1 is identifiable with high probability if
kA+kB+kC≥2F+2,(14)
View Sourceand Sa∈RL×I, Sb∈RM×J, Sc∈RN×K are all SRHT matrices, with number of rows
LMN=Ω(ϵ−2(lnkA)(kA−−−√+lnI−−−√ )2),=Ω(ϵ−2(lnkB)(kB−−−√+lnJ−−−√ )2),=Ω(ϵ−2(lnkC)(kC−−−√+lnK−−−−√ )2),
View SourceRight-click on figure for MathML and additional features.respectively, where ϵ∈(0,1) is the accuracy parameter in (13).

This result is similar to Theorem 3. The key difference is that we replace the dense compression matrices with sketching matrices. This has a tremendous impact on the computation complexity, see Table 1 and Fig. 5. Moreover, the argument used in [20] to establish unique factorization of compressed tensors does not apply here, as it hinges on compression matrices drawn from absolutely continuous distributions (c.f. Theorem 3), which is not the case for SRHT matrices. Before proving Theorem 4, several remarks are in order.

TABLE 1 Time Complexity Comparison


Fig. 3.
Schematic of PARCUBE [21].

Show All


Fig. 4.
The NMSE of estimating A.

Show All


Fig. 5.
Run time comparison for CPD.

Show All

Remark 2.
In Theorem 4, we assume dimensions I,J,K are powers of 2. In practice, one can simply pad the tensor with slabs of all zeros if they are not. The redundant dimensions can be removed in the final factors after combining.

Remark 3.
In many cases of practical interest, the tensor data is “long”, in the sense that one of the dimension is much larger than the others. For instance, the spectrogram tensor in analyzing EEG or MEG data (see e.g., [37]) has three modes: time, frequency, and channels. The number of frequency bands and number of channels are often on the order of hundreds, whereas the time index can be in the order of hundreds of thousands, due to the high temporal resolution and long duration of EEG and MEG recordings. In such a case, one might be interested in analyzing the frequency mode and channel mode factors, but not the full temporal factor – which is very long, and potentially overly detailed. Our method can be easily adapted to this case. Assuming we are interested in compressing the third mode, we perform sketching on the third mode. For this setting, we have the following corollary.

Corollary 1.
For tensor x=(C⊙B⊙A)1, the CPD of the partially sketched tensor y=(Sc⊗I⊗I)(C⊙B⊙A)1 is identifiable with high probability, provided kA+kB+kC≥2F+2 and Sc∈RN×K is SRHT with
N=Ω(ϵ−2(ln(kC))(kC−−−√+ln(K)−−−−−√ )2),
View Sourcerows, where ϵ∈(0,1) is the accuracy parameter in (13).

To prove Theorem 4, we provide the following lemma. Consider a sketching matrix S∈RL×I and a matrix A∈RI×F, we show the following result on the Kruskal rank of the sketched matrix.

Lemma 1.
Let S∈RL×I be an SRHT matrix with
L=Ω(ϵ−2(lnkA)(kA−−−√+lnI−−−√ )2),(15)
View Sourcerows. Then, with high probability, k(SA)=kA.

The proofs of Lemma 1 and Theorem 4 can be found in the appendix, which can be found on the Computer Society Digital Library at http://doi.ieeecomputersociety.org.ezproxy.auckland.ac.nz/10.1109/TKDE.2020.2982144.

3.2 Combining the Results
The combining process generally follows [20], [21], but we develop special treatments to fully exploit the structure of SRHT matrices to get away with much lighter computations. We illustrate the combining process using factor A, and the other factors can be combined following the same procedure. To avoid clutter, we denote the sketching matrix for the first mode as S, instead of Sa as above.

Suppose we spawn T processes to compress the tensor, with each compressed tensor denoted as Yt,t∈{1,…,T}. We next perform CPD on each of these small tensors in parallel, and obtain [At,Bt,Ct],t∈{1,…T}. Then we can write
At=StAΠtΛt,(16)
View SourceRight-click on figure for MathML and additional features.which means that we still have individual permutation (Πt) and scaling (Λt) ambiguities for each t. This is because factoring each small tensor independently leads to possibly different scaling and permutation of the (compressed) true factors, as indicated by the subscript t in (16). Only after reconciling these different permutations and scalings can we combine the results to recover the true factors [A,B,C].

To resolve permutation ambiguity, we keep two common rows in each compression matrix S, denote this part as S¯¯¯¯, then the corresponding part in factor A is
A¯¯¯¯t=S¯¯¯¯AΠtΛt.(17)
View SourceDividing each column of A¯¯¯¯t by its largest magnitude element, we get
Aˆt=S¯¯¯¯AΠtΛ.(18)
View SourceOne can see that scaling ambiguity is fixed: each replica now has a common scaling matrix Λ. Next, we will match each replica to the first copy Aˆ1, i.e., we will solve the following problem:
minΠ∈PF  ∥Aˆ1−AˆtΠ∥2F,(19)
View SourceRight-click on figure for MathML and additional features.where we use PF to denote the set of permutation matrices of size F×F. With a little manipulation, problem (19) is equivalent to
maxΠ∈PFtr(AˆT1AˆtΠ).(20)
View SourceProblem (20) is known as the Linear Assignment Problem (LAP), and can be solved efficiently by the Hungarian algorithm [38].

With a little abuse of notation, we denote the resulting permutation matrix from (20) as Πt,t∈{2,…,T}. We use these Πt to permute the results At in (16), and get the following:
Aˇt=StAΠΛt.(21)
View SourceNow the only remaining ambiguity is scaling Λt, which can be removed by dividing (elementwise) Aˇt with one of the common rows. After this step, we get
Aˇt=StAΠΛ.(22)
View SourceAfter fixing scaling and permutation, we can combine the results as follows:
⎡⎣⎢⎢⎢⎢⎢Aˇ1Aˇ2⋮AˇT⎤⎦⎥⎥⎥⎥⎥=⎡⎣⎢⎢⎢⎢S1S2⋮ST⎤⎦⎥⎥⎥⎥AΠΛ.(23)
View SourceBy SRHT definition, we have Si=PiHD, and we can write
⎡⎣⎢⎢⎢⎢⎢Aˇ1Aˇ2⋮AˇT⎤⎦⎥⎥⎥⎥⎥=⎡⎣⎢⎢⎢⎢P1P2⋮PT⎤⎦⎥⎥⎥⎥HDAΠΛ:=PˆHDAΠΛ,(24)
View SourceRight-click on figure for MathML and additional features.where we have defined Pˆ as the concatenation of Pt's. It can be easily seen that inverting Pˆ is simple, since each Pt is a sampling matrix, i.e., each row of Pt contains only one nonzero value. After inverting Pˆ, we can invert H using the fast inverse Hadamard transform, and invert D with a simple scaling operation. The most expensive part is the inverse Hadamard transform operation, which costs O(Iln(I)F).

The overall algorithm is summarized in Algorithm 1. The combining step is presented in Algorithm 2.

Algorithm 1. ParaSketch
Input: Data tensor X–––, number of replicas T, compression dimension L,M,N, CPD rank F.

Output: Factors A,B,C.

Perform T compressions as described in Section 3.1, yielding small tensors {X1,X2,…,XT};

Perform CPD on the compressed tensors, yielding {{A1,B1,C1},…,{AT,BT,CT}};

Perform combining procedure to get A,B,C as described in Algorithm (2).

Algorithm 2. Combining Procedure
Input: {At},t∈1,…,T

Output: A

Determine permutation matrices for each compressed small tensor by solving (19), and permute the factors in (16) to obtain (21);

Resolve scaling ambiguity by dividing entries in each column with values in a common row;

Invert Pˆ, H, D in (24) in turn.

Remark 4 (Number of replicas needed).
By our construction, each compression shares H and D. To ensure inversion in (24) can be carried out, we need each row index in matrix HDAΠΛ to be sampled at least once. That is, Pˆ cannot have a column that is all zero. Since all the sampling is performed independently and uniformly, this sampling procedure corresponds to the famous Coupon Collector's Problem. In this problem, it is known (see e.g., [39]) that one needs to try O(nln(n)) times in order to collect n coupons. In our problem, this means that the number of replicas T is in the order O(Iln(I)/L).

Remark 5 (Data compression ratio).
Suppose the original large data tensor has size I×I×I, and each mode is compressed to dimension L, then data compression ratio is I3/(TL3). As argued above, we need T to be T=ηIln(I)/L, where η is some constant. In this case, the compression ratio can be expressed as I2/(ηL2lnI). In our simulations, we observed that once η exceeds 1.5, we get exact recovery (up to numerical precision) of the factors in noiseless cases, suggesting that the compression ratio can be very high when L≪I.

3.3 Complexity Analysis
To simplify analysis, we focus on the first mode. In PARACOMP, to ensure the first mode factor of the original tensor can be recovered from that of the compressed tensors, one needs the number of replicas to be T≥I/L. Note that this means PARACOMP needs a factor of ln(I) fewer replicas. However, the cost of multiplying a dense compression matrix of size I×L with a dense tensor of size I×J×K is O(LIJK). In our construction, applying the compression costs O(IJKlnL). The total number of rows in PARACOMP is O(I), hence the total computations are O(I2JK). The complexity of ParaSketch is dominated by the Hadamard transform, which takes time O(IlnI) for a length I vector, and O(IJKlnI) for the whole tensor.

In the final combining procedure, our method costs O(IFlnI), while PARACOMP costs O(I3) in general. Note that in principle one can invoke the property that a random Gaussian matrix is approximately orthogonal for large I, and replace the inversion with transpose, resulting in lower cost of O(I2F)—but solution accuracy deteriorates if such an approximation is adopted. Time complexity comparison of PARACOMP and ParaSketch is summarized in Table 1, see Fig. 5 for numerical experiment results.

In terms of space (memory) complexity, both algorithms require storage of the compressed tensors, as well as the compression matrices for use in the combining step. To simplify analysis, we focus on a tensor of size I×I×I. For PARACOMP, the storage requirement for the compressed tensors is O(TPARACOMPL3), and O(TPARACOMPLI) for the compression matrices. For ParaSketch, the storage of compressed tensors is O(TParaSketchL3). The Hadamard matrices in compression do not need to be stored, and the total storage in compression is due to {Pt} and D, which incur O(TParaSketchL) storage requirement.

The difference in TPARACOMP and TParaSketch makes comprehensive comparison of the respective memory footprints challenging. Here we let TPARACOMP=⌈I/L⌉, since this is the minimal number of replicas needed to recover the factors of the original tensor in PARACOMP, and TParaSketch=O(⌈(IlnI)/L⌉), since this ensures successful recovery w.h.p, as discussed in Remark 4. With these choices, we get the storage complexity O(IL2+I2) for PARACOMP, and O(IL2lnI+IlnI) for ParaSketch. Further, if we assume the compression dimension to be L=I–√, we get space complexity O(I2) for PARACOMP, and O(I2lnI) for ParaSketch.

Remark 6.
If one aims at achieving full parallelism when factoring all the compressed tensors, the proposed ParaSketch method takes more computational resources than PARACOMP: a factor of (lnI) more storage is needed, as well as parallel processors. However, when resources are limited, one can instead instantiate these small tensors in batch, and discard them after factorization. In Section 5, we show that factoring small tensors takes a small fraction of time compared to compression and combining in PARACOMP. Hence we can afford to factor these small tensors in sequential batches (and perform parallel factorization within each batch), so long as the compression and combining time are reduced dramatically—which is exactly the case of ParaSketch (c.f. Fig. 5).

3.4 Joint Factorization of Sketched Tensors
We next explore a new baseline method, to better understand the trade-off between parallelism speed and accuracy of the final result. After the compression process, the randomly sketched tensors can instead be factored jointly in order to obtain the latent factors of the original tensor. The joint factorization is solved as a whole, hence the benefits of parallel computing as in ParaSketch are lost—but the resulting factors will be more accurate than those of ParaSketch, as we shall see in Section 5. The reason for this is that one exploits all sketched data simultaneously, and avoids the noisy division and permutation-matching which are fallible in low SNR and/or high-rank scenarios. The joint factorization formulation is as follows:
minA,B,C∑t=1T∥Yt−( (Sat A)⊙(Sbt B)) (Sct C)T ∥2F,(25)
View Sourcewhere Yt is the mode-3 matricization of the tth sketched tensor, and Sat, Sbt and Sct are the sketching matrices of the tth sketched tensor in the first, second and third mode, respectively. This optimization problem is clearly nonconvex since the unknowns A,B,C are multiplied together. However, we can adopt an alternating minimization (AltMin) approach, which updates each factor while fixing the other two in a cyclic fashion, enjoys a closed form optimal solution for each individual factor.

In order to update the matrix C, we exploit one of the Kronecker product properties to re-write (25) as
minA,B,C∑t=1T∥yt−(Sct⊗((SatA)⊙(SbtB)))vec(CT)∥2F,(26)
View Sourcewhere yt is the vectorization of Yt. In the AltMin approach, instantiating the Kronecker product in (26) represents a computational challenge. The size of the Kronecker product is (LMN×FK). In addition, similar problems need to be solved for A and B at each iteration.

In the joint factorization approach, the original factors are estimated directly. Therefore, there is no need for reconciling the permutation and scaling as in the original ParaSketch procedure. While the joint factorization approach constitutes the direct solution to obtain the original factors given the sketched tensors, it represents a heavy computational burden. On the other hand, it is often more accurate in terms of estimating the factors.

SECTION 4ParaSketch for BTD
We now show how to extend the ParaSketch framework to BTD. Let the rank of Af and Bf to be R for all f=1…F. To apply the compression strategy presented in Section 2 to a tensor that conforms to a BTD model, we first note that
vec(X–––)=∑f=1Fvec((AfBTf)∘cf)(27)
View SourceRight-click on figure for MathML and additional features.
=∑f=1Fcf⊙((Bf⊙Af)1R).(28)
View SourceThen the compression can be written as
y=(W⊗V⊗U)vec(X–––)(29a)
View Source
=∑f=1F(W⊗V⊗U)(cf⊙((Bf⊙Af)1R))(29b)
View SourceRight-click on figure for MathML and additional features.
=∑f=1F(Wcf)⊙((V⊗U)(Bf⊙Af)1R)(29c)
View SourceRight-click on figure for MathML and additional features.
=∑f=1F(Wcf)⊙[((VBf)⊙(UAf))1R],(29d)
View Source

where we applied the mixed product rule: (A⊗B)(C⊙D)=(AC)⊙(BD) in obtaining (29c) and (29d). We can see that the compressed tensor admits a BTD model with factors being compressed, as follows:
Y––=∑f=1F(((UAf)(VBf)T)∘(Wcf)).(30)
View Source

To characterize the identifiability condition of the compressed BTD model, we first introduce the following lemma.

4Lemma 2.
Let S be an SRHT matrix, and A=[A1,…,AF] be a partitioned matrix, where each block has dimension I×R. Suppose S has dimension L×I, where L satisfies
L=Ω(ϵ−2ln[k′AR](k′AR−−−−√+ln(I)−−−−√ )2),(31)
View Sourcethen with high probability, we have
k′(SA)=k′A.(32)
View Source

The proof is relegated to the appendix, available in the online supplemental material. With this lemma in place, we are ready to state the following theorem.

4Theorem 5.
Let X–––∈RI×J×K admit a rank-(R,R,1) BTD. If we compress this tensor by SRHT Sa∈RL×I, Sb∈RM×J, and Sc∈RN×K, where the compression dimensions satisfy
LMN=Ω(ϵ−2ln[k′AR](k′AR−−−−√+ln(I)−−−−√ )2)=Ω(ϵ−2ln[k′BR](k′BR−−−−√+ln(J)−−−−√ )2)=Ω(ϵ−2ln[kC](kC−−−√+ln(K)−−−−−√ )2),
View SourceRight-click on figure for MathML and additional features.and in addition
min(N,kC)=F(33)
View Source
k′A+k′B≥F+2,(34)
View Sourcethen with high probability (SaA,SbB,ScC) is essentially unique for the rank-(R,R,1) BTD of Y––.

The proof of this theorem can be found in the appendix, available in the online supplemental material.

Unlike CPD, for BTD, the roles played by the three different modes are different. The first and second modes are “coupled” since they form low-rank matrices within each term of the BTD. For ease of implementation, we focus on a special scenario, where one of the “coupled” modes is small, and we compress the larger mode—assuming that to be the first mode. Note that compression on both the first and second modes together will result in a low-rank matrix recovery problem in the combining step, which is computationally challenging and devising a computationally efficient algorithm for this step is of interest in its own right (and beyond the scope of this work). We therefore consider compressing the first and third modes. Recall that in Section 3.2, we align factors estimated from different replicas by introducing a small number of anchor rows in factor A. Due to the special structure of BTD, we introduce anchor rows in the third mode, since it is uniquely identified up to column permutation—whereas the first and second modes are not.

Suppose we create T replicas with sketching matrices {Sat,Sct}Tt=1, i.e.,
yt=(Sct⊗I⊗Sat)vec(Xt).
View SourceBy applying the nonlinear least square (NLS) algorithm proposed in [40] on yt, we obtain {M˜f,t}Ff=1 and Ct˜, where M˜f,t is the low-rank matrix product of the first and second mode factors in the fth term of replica t. We let each Sct have the first two rows in common, so that we can reconcile the permutations as in Section 3.2. For the C factor, we subsequently resolve the scaling ambiguity as in Section 3.2. The procedure is exactly the same as in Algorithm 2. Once we align the permutation of C, we can permute the corresponding low-rank matrices {M˜f,t}Ff=1 since the permutation is common within each replica, by virtue of the essential uniqueness property of BTD.

Recovery of the low-rank matrix requires more care. Since there is inherent scaling ambiguity between columns of C and the corresponding low-rank matrix, we need a method to align the scaling of the low-rank matrices from different replicas. For this purpose, we let Sat's have the first row in common. By this design, the (1, 1) elements of the low-rank matrices from different replicas are common, and dividing by this element unifies the scaling of different replicas, and we denote the result as {Mˆf,t}Ff=1

After aligning permutation and scaling, for each low-rank matrix AfBTf, we have the following problem:
minAf,Bf∑t=1T∥∥Mˆf,t−SatAfBTf∥∥2F.(35)
View SourceLet Sa=[Sa1;…;SaT] be a vertical concatenation of Sat's, and Mˆf=[Mˆf,1;…;Mˆf,T] be a vertical concatenation of Mˆf,t's. Problem (35) has a closed-form solution
AfBTf=((Sa)TSa)−1/2URΣRVTR,(36a)
View Source
UΣVT=((Sa)TSa)−1/2(Sa)TMˆf(36b)
View Source

where
((Sa)TSa)−1/2=D−1H−1((Pa)TPa)−1/2H−1D−1.(37)
View SourceRight-click on figure for MathML and additional features.Equation (36b) is the Singular Value Decomposition (SVD), and UR (VR) collects the R columns of U (V) that correspond to the largest R singular values, while ΣR is Σ with the remaining singular values (other than the largest R singular values) replaced by zeros. Symbol Pa denotes the vertical concatenation of Pat since Sat=PatHD.

The right hand side (RHS) of (36a) may seem intimidating on first sight, but it is in fact easy to compute. By the design of Pat, the product ((Pa)TPa) is a diagonal matrix. Then one can see that after computing the URΣRVTR term, we can perform the multiplication from right to left, and the multiplications are either an inverse Hadamard transform, or multiplying with a diagonal matrix, which are all light-weight operations, thanks to the sketching scheme we proposed. Similarly, we can form the RHS of (36b) from right to left, enjoying the light computation of the inverse Hadamard transform and multiplication with diagonal matrices.

In summary, we have shown that for the BTD model, similar sketching and combining techniques can be applied, with identifiability guarantees. The combining procedure for the coupled modes is different, and we summarize it in Algorithm 3. We remark again that we aim at recovering only the product terms AfBTf in Algorithm 3, since there is inherent linear transformation ambiguity between Af and BTf, per the definition of BTD.

SECTION Algorithm 3.Combining Procedure for Coupled-Modes of BTD
Input: {M˜f,t},f=1,…,F,t=1,…,T

Output: {AfBTf},f=1,…,F

Permute factors using permutation matrices determined from C factor;

Resolve scaling ambiguity by dividing the (1, 1) entries, yielding {Mˆf,t},f=1,…,F,t=1,…,T;

For each f=1,…,F, solve for AfBTf according to (36a).

SECTION 5Numerical Results
In this section, the performance of our presented approach for different problem settings is measured. For CPD, we compare the proposed ParaSketch with PARACOMP. PARCUBE is excluded from comparison since it is designed to approximate a tensor, not to recover the true underlying factors. We also include direct CPD (without any compression) as a baseline, for comparison purposes. For the case of BTD, we will compare with the NLS [40] method, which is the state-of-the-art algorithm for BTD. We also note that ParaSketch is a meta-algorithm that can use any tensor decomposition algorithm as a building block for decomposing the individual tensor sketches. We conduct our experiments on a Linux workstation with 16 parallel processors, and 128 GB RAM.

We first focus on CPD. Synthetic data is generated to test factor estimation accuracy and run time performance. Specifically, we generate factors {A,B,C}, and use them to synthesize (see (1)) tensor X–––, which is then fed into different algorithms to estimate {A,B,C}. For factor estimation, the performance is measured by normalized mean-squared-error (NMSE), which is defined as
NMSE=minπ∈Π,cf∈{1,−1}1F∑f=1F∥∥∥∥A(:,f)∥A(:,f)∥2−cfAˆ(:,πf)∥Aˆ(:,πf)∥2∥∥∥∥22,(38)
View SourceRight-click on figure for MathML and additional features.where Aˆ is the estimated factor, A is the ground truth factor, Π is the set of all permutations of the set {1,2,…,F}, and πf is the corresponding f after applying the permutation π. The permutation π is needed due to the inherent permutation ambiguity of CPD. Finding the best π is the celebrated linear assignment problem, which can be efficiently solved by the Hungarian algorithm. Symbol cf is to resolve sign ambiguity.

5.1 Comparing ParaSketch to Baselines
In the first experiment, we generate noisy data with additive Gaussian noise. The dimensions of the considered tensor are I=512, J=512, K=512, and F=10. The dimensions of each sketched replica are fixed to L=64, M=64, N=64. The noise power is set to σ=0.01. Note that this is a dense tensor, which has about 0.13 billion entries. Also note that we create moderate tensors to facilitate comparison with the direct CPD, which doesn’t scale well for dense tensors. The number of common rows in each replica is set to 3. The factor A is generated in MATLAB as randn(I,F), and B, C are generated in a similar fashion. We vary the number of replicas T such that η=TLIln(I) takes values between 1 and 3.5. To factor the compressed small tensors, we use the same CPD solver provided in the N-way toolbox1 [41] for both ParaSketch and PARACOMP. For each parameter configuration, 50 randomly generated problem instances are created, and the result is averaged across these instances.

The results are shown in Fig. 4. As can be seen, the proposed ParaSketch performs as well as PARACOMP once enough replicas are employed. As we point out in Remark 5, the data compression ratio is I2ηL2lnI. In this experiment, we have I=512,L=64, the compression ratio is I2ηL2lnI=3.55 when η=2. This means when only 28 percent of the original amount of data is used for factorization, an accurate estimation of the factors is achieved. Note that the proposed ParaSketch achieves similar factor estimation performance as PARACOMP, at much lighter computation cost (see Table 1 and Fig. 5).

In the second experiment, we compare the run time performance of the proposed algorithm against baseline methods. Specifically, we vary dimension I and fix J=256, K=256 when generating data. Accordingly, the compression dimensions are set to M=N=64, and L=I/8. The rank is set to F=10, and the number of common rows is set to 3. For ParaSketch, the ratio is fixed to η=1.8. Recall that the number of replicas for ParaSketch will be TParaSketch=η(IlnI/L). For PARACOMP, we set TPARACOMP=3I, since we observed that this gives good factor estimation accuracy. We repeat each experiment 20 times with different randomly generated data, and report the average run time. In addition to the total run time, we also record and report the part of time spent on compression and combining stages of ParaSketch and PARACOMP. We include two other strong baselines for comparison: the CP-OPT method proposed in [42] and the NLS method from [40]. The CP-OPT method is a first-order method, aiming at tackling large scale tensor factorization. The NLS method [40] exploits structure of tensor factorization to derive an efficient implementation of Levenberg- Marquardt algorithm [43]. Both these two methods have well-documented success in the literature. For this experiment, we use a stopping criterion of 10−6 tolerance for all the algorithms: if the cost function changes less than this threshold, the algorithms are terminated, otherwise the algorithm is run till the maximum number of steps set by the authors of each algorithm.

The run time results are shown in Fig. 5. As we can see, the proposed ParaSketch scales much better compared with PARACOMP. More importantly, we can see that for PARACOMP, most of the time is spent on the compression and combining part. This highlights the merit of the propose method: For large tensors, compression and combining can be a performance bottleneck for PARACOMP, and using SRHT matrices to perform compression can greatly alleviate this issue. Also note that the runtime results in Fig. 5 verify the complexity analysis presented in Table 1. The methods CP-OPT and NLS require longer running time compared with the proposed ParaSketch especially when the tensor gets large. This comparison highlights the necessity of exploiting parallel processing for large scale tensor factorization.

In Fig. 6, we show factor estimation performance under different noise power levels. The noise level is quantified by signal to noise ratio (SNR), which is defined as
SNR=10log10(∥X–––∥2F(# of elements in X–––)×σ2),(39)
View SourceRight-click on figure for MathML and additional features.where σ2 is the variance of Gaussian noise added to each entry of the tensor. The dimensions of the tensor are shown in the figure. As expected, the estimation performance improves as the SNR get larger (i.e., relatively weaker noise). Again, one can observe that the proposed method achieves similar estimation performance as PARACOMP.


Fig. 6.
Factor estimation performance under different SNR.

Show All

5.2 Comparison With Joint Factorization Approach
In order to identify the advantages and disadvantages of the ParaSketch approach compared to the joint factorization approach, we conduct several simulations assessing the estimation accuracy of the original factors as well as the computational performance.

First, we test the ability of the ParaSketch algorithm and the joint factorization approach to identify the low-rank factors of the original tensor. We measure and report the averaged normalized mean squared error (NMSE) in estimating the low-rank factors over 100 simulations. In each simulation, the size of the original tensor is 512×128×32 where the rank of the underlying factors is 5. A total of 128 sketched tensors of size 64×16×4 are created. Then, both approaches are used to identify the factors of the original tensor. From Fig. 7, it is clear that the joint approach is more accurate in identifying the factors when the noise level is high. The ParaSketch algorithm requires higher SNR in order to identify the low-rank factors with an acceptable accuracy.


Fig. 7.
Factor estimation performance under different SNR. PS – ParaSketch; J – Joint factorization.

Show All

However, run time for the algorithms shows large difference. For this experiment, the ParaSketch method factors the sketched tensors on all the 16 parallel processors. The average time needed by the joint factorization approach is 150.0 seconds, and the average time needed by ParaSketch to factor the sketched tensors and combine the resulting factors is 1.0 seconds—a 150 times difference. Note also that both algorithms offer higher accuracy in estimating factor matrices of the smaller modes. An explanation for this is that the conditional least squares updates for the smaller modes are more over-determined, so assuming reasonable estimates for the other modes gives a higher ‘coherent combining’ advantage for these modes.

In the next experiment, we compare the accuracy in estimating the original tensor factors using both approaches for different ranks. Again, we average the NMSE of the estimated low-rank factors over 100 simulations. The size of the original tensor is 64×64×32 where the noise power is set such that the signal to noise ratio is 70 dB. A total of 64 sketched tensors of sizes 16×16×8 are created, and then used to estimate the factors of the original tensor. In Fig. 8, the estimation performance is depicted for both methods. It can be seen that the considered approaches can accurately identify the low-rank factors when the rank of the factors is relatively small. However, when the rank of the original tensor grows, the joint factorization approach is more robust in identifying the low-rank factors than the ParaSketch approach. This confirms our intuition that using parallel processing and combining results from multiple processors can introduce some accuracy degradation, as discussed in Section 1. On the other hand, Fig. 9 depicts the average time required by both methods to estimate the low-rank factors from the sketched tensors. The ParaSketch approach is a least an order (and sometimes close to two orders) of magnitude faster than the joint factorization approach for all values of rank.

Fig. 8. - 
Factor estimation performance for different ranks. PS – ParaSketch; J – Joint factorization.
Fig. 8.
Factor estimation performance for different ranks. PS – ParaSketch; J – Joint factorization.

Show All

Fig. 9. - 
Computational time of ParaSketch and joint factorization for different ranks.
Fig. 9.
Computational time of ParaSketch and joint factorization for different ranks.

Show All

5.3 Simulation for BTD
We next perform simulations to evaluate the proposed method for BTD. As mentioned earlier, the state of the art algorithm for BTD is NLS. We compare the proposed method with directly applying NLS to the large tensor. The subproblems in the proposed method are also solved with the same NLS method. In this experiment, the initialization method for NLS is set to generalized eigenvalue decomposition, as implemented in [44]. We set the dimensions to be J=256,K=1024, and vary I. The compression dimensions are L=I/8 and N=128. The rank parameters are set to F=R=4, i.e., there are four “block terms”, within which the first and second modes form rank-4 matrices. Zero-mean Gaussian noise with standard deviation σ=10−3 is added to the elements of the generated tensor.

The result for run time comparison is shown in Fig. 10. As can be seen, the run time of the proposed method scales more favorably than directly applying NLS on the large tensor.


Fig. 10.
Run time comparison for BTD.

Show All

BTD brings some complications for testing factor estimation performance: the first and second modes have linear-transformation ambiguity within each block term, which means e.g., Aˆf and Bˆf are not essentially unique as opposed to CPD. In order to calculate NMSE as defined in (38), we adopt the following strategy: we multiply each pair of (Aˆf, Bˆf) and vectorize it into a vector of size IJ×1, which are then collected into a matrix of size IJ×F. This matrix is compared, using metric (38), with the matrix produced from the ground truth factors with the same “multiplying-vectorization” process. The results are shown in Fig. 11. As expected, notwithstanding some degradation in accuracy compared to applying NLS directly, the proposed method estimates factors with high accuracy.


Fig. 11.
NMSE comparison for BTD.

Show All

5.4 Real World Data Mining
In the last experiment, we test our algorithm on a dataset of taxi trajectories in Beijing during the Chinese New Year (a major Chinese festival) week of 2008 [45]. We construct the tensor by discretizing the latitude and longitude to a 128×128 grid, and considering the time as the third dimension of the tensor. Therefore, each element in the tensor is an integer that represents the number of taxis that were at the corresponding area at a specific moment in time. Then, the ParaSketch algorithm for CPD is used to find the low rank components of the 128×128×8980 tensor. We use 200 replicas of dimensions 16×16×256 in order to perform ParaSketch. We set the rank parameter in ParaSketch to 5 in this experiment.

In Fig. 12, we visualize the resulting most significant factor, which is defined as having the largest sum of squared ℓ2-norm of the three vectors corresponding to a rank-1 term. In the top sub-figure, the three markers correspond to the three largest absolute values of the rank-1 matrix formed by multiplying the first and second mode vectors. After cross-examining with the map of Beijing, we discover that these locations correspond to famous attractions: Temple of Heaven, Longtan Lake Park, and Happy Valley Amusement Park. The high intensity of taxi activities highlights their popularity during this festive period. In the bottom sub-figure, we show the magnitude of corresponding vector in the third mode, which contains temporal information. As expected, high activities are observed during the day, and relatively low activities in the night. More interestingly, we see that there is a decline in taxi activity for the last 3 days. Inspection of the exact dates reveals that these days are the New Year's Eve (Day 5) and Spring Festival (Day 6, 7)—those are days for family reunions, hence fewer people on the road.


Fig. 12.
Beijing taxi trajectory data analysis. Top: magnitude of the rank-1 matrix, formed by multiplying the first and second mode vectors of the most significant rank-1 factor of the tensor, visualized on the map. The red circles identify the three locations with the largest magnitude. Bottom: the corresponding third mode, showing temporal variation.

Show All

SECTION 6Conclusion
We propose an algorithm to facilitate parallel CPD and BTD on large tensor data. The approach provides great acceleration over existing prior art, rendering itself suitable for much larger datasets. Our analysis establishes the correctness of the proposed algorithm, i.e., identifiability of the latent factors of the compressed tensors and the original uncompressed tensor, for both CPD and BTD. We also characterized recovery conditions for the proposed approach, i.e., the number of rows in the sketching matrices and the number of replicas needed, to ensure recovery of the factors of the large tensor data. Numerical results on synthetic as well as real world data confirm the efficacy of the proposed method.