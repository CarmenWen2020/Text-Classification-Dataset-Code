Modern industrial and data center networks adopt multipath TCP (MPTCP) to achieve low communication latency. To make full use of multiple paths, MPTCP divides one TCP connection into multiple subflows. Due to the connection-level ACK blocking, however, MPTCP suffers from the traffic burstiness, which leads to large queueing delay, frequent packet drops, and even TCP timeout. To mitigate the traffic burstiness caused by ACK blocking, we propose MPTCP-RB (Multipath TCP with Reduced Burstiness), which interleaves consecutive packets across multiple subflows, instead of assigning consecutive packets to individual subflows. MPTCP-RB is implemented as a loadable Linux kernel module, working on any traditional MPTCP-based schemes without any other modifications on existing network stacks. The testbed and NS2 experiments show that, MPTCP-RB reduces the average flow completion time by up to 91% compared with the state-of-the-art MPTCP-based schemes.

Previous
Next 
Keywords
Data center networks

MPTCP

Traffic burstiness

1. Introduction
Nowadays, network latency is essential to the performance of distributed applications (Alizadeh et al., 2011, Montazeri et al., 2018), the energy consumption (Jiang et al., 2020), and the user experiences (Jiang et al., 2021). Especially in the case of data center, parallel computing and storage applications predominate over others. Due to their distributed nature, the communication among thousands of servers in data centers plays an important role and has significant impact on application performance. For example, the network latency between communication pairs can take over 50% of job completion time in Facebook’s Hadoop cluster (Chowdhury et al., 2011) or even up to 85% of end-to-end application latency in Memcached system (Huang et al., 2016).

Massive literature works (Alizadeh et al., 2011, Montazeri et al., 2018, Li et al., 2019, Hu et al., 2020a, He et al., 2016, Bai et al., 2017, Lee et al., 2015) are presented to reduce network latency, i.e., flow completion time (FCT). Unfortunately, these protocols do not consider the features of modern data center networks (DCNs) in their designs, which have multiple paths and high-aggregate bandwidth for end-host pairs (Al-Fares et al., 2008, Chen et al., 2019, Greenberg et al., 2009, Farrington and Andreyev, 2013). Although state-of-the-art load balancers are proposed (Hopps, 2004, Vanini et al., 2017, Alizadeh et al., 2014, Zhang et al., 2017, Katta et al., 2016) to make full use of multiple paths in company with these protocols, most load balancers require hardware modifications on switches. As a result, it is hard to deploy the load balancers in large-scale data center networks (Dong et al., 2018). Multi-path TCP (MPTCP) is proposed to achieve both better utilization of network resources and lower deployment cost by only modifying the network stack of end-hosts (Bonaventure et al., 2017, Anon., 0000, Ford et al., 2013). As an extension to TCP, MPTCP leverages multiple subflows on different paths to achieve high throughput and dynamic traffic migration. Many MPTCP-based schemes have shown their unique advantages in feasible deployment, low FCT, and high goodput in DCNs (Dong et al., 2018, Kheirkhah et al., 2016).

However, MPTCP suffers from burstiness issues when deployed in DCN. Similar to the TCP protocol, MPTCP uses the ACK packets (i.e., Data ACK) to drive the sliding of MPTCP-level sending window, known as the ACK-clocking. The default MPTCP scheduler at senders allocates data packets with consecutive sequence numbers to individual subflows. Due to the diverse path situation, the data packets from multiple subflows might arrive at the receiver out of order on MPTCP-level. The out-of-order arrival of data packets intermittently blocks the Data ACK packets, known as the ACK blocking issue. ACK blocking defers the transmission of subsequent data packets, causing burstiness in the next round (Shi et al., 2018). This traffic burstiness can easily drain the shallow buffers in commodity switches, ending with frequent packet drops, retransmission, and even TCP timeout under highly concurrent traffic.

To solve the above burstiness problem fundamentally, we propose MultiPath TCP with Reduced Burstiness (MPTCP-RB) for DCNs by only modifying the MPTCP scheduler. In our design, rather than allocating the consecutive packets inside each subflow in state-of-the-art MPTCP schedulers, packets are interleaved across multiple subflows. The interleaving of packets guarantees that packets can arrive at the receiver in order on MPTCP-level no matter the round trip time (RTT) among subflows is similar or not, which mitigates the ACK blocking issue. As a result, the MPTCP-level sending window slides smoothly, reducing the burstiness of data packets.

In summary, we highlight our contributions as follows.

•
We provide an extensive study to reveal that allocating consecutive packets to individual subflows fundamentally leads to the out-of-order arrival of packets and ACK blocking on the MPTCP-level. Through empirical studies and theoretical analysis, we show that MPTCP-level ACK blocking intensifies traffic burstiness and degrades network performance.

•
We propose a novel scheme MPTCP-RB, in which packets are interleaved across subflows by the scheduler according to different network states, aiming at the in-order arrival of packets at the MPTCP-level. MPTCP-RB features its ability to reduce burstiness through avoiding ACK blocking.

•
We implement the MPTCP-RB in a light-weight manner. It works as a plugin module that can be loaded into MPTCP-enabled operating systems without any modification on the kernel, making it not only readily deployable in commercial data centers but also compatible with other components in MPTCP.

•
We evaluate our design on real testbed as well as with large-scale NS2 simulations.2 The results demonstrate that MPTCP-RB achieves up to 91% improvement in reducing FCTs compared to the state-of-the-art approaches.

The rest of this paper is structured as follows. The rationale of our design is described in Section 3. In Section 4, we present the details of MPTCP-RB. The modeling analysis is addressed in Section 5. The NS2 simulation and real testbed results are shown in Sections 6 Simulation evaluation, 7 Testbed evaluation, respectively. We demonstrate literature works and discuss their pros and cons in Section 2. Finally, we give our conclusion in Section 8.

2. Related works
Many state-of-the-art schemes are proposed to improve MPTCP performance in data center networks. Specifically, Raiciu et al. (2011a) show that MPTCP greatly improves link utilization in data center networks by using multiple subflows to balance traffic over parallel paths. DCMPTCP (Dong et al., 2018) controls the establishment of subflows to alleviate congestion for the many-to-one communication pattern in data centers. MMPTCP (Kheirkhah et al., 2016) randomly scatters packets of short flows to achieve high aggregated throughput. For long flows, MMPTCP degrades to regular MPTCP to balance the traffic load on multiple paths. To leverage the multipath diversity, once detecting packet loss on a subflow, FUSO (Chen et al., 2016) immediately retransmits the unacknowledged packets over another subflow that is not or less congested and has spare congestion window.

To improve MPTCP throughput over multiple heterogeneous paths such as cellular and WiFi, STMS (Shi et al., 2018) allocates more packets to the subflows on faster paths according to the difference between one-way delay of its two subflows. DEMS (Guo et al., 2017) decouples the data delivery among subflows by sending the data of a chunk in the opposite direction. Though it aims at simultaneous subflow completion at the receiver in heterogeneous networks, it requires large receiving buffer to accommodate the disordered packets from two subflows. There are also several works in avoiding ACK blocking in these heterogeneous networks. For instance, ECF (Lim et al., 2017) allocates the packets to the slow subflow only if it could reduce the flow completion time. BLEST (Ferlin et al., 2016) considers Head-of-Line blocking issue in the receiving buffer. It estimates whether a path will cause Head-of-Line blocking and adaptively schedules packets to subflow to prevent blocking. To achieve low latency, ReMP (Frömmgen et al., 2016) transmits data redundantly over multiple paths and exchanges bandwidth for latency. DAPS (Kuhn et al., 2014) tries to reduce the buffer blocking time of receivers through scheduling suitable number of consecutive packets on the sender side according to subflow RTT. In summary, all of schemes focus on heterogeneous networks which is quite different in DCNs. They still allocate the consecutive packets to individual subflow in most cases (e.g., congestion window is available), and omits the traffic burstiness brought by the stalled packets. Unfortunately, the traffic burstiness is fatal to datacenter applications and can lead to severe performance deterioration.

To better illustrate the differences of proposed MPTCP-RB against the existing solution, Fig. 1 shows the classification of current MPTCP schemes including MPTCP-RB (tracked by the thick red line and labeled in red). Unlike the aforementioned schemes, MPTCP-RB reduces FCTs by addressing the inherent limitation of MPTCP, i.e., traffic burstiness caused by consecutive packet allocation whenever congestion windows are available. Through interleaving packets across multiple subflows whenever congestion window is full or not, MPTCP-RB ensures the in-order arrival of packets at receivers to smoothly slide the MPTCP-level sending window. Therefore, MPTCP-RB mitigates the blocking of Data ACK and greatly reduces the traffic burstiness in both homogeneous (e.g., normal DCN) and heterogeneous (e.g., asymmetry DCN due to link failures) networks.

Fig. 1. The classification of MPTCP schemes.

Additionally, this paper differs from our previous works which are irrelevant to MPTCP and mainly focus on traffic load balancing (Hu et al., 2021, Huang et al., 2021b, Huang et al., 2021a, Liu et al., 2020, Zou et al., 2020), transmission controls (Zou et al., 2021a, Zou et al., 2021b, Huang et al., 2020, Zhang et al., 2020, Hu et al., 2020b), and buffer management of switches (Liu et al., 2020a, Liu et al., 2019a, Lyu et al., 2021). This is our first paper in solving the traffic burstiness issue caused by the consecutive packet allocation in MPTCP.

3. Motivation
In this section, we investigate the impact of MPTCP-level ACK blocking. We argue that the current MPTCP scheduler causes the ACK blocking and large traffic burstiness, resulting in long tail of flow completion time.

3.1. Impact of ACK blocking
MPTCP enables simultaneous data transmission over different paths by spreading data across multiple subflows. At the MPTCP sender, the scheduler decides which subflow the current packets should be assigned to. Moreover, MPTCP uses the Data ACK as MPTCP-level ACK to control the sliding window (Shi et al., 2018, Raiciu et al., 2012).

Fig. 2 gives an example to show how the MPTCP sending window slides. MPTCP leverages a single shared buffer and Data ACK to avoid deadlock (Raiciu et al., 2012). The packets (or segments in MPTCP-level view) that the scheduler can assign to subflows are bounded by the size of MPTCP sending window (or shared sending buffer) (Shi et al., 2018). Once the packet on the left edge (e.g., packet number 11 in the figure) is ACKed by the Data ACK, the MPTCP sending window will slide right by one packet.

Fig. 2. Default scheduler in MPTCP.

However, if all packets in the sending window are sent out while the packet on the left edge is still not ACKed, the whole transmission of all subflows stops, known as ACK blocking. Once the delayed Data ACK arrives at the sender, the congestion windows of subflows are quickly filled by the scheduler and then all packets assigned to subflows are flushed out, leading to traffic burstiness.

To better illustrate this issue, Fig. 3 shows two examples. In Fig. 3(a), one MPTCP flow has two subflows with same RTT. Note that, similar RTT among subflows is quite common in DCNs since they usually have symmetric topology. We assume that the two subflows share the same bottleneck link (i.e., the last hop to the receiver) that can accommodate 2 packets. The packets are marked with MPTCP-level sequence numbers. Initially, the scheduler respectively allocates packet 0–1 and packet 2–3 to subflow 1 and 2 at the beginning of the first RTT. However, when packet 0 and 2 arrive at the receiver, only the Data ACK of packet 0 returns to the sender, because there is a hole (packet 1 is not received yet) in receiving buffer. After receiving packet 1 and 3, the receiver sends back the Data ACKs of packet 1–3 at the end of first RTT. Then, a burst of 3 packets is injected into the network in the second RTT. Unfortunately, since the bottleneck link can only accommodate 2 packets, packet 7 is dropped. Therefore, one extra RTT is required to retransmit packet 7.

The ACK blocking issue becomes much severe under more subflows. Fig. 3(b) shows the case of four subflows. The arrival of Data ACK 1–7 triggers the burstiness of 7 packets, resulting in the loss of packet 13–15. Since all packets in the congestion window of subflow 4 are dropped, subflow 4 has to wait for one retransmission timeout (RTO) to trigger retransmission. In this case, the MPTCP suffers from a timeout event, which significantly enlarges the flow completion time.


Fig. 3. An example of MPTCP blocked transmission. In both subfigures, the maximum affordable burstiness in the bottleneck equals to the number of subflows.

Fig. 4. Details of the transmission in an MPTCP flow.

3.2. Empirical analysis
To further test the impact of ACK blocking, we conduct NS2 experiments under the 4-rack Leaf-Spine topology. The round trip time and link bandwidth of all inter-rack paths are 100 μ and 2 Gbps, respectively. The queue size of switch buffer is 60 packets. To produce the congestion bottleneck, the link bandwidth of the last hop to the receiver is set as 1 Gbps.

We first investigate the impact of ACK blocking on a single MPTCP flow, which transmits 150 packets of data from one rack to another. Fig. 4(a) shows the CDF of out-of-order delay (i.e., the time difference between the time when a packet arrives at the receiver and when it is submitted to the application layer). With more subflows, the packets experience larger out-of-order delay. In other words, there are more blocked packets under more subflows. As a result, the Data ACK of the last packet that fills up the hole in the receiving window will trigger a sharply increasing number of packets in the next round.

We track the Data ACK sequence number at the receiver. Fig. 4(b) shows that, compared with 2 subflows, the leaps of Data ACK sequence number become more frequent (e.g., twice) and larger (e.g., up to the more than 30 packets) under 6 subflows.

To better illustrate the impact of surges in MPTCP sequence numbers on traffic burstiness, we plot the queue length in the ToR switch connected with the receiver. Fig. 4(c) shows that the queue length increases significantly right after the leaps of sequence numbers. Moreover, the larger queue length leads to a higher risk of packet drops, especially under more concurrent subflows.

Next, we increase the number of MPTCP flows and evaluate their impacts on traffic burstiness. In this test, each MPTCP flow only consists of 2 subflows. Fig. 5(a) shows that, MPTCP flows suffer from larger out-of-order delay under high flow concurrency, resulting in larger leaps of Data ACK shown as Fig. 5(b). Correspondingly, in Fig. 5(c), the switch queue length reaches its maximum value (60 packets) under the traffic burstiness. Note that, several flows even experience the timeout (i.e., 200 ms RTO) in the case of 16 concurrent MPTCP flows due to the extremely high burstiness.

Fig. 5. The details of transmission in tail MPTCP flow, the variation of queue length at the switch, and the average FCT of all MPTCP flows.

Finally, we plot the CDF of average FCTs under different numbers of concurrent MPTCP flows in Fig. 5(d). FCT is measured as the time difference between the time when the first packet is sent and when the last packet is received. It is an important metric for delay-sensitive applications (Dukkipati and McKeown, 2006, Zats et al., 2012). With more concurrent MPTCP flows, the average FCT grows rapidly due to the high burstiness and its consequences including larger queue length and even timeout.

3.3. Summary
Based on the above investigation, we make the following conclusions: (i) the absent packets in MPTCP break the ACK-clocking and block the transmission of MPTCP flows; (ii) the arrival of tail packets leads to traffic burstiness, resulting in higher queue length, larger queueing delay, and even timeout; (iii) this issue becomes more severe under highly concurrent traffic, i.e., more concurrent MPTCP flows and more subflows in each MPTCP, in data center networks. These observations motivate us to investigate a novel approach to reduce the burstiness brought by MPTCP while keeping modifications as minimum as possible to provide compatibility with traditional MPTCP. Our method will be presented in the rest of the paper.

4. MPTCP-RB design
In this section, we first briefly show how a proper scheduler helps MPTCP to reduce traffic burstiness. Then, we describe the architecture and details of our design.

4.1. How to bring ACK-clocking back
As summarized in Section 3, the traffic burstiness brought by ACK blocking is an inherent limitation of MPTCP because the native MPTCP scheduler always tries to assign consecutive packets to a subflow until its congestion window is full. The ACK blocking easily breaks the MPTCP-level ACK-clocking and results in traffic burstiness.

Therefore, a fundamental solution to this issue should free the Data ACK from being blocked at MTPCP-level. To achieve this goal, the MPTCP packets should arrive in order at the receiver side. Then received packets can be ACKed immediately without delay. Consequently, the congestion window will move smoothly to the right, i.e., ACK-clocking, without causing traffic burstiness.

We show an example in Fig. 6, in which the packets are interleaved among subflows. In Fig. 6(a), packets with even MPTCP sequence numbers are assigned to subflow 1, whereas subflow 2 sends packets with odd MPTCP sequence numbers. Then the packets arrive at the receiver with the sequence numbers of 0, 1, 2, and 3, triggering 4 Data ACKs. Whenever the sender receives a Data ACK, the MPTCP sending window moves right by one packet and then a new packet is sent out. Thus, the sender delivers 4 new packets in the second RTT without burstiness. This mechanism also works well with more subflows, e.g., four subflows in Fig. 6(b). In this case, the packets are paced naturally by Data ACKs, successfully maintaining ACK-clocking.

Fig. 6. Examples of the solution, which avoids burstiness of traffic by interleaving consecutive packets among subflows. The maximum affordable burstiness in the bottleneck equals to the number of subflows in both subfigures.

4.2. Design overview
We demonstrate how to interleave the packets among subflows in a light-weight manner without massive modification on traditional MPTCP. We note that, the scheduler decides which subflow that a packet should be assigned to. This feature makes the scheduler itself a perfect position to interleave the consecutive packets among subflows.

We propose our scheme MPTCP-RB to interleave packets across multiple subflows. Moreover, MPTCP-RB calculates the optimal number of packets allocated to each subflow that can guarantee in-order arrival according to the subflow latency. It is designed as a plugin module that is compatible to existing MPTCP sender with only the scheduler modified. Fig. 7 shows the MPTCP-RB architecture, which consists of two components. The first one periodically updates the allocation pattern for each subflow. The second one pre-allocates packets to corresponding subflows according to the allocation pattern.

4.2.1. Gap calculation
Firstly, according to the RTTs of subflows, MPTCP-RB periodically calculates the number of consecutive packets, namely, the gap, for each subflow to provide in-order arrival at the receiver. In other words, the calculation result of this module is the packet allocation pattern of MPTCP scheduler. If the subflows have identical RTTs, the gap is only 1, meaning that the packets are pre-allocated to subflows in a per-packet round-robin fashion. Otherwise, the allocation pattern is calculated based on the diverse RTTs of subflows. For instance, if the RTT ratio between 2 subflows is 4:1, to obtain in-order arrival of 5 packets at the receiver, the slow subflow should be pre-allocated with only 1 packet while the fast one getting 4 consecutive packets. That is, the gaps for fast and slow subflows are 4 and 1, respectively.

4.2.2. Packet pre-allocation
When new packets from the upper layer arrive at the scheduler, MPTCP-RB pre-allocates these packets to the corresponding subflows according to the current allocation pattern and the MPTCP sequence numbers of arrival packets. Specifically, MPTCP-RB pre-allocates packets with large sequence number over slow subflow to avoid out-of-order arrival at the receiver. Given the above example, the 5th packet is pre-allocated to the slow subflow, while the 1st4th packets to the fast subflow. Note that, if the allocation pattern is not ready, e.g., not all RTTs of subflows are available at the beginning of transmission, the packets are allocated in a way just like all subflows have identical RTTs.

4.3. Design details
4.3.1. Details of packet pre-allocation
The core of our design is to calculate the packet allocation pattern. Algorithm 1 shows the novel packet allocation process in MPTCP-RB. The idea is to assign the packet with greater MPTCP sequence number to the subflow with larger RTT. Specifically, the sender initializes the packet gap which is inverse to the measured RTT of each flow. Then, the pattern length is updated as the sum of all gaps for subflows. After the initialization, the function gapCalculation() is called to compute the allocation pattern.

4.3.2. Details of gap calculation
Generally, gapCalculation() function divides the allocation pattern into smaller partitions bounded by different left and right bounds as shown in Algorithm 2. In each partition, the packet with the largest sequence number is marked with the subflow with the greatest RTT in the current set of subflows, if the number of marked packets is still less than the gap of their corresponding subflow. Then, the algorithm deletes the subflow with greatest RTT from the subflow set, shrinks the partition bound, and recursively repeats the previous steps. The recursion ends if there is only one subflow left in the subflow set, and then all the unmarked packets are marked with the subflow with the smallest RTT.

4.3.3. Case study
We give an example in Fig. 8 for better understanding how MPTCP-RB allocates packets to subflows. In the example, one MPTCP consists of four subflows (subflow AD) with different RTTs. In Fig. 8, the RTT ratio of 4 subflows is 1:2:3:5, then the gaps of subflow AD are 5, 3, 2 and 1, respectively. The length of allocation pattern is initialized as 11. We assume that the congestion window of each subflow is 1.

In the first round, the last packet in the allocation pattern is marked with subflow D (i.e., the subflow with the greatest RTT in the subflow set) and then subflow D is deleted from the subflow set. Moreover, the partition bound is shrunk to packet 110. Next, the pattern is divided into two partitions, i.e., packet 15 and packet 610, because the gap ratio of subflow C to D is 2:1. In the following rounds, the last packet of each partition is marked with the subflow with the greatest RTT in the current subflow set. After deleting the subflow with the greatest RTT from the subflow set and shrinking the partition bound, each partition in the pattern is divided into multiple smaller partitions again, according to the gap ratio of the greatest RTT in the current subflow set to the previously deleted one. Note that, the number of packets that a subflow can be assigned is bounded by its gap. For example, in the 3rd round, the 9th packet is unmarked because the gap of subflow B is 3. Finally, when only one subflow (e.g., A) is left in the subflow set, all rest packets are marked with subflow A.

Fig. 8. Examples of scheduling packets among subflows with different RTTs.

Additionally, updating of allocation pattern is triggered every  interval. Here,  is an empirical value as 10 ms (Chowdhury and Stoica, 2015). Finally, once receiving a packet from the upper layer, MPTCP-RB schedules the packet according to the allocation pattern. Note that, the packets with MPTCP sequence number greater than the length of allocation pattern still use the same allocation pattern repeatedly until it is updated. For instance, packet 12 and 13 in the previous example are respectively allocated to subflow A and B by taking the first and second elements in the allocation pattern.

5. Model analysis
5.1. Bursty strength
Firstly, we analyze the traffic burstiness from the microscopic point of view. We define the bursty strength as the number of packets arriving at the switch in one packet’s transmission time. Suppose one packet is 1.5 KB, the transmission time of a packet is 12 μ for 1 Gbps output port. In this case, if 10 packets simultaneously arrive at the switch and compete for the same output port during the interval of 12 μ, the bursty strength is 10.

We assume that  MPTCP flows send data through  paths to a single receiver. To make full use of multiple paths, each MPTCP flow consists of  subflows. For simplicity, as same as in Alizadeh et al. (2011) and Shan and Ren (2017), we assume that all MPTCP flows are synchronized.

5.1.1. MPTCP
At the MPTCP sender, the default scheduler allocates data packets with consecutive sequence numbers to individual subflow. Fig. 9 shows the packets sent by each subflow during one RTT. Given the congestion window , each subflow sends  packets with consecutive sequence numbers.

To maintain the ACK-clocking of TCP, the MPTCP-level sending window slides right by one packet when one ACK packet arrives at the sender, triggering a new data packet. When each packet in the 1st subflow arrives at the receiver, a new packet will be triggered in the next round. However, if the packets are not in the 1st subflow and the last column, their ACK packets may be blocked by the packets in subflows with smaller subflow numbers without triggering any new data packets. Fig. 9 shows the worst case when subflows are blocked by packets in the last column. Even if subflow  have finished transmissions in the current round, all ACKs of data packets in them are unable to be transmitted until 
 arrives at the receiver. In this case, the arrival of packet 
 will trigger  data packets in the next round.

Fig. 9. An example of packet allocation in a traditional MPTCP flow.

In a nutshell, each packet in the 1st subflow except the last packet 
 ( packets in total) triggers 1 packet, while each packet in the last column ( packets in total) triggers  packets ( is the index of subflow that the packet belongs to). The rest packets trigger 0 packet. Therefore, for MPTCP, we can get the average value of the bursty strength as (1)
 
 

5.1.2. MPTCP-RB
MPTCP-RB interleaves the consecutive packets across multiple subflows. As shown in Fig. 10, the packets in the same column (or batch) have consecutive MPTCP-level sequence numbers. Since the packets in each subflow arrive at the receiver in order, the maximum number of packets in the next round triggered by one packet is the number of subflows . Taking packet 
 as an example, its arrival only slides the MPTCP-level sending window from 
 to 
 if the other subflows have finished their transmissions in the current round. That is, MPTCP-RB reduces the bursty strength to .

Note that, in the last column, the bursty strength is smaller than , because the arrival of the packets in the last column can only slide the sending window from the arrival packet to 
. To sum up, each packet except the one in the last column ( packets in total) triggers  packets, while each packet in the last column ( packets in total) triggers  packets ( is the index of subflow that the packet belongs to). Therefore, we get the average value of bursty strength for MPTCP-RB as (2)
 
 
Fig. 10. An example of packet allocation in an MPTCP-RB flow.

5.1.3. Impacts on packet loss
We assume that all  synchronized MPTCP flows fairly share the bottleneck link (i.e., the last hop) using statistical multiplexing. Given the number of subflows  for each MPTCP flow, switch buffer size  and bursty strength , we calculate the packet loss rate  under traffic burstiness.

Microscopically, the switch buffer can accommodate  packets during the period of simultaneous arrivals of  packets. Hence, the packet loss rate  is (3)
 

With Eqs. (1), (3), we get the packet loss rate 
 of MPTCP as (4)
 

Similarly, we obtain the packet loss rate 
 of MPTCP-RB as (5)
 

5.2. Multipath throughput modeling
We establish the multipath throughput model to compare the flow completion time of MPTCP and MPTCP-RB. We assume that  MPTCP flows share the bottleneck link. Each MPTCP flow has  subflows.

We plot the total congestion window  of  subflows in Fig. 11. According to the additive-increase multiplicative-decrease (AIMD) policy of congestion control scheme, once experiencing packet loss, one subflow cuts its congestion window by  (1/2 by default). Otherwise, each subflow increases its congestion window by  (1 by default) every round trip time. Thus, for  subflows, the total congestion window size  increases by  per RTT. After a packet loss event, it takes 
 
 RTTs to recover the congestion window.


Fig. 11. Total congestion window of multiple MPTCP flows.

Given the packet loss rate , we have the following equation: (6)
 
 
 
 
 
 

With Eq. (6), we have (7)
 

Then, we calculate the total throughput  as (8)
 
 
 
 

The average throughout of subflow is 
 
. Given the flow size of each subflow as , the flow completion time  is .

Finally, we compare the calculated FCTs of MPTCP and MPTCP-RB in Fig. 12. The number of MPTCP flows is fixed to 12 and the number of subflows increases from 2 to 10 in Fig. 12(a). Under heavy congestion with packet loss (i.e., ), MPTCP-RB obtains lower FCT than MPTCP. When the number of subflows increases, the improvement of MPTCP-RB decreases due to the higher packet loss rate. Moreover, we set the number of subflows to 6 and change the number of MPTCP flow. As shown in Fig. 12(b), the result is similar to that of Fig. 12(a). However, MPTCP-RB still reduces the FCT by approximate 30% when the network becomes very congested, e.g., 15 MPTCP flows in Fig. 12(b).


Fig. 12. The FCTs of different schemes with buffer size  as 60 packets, congestion window  as 8 packets,  as 6 ms, and  as 1500 bytes. Each subflow sends 500 packets.

6. Simulation evaluation
In this section, we conduct large-scale simulation to thoroughly test the performance of MPTCP-RB under different network environments with NS2, a prevalent packet-level network simulator used by many researchers (Saeed et al., 2020, Narayan et al., 2018, Bai et al., 2020, Cho et al., 2017).

We use a Leaf-Spine topology with multiple equal-cost paths between host pairs. The test settings are as follows unless otherwise stated. The buffer size of each switch is 100 packets and the round trip propagation time is 100 μ except the experiments in Section 6.2. The default sending buffer is 64 KB for each MPTCP flow as suggested in Sarwar et al. (2012). During the experiments, each sender transmits 150 packets (1.5 KB size) to the receiver in another rack, generating inter-rack traffic. The starting time flows obeys Poisson distribution with parameter  = 2.5 (Liu et al., 2019b).

6.1. Benchmarks
In this test, we use the 8-rack Leaf-Spine topology with 8 core switches (Zhang et al., 2017). There are 10 end-hosts in each rack. The link bandwidth between ToR and core switch is 1 Gbps. The link capacities from senders to their ToR switches and the ToR switches to receivers are 2 Gbps and 1 Gbps, respectively.

We compare the MPTCP-RB with the classical MPTCP schedulers. As the default scheduler in MPTCP, MinRTT (Raiciu et al., 2012) always tries to allocate consecutive packets to the subflow with the smallest RTT until its congestion window is full. By contrast, RoundRobin (Paasch et al., 2014) fills the congestion window of each subflow in a per-flow round-robin fashion. It is different from MPTCP-RB’s per-packet round-robin when RTTs of subflows are the same, because RoundRobin scheduler allocates consecutive packets in a single subflow. We measure the out-of-order delay at the receivers, bursty strength and packet loss rate at switches.

Fig. 13(a) shows the Cumulative Distribution Function of out-of-order delay with 10 MPTCP flows and 2 subflows in each MPTCP flow. MPTCP-RB experiences very low out-of-order delay due to its efforts in avoiding ACK blocking. By contrast, the traditional schemes (i.e., MinRTT and RoundRobin) suffer from the out-of-order problem since they allocate consecutive packets to individual subflows. For example, almost 99% packets of MPTCP-RB experience less than 200 μ out-of-order delay, which is much smaller than that of MinRTT (e.g., 900 μ) and RoundRobin (e.g., 1400 μ).

Fig. 13(b) and (c) show the maximum bursty strength under varying flow concurrency. The number of subflows in each MPTCP flow and the number of MPTCP flows are 2 and 4 in Fig. 13(b) and (c), respectively. By addressing the ACK blocking issue, MPTCP-RB mitigates the maximum traffic burstiness by up to 33% compared with MPTCP. Consequently, as shown in Fig. 13(d) and (e), MPTCP-RB successfully decreases the packet loss rate by up to 11% under different numbers of MPTCP flows and subflows.

6.2. Impact of network parameters
In this part, we test the performances of different schemes under the impact of various network parameters. The test topology is as same as that in Section 6.1. Moreover, we add a state-of-the-art scheme STMS (Shi et al., 2018), which allocates more packets to the faster subflow to alleviate the MPTCP throughput degradation under heterogeneous path conditions. Since STMS supports only 2 subflows, we fix the number of subflows for all schemes as 2 in this and the following section. The number of MPTCP flows is fixed as 16.

Firstly, we increase the propagation delay of one path by multiple times to produce asymmetry. Fig. 14(a) shows that, the average FCTs of all schemes increase significantly with greater delay asymmetry except MPTCP-RB. Under MinRTT, STMS and RoundRobin, since more packets are easily blocked under larger asymmetry, once the Data ACKs of stalled packets return to senders, severe traffic burstiness is triggered, resulting in frequent packet drops and large FCT. By contrast, benefitting from the interleaving of packets across subflows according to their different RTTs, MPTCP-RB ensures that the packets arrive at receivers in order, reducing the traffic burstiness. As a result, the FCTs of MPTCP-RB stay low under varying delay asymmetry.

Fig. 14. Performances of schemes under different network parameters.

Next, we evaluate the impact of asymmetric bandwidth. We reduce the bandwidth of one path by 10%80%. Fig. 14(b) shows that MPTCP-RB exhibits good robustness to asymmetric bandwidth due to its self-adaptive mechanism during interleaving packets across different subflows, achieving the lowest FCTs in four schemes.

Moreover, we investigate the impact of the switch buffer size. As shown in Fig. 14(c), since larger switch buffer can absorb severer burstiness, average FCTs of all schemes decrease with larger buffer size. Compared with the other schemes, MPTCP-RB shows its unique advantage in reducing burstiness, even when the buffer size is very small, e.g., 60 packets.

Finally, we test the FCTs of all schemes with different end-host buffer sizes. Fig. 14(d) shows that the FCTs of all schemes decrease with larger sender and receiver buffer sizes. The reason is that larger end-host buffer enable senders to transmit more data before the transmission is really blocked or halted by the out-of-order ACK. Namely, in this case, the transmission will continue and will not be really blocked if the missing Data ACK returns to the sender before the sender transmits over all the packets in the buffer. Note that, greater end-host buffer increases the probability of this case, reducing the transmission blocking time and resulting in lower FCT. Since MPTCP-RB interleaves the packets across multiple subflows to guarantee that packets can arrive at the receiver in order on MPTCP-level, the sending window slides smoothly and continuously (i.e., no out-of-order ACKs happen) no matter how many end-host buffer remains. Therefore, the average FCT of MPTCP-RB does not change so much and it still performances well even when the end-host buffer is small. For example, MPTCP-RB reduces FCT by up to 91% in case of 64 KB buffer size.

6.3. Realistic workload
We conduct large-scale NS2 simulation with realistic workloads. We follow the same flow size distribution of four different application workloads (i.e., Data Mining, Web Search, Cache Follower, and Web Server) (Cho et al., 2017) shown in Table 1. In this experiment, we extend the network scale to 40 racks and 20 core switches. The bandwidth of all links is 1 Gbps. Since most of the traffic load in data centers is less than 10% (Roy et al., 2015). The experiment is conducted under the worst case (i.e., with the traffic load of 10%). The sender and receiver are randomly selected among all end-hosts.

Fig. 15(a), (b) and (c) show the average, 95th and 99th percentile FCT of short flows (1 MB), respectively. MPTCP-RB achieves the lowest FCT in all schemes. STMS obtains better performance than MinRTT and RoundRobin because STMS adjusts the number of packets allocated to subflows according to subflow RTT. However, STMS still suffers from the long FCT due to the burstiness raised by the allocation of consecutive packets to each subflow, especially in the case of tail FCT (e.g., about 200 ms of 99th percentile FCT in Data Mining application).


Table 1. The flow size distribution of 4 different workloads.

Data Mining	Web Search	Cache Follower	Web Server
0  10 KB	78%	49%	50%	63%
10 KB  100 KB	5%	3%	3%	18%
100 KB  1 MB	8%	18%	18%	9%
 1 MB	9%	20%	29%	–
Average flow size	7.41 MB	1.6 MB	701 KB	64 KB
The average throughputs of long flows are shown in Fig. 15(d). Because no long flows (1 MB) exist in the Web Server application according to the workload given in Cho et al. (2017), we only measure the throughputs of long flows in Data Mining, Web Search, and Cache Follower. Although the improvement is not very significant in Data Mining workload, MPTCP-RB still outperforms the other schemes by up to 66% and 62% in Web Search and Cache Follower, respectively.


Fig. 15. FCTs and throughputs of different schemes under the realistic workload. Specifically, DM, WSc, CF, and WSv stand for Data Mining, Web Search, Cache Follower, and Web Server, respectively.

6.4. Performance under fat-tree topology
In this subsection, we conduct experiments under another prevalent data center topology called fat-tree. The bandwidth is 10 Gbps for the links between switches, while the bandwidth of the links between switches and end-hosts is 1 Gbps. The sender and receivers are randomly selected among all end-hosts. Other network parameters like buffer size, round-trip propagation delay, default sending buffer, traffic load, and the number of subflows are as same as the ones in the previous Leaf-Spine topology.

In the experiments, we vary the network scale by increase the number of pods from 4 (i.e., 16 end-hosts in total) to 16 (i.e., 1024 end-hosts in total). The results are shown in Fig. 16. Though the median FCT are quite similar for all the 4 schemes, MPTCP-RB successfully reduces tail FCT (i.e., 95th percentile FCT) significantly by avoiding the severe packet drops and timeouts.

7. Testbed evaluation
In this test, we build a small-scale testbed to evaluate the performances of MinRTT, Round-robin, STMS, and MPTCP-RB. We have deployed MPTCP-RB in a small testbed, which consists of 4 DELL workstation servers with Intel Xeon E5-2687Wv4 3.00 GHz 12-core CPU, 32 GB 2400 MHz RAM, 256 GB SSD, and Intel X520 dual-port 10 GbE NICs. We emulate a simple many-to-one topology, in which one server works as a switch via DPDK (Intel, 0000). Each server connects with this switch using two 10 Gbps links and each sender transmits data with two subflows (i.e., one link for one subflow).

All servers run Ubuntu 14.04 with MPTCP version 0.90. The MPTCP-RB module is loaded on senders. Note that, since our MPTCP-RB scheduler is a plugin module, it is compatible with the latest MPTCP version 0.95 (Anon., 2019). In order to produce congestion bottlenecks with the limited number of servers, the bandwidth of TX links in the switch is set to a quarter of the original one (i.e., 2.5 Gbps) in all experiment scenarios. The overall switch buffer size is set as 256 KB. Moreover, the propagation delay is approximately 1 ms per hop. The default packet size and RTO
 are 1.5 KB and 200 ms, respectively.

7.1. Different congestion control algorithms
In this part, we test the performance of MPTCP-RB under different MPTCP dedicated congestion control algorithms, e.g., LIA (Raiciu et al., 2011b), OLIA (Khalili et al., 2013), BALIA (Peng et al., 2016), and wVegas (Cao et al., 2012). We use iperf to generate multiple concurrent MPTCP flows, each of which transmits 20 MB data using two subflows. We alter the traffic load by increasing the number of MPTCP flows from 10 to 60.

Fig. 17 shows the average FCTs with four congestion control algorithms. Without loss of generality, MPTCP-RB has the lowest average FCTs under all congestion control algorithm cases, since MPTCP-RB’s operation of interleaving packets across subflows is independent of congestion control algorithms. For instance, MPTCP-RB reduces average FCT by up to 47% compared to STMS in Fig. 17(a). Note that, modern DCNs have more bandwidth (e.g., 40 Gbps or higher) and much more concurrent flows like over thousands of them (which means smaller buffer sizes for each flow), the burstiness issues can only be more severe. Our MPTCP-RB is dedicated for this burstiness cases in DCNs by interleaving the packets among subflows no matter how large congestion window is not used by individual subflow, whereas others still try to first fill up all the spare congestion window before taking their actions. Therefore, MPTCP-RB outperforms other schemes even if they are aware of ACK-blocking.


Fig. 17. FCTs under 10 Gbps links.

7.2. Mixed workloads
Here, we evaluate the performance of different schedulers under mixed workloads. These workloads consist of 50 flows, including 20% of long flows and 80% of short flows according to Roy et al. (2015). Each long flow transmits 2 GB data, whereas each short flow has only 2 MB data. In Fig. 18, we plot different types of short flows’ FCTs and average throughput of long flows.

Generally, MPTCP-RB has the lowest FCTs (including the tail flows) for short flows and highest average throughput for long flows thanks to the less burstiness through its interleaving action. STMS outperforms the rest two schedulers due to its ACK-blocking awareness mechanism. But it still suffers from the burstiness issues caused by its consecutive packet allocation. Consequentially, it fails to achieve better performance compared with MPTCP-RB.

Fig. 18. FCTs of short flows and throughput of long flows under 10 Gbps links.

7.3. System overhead
We assess the system overhead of different schemes at both the sender and the receiver under 10 Gbps link. In this test, there are varying number of MPTCP flows and each of them has 2 subflows. We measure the average CPU usage and overall memory consumption. As shown in Fig. 19(a), the CPU usage of all schemes at the sender rises with more concurrent MPTCP flows. Though MPTCP-RB has recursion operations in calculating the allocation pattern, it only increases the CPU usage by approximate 1% at maximum compared with the other schemes. Thanks to MPTCP-RB’s in-order arrival of packets, the out-of-order issue at the receiver is mitigated. Therefore, Fig. 19(b) shows that the CPU usage of the receiver is decreased by about 6% at maximum. The memory consumption at either the sender or the receiver is also very small (less than 0.1%) for all schemes, even if the number of MPTCP flows increases to 20.

8. Conclusion
In this paper, we present a novel MPTCP scheduler MPTCP-RB and address its design, implementation, and analysis. Compared with traditional MPTCP, the proposed MPTCP-RB mitigates the traffic burstiness issue in data centers through interleaving packets across multiple subflows at only the sender side, resulting in small queueing delay, low packet loss rate, less number of retransmissions, and thus short flow completion time in a wide range of network scenarios. Additionally, to boost the performance of MPTCP-RB, high accurate measurement of RTT is required. But it is challenging for operating system kernels to preciously measure the RTT, especially when virtualization technology is employed. A solution to this issue is to use hardware timestamps with the help of emerging hardwares like smart NICs and we leave this as our future work.