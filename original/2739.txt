Weighted finite-state transducers have been shown to be a general and efficient representation in many applications such as text and speech processing, computational biology, and machine learning. The composition of weighted finite-state transducers constitutes a fundamental and common operation between these applications. The NP-hardness of the composition computation problem presents a challenge that leads us to devise efficient algorithms on a large scale when considering more than two transducers. This paper describes a parallel computation of weighted finite transducers composition in MapReduce framework. To the best of our knowledge, this paper is the first to tackle this task using MapReduce methods. First, we analyze the communication cost of this problem using Afrati et al. model. Then, we propose three MapReduce methods based respectively on input alphabet mapping, state mapping, and hybrid mapping. Finally, intensive experiments on a wide range of weighted finite-state transducers are conducted to compare the proposed methods and show their efficiency for large-scale data.

Introduction
Weighted finite-state transducer (WFST) has been used in a wide range of applications such as digital image processing [1], speech recognition [2], large-scale statistical machine translation [3], cryptography [4], recently in computational biology [5] where pairwise rational kernels are computed for metabolic network prediction and many other applications [6,7,8]. Weighted finite-state transducers are finite-state machines in which each transition in addition to its input symbol is augmented with an output symbol from a possibly new alphabet, and carries some weight element of a semiring. Transducers can be used to define a mapping between two languages. In some cases, The weight represents the uncertainty or the variability of information. For example, weighted transducers introduced in speech recognition [9] are used to assign different pronunciations to the same word with different ranks or probabilities. In classification and learning, kernel methods, like support vector machines, are widely used [10]. In [11], Mohri et al. have introduced the theory of rational kernel. The computation of a rational kernel can be done efficiently using a fast algorithm for the composition of weighted transducers. Computing the composition of WFSTs is basically based on the standard composition of unweighted finite-state transducers. It takes as input, two or more WFSTs (𝑖)1≤𝑖≤𝑛, and outputs the composed WFST  realizing the composition of all input WFSTs, such that the input alphabet of 𝑖+1 coincides with the output alphabet of 𝑖. The time complexity for computing this operation is shown to be 𝑂(∏𝑛𝑖=1|𝑖|) where |𝑖| represents the number of states in 𝑖 [11, 12] (i.e. if there are n input WFSTs, each having m states, the resulting WFST can reach the exponential bound of 𝑚𝑛 states). The complexity issue of the composition computation leads us to devise efficient methods in large scale.

In this work, we tackle the problem of the composition of WFSTs in the MapReduce framework, which is introduced by Google as a simple parallel programming model [13]. MapReduce is considered as a single Instruction Multiple Data (SIMD) architecture [14] that can be easily implemented over the Hadoop Apache framework [15]. We also analyze the cost model for this problem following the optimization approach introduced by Afrati et al. [16]. The cost model includes the communication cost which is the amount of data transmitted during MapReduce computations and the replication rate which represents the number of key-value pairs generated by all the mapper functions, divided by the number of inputs. A growing number of papers deal with MapReduce algorithms for various problems [17,18,19,20]. Recently, Grahne et al. [21, 22] have implemented efficiently the intersection and the minimization operations of finite automata in MapReduce. To the best of our knowledge, this paper is the first approach for computing the composition of WFSTs in MapReduce. We propose three methods to perform this operation in large scale respectively based on states mapping, input alphabet mapping, and a hybrid input and output alphabets mapping.

The remainder of the paper is structured as follows. "MapReduce framework" section ncludes necessary technical definitions. "The composition of WFSTs in MapReduce" section is a reminder of the fundamental of MapReduce framework. "Methods" section presents a cost model analysis of the composition operation in MapReduce using Afrati et al. model. "Results and discussion" section presents and analyses three MapReduce methods that compute the composition of many WFSTs. Some comparative and extensive experiments are also conducted in this section to show the efficiency of our methods in large scale. "Conclusion" section concludes the paper.

Preliminaries
In this section, we introduce briefly the notion of weighted finite transducers. For further details in formal aspects of finite automata theory, we particularly recommend reading [23] or in French [24].

Semirings. A system (𝕂,⊕,⊗,0⎯⎯,1⎯⎯) is a semiring when (𝕂,⊕,0⎯⎯) is a commutative monoid with identity element 0; (𝕂,⊗,1⎯⎯) is a monoid with identity element 1⎯⎯); ⊗ distributes over ⊕; and 0⎯⎯ is an annihilator for ⊗: for all 𝑎∈𝕂,𝑎⊗0⎯⎯=0⎯⎯⊗𝑎=0⎯⎯. Thus, a semiring is a ring that may lack negation. Some familiar semirings include the boolean semiring (𝔹,∨,∧,0,1), the tropical semiring (ℝ+∪{∞},𝑚𝑖𝑛,+,∞,0), and the real semiring (ℝ,+,×,0,1) [25].

Weighted finite-state transducers. Let A and B be two alphabets. WFST Sometimes called 𝕂-transducers over 𝐴∗×𝐵∗ are transducers endowed with weights in the semiring 𝕂. The transition labels of a WFST are in (𝐴∪{𝜀})×(𝐵∪{𝜀}) (𝜀 is the empty word). In this work, we restrict the transition labels to be in 𝐴×𝐵. Formally, a WFST  is an 8-tuple (𝐴,𝐵,𝑄,𝐼,𝐹,𝐸,𝜆,𝜌) where: A is the finite input alphabet of the transducer; B is the finite output alphabet; Q is a finite set of states; 𝐼⊆𝑄 the set of initial states; 𝐹⊆𝑄 the set of final states; 𝐸⊆𝑄×(𝐴×𝐵)×𝕂×𝑄 a finite set of transitions; 𝜆:𝐼→𝕂 the initial weight function; and 𝜌:𝐹→𝕂 the final weight function mapping F to 𝕂.

The size of , denoted by ||, is the number of its transitions. Given a transition 𝑡∈𝐸, we denote by s[t] its origin or start state, d[t] its destination state or next state, and w[t] its weight. A path 𝜋=𝑡1…𝑡𝑘 is an element of 𝐸∗ with consecutive transitions: 𝑑[𝑡𝑖−1]=𝑠[𝑡𝑖],𝑖=2,…,𝑘. We extend s and d to paths by setting: 𝑠[𝜋]=𝑠[𝑡1] and 𝑑[𝜋]=𝑑[𝑡𝑘]. The weight function w can also be extended to paths by defining the weight of a path as the ⊗-product over the semiring 𝕂 of the weights of its constituent transitions: 𝑤[𝜋]=𝑤[𝑡1]⊗…⊗𝑤[𝑡𝑘]. Let 𝑞,𝑞′∈𝑄, we denote by 𝑃(𝑞,𝑞′) the set of paths from q to 𝑞′ and by 𝑃(𝑞,𝑥,𝑦,𝑞′) the set of paths from q to 𝑞′ with input label 𝑥∈𝐴∗ and output label 𝑦∈𝐵∗. These definitions can be extended to subsets 𝑅,𝑅′⊆𝑄, by: 𝑃(𝑅,𝑥,𝑦,𝑅′)=⋃𝑞∈𝑅,𝑞′∈𝑅′𝑃(𝑞,𝑥,𝑦,𝑞′). A WFST  is regulated if the output weight associated by  to any pair of input-output string (x, y) by:

[](𝑥,𝑦)=⨁𝜋∈𝑃(𝐼,𝑥,𝑦,𝐹)𝜆[𝑠[𝜋]]⊗𝑤[𝜋]⊗𝜌[𝑑[𝜋]]
is well-defined and in 𝕂. We have [](𝑥,𝑦)=0 when 𝑃(𝐼,𝑥,𝑦,𝐹)=∅.

Composition of weighted finite transducers. Composition is a fundamental operation used to create complex weighted transducers from simpler ones. Let 𝕂 be a commutative semiring and let 1 and 2 be two WFSTs such that the input alphabet of 2 coincides with the output alphabet of 1. Assume that the infinite sum

⨁𝑧1(𝑥,𝑧)×2(𝑧,𝑦) is well-defined and in 𝕂 for all (𝑥,𝑦)∈𝐴∗×𝐵∗. Then, the composition of 1 and 2 produce a WFST denoted by 1∘2 and defined for all x, y by [11]:

[1∘2](𝑥,𝑦)=⨁𝑧1(𝑥,𝑧)⊗2(𝑧,𝑦)
There exists efficient composition algorithm for WFSTs [26]. States in the composition 1∘2 of two WFSTs 1 and 2 are identified with pairs of a state of 1 and a state of 2. Its initial state is the pair of the initial states of 1 and 2. Its final states are pairs of a final state of 1 and a final state of 2. The following rule specifies how to derive a transition of 1∘2 from appropriate transitions of 1 and 2:

(𝑞1,𝑎,𝑏,𝑤1,𝑞2) and (𝑞′1,𝑏,𝑐,𝑤2,𝑞′2)⟹((𝑞1,𝑞′1),𝑎,𝑐,𝑤1⊗𝑤2,(𝑞2,𝑞′2))
In the worst case, all transitions of 1 leaving a state q match all those of 2 leaving state 𝑞′, thus the space and time complexity of composition is quadratic: 𝑂(|1||2|). See [9] for a detailed presentation of the algorithm. The following Fig. 1 illustrates WFSTs composition.

Fig. 1
figure 1
(1) Weighted transducer 1 and (2) weighted transducer 2 over the tropical semiring. (3) Composition of 1 and 2

Full size image
In a general case, when considering the composition of many WFSTs, noted (𝑖)1≤𝑖≤𝑛, the space and time complexity of composition is 𝑂(∏𝑛𝑖=1|𝑖|). In this work, we propose a simple and efficient parallel methods to compute the composition of many WFSTs in MapReduce framework.

MapReduce framework
Big Data is a large and heterogeneous collection of datasets which makes it difficult to process using traditional data processing tools. Nowadays, the collected datasets come mostly from social networks and scientific applications. To overcome the computational and storage of Big data challenges, various solutions have been successfully proposed. Among the popular approaches, there is the famous Hadoop MapReduce framework [15].

In this section, we will focus on how distributed computing program works over the Hadoop MapReduce Model. First, the Hadoop framework and the MapReduce programming model will be briefly presented. Then, we describe how this system processes units of data ⟨𝑘𝑒𝑦,𝑣𝑎𝑙𝑢𝑒⟩ in parallel MapReduce approach.

Hadoop framework
The Apache Hadoop is one of the most popular open source framework for the clustered environment that allows reliable, scalable, and distributed storage. It also helps with the processing of large datasets through the simple programming models. It manages computer clusters built from single to thousands of machines, each offering local computation and storage. The failure of a node in the cluster is automatically managed by re-assigning its task to another node [15].

Fig. 2
figure 2
Overview of Apache Hadoop. https://medium.com

Full size image
The project Apache Hadoop includes four fundamental modules: Hadoop Common or Hadoop Core, HDFS, YARN and Hadoop MapReduce. The following Fig. 2 schematizes these components.

Hadoop Common or Hadoop Core provides essential services and basic processes such as abstraction of the underlying operating system and its file system. It also contains the necessary Java libraries and scripts required to start Hadoop. The Hadoop Common package also provides source code and documentation, as well as a contribution section that includes different projects from the Hadoop Community [15].

Hadoop Distributed File System (HDFS) is a distributed file system developed by Apache Hadoop. It ensures high-throughput storage and access to application data on the community machines thus providing very high aggregate bandwidth across the cluster [15], high fault tolerance and native support of large data sets [27].

Hadoop Yet Another Resource Negotiator (YARN) is a platform to manage cluster resources and schedule tasks. It was added in the Hadoop 2.0 version to increase the capabilities by solving the limit of 4000 nodes and Hadoop’s inability to perform fine-grained resource sharing between multiple computation frameworks [28].

Hadoop MapReduce is an implementation of the MapReduce programming model based on YARN system for parallel processing of large data [29].

The MapReduce programming model
MapReduce is a programming model proposed by Google in 2004 [13] that provides parallel processing of large-scale data. It is easy to use and expresses a large variety of problems as MapReduce computation in a flexible way, which simplifies the data processing in large scale [13]. MapReduce programming model is a system to process the basic unit of information in a ⟨𝑘𝑒𝑦,𝑣𝑎𝑙𝑢𝑒⟩ pair, where key and value are two objects.

This computational model has three principal steps: Map, Shuffle and Reduce as schematized in Fig. 3.

Fig. 3
figure 3
The principal steps of MapReduce [30]

Full size image
During the map step, the model reads each ⟨𝑘𝑒𝑦,𝑣𝑎𝑙𝑢𝑒⟩ pair from a given input files. Then the Mapper operates on one pair at a time by calling the map function defined by the user, produces as output a finite multi-set of new ⟨𝑘𝑒𝑦,𝑣𝑎𝑙𝑢𝑒⟩ pairs, and determines new pair’s sets through a hash function. This allows different machines to process the inputs of a different map in an easy parallel way. The shuffle step occurs automatically, it is done by Hadoop to manage the exchange of the intermediate data from the map task to the reduce task. One can be divide this step into three phases. The sort phase produces the set of intermediate keys received from the buffered mapper in a particular order. It assists the reducer to know that a new reduce task should start when the next key in the sorted input data is different from the previous one. The merge phase group all intermediates input values having the same key key in one list and create the pair (𝑘𝑒𝑦,𝑙𝑖𝑠𝑡𝑜𝑓⟨𝑣𝑎𝑙𝑢𝑒⟩). The partitioner phase determines in which reducer a pair (𝑘𝑒𝑦,𝑙𝑖𝑠𝑡𝑜𝑓⟨𝑣𝑎𝑙𝑢𝑒⟩) will be sent. It is based on a hash function that associates and sent a pair to a reducer. In the last step, the reducers that receive the sorted (𝑘𝑒𝑦,𝑙𝑖𝑠𝑡𝑜𝑓⟨𝑣𝑎𝑙𝑢𝑒⟩) pairs can be executed simultaneously while operating on different keys, they reduce a set of intermediate values which share a key to a new smaller set of values. In our problem, each reducer emits zero, one or multiple outputs for each input (𝑘𝑒𝑦,𝑙𝑖𝑠𝑡𝑜𝑓⟨𝑣𝑎𝑙𝑢𝑒⟩) pairs.

The composition of WFSTs in MapReduce
In this section, we present three methods to perform WFSTs composition using MapReduce framework. We also study the communication cost based on Afrati et al. model [16] and analyze the replication rate for combining WFSTs.

Generic MapReduce algorithm for the composition of WFSTs
In the following, we present a general approach to perform the composition of WFSTs using a single round of MapReduce. Furthermore, we define and detail respectively the map and reduce functions.

The preprocessing phase in our algorithm store in a text file all transitions of WFSTs (𝑖)1≤𝑖≤𝑛, each having m states. A transition t from a WFST 𝑖 will be represented as a 4-tuple as follows : (t,type(s[t]),type(d[t]),index(t)), where type() is a function that maps a state to an element of the set {i (initial), f (final), if ( initial and final), s (simple)} and 𝑖𝑛𝑑𝑒𝑥(𝑡)=𝑖 gives the WFST order index in the composition having the transition t.

figure a
The Map function produces a set of key-value pairs from each input record. In other words, the input transition is replicated and associated with all the keys generated from it based on the mapping method (line 2 of Algorithm 1). The intermediate outputs add a replication rate factor in the cost of MapReduce algorithm. The outputs from the Map function are fed into the Shuffle step. We recall that the Shuffle step occurs automatically in our implementation.

figure b
The Reduce function performs the composition of different transitions lists produced from the Shuffle step as follows: One transition is selected from each WFST such that the input symbol of WFST 𝑖+1 coincides with the output symbol of the previous WFST 𝑖. Moreover, the Reduce function group the transitions w.r.t. their WFST index in a list of sets (line 2 of Algorithm 2), and compute the Cartesian product of those sets (line 4 of Algorithm 2).

In the following, we will discuss the communication cost of the proposed algorithm according to Afrati et al. model [16].

The communication cost model
The communication cost model introduced by Afrati et al. [16] is powerful and simple. This model gives a good way to analyze problems and optimize the performance on any distributed computing environment by explicitly studying an inherent trade-off between communication cost and parallelism degree. By applying this model in a MapReduce framework, we can determine the relevant algorithm for a problem by analyzing the Trade-off between reducer size and communication cost in a single round of MapReduce computation. There are two parameters that represent the trade-off involved in designing a good MapReduce algorithm: the first one is the reducer size, denoted by q, which represents the size of the largest list of values 𝑙𝑖𝑠𝑡𝑜𝑓⟨𝑣𝑎𝑙𝑢𝑒⟩ associated with a key key that a reducer can receive. The global cost is the sum of the computation costs over each reducer processing all its associated values. The second parameter is the amount of communication between the map step and the reduce step. The communication cost, denoted by r, is defined as the average number of key-value pairs that the mappers create from each input. Formally, suppose that we have p reducers and 𝑞𝑖≤𝑞 inputs are assigned to the 𝑖th reducer. Let |I| be the total number of different inputs, then the replication rate [16] is given by the expression 𝑟=∑𝑝𝑖=1𝑞𝑖/|𝐼|.

Notice that limiting reducer size enables more parallelism. Small reducers size force us to redefine the notion of a key in order to allow more, smaller reducers and thus allow more parallelism with available nodes.

Lower bound on the replication rate
The replication rate is intended to model the communication cost, which is the total amount of information sent from the mappers to the reducers. The trade-off between reducer size q and replication rate r, is expressed through a function f, such that 𝑟=𝑓(𝑞). The first task in designing a good MapReduce algorithm for a problem is to determine the function f, which gives us a lower bound of the replication rate r [16].

Let us now derive a tight upper bound, namely g(q), on the number of outputs that can be produced by a reducer of size q for WFSTs composition. If there are n deterministic WFSTs (𝑖)1≤𝑖≤𝑛, each one having |𝑖||𝐴𝑖| transitions for each input alphabet. Let  be the result of the composition 1∘2∘…∘𝑛. To compute a transition in , a reducer needs to receive n transitions, one from each WFST 𝑖. Then, the WFST  can reach ∏𝑛𝑖=1|𝑖| transitions. Assume that a reducer of size q receives 𝑞𝑛 transitions from each WFST 𝑖 evenly distributed such that the input alphabet of 𝑖+1 coincides with the output alphabet of 𝑖. The following lemma gives us an upper bound on the output of a reducer.

Lemma 1
When computing the composition =1∘2∘…∘𝑛 a reducer of size q can produce no more than 𝑔(𝑞)=𝑞𝑛 outputs.

From [16], one can compute a lower bound on the replication rate for the composition of WFSTs as a function of q using the following expression:

𝑟≥𝑞×|𝑂|𝑔(𝑞)×|𝐼|
where |I| denote the input size, and |O| denote the output size. The input size for our problem is the sum of transitions from all input WFSTs 𝑖 , that is |𝐼|=∑𝑛𝑖=1|𝑖|, and the size of the output is the size of  i.e. |𝑂|=|𝐴1|×∏𝑛𝑖=1|𝑖|∏𝑛𝑖=1|𝐴𝑖|.

Consequently, the lower bound on the replication rate for the composition of WFSTs will be as follows.

Proposition 1
The replication rate r for the composition =1∘2∘…∘𝑛 is

𝑟≥𝑛×|𝐴1|×∏𝑖=1𝑛|𝑖|∑𝑖=1𝑛|𝑖|×∏𝑖=1𝑛|𝐴𝑖|
Methods
This section includes the description of three mapping methods in order to design a suitable key format that maps a set of transitions to the same reducer. Explicitly, we will define the function getTransitionFrom(t) called in line 2 of Algorithm 1. In this section, we also present a formal analysis of the communication cost by computing an upper bound on the replication rate for each mapping method. Recall that we consider the composition of n WFSTs, each having m states.

States based mapping method
In our first method, for a transition 𝑡∈𝐸𝑖 from a WFST 𝑖, the map function generates a set of keys of the form 𝐾𝑠𝑡𝑎𝑡𝑒=(𝑖1,𝑖2,…,ℎ(𝑠[𝑡]),…,𝑖𝑛) , where h be a hash-function from 𝑄𝑖 to {1,…,𝑚} and 𝑖𝑗∈{1,…,𝑚}. Consequently, the mappers produce the set of key-value pairs of the form ⟨(𝑖1,𝑖2,…,ℎ(𝑠[𝑡]),…,𝑖𝑛),𝑡⟩. By way of explanation, suppose we have 𝑚𝑛 reducers, then each transition is sent to 𝑚𝑛−1 reducers.

The function g(q) will be affected by the presence inside the same reducer of some transitions that cannot be combined. This give us a new upper bound on the number of outputs each reducer can produce, formally

𝑔(𝑞)=|𝐴1|
The following proposition gives an upper bound on the replication rate.

Proposition 2
The replication rate r in the state-based mapping scheme is

𝑟≤𝑞×∏𝑖=1𝑛|𝑖|∑𝑖=1𝑛|𝑖|×∏𝑖=1𝑛|𝐴𝑖|
Notice that from Propositions 1 and 2, the upper bound on replication rate exceeds the lower bound by a factor of 𝑞𝑛×|𝐴1|. As a consequence, this mapping scheme is suitable when considering a small set of input alphabets.

Input alphabet based mapping
In the second method, transitions will be mapped to their input alphabet. Let us define the key 𝐾𝑖𝑛𝑝𝑢𝑡=(𝑗1,𝑗2,…,𝑗𝑖,…,𝑗𝑛) associated with an input symbol a and let ℎ𝑎 be a hash-function with range {1,2,…,𝑘}. One can associate a transition 𝑡=(𝑠𝑖,𝑎𝑖,𝑏𝑖,𝑤𝑖,𝑑𝑖)∈𝐸𝑖 from the WFST 𝑖 to a key 𝐾𝑖𝑛𝑝𝑢𝑡 if there exists some 𝑗𝑖=ℎ𝑎(𝑎𝑖). Thus, we have ∏𝑛𝑗=1|𝐴𝑗| available reducers and each transition from 𝑖 is send to ∏𝑛𝑗=1|𝐴𝑗||𝐴𝑖| reducers. Since the map task processes ∑𝑛𝑖=1|𝑖| transitions and each WFST 𝑖 have |𝑖||𝐴𝑖| transitions associated with an input symbol 𝑐∈|𝐴𝑖|, the total number of transitions sent to each reducer is 𝑛×|𝑖||𝐴𝑖| which can be approximated by 𝑛×𝑚. However, the function g(q) is influenced by the presence of incompatible output and input symbols combination inside a reducer. This gives us a new upper bound on the number of outputs each reducer can produce, formally

𝑔(𝑞)=|𝑄1|
Proposition 3
The replication rate in the input alphabets based mapping scheme is

𝑟≤𝑞×|𝐴1|×∏𝑖=1𝑛|𝑖||𝑄1|×∑𝑖=1𝑛|𝑖|×∏𝑖=1𝑛|𝐴𝑖|
From the Propositions 1 and 3, the upper bound for the replication rate overtake the theoretical lower bound by a factor of 𝑞𝑛×|𝑄1|. Therefore, the input alphabet based mapping is best suited for situations where the considered WFSTs sizes are small.

Hybrid mapping based on both input and output alphabets
In the last method, we propose a hybrid mapping based on both input and output alphabets. In other words, keys will be associated to a pair of input and output symbols. Formally, a transition 𝑡∈𝐸𝑖 from a WFST 𝑖 will be paired to a set of keys having the form 𝐾ℎ𝑦𝑏𝑟𝑖𝑑=(𝑗1,𝑗2,…,ℎ𝑖𝑎(𝑡),ℎ𝑜𝑏(𝑡),…,𝑗𝑛), where ℎ𝑖𝑎() be an input symbol hash functions from ⋃𝑛𝑖=1𝐴𝑖 to {1,2,…,𝑘} and ℎ𝑜𝑏 be from be an output symbol hash functions from ⋃𝑛𝑖=1𝐵𝑖 to {1,2,…,𝑘}. Transitions from 1 will be mapped according to their output symbols and those of 𝑛 according to their input symbols. Consequently the number of reducers is 𝑘𝑛. However the function 𝑔(𝑞)=|𝑄1|.

Since transitions from 2 until 𝑛−1 are sent to 𝑘𝑛−2 reducers, the following Proposition holds.

Proposition 4
The replication rate in the hybrid mapping method is strictly less than the replication rate in the input symbol mapping method.

By comparing the Propositions 2, 3, and 4, we deduce that the upper bound of the replication rate in the hybrid mapping method is the closest one to the theoretical lower bound. Thus, this method is best suited for situations when the alphabet size is less than or equal to the number of states.

Results and discussion
This section includes extensive experiments to evaluate the efficiency and effectiveness of the proposed methods in term of communication cost and execution time for computing the composition of five WFSTs 1∘2∘…∘5. Experiments are conducted on a large variety of WFST data sets randomly generated using FAdo library [31] with various combinations of attributes including a number of states |Q|, input alphabet size |A|, and output alphabet size |B|.

Cluster configuration
Our experiments were run on Hadoop on the French scientific testbed Grid’5000 [32] at the site of Lille. We used for our experiments a cluster having 15 nodes, 30 CPUs, 300 cores. Each node is a machine equipped with two Intel Xeon E5-2630 v4 with 10-cores processors, 256 GB of main memory, and two disk drives (HDD) at 300 GB. The machines are connected by 10 Gbps Ethernet network, and run 64-bit Debian 9. The Hadoop version installed on all machines was 2.7.

Data generation method
We randomly generated a large variety of WFST data sets in two phases. First, we generated a set of deterministic finite automata using FAdo library [31], which is an open source project providing a set of tools for the symbolic manipulation of automata. FAdo is based on enumeration and generation of initially connected deterministic finite automata [33]. In the second phase, we implemented a function that randomly adds weights and some nondeterministic degree on the output symbols over transitions of the finite automata of the first phase with a uniform distribution. Our generation technique produces WFSTs based on two parameters: the number of states m and the input alphabet size k. Thus, one can define the transition density of the generated WFST as the ratio |𝐸|𝑘×𝑚, the final state density as the ratio |𝐹|𝑚 and consider a unique initial state as in [34].

Communication cost analysis
Let us evaluate the communication cost of the proposed methods on large scale WFST data sets. The communication cost is defined as the total number of key-value pairs transferred from the map phase to the reduce phase. It can be optimized by minimizing the replication rate parameter i.e. the number of input copies sent to the reducers.

The following table gives us the relationship between the considered data set sizes and the communication cost:

Input size	Communication cost (GB)	Output size
File size (kb)	∑51|𝐸𝑖| (×103)	|𝑄𝑖|	|𝐴𝑖|	|𝐵𝑖|	Hybrid mapping	Input alphabet mapping	States mapping	|E| (×109)	Size (GB)
132	6	75	16	16	10	22	10,699	38	1765
227	10	63	32	32	252	601	9032	31.8	1486
342	15	47	64	64	5896	14,402	4189	14.7	684
411	18	60	60	60	5465	13,327	13,327	46.7	2194
The obtained results show clearly that in general, the communication cost using the hybrid mapping method is minimal in all situations i.e. for all combinations of state sizes and alphabet sizes. This is due to the fact that the number of the transition copies sent to the reducers with this method is less than the other ones as proved formally in Proposition 4. In some particular cases of WFSTs, when the state size is less than the alphabet size, the state based mapping has less communication cost. This coincides with the Propositions 2 and 3.

Computation cost analysis
The Computation cost is the time required to execute a MapReduce job. The graphs below, Figs. 4, 5, 6 and 7, show comparative results in term of the execution time of the three methods: states based mapping (State), input alphabet based mapping (Input), and the hybrid based mapping (InOut).

Fig. 4
figure 4
Execution times of three methods for the alphabet size K=16

Full size image
Fig. 5
figure 5
Execution times of three methods for the alphabet size K=32

Full size image
Fig. 6
figure 6
Execution times of three methods for the alphabet size K=64

Full size image
Fig. 7
figure 7
Execution times of three methods when the number of states equals the alphabet size

Full size image
Figures 4, 5 and 6 present the execution time of the three proposed methods for different data sets sizes when varying the alphabet size to be 16, 32 or 64. In the three cases, the growth rate of hybrid and input alphabet mapping are close together and much less of states mapping. For example when K = 16, the growth rate of InOut, Input and State are 21.06, 21.15 and 255.80, respectively. Figure 7 shows a comparison of the three methods when the alphabet size equals the number of states. As foreseen, the hybrid method is clearly more efficient when the alphabet size is less than or equal to the number of states, and the execution time by the transition in this method is lower than two other methods.

Minimizing the replication rate decreases the time used by the mappers to replicate each transition, and avoid the existence of transitions that cannot be combined inside the same reducer. At the same time, it reduces the number of transitions that are assigned to a reducer. On the other hand, using an adequate number of reducers diminishes the waiting time a reducer spends to use a CPU.

Conclusion
In this paper, we presented a new parallel approach to compute the composition of WFSTs on a large scale in MapReduce framework. We described in detail three methods to carry out this task using a single round of MapReduce. Moreover, we analyze the communication and computation cost for each method. Finally, we evaluated the performance of the three methods on different data sets. The obtained results show that the best method in terms of the execution time is the one that minimizes the number of reducers and optimizes the inputs replication rate.

As a perspective, this work is considered as the first step to apply this method on real world problems. First target application is the design of a new distributed cryptosystem based on finite automata, the so-called finite automaton public key cryptosystem. In this application, the public key is a composition of n + 1 finite automata, and, the private key is the n + 1 weak inverse finite automata of them [4]. The second target application is in natural language processing to handle tasks such as translation between two languages, using one or multiple intermediate languages and for speech recognition. We also hope to extend and, test this method on a multi-nodes cluster environment with the GPU and OpenMP to accelerate the composition algorithm for large-scale computing.