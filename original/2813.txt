In the next decade, machine vision technology will have an enormous impact on industrial works because of the latest technological advances in this field. These advances are so significant that the use of this technology is now essential. Machine vision is the process of using a wide range of technologies and methods in providing automated inspections in an industrial setting based on imaging, process control, and robot guidance. One of the applications of machine vision is to diagnose traffic accidents. Moreover, car vision is utilized for detecting the amount of damage to vehicles during traffic accidents. In this article, using image processing and machine learning techniques, a new method is presented to improve the accuracy of detecting damaged areas in traffic accidents. Evaluating the proposed method and comparing it with previous works showed that the proposed method is more accurate in identifying damaged areas and it has a shorter execution time.

Introduction
Over the past three decades, visual or computer technology has been widely used in the military space industry and to a limited extent in industry. The new technology, the lack of a cost-effective system in the market, and the lack of specialists in this field have made this technology not widely used. Until recently, used cameras and sensors were usually customs made to be used for a specific purpose. Also, the process of building very large integrated circuits was not advanced enough to make high-resolution solid-state sensors. The use of the mentioned sensors required the development of special software for it. This software’s usually also needed computers with high processing power. In addition to all this, engineers had to receive the necessary training after graduation. Because the visual machine course was not offered in the classical form at the level of conventional engineering education in universities [1, 2].

Visual machine techniques will have a significant impact on all industrial works in the next decade due to the latest technological advances in this, and these advances are such that the use of this technology is now vital. Machine vision involves the process of applying a wide range of technologies and methods in providing automated inspection based on imaging, process control, and robot guidance in industrial applications. While the scope of machine vision is wide and it is difficult to provide a comprehensive definition for it, here is a general definition accepted for machine vision: Machine vision involves the analysis of images in data mining. To control a process or activity [3, 4].

Machine vision is frequently used to detect traffic accident damage as well as diagnose the crash site's condition [5, 6]. This article proposes a new method for improving the accuracy of detecting damaged areas in traffic accidents using image processing and machine learning techniques. In this method, first, the image of the car is categorized using image segmentation and then the damaged areas will be identified using car vision techniques. By combining the particle optimization algorithm with a voting classifier, the proposed method is able to greatly improve the final accuracy of crash detection.

In the continuation of this article, in “Related works” section, a review of the research literature in the field of machine vision will be done. In “Proposed method” section, the proposed method will be stated. In “Experimental results” section, the proposed method will be evaluated, and the performance of the proposed method will be compared with previous methods. Finally, in “Conclusion” section, the final summary of this article will be made.

Related works
Computer vision is one of the most modern and diverse branches of artificial intelligence, which, by combining image processing methods and machine learning tools, enables computers to intelligently see objects, landscapes, and intelligently "understand" their various properties. Of all the branches of artificial intelligence, perhaps the most practical is computerization and mechanization of visual systems. The scope of application of this growing branch of technology is very wide and ranges from common applications such as production line quality control and video surveillance to new technologies such as driverless cars. The range of applications of this technology varies according to the techniques used in them [7].

The main tasks in computer vision include the following [8]:

Object detection: Detects the presence or state of an object in an image. For example, we can search for digital images based on their content (content-oriented retrieval of images), identify human faces and their position in photographs, and estimate the three-dimensional state of humans and their organs.

Tracking,

Scene interpretation,

Self-location,

Computer vision systems.

A system of computer vision can be divided into the following subsystems:

A.
Imaging: the image or sequence of images is taken with an imaging system (camera, radar, tomography system). Usually, the imaging system must be set up before use.

B.
Preprocessing: the purpose of this step is to reduce noise (noise reduction—to separate the signal from the noise) and to reduce the total amount of data. This is typically done using a variety of image processing (digital) methods. Such as image sampling, application of digital filters, image segmentation, estimation of heterogeneity in high-resolution images, and multi-precision analysis.

C.
Feature extraction: the purpose of feature extraction is to reduce most of the data to a feature set, which should be safe from disturbances such as lighting conditions, camera position, noise, and distortion.

D.
Recording: the purpose of the recording step is to establish a correspondence between the properties of the captured set and the properties of the known objects in a model database or the properties of the original image.

Machine vision technology and digital imaging include processes that require the application of various computer software engineering sciences. This process can be divided into several main categories:

1.
Creating an image in a digital form,

2.
Using computer techniques to process or modify image data,

3.
Reviewing and using the processed results for purposes such as robot guidance or controlling automated equipment, quality control of a production process, or providing information for statistical analysis in a computer production system.

Image segmentation is used in areas such as image processing, machine vision, medical image processing, digital libraries, content-based data retrieval in images and videos, data transfer over the Internet, and image compression. One of the most important issues in image processing, identification, and separation of the image into its components is fragmentation, which determines the success or failure of image analysis methods. The first step in image analysis is segmentation. Fragmentation is a process that divides an image into its main parts. This means that the various objects in the image are separated according to the intended use to make the image analysis easier in the next steps. One of the applications of fragmentation is in image processing, which is considered in most branches of science and industry today, and in many of these branches, the identification of the main components of the image is very important [10, 11].

Reference [12] uses a combination of c-means fuzzy clustering algorithm and particle optimization algorithm for image segmentation. In the method proposed in this paper, instead of using the Euclidean distance criterion in fuzzy clustering, the Mahalabonis distance criterion [12] is used. The particle optimization algorithm has also been used to optimize clustering centers. The simulation results showed better performance of the method proposed in this paper than the previous methods.

In 2013, Xiang et al. [13] used a combination of c-means fuzzy clustering algorithm with area segmentation algorithms for the color image segmentation problem, which is one of the most important issues in machine vision and pattern recognition. In this method, the threshold of the histogram function is used to divide the image into several areas. Finally, these regions are used to initialize the c-means fuzzy clustering algorithm. The experimental results showed that the proposed method has good accuracy and speed in image segmentation.

Reference [14] presents an improved method for image segmentation using the c-means fuzzy clustering algorithm and the particle optimization algorithm. In the proposed method in this paper, three different techniques have been used to increase the performance of image segmentation: (1) Improving the initialization in the fuzzy clustering algorithm using the particle optimization algorithm. (2) Using the Mahalanobis distance criterion [12] to reduce the effect of the geometric shape of the colors. Correct the final clustering results for incorrectly clustered pixels. The simulation results showed the performance improvement of the proposed method compared to the previous works.

One of the most recent studies conducted in 2015 on the issue of image segmentation is the paper presented by Ayala et al. [15]. In this paper, the Differential Evolutionary Algorithm (BDE) is used to segment the images. In the method presented in this paper, the BDE algorithm is used to determine the n−1 threshold used in color image segmentation. The simulation results showed that the proposed algorithm has better performance than other evolutionary algorithms used for image segmentation.

Another study published in 2015 used Markov’s hidden model for color images [16]. In the segmentation method presented in this paper, expectation–maximization, parameter estimation and Bayesian multiscale have been used. In the experiments, the proposed method was compared with the latest image segmentation methods and the results showed better performance of the proposed method in this paper.

Numerous methods also use image processing and machine learning techniques to classify images. In most early studies of image classification, of the many visual features of each image, only one has been used. As a result, it can be expected that the accuracy of these systems is not very good because in general, an image has several visual characteristics. Recently, effective research has been conducted on image retrieval using a combination of color and texture properties. In [17], two-dimensional or one-dimensional histograms of color coordinates are used as color features, and differences extracted by analyzing discrete causal frameworks are used as texture features. Lane et al. [18] suggest three-image properties for use in image retrieval. The characteristic of the first image is based on the color distribution and is called the adaptive color histogram. The properties of the second and third images are called the simultaneous matrix and the gradient histogram, respectively, and are based on the properties of color and texture. Also in [19] the combination of genetic algorithm and feature selection has been used to retrieve and categorize images. In the method presented by them, in the first step, the initial features of the image are extracted, and then, based on a feature selection method based on a genetic algorithm, the final features of the image are selected for the image classification operation.

In [20] Hee et al. Proposed a new method in which texture image properties are extracted using inseparable filter wavelets to retrieve the tissue image. This new method provides more information about the edge and direction of texture images compared to previous tensor product wavelets (such as db wavelets). Tagarakis et al. [21] proposed a method for non-rotational tissue retrieval that uses the non-Gaussian behavior of sub band coefficient distributions and shows tissue information with a guided pyramid. Hahn et al. [22] proposed immutable Gabor representations in which each representation requires only a few summaries of Gabor filter impact responses; Texture properties are extracted from these new representations to retrieve the unchanged texture image.

In the field of image retrieval, Nastagal [23] has introduced a method for calculating the fuzzy similarity criterion that uses the fuzzy part of the HSI color space. In this method, if there is no color in one image to calculate the degree of similarity of the ratio of that color between two images, by increasing the color ratio in the other image, the similarity criterion remains constant at zero; Although this method makes perfect sense, it does not make sense. Besides, the color spectrum feature includes 360 elements that are divided into 8 sections. The number of these sections is not enough, and very large errors appear in the calculation of the complex image similarity criterion.

In the field of image classification, Liu Pengio et al. [24] have proposed a modified fuzzy c-means algorithm to solve the problem of large-scale image retrieval and classification, in which the initial condition of time-saving is considered. In this method, some images are always considered as just one large cluster, and therefore the classification result did not work well.

Proposed method
One of the applications of machine vision is in crash detection. In fact, due to the high number of accidents in Iran and the need for the timely presence of relief and law enforcement centers, the existence of an intelligent system that can detect the accident and inform the relevant centers can be very useful in speeding up the accident detection. In this study, an attempt is made to present a new method for crash detection as well as diagnosis of damaged parts of the car. The Fig. 1 shows the general flowchart of the proposed method.

Fig. 1
figure 1
Overall flowchart of the proposed method

Full size image
Initial car image segmentation
To identify different areas of the machine, it is necessary to perform the image segmentation step beforehand. In fact, by dividing the image and splitting the image into different parts. Then a label can be assigned to each of these parts in the next step, and the image classification step can be performed. The main innovation of this section is to provide a method to improve the segmentation of color images of accidents. The research tries to study the previous methods for color segmentation of color images to consider the strengths and weaknesses of each and to present a new method for color segmentation.

Previous segmentation-based methods in crash prediction have the following four major weaknesses:

Inefficiency for large images

Inability to process blurry images

Weakness to segment images in different lighting modes

Noise sensitivity

This article tries to provide solutions to all these problems.

In the first stage of image segmentation, the step of reducing the initial noise of the image is performed. In this article, Gaussian filler is used to reduce image noise. Gaussian filter is one of the conventional filters in reducing digital image noise that by adjusting the dimensions and variance of this filter, the effect of noise reduction and smoothing can be controlled. Since the noise added by the scanners is generally Gaussian, the use of this filter as a preprocessing step in image processing is quite common, which has also been used in the proposed algorithm. In the proposed method, using a Gaussian filter with a size of 9×9, the noise caused by imaging and digitizing it is reduced and its image information is extracted for the next parts.

After reducing the image noise, the image histogram is used to optimize the cluster centers in the image segmentation. A graph that shows the number of repetitions of different levels of gray in an image is called a histogram. In this diagram, the x-axis represents each of the gray planes and the y-axis represents the number of pixels corresponding to the different gray levels. To get the image histogram, it is enough to calculate the number of pixels of each brightness level by traversing the total pixels of the image.

The image histogram pan has peaks that can be considered as the center of the parts. Therefore, if the location of the separation of these peaks is detected, any part in the image can be identified. The histogram of each image is different and the location of the histogram peaks is not necessarily a specific region, and in some cases, the graph of each histogram increases with a slight decrease towards the other peak. So we need to use an algorithm to find the pixels around a peak to distinguish it from the existing peaks.

To calculate the competency function, we used a combination of two goals: the sum of the differences between the clustering centers and the pixels belonging to that class, as well as the distance between the centers. The smaller the difference between the clustering centers and the pixels belonging to that class and the greater the distance between the clustering centers, the higher the fit function. The objective function of the optimization algorithm is the sum of two different functions, one of which must be minimized and the other on should be maximized.

The sum function of the difference between the clustering centers, and the pixels belonging to the same class is shown below.

∑𝑖=1𝑐∑𝑘=0255𝐷(𝑉𝑖,𝑋𝑘)×ℎ(𝑘)
(1)
where, 𝑘 is The range of intensity values from 0 (black) to 255 (white), and 𝑐 is the number of clusters, 𝐷(𝑉𝑖,𝑋𝑘) represents the distance of 𝑃𝑖𝑥𝑒𝑙𝑘 from the 𝐶𝑒𝑛𝑡𝑒𝑟𝑖. Also, h (k) represents the normalized histogram of 𝐿𝑖𝑔ℎ𝑡_𝐼𝑛𝑡𝑒𝑛𝑠𝑖𝑡𝑦𝑘. As can be seen in the above relation, in this relation the smaller the total distance of different pixels from the center of their respective clusters, the smaller the objective function will be. In this regard, the histogram value of each pixel also affects the value of the objective function. The larger the histogram of a pixel, the greater the effect on the value of the objective function. In the proposed method in this study, it is preferred to select the centers of each piece close to the peaks of the histogram to reduce the segmentation error. Combining this goal, which is the error of clustering different parts to distance the centers of the clusters from each other, can provide a suitable objective function in segmenting images. In this paper, the particle swarm optimization (PSO) algorithm will be used to optimize the specified fit function. In fact, in this paper, the PSO is used to optimize the peaks as well as the image segmentation. PSO is a universal method of minimization that can be used to deal with problems whose answer is a point or surface in n-dimensional space. In such a space, an initial velocity is assigned to them, and communication channels between the particles are considered. These particles then move in the response space, and the results are calculated based on a “competency criterion” after each time interval. Over time, particles accelerate toward particles that have a higher competency standard and are in the same communication group. Although each method works well in a range of problems, the particle optimization algorithm of the method has shown great success in solving continuous optimization problems.

At each step of the PSO algorithm, the position and velocity of each particle are updated. Two criteria are used to do this. The first is the best position the particle has ever reached, known as the pbest, and the second is the best position ever obtained by the particle population. This position is displayed with gbest. In the particle optimization algorithm, each particle is displayed in the search space with the 𝑥𝑖 vector. It also has a velocity of 𝑣, which is indicated by the vector 𝑣𝑖. In each repeated iteration, the velocity and location of each particle are measured using the following equations.

𝑥=𝑥+𝑣
(2)
𝑣=𝑣×𝜔+𝐶1×𝑟1×(𝑝𝑏𝑒𝑠𝑡−𝑥)+𝐶2×𝑟2×(𝑔𝑏𝑒𝑠𝑡−𝑥)
(3)
Here, 𝑥 is the position of particle, 𝑣 is velocity of particles, 𝜔 is an input weight that determines the effect of velocity on the previous repetition on current velocity. Also, 𝐶1 and 𝐶2 are two random values between zero and one. Also in Eq. (3) the × operator indicates a Vector Product between to matrix. In this equation, the position of the particles is updated. The parameter w is a constant weight of inertia, and for the classic version of PSO, this value is a constant positive value. In the classic version of PSO, the value of the parameter w is positive. This parameter is important for balancing global search, which is also called exploration (when higher values are set) and local search (when lower values are set). One of the most important differences between the classical PSO algorithm and other versions derived from this algorithm is the w parameter. The velocity that updates the first expression in the equation is the internal multiplication of the parameter w and the anterior velocity of the particle. Therefore the previous motion of the particle is displayed to the current motion. Hence, for example, if w = 1, the motion of the particle is completely affected by its previous motion; Therefore, the particle may continue to move in the same direction.

The pseudocode of PSO algorithm for this work is shown in Algorithm 1.

figure a
Classification of different areas of the car
After the segmentation stage, it is the turn of the image classification stage. The main purpose of many image processing algorithms is to extract specific features of the image, so that based on these properties, interpretations of the image appear or decisions are made. Such operations are frequently used, especially in the field of machine vision.

In image processing, image classification and retrieval are two very important operations. Image recovery methods are usually based on the color, texture, shape, and meaning of the image. Today, the key parts of image classification and retrieval are based on three aspects. (1) Different methods of calculating the degree of similarity, which include extracting the color feature and combining several features, (2) Reciprocal retrieval figures using the feedback grid, and (3) Image classification using the fuzzy clustering method. Recently, accuracy and recall rates have been used to evaluate the performance of image retrieval results.

In recent years, due to the growing need for optimal image classification in large databases, extensive research has been conducted in the field of image classification. The first ideas were put forward in the early 1990s, in which images were categorized without regard to their visual features and based solely on text annotations; In such a way that the concepts in the image were recognized by the operator and stored in the database as the keywords of the image. In this way, users had access to the relevant images using their desired keywords. This method is called text-based image classification [25].

In the field of image classification, various methods have been introduced to calculate the fuzzy similarity criterion, which uses the fuzzy part of the HSI color space. In this method, if there is no color in one image to calculate the degree of similarity of that color ratio between two images, by increasing the color ratio in the other image, the similarity criterion remains constant at zero; Although this method is generally logical, its implementation does not seem reasonable [26]. In addition, the color spectrum feature includes 360 elements that are divided into 8 sections. The number of these sections is not enough, and very large errors appear in the calculation of the complex image similarity criterion. Various things have been done in the field of image classification. In some previous work, to solve the problem of retrieving and classifying large-scale images, a fuzzy c-means algorithm has been proposed in which the initial condition of saving time is considered, but in this method, some images Are always considered as a single large cluster and therefore, the result of classification has not been very efficient [27, 28].

The HSI color model (hue, saturation, color intensity) is closely related to human interpretation of color. The HSI model also has the advantage of separating color and gray information in an image.

HSI color space is introduced when the user wants to specify color properties. This model needs a strong cluster of skin colors for better performance. Users can specify color properties by specifying this color model. For this model to perform better, a cluster of skin colors is necessary.

The HSI color model in a color image separates the light intensity components from the color carrier information (shade and saturation). As a result, this color space will be suitable for color-based image processing algorithms. Important components of the HSI color space are the axis of vertical light intensity, the length of the vector from the origin to the color point, and the angle that this vector has with the red axis.

In this paper, a method of classifying and retrieving different parts of an image using clustering and fuzzy similarity criteria is introduced. In this method, the criterion of similarity of two images, even when there is no color in one of the two images, seems reasonable. On the other hand, choosing a clustering center would be much more logical than the method mentioned in the previous methods, and thus, the classification result would be more efficient.

In most of the fuzzy clustering methods previously proposed for image retrieval, some images are always considered as just one large cluster, and therefore the classification result does not work well. In this paper, a method for classifying and retrieving a color image using clustering and fuzzy similarity criteria is introduced. In this method, the criterion of similarity of two images, even when there is no color in one of the two images, seems reasonable. On the other hand, choosing a clustering center would be much more logical than the method mentioned above, and thus the classification result would be more efficient.

In this paper, a method is proposed in which the similarity between two images is measured by considering the color characteristics of an image such as the color spectrum. The F-test criterion has also been used in this study; The criterion that finds the best threshold for finding fuzzy segmentation [29]. In this paper, a framework is introduced in which the images can be compared with each other concerning the fuzzy similarity criterion, which is calculated based on the color spectrum feature vector, and thus, the image can be retrieved. Image classification is also done using fuzzy clustering.

Final areas damage detection
At this stage of the proposed method, the final diagnosis should be made based on the training phase. The image pattern of the crash car, as well as the damaged parts of the car, should be identified. An important issue concerning data mining applications such as pattern recognition is the problem of large data sets in which the number of attributes is much larger than the number of patterns. For example, in recognizing control chart patterns that include very large datasets, the classifier parameters also increase. As a result, the performance of the classifier decreases dramatically. High-dimensional datasets reduce the performance of the classifier in two ways. On the one hand, with increasing the size of the data, the computational volume increases, and on the other hand, the model based on high-dimensional data has low generalizability, and the probability of over-fitting increases [30,31,32]. As a result, reducing the size of the problem can both reduce computational complexity and improve the performance of classification algorithms. It can be said that due to the special characteristics of the data that the data are independent and all-distributed, abnormal, autocorrelated, and multivariate, the traditional methods of detecting the control chart pattern for this data will not be very efficient. As a result, it is necessary to first reduce the data dimensions using a dimensional reduction method and then use an efficient classification algorithm for the final diagnosis.

In this paper, a new graph-based technique will be used for the feature extraction step. To do this, first, the initial features of the image are represented as a graph, and then based on the represented features and using the graph fitting technique, the final features are selected for the pattern prediction step.

Many existing methods use a single classification to identify the pattern of the accident [33,34,35]. Since each classification has its own characteristics, individual classifications have less combinability than a combination of multiple classifiers. In other words, multiple classifications are usually more accurate because of the variety of constructor classifiers and their function [36,37,38,39]. In this paper, an attempt is made to use a strategy based on the classifier combination using the majority voting mechanism to effectively combine the predictions from different classification algorithms [40,41,42]. For this purpose, in this paper, we used five classifiers: Support Vector Machine (SVM), decision tree (DT), nearest neighbor (KNN), simple Bayesian (NB) and Neural Network (NN). The subset of the feature selected in the previous input step constitutes the control pattern recognition model. Given that the final prediction of the crash pattern from the prediction combination of each of these classifiers is done using a learning mechanism, the final prediction model of the classifier is based on the majority vote. In this research, for the first time, a classifier combination based on majority voting and the graph fitting reduction technique [43] is used to improve the accuracy of accident detection and the detection of damaged areas.

Experimental results
In this section, the performance of the proposed methods for the problem of detecting damaged areas in traffic accidents is evaluated. For this purpose, the proposed method of one of the new methods in this field is compared.

MATLAB programming language is used in this article to implement feature selection methods. Also, all the tests in this article were performed on a system with a 2.3 GHz Corei3 processor and 2 GB of internal memory (RAM). In the rest of this section, the data sets used to identify accident-affected areas, as well as practical results, are described, respectively.

In this work, a set of crash images collected from YouTube and used in the base article was selected to evaluate the proposed method. This data set includes 300 surveillance videos at a speed of 30 frames per second (FPS) from which various images of accidents were extracted. All data samples tested by this model are CCTV footage recorded at road junctions around the world. This dataset includes accidents in various environmental conditions such as strong sunlight, daylight hours, snow, and night hours.

Figure 2 shows an example of the images in this dataset.

Fig. 2
figure 2
An example of the used images data

Full size image
The proposed methods include various parameters that must be quantified before the classifier design process. It should be noted that some of these parameters are not limited to these proposed methods, and in many classification design methods based on evolutionary algorithms, these parameters must also be set. The appropriate values for some of these parameters have been selected by trial and error after several initial runs, so they do not mean the best value for these parameters. The number of iterations (I) parameter indicates the number of iterations of the particle swarm optimization algorithm. In most cases, the proposed method with less than 100 iterations will be able to search for the optimal subset. The higher this parameter is set to a larger number, the higher the execution time of the algorithm and the higher the chance of achieving the global optimal. Of course, it should be noted that in many evolutionary algorithms, after a certain number of iterations, the algorithm converges, and then there is no change in the population. The population size parameter (A) is a parameter affecting the performance of the particle optimization algorithm that must be properly quantified. This parameter specifies the number of particles in the algorithm. In the proposed methods, the value of this parameter is set to 50. It is clear that the larger the value of this parameter, the more particles there are in the population and the higher the probability of searching the entire problem space. On the other hand, too much value for this parameter may prevent the convergence of the particle optimization algorithm. Therefore, the correct value of this parameter so that it is neither too big nor too small has a huge impact on the convergence of the algorithm. The other two parameters in the particle optimization algorithm are C1 and 𝐶2, which determines the importance of the individual and social position of each particle. In the proposed method, these two parameters are set to 1.5. Table 1 shows the values and specifications of these parameters.

Table 1 The parameters of the proposed method
Full size table
In this section, we introduce the classification evaluation criteria that we use to evaluate the performance of the proposed systems.

The three criteria that are mainly used to evaluate the quality of performance of algorithms for detecting damaged areas are Sensitivity, Specificity, and Classification Rate, which are introduced in the following terms, respectively.

𝑆𝑒𝑛𝑠𝑖𝑡𝑖𝑣𝑖𝑡𝑦=𝑇𝑃𝑇𝑃+𝐹𝑁,
𝑆𝑝𝑒𝑐𝑖𝑓𝑖𝑐𝑖𝑡𝑦=𝑇𝑁𝑇𝑁+𝐹𝑃,
𝐶𝑙𝑎𝑠𝑠𝑖𝑓𝑖𝑐𝑎𝑡𝑖𝑜𝑛𝑅𝑎𝑡𝑒=𝑇𝑃+𝑇𝑁𝑇𝑃+𝐹𝑃+𝑇𝑁+𝐹𝑁,
where TP, TN, FP, and FN represent True Positive, True Negative, False Positive, and False Negative, respectively.

Another criterion used in this article to compare different methods is execution time. In fact, using this criterion, the computational complexity of different methods is examined. Clearly, the less time a classifier is able to identify damaged areas, the more efficient it is and the less computationally complex it is.

In experiments on the proposed method, the data set is randomly divided into training data, experimental data. For this purpose, 70% of the data set is considered educational data, and the remaining 30% is experimental data. Also, in all experiments, after identifying the educational and experimental sets, each classification method is performed ten times, and an average of ten different implementations is used to compare different methods.

Figures 3 and 4 show an example of the area detected in the image and area detection using the proposed method.

Fig. 3
figure 3
First example of areas damage detection

Full size image
Fig. 4
figure 4
First example of areas damage detection

Full size image
In Table 2, the proposed method, which uses segmentation based on the particle optimization algorithm in the standard mode, is compared with Two-stream Convolutional Networks method [5]. In this table, Sensitivity, Specificity, and Classification rates are evaluated in the proposed method and the base paper method.

Table 2 Comparison of the proposed method with Two-stream Convolutional Networks method
Full size table
Table 3 also compares the performance of the proposed method when using a hybrid classifier compared to a single classifier mode. The results of this table show to what extent the hybrid classifier-based method has improved performance. As each classification has its own characteristics, single classifications are less effective than hybrid classifiers. Due to the variety of constructor classifiers and their functions, multiple classifications are usually more accurate.

Table 3 Comparison of the single classifier with the embedded classifier
Full size table
Table 4 also shows the execution time for the proposed method and Two-stream Convolutional Networks method, respectively. As the results of this experiment show, the proposed method has a shorter execution time. Due to the fact that the deep learning method requires more training and its convergence time is longer, the proposed method has less implementation time.

Table 4 Comparison of expectation time in proposed method and Two-stream Convolutional Networks method
Full size table
Figure 5 also analyzes the sensitivity of the proposed method to the particle number parameter in the PSO algorithm. As can be seen in this figure, when the number of particles is considered equal to 100 particles, the proposed method has the highest accuracy.

Fig. 5
figure 5
Sensitivity of the proposed method

Full size image
Also, in the last experiment, the proposed method was compared with iterated algorithms for optimization, such as Newton and the Steepest Gradient. The result of this experiment is shown in Fig. 6. As seen in the reported result of the Fig. 6, the proposed method is much better than the Newton and the Steepest Gradient algorithm.

Fig. 6
figure 6
Comparison of different methods

Full size image
Conclusion
Crash detection is one of the applications of machine vision. In fact, it will be very useful to have an integrated system capable of detecting accidents and informing the appropriate agencies as soon as an accident occurs due to the large number of accidents in the world and the necessity that relief and law enforcement agencies respond immediately to accidents. This study proposes a novel method to diagnose the accident and to identify the damaged parts of the car. A car image is classified using image segmentation, followed by car vision methods to identify the damaged areas of the image. In comparison to previous works, the proposed method is more accurate in identifying damaged areas and takes less time to execute. Like most car vision-based crash detection methods, the proposed method in this research required a lot of training data. As a result, future work should focus on methods that require less training data.

Abbreviations
DEG:
Differentially expressed genes

ODE:
Ordinary differential equation

PDE:
Partial differential equation

SVM:
Support vector machines

PCA:
Principal components analysis

ML:
Machine learning

NSCLC:
Non-small cell lung cancer

FS:
Fisher score

LOO:
Leave one out

