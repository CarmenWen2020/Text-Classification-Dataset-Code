With the rapid development of cloud storage, an increasing number of data owners prefer to store their large-scale data on the remote cloud for greatly reducing the burden of maintaining the data by themselves. Since different cloud storage service providers offer distinct quality of data storage services, secure outsourced data migration becomes a fundamental requirement for data owners to change their cloud storage service providers. As a result, how to guarantee the correctness and integrity of the outsourced data blocks when they are being migrated from one semi-trusted cloud host to another becomes a primary concern of data owners. To solve this problem, we propose a blockchain-based publicly verifiable data migration scheme supporting efficient data integrity checking for cloud storage in this paper. Specifically, the data owner can dynamically change the cloud storage service providers and securely migrate the outsourced data from one cloud host to another, without retrieving the transferred data from the original cloud host, and checking the transferred data integrity in the target cloud host. By making use of blockchain, our proposed scheme can simultaneously achieve data migration and integrity checking without interacting with a third party auditor, thus avoiding the problems of service interruption and privacy leakage caused by the single-point-of-failure of the third party auditor, which is much better than the previous schemes. Meanwhile, our proposed scheme is provable secure in the random oracle model. Finally, we also develop a prototype implementation of our proposed scheme and compare the efficiency with some existing schemes. The experimental result demonstrates that our proposed scheme is more efficient and practical.

Previous
Next 
Keywords
Cloud computing

Data migration

Data integrity

Blockchain

1. Introduction
Like artificial intelligence (AI) (Al-Janabi et al., 2020c, Al-Janabi et al., 2020b) and Internet of things (IoTs) (Al-Janabi, 2020a), cloud computing is also a newly-developing and promising computing paradigm, which can be viewed as the fusion and application of parallel computing, grid computing and distributed computing (Buyya et al., 2009, Yang et al., 2020a). Generally speaking, cloud computing connects a large number of distributed computational resources, network bandwidths and storage mediums together through network. Thus, cloud computing can establish a computing resource pool to provide tenants with plenty of on-demand services, for example, cloud storage (Xue et al., 2018), outsourced data sharing (Zhang et al., 2020), outsourced data integrity checking (Pitchai et al., 2019, Wei et al., 2020), and so on. These services, especially cloud storage has been widely applied in the daily life and work. Cloud storage, one of the most attractive services provided by cloud computing, can ubiquitously offer convenient data storage service in the manner of pay-as-you-go (Yang et al., 2019a, Yu and Wang, 2017). By employing cloud storage, resource-constraint data owners, including individuals and enterprises can outsource their large-scale data to the cloud host for greatly reducing local computational overhead and storage burden. As a result, more and more data owners prefer to embrace cloud storage (Yang et al., 2020b).

Due to the promising market prospect, an increasing number of enterprises, such as Alibaba, Dropbox, Amazon and Google invest cloud storage and provide data owners with convenient data storage services (Persico et al., 2016, Alotaibi et al., 2019, Zhang and Ravishankar, 2019). However, their cloud storage services are characterized by different prices, access speed, reliability, security, storage capacity, etc. Thus, for enjoying more suitable cloud storage services, data owners might want to dynamically change their cloud storage service providers and migrate their outsourced data from the original cloud host to the target cloud host (Yang et al., 2019b). As a result, outsourced data migration is more and more frequent, and it will become one of the most fundamental requirements for data owners. The investigation report of Cisco demonstrates that 95% of the total data traffic will be the cloud data traffic by the end of 2021. Meanwhile, 14% of the total cloud data traffic is predicted to be the cloud data traffic among different cloud hosts (Liu et al., 2019). Therefore, how to efficiently and securely migrate outsourced data blocks without downloading them would become a primary concern of data owners.

To securely migrate outsourced data, an application called Cloudsfer had been designed, which aimed to protect the data confidentiality during the migration process (Hajnal et al., 2018). Specifically, Cloudsfer adopted data encryption technique to encrypt the transferred data, which can prevent the malicious adversaries from digging privacy information from the transferred data. However, during the data migration process, Cloudsfer cannot guarantee the transferred data integrity against the selfish cloud hosts and malicious attackers. On one hand, the original cloud host requires to pay plenty of computational resources to achieve outsourced data migration, which may greatly affect the access speed of the other data owners. As a result, to save computational resources, the original cloud host might only migrate part of the data, or even deliver some unrelated data to cheat the data owner and the target cloud host. On the other hand, the malicious attackers might destroy the transferred data blocks during the data migration process (e.g., delete some data, insert some unrelated data, etc.), resulting in data pollution.

Although plenty of solutions have been proposed to guarantee transferred data integrity, there still remain some severe challenges that urgently required to be solved well. Firstly, most of the existing solutions must rely on a trusted third party auditor (TPA) to maintain related keys or check data migration result. However, the single-point-of-failure of the TPA may cause service interruption or privacy leakage. Specifically, like the other ordinary entities, the TPA may refuse to serve when the computational overhead continuously increasing, resulting in service interruption. Meanwhile, the TPA may be frequently attacked by the adversary, which would cause privacy leakage. Furthermore, the hardware/software failures of the TPA will also cause privacy information disclosure. Therefore, the TPA would be a bottleneck which impedes the development and application of cloud data migration system. Secondly, the computational overhead of the existing schemes approximatively linearly increases with the number of outsourced data blocks, or contain some complex protocols and computations, such as provable data possession (PDP) protocol, modular exponentiation calculations, homomorphic encryption/authentication, bilinear pairings and so on. As a result, the computational efficiency of the existing schemes is not attractive, thus cannot perfectly suitable for large-scale outsourced data migration scenarios in cloud storage.

Blockchain is characterized by decentration, persistency, anonymity and auditability, which can provide a potential solution to deal with the above problems in the existing outsourced data migration schemes. However, to the best of our knowledge, there is no blockchain-based publicly verifiable outsourced data migration scheme supporting efficient data integrity checking without TPA in cloud storage environment. Therefore, in this paper, we aim to adopt blockchain to solve the essential but challenging problem of publicly verifiable outsourced data migration with efficient integrity checking.

1.1. Our contributions
In this paper, we design a novel publicly verifiable outsourced data migration scheme supporting efficient data integrity checking for cloud storage. In our proposed scheme, we mainly consider a rational cloud host who maintains the outsourced data and generates a simple evidence to demonstrate the data is honestly stored. Moreover, because of the limited storage resource and network bandwidth of the data owner, we can achieve the proof of publicly verifiable data migration with minimum resource overhead. Further, the contributions of this paper can be summed up as the following three folds.

•
We adopt consortium blockchain to design a publicly verifiable outsourced data migration scheme supporting efficient data integrity checking, in which the actual outsourced data is stored in the cloud hosts instead of the consortium blockchain. Generally speaking, our proposed scheme not only allows the data owner to change the cloud storage service providers and migrate the outsourced data blocks without retrieving them, but also allows the target cloud host to check the transferred data integrity before accepting them, which can guarantee that the transferred data blocks are migrated without any pollution.

•
In our proposed scheme, the data owner can easily achieve data migration and integrity checking without interacting with a trusted third party auditor, which is able to avoid the problem of single-point-of-failure caused by the third party auditor in the existing solutions. Thus, our solution not only can effectively prevent system from service interruption, but also can effectively prevent privacy information from leakage. Meanwhile, our proposed scheme can achieve public verifiability of the data migration result. Moreover, we provide the formal security proof that can demonstrate the security of our proposed scheme.

•
In our proposed scheme, the theoretical computational complexity in outsourced data migration and data integrity checking processes logarithmically increases with the total number of outsourced data blocks approximatively, which enables our proposed scheme to be quite suitable for large-scale data migration scenarios in cloud storage environment. Meanwhile, we implement our proposed scheme through simulation experiments and provide the approximate efficiency evaluation, which can intuitively demonstrate that our proposed scheme is particularly efficient and practical in the real world applications.

This paper is the extension of our previous work which was presented at the 15th International Conference on Wireless Algorithms, Systems, and Applications (WASA-2020) (Yang et al., 2020c). In the following, we will present the main differences between this paper and the previous conference version. Firstly, we enrich the background and related work in Section 1. Secondly, we add an overview of our proposed scheme in Section 4.1, and then we improve our proposed scheme in Section 4.2. Next in Section 5, we add the formal security analysis which can prove that our proposed scheme can satisfy all of the desired security requirements that mentioned in Section 3.3. Finally, we implement our proposed scheme and provide the performance evaluation in Section 6.

1.2. Related works
In cloud computing, the existence of outsourced data, i.e., outsourced data integrity, has been studied for a long time (Guo et al., 2019, Wang and Zhang, 2013, Wang et al., 2016, Li et al., 2017a, Chen and Curtmola, 2012, Yuan et al., 2020). Currently, outsourced data integrity checking protocols can be divided into PDP protocols and proof of retrievability (PoR) protocols based on whether it can recover the original data. In 2007, Ateniese adopted sampling to present the first PDP model, which can greatly reduce the computational overhead and communication cost during the integrity verification process (Ateniese et al., 2007). Barsoum and Hasan designed a map-based provable multi-copy dynamic data possession (MB-PMDDP) protocol, which can support block-level dynamic operations and guarantee the data integrity (Barsoum and Hasan, 2014). To achieve efficient dynamic PDP (D-PDP), Wang et al. designed a D-PDP model, which can simultaneously satisfy the requirement of batch-update verification (Junxiang and Shengli, 2012). To achieve data integrity checking in public clouds, Wang presented a new concept called proxy PDP (PPDP), and then adopted bilinear pairings to design an efficient PPDP scheme (Wang, 2012). In 2017, Wang et al. formalized the model of online/offline PDP (OOPDP) and then they proposed two efficient OOPDP instantiations (Wang et al., 2017).

Chen et al. adopted blockchain to design a DPDP model for smart cities (Chen et al., 2020). Then, they used multi-replica storage tricks to propose a concrete construction of decentralized PDP. Zhang and Dong designed an efficient identity-based outsourced data integrity verification protocol that is provable secure in the random oracle model (Zhang and Dong, 2016). Meanwhile, they extended their protocol to support batch verification in multi-user setting. Li et al. proposed a primitive of fuzzy identity-based data auditing and then utilized biometrics to propose a concrete construction of fuzzy identity-based auditing scheme, which can satisfy the property of error-tolerance (Li et al., 2017b). Yu et al. designed an attribute-based outsourced data integrity verification scheme, which offered attribute privacy-preserving and collusion-resistance (Yu et al., 2017). In their scheme, they introduced attribute-based outsourced data integrity verification to solve complex key management challenge. Zhou et al. investigated the challenging problem of data integrity verification in cloud-based electronic medical record (EMR) system (Zhou et al., 2020). Then, they designed a new data structure called multicopy Merkle hash tree (M2HT) and utilized it to propose a multi-copy PDP scheme supporting data dynamics. However, all of the above solutions for outsourced data integrity verification did not consider the problem of outsourced data migration, which has become one of the most fundamental requirements for the data owners in cloud storage.

To achieve outsourced data migration between two different cloud hosts, Yu et al. proposed an improved PDP scheme, which can also support secure data migration in cloud computing (Yu et al., 2015). In their scheme, the data owner can migrate outsourced data blocks from the original cloud to the target cloud without downloading them. Then, the data owner and the target cloud can verify the transferred data integrity. To the best of our knowledge, their scheme is the first solution for the problem of efficient and secure outsourced data migration. In 2016, Ni et al. designed a secure outsourced data transfer (SODT) scheme (Ni et al., 2016). In their proposed scheme, they utilized improved broadcast ciphertext-policy (BCP) encryption, proxy re-encryption (PRE) and polynomial-based authenticators (PA) to achieve data confidentiality, data transfer and data integrity verification. In 2017, Xue et al. combined Rank-based Merkle hash tree (RMHT) and PDP to design a novel data transfer scheme, which can greatly improve the efficiency (Xue et al., 2017). However, Liu et al. pointed out that there is a severe security flaw in scheme (Xue et al., 2017). That is, the malicious cloud can tamper the data block and forge a corresponding block tag which can successfully pass the verification. Then, they proposed an improved scheme to resist this flaw and reduce the computational overhead in data integrity checking phase (Liu et al., 2018).

However, the above solutions all rely on a TPA to maintain key or verify data migration result. To remove the TPA, Wang et al. proposed the concept of provable data possession with outsourced data transfer (DT-PDP) (Wang et al., 2019). Then, they adopted bilinear pairings to propose a concrete DT-PDP scheme. Wang et al. utilized homomorphic encryption (HE) and homomorphic authenticators (HA) to achieve proof data possession and verifiable data transfer (Wang et al., 2018). In 2018, Yang et al. adopted vector commitment (VC) to design an improved data migration scheme, which does not require any TPA (Yang et al., 2018). However, the schemes (Wang et al., 2019, Wang et al., 2018, Yang et al., 2018) all need to execute some complex protocols or to pay some expensive computational overhead, which will greatly affect the efficiency. To solve this problem, Yang et al. adopted counting Bloom filter (CBF) to propose a new data transfer scheme, whose computational cost is independent of the total number of outsourced data blocks (Yang et al., 2020d). However, in their scheme, the data owner needs to maintain the hash values of the outsourced data blocks, resulting in heavy storage overhead (see Table 1).


Table 1. Function comparison among different schemes.

Function	Technique	Confidentiality	Integrity	Data migration	Without TPA
Scheme (Ateniese et al., 2007, Barsoum and Hasan, 2014, Junxiang and Shengli, 2012)	PDP	×		×	
Scheme (Wang, 2012)	PPDP	×		×	×
Scheme (Chen et al., 2020)	Blockchain+PDP	×		×	
Scheme (Li et al., 2017b)	Fuzzy identity	×		×	×
Scheme (Chen et al., 2020)	Attribute	×		×	
Scheme (Zhou et al., 2020)	M2HT	×		×	
Scheme (Yu et al., 2015)	PDP				×
Scheme (Ni et al., 2016)	BCP+PRE+PA				×
Scheme (Xue et al., 2017, Liu et al., 2018)	PDP+RMHR	×			×
Scheme (Wang et al., 2019)	Bilinear pairings	×			
Scheme (Wang et al., 2018)	HE+HA				
Scheme (Yang et al., 2018)	VC+signature				
Scheme (Yang et al., 2020d)	CBF+Hash				
1.3. Organizations
The rest of this paper is organized as follows. In Section 2, we describe the preliminaries which will be utilized as the building blocks of our proposed scheme. In Section 3, we present the problem statement in detail. Specifically, we first formalize the system model. Then, we present the basic assumptions and finally identify the security requirements. Subsequently, we present our novel publicly verifiable outsourced data migration scheme supporting data integrity checking in Section 4. Then, in Section 5 and Section 6, we respectively provide the formal security proof and the approximate performance evaluation. Finally, we briefly conclude this paper in Section 7.

2. Preliminaries
Blockchain was first used as the underlying technology of Bitcoin, which is based on the blockchain technology (Kwon et al., 2019, Zhang and Lee, 2019). Blockchain technology is the fusion and application of data storage, data processing and data exchange among multiple participants based on peer-to-peer network communication technology, distributed storage technology, modern cryptography technology, and so on. Generally speaking, blockchain can be viewed as a distributed database which maintains plenty of public transactions (Yang et al., 2018b). Currently, blockchain network can be divided into private blockchain, public blockchain and consortium blockchain. The architecture of these three categories of blockchain are the same. In this paper, we adopt consortium blockchain as the building block to establish our publicly verifiable data migration system supporting efficient integrity checking.

Similar to the public blockchain and the private blockchain, the consortium blockchain is also equipped with the characteristics of anonymity, persistency, decentralization and auditability. Because of the need of registration permission, the consortium blockchain is also called permissioned blockchain. In the consortium blockchain network, all the data items (e.g., transactions) are either public or private. Meanwhile, note that every participant must register before joining in the consortium blockchain network. Subsequently, each participant operates one or more nodes. Moreover, only part of the participants have the authority and they are chosen in advance. Therefore, consortium blockchain can be seen as semi-decentralized.

3. Problem statement
In this following section, we firstly formalize the system model of our proposed scheme. Then, we present the basic assumptions and finally identify the security requirements.

3.1. System model
In our architecture, we mainly consider the following specific scenario. For enjoying more suitable data storage service or some other uncontrollable factors, the data owner might change the cloud storage service providers, and migrate some outsourced data blocks or even the whole outsourced file from the original cloud host to the target cloud host. In our system, we adopt consortium blockchain to establish the outsourced data migration scheme supporting data integrity checking. Hence, the system model contains the following entities: data owner, managers and cloud hosts, and nodes.

(1) Data owner. Data owner is a resource-constraint entity. Therefore, the data owner wants to outsource his large-scale data to the cloud host for greatly reducing local storage burden and computational overhead. Subsequently, the data owner might change the cloud storage service providers and migrate his outsourced data from the original cloud host to the target cloud host.

(2) Managers and cloud hosts. Managers are semi-trusted, who work together with cloud hosts to provide data owners with on-demand data storage services. Generally speaking, we can assume that the managers have the ability to verify, store and manage the outsourced data. Specifically, the mangers store the data in the cloud hosts after verification. In our proposed scheme, the system model refers to two cloud hosts: the cloud host  and the cloud host . We assume that the cloud host  is the original cloud host. At the beginning, the managers store the data in the cloud host . Later, the data owner will change the cloud storage service providers and require the cloud host  to migrate the data blocks to the cloud host . The cloud host  is the target cloud host, which receives the transferred data blocks from the cloud host  and checks the received data integrity. Moreover, the managers can be viewed as the blockchain members in a consortium blockchain network.

(3) Nodes. Nodes in our system model are controlled by the blockchain members. Moreover, the nodes are dynamic, that is, the blockchain members not only can add some new nodes, but also can remove some old nodes. In our system model, the nodes execute some distinct algorithms (e.g., consensus algorithm) to maintain a public ledger that stores the transactions. The transactions contain the auxiliary authentication information which will be utilized to verify the data integrity.

Then, Fig. 1 demonstrates the system model of our proposed scheme, which can be seen as a three-layer architecture. The first layer is the data owner layer, in which data owners register on the managers and outsource their large-scale data. The second layer is the data migration service layer, which has the following two main functions. Firstly, it provides on-demand data storage services to all of the data owners and manages large-scale outsourced data for them. Subsequently, this layer can complete outsourced data migration when the data owner changes the cloud storage service providers. Secondly, the members of this layer work together to maintain the blockchain network. The third layer is the public verification service layer, which maintains a tamper-resistant database that stores the public auditing information for data owners to check the integrity and availability of the outsourced data. Due to the openness, we can assume all of data owners and managers can access blockchain database. In our proposed scheme, all of the participants can check the data integrity without interacting with a third party auditor.



Fig. 1. The system model of our scheme.

3.2. Basic assumptions
Like all of the existing outsourced data migration schemes, our proposed scheme also contains some recognized assumptions and the details are as follows.

(1) We assume that the data owner is a legal client of the cloud host  and the cloud host . Generally speaking, before enjoying data storage services, the data owner must register on the system and pass the identity authentication by the cloud storage service providers. For simplicity, we assume that the data owner has passed the authentication and become a legal client of the cloud host  and the cloud host , which is acceptable and realizable.

(2) We assume that the cloud host  and the cloud host  will never collude together to maliciously cheat the data owner, which means that the cloud host  and the cloud host  follow the protocol specification exactly and independently. The cloud host  and the cloud host  belong to different enterprises and they are competitors. For economic interests, they will not be on the same side to cheat the data owner. Meanwhile, if the cloud host  and the cloud host  collude together, they can be viewed as a cloud host, then the data migration is meaningless. Therefore, this assumption is reasonable.

(3) We assume that the cloud host  will never deliberately interrupt the data migration and maliciously slander the cloud host . Note that the goal of the cloud host  is to provide data owner with data storage service and benefit from it. Meanwhile, the cloud host  cannot obtain any economic benefit from maliciously slandering the cloud host , as the data owner has already stopped the data storage service of the cloud host . Therefore, the cloud host  might try to accomplish the data migration, rather than corrupting it to slander the cloud host  maliciously.

3.3. Security requirements
Our proposed scheme should satisfy the following four security properties.

(1) Data confidentiality. Data confidentiality means the sensitive information that contained in the outsourced file should not be leaked. Therefore, the outsourced file should be encrypted with a secure symmetric encryption algorithm before uploading, and the attackers cannot correctly decrypt the ciphertext without the corresponding data decryption key.

(2) Data integrity. In our proposed scheme, data integrity is defined that the cloud host  must migrate the outsourced data blocks honestly. Meanwhile, the transferred data blocks cannot be destroyed during the migration process. Otherwise, the data pollution will be detected by the data owner and the cloud host  through data integrity checking.

(3) Publicly verifiability. Publicly verifiability means that the verification process of outsourced data migration result neither relies on a third party auditor nor requires any private information of the data owner and the cloud hosts. Therefore, any verifier who owns the related evidence all can check the data migration result.

(4) Non-frameability. In our proposed scheme, non-frameability is defined that the fairness in the process of data migration should be guaranteed. Specifically, on one hand, the honest participant cannot be framed by the dishonest participant. On the other hand, the dishonest participant cannot be shielded, that is, the dishonest acts should be proved and caught.

4. Our scheme
In the following part, we first give an overview of our proposed scheme. Then, we minutely present our novel scheme, which can simultaneously achieve publicly verifiable data migration and efficient data integrity checking.

4.1. Overview
In this paper, we consider the problem of publicly verifiable outsourced data migration with efficient integrity checking, which was proposed in scheme (Yu et al., 2015). It suffers from a trust problem, i.e., the data owner does not believe that the cloud host  might correctly perform data migration. To solve this issue, a few solutions have been proposed in the previous works, which rely on a third party auditor, resulting in the problem of single-point-of-failure. Moreover, the existing solutions need to cost some expensive computational overhead.

To solve the above problems, we design a blockchain-based publicly verifiable data migration scheme supporting efficient data integrity checking for cloud storage, as demonstrated in Fig. 2. Firstly, the data owner builds a MHT to maintain the outsourced data blocks and sends the entire tree to the manager. Then, the manager checks the correctness of the tree and stores it in the cloud host . Next, the nodes in the blockchain network verify the corresponding auxiliary authentication information and add a new block into the blockchain network, which maintains the transactions. When the data owner changes the cloud storage service providers, the cloud host  migrates the outsourced data blocks to the cloud host . Subsequently, the cloud host  retrieves the auxiliary authentication information from the blockchain network and checks the received data integrity. Finally, the cloud host  maintains the transferred data and returns a migration evidence, which will be used to check the migration result by the data owner.



Fig. 2. The main process of our scheme.

4.2. The concrete construction
In the following section, we will detailedly present our publicly verifiable outsourced data migration scheme supporting efficient data integrity checking. Specifically, our proposed scheme contains five steps and the concrete construction is as follows.

 Firstly, we can assume that the data owner, the cloud host  and the cloud host  has already owned the elliptic curve digital signature algorithm (ECDSA) public/private pair 
, 
 and 
, respectively. Correspondingly, we let 
, 
 and 
 be the public key certificates and 
 be the ECDSA signature generated on the message  with the private key 
, where . Subsequently, the data owner chooses a secure one-way collision-resistant hash function . Meanwhile, the data owner names the outsourced file  with file name 
, which is unique in the file storage system.

 The outsourced file always contains some sensitive information that should be kept secret from the data owner’s point of view. Hence, the data owner should encrypt the outsourced file to prevent the privacy information from leakage. Then, the data owner outsources the corresponding ciphertext to reduce local storage overhead. The data outsourcing step is depicted as follows.

(1) Data encryption. The data owner firstly computes a data encryption key 
. Secondly, the data owner utilizes the data encryption key  to encrypt the outsourced file as 
, where  is an indistinguishability under chosen-plaintext attack (IND-CPA) secure symmetric encryption algorithm. Subsequently, the data owner divides the corresponding ciphertext  into  blocks 
. Finally, the ciphertext  can be denoted by 
.

(2) Tree building. The data owner would first compute the hash values 
, where . Then, let 
 denote 
. Next, the data owner utilizes the hash values 
 to build a Merkle hash tree, where 
 is maintained by the -th leaf node of the tree. Then, the data owner generates a signature  on the hash value of the Merkle root node 
, i.e., 
, where  is the signature generation algorithm of ECDSA. Subsequently, the data owner sends the outsourced data set 
 to the manager. Finally, the data owner deletes the local backup of  and .

Data Storage. In our construction, every outsourced file generates a Merkle hash tree. When the manager receives an outsourced data set, he should execute the following processes.

(1) Store. Upon receiving , the manager utilizes 
 to rebuild the Merkle hash tree for the outsourced file and generates a new hash value of the Merkle root node 
. Then, the manager compares 
 with 
 that received from the data owner. Meanwhile, the manager utilizes 
 and 
 to check the validity of the signature . After a successful verification, the manager stores the entire Merkle hash tree in the cloud host .

(2) Transaction generation. The manager builds a transaction, which contains some related auxiliary authentication information for the outsourced file. Specifically, the auxiliary authentication information consists of 
, where  denotes the information for the nodes to verify the Merkle root node. Subsequently, the manager sends the transaction to the blockchain network.

(3) Block generation. Upon receiving the transaction from the manager, the nodes in the blockchain network verify the auxiliary authentication information and add a new block into the blockchain. Specifically, the nodes perform the following steps.

 The blockchain nodes are able to verify the correctness of 
 by utilizing  to re-compute a new hash value of the Merkle root node, 
. Subsequently, the blockchain nodes check whether the equation 
 holds.

 The blockchain nodes utilize 
 and 
 to check whether the signature  is a valid signature on the hash value of the Merkle root node 
.

If the equation 
 holds and the signature  is valid, the blockchain nodes will perform the practical Byzantine fault tolerant (PBFT) consensus algorithm. Subsequently, the blockchain network synchronizes the transaction that contains related auxiliary authentication information for the outsourced file. Finally, a new block which maintains the above transaction is produced in the blockchain network, as demonstrated in Fig. 3, in which every block contains three parts. The first part comprises the Merkle root node 
, the timestamping , the signature  and the information  which will be utilized to verify the Merkle root node. The second part is the hash value , which represents the hash value of the previous block. The final part maintains a random number nonce. Later, all the data owners are able to browse the ledgers and read the information in the blockchain because the transactions in the block are public to them. Moreover, the manager maintains a pointer list, which can directly point to the specific block related to a file.

Data Migration. For enjoying more suitable data storage service or some uncontrollable factors, the data owner prefer to dynamically change the cloud storage service providers and migrate some outsourced data blocks, or even the whole outsourced file from the cloud host  to the cloud host . The concrete data migration processes are described as follows.

(1) The data owner first generates a set of outsourced data block indices , which represents the indexes of outsourced data blocks that need to be migrated from the cloud host  to the cloud host . Then, the data owner utilizes his private key 
 to generate a signature 
, where 
 represents the timestamp. Next, the data owner generates a data migration command 
. Finally, the data owner sends  to the cloud host  and the cloud host .

(2) After receiving the data migration command , the cloud host  first checks whether  is valid through signature verification. Specifically, the cloud host  verifies the validity of signature 
 and timestamp 
. If the verification is failed, the cloud host  aborts and returns failure; otherwise, the cloud host  utilizes privacy key 
 to generate a signature 
 and returns it to the data owner. Meanwhile, the cloud host  sends the data blocks 
 to the cloud host , along with the signature 
.

 The data migration would cost plenty of computational resources, so the cloud host  might only send part of the data, or even deliver some unrelated data to cheat the data owner and the cloud host . Moreover, the data blocks might be destroyed during the migration process. As a result, the cloud host  might want to check the transferred data integrity according to the pre-algorithm as follows.

(1) Upon receiving  from the data owner, 
 and 
 from the cloud host , the cloud host  firstly checks the validity of  and 
 through signature verification. If the verification is failed, it represents that the cloud host  arbitrarily migrated the outsourced data blocks, i.e., the data migration is illegal. Therefore, the cloud host  aborts and returns failure. Otherwise, if the verification is successful, it means that the data owner has indeed required to migrate the data blocks and the cloud host  executed the data migration operation according to the data owner’s command.

(2) If the verification is successful, the cloud host  further checks the received data blocks integrity. Specifically, the cloud host  first retrieves the related auxiliary authentication information 
 from the blockchain network. Subsequently, the cloud host  can utilize 
 and  to rebuild a new Merkle hash tree and obtain a new Merkle root node 
. Finally, the cloud host  verifies the correctness of the equation 
 and the validity of the signature .

(3) If the verification is failed, the cloud host  can think that the transferred data blocks 
 are not intact, thus requiring the cloud host  to re-send them again; otherwise, the cloud host  thinks that the transferred data blocks 
 are intact. Therefore, the cloud host  maintains the transferred data blocks 
 and computes a signature 
. Next, the cloud host  informs the data owner the success of data migration and returns the signature 
 as a data migration evidence. As a result, the data owner can check the data migration result by verifying 
. If the signature 
 is valid, the data owner can think the cloud host  has received the integrated data blocks and maintained them honestly.

In our construction, the cloud host  and the cloud host  would never collude together to cheat the data owner maliciously. In the meantime, the cloud host  will never arbitrarily interrupt the data migration and maliciously slander the cloud host . As a consequence, the cloud host  will return the honest data migration result to the data owner. In other words, the data owner can think that the outsourced data migration is successful if the signature 
 is valid, which is returned by the cloud host .

5. Security analysis
In this following section, we formally prove that our proposed scheme is able to satisfy the desired security requirements of data confidentiality, data integrity, public verifiability and non-frameability.

Lemma 1

Let  be an IND-CPA secure symmetric encryption algorithm and  be a pseudo-random function, where 
, , 
. Subsequently, the algorithm 
 is IND-CPA secure, where 
.

Proof

 is IND-CPA secure means that for any two messages ,  and any probabilistic polynomial time (PPT) adversary , there always exists a polynomial time algorithm  and an integer , when , the following equation holds: 
 

In the following, we will define some games. Specifically, we first provide the depiction of Game 0.




Subsequently, we briefly describe Game 1.



Finally, we present the Game 2 in detail.


Besides, we can easily prove that Game 1 and Game 2 are indistinguishable. Because the proof process is very similar, so we do not repeat it here.

Theorem 1

The proposed scheme can guarantee data confidentiality.

Proof

Data confidentiality means that any adversary cannot correctly decrypt the ciphertext without the corresponding data decryption key. According to Lemma 1, the security of outsourced file is ensured by the chosen encryption algorithm. In our proposed scheme, the data owner utilizes IND-CPA secure advanced encryption standard () algorithm to encrypt the file before outsourcing. Meanwhile, the data owner maintains the corresponding decryption key so secret that no one else can obtain it. As a result, the PPT adversary cannot obtain any plaintext information from the ciphertext without the decryption key. That is, our proposed scheme can achieve data confidentiality. □

Theorem 2

The proposed scheme can guarantee data integrity.

Proof

In our proposed scheme, data integrity is defined that the transferred data cannot be maliciously polluted during the migration process. Otherwise, the cloud host  can detect the malicious data tampering. Specifically, the cloud host  received data blocks 
 and signature 
 from the cloud host . Meanwhile, the cloud host  can retrieve the auxiliary authentication information 
 from the blockchain network. The persistency and auditability of blockchain can always prevent the auxiliary authentication information from being tampered. That is, the auxiliary authentication information is fully trusted. Therefore, the cloud host  can utilize 
 and  to rebuild the Merkle hash tree and obtain a new Merkle root node 
. Then, the cloud host  verifies whether the equation 
 holds and checks whether  is a valid signature on 
. If the data blocks have be polluted, no one can forge some new data blocks to simultaneously make the equation 
 hold and the signature  valid. Therefore, if and only if 
 holds and  is valid, the cloud host  can believe that the cloud host  honestly migrated the data blocks and the transferred data blocks are not polluted during the migration process. That is, our proposed scheme can guarantee data integrity. □

Theorem 3

The proposed scheme can guarantee public verifiability.

Proof

In our proposed scheme, public verifiability means that the verification process does not require a third party auditor and any private information of the participants. Specifically, the cloud host  utilizes the public certificate 
 to verify the correctness of the data migration command . If and only if the verification succeeds the cloud host  will migrate 
 and 
 to the cloud host . The cloud host  can retrieve the auxiliary authentication information from the blockchain. Then, the cloud host  can check the data integrity without a third party and any private information. After a success migration, the cloud host  signs a signature 
 as the migration evidence and returns it to the data owner. Therefore, the data owner can check the data migration result through signature verification. If the signature 
 is valid, the data owner can think that the data blocks have been migrated successfully because the cloud host  might try his best to accomplish the data migration, rather than corrupting it to slander the cloud host  maliciously. The verification processes do not require a third party auditor and any privacy information, so we can think that our proposed scheme can achieve public verifiability. □

Theorem 4

The proposed scheme can guarantee non-frameability.

Proof

The non-frameability means that the dishonest data owner and the cloud host  cannot maliciously slander each other successfully. Then, we analyze the non-frameability under the dishonest data owner and malicious cloud host , respectively

Firstly, the dishonest data owner might deny his outsourced data migration command and slander that the cloud host  arbitrarily migrated the data. In this case, the cloud host  can demonstrate the outsourced data migration command , which contains a signature 
 that can only be generated by the data owner with his private key 
. Therefore, the data migration command  can be seen as an evidence, which can prove that the data owner had required the cloud host  to migrate the data blocks. Hence, the data owner cannot slander the cloud host  successfully. Secondly, the malicious cloud host  might not migrate the data blocks honestly. Then, if the dishonest operation is detected by the data owner, the malicious cloud host  may slander that the data owner had never required to migrate the data. In this scenario, the data owner can show the signature 
 which can be viewed as the respond to the data migration command. Meanwhile, only the cloud host  can generated the signature 
. Therefore, the malicious cloud host  cannot slander the data owner successfully. □

From the above formal security proof, we prove our proposed scheme can fulfill all of the desired security requirements which mentioned in Section 3.2.

6. Performance evaluation
In this section, we firstly provide the theoretical computational complexity comparison and communication overhead analysis. Then, we implement our proposed scheme and provide the efficiency evaluation.

6.1. Efficiency comparison
Computational complexity. In this part, we analyze the theoretical computational complexity of our proposed scheme, then we provide the efficiency comparison. Firstly, we define some symbols that will be used in the comparison. Symbols  and  respectively represents the data encryption calculation and hash computation operation. Meanwhile, we denote by  a signature generation calculation and  a signature verification operation. Moreover, we utilize symbol  to represent the number of outsourced data blocks, and symbol  to represent the number of transferred data blocks. Then, we assume that  blocks are added to the blockchain after the file  is stored successfully. Finally, the comparison results are demonstrated in Table 2.

For simplicity, we ignore some other computational overhead in Table 2, for example, multiplication computations, addition calculations, and so on. Then, from Table 2 we can easily find that our proposed scheme and scheme (Yang et al., 2020d) both do not contain some complex protocols and calculations, and they cost the same computational overhead to migrate the outsourced data blocks. Moreover, in data outsourcing process, there are  more hash calculations in our proposed scheme than in scheme (Yang et al., 2020d). Meanwhile, scheme (Yang et al., 2020d) requires much more hash calculations in data storage and migration checking phases. Although hash computation is quite efficient, it will take lots of time to compute these hash values when the number of outsourced/transferred data blocks continuously increasing. As a result, compared to solution (Yang et al., 2020d), the overall efficiency of our proposed scheme is much more attractive. Therefore, we can think that our proposed scheme is more efficient than scheme (Yang et al., 2020d).


Table 2. Comparison between two schemes.

Scheme	Scheme (Yang et al., 2020d)	Our proposed scheme
Cost (Data Outsourcing)		
Cost (Data Storage)		
Cost (Data Migration)		
Cost (Migration Checking)		
Communication overhead. For achieving data outsourcing, the data owner in our proposed scheme needs to send the data set 
 to the cloud host , while the data owner of scheme (Yang et al., 2020d) requires to send the data set 
 to the cloud host . To achieve data migration, the data owner of our proposed scheme sends the data migration command 
 to the cloud host . Then, the cloud host  migrates the outsourced data blocks 
 and the signature 
 to the cloud host . However, the data owner of scheme (Yang et al., 2020d) sends the data migration command 
 to the cloud host . Meanwhile, the cloud host  migrates the outsourced data blocks 
 and their indexes 
 to the cloud host . Hence, the communication overhead both is closely related to the size and the number of outsourced/transferred data blocks.

6.2. Experimental results
In this following part, we implement our proposed scheme and provide the simulation results. More specifical, we adopt the pairing-based cryptography (PBC) library and open secure socket layer (OpenSLL) library to implement the related cryptography algorithms. All of the experiments are performed on a Linux machine equipped with Intel(R) Core(TM) i7-7700 processor that running at 8 GB memory and 3.60 GHz.

To eliminate the requirement for a TPA, we store the auxiliary authentication information in a consortium blockchain network. In our simulation experiments, we utilize  programming language to implement our proposed scheme. Meanwhile, the data encryption/decryption keys and the secure parameters are 1024-bit-long. Then, we leverage the cryptographic hash function -256 to compute the related keys and the hash values in the blockchain. Moreover, we choose the  as the data encryption algorithm.

(1) Time cost of data outsourcing. In this experiment, we respectively test the communication time of sending the outsourced data set and the computational overhead of performing the main operations.

Computational overhead. In this step, the main computations are hash calculation and data encryption operation. From Table 2, we can find that the computational overhead increases with the size of encrypted file and the number of outsourced data blocks. In this simulation experiment, we increases the size of encrypted file from 0.5 MB to 5 MB with a step for 0.5 MB. Meanwhile, for simplicity, we increase the number of outsourced data blocks from 1000 to 9000 with a step for 4000. Then, we test the approximate computational time cost, as demonstrated in Fig. 4, Fig. 5.

We can easily find that the computational time cost of performing the main operations linearly increases with the size of encrypted file and the number of outsourced data blocks approximatively. Meanwhile, although the growth rates of the two schemes are almost the same, scheme (Yang et al., 2020d) would cost a litter less computational time overhead because our proposed scheme requires to execute more hash calculations. For example, when the number of outsourced data blocks reaches 1000 and the size of encrypted file reaches 5 MB, the computational time cost of scheme (Yang et al., 2020d) is 13.3 ms, while the computational time cost of our proposed scheme is about 14.0 ms. The difference of the computational time cost is absolutely acceptable. Moreover, note that the data encryption operation is one-time and can be performed off-line by the data owner. Therefore, we can think that the data encryption step will not greatly affect the overall efficiency of our proposed scheme.

Fig. 4. Computational time cost of our scheme.


Fig. 5. Computational time cost of scheme (Yang et al., 2020d).

Communication overhead. In data outsourcing process, the communication overhead is closely related to the size of outsourced file and the number of outsourced data blocks. In this simulation experiment, we fix the size of every outsourced data block in 1 KB for simplicity. Next, we will increase the number of outsourced data blocks from 100 to 1000 with a step for 100 and uploading them to the cloud host . Then, we test the approximate communication time overhead, as demonstrated in Fig. 6.

From Fig. 6 we can intuitively find that the communication time overhead of the two schemes both increases with the number of outsourced data blocks. Meanwhile, the growth rate of our proposed scheme is relatively lower than that of scheme (Yang et al., 2020d), which is because except the outsourced data blocks and corresponding hash values, scheme (Yang et al., 2020d) still requires to upload the indexes of the data blocks to the cloud host . Although the communication time cost increases with the number of outsourced data blocks, the communication time cost is absolutely acceptable and the data outsourcing operation is one-time for every outsourced file. Moreover, the communication time cost of our proposed scheme is less than that of scheme (Yang et al., 2020d).


Fig. 6. Communication overhead of data outsourcing.

Overall, we can think from Fig. 4 to Fig. 6 that our proposed scheme is efficient both in computation and communication for data outsourcing step.

(2) Time cost of data storage. This process mainly completes outsourced data storage and corresponding storage result verification, which do not involve in any information transmission. Therefore, we test the computational time cost of performing the main operations.

Computational overhead of storage. To achieve data storage, the computational overhead increases with the number of outsourced data blocks. Therefore, in this simulation experiment, we increase the number of outsourced data blocks from 1000 to 10000 with a step for 1000 and test the approximate computational time cost, as demonstrated in Fig. 7.

We can easily find from Fig. 7 that the computational time cost of the two schemes both linearly increases with the number of outsourced data blocks approximatively. However, the growth rate of our proposed scheme is relatively lower than that of scheme (Yang et al., 2020d). Meanwhile, the computational time overhead of our proposed scheme is much less than that of scheme (Yang et al., 2020d), which is because our proposed scheme needs to execute much less hash calculations than scheme (Yang et al., 2020d). Therefore, we can think that our proposed scheme is more efficient to store the outsourced data blocks.


Fig. 7. Computational time cost of storage.

Computational overhead of storage verification. In this simulation experiment, we also increase the number of outsourced data blocks from 1000 to 10000 with a step for 1000 and test the approximate computational time overhead. Then, the comparison result is demonstrated in Fig. 8.

We can find from Fig. 8 that the computational time cost of the two schemes both increases with the number of outsourced data blocks, while the growth rate of scheme (Yang et al., 2020d) is relatively higher than that of our proposed scheme. Meanwhile, the computational time cost of scheme (Yang et al., 2020d) is much more than that of our proposed scheme, because our proposed scheme needs to verify a signature and compute 
 hash values. However, scheme (Yang et al., 2020d) requires to compute  hash values and verify a signature. Therefore, we can think that our proposed scheme is much more efficient to verify data storage result.



Fig. 8. Computational time cost of storage verification.

Fig. 7, Fig. 8 demonstrate that the computational time cost of our proposed scheme is both less than that of scheme (Yang et al., 2020d). Hence, we can think that our proposed scheme is more efficient than scheme (Yang et al., 2020d) in data storage phase.

(3) Time cost of data migration and migration checking. We respectively test the main computational overhead of performing the main operations and the communication overhead of migrating the transferred data blocks.

Computational overhead. In this experiment, we test the computational overhead of performing the main operations. Specifically, we fix the number of outsourced data blocks  and the new added blocks of consortium blockchain  for simplicity. Meanwhile, we increase the number of transferred data blocks from 100 to 1000 with a step for 100. Then, we test the approximate computational time overhead, as demonstrated in Fig. 9.

We can obviously find from Fig. 9 that the computational time cost of our proposed scheme and scheme (Yang et al., 2020d) both increases with the number of transferred data blocks. However, we can see that scheme (Yang et al., 2020d) costs much more computational time overhead than our proposed scheme. Meanwhile, the growth rate of our proposed scheme is relatively lower than that of scheme (Yang et al., 2020d). Predictably, scheme (Yang et al., 2020d) will cost much more computational time overhead than our proposed scheme under the same number of transferred data blocks. Therefore, we can think that our proposed scheme is much more efficient from the point of computational overhead.



Fig. 9. Computational time cost of data migration and checking.

Communication overhead. In this experiment, we mainly test the communication time cost of migrating the outsourced data blocks, which is related to the size and the number of the transferred data blocks. For simplicity, we fix the size of transferred data blocks in 1 KB. Then, we increase the number of transferred data blocks from 100 to 1000 with a step for 100 and test the communication time cost, as demonstrated in Fig. 10.

From Fig. 10 we can intuitively find that our proposed scheme costs much less communication time overhead than scheme (Yang et al., 2020d). Meanwhile, although both the communication time cost of the two schemes linearly increases with the number of transferred data blocks approximatively, the growth rate of our proposed scheme is relatively lower than that of scheme (Yang et al., 2020d). Moreover, the cloud host  migrates the transferred data blocks to the cloud host . Note that the cloud host  and the cloud host  own almost limitless network bandwidths. Therefore, the communication time overhead is absolutely acceptable from both the data owner and the cloud hosts’ points of view.


Fig. 10. Communication time cost of data migration.

In Fig. 9, Fig. 10, the time cost of our proposed scheme both is less than that of scheme (Yang et al., 2020d). Hence, we can directly think our proposed scheme is more efficient in data migration and migration checking phases.

7. Discussions and conclusions
Based on the principle that making full use of resources, including computational resources, storage mediums and network bandwidths, we find that the people have tended in recent years to embrace cloud storage services. In cloud storage, finding a suitable solution to the problem of changing the cloud storage service providers and migrating the outsourced data is a very difficult and important challenge.

The main purpose is to preserve the outsourced data from being polluted during migration as well as reducing computational overhead. This paper solves two main points: how to simultaneously achieve publicly verifiable outsourced data migration and efficient data integrity checking without a TPA at the low computational overhead and communication burden.

The paper conclusions drawn from this work are summarized as follows: we proposed a blockchain-based publicly verifiable outsourced data migration scheme, in which the data owner can dynamically change the cloud storage service providers and efficiently migrate the outsourced data blocks from the original cloud host to the target cloud host without retrieving them. When receiving the transferred data blocks, the target cloud host can check the received data integrity, which can guarantee the transferred data integrity and availability against the semi-honest original cloud host and the malicious attackers. Meanwhile, our proposed scheme can achieve verifiability without interacting with a third party auditor. Moreover, we provide the formal security proof which proves that our proposed scheme can satisfy the desired security requirements. Finally, the implementation and efficiency evaluation demonstrate that our proposed scheme is efficient and practical in real-world applications.

There are plenty of authentication data constructs to achieve data migration and integrity checking in cloud storage, but blockchain was chosen because blockchain is characterize by decentralization, persistency and auditability. In general, the main purposes of our proposed scheme are attempting to answer the following challenging problems.

How the proposed scheme reduce the dependency on TPA compared to other such comparable schemes? All of the above techniques utilize the traditional authentication data constructs, while we adopt blockchain as building block to design solution. In our proposed scheme, the hash values of outsourced data blocks are stored in the blockchain. Then, everybody can access the hash values due to the openness. Meanwhile, blockchain is characterized by decentralization, thus removing the TPA during the data migration and integrity checking processes. Hence, by taking the advantages of blockchain, our proposed scheme can achieve verifiable data migration and efficient data integrity checking without interacting with a TPA.

How the proposed scheme satisfy the high-efficiency compared to other such comparable schemes? In the existing schemes, the expensive computational overhead comes from complex protocols (such as PDP protocol and PoR protocol) and calculations (such as modular exponentiation calculation, bilinear pairings calculation and homomorphic authentication operation). However, our proposed scheme does not contain any complex protocols and calculations. The main calculations of our proposed scheme are hash calculation, signature generation and signature verification. Hence, our proposed scheme can satisfy the high-efficiency and practicability.

