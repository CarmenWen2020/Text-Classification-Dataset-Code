Detection and removal of outliers in a dataset is a fundamental preprocessing task without which the analysis of the data can be misleading. Furthermore, the existence of anomalies in the data can heavily degrade the performance of machine learning algorithms. In order to detect the anomalies in a dataset in an unsupervised manner, some novel statistical techniques are proposed in this paper. The proposed techniques are based on statistical methods considering data compactness and other properties. The newly proposed ideas are found efficient in terms of performance, ease of implementation, and computational complexity. Furthermore, two proposed techniques presented in this paper use transformation of data to a unidimensional distance space to detect the outliers, so irrespective of the data’s high dimensions, the techniques remain computationally inexpensive and feasible. Comprehensive performance analysis of the proposed anomaly detection schemes is presented in the paper, and the newly proposed schemes are found better than the state-of-the-art methods when tested on several benchmark datasets.

Introduction
An observation in a dataset is considered an outlier if it differs significantly from the rest of the observations. The problem of finding patterns in data that deviate from the expected behavior is called the anomaly detection or the outliers’ detection problem. Outliers in data can occur due to the variability in measurements, experimental errors, or noise [1], and the existence of outliers in data makes the analysis of data misleading and degrades the performance of machine learning algorithms [2, 3].

Several techniques have been developed in the past to detect outliers in data [4,5,6]. The techniques for outlier detection can be broadly classified as methods based on: (i) Clustering [7], (ii) Classification [8], (iii) Neighbor based [9], (iv) Statistical [10], (v) Information-Theoretic [11], and (vi) Spectral methods [12]. The working of classification-based methods mostly relies on a confidence score, which is calculated by the classifier while making a prediction for the test observation. If the score is not high enough, the observation is not assigned any label and is considered an outlier. Some clustering-based methods identify the outliers by not forcing every observation to belong to a cluster, and the observations that are not assigned to any cluster are identified as outliers. The nearest neighbor techniques are mostly based on a calculation of the distance or similarity measure between the observation and its neighboring observations. Suppose the calculation is greater than a certain threshold, that means that the observation lies far apart from the rest of the observations and is considered as an outlier. Statistical methods usually fit a statistical distribution (mostly normal distribution) to the data and conduct a statistical inference test to see if the observation belongs to the same distribution or not. If not, the observation is marked as an outlier. Information-theoretic techniques use different information theoretic measures for example entropy, relative entropy, etc., to analyze the information content of the data. These techniques are based on an assumption that the outliers or anomalies in the data induce irregularities in the information content. Spectral methods transform the data to a new dimensional space such that the outliers are easily identified and separated from the data in the new space. Furthermore, some outlier detection techniques are also based on geometric methods [13] and neural networks [14].

All the techniques mentioned above are based on some assumptions and all the techniques have some pros and cons as described in Table 1. The ideas proposed in this work are based on the novel statistical methods considering data properties like compactness. Where, the compactness of data is estimated either by the interactions between different kernel function tails, new and adapted kernel functions are proposed, or by the variance of independent gaussian distributions over different regions that are captured as a new clustering idea. Moreover, contrary to the existing approaches, statistical methods are modeled based on the transformation of data into a unidimensional distance space. The newly proposed methods are based on boxplot adjustment, kernel based probability density estimation and neighborhood information. The aim here is to utilize the power of some statistical methods and to enhance the performance of the outlier detection algorithms in an unsupervised way, while keeping their implementation easy and computationally efficient. The proposed methods are evaluated using both the synthetic and the real datasets and are found better in scenarios where the traditional approaches fail to perform well. Especially the cases where the data is contaminated with a mixture of different noise distributions.

Table 1 Assumptions, advantages, and disadvantages of different outlier detection approaches
Full size table
The rest of the paper is organized as follows: “Background” section, “Proposed methods” section, Evaluation on synthetic datasets “Evaluation using synthetic examples” section, Evaluation on real data “Evaluation of boxplot adjustments using a real example” section, “Comparison with State-of-Art” section and “Conclusions” section.

Background
After the initial pivotal works for outlier detection based on the distance measure [15, 16], several new methods based on the distance measure were also proposed by different authors in literature [17, 18]. The difference between the latterly proposed methods and the previous studies is the use of nearest neighbors in distance calculation. Among the variants of the actual work, either a single distance based on the kth closest neighbor is calculated [19] or the aggregate of the distance of k closets points is calculated [20]. Among other unsupervised outlier detection algorithms are the local approaches originated from the concept of Local Outlier Factor [21].

Furthermore, boxplot outlier detection scheme is also one of the fundamental unsupervised approach and the concept of univariate boxplot analysis was first proposed by Tukey et. al. [22]. In a univariate boxplot, there are five parameters specified as: (i) the upper extreme bound (UE), (ii) the lower extreme bound (LE), (iii) the upper quartile Q3 (75th percentile), (iv) the lower quartile Q1 (25th percentile) and (v) the median Q2 (50th percentile). The best way to estimate the extreme boundaries is to estimate Probability Density Function (PDF), 𝑓(𝑥), at first step from where the boundaries will be defined, as follows:

⎧⎩⎨⎪⎪⎪⎪𝑈𝐸:𝜏2=𝑃(𝑋>𝑈𝐸)=∫𝑈𝐸+∞𝑓(𝑥)𝑑𝑥𝐿𝐸:𝜏2=𝑃(𝑋<𝐿𝐸)=∫−∞𝐿𝐸𝑓(𝑥)𝑑𝑥.
(1)
where 𝜏 is the significance level, the region of suspected outliers is defined for 𝜏=0.05 and the region of extremely suspected outliers is defined for 𝜏=0.01. The Eq. (1) estimates well the boundaries only if the distribution is unimodal, i.e., a distribution that has single peak or at most one frequent value.

However, in a standard boxplot the UE and LE values are computed and well estimated only under the assumption that the PDF is symmetric, as:

{𝐿𝐸=𝑄1−1.5(𝐼𝑄𝑅),𝑈𝐸=𝑄3+1.5(𝐼𝑄𝑅).
(2)
where the term IQR is defined as the Inter Quartile Range and is given by:

𝐼𝑄𝑅=𝑄3−𝑄1.
(3)
A common practice to identify the outliers in a dataset using a boxplot is to mark the points that lie outside the extreme values, that is, the points greater than UE and less than LE are identified as outliers. This version of outlier detection scheme works well for the symmetric data. However, for skewed data different other schemes are proposed in the literature. For example, different authors have used the semi-interquartile range i.e. 𝑄3−𝑄2 and 𝑄2−𝑄1 to define the extreme values as:

{𝐿𝐸=𝑄1−𝑐1(𝑄2−𝑄1),𝑈𝐸=𝑄3+𝑐2(𝑄3−𝑄2).
(4)
where 𝑐1 and 𝑐2 are the constants and different authors have adjusted their values differently for example, 𝑐1=𝑐2=1.5 [23] 𝑐1=𝑐2=3 [24] or calculation based on the expected values of the quartiles [25] and few more adjustments to the boxplot for outliers detection are also available, for example [26].

The traditional methods of boxplot for detecting the outliers sometimes fails in situations where the noise in the data is a mixture of distributions, multimodal distribution, or in the presence of small outlier clusters. In this paper, some novel statistical schemes based on (i) the boxplot adjustments, and (ii) a new probability density estimation using k-nearest neighbors’ distance vector are proposed to overcome the problem faced by traditional methods. These proposed methods are described in detail in the next section.

Proposed methods
Boxplot adjustments using D-k-NN (BADk)
Traditional boxplot identifies the outliers from unique dimensions. One useful idea other than using all the dimensions for identifying outliers is to transform the data into a unidimensional distance space and to identify the outliers in new space. This can simply be done by measuring the distance between data points considering all the dimensions and calculating the resulting distance vector. The idea of using a single dimension distance vector is useful not only for avoiding problem of sorting data in high dimension but also in terms of computational cost, and can be further enhanced in terms of performance by extending it to consider k number of neighbors in the distance calculation. This idea of boxplot adjustment based on the Distance vector considering k number of Nearest Neighbors (D-k-NN) is presented here and the resulting extreme values estimation from the modified boxplot are found to be quite useful in identifying the right outliers. Furthermore, the proposed scheme is useful in the cases where the distribution of the noise is not normal or is a mixture of different distributions, and can identify small outlier clusters in the data.

Suppose a dataset in ℝ𝑁, this dataset is transformed from N dimensional space to a unidimensional distance space by using a distance metric such as ‘Euclidian distance’. This is done by computing the distance of each observation in N dimensional space to its kth closest neighbor. This transformation results in a set that contains the distance of each observation to its kth closest neighbor, and the resulting set is represented as 𝑑𝑘∈ℝ. This transformation can be represented as:

𝑑𝑘:ℝ𝑁→ℝ
(5)
The set 𝑑𝑘 is used for computing the extreme value of the boxplot as follows:

{𝐿𝐸𝑑𝑘=𝑄1𝑑𝑘−𝑐1(𝑄2𝑑𝑘−𝑄1𝑑𝑘),𝑈𝐸𝑑𝑘=𝑄3𝑑𝑘+𝑐2(𝑄3𝑑𝑘−𝑄2𝑑𝑘).
(6)
From the extreme values defined in (6), the outliers are identified as points those lie outside the boundaries of 𝐿𝐸𝑑𝑘 and 𝑈𝐸𝑑𝑘. The two constant values 𝑐1 and 𝑐2 are adjustable with respect to the dataset under consideration and selection of smaller values for these constants results in more points being marked as outliers. The suggested values of these constants by different authors in the literature are 𝑐1=𝑐2=1.5 or 𝑐1=𝑐2=3.

Furthermore, another useful idea to identify the outliers in a data is to adjust the UE and LE values of a boxplot as follows:

⎧⎩⎨⎪⎪𝐿𝐸=𝑄1𝑑𝑘−𝑐1×𝑣𝑎𝑟(𝑋.1𝑋<𝑄2𝑑𝑘)‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾√,𝑈𝐸=𝑄3𝑑𝑘+𝑐2×𝑣𝑎𝑟(𝑋.1𝑋≥𝑄2𝑑𝑘).‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾√
(7a)
or

⎧⎩⎨⎪⎪𝐿𝐸=𝑄1𝑑𝑘−𝑐1×𝑣𝑎𝑟(𝑋.1𝑋<𝑄1𝑑𝑘),‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾√𝑈𝐸=𝑄3𝑑𝑘+𝑐2×𝑣𝑎𝑟(𝑋.1𝑋≥𝑄3𝑑𝑘).‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾√
(7b)
where 𝑣𝑎𝑟 is defined as the variance and the quartiles are computed from the set 𝑑𝑘∈ℝ. The extreme values can also be estimated based on the calculation of the separation threshold between centers of two variances, see Eq. (9), as:

{𝐿𝐸=𝑀−𝑐1×𝑣𝑎𝑟(𝑋.1𝑋<𝑀),𝑈𝐸=𝑀+𝑐2×𝑣𝑎𝑟(𝑋.1𝑋≥𝑀).
(8)
where 𝑀 is a value that separate the one-dimension region in order to calculate the variance of two centers. Let 𝑥𝜖ℝ be any random variable with PDF 𝑓(𝑥); and the values of 𝜇1 and 𝜇2 are calculated such that:

⎧⎩⎨⎪⎪⎪⎪⎪⎪⎪⎪(𝜇∗1,𝜇∗2)=arg(𝜇1,𝜇2)[min𝑀,𝜇1,𝜇2[∫−∞𝑀(𝑥−𝜇1)2𝑓(𝑥)𝑑𝑥+∫𝑀∞(𝑥−𝜇2)2𝑓(𝑥)𝑑𝑥]],𝑉𝑎𝑟1=∫−∞𝑀(𝑥−𝜇1)2𝑓(𝑥)𝑑𝑥,𝑉𝑎𝑟2=∫𝑀∞(𝑥−𝜇2)2𝑓(𝑥)𝑑𝑥.
(9)
Both, 𝑉𝑎𝑟1 and 𝑉𝑎𝑟2 can be partially differentiated with respect to 𝜇1 and 𝜇2 respectively, to find the minimum. After simplification the minimization occurs when:

⎧⎩⎨⎪⎪𝜇1=𝐸(𝑋−)𝑃(𝑋−),𝜇2=𝐸(𝑋+)𝑃(𝑋+),
(10)
where 𝑋−=𝑋.1𝑋<𝑀 and 𝑋+=𝑋.1𝑋≥𝑀. and the value of 𝑀 is calculated as:

𝑀=[𝐸(𝑋−)𝑃(𝑋−)+𝐸(𝑋+)𝑃(𝑋+)].12
(11)
For further details on the idea proposed in (8)–(11), the readers are referred to [27].

Detecting outliers based on Boxplot is efficient only if the data is unimodal distribution. To overcome the drawbacks of the boxplot estimation, some other statistical methods based on the probability density estimation computed from either the set 𝑑𝑘∈ℝ or the actual data 𝐷∈ℝ𝑁 are also proposed for outlier’s detection, which are discussed below.

Joint probability density estimation using D-k-NN
The methods proposed in this section compute the set 𝑑𝑘 from the actual data and utilize it for estimating some parameters of the joint distribution function. Three different schemes are proposed here which are described as follows:

Scheme 1: Normal distributions are often used for representing the real value random variables with unknown distributions [28, 29]. The joint probability density function of independent and identically normal distribution is given as:

𝑓(𝑥1,…,𝑥𝑁)=1(𝜁2𝜋‾‾‾√)𝑁𝑒∑𝑖=1𝑁−12(𝑥𝑖−𝜇𝑖𝜁)2
(12)
where 𝜁 is the standard deviation modeled differently in (15) and (17), 𝜇 is the mean of the random variable and N is the dimension of the data. Here, some functions based on the normal distribution to identify the outliers in a dataset are proposed. Suppose a two-dimensional dataset 𝐷(𝑥,𝑦), we can define a separation threshold 𝑇 based on the normal distribution for detecting the outliers such that:

⎧⎩⎨⎪⎪𝑍=∑𝑖=1𝑛𝑓(𝑥𝑖,𝑦𝑖),𝑇=𝛼𝑚𝑎𝑥(𝑍).
(13)
where Z is joint probability distribution function after normalization, 𝑛 is the total number of observations and the function 𝑓(𝑥,𝑦) can be defined as:

𝑓(𝑥,𝑦)=12𝜋𝜁𝐼𝑒−((𝑥−𝑥𝑖)2+(𝑦−𝑦𝑖)22𝜁2);𝑖=1,2,…,𝑛.;𝐼=0,1,2.
(14)
The 𝜎 in Eq. (14) can be computed as:

𝜁=𝛽𝑄3𝑑𝑘.
(15)
where 𝑄3𝑑𝑘 is the third quartile computed from the set 𝑑𝑘 as defined in Eq. (5) and 𝛽 is a constant value. The points below the threshold value 𝑇 defined in Eq. (13) are considered as outliers and the points above 𝑇 are considered normal inlier data points. The 𝛼 used in Eq. (13) is the significance value and it can be used to control the percentage amount of data to be removed as outliers.

Scheme 2: To better detect the outliers, a better function 𝑓(𝑥,𝑦) needs to be constructed in order to weaken the position of the outlier in terms of support and amplitude of the function. Furthermore, another scenario can be defined to detect the outliers based on the threshold defined in (13) by using the below function:

𝑓(𝑥,𝑦)=𝜁2𝜋𝑒−((𝑥−𝑥𝑖)2+(𝑦−𝑦𝑖)2𝜁2);𝑖=1,2,…,𝑁.
(16)
𝜁=𝛾(1+𝑑𝑘)2
(17)
where 𝑘 defines the kth closest neighbor for the distance metric and 𝛾 is a constant whose value can be adjusted to control the smoothness of the gaussian distribution. The concept is demonstrated in Fig. 1, where (a) shows the effect of traditional gaussian approach on compression and (b) shows the effect of proposed scheme 2 on compression.

Fig. 1
figure 1
a Effect of traditional gaussian approach on compression. b Effect of proposed scheme 2 approach on compression in x and y axes

Full size image
Both of the above schemes proposed in this section are based on a single gaussian distribution and are expected to work well for the datasets which can be well approximated using a single gaussian distribution. However, if a dataset can be better approximated using multiple gaussians then a better idea is to use a model based on the variable number of gaussians. A new and robust estimation of multiple gaussian distribution is proposed in the next subsection.

Scheme 3: The scenarios where the data is estimated using a gaussian distribution, the outliers are identified as the points lying on the extreme tails of the gaussian distribution, as shown in Fig. 2a. However, if the better estimation of underlying data is possible through multiple gaussians, the outliers located at the connecting points of different gaussians might remain unidentified using a single gaussian estimation. In order to identify the outliers existing at the connecting points of the multiple gaussians, an idea based on multiple gaussian estimation is proposed, where a Rejection Area (RA) is defined and computed as:

{𝑅𝐴={𝑥⃗ :𝑓(𝑥⃗ )≤𝐶𝑣},𝑓(𝑥∈𝑅𝐴)=𝜏.
(18)
where 𝐶𝑣 is defined as a critical value or a threshold value below which is the rejection area or where the outliers are identified, and 𝜏 is the significance level. The concept is shown in Fig. 2b, where as an example a single dimensional data is estimated using two gaussians and the outliers can be identified as the points below 𝐶𝑣.

Fig. 2
figure 2
An Example of Gaussian estimation and marking of critical value for outlier detection. a Points those lie outside the red boundaries are considered outliers. b The points below the critical value are identified as outliers

Full size image
In order to find the optimum number of gaussians that better approximate the joint probability distribution for a given dataset the sorted values of the vector 𝑑𝑘 can be utilized. For example, in Fig. 3 the graph of sorted values of the vector 𝑑𝑘 is shown and the best value of number of gaussians can be estimated by taking the value where the graph takes off sharply.

Fig. 3
figure 3
Plot of sorted values of the vector 𝑑𝑑𝑘𝑘

Full size image
Each estimated gaussian represent a region and for each gaussian inside a region the values of mean and variance can be computed as:

𝜇𝑖=∑𝑖∈𝑅𝑗𝑥𝑖𝑛𝑖,j=1,2,…., m
(19)
𝑉𝑎𝑟(𝑥)=∑𝑖∈𝑅𝑗(𝑥𝑖−𝜇𝑖)2𝑛𝑖−1,j=1,2,..., m
(20)
where 𝑅𝑗 represents the jth region, m is the total number of estimated gaussians and 𝑛𝑖 is the total number of elements in the respective region. The combined multiple gaussians model is then estimated by:

𝑥∼∑𝑖=1𝑚𝛼𝑖𝑁(𝜇𝑖,𝐶𝑖)
(21)
where

𝛼𝑖=𝐶𝑎𝑟𝑑(𝑅𝑖)𝑁and∑𝛼𝑖=1.
(22)
In order to determine the regions, lets define an application 𝑆𝑈 that sorts any given sequence 𝑈𝑖,𝑖=1,…,𝑁 such that 𝑈𝑆𝑈(1)≤𝑈𝑆𝑈(2)≤⋯≤𝑈𝑆𝑈(𝑁). For any given data 𝑋→, the sorted data can be represented as 𝑋→𝑆𝑈(𝑖) and suppose that Δ𝑋−→−𝑆𝑈(𝑖) represents the difference between two consecutive elements of 𝑋→𝑆𝑈(𝑖). Similarly, the sorted difference can be represented as Δ𝑋−→−𝑆Δ𝑋(𝑆𝑈(𝑖)). In order to define the regions, the elements are grouped together sequentially untilΔ𝑋𝑆Δ𝑋(𝑆𝑈(𝑖))≤Δ𝑋−→−𝑆Δ𝑋(𝑆𝑈(𝑁−𝑚+1)), once this condition is not true, start grouping the remaining elements as a new region until all the elements are assigned to a region.

Evaluation using synthetic examples
The ability of proposed methods is demonstrated here by the use of some two-dimensional synthetic datasets. The results for each of the proposed method are discussed in the following subsections.

Evaluation of boxplot adjustments using D-k-NN (BADk)
Figure 4a shows an example of the data used for evaluating the proposed methods to detect the outliers. From the data shown in Fig. 4a, it can be seen that the actual data is composed of different clusters with different shapes and is contaminated by a mixture of sinusoidal and gaussian distribution of noise. The aim here is to detect the noise as outliers and different shape clusters as inliers. The traditional boxplot is used to detect the outliers from the data and the resulting boxplot is shown in Fig. 4b. It can be seen from the boxplot in Fig. 4b that the traditional boxplot is unable to identify any outliers in the data.

Fig. 4
figure 4
a Original data representing clusters with different shapes and a mixture of sinusoidal and gaussian noise. b Traditional boxplot showing no outliers/anomalies

Full size image
The same dataset is used to evaluate the proposed adjusted boxplot with extreme values define in Eq. (6) and the results are shown in Fig. 5. Different values of 𝑘 are used to see how it effects the outcome in identifying the outliers. It can be observed from the results shown in Fig. 5 that for the smaller values of 𝑘 only the gaussian noise is identified and while we keep on increasing the value of 𝑘 the outliers with sinusoidal distribution are also identified.

Fig. 5
figure 5
Outliers detection from the data shown in Fig. 4a, using the proposed boxplot with extreme values defined in Eq. (6) for different values of 𝑘. The data in red is identified as the inliers while the data in green is identified as the outliers

Full size image
However, after a certain value of 𝑘 the data points from the actual clusters (inliers) are also marked as the outliers, while the outliers started to reappear as the inliers. This shows that although the selection of value of k is flexible in this case, still an optimum value of 𝑘 has to be selected for the optimum performance based on the data. Another example shown in Fig. 6a with a different distribution of noise is also tested for evaluating the ability of the proposed method in (6) for outlier detection. The results for this example are shown in Fig. 7 using three different values of 𝑘. It can be seen from the results in Fig. 7 that the selection of value of k is very flexible and still the proposed method performs well in terms of outlier’s detection. During all the experiments performed, the values of constants are fixed to 𝑐1=𝑐2=1.5.

Fig. 6
figure 6
a An example of dataset having different shapes of inlier clusters contaminated with noise. b Traditional boxplot showing no outliers/ anomalies

Full size image
Fig. 7
figure 7
Outliers detection from the data shown in Fig. 6a, using the proposed boxplot with extreme values defined in Eq. (6) for different values of 𝑘. The data in red is identified as the inliers while the data in green is identified as the outliers

Full size image
Evaluation (joint probability density estimation) scheme 1
The density estimation refers to estimation of an unobservable Probability Density Function (PDF) associated with an observable data. The PDF gives an estimate of the density according to which a large population in a data is distributed. In this proposed method, the PDF is computed by placing a gaussian density function at each data point, and then summing the density functions over the range of data, and a threshold value α defines the margin between the inlier data and the outliers. The value of α is computed as a percentage amount of the maximum value of the PDF. The value of 𝜎 in Eq. (14) is computed utilizing the 𝑑𝑘 vector as defined in Eq. (15).

The results for scheme 1 when evaluated using the same example data as shown in Fig. 4a are given in Fig. 8. The example is evaluated using only two different values of α and a fixed value of 𝛽. The outliers are shown in green color and the inlier data is shown in red color. Figure 8 also shows the associated 3D plots of the probability density estimations computed using Eq. (14). For α = 0.1 the proposed method is able to identify the outliers having gaussian distribution only while placing α = 0.3 the proposed method has identified both the gaussian and the sinusoidal outliers in the data. Figure 9 shows the results for the second example with a different noise distribution with fixed values of α and 𝛽 using Eq. (14).

Fig. 8
figure 8
Outlier Detection results using scheme 1 with two different values of α = 0.1 and α = 0.3. Outliers are shown in green and the inlier data in red

Full size image
Fig. 9
figure 9
Second example of Outlier Detection results using scheme 1 with α = 0.3, β = 3 and k = 1. Outliers are shown in green and the inlier data in red

Full size image
Evaluation (joint probability density estimation) scheme 2
The results for scheme 2 proposed in Eqs. (16)–(17) with different value of parameters are shown in Fig. 10. It can be observed from the results in Fig. 10 that the small value of γ produces sharp density distribution while a larger value of γ produced a smoother distribution. For a small value of γ the inlier data points are also identified as the outliers which is not the case with a comparatively larger value of γ for this particular dataset. However, optimum values for the parameters need to be tuned to get the optimum results using this scheme.

Fig. 10
figure 10
Outlier detection results using scheme 2 with different values of α, γ and k. Green points represent outliers and red points represent the normal data points

Full size image
Evaluation (joint probability density estimation) scheme 3
The idea proposed in Eq. (18) based on the different ways of estimation of gaussians of the distance vector 𝑑𝑘 is evaluated on three different synthetic examples having different distribution of noise and the results are shown in Fig. 11. It can be seen from the visual results depicted in Fig. 11 that this scheme is successful in identifying the outliers of different distributions and even the noisy data that lies in close proximity to the inlier data. The value of G represents the number of gaussians estimated from the 𝑑𝑘.

Fig. 11
figure 11
Results achieved on three different datasets with different distribution of noise using the proposed GMM estimation of 𝑑𝑘. The data points in red are the inliers and the green data points are identified as the outliers

Full size image
Evaluation of boxplot adjustments using a real example
The ideas proposed for boxplot adjustments in Eq. (6) and Eq. (7) are also evaluated on a real dataset. The dataset used is a subset of the original KDD Cup 1999 dataset from the UCI machine learning repository, the subset used is still a large data containing 95,156 observations and three attributes. The dataset is publicly available online.Footnote1 The ground truth of the dataset used is shown in Fig. 12a, where the blue data points represent the actual inliers and the yellow points represent the actual outliers. The results for boxplot using extreme values defined in Eq. (6) are shown in Fig. 12b and the achieved value for Area Under Curve (AUC) evaluation parameter is 0.83 for this dataset.

Fig. 12
figure 12
a Ground truth of the real data used to evaluate the proposed methods, blue data points are inliers and yellow are the outliers. b Results achieved using the boxplot with extreme values proposed in Eq. 6. Red points are inliers and green are outliers

Full size image
The results achieved for the proposed idea in (7) are shown in Fig. 13a, b, respectively for Eqs. 7a and 7b. The detected outliers are shown in green color and the inliers are shown in red color. The achieved value of AUC using both Eq. 7a and 7b is 0.833.

Fig. 13
figure 13
a Results achieved using the boxplot with extreme values proposed in Eq. 7(a). Red points are inliers and green are outliers. b Ress achieved using the boxplot with extreme values proposed in Eq. 7(b). Red points are inliers and green are outliers

Full size image
Comparison with State-of-Art
The proposed schemes are compared with several state of the art unsupervised outlier detection algorithms of similar kind, using a variety of benchmark datasets reported in [30]. The details of the benchmark datasets used for comparison are given in Table 2. The algorithms used for comparison include kNN [19], kNN-weight (kNNW) [20, 31], Outlier detection using Indegree Number (ODIN) [32], Local Outlier Factor (LOF) [21], Simplified LOF (SLOF) [33], Connectivity based Outlier Factor (COF) [34], Influenced Outlierness (INFLO) [35], Local Outlier Probabilities (LoOP) [36], Local Distance-based Outlier Factor (LDOF) [37], Local Density Factor (LDF) [38], Kernel Density Estimation Outlier Score (KDEOS) [39], Multi-Objective Generative Adversarial Active Learning (MO-GAAL) [40], Single-Objective Generative Adversarial Active Learning (SO-GAAL) [40], Artificially Generating Potential Outliers (AGPO), Active-Outlier method (AO) [41], Gaussian mixture model (GMM) [42], Parzen [43], One-Class Support Vector Machine (OC_SVM) [44], and Fast Angle-Based Outlier Detection (FastABOD) [45].

Table 2 Details of benchmark datasets used for evaluation and comparison with State-of-art
Full size table
Initially, some of the fundamental outlier detection algorithms are compared with the proposed algorithms using the same three synthetic datasets. To test the unsupervised outlier detection methods, the most popular evaluation measure proposed in literature is based on the Receiver Operating Characteristics (ROC) and is computed as the Area Under Curve (AUC) [30]. The ROC AUC is computed for these three datasets using the proposed schemes and are compared with some of the fundamental state-of-art algorithms in Table 3. Hyperparameters of all the methods are tuned and the best results are reported. It can be seen from the results in Table 3 that the proposed schemes are performing better than the existing algorithms. As these three datasets are only two dimensional the visual comparison is also possible which is provided in Fig. 14. From the visual inspection it is clearer that the newly proposed methods are better than the existing ones in identifying the outliers lying in close proximity to the inliers. Although, all the proposed schemes out-performed the existing approaches when tested on two dimensional synthetic datasets, only BADk and scheme 3 are recommended for large scale datasets. This is because the computational complexity of scheme 1 and scheme 2 is relatively higher than the BADk and scheme 3, as given in Table 4. Therefore, we recommend scheme 1 and scheme 2 only for dataset with dimensions less than or equal to 3 and for small datasets. For large scale datasets we recommend using scheme 3 and BADk. However, with a compromise on the computational complexity, scheme 1 and scheme 2 have the ability to perform better in terms of ROC AUC on individual datasets. Although, the best running time complexity is achieved by LOF but the proposed methods are performing much better than LOF in terms of ROC AUC values on individual datasets.

Table 3 ROC AUC comparison using three synthetic datasets
Full size table
Fig. 14
figure 14
Visual comparison for outlier detection using three synthetic datasets. Row 1–5: state-of-art methods and Row 6–9: the newly proposed methods

Full size image
Table 4 Comparison based on computational time using three synthetic datasets
Full size table
Furthermore, the proposed methods are evaluated and compared with eight existing approaches using five real benchmark datasets and the results are reported in Table 5. From the results in Table 5 it can be seen that the proposed schemes outperformed the existing approaches when tested on five real benchmark datasets. As, the proposed BADk and scheme 3 performed better than the proposed scheme 1 and scheme 2 when tested on synthetic datasets in terms of average ROC AUC value and computational time, therefore only these two methods are included for comparison with existing approaches using the real datasets.

Table 5 Comparison with existing approaches using some real datasets
Full size table
In order to perform more comprehensive comparison, ten more benchmark datasets and twelve state-of-art methods reported in [30] are also used and are compared with newly proposed unsupervised outlier detection methods. The results for these state-of-art methods and the newly proposed methods for the ten benchmark datasets are given in Table 6. It can be seen from the results in Table 6 that the newly proposed methods are clearly outperforming the existing algorithms in most of the cases. However, for two of the datasets (Hepatits and Parkinson) the proposed scheme 3 is performing equally well as compared to the LDF and both these methods are performing best as compared to the existing state-of-art approaches. For the Shuttle and the WPBC datasets the LDF approach is outperforming all other methods. However, the difference between the performance of LDF with proposed schemes on these two datasets is marginal, especially on WPBC. Furthermore, the two newly proposed methods BADk and scheme 3 make use of the distance vector for detection of outliers, so irrespective of the increasing dimensions of the input data the computational complexity of these proposed algorithms remains low.

Table 6 ROC AUC values computed on different benchmark datasets using State-Of-Art algorithms and the proposed schemes
Full size table
A visual comparison of proposed methods with the existing state-of-art methods is also provided in Fig. 15. From the results in Fig. 15, it is clearer that the newly proposed scheme 3 is outperforming rest of the methods in terms of AUC. Furthermore, the required computational cost for the proposed method is also low because of using the 𝑑𝑘 vector for outlier detection, instead of using the entire input data dimensions. As the proposed method is using only a single dimension distance vector of outlier detection, this makes it independent of the dimensions of the input data in terms of computational cost, which in turn makes it more feasible for high dimensional data.

Fig. 15
figure 15
Comparison of the proposed schemes with the state-of-art methods using 10 benchmark datasets for outlier detection. Y-axis represents the computed ROC AUC values

Full size image
Conclusions
Outlier detection is one of the most important preprocessing steps in data analytics, and for best performance consideration, it is considered a vital step for machine learning algorithms. Different methods are presented in this paper, keeping in view the need for a robust and easy-to-implement outlier detection algorithm. The newly proposed methods are based on novel statistical techniques considering data compactness, which resulted in an added advantage of easy implementation, improved accuracy, and low computational cost. Furthermore, to demonstrate the proposed ideas' performance, several benchmark multidimensional datasets and three complex synthetic two-dimensional datasets containing the different shapes of clusters contaminated with a mixture of varying noise distributions are used. The proposed methods are found accurate and better in terms of outlier detection as compared to the state-of-art. It is also an observation that some of the fundamental state-of-art methods cannot detect the outliers in scenarios where the outliers are a mixture of two different distributions. Moreover, two of the newly proposed schemes use only a single dimension distance-vector instead of utilizing the entire data dimensions for outlier detection. This makes the proposed methods more feasible and computationally inexpensive, irrespective of the input data's large sizes and growing dimensions.

Moreover, the evaluation of proposed unsupervised outlier detection methods on several benchmark real datasets reveal the usefulness of the proposed methods in detection of multivariate outliers in real datasets. The work can be extending by performing optimization on distance calculation method for the proposed scheme 3 and BADk. This will further enhance the computational complexity of these methods. Moreover, investigation of other distance metrics other than Euclidian can also be studied in future, as this metric suffers a lot for high dimensions.

Abbreviations
ABOD:
Angle-Based Outlier Detection

AUC:
Area Under Curve

BADk:
Boxplot adjustments using D-k-NN

COF:
Connectivity based Outlier Factor

D-k-NN:
Distance vector considering k number of Nearest Neighbors

INFLO:
Influenced Outlierness

KDEOS:
Kernel Density Estimation Outlier Score

LDF:
Local Density Factor

LDOF:
Local Distance-based Outlier Factor

LE:
Lower extreme bound

LOF:
Local Outlier Factor

LoOP:
Local Outlier Probabilities

ODIN:
Outlier Detection using Indegree Number

PDF:
Probability Density Function

ROC:
Receiver Operating Characteristics

SLOF:
Simplified LOF

UE:
Upper extreme bound

