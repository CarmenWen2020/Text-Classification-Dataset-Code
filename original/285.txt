In the era of big data, increasingly massive volumes of data is generated and published consecutively for both research and commercial purposes. The potential value of sensitive information also attracts interest from adversaries and thereby arises public concern. Current research mostly focuses on privacy-preserving data publishing in a statistic manner rather than taking the dynamics and correlation of context into consideration. Motivated by this, we propose a novel idea that combining differential privacy and generative adversarial nets. Generative adversarial nets and its extensions are used to generate a synthetic dataset with indistinguishable statistic features while differential privacy guarantees a trade-off between privacy protection and data utility. By employing a min-max game with three players, we devise a deep generative model, namely DP-GAN model, for synthetic data generation while fulfilling the privacy constraints in a differentially private manner. Extensive simulation results on a real-world dataset testify the superiority of the proposed model in terms of privacy protection, data utility, and efficiency.

Previous
Next 
Keywords
Continual data release

Differential privacy

Generative adversarial nets

1. Introduction
Nowadays, with the explosive growth of data in terms of volume, velocity, and variety and rapid development of the Internet of Things (IoT), a massive volume of streaming data generated by individuals is being collected and analyzed due to its great potential for both research and commercial uses (Lin et al., 2017; Wu et al., 2016; ; Shaikh and Patil, 2018). Publicly released data that is constantly published and evolving from time tends to provide more leverages to the adversaries and lead to severer privacy leakages (Chan et al., 2011; Qu et al., 2018a). Facebook as one of the well-known online social media experienced a severe data breach in 2014, affecting at least 50 million users (Cadwalladr and Graham-Harrison, 2018). Millions of social media data that contains personal information was harvested and analyzed to manipulate political campaigns by targeting individuals with personalized political advertisements (Cadwalladr and Graham-Harrison, 2018). Thereby, these continually updating data that contains personally identifiable information requires strong privacy protection to avoid sensitive information leakages and violation of individual's privacy (Dwork et al., 2010; Chan et al., 2011; Wang et al., 2018).

Differential privacy is widely-utilized as a theoretical foundation of privacy guarantees. It aims to address the issue of privacy-preserving data analysis (Dwork, 2006; Dwork et al., 2010). State-of-the-art differentially private algorithms perform well on data publishing in a statistical manner, without the consideration of the dynamic change of data and correlation of context. Research on differential privacy with consecutively data publishing has yet to be well-developed (Cao et al., 2017, Yang et al., 2017). In the statistical setting, a trusted curator possesses a large volume of static dataset. The computational result remains unchanged, up to any stochasticity introduced for privacy. Thus, differentially private algorithms in a statistic manner are independent of time (Dwork, 2006; Dwork Rothet al., 2014). In order to cope with explosive data traffic and continual data release, the statistical setting can no longer fulfil current security needs, especially in the big data scenario (Cao et al., 2017, Lu etal., 2018). Therefore, Dwork et al. initiated a study named differential privacy under continual observation (Dwork et al., 2010). It intends to address such a privacy issue that involves repeated computations for constant monitoring.

Generally, differentially private approaches involve introducing random controllable noise into the aggregate data. The type and the amount of introduced noise are the main means to conceal target information. In order not to affect the outcome, correctly capturing distributional information of the data is the key (Dwork Rothet al., 2014; Inan et al., 2018). The demand of learning underlying data distribution and improving the level of data utility (Soria-Comas et al., 2017; Kalantari et al., 2018) can be satisfied by utilizing deep generative models, especially by using generative adversarial nets (GAN) (Xu et al., 2019; Turhan and Bilge, 2018; Qu et al., 2019).

Recently, Chongxuan et al. proposed a variant of GANs, which updates the two-player game into a three-player game (Li et al., 2017). A classifier is added to improve GANs‚Äô performance of classification in semi-supervised learning. Such a variant is called triple-GANs. The authors demonstrated that triple-GANs solve the existing two-player problems. To some extent, the learning abilities of generator and discriminator are enhanced (Li et al., 2017; Salimans et al., 2016). Thereby, we raised a question: "can a three-player adversarial framework address privacy issues on continual data release?"

The core objective of privacy-preserving countermeasures implementation is optimising the trade-off. It guarantees an optimised balance between privacy preservation and data utility (Gu et al., 2019, Qu etal., 2018b, Qu etal., 2019). To some extent, rigorous privacy requirements inhibit the analytical utility of data. In particular, standard formalization of differential privacy suppresses accuracy of the protected results (Soria-Comas et al., 2017; Qu et al., 2019). Thereby, we aim to provide privacy guarantees without lessening the analytical utility of data.

We summarize the contributions of our work as follows.

‚óè
We propose a generative adversarial nets (GAN) enhanced differential privacy approach, namely DP-GAN. The differential privacy approach targets for continual data release. The GAN is employed to synthesis data rather than directly introducing noise into the data. We utilize a three-player framework under GAN and formulate a corresponding min-max equation.

‚óè
We introduce a differential privacy identifier as the third player in GAN. Such that, generator games with both discriminator and identifier to some extent. This identifier establishes privacy constraints based on differential privacy and user-level pan-privacy.

‚óè
We conduct extensive experiments on real-world datasets. We evaluate privacy protection level, data utility of the samples and efficiency. The experimental results show the superiority of the proposed model, compared with baseline models.

The rest of the paper is structured as follows. We briefly outline the related work in Section 2. Then, we depict the theoretical approach and the proposed DP-GAN framework in Section 3. Next, we present our experimental setting and results of model performance evaluations in Section 4. A summary and future work are shown in Section 5.

2. Related work
In this section, we briefly outline a literature review of relevant topics, i.e., differential privacy and generative adversarial nets with differential privacy.

2.1. Differential privacy
Differential privacy represents a strict mathematical interpretation of privacy-preserving data analysis. It describes privacy through computationally rich algorithms (Dwork Rothet al., 2014). The traditional differential privacy mechanism is to control the random noise introduced into aggregated data. And, the commonly used mechanisms are Laplace mechanism, Gaussian mechanism, and exponential mechanism (Dwork Rothet al., 2014). The Laplace mechanism adds independent noise that conforms to the Laplace distribution, in which probability density function has mean zero and standard deviation  (Dwork et al., 2006; Dwork Rothet al., 2014). Laplace mechanism intends to compute function f and disturbs each coordinate with noise sampled from the Laplace distribution. The amount of noise is determined by the ‚Ñì1 sensitivity of f. The Gaussian mechanism introduces Gaussian noise into the true answers to conceal and scales the noise to the ‚Ñì2 sensitivity Œîf (Dwork et al., 2006; Dwork Rothet al., 2014). The exponential mechanism randomizes the results for non-numeric queries with a score function q(D, œÜ) that weighs the quality of an output, œÜ. The score function can be described as application-dependent, which means score function varies based on different applications (Dwork Rothet al., 2014).

2.2. Generative adversarial nets
Deep generative models face the difficulty of solving many intractable probabilistic problems in maximum likelihood estimation. Also, it is hard for them to take advantage of piece-wise linear units in the generative context. Goodfellow et al. proposed an adversarial generative model called GAN to address such difficulties (Goodfellow et al., 2014). Recently, GANs have drawn a lot of attention as a promising candidate in semi-supervised learning (Li etal., 2017, Radford et al., 2015). Traditional GAN can only guarantee on realistic sample generation. It ignores the requirement for samples to meet the condition description. In order to solve this problem, Mirza and Osindero devised Conditional Generative Adversarial Networks (CGANs) (Mirza and Osindero, 2014). It introduces the conditional variable in both generator and discriminator to apply conditions and restrict model through additional information. It synthesises data-label pairs following the real samples that are dominated by the same conditional variables (Mirza and Osindero, 2014). However, one of the major problems with the existing two-player generative models is generator and discriminator may not achieve optimal status simultaneously (Li et al., 2017). In semi-supervised learning (SSL), the two-player framework compels discriminator to play two crucial roles at the same time, which are identifying fake samples and doing label prediction of unlabeled real samples (Li et al., 2017). CGANs have the same problem. Later, Chongxuan et al. proposed a three-player variant of GAN, namely triple-GAN to address this issue (Li et al., 2017). Triple-GANs formulate a game with three players by devising a classifier under GANs. The classfier is designed to undertake the label prediction work and lessen the burden of discriminator. Such that, all of the perceptrons can be optimal at the same time. In (Li et al., (2017)), the authors demonstrated triple-GANs‚Äô outstanding performance in classification and class-conditional image generation in SSL.

2.3. Generative adversarial nets with differential privacy
Due to GANs' superior performance on estimating the underlying distribution and synthesizing samples, some differentially private approaches start to use GANs framework. It has proven feasibility on a combination of differential privacy and GANs (Ho etal., 2019, Qu et al., 2020). Differentially private GAN (DPGAN) model follows conventional differentially private mechanisms by introducing deliberately noise to gradients of the Wasserstein distance during the learning procedure (Xie et al., 2018). It intends to do gradient clipping and clip solely on weights (Xie et al., 2018). Similarly, a model named PATE-GAN addresses the problem of noisy gradients during training in GAN's framework (Jordon et al., 2019). It modifies the Private Aggregation of Teacher Ensembles (PATE) framework for using with GAN. And it can closely bound the effect of a single sample on the model (Jordon et al., 2019). Like our model, generative adversarial net driven differential privacy approach (GAN-DP) adds one more perceptron as a identifier (Qu et al., 2019). Generator produces differentially private noise. And, discriminator and identifier compete with each other to derive the Nash Equilibrium (Qu et al., 2019). These models all demonstrate applying GAN framework is capable of generating private synthetic data in a differentially private manner.

However, such models mainly contribute to privacy preservation in a static manner and overlook privacy leakage under continual outputs, such as the temporal correlation of continual data publishing (Cao et al., 2017, Ho etal., 2019, Qu et al., 2020).

Our approach applies GAN's framework and formulates a game with three players to preserve privacy in continual data release. Our work provides a new perspective of solving the problem regarding privacy leakage during continual data publishing.

3. System modelling
In this section, we illustrate the proposed privacy-preserving framework of DP-GAN. The proposed approach utilizes a min-max game with three players. Different from triple-GAN, we devise a novel perceptron as the third player in GAN, namely differential privacy identifier. It provides privacy constraints relied on standard differential privacy and user-level privacy, thereby offering privacy guarantees on consecutively data publishing. A Table of Notations is provided in Table 1.


Table 1. Notations in differential privacy.

Notations	Explanation
ùìß	Universe
x	Dataset
y	Neighbour Dataset
œµ	Privacy Budget
Œ¥	Confidence Parameter
ùìú	Mechanism
Œîf	Sensitivity
S	Data Stream Prefixes
S‚Ä≤	Adjacent Data stream Prefixes
I	Set of Internal States
œÉ	Set of Possible Output Sequences
t	Time Sequence
‚Ñì(‚ãÖ)	Loss Function
Pr(‚ãÖ)	Probability
Alg(‚ãÖ)	Algorithm
3.1. Differential privacy
A differentially private mechanism conducts on a database or a dataset, which usually is the collections of individuals' records. Each row in the database or the dataset represents a record of a single individual. Differential privacy ensures the presence or absence of any individual will not cause any essential change in the output of the algorithm. This is addressed by focusing on the probability of a given output of a privacy mechanism and probability's variation by adding or removing of any row (Dwork et al., 2010). Let (x, y) be a pair of databases as being collections of records from a universe ùìß with the size of ùìß. Specifically, one is a subset of the other by missing at most one row compared to the larger database, implying two databases only varying in at most one row. (Dwork, 2006; Dwork Rothet al., 2014).

Definition 1

(Dwork Rothet al., 2014) A randomized algorithm ùìú with domain 
 is (œµ, Œ¥)- differential private if for all ùìú and for all 
 such that ‚Äñx ‚àí y‚Äñ1 ‚â§ 1:(1)ùìúùìúwhere the probability is over the coin flips of ùìú.

In (œµ, Œ¥)-differential privacy, œµ denotes the privacy loss, namely, the privacy budge. Œ¥ is a confidence parameter. If Œ¥ = 0, the randomized algorithm ùìú fulfils the definition of œµ-differential privacy. That is pure differential privacy. Otherwise, (œµ, Œ¥)-differential privacy lessens privacy protection level for the events with low probability. The (œµ, Œ¥)-differential privacy is referred as approximate differential privacy (Zhu et al., 2017).

3.1.1. The privacy budget composition
Referring to Definition 1, œµ denotes the privacy budget, which dominates the desired degree of privacy protection that can be achieved via algorithm ùìú (Zhu et al., 2017). In other words, the privacy budget represents the allowed maximum privacy loss through lessening the level of privacy preservation in (œµ, Œ¥)-differential privacy. Specifically, a smaller value of œµ indicates a stronger privacy guarantee provided by the differentially private algorithm (Lu and Shen, 2017).

Definition 1 exhibits that privacy loss in a differentially private manner can be quantified. In general, privacy loss can be cumulative as well (Dwork Rothet al., 2014; Abadi et al., 2016). In terms of the privacy budget, two widely-utilized privacy budget compositions theorems, i.e., sequential composition and parallel composition, are illustrated as follows:

Theorem 1

(Parallel Composition) (Zhu et al., 2017) Suppose we have a set of privacy mechanisms ùìúùìú
ùìú
. If each ùìú
 provides a œµi-differential privacy guarantee on a disjointed subset of the entire dataset, ùìú will provide (max{œµ1, ‚Ä¶, œµm})-differential privacy.

Theorem 2

(Sequential Composition) (Zhu et al., 2017) Suppose a set of privacy mechanisms ùìúùìú
ùìú
 are sequentially performed on a dataset, and each ùìú
 provides a œµ-differential privacy guarantee, ùìú will provide (m ‚ãÖœµ)-differential privacy.

To establish and analyze complex differentially private algorithms, Theorem 1 and Theorem 2 enable us to simplify the complex algorithms for a better understanding of differentially private behaviour under compositions (Dwork Rothet al., 2014).

3.2. Differential privacy under continual observation
In the study of differential privacy under continual observation, Dwork et al. introduced the concepts of pan-privacy and user-level/event-level privacy (Dwork et al., 2010). Such concepts have established the theoretical foundation of privacy preservation of continual data publishing.

3.2.1. Pan-privacy
In the setting of continual data release, we assume a data stream of unbounded length consisted of elements in a universe ùìß. In differentially private mechanisms, a streaming algorithm exists. It experiences a sequence of internal states and generates a sequence of outputs (Dwork Rothet al., 2014). If the internal state of the algorithm is differentially private, then the algorithm is pan-private (Dwork et al., 2010; Dwork Rothet al., 2014). A pan-private algorithm hides the evolving process of its internal state from one intrusion, even if it occurs at an unforeseen time. In the literature, pan-privacy is referred as the orthogonal presentation of differential privacy preservation under continual observation (Dwork et al., 2010; Dwork Rothet al., 2014; Chan et al., 2011).

Remarkably, continual outputs and pan-privacy are two intriguing and non-trivial properties. To achieve private continual data publishing, combining these two is much influential than using each individually (Dwork et al., 2010).

3.2.2. Event-level and user-level (Pan-)Privacy
Intuitively, continual observation algorithms are seen as operating at discrete time intervals. At each interval, the algorithm computes when receiving an input and then output a result (Dwork et al., 2010).

In the literature, a notion of adjacent stream prefixes is used to define privacy. The definition of adjacency defines the length of adjacent strings can be radically different. And, it was further modified to introduce time into the definition as follows (Dwork et al., 2010). Let S and S‚Ä≤ be stream prefixes of symbols drawn from a universe of possible input symbols, X. Then, Adj(S, S‚Ä≤) if and only if ‚àÉx, x‚Ä≤ ‚àà X and ‚àÉT ‚äÜ [|S|], such that S|T:x‚Üíx‚Ä≤ = S‚Ä≤. T represents a set of indices in the S. It means if we replace a certain instances of x in S with instances of x‚Ä≤, then we get S's adjacent pair, S‚Ä≤ (Dwork et al., 2010).

The definition of adjacency anticipates user-level privacy (Dwork et al., 2010). As stated in (Dwork Rothet al., 2014), user-level pan-privacy preserves the presence or absence of a target event in the stream, independent of the number of time or pattern that the event arises. Whereas event-level privacy purely secures the privacy of individual accesses. As mentioned in (Dwork et al., (2010)), event-level adjacency is a special case of adjacency. It limits the amount of instances of one symbol replaced by another to at most 1. It can be seen that event-level privacy provides much more freedom of privacy preservation compared to user-level privacy (Dwork Rothet al., 2014). Therefore, we choose user-level privacy over event-level privacy in our work.

In the setting of continual data publishing, we assume that the length of a data stream is unbounded, consisted of elements from a universe ùìß. The proposed streaming algorithm undergoes a progression of internal states and generates an array of outputs. We assume that the adversary can only access the information regarding internal states and the output sequences, while the data stream and the length of the input sequence are not exposed to the adversary (Dwork Rothet al., 2014). We expect a data stream and its adjacent stream to be ùìß-adjacent if we want user-level privacy to hold between them. More details are illustrated in Section 3.5.

3.3. Generative adversarial nets
GAN models characterize the true data distribution to synthesize new samples. In GANs, generator G learns the distributional information and generates synthetic samples. Discriminator D identifies whether a given sample lies on the true data distribution or originates from generator G. (Goodfellow et al., 2014; Audebert et al., 2018). The objective is to drive those two players to contest with each other, so that both of them improve their method and achieve optimum. (Goodfellow et al., 2014; Li et al., 2017; Kawai et al., 2018). Thus, the adversarial modelling framework of GAN refers as a two-player game. Specifically, it forms a min-max problem in training process as(2) 
  
 
 where pz(z) is a prior on input noise variables z, a simple distribution (e.g., uniform or standard norm). Then, G generates a sample G(z) and pg denotes the sample distribution from G. In Eqn. (2), a optimal discriminator satisfies the equilibrium D(x) = p(x)/(pg(x) + p(x)). And, the global equilibrium is pg(x) = p(x), that indicates the sample distribution from G will converge to the true data distribution (Goodfellow et al., 2014; Li et al., 2017).

As stated in Section 1, differentially private algorithms aim to correctly capture the underlying distribution in order to meet the rigorous privacy constraints (Dwork Rothet al., 2014; Inan et al., 2018). Considering GAN's ability to learn true data distribution, we intend to use a GAN model to address the trade-off in privacy-preserving countermeasures implementation. However, the generation performance of existing GANs is limited by their two-player framework. It can be further improved by updating the model into three-player adversarial framework (Li et al., 2017).

3.3.1. Triple-GANs
Triple-GAN is a three-player game-theoretical framework to address classification tasks in SSL, where a partially labeled dataset is given. A classifier and a generator produce pseudo labels given real data and pseudo data given real labels, respectively (Li et al., 2017). A discriminator identifies whether a data-label pair originates from the true labeled dataset or not. Those three networks defines three joint distributions, i.e. the true data-label distribution, p(x, y), and the conditional distributions, pc(y|x) ‚âà p(y|x) for classifier and pg(x|y) ‚âà p(x|y) for generator (Li et al., 2017).

The training process in Triple-GANs is a three-player mini-max game:(3) 
  
 
 where Œ± ‚àà (0, 1) is a constant that implies the relative importance of generation and classification in the Triple-GANs system (Li et al., 2017). In particular, if Œ± is close to 1, it indicates the adversarial model significantly favours the classification performance and neglects the generation performance (Li et al., 2017).

The equilibrium status achieves in Eqn. (3) if and only if p(x, y) = (1 ‚àí Œ±)pg(x, y) + Œ±pc(x, y). It suggests generator G and classifier C lean along with each other toward the data distribution, implying the adversarial game between these two. Unfortunately, it fails to address the global optimum of the equilibrium, i.e., p(x, y) = pg(x, y) = pc(x, y). To solve such a problem, the author introduced the standard supervised loss to C, ùì°
 in the minimax equation (Li et al., 2017). The min-max game with three players is redefined as follows:(4) 
  
 
ùì°
 

In the nonparametric setting for triple-GANs theoretical formulation, given that any fixed classifier C and generator G, the optimal discriminator D satisfies the following equation, 
 
 where pŒ±(x, y) = (1 ‚àí Œ±)pg(x, y) + Œ±pc(x, y) and Œ± ‚àà (0, 1) (Li et al., 2017).

Furthermore, pseudo discriminative loss ùì°
 is proposed to address the problem regarding the insufficiency of label information in SSL so as to optimize C's performance on the synthetic samples in a supervised manner. 
ùìü
 acts as a weighted hyperparameter for the pseudo discriminative loss (Li et al., 2017).

However, as stated above, in order to optimize the classification performance of C and further improve the predictive ability of the model, pseudo discriminative loss ùì°
 is constantly computed and updated during stochastic gradient descent training of Triple-GAN (Li et al., 2017). As a result, it raises the requirements of computational power for training the model and lessens the efficiency.

3.4. Three-player framework of DP-GAN model
Inspired by the three-player adversarial framework, we propose a differentially private model under GAN. The objective is to assign privacy labels y to given samples based on their security level as well as to synthesis data x conditioned on y. This model consists of one generator networks, one discriminator network, and one conditional network. In particular, the devised conditional network purely classifies samples by whether following a differentially private manner or not. In this case, the devised model is an unsupervised learner without supervision beyond classifier. The corresponding classifier is called differential privacy identifier I. A generator G samples data-label pairs in order to fool a discriminator D. It aims to learn distributional information and privacy knowledge from a discriminator D. A discriminator D examines whether a received sample-label pair lies on the true data distribution and follows a differentially private manner. Discriminator learns the privacy knowledge from identifier I. In this three players model, generator G competes with discriminator as well as differential privacy identifier. We expect the generated data not only converge to the true data distribution, but also have correct information regarding privacy protection level.

In the proposed model, the three components work as preceptrons. Differential privacy identifier I produces a privacy label y for a given data x, in which I examines x by differential privacy constraints. It forms a conditional distribution pi(y|x). Hereby, the data-label pair is expressed as data that draws from a joint distribution pi(x, y) = p(x)pi(y|x). Generator G produces a pseudo data x for a given label, in which the conditional distribution is pg(x|y) and the joint contribution is pg(x, y) = p(y)pg(x|y). G receives an input noise variable z that is sampled from a simple distribution pz(z) (e.g., uniform or standard normal), hereby x is defined by x = G(y, z). Discriminator D receives the pseudo input-label pairs (x, y) produced by both G and I for examination. The previous accepted sample-label pairs are sent back to I and D to leverage more positive data information and improve their performances.

The proposed model is a time-related adversarial generative model to address the privacy issue regarding continual data publishing. It intends to synthesize data, which fulfils two stringent differentially private requirements as stated in Section 3.5. I needs to obtain not only the given data at the present time slot but also the accepted data-label pairs at the previous time slot. Thus, as shown in Fig. 1, we introduce pp(x, y), as the joint distribution of accepted data-label pairs at the previous time slot.

Fig. 1

Fig. 1. The blocks that marked as I, D and G represent Differential Privacy Identifier, Discriminator and Generator respectively. ‚ÄúA‚Äù and ‚ÄúR‚Äù stand for acceptance in D and rejection in D correspondingly.

We present our devised three-player adversarial model as a mini-max equation:(5) 
  
 
ùì°
 where Œ± ‚àà (0, 1) is a constant that shows the relative importance of privacy guarantee and analytical utility. Œ± indicates whether the model focuses more on privacy protection or on data utility. It implies that the relaxation of differential privacy is applicable in Eqn. (5) in order to improve data utility. The value of Œ± is dynamically adjustable. It can be personalized for various scenarios and depends on the needs of favouring privacy protection or data utility. Specifically, if Œ± is close to 1, it indicates that the adversarial model focus on enhancing privacy preservation rather than data utility. In Section 4, we conduct experiments based on one specific scenario, where privacy guarantee and analytical utility are equally important. That is, Œ± = 0.5. ùì°
 mentioned in Section 3.3.1 is to address the unique global optimum issues regarding the equilibrium p(x, y) = pg(x, y) = pi(x, y).

For theoretical analysis in the non-parametric setting, we assume that for any fixed differential privacy identifier I and generator G, the optimal discriminator D is 
 
 where pŒ±(x, y) = (1 ‚àí Œ±)pg(x, y) + Œ±pi(x, y).

In addition, identifier I strictly follows two privacy constraints defined by differential privacy as mentioned in Section 3.5. The proposed generative model serves as an unsupervised learner. It does not have the problem with insufficiency of label information. Thereby, we do not consider pseudo discriminative losses in the training procedure.

The training procedure of our generative model is described in Algorithm 1, where Œ∏i, Œ∏d, and Œ∏g denote trainable parameters in I, D, and G correspondingly. For the training process of GAN, there is a nested FOR loop in stochastic gradient descent training of GAN. The complexity of such a training process is O(n2). For DP-GAN, we introduce a FOR loop to iterate through each time slot. In each iteration, we add an operation for the identifier. Thus, the complexity of Algorithm 1 is O(2n3).


Algorithm 1. Minibatch stochastic gradient descent training of DP-GAN model

1: for number of time slots do
2: for number of training iterations do
3: Sample a batch of pairs (xg, yg) ~ pg(x, y) of size mg, a batch of pairs (xi, yi) ~ pi(x, y) of size mi and a batch of labeled pairs (xd, yd) ~ pd(x, y) of size md.
4: Update D by ascending along its stochastic gradient:
 
 
 
5: Compute and unbiased estimators 
ùì°
 of ùì°
.
6: Update I by descending along its stochastic gradient:
 
ùì°
7: Update G by descending along its stochastic gradient:
 
8: end for
9: end for
3.5. Differential privacy identifier
As mentioned in Section 3.2.2, user-level privacy preserves the presence or absence of target information in the stream, independent of the number of time or pattern that the information arises. We use the definition of adjacency to anticipate user-level privacy (Dwork et al., 2010). We define our differentially private approach as a user-level private streaming algorithm.

In continual data publishing, we assume a data stream composed of elements in a universe ùìß with unbounded length. We expect our streaming algorithm to undergo a sequence of internal states and produce an array of outputs. In the setting of user-level privacy, we assume the adversary is able to access not only the output sequence but also internal states (Dwork Rothet al., 2014). Whereas, the data stream and the length of the input sequences remain private to the adversary (Dwork Rothet al., 2014). The definitions that employed in our user-level private algorithm are shown in Definition 2 and Definition 3.

Definition 2

(Dwork Rothet al., 2014) (ùìß-Adjacent Data Streams) We think of data streams as being of unbounded length; prefixes have finite length. Data streams S and S‚Ä≤ are ùìß-Adjacent if they differ only in the presence or absence of all occurrences of a single element ùìß. We define ùìß-adjacency for stream prefixes analogously.

Definition 3

(Dwork et al., 2010) Let Alg be an algorithm. Let I denote the set of internal states of the algorithm, and œÉ the set of possible output sequences. Then algorithm Alg mapping data stream prefixes to the range I √ó œÉ, is user-level pan-private (against a single intrusion) if for all sets I‚Ä≤ ‚äÜI and œÉ‚Ä≤ ‚äÜœÉ, and for all pairs of ùìß-adjacent data stream prefixes S, S‚Ä≤(6)
where the probability is over the coin flips of Alg.

Our approach applies the Definition 2 in identifier I for privacy preservation on streaming data. We treat the streaming data separately with respect to time. Thereby, the proposed model divides streaming data into multiple time slots. And, it processes each divided data streaming at discrete time intervals. At each time unit, synthesis data stream accepted by discriminator D is not only differentially private but also the adjacent data stream to the previous accepted one. Such a pair of data stream refers to ùìß-adjacent data stream in our model.

Based on Definition 3, all pairs of ùìß-adjacent data stream should satisfy the formula shown in Eqn. (6). In order to better fit the scenario of continual data release, we devise a user-level private algorithm by modifying Eqn. (6). Let St denote data stream S at time slot t and St‚àí1 denote data stream S at previous time slot t ‚àí 1. Such that, we expect all possible pairs of data streams to fulfil the modified formula as follows:(7)

At a given time slot t, the proposed approach classifies data to examine whether it follows the privacy constraints defined by standard differential privacy and user-level privacy.

We apply pure differential privacy, that is œµ-differential privacy to define a high level of privacy preservation in our model. We alter Eqn. (1) that is stated in Definition 1 to be dependent on time, the mathematical interpretation of differential privacy with respect to time as follows:(8)ùìú
ùìú

At each discrete time interval, discriminator D only accepts an input data stream if it follows standard differential privacy in Eqn. (8) and user-level privacy in Eqn. (7). The input data that failed to any of the two conditions stated as above is categorized into unsatisfactory data in privacy protection.

We formulate a game involves three players by introducing an identifier I under GANs‚Äô framework. We use such an adversarial generative model to produce pseudo data with the correct label of its privacy protection level. We define our model, namely, DP-GAN, as an unsupervised learner for purely relying on identifier I in labelling. In particular, in DP-GAN model, class-conditional generator G solely focuses on generating data-label pairs and fools discriminator D to accept them. The formulation of data-label pairs is driven by not only the underlying data distribution but also the internal privacy constraints in Identifier I. It provides privacy label information to discriminator D so that D could identify unwanted data-label pairs. Intuitively, generator G competes with both discriminator D and identifier I, thereby intensifying the generated data in a differentially private manner.

4. Performance evaluation
In this section, we present our experiments and results on real-world datasets. Privacy protection, data utility, and efficiency are used to evaluate the performances. The evaluation results show the superiority of the proposed model and confirm the significance of this work. The algorithms are implemented using MATLAB and are executed on Mac OS platform with Core I5@2.7GHz CPU and 8G memory.

We evaluate our model on the Yelp challenge dataset round 2019 (Yelp, 2019) released by Yelp Inc. In the yelp dataset, we mainly focus on the information of business which corresponds to location or even the trajectory, respectively. The details of the Yelp dataset is as follows. The data is collected from metropolitan areas. There are totally 6.68 M reviews and 1.22 M tips by 1.63 M users for 192K businesses. In the business part, there are over 1.2 M business attributes, e.g., hours and parking availability. In this setting, the consecutively published location data will form activity trajectories and has a potential risk of compromising privacy. We obtain three pieces of random data from the Yelp dataset as three individual records.

We utilize two baseline models for comparison of the proposed model in performance. First, in order to examine DP-GAN's performance on data utility improvement, we compare the proposed model with Laplace mechanism, the most widely-used differentially private model. Second, GAN has an outstanding performance on synthetic data generation. It indicates GAN is a competitive candidate in preserving data utility while guaranteeing privacy level. Thus, we choose GAN as another baseline model in our evaluation. In addition, we assume the proposed model treats the needs of privacy guarantee and analytical utility equally. Hereby, we set the hyperparameter Œ± = 0.5 in the proposed model. It also follows the existing benchmark setting shown in (Li et al., (2017)).

4.1. Privacy protection
In terms of privacy protection evaluation, we establish a continual data releasing scenario. In Fig. 2, we simulate the trajectory formed by consecutively position data publishing in the 2-dimensional x-y axis. We compare the trajectories of generated data with the trajectory of raw data. In privacy-preserving countermeasure implementation, the expected model should be capable of finding an optimised trade-off. The size of noise between the generated data and raw data depicts such a trade-off. Specifically, the size of noise should neither be too small to compromise the true trajectory information, nor too big to result in poor analytical utility.

Fig. 2

Fig. 2. Evaluation on personalized privacy protection.

In Fig. 2, we evaluate three models with three individual records. From Fig. 2a‚Äìc, the three experimental outcomes shows all the models are able to offer privacy protection by deviating from the true trajectory as shown by green solid line. The generated noises are random and vary at each sampling time slot. Both GAN and DP-GAN are well-performed on all three records, compared to Laplace approach. The distribution of the generated data from GAN and DP-GAN mostly complies with the raw data, which means the data utility could be saved to a satisfying degree. The comparison in terms of data utility will be further discussed in Section 4.2. Thereby, we evaluate privacy preservation level by the deviation distance from the generated trajectory and the true trajectory. A small injected noise may reveal true information to adversaries, and lower the level of privacy preservation. For DP-GAN, the average size of noise from Fig. 2a‚Äìc are 0.07, 0.14, and 0.11 respectively. The maximum size of noise are 0.18, 0.30, and 0.27 respectively. For GAN, the average size of noise are 0.05, 0.08, and 0.08 respectively. The maximum size of noise are 0.09, 0.21, and 0.20 respectively. Therefore, DP-GAN offers a stronger privacy protection in comparison of GAN.

4.2. Data utility
As mentioned in Section 4.1, both GAN and DP-GAN preserve data utility to a satisfying level. In this work, we further examine their performances in terms of analytical utility. The scenario and experiment setup remains unchanged. The experimental outcomes on three pieces of random data prove the validity of our model.

In Fig. 3, the Laplace mechanism has the highest RMSE at all sampling time for all three individual records. This indicates it has the poorest data utility preservation. This result is consistent with the result shown in Fig. 2. The generated trajectory from Laplace are quite different from true trajectory, which implies its poor data utility. For DP-GAN, its overall performance is satisfactory. In Fig. 3b and c, DP-GAN demonstrates better or equal performance compared to GAN. While, in Fig. 3a, DP-GAN is slightly worse than GAN. The maximum RMSE difference between DP-GAN and GAN is 0.06. Whereas, DP-GAN model demonstrates a better privacy preservation shown in Fig. 2a. Apparently, DP-GAN exhibits a better trade-off between privacy protection and data utility. The differential privacy identifier may constraint synthetic data generation in order to preserve privacy. Thereby, it causes harm to the data utility.

In Fig. 4, we use a different evaluation metric, Pearson Correlation, for further evaluation. Pearson's correlation shows the degree of linear correlation between the generated data and raw data. Fig. 4 presents experimental results similar to those shown in Fig. 3. It shows DP-GAN perform slightly worse or equally for data utility in comparison with GAN. In Fig. 4a, the data utility scores of DP-GAN vary at different time slots. The score differences between DP-GAN and GAN vary from 0 to 0.07. It also indicates that DP-GAN scarifies data utility to compensate for privacy preservation. In Addition, Laplace model has the lowest data utility among all three models.

From the experimental outcomes shown in Section 4.2 and Section 4.1, it is evident that the proposed model shows the best performance by striking the most optimised balance between privacy preservation and data utility. The second optimal model is GAN, where its trajectory is the closest to the true trajectory compared to DP-GAN and Laplace model. However, it provides the lowest degree of privacy guarantee with the smallest noise injection among three models. It means GAN sacrifices privacy protection level in order to enhance data utility. Besides, it fails to theoretically provide strict privacy protection to the published data compared with DP and its variants. Laplace model shows the worst performance by purely focusing on privacy protection without considering data utility. That is contrary to privacy-preserving countermeasure implementation. We are not surprised by such an experimental outcome, considering Laplace cannot perform well when the input data are sparse.

4.3. Efficiency
In terms of efficiency measurement, we use the learning rate to evaluate the proposed model and two baselines. In general, a faster convergence speed with less iteration time indicates a faster learning rate. Thereby, a model can show its efficiency by exhibiting a fast learning rate in training. We illustrate experimental result in Fig. 5. Each step contains 50 iteration times. It can be seen that both GAN and DP-GAN converge with the increase of iteration steps. Laplace model does not have learning process, thereby it does not converge. GAN has a faster convergence speed at the same iteration time. It converges at around 42 steps, that is 2100 iteration times. DP-GAN reaches convergence at 50 steps, that is 2500 iteration times. Although the convergence speed of DP-GAN is slower at the beginning, its iteration time for final convergence is roughly the same as GAN. So, it does not affect the ultimate efficiency. This result is consistent with our analysis since we introduce one more player into the adversarial framework. It increases the needs of computational power and causes a lower learning rate. Therefore, the convergence speed of DP-GAN is slightly lower than GAN, which is acceptable.

Fig. 5

Fig. 5. Evaluation on convergence efficiency in terms of iteration times.

5. Summary and future work
In this paper, we proposed a differentially private model that employing generative adversarial game framework with three players, namely DP-GAN. We devised a novel perceptron, differential privacy identifier, to enhance synthetic data in a differentially private manner. In the setting of continual data publishing, we conduct extensive evaluations in terms of privacy guarantees level, data utility, and efficiency. The experimental result shows the superiority of DP-GAN model compared with the baseline models.

In future work, we plan to manage the resource allocation of the two games to improve the efficiency and lessen the requirements of computational power. In addition, we intend to use generative adversarial nets to further optimize privacy protection by deriving the minimum privacy budget.