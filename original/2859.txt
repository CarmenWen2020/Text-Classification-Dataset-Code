How can we transfer the knowledge from a source domain to a target domain when each side cannot observe the data in the other side? Recent transfer learning methods show significant performance in classification tasks by leveraging both source and target data simultaneously at training time. However, leveraging both source and target data simultaneously is often impossible due to privacy reasons. In this paper, we define the problem of unsupervised domain adaptation under blind constraint, where each of the source and the target domains cannot observe the data in the other domain, but data from both domains are used for training. We propose TAN (Transfer Alignment Network for Blind Domain Adaptation), an effective method for the problem by aligning source and target domain features in the blind setting. TAN maps the target feature into source feature space so that the classifier learned from the labeled data in the source domain is readily used in the target domain. Extensive experiments show that TAN (1) provides the state-of-the-art accuracy for blind domain adaptation outperforming the standard supervised learning by up to 9.0% and (2) performs well regardless of the proportion of target domain data in the training data.

Access provided by University of Auckland Library

Introduction
How can we minimize data distribution gap between source and target domains under constraint of data visibility, such that each side cannot observe the data in the other side? Recent state-of-the-art deep learning algorithms perform well on test data when given abundant labeled training data. However, in real applications, collecting a large amount of labeled data needs tedious labor work, while unlabeled data are abundant. Given a certain number of unlabeled data from a target domain, unsupervised transfer learning exploits the knowledge from a related source domain, which has enough unlabeled data accompanying small number of labeled data, to improve the model performance on the target domain. Since the source domain is related to but different from the target domain, data distribution shift exists between the two domains. Domain adaptation [16, 21] attempts to reduce the data distribution shift by effectively transferring knowledge from the source to the target domains. Recent state-of-the-art methods focus on digging out domain-invariant features and learn transferable representations to improve transferability. Cao et al. [6] proposed transfer learner by extracting invariant feature representations, while [26] applied generative adversarial model and generated feature extractor to confuse domain classifier. The premise of applying such methods is that source and target data are visible simultaneously. However, in real-world settings including hospital cases, it is not allowed to access source and target data simultaneously for privacy reason. For example, if hospital A (source) already trained a reliable model using its own data, hospital B (target) can distill knowledge by transferring the pre-trained model but cannot access the hospital A’s data because of privacy.

Fig. 1
figure 1
Comparison between domain adaptation with or without blind constraint. a Domain adaptation without blind constraint leverages both source and target data simultaneously at training time. b Domain adaptation with blind constraint leverages only one of the source and the target data for training at a specific time. That is, it first uses source data (without accessing target data) and then uses target data (without accessing source data) for training

Full size image
In this paper, we define the problem of unsupervised domain adaptation under blind constraint which restricts mutual accesses between the source and the target domains. As shown in Fig. 1, general domain adaptation can leverage the source and the target data simultaneously, but domain adaptation under blind constraint cannot utilize the two data simultaneously because of the restriction on data accessibility. Due to the constraint, existing methods on domain adaptation are not suitable for the new problem. We are exposed to two challenges: (1) since the source and the target data are not visible simultaneously, we have no way of knowing how much the data distribution shifts between the two domains, and (2) the target data cannot be used to train a meaningful classifier due to the missing label.

To address the challenges, we propose TAN (Transfer Alignment Network for Blind Domain Adaptation), an effective method for blind domain adaptation which resolves the data distribution shift problem by aligning the source and the target feature spaces. TAN maps the target features into the source feature space in order to utilize the classifier trained in the source domain to the target data. The experimental results show the superior performance of TAN.

Our contributions in this paper are the following:

Problem Definition. We propose a new problem of unsupervised domain adaptation under blind constraint, when the source data contain both unlabeled and labeled data, and the target data are unlabeled. The problem is challenging due to the blind setting where it is impossible to observe both the source and the target data simultaneously.

Method. We introduce an accurate method TAN for blind unsupervised domain adaptation. TAN aligns data distribution of source and target domains despite the blind setting.

Experiments. Extensive experiments on real-world datasets show that TAN provides the state-of-the-art performance outperforming the standard supervised learning approach by up to 9.0%. We also observe that our idea of transfer aligner helps improve the accuracy, and such improvement is consistent for varying amounts of target unlabeled data.

In the rest of this paper, we describe preliminaries, proposed method, experimental results, related works, and conclusion. Table 1 provides the definitions of symbols used. The code of TAN is available in https://github.com/snudatalab/TAN.

Table 1 Table of symbols
Full size table
Preliminaries
In this section, we provide brief description of autoencoder pre-training and transfer learning with fine-tuning.

Autoencoder pre-training
In order to leverage unlabeled data in neural network training, the most common approach is to initialize parameters by weights pre-trained using autoencoder [11]. Autoencoder [4] is trained to reconstruct feature vector from code constrained either in dimensionality or sparsity. After training, the encoded vector from each input feature has enough information to reconstruct the original feature and represents the hidden structure in the input feature space.

Autoencoder is composed of two parts: encoder and decoder. Encoder maps the input feature into a constrained code, and decoder maps the code back to feature reconstruction. Given input feature vector x, encoder f maps x to code h=f(x). The decoder g maps h to reconstruction x^=g(h)=g(f(x)). The model is trained to minimize the reconstruction error ∑i||x^i−xi||2 where x^i is a reconstruction g(f(xi)) from input xi.

Given large amount of unlabeled data and few labeled data, we use code vector f(x) from encoder as an input in a classifier to effectively train it using the few labeled data. If the code vector has enough information to reconstruct the input feature vector, then it would contain enough information for the classifier as well.

Transfer learning with fine-tuning
Transfer learning with fine-tuning approach is a common approach in neural network training. In the training process, a neural network classifier learns useful intermediate hidden units [3]. These hidden units provide enough information for a simple classifier to achieve the state-of-the-art performance not only in the same task [36] but also in related tasks [35]. This property has been exploited to use a neural network trained on a source domain to initialize a model in a target domain. With careful analysis [43], it has been shown that transferring pre-trained neural network features improves performance. Fine-tuning is important to transform source-specific features into target-specific features and to alleviate layer co-adaptation problem. It has been a common practice in neural network transfer learning to adapt parameters that are pre-trained from a source domain and fine-tune the model on a target domain.

Proposed method
In this section, we precisely define the problem of blind unsupervised domain adaptation and its challenges. Then, we describe our method TAN in detail.

Problem definition and challenges
In unsupervised domain adaptation, a source domain has abundant unlabeled data Xs,u={xs,ui}ns,ui=1 and small number of labeled data Xs,l={(xs,li,ys,li)}ns,li=1. A target domain has only limited number of unlabeled data Xt,u={(xt,ui)}nti=1. Note that the number ns,u of unlabeled samples from the source domain is larger than the number nt from the target domain. Both of the source and the target data contain the same number K of classes. The source and the target data are sampled from distributions p and q, respectively, where p≠q. Our goal is to learn a classifier which performs well in a target classification task under blind constraint, where each domain cannot observe the data in the other domain.

Due to the blind constraint, (1) existing domain adaptation models cannot be implemented directly, and (2) the distribution discrepancy between the source and the target data is not measured easily. Another complication is that the target data have no labels, which makes it impossible to train a classifier if using only the target data without accessing the labeled data at the source domain. We need a novel method to match feature distributions of source and target domains in the blind setting.

Transfer Alignment Network for Blind Domain Adaptation
The target data cannot be used to generate a classifier due to missing labels, which forces us to utilize the source domain data to train a classifier and to make use of it for the target domain. Even though there are a large amount of data in the source domain, most of them are unlabeled; it is hard to train a good classifier using only the small amount of labeled data in the source domain. Thus, we first use the source unlabeled data to train an autoencoder to extract meaningful features. Then, the source labeled data go through the autoencoder to become valuable features, and we utilize the features with labels to train a classifier. Due to the blind constraint, we can transfer only the model trained in the source domain to the target domain but not the source data. After transferring the model to the target domain, we fine-tune the autoencoder with the target unlabeled data, but save the autoencoder before the fine-tuning for a reason described soon. If we directly feed the extracted target domain features into the pre-trained classifier, it would seriously damage the accuracy because of the distribution discrepancy between the source and the target data.

Our key idea to tackle this problem is to align the distribution of the source domain and the target domain, using only the target data (without the source data) and the saved autoencoder trained from the source data. Although we cannot observe the source data when using the target data for training, we do have the saved autoencoder which contains knowledge of the source data. Given the target unlabeled data as input, we add and train a transfer aligner on top of the fine-tuned autoencoder such that (1) the output from the saved autoencoder and (2) the output from the transfer aligner are similar. After being passed through the transfer aligner, the target domain features have a distribution close to the source distribution, so that we can use the classifier trained in the source domain to classify the target unlabeled data.

Fig. 2
figure 2
Structure of TAN. In training step 1, we train an autoencoder with unlabeled source instances. In training step 2, we train a stack of classifier (MLP) and the encoder using labeled source instances. In training step 3, we transfer the model to a target domain and fine-tune the autoencoder on unlabeled target instances. In training step 4, a transfer aligner is trained to map the target feature space back to the source feature space using unlabeled target instances. At test time, each instance goes through the final model consisting of (1) the fine-tuned autoencoder, (2) the transfer aligner trained at step 4, and (3) the source classifier trained at step 2

Full size image
Figure 2 illustrates the architecture of our method TAN. The whole architecture consists of encoder, transfer aligner, and multilayer perceptron. We train an autoencoder with unlabeled source instances in Step 1. The blue rectangle in Fig. 2 is a feature vector extracted from the encoder. In Step 2, we take the encoder trained from Step 1 and add a multilayer perceptron on top of it. The multilayer perceptron is trained with labeled source instances, while the encoder is not updated. In Step 3, we transfer the model to the target domain, but only fine-tune the autoencoder using unlabeled target instances. In Step 4, the transfer aligner is trained to directly map the target code vector into the source code space using the unlabeled target instances. We train the transfer aligner falign to map the code of the target encoder ft,encode to the code of the source encoder fs,encode for the target unlabeled data Xt,u. As shown in Fig. 2, the encoder, the transfer aligner, and the classifier are stacked together for the complete model. We test the final model with unlabeled target test data. The whole algorithms are described in Algorithms 1 and 2. We further elaborate on each step as follows:

figure c
figure d
Step 1. The encoder fencode and the decoder fdecode are trained on source unlabeled data Xs,u. The model is trained to minimize the following autoencoder reconstruction objective function.

∑xi∈Xs,u||xi−fdecode(fencode(xi))||2
(1)
Step 2. We train a classifier fclassify on top of the previous encoder fencode on source labeled data Xs,l with fine-tuning. The training process minimizes the following cross-entropy objective function.

−∑xi∈Xs,l∑k∈K1{yi=k}logfclassify(fencode(xi))k
(2)
We call the encoder fencode and the classifier fclassify at this step as source encoder fs,encode and source classifier fs,classify, respectively. The snapshot of current parameters of the source encoder fs,encode is stored separately.

Step 3. The encoder fencode and the decoder fdecode are fine-tuned on target unlabeled data Xt,u again with the following standard autoencoder reconstruction objective function.

∑xi∈Xt,u||xi−fdecode(fencode(xi))||2
(3)
We call the encoder fencode with fine-tuned parameters at this step as target encoder ft,encode. The snapshot of current parameters of the target encoder ft,encode is again stored separately.

Step 4. The transfer aligner falign is trained to map the target code ft,encode(x) to the source code fs,encode(x) on the target unlabeled data Xt,u. The objective function is L2 distance between the source code fs,encode(xi) and the mapped target code falign(ft,encode(xi)):

∑xi∈Xt,u||fs,encode(xi)−falign(ft,encode(xi))||2
(4)
In this step, only the transfer aligner falign is trained, while the encoders are fixed.

Experiments
Table 2 Statistic of the datasets
Full size table
In this section, we present experimental results to answer the following questions.

Q1. Predictive Performance. What are the accuracies of TAN and competitors for blind domain adaptation? (Sect. 4.2)

Q2. Transfer Aligner. Does the benefit of transfer aligner come from alignment of features rather than deployment of more layers? (Sect. 4.3)

Q3. Amount of Target Unlabeled Data. How do TAN and competitors perform for various amounts of target unlabeled data? (Sect. 4.4)

Experimental setting
Datasets
We conduct experiments on five multivariate datasets and an image dataset used for standard benchmark summarized in Table 2: HIGGS,Footnote1 HEPMASS,Footnote2 SUSY,Footnote3 Sensorless,Footnote4 Gas,Footnote5 and Office-31Footnote6.

Multivariate data. We choose five multivariate datasets from UCI Repository, considering the number of instances, classes, and feature dimensions. HEPMASS, HIGGS, and SUSY are sensor observations of particle collision experiments where labels are binary and denote the actual generation of the particles of interest. Sensorless and Gas are also sensor observations where labels denote the diagnosis of motor status and the type of gas in chemical reaction, respectively.

Image data. Office-31 is a standard benchmark for domain adaptation, which consists of 4,652 images with 31 classes. The dataset is from 3 domains. Amazon (A) contains images from amazon.com. Webcam (W) and DSLR (D) contain environment images taken by a webcam or a DSLR camera, respectively. We consider all possible transfer tasks, A→W, W→A, A→D, D→A, W→D, and D→W.

Data preprocessing
Fig. 3
figure 3
Selecting initial centroids c1, c2, and c3 for K-means (k=3). a Randomly select a point (⊕). b Select the farthest point (⊗) from the ⊕ point. c Select the point with the farthest shortest distance to the existing two centroids. d Select the point with the farthest shortest distance to the existing three centroids

Full size image
Multivariate data. In order to perform the domain adaptation experiments, we require a dataset with source/target labeled/unlabeled instances. We split each dataset into source and target datasets with 9:1 ratio. Note that the source and target domains should have the same feature space but different feature distributions in domain adaptation. Therefore, we apply K-means clustering to divide the total data into 10 clusters since the ratio of the source data to the target data is 9:1. One cluster represents the target domain, and the rest represents the source domain so that instances in each domain are closer to each other compared to those in other domains. The steps for selecting initial centroids of K-means clustering are as follows. First of all, we select a random point xr. Then, we set the farthest point c1 from xr as the first centroid. The 2nd centroid c2 is determined to be the point with the farthest distance to c1. The kth centroid ck for k=3,4,...,10 is determined as the point with the farthest shortest distance to the existing centroids: ck=argmaxximin(||xi−c1||2,...,||xi−ck−1||2). We pick the farthest point to clearly distinguish domains. Figure 3 shows the process of selecting initial centroids when k is 3. We repeat this process 5 times with different random points xr and select the set of centroids with the smallest intra-cluster variance after clustering. Compared to the method that randomly initializes 10 centroids, our initialization method more evenly distributes the centroids. The found 10 clusters can be divided into source and target data with 9:1 ratio, which leads to 10 possible combinations of source and target data. We select the cluster with the largest H-divergence [2] to other clusters as the target data. For efficient computation of H-divergence, we approximate it using Proxy A-distance [2] d^A=2(1−2ϵ) where ϵ is the error of correctly discriminating source and target domains. Because target data instances are sampled from only one cluster, source and target datasets have different feature distribution as intended. We use 10% of the target data as target test data and the rest of the data as target training data. The amount of the source test data is the same as that of the target test data because it is enough to test the source model. Furthermore, we divide the source training data into source unlabeled and source labeled data with 9:1 ratio. The final data ratio is Source Unlabeled (Training) : Source Labeled (Training) : Source Labeled (Test) : Target Unlabeled (Training) : Target Labeled (Test) = 80.1 : 8.9 : 1 : 9 : 1.

Image data. Office-31 dataset has three domains: Amazon, Webcam, and DSLR. We select two of them as a source, and the rest as a target, which makes total 6 combinations. We split data into train and test data with 8:2 ratio in each domain; we also divide the source training data into source unlabeled and source labeled with 9:1 ratio. The ratio in source data is Source Unlabeled (Training) : Source Labeled (Training) : Source Labeled (Test) = 7.2 : 0.8 : 2. The ratio in target data is Target Unlabeled (Training) : Target Labeled (Test) = 8 : 2.

Model settings
Multivariate data. We examine the effect of transfer aligner for domain adaptation with the following architecture: 80-40-20-10 autoencoder, 30-80-150 transfer aligner, and 300-300-300-300-300 classifier on multivariate dataset. We apply ReLU activation function for all hidden layers (including hidden layers in encoder, decoder, transfer aligner, and MLP), and softmax function for the prediction layer.

Table 3 Architecture of the convolutional autoencoder for image data
Full size table
Image data. We apply 2D convolution layers for the encoder, and 2D transposed convolution layers for the decoder. We use 3×3 kernels with stride 2 and padding size 1. The detailed architecture of the convolutional autoencoder for image data is shown in Table 3. To reconstruct images at the end of the decoder, the value of each pixel of the reconstructed images should be in range 0 to 255; we normalize the input data, add a sigmoid function at the end of the decoder, and multiply 255 to the output of the sigmoid.

We implement all methods based on the Tensorflow framework. All experiments are carried out on Geforce RTX 2080Ti, and our proposed method TAN takes about 12 gpu-hours to train the model on HIGGS, HEPMASS, and SUSY, 6 gpu-hours on Sensorless and Gas, and 10 gpu-hours on Office-31 data. All the other competitors take less time than TAN.

Competitors
We compare TAN with the following baselines.

S(UL): a stack of an encoder and a neural network classifier trained using source data (unlabeled data and labeled data, respectively) and tested on target data without fine-tuning. This corresponds to using only Steps 1 and 2 of Fig. 2 in training.

S(UL)-T(U): this model retrains the S(UL) model with the target unlabeled data. This corresponds to using Steps 1, 2, and 3 of Fig. 2 in training. In another point of view, this model is similar to TAN, but without the transfer aligner in Step 4.

S(UL)-T(U)-Large: this model is similar to S(UL)-T(U), but contains more layers and parameters in MLP of Step 2 so that the total number of parameters is close to that of TAN.

Predictive performance (Q1)
Table 4 Test accuracy (%) of TAN and baseline models on multivariate datasets. TAN gives the best accuracy
Full size table
Table 5 Test accuracy (%) of TAN and baseline models on Office-31 dataset. TAN gives the best accuracy in most cases
Full size table
The accuracy on target test set is summarized in Tables 4 and 5. TAN achieves the best predictive performance outperforming the baseline methods in 11 out of 12 cases. The accuracy boost of TAN compared to naive supervised learning (S(UL)) is up to 9.0% and 3.9% on multivariate and image datasets, respectively. This shows that using the target unlabeled data via transfer learning helps improve the accuracy. Also, TAN shows much better accuracy than S(UL)-T(U) which further fine-tunes the model with the target unlabeled data. The reason is that S(UL)-T(U) causes the mismatch of distributions in the source and the target feature spaces. On the other hand, TAN uses the transfer alignment layer for matching the distributions which leads to superior performance in blind domain adaptation.

Transfer aligner (Q2)
We examine the effect of transfer aligner in TAN. Specifically, we verify the hypothesis that the superior performance of TAN does not come from its mere increase in its layer size. To verify it, we train a competitor S(UL)-T(U)-Large which uses more layers compared to S(UL)-T(U) so that the number of parameters is the same as that of TAN. Table 8 compares the architectures of TAN and S(UL)-T(U)-Large on multivariate and image datasets, respectively. Instead of utilizing the transfer aligner, S(UL)-T(U)-Large extends the size of MLP to ensure the same total number of weights in the whole network.

Tables 6 and 7 show TAN consistently outperforms S(UL)-T(U)-Large with better accuracy up to 5.7% and 10.3% on multivariate and image datasets, respectively. This result verifies that the superior accuracy of TAN comes from its careful design of transfer aligner, not from its increase in the number of parameters.

Table 6 Effect of transfer aligner in test accuracy (%) for multivariate datasets. The superior accuracy of TAN comes from its transfer aligner, not from the increased number of parameters
Full size table
Table 7 Effect of transfer aligner in test accuracy (%) for Office-31 dataset. The superior accuracy of TAN comes from its transfer aligner, not from the increased number of parameters
Full size table
Table 8 Architecture of TAN and S(UL)-T(U)-Large on multivariate and image datasets
Full size table
Amount of target unlabeled data (Q3)
We vary the size of the target unlabeled data and inspect the performance of TAN and competitors. For each dataset, we set the proportion of sampled target unlabeled training data to 10%, 33%, and 67% of the total target training data, to ensure that the numbers of target unlabeled instances are at least 100. As shown in Figs. 4 and 5, TAN consistently outperforms baselines in 42 out of 48 cases, showing the stability of transfer aligner in our method. See Tables 9 and 10 for details of the accuracy.

Fig. 4
figure 4
Test accuracy of TAN and baseline models for varying ratios of target unlabeled data. TAN gives the best accuracy regardless of the ratio of target unlabeled data in all datasets

Full size image
Fig. 5
figure 5
Test accuracy of TAN and baseline models for varying ratios of target unlabeled data. TAN gives the best accuracy in most cases

Full size image
Related work
Training deep neural networks requires a massive amount of data; however, task-specific labeled data are often scarce. A common approach for the problem is to extract more information from other sources of data: unlabeled datasets from the same domain, or datasets from other domains.

Transfer learning [29, 41] aims to transfer a machine learning model in a source domain to a different target domain having different distribution from that of its training data. Generalizable knowledge learned from the source domain is leveraged to learn a better model in the target domain. There have been many researches applying transfer learning to achieve the state-of-the-art performance in both computer vision [1, 14, 22, 30] and natural language processing  [8, 9, 15, 42] domains.

Existing methods on deep transfer learning frequently minimize feature distribution discrepancy between source and target domains [5, 12, 13, 23,24,25, 37,38,39, 44]. These methods utilize source and target data together in the training process to calculate and optimize the feature distribution discrepancy. Other works on transfer learning iteratively assign pseudo-labels for unlabeled target data and train models using the assigned pseudo-labels [6, 7, 17, 20, 27, 31, 32, 34]. These methods iteratively fine-tune models on both source and target data to provide better pseudo-labels. Most recently, proposed works [10, 18, 26, 28, 33] exploit adversarial training to learn the structure in feature space. These studies measure the mismatch in data distribution by utilizing both source and target data. Unlike the aforementioned approaches, our goal is to perform domain adaptation when both of source and target data are not visible simultaneously.

Although existing blind domain adaptation methods  [19, 40] do not observe target data when using source data for training, they utilize only the labeled source data, not the unlabeled source data, and they often do not use the target data for training. On the other hand, our goal is to solve the blind domain adaptation problem when the source data contain both labeled and unlabeled data, and both of source and target data are used for training, even though they are not visible simultaneously.

Table 9 Test accuracy (%) of TAN and baseline models on multivariate datasets under different sampling ratios (SR)
Full size table
Table 10 Test accuracy (%) of TAN and baseline models on Office-31 dataset under different sampling ratios (SR)
Full size table
Conclusion
We propose TAN, an accurate deep transfer learning method for blind domain adaptation. Since the target domain data have no labels, which makes it infeasible to train a classifier in the target domain, we utilize the classifier trained on a source domain to discriminate the target data. We minimize the domain discrepancy between source and target feature spaces, in order to make the classifier have a better performance on the target domain data. However, source and target data are not visible simultaneously under the blind constraint setting, which means general domain adaptation methods cannot be applied to the problem. To address this issue, TAN exploits a transfer aligner that maps the target feature space to the source feature space without accessing any source data. Extensive experiments show that TAN provides the best transfer accuracy under the blind constraint and shows stable performance for varying amounts of target unlabeled data. Future works include extending TAN to heterogeneous transfer learning.