A crucial technical challenge for cybercriminals is to keep control over the potentially millions of infected devices that build up their botnets, without compromising the robustness of their attacks. A single, fixed C&C server, for example, can be trivially detected either by binary or traffic analysis and immediately sink-holed or taken-down by security researchers or law enforcement. Botnets often use Domain Generation Algorithms (DGAs), primarily to evade take-down attempts. DGAs can enlarge the lifespan of a malware campaign, thus potentially enhancing its profitability. They can also contribute to hindering attack accountability.

In this work, we introduce HYDRAS, the most comprehensive and representative dataset of Algorithmically-Generated Domains (AGD) available to date. The dataset contains more than 100 DGA families, including both real-world and adversarially designed ones. We analyse the dataset and discuss the possibility of differentiating between benign requests (to real domains) and malicious ones (to AGDs) in real-time. The simultaneous study of so many families and variants introduces several challenges; nonetheless, it alleviates biases found in previous literature employing small datasets which are frequently overfitted, exploiting characteristic features of particular families that do not generalise well. We thoroughly compare our approach with the current state-of-the-art and highlight some methodological shortcomings in the actual state of practice. The outcomes obtained show that our proposed approach significantly outperforms the current state-of-the-art in terms of both classification performance and efficiency.

Previous
Next 
Keywords
Malware

Domain Generation Algorithms

Botnets

DNS

Algorithmically Generated Domain

1. Introduction
The continuous arms race between malware authors and security researchers has pushed modern malware to evolve into highly sophisticated software, capable of infecting millions of devices. The vast amount of sensitive information that can be extracted from compromised devices, coupled with the harnessing of their resources and processing power, provides a wide range of monetisation methods fuelling a flourishing worldwide underground economy.

While device infection is the key that paves the way in, the main objectives are generally persistence and orchestration. An orchestrating entity, the botmaster, manages infected devices (bots) which in many cases can scale to the order of millions, creating a botnet (Singh et al., 2019). The botmaster manages a Command and Control (C&C) server that communicates with the bots. This communication must preserve some degree of unlinkability to thwart any attempts to identify the botmaster. To ensure unlinkability, and as a counter-measure against take-down operations, botnets frequently make use of domain fluxing (Perdisci et al., 2012, Zang et al., 2018) through Domain Generation Algorithms (DGAs). DGAs produce a vast amount of domain names, which bots try to communicate with iteratively to find the actual C&C server. However, only a small part of them is registered and active, creating a hydra effect (Nadji et al., 2013). The botmaster may regularly pivot control between domains, thus hampering the task of seizing control of the botnet. This is helped by the fact that an outsider cannot determine which domains will be used, nor statically block all these requests. The latter stems from the fact that there are too many domains, and the seed yielding a particular sequence of domains might be unknown or change frequently. Fig. 1 illustrates the modus operandi of a typical DGA-powered botnet.


Fig. 1. The modus operandi of a typical DGA-powered botnet (Patsakis et al., 2020).

Currently, there are several families of DGAs employed by various malware with varying rates of requests and different characteristics. This heterogeneous landscape hinders the timely and accurate detection of an Algorithmically-Generated Domain (AGD) (Yadav and Reddy, 2012) request, which could serve as a precise indicator of compromise (IoC) of a host at the network level. Recent research tries to categorise DNS requests per DGA, often exploiting WHOIS-based features. From the perspective of an ISP, CSIRT or CERT such an approach might be beneficial. However, we argue that in terms of endpoint security that strategy cannot be considered adequate. First, the network operator does not generally know which concrete DGA is utilised by the malware that infected a given host in her network. Second, the utilisation of the WHOIS database introduces a significant time delay that in many situations cannot be tolerated.

1.1. Contributions
Motivated by the continuous evolution of DGAs we introduce a dataset collecting real-world domains called HYDRAS, which consists of more than 95 million domains belonging to 105 unique DGA families. To the best of our knowledge, this is the largest and most representative DGA dataset to date.

During the analysis of our dataset, the possibility of differentiating between benign and malicious requests in real time is discussed, as well as the identification of the malware families using them. Based on information learned from the analysis of our dataset, a novel feature set is designed and implemented, which includes lexical and statistical features over the collected DGAs, as well as English gibberish detectors. Using the proposed feature set and a Random Forest as a representative of ensemble classifiers, we perform a thorough evaluation of our dataset and show that our feature set together with the Random Forest classifier outperform the state-of-the-art approaches in terms of both classification performance and overhead.

Next, inherent biases in related works are highlighted. These biases can be attributed to the suboptimal selection of datasets and/or features, preventing their application in general, real-world scenarios. For example, employing a dataset comprising only a few families that exhibit very characteristic patterns might ease the classification task, providing accurate detection rates (e.g., the generators for cryptolocker, ramnit, geodo, locky, tempedreve, hesperbot, fobber and dircrypt provide a uniform distribution of letters, Tran et al., 2018, Woodbridge et al., 2016), but will inevitably lead to ad-hoc solutions that are too specific and cannot be generalised. This is, unfortunately, a common practice in the existing literature.

A typical example is only considering families like bamital, CCleaner or chir in the datasets, which all produce hexadecimal values of specific length as second-level domains (SLDs). It is obvious that one can easily differentiate benign domains from such DGAs with almost 100% accuracy by merely checking whether the SLD is a hex value of a specific length. Nonetheless, not all DGAs families are so easy to detect in real scenarios.

Due to the particularities of this research field and the methodologies used by the current state-of-the-art, we also highlight some recommendations for fairer future evaluations. These are particularly relevant for comparing the results of our experiments with other approaches.

1.2. Organisation
The rest of this work is organised as follows. In Section 2, DGA-related preliminaries are briefly described, and in Section 3 a thorough review of related work is provided. Then, in Section 4, our dataset is detailed. Afterwards, we describe our approach, including methodology, feature extraction and the tools and algorithms employed. In Section 5, we describe the proposed features. We provide the results of our experiments in Section 6, Sections 7 Classification of adversarially designed AGDs, 8 Overhead of our approach, where they are compared to the state-of-the-art. In Section 9, an analysis of the outcomes as well as a methodological analysis and comparison with the literature is provided. Finally, the paper concludes in Section 10, discussing open issues for future research.

2. Background
DGAs are one of the main pillars behind the success of botnets. They were first conceived more than a decade ago, and they have been steadily refined over the years by successive generations of malware developers. These algorithms generate a set of AGDs to communicate with C&C servers, thus eliminating the risks associated with using static IP addresses (Antonakakis et al., 2017, Nadji et al., 2017). In Patsakis and Casino (2019), the authors generalise the notion of DGAs by extending them to other protocols beyond DNS, and they propose the term Resource Identifier Generation Algorithms (RIGAs). The authors show how decentralised permanent storage (DPS) has some potential drawbacks and exploitable characteristics for armouring a botnet, a fact that has already been exploited in the real world (Anomali Labs, 2019) due to the immutability properties of DPS. Fig. 2 depicts the hierarchy of RIGAs.

In its most basic form, DGAs create a set of domain names by using a deterministic pseudo-random generator (PRNG) (Sood and Zeadally, 2016, Perdisci et al., 2012). Therefore, (infected) devices belonging to a botnet query a set of domains generated by the DGA until they are correctly resolved to a valid IP, corresponding to the C&C server. Since the location of the C&C server dynamically changes, blacklisting domains is a very inefficient protection technique. Additionally, this makes seizing the botnet much more difficult, since one would need to take (register) all domain names generated by the DGA (with a given seed) for disrupting the botmaster only for a short amount of time. This process will generally be very costly, typically involving thousands of domain names for stopping the botnet for just a day. Hence, the botmaster benefits from this asymmetry between the high ratio of generated domains to registered ones. That makes her operation cheap, as compared to the cost of defending against it, which involves registering all possible domains.


Fig. 2. RIGA generation flow, including diverse context protocols (e.g., DPS and DNS).

3. Related work
3.1. Traditional approaches
According to the literature, there are two main DGA families: (i) Random-based DGA methods, which use a PRNG to generate a set of characters that form a domain name, and (ii) Dictionary/Wordlist-based DGA methods, which use a dictionary to produce domains. Nevertheless, one may also consider other types of DGA families, which use more subtle approaches, i.e. valid domains that were previously hacked to hide their C&C servers (i.e. domain shadowing) (Liu et al., 2017) as well as DGAs that generate domains that are very similar to existing valid domains (Bader, 2015), further hindering the detection task. Considering the dependency of the pre-shared secret (or seed) on time, Plohmann et al. (2016) further categorise DGAs into: (i) time-independent and deterministic, (ii) time-dependent and deterministic, and (iii) time-dependent and non-deterministic.

In the case of random-based DGA detection, a common practice is to analyse some features of the domain names and their lexical characteristics to determine whether a DGA has generated them (Aviv and Haeberlen, 2011, Yadav et al., 2012). Moreover, auxiliary information such as WHOIS and DNS traffic (e.g. frequent NXDomain responses) is often used to detect abnormal behaviours (Zhou et al., 2013, Jiang et al., 2010, Antonakakis et al., 2012). Other approaches use machine learning-based techniques and combine the previous information to identify Random-based DGA such as in Jiang et al., 2010, Yadav and Reddy, 2012, Manadhata et al., 2014 and Zhao et al. (2015).

Due to their inherent construction, wordlist-based DGA detection represents a challenging task for classifiers. In this regard, the common approach is to use machine learning approaches (e.g., feature-based classification and deep learning) to distinguish between benign and malicious DGAs. The use of random forest classifiers (RF) based on a set of features such as word correlation, frequency, and part-of-speech tags was first proposed in Yang et al. (2018). Similarly, Selvi et al. suggested the use of RF with masked n-grams as a feature, achieving a remarkable accuracy in the binary classification task (Selvi et al., 2019).  Berman (2019) put forward a methodology based on Capsule Networks (CapsNet) to detect AGDs; the author compared his method with well-known approaches such as RNNs and CNNs, and the outcomes showed similar accuracy yet better computational cost. Xu et al. (2019) suggested the combination of n-gram and a deep CNNs to create an n-gram combined with character-based domain classification (n-CBDC) model that does not require domain feature extraction. Vinayakumar et al. (2019) implemented a set of deep learning architectures with Keras and classical machine learning algorithms to classify DGA families. Their best configuration uses RNNs and SVMs with a radial basis function (SVM-RBF). Yang et al. (2020) present a heterogeneous deep neural network framework, which extracts the local features of a domain name as well as a self-attention based Bi-LSTM to extract further global features. Their outcomes showed higher accuracy than traditional DGA classifiers. Finally, a recent approach based on the probabilistic nature of wordlist-based DGAs was proposed in Patsakis and Casino (2021). In their work, Patsakis et al. proposed the combination of feature-based extraction with a probabilistic-based threshold to fully capture wordlist-based AGDs. Moreover, their method was capable of detecting both real-world and custom DGAs created to fool traditional detectors.

3.2. Adversarial and anti-forensic approaches
Recently the exciting development of deploying anti-forensic techniques in DGAs has become popular. This aims to create hard-to-detect DGA families and to fight against high performing classifiers. Anderson et al. (2016) proposed a generative adversarial network (GAN), which can learn from and bypass classical detectors. Afterwards, they improved the performance of AGD detectors after training them with the data generated by the GAN. Alaeiyan et al. (2020) proposed a DGA family created with a genetic algorithm considering lexical features such as pronounceability. Their experiments showed that such a DGA family was hard to detect by classical approaches. In a similar vein, Yun et al. (2020) used n-gram distribution and the pronounceability/readability of domains as a basis to create a novel DGA based on neural language models and the Wasserstein GAN (WGAN), which reduced detection rates in traditional DGA techniques.

Spooren et al. (2019) showed that their deep learning RNN performs significantly better than classical machine learning approaches. Besides, the authors stressed that one of the issues of manual feature engineering is that an adversary may adapt her strategy if she knows which features were used in the detection. Fu et al. (2017) proposed two DGAs using hidden Markov models (HMMs) and probabilistic context-free grammars, which were tested on state-of-the-art detection systems. Their results revealed their DGAs hindered the detection rate known detectors.

Finally, and due to the widespread use of covert or encrypted communication channels in DNS (e.g., DNSCurve, DNS over HTTPS and DNS over TLS) and in C&C connections in general (Zander et al., 2007, Homoliak et al., 2014), malware creators have an additional tool to hide their activity, rendering many traditional DGA detection useless.

Nevertheless, as shown by Patsakis et al. (2020), NXDomain detection can still be carried out in such a scenario. This also applies to feature extraction, so DGA families can still be classified with high performance.

4. The HYDRAS dataset
In this section, the HYDRAS dataset is introduced, which consists of a collection of benign and AGD domains, both real-world as well as adversarial. The name of the dataset originates from the insightful parallelism suggested by Nadji et al. (2013) between DGA-powered botnets and the mythical ancient Greek monster.

Benign domains are sampled from the Alexa 1M dataset. But since the Alexa dataset contains sites, not domains, it had to preprocessed. First, all top-level domain names (e.g., .com, .org) are removed from each entry and only the SLD are kept. Then, the duplicates were pruned since some web pages have multiple entries in the dataset (e.g., google.com and google.co.in) or been subdomains of identical services (e.g., various blogs of blogspot.com). Finally, all internationalised domain names were removed, since they are encoded using Punycode1 representation. After preprocessing the 1M Alexa dataset, the final dataset contains 915,994 unique domains.

The use of small and unrepresentative datasets, unfortunately very frequent in the literature, leads to several biases and other issues that can easily lead towards wrong analysis and misleading conclusions. For instance, the public feed of DGAs provided by the Network Security Research Lab at 3602 as well as the DGArchive (Plohmann et al., 2016) provide real-world datasets with millions of samples from many DGA families. Nonetheless, despite the numerous samples in both these datasets, many malware families are significantly underrepresented. A demonstrative example is the xshellghost family in the 360 dataset, which contains only a single sample at the time of writing. Thankfully, the researchers at 360 have reversed the code of this DGA.3


Table 1. Distribution of records per DGA in our dataset. DGAs in green denote those which were frequently underrepresented, so they were run to create more samples, while purple indicates adversarial ones.



Since the provision of many samples is required to perform an adequate evaluation of any detection technique, we utilised the available code of poorly represented DGAs to enlarge our dataset. The dataset was initialised with several public DGA repositories, e.g., J. Bader’s (Bader, 2020), A. Abakumov’s (Abakumov, 2020), and P. Chaignon’s (Chaignon, 2020). As explained above, we additionally used DGA code available at these and other repositories to generate additional samples for underrepresented DGA families. In these cases, a few random seeds and/or an extended date range to obtain new samples was used.

Since we used the code of the DGAs, the added domains have identical characteristics to original ones and might occur in the real-world. Thus, these AGDs could have been collected in a real setting. Moreover, the SLDs of three adversarially designed DGAs, namely deception, deception2 (Spooren et al., 2019) and khaos (Yun et al., 2020) were added.

In summary, our dataset consists of 95,325,598 AGDs belonging to a total of 109 families, from which 105 are unique.4 The families included, along with their corresponding number of collected samples, are reported in Table 1. The dataset is available for download at https://zenodo.org/record/3965397 (Casino et al., 2020).

5. Proposed features
We thoroughly analysed the AGDs in our dataset, as well as the ideas behind existing AGD detection approaches in the literature. We found out that the basic strategy for detecting non-wordlist-based DGAs is to take advantage of the fact that they, in general, make little effort to be human-memorable, as they typically are randomly generated. Moreover, even if they show a high correlation with readable words in terms of vowel/consonant usage, etc., the generated domains are expected to contain zero to only a few words having a short length.

5.1. Approach to feature extraction
A general description of our approach is as follows: On receiving a domain name, we first cache it to see correlations with previous ones. Then, we try to determine whether the SLD matches some specific patterns, e.g., whether it is a hex value, its combination of vowels/consonants, length, etc. Later, after removing all digits, we try to break the remaining characters into words. Within these words, the short ones (e.g., stop words, articles) are pruned and study the remaining to determine whether they are real words or just gibberish. Moreover, the entropy of the domain is computed and a subset of the patterns created during the correlation process. All the above provide us with several features that can be efficiently used to determine whether a domain name is benign or not, without the need for external information (e.g., WHOIS) or waiting for the DNS resolution revealing whether it is an NXDomain. In this way, a significant number of requests are pruned, regardless of their outcome.

Using the insights from our analysis of DGA families in the dataset, several features were engineered, defined in Table 2. The first set of parameters is computed when trying to identify valid n-grams and words. For the former, we train our n-gram model with Alexa n-grams and lengths three, four and five. For the latter, the wordninja5 word splitter was used, which probabilistically analyses its input using NLP based on the unigram frequencies of the English Wikipedia. Hence, the domain is split into meaningful words, according to a minimum word-length . Therefore, only terms which contain at least  characters are considered as significant. Then, we compute the percentage of the domain characters which are meaningful, by calculating the ratio  between characters belonging to words and the domain’s total length. Next, two more sets of features are computed according to statistical attributes as well as ratios using the previously calculated features.


Table 2. Features used in our approach and their corresponding description.

Feature set	Notation	Description
Alphanumeric sequences		Domain without TLD
 without digits
Set of 3-grams of 
Set of 4-grams of 
Set of 5-grams of 
Domain concatenated words
Domain concatenated words with spaces
 concatenated words
 concatenated words with spaces
Domain concatenated words of length  2
Domain concatenated words of length  3
Statistical attributes		The domain name is represented with hexadecimal characters
The length of 
The number of digits in 
The number of dots in the raw domain
The maximum number of consecutive consonants 
The maximum number of consecutive vowels 
Number of words with more than 2 characters in 
Number of words with more than 3 characters in 
Ratios		Ratio of consonants and vowels of 
Ratio of benign grams in 
Ratio of benign grams in 
Ratio of benign grams in 
Ratio of grams that contain a vowel in 
Ratio of grams that contain a vowel in 
Ratio of grams that contain a vowel in 
 divided by 
 divided by 
 divided by 
 divided by 
 divided by 
 divided by 
 divided by 
Gibberish probabilities		Gibberish detector 1 applied to 
Gibberish detector 1 applied to 
Gibberish detector 1 applied to 
Gibberish detector 1 applied to 
Gibberish detector 1 applied to 
Gibberish detector 1 applied to 
Gibberish detector 2 applied to 
Gibberish detector 2 applied to 
Gibberish detector 2 applied to 
Gibberish detector 2 applied to 
Gibberish detector 2 applied to 
Gibberish detector 2 applied to 
Entropy		Entropy of 
Entropy of 
Entropy of 
Entropy of 
Entropy of 
Entropy of 
Gibberish Detection. In addition, a Gibberish detection layer is used, which consists of two methods. The first one is a 2-character Markov chain Gibberish detector,6 which is trained with English text to determine how often characters appear next to each other. Therefore, a text string is considered valid if it obtains a value above a certain threshold for each pair of characters. The second is a Gibberish classifier.7 In this case, the method checks mainly three features of the text: whether (i) the amount of unique characters is within a typical range, (ii) the number of vowels is within a standard range and (iii), the word to char ratio is in a healthy range. Finally, the entropy of a subset of the alphanumeric sequences is computed, to enrich the feature set.

An exemplified overview of the feature extraction process is illustrated in Fig. 3.

A Comparison with the State-of-the-Art. Despite the fact that n-grams and some of the ratio features used in this paper are well-known and have been previously used in the literature, the combination presented in this work is novel. Moreover, we propose the use of two different Gibberish detectors, the vowel distribution of the specific n-grams computed from the Alexa domains, the statistical features computed over the different length-based words extracted by wordninja, and the entropy used in a subset of this novel features.


Fig. 3. Exemplified overview of the feature extraction process.

6. Classification experiments
We assess the power of our proposed features (see Section 5) to differentiate between malicious and benign domains (i.e., binary classification), as well as between several families of DGAs (i.e., multiclass classification). Since both empirical and theoretical results have shown that a combination of models (in an ensemble) can increase classification performance (Dietterich, 2000, Valentini and Masulli, 2002, Barandela et al., 2003, Kuncheva, 2014) even in the case of imbalanced datasets (Barandela et al., 2003), we opt for a Random Forest, which is a non-parametric ensemble classifier. Random Forest has previously achieved outstanding performance results in DGA classification tasks (Alaeiyan et al., 2020, Anderson et al., 2016, Selvi et al., 2019).

The hyperparameters of the Random Forest algorithm were tuned with grid search, to maximise classification performance in the task of distinguishing between benign and malicious domains over a subset of our dataset. We found that best performance is achieved using an ensemble of 100 decision trees with unlimited depth and bootstrap aggregation (i.e., bagging), where each new tree is fitted from a bootstrap sample of the training data (Breiman, 1996).

All our experiments were performed on a system equipped with an NVIDIA TITAN Xp PG611-c00 to speed-up the computations, utilising the scikit-learn8 library. The performance of the trained classifiers is evaluated using the standard classifications metrics of Precision, Recall,  score and the area under the curve (AUC).

In all experiments, the same feature set9 were used and employed standard 10-fold cross-validation to avoid overfitting and get a roughly unbiased estimate of the performance of the trained models. Although the same feature sets for all experiments were used, we optimised weights of the features per DGA family, and thus targeting a binary classification (i.e., per DGA detection). In a binary classification, this is a justified setting since feature weights for a particular DGA family are not expected to vary in time. However, this is opposed to multi-class classification (requiring frequent feature/weight tuning), which we argue is not convenient for DGA detection since it deals with the more challenging classification problem.10 Also, it should be noted that the multiclass classification is not the focus of this work, and those experiments carried out only for the sake of comparison with state-of-the-art approaches.

The set of experiments ran in this work aims to provide a solid proof of performance and accuracy for our approach. First, the detailed outcomes when applied to the HYDRAS dataset are provided. Next, we select two well-known state-of-the-art proposals using a method similar to ours, also based on Random Forest and implementing their own set of features. We compared the performance of such solutions to that of our method by applying them to the HYDRAS dataset.


Table 3. Performance measures for binary classification (in percentage).

Class	Prec.	Recall			AUC	Class	Prec.	Recall			AUC	Class	Prec.	Recall			AUC
bamital	100	100	100	0	100	gspy	99.67	100	99.83	0.29	100	qakbot	100	99.97	99.98	0.01	99.99
banjori	99.99	99.74	99.87	0.01	99.87	hesperbot	100	99.85	99.92	0.01	99.93	qhost	100	100	100	0	100
bedep	100	100	100	0	100	infy	100	99.97	99.98	0.01	99.98	qsnatch	99.77	99.86	99.81	0.02	99.81
beebone	100	99.07	99.53	0.40	99.54	khaos	99.47	96.47	97.95	0.10	97.98	ramdo	100	100	100	0	100
bigviktor	91.07	76.44	83.11	0.32	87.85	kingminer	100	97.62	98.8	<0.01	98.81	ramnit	100	99.95	99.97	0.02	99.98
blackhole	100	99.86	99.93	<0.01	99.93	locky	100	99.79	99.9	0.04	99.9	ranbyus	100	99.99	100	0	100
bobax/
/kraken
/oderoor	100	99.62	99.81	0.01	99.81	madmax	100	99.98	99.99	<0.01	99.99	redyms	100	99.63	99.82	0.32	99.82
ccleaner	100	100	100	0	100	makloader	100	100	100	0	100	rovnix	100	100	100	0	100
chinad	100	100	100	0	100	matsnu	95.73	97.74	96.72	0.2	96.69	shifu	100	99.63	99.82	0.01	99.82
chir	100	100	100	0	100	mirai	100	99.96	99.98	<0.01	99.98	shiotob/
/urlzone
/bebloh	100	99.99	99.99	<0.01	99.99
conficker	99.99	99.64	99.81	0.02	99.82	modpack	100	100	100	0	100	simda	99.99	99.66	99.83	0.01	99.83
corebot	100	99.98	99.99	<0.01	99.99	monerodownloader	100	100	100	0	100	sisron	100	100	100	0	100
cryptolocker	100	99.98	99.99	<0.01	99.99	monerominer	100	100	100	0	100	sphinx	100	100	100	0	100
cryptowall	100	99.87	99.93	0.02	99.94	murofet	100	99.99	100	<0.01	100	suppobox	96.84	98.3	97.57	0.02	97.55
darkshell	100	97.5	98.73	0	98.75	murofetweekly	100	100	100	0	100	sutra	100	99.97	99.98	<0.01	99.98
deception	99.03	97.00	98.00	0.07	98.02	mydoom	100	99.6	99.8	0.01	99.8	symmi	100	93.85	96.83	<0.01	96.92
deception2	98.25	96.15	97.19	0.1	97.22	necurs	100	99.89	99.95	0.02	99.95	szribi	99.98	99.68	99.83	0.03	99.83
diamondfox	100	97.13	98.55	<0.01	98.57	nymaim	100	99.6	99.8	0.03	99.8	tempedreve	100	99.56	99.78	0.02	99.78
dircrypt	100	99.94	99.97	<0.01	99.97	nymaim2	98.35	97.99	98.17	0.07	98.17	tinba	100	99.92	99.96	0.02	99.96
dmsniff	100	95.71	97.81	<0.01	97.86	omexo	100	100	100	0	100	tinynuke	100	100	100	0	100
dnschanger	100	99.93	99.96	0.02	99.97	padcrypt	100	100	100	0	100	tofsee	99.94	99.92	99.93	0.02	99.95
dromedan	100	100	100	0	100	pandabanker	100	100	100	0	100	torpig	100	99.79	99.89	0.01	99.9
dyre	100	100	100	0	100	pitou	100	99.89	99.94	0.02	99.95	tsifiri	100	100	100	0	100
ebury	100	100	100	0	100	pizd	99.43	99.62	99.52	0.09	99.52	ud2	100	100	100	0	100
ekforward	100	100	100	0	100	post	100	100	100	0	100	ud3	100	100	100	0	100
emotet	100	100	100	0	100	proslikefan	100	99.63	99.81	0.04	99.82	ud4	100	96.19	98.06	0.43	98.1
enviserv	100	100	100	0	100	pushdo	99.94	98.99	99.46	0.02	99.46	vawtrak	99.92	99.44	99.68	0.01	99.68
feodo	100	100	100	0	100	pushdotid	100	99.62	99.81	0	99.81	vidro	100	99.78	99.89	0.03	99.89
fobber	100	99.85	99.92	<0.01	99.93	pykspa	100	99.7	99.85	0.01	99.85	vidrotid	100	96.04	97.98	<0.01	98.02
fobber_v1	100	100	100	0	100	pykspa_v1	100	99.28	99.64	0	99.64	virut	99.97	99.99	99.98	<0.01	99.98
fobber_v2	100	99.78	99.89	0.1	99.89	pykspa_v2_fake	100	99.77	99.89	0.01	99.89	volatilecedar	99.93	100	99.97	0.06	100
gameover	100	100	100	0	100	pykspa_v2_real	100	99.77	99.88	0.01	99.88	wd	100	100	100	0	100
geodo	100	100	100	0	100	pykspa2	100	99.12	99.56	0.06	99.56	xshellghost	100	99.93	99.97	0.01	99.97
gozi	95.28	95.93	95.6	0.11	95.59	pykspa2s	100	97.47	98.72	<0.01	98.74	xxhex	100	99.96	99.98	0.02	99.98
goznym	100	99.27	99.63	0.16	99.63	qadars	100	99.98	99.99	0.01	99.99	zloader	100	100	100	0	100
6.1. Binary classification using the HYDRAS dataset
In the current experiment, several binary Random Forest classifiers that correspond to DGA family detectors were cross-validated — each detector is represented by a single such classifier. To build an input sub-dataset of each DGA family detector, random sampling was employed without replacement on AGDs from the corresponding family (or benign samples) to fit a 1:1 ratio with the benign domains, resulting in a balanced sub-dataset. To ensure the statistical significance of the results, each cross-validation execution was repeated 100 times (with different randomly selected sample subsets). In detail, for each DGA family in our dataset we ensured 1:1 ratio with benign domains, with the dataset size per each DGA detector of family  equal to: (1)where  represents samples of Alexa dataset and  represents samples of a particular family  in the HYDRAS dataset . Hence, in the case that the number of samples in  is greater than in , we employ random sampling without replacement (across repeated experiments) on  to reduce its size to the size of . In the opposite case, when the size of  is greater than the size of , the same random sampling is employed to reduce the size of , ensuring 1:1 ratio. Note that for each of the 100 runs of cross-validation, different sample sets were randomly selected from the Alexa dataset as benign class representatives.11 Also, note that due to the particular format in which several families are available in their reverse-engineered form, as well as in the AGD repositories used to initialise our dataset, the  feature could not be used homogeneously, and thus, was excluded from this experiment.

The averaged outcomes of the binary classification can be seen in Table 3. We can observe that most of the DGA families are classified almost without errors, obtaining precision and recall metrics of above 99.9%. Even in the case of families with small representation (e.g., darkshell, omexo, qhost, and ud3), the classifier can discern between benign and malicious domains in almost 100% of cases, with only a few exceptions. Moreover, the standard deviation of the  (i.e., ) achieves values 1% (in most cases only 0.01%), which showcases the robustness of both the classifier and the proposed feature set.

The lowest accuracy was obtained by bigviktor (with a precision of 91.07% and a recall of 76.44%) followed by suppobox, gozi, matsnu, khaos and symmi with  scores ranging between 95% and 98%. This is due to the fact that most of these families use a composition of English dictionary words to create AGDs, so the extracted lexical features are not always able to properly differentiate them from our benign dataset or are adversarial. In the case of such dictionary-based families, a further enhancement based on probabilistic-based methodologies12 is an interesting but challenging future research direction. In the case of adversarially designed DGAs, the accuracy obtained is close to the one reported for the dictionary-based DGAs, which showcases the difficulty of capturing such families. A more detailed comparison and analysis of adversarially designed DGAs is later presented in Section 7. Overall, the outstanding detection rates showed in Table 3 by using the same feature set across such a big dataset proves the robustness and adaptability of our approach. Note that the more divergent families (and samples) are, the more difficult is to select a common set of features which can capture them accurately.

Feature Weighting. For the sake of clarity, the average feature weight is depicted in the binary classification task in A and Table 12. The features exhibiting a high influence on the binary classification are , , , , , and . Therefore, the n-grams, as well as the length of the domain and the valid words it contains, seem to be the most relevant features. Besides that, the relevance of each feature varies according to the family. Feature weights provide some insights into how to try to further enhance the performance of our method, e.g., by employing various feature selection methods to reduce the number of features, which might be convenient in power-constrained devices such as IoT device or smartphones.


Table 4. A comparison with state-of-the art (binary classification).

Choudhary et al. (2018)	Woodbridge et al. (2016)	Our method		Choudhary et al. (2018)	Woodbridge et al. (2016)	Our method		Choudhary et al. (2018)	Woodbridge et al. (2016)	Our method
Class							Class							Class						
bamital	99.99	<0.01	99.99	<0.01	100	0	gspy	99.82	0.32	100	0	99.83	0.29	qakbot	97.93	0.17	99.98	0.02	99.98	0.01
banjori	96.53	0.04	99.75	0.01	99.87	0.01	hesperbot	97.18	0.09	99.91	0.02	99.92	0.01	qhost	98.61	2.41	97.19	1.18	100	0
bedep	99.15	0.03	100	0	100	0	infy	99.65	0.11	99.98	<0.01	99.98	0.01	qsnatch	95.15	0.14	96.38	0.09	99.81	0.02
beebone	97.05	4.52	98.86	0.78	99.53	0.4	khaos	59.68	0.56	85.78	0.18	97.95	0.1	ramdo	99.84	0.02	99.99	<0.01	100	0
bigviktor	80.82	0.93	85.47	0.79	83.11	0.32	kingminer	99.40	0.2	99.93	0.11	98.8	<0.01	ramnit	97.25	0.18	99.95	0.02	99.97	0.02
blackhole	99.82	0.14	99.93	<0.01	99.93	<0.01	locky	95.18	0.14	99.84	0.01	99.99	0.04	ranbyus	99.54	0.02	99.99	<0.01	100	0
bobax/
/kraken
/oderoor	95.18	0.09	99.74	0.03	99.81	0.01	madmax	99.00	0.05	99.99	<0.01	99.99	<0.01	redyms	98.37	0.54	99.46	0.94	99.82	0.32
ccleaner	99.87	0.01	100	0	100	0	makloader	99.94	0.11	99.94	0.11	100	0	rovnix	99.97	0.01	99.99	<0.01	100	0
chinad	99.95	<0.01	99.99	<0.01	100	0	matsnu	85.77	0.37	85.74	0.26	96.72	0.2	shifu	97.15	0.20	99.52	0.05	99.82	0.01
chir	100	0	100	0	100	0	mirai	99.09	0.09	100	0	99.98	<0.01	shiotob/
/urlzone
/bebloh	99.10	0.02	99.99	0.01	99.99	<0.01
conficker	93.02	0.16	99.02	0.08	99.81	0.02	modpack	96.49	1.47	99.92	0.13	100	0	simda	95.22	0.19	95.04	0.28	99.83	0.01
corebot	99.78	0.05	99.99	0.01	99.99	<0.01	monero-
downloader	99.91	0.02	100	0	100	0	sisron	99.86	0.04	100	0	100	0
cryptolocker	98.87	0.02	99.99	0.01	99.99	<0.01	monerominer	99.95	0.01	100	0	100	0	sphinx	99.73	0.02	99.99	0.01	100	0
cryptowall	97.13	0.17	99.96	0.01	99.93	0.02	murofet	99.3	0.06	100	0	100	<0.01	suppobox	79.64	0.42	80.32	0.38	97.57	0.02
darkshell	99.18	0.71	99.59	0.71	98.73	0	murofetweekly	99.99	<0.01	100	0	100	0	sutra	99.73	0.07	99.98	<0.01	99.98	<0.01
deception	64.96	0.52	86.31	0.19	98.00	0.07	mydoom	98.61	0.15	99.80	0.03	99.80	0.01	symmi	90.46	1.05	97.42	0.43	96.83	<0.01
deception2	57.80	0.27	82.14	0.49	97.19	0.10	necurs	96.84	0.09	99.94	0.01	99.95	0.02	szribi	97.64	0.12	99.58	0.04	99.83	0.03
diamondfox	98.60	0.39	99.39	0.21	98.55	<0.01	nymaim	93.17	0.16	98.99	0.12	99.80	0.03	tempedreve	94.68	0.06	99.54	0.05	99.78	0.02
dircrypt	97.65	0.02	99.97	<0.01	99.97	<0.01	nymaim2	77.62	0.45	80.01	0.52	98.17	0.07	tinba	99.23	0.05	99.97	0.01	99.96	0.02
dmsniff	96.24	1.15	100	0	97.81	<0.01	omexo	100	0	99.60	0.70	100	0	tinynuke	99.99	0.01	100	0	100	0
dnschanger	98.64	0.03	99.97	0.01	99.96	0.02	padcrypt	99.82	0.03	99.99	<0.01	100	0	tofsee	99.92	0.03	99.88	0.12	99.93	0.02
dromedan	98.53	0.04	100	0	100	0	pandabanker	99.95	0.01	100	0	100	0	torpig	95.80	0.14	99.08	0.09	99.89	0.01
dyre	99.99	<0.01	100	0	100	0	pitou	99.22	0.01	99.59	0.14	99.94	0.02	tsifiri	100	0	100	0	100	0
ebury	99.83	0.09	100	0	100	0	pizd	88.29	0.10	88.27	0.26	99.52	0.09	ud2	99.97	0.06	100	0	100	0
ekforward	99.28	0.09	99.96	0.01	100	0	post	99.99	0.01	100	0	100	0	ud3	100	0	98.37	1.41	100	0
emotet	99.77	0.04	99.99	0	100	0	proslikefan	94.13	0.14	99.48	0.06	99.81	0.04	ud4	96.40	0.35	100	0	98.06	0.43
enviserv	99.23	0.13	99.93	0.12	100	0	pushdo	92.00	0.14	97.52	0.06	99.46	0.02	vawtrak	90.53	0.24	95.56	0.18	99.68	0.01
feodo	99.46	0.12	100	0	100	0	pushdotid	98.15	0.09	99.80	0.04	99.81	0	vidro	95.77	0.19	99.82	0.03	99.89	0.03
fobber	98.64	0.17	99.96	<0.01	99.92	<0.01	pykspa	94.12	0.24	99.51	0.03	99.85	0.01	vidrotid	93.03	0.97	97.06	<0.01	97.98	<0.01
fobber_v1	99.92	0.08	100	0	100	0	pykspa_v1	93.80	0.51	99.55	0.04	99.64	0	virut	97.02	0.16	97.68	0.03	99.98	<0.01
fobber_v2	98.20	0.54	100	0	99.89	0.10	pykspa_v2_fake	96.41	0.14	99.63	0.02	99.89	0.01	volatilecedar	99.37	0.21	99.83	0.15	99.97	0.06
gameover	99.97	0.02	100	0	100	0	pykspa_v2_real	95.60	0.19	99.46	0.03	99.88	0.01	wd	99.99	<0.01	100	0	100	0
geodo	99.79	0.09	99.96	0.03	100	0	pykspa2	92.63	0.87	99.18	0.03	99.56	0.06	xshellghost	98.63	0.08	99.96	0.01	99.97	0.01
gozi	88.65	0.41	92.45	0.27	95.60	0.11	pykspa2s	92.21	0.29	99.07	0.29	98.72	<0.01	xxhex	99.60	0.03	100	0	99.98	0.02
goznym	92.89	0.77	99.68	0.08	99.63	0.16	qadars	99.70	0.08	99.99	<0.01	99.99	0.01	zloader	99.93	0.01	100	0	100	0
6.2. Binary classification — comparison with state-of-the-art approaches
In this experiment, we compare the quality of our features and classification approach with two well-known approaches from the literature, namely the one proposed by Choudhary et al. (2018) and the method leveraged by  Woodbridge et al. (2016). Therefore, each feature set was implemented according to the corresponding specifications, and applied them to the HYDRAS dataset. Thereafter, the same setup was used than in our binary classification experiment and computed the -score for each method. The outcome of this experiment is reported in Table 4.

As it can be observed, all proposals succeed in capturing most of the families. The statistical features and n-gram-based features used in the different methods can recognise the patterns in the AGDs which belong to classical DGA families (i.e. the old ones which exhibit random-based generation patterns). In the case of more sophisticated families such as rovnix, volatilecedar, beebone, banjori, locky, pushdo, proslikefan, symmi, goznym and pykspa, all methods succeed to differentiate them from benign domains. Nevertheless, the method of Choudhary et al. reported notably worse accuracies for pushdo, proslikefan, banjori, beebone, symmi, pykspa and the variants of pyskpa than the method of Woodbridge et al. and our approach.

In the case of the rest of dictionary-based families, the adversarially-generated DGAs, and other novel DGA families, our method clearly outperforms the other approaches in most cases, with exception of bigviktor, in which the outcomes obtained by all methods are similar. For instance, in the case of simda, vawtrak, gozi and qsnatch, our method obtains a  3% higher than the rest of methods.

In the case of more sophisticated dictionary-based families such as matsnu, nymaim and suppobox, our method outperforms the rest of approaches by approximately 10% in the case of matsnu, and close to 18% in the case of nymaim and suppobox. Finally, the highest difference was observed in the comparison with the adversarially generated DGAs.

In this regard, for the deception DGA, our approach outperforms Choudhary et al. by a 33% and Woodbridge et al. by a 12%. The deception2 family exhibits similar outcomes, yet this time our approach outperforms Choudhary et al. by a 40% and Woodbridge et al. by a 15%. Finally, for the khaos DGA, the differences are close to 38% when comparing to Choudhary et al. and approximately 12% in the case of Woodbridge et al.

The average  measure per family, as well as the standard deviation of the total average, are depicted in Table 5. As it can be observed, since a significant amount of DGA families are detected with high performance by all approaches, the average  outcomes are high in all methods. Nevertheless, the robustness of our methodology is highlighted by the  value, since it translates into a very high detection rate across all families, outperforming the rest of approaches.

The aforementioned comparison supports our idea to use a novel and upgradeable dataset such as HYDRAS, since most of the families present in the datasets used in the state-of-the-art approaches belong either to the random-based DGA category or to the thoroughly analysed set of dictionary-based families with specific patterns. In both cases, such families can be captured with high classification performance by using well-known feature sets. The latter implies that researchers should evaluate their methods with novel more complex families since approaches that test their accuracy with old datasets are no longer proving their validity versus the current DGA landscape.

6.3. Binary classification using other datasets
To compare the quality of our approach when applied to other datasets, two datasets from the recent literature are selected. First, we selected the dataset presented in Anand et al. (2020), which contains 50,600 samples that are split to benign and malicious AGDs in 50:50 ratio (i.e., 25,300 benign samples and 25,300 malicious ones). The authors used several machine learning methods and reported accuracy between 94.9% and 97.0% for the C5.0 algorithm — the one that achieved the highest accuracy. In our case, using the same dataset, we achieved the accuracy of 98.9%, which is between 1.9% and 4% higher than any of the proposed methods in Anand et al. (2020).

The second comparison was done using the dataset created in Selvi et al. (2019). In this case, the authors tested their approach with a dataset consisting of 64,000 samples — similar to the previous case, the samples were split in 50:50 ratio for the benign and malicious classes. The authors of Selvi et al. (2019) used a Random Forest classifier and achieved an accuracy of 98.9% by using their 2-gram setup with 34 features. In our case, the accuracy achieved was 96.7%, that is only 2% below the outcome achieved by the authors. Nevertheless, the main drawback of their approach is the computational time required to compute such n-grams, which grows exponentially. In fact, the authors of Selvi et al. (2019) needed 1.21 h for execution of a complete n-fold experiment with three repetitions. In our case, the same experiment took approximately one minute. The latter emphasises that the trade-off between accuracy and computational time is also an important aspect (see Section 8 for the details).

6.4. Multiclass classification using other datasets
In this experiment, we aim at predicting the DGA families, given a set of AGDs, in a multiclass classification setting. We argue that this experiment has low practical utility in contrast to binary classification, and is provided only due to a fair comparison with related work.

A fair comparison of the multiclass classification’s performance cannot be made if different approaches use different datasets. Although the best option to compare a performance of related work vs. our approach would be to use the HYDRAS dataset, several problems arise: (1) many implementations of feature extractors are unavailable, (2) classifiers need to be fine-tuned and details of parameters are often not presented in papers. Intuitively, due to a large number of DGA families contained in the HYDRAS dataset, the multiclass classification using HYDRAS dataset yields worse outcomes than in small datasets. This is the common problem of multiclass classification, especially when the classes are not well separated (Silva-Palacios et al., 2017).

Therefore, this experiment is based on another dataset introduced in Bader (2020), which was already evaluated by several papers. For example, this dataset was evaluated by Alaeiyan et al. (2020). We further extend the comparison with the other two approaches reported in the literature, namely DeepDGA (Anderson et al., 2016) and Phoenix (Schiavoni et al., 2014). We performed a multiclass classification using our Random Forest classifier and our proposed feature set, this time slightly changing its configuration from the one used for the binary classification (i.e., an ensemble of 200 trees with unlimited depth). The repository of this dataset no longer contained samples of the RunForestrun family, so it was not included in the comparison. Moreover, the Tinba family had repeated SLD entries, which would lead to a biased and possibly unrepresentative classification (i.e., the same samples could easily end up both in the training and testing sets, thus reporting a 100% detection in most of the validations partly due to this fact). Therefore, a subset of the Tinba samples that are presented in our dataset was used. It is worth to note that this issue was ignored or not reported by the rest of approaches using this dataset.


Table 5. Average outcomes per DGA class in the binary classification comparison.

Choudhary et al. (2018)	Woodbridge et al. (2016)	Our method
Average 	96.019	98.323	99.454
7.424	4.289	1.810

Table 6. Multiclass classification outcomes in percentages, using different performance metrics. The averages are weighted according to the number of samples in each family.

Phoenix (Schiavoni et al., 2014)	DeepDGA (Anderson et al., 2016)	Alaeiyan et al. (2020)	Our method
Class	Precision	Recall		Precision	Recall		Precision	Recall		Precision	Recall	
banjori	100	100	100	100	100	100	100	100	100	99.80	100	99.90
chinad	74.2	84.1	78.84	98.8	96.3	97.53	92.3	19.5	32.20	87.99	97.27	92.39
corebot	88.4	97.4	92.68	100	100	100	100	97.4	98.68	100	72.50	84.06
dircrypt	0	0	0	0	0	0	0	0	0	0	0	0
downloader	100	100	100	100	100	100	100	100	100	100	100	100
dnschanger	0	0	0	0	0	0	0	0	0	0	0	0
fobber	3.2	2	2.46	3.1	7.6	4.40	0	0	0	37.62	12.67	18.95
gozi	7.1	50.0	12.43	3.0	3.0	3.00	0	0	0	0	0	0
javascript	2.0	3.4	2.52	0	0	0	0	0	0	0	0	0
locky	0	0	0	0	0	0	0	0	0	0	0	0
murofet	99.9	98.0	98.94	100	98.0	98.99	99.7	99.4	99.55	99.91	99.79	99.85
necurs	22.2	15.8	18.46	28.0	32.7	30.17	30.2	5.1	8.73	32.24	8.15	13.02
newgoz	97.2	94.1	95.62	100	99.6	99.80	98	89.9	93.78	99.40	100	99.70
kraken	88.1	52.3	65.64	90.7	60.3	72.44	98.5	90	94.06	99.30	99.93	99.61
padcrypt	79.3	100	88.46	88.5	100	93.90	100	65.2	78.93	87.50	29.17	43.75
proslikefan	3.0	19.4	5.20	4.0	23.5	6.84	0	0	0.00	25.00	2.00	3.70
pykspa	85.2	61.4	71.37	89.1	80.3	84.47	84.7	99.4	91.46	83.50	86.86	85.15
qadars	60.4	81.1	69.24	85.2	84.7	84.95	96.9	16.3	27.91	69.70	34.50	46.15
qakbot	53.5	55.0	54.24	57.9	56.6	57.24	54.5	82.1	65.51	65.69	76.70	70.77
ramnit	1.2	3.3	1.76	0	0	0	0	0	0	0	0	0
ranbyus	2.1	6.5	3.17	0	0	0	0	0	0	0	0	0
shiotob	96.7	81.6	88.51	98.3	89.8	93.86	84.7	91.4	87.92	92.26	90.00	91.12
simda	63.0	99.0	77.00	98.3	89.8	93.86	89.6	100	94.51	91.33	99.00	95.01
sisron	100	100	100	100	100	100	100	100	100	100	100	100
suppobox	32.4	79.3	46.00	67.4	74.6	70.82	97.3	68.5	80.40	90.61	98.43	94.36
symmi	98.3	96.6	97.44	98.3	100	99.14	98.3	100	99.14	92.31	56.25	69.90
tempedreve	27.6	67.1	39.11	43.8	96.3	60.21	57.2	75.1	64.94	59.98	71.94	65.42
tinba	25.3	64.6	36.36	49.9	98.2	66.17	100	99.7	99.85	99.52	99.94	99.73
vawtrak	30.9	9.7	14.77	68.3	87.5	76.72	100	8.3	15.33	69.47	66.00	67.69
Total	93.25	90.46	91.40	94.64	92.49	93.28	94.49	95.20	94.41	95.39	95.49	95.25
The outcomes of the multiclass classification are depicted in Table 6. Even though in some cases Phoenix and DeepDGA showed better performance (e.g., for Padcrypt, Qadars, and Symmi), our method outperformed the rest, both in accuracy and recall, followed by the method proposed in Alaeiyan et al. (2020). Furthermore, we observed that there were relatively small differences for most families, and for the most part all methods reported high accuracy for similar families. In some cases, the reported performance metrics were equal to zero, which is related to the lack of samples. The latter means that the classifier could not be properly trained.

We can observe in Table 12 of Appendix that feature relevance in the multiclass setting differs substantially from the reported in the binary classification. In other words, the relevance of the features will be different according to the particularities of the collected samples, highlighting the importance of using the same dataset to avoid biased comparisons across different approaches. In the particular case of the dataset selected to perform the multiclass classification, the features , , , , and  were the most relevant. The latter means that the length (i.e., specific families create only AGDs of fixed length), the number of digits, the meaningful words in the SLD, and the entropy of the SLD enabled to predict which specific family created a given AGD, with a high precision.

7. Classification of adversarially designed AGDs
To further assess the quality of our selected features, we opted to use it against three especially “hard to detect” DGAs. These DGAs, deception, deception2 (Spooren et al., 2019), and khaos (Yun et al., 2020) are specially crafted, using machine learning methods, to evade detection (see Section 2). While our features are generic and not targeted towards identifying any particular set of these families, we also manage to detect adversarially designed AGDs with significantly better performance than in previous works (see Table 8). In detail, the precision achieved by our approach is by 15% to 30% better. Similarly, the recall and F1 score are by more than 10% better in almost all cases. It may also be observed that in some cases, the detection rates slightly vary if the ratio of malicious to benign samples is increased. This fact will be discussed more thoroughly in Section 9. Nonetheless, the F1 score is at least 92.48%, which indicates that our method is very effective even when confronted against specially crafted DGAs — a challenge that is very close to represent the most challenging scenario.

7.1. Invalid domains
It should be noted that, during our study, many invalid domains generated by these DGAs were identified. To the best of our understanding, the researchers simply left the neural networks to generate AGDs that bypassed the filters, without double-checking their validity in real scenarios. As a result, the neural networks identified that the use of the hyphen character managed to bypass some filters, priming them to overuse it. Therefore, one can observe thousands of domains which either start or finish with a hyphen, which are unfortunately invalid according to RFC 1123 (Braden, 1989). Similarly, many of them do not conform to RFC 5891 for Internationalised Domain Names (IDNs) (Klensin, 2010), by having hyphens as third and fourth characters, but not starting with an “xn”, so they are rejected by ICANN.13 The issue is particularly relevant in the DGAs generated by Spooren et al. (2019) spanning across 1.64% of the samples. In the khaos family, the dataset contains 19 IDNs. Note that in all following experiments, only DGAs which do not produce IDNs are considered.

7.2. Detection of unknown families
We performed another experiment to test the capability of our approach to detect previously unknown DGA families. In this regard, a leave-one-out experiment were leveraged with the same configuration as in the binary classification experiments (i.e., 10-fold cross validation) but in contract to it the target family was completely hidden to the training phase. In other words, we tried to predict whether a set of AGDs is benign or malicious without previous knowledge of the DGA family generating them. The outcome of this experiment is reported in Table 7. As it can be observed in the table, our approach is able to correctly classify most of the samples, achieving a slightly lower -score than the one reported in our binary classification (see Table 8), due to a general decrease of the recall values. Note that the most affected family is deception2, yet our approach still outperforms the original works in which these families were proposed, thus showcasing the robustness of our features once again.


Table 7. A detection of unknown DGA families represented by adversarially designed DGAs (leave-one-out experiment).

DGA	Precision	Recall	F1
khaos	100	85.40	92.12
deception	99.99	84.71	91.72
deception2	99.99	73.08	84.44

Table 8. Binary classification against adversarially designed DGAs. First row of each family denotes the reported results in the original work.

DGA	Method	Precision	Recall	F1
khaos	Yun et al. (2020)	68.00	98.00	80.30
Our approach — ratio 1:1	99.47	96.47	97.95
Our approach — ratio 1:10	96.08	90.73	93.32
Our approach — ratio 1:100	96.55	89.63	92.96
deception	Spooren et al. (2019)	84.40	87.10	85.72
Our approach — ratio 1:1	99.03	97.00	98.00
Our approach — ratio 1:10	96.21	93.86	95.02
Our approach — ratio 1:100	96.12	93.29	94.68
deception2	Spooren et al. (2019)	77.50	81.50	79.45
Our approach — ratio 1:1	98.25	96.15	97.19
Our approach — ratio 1:10	94.44	91.29	92.84
Our approach — ratio 1:100	94.56	90.50	92.48
8. Overhead of our approach
We measured several statistics during our experiments with the intention to demonstrate the practical aspects of our approach. We measured the duration of the features computation and the prediction time in both the binary and multiclass setting with our dataset. The average time required to compute all the features for an SLD is 1.48 ms, while the prediction times from both classification experiments are depicted in Fig. 4. Note that the figure does not include the training time, which scales linearly with the size of the dataset. However, training is an action that is very rare (e.g., repeated after weeks or months) and it can be performed offline. Hence, it does not incur performance degradation to the operation of AGD detector. Finally, both the features computation time and the prediction times are measured without any parallelisation to enable a fair comparison with related work.

8.1. Binary classification
In terms of performance comparison for the binary classification, the work proposed in Anand et al. (2020) did not report any computational cost nor performance metrics. In the case of Selvi et al. (2019), the authors reported a total experiment time of 1.21h in their 2-gram setup with 34 features, which is the one that yielded the highest accuracy. In our case, the time required for the same experiment, including the n-fold validation, is between 10–20 s, and close to one minute including the feature computation of all the SLDs of the dataset, without considering parallelisation. Therefore, the trade-off between accuracy (which in our case is just 2% lower) and computational time (i.e. two orders of magnitude faster) of our method is clearly outperforming the method presented in Selvi et al. (2019).

8.2. Multiclass classification
Considering related works that have studied the multiclass classification (see Table 6), a fine-grained comparison of processing performance was not possible since the achieved performance was not systematically reported and in some cases not stated at all. Moreover, such performance highly varies depending on the exact hardware configuration, and thus, the exact replication of each of the environments used is challenging. For instance, the authors of Schiavoni et al. (2014) report only that experiments required time in the order of minutes. In the case of Anderson et al. (2016), authors stated that their model required expensive training periods of 14 h (considering 300 epochs and Alexa subsampling), as well as a classification time of 7 min for approximately 13k samples (i.e. 0.03 s per sample) in a GPU-based setup. In work presented in Alaeiyan et al. (2020), the authors reported a feature computation time of around 217 min for 252,757 samples, which implies approximately 0.05 s per sample. Moreover, the authors also specified a classification time of around 60 min, which translates into 0.014 s on average to classify a sample.

In sum, our approach outperformed related work in the multiclass experiment in time requirements by one order of magnitude for both the feature computation as well as in the prediction. This means that our method is suitable for real-time AGD detection and classification, even in environments with very high traffic volumes.

9. Discussion
In this section, we focus on a quantitative comparison of the most relevant DGA detectors of the related work, and we analyse the existing limitations in the literature.

9.1. Quantitative comparison
The DGA research field has several open challenges; one of them is the continuous appearance of new families. In contrast to other research fields that rely on standardised benchmarks (e.g., computer vision), DGA-based datasets need to be updated frequently to be able to prove the performance of the detection methods is of relevance in real scenarios. Recently, Zago et al. (2020) created a balanced and structured dataset containing 38 families. Nevertheless, although their approach is sound, it does not keep pace with the recent evolution of malware campaigns. Solving this issue is hard, and although there are research efforts in adversarial classification (see Section 2), we argue that further research should focus on upgradeable versions of datasets and include version tracking. In our dataset, we introduce a collection of 105 families, which better reflects the complexity and challenge of the current landscape. Moreover, it is worth to note that obtaining enough samples of specific families can be a cumbersome and difficult task due to a number of reasons, such as the inner algorithmic structure of the DGA14 or the fact that it has not been reverse engineered yet. Therefore, we argue that obtaining a perfectly balanced dataset that contains all possible DGA families is extremely difficult, borderline impossible. In this regard, it should be stressed that due to the heterogeneity and non-replicability of some datasets as well as the range of different techniques applied in related work, a perfect and fair comparison is unfeasible.

As it can be observed in Table 9, the results of our work were achieved using the biggest dataset in terms of the number of DGA families (and samples). This compares favourably and leads to a much more challenging classification task when compared to related work. Due to the continuous evolution of malware, a high number of supported DGA families (including the most recent ones) is a critical capability required of any successful DGA detector that can be deployed in real scenarios. Note that data sources of related works in the table share some common repositories such as Bambenek, DGArchive, and NetLab 360. However, only a few authors used reverse-engineered DGAs to populate their datasets, as seen in Table 9 and described in Section 4. With regard to the detection method used, LSTM is the most prevalent, followed by ML classifiers, from which RF is known to report the best classification performance. Regarding the features, both lexical (e.g., ratios of letters, n-grams, words) and entropy-based ones seem to occur most widely.


Table 9. A quantitative comparison of our work with the most relevant state-of-the-art approaches.

Ref.	Features	# DGAs	AGD samples	Method	Dataset source
Anderson et al. (2016)	Lexical, entropy	10	110,000	GAN, LSTM, RF	Manually crafted
Schiavoni et al. (2014)	Lexical	5	1,153,516	DBSCAN	SIE framework, Exposure blacklist and other public implementations
Curtin et al. (2019)	WHOIS, lexical, smashword score	41	1,280,000	RNN	DGArchive and Several GitHub repositories
Koh and Rhodes (2018)	Lexical	4	4,000	ELMo	Several GitHub repositories
Yu et al. (2017)	Time-based, query response, domain name	19	4,739,563	LSTM/CNN	Farsight security/DGArchive
Tran et al. (2018)	Domain name	37	169,831	LSTM	Bambenek
Lison and Mavroeidis (2017)	Domain name	58	2,900,000	RNN	DGArchive, Bambenek
Yu et al. (2018)	Lexical	N/A	1,000,000	CCN/RNN	Bambenek
Mac et al. (2017)	Entropy, lexical	37	81,490	Several ML methods	Bambenek
Choudhary et al. (2018)	Lexical	19	34,264,306	Random Forest and DNN	Bambenek, DGArchive
Li et al. (2019)	Lexical, query response	5	160,000	Several ML methods	Bambenek
Jyothsna et al. (2018)	Lexical	19	245,872	DNN	Bambenek, Netlab 360
Chen et al. (2018)	Domain name	60	1,687,806	LSTM	Bambenek, Netlab 360
Sivaguru et al. (2018)	Time-based and domain name	15	551,086	Several Binary Classificators	Real traffic, Bambenek
Attardi and Sartiano (2018)	Domain name	19	135,056	LSTM, BLSTM	Bambenek and Netlab 360
Zago et al. (2019)	Entropy, lexical, similarity	17	16,000	Several ML methods	Netlab 360, DGArchive, DNS-BH
Bharathi and Bhuvana (2019)	Domain name	19	245,872	LSTM, BLSTM	Bambenek, Netlab 360
Khehra and Sofat (2018)	Entropy, lexical	5	272,209	CNN/RNN	Stratosphere dataset (Stratosphere Labs, 2020)
Zago et al. (2020)	Lexical	38	30,799,449	Several ML methods	UMUDGA
Anand et al. (2020)	Lexical	19	25,300	Several ML methods	Netlab 360
Yang et al. (2020)	Domain name	20	100,000	BLSTM, HDNN	Fu et al. (2017)
Alaeiyan et al. (2020)	Lexical, pronounceability	30	252,757	Genetic algorithm and RF	Bader (2020)
Almashhadani et al. (2020)	Entropy, randomness, lexical	20	208,190	Several ML methods	DGArchive, Bambenek
Selvi et al. (2019)	Entropy, lexical	26	252,757	RF	Bader (2020)
Our approach	Entropy, lexical, gibberish	105	95,325,598	RF	HYDRA Dataset (Casino et al., 2020)
The methods that use side-information (e.g., WHOIS, timing, etc.) cannot prevent compromised hosts from contacting the C&C server, and thus bring an additional cost in terms of time which makes them prohibitively slow for real-time detection and incident response. Undeniably, caching and whitelisting can significantly reduce such a cost; however, this is expected to occur every time the host has to connect with a new domain or a DGA has a new seed, which is unrealistic in most scenarios.

9.2. Fair comparison and evaluation of reproducibility
As previously reported in Section 6, the comparison of any two approaches should be made under the same contextual settings (i.e., benchmarks and performance metrics); otherwise, the interpretation of the results might be biased and unduly favour an approach with less challenging settings. To analyse the quality and the methodologies used in related research, we reviewed the works from Table 9 in terms of reproducibility and presentation of the outcomes. In detail, we verified whether the authors explicitly reported their evaluation methodology, their dataset collection procedure (for reproducibility purposes), and sufficient details about their outcomes (for a fair comparison). The results of this effort are shown in Table 10, where we observe that there are some serious methodological issues mainly due to the lack of experimental setup description, biased performance measures (e.g not reporting widely used metrics to enable fair comparison) being used and/or extremely imbalanced datasets (e.g. not enough samples for unbiased training), and the aggregation of classification results by averaging, while not reporting information about some poorly performing families. Note that Table 10 is not intended to criticise the related works, on the contrary, it aims to establishing a common ground to improve the transparency and contributions of the literature.

Moreover, when the reported results are aggregated (i.e. the outcomes are not reported per class but as an overall aggregate), the unbalanced nature of the dataset, as well as the fact of hiding the classification performance per family hinders the objective interpretation of the results (e.g., a very small sample set could not reflect the characteristics of a family, and specific sampling ratios of benign to malicious domains might result in statistically biased outcomes). This, in turn, translates into approaches that might obtain highly accurate results for some families, while they are unable to detect other families; however, the occurrence of this phenomenon cannot be discerned from the reported results, and thus a fair comparison cannot be made.


Table 10. Methodological limitations of related work.

Methodological limitations	References
Not reported samples, extremely imbalanced benchmarks or lack of robust performance measures	Yu et al., 2017, Curtin et al., 2019, Tran et al., 2018, Lison and Mavroeidis, 2017, Yu et al., 2018, Mac et al., 2017, Chen et al., 2018, Anand et al., 2020, Chen et al., 2018, Zago et al., 2019, Khehra and Sofat, 2018
Aggregated classification outcomes	Tran et al., 2018, Lison and Mavroeidis, 2017, Yu et al., 2018, Choudhary et al., 2018, Jyothsna et al., 2018, Chen et al., 2018, Chen et al., 2018, Sivaguru et al., 2018, Anand et al., 2020, Attardi and Sartiano, 2018, Zago et al., 2019, Bharathi and Bhuvana, 2019, Khehra and Sofat, 2018, Zago et al., 2020, Alaeiyan et al., 2020, Almashhadani et al., 2020, Yang et al., 2020, Selvi et al., 2019
The same benchmark settings in the case of multiclass classification are even more critical to allow for a fair comparison since the more families used the more difficult is to classify them, especially taking into account their random nature. Moreover, since several DGAs can create the same pattern, the more samples collected the more possibilities of overlapping the domain names (Alaeiyan et al., 2020, Zago et al., 2020, Patsakis and Casino, 2021, Mac et al., 2017), which increases chances for misclassification thus making the problem more challenging. Nevertheless, in the case of binary classification with a statistically sound methodology, highly accurate detection of underrepresented families indicates the robustness of the selected features, thus showcasing the high performance of the detection method even in extreme cases.

9.3. Sound evaluation methodology
We argue that an objective comparison of the results among different approaches is possible only through a sound evaluation methodology. To do so, it is imperative to include the reporting of the performance over data with the same ratio of AGD samples to benign domains. Moreover, such experiments should be repeated several times with different sample sets (e.g., 100 times in our case and using 10-fold cross-validation in each iteration), since methodologies using a single run of n-fold cross-validation only shuffles samples within the selected sample set. For instance, one could perform a 1:1 ratio classification between Alexa and a malicious family with 50 samples. In this setup, which is frequently adopted by ML practitioners in this field, only 50 samples of Alexa would be selected and shuffled in the cross-validation. Therefore, the rest of the Alexa samples will not be used unless the experiments are repeated with different samples to produce a statistically sound outcome. In this regard, repeating the experiments and selecting a different set of samples in each iteration provides a better representation of the characteristics of each family and hence a much more realistic accuracy. In this setup, low values of the standard deviation in the results indicate the desirable stability and robustness of the method. Unfortunately, this methodology is rarely adopted in the field.

9.4. Ratios of malicious to benign samples
The well-known imbalance problem (Liu et al., 2009) argues that there are much less malicious events than benign ones when performing, e.g., traffic analysis and intrusion detection in real scenarios (Shabtai et al., 2012, Wang et al., 2020, Liu et al., 2018). Nevertheless, in the area of DGA analysis, there is no wide consensus on the common ratio of benign to malicious domains that one can commonly find in real-world settings. This is due to some DGAs generating only a few domains per day, while others might create them in the hundreds or thousands. Therefore, we made a conscious effort to evaluate the performance of our approach under malicious to benign ratios different than 1:1, and hence we explored ratios of 1:10 and 1:100 as well. During our experiments, if a malicious family had more samples than the size of the benign dataset, these were randomly under-sampled, to obtain the desired ratios. Table 11 shows the results obtained by using the  measure and its standard deviation across 100 repetitions of the 10-fold cross-validation, selecting different samples in each iteration.

When we compare the outcomes obtained across all ratios (i.e. 1:1, 1:10, and 1:100), we can observe that dictionary-based and adversarial families obtain slightly worse accuracy when the malicious to benign ratio is increased. The latter occurs because these DGAs create domains that have structural similarities with Alexa domains, which increases the difficulty of the classification task due to the overlapping features. Nevertheless, as it can be observed in the rest of cases, the variance of the outcomes according to each sampling ratios is minimal, which denotes stable results. This means that our approach and its features represent homogeneously benign domains, and thus, they are able to accurately distinguish them from malicious ones, regardless of the sample ratio. This showcases the quality of the feature selection as well as the statistical confidence of the classification.


Table 11. Binary classification outcomes with different ratios of malicious to benign domains (we always assume a higher number of benign domains in the ratios).

Ratio 1:10	Ratio 1:100		Ratio 1:10	Ratio 1:100		Ratio 1:10	Ratio 1:100
Class					Class					Class				
bamital	99.98	0.03	99.97	0.03	gspy	100	0	100	0	qakbot	99.88	0.03	99.87	0.03
banjori	99.60	0.13	99.51	0.18	hesperbot	99.90	0.05	99.90	0.05	qhost	100	0	100	0
bedep	99.93	0.03	99.93	0.03	infy	99.92	0.03	99.93	0.08	qsnatch	99.07	0.36	98.82	0.14
beebone	99.53	0.40	99.53	0.40	khaos	93.33	0.69	92.96	0.83	ramdo	99.98	0.03	99.98	0.03
bigviktor	93.56	0.58	93.26	0.28	kingminer	98.80	<0.01	98.80	<0.01	ramnit	99.90	0.05	99.93	0.03
blackhole	100	0	99.98	0.04	locky	99.63	0.06	99.77	0.12	ranbyus	99.93	0.06	99.95	0.05
bobax/
/kraken
/oderoor	99.82	0.03	99.68	0.12	madmax	99.90	0.05	99.93	0.06	redyms	100	0	100	0
ccleaner	100	0	99.98	0.03	makloader	99.94	0.11	100	0	rovnix	100	0	99.98	0.03
chinad	99.97	0.03	100	0	matsnu	91.86	0.67	91.58	0.46	shifu	99.68	0.10	99.70	0.05
chir	100	0	100	0	mirai	99.95	0.05	99.90	0.05	shiotob/
/urlzone
/bebloh	99.95	0.05	99.98	0.03
conficker	99.45	0.05	99.23	0.13	modpack	100	0	100	0	simda	99.03	0.10	99.09	0.14
corebot	99.97	0.03	99.97	0.03	monerodownloader	100	0	100	0	sisron	100	0	100	0
cryptolocker	99.95	0.05	99.95	0.05	monerominer	100	0	100	0	sphinx	100	0	99.97	0.06
cryptowall	99.87	0.06	99.95	0.05	murofet	100	0	99.95	<0.01	suppobox	92.39	0.37	91.92	0.59
darkshell	100	0	99.58	0.73	murofetweekly	100	0	100	0	sutra	99.95	0.05	99.97	0.06
deception	95.02	0.29	94.69	0.44	mydoom	99.73	0.03	99.75	0.05	symmi	96.55	0.48	96.55	0.48
deception2	92.85	0.86	92.49	0.46	necurs	99.85	0.09	99.87	0.03	szribi	99.70	0.09	99.60	0.17
diamondfox	98.61	0.10	98.67	0.10	nymaim	99.46	0.12	99.38	0.10	tempedreve	99.62	0.06	99.7	0.10
dircrypt	99.85	<0.01	99.92	0.03	nymaim2	94.53	0.58	94.37	0.41	tinba	99.92	0.03	99.95	0.05
dmsniff	98.06	0.43	98.32	0.39	omexo	99.55	0.69	99.60	0.70	tinynuke	100	0	100	0
dnschanger	99.95	0.05	99.88	0.03	padcrypt	99.98	0.03	100	0	tofsee	99.90	0.05	99.80	0
dromedan	99.9	0.10	99.93	0.03	pandabanker	99.98	0.03	99.97	0.06	torpig	99.48	0.13	99.62	0.03
dyre	100	0	100	0	pitou	99.87	0.03	99.80	0.13	tsifiri	99.16	0.83	99.72	0.49
ebury	99.98	0.03	100	0	pizd	96.90	0.09	96.61	0.25	ud2	100	0	99.97	0.06
ekforward	99.92	0.03	99.93	0.03	post	100	0	100	0	ud3	100	0	100	0
emotet	99.98	0.03	99.98	0.03	proslikefan	99.75	0.09	99.51	0.15	ud4	98.30	0.43	98.30	0.43
enviserv	99.97	0.06	100	0	pushdo	98.46	0.06	98.28	0.16	vawtrak	98.77	0.19	98.54	0.36
feodo	99.93	0.12	99.93	0.12	pushdotid	99.72	0.08	99.73	0.06	vidro	99.83	0.08	99.88	0.03
fobber	99.93	0.03	99.88	0.03	pykspa	99.67	0.08	99.67	0.18	vidrotid	97.82	0.28	97.98	<0.01
fobber_v1	100	0	100	0	pykspa_v1	99.58	0.03	99.63	0.08	virut	99.80	0.09	99.88	0.03
fobber_v2	100	0	100	0	pykspa_v2_fake	99.65	0.05	99.51	0.08	volatilecedar	99.90	0.10	99.93	0.12
gameover	100	0	99.98	0.03	pykspa_v2_real	99.55	0.09	99.58	0.03	wd	100	0	100	0
geodo	100	0	100	0	pykspa2	99.52	0.04	99.56	<0.01	xshellghost	99.93	0.03	99.85	0.05
gozi	93.04	0.19	92.68	0.52	pykspa2s	98.29	0.15	98.20	<0.01	xxhex	100	0	99.95	0.09
goznym	99.54	0.08	99.49	0.08	qadars	99.93	0.03	99.93	0.03	zloader	100	0	100	0
A proper methodology should also be considered when using automated approaches for machine learning, such as H2O,15 auto-sklearn,16 AutoKeras17 etc. Such libraries may hyper-optimise parameters for many methods and generate a model which maximises, for instance, the  score. We argue that this unique win should not be considered as the best method since, as discussed above, this solution has to be weighed along with the efficiency of the rest of the models over the same family, and considering several repetitions, only in this way providing statistical soundness.

10. Conclusion
Nowadays, modern malware has evolved into highly sophisticated software, which can be used to infect millions of devices. This enables hard-to-detect and resilient malware campaigns, which have turned cybercrime into a profitable “business”. To enable faster and more accurate botnet detection, and to speed-up take-down operations, a new DGA detection method using machine learning is presented. In essence, our method stands out from the rest in terms of accuracy and performance because we use more comprehensive features and a broader and more representative dataset. We only identified a case in which our outcomes were slightly below these obtained by other methods, yet the time required was between one and two orders of magnitude lower in our case. The relevance of our features is manifested in three ways. First, it achieves an almost optimal detection rate in the binary classification problem for the broadest possible set of DGA families. Second, our features allow us to outperform the current state-of-the-art also in multiclass classification, using the same datasets presented in other works. Finally, our approach was able to detect adversarially designed DGAS, including the experiments in which our system was not trained to detect such families (i.e. assuming no previous knowledge).

Additionally, our methodology is more rigorous than most seen in the field to date, avoiding common pitfalls in the literature that focus on DGAs with many non-obvious constraints. Setting aside feature extraction, our work highlights the inherent biases of datasets and methodologies in previous literature that report many close to perfect results; however, these results may be true for only a very limited and unrepresentative number of DGA families. Notably, we stress the methodological errors in the use of machine learning with, e.g., the use of very few samples and in some cases aggregated classification outcomes preventing a clear comparison. In this regard, a dataset with more than 95 million AGDs is constructed and shared, providing the extracted features to the community. While this facilitates the reproducibility of our results, we also allow fellow researchers to use a significantly richer baseline dataset, both in terms of number of families and samples.

In future work, we aim to enhance our semantic classification by using other training sources in order to increase the accuracy of both English and non-English domain names. Moreover, we will explore wordlist-based DGA detection in more depth by using probabilistic approaches based on word repetition and similar features. Finally, we will study the impact of dimensionality reduction techniques in our dataset.