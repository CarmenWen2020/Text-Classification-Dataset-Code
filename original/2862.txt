Similarity-preserving hashing is a core technique for fast similarity searches, and it randomly maps data points in a metric space to strings of discrete symbols (i.e., sketches) in the Hamming space. While traditional hashing techniques produce binary sketches, recent ones produce integer sketches for preserving various similarity measures. However, most similarity search methods are designed for binary sketches and inefficient for integer sketches. Moreover, most methods are either inapplicable or inefficient for dynamic datasets, although modern real-world datasets are updated over time. We propose dynamic filter trie (DyFT), a dynamic similarity search method for both binary and integer sketches. An extensive experimental analysis using large real-world datasets shows that DyFT performs superiorly with respect to scalability, time performance, and memory efficiency. For example, on a huge dataset of 216 million data points, DyFT performs a similarity search 6000 times faster than a state-of-the-art method while reducing to one-thirteenth in memory.

Access provided by University of Auckland Library

Introduction
Similarity search of vectorial data in databases has been a fundamental task in recent data analysis and has various applications such as near-duplicate detection in a collection of web pages [19], context-based retrieval in images [35], and functional analysis of molecules [20]. In recent years, databases for these applications have become larger and the vectorial data of these databases have also become dimensionally higher, making it difficult to apply existing similarity search methods to such large databases. Therefore, it is necessary to develop much more powerful similarity search methods to analyze large databases efficiently.

Similarity-preserving hashing is a powerful technique that approximates a similarity measure by randomly mapping data points in a metric space to strings of discrete symbols (i.e., sketches) in the Hamming space. Similarity search problems for various similarity measures can be approximately solved as the Hamming distance problem for sketches (i.e., computations of the number of positions at which the corresponding integers between two sketches are different). Thus far, many hashing techniques producing binary sketches have been developed as reviewed in [8]; accordingly, quite a few similarity search methods especially for binary sketches have been proposed for decades (e.g., [14, 16, 33, 34]). In recent years, many types of hashing techniques intending to produce integer sketches have been developed for various similarity measures. Examples are b-bit minwise hashing for Jaccard similarity [28], 0-bit consistent weighted sampling (CWS) for min–max kernels [26], 0-bit CWS for generalized min–max kernels [27], and locality-sensitive hashing for curves [13]. There is a strong need to develop efficient solutions for the general Hamming distance problem for not only binary sketches but also integer sketches; however, few similarity search methods designed on the general problem have been proposed [21, 23, 42].

In real data analysis situations, data are generated at certain time intervals. Accordingly, similarity searches for such data need a dynamic update of data structures, where data structures for similarity searches are updated for inserting/deleting a data into/from data structures without rebuilding those data structures from scratch. Similarity search with dynamic updates is required in many real-world applications such as image [14, 38], social media [3, 36], transaction databases [11], and spatial data [29, 39].

While quite a few numbers of similarity searches over sketches have been proposed thus far, they have several drawbacks in that (i) they do not support dynamic updates of data structures [16, 21, 23, 34] or (ii) they scale poorly [33, 42] or (iii) their applicability is limited to binary sketches [14]. Consequently, an important open challenge is to develop a fast, scalable, and dynamic similarity search over integer sketches.

Our contributions
In this paper, we propose dynamic filter trie (DyFT), a dynamic similarity search method for both binary and integer sketches. DyFT performs similarity searches by traversing an edge-labeled tree called trie [15]. Trie is a dynamic data structure for storing a set of strings and enables its dynamic updates for inserting/deleting a string into/from the trie. A standard implementation of trie with pointers consumes a large amount of memory for storing a large set of strings. Although a number of fast and space-efficient tries have been developed thus far, they are designed for exact string searches. Thus, similarity searches over those tries are not necessarily fast and space-efficient. We present a new cost model for similarity searches that optimizes the trie structure enabling fast similarity searches from the number of sketches and the maximum integer among sketches. DyFT determines its structure based on the cost model and performs similarity searches over the optimized trie, achieving high search performance and space-efficiency.

Sketches are strings of nonnegative integers. On the other hand, researches on trie so far have been conducted for processing strings of byte integers in text processing. Such a representation gap between sketches in similarity search and strings in text processing generates difficulty for implementing DyFT. We present modified adaptive radix tree (MART), a new efficient implementation of DyFT for sketches of nonnegative integers, which enables to change data structures adaptively depending on the configuration of DyFT nodes.

We perform extensive experiments and show DyFT performs superiorly compared to state-of-the-art similarity search methods for both binary and integer sketches concerning scalability, time performance, and memory efficiency. For example, on a huge dataset of 216 million sketches, DyFT performs a similarity search 6000 times faster than HWT, while reducing to one-thirteenth in memory.

This paper is a full version of [22] that includes details of (i) our method and experimental results, (ii) search algorithms on tries, and (iii) implementations of DyFT.

Organization of this paper
In Sect. 2, we formulate the similarity search problem. In Sect. 3, we review state-of-the-art similarity search methods. In Sect. 4, we introduce trie-based similarity search and set up the implementation issues to be addressed. In Sects. 5 and 6, we present our methods DyFT and MART, respectively. In Sect. 7, we provide an an extensive experimental analysis. In Sect. 8, we conclude the paper.

Problem statement
Sketch x of length m is an m-dimensional vector of nonnegative integers from alphabet Σ={0,1,…,σ−1} of alphabet size σ, i.e., x∈Σm. The i-th element of x is denoted by x[i]. The Hamming distance between sketches x and y is the number of positions at which the corresponding elements are different, formally defined as

H(x,y)=∑i=1m{10(x[i]≠y[i])(x[i]=y[i]).
We assume m=O(w) for word size w. Then, H(x, y) can be computed in O(logσ) time by performing ⌈log2σ⌉ sets of bitwise XOR and popcount operations [42].

A database X={x1,x2,…,xn} is a dynamic set consisting of n sketches, and it supports the insertion of a new sketch xi and deletion of sketch xi. The general Hamming distance problem for a given sketch y and radius r is to find all the sketches whose Hamming distance to sketch y in X is at most r, i.e., R={xi∈X:H(xi,y)≤r}.

Example
We have a database X of four sketches x1=``111020'', x2=``001020'', x3=``032021'', and x4=``113021'', where m=8 and σ=4. Given a sketch y=``111021'' and r=1, the similar sketches are R={x1,x4} because H(x1,y)=1, H(x2,y)=3, H(x3,y)=3, and H(x4,y)=1.

Literature review
Table 1 Summary of similarity search methods. V is verification time for candidates obtained from each similarity search method
Full size table
Many similarity search methods on Hamming distance have been proposed for decades. Several recent studies have focused on static settings [16, 21, 23, 34]. Theoretical aspects have also been argued [4, 9, 12]. In this section, we briefly review state-of-the-art similarity search methods for binary and integer sketches, and they are applicable to dynamic datasets. Table 1 summarizes state of the arts.

A seminal work for binary sketches is multi-index hashing (MIH) developed by Norouzi et al. [33]. MIH is based on the multi-index approach [17], and it enables quick similarity searches even with large r. A key observation is that two similar sketches must have similar parts. Thus, MIH partitions each sketch into q blocks of short sketches and builds q hash tables from the short sketches in each block. The similarity search first obtains a set of candidate solutions R′⊇R by retrieving each hash table with a small radius ⌊r/q⌋ and then removes false positives from R′ by computing the Hamming distances.

The number of blocks offering the best search performance is determined by the configuration of the dataset. Norouzi et al. [33] empirically demonstrated that the best performance of MIH is often achieved when q=m/log2n. They also showed that setting q to a number apart from m/log2n significantly degrades performance. Thus, MIH is unsuitable for dynamic problem settings where database size n varies.

Hamming weight tree (HWT) developed by Eghbali et al. [14] is a state-of-the-art similarity search method to solve the issue of MIH. Instead of using hash tables, HWT uses a search tree constructed based on Hamming weight (i.e., the number of ones appearing in a binary sketch). However, the similarity search takes O(mlogm(logn)4r) time and slows down dramatically for a large database of n. In addition, those similarity search methods were designed for binary sketches, and they are not necessarily suitable for integer sketches.

HmSearch developed by Zhang et al. [42] is a multi-index similarity search method designed for integer sketches. HmSearch reduces the general Hamming distance problem with radius r to small problems with radius one by tuning the number of blocks and preregistering candidate solutions in hash tables. However, this approach that preregisters candidate solutions consumes a large amount of memory and requires a large amount of update time.

Gog and Venturini [16] proposed an idea that defines ⌊r/2⌋+1 blocks to produce small problems with radius one and bypasses preregistering candidate solutions stored in hash tables. They presented a simple variant of HmSearch, which is referred to as GV in this paper. The similarity search is performed with the same algorithm as that of MIH. Thus, GV can be considered as a simple modification of MIH for integer sketches and has the same issue as MIH, which causes inefficiency in dynamic problem settings where n is variable. Moreover, GV’s search speed was much slower than HmSearch’s, as experimentally demonstrated in Sect. 7.

Despite the importance of dynamic similarity search methods for the general Hamming distance problem, there is no efficient method. The main reason is that most methods rely on the multi-index approach using hash tables, which require setting the appropriate number of blocks depending on variable parameter n. Although HWT attempts to address that issue using a tree structure, it is inefficient for large datasets and is inapplicable to integer sketches.

Trie-based similarity search
A key idea behind our similarity search DyFT is to build a trie data structure on X. The trie enables quick similarity searches and has been successful for static applications [21, 23]. In this section, we first introduce the data structure and the search algorithm and then discuss implementation issues of dynamic tries for similarity searches.

Data structure
Trie is an edge-labeled tree storing sketches in X. Each node is associated with the common prefix of a subset of the sketches, and each leaf is associated with a particular sketch in X. Each edge has an integer organizing sketches as a label. All outgoing edges of an inner node are labeled with distinct integers. The downgoing path from the root to each leaf corresponds to the sketch associated with the leaf. Each leaf has a list of associated sketch ids, so-called posting list.

Fig. 1
figure 1
Example of trie for database X of eight sketches. The dashed blue arrows show the traversal for search with y=``111020'' and r=1 (Color figure online)

Full size image
Example
Figure 1 shows an example of a trie data structure for database X of eight sketches. Sketch x1=``111020'' is associated with the fourth leaf from the left and can be reorganized by concatenating edge labels on the path from the root to the leaf.

Search algorithm
The exact search for a given sketch y traverses trie nodes from the root by using the integers of y. If we reach a leaf, y is stored in the trie, and the corresponding sketch ids can be obtained from the posting list. A simple extension of the exact search implements the similarity search with radius r. The similarity search traverses trie nodes from the root by using the integers of y with at most r errors allowed. In other words, we count the number of errors from the root to each node v visited in the traversal and, if the number exceeds r, stop traversing down to all the descendants under v. The solution R is the set of all sketches associated with leaves reachable within r errors.

The similarity search can prune unnecessary portions of the search space and can be quickly performed for a small radius r. The time complexity is O(mr+2) [1]. Note that, although Arslan and Eǧecioǧlu [1] derived the complexity assuming the perfect binary trie with σ=2, it does not vary for any σ.

Example
Figure 1 illustrates the similarity search for query sketch y=``111020'' and radius r=1. We traverse trie nodes from the root as depicted in the dashed blue arrows while comparing the integers of y to each edge label visited. The error labels are colored in red. If the number of errors from the root exceeds r=1, we stop the traversal as indicated by the blue dashed arrows no longer being drawn. We can reach the leaves associated with x1 and x7 and obtain the solution R={x1,x7}.

Implementation issues
Each inner node in a trie is implemented as a mapping from edge labels to child pointers. A trie storing a large database X maintains many pointers and consumes a large amount of memory. A well-known technique for substantially reducing memory consumption is to omit nodes around leaves. Thus far, a number of memory-efficient trie data structures have been developed by leveraging this technique, e.g., [2, 18, 25, 40]. However, these data structures were designed for exact string searches and inefficient for similarity searches.

For example, HAT-trie [2] is the leading data structure employing the reduction technique. Briefly, HAT-trie stores only some of trie nodes around the root and replaces the other nodes into hash tables. In the exact search, HAT-trie only needs to check one hash table reached by the traversal and performs well. In the similarity search, however, it has to check all entries on some hash tables and is not efficient trivially. Other data structures [18, 25, 40] replace non-branching nodes with strings. However, this approach is equivalent to our reduction approach using a fixed threshold described in Sect. 5, and it is inefficient for dynamic datasets as demonstrated in Sect. 7.

Another interest is an efficient implementation of trie nodes. Most of existing implementations were designed for general strings of byte characters, i.e., σ=256. This is because tries have been traditionally used for text processing. In our applications, however, σ is a given parameter depending on similarity-preserving hashing. For example, σ≤4 is recommended in b-bit minwise hashing [28]; σ≥16 is recommended in 0-bit CWS [26, 27].

Consequently, there is no dynamic and scalable trie data structure for similarity searches on integer sketches. We have the following two issues to be addressed:

1.
Scalability There is no node reduction technique for similarity searches.

2.
Generality There is no node implementation technique for integer sketches with parameter σ.

In the next sections, we present a new method to solve the issues and realize efficient similarity search using dynamic tries. Section 5 presents DyFT, a new similarity search method for the first issue. Section 6 presents an efficient node implementation of DyFT nodes for the second issue.

DyFT
DyFT is a dynamic similarity search method using the trie data structure and is designed for solving the scalability issue. In this section, we first introduce the basic idea, then present the optimization approach, and finally give the complexity analyses.

Fig. 2
figure 2
Example of DyFT for database X. Search for y with r=1 traverses nodes along blue dashed arrows and reaches the posting lists containing x1, x4, and x7. The solution R={x1,x7} is obtained by verifying H(x1,y)=0, H(x4,y)=2, and H(x7,y)=1 (Color figure online)

Full size image
Fig. 3
figure 3
Example of inserting x9 to Lv in Fig. 2. If |Lv| is more than τ, we split v into two leaves

Full size image
Basic idea
The basic idea is to allow false positives and store only some of trie nodes around the root. In other words, DyFT exploits the trie search algorithm for filtering out dissimilar sketches and aims to obtain solution candidates. Figure 2 shows an example of DyFT for the database in Fig. 1. Compared to the original trie in Fig. 1, the number of DyFT nodes is decreased from 41 to 8. A leaf v at level ℓ reached by sub-sketch x′∈Σℓ is associated with all sketches in X starting with x′. For example, in Fig. 2, the leaf reached by “03” is associated with sketches x3 and x8 starting with “03”. Every leaf v has the posting list of associated sketches. We denote the posting list by Lv and its length by |Lv|.

The similarity search for given y and r traverses DyFT nodes in the aforementioned manner. For a leaf v reached within r errors, each sketch xi∈Lv is verified by checking whether H(xi,y)≤r. Figure 2 shows a search example, and Search in Algorithm 1 shows the search algorithm.

figure c
We now present the insertion algorithm. Initially, the DyFT structure for an empty X consists only of the root with an empty posting list. Given a sketch xi, we traverse DyFT nodes using xi and visit the deepest reachable node v. If v is an inner one, we insert a new leaf u from v and associate a new posting list Lu storing xi. If v is a leaf, we append xi to Lv; then, DyFT determines whether leaf v should be split. If |Lv| is longer than a threshold τ, we create new leaves from v and split Lv into disjoint short lists (Fig. 3). Insert in Algorithm 1 shows the insertion algorithm.

The deletion algorithm is symmetrical to that of insertion. We remove xi from Lv for leaf v reached by xi. If Lv becomes empty, we remove leaf v from DyFT.

Optimal threshold
The search performance of DyFT is affected by threshold τ. If τ is large, the verification time for Lv becomes large. If τ is small, DyFT defines many nodes and the traversal time becomes large. Thus, we need to set a reasonable value of τ. Such a reasonable value of τ can be determined according to the configuration of X and given parameters such as n, σ, and r; however, it is impossible to search for such a reasonable value in dynamic settings. To address this issue, we first construct a search cost model assuming that sketches are uniformly distributed in the Hamming space and then determine an optimal threshold τ∗ minimizing the search cost.

By fixing r and σ, we consider the reach probability for node v at level ℓ, which is the probability to reach v within r errors using a random sketch x∈Σℓ from a uniform distribution. Let v be traversed from the root node using sketch ϕ(v)∈Σℓ. The set of all sketches reachable to v within r errors is {x∈Σℓ:H(x,ϕ(v))≤r} whose cardinality is

N(ℓ)=∑k=0r(ℓk)(σ−1)k.
As the number of all possible sketches of length ℓ is σℓ, the reach probability of a node at level ℓ is

P(ℓ)={1N(ℓ)/σℓ(ℓ≤r)(ℓ>r).
It holds that P(ℓ)>P(ℓ+1) for ℓ≥r.

We define the search cost of node v at level ℓ for random sketch x∈Σℓ by multiplying the reach probability by the computational cost. When we visit an inner node v at level ℓ during similarity search, we try to descend to the children of v. Then, we have two cases whether (i) H(x,ϕ(v))<r or (ii) H(x,ϕ(v))=r. In case (i), we check all the children in O(σ) time (Lines 10–14 in Algorithm 1). In case (ii), we look up the child in O(1) time (Lines 16–17 in Algorithm 1). Case (ii) occurs for sketches in {x∈Σℓ:H(x,ϕ(v))=r} whose cardinality is

N2(ℓ)=(ℓr)(σ−1)r.
The occurrence probability of case (ii) is N2(ℓ)/N(ℓ), and the computational cost of v is

Fin(ℓ)=(1−N2(ℓ)N(ℓ))×σ+N2(ℓ)N(ℓ).
Thus, the search cost of inner node v at level ℓ is

Cin(v)=P(ℓ)×Fin(ℓ).
When we visit a leaf v at level ℓ, we verify all sketches associated with Lv, and the search cost is

Cleaf(v)=P(ℓ)×|Lv|×⌈log2σ⌉.
We fix the optimal threshold τ∗ based on the search cost model. After appending a new sketch to Lv, τ∗ can be used to determine whether to split v depending on |Lv| to maintain the smaller search cost. If v is not split, then the search cost is Cleaf(v). If v is split into k new leaves u1,u2,…,uk, then the new search cost is

Cin(v)+∑i=1kCleaf(ui).
We assume that node v is at level ℓ≥r. Since the total length of Lu1,Lu2,…,Luk is |Lv|, it holds that

∑i=1kCleaf(ui)=P(ℓ+1)×|Lv|×⌈log2σ⌉.
Thus, splitting v can maintain the smaller search cost if

|Lv|>P(ℓ)P(ℓ)−P(ℓ+1)×Fin(ℓ)⌈log2σ⌉=:τ∗.
(1)
Given r and σ, the optimal thresholds τ∗ are determined for each level ℓ and pre-computable. Figure 4 shows optimal thresholds τ∗ for various parameters r and σ.

Fig. 4
figure 4
Optimal thresholds τ∗ for various parameters

Full size image
Exception case We need to address the exception when ℓ<r, because the divisor of τ∗ becomes zero, i.e., P(ℓ)=P(ℓ+1). The occurrence of the exception is intuitively correct because the search always traverses all nodes at level ℓ≤r, and splitting a leaf at level ℓ<r just generates redundant nodes locally.

We fix τ∗ to zero for ℓ<r since we cannot determine τ∗ by Eq. (1). Instead, we incrementally compute the total search cost of DyFT, defined as

Ctrie=∑v∈VinCin(v)+∑v∈VleafCleaf(v),
where Vin and Vleaf are sets of inner nodes and leaves in DyFT, respectively. In the search phase, we compare the current cost Ctrie with the computational cost of linear search for X, i.e., Cls=n×⌈log2σ⌉. If Cls≤Ctrie, we perform linear search for xi∈X to avoid redundant node traversal; otherwise, we perform Search in Algorithm 1. Algorithm 2 shows the modified search algorithm. The switching approach enables us to select the faster search algorithm depending on the configuration of DyFT.

figure d
Weighting factor In practice, the computational costs of Cin and Cleaf depend on the implementation of DyFT and the configuration of a computing machine. To address the gap between the theoretical and practical costs, we introduce a weighting factor for inner nodes Win and adjust the search cost for inner node v by Win×Cin(v). We search a value of Win that supports fast searches by using a synthetic dataset of random sketches generated from a uniform distribution.

Complexities
We simply assume that τ is constant and derive the complexities shown in Table 1. The similarity search consists of traversing DyFT nodes, accessing posting lists, and verifying candidates. The number of traversed nodes is bounded by O(mr+2) when assuming the complete σ-ary trie [1]; thus, the traversal time is O(mr+2). The access time for each posting list is O(1) because the length of each posting list is bounded by constant τ. Therefore, the search time complexity is O(mr+2)+Vdyft, where Vdyft is the verification time for the obtained candidates.

Insertion is performed by traversing DyFT nodes in O(m) time and splitting the posting list in O(1) time. Deletion is also performed by traversing DyFT nodes in O(m) time and removing a leaf in O(1) time. Thus, the update time is O(m). The memory complexity is O(mn) since the number of nodes is bounded by O(mn).

Multi-index variant DyFT+ The similarity search of DyFT is inefficient for large r as the complexity is exponential to r. We can relax the time using the multi-index approach [17]. In the same manner as MIH, we define q DyFT structures for each block. We call this multi-index variant DyFT+. The similarity search is performed on q small DyFT structures with block length m/q and threshold ⌊r/q⌋. The time complexity is O(q(m/q)r/q+2)+Vdyft+, where Vdyft+ is the verification time for the obtained candidates. The update time and memory complexities are the same as those of DyFT.

Implementation of DyFT nodes
A node implementation is also significant to enhance the performance of DyFT. This section presents modified adaptive radix tree (MART), which is an efficient node implementation for DyFT. We first give observations for node implementations and then present our scheme of implementing MART. Subsequently, we describe the data structure of MART.

Observation and implementation scheme
We consider a data structure for an inner node that maps edge labels to child pointers. A simple data structure referred to as the array form is a pointer array of length σ whose c-th element has the child pointer with edge label c. The array form can directly obtain the child pointer for a given c. Using the array form as a baseline, we provide the following observations for node implementations.

Observation A For binary sketches (i.e., σ=2), the array form is memory-efficient because most inner nodes have two children and most elements of the array are used. By chunking bits in binary sketches and suppressing the height of DyFT, we can reduce cache misses caused by node-to-node traversals and enhance time performance, as observed in prior studies [5, 6, 25].

Observation B For integer sketches with large σ, inner nodes around the root have many children, but those around leaves have few children. The array form is inefficient for nodes with few children because most elements of the array are empty. Memory efficiency can be improved by introducing several data structures depending on the number of children, as suggested in prior studies [21, 25, 41]. Although adaptive radix tree (ART) [25] is a successful data structure in this approach, it was designed for byte edge labels and lacks generality to σ.

Our scheme We assume σ≤256, following practical settings of similarity-preserving hashing techniques [27, 28, 37]. MART reorganizes integer sketches into byte sketches to suppress DyFT’s height (from Observation A) and represents DyFT nodes from byte sketches using a modified ART data structure (from Observation B). Sections 6.2 and 6.3 present the former and latter approaches, respectively.

Byte packing and fast computation
To efficiently handle integer sketches as byte sketches, we pack z=⌊logσ(256)⌋ integers c1,c2,…,cz into byte b=∑zi=1ciσi−1<σz≤256. In this manner, we convert an integer sketch x=(c1,c2,…,cm)∈Σm into byte sketch x′=(b1,b2,…,bm′) of length m′=⌈m/z⌉. In what follows, H(b,b′) denotes the Hamming distance between two integer sequences c1,c2,…,cz and c′1,c′2,…,c′z packed in two bytes b and b′, respectively.

Through the packing, we build a DyFT structure from byte sketches and perform the similarity search using a given byte sketch. When we visit an inner node v during the search, we face the small problem corresponding to Lines 9–17 in Algorithm 1.

Problem 1
Given an inner node v, byte label b, and radius r′, find children u1,u2,…,uk of v with edge byte labels a1,a2,…,ak such that H(ai,b)≤r′.

Fig. 5
figure 5
Example of Problem 1 when r′=1 and Σ={0,1,2,3}. The byte labels are denoted in unpacked format. Children u1 and u2 are the solution because H(a1,b)=1 and H(a2,b)=0, while u3 and u4 are not the solution because H(a3,b)=2 and H(a4,b)=3

Full size image
Figure 5 shows an example of Problem 1. If r′=0, we just look up a child with edge label b. If r′>0, we have the two approaches: LinearScan visits all children of v and computes the Hamming distances for the edge labels; BruteForce generates a set of all byte labels A={a:H(a,b)≤r′} and looks up the children of v with edge labels a∈A. MART performs one of these approaches according to the configuration of a given inner node, as presented in Sect. 6.3.

To quickly perform the approaches without unpacking byte labels, we introduce two σz×σz tables H and A. H is used in LinearScan, whose b-th row stores the Hamming distances between b and all byte labels a, i.e., H[b,a]:=H(a,b). A is used in BruteForce, whose b-th row stores all byte labels a sorted in ascending order of H(a, b). We can simply generate A by scanning the elements of A[b,i] for i=0,1,… until we encounter H[b,A[b,i]]>r′. Both H and A are implemented as simple tables of byte elements and can be precomputed. Thus, H and A contribute to quickly solving Problem 1 with only up to 128 KB of memory without unpacking byte labels.

Example
In Fig. 5, as the alphabet size σ=4, z=⌊log4(256)⌋=4 integers are packed into a byte label, e.g., integers a1=``0201'' are packed into 0⋅1+2⋅4+0⋅16+1⋅64=120=72, and integers b=``0231'' are packed into 0⋅1+2⋅4+3⋅16+1⋅64=120. The (120, 72)-th element of H stores the result of H(a1,b), i.e., H[120,72]=1, which enables LinearScan to compute the Hamming distance without unpacking a1 and b. For all the pairs between label b=``0231'' and all possible labels a consisting of z=4 integers, there is only one label a such that H(b,a)=0, i.e., a=``0231''; hence, the first element of A[120] stores the label, i.e., A[120,0]=``0231''. There are 12 labels a such that H(b,a)=1; hence, the subsequent 12 elements of A[120] stores the labels, i.e., A[120,1]=``1231'', A[120,2]=``2231'', A[120,3]=``3231'', and so on. Scanning A[120] enables BruteForce to quickly generate A for b, i.e., H[120,A[120,0]]=0, H[120,A[120,1]]=1, H[120,A[120,2]]=1 and so on.

Fig. 6
figure 6
MART representations for node v with k children u1,u2,…,uk. The child pointer to u2 with edge label 3 is stored in Ptr[1] such that Key[1]=3 in NodeS, Ptr[Idx[3]=1] in NodeD, and Ptr[3] in NodeF

Full size image
Adaptive data structure for inner nodes
figure e
Although ART [25] is a space-efficient data structure for representing inner nodes with byte labels, the design is for standard trie structures and is redundant for DyFT. For example, the path-compression technique of ART is not necessary for DyFT. MART simply modifies ART and represents inner nodes of DyFT. MART uses the following three types of data structures depending on the number of children. Let us consider representing an inner node v with k children. The three types of data structures are illustrated in Fig. 6, and their algorithms to Problem 1 are presented in Algorithm 3.

NodeS (Node Sparse) is a data structure for storing node v with k children of no more than K, where K is a constant parameter. It consists of two arrays Key and Ptr. Key is a byte array of length K that stores edge labels from v. Ptr is a pointer array of length K such that Ptr[i] stores the child pointer with edge label Key[i]. We maintain the arrays such that the first k elements are used. Problem 1 is solved by performing LinearScan for the first k elements of Key. If r′=0, modern CPUs can quickly search the elements using SIMD instructions in parallel, as presented in [25]. NodeSearchS shows the algorithm.

NodeD (Node Dense) is a data structure for storing node v with k children no more than K. It consists of two arrays Idx and Ptr. Idx is a byte array of length 256 to indicate positions of Ptr. Ptr is a pointer array of length K such that Ptr[Idx[b]] stores the child pointer with edge label b. Idx[b]=K+1 indicates that there is not a child pointer with b. Problem 1 is solved by performing BruteForce for Idx. NodeSearchD shows the algorithm.

NodeF (Node Full) is a data structure for very large k and consists of pointer array Ptr of length 256 such that Ptr[b] stores the child pointer with edge label b. The data structure is identical to the array form. Problem 1 is solved by performing BruteForce for Ptr. NodeSearchF shows the algorithm.

Every data structure has a header of one byte to store the value of k. Let w be the word size in bits such as 32 or 64 bits. NodeS consumes 8+8K+wK bits, NodeD consumes 8+8⋅256+wK bits, and NodeF consumes 8+256w bits. NodeS is the most memory-efficient but uses LinearScan taking O(k) time. NodeD is more memory-efficient than NodeF when K<256−2048/w.

With respect to time and space, NodeS is efficient for small K, and NodeD is efficient for large K. We define NodeS with K=2,4,8,16,32 and NodeD with K=64,128. An inner node with k children of no more than 128 is represented in NodeS or NodeD such that K is the smallest and no less than k. An inner node with k children of more than 128 is represented in NodeF. This adaptive selection allows child pointers to be stored space-efficiently.

Compact implementation for leaves
Finally, we briefly present a compact implementation of leaves. Each leaf is represented as a pointer to the posting list. We compress the pointers using a sparse direct address table [33] that groups g pointers by concatenating the g posting lists and reduces the number of pointers by a factor of g. Given a leaf, the sparse direct address table can access the corresponding posting list using the identifier in O(g/w) time. DyFT sets g=w to perform the access in constant time. The implementation details are presented in [33, Sect. 6].

Experiments
In this section, we provide extensive experimental analysis to show that (i) our approaches in Sects. 5 and 6 solve the issues of the trie-based similarity search presented in Sect. 4, and (ii) our data structure performs superiorly compared to state-of-the-art similarity search methods with respect to scalability, time performance, and memory efficiency.

We evaluated the performances of DyFT and DyFT+ using the four real-world vector datasets.

Text1M consists of 999,994 pre-trained continuous word vectors from English Wikipedia 2017 using fastText [32], where each vector is a real number vector of 300 dimensions.

MNIST8M consists of 8,100,000 images of handwritten digits from 0 to 9, where each image is represented as a real number vector of 784 dimensions [30].

Review13M consists of 12,886,488 book reviews in English from Amazon [31]. Each review is represented as a 9,253,464-dimensional binary fingerprint of which each dimension represents the presence or absence of a word.

CP216M consists of 216,121,626 compound-protein pairs in the STITCH database [24], where each pair is represented as a 3,621,623-dimensional binary fingerprint.

We produced binary and integer sketches from the datasets as follows. We converted real number vectors in Text1M and MNIST8M into binary sketches using Charikar’s simhash algorithm [10] and into integer sketches using the GCWS algorithm [27]. The former algorithm preserves the Cosine similarity, and the latter algorithm preserves the generalized min–max similarity. We converted binary vectors in Review13M and CP216M into binary or integer sketches using Li’s mihhash algorithm [28]. The algorithm preserves the Jaccard similarity.

We constructed an index of similarity search methods by inserting sketches in a dataset in random order. We measured the elapsed insertion time and required memory usage for the construction. We produced a query set by randomly sampling 1000 sketches from each dataset and measured the average similarity search time per query.

We evaluated σ=16 for integer sketches following the practical considerations in [27, 28]. We evaluated DyFT and HWT (without the multi-index approach) using short sketches of m=32 and small radii r≤4. We evaluated DyFT+, MIH, HmSearch, and GV (with the multi-index approach) using long sketches of m=64 and large radii r≤10. We fixed Win=0.5 based on experiments using a dataset of 10 million random sketches.

Fig. 7
figure 7
Frequency of occurrence of each integer value in sketches for each dataset in percentage, which is the statistic related to the skewness of the dataset. The left and right columns show the results for binary and integer sketches, respectively. The results are sorted so that the skewness is easier to see

Full size image
Figure 7 shows the frequency of occurrence of each integer value in sketches for each dataset, which is the statistic related to the skewness of the dataset. CP216M is the dataset closest to uniformly distributed sketches, since each frequency has almost the same. On the one hand, Text1M and MNIST8M are very skewed. As DyFT is designed on the assumption of uniformly distributed sketches, the statistics will be important to analyze the performance of DyFT.

We conducted all experiments on one core of quad-core Intel Xeon CPU E5–2680 v2 clocked at 2.8 GHz in a machine with 256 GB of RAM running the 64-bit version of CentOS 6.10 based on Linux 2.6. We implemented all data structures in C++17 and compiled source codes using g++ version 7.3.0 with optimization flags -O3 and -march=native. The code used in our experiments is available at https://github.com/kampersanda/dyft, and we describe the implementation details in Appendix.

Fig. 8
figure 8
Results for optimal threshold τ∗ and fixed thresholds τ=1,10,100 on MNIST8M and CP216M. The charts show average search time in milliseconds for varying the number of sketches n plotted in logarithmic scale

Full size image
Result for various thresholds
We analyzed DyFT’s performance with optimal threshold τ∗ and fixed thresholds τ=1,10,100. Figure 8 shows the results of search time on MNIST8M and CP216M. The search time with τ∗ was the fastest in most cases, even for skewed dataset MNIST8M, although the value of τ∗ was determined by assuming random sketches from a uniform distribution. The effectiveness of τ∗ could be observed especially when σ=16 and r=4. The search times with fixed thresholds τ were reversed according to n, i.e., setting τ=1 provided faster searches for large n while setting τ=100 provided faster searches for small n. This demonstrated that τ is not efficient in dynamic settings where n is varied. On the other hand, τ∗ maintained the fastest similarity search speed even when n was varied.

Fig. 9
figure 9
Results for node reduction ratio by DyFT on Text1M and Review13M. The charts show the ratio of the number of DyFT nodes to the number of original trie nodes in percentage

Full size image
Result for node reduction
We analyzed the performance of node reduction by DyFT. Figure 9 shows the reduction ratio to the original trie (without node reduction) on Text1M and Review13M. For all cases, the number of DyFT nodes was almost 5–15% of the number of original trie nodes, and DyFT significantly reduced redundant nodes for memory efficiency. The larger radius r was, the higher the reduction ratio became. This is because the value of τ∗ becomes large as r increases, as shown in Fig. 4. When σ=16, the value of τ∗ does not change much within r∈[1,4], and the reduction ratio was the same.

Result for implementations of nodes
We compared the performances of MART, the array form (Array), and the original ART [25]. We evaluated each data structure when implementing inner nodes of DyFT. Both Array and ART did not apply the byte-packing technique. The aim of the comparison with ART is to observe the effectiveness of the byte-packing technique; hence, we did not implement unnecessary techniques of ART such as path compression.

Table 2 Results for node implementations on Review13M (m=32)
Full size table
Table 2 shows the results of search time, insertion time, and memory usage on Review13M. They demonstrated the validity of our observations in Sect. 6.1. The search time of MART was the fastest in all cases. Compared to Array, MART was at most 6.3× faster for binary sketches and at most 1.5× faster for integer sketches. This suggests that suppressing DyFT’s height with the byte-packing technique provides fast retrieval on Observation A. Similarly, the insertion time of MART was the fastest for binary sketches due to the byte-packing technique, although Array was the fastest for integer sketches due to the simplest data structure. With respect to memory usage, Array was the smallest for binary sketches but largest for integer sketches on Observations A and B; ART and MART were the smallest for integer sketches on Observation B. Overall, MART achieved relevant space-time trade-offs for both binary and integer sketches.

Result for DyFT on binary sketches
We compared the performances of DyFT and HWT. HWT is the state-of-the-art method designed for dynamic similarity searches on binary sketches [14]. We implemented HWT using the original source code available at https://github.com/sepehr3pehr/hwt.

Fig. 10
figure 10
Results for DyFT and HWT. a average search time in milliseconds for varying the number of sketches n. b insertion time in minutes for varying n. c memory usage in GB for varying n. They are plotted in logarithmic scale

Full size image
Figure 10 shows the results of search time, insertion time, and memory usage. As n increased, the search time of DyFT became faster than that of HWT. This result is consistent with the search time complexities of DyFT and HWT, as HWT’s complexity contains the factor of O(logn). On the largest dataset CP216M, DyFT was at most 6000× faster than HWT when r=2. Although HWT’s insertion time complexity O(mlogm) is worse than DyFT’s complexity O(m), the measured insertion times were not much different because m was not significant. Although the memory complexities of DyFT and HWT are the same, DyFT was at most 13× more memory-efficient than HWT because of the node-omitting approach and MART.

Fig. 11
figure 11
Comparison results for DyFT+, MIH, and HSV on binary sketches. a average search time in milliseconds for varying radius r. b–d Average search time in milliseconds, insertion time in seconds, and memory usage in GB, for varying the number of input sketches n. The search time of HSV on CP216M when r=2 is not plotted since we could not construct the complete index within 256 GB of memory. They are plotted in logarithmic scale

Full size image
Result for DyFT+ on binary sketches
We compared the performances of DyFT+, MIH, and HSV on binary sketches. MIH is an early similarity search method using the multi-index approach [33]. We implemented MIH using the original source code available at https://github.com/norouzi/mih. HSV is a variant of HmSearch optimized for binary sketches [42]. We implemented HSV applicable to dynamic settings using std::unordered_map. We tested q=2,4 for DyFT+ and MIH to observe the effect of the number of blocks on performance. Note that the only difference between DyFT+ and MIH is whether a DyFT or hash-table structure is used to implement the index.

Figure 11 shows the results of search time, insertion time, and memory usage. Since HSV was not competitive, we consider only on DyFT+ and MIH. We first focus on the average search time for varying r in Fig. 11a. The search times of DyFT+ and MIH were not much different when all sketches in the dataset were inserted. Both DyFT+ and MIH with q=2 performed superiorly when the dataset had large n. We now focus on the average search time for varying n in Fig. 11b. As reviewed in Sect. 3, the performance of MIH significantly degraded according to n. MIH with q=2 was fast when n was large, but very slow when n was small. DyFT+ maintained faster searches even when n was small. For insertion time and memory usage in Fig. 11c and d, MIH with q=2 was significantly worse when n was small. The result demonstrated that DyFT+ with q=2 is an excellent similarity search method if the dataset is dynamic and expected to be large.

Fig. 12
figure 12
Comparison results for DyFT+, GV, and HSD on integer sketches. a Average search time in milliseconds for varying radius r. b–d Average search time in milliseconds, insertion time in seconds, and memory usage in GB, for varying the number of input sketches n. Some results of HSD on CP216M are not plotted since we could not complete to construct the index within 256 GB of memory. They are plotted in logarithmic scale

Full size image
Result for DyFT+ on integer sketches
We compared the performances of DyFT+, GV, and HSD on integer sketches. GV is a simple modification of MIH based on the idea in [16]. HSD is a variant of HmSearch optimized for integer sketches [42]. We implemented GV and HSD applicable to dynamic settings using std::unordered_map. The only difference between DyFT+ and GV is whether a DyFT or hash-table structure is used to implement the index. To fairly compare DyFT+ with GV, we set q=⌊r/2⌋+1 in DyFT+ in the same manner as GV.

Figure 12 shows the results of search time, insertion time, and memory usage. We first focus on the average search time in Fig. 12a and b. GV was not competitive to DyFT+ and HSD. DyFT+ outperformed HSD in most cases. We now focus on the insertion time and memory usage in Fig. 12c and d. HSD was not competitive to DyFT+ and GV, as reviewed in Sect. 3. The insertion time of GV was the fastest because of its very simple data structure. The memory usage of DyFT+ was the smallest because of the node-omitting approach and MART. The result demonstrated that DyFT+ is a fast, scalable, and dynamic similarity search method on integer sketches.

Summary of experimental results
The results in Sects. 7.1–7.3 showed that our approaches can solve the implementation issues of the trie-based similarity search introduced in Sect. 4. Although the memory efficiency of DyFT was improved by node reduction, the simple reduction approach with fixed thresholds decreased the search performance when the dataset size changed. Meanwhile, with the optimal threshold τ∗, DyFT was able to maintain a fast similarity search. In addition, MART enhanced the performance of DyFT compared to prior data structures.

The results in Sects. 7.4–7.6 showed that DyFT outperformed the prior search methods with respect to time, space, and scalability. Concerning binary sketches, the search performances of HWT and MIH decreased when the dataset size was changed, while DyFT maintained fast searches. In addition, the update time and memory efficiency of DyFT were the best or second-best in all cases. Concerning integer sketches, DyFT achieved the fastest search time and the lowest memory usage in most cases. Although GV had the fastest update time, GV’s search speed was much slower than DyFT’s. Accordingly, DyFT was the most scalable method that always achieved fast similarity search and low memory usage.

It should be remarked that DyFT is designed for uniformly distributed sketches. The assumption of uniform distribution is often used when designing similarity search methods for random sketches (e.g., MIH and HWT). In our experiments, this assumption was reasonable since we used well-maintained open datasets. On the other hand, for datasets without such an assumption, the performance of DyFT is not guaranteed.

Conclusion
We presented a dynamic similarity search method called DyFT and its multi-index variant called DyFT+ for the general Hamming distance problem. DyFT employs a trie structure for similarity search. We omit many DyFT nodes to reduce the memory consumption while maintaining fast similarity searches based on the search cost model. We also presented an efficient node implementation called MART to achieve fast searches and memory efficiency for any value of σ. Our experimental analyses using real-world datasets demonstrated that DyFT and DyFT+ outperform state-of-the-art similarity search methods.

DyFT can efficiently solve the Hamming distance problem on sketches with uniform distribution. While the Hamming distance problem is also important for text data [7], the text data is not necessarily uniformly distributed, which makes it difficult to directly apply DyFT to the Hamming distance problem on text data. Thus, a future work is to develop a similarity search method for text data with arbitrary distributions by leveraging ideas behind DyFT.