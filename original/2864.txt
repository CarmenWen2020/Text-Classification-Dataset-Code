In text categorization, Vector Space Model (VSM) has been widely used for representing documents, in which a document is represented by a vector of terms. Since different terms contribute to a document’s semantics in various degrees, a number of term weighting schemes have been proposed for VSM to improve text categorization performance. Much evidence shows that the performance of a term weighting scheme often varies across different text categorization tasks, while the mechanism underlying variability in a scheme’s performance remains unclear. Moreover, existing schemes often weight a term with respect to a category locally, without considering the global distribution of a term’s occurrences across all categories in a corpus. In this paper, we first systematically examine pros and cons of existing term weighting schemes in text categorization and explore the reasons why some schemes with sound theoretical bases, such as chi-square test and information gain, perform poorly in empirical evaluations. By measuring the concentration that a term distributes across all categories in a corpus, we then propose a series of entropy-based term weighting schemes to measure the distinguishing power of a term in text categorization. Through extensive experiments on five different datasets, the proposed term weighting schemes consistently outperform the state-of-the-art schemes. Moreover, our findings shed new light on how to choose and develop an effective term weighting scheme for a specific text categorization task.

Access provided by University of Auckland Library

Introduction
As the explosive increase in the number of digital documents online, text categorization (TC), which automatically classifies natural language documents into some pre-defined thematic categories, has become an effective technique to organize these unstructured data [35]. To perform a TC task, an important step is document representation which transforms text documents into compact formats. In the widely used Vector Space Model (VSM), documents are represented as vectors of terms, such as words and phrases in documents [68]. Since different terms have different importance degrees in indicating the semantics of a document, term weighting schemes, which assign appropriate weights to terms, are often used in document representation to improve classification performance. Intuitively, term weighting schemes map documents into proper positions in a vector space, e.g. documents in the same class are closer to themselves than to those across different classes. This helps classifiers categorize documents more effectively and achieve better performance [4].

Depending on whether a method makes use of category information of training data, i.e. the membership of training documents in pre-defined categories of a TC task, term weighting schemes are often classified into unsupervised schemes if category information is not used and supervised schemes otherwise [41, 68]. Most traditional schemes, such as tf, tf⋅idf [36], BM25 [64] and their variants [44, 56], are unsupervised methods. These schemes are originally proposed in the field of information retrieval (IR) and focus on measuring the utility of a term in distinguishing a document from other documents, i.e. the relevance between a term and a document. However, the main objective of term weighting in TC is to measure how well a term can distinguish different categories in a corpus, i.e. the relevance between a term and a category. Thus, most unsupervised schemes cannot effectively quantify a term’s discriminating power in the context of TC.

To better measure the relevance between terms and categories, researchers have recently exploited category information of training documents in TC tasks and proposed various supervised term weighting schemes, such as chi-square test (chi), information gain (ig) [4], gain ratio (gr) and other new methods [8, 38,39,40]. However, not all the supervised schemes can achieve good performance in TC. Previous studies have shown that some supervised schemes, such as chi and ig, perform worse than unsupervised schemes [9, 31, 39, 41, 63]. Yet it remains unclear why these supervised schemes, which have sound mathematical foundations and are expected to have good performance in theory, instead perform poorly in practice.

Moreover, most supervised schemes weight a term “locally” with respect to a specific positive category (PC). Specifically, given a PC, all categories in a corpus other than the PC are combined as a single negative category (NC). Then, each term is weighted based on its distributions in PC and NC [30, 68]. This approach has the following limitations.

In multi-class classifications, PC only contains a single class, while NC is a combination of multiple classes. This leads to a high imbalance between PC and NC. As a result, the distributional statistics of a term in NC, such as the number of term’s absence in NC, tend to have large values, thereby dominating the resulting weights. However, as we shall discuss in Sect. 3, these statistics can lead to a misleading measure for a term’s discriminating power.

As multiple categories are combined as a single NC, fine-grained information on how a term distributes in individual categories of NC is neglected, such as which categories a term occurs in. This leads to information loss and makes it hard to measure the power of a term in discriminating different categories of NC.

Given the category-specific nature, these schemes require knowing the category of a document beforehand to weight a term in the document. However, categories of test documents are often unknown before categorization. Thus, a challenge in these schemes is how to properly represent test documents.

Fig. 1
figure 1
Distributions of “author” and “OPEC” in 8 categories of Reuters

Full size image
To address these limitations, we propose to measure a term’s discriminating power in TC based on its “global” distribution across all categories in a corpus. Specifically, we measure how concentratedly a term occurs across categories. We assume that a term concentrated in a small number of categories has more discriminating power in TC. To illustrate this, Fig. 1 shows the numbers of occurrences of two terms “author” and “OPEC” in eight categories of Reuters-21578 [45]. We see that “author” is scattered across multiple categories, while “OPEC” is mainly concentrated in a single category. Intuitively, a term scattered across categories has limited specificity in referring to a category (called category-general terms), while a term concentrated in a specific set of categories shows more specificity in referring to these categories (called category-specific terms). Thus, given a document that contains these two terms, the category-specific term “OPEC” is more indicative of the semantics/category of the document than the category-general term “author”. In this light, the degree of concentration that a term occurs across categories is a good measure of the discriminating power of the term in TC.

To quantify the concentration of a term distributed in categories, we adopt entropy, a widely used measure of uncertainty in information theory [34] and propose a series of supervised entropy-based term weighting schemes for TC, particularly in VSM. The main contributions of this work are as follows.

We first study the characteristics of previous term weighting schemes in TC and examine why some supervised schemes with sound mathematical foundations perform poorly with both analytical explanations and empirical evaluations. To the best of our knowledge, this study is the first to provide a systematic analysis on this research question.

We then explore using entropy to quantify a term’s discriminating power in TC. In our prior work [75], we have proposed two entropy-based term weighting schemes, i.e. distributional concentration (dc) and balanced distributional concentration (bdc). In this work, we propose a variety of extensions to dc and bdc in order to better cope with various circumstances in TC. Unlike previous schemes that often weigh terms based on a specific PC, the proposed schemes are primarily category-independent.

To explore whether the proposed entropy-based schemes provide effective measures of the discriminating power of a term in TC, we compare the proposed schemes with nine state-of-the-art term weighting schemes on five real-world datasets from different domains. The experimental results demonstrate the effectiveness of the proposed entropy-based schemes in TC tasks.

By using different experimental settings, we conduct a detailed analysis on how a scheme performs in different situations and why, which gives deeper insights into how to select an effective term weighting scheme in a TC task.

The remainder of this paper is organized as follows. In Sect. 2, we review the state-of-the-art term weighting schemes in TC. Section 4 presents the proposed term weighting schemes. The experimental evaluations are described in Sects. 5 and 6. In Sect. 7, we discuss the rationality of our proposed methods. We highlight the directions of future work in Sect. 8.

Related work
Document representation is an essential process to perform TC tasks; it transforms the raw textual documents into some compact formats so that documents can be processed by classifiers. In the widely used Vector Space Model (VSM), e.g. Support Vector Machine (SVM) and k-Nearest Neighbours (k-NN) models [13, 66], documents are often represented as vectors of terms. Terms are some content descriptive features of documents, such as single words, word n-grams, phrases, or any other components in a document. As different terms contribute to the semantics/topic of a document in various degrees, each term is often associated with a weight in these vectors, so as to reflect how useful a term is in distinguishing different categories in classification.

To properly weight terms, a variety of term weighting schemes has been developed, including statistics-based schemes [4, 9, 41, 63] and lexicon-based schemes [49, 77]. Statistics-based schemes work based on the distributional statistics of terms in a corpus, e.g. term frequency and document frequency. In contrast, lexicon-based schemes often use external lexicon or other knowledge bases to measure the semantic similarity or distance between terms and a category (often represented by some key terms). Given the need of access to external resources and the challenge in dealing with new words that do not exist in lexicon, lexicon-based schemes often have more complexity and less flexibility than statistics-based schemes [9]. Moreover, prior studies [49, 77] have not shown that lexicon-based schemes have significant advantages over statistics-based schemes. Hence, statistics-based schemes are still the dominant approach in the literature. In this study, we also focus on the statistics-based schemes.

Although our focus in this work is to examine term weighting schemes, another related area is word embedding methods, such as continuous bag-of-words (CBOW) and Skip-Gram, which have been shown to be effective in learning low-dimensional representations for text data over recent years [27, 43, 51]. These methods use neural networks to generate similar representations for semantically related words and have enhanced performance of many natural language processing tasks, e.g. latent semantic analysis [50], syntactic parsing [70], machine translation [91] and TC [46, 54, 74]. Despite these advances, term weighting schemes are still useful methods in document representation and can complement word embedding methods in the following ways. First, embedding models often require large-scale knowledge resources to extract robust semantic patterns. Previous studies have shown that word embeddings trained on small datasets are less effective than randomly initialized vectors [81] and perform worse than approaches based on term weighting schemes in TC [26]. This reduces the applicability of embedding methods in situations where large domain-specific corpora are not available. Although one can use pre-trained embedding models based on general knowledge resources, e.g. GloVe [59], BERT [18] and other models [5, 62, 87], in these situations, semantic taxonomy in the general knowledge base can differ from those in specific domains [24]. For example, the term “kill” carries diffident semantic meanings in the contexts of news and software engineering, respectively. Moreover, recent studies have shown that pre-trained word embeddings contain biases in data on which they are trained and these biases can be amplified in downstream applications [6, 57, 89]. So far, existing bias removal techniques, both as a post-processing step [6] and as a part of the training procedure [90], are not sufficient to remove these biases [29, 73]. In contrast, term weighting schemes are less sensitive to the sizes of corpora and hence can complement word embedding methods when large domain corpora are not available, without introducing external biases. Second, word embedding models have largely focused on generating semantically rich representations for individual words, and often represent a document by summing or averaging embeddings of individual words in the document [43, 50]. In this way, each word contributes equally to document embeddings, which is not efficient, since some words are more critical than others in expressing the semantics of a document. Thus, recent studies have proposed using term weighting schemes, such as tf⋅idf, to aggregate individual word embeddings into document embeddings [3, 53, 85]. These approaches have provided a simple but tough-to-beat baseline in a number of applications such as sentiment analysis [85], question classification [53] and textual similarity measurement [12, 48], even outperforming sophisticated, computationally intensive document embedding methods like Doc2Vec [43], Skip-Thought Vectors [37] and Doc2VecC [10], particularly on short-text corpora such as tweets and online product reviews [80], and supervised neural-based methods including RNN and LSTM [3].

Term weighting schemes are often classified into unsupervised ones and supervised ones [16], depending on whether a scheme makes use of the category information of training documents. Next, we briefly review previous methods; detailed introductions can be found in previous studies [41, 63, 68].

Unsupervised term weighting schemes
Most unsupervised schemes are originally proposed in the information retrieve (IR) literature, such as binary, tf, tf⋅idf and other variants [44, 65]. These schemes weight terms by counting the occurrences of terms in documents, without considering the category information of documents. For example, term frequency (tf) weights a term by the term’s raw frequency in a document [47]. Replacing any tf which is greater than 0 with 1, we can reduce tf to the binary scheme [79]. Other variants of tf include log(tf), log(tf+1) and log(tf)+1 [7]. In general, tfs capture the local weight of a term in a single document and they are often combined with other global factors together to weight terms.

Inverse document frequency (idf) [36] is the most popular global factor and has been successfully used in IR-related fields [65, 67]. The idf factor measures how commonly a term occurs across all documents. Combined with tf, the tf⋅idf value of term t in document d is typically calculated by

tf⋅idf(t,d)=tf(t,d)×Ndf(t),
(1)
where N denotes the total number of documents in a corpus. tf(t, d) denotes the term frequency of t in document d and df(t) denotes the number of documents that contain term t (called document frequency). So far, many variants of tf⋅idf have been proposed, such as BM25 [64]. Dumais [22] replaced idf with an entropy-based factor and found that this scheme outperforms tf⋅idf. Recently, tf⋅tdf has been proposed in [56], which is reported to outperform many other schemes such as BM25.

Supervised term weighting schemes
To date, supervised term weighting methods are mainly developed in two ways. One approach is function-based methods which weight terms by metric functions, such as chi-square test (chi), information gain (ig) and mutual information (mi) [4, 40]. Another approach is model-based methods which weight terms by using a search algorithm or by interacting with classifier models [1, 32, 55, 58, 71]. Typically, model-based methods require iterative learning procedures to obtain optimal weights, which is computationally expensive and time-consuming, especially in large-scale datasets. Moreover, model-based methods are often classifier-specific, i.e. the resulting weights may work well for a particular classifier, but not for other classifiers, and thereby likely to suffer from overfitting issues [41]. In contrast, function-based methods can easily scale up and are more robust across classifiers. In this work, we focus on the function-based methods and all the supervised schemes discussed below are function-based.

Table 1 A contingency table between term t and category ζi
Full size table
Before we review the supervised term weighting schemes, we first introduce notations widely used in these schemes. Given a term t that occurs in a document from category ζi of a corpus, ζi is regarded as the positive category (PC) and all other categories ζi¯¯¯¯ are combined together as negative category (NC). Based on the occurrences of t in PC and NC, we can compute a contingency table as shown in Table 1, where

a is the number of documents that contain t in PC;

b is the number of documents that do not contain t in PC;

c is the number of documents that contain t in NC;

d is the number of documents that do not contain t in NC;

and N=a+b+c+d is the total number of documents.

We now review the supervised term weighting schemes. Most supervised schemes are developed from feature-selection methods [30, 83, 84]. In these schemes, terms with a higher correlation with PC are often assumed to have more discriminating power in TC. To measure such correlation, chi, ig [4], mi [83], gain ratio (gr) [16], odds ratio (or) [52], G2 test (or Log-Likelihood Ratio) [23], eccd [42], and robust copula dependence (RCD) [8] are widely used. Based on the notations in Table 1, several common feature-selection-based schemes can be computed as:

chi(t,ζi)=N(ad−bc)2(a+c)(b+d)(a+b)(c+d),
(2)
ig(t,ζi)=aNlogaN(a+c)(a+b)+bNlogbN(b+d)(a+b)+cNlogcN(a+c)(c+d)+dNlogdN(b+d)(c+d),
(3)
gr(t,ζi)=ig(t,ζi)−(a+b)Nlog(a+b)N−(c+d)Nlog(c+d)N,
(4)
or(t,ζi)=adbc,
(5)
eccd(t,ζi)=ad−bc(a+b)(c+d)×Hmax−H(t)Hmax,
(6)
where H(t) and Hmax denote the entropy of term t in the categories of a corpus and the maximal entropy, respectively.

Recently, some new supervised schemes have been proposed. Soucy and Mineau [72] proposed a scheme based on statistical confidence intervals. Although this scheme outperforms tf⋅idf and tf⋅gr, it is complicated and difficult to implement. Relevance frequency (rf) uses the statistics in term-relevant documents to measure a term’s discriminating power [40, 41], defined as:

rf(t,ζi)=log(2+amax(1,c)).
(7)
This scheme and its variants, such as a logarithmically scaled rf (called vrf) [63] and a probabilistic rf (called trr) [38], have achieved good performance in TC. Another new scheme is iqf⋅qf⋅icf [63]. Unlike other schemes, iqf⋅qf⋅icf includes a factor called category frequency (cf) to consider the number of categories that a term occurs in. Given a corpus with N documents from a set of categories C, iqf⋅qf⋅icf is defined as:

iqf⋅qf⋅icf(t,ζi)=logNa+c×log(a+1)×log(|ζ|cf+1),
(8)
where |ζ| is the number of categories. Another two related schemes are inverse gravity moment (igm), which measures the inter-category distributional concentration of a term [9], and regularized entropy (re) which measures the imbalance of a term’s distribution between PC and NC [78].

Document representation policies
The next processing is to represent documents based on weights obtained by the above schemes. In unsupervised schemes, each term is assigned with an identical weight. Thus, both training documents (i.e. with known class labels) and test documents (i.e. without known classes) use an identical weight for a term in document representation, regardless of documents’ classes. We call these schemes that assign identical weights category-independent schemes. In contrast, supervised schemes often assign category-specific weights, i.e. weights of a term are selected according to the category labels of documents that contain the term. We call these schemes that assign weights with respect to a specific category category-specific schemes. For the category-specific schemes, two representation policies are often used [68].

The first one is the local policy, in which a term t is assigned different weights for different categories ζi. As the category labels of training documents are known beforehand in TC, the local policy is widely used to represent training documents, so as to effectively exploit their category information [4, 38]. This policy, however, can hardly be used to represent test documents, as their category labels are unknown. To address this, the second policy, i.e. the global policy, is proposed, in which each term is given the same weight for all categories [4, 38, 68]. To obtain an identical weight, some globalization methods are used, such as the sum wsum(t)=∑|ζ|i=1w(t,ζi), the average wavg(t)=∑|ζ|i=1P(ζi)w(t,ζi), or the maximum wmax(t)=max|ζ|i=1w(t,ζi) of a term’s category-specific weights w(t,ζi). Typically, wmax(t) outperforms other methods [16, 83]. However, as we shall discuss in Sect. 7, these methods can lead to different criteria on weighting terms in training and test data.

Analysis of terms’ discriminating power
To develop a term weighting scheme, a critical step is understanding what type of terms have high discriminating power in TC. To this end, we take 5 terms that are randomly selected from a dataset used in our experiments for example. Given a corpus (a subset of the Snippets dataset in our experiments) with N=1000 documents from 4 categories of Business (Bus), Computers (Com), Education (Edu) and Engineering (Eng) (250 documents for each category), the occurrences of each term in different categories are shown in Table 2. Since these documents are short search snippets, we assume that each term occurs only once in a document for an intuitive comparison.

We first examine how unsupervised schemes work in these examples. We take the widely used idf for example.

Example 1
Consider the two terms “IPO” and “AMD”. As the df values of “IPO” and “AMD” are 23 and 20, idf assigns a larger weight to“AMD” than “IPO”. However, “IPO” only occurs in Bus, while “AMD” scatters across Bus and Com. As a good indication of Bus, “IPO” intuitively should be weighted more than “AMD”. Since idf does not capture the strength of category-term relatedness, it fails to properly measure the discriminating power of these terms in TC.

Next, we examine how supervised schemes weight these terms by taking the classic chi and ig for example. To run chi and ig, we use the category Bus as PC.

Example 2
Consider the two terms “IPO” and “science” in Table 2. “IPO” only occurs in PC, while most occurrences of “science” are in NC. Intuitively, the correlation between “IPO” and Bus is much higher than that between “science” and Bus. However, weighting with chi (as well as ig) [41, 68], the correlation of “science” to Bus is instead higher than that of “IPO”, which is counterintuitive.

Table 2 Examples of term weighting, where PC and NC denote positive category and negative category of a term, respectively
Full size table
Table 3 Contingency tables between terms and category Bus
Full size table
To explore this issue, we build contingency tables between category Bus and terms “IPO”, “science” in Table 3 based on the notations in Table 2. For term ti and PC ζj, we can calculate the mean square contingency coefficient (also known as the ϕ coefficient) [76] to quantify the association for binary variables. We find that ϕ(IPO,Bus)=0.27 and ϕ(science,Bus)=−0.28, suggesting that the presence of “IPO” is positively associated with the presence of Bus, while the presence of “science” is negatively associated with the presence of Bus. Since |ϕ(IPO,Bus)|<|ϕ(science,Bus)| and chi=N×ϕ2, we yield chi(IPO,Bus)<chi(science,Bus). Thus, the values of chi fail to capture the polarities of these associations. Similar problems also exist in the results of ig.

Also, we see that the values of d in the contingency tables (i.e. 750 and 502) are much larger than those of a, b and c in Table 3, due to two reasons. First, since document vectors are often very sparse [19], the number of documents that do not contain a term n(ti¯¯¯¯)=b+d is often very large. Second, since NC in a multi-class classification task is a collection of many classes, the number of documents in NC, n(ζj¯¯¯¯)=c+d, also tends to be large. Given d≥0 and the constant value of N in a corpus, the large values of n(ti¯¯¯¯) and n(ζj¯¯¯¯) can lead d=a+n(ti¯¯¯¯)+n(ζj¯¯¯¯)−N to be large. The large values of d are likely to dominant the resulting weights of chi and ig. However, d is neither directly relevant to term ti nor to PC; it is less useful to indicate the discriminating power of a term [41]. Taken together, these classic supervised schemes cannot effectively measure the discriminating power of a term.

Unlike these classic schemes, new schemes, such as rf [41] and its variants [39, 63], only use term-relevant statistics (i.e. a and c), which can avoid the above issues. However, as the occurrence counts of a term in individual categories of NC are added up as a single number, fine-grained information on the distribution of a term in NC, e.g. the number of categories in which a term occurs, is ignored in these new schemes. This can lead to misleading results. Let us take the raw rf [41] for example.

Example 3
Consider the two terms “AMD” and “private” that have the same number of occurrences in PC, but with different numbers of occurrences in NC, i.e. 8 for “AMD” and 7 for “private”. Measured by rf, “AMD” has lower discriminating power than “private”. However, “AMD” occurs in a smaller number of categories than “private”. Intuitively, a term which occurs in a smaller number of categories can indicate the category of a document with a high degree of certainty. Thus, “AMD” has more discriminating power than “private”, which is opposite to the results of rf.

Although some schemes, such as iqf⋅qf⋅icf [63], count the number of categories that a term occurs, a single number can hardly reflect the fine-grained distribution information, such as the probability of a term distributed in each category. For example:

Example 4
Consider the terms “private” and “market”. As “private” occurs in a smaller number of categories than “market”, iqf⋅qf⋅icf assigns a larger weight to “private”. However, comparing the distributions of two terms in categories, we find that “market” has a high probability to occur in Bus; the degree of certainty that “market” indicates Bus is actually higher than that of “private”. Thus, “market” should have more discriminating power than “private”, which is against the iqf⋅qf⋅icf results.

Therefore, previous supervised schemes that weight terms “locally” with respect to a specific PC can hardly capture the fine-grained distribution information of a term in corpus.

Moreover, most previous supervised term weighting schemes are category-specific, while the category labels of test documents are unknown. This leads to a challenge on how these supervised schemes produce proper representations for test documents. To illustrate this, we take tf⋅chi for example.

Example 5
Given two training documents from different categories, namely d1=⟨math:0.7, homework:0.2⟩ ∈ Education and d2=⟨math:0.1, programmer:0.5⟩ ∈ Computers, if we use d2 as a test document d2^, we obtain a test vector d2^=⟨math:0.7, programmer:0.5⟩ using the wmax(t) weight of each term across categories. Since cos(d1,d2^)=0.78 is larger than cos(d2,d2^)=0.73, d2^ is classified into Education, while d2^ actually belongs to Computers. Even if we use other globalization methods, such as wsum(t) and wavg(t), the results are still incorrect in this case. In contrast, if we use category-independent schemes, e.g. binary representation, to represent documents, cos(d1,d2^)=0.5 is smaller than cos(d2,d2^)=1.0, and d2^ is correctly classified into Computers.

Entropy-based term weighting schemes
Intuition of entropy-based TC schemes
To capture fine-grained information that a term occurs in a corpus, we propose an alternative approach that weights terms by measuring the concentration of a term distributed in the categories of a corpus. Unlike previous methods that aggregate categories into PC and NC, our approach measures the distribution of a term over all individual categories “globally”. To illustrate the intuition of our methods, let us revisit the example in Table 2.

Example 6
Since “IPO” only occurs in a single category Bus, it can be a good indication of Bus and has more discriminating power than other terms. As most occurrences of “market” concentrate in Bus, compared with “AMD”, “private” and “science”, “market” has a higher degree of certainty to indicate a category. Hence, we consider that “market” has more discriminating power than these terms. Similarly, “AMD” has a higher degree of certainty to indicate the category of a document than “private”, since “AMD” is distributed in a smaller number of categories than “private”. Thus, we consider that “AMD” has more discriminating power than “private”. Finally, “science” scatters across many categories without much specificity to a category. Hence, “science” seems to be the most category-general and have the lowest discriminating power among these terms.

These observations can be summarized as follows.

Observation 1
Given two terms t1 and t2 in a document, if t1 concentrates within a smaller number of categories in a corpus while t2 evenly scatters across a larger number of categories, then t1 has a higher degree of certainty to indicate the document’s category than t2, and t1 has more discriminating power than t2 in TC.

To quantify the degree of concentration or certainty of a term distributed in the categories of a corpus, we adopt entropy, a classic measure of uncertainty in information theory [34, 69]. Formally, given a term t occurring in the categories of a corpus {ζ1,…,ζ|ζ|} and P(ζi|t) being the probability that t occurs in ζi, the entropy of this term with respect to categories is:

H(t)=−∑i=1|ζ|P(ζi|t)logbP(ζi|t),
(9)
where b is the logarithmic base and |ζ| is the number of categories in the corpus. Generally, a term that concentrates within a smaller number of categories has lower entropy than a term that scatters across a larger number of categories. Accordingly, we have:

Assumption 1
Given two terms in a document, if a term has a lower entropy than another term with respect to the set of categories in a corpus, we assume that the former term has more discriminating power than the latter term in TC. For example, as shown in Fig. 1, “OPEC” mainly occurs in the category of “crude” and has a lower entropy with respect to categories, while “author” scatters across many categories and has a higher entropy. Intuitively, “OPEC” is more useful to discriminate different categories than “author”.

Apart from empirical observations, the notion of entropy also makes sense of our intuition. Specifically, higher entropy of a term with respect to categories denotes a high degree of uncertainty on which category (or categories) the term indicates, i.e. less effectiveness to distinguish different categories in TC. Hence, entropy is a reasonable metric to measure the discriminating power of a term in TC. In this light, we propose a series of entropy-based term weighting schemes in the following.

Distributional concentration
Our first entropy-based scheme called distributional concentration (dc) is defined as:

dc(t)=1−H(t)=1+∑i=1|ζ|P(ζi|t)logbP(ζi|t)=1+∑i=1|ζ|f(t,ζi)f(t)logbf(t,ζi)f(t),
(10)
where H(t) is the entropy of term t with respect to the categories of a corpus. P(ζi|t)=f(t,ζi)f(t) is the probability of t appearing in category ζi. f(t,ζi) is the frequency of t in ζi, and f(t) is the sum of frequencies of t in all categories, i.e. f(t)=∑|ζ|i=1f(t,ζi). As the value of H(t) lies in the range [0,logb(|ζ|)], where the minimum and maximum values are obtained when t only occurs in a single category and t has the same frequency in each category, respectively, we normalize the values of dc into the range [0, 1] by setting the logarithmic base b=|ζ|. This allows the values of dc to be comparable across terms and corpora.

As shown in Table 2, dc assigns weights that are in line with our analysis on the discriminating power of terms in Example 6. Unlike unsupervised schemes such as idf, dc is supervised and it exploits the category information of training data. Unlike feature-selection-based schemes, dc excludes the statistics of term-irrelevant documents, which prevents these statistics from dominating weighting results and reducing the effectiveness of results. Unlike category-specific schemes such as rf and iqf⋅qf⋅icf, dc is category-independent—it weights terms based on the globally distributional concentration of a term in a set of categories. Thus, the distribution information of a term in each individual category can be exploited effectively. Further, dc does not split categories into PC and NC; it assigns weights without need of a pre-specified PC. This allows dc to directly represent test documents even though their class labels are unknown, without relying on the globalization methods discussed in Sect. 2.3.

Balanced distributional concentration
The scheme dc proposed above merely makes use of the distribution information that a term distributes over categories, regardless of the sizes of these categories. In most real-world TC tasks, categories vary in sizes, i.e. containing different numbers of documents. Given a larger sample of documents in large-sized categories, terms tend to have more occurrences in large-sized categories than in small-sized categories. Hence, merely counting absolute frequencies of a term in categories and ignoring the sizes of these categories can over-estimate the significance of a term in indicating large-sized categories. This can further lead to the bias towards large-sized categories in classification. To illustrate this, let us consider an example in Table 4.

Table 4 Term “interest” in categories of “earn” and “interest”
Full size table
Example 7
Table 4 shows the frequencies of term “interest” in two different-sized categories “earn” and “interest” of Reuters, where f(t,ζi) is the frequency of term t in category ζi and f(ζi) is the frequency sum of all terms in category ζi. Intuitively, the term “interest” is more related to category “interest” than category “earn”. However, as indicated by f(t,ζi), the term “interest” is more significant to indicate “earn” than “interest”, which is not reasonable. This issue can be addressed by considering the sizes of categories. Since a large-sized category often has larger f(ζi), we use f(ζi) to approximate the size of a category. We see that, relative to category sizes f(ζi), the significance of f(t,ζi) in the large-sized category “earn” is less than that in the small-sized category “interest”, i.e. P(t|ζi)=f(t,ζi)f(ζi).

To avoid the bias towards large-sized categories, we normalize the frequency of a term in each category over category sizes. Specifically, we replace the absolute frequency (i.e. f(t,ζi)) in Eq. 10 with the conditional probability of a term occurring in its relevant categories (i.e. P(t|ζi)=f(t,ζi)f(ζi)). We call this pre-processing balancing processing. As shown in Table 4, “interest” is more significant to indicate “interest” than “earn” measured by P(t|ζi), which is more reasonable. On this basis, our second scheme named balanced distributional concentration (bdc) is:

bdc(t)=1−Hb(t)=1+∑i=1|ζ|Pb(ζi|t)logb[Pb(ζi|t)]=1+∑i=1|ζ|P(t|ζi)∑|ζ|j=1P(t|ζj)logbP(t|ζi)∑|ζ|j=1P(t|ζj),
(11)
where Hb(t)=−∑|ζ|i=1Pb(ζi|t)logb[Pb(ζi|t)] is the balanced entropy of term t. Pb(ζi|t)=P(t|ζi)∑|ζ|j=1P(t|ζj) is the balanced probability of t appearing in category ζi based on the conditional probability P(t|ζi). Note that, given a corpus with a uniform distribution on category sizes, i.e. ∀i,j∈{1,…,|ζ|},i≠j:f(ζi)=f(ζj), bdc reduces to dc.

Smoothed distributional concentration
Although discriminative terms often exhibit lower entropy across categories, noise terms which occasionally occurs in a small number of documents can also have lower entropy. In other words, noise terms may be over-estimated in the above schemes like dc and bdc. Let us consider the two terms in Table 5 for example.

Table 5 Comparison of weights of dc and sdc
Full size table
Example 8
Table 5(a) shows the distributions of terms “IPO” and “compliat” in each category. Both of the two terms only occur in the category Bus and hence, are assigned with the same weight by dc. However, we notice that the occurrences of “compliat” in the whole corpus are much fewer than those of “IPO”. Compared with “IPO”, “compliat” is more likely to be a noise term rather than a useful term—it appears that “compliat” is a typo for “complaint”. Hence, it is inappropriate to assign the same weight to “IPO” and “compliat”.

To capture important patterns in data and avoid noise, we incorporate smoothing methods [86] into entropy-based term weighting schemes and propose smoothed distributional concentration (sdc). The basic idea of sdc is that the distribution of a noise term over categories tends to be more uniform and the corresponding entropy tends to increase after smoothing processing. This can reduce noise terms’ weights and hence, reduce their effects in classification. For example, by applying the Laplace smoothing [11] to “IPO” and “compliat”, we obtain the smoothed distributions of these terms in Table 5(b). Calculating dc again based on these smoothed distributions, the new dc value (i.e. sdc) of “compliat” becomes much smaller than that of “IPO”. Based on this, we define sdc as:

sdc(t)=1−Hs(t)=1+∑i=1|ζ|Ps(ζi|t)logbPs(ζi|t),
(12)
where Hs(t)=−∑|ζ|i=1Ps(ζi|t)logbPs(ζi|t) is the entropy of term t over categories after smoothing processing and Ps(ζi|t) is the probability of t in ζi after smoothing. Applying different smoothing methods [86], Ps(ζi|t) can be calculated differently. For example, using the Laplace smoothing, Ps(ζi|t) is:

Ps(ζi|t)=f(t,ζi)+αf(t)+|ζ|α,
(13)
where α>0 is the smoothing parameter, and α=0 indicates non-smoothing so that sdc reduces to dc. We use the popular add-one smoothing (i.e. setting α=1) in this work.

Categorical relevance factor
The above term weighting schemes aim to capture useful information in data. As a connection between raw data and classifier algorithms, term weighting schemes need not only to exploit the data information, but also to fit the classifiers in use. For example, in inherently binary classifiers (e.g. SVM), a dominant approach to perform a multi-class classification task is to decompose the task into multiple binary classification sub-tasks [15], where each binary classifier is built to distinguish a category (i.e. PC) from others [33]. Given the category-specific manner, schemes that take the category-specific relevance between a term and PC into account may be useful to improve the performance of these binary classifiers. Let us take the two terms in Table 6 for example.

Table 6 Examples of weighting factor cr
Full size table
Example 9
Table 6 shows the distributions of terms “lead” and “subject” in the categories of snippets subset. Since the two terms occur in categories with an approximately equal entropy, their dc weights are very close. However, in a binary classifier for example, weighting the discriminating power of a term is transformed into weighting the utility of a term in distinguishing PC from NC. Intuitively, “lead” is more useful to distinguish the Bus category from other categories than “subject”, since “lead” is more relevant to Bus than “subject” according to their distributions in categories.

To correspond to binary classifiers, we propose a category-specific factor named categorical relevance (cr). This factor can be added to the schemes proposed above, so as to measure the relevance between a term and PC. We build cr on the following considerations that are similar to tf⋅idf, but it is used with respect to categories. First, terms more frequently occurring in PC are intuitively more relevant to PC and can be more useful to distinguish PC from NC. Second, sine category-general terms that have low discriminating power in TC may have high frequencies across categories, including PC, the weights of highly frequent terms should be diminished. Taken together, we define cr as:

cr(t,ζi)=logβ(β+f(t,ζi)f(t)),
(14)
where β is the logarithmic base. f(t,ζi) is the frequency of term t occurring in category ζi and f(t) is the sum frequency of term t in all categories, i.e. f(t)=∑|ζ|i=1f(t,ζi). Note that cr has two properties. First, if f(t,ζi)=0, then cr(t,ζi)=1. Second, different values of β lead to different effects of cr in the resulting weights. For example, the influence of cr decreases as the increase in β, since a larger value of β reduces the significance of f(t,ζi)f(t) and makes values of cr close to 1. We use β=2 in our experiments. As shown in Table 6, measured by cr, “lead” is more useful than “subject” in discriminating Bus from other categories, which is more reasonable.

Unified entropy-based term weighting schemes
Since bdc, sdc and cr extend dc from three distinct perspectives, these extensions can be combined with one another, leading to a series of unified schemes. For example, if we first smooth the distribution of a term in categories with Laplace smoothing and then normalize the smoothed frequency of a term in each category with the prior information of category sizes, we can obtain a balanced smoothed distributional concentration (bsdc):

bsdc(t)=1−Hbs(t)=1+∑i=1|ζ|Pbs(ζi|t)logb[Pbs(ζi|t)],
(15)
where Hbs(t) denotes the entropy of term t with respect to the categories of a corpus after balancing and smoothing processing. Pbs(ζi|t)=f(t,ζi)+αf(ζi)+|V|α∑|ζ|i=1f(t,ζi)+αf(ζi)+|V|α denotes that balanced smoothed probability of appearing in categories ζi in all occurrences of term t. |V| denotes the total number of unique terms in a corpus. We can further append cr factor to bsdc and derive a new unified scheme (cr⋅bsdc) as:

cr⋅bsdc(t,ζi)=cr(t,ζi)×bsdc(t).
(16)
Moreover, combined with the local weighting factor tf (or other variants) by multiplication operation, we calculate the final weight of term t in d from category ζi as:

tf⋅cr⋅bsdc(t,d,ζi)=tf(t,d)×cr(t,ζi)×bsdc(t).
(17)
Table 7 Summary of entropy-based term weighting schemes
Full size table
Other unified schemes can be obtained in similar ways. All possible combinations are listed in Table 7. The schemes with cr assign category-specific weights to terms, while those without cr assign category-independent weights. For those with cr, we assume that if term t has no occurrences in category ζi (i.e. f(t,ζi)=0), then the discriminating power of t depends only on its global distributional concentration. That is, if f(t,ζi)=0, then cr(t,ζi)=1 and hence cr⋅dc=dc, cr⋅bdc=bdc and cr⋅sdc=sdc. Also, since a larger value of β makes values of cr close to 1 (see Eq. 14), the schemes with cr will behave similarly to those without cr (e.g. cr⋅dc≈dc) if a large value of β is used in cr. Moreover, when applying category-specific schemes like cr⋅dc, cr⋅bdc and cr⋅sdc to represent test documents of which class labels are unknown, we set f(t,ζi)=0. Then, test documents are represented by dc, bdc and sdc, respectively, which eliminates the dependency of these category-specific schemes on globalization methods (in Sect. 2.3).

Connections to existing schemes
So far, we have demonstrated the proposed entropy-based term weighting schemes. Although igm builds on a similar intuition to dc [9], igm uses gravity moment to measure the distributional concentration of a term across categories, while the proposed dc uses entropy. Also, igm is computed based on the absolute frequencies of a term in categories. As discussed in Sect. 4.3, this can lead to the bias towards large-sized categories, particularly in a corpus with a skewed category distribution.

We further examine differences between the proposed schemes and previous entropy-based schemes such as ig, mi, eccd and re. First, compared to the unsupervised scheme that measures the entropy of a term with respect to documents [22], our schemes are supervised methods and measure the entropy of a term with respect to categories. Second, although ig [4] involves the calculation of entropy, it weights the discriminating power of term t by measuring information change of category distribution in a corpus after setting t as a separator. Entropy is to quantify the information of states before and after setting separator t in ig. In contrast, we weight the discriminating power of a term based on categorical specificity/concentration of the term and use entropy to measure the specificity degrees directly. Third, mi [83] measures the mutual dependence between a term and a single category. In contrast, our schemes weight a term based on the term’s distributional concentration across all categories. Although the averaged mi in [83] can be interpreted as the reduction on the uncertainty of category distribution in a corpus due to the category information of a term [14], our schemes weight a term based on the term’s categorical certainty alone, i.e. self-information of a term rather than mutual information. Finally, entropy-based eccd [42] and re [78] are proposed for feature selection. These schemes have the same limitations of feature-selection-based schemes like chi and ig, i.e. overlooking fine-grained distribution information of a term in NC. In contrast, our schemes explore detailed distribution information of a term in each individual category.

Beyond the conceptual differences, we next compare the proposed schemes and existing schemes based on experiments.

Experiments
In this section, we present empirical analyses of different term weighting schemes in TC. Our main aim is to evaluate the effectiveness of our proposed entropy-based schemes and gain deeper insights into term weighting schemes in TC. In particular, we focus on the following two evaluation objectives.

O1: Do the proposed entropy-based term weighting schemes perform better than existing methods in TC tasks?

O2: What are the major characteristics of different schemes and how does each scheme perform in different situations?

In the following, we provide details of datasets, experimental settings and evaluation metrics used in the experiments.

Datasets
Five real-world datasets with different sizes, domains, class numbers and document lengths are used in our experiments.

ReutersFootnote1: In this dataset, some documents have no class labels, while some are labelled with multiple classes. For simplicity, we only consider those labelled with a single class. This remains 7674 documents from the 8 largest categories. Using the ModApte split [2], we partition the data into a training set of 5485 documents and a test set of 2189 documents. The category distribution is highly skewed in Reuters. The largest class has 51.7% of training data, and 75% of classes have less than 4.6% of training data.

SnippetsFootnote2: This dataset consists of 12,340 search snippets which are the results of web search transactions using pre-defined phrases of 8 different categories [60, 88]. For each query phrase put into Google search engine, the top 20 (for queries in training set) or 30 (for queries in test set) snippets from the search results are used to construct the dataset. Compared to other datasets, the documents in Snippets data are short.

NewsgroupsFootnote3: This corpus is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different categories. We use the “bydate" version which is widely used in previous studies [40, 41, 63]. In this version, documents are sorted by date and split into 60% for training and 40% for test.

CadeFootnote4: This dataset is a subset of web pages extracted from the CADE Web Directory, which contains 40,949 web pages, classified into 12 categories by human experts, in Brazilian Portuguese. As there is no standard training/test split for this dataset, we evaluate on this dataset with the fivefold cross-validation method.

OhsumedFootnote5: The Ohsumed corpus includes 50,216 high-quality documents of medical abstracts from the MeSH categories of the year 1991. We study a classic task in previous studies [35, 41], i.e. categorizing the 23 cardiovascular diseases categories. Similar to Reuters corpus, we only use documents labelled with a single class from all 34,389 cardiovascular diseases abstracts. The fivefold cross-validation is also used for this dataset.

Table 8 Data statistics
Full size table
Table 9 Summary of baseline schemes
Full size table
All documents are processed with the common pre-processing: converting words into lower case; punctuation removal, as well as standard stop words and strings containing numbers and URLs; stemming words with the Porter stemming algorithm;Footnote6 erasing the infrequent words that occur less than 5 times. Table 8 shows the resulting statistics of each dataset, including document number (#Doc), training size (#Train), test size (#Test), feature size (#Feature), class number (#Class), average document length (#Length) and the average times of each term used per document (#Time).

Experimental settings
We compare the proposed schemes with nine baseline methods. Some baselines (e.g. tf and tf⋅idf) are the widely used schemes, some (e.g. tf⋅ig and tf⋅eccd) are entropy-related schemes, and the rest (e.g. tf⋅chi, tf⋅rf, iqf⋅qf⋅icf, tf⋅tdf and tf⋅trr) are the state-of-the-art ones that have been shown to achieve good performance in TC. The notations and descriptions of nine baseline schemes are listed in Table 9. To eliminate the effect of document length, we use the cosine normalization in document representation. Given a weight of term t in document d, the normalized weight is w(t,d)′=w(t,d)/∑tw(t,d)2−−−−−−−−−√ [36]. We adopt the wmax(t) globalization method to represent test documents for category-specific schemes, such as tf⋅chi, tf⋅ig, tf⋅eccd, tf⋅rf, tf⋅trr and iqf⋅qf⋅icf. Following past studies [41, 63], we choose the widely used k-NN and SVM as benchmark classifiers. In our implement of k-NN, we use the cosine similarity as the distance measure, and the vote of each neighbour is weighted by the values of similarity [4]. The SVM classifier we use is the LIBLINEARFootnote7 package with default parameters [25]. We tried several different parameters in our preliminary analysis but did not find significant differences between results with default parameters and those with non-default ones. We use the linear SVM since linear model is reported to outperform nonlinear ones in TC [21, 82].

Performance evaluation
Precision and recall are two common metrics to evaluate the performance of classification [61, 72]. However, it is known that a higher precision may be achieved at the cost of a lower recall. To combine those two metrics, F1 score [61] is typically used and it is calculated as:

F1=2⋅precision⋅recallprecision+recall,
(18)
where F1 reaches its peak when precision is equal to recall. Hence, F1 score is an estimation of the breakeven point where precision and recall achieve balanced results [72]. The higher values of F1 denote the better performance. Typically, F1 is measured in two ways, i.e. a mircoaverage F1 (MicroF1) and a macroaverage F1 (MacroF1). The performance on smaller categories is more emphasized by MacroF1 while less by MicroF1.

Results and analyses
In this section, we analyse experimental results and address our evaluation objectives accordingly.

Table 10 Comparison of term weighting schemes with k-NN in multi-class classifications
Full size table
Table 11 Comparison of term weighting schemes with SVM in multi-class classifications
Full size table
Overall comparisons
To address O1, i.e. do the proposed methods perform better than existing methods in TC tasks, we compare the proposed schemes with nine baseline schemes in five TC tasks. Tables 10 and 11 summarize results of each scheme in five multi-class classification tasks with k-NN and SVM, respectively. We run k-NN with different values of k∈[1,100] and select the best results for each scheme. Compared to baseline schemes, the proposed schemes achieve the best results in all datasets with k-NN (Table 10). For example, the proposed tf⋅bdc outperforms the best performing baseline tf⋅trr on Cade by 5% in MicroF1 and 4% in MacroF1. Unlike k-NN that classifies data mainly relying on the distances of feature vectors, SVM trains a set of parameters which combine with the feature vectors to find a decision hyperplane [4]. The comparison results with SVM are thus slightly different from those with k-NN. However, the proposed schemes perform steadily and achieve comparable results across different datasets with SVM (Table 11). These results strongly illustrate the effectiveness of the proposed methods.

Table 12 Results of statistical significance tests
Full size table
To assess whether there are statistically significant differences in the above comparison results, we follow prior studies [41, 63] and use McNemar’s test [20] to determine whether one scheme works better than another. Since there are totally 17 term weighting schemes in question, it is hard to test all of the pairwise schemes. We here select nine well performing schemes, including tf⋅idf, tf⋅tdf, tf⋅rf, tf⋅trr, iqf⋅qf⋅icf, dc, bdc, tf⋅dc and tf⋅bdc. Table 12 lists the results of statistical significance tests on all datasets and classifiers. The results of k-NN are obtained at k=10 where most schemes reach their best performance. Schemes without significant differences are grouped together in one set, and “⋙”, “≫” and “>” denote having better performance at the significance level (i.e. p-value) 0.001, 0.01 and 0.05, respectively. The results show that the proposed schemes consistently have comparable performance to the state-of-the-art schemes and achieve significant improvements over baseline schemes in most cases.

Analyses of individual schemes
We address O2, i.e. what are the major characteristics of different schemes and how does each scheme perform in different situations, by analysing the results of each scheme in detail.

Normalized schemes boost performance on small-sized categories. Compared with dc and tf⋅dc, the balanced bdc and tf⋅bdc generally perform better (see the first 4 rows of the bottom part in Table 10). Moreover, the improvements are more significant when measured in MacroF1, e.g. tf⋅bdc outperforms tf⋅dc by 0.1% in MicroF1 but by 2% in MacroF1. As the performance on small-sized categories is more emphasized by MacroF1, these results show that bdc boosts the performance on small-sized categories, confirming our analysis that bdc effectively avoids the bias towards large-sized categories in Sect. 4.3.

Smoothed schemes improve on a corpus with noise. The smoothed tf⋅sdc only performs better than tf⋅dc on Reuters and Cade, without improvements on other datasets (Table 10), which is unexpected. To further verify this result, we tried other smoothing methods including Jelinek–Mercer, Dirichlet, absolute discounting and two-stage smoothing methods [86]. We have not observed consistent improvements either. Even worse, we find that smoothing methods reduce the performance of dc on some high-quality corpora like Ohsumed. A possible reason is that, apart from noise terms, the importance of some discriminative terms with low frequencies are also diminished after smoothing processing. Thus, we suppose that the performance of sdc is related to the quality of a corpus, and it works better on a corpus with much noise.

Local weights are emphasized in SVM. Compared to dc and bdc alone, tf⋅dc and tf⋅bdc that include the tf factor often perform better in SVM, respectively (Table 11). This implies that the local weight of a term in a document, captured by tf, may be more emphasized in SVM. This finding is supported by evidence that tf⋅rf and tf⋅trr, which include the tf factor, often perform better than iqf⋅qf⋅icf with SVM (except on Newsgroups). In contrast, iqf⋅qf⋅icf performs better with k-NN.

Category-specific schemes work well for SVM. Comparing the proposed schemes with and without a cr factor in SVM (see the last 6 rows of Table 11), those with cr typically perform better than those without cr. This is expected, since the category-specific factor cr is proposed to improve the performance of inherently binary classifiers like SVM. In contrast, the best results with k-NN are often achieved by those without cr (see Table 10). Taken together, these results suggest that the category-specific schemes appear to be more preferable for SVM to achieve better performance, which supports our analysis of cr in Sect. 4.5.

Feature-selection-based schemes perform poorly in multi-class classifications. In line with prior studies [41], feature-selection-based schemes, such as tf⋅chi, tf⋅ig, and tf⋅eccd, perform poorly on most datasets. Since eccd can capture whether a correlation between a term and a category is positive or negative, we see that tf⋅eccd often outperforms tf⋅chi and tf⋅chi across datasets. This aligns with prior evidence that schemes which can reflect the polarity of a correlation (e.g. orFootnote8) often outperform those which cannot (e.g. chi and ig) in TC [17, 41, 52], which also confirms our analyses of chi and ig in Sect. 3.

Large values of k are needed to obtain optimal performance in k-NN using tf⋅idf and its variants. To learn more about the performance of each scheme in mapping documents in the vector space, we compare several well-performing schemes with different values of k in k-NN. Figures 2 and 3 plot the results on Snippets and Newsgroups, respectively. Since the categories in these two datasets are relatively balanced, the results of MicroF1 and MacroF1 have similar trends. We only show the MicroF1 results here. We find that k-NN combining with tf⋅idf and its variant tf⋅tdf performs better gradually as k increases. A possible reason is that tf⋅idf-based schemes tend to assign large weights to the terms with a low document frequency, namely rare terms in a corpus. However, some rare terms may be noise. More neighbours help to reduce the influence of noise terms and improve performance. In contrast, dc and bdc reach their peak values at a small value k=10. This indicates that the proposed schemes map semantically related documents in the vector space more closely.

Fig. 2
figure 2
MicroF1 results of term weighting schemes on Snippets using k-NN with different values of k

Full size image
Fig. 3
figure 3
MicroF1 results of term weighting schemes on Newsgroups using k-NN with different values of k

Full size image
Discussion
In the above experiments, we have shown that the proposed entropy-based term weighting schemes outperform most existing methods in TC tasks. Based on our analyses in Sects. 2 and 3, this is mainly because (i) most existing schemes cannot assign proper weights to indicate a term’s discriminating power in TC, (ii) previous supervised schemes often assign category-specific weights which can hardly represent test documents properly, and (iii) an imbalance between PC and NC in multi-class classifications can reduce the effectiveness of feature-selection-based term weighting schemes. In this section, we provide more direct evidence to support these arguments, so as to better understand why the proposed schemes perform well in the above experiments.

Differences of terms’ weights
We first explore how the proposed schemes differ from existing schemes in weighting a term’s discriminating power in TC. To this end, we compare the distributions of weights generated by different schemes for the same set of terms. Figure 4a shows the correlation between dc and rf weights of each term in Snippets and Fig. 4b shows the correlation between dc and iqf⋅qf⋅icf, where rf and iqf⋅qf⋅icf values are wmax(t) weights. We see that the weights of dc are correlated with those of rf and iqf⋅qf⋅icf (with Kendall’s τ≥0.64). It is thus not surprising that these schemes have comparable performance in the above evaluations. However, for the terms whose dc < 0.5, namely terms occurring in multiple categories, their rf weights are concentrated in the interval [1, 2] and iqf⋅qf⋅icf in the interval [20, 40]. Relative to the ranges of rf and iqf⋅qf⋅icf values, namely [1, 7] and [0, 120], respectively, these weights are much close and can hardly reflect the difference between these terms. This is because rf and iqf⋅qf⋅icf group multiple categories together as NC, without taking the distribution information of a term in individual categories of NC into account. The loss of such information leads to fine-grained difference between terms being neglected. In contrast, the probability that a term occurs in each category is considered in dc, so that the difference on terms’ discriminating power are reflected more effectively. Thus, as shown in Fig. 4, dc weights are more distinguishable one another. This may be one reason why the proposed schemes perform well.

Fig. 4
figure 4
Correlations of weights on Snippets, measured by Kendall’s τ ranking correlations in parentheses

Full size image
Another finding in Fig. 4 is that the rf and iqf⋅qf⋅icf weights of terms whose dc =1, namely terms only occurring in a single category, vary greatly. As rf and iqf⋅qf⋅icf use absolute values of term frequency to calculate weights, some terms that occur in a single category may have small weights due to their low frequencies. For example, “Adidas” which only occurs once in the Sports category of Snippets has a small value of rf=1.58, indicating its low discriminating power. However, these terms are good indications of a category and should be given larger weights. In our schemes, we use the probability of a term distributed in each category rather than its absolute frequency. Thus, the proposed dc assigns “Adidas” a large weight of dc=1, highlighting its high discriminating power. This avoids under-estimation for the importance of these category-specific terms in TC, which may be another reason why our schemes have good performance.

Representations of test documents
As discussed in Example 5, previous supervised term weighting schemes cannot properly represent test documents. To illustrate the effect of this issue on classification performance, we set the training data of Reuters and Snippets as test data, respectively, and examine whether the most similar document of a test document matches itself. We use wmax(t) weights of terms as globalization method.

Table 13 lists the matching accuracy of each scheme. We see that category-specific schemes such as tf⋅chi, tf⋅ig, tf⋅eccd and tf⋅rf often have lower accuracy than category-independent schemes such as tf, tf⋅idf, tf⋅dc and tf⋅bdc. This can be explained as follows. In category-specific schemes, the relative importance of terms in a training document is weighted with respect to a specific category, i.e. a category labelled for the document. In contrast, test documents are represented with wmax(t), and the wmax(t) values of different terms may be with respect to different categories. As shown in Example 5, “math” is less important than “programmer” in d2, with respect to category Computers. However, in the vector of d2^, “math” is instead more important than “programmer” in d2^, across categories Education and Computers. This leads to two different weighting criteria in representing training and test documents and reduces the matching accuracy. In contrast, category-independent schemes, e.g. tf⋅idf, tf⋅dc and tf⋅bdc, use identical weights to represent training and test documents, which enables documents being represented in the same weighting criterion.

Effect of imbalanced categories
Table 13 Matching accuracy of different term weighting schemes
Full size table
As discussed in Sect. 3, a key factor that limits the performance of feature-selection-based schemes is deemed to be unbalanced sizes between PC and NC in multi-class classification tasks, particularly in a corpus with a large number of categories. If this is true, these schemes are expected to perform better in cases where PC and NC are more balanced, such as binary classifications with two categories of a similar size.

Table 14 Comparison of term weighting schemes in binary classifications
Full size table
To verify this hypothesis, we compare the performance of each feature-selection-based scheme in binary classification with that in multi-class classification. Since there is a relatively large number of categories and each category has a similar number of documents in the Newsgroups dataset, we build a binary-classification dataset based on two categories of Newsgroups (called Newsgroups2), i.e. contains 994 documents from category rec.sport.baseball and 997 documents from category rec.sport.hockey. Table 14 shows the results of different schemes on the binary classification with k-NN and SVM classifiers. We see that the proposed bdc achieves the best results, illustrating that the proposed schemes not only perform well in multi-class classifications, but also in binary classifications. More importantly, we find that most feature-selection-based schemes perform relatively better in binary classifications than in multi-class classifications. For example, using tf as a baseline, we see that tf⋅chi and tf⋅ig generally perform better than tf on Newsgroups2. In contrast, as shown in Tables 10 and 11, tf performs better on Newsgroup. This finding confirms our hypothesis that an imbalance between PC and NC can reduce the performance of feature-selection-based schemes.

Conclusion and future work
Term weighting is an effective process to improve TC performance. In this paper, we explore the advantages and weaknesses of the state-of-the-art term weighting schemes in TC. Moreover, we explore to measure the discriminating power of a term by its global distributional concentration across categories in a corpus and propose a series of supervised entropy-based term weighting schemes. Our experimental results demonstrate the effectiveness of the proposed schemes. At the end of this paper, we answer the questions raised in Sect. 1 based on our findings.

1. Why do some supervised term weighting schemes, which have sound mathematical foundations and are expected to have good performance in theory, instead perform poorly in practice?

Based on our analyses, this is mainly because:

Some supervised schemes such as chi and ig fail to reflect whether a term has a positive or negative correlation with a category, and the discriminating power of terms with a negative correlation is often over-estimated.

The imbalance between PC and NC in multi-class classifications can reduce the effectiveness of these schemes in reflecting terms’ discriminating power in TC.

Most supervised schemes assign category-specific weights to terms, which can hardly properly represent test documents whose category labels are unknown.

2. Do the proposed entropy-based schemes provide effective measures of the discriminating power of a term in TC?

In a series of experiments with different settings, we find that the performance of a term weighting scheme is closely related to the datasets, classifiers and classification types, which aligns with previous studies [41]. It is thus difficult to find a single scheme that consistently performs best in various circumstances. However, the experimental results show that the proposed entropy-based schemes such as bdc and cr⋅bdc often outperform most state-of-the-art schemes. Although some previous methods perform better than our methods in some cases, the statistical significance tests have not shown that these improvements are significant. Thus, we argue that the proposed entropy-based schemes reflect the discriminating power of a term in TC more effectively than most previous schemes.

3. What are the characteristics of different types of schemes and how to select a proper term weighting scheme for a given TC task?

In line with previous studies [41], we find that there are many factors influencing TC performance, such as classifiers in use, benchmark datasets and the type of a classification task. Specifically, we have the following findings that may provide hints on how to select a proper scheme for a given TC task.

When tf⋅idf or its variants are used with k-NN, more neighbours (i.e. larger k) are needed to alleviate the influence of noise and achieve optimal performance.

Feature-selection schemes, e.g. tf⋅chi, perform better in binary classifications than in multi-class classifications.

Normalizing the frequencies of a term in its relevant categories (i.e. the processing in bdc) effectively improves the classification performance on small-sized categories when a corpus has a skewed category distribution.

Smoothing the frequencies of a term in its relevant categories (i.e. the processing in sdc) can alleviate the influence of noise terms. However, the performance of this processing is highly related to the quality of a corpus, i.e. the amount of noise existing in the corpus.

The local weight of a term in documents (reflected by tf) is more emphasized in SVM, and hence, the schemes with tf often perform better than those without tf in SVM.

The performance of k-NN is more sensitive to the term weighting schemes in use than that of SVM.

Category-specific schemes (e.g. cr⋅bdc) are often more preferable for inherently binary classifiers such as SVM than multi-class classifiers such as k-NN. In contrast, category-independent schemes (e.g. bdc) are more preferable for multi-class classifiers than binary classifiers.

There are several directions for the future work. First, compared with the big data generated in real-world applications, the sizes of datasets in our experiments are relatively small. We intend to verify whether our methods can achieve good performance in huge text collections. Second, although we conducted experiments with different settings including term weighting schemes, corpora, classifiers and classification types, TC tasks involve setting a much larger number of additional parameters, such as stop-word removal, feature selection and various parameters used in a classifier model. Moreover, real-world TC applications often seek to optimize multiple competing objectives, usually precision and recall, leading to a trade-off curve like receiver operating characteristics (ROC) curves. Previous studies have shown that multi-objective evolutionary algorithms (MOEAs) provide an effective approach to find parameters that simultaneously optimize multiple objectives for TC tasks from a large parameter space and locate the Pareto front, namely a surface that describes the trade-off among different objectives [28]. Thus, future work needs to investigate the use of an MOEA to estimate optimal model parameters in TC tasks, so as to gain a deeper understanding of the behaviour of a solution (e.g. a term weighting scheme) and offer the human operator a set of solutions adaptable to different situations. Finally, existing term weighting schemes have largely focused on one-hot representations of textual documents, while recent studies have extended this line of research to distributed representations [3, 53, 85]. Thus, future work will also explore if we can incorporate the proposed entropy-based term weighting methods to improve the performance of embedding methods.