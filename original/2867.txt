Handling missing values and large-dimensional features are crucial requirements for data-driven fault diagnosis systems. However, most intelligent data-driven diagnostic systems are not able to handle missing data. The presence of high-dimensional feature sets can also further complicate the process of fault diagnosis. This paper aims to devise a missing data imputation unit along with a dimensionality reduction unit in the pre-processing module of the diagnostic system. This paper proposes a novel pooling strategy for missing data imputation (PSMI). This strategy can simplify complex patterns of missingness and incrementally update the pool. The pre-processing module receives incomplete observations, PSMI estimates missing values, and, then, the dimensionality reduction unit transforms completed observations onto a lower-dimensional feature space. These transformed observations are then fed as inputs to the fault classification module for decision making and diagnosis. This diagnostic scheme makes use of various state-of-the-art missing data imputation, dimensionality reduction and classification algorithms. This enables a comprehensive comparison and allows to find the best techniques for the sake of diagnosing faults in the Tennessee Eastman process. The obtained results show the effectiveness of the proposed pooling strategy and indicate that principal component analysis imputation and heteroscedastic discriminant analysis approaches outperform other imputation and dimensionality reduction techniques in this diagnostic application.

Access provided by University of Auckland Library

Introduction
Fault diagnosis plays an important role in improving the performance and increasing the safety and reliability of industrial processes [26]. Faults are unexpected events that avoid a system or its units to deliver the designated set of services. Faults are known to cause poor performances in equipment or cause them to operate adversely, which can lead to production stoppage and financial losses [9, 18]. Hence, a fast and accurate diagnosis of faults is of paramount importance for industrial processes [6,7,8, 23, 30].

Fault diagnosis techniques can be divided into two categories [26]: data-driven and model-based methods. Model-based approaches use quantitative models and explicit equations to estimate the states or parameters of the system. However, given a complex system, it is hard to identify an accurate model, which follows the process behaviour in real operational conditions [26]. Data-driven diagnostic schemes usually use a set of observed features, in order to train a diagnostic model [14]. Although these data-driven diagnostic schemes tend to be accurate, fast and less complex, their performance is highly dependent on the quality and representativeness of the collected data [10, 22].

Industrial processes are usually being monitored with various sensors and equipment. This often generates huge amount of data, which result in the curse of dimensionality [4, 16]. Collecting noisy and redundant features usually decreases the diagnostic performance. Mapping the data onto a lower-dimensional feature space is necessary, therefore, for a proper diagnosis of faults. It also reduces the required time and space, lowers the complexity of the decision-making process and increases the accuracy of the diagnostic system, while preserving the variability of the original data to a great extent [5, 34]. Dimensionality reduction can be divided into two categories [42]: feature selection and feature extraction. Feature selection aims to remove redundant and irrelevant features from the original data, in order to reduce the dimension [16]. Feature extraction aims to transform the original data onto a lower-dimensional feature space. This paper makes use of feature extraction techniques in order to reduce the dimension of the feature space and the computational time, while increasing the accuracy of the diagnostic system [5].

Collecting incomplete observations is an unavoidable issue in the online industrial processes. This is due to data transmission errors, incorrect measurements and faulty sensors [15, 25, 31, 32]. Most of the intelligent fault classifiers are designed to work with complete observations and are not suited to handle incomplete ones. Consequently, the missing observations turn out to be a serious challenge for the fault classifiers, causing negative impacts on the decision-making process [29, 39]. This paper intends to deal with diagnosing faults under incomplete scenarios.

The two most commonly used methods of handling missing data are discarding and imputation. Discarding, which is the most common method, ignores missing observations. However, it leads to loss of critical information and dependencies, which may result in inaccurate results. Imputation, on the other hand, replaces the missing observations with the estimated values [27, 33]. Missing data imputation techniques are classified under two major categories, as single and multiple imputation techniques [33]. Multiple imputation techniques are more complex and computationally expensive and, thus, not suitable for online monitoring applications. Single imputation techniques are usually simpler to implement and fast, making them an ideal choice for online monitoring applications [33].

Data-driven diagnostic systems are indeed predictive models that are constructed by means of data collected from the industrial systems. The absence of complete data hampers the diagnostic performance in both training and test phases. Incomplete observations during the training (offline) phase are usually treated by means of complete observations. However, the main issue is during the test (online phase), where the observations become available one by one or in different batches for decision making. The data-driven diagnostic systems are usually cannot handle incomplete observations and thus the state of the systems is under question. The presence of missing data during the test/online phase can hamper diagnostic performance, no matter observations emerge in a batch-wise fashion or a one-by-one manner. In the former, the presence of a large number of missing values in the batch of data and their complex missingness pattern can complicate missing estimation. The latter suffers from the lack of donors for missing estimation since an incomplete observation emerges individually at once.

This paper proposes a novel pooling strategy for missing data imputation (PSMI), which aims to reduce the complexity of the missingness structure in the batch of data as well as the imputation error. PMSI forms a pool of donors to estimate missing values whenever an incomplete observation emerges during the online/test phase. During the online/test phase, PMSI enables the diagnostic system to treat missing data (one-by-one fashion) by resorting to the pool of donors and improves the imputation performance (batch-wise fashion) by simplifying complex patterns of missingness that may exist in the batch of data. Any arbitrary and random pattern of missingness can be transformed to a univariate missing pattern that can be simply treated by means of any missing data imputation technique. The proposed strategy also incrementally updates the pool and allows imputing the missing observations by resorting to both complete observations and recently imputed ones.

This paper also proposes a diagnostic scheme, which contains two main modules for the pre-processing and the decision-making processes. The former module includes several state-of-the-art missing data imputation and dimensionality reduction (DR) techniques. This module generates complete sets of observations with a lower number of features, in order to reduce the dimension of the data and increase the diagnostic accuracy. Thus, the proposed scheme enables a comprehensive comparison, which results in finding the best combination of techniques for diagnosing faults with missing observations in the Tennessee Eastman process.

Various state-of-the-art missing data imputation techniques are used in the pre-processing module of the diagnostic system to treat incomplete observations. These missing data imputation techniques are compared together in terms of normalized root mean square (NRMS) error. The attained results by each missing data imputation technique show the efficiency of the proposed strategy. PMSI is also compared with an un-pooled strategy, in which incomplete observations are accumulated, and, then, treated by missing data imputation techniques. The attained results show the efficiency of PMSI in terms of NRMS and the performance of the decision-making module. Besides, PMSI enables online decision making, while the un-pooled strategy cannot immediately make a decision and requires to collect a certain number of observations, which results in delay in decision making that is a very important issue in online diagnostic applications.

The remainder of the paper is structured as follows: Sect. 2 of the paper briefly presents the design of the diagnostic system. Section 3 presents the selected techniques for missing data imputation and dimensionality reduction. Section 4 discusses and compares the experimental results. Conclusions are presented in Sect. 5.

Design of the diagnostic system
Most intelligent diagnostic classifiers rely on the complete datasets to learn and diagnose process faults [32]. However, fault classification becomes a challenging task in the presence of missing data. On the other hand, incomplete features are often being collected due to failures in sensors, transmission devices and system process. Missing data mechanisms can be broadly classified under three categories, namely Missing at Random (MAR), Missing Not at Random (MNAR) and Missing Completely at Random (MCAR) [35]. This paper focuses on the MCAR structure, that is usually generated due to failures in the sensors and data collection process.

Diagnostic accuracy is highly dependent on the availability of useful data, and, thus, presence of missing data can negatively impact on the diagnostic performance [32]. Therefore, to improve the diagnostic performance, there is a need to estimate missing values prior to fault classification.

Figure 1 illustrates the workflow of the diagnostic system. The proposed diagnostic model contains two modules for pre-processing and fault classification. The pre-processing module itself contains two units for missing data imputation and dimensionality reduction, which aim to estimate missing values and reduce the feature dimension, respectively. In the first unit, this work proposes a pooling strategy for missing data imputation (PSMI).

Fig. 1
figure 1
Block diagram of the diagnostic system. The incomplete observations are imputed through PSMI and, consequently, transformed onto a lower-dimensional feature space by means of the dimensionality reduction unit and, then, fed into the fault classification module for decision making

Full size image
The collected data is split into training and test subsets using a tenfold cross-validation strategy as shown in Fig. 1. In order to evaluate the performance of the diagnostic system, an incomplete scenario is generated by inducing missing values in various observations of the test subsets, in a completely random order. The pattern of missingness, which is created in the test subset, is usually an arbitrary pattern, which further complicates the imputation process. PSMI is proposed in order to reduce the impact of the pattern of missingness. In this strategy, as illustrated in Fig. 1, the complete set of observations of the training subset are forming a pool, that is used during the imputation process. The test subset contains missing features with an arbitrary pattern of missingness. However, test observations are fed into the pool one by one. A test observation without any missing feature is automatically added to the pool. If a test observation contains any missing feature, the imputation technique makes use of the pool of observations and the current missing observation (i.e. the target observation) to estimate the missing features of the target observation and, thus, creates a complete observation. This imputed observation is then added to the pool for the future use, while also being fed to the subsequent unit for dimensionality reduction. The dimensionality reduction unit transforms observations onto reduced feature space and, then, sends reduced observations to the fault classification module for decision making.

The imputation is performed whenever a new incomplete observation is added to the pool. PSMI uses three state-of-the-art algorithms for estimation of missing data including extreme learning machine imputation (ELMI), k-nearest neighbours imputation (kNNI) and principal component analysis imputation (PCAI). PSMI helps in transforming any arbitrary pattern of missingness that may appear in the test subset into a simpler pattern of missingness (i.e. univariate or monotone). Algorithm 1 describes the pooling strategy.

The pool P is initialized using the training subset Xm×n, where m stands for the number of observations and n stands for the number of features. The test subset Yo×n contains o observations. Each of the test observation yt can be split into complete yobst and incomplete ymist vectors, where t=1,…,o.

Each complete observation yt is fed into the pool. For an incomplete observation, the imputation module splits the pool subset P into observed Pobs and target Pmis features, with the number of target features identical to the missing features. The observed features are fed as the training inputs to one of the imputation techniques Ωw, where w=1,2,3, along with the target features Pmis as the targets to train a prediction model P^. This step is defined inside a function, called CreateModel. The next step, which is defined inside another function, called PredictData, uses that predictive model P^ and complete features yobst to estimate the missing features yt^mis. The complete and estimated features are combined together to create a complete subset.

figure e
The completed subset is then fed into the dimensionality reduction (DR) unit. High-dimensional features can often lead to unreliable predictions and a large computational time. This requires the subset to be transformed onto a more informative feature subset with a lower dimension. Here, the DR module uses two state-of-the-art supervised methods, namely neighbourhood components analysis (NCA) and heteroscedastic discriminant analysis (HDA), and two unsupervised methods, namely extreme learning machines (ELM) and principal components analysis (PCA). These state-of-the-art DR methods are selected and used in the DR unit for the sake of comparison. Several low-dimensional models are generated through different DR methods by means of the training subset. Each test observation is applied to these models, in order to be transformed onto a lower-dimensional feature space. These completed observations of the lower dimension are then fed to the fault classification module for the decision making. This module also contains four classifiers, namely extreme learning machine (ELM), k-nearest neighbours (kNN), linear discriminant analysis (LDA) and heteroscedastic discriminant analysis (HDA). These state-of-the-art classifiers are selected and used in the fault classification module for the sake of comparison. The fault classification performance is analysed through the accuracy (ACC) and the area under the curve (AUC) measures [28].

Diagnostic scheme
The proposed scheme is designed for fault classification from high-dimensional features under incomplete scenarios. It contains pre-processing and decision-making modules. The pre-processing module uses the pooling strategy for missing data imputation (PSMI) along with the dimensionality reduction (DR) unit. They are designed into a serial configuration to impute missing values and return complete observations and, then, reduce the data into a lower dimension feature space. This provides the decision-making module with complete and informative observations of the lower dimension.

Missing data imputation
The pooling strategy for missing data imputation (PSMI) consists of three imputation techniques that uses the pool of observations to train a model that can be used to estimate missing scores in the target observation. Once the imputation is performed, the completed observation is appended into the pool.

Principal component analysis imputation (PCAI)
PCAI makes use of the regularized iterative PCA algorithm to impute missing scores [12, 19]. The algorithm initially estimates the missing values with the feature means and, then, applies PCA on the completed dataset. Thereafter, it imputes the missing values with the regularized reconstruction with a predefined order [19]. These steps are repeated until convergence criterion has been met.

k-Nearest neighbour imputation (kNNI)
kNNI is based on the k-nearest neighbour algorithm, which replaces the missing feature values with a weighted mean of the k-nearest neighbour features. The weights for the neighbours are inversely proportional to the similarly measure, like Euclidean distances or Pearson correlation, from the neighbouring features [3, 38].

Extreme learning machine imputation (ELMI)
ELMI estimates missing values by means of the extreme machine learning regression algorithm [24]. The algorithm trains a regression model using the complete features as inputs and the missing features, target features, as targets [17]. Using the trained model, the algorithm regresses the complete features of the incomplete subset to impute the missing values of the incomplete, target, features.

Dimensionality reduction
The DR unit consists of four state-of-the-art feature extraction methods. These techniques calculate the transformation matrix W [41]. This transformation matrix W, applies linear transformation in order to project the complete dataset X onto a lower-dimensional feature space, while retaining maximum knowledge about the original dataset.

Heteroscedastic discriminant analysis (HDA)
HDA is a linear dimensionality reduction algorithm, which is proposed for datasets with the normal distribution between each class, allowing for the heteroscedasticity of the given data [21].

Considering an input data of X=X1∪X2∪⋯∪Xl belonging to l classes of n observations, such that Xi={x1i,x2i,…,xni}, the multi-class HDA criterion aims to attain the matrix W, that maximizes the following function:

ζHDA(W)=∑i=1k−1∑j=i+1kρiρjtr{(WσωWT)−1WSσ1/2ω[(σ−1/2ωσijS−1/2ω)−1/2×σ−1/2ωσηijσ−1/2ω(σ−1/2ωσijσ−1/2ω)−1/2+1πiπj(log(σ−1/2ωσijσ−1/2ω)−πilog(σ−1/2ωσiσ−1/2ω)−πjlog(σ−1/2ωσjσ−1/2ω))]σ1/2ωWT}
(1)
where ρi stands for the a priori probability of class i, σi stands for the within-class covariance matrix of class i, μi stands for the mean vector of class i, μ stands for the estimated overall mean, σω stands for the average within-class scatter matrix, σϵ stands for the between-class scatter matrix, σij stands for the average pairwise within-class scatter matrix, σϵij stands for the pairwise between-class scatter matrix, πi and πj stand for the relative priors [21], which can be calculated using the following set of equations:

ρi=|Xi|/|X|σω=∑i=1kρiσiσϵ=∑i=1k(μi−μ)(μi−μ)Tμ=∑i=1kρiμiσϵij=(μi−μj)(μi−μj)Tσij=πiσi+πjσjπi=ρi/(ρi+ρj)πj=ρj/(ρi+ρj).
W can be calculated by solving the eigenvalue decomposition of the following equation and, then, selecting the largest m eigenvectors.

σHDA=∑i=1k−1∑j=i+1kρiρjtr{σ−1ωσ1/2ω[(σ−1/2ωσijσ−1/2ω)−1/2×σ−1/2ωσϵijσ−1/2ω(σ−1/2ωσijσ−1/2ω)−1/2+1πiπj(log(σ−1/2ωσijσ−1/2ω)−πilog(σ−1/2ωσiσ−1/2ω)−πjlog(σ−1/2ωσjσ−1/2ω))]σ1/2ω}.
(2)
Neighbourhood components analysis (NCA)
NCA is a dimensionality reduction algorithm, which is proposed for learning a Mahalanobis distance measure that is used in the kNN classification algorithm [13]. This algorithm is restricted to learning the Mahalanobis distance metrics that can be represented by the symmetric positive semi-definite metrics and estimated using the inverse square roots [13].

This algorithm makes use of a differentiable cost function, that is based on a stochastic neighbour assignment in the transformed space, which can be considered an effective measure compared to estimating the actual leave-one-out classification error [13].

The probability ρij that the jth point is selected by the ith point as a neighbour can be computed as follows:

ρij=exp(−∥Wxi−Wxj∥2∑k≠iexp(−∥Wxi−Wxk∥2
(3)
where W stands for the transformation matrix. The probability ρi stands for the correct classification of the point i is given by the following equation:

ρi=∑j∈liρij
(4)
where li stands for all points of the same class i. This algorithm requires to obtain the matrix W, which can maximize the number of correctly classified observations, as follows:

ζNCA(W)=∑i∑j∈liρij=∑iρi.
(5)
The gradient-based method along with the gradient operator of Eq. (6) is used to maximize ζNCA(W):

∇ζNCA∇W=2W∑i{ρi∑kρikxikxTik−∑j∈liρijxijxTij}
(6)
where xij=xi−xj.

Principal component analysis (PCA)
PCA is a popular unsupervised linear dimensionality reduction technique. It seeks for a projection that best represents the data [2, 40].

Considering an input data of X={x1,x2,…,xn}, the projected data is given by yi=Wxi for i∈(1,…,n). The scatter matrix is calculated by the following equation:

σ=∑j=1n(xj−μμ)(xj−μμ)T
(7)
where

μμ=1n∑j=1nxj.
(8)
σ is decomposed into ψ and Λ, where ψ=(ψ1,ψ2,…,ψd) stands for the eigenvectors and Λ=diag(λ1,λ2,…,λd) stands for the eigenvalues. The eigenvectors and eigenvalues are arranged so that λ1>λ2>⋯>λd. In order to reduce the data to a lower dimension m, the matrix W can be reformulated as W=[(ψT1,ψT2,…,ψTm)]T.

Extreme learning machine (ELM)
ELM is a generalized form of the single hidden layer feedforward network (SLFN) [17]. Considering an input data of X={x1,x2,…,xn}, b hidden nodes are generated to approximate the input–output relation among these n pairs of input observations with a zero error. This can be done by resorting to random weights and bias, (υi,νi), and the output weight vector Γi, which satisfy the following:

∑i=1bΓif(υi⋅xj+νi)=yj+εj,j=1,…,n
(9)
where ε stands for the error or noise, b stands for the number of hidden nodes, f stands for the hidden layer activation function and yj stands for the output vector at the jth row. This can be reformulated as follows:

GΓ=Υ
(10)
where

G=⎡⎣⎢⎢g(x1)⋮g(xn)⎤⎦⎥⎥=⎡⎣⎢⎢f(υ1⋅x1+ν1)⋮f(υ1⋅xn+ν1)⋯⋱⋯f(υb⋅x1+νb)⋮f(υb⋅xn+νb)⎤⎦⎥⎥n×b
(11)
Γ=⎡⎣⎢⎢ΓT1⋮ΓTb⎤⎦⎥⎥b×o
(12)
and

Υ=⎡⎣⎢⎢yT1⋮yTn⎤⎦⎥⎥=⎡⎣⎢⎢y11⋮yn1⋯⋱⋯y1o⋮yno⎤⎦⎥⎥n×o.
(13)
G stands for the randomized matrix of the hidden layer output, o stands for the number of target features and Υ stands for the training data target matrix. The ith column of the matrix G stands for the ith hidden node output w.r.t. the set of n inputs, {x1,x2,…,xn} and the jth row of the matrix G stands for hidden layer feature mapping w.r.t. the jth input xj, i.e. g(xj)={f(υ1∗xj+ν1)),…,f(υL∗xj+νL)}.

For the purpose of dimensionality reduction [20], the input bias is set to 0 (i.e. νi=0) and the orthogonal random weight matrix υ, is calculated using a modified version of Eq. 11 where υ={υ1,υ2,…,υb} and υTυ=I. W can be calculated using the following equation:

W=Γ=υTψψT
(14)
where ψ stands for the eigenvectors of the covariance matrix XTX [20].

Fig. 2
figure 2
Tennessee Eastman process workflow [36]

Full size image
Fault classifiers
The fault classification task is performed using four state-of-the-art data-driven classifiers, namely heteroscedastic discriminant analysis (HDA) [21], linear discriminant analysis (LDA) [11, 37], k-nearest neighbour (kNN) [1] and extreme learning machines (ELM) [17]. All of these intelligent classifiers are tuned to handle classification of multi-class datasets.

Experimental results
In this paper, a pooled missing data imputation strategy and multi-class dimensionality reduction methods are used to form the pre-processing module of the diagnostic system. This proposed diagnostic technique is validated using an industrial process, called Tennessee Eastman (TE) process [6]. The Tennessee Eastman process is a popular industrial process, that has been extensively used to evaluate designed diagnostic and control systems [6]. The TE process consists of an exothermic two-phase reactor, a flash separator and a reboiled striper, as shown in Fig. 2. The simulated Tennessee Eastman process contains 1 normal condition and 21 faulty conditions.

In this paper, various sets of observations from the normal state and four faulty states, i.e. faults 1, 2, 6 and 7, are selected to evaluate the proposed diagnostic system. The description of the selected faulty classes is reported in Table 1. The training subset contains 500 observations from the normal state and 480 observations from each faulty states. Each observation contains 52 features. The testing subset contains 960 observations with 52 features.

Table 1 Description of the process faults and normal state
Full size table
The incomplete test subset is imputed using both pooled and un-pooled strategies. Table 2 shows the comparison between normalized root mean square (NRMS) values based on the imputed dataset generated by the different missing data imputation techniques. It can be seen that the pooled strategy generates a slightly lower NRMS value, which indicates that the imputed scores are closer to the real scores. The major benefit of the pooled strategy is, however, the reduction in complexity of the missing data imputation technique achieved through univariate and monotone patterns.

Table 2 Comparison between pooled (PMSI) and un-pooled missing data imputation strategies in terms of NRMS
Full size table
The incomplete observations are imputed using both pooled and un-pooled strategies. Table 3 shows the comparison between the averaged ACC and AUC values attained by classifiers through each imputation technique and strategy. It can be seen that the pooled (PMSI) strategy outperforms the un-pooled strategy in terms of both ACC and AUC, which indicates that the imputed scores by means of PMSI can improve the diagnostic performance. The obtained results confirm the efficiency of PMSI in handling random missing features in online diagnostic applications.

Table 3 Comparison between pooled (PMSI) and un-pooled missing data imputation strategies in terms of ACC and AUC of the decision-making module
Full size table
Fig. 3
figure 3
The distribution of the accuracy measures attained through each dimensionality reduction method combined with missing data imputation methods

Full size image
The completed and transformed observations are used as inputs for the purpose of fault classification. The classification performance is measured and compared in terms of accuracy and receiver operating characteristic curve for the different techniques. Accuracy (ACC) can be termed as the degree of closeness of prediction of the normal and faulty classes to the original classes. The receiver operating characteristic (ROC) curve is a plot that illustrates the classification performance by varying the discrimination threshold. The curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) of each fault class. The area under the ROC curve (AUC) is selected as a performance measure. For the multi-class dataset, ROC curve is generated for every pair of the normal and faulty classes and the average AUC is then calculated.

Figure 3 illustrates the distribution of the performance measures, in terms of accuracy, attained by each dimensionality reduction method. The attained accuracy measures through each dimensionality reduction method are shown by blue circles, where the average of those measures attained through each imputation method are connected together by solid lines with distinct markers. This enables to compare the performance of missing data imputation methods combined with each dimensionality reduction method. Figure 3 shows that PCAI outperforms other imputation methods, where combined with most of dimensionality reduction techniques. This figure also shows a clear advantage of using HDA as a dimensionality reduction (DR) technique along with all three missing data imputation techniques. Both kNNI and PCAI perform closely to each other, however, PCAI results in a better performance measure on average.

Fig. 4
figure 4
The distribution of the accuracy measures attained by each fault classifier through missing data imputation methods

Full size image
Figure 4 illustrates the distribution of the performance measures, in terms of accuracy, attained by each fault classifier (FC). The attained accuracy measures through each fault classifier are shown by blue circles, where the average of those accuracy measures attained by each missing data imputation method are connected together by solid lines with distinct markers. This enables to compare the performance of each missing data imputation method combined with each fault classifier. Figure 4 shows that PCAI outperforms other imputation methods, where combined with most of the fault classifiers. The figure shows that HDA outperforms other classifiers, regardless of the missing data imputation technique that is used to estimate the missing values a priori. PCAI shows a slight advantage over kNNI for missing data imputation technique when combined with all four classifiers. HDA outperforms other selected dimensionality reduction techniques since it makes use of the linear transformation and directed distance matrix. HDA uses the weighted sum of the corresponding directed distance matrices instead of the well-known between-class scatter matrix. On the other hand, PCAI outperforms other missing data imputation techniques since it considers the global similarity among all features.

Figure 5 illustrates the distribution of the performance measures, in terms of accuracy, attained by each fault classifier. The attained accuracy measures through each fault classifier are shown by blue circles, where the average of those measures attained by each dimensionality reduction method are connected together by solid lines with distinct markers. This enables to compare the performance of each dimensionality reduction method combined with each fault classifier. Figure 5 shows that HAD and NCA outperform other imputation methods, where combined with most of classifiers. The figure shows HDA generates the best overall performance as a fault classifier combined with all four dimensionality reduction techniques. Both PCA and HDA are shown to generate a steady performance when combined with any classifier. NCA and ELM show huge improvement in performance when combined with HDA as a fault classifier.

Fig. 5
figure 5
The distribution of the accuracy measures attained by each fault classifier through dimensionality reduction methods

Full size image
Fig. 6
figure 6
Distribution of the diagnostic performance attained through each imputation technique

Full size image
Figure 6 illustrates the performance obtained through each missing data imputation technique in terms of diagnostic accuracy and AUC. The boxes in the figure depict the distribution range of accuracy or AUC values between the first and third quartile, the lines inside each box depict the median value and the dash lines depict the outlier range. Panel (a) shows PCAI slightly outperforms other imputation techniques in terms of accuracy. Imputation by PCAI for certain sets of observations results in 100% diagnostic performance. On the other hand, ELMI produces the largest range of variation in diagnostic performance among all imputation techniques. Panel (b) shows PCAI slightly outperforms compared to other imputation techniques in terms of AUC.

Fig. 7
figure 7
Distribution of the diagnostic performance attained through each dimensionality reduction technique

Full size image
Figure 7 illustrates the diagnostic performance obtained through each dimensionality reduction technique in terms of accuracy and AUC. Panels (a) and (b) show that the supervised techniques (NCA and HDA) outperform the two unsupervised (ELM and PCA) dimensionality reduction techniques. HDA results in the best performance as a dimensionality reduction tool and also causes the least variation in diagnostic performance.

Figure 8 illustrates the distribution of the diagnostic performance obtained through each fault classifier in terms of accuracy and AUC. The figure reiterates that HDA obtains the best performance and the most stable results compared to other classifiers in terms of accuracy and AUC, as seen in both panels.

Fig. 8
figure 8
Distribution of the diagnostic performance attained by each fault classifiers in terms of accuracy

Full size image
Conclusion
This paper proposes an efficient scheme for fault diagnosis with incomplete and high-dimensional features. The proposed scheme contains three modules, for missing data imputation, dimensionality reduction, and fault classification. This work proposes a missing data imputation strategy, which uses a pooling mechanism to create a complete set of features and alleviates complex missingness structures into simple structures, such as univariate and monotone structures. The fault classification performances are evaluated both in terms of accuracy and area under curve performance measures. The obtained results confirm the efficiency of the scheme in handling random missing features in an online industrial application. A comparative study of the results illustrates that, in terms of both accuracy and area under curve, PCAI generates the best performance in the missing data imputation, while HDA outperforms other competitors as both dimensionality reduction and fault classification tools. Thus, it can be concluded that the combination of PCAI-HDA will be an ideal tool for the proposed diagnostic system.