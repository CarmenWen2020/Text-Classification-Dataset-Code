Model complexity is a fundamental problem in deep learning. In this paper, we conduct a systematic overview of the latest studies on model complexity in deep learning. Model complexity of deep learning can be categorized into expressive capacity and effective model complexity. We review the existing studies on those two categories along four important factors, including model framework, model size, optimization process, and data complexity. We also discuss the applications of deep learning model complexity including understanding model generalization, model optimization, and model selection and design. We conclude by proposing several interesting future directions.

Access provided by University of Auckland Library

Introduction
Mainly due to its superior performance, deep learning is disruptive in many applications, such as computer vision [40], natural language processing [55], and computational finance [91]. At the same time, however, a series of fundamental questions about deep learning models remain, such as why deep learning can achieve substantially better expressive power comparing to classical machine learning models, how to understand and quantify the generalization capability of deep models, and how to understand and improve the optimization process. Model complexity of deep learning is a core problem and is related to many those fundamental questions.

Model complexity of deep learning is concerned about, for a certain deep learning architecture, how complicated problems the deep learning model can express [15, 44, 70, 89]. Understanding the complexity of a deep model is a key to precisely understanding the capability and limitation of the model. Exploring model complexity is necessary not only for understanding a deep model itself, but also for investigating many other related fundamental questions. For example, from the statistical learning theory point of view, the expressive power of a model is used to bound the generalization error [69]. Some recent studies propose norm-based model complexity [60] and sensitivity-based model complexity [76, 81] to explore the generalization capability of deep models. Moreover, detecting changes of model complexity in a training process can provide insights into understanding and improving the performance of model optimization and regularization [44, 74, 89].

The investigation of machine learning model complexity dates back to decades ago. A series of early studies in 1990s discuss the complexity of classical machine learning models [16, 20, 21, 98]. One representative model is decision tree [19], whose complexity is always measured by tree depth [20] and number of leaf nodes [16]. Another frequent subject in model complexity analysis is logistic regression, which is the foundation of a large number of parameterized models. The model complexity of logistic regression is investigated from the perspectives of Vapnik–Chervonenicks theory [26, 96], Rademacher complexity [46], Fisher Information matrix [21], and the razor of model [6]. Here, the razor of model is a theoretical index of the complexity of a parametric family of models comparing to the true distribution. However, deep learning models are dramatically different from those classical machine learning models discussed decades ago [70]. The complexity analysis of classical machine learning models cannot be directly applied or straightforwardly extended to deep models.

Recently, model complexity in deep learning has attracted more and more attention [13, 60, 70, 78, 81, 89]. However, to the best of our knowledge, there is no existing survey on model complexity in deep learning. The lack of survey on this emerging and important subject motivates us to conduct this survey of the latest studies. In this article, we use the terms “deep learning model” and “deep neural network” interchangeably.

There are prolific studies on complexity of classical machine learning models decades ago, which are summarized in excellent surveys [20, 21, 61, 93]. In the following we very briefly review the complexity of several typical models, including decision tree, logistic regression and Bayesian network models. We also discuss the differences between the model complexity of deep neural networks and that of those models.

There are relatively well-accepted standard measurements for complexity of decision trees. The complexity of a decision tree can be represented by the number of leaf nodes [16, 61] or the depth of the tree [20, 98]. Since a decision tree is constructed by recursively splitting the input space and proposing a local simple model for each resulting region [71], the number of leaf nodes in principle uses the number of resulting regions to represent the complexity of the tree. The depth of a decision tree as the complexity measure quantifies in the worst case the number of queries needed to make a classification [20]. A series of studies [16, 61] investigate tree optimization based on the accuracy/complexity trade-off. Furthermore, Buhrman and De Wolf [20] associate the complexity of decision trees with the complexity of functions represented by certificate complexity [4], sensitivity complexity [28], and approximating polynomials [80], and use these complexity measures of functions to bound the complexity of decision trees.

Logistic regression is the foundation of a large number of parameterized models [21, 46]. In the early 1990s, the degree of effective freedom is proposed as a complexity measure for linear models and penalized linear models, which is represented by Vapnik–Chervoneniks (VC) dimension [26]. Besides, Rademacher complexity and Gaussian complexity [8, 12] are also used to measure model complexity of logistic regression models [46]. Comparing to VC dimension, Rademacher complexity takes data distribution into consideration and therefore reflects finer-grained model complexity. Later, Bulso et al. [21] and Balasubramanian [6] suggest that model complexity of logistic regression models is related to the number of distinguishable distributions that can be represented by the models. Bulso et al.  [21] define a complexity measure of logistic regression models based on the determinant of the Fisher Information matrix.

Spiegelhater et al. [93] systematically investigate the model complexity of Bayesian hierarchical models. A unique challenge in measuring the complexity of Bayesian hierarchical models is that the number of model parameters is not clearly defined. Spiegelhater et al. [93] define a model complexity measure by the number of effective parameters. Using the information theoretic argument, they show that this complexity measure can be estimated by the difference between the posterior mean of the deviance and the deviance at the posterior estimates of the parameters of interest, and is approximately the trace of the product of Fisher’s information [33] and the posterior covariance matrix. In addition, they suggest that adding such a complexity measure to the posterior mean deviance gives a deviance information criterion for comparing models.

Complexity measures are often model specific. The definition of model complexity largely depends on model structures. Different model frameworks often call for different definitions of complexity measures according to their structural characteristics. The complexity measures of different model frameworks usually cannot be directly compared.

Deep learning models are structurally different from traditional machine learning models and have dramatically more parameters. Deep learning models are always substantially more complex than traditional models. Therefore, the previous methods of model complexity of traditional machine learning models cannot be directly applied to deep learning models to obtain valid complexity measures. For example, measuring the complexity of a decision tree by the depth of the tree [20, 98] and the number of leaf nodes [16, 61] is obviously not applicable to deep learning models. Measuring model complexity by the number of trainable parameters [46] has a very limited effect on deep learning models since deep learning models are often over-parameterized.

The rest of this survey is organized as follows. In Sect. 2, we briefly introduce deep learning model complexity. In Sect. 3, we review the existing studies on the expressive capacity of deep learning models. In Sect. 4, we survey the existing studies on the effective complexity of deep learning models. In Sect. 5, we discuss the applications of deep learning model complexity. In Sect. 6, we conclude this survey and discuss some future directions.

Deep learning model complexity
In this section, we first divide deep model complexity into two categories, expressive capacity and effective model complexity. Then, we discuss a series of important factors of deep learning model complexity, and group the representative existing studies accordingly.

What is deep learning model complexity?
The term “model complexity” may refer to two different meanings in deep learning. First, model complexity may refer to capacity of deep models in expressing or approximating complicated distribution functions [13]. Second, it may describe how complicated the distribution functions are with some parameterized deep models [89]. These two meanings are captured by the notions of model expressive capacity and model effective complexity, respectively.

Expressive capacity, also known as representation capacity, expressive power, and complexity capacity [60, 86], captures the capacity of deep learning models in approximating complex problems. Informally, the expressive capacity describes the upper bound of the complexity of any model in a family of models.

This notion is consistent with the description of hypothesis space complexity [69, 96]. The hypothesis space is a family of hypotheses, such as the family of all neural networks with a fixed model structure. Considering the hypothesis space represented by a fixed model structure, the model expressive capacity is also the hypothesis space complexity. In statistical learning theory, the complexity of an infinite hypothesis space is represented by its expressive power, that is, the richness of the family of hypothesises [69]. A notion to capture hypothesis space complexity is Rademacher complexity [12], which measures the degree to which a hypothesis space can fit random noise. Another notion is VC dimension [26], which reflects the size of the largest set that can be shattered by the hypothesis space. Exploring expressive capacity helps to obtain the guarantee of learnability of deep models and derive generalization bounds [69].

Effective model complexity, also known as practical complexity, practical expressivity, and usable capacity [37, 81], reflects the complexity of the functions represented by deep models with specific parameterization [89]. The effective model complexity is for a model with parameters fixed. The study of effective model complexity helps the exploration of various aspects of deep models, such as understanding the optimization algorithms [81], improving model selection strategies [72], and improving model compression [25].

Expressive capacity and effective model complexity are closely related but are two different concepts. Expressive capacity describes the expressive power of the hypothesis space of a deep model. Effective model complexity explores the complexity of a specific hypothesis within the hypothesis space. Let us use an example to demonstrate the distinction and relationship between model expressive capacity and effective model complexity.

Example 1
(Difference between expressive capacity and effective complexity) ***Consider unary polynomial function f(x)=ax2+bx+c. The expressive capacity of f(x) is unary quadratic. In other words, f(x) cannot express a function more complicated than a unary quadratic polynomial. When assigning different values to the parameters a, b and c, the corresponding effective complexity may be different. With parameters a=0, b=1 and c=1, for example, f(x)=x+1, the effective complexity becomes linear, which is obviously lower than the expressive capacity.

Denote by H the hypothesis space of a fixed deep learning model structure, and by h∈H a hypothesis (i.e., a deep learning model) in H. The effective model complexity is the complexity of a specific hypothesis, written as EMC(h). The expressive capacity of the deep model, denoted by MEC(H), can be written in the form of

sup{EMC(h):h∈H}
This reflects the relationship between model expressive capacity and effective model complexity.

Because of the complex, over-parameterized architectures, deep learning models usually have high expressive capacity [70, 87]. However, a series of studies [5, 37, 81] find that the effective complexity of a trained deep model may be much lower than the expressive capacity.

Informally and intuitively, a deep learning model can be regarded as a “container” of knowledge learned from data. The same model architecture as a “container” may contain different amounts of knowledge by learning from different data and thus equipped with different parameters. The expressive capacity can be regarded as the upper bound of the amount of knowledge that a model architecture can hold. The effective model complexity is concerned about, for a specific model, a specific training dataset, how much knowledge it actually holds.

Important factors of deep learning model complexity
Bonaccorso [17] points out that a deep learning model consists of a static structure part and a dynamic parametric part. The static structure part is always determined before the learning process by the model selection principle, then stays immutable once decided. The parametric part is the objective of the optimization and is determined by a learning process. Both the static and dynamic parts contribute to model complexity. We refine this division and summarize four aspects affecting model complexity, including both expressive capacity and effective complexity.

Model framework:
The choice of model framework affects model complexity. The factor of model framework includes model type (e.g., feedforward neural network, convolutional neural network), activation function (e.g., Sigmoid [29], ReLU [73]), and others. Different model frameworks may require different complexity measure criteria and may not be directly comparable to each other.

Model size:
The size of deep model affects model complexity. Some commonly used measures of model size include the number of parameters, the number of hidden layers, the width of hidden layers, the number of filters, and the filter size. Under the same model framework, the complexities of models with different sizes can be quantified by the same complexity measure criteria and thus become comparable [54].

Optimization process:
The optimization process affects model complexity, including the form of objective functions, the selection of learning algorithms, and the setting of hyperparameters.

Data complexity:
The data on which a model is trained affect model complexity, too. The major factors include the data dimensionality, data distribution [69], information volume measured by Kolmogorov complexity [22, 59], and some others.

Among these four aspects, model framework and model size mainly affect the static structural part of a deep model, and optimization process and data complexity mainly affect the dynamic parametric part.

Table 1 Summarize the aspects affecting the expressive capacity and effective complexity, respectively
Full size table
The effect of these four aspects on expressive capacity can be understood through the influence on the hypothesis space. A selected model framework corresponds to a hypothesis space H. Each hypothesis h∈H represents a model with the given framework. Once the model size is determined, the hypothesis space shrinks to a subset of H. For example, suppose we set up two models with depth l1 and l2 (l1≤l2), respectively, and both with width m, the corresponding hypothesis spaces are H1 and H2, respectively. We have H1⊂H2. This is because, for each h∈H1, where h is a hypothesis and thus a deep network model in this context, there exists h′∈H2 whose first l1 layers are identical to those in h and the subsequent layers are identical mappings. It is easy to show that the expressive capacity of H1 will not exceed the expressive capacity of H2. Recently, model framework, model size and their effects on expressive capacity of deep models have been well explored [13, 51, 64, 89].

The choice of data distribution and optimization algorithm will further reduce the scope of the hypothesis space, thereby affecting the expression capacity. For example, Rademacher complexity is a data-dependent expressive capacity measure taking into account the effect of data distribution [12, 69]. However, to the best of our knowledge, the effect of data complexity and optimization process on expressive capacity of deep learning models is still rarely explored.

These four aspects also affect the effective model complexity. In general, model framework and model size decide the available range of effective model complexity. The effective complexity of a model with fixed parameters is a value in this range selected by optimization process and training data. The optimization process affects the effective complexity, for example, adding L1 regularization constrains the degrees of freedom in a deep neural network, and thus constrains the effective model complexity [44, 46]. The training data affects the effecive complexity, for example, using the same model and the same optimization process, the effective complexity of a model trained on linearly classifiable data is much lower than that trained on the ImageNet [31, 59]. Specifically, the effect of training data complexity and optimization process on effective complexity are reflected in the values of model parameters.

In Table 1 we list the corresponding aspects affecting expressive capacity and effective complexity, respectively. In Table 2 we list some representative studies on the two major problems, expressive capacity and effective complexity.

Table 2 Overview of a collection of studies on model complexity of deep neural networks
Full size table
Existing studies on deep learning model complexity: an overview
The literature on deep model complexity can be categorized from two different angles. The first angle focuses on whether an approach is model-specific or cross-model. The second angle focuses on whether an approach develops an explicit complexity measure. These two angles are applicable to both expressive capacity and effective complexity.

Model-specific versus cross-model
Based on whether an approach focuses on one type of models or crossing multiple types of models, the existing studies on model complexity can be divided into two groups, model-specific approaches and cross-model approaches.

A model-specific approach focuses on a certain type of models, and explores complexity based on structural characteristics. For example, Bianchini et al. [14, 15] and Hanin et al. [37] study model complexity of fully connected feedforward neural networks (FCNNs for short), Bengio and Delalleau [13, 30] focus on model complexity of sum-product networks. Moreover, some studies further propose constraints on the activation functions to constrain the nonlinearity properties. For example, Liang et al. [60] assume that activation functions σ(⋅) satisfy σ(z)=σ′(z)z.

An approach is cross-model when it covers multiple types of models rather than one specific type of models, and thus can be applied to compare two or more different types of models. For example, Khrulkov et al. [51] compare the complexity of general recurrent neural networks (RNNs for short), convolutional neural networks (CNNs for short) and shallow FCNNs by building connections among these network architectures and tensor decompositions.

Measure-based versus reduction-based
According to whether an explicit measure is designed, the state-of-the-art model complexity approaches can be divided into two groups, measure-based approaches and reduction-based approaches.

A deep model complexity study is measure-based if it defines an appropriate quantitative representation of model complexity. For example, the number of linear regions is used to represent the complexity of FCNNs with piecewise linear activation functions [37, 70, 81, 89].

A complexity approach is reduction-based if it investigates a model complexity problem by reducing deep networks to some known problems and functions, and does not define any explicit complexity measure. For example, Arora et al. [3] build connections between deep networks with ReLU activation functions and Lebesgue spaces. Khrulkov et al. [51] connect deep neural networks with several types of tensor decompositions.

Expressive capacity of deep learning models
Expressive capacity of deep models is also known as representation capacity, expressive power, and complexity capacity [60, 86]. It describes how well a deep learning model can approximate complex problems. Informally, the expressive capacity describes the upper bound of the complexity in a parametric family of models.

Expressive capacity of deep learning models has been mainly explored from four aspects.

Depth efficiency analyzes how deep learning models gain performance (e.g., accuracy) from the depth of architectures.

Width efficiency analyzes how the widths of layers in deep learning models affect model expressive capacity.

Expressible functional space investigates the functions that can be expressed by a deep model with a specific framework and specified size using different parameters.

VC dimension and rademacher complexity are two classic measures of expressive capacity in machine learning.

In this section, we review these four groups of studies.

Depth efficiency
A series of recent studies demonstrate that deep architectures significantly outperform shallow ones [13, 67]. Depth efficiency [64], which is the effectiveness of depth of deep models, has attracted a lot of interest. Specifically, the studies on depth efficiency analyze why deep architectures can obtain good performance and measures the effects of model depth on expressive capacity. We divide the studies of depth efficiency into two subcategories: model reduction methods and expressive capacity measures.

Model reduction
One way to study the expressive capacity of deep learning models is to reduce deep learning models to understandable problems and functions for analysis.

To investigate depth efficiency, one intuitive idea is to compare the representation efficiency between deep networks and shallow ones. Bengio and Delalleau [13, 30] investigate the depth efficiency problem on deep sum-product networks (SPN for short). An SPN consists of neurons computing either products or weighted sums of their inputs. They consider two families of functions built from deep sum-product networks. The first family of functions is F=∪n≥4Fn, where Fn is the family of functions represented by deep SPNs with n=2k inputs and depth k. The second family of functions is G=∪n≥2,i≥0Gi,n, where Gi,n is the family of functions represented by SPNs with n inputs and depth 2i+1.

Then, Bengio and Delalleau establish the lower bounds on the number of hidden neurons required by a shallow sum-product network to represent those families of functions. To approach a function f∈F, a one-hidden-layer shallow SPN needs at least 2n√−1 hidden neurons and at least 2n√−1 product neurons. Similarly, to approach a function g∈G, a one-hidden-layer shallow SPN needs at least (n−1)i hidden neurons. The comparison of deep and shallow sum-product networks representing the same function indicates that, to represent the same functions, the number of neurons in a shallow network has to grow exponentially but only a linear growth is needed for deep networks.

Mhaskar et al. [67] study functions representable by hierarchical binary tree networks, comparing with shallow networks. Figure 1 demonstrates shallow networks and hierarchical binary tree networks. Denote by Sm the class of shallow networks with m neurons, in the form of

x→∑k=1makσ(wk⋅x+bk)
where wk∈Rd, bk,ak∈R are the parameters for the kth hidden neuron, σ is the activation function. Denote by WNNr,d the class of functions with continuous partial derivatives of order ≤r with certain assumptions (see [67] for detail). Correspondingly, a hierarchical binary tree network is a network with the structure

f(x1,…,xd)=hl1(h(l−1),1(…(h11(x1,x2),h12(x3,x4),…),h(l−1),2(…))
(1)
where each hij is in Sm, l is the depth of the network, hl1 corresponds to the root node of the tree structure. See Fig. 1b as an example of the hierarchical binary tree with depth=3. Denote by Dm the class of hierarchical binary tree networks. Let WNNH,r,d be the class of functions with the structure of Eq. (1), where each hij is in WNNr,2. Define by infP∈V||f−P|| the approximation degree of V to function f, where V={Sm,Dm}.

Fig. 1
figure 1
8 input dimensions fed into a shallow network (a) and a hierarchical binary tree network (b), studied by Mhaskar et al. [67]

Full size image
Mhaskar et al. [67] demonstrate that, to approximate a function f∈WNNr,d to approximation degree ϵ, a shallow network in Sm requires O(ϵ−d/r) trainable parameters. Meanwhile, to approximate a function f∈WNNH,r,d to the same approximation degree, a network in Dm requires only O(ϵ−2/r) trainable parameters. Then, Mhaskar et al. [67] compare shallow Gaussian networks with hierarchical binary tree structures (Eq. (1)) where each hij computes a shallow Gaussian network, and obtain a similar conclusion. They demonstrate that functions with a designed compositional structure can be approximated by both deep and shallow networks to the same degree of approximation. However, the numbers of parameters of deep networks are much less than those of shallow networks.

Arora et al. [3] investigate the importance of depth on deep neural networks with ReLU activation function. First, they investigate neural networks with one-dimensional input and one-dimensional output. They prove that given any natural numbers k≥1 and w≥2, there exists a family of functions that can be represented by a ReLU neural network with k hidden layers each of width w. However, to represent this family of functions, a network with k′<k hidden layers requires at least 12k′wkk′−1 hidden neurons. Then, they investigate ReLU neural networks with d input dimensions. They prove that, given natural numbers k,m,d≥1 and w≥2, there exists a family of Rd→R functions that can be represented by a ReLU network with k+1 hidden layers and 2m+wk neurons. This family of functions is constructed using the zonotope theory from the polyhedral theory [102]. However, to represent this family of functions, the minimum number of hidden neurons that a ReLU neural network with k′≤k hidden layers requires is

max{12(k′wkk′d)(m−1)(1−1d)1k′−1,k′(wkk′d1k′)}.
To investigate when deep neural networks are provably more efficient than shallow ones, Kuurkova [54] analyzes the limitation of expressive capacity of shallow neural networks with Signum activation function. The Signum activation function is defined as

sgn(z)={−11 for  for z<0z≥0
Kuurkova proves that there exist functions that cannot be L1 sparsely represented by one-hidden-layer Signum neural networks, which have a limited number of neurons and a limited sum of absolute values of output weights (i.e., L1-norm). Such functions should be nearly orthogonal to any function f from the class of Signum perceptrons {sgn(vx+b):X→{−1,1}|v∈Rd,b∈R}. The functions generated by Hadamard matrices are such examples. A Hadamard matrix of order n is a n×n matrix with entries in {−1,1} such that any two distinct rows or columns are orthogonal. Kuurkova [54] proves that the functions induced by n×n Hadamard matrices cannot be computed by shallow Signum networks with both the number of hidden neurons and the sum of absolute values of output weights smaller than n√⌈log2n⌉.

To further illustrate the limitation of shallow networks, Kuurkova [54] compares the representation capability of one and two hidden layer networks with Heaviside activation function. The Heaviside activation function is defined as

σ(z)={01 for  for z<0z≥0
(2)
Let S(k) be a 2k×2k Sylvester–Hadamard matrix, which is constructed by starting from S(2)=(111−1) and then recursively iterating S(l+1)=S(2)⊗S(l). Kuurkova [54] shows that, to represent a function induced by S(k), a two-hidden-layer Heaviside network requires k neurons in each hidden layer. However, to represent such a function, a one-hidden-layer Heaviside network requires at least 2kk hidden neurons, or the absolute value of some output weights is no less than 2kk.

In summary, the model reduction approaches reduce neural networks to some kind of functions [3, 13, 54, 67], and investigate the effects of model depth on the capacity to express function families.

Expressive capacity measures
To investigate depth efficiency, another idea is to develop an appropriate measure of expressive capacity and study how expressive capacity changes when the depth and layer width of a model increase.

Montufar et al. [70] focus on fully connected feedforward neural networks (FCNNs) with piecewise linear activation functions (e.g., ReLU [73] and Maxout [35]), and propose to use the number of linear regions as a representation of model complexity (Fig. 2). The basic idea is that an FCNN with piecewise linear activation functions divides the input space into a large number of linear regions. Each linear region corresponds to a linear function. The number of linear regions can reflect the flexibility and complexity of the model.

Fig. 2
figure 2
An example from Montufar et al. [70] illustrating the advantage of deep models. A deep ReLU network (the dotted line) captures the boundary more accurately by approximating it with a larger number of linear regions than a shallow one (the solid line)

Full size image
Montufar et al. [70] investigate FCNNs with two types of piecewise linear activation functions: ReLU [73] and Maxout [35]. They prove that, under certain parameter settings, the model expressive capacity of a ReLU network N, which is represented by the maximum number of linear regions and denoted by MEC(N), is bounded by

MEC(N)≥(∏i=1l−1⌊mim0⌋m0)∑j=0m0(mlj)
(3)
where l is the number of hidden layers, mi is the width of ith hidden layer, m0=d is the dimensionality of the input. Based on this bound, they show that a ReLU network with l hidden layers and layer width mi≥m0 is able to approximate any piecewise linear function that has Ω((mm0)(l−1)m0mm0) linear regions.

Montufar et al. [70] also prove that, for the rank-k Maxout activation function, the expressive capacity of a one-hidden-layer Maxout network with m neurons is bounded by MEC(N)≥kmin(d,m) and MEC(N)≤min{∑dj=1(k2mj), km}. A rank-k Maxout network, which consists of l hidden layers and the width of each layer equals m, can compute any piecewise linear function with Ω(kl−1km) linear regions. In conclusion, the maximum number of linear regions generated by a FCNN with piecewise linear activation functions increases exponentially with model depth.

Montufar et al. [70] provide an explanation for the depth efficiency. They suggest that the intermediary layer of a deep model is able to map several pieces of the inputs into the same output. As the number of layers increases, the layer-wise compositions of the functions re-use lower-level computation exponentially. This allows deep models to compute highly complex functions, even with relatively fewer parameters.

Serra et al. [92] improve the bounds of the maximum number of linear regions proposed by Montufar et al. [70] (Eq. (3)). Given a deep ReLU neural network with l layers, let mi be the width of ith hidden layer with mi≥3d, d is the input dimension. Serra et al. [92] prove that the maximal number of linear regions of this neural network is lower bounded by

MEC(N)≥(∏i=1l−1(⌊mid⌋+1)d)∑j=0d(mlj)
and is upper bounded by

MEC(N)≤∑(j1,…,jl+1)∈J∏i=1l+1(miji)
where J={(j1,…,jl+1)∈Zl+1:0≤ji≤min{d,m1−j1,…,mi−1−ji−1,mi}}.

Bianchini and Scarselli [14, 15] design a topological measure of model complexity for deep neural networks. Given an FCNN with single output, denoted by N:Rd→R, they define S={x∈Rd|N(x)≥0} the set of instances being classified by N to the positive class. The model complexity of neural network N is measured by B(S), the sum of the Betti numbers of set S, that is,

MEC(N)=B(S)=∑i=0d−1bi(S)
where bi(S) denotes the ith Betti number that counts the number of (i+1)-dimensional holes in S. The Betti number is generally used to distinguish between spaces with different characteristics in algebraic topology [18]. S contains instances positively classified by the network N. Therefore, B(S) can be used to investigate how S is affected by the architecture of N and can represent the model complexity.

Bianchini and Scarselli [14, 15] report upper and lower bounds of B(S) for a series of network architectures (Table 3). In particular, Bianchini and Scarselli [14, 15] demonstrate that B(S) of a single-hidden-layer network grows polynomially with respect to the hidden layer width. That is, B(S)∈O(md), where m is the width of the hidden layer. They also prove that B(S) of a deep neural network grows exponentially in the total number of hidden neurons. That is, B(S)∈O(2M), where M is the total number of hidden neurons. This indicates that deep neural networks have higher expressive capacity, and therefore are able to learn more complex functions than shallow ones.

Table 3 Upper and lower bounds of B(S) given by Bianchini and Scarselli [14, 15], for networks with a number of M hidden units, a number of d inputs and a number of l hidden layers
Full size table
Bianchini and Scarselli [14, 15] suggest that the layer-wise composition mechanism of a deep model allows the model to replicate the same behavior in different regions of the input space, thus makes depth more effective than width.

Width efficiency
In addition to depth efficiency, the effect of width on expressive capacity, namely width efficiency, is also worth exploring. Width efficiency analyzes how width affects the expressive capacity of deep learning models [64]. Width efficiency is important for fully understanding expressive capacity and helps to validate the insights obtained from depth efficiency [64].

Lu et al. [64] investigate the width efficiency of neural networks with ReLU activation function. They extend the universal approximation theorem [7, 43] to width-bounded deep ReLU neural networks. The classical universal approximation theorem [7, 43] states that, one-hidden-layer neural networks with certain activation functions (e.g., ReLU) can approximate any continuous functions on a compact domain to any desired accuracy performance. Lu et al. [64] show that, for any Lebesgue-integrable function f:Rd→R and any ϵ>0, there exists a ReLU network N:Rd→R that can approximate f to ϵ L1-distance. That is,

∫Rd|f(x)−FN(x)|dx<ϵ.
Here FN is the function represented by the neural network N. The width mi of each hidden layer of N satisfies mi≤d+4.

Moreover, to explore the role of layer width in expressive capacity quantitatively, Lu et al. [64] raise the dual question of depth efficiency. That is, whether there exists wide, shallow ReLU networks that cannot be approximated by any narrow, deep neural network whose size is not substantially increased. Denote by FA:Rd→R the function represented by a ReLU neural network A whose depth h=3 and whose width of each layer is 2k2, where k is an integer such that k≥d+4. Let FB:Rd→R be the function represented by a ReLU neural network B whose depth h≤k+2 and whose width of each hidden layer mi≤k3/2, where the parameter values of B are bounded by [−b,b]. For any constant b>0, there is ϵ>0 such that FA can never be approximated by FB to ϵ L1-distance. That is,

∫Rd|FA(x)−FB(x)|dx≥ϵ.
This indicates that there exists a family of shallow ReLU neural networks which cannot be approximated by narrow networks whose depth is constrained by polynomial bounds.

The polynomial lower bound for width efficiency is smaller than the exponential lower bound for depth efficiency [13, 15, 70]. That is, to approximate a deep model whose depth increases linearly, a shallow model requires at least an exponential increase in width. To approximate a shallow, wide model whose width increases linearly, a deep, narrow model requires at least a polynomial increase in depth. However, Lu et al. [64] point out that depth cannot be strictly proved to be more effective than width since a polynomial upper bound for width is still lacking. The polynomial upper bound ensures that to approximate a shallow, wide model, a deep, narrow one requires at most a polynomial increase in depth.

Expressible functional space
In addition to the studies of depth efficiency and width efficiency, the third line of works explore the classes of functions that can be expressed by deep learning models with specific frameworks and specified size. This line of works explore the expressible functional space of deep learning models, including model-specific and cross-model approaches.

Model-specific approaches
Arora et al. [3] investigate the family of functions representable by deep neural networks with ReLU activation function. They prove that every piecewise linear function f:Rd→R can be represented by a ReLU neural network that consists of at most ⌈log2(d+1)⌉ hidden layers. The family of piecewise linear functions is dense in the family of compactly supported continuous functions, and the family of compactly supported continuous functions is dense in Lebesgue space Lp(Rd) [3, 23].

The Lebesgue space Lp(Rd) is defined as the the family of Lebesgue intergrable functions f for which ∫Rd|f|<+∞ [23]. Define Lp norm [23] as ||f||p=[∫Rd|f|p]1/p. Then, the above conclusion can be extended to the Lp(Rd)-space. That is, every function f∈Lp(Rd) can be approximated to arbitrary Lp norm by a ReLU neural network which consists of at most ⌈log2(d+1)⌉ hidden layers.

Gühring et al. [36] study the expressive capacity of deep neural networks with ReLU activation function in Sobolev space [1]. Given Ω⊂Rd, p∈[1,∞], n∈N, Lp(Ω) is the Lebesgue space on Ω, the Sobolev space [84] is defined as

Wn,p(Ω)={f∈Lp(Ω):Dαf∈Lp(Ω) for ∀α∈Nd0,|α|≤n},
where Dαf is the αth order derivative of f. Sobolev norm is defined as

||f||Wn,p(Ω)=⎛⎝∑0≤|α|≤n||Dαf||pLp(Ω)⎞⎠1/p.
Gühring et al. [36] analyze the effect of ReLU neural networks in approximating functions from Sobolev space, and establish upper and lower bounds on the sizes of models to approximate functions in the Sobolev space. Specifically, define a subset of Sobolev space f as

Fn,d,p,B={f∈Wn,p((0,1)d):||f||Wn,p((0,1)d)≤B}.
The upper bound shows that, for any d∈N, n∈N,n≥2, ε>0, 0≤s≤1, 1≤p≤+∞, and B>0, for any function f∈Fn,d,p,B, there exists a neural network Nε and a choice of weights wf such that

||Nε(⋅|wf)−f(⋅)||Ws,p((0,1)d)≤ε
where Nε represents a ReLU neural network consisting of at most clog2(ε−nn−s) layers and cε−dn−slog2(ε−nn−s) neurons, and with non-zero parameters. The value of constant c depends on the value of d, p, n, s and B.

Besides, the lower bound [36] shows that, for any d∈N, n∈N,n≥2, ε>0, B>0, k∈{0,1}, with p=+∞, for any function f∈Fn,d,p,B, there exists a ReLU neural network Nε such that

||Nε(⋅|wf)−f(⋅)||Wk,∞((0,1)d)≤ε
where Nε has at least c′ε−d2(n−1) non-zero weights.

Kileel et al. [52] explore the functional space of deep neural networks with polynomial activation functions. A polynomial activation function ρr(z) raises z to the power of r. They suggest that, with polynomial activation functions, the study of model complexity can be benefitted from the application of powerful mathematical machinery of algebraic geometry. In addition, polynomials can approximate any continuous activation function, thus help to explore other deep learning models.

Given a deep polynomial neural network N:Rd→Rc with depth h and polynomial degree r, let m={m0,m1,…,mh} represent the architecture of N with the width mi of layer i and m0=d, mh=c. Let Fm,r be the functional space of N. Kileel et al. [52] define the functional variety Vm,r as Vm,r=Fm,r¯¯¯¯¯¯¯¯¯¯, which is the Zariski closure of the functional space Fm,r. “The Zariski closure of a set X is the smallest set containing X that can be described by polynomial equations.” [52] The functional variaty Vm,r can be much larger than the functional space Fm,r. Kileel et al. [52] propose to use the dimensionality of functional variety, denoted by dimVm,r, as the representation of the expressive capacity of deep polynomial neural networks, written as

MEC(Nm,r)=dimVm,r.
To measure dimVm,r, Kileel et al. [52] build connections between deep polynomial networks and tensor decompositions. Specifically, polynomial networks with h=2 are connected to CP tensor decompositions [56], and deep polynomial networks are connected to an iterated tensor decomposition [65]. Based on decompositions, they prove that, for any fixed m, there exists r~=r(m) such that for any r>r~, dimVm,r is bounded by

dimVm,r≤min(mh+∑i=1h(mi−1−1)mi,mh(m0+rh−1+1rh−1)).
Besides, Kileel et al. [52] prove a bottleneck property of deep polynomial networks. That is, a too narrow layer is a bottleneck and may “chock” the polynomial network such that the network can never fill the ambient space. The ambient space Symr(Rd) is the space of homogeneous polynomials of degree r in d variables. A polynomial network filling the ambient space satisfies Fm,r=Symrh−1(Rd)c. They show that, network architectures filling the ambient space can be helpful for optimization and training.

Cross-model approaches
In addition to model-specific approaches, expressible functional space can be investigated in a cross-model manner. Specifically, Khrulkov et al. [51] study the expressive capacity of recurrent neural networks (RNNs). They investigate the connections between network architectures and tensor decompositions, then make comparison between the expressive capacity of RNNs, CNNs, and shallow FCNNs.

Fig. 3
figure 3
Examples of networks corresponding to various tensor decompositions [51], from the left are TT decomposition, CP-decomposition, HT-decomposition

Full size image
Let X∈Rn1×n2×…×nd be a d-dimensional tensor. The Tensor Train (TT) decomposition [83] of a tensor X is computed by

Xi1i2…id=∑α1=1r1…∑αd−1=1rd−1Gi1α11Gα1i2α22…Gαd−1idd
where the tensors Gk∈Rrk−1×nk×rk are called TT-cores. Khrulkov et al. [51] introduce bilinear units to represent TT-cores. Given x∈Rm,y∈Rn and G∈Rm×n×k, a bilinear unit performs a map G:Rm×Rn→Rk, written as G(x,y)=z. Based on the bilinear units, they show that a recurrent neural network realizes the TT-decomposition of the weight tensor (Fig. 3a). Similarly, Khrulkov et al. [51] show that the Canonical (CP) decomposition [24], with the form of

Xi1i2…id=∑α=1rvi11,αvi22,α…vidd,α,
corresponds to a one-hidden-layer FCNN (Fig. 3b). Each unit of the network is denoted by Gα=vi11,αvi22,α…vidd,α, where vi,α∈Rni. The Hierarchical Tucker (HT) decomposition [27] corresponds to an CNN structure (Fig. 3c).

Table 4 Comparison of the expressive power of various network architectures given by Khrulkov et al. [51]
Full size table
Khrulkov et al. [51] propose the rank of tensor decomposition as a measure of neural network complexity, since the rank of decompositions corresponds to the width of networks. Based on the correspondence relationships between neural networks and tensor decompositions, they compare the model complexity of RNNs, CNNs, and shallow FCNNs. The main conclusions are summarized in Table 4. In particular, given a random d-dimensional tensor whose TT-decomposition is with rank r and mode size n, this tensor has exponentially large ranks of CP-decomposition and HT-decomposition. That tells, to approximate a recurrent neural network, a shallow FCNN or CNN requires an exponentially larger width.

VC dimension and rademacher complexity
The VC dimension and Rademacher complexity are widely used to analyze the expressive capacity and generalization of classical parametric machine learning models [8, 12, 26, 69, 95]. A series of works investigate the VC dimension and Rademacher complexity of deep learning models.

The VC dimension is an expressive capacity measure that reflects the largest number of samples that can be shattered by the hypothesis space [69]. A higher VC dimension means the model can shatter a larger number of samples and thus the model has a higher expressive capacity. Maass [66] studies the VC dimension of feedforward neural networks with linear threshold gates. The linear threshold gate means that each neuron is composed of a weighted sum function followed by a Heaviside activation function (Eq. (2)). Let W be the number of parameters in the network and L be the depth of the network. Maass [66] proves that for L≥3, the VC dimension of such networks is Θ(WlogW).

Bartlett et al. [11] investigate the VC dimension of feedforward neural networks with piecewise polynomial activation functions. A piecewise polynomial activation function with p pieces has the form σ(z)=ϕi(z), where z∈[ti−1,ti), i∈1,…,p+1, and ti−1<ti. Each ϕi is a polynomial function of degree no more than r. Let W be the number of parameters in the network and L be the depth of the network. Bartlett et al. [11] prove that the upper bound for the VC dimension of such networks is O(WL2+WLlogWL) and the lower bound for the VC dimension is Ω(WL). Later, Bartlett et al. [10] improve this lower bound to Ω(WLlog(W/L)).

Bartlett et al. [10] study the VC dimension for deep neural networks with piecewise linear activation functions (e.g., ReLU). Given a deep neural network with L layers and W parameters, they prove that the lower bound for VC dimension of such networks is Ω(WLlog(W/L)) and the upper bound for VC dimension is O(WLlogW).

Rademacher complexity captures the capacity of a hypothesis space to fit random labels as a measure of the expressive capacity [69]. A higher Rademacher complexity means that the model can fit a larger number of random labels and thus the model has a higher expressive capacity. Bartlett et al. [9] investigate the Rademacher complexity of deep neural networks with ReLU activation function. Given a deep ReLU neural network with L layers, let Ai be the parameter matrix of layer i, and X∈Rn×d the data matrix, where n is the number of instances and d is the input dimension. Bartlett et al. [9] prove that the lower bound for the Rademacher complexity of such networks is Ω(||X||F∏i||Ai||σ), where ||⋅||σ is the Spectral norm, and ||⋅||F is the Frobenius norm. Neyshabur et al. [77] prove a tighter lower bound for two-layer ReLU neural networks. Suppose ||A1||σ≤s1, ||A2||σ≤s2, and s1s2 is the Lipschitz bound of the function represented by the network. They prove that the Rademacher complexity is lower bounded by Ω(s1s2m−−√||X||F/n), where m is the width of the hidden layer. This lower bound improves the bound [9] by a factor of m−−√.

Yin et al. [99] study the Rademacher complexity of adversarial trained neural networks. Given a feedforward neural network with ReLU activation function, denoted by f, let L be the depth of the network, and Ai the parameter matrix of layer i. The function family represented by f with adversarial loss function can be written as

F={fA:minx′∈B(ϵ)yfA(x),∏i=1L||Ai||σ≤r}
where B(ϵ)={x′∈Rd:||x′−x||≤ϵ} is the set of samples perturbed around x with l∞ distance ≤ϵ. Yin et al. [99] prove that the lower bound for the Rademacher complexity of F is Ω(||X||F/n+ϵd/n−−−√). This lower bound exhibits an explicit dependence on the input dimension d.

Some studies [75, 100] suggest that deep learning models are often over-parameterized in practice and have significantly more parameters than samples. In this case, the VC dimension and Rademacher complexity of deep learning models are always too high, so the practical guidance they can provide is weak.

Effective complexity of deep learning models
Effective complexity of deep learning models is also known as practical complexity, practical expressivity, and usable capacity [37, 81]. It reflects the complexity of the functions represented by deep models with specific parameterizations [89].

Effective complexity of deep learning models has been mainly explored from two different aspects.

General measures of effective complexity design quantitative measures for effective complexity of deep learning models.

Investigations into the high-capacity low-reality phenomenon find that effective complexity of deep learning models may be far lower than their expressive capacity.

In this section, we review these two groups of studies.

General measures of effective complexity
Compared to expressive capacity, the study of effective model complexity has stronger requirements for sensitive and precise complexity measures. This is because the effective complexity cannot be directly derived from the model structure alone [44]. Different parameter values of the same model structure may lead to different effective complexity. An effective complexity measure is expected to be sensitive to different parameter values used in models with the same structure.

A series of works devote to proposing feasible measures for effective complexity of deep learning models. A major group of the complexity measures depends on the linear region splitting of piecewise linear neural networks in the input space. We start with those methods and then discuss the others.

Piecewise linear property
It is well known that a neural network with piecewise linear activation functions generates a finite number of linear regions in the input space [44, 70, 89]. This property is called the piecewise linear property and is demonstrated in Fig. 4. The number of linear regions as well as the density of such regions can usually reflect the effective complexity. Therefore, a series of studies on effective complexity starts from piecewise linear activation functions (e.g., ReLU, Maxout) or are based on the piecewise linear property.

Fig. 4
figure 4
A 2-hidden-layer ReLU network divides the 2-dimensional input space into a number of linear regions, studied by Hanin and Rolnick [37]. Upon each hidden neuron shows the linear regions divided by the current neuron

Full size image
Raghu et al. [89] propose two interrelated effective complexity measures for deep neural networks with piecewise linear activation functions, such as ReLU and hard Tanh [82]. Specifically, they define trajectory x(t) between two input points x1,x2∈Rd, which is a curve parametrized by a scalar t∈[0,1], with x(0)=x1 and x(1)=x2. The first effective complexity measure they propose is the number of linear regions in the input space when sweeping an input point through a trajectory path x(t), that is, the effective complexity EMC(N) of model N is

EMC(N)=T(N(x(t);W))
where T is the number of linear regions passing through the trajectory x(t) and W is a specific set of model parameters. The second effective complexity measure they propose is the trajectory length l(x(t)) defined as

l(x(t))=∫t||dx(t)dt||dt
which is the standard arc length of the trajectory x(t). They prove the proportional relationship between these two complexity measures. To obtain the estimated value of effective complexity, Raghu et al. [89] bound the expected value of trajectory length of any layer in a deep ReLU neural network. Specifically, given a deep ReLU neural network whose weights are initialized by N(0,σ2w/m) and the biases are initialized by N(0,σ2b). Let z(i)(x(t)) denote the new trajectory obtained after the transformation of the first i hidden layers of the trajectory x(t). The expected value of its trajectory length can be bounded by

E[l(z(i)(x(t)))]≥O(σwm−−√m+1−−−−−√)il(x(t))
where m denotes the hidden layer width. Similarly, for a hard Tanh neural network under the same initialization, the trajectory length is bounded by

E[l(z(i)(x(t)))]≥O⎛⎝⎜⎜⎜σwm−−√σ2w+σ2b+mσ2w+σ2b−−−−−−√−−−−−−−−−−−−−−−−−−√⎞⎠⎟⎟⎟il(x(t)).
Fig. 5
figure 5
A circular trajectory (the left most) is fed into a Tanh neural network, then the following images show the trajectory after the transformation of each hidden layer in turn [89]. It shows the increase of the length of the trajectory after the layer transformation

Full size image
Using these two complexity measures, Raghu et al. [89] explore the performance of deep neural networks and report some findings. First, the effective complexity grows exponentially with respect to the depth of the model and polynomially with respect to the width. Figure 5 is an example showing the evolution of a trajectory after each hidden layer of a deep network. Second, how the parameters are initialized affects the effective complexity. Third, injecting perturbations to a layer leads to exponentially larger perturbations at the remaining layers. Last, regularization approaches, such as Batch Normalization [45], help to reduce trajectory length. This explains why Batch Normalization helps model stability and generalization.

Novak et al. [81] empirically investigate the relationship between model complexity and generalization of neural networks with piecewise linear activation functions. They propose to use model sensitivity to measure effective complexity. Model sensitivity, also known as robustness, reflects the capacity of a model to distinguish between different inputs at small distances. Novak et al. [81] introduce two sensitivity metrics, input-output Jacobian norm and trajectory length [89].

First, based on the piecewise linear property, Novak et al. [81] propose the Jacobian norm to measure the local sensitivity under the assumption that the input is perturbed within the same linear region. This Jacobian norm can further represent the effective model complexity. That is,

EMC(N)=Ex∼D[||J(x)||F]
where J(x)=∂fN(x)/∂xT is the Jacobian of class probabilities, fN is the function represented by the network N, D is the data distribution, and ||⋅||F is the Frobenius norm.

Second, they propose a trajectory length metric similar to what developed by Raghu et al. [89] as a sensitivity measure when the input is perturbed to other linear regions, further as a measure of effective complexity, written as

EMC(N)=Ex∼D[t(x)]
where t(x) is a trajectory length defined by the number of linear regions passing through a trajectory τ(x), that is,

t(x)=∑i=0k−1||c(zi)−c(z(i+1)%k)||1≈∮z∈τ(x)||∂c(z)∂(dz)||1dz
where z0,…,zk−1 are k equidistant points on the trajectory τ(x), c(z) is the status encoding of all hidden neurons at point z.

Using these two complexity measures, Novak et al. [81] study the correlation between complexity and generalization. They demonstrate that neural networks have strong robustness in the vicinity of the training data manifold where deep models have good generalization capability. They also show that the factors associated with poor generalization (e.g., full-batch training, random labels) correspond to weaker robustness, and the factors associated with good generalization (e.g., data augmentation, ReLU) correspond to stronger robustness.

To develop a measure of effective complexity for general smooth activation functions, Hu et al. [44] propose an effective complexity measure for deep neural networks with curve activation functions (e.g., Sigmoid [53], Tanh [48]). Motivated by the piecewise linear property, they suggest that, using a piecewise linear function with a minimal number of linear regions to approximate a given network, the number of linear regions of the approximation function can be a measure of effective complexity of the given network. They learn a piecewise linear approximation of a deep neural network with curve activation functions, which is called the Linear Approximation Neural Network (LANN for short). The LANN is constructed by learning a piecewise linear approximation function for the curve activation function on each hidden neuron. Specifically, Hu et al. [44] define the approximation error of a LANN g to the target network f as E(g;f)=E[|g(x)−f(x)|]. They analyze how the approximation error at a certain layer is propagated through the remaining hidden layers, and obtain the influence of the approximation error of each hidden neuron on E(g;f). That is,

E(g;f)=∑i,j1c∑( |Vo|∏q=Li+1E[|Jq|] )∗,j(E[ei,j]+E[|ϵ^i,j|])
where Jq is the Jacobian matrix of layer q of the network f, ei,j is the approximation error of g on a specific neuron {i,j}, Vo is the weight matrix of the output layer, and ϵ^i,j is the negligible estimation error on neuron {i,j}. Given an approximation degree λ, the LANN with the smallest number of linear regions is constructed to meet the requirement E(g;f)≤λ. The approximated number of linear regions of the LANN is used as the effective complexity measure, that is,

EMC(f)λ=d∑i=1Llog(∑j=1miki,j−mi+1)
where ki,j is the number of linear pieces of the approximation function on neuron {i,j}, mi is the width of layer i, and L is the depth of f.

Using the complexity measure, Hu et al. [44] investigate the trend of model complexity in the training process. They show that the effective complexity increases with respect to the number of training iterations. They also demonstrate that the occurrence of overfitting is positively correlated with the increase of effective complexity, while regularization methods (e.g., L1,L2 regularizations) suppress the increase of model complexity (see Fig. 6).

Fig. 6
figure 6
Influence of regularization approaches on effective complexity [44]. This figure shows that the decision boundaries of models trained on the Moon dataset. NM, L1, L2 are short for normal train, train with L1, and L2 regularization, respectively. In brackets are the value of effective complexity measure

Full size image
The piecewise linear property may provide novel opportunities for capturing model complexity in deep learning. In addition to the above studies on effective complexity, piecewise linear neural networks or the piecewise linear property can also help the exploration of expressive capacity (see Section 2, [4, 36, 64, 70, 92]). Piecewise linear activation functions, especially ReLU, are popular and effective activation functions in many tasks and applications [58, 70]. The local linear characteristic and a finite number of regional divisions facilitate quantifying and analyzing neural network model complexity with piecewise linear activation functions.

Other measure metrics
There are other effective complexity measures based on ideas other than the piecewise linear property.

Nakkiran et al. [74] introduce an effective complexity measure by investigating the double descent phenomenon. The double descent phenomenon of deep neural networks is that, as the model size, training epochs, or size of training data increases, the test performance often first decreases and then increases. They suggest that to help capture the double descent effect in training, a complexity measure should be sensitive to training processes, data distribution, and model architectures. They propose such an effective complexity measure, that is, the maximum number of samples on which the model can be trained to close to zero training error, written as

EMCD,ϵ(T)=max {n|ES∼D[ErrorS(T(S))]≤ϵ}
where D is the data distribution, ϵ is the threshold of training error, T is the training procedure, ErrorS is the mean error of the model on the training set S, and n the size of training set S. Based on the effective complexity measure EMCD,ϵ(T), they investigate the evolution of the training process and show that, if EMCD,ϵ(T) is sufficiently smaller or sufficiently larger than n, the size of the training set, then any perturbation of T that increases its effective complexity EMCD,ϵ(T) helps to improve the test performance. However, if EMCD,ϵ(T)≈n, then any perturbation of T that increases its effective complexity EMCD,ϵ(T) hurts the test performance.

To approach the generalization problem of deep learning models, Liang et al. [60] introduce a new notion of model complexity measure, the Fisher-Rao norm. Their study focuses on deep fully connected neural networks whose activation function σ(⋅) satisfies σ(z)=σ′(z)z. The model complexity represented by Fisher-Rao norm is given by

EMC(N)=||θ||2fr=⟨θ,I(θ)θ⟩
where θ is the set of parameters (i.e., weights, bias) of neural network N, ⟨⋅,⋅⟩ is the inner product, and I(θ) is the Fisher information matrix, written as

I(θ)=E[∇θℓ(N(X),Y)⊗∇θℓ(N(X),Y)]
where ℓ(⋅,⋅) is the loss function and ⊗ is the tensor product. Liang et al. [60] introduce the geometric invariance property that a complexity measure should satisfy in order to study generalization capability. The invariance property essentially says that, since many different continuous operations may lead to exactly the same prediction, thus the generalization should only depend on the equivalence classes obtained by these continuous transformations. Specific parameterization should not affect the generalization. The complexity measure used to investigate generalization should satisfy the invariance property. They demonstrate that this Fisher-Rao norm satisfies the invariance property. Let θ1,θ2 denote two parameter settings of a model f. If fθ1=fθ2, then their Fisher-Rao norms are equal, that is, ||θ1||fr=||θ2||fr. In particular, the Fisher-Rao norm remains invariant during the node-wise rescaling of a model.

Effective complexity can be measured by the size of training samples that a model achieves zero training error [74], or by the Fisher-Rao metric [60]. More effective complexity measures along the line may be developed.

High-capacity low-reality phenomenon
Several studies explore the gap between the effective complexity and the expressive capacity of deep learning models.

Ba and Caruana [5] show that shallow fully connected neural networks can learn complex functions previously learned by deep neural networks, sometimes even only requiring the same number of parameters as the deep networks. Specifically, given a well-trained deep model, they propose to train a shallow model based on the outputs of the deep model, to mimic the deep model. They show that, the shallow mimic model can achieve an accuracy as high as the deep model. However, the shallow model cannot be trained directly on the original labeled training data to achieve the same accuracy. This is also well recognized as knowledge distillation [41].

Based on this phenomenon, Ba and Caruana [5] conjecture that the strength of deep learning may arise in part from a good match between deep architectures and current training algorithms. That is, compared with shallow architectures, deep architectures may be easier to train by the current optimization techniques. Moreover, they propose that when it is able to mimic the function learned by a deep complex model using a shallow model, the function learned by the deep model is not really too complicated to learn. This study suggests that there may be a big gap between the practical effective complexity of a deep learning model and the theoretical bound of its expressive capacity. We call this the high-capacity low-reality phenomenon.

Hanin and Rolnick [37] investigate the high-capacity low-reality phenomenon in fully connected neural networks with piecewise linear activation functions, especially ReLU. They propose two effective complexity measures using the number of linear regions in the input space and the volume of boundaries between these linear regions.

First, they investigate ReLU neural networks whose dimensionality of input and that of output are both equal to 1, and use the number of linear regions as the effective complexity measure. They show that the average number of linear regions grows linearly with respect to the total number of neurons, far below the exponential upper bound. Specifically, given a ReLU neural network N:R→R whose weights and biases of neurons z are randomly initialized and are bounded by E[||∇z(x)||]≤C for some C>0, the average number of linear regions is proportional to the product of the number of neurons and the size of training set, that is,

E[#{linear regions in S}]≈|S|⋅T⋅M
where S is the training dataset, T is the number of breakpoints in the activation function (for ReLU, T=1), and M is the total number of hidden neurons.

Second, they investigate ReLU neural networks whose input dimensionality exceeds 1, denoted by N:Rd→R (d>1), and use the volume of boundaries between linear regions in the input space as an estimation of model complexity. That is,

EMC(N)=volumed−1(BN∩K)volumed(K)
where BN={x|∇N(x) is not continuous at x} represents the boundaries of the linear regions formed by N, K∈Rd is the data distribution. They prove that, under the same parameter initialization assumption as d=1, the expected value of the volume of linear region boundaries is approximately equal to

E[EMC(N)]≈T⋅M
This demonstrates that the average size of the region boundaries depends only on the number of neurons, not on the depth of the model. They conclude that the effective complexity of deep neural networks may be much lower than the theoretical bound. That is, the function learned by deep neural networks may not be more complex than that learned by shallow ones.

Discussion
Effective model complexity is a relatively new, promising and useful problem in deep learning. Detecting effective model complexity during training helps to investigate the usefulness of optimization algorithms [47], the role of regularizations [44, 89], and generalization capability [60, 81]. Furthermore, effective model complexity can be used to describe model compression ratio, since effective model complexity can be considered as a reflection of the information volume in the model [32]. Effective complexity can also be used for model selection and design to balance resource utilization and model performance.

In addition to the effective complexity measures and the high-capacity low-reality phenomenon, there are a series of interesting problems about effective complexity of deep learning models. For example, the cross-model comparison of effective complexity is worth exploring. That is, how to compare the effective complexity of multiple models with different architectures, and how the effective complexity is affected by the choice of different model architectures. Moreover, can one specify the granularity of effective complexity measures? Different cases may have different requirements for effective complexity. Correspondingly, the application scopes and granularities of effective complexity measures should be specified and clarified. Typically, effective complexity measure by the number of non-zero parameters is obviously not sufficient to study the optimization process.

Application examples of deep learning model complexity
Model complexity of deep learning has many applications. In this section, we review three interesting applications of deep learning model complexity, namely understanding model generalization capability, model optimization, and model selection and design.

Model complexity in understanding model generalization capability
Deep learning models are always over-parameterized, that is, they have far more model parameters than the optimal solutions and the number of training samples. However, it is often found that large over-parameterized neural networks exhibit good generalization capability [49, 76, 100]. Some studies even find that larger and more complex networks usually generalize better [81]. This observation is in contradiction with the classical notion of function complexity [81] and the well-known Occam’s razor [90], which prefer simple models. What leads to the good generalization capability of over-parameterized deep learning models?

In statistical learning theory, expressive capacity (i.e., hypothesis space complexity) is used to bound generalization error [69]. Specifically, let F be the set of functions representable by a certain model structure. Let fA(D) be a function f∈F learned by algorithm A on training dataset D. Let ED(fA(D)) be the empirical error of fA(D) and E(fA(D)) the generalization error of fA(D). The gap between generalization error and emperical error is bounded by

E(fA(D))−ED(fA(D))≤supf∈F{E(f)−ED(f)}
(4)
The right-hand side can be quantified by analyzing the expressive capacity (e.g., Rademacher complexity) [49]. For example, Zheng et al. [101] analyze generalization error of deep ReLU neural networks using the basis-path norm, a norm measure based on the basis paths. Zheng et al. [101] suggest that there exist a small group of basis paths in a ReLU neural network, which are linearly independent. Each input-output path of a ReLU neural network can be expressed in the form of multiplication and division of the basis paths. Therefore, the basis paths can be used to explain the generalization behavior of ReLU neural networks. Zheng et al. [101] prove that the generalization error of ReLU neural networks (i.e. the right-hand side of Eq. (4)) can be bounded by a function of basis-path norm.

A series of studies investigate model complexity measures that can explain generalization capability of deep learning models [2, 60, 76, 81]. Neyshabur et al. [76] suggests that, from the perspective of generalization, a complexity measure should satisfy the following property: a learned model with lower complexity generalizes better. In particular, they list several requirements which are summarized from observed empirical phenomena and are expected to be satisfied by complexity measures.

With zero training error, a network trained on real labels, which leads to good generalization, is expected to have much lower complexity than a network trained on random labels.

Increasing the number of hidden units or the number of parameters, which leads to a decreased generalization error, is expected to decrease the complexity measure.

When training the same architecture on the same training dataset using two different optimization algorithms, if both lead to zero training errors, the model with better generalization is expected to have lower complexity.

Based on these desiderata, Neyshabur et al. [76] investigate several complexity measures including norms [79], robustness [97], and sharpness [50]. They show that, these measures can meet some of the above requirements, but not all.

Novak et al. [81] define two complexity measures from the perspective of model sensitivity, and identify an empirical correlation between the complexity measures and model generalization capability. They show that operations that lead to poor generalization, such as full batch training, correspond to high sensitivity, and in turn imply high effective model complexity. Similarly, operations that lead to good generalization, such as data augmentation, correspond to low sensitivity, and thus imply low effective model complexity.

Liang et al. [60] define a complexity measure using the Fisher-Rao norm to investigate model generalization capability. They suggest that a complexity measure used to study generalization should satisfy the invariance property. The invariance property requires that the generalization capacity depends on the equivalence classes obtained by deep models. In other words, many different parameterizations may lead to the same prediction. Thus, the specific parameterization of deep models should not affect the generalization and the complexity measure. They show that the Fisher-Rao norm honors this invariance property and thus is able to explain the generalization capability of deep learning models.

Model complexity in optimization
Model optimization is concerned about how and why a neural network model can be successfully trained [86, 94]. Specifically, the optimization of a deep model is to determine model parameters to minimize a loss function in general non-convex. The loss function is typically designed based on the understanding of a problem and the requirements of the model, and thus generally includes a performance measure, which is evaluated on the training set, and other constraint terms [34].

Model complexity is widely used to provide a metric to make optimization traceable. For example, a measure metric of the effective model complexity of neural networks helps to monitor the changes of a model during the optimization process and understand how the optimization process progresses [44, 47, 74, 89]. Such a metric also helps to verify the effectiveness of new improvements of optimization algorithms [39]. For example, Nakkiran et al. [74] investigate the double descent phenomenon during training using effective complexity measured by the maximal size of the dataset on which zero training error can be achieved. They show that the double descent phenomenon can be represented as a function of the effective complexity. Raghu et al. [89] and Hu et al. [44] propose new regularization methods and demonstrate the effectiveness of these regularizations through their impact on complexity.

The study of model complexity inspires explorations on the effectiveness of optimization approaches. Hanin and Rolnick [37] use the boundary volumes of linear regions as the complexity measure of ReLU neural networks, and find that during the training, the average boundary volume is always linearly proportional to the number of neurons, irrelevant to the depth of the model. This demonstrates that deep models do not always learn more complex functions than shallow ones, the success of deep learning may be related to optimization algorithms. Ba and Caruana [5] suggest that the great performance of deep learning may be due to the fact that deep models are easier to train than shallow architectures using the current optimization algorithms [37, 81]. This calls for further exploration of the effectiveness of optimization approaches and the relationship with model structures.

Model complexity in model selection and design
Given a specific learning task, how can we determine a feasible model structure for the task? Given a variety of models with different architectures and different complexity, how can we pick the best model from them? This is the model selection and design problem [71].

In general, model selection and design is based on the tradeoff between prediction performance and model complexity [61, 72]. On one hand, making predictions with high accuracy is the essential goal of learning a model [69]. A model is expected to be able to capture the underlying patterns hidden in the training data and achieve predictions of accuracy as high as possible. In order to represent a large amount of knowledge and obtain high accuracy, a model with a high expressive capacity, a large degree of freedom and a large training set is required [13]. To this extent, a model with more parameters and higher complexity is favored. On the other hand, an overly complex model may be difficult to train and may incur unnecessary resource consumption, such as storage, computation and time cost [72]. Unnecessary resource consumption should be avoided particularly in practical large scale applications [42]. To this extent, a simpler model with comparable accuracy is preferred than a more complicated one.

To maintain a good tradeoff between accuracy and complexity, a model selected is expected to be complex enough to fit the given data and achieve high accuracy. At the same time, the model should not be highly over-complicated. Understanding model complexity and developing an effective complexity measure are the premise for good model selection strategies. For instance, Michel and Nouy [68] propose a model selection strategy for tree tensor networks (i.e., sum-product neural networks). Their method combines the empirical risk minimization and model complexity penalty to select a model from a family of models.

Neural architecture search (NAS for short) is a popular solution to deep learning model selection [57, 62, 63, 103], which automatically selects a good neural network architecture for a given learning task. Since an overly complex model may take too-long training time and thus may become a serious obstacle of neural architecture search [57, 62], the accuracy-complexity tradeoff is an important consideration in neural architecture search. Liu et al. [62] propose Progressive Neural Architecture Search, which searches for convolutional neural network architectures in the increasing order of model complexity. Therefore, Progressive Neural Architecture Search favors low complexity models that meet the requirement on prediction accuracy. Laredo et al. [57] propose Automatic Model Selection, which searches for fully connected neural networks that yield a good balance between prediction accuracy and model complexity.

Radosavovic et al. [88] investigate the network design spaces of model selection and design approaches. They propose to compare design spaces by contrasting the estimated distributions of model complexity in the network design spaces. Using their proposed method of comparing the distributions of model complexity of network design space, they investigate several popular NAS approaches, such as ENAS [85] and DARTS [63], and find that there are significant differences between the network design spaces of these approaches.

Conclusions and future directions
In this paper, we survey model complexity in deep learning. We summarize four aspects affecting deep learning model complexity, and two angles to overview existing studies on deep learning model complexity. We discuss the two major problems of deep learning model complexity, namely the model expressive capacity and effective model complexity. We overview the state-of-the-art studies on the expressive capacity from four aspects: depth efficiency, width efficiency, expressible functional space, and VC dimension and Rademacher complexity. We overview the state-of-the-art studies on the effective complexity from two aspects: general measures of effective complexity and the high-capacity low-reality phenomenon. We discuss the application of deep learning model complexity, especially in generalization capability, optimization, model selection and design.

Model complexity of deep learning is still in its infant stage. There are many interesting challenges for future works.

Expressive capacity of deep learning models is a challenging problem. For example, in most cases, deep learning models are over-parameterized and have sufficient expressive power for given tasks and data. A natural question is what expressive capacity is sufficient for a given task. In other words, can we obtain a lower bound of expressive capacity of deep learning models that are sufficient for a given task? Does a narrow layer limit the expressive capacity of a model even if the model itself has a large number of parameters?

Several studies explore the bottleneck of model size (i.e., depth, width) in the expressive capacity. That is, when the model size may become a bottleneck that restricts the expressive capacity. For example, Hanin and Sellke [38] and Lu et al. [64] discover that any deep ReLU network with width constrained by the input dimensionality has very limited expressive power. Serra et al. [92] find that smaller widths in the first few layers of a deep ReLU network cause a bottleneck on the expressive power. Kileel et al. [52] identify a bottleneck of layer width of deep polynomial networks. In a deep polynomial network, if there is a very narrow layer, no matter how wide the other layers are, the network can never correspond to a convex functional space. A convex functional space benefits the optimization and makes the model easier to train. Research on the bottleneck of expressive capacity may help to tackle many other problems, such as model design, model selection, model compression, and pruning.

Though some progress has been made, effective complexity measures are still a largely under-developed direction in deep learning. Comparing to expressive capacity of deep learning models, measuring effective model complexity is even more challenging. Effective complexity measures have to be capable of capturing fine granularity differences between two models, such as the same model architecture with two different optimization algorithms. Several previous studies [44, 60, 74, 81, 89] define and explore effective complexity measures mainly from the perspective of piecewise linear property [44, 81, 89], Fisher-Rao metric [60], or the size of trainable samples [74]. However, the effective complexity measure is still a largely unexplored and valuable direction.

Last, cross-model complexity comparison is a promising direction. Given several models with different model frameworks and different model sizes, how can we compare their expressive capacity? After these models are trained sufficiently on the same dataset, such as obtaining zero training error, how can we compare their effective complexity and further understand their generalization capability? In these cases, the cross-model complexity comparison is useful. Comparing model complexity crossing different deep models can in general help many problems of deep learning, in particular model selection and design. However, the exploration of cross-model comparison of expressive capacity or effective complexity of deep learning models is still very limited. Khrulkov et al. [51] compare expressive capacity between shallow FCNNs, CNNs, and RNNs by connecting network architectures to tensor decompositions. However, many more sophisticated models are not involved and need to be further explored.