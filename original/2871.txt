Given sequential news watch logs of users, how can we accurately recommend news articles? Compared to other items (e.g., movies and e-commerce products) for a recommendation, the worth of news articles decays quickly and massive news articles are published every second. Moreover, people frequently watch popular news articles regardless of their personal tastes to browse remarkable events at a specific time. Current state-of-the-art methods, designed for other target item domains, show low performance when they are used for news recommendation because of these peculiarities of news articles. In this paper, we propose PGT (News Recommendation Coalescing Personal and Global Temporal Preferences), an accurate news recommendation method designed with consideration of the above properties of news articles. PGT sufficiently reflects users’ behaviors by utilizing latent features extracted from both personal and global temporal preferences. Furthermore, we propose an attention-based architecture to extract adequate coalesced features from both of the preferences. We carefully tune each component of PGT to find optimal architecture. Experimental results show that PGT provides the most accurate news recommendation, giving the state-of-the-art accuracy.

Access provided by University of Auckland Library

Introduction
Given news articles and historical watch logs of users, how can we accurately recommend news articles to the users [1,2,3,4,5,6]? Nowadays, online news services have become a main source of news. Although most online news services provide search engines through various user interfaces (e.g., keyword search), finding articles satisfying a user’s interest is difficult since a massive amount of news articles are released everyday. Thus, it is crucial but challenging for online news providers to recommend personalized news articles for users to alleviate information overload and improve their experiences.

In online news ecosystem, the number of news items consumed by customers is extremely skewed toward a little portion of spotlighted items. Figure 1a shows our observations about these skewness patterns in Adressa dataset which is generated from the news watch logs in the Adresseavision, a newspaper media in Norway (see Sect. 4.1 for details); x-axis indicates the popularity ranks of news which could vary over time, and y-axis indicates the number of user interactions. As shown in the figure, users tend to prefer popular news articles mostly; we call this pattern as the popularity pattern. For instance, the top-7 most popular articles account for 80% of total consumption on average. Meanwhile, customers prefer relatively fresh news since the value of news expires quickly over time. Figure 1b shows our observations about this pattern in Adressa dataset; x-axis indicates the age of articles, and y-axis represents the average interaction ratio; given an article, its interaction ratio is computed by dividing the number of users’ interactions to the article divided by the total user interactions to the whole articles. As shown in the figure, the interaction ratio decreases sharply as the age of the article increases; we call this pattern as the freshness pattern. From these observations, we assume that these patterns reflect that user preference is temporal, and thus, the challenge of designing an accurate news recommender systems is to consider time-dependent preference while taking into account each user’s persistent preference as well. Note that the popularity and freshness patterns are analyzed from the history of user behaviors without any influence from our recommender systems. To the best of our knowledge, previous works for news recommendations neglect to model such patterns though they should be taken seriously when designing practical news recommender systems.

Fig. 1
figure 1
a popularity pattern: users tend to prefer popular news. b freshness pattern: users prefer fresh news

Full size image
In this paper, we propose PGT (News Recommendation Coalescing Personal and Global Temporal Preferences), a novel news recommendation method considering both of personal and global temporal preferences. PGT gives a recommendation for each user at time t leveraging (1) the global temporal preference at time t, and (2) watch history of the user before t.

1.
Global temporal preference A global temporal preference is a low-rank latent vector representing both popularity and freshness patterns at time t. To model this, PGT selects (1) document vectors of the most popular n articles at time t, which represent popularity pattern, and (2) document vectors of the most fresh m articles at time t, which represent freshness pattern. Consolidating both patterns is challenging since document vectors fed as input keep changing over time. To deal with this challenge, we combine these document vectors for the two different patterns by the self-attention [7] with a scaled dot product (details in Sect. 3.2).

2.
Watch history A user’s historical watch log epitomizes the user’s personal preference. The goal of PGT is to extract a user’s personal preference from a sequential pattern in the watch log with regard to the global temporal preference. For the purpose, PGT uses a bidirectional long short-term memory (BiLSTM) which has shown the best performance in encoding session-based sequential data [6] where a session is a set of consecutive behaviors of a user. Moreover, PGT combines the hidden states of BiLSTM by an attention network using the global temporal preference as context to handle time-dependent preference of a user (details in Sect. 3.3).

Fig. 2
figure 2
Illustration of personal and global temporal preferences. The global temporal preference is a time-dependent preference extracted from popular and fresh articles at a recommendation time. PGT extracts individual user’s personal preference from the user’s watch history

Full size image
The final goal of PGT is to recommend a subset of candidate articles to users with regard to their own preference. To handle this, PGT ranks candidate articles based on the similarity between the embedding vectors of the candidates and a prediction vector. The prediction vector is generated by using a fully connected neural network aggregating features of both personal and global temporal preferences at time t.

Table 1 List of symbols
Full size table
Our main contributions are as follows:

Modeling personal and global temporal preference We introduce global temporal preference which represents common and time-dependent interests shared within all users in news services at a specific time. PGT models the influences of global temporal preference on each user’s personal preference (see Fig. 2).

Attention-based architecture coalescing preferences We propose an attention-based network architecture to dynamically control weights of features in (1) the representation of global temporal preference and (2) the representation of personal preference. The global temporal preference vector is used as context in the attention network to seize the time-dependent preference of a user. With the attention, we expect that PGT effectively reflects a quick change of personal preference in the online news ecosystem.

Experiment Extensive experiments show that PGT provides the best accuracy, outperforming competitors by significant margins (see Fig. 5). We also present detailed analysis of hyperparameters.

In the rest of paper, we review related works in Sect. 2, introduce our proposed method PGT in Sect. 3, evaluate PGT and competitors in Sect. 4, and conclude in Sect. 5. Table 1 lists the symbols used in this paper. Our source code is available at https://github.com/snudatalab/PGT.

Related works
We discuss previous works on news recommendation systems.

Early studies on recommendation systems use variants of recurrent neural network (RNN) to model input sequences [8, 9]. However, these methods usually suffer from the cold-start problem. To deal with the cold-start problem, several studies enrich the embedding of newly published articles by utilizing the meta-information of articles [10]. Okura et al. [2] proposed a news recommender system based on variational autoencoder (VAE) and RNN. They used a method based on VAE to learn embedding vectors of articles, such that vectors in the same categories become similar. After learning article embeddings, they used RNNs to predict the next article vector that a user is likely to watch. Park et al. [3] proposed a news recommendation system based on RNN to model each user’s personal preference. They reranked the candidates by each user’s long-term categorical preference which is the weighted sum of categories of news articles that the user has seen; this categorical preference improved the accuracy.

Recent studies proposed attention-based methods to model users’ behaviors without RNNs. Wang et al. [11] proposed a deep news recommendation method with a CNN model [12] and a news-level attention network. They enhanced their method using the embeddings of the entities extracted from a knowledge graph. Chuhan et al. [5] used attention networks at both word level and news level to highlight informative words and news. They also proposed personalized attention by using the embedding of user ID as context to differentially attend to important words and news according to personal preference.

Note that none of the above methods model the global temporal preference, and its coalesced relation to personal preference. Thus, they show inferior performance compared to our proposed PGT (see Sect. 4). A preliminary version of PGT appeared in Koo et al. [13]; we further design various attention functions and perform extensive experiments to better understand the behavior of PGT in this work.

Proposed method
We propose PGT, an accurate method for news recommendation. We first provide a brief overview of our method in Sect. 3.1. Then, we describe how to generate a representation for the global temporal preference in Sect. 3.2, and a representation for the personal preference in Sect. 3.3. We explain the method to rank the candidate articles in Sect. 3.4. Finally, we explain how PGT is applicable in practical services in Sect. 3.5.

Fig. 3
figure 3
Architecture of PGT. To recommend news articles to a user at time t, PGT generates a prediction vector y^y^t using previous article ss1,⋯,sst−1 in the session of the user, popular articles populart at time t, and fresh articles fresht at time t. PGT ranks candidate articles candt based on their similarities to the prediction vector y^y^t. Note that we use a multilayer perceptron (MLP) layer to extract a concise feature representation by decreasing feature dimension. The attention network aggregates a set of feature vectors with individual weights calculated from its context

Full size image
Overview
We design PGT to improve the accuracy of news recommendation by balanced extraction of latent features from both of personal and global temporal preferences. PGT combines these two preferences to reflect users’ online news watch behaviors to provide personalized recommendation while considering time-dependent preference. The goal of PGT is to address the following challenges in a news recommendation system.

1.
Cold-start problem News recommendation systems need to recommend newly published articles which have no explicit user feedback since most users prefer fresh news articles. How can we rank these fresh articles when recommending them to users considering their preferences?

2.
Popularity and freshness patterns As shown in Fig. 1, the novelty of news expires promptly. How can we dynamically capture the personal and global temporal preferences to swiftly trace the trend of news?

We address the challenges by the following ideas:

1.
Global temporal preference PGT extracts time-dependent features from popular and fresh articles at a recommendation time. PGT models global temporal preference from these features to rank candidate articles accurately (details in Sect. 3.2).

2.
RNN attention using global temporal preference as context PGT encodes each user’s personal preference latent in his watch history by an RNN with an attention network. The global temporal preference is used as context of the attention network to determine the importance of each user behavior depending on the popularity and freshness patterns (details in Sect. 3.3).

Figure 3 shows the overall architecture of PGT. We use Doc2Vec [14] to get embedding vectors for popular articles populart, fresh articles fresht, articles in a session, and candidate articles candt at time t. Doc2Vec vectorizes a document such that documents containing similar words have similar vector representations. PGT generates a prediction vector y^y^t from personal preference utut and global temporal preference τtτt. Finally, PGT ranks candidate articles candt by their similarities to y^y^t.

Global temporal preference
To encode common interests shared by all users at a specific time, PGT generates a single vector of global temporal preference representing time-dependent features extracted from several popular and fresh articles. To achieve the purpose, we adapt self-attention [7]; however, instead of generating vector representation for each key, we generate a single vector by computing the weighted average of article vectors using the attention weights. We expect that this variant of self-attention effectively highlights the common features emerging from popular and fresh articles.

To collect popular articles, we need to determine how to measure the popularity of an article at a specific time. We assume that the popularity of an article is proportional to its recent view count. We calculate the view count of each article during the recent 3 hours from each time step and pick top-kp popular articles. Our method to get fresh articles is more straightforward. For each article, we record the latest watch time by users. We pick top-kf fresh articles at a specific time where freshness is determined by the latest watch time.

PGT generates the global temporal preference vector ττt as follows:

vgiττt=1de−−√eeTi∑eej∈Eeej,=Wga∑eei∈Eexp(vgi)∑jexp(vgj)eei+bbga,
where a linear layer, consisting of parameter Wga and bbga, calculates the global temporal preference vector ττt∈Rn at time t. The weighted average of popular and fresh article vectors eei∈E={populart∪fresht} passes through a linear layer where the weights come from the unnormalized attention score vgi for each article i. de denotes the dimensionality of an article vector ee.

For instance, consider two popular articles and one fresh article. We denote the three article vectors as E={ee1,ee2,ee3}, respectively. The unnormalized attention scores of the three articles are calculated as follows:

vg1=1de−−√eeT1∑eej∈Eeej,vg2=1de−−√eeT2∑eej∈Eeej,vg3=1de−−√eeT3∑eej∈Eeej.
Note that if an article vector is far from the others in E, its attention score is low. This leads to reflect more information of the major articles of E in the global vector. The scores are normalized in a softmax manner so that their sum equals 1. The global temporal preference vector ττt is then computed through a linear layer after {ee1,ee2,ee3} are summed up with the attention scores. Assume the attention scores for {ee1,ee2,ee3} are calculated as 0.4, 0.2, and 0.4, respectively. The global preference vector ττt is then calculated as follows:

ττt=Wga(0.4ee1+0.2ee2+0.4ee3)+bbga.
The information of ee1 and ee3 is considered equally large, but that of ee2 is considered relatively less since ee2 is far from ee1 and ee3.

Personal preference
Considering individual user’s personal preference is crucial for personalized news recommendations. To extract each user’s personal preference well, we exploit our observation that each previous behavior of a user corresponds to a preference at that time. From this observation, we extract each user’s personal preference from its previous watch log using an RNN model and aggregate them by an attention network to generate a vector representing the user’s news preference. We use bidirectional LSTM (BiLSTM) in the RNN model since it is well known to summarize both the preceding and the following behaviors. Note that the hidden state hhi of BiLSTM represents the behavior of the user at time i. Then, we aggregate all previous hidden states hh1,…,hht−1 with an attention network by utilizing the global temporal preference vector ττt as context vector. The attention network traces the popularity and freshness patterns of online news services to highlight important previous hidden states. Note that we record all previous hidden states of BiLSTM as a user’s watch history for efficient computation.

PGT generates a personal preference vector uut as follows:

hhivuiuut=f(ssi;θ),=attn(ττt,hhi),=∑iexp(vui)∑jexp(vuj)hhi,
where f is a BiLSTM function, hhi is i-th hidden state vector in the session, and ssi∈Rde denotes the representation of the i-th selected article in the session. An attention function attn(ττt,hhi) takes each hidden state hhi and the global temporal preference ττt as inputs and then calculates the unnormalized attention score vui. We generate the personal preference vector uut at time t by calculating the weighted average of hidden states using attention scores.

We adapt various attention functions of attn(hhi∈Rnh,ττt∈Rnτ) (see Table 2) to find an optimal architecture as follows:

Table 2 Variants of attention functions
Full size table
PGT A1: in this variant, we use a linear layer to calculates vui.

vui=wa1[ττt,hhi]+ba1
where a linear layer, consisting of parameters wa1∈R1×nhnτ and ba1∈R, takes a concatenated vector [ττt,hhi] of ττt and hhi as input and then calculates vui for each hidden state hhi.

PGT A2: this variant adapts the Bahdanau attention [15] to PGT.

vui=wwa2Ttanh(Wa2[ττt,hhi])
where a concatenated input vector [ττt,hhi] is multiplied to Wa2∈Rna2×nτnh to produce an intermediate output vector, and then, the intermediate output is passed through the hyperbolic tangent function. Finally, this variant performs a dot product with a parameter vector wwa2∈Rna2 to calculate vui.

PGT A3: in this variant, a general score function [16] is adapted to PGT.

vui=ττtTWa3hhi
where a parameter Wa3∈Rnτ×nh is multiplied to ττt and hhi.

PGT A4: we perform a dot product of ττt and hhi to calculate vui in this variant.

vui=ττtThhi
where we set the dimension of ττt and hhi to be the same and then calculate a dot product of them without any learnable parameter.

PGT A5: this variant uses a scaled dot product to calculate vui.

vui=ττtThhin−−√
where we scale an unnormalized attention score vui using the dimension size n of ττt∈Rn and hhi∈Rn.

We show that PGT A3 shows the best experimental results compared to the other variants (see Sect. 4.6).

For instance, consider two articles in a session. We denote the two articles as ss1 and ss2, respectively. The hidden state vectors of the articles in a BiLSTM function f are represented as hh1=f(ss1;θ) and hh2=f(ss2;θ), respectively. If we utilize PGT A3 for attn(⋅), the unnormalized attention scores of the two articles are calculated as follows:

vu1=ττtTWa3hh1,vu2=ττtTWa3hh2.
Note that if an article vector is close to the linearly transformed global preference vector ττTtWa3, the attention score of it is calculated high. This enables to reflect more information of the articles which are highly correlated with the global preference vector ττt in representing the personal preference. The scores are normalized in a softmax manner so that their sum equals 1. The personal preference vector uut is then calculated by the weighted sum using the attention scores. Assume the attention scores for ss1 and ss2 are 0.7 and 0.3, respectively. The personal preference vector uut is then calculated as uut=0.7hh1+0.3hh2. The information of ss1 is considered more than that of ss2 since ss1 is more related to the global preference vector ττt than ss2 is.

Ranking candidate articles
Based on the preference of a user at each recommendation time, we rank candidate articles to recommend a subset of them to the user. Ideally, candidate articles should include all of the existing news articles; however, in practice, we need to narrow down candidates for guaranteeing scalability of recommender systems. We select candidate articles by removing unpopular articles at each time step. In the training process, we ensure that the selected article is in the candidate articles for each interaction by replacing the most unpopular candidate article with the actually selected article.

Given a session of a user, let sst∈Rde denote the representation of the t-th selected article in the session. ττt∈Rn represents the global temporal preference at the t-th time step in the session. As discussed in Sect. 3.2, PGT generates τt using both popular articles and fresh articles; this allows PGT to consider popularity and freshness as important factors for recommendation, which alleviates the cold-start problem. uut∈Rn denotes a user’s personal preference at the t-th time step in the session, allowing personalized news recommendation.

We generate the prediction vector y^y^t as follows:

y^y^t=Wo[ττt,uut]+bbo
where the vector created by concatenating ττt and uut is passed through a linear layer consisting of parameters Wo and bbo. Then, we rank candidate articles at the t-th time step based on its similarity to y^y^t.

We train PGT to minimize the L2 distance between the truly selected article vector sst and the prediction vector y^ty^t as follows:

L(sst,y^y^t)=∥sst−y^y^t∥2.
We backpropagate gradients calculated from this loss function to the MLP, the attention network, and the RNN, while fixing the article embeddings. As a result, PGT is trained to reduce the distance between y^y^t and sst, which leads to increasing the distance between y^y^t and vectors of unselected articles.

Fig. 4
figure 4
Illustration of PGT implementation in practice

Full size image
Implementation in practice
Figure 4 depicts the implementation example of PGT when one designs a practical recommender system. To provide news recommendation using PGT efficiently, the system needs to prepare (1) Doc2Vec embedding vectors of all articles, (2) each user’s watch history in the order of interaction time, and (3) the global preference vector for each inference time.

For instance, consider newly published news articles. They are first vectorized by Doc2Vec. The system encodes a user’s watch history as a sequence of article ids to reduce the memory consumptions. When a user logs in to the service, the personal preference vector of the user is extracted by PGT using the user’s previous watch history. Meanwhile, the system keeps updating the global preference vector using all user interactions. A prediction vector is computed with the prepared personal preference vector and the global preference vector at the inference time. Finally, the system prepares a set of candidate articles by its own policy such as popularity and freshness of news and ranks the candidate articles by utilizing the prediction vector. Then, highly ranked articles are recommended to the user.

Experiment
We run experiments to answer the following questions.

Q1. Accuracy (Sect. 4.2). How well does PGT recommend news articles?

Q2. Effect of modeling global temporal preference (Sect. 4.3). Does the modeling of global temporal preference help improve the accuracy?

Q3. Effect of attention network in personal preference (Sect. 4.4). How well does the attention network for the personal preference help improve the accuracy?

Q4. Optimal numbers of popular and fresh articles in global temporal preference (Sect. 4.5). What is the best number of popular and fresh articles in global temporal preference to improve the accuracy?

Q5. Optimal attention network in personal preference (Sect. 4.6). What is the best architecture of attention network in personal preference?

Q6. Optimal recurrent network in personal preference (Sect. 4.7). What is the best structure of recurrent network in learning personal preference from the user watch history?

Experimental settings
Dataset We evaluate with ADRESSA [17] and GLOBO [18] which are session-based datasets of news watch history (see Table 3). Adressa datasetFootnote1 is generated from the news watch behaviors of users in the Adresseavision, a newspaper media in Norway. The one week version ADRESSA 1W of the dataset contains news information from 1 to 7 January 2017. The full version ADRESSA 10W of the dataset contains news information from 1 January to 31 March 2017. The dataset contains URLs of all articles, and contents of a subset of the articles; articles with invalid URLs are removed. The second dataset GLOBOFootnote2 [18] contains news information from a news portal G1 (G1.com) from 1 to 16 October 2017. Instead of revealing the original news contents, it provides the embedding vector for each news article.

Table 3 Description of news datasets
Full size table
Competitor We compare PGT to the following competing methods.

POP This method recommends the most popular items regardless of each user’s personal preference.

Park et al. [3] To recommend the next article, this method ranks the candidate articles using a hidden vector generated from RNNs and then reranks candidates by each user’s long-term categorical preference. The authors also proposed a CNN model to infer missing categories of articles from their contents.

Okura et al. [2] This method recommends articles based on the similarity of articles using dot products of their vector representations. The method generates similar vector representations to articles with similar categories.

WEAVE&REC [4]. This method utilizes the content of news articles as well as the sequence in which the articles were read by users. The authors use three-dimensional CNN to embed both (1) the word embeddings of articles and (2) the sequence of articles selected by users at the same time.

HRAM [6]. This method aggregates outputs of two heterogeneous methods which are (1) user-item matrix factorization to model the interaction between users and items and (2) attention-based recurrent network to trace the interest of each user.

NPA [5]. This method uses personalized attention at both word level and news level to highlight informative words and news. The personalized attention uses the embedding of each user as context vector to differentially attend to important words and news according to personal preference.

Vectorized representation of article For ADRESSA, we train Doc2Vec model [14] with Gensim [19], which has shown a good performance on news recommendation [11]. We use sentences of each article if provided by the dataset, or use crawled sentences using the URL of it otherwise. We set the dimension of embedding vector to 1000, and the size of window to 10. We initialize the learning rate α of Doc2Vec to 0.025 and decrease α by 0.001 for every 10 epochs. Note that these values are selected since they give the best result. For GLOBO, we use the provided embedding vector for each article.

Evaluation metrics We evaluate the accuracy of methods using hit rate (HR) and mean reciprocal rank (MRR). Given the probability ranks of truly seen articles, we calculate HR@5 and MRR@20 as follows:

HR@5MRR@20=1|I|∑i∈I|{ri|ri<=5}|=1|I|∑i∈Ici,ci={1ri,0,if ri≤20otherwise
where i∈I is an index of an article in the test data, and ri is the estimated rank of i by a method. HR@5 is the proportion of predictions where the truth is within the top 5 articles with the highest scores. MRR@20 gives a higher score when ri is more accurate, but scores nothing if ri is above 20.

Model training All of the competitors and our method are trained using the same hardware and early stop policy. We divide our session data into training, validation [20], and test sets with ratio of 8:1:1 based on the user interaction time in a session. For example, when a dataset consists of user interactions from 1 to 10 May, data from 1 to 8 May are used as a training set, data on 9 May as a validation set, and data on the last day as a test set. This setting is useful to show the effect of the cold-start problem, since several fresh articles in the test set are not included in the training set.

Hyperparameters We train methods to maximize the similarity between a prediction vector and the corresponding selected article vector for every time step. For PGT, we use the mean squared error (MSE) of the two vectors as a loss function, and Adam optimizer [21] as an optimizer to update weights with a learning rate of 0.003, since it gives the best result compared to stochastic gradient descent (SGD) [22] and Adagrad [23]. For global temporal preference in PGT, we set the numbers of popular and fresh articles to 5 and 3, respectively, because it gives the best accuracy (see Sect. 4.5). To model personal preference in PGT, we select PGT A3 as an attention function because it outperforms other variants (see Sects. 3.3 and 4.6). We set the dimension of document vector to 1000 and then extend it to 1024 using an MLP to generate global temporal preference vector. We set the size of hidden state in BiLSTM to 1024 with a single layer when modeling personal preference from the user watch histories. PGT generates a personal preference vector by aggregating the hidden states using an attention network, so the dimension of the generated vector is the same as that of the hidden state vector. We use the hyperbolic tangent as a nonlinear activation function, but omits it for the last MLP since it gives the best accuracy. For the competitors, we follow their best settings. We use mini-batched inputs of size 512 to feed models during training. When the validation loss keeps increasing for 10 epochs, we early stop training to prevent overfitting. All methods in our experiments early stopped before 200 epochs of training.

Recommendation accuracy
Table 4 PGT shows the best performance for all of the datasets
Full size table
Fig. 5
figure 5
Performance comparison between PGT and competitors. PGT gives the best accuracy, by considering both of global and personal preferences

Full size image
Table 4 and Fig. 5 show accuracies of PGT and competitors; WEAVE&REC and NPA, which require news contents, are not evaluated for GLOBO since it does not provide news contents. Note that PGT gives the best accuracy compared to the competitors. PGT -T and PGT -A in Fig. 5 are the variants of PGT described in Sects. 4.3 and 4.4, respectively. We have the following observations from the results.

First, even though POP recommends news articles based only on the general popularity, the popularity pattern of news data (shown in Fig. 1) makes POP a strong baseline showing a good performance. Meanwhile, the performances of the other competitors (Park et al. [3], Okura et al. [2], HRAM, WEAVE&REC, and NPA) are less accurate especially in ADRESSA. The reason is that they consider only personal preference, while users’ behaviors in ADRESSA are more skewed toward the popularity pattern. Note that HR@5 of POP is the probability of watching one of the top 5 most popular articles. HR@5 of POP in ADRESSA 10W, ADRESSA 1W, and GLOBO are 0.5672, 0.4988, and 0.2845, respectively; this shows that users’ interactions in ADRESSA are more skewed to popular articles which are more related to global temporal preference rather than personal preference. On ADRESSA 10W where users’ interactions are skewed to popular articles, MRR@20 of Park et al. [3], Okura et al. [2], WEAVE&REC, and HRAM decreases by 0.1274, 0.1415, 0.1634, and 0.1204, respectively, compared to that of POP; the result indicates that these methods less utilize popularity information. On the other hand, our PGT shows the best performance even on ADRESSA compared to the others by appropriately attending to personal and global temporal preferences.

Second, the methods modeling each user’s watch history by utilizing attention network (PGT, HRAM, and NPA) are more accurate compared to the other competitors (Park et al. [3], Okura et al. [2], and WEAVE&REC). The attention network dynamically attends to important previous behaviors and thus increases recommendation accuracy. Meanwhile, due to the property of RNN, inference in RNN-based methods (Park et al. [3] and Okura et al. [2]) often neglects users’ long-term behaviors. WEAVE&REC captures a temporal pattern of each user’s watch history using 3-D CNN, but gives poor recommendations to fresh users because of their insufficient watch histories.

Finally, HRAM and NPA, which train an embedding for each user, show worse performance compared to our proposed PGT. Note that we divide the dataset into training, validation, and test sets based on user interaction time (see Sect. 4.1), since such setting is more realistic for online news recommendation. However, this makes it very hard for HRAM and NPA to train the embeddings of fresh users well. On the other hand, PGT performs accurate news recommendation even in this case, by exploiting the global temporal preference.

Effect of modeling global temporal preference
The global temporal preference helps PGT overcome the cold-start problem. We assume that all neural network-based methods suffer from the cold-start problem more severely on ADRESSA 10W since the time gap between training and test sets of it is the longest among all the datasets. In Table 4, we compare the performances of PGT and the other neural network-based methods on ADRESSA 1W and ADRESSA 10W to show how well the global temporal preference helps preserve the accuracy from the cold-start problem, since the other methods neglect the global temporal preference. MRR@20s of Park et al. [3], Okura et al. [2], WEAVE&REC, HRAM, and NPA decrease by 26.78%, 30.56%, 30.29%, 26.89%, and 21.42%, respectively, on ADRESSA 10W compared to those on ADRESSA 1W; on the other hand, MRR@20 of PGT decreases only by 13.91% on the same setting. This shows that PGT better handles the cold-start problem.

Table 5 The global temporal preference and the attention network of PGT improve the accuracy of recommendation
Full size table
We perform an ablation study to show the effects of the global temporal preference in PGT with regard to the recommendation accuracy. We evaluate the performance of PGT -T, a variant of PGT that does not use the global temporal preference, but keeps the attention network of BiLSTM by using the most recent hidden state vector as the context of attention. Columns PGT -T and PGT of Table 5 show that (1) MRR@20 of PGT improves by 22.17%, 12.76%, and 5.66% on ADRESSA 1W, ADRESSA 10W, and GLOBO, respectively, and (2) HR@5 of PGT improves by 28.20%, 12.35%, and 6.26% on ADRESSA 1W, ADRESSA 10W, and GLOBO, respectively, compared to PGT -T which does not use the global temporal preference. This result shows that the global temporal preference helps model popularity and freshness patterns well, leading to a better performance.

Effect of attention network in modeling personal preference
We perform an ablation study to show the effects of the attention network in modeling personal preference. In the study, we evaluate the performance of PGT -A, a variant of PGT, where we set uniform weights when combining hidden states in the attention of the session RNN. Columns PGT -A and PGT of Table 5 show the accuracy improvements of PGT by the attention network. MRR@20 of PGT increases by 0.74%, 12.10%, and 1.68% on ADRESSA 1W, ADRESSA 10W, and GLOBO, respectively. HR@5 of PGT increases by 2.73%, 2.90%, and 3.41% on ADRESSA 1W, ADRESSA 10W, and GLOBO, respectively. This shows that the attention network highlights important previous hidden states, leading to a superior accuracy.

Fig. 6
figure 6
Attention weights of previous hidden states derived from a sample news watch log. The figure shows that the attention network reacts differently to the same news watch log as the inference time is changed. At times t1 and t2 where ‘sport’ and ‘general’ categories are popular, respectively, the attention network gives more weights to articles in the same categories

Full size image
To demonstrate the effects of the attention network more intuitively, we additionally perform a case study of the attention network to see visually whether it effectively highlights important users’ behaviors by considering popular topics. We sample a news watch log of a user and perform a news recommendation using PGT at different inference times t1 and t2. From the different sets of popular and fresh articles, we generate global temporal preference vectors for the inference times. We then generate the personal preference vector of the user by passing through the news watch log to BiLSTM and aggregating all of hidden states by the attention network. Note that we feed the same news watch logs to BiLSTM but the different global temporal preference vector as the context of attention network for each inference time. Figure 6 shows a heat map visualizing attention weights of a sample news watch log at two different inference times t1 and t2; a darker cell means a higher weight. At times t1 and t2 where ‘sport’ and ‘general’ categories are popular, respectively, the attention network gives more weights to articles in the same categories. This result illustrates that PGT dynamically models the personal preference by considering the popular topics at the inference time.

Optimal numbers of popular and fresh articles
Table 6 Grid search of the optimal numbers of popular and fresh articles in PGT
Full size table
We perform extensive experiments to find optimal numbers of popular and fresh articles in global temporal preference in PGT. Table 6 shows the grid search results when adjusting the number kp of popular articles and the number kf of fresh articles. With too small kp and kf, PGT suffers from a lack of features representing global temporal preferences; on the other hand, excessive kp and kf lead to overfitting. PGT shows the best validation accuracy with 5 popular articles and 3 fresh articles on ADRESSA 1W, ADRESSA 10W, and GLOBO.

Optimal attention network in personal preference
We evaluate which attention network introduced in Sect. 3.3 gives the best performance. Note that the goal of the attention network is to aggregate all the hidden states of BiLSTM by highlighting important user behaviors.

Table 7 Accuracy of various attention networks for modeling personal preference
Full size table
Table 7 shows the accuracy of various attention networks used to model the personal preference of PGT. PGT A3 shows the best accuracy on ADRESSA 1W and ADRESSA 10W. The accuracy of PGT A3 on GLOBO is not the best but still acceptable. Meanwhile, the accuracy of PGT A1, PGT A4, and PGT A5 is poor on all the datasets. PGT A2 shows similar accuracy as PGT A3 on ADRESSA 1W and GLOBO, but inferior on ADRESSA 10W. We conclude that PGT A3 shows accurate recommendations in a stable manner compared to the other variants.

Optimal recurrent network in personal preference
Wideness vs deepness We evaluate optimal architectures with regard to wideness and deepness of recurrent network in personal preference. A deeper and wider neural network generally improves the representative power of the network [24], but it is not always the best option. A narrow but deep network has a high model capacity, but it might lead to an overfitting. Thus, finding a balance in deepness and wideness of the network is important [25].

Table 8 Evaluation of wideness versus deepness of PGT in terms of accuracy
Full size table
Table 8 shows the accuracy of PGT with varying deepness and wideness of recurrent networks in personal preference. We adjust the number of layers and size of the hidden state while keeping similar number of learnable weights. Note that the most shallow model provides the best overall performance; the deeper layers seem to make the model overfitted.

Effect of BiLSTM We evaluate the effect of bidirectional LSTM on modeling personal preference. We compare PGT with the following variants.

1.
PGT F: uses only the forward hidden states of BiLSTM.

2.
PGT B: uses only the backward hidden states of BiLSTM.

Table 9 Both forward and backward directions of hidden states in BiLSTM help improve the recommendation accuracy of PGT. PGT outperforms both (1) PGT F, a variant of PGT using only the forward hidden states, and (2) PGT B, a variant of PGT using only the backward hidden states
Full size table
Table 9 shows the result. Note that the accuracy of PGT is maximized when utilizing both the forward and backward hidden states of BiLSTM. Compared to PGT, (1) MRR@20 of PGT F decreases by 0.2%, 3.7%, and 1.6% on ADRESSA 1W, ADRESSA 10W, and GLOBO, respectively, and (2) MRR@20 of PGT B decreases by 10.7%, 8.3%, and 8.0% on ADRESSA 1W, ADRESSA 10W, and GLOBO, respectively.

Conclusion
We propose PGT, a novel news recommender system exploiting both personal and global temporal preferences to precisely reflect users’ behaviors. We observe that the popularity and the freshness of articles play important roles in users’ watch behaviors. Based on the observation, we devise the concept of global temporal preference to news recommender system, to give accurate recommendation results based on time. We also propose an attention-based architecture to effectively deal with changes of users’ personal preferences, with regard to the global temporal preference. Through experiments, we show that PGT gives the most accurate news recommendation, outperforming all competitors by a significant margin.

Even though PGT provides an accurate news recommendation, it still has several limitations. First, PGT neglects the cascading effects of recommendation. For instance, popular articles are getting more likely to be recommended by PGT since it reflects the popularity pattern in the attention mechanism. Second, PGT simply uses Doc2Vec to represent a news. Doc2Vec interprets the meaning of article through words that appear together, but it is difficult to interpret the contextual information. Thus, the performance of PGT is bounded to the representation capability of Doc2Vec. Other representation method could be used for PGT as well. Lastly, PGT captures a user’s personal preference only from the user’s watch history session. However, utilizing multiple heterogeneous information such as users’ click history and genres of articles may be useful for recommendation to newly joined users. Our future works include extending the PGT to handle such limitations.