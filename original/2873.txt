The minibatching technique has been extensively adopted to facilitate stochastic first-order methods because of their computational efficiency in parallel computing for large-scale machine learning and data mining. Indeed, increasing the minibatch size decreases the iteration complexity (number of minibatch queries) to converge, resulting in the decrease of the running time by processing a minibatch in parallel. However, this gain is usually saturated for too large minibatch sizes and the total computational complexity (number of access to an example) is deteriorated. Hence, the determination of an appropriate minibatch size which controls the trade-off between the iteration and total computational complexities is important to maximize performance of the method with as few computational resources as possible. In this study, we define the optimal minibatch size as the minimum minibatch size with which there exists a stochastic first-order method that achieves the optimal iteration complexity and we call such a method the optimal minibatch method. Moreover, we show that Katyusha (in: Proceedings of annual ACM SIGACT symposium on theory of computing vol 49, pp 1200–1205, ACM, 2017), DASVRDA (Murata and Suzuki, in: Advances in neural information processing systems vol 30, pp 608–617, 2017), and the proposed method which is a combination of Acc-SVRG (Nitanda, in: Advances in neural information processing systems vol 27, pp 1574–1582, 2014) with APPA (Cotter et al. in: Advances in neural information processing systems vol 27, pp 3059–3067, 2014) are optimal minibatch methods. In experiments, we compare optimal minibatch methods with several competitors on L1-and L2-regularized logistic regression problems and observe that iteration complexities of optimal minibatch methods linearly decrease as minibatch sizes increase up to reasonable minibatch sizes and finally attain the best iteration complexities. This confirms the computational efficiency of optimal minibatch methods suggested by the theory.

Access provided by University of Auckland Library

Introduction
Along with the increasing demand of large-scale machine learning and data mining, its efficient computation has become much more important. Among several efficient optimization techniques, stochastic optimization has been a mainstream for accelerating the execution of large-scale machine-learning training. Several attempts at enhancing stochastic optimization have been carried out, such as Nesterov-type acceleration and variance reduction techniques. Moreover, minibatch techniques are nowadays often used to scale up to massive datasets under parallel computation [5,6,7,8,9,10,11,12,13]. By combining these sophisticated techniques, optimization methods have been progressively improved over the past few years. These methods are validated based on analyses of their theoretical convergence rate. Besides upper bound, it is also important to derive the lower bound of computational complexity which cannot be outperformed by any method to determine which method is the optimal one.

As for the convex regularized empirical risk minimization problem, which is a typical machine learning problem setting, optimal first-order methods [1, 2] that perform the smallest number of stochastic gradient evaluations have already been obtained via accelerated stochastic variance-reduction methods. Before these methods were proposed, several variance reduction techniques such as SAG [14], SDCA [6], SVRG [15], and SAGA [16] had been developed. These methods achieve linear convergence rates for strongly convex objectives, whereas the vanilla stochastic gradient descent (SGD) can only achieve sublinear convergence rates. An acceleration technique called Acc-SVRG for SVRG that employs Nesterov’s acceleration was proposed by [3], which used a minibatch technique. Meta algorithms to accelerate existing methods by combining with an inexact proximal point method were also proposed in [17, 18]. Katyusha [1] and DASVRDA [2] are recently proposed pure first-order primal methods for accelerating SVRG. Instead of applying meta algorithms, Katyusha employs the so-called “negative-momentum” to stabilize the accelerated updates and DASVRDA employs the “outer-acceleration” technique in addition to the inner-acceleration technique of Acc-SVRG. These methods can achieve the optimal total computational complexity as described by [19] and [20], that is, they require the smallest number of gradient evaluations (i.e., number of access to an example) to achieve a pre-specified accuracy.

One big issue here is that the lower bound of the optimal computational complexity has been evaluated without taking into account the minibatch strategy, that is, only one example per iteration is considered for the first-order stochastic methods. This is problematic because the minibatch strategy is quite standard nowadays and is effective in parallel computing for performing faster computations. On the other hand, an extreme opposite case is the deterministic method, which uses all the data per iteration instead of one example. Moreover, it is well known that Nesterov’s acceleration achieves the optimal “iteration complexity" in the case of deterministic first-order methods, that is, it requires the smallest number of iterations (i.e., number of minibatch queries). However, the deterministic method requires O(n) computations per iteration, where n is the sample size, and thus does not achieve the optimal total computational complexity. Here, we have two extremes: (1) accelerated stochastic methods achieving the optimal total computational complexity with minibatch size of one, and (2) the accelerated deterministic method achieving the optimal iteration complexity with minibatch size of n. We can see that the minibatch size implies a trade-off between total computational complexity and iteration complexity. The running time is proportional to the iteration complexity if processing a minibatch in parallel and ignoring communication cost, hence, the running time can be reduced by taking large minibatch sizes. However, unnecessarily large minibatches are not preferable because this gain is usually saturated and the total computational complexity is deteriorated. Hence, the determination of an appropriate minibatch size which controls the trade-off between the iteration and total computational complexities is important to maximize performance of the method with as few computational resources as possible. The fundamental question we aim to address is

How small can the minibatch size be while maintaining the optimal iteration complexity?

Table 1 The optimal computational complexities for strongly convex problems with the minibatch size b, the sample size n, the condition number κ, and the required accuracy of the solution ϵ
Full size table
Table 2 The optimal computational complexity for non-strongly convex problems with the minibatch size b. the sample size n, the Lipschitz smoothness L, and the required accuracy of the solution ϵ
Full size table
In this paper, we investigate the smallest minibatch size to bring out the computational efficiency under parallel computation environment in the minimax sense. That is, we define the optimal minibatch size as the smallest minibatch size with which there exist a stochastic first-order method that achieves the optimal iteration complexity and we call such a method the optimal minibatch method. In our theory, we precisely estimate the optimal minibatch size with the existence of optimal minibatch methods and show this size does not affect the optimal total complexity as well. Interestingly, we can show that the optimal minibatch size can be maximally reduced to n−−√ instead of n when a problem is strongly convex and ill-conditioned or when a problem is non-strongly convex and a required accuracy of the solution is sufficiently small. This is quite beneficial for parallel computation because only O(n−−√) parallelization is sufficient for achieving the best running time. Moreover, we also introduce concrete algorithms that perform this optimal minibatch size strategy. Our contribution is summarized as follows (see also a summary of the optimal minibatch sizes and computational complexities in Table 1 for strongly convex problems and Table 2 for non-strongly convex problems):

We specify the minimax optimal minibatch size. Let L be the smoothness parameter, μ be the strong convexity parameter, and κ:=L/μ be the condition number. Let ϵ>0 be a required accuracy of the solution. Then, the optimal minibatch size is O~(n−−√+n/κ−−√) for strongly convex problems and is O~(n−−√+nϵ/L−−−√) for non-strongly convex problems, where O~ hides logarithmic terms.

We show that Acc-SVRG [3] with the approximate proximal point algorithm (APPA) [18], Katyusha [1], and DASVRDA [2] are the (nearly) optimal mini-batch methods that achieve the optimal iteration complexity using the optimal minibatch size.

We prove that optimal minibatch methods can be characterized by the methods that achieve the optimal iteration and total complexities simultaneously for some minibatch sizes.

We validate our theory in numerical experiments on L1-and L2-regularized logistic regression problems. We observe that iteration complexities of optimal minibatch methods linearly decrease as minibatch sizes increase up to reasonable minibatch sizes and finally attain the best iteration complexities. This indicates the existence of the optimal minibatch size and total computational efficiency is lost if using minibatch sizes larger than optimal one as predicted from our theory.

Related Work. Stochastic optimization methods often suffer from the variance of gradient estimation, resulting in the slow convergence regarding the number of iterations. Therefore, the development of the method to reduce this variance is very important research direction. For this purpose, several stochastic estimators of the gradient such as SAG [21], SVRG [15], SAGA [16], and SARAH [22] were developed and certain speedup by the variance reduction was achieved. Moreover, several acceleration methods [1, 2] were also proposed.

Another popular way for variance reduction is minibatching where a stochastic gradient estimator consist of a bunch of examples. The advantage of minibatching is the speedup under parallel computation. Stochastic methods without minibatching are difficult to parallelize because they are inherently sequential, whereas minibatching methods can be easily parallelized by processing minibatches with several computational nodes. Because of this, many studies were devoted to developing efficient minibatching methods and complexity analysis. For instance, [23] studied a stochastic gradient descent with minibatching and noted the benefit of minibatching under parallel computation and [5] introduced a minibatch variant of accelerated gradient descent with a convergence rate analysis. Combinations of minibatching with a dual coordinate method and variance reduction method were also proposed in [6, 12] and in [11], respectively. Moreover, the existence of the optimal minibatch method was indicated in [3] by proposing Acc-SVRG which is a combination of accelerated method, variance reduction, and minibatching. There are many other machine learning methods that incorporate minibatches [7,8,9,10, 13].

Recently, [24] has shown linear speedup of total computational complexity of SAGA [16] up to a certain minibatch size, which is also referred as to the optimal minibatch in [24]. This feature is similar to the optimal minibatch methods in our paper. However, we note that SAGA is a variance reduced method for stochastic gradient descent, resulting in the same iteration complexity as that of gradient descent. On the other hand, we define the optimal minibatch methods as the methods using the optimal minibatch size that achieve the optimal iteration complexity as fast as accelerated gradient descent [25]. Thus, notions of these two optimal minibatch sizes are different from each other.

MapReduce is another famous distributed processing scheme which aims at scaling up to large-scale datasets. The MapReduce framework divides input data into data chunks and distributes them to computational nodes. MapReduce mainly consists of map and reduce operators which manipulate and merge key/value pairs and these functions are automatically executed in parallel. To extract useful knowledge from the bigdata, many machine learning methods run on the MapReduce framework were extensively proposed. For instance, a learning tree model [26], k-NN join [27], hierarchical clustering [28], density-based clustering [29], k-means clustering [30], and extreme learning machine [31,32,33,34] were adapted to the MapReduce framework.

Preliminary
In this section, we introduce some notations and definitions. The problem treated in this paper is formalized as follows:

minx∈Rd{f(x)=1n∑i=1ngi(x)},
(1)
where n∈N is the number of functions and gi is the Lipschitz smooth (strongly) convex function. Lipschitz smoothness and strong convexity are frequently used in optimization literature to ensure the convergence of stochastic optimization methods. Let ⟨⋅,⋅⟩2 and ∥⋅∥2 denote the Euclidean inner product and norm, respectively.

Definition 1
Let g:Rd→R be a differentiable function. We say g is L-smooth if ∃L>0, ∀x,∀y∈Rd,

∥∇g(x)−∇g(y)∥2≤L∥x−y∥2.
We say g is μ-strongly convex if ∃μ>0, ∀x,∀y∈Rd,

g(x)+⟨∇g(x),y−x⟩2+μ2∥y−x∥22≤g(y).
We say g is non-strongly convex (that is, 0-strongly convex) if g is not strongly convex but convex.

For strongly convex problems, the ratio κ=L/μ is referred to as the condition number, which is a very important value because it usually affects the convergence speed of gradient-based methods. Depending on the number of instances n, the Lipschitz smoothness L, the strong convexity μ, and the required accuracy of the solution ϵ, optimal complexities can be explained. In this paper, we assume that functions gi are L-smooth and μ-strongly convex including the case of μ=0.

Stochastic minibatch methods
It is well known that the convergence rates of stochastic gradient methods are essentially composed of two terms: one is a deterministic optimization term (a rate in the noiseless case) and the other is a variance term. Thus, variance reduction techniques are important for accelerating the convergence speed of stochastic methods. The simplest technique is the minibatching where multiple instances are used in a single stochastic gradient estimator. Concretely, a step of stochastic gradient descent with minibatching at k-th iterate xk can be described as follows:

xk+1←xk−ηk∇gIk(xk),
where gIk denotes the average 1b∑i∈Ikgi of functions indexed by a minibatch Ik of size b sampled from {1,…,n} randomly or deterministically. There are also several minibatch variants of stochastic optimization methods other than the vanilla stochastic gradient descent. A first-order stochastic minibatch method is formally defined as follows.

Definition 2
Let Ik⊂{1,…,n} be a randomly or deterministically sampled minibatch of size b. We call an algorithm A(b) a first-order stochastic minibatch method for solving problem (1) if A(b) can only access oracle [gIk,∇gIk] at the k-th iterate and A(n) becomes a deterministic optimization method (i.e., a type of gradient methods).

Note that a deterministic sampling is also considered to cover SVRG and so on. As described later, we consider a more specific algorithm class A of A(b) for which a worst-case example with respect to the number of stochastic gradient evaluations exists. For instance, such an algorithm class is given by [19] and [20].

Computational complexity
To evaluate the performance of stochastic optimization methods for problem (1), we need to introduce the convergence criterion and computational complexities. Because global convergence is guaranteed in most of stochastic optimization methods for convex problems, the objective gap is adopted as a convergence criterion. That is, the convergence criterion is the following. For a given threshold ϵ>0,

E[f(xk)−f∗]≤ϵ,
where xk is k-th iterate obtained by the stochastic optimization method, f∗=infx∈Rdf(x), and the expectation is taken with respect to the history of stochastic gradients.

The performance of stochastic optimization methods can be evaluated by the cost spent to find an ϵ-accurate solution in terms of the above criterion. There are two types of costs commonly used in the (stochastic) optimization literature.

Definition 3
For an algorithm A(b) and an objective function f, the total complexity T(A(b), f) is the number of stochastic gradient evaluations and the iteration complexity I(A(b), f) is the number of minibatch queries to obtain an ϵ-accurate solution. Note that the relationship bI(A(b),f)=T(A(b),f) holds.

Remark
Some stochastic methods, such as SVRG [15] involve calculations of the exact gradient ∇g. When considering the iteration complexity, we take into account the cost ⌈n/b⌉ for this step.

Note that these two complexities coincide for stochastic methods without minibatching. However, these are different from each other for stochastic minibatch methods.

Optimal complexities for strongly convex problems
In this subsection, we briefly explain the optimal iteration and total complexities for strongly convex problems. As for the total complexity of first-order stochastic minibatch methods A(b), the cost of Ω(n+nκ−−−√log(1/ϵ)) is known to be a worst-case lower bound [19, 20], where κ=L/μ is the condition number. A lower bound on the iteration complexity can be also derived from this via the relationship: T(A(b),f)=bI(A(b),f). That is, the following statement holds.

Proposition 1
Let A(b) be a stochastic method for which the above worst-case lower bound on the total complexity holds. For L≥μ>0, there exists an n-finite sum f of L-smooth and μ-strongly convex functions such that

I(A(b),f)=1bΩ(n+nκ−−−√log(1/ϵ)).
Proof
From the assumption, there exists an n-finite sum of convex functions f such that T(A(b),f)=Ω(n+nκ−−−√log(ϵ)). Noting that T(A(b),f)=bI(A(b),f), we can finish the proof. □

However, a lower bound in this proposition is rather meaningless for large minibatch sizes. On the other hand, we can also derive a worst-case lower bound Ω(κ−−√log(1/ϵ)) on the iteration complexity of A(b) through a lower bound for first-order deterministic gradient methods [25]. That is, it is obtained by setting all functions gi to the same worst-case example g for the deterministic algorithms. In summary, the following bound on the iteration complexity holds:

I(A(b),f)≥max{1bΩ(n+nκ−−−√log(1/ϵ)),Ω(κ−−√log(1/ϵ))}.
However, the above construction is quite restrictive in the stochastic setting, so that we here put forth the following proposition which enables us to construct considerably wider class of examples of the lower bound on the iteration complexity from that for deterministic methods. Note that the assumptions made in Proposition 2 hold for many of the methods in which we are interested such as SVRG, Acc-SVRG (with APPA), Katyusha, and DASVRDA.

Proposition 2
Let f=1n∑ni=1gi be an n-finite sum of convex quadratic functions. For a stochastic minibatch method A(b), assume that A(n) becomes a first-order deterministic method and expected iterates E[xk] of A(b) coincide with those of A(n). Moreover, assume I(A(b), f)) to be equal to or larger than the number of parameter updates. Then, I(A(b),f)≥I(A(n),f).

Proof
Let xk denote the k-th iterate of A(b) and f∗=infRdf(x). From Jensen’s inequality, we get E[f(xk)]−f∗≥f(E[xk])−f∗. This proves the proposition. □

Several important consequences can be deduced from this proposition. First, the amplitude of the function class yielding the lower bound Ω(κ−−√log(1/ϵ)) can be verified because a worst-case example for first-order deterministic methods can be constructed as a quadratic function [25] and there are countless ways of decomposing it into finite sums while maintaining given smoothness and strong convexity. Secondly, the iteration complexities of stochastic variants of non-accelerated deterministic gradient methods cannot reach the optimal iteration complexity of O(κ−−√log(1/ϵ)) even when using a huge minibatch because iteration complexities of non-accelerated methods are slower than the optimal one. Such examples include SGD, SVRG, and SAGA. A question regarding the lower bound on the iteration complexity is whether or not the above lower bound is achievable by a stochastic minibatch method A(b) with a efficient size of minibatch. In this paper, we will give a positive answer to this question.

Optimal complexities for non-strongly convex problems
For non-strongly convex problems, counterparts of results in the previous subsection also hold. As for the total complexity of first-order stochastic minibatch methods A(b), the cost of Ω(n+nL/ϵ−−−−√) is known to be a worst-case lower bound [19, 20]. Hence, by the similar argument as Proposition 1, we get a lower on the iteration complexity:

Proposition 3
Let A(b) be a stochastic method for which the above worst-case lower bound on the total complexity holds. For L>0 and ϵ>0, there exists an n-finite sum f of L-smooth convex functions such that

I(A(b),f)=1bΩ(n+nL/ϵ−−−−√).
In addition to this lower bound, Ω(L/ϵ−−−√) is known to be a worst case lower bound on the iteration complexity of first order deterministic gradient methods A(n) [25] for non-strongly convex problems. Thus, by setting all functions gi to the same worst-case example g for the deterministic algorithms A(n), we obtain a lower bound on I(A(b), f):

I(A(b),f)≥max{1bΩ(n+nL/ϵ−−−−√),Ω(L/ϵ−−−√)}.
Moreover, Proposition 2 also holds for non-strongly convex problems and we notice that this lower bound is not attained by non-accelerated methods such as SGD, SVRG, and SAGA because of the same reason discussed in the previous subsection. In this paper, we show that this lower bound is almost attained by optimal minibatch methods with the efficient size of minibatch.

Benefit of minibatching
In this section, we introduce a notion of minibatch optimality from the viewpoint of reducing running time. A benefit of minibatching is speeding up through the parallel computing of stochastic gradients [5,6,7,8,9,10,11,12,13]. If we ignore the communication costs and use a number of processors as many as minibatch size, running time almost corresponds to the iteration complexity. As described earlier, iteration complexities of most of stochastic gradient methods can be decomposed into a deterministic term and a variance term. For instance, when the variance of the stochastic gradient is σ2, the iteration complexities of uniformly optimal methods [35] with minibatch size b without variance reductions are as follows. For strongly convex problems, the complexity is

κ−−√log(Lϵ)deterministic term+σ2μbϵvariance term,
and for non-strongly convex problems the complexity is

Lϵ−−√deterministic term+σ2bϵ2variance term.
From these rates, we find that the theoretical limit of running time corresponds to the deterministic term and find that unrealistically large minibatch of sizes Ω(σ2/(κ−−√μϵ)) and Ω(σ2/(ϵ3/2L−−√)) are required to reach this limit for strongly convex and non-strongly convex problems, respectively. This phenomenon is also common in other methods because the variance term is usually much slower than the deterministic term. Since, such huge minibatches affect the total complexity, it is quite important to investigate the smallest minibatch size needed to achieve the optimal iteration complexity in the minimax sense and construct an optimal minibatch method combined with efficient variance reduction and acceleration techniques.

In summary, we are interested in the optimal minibatch size.

Definition 4
(Optimal minibatch size for strongly convex problems). Let A be a given algorithm class of first-order minibatch methods A(b) and F be the function class of n-finite sums of L-smooth and μ-strongly convex functions. The optimal minibatch size is defined as follows:

minA(b)∈Amaxf∈Fb  s.t.  I(A(b),f)=O(κ−−√log(1/ϵ)),
(2)
In addition, we call A is the optimal minibatch methods for strongly convex problems if a stochastic method A(b) using the optimal minibatch size b satisfies the optimal iteration complexity for strongly convex problems.

As for the non-strongly convex problems, we define an optimal minibatch size in the minimax sense by replacing a constraint in (2) with the constraint: I(A(b),f)=O(L/ϵ−−−√).

Definition 5
(Optimal minibatch size for non-strongly convex problems). Let A be a given algorithm class of first-order minibatch methods A(b) and F be the function class of n-finite sums of L-smooth convex functions. The optimal minibatch size is defined as follows:

minA(b)∈Amaxf∈Fb  s.t.  I(A(b),f)=O(L/ϵ−−−√),
(3)
In addition, we call A is the optimal minibatch methods for non-strongly convex problems if a stochastic method A(b) using the optimal minibatch size b satisfies the optimal iteration complexity for non-strongly convex problems.

Optimal minibatch methods
Acc-SVRG with or without APPA
Before providing a lower bound on minibatch size, we introduce some minibatch-efficient methods: Acc-SVRG [3] with or without APPA [18], Katyusha [1], and DASVRDA [2]. As we will show later in Sect. 5, these methods are (nearly) optimal in terms of the minibatch size.

Acc-SVRG [3] is the first method to exhibit minibatch efficiency. That is, Acc-SVRG succeeds in achieving the optimal iteration complexity with an efficient minibatch size, although it cannot accelerate the total complexity. However, there is room for improvement for ill-conditioned problems: κ>n for strongly convex problems and L/ϵ>n for non-strongly convex problems, by combining Acc-SVRG with an acceleration technique called APPA, which has not been mentioned in the literature. Hence, we now review these algorithms. For a detailed theoretical analysis, see the Appendix.

figure d
Acc-SVRG is a multi-stage method where Nesterov’s accelerated gradient steps [25] using SVRG [15] with minibatching are performed in each stage for acquiring both sufficient acceleration of convergence and variance reduction. The overall procedure of Acc-SVRG is described in Algorithm 1 and the relationship of ingredients of the method is depicted in Fig. 1.

Fig. 1
figure 1
The figure depicts ingredients of Acc-SVRG with APPA. Acc-SVRG achieves the optimal iteration complexity with an efficient minibatch size when the number of examples is large compared to the condition number because of acceleration and variance reduction. In ill-conditioned case where the number of examples is much smaller than the condition number, total computational complexity is improved by the combination with APPA, resulting in the improvement of minibatch efficiency

Full size image
Strongly convex problem
When n≥κ, by setting b=O(n/(κ−−√log(1/ϵ))), η=O(1/L), and m=O(κ−−√), the following complexities I(A(b), f) and T(A(b), f) are obtained [3]:

I(A(b),f)T(A(b),f)=O(κ−−√log(1/ϵ)),=O(nlog(1/ϵ)).
Hence, the optimal iteration complexity and the nearly optimal total complexity are achieved with a reasonable minibatch size in this case.

As for the case of n<κ, convergence property can be improved by applying APPA. In APPA, the objective function (1) with a proximal term at the current iterate is approximately minimized successively by using an inner-solver M and taking acceleration steps. The overall scheme of APPA is described in Algorithm 2. The total computational complexity of APPA when applying to SAG, SAGA, MISO, and SVRG without minibatching are well known, that is, it can be improved from O(κlog(1/ϵ)) to the optimal total complexity O~(nκ−−−√log(1/ϵ)), where O~ hides extra logarithmic factors. As for Acc-SVRG with APPA, we give the following result (see Appendix for the detailed derivation).

figure e
Theorem 1
Assume that κ>3n and that gi are L-smooth and μ-strongly convex functions. Set the parameters in APPA as follows: γ=Ln−μ and Tc=O(γ/μ−−−√log(1/ϵ)). Set the parameters in Acc-SVRG as follows: T=O(log(γ/μ)), m=O(n−−√), b=O(n−−√), and η=O(1/(L+γ)). Then, the iteration complexity and the total complexity are

I(A(b),f)T(A(b),f)=O~(κ−−√log(1/ϵ)),=O~(nκ−−−√log(1/ϵ)).
Compared to APPA with the other methods, a remarkable feature is that an almost-optimal iteration complexity and almost-optimal total complexities are achieved simultaneously with an efficient minibatch size of O(n−−√), even in the case of κ>n. Although we here incorporate APPA with Acc-SVRG, the Catalyst approach proposed by [17, 36], which is another proximal acceleration scheme, has similar properties.

Non-strongly convex problem
Even when problems are non-strongly convex, we can derive complexities of Acc-SVRG with or without APPA from complexities for strongly convex problems at the price of log(1/ϵ) factor by adding ϵ-weighted L2-regularization term (0<ϵ<L). In this setting, the problems become ϵ-strongly convex and L-Lipschitz smooth. Therefore, when n≥L/ϵ, by setting b=O(nϵ/L−−−√), η=O(1/L), and m=O(L/ϵ−−−√), the following complexities I(A(b), f) and T(A(b), f) of Acc-SVRG without APPA are obtained:

I(A(b),f)T(A(b),f)=O(L/ϵ−−−√log(1/ϵ)),=O(nlog(1/ϵ)).
As for the case of n<L/ϵ, Acc-SVRG with APPA achieves the following complexities with the efficient minibatch size b=O(n−−√):

I(A(b),f)T(A(b),f)=O(L/ϵ−−−√log(1/ϵ)),=O(nL/ϵ−−−−√log(1/ϵ)).
Hence, Acc-SVRG with or without APPA almost achieves the optimal iteration complexity and the optimal total complexities with a reasonable minibatch size for non-strongly convex problems.

Katyusha and DASVRDA
Katyusha [1] and DASVRDA [2] also have the minibatch-efficient property. These methods incorporate different additional steps from APPA, with vanilla Acc-SVRG to improve its performance in the ill-conditioned case of κ>n or L/ϵ>n. More specifically, Katyusha uses negative momentum to reduce a fraction of the stochastic gradients and DASVRDA essentially uses Nesterov’s acceleration in the outer loop of Acc-SVRG. As shown by [1, 2], for strongly convex problems, both algorithms exhibit the following total complexity with b:

O((n+(b+n−−√)κ−−√)log(1ϵ)).
Thus, the optimal iteration complexity is achieved by b=O(n−−√+n/κ−−√). Moreover, for non-strongly convex problems, both algorithms exhibit the following total complexity with b:

O(nlog(1ϵ)+(b+n−−√)Lϵ−−√).
Thus, the optimal iteration complexity is achieved by b=O(nϵL−−√log(1ϵ)+n−−√). In summary, these methods are also minibatch-efficient like Acc-SVRG (with APPA).

Theoretical comparison
We summarize the best known iteration complexities of several variance reduction methods (SAG, SVRG, SAGA, SVRG with APPA, APCG, SPDC, Acc-SVRG (with APPA), Katyusha, and DASVRDA) in Table 3 for strongly convex problems and Table 4 for non-strongly convex problems. along with the minimal required minibatch sizes for these complexities. The notation O~ hides extra logarithmic terms stemming from APPA. As a reference, we also list the general iteration complexities of these methods using an arbitrary minibatch size of b in the rightmost column of Tables 3 and 4. Note that we derive complexities at the price of log(1/ϵ) factor made by adding ϵ-weighted L2-regularization term for some methods where direct analyses are not provided in the non-strongly convex setting. In both cases of strongly convex and non-strongly convex problems, we see that Acc-SVRG (with APPA), Katyusha, and DASVRDA can achieve the almost optimal iteration complexity with an efficient size of minibatch which is actually optimal up to logarithmic factors as shown Sect. 5, whereas the other accelerated methods (SVRG with APPA, APCG, SPDC) require b=O(n) to obtain the optimal iteration complexity and non-accelerated methods (SAG, SVRG, SAGA) cannot obtain the optimal iteration complexity.

Table 3 The best achievable iteration complexities with minimal minibatch sizes for strongly convex problemst
Full size table
Table 4 The best achievable iteration complexities with minimal minibatch sizes and total complexities for non-strongly convex problems
Full size table
Optimal minibatch size
Strongly convex problem
In the previous section, we introduced methods that achieve the almost optimal iteration complexity for strongly convex problems with the following minibatch sizes:

bn,κ=def{O(nκ√log(1/ϵ))O(n−−√)(n≥κ),(n<κ),
(4)
where big-O notation is taken with respect to n→∞,κ→∞, and ϵ→0.

We here show that these minibatch sizes are optimal in the minimax sense (2) over a specific algorithm class A. Let A be the class of first-order stochastic minibatch methods A(b) for which the worst-case lower bound O(n+nκ−−−√log(1/ϵ)) hold on the total complexity [19], which includes the methods in which we are interested.

Proposition 4
Let L≥μ>0 be positive values. Let A(b)∈A be a first-order stochastic minibatch method. If A(b) is the optimal method with respect to the iteration complexity, then there exists an n-finite sum f composed of L-smooth and μ-strongly convex functions such that

b=Ω(max{nκ−−√log(1/ϵ),n−−√}).
Proof
From the assumption, there exists an n-finite sum f of L-smooth and μ-strongly convex functions such that

T(A(b),f)=Ω(n+nκ−−−√log(ϵ)).
Noting that T(A(b),f)=bI(A(b),f) and A(b) achieves the optimal iteration complexity, we get

bκ−−√log(1/ϵ)=Ω(n+nκ−−−√log(1/ϵ)).
□

Proposition 4 provides the lower bound on the optimal minibatch size (2) for strongly convex problems, which almost corresponds to minibatch size (4) attained by minibatch efficient methods. Therefore, we conclude that minibatch sizes (4) are actually (nearly) optimal up to logarithmic factors and Acc-SVRG (with APPA), Katyusha, and DASVRDA are the optimal minibatch methods for strongly convex problems.

Non-strongly convex problem
For non-strongly convex problems, we explained that Acc-SVRG (with APPA), Katyusha, and DASVRDA achieve the almost optimal iteration complexity with the following minibatch sizes:

bn,L/ϵ=def⎧⎩⎨O(nϵL−−√)O(n−−√)(n≥L/ϵ),(n<L/ϵ),
(5)
where big-O notation is taken with respect to n→∞,L→∞, and ϵ→0.

We show that these minibatch sizes are optimal in the minimax sense (3) over a specific algorithm class A. Let A be the class of first-order stochastic minibatch methods A(b) for which the worst-case lower bound O(n+nL/ϵ−−−−√) hold on the total complexity [19], which includes the methods in which we are interested.

Proposition 5
Let L>0 and ϵ>0 be positive values. Let A(b)∈A be a first-order stochastic minibatch method. If A(b) is the optimal method with respect to the iteration complexity, then there exists an n-finite sum f composed of L-smooth convex functions such that

b=Ω(max{nϵL−−√,n−−√}).
Proof
From the assumption, there exists an n-finite sum f of L-smooth convex functions such that

T(A(b),f)=Ω(n+nL/ϵ−−−−√).
Noting that T(A(b),f)=bI(A(b),f) and A(b) achieves the optimal iteration complexity, we get

bL/ϵ−−−√=Ω(n+nL/ϵ−−−−√).
□

Proposition 5 provides the lower bound on the optimal minibatch size (3) for non-strongly convex problems, which almost corresponds to minibatch size (5) attained by minibatch efficient methods. Therefore, we conclude that minibatch sizes (5) are actually (nearly) optimal up to logarithmic factors and Acc-SVRG, Katyusha, and DASVRDA are the optimal minibatch methods for non-strongly convex problems.

Characterization of optimal minibatch method
An interesting common feature among optimal minibatch methods is that they achieve both the optimal iteration and the optimal total complexities simultaneously. Indeed, we can confirm that this condition is a characterization of the optimal minibatch methods for strongly convex and non-strongly convex problems.

Proposition 6
Consider strongly convex problems or non-strongly convex problems. A stochastic minibatch method A is an optimal minibatch method if and only if A(b) achieves both the optimal iteration and the optimal total complexities simultaneously for some minibatch sizes b.

Proof
We first consider strongly convex problems. Let A be an optimal minibatch method for strongly convex problems. Then, for given L≥μ>0, and an arbitrary n-finite sum of L-smooth and μ-strongly convex functions, A(bn,κ) achieves an optimal iteration complexity I(A(bn,κ),f)=O(κ−−√log(1/ϵ)), where bn,κ is the optimal minibatch size (4). Then, we get

T(A(bn,κ),f)=bn,κI(A(bn,κ),f)={O(n)O(nκ−−−√log(1/ϵ))(n≥κ),(n<κ).
Thus, A(bn,κ) achieves the optimal total complexity.

Next, for minibatch size b and stochastic method A, we assume that A(b) is an optimal method in terms of both iteration and total complexities for strongly convex problems. Let f be a n-finite sum of L-smooth and μ-strongly convex functions such that I(A(b),f)=Ω(κ−−√log(1/ϵ)). Since T(A(b),f)=O~(n+nκ−−−√log(1/ϵ)), we get

b=T(A(b),f)/I(A(b),f)=O~(nκ−−√log(1/ϵ)+n−−√).
From Proposition 4 and this equation, we see that b=Θ(bn,κ) and A(b) is an optimal minibatch method for strongly convex problems.

Next, we consider non-strongly convex problems. Let A be an optimal minibatch method for non-strongly convex problems. Then, for given L>0, ϵ>0, and an arbitrary n-finite sum of L-smooth convex functions, A(bn,L/ϵ) achieves an optimal iteration complexity I(A(bn,L/ϵ),f)=O(L/ϵ−−−√), where bn,L/ϵ is the optimal minibatch size (5). Then, we get

T(A(bn,L/ϵ),f)=bn,L/ϵI(A(bn,L/ϵ),f)={O(n)O(nL/ϵ−−−−√)(n≥L/ϵ),(n<L/ϵ).
Thus, A(bn,L/ϵ) achieves the optimal total complexity.

Next, for minibatch size b and stochastic method A, we assume that A(b) is an optimal method in terms of both iteration and total complexities for non-strongly convex problems. Let f be a n-finite sum of L-smooth convex functions such that I(A(b),f)=Ω(L/ϵ−−−√). Since T(A(b),f)=O(n+nL/ϵ−−−−√), we get

b=T(A(b),f)/I(A(b),f)=O(nϵ/L−−−√+n−−√).
From Proposition 5 and this equation, we see that b=Θ(bn,L/ϵ) and A(b) is an optimal minibatch method for non-strongly convex problems. This concludes the proof. □

Therefore, we find that non-optimal methods in terms of the total complexity are also non-optimal minibatch methods and find that an unnecessarily large minibatches are wasteful for optimal minibatch methods because there is no room for improvement of computational complexities.

Remark
As for the characterization of optimal minibatch methods, we can deduce that the latter condition in Proposition 6 is tight in the following minimax sense. Consider strongly convex problems with n∈N and L≥μ>0. Then, for an optimal minibatch method A(bn,κ)∈A, there exists n-finite sum f of L-smooth and μ-strongly convex functions such that

I(A(bn,κ),f)T(A(bn,κ),f)=Θ(κ−−√log(1/ϵ)),=bn,κI(A(bn,κ),f)=Θ(n+nκ−−−√log(1/ϵ)).
Next, consider non-strongly convex problems with n∈N, L>0, and ϵ>0. Then, for an optimal minibatch method A(bn,L/ϵ)∈A, there exists n-finite sum f of L-smooth convex functions such that

I(A(bn,κ),f)T(A(bn,κ),f)=Θ(L/ϵ−−−√),=bn,L/ϵI(A(bn,L/ϵ),f)=Θ(n+nL/ϵ−−−−√).
Numerical experiments
In this section, we conduct numerical experiments to validate our theoretical analysis. In particular, we numerically verify that the minibatch sizes (4) and (5) are optimal and actually unnecessarily large minibatches are not helpful.

Experimental settings
Experimental settings are summarized below (see also Appendix for more details).

Task The standard L1-and L2-regularized logistic regression problem for binary classification was used, i.e., f(x)=1/n∑ni=1gi(x), gi(x)=log(1+exp(−bi⟨ai,x⟩))+λ1∥x∥1+λ2/2∥x∥22, where (ai,bi)∈Rd×{±1} is a single example in the data set and λ1 and λ2 are regularization parameters.

Data sets We used three publicly available data sets: a9a, rcv1 and sido0. Their sizes n and the numbers of features d are listed in Table 5. We normalized the feature vector ai of each example in these data sets so that ∥ai∥2=1 for i∈{1,…,n}. Hence, the upper bound of the smoothness parameter L of the objective was maxi∥ai∥22/4=1/4.

Methods We compare five well-known stochastic gradient methods in minibatch settings: SVRG (without and with APPA), Acc-SVRG (without and with APPA), and DASVRDA. Note that these methods requires the same order of the computational time and memory cost per single minibatch oracle.

Minibatch sizes and regularization parameters The minibatch sizes used in our experiments ranged in {2k∣k∈{4,5,…,14}}. The L1 and L2-regularization parameters (λ1,λ2) ranged in {(0,10−2), (0,10−4), (0,10−6), (0,10−8)} ∪ {(10−2,0), (10−4,0), (10−6,0), (0,0)}. When λ2>0, λ2 determines the strong convexity parameter μ of the objective. Hence, theoretically, optimization becomes difficult and acceleration methods become more effective than non-accelerated methods as λ2 decreases. On the other hand, when L2 -regularization parameter λ2=0, the objective is generally not strongly convex and theoretically accelerated methods are always superior to non-accelerated methods.

Parameter tuning For each data set, minibatch size, and regularization parameter, we fairly and automatically tuned the implemented algorithms to use the best hyper parameters in terms of train loss. The tuned parameters of the implemented algorithms are listed in Table 6.

The entire procedure of our numerical experiments is illustrated in Fig. 2.

Table 5 Summary of the data sets used in experiments
Full size table
Table 6 List of the tuned hyper parameters of the implemented algorithms
Full size table
Fig. 2
figure 2
Illustration of the entire procedure of our numerical experiments. At step 1, we normalize the input data so that all the feature vectors have Euclidean norm 1. At step 2, we run a optimizer 50 epochs and record the train loss for each hyper parameters candidate. Then we select the best hyper parameters that led to the minimum train loss. At step 3, we run the optimizer 2, 000 epochs using the best hyper parameters determined at step 2 and evaluate the necessary number of minibatch oracles to satisfy f(x)−f(x∗)≤ε. This procedure was independently executed for each optimizer, (b,λ1,λ2) and data set

Full size image
Numerical results1: strongly convex cases
Figure 3 shows comparisons of the different methods described above for several settings for strongly convex cases. "# of minibatch oracles" means the necessary numbers of minibatch oracles for achieving the objective gap f(x)−f(x∗)<ε. We chose ε=10−4=O(1/n). This choice is reasonable because O(1/n) is known as a typical minimax optimal statistical error and achieving this rate is one of the goals in empirical risk minimization. For each minibatch size, the methods were terminated after 2000 passes over the data. Plots are not depicted for the methods that did not achieve the desired accuracy. From Fig. 3, we can make the following observations and considerations:

The case where λ2=10−2

(Observations) The necessary number of minibatch oracles for all methods decreased almost linearly up to approximately b=104. All methods performed almost equally.

(Considerations) In this case, the condition number κ becomes O(101)∼O(102), which is much smaller than O(n). Hence, the theoretically deduced iteration complexities of all methods are O~(n/b) up to the minimal minibatch sizes for the best iteration complexities. The minibatch sizes are in the range from n/κ=O(103) to n=O(104). Therefore, the theoretical results match the observations.

The cases where λ2=10−4,10−6

(Observations) For small minibatch sizes, SVRG with APPA, Acc-SVRG, Acc-SVRG with APPA, and DASVRDA significantly outperformed vanilla SVRG. The iteration complexity of SVRG with APPA did not linearly scale w.r.t. minibatch size. The ones of Acc-SVRG, Acc-SVRG with APPA, and DASVRDA decreased linearly, and their decrease rate slowed down or stopped at approximately b=27∼29, which is close to O(n−−√).

(Considerations) In these cases, the condition number κ becomes roughly 103∼106, and there is not a big difference from O(n). SVRG with APPA can outperform SVRG and perform as well as Acc-SVRG with APPA and DASVRDA for small minibatch sizes because these methods have a similar theoretical performance when b=1. However, the iteration complexity of SVRG with APPA only sub-linearly scales w.r.t. minibatch size, and its decreasing rate slowed down as the minibatch size became large. In contrast, Acc-SVRG with APPA and DASVRDA achieved linear decreasing rates w.r.t. minibatch size b up to b=O(n−−√). The above observations validate the theoretical implications.

The case where λ2=10−8

(Observations) DASVRDA significantly outperformed the other methods for all data sets. Moreover, Acc-SVRG with APPA outperformed vanilla Acc-SVRG for a9a and rcv1. The iteration complexities of Acc-SVRG with APPA and DASVRDA decreased linearly up to b=27∼29.

Fig. 3
figure 3
Comparisons of several minibatch stochastic gradient methods for minimizing logistic loss with L1-regularization parameter λ1=0 and L2-regularization parameter λ2∈{10−2,10−4,10−6,10−8} for the a9a, rcv1, and sido0 data sets. The y-axis is the number of minibatch oracles for achieving the accuracy ε=10−4 and the x-axis is the minibatch size used in common for all methods. For each minibatch size, the methods were terminated after 2000 passes over the data. Plots are not depicted for the methods that did not achieve the desired accuracy

Full size image
Fig. 4
figure 4
Comparisons of several minibatch stochastic gradient methods for minimizing logistic loss with L1-regularization parameter λ1∈{10−2,10−4,10−6,0} and L2-regularization parameter λ2=0 for the a9a, rcv1, and sido0 data sets. The y-axis is the number of minibatch oracles for achieving the accuracy ε=10−4 and the x-axis is the minibatch size used in common for all methods. For each minibatch size, the methods were terminated after 2000 passes over the data. Plots are not depicted for the methods that did not achieve the desired accuracy

Full size image
(Considerations) In this regime, the condition number κ becomes O(107)∼O(108), which is much higher than O(n). Hence, accelerated methods are quite beneficial. Indeed, optimal minibatch methods: Acc-SVRG with APPA and DASVRDA can converge stably because of its minibatch efficiency and can show the aforementioned linear decreasing property up to b=O(n−−√). Acc-SVRG without APPA somewhat lacked stability compared to these two methods because a larger minibatch size than optimal size is essentially required to exhibit the comparable iteration complexity. The other methods: SVRG and SVRG with APPA much suffered from a huge condition number and failed to converge in many cases. Therefore, these observations can be adequately explained with our theoretical results.

Numerical results2: non-strongly convex cases
Figure 4 shows comparisons of the different methods described above for several settings for non-strongly convex cases. "# of minibatch oracles" means the necessary numbers of minibatch oracles for achieving the objective gap f(x)−f(x∗)<ε. We chose ε=10−4=O(1/n). This choice is reasonable because O(1/n) is known as a typical minimax optimal statistical error and achieving this rate is one of the goals in empirical risk minimization. For each minibatch size, the methods were terminated after 2000 passes over the data. Plots are not depicted for the methods that did not achieve the desired accuracy. From Fig. 4, we can make the following observations and considerations:

(Observations) For all the cases except the case λ1=0.01 on rcv1Footnote1, the optimal minibatch methods (Acc-SVRG with APPA and DASVRDA) typically outperformed other methods in terms of the number of minibatch oracles for all minibatch sizes and again showed the linear decreasing property up to nearly b=O(n−−√) and the saturation after the threshold. Particularly, DASVRDA showed the most stable convergence in our experiments.

(Considerations) In our settings, the desired accuracy ε was set to 10−4=O(1/n). Hence, our theory predicts that the optimal minibatch methods show the linear decreasing property up to b=O(nε√+n−−√)=O˜(n−−√) for all λ1, that is exactly what we observed. The convergence of APPA methods is possibly unstable because they result in additional hyper-parameters to be tuned.

Discussion
As discussed earlier, non-accelerated methods, such as SAG, SVRG, and SAGA, where A(n) cannot achieve the optimal iteration complexity, are also not optimal minibatch methods. Moreover, unlike Acc-SVRG, these methods do not become optimal minibatch methods even when applying APPA acceleration, although they can attain the optimal total complexity. By fixing μ=1 in APPA, the iteration complexity of SVRG with APPA up to the logarithmic factor is reduced to the following: for γ≥1,

γ−−√(nb+κ+γ1+γ).
Because (κ+γ)/γ−−√, which corresponds to the second term, takes its minimum value of O(κ−−√) at γ=κ, we immediately notice from the first term that b=Ω(n) is required to achieve the optimal iteration complexity. Therefore, SVRG with APPA is not the optimal minibatch method. Hence, an acceleration in inner loop seems to be needed.

In the rest of this section, we give an intuitive explanation on the requirements of the optimal minibatch methods. Because the iteration complexity of stochastic minibatch methods is lower-bounded by that of the deterministic variant (as shown in this paper), an acceleration scheme, such as Nesterov’s method, is required. However, Nesterov-like acceleration is very sensitive to noise and generally cannot attain the optimal iteration complexity under stochastic settings. Thus, a variance-reduce method is also needed. As shown in previous analyses [3, 5], Nesterov-like acceleration with either SVRG and a minibatch size of 1 or small-size minibatching is not enough to achieve the optimal iteration complexity. Namely, under stochastic settings, we observe that (i) Nesterov’s acceleration combined with small-size minibatching has almost the same convergence rate as that of SGD [5] and (ii) Nesterov’s acceleration combined with SVRG without minibatching has the same iteration complexity as SVRG [3]. (iii) large-size minibatching combined with SVRG has almost the same iteration complexity as the gradient descent method, as discussed earlier. Therefore, a combination of three techniques: Nesterov-like acceleration, minibatching, and SVRG, appear to be essential to attain the optimal iteration complexity with an efficient minibatch size. This requirement is sufficient in the case of n≥κ, as shown for Acc-SVRG, but an additional technique is also needed in the case of n<κ. One potential additional technique is the acceleration of the outer-loop as APPA and DASVRDA, and another is the negative momentum used in Katyusha. We have rigorously supported that these observations by providing a characterization of optimal minibatch methods in the paper.

Conclusion
We presented the optimal iteration complexity of stochastic minibatch methods for finite-sum convex problems by utilizing existing lower bounds for both deterministic and stochastic first-order methods, and derived the optimal minibatch sizes in the minimax sense for achieving the optimal iteration complexities for both strongly convex and non-strongly convex problems. We showed the existence of the optimal minibatch methods: Acc-SVRG (with APPA), Katyusha, and DASVRDA and showed that the other methods including SVRG, APPA, SPDC, and APCG are not minibatch optimal. Moreover, we characterized the optimal minibatch methods by the methods that achieve both the optimal iteration and total complexity simultaneously for some minibatch sizes. Finally, our theoretical analysis was validated experimentally on L2-regularized logistic regressions.