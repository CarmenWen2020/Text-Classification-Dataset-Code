Feature selection has received considerable attention over the past decade. However, it is continuously challenged by new emerging issues. Semi-supervised multi-label learning is one of these promising novel approaches. In this work, we refer to it as an approach that combines data consisting of a huge amount of unlabeled instances with a small number of multi-labeled instances. Semi-supervised multi-label feature selection, like conventional feature selection algorithms, has a rather poor record as regards stability (i.e. robustness with respect to changes in data). To address this weakness and improve the robustness of the feature selection process in high-dimensional data, this document develops an ensemble methodology based on a 3-way resampling of data: (1) Bagging, (2) a random subspace method (RSM) and (3) an additional random sub-labeling strategy (RSL). The proposed framework contributes to enhancing the stability of feature selection algorithms and to improving their performance. Our research findings illustrate that bagging and RSM help improve the stability of the feature selection process and increase learning accuracy, while RSL addresses label correlation, which is a major concern with multi-label data. The paper presents the key findings of a series of experiments, which we conducted on selected benchmark data sets in the classification task. Results are promising, highlighting that the proposed method either outperforms state-of-the-art algorithms or produces at least comparable results.

Access provided by University of Auckland Library

Introduction
Feature selection is of vital importance to data mining owing to its ability to improve performance and reduce time and cost by removing irrelevant/redundant features. A large number of researchers have been interested in different approaches to feature selection. However, filter methods have so far proven to achieve the best trade-off between better performances and lower computational complexity.

Filter methods use general characteristics of the data to provide features with scores reflecting certain statistical criteria. The final selection is made on the basis of scores; features ranked above/below a specific threshold are selected as the most relevant.

The selection criteria, which are used to compute feature scores, may vary significantly according to the degree of supervision in data sets. In fact, the selection process in a supervised context relies on assessing the level of correlation between features and class labels [18]. Whereas in unsupervised feature selection, due to the lack of prior domain knowledge, the relevance of a feature is determined by the variance or its separability power in general[21]. Another particular configuration arises from the fact of the availability in real-world applications of tiny labeled-samples alongside a huge amount of unlabeled data, which we refer to as semi-supervised. In this case, mixed criteria from both parts can be used to spot the most useful features [67].

Moreover, with the presence of supervision information, expressed by means of labels, two possible scenarios may occur: single-label, and multi-label data. Whilst in single-label data sets, examples belong to only one class label at a time and the classes are mutually exclusive, multi-label examples are allowed to be associated with more than one class label simultaneously without any exclusivity [54]. This has led to a large diversity of research directions in feature selection. Single-label feature selection has widely been investigated and has yielded various solutions such as ReliefF, information gain, and Correlation-based feature selection (CFS), to name but a few [25]. Feature selection in multi-label data sets, however, is still at its early stages and has promising development prospects [54].

Like learning algorithms, feature selection struggles to deal with high dimensionality of data. In fact, when the number of features increases significantly, most of feature selection algorithms usually lose in accuracy. On the other hand, filter feature selection algorithms are peculiarly dependent on data and have great variance. This means that a slight change in data lead to skewed feature scores, which will eventually bias the whole selection procedure and lead to unstable feature selection algorithms. In this context, a feature selection algorithm is considered to be stable only when it produces similar features under the training data variation. There are several metrics to quantify the stability of a feature selection algorithm [40]. The work presented in this paper is primarily motivated by these two issues.

In this regard, ensemble methods are good artifacts that offer satisfying solutions to overfitting and stability issues. In fact, Bauer and Kohavi [6] showed empirically that bagging reduces the variance and improves the stability of the underlying classifiers. Besides, many researchers claim that overfitting can be avoided by projecting the high dimensional data on several lower dimensional spaces as in the case of random subspace method.

A large body of research proposed to use ensemble methods in multi-label learning. In Tsoumakas and Vlahavas [57], the random k-label sets (RAkEL) constructs an ensemble of classifiers, each of which is trained using a different small random subset of the original set of labels. RAkEL has the merit of dealing efficiently with label correlation. Tsoumakas et al. [55] propose a method that uses a hierarchy of multi-label classifiers (HOMER), where each classifier operates on a subset of labels. Read et al. [45] introduce an ensemble method, called ensemble of classifier chains (ECC), which uses a chain of binary classifiers, each of which is trained upon the prediction of previous ones. In multi-label text categorization, a robust ensemble learning method called BoosTexter uses Adaboost.MH and Adaboost.MR algorithms [49] by applying Adaboost [23] on weak classifiers.

Similarly, this paper uses the ensemble paradigm with the aim of improving the robustness of the feature selection process in high-dimensional data with a small number of multi-labeled samples, where the selection of stable feature subsets is more problematic. To this end, this paper proposes an extension to the S-CLS method [1] by combining three random sampling techniques, each of which is iteratively applied on a different level of a data set: instance, dimensional, and label space levels. More specifically, bagging is used to draw bootstrap samples from the original data, whilst Random Subspace Method (RSM) samples through the feature space, and a random sampling without replacement is performed in the label space. We call this method RSL, which is similar to RSM but performs on the label set. The purpose of this approach is three-fold: while bagging and RSM help reduce variance and improve the stability of the feature selection process, and increase learning accuracy, RSL addresses particularly label correlation, which is a major concern when dealing with multi-label data. It is worth noting that the same approach was applied in Nasierding et al. [39] for classification purposes. This paper also presents an explanation of S-CLS in the light of the spectral graph theory. Besides, the score function of S-CLS has been modified in order to be more representative of the similarity between instances in terms of their label subsets. The new score function is applicable to classification as well as to regression problems.

The second part of the paper is structured as follows: Sect. 2 discusses issues related to semi-supervised feature selection, multi-label learning and basic elements of ensemble learning. Section 3 provides a detailed introduction to S-CLS, the algorithm we use as a base learner in our ensemble method. Section 4 explains in depth the proposed method. Subsequently, Sect. 5 describes the experimental framework, including a detailed description of the test method along with a detailed comparison of results with state-of-the-art algorithms. Section 6 presents a practical application to E-mail filtering to test the effectiveness of our proposal. Finally, the paper wraps up key findings and identifies areas for further research in Sect. 7.

Related works
This section briefly reviews relevant research related to semi-supervised feature selection on single-label data sets. It presents key characteristics of multi-label learning. At the end of this section, we shed light on some basics of ensemble learning.

Semi-supervised feature selection
Feature selection in semi-supervised context is generally guided by pairwise constraints that determine whether two instances belong to the same class or not, called must-link\cannot-link, respectively. In this regard, several research studies have been conducted, leading to a wide range of methods and algorithms. In Zhang et al. [62], a method called SSDR uses Must-link and Cannot-link constraints as well as unlabeled data to select the appropriate feature spaces that preserve both the local data structure and the pairwise constraints. Kalakech et al. [30] attempted to reduce the sensitivity of feature selection to changes in the constraint subsets. The solution was a simple combination of unsupervised and supervised methods (Laplacian and Constraint Scores [26, 61]) by a simple multiplication of their corresponding objective functions. Benabdeslem and Hindawi [8] introduced a more elaborate coupling between both scores in a new score function called the Constrained Laplacian Score (CLS). There is a claim that constraint selection prior to feature selection can improve performances. This issue has been addressed in a method called CSFSR [9], which also addresses redundant features. In Liu and Zhang [37], a pairwise constraint-guided sparse (CGS) learning method for feature selection has been proposed, in which the must-link and cannot-link constraints are used as discriminating regularization terms that focus directly on the local discriminative structure of data.

Recently, more advanced techniques were devised to solve feature selection. Li et al. [34] proposed a feature selection technique based on regularized regression equipped with a generalized uncorrelated constraint. Zhang and Li [65] propose a novel feature selection method, taking advantage of previous knowledge regarding pairwise constraints, minimization of data reconstruction error, and the graph embedding. Benouini et al. [10] proposed a new feature selection algorithm for a neighborhood rough set model based on Bucket and Trie structures, which can guarantee finding the optimal minimum reduction by adopting a global search strategy. Zhao et al. [66] proposed a framework that involves the CNN-based feature extraction from the MINST data set. Wang et al. [58] proposed a multi-level framework for fusion of features and classifiers in the setting of granular computing. Salmi et al. [47] introduced a new constraint score based on a similarity matrix, computed in the selected feature subspace and used to assess the relevance of a subset of features at a time.

Multi-label learning
Learning from multi-labeled data originated from works on text categorization, as documents may be related to different topics simultaneously. Recently, multi-label learning has been used in numerous applications ranging from protein function analysis [5] to semantic scene annotation [13, 42], and sentiment categorization [53]. Zhang and Fang [63] proposed a novel two-stage Partial multi-label learning (PML) that works by eliciting credible labels from the candidate label set for model induction. Sun et al. [52] present a novel Partial Multi-label Learning (PML) that removes the noisy labels from the multi-label sets. Another closely related task is multi-output regression [50], where target variables are real-valued. Multi-output regression arises in several interesting domains, such as predicting the wind noise of vehicle components, stock price prediction, and ecological modeling [31, 32].

Seen as an extension to single-label classification, approaches to cope with multi-labeled data are generally based on traditional algorithms. These approaches fall into two main categories: (1) transformation-based, and (2) algorithm adaptation [54]. The former methods are two-pass. In the first step the original problem is transformed into one or more single-labeled problems; any single-label algorithm can then be used to find the final solution. The major drawback of these methods is that they cannot deal efficiently with label correlations. On the other hand, algorithm adaptation methods suggest extending well-known single-label algorithms directly and without resorting to any transformation of the original data.

Similarly, feature selection methods for multi-label learning proposed rethinking single-label feature selection algorithms according to the aforementioned frameworks. Solutions proposed so far are mostly based on problem transformation methods, especially Binary Relevance and Label Powerset.

In Binary Relevance, for the prediction of a new example, we firstly transform the multi-label data set D into D|Y| data sets, where Y is the label space. Each resulting data set contains all the examples of the original multi-label data set, labeled by 1 or 0, depending on whether the label yi figured in the label subset of the corresponding example or not, respectively. Afterwards, |Y| binary classifiers are built, and the final solution will be the subset of labels positively predicted by every binary classifier [56]. Undoubtedly, Binary Relevance does not take into account correlations between labels.

As its name suggests, the Label Powerset method creates a new class for each subset in the power set of Y generating a multi-class data set, upon which we can apply any traditional multi-class classifier. The final solution will be the subset of labels corresponding to the predicted class [56]. This approach does take into account correlations between labels. However, it still has a serious disadvantage, as it can ends up with a large number of generated classes associated with only a few number of examples. This issue, commonly known as class imbalance, could bias the whole learning process.

Ensemble learning
Ensemble Methods are learning algorithms that combine multiple learners to classify new examples. It has been experimentally established that ensembles are, overall, far more accurate than their individual constituents, provided that the individual members are as diverse as possible [14]. Ultimately, the real challenge will be to find appropriate techniques for creating diversity within the ensembles.

Accordingly, the combining schemes developed so far are either based on manipulating data, or on manipulating learning algorithms. Manipulation of data is based on well-known methods like: boosting, bagging, Random Subspace Method [14, 24, 28]. These methods commonly employ the same base model on generated replicates of the original data. In particular, the Random Subspace Method proceeds by a random selection of feature subspaces to build individual classifiers, while both bagging and boosting train base classifiers by resampling training sets. The final classification decision is usually obtained by a simple majority voting. The difference between Bagging and Boosting lies in the resampling technique. While the former obtains a bootstrap sample by uniformly sampling with replacement from the original data set, the latter resamples or reweighs the data by placing stronger emphasis on instances that were misclassified by previous classifiers. These ensemble techniques can also be combined to obtain more viable classifiers as illustrated by Breiman [15], who combined bagging and random subspaces to form random forests. On the other hand, ensemble methods based on algorithm manipulation, like stacking and error-correcting output codes, combine multiple models of different types, which are then applied on a single data set [20, 59].

Similarly to the case of ensemble classification, ensemble techniques could be used to improve the stability of feature selection algorithms. In this respect, numerous research studies have investigated the use of ensemble techniques in single-label feature selection. Benabdeslem et al. [7] present an ensemble-based framework called EnsCLS (Ensemble Constraint Laplacian Score) for semi-supervised feature selection. EnsCLS consists in a resampling of data (bagging) and a random subspace strategy on features. While the CLS score is used as a base selector to measure feature relevance across the generated replicates of the original data, and the score average of all features is considered. Sun and Zhang [51] build ensembles by exploiting pairwise constraints. The algorithm performs multiple Constraint Score on multiple bootstrapped constraint subsets. The proposed algorithm, called the Bagging Constraints Score (BCS), constructs individual components using different constraint subsets generated by resampling pairwise constraints in the given constraint set. Instead of employing one base single selector on multiple modified versions of the data set, Saeys et al. [46] look into the use of ensemble feature selection techniques by combining multiple (unstable) feature selectors to yield a more robust and better performing ensemble selector. To further ensure diversity among the ensemble, bagging was also used to generate different bootstrap samples of the original data. Liu and Cocea [36] offered a nature-inspired approach of ensemble learning to improve overall accuracy in the setting of granular computing. Pes [41] provides extensive empirical evidence encompassing different kinds of selection algorithms and different application domains to test the applicability and utility of ensemble feature selection for high-dimensional data. Another recent survey was presented by Bolón-Canedo and Alonso-Betanzos [11]. Authors provided a general overview and discussed various issues related to ensemble methods for single-label feature selection. Surprisingly, little focus was placed on the application of ensemble methods to multi-label feature selection in spite of their proven potential. This research proposes an ensemble method by data manipulation, more specifically by using bagging and Random Subspace Method in a unified framework.

Semi-supervised feature selection, multi-label learning, and ensemble learning have, for the most part, been considered separately. In this work, we consider combining these tasks to improve feature selection when data are partially labeled and multi-labeled, simultaneously. In particular, we propose an ensemble method that uses a variant of CLS as a base algorithm, applicable to data associated with more than one label at the same time, referred to as multi-label learning. In addition to bagging and RSM, the proposed framework encompasses another combining strategy that randomly samples without replacement in the label space (RSL). Hence, the resulting data replicates will be modified along the instance, the feature and, the label spaces. This will not only allow us to enrich diversity, but also to deal efficiently with feature and label correlation.

Soft-constrained Laplacian score: S-CLS
This section is dedicated to a detailed presentation of S-CLS [1], which we use subsequently as a base algorithm for building our ensemble framework. S-CLS is a feature selection approach designed to support feature selection in semi-supervised multi-label context.

S-CLS can be thought of as a multi-label extension of CLS [8]. To meet this goal, S-CLS combines Laplacian and Constraint Scores [27, 61] in a mixed selection criterion applicable to semi-supervised multi-labeled data. In that case, two special configurations may occur: if all data is labeled (X=XL) then S-CLS is regarded as a multi-label generalization of the Constraint Laplacian Score. On the contrary, if all data are unlabeled, i.e. X=XU, S-CLS works the same as the conventional Laplacian Score. In a nutshell, S-CLS aims at selecting features that best comply with the constraints, while at the same time preserving the geometrical structure of the data.

In other words, a discriminative feature should have close values for instances which are neighbors or connected by a must-link, and inversely disparate values for instances in the Cannot-link constraint set or those which are geometrically far apart. The neighborhood is determined by means of the Euclidean Distance.

Constraints expressed by the two sets CL and multi-label learning (ML) are pairwise constraints, which can be explicitly provided by users or automatically driven from prior domain knowledge. In the second case, labels can be used to decide whether a pair of instances belong to the Must-link or Cannot-link sets. This assumption holds true only for single-label learning, where each instance is associated with one label. Hence, pairwise similarity between labels suffices to determine the membership of each pair of instances. On the contrary, in multi-label learning, examples are associated with subsets of labels instead, thus making it more tricky to build these constraints. In this regard, a new parameter is defined in the objective function, called soft constraint and denoted by P. In the next section, we provide some notations needed for the definition of the Soft Constrained Laplacian Score.

Each instance xi in the labeled part of data XL, is associated with a target vector yi=(yi1,…,yil,…,yiC), where the lth element is equal to 1 if xi is labeled by the label l, and 0 otherwise. To quantify the resemblance between two training instances in terms of their corresponding class labels, we use the Jaccard Index which measures the pairwise similarity among label subsets. Accordingly, the soft constraint will be computed as follows:

Pi,j=|yi∩yj||yi∪yj|
(1)
In the case of continuous values as in multi-output data [12], where instances are associated with subsets of real values, the Jaccard Index could be replaced by an Euclidean Distance. In this case, the feature selection algorithm will still be applicable to multi-output regression.

The greater Pij, the more the instances are similar, and the more powerful the soft constraint is. Pij=1 means that xi and xj must be linked (i.e. are in the Must-link set, to draw a parallel with conventional constraints); inversely, if Pij=0, xi and xj cannot be linked (i.e. are in the Cannot-link set). Thus, Pij can be considered as a linkage strength between xi and xj. This approach will not only make it possible to deal with constraint in multi-label learning, but also represents an intuitive manner to decide on the similarity between instances according to their labels.

In addition, labels could be weighted to favor important labels. In that case, more knowledge would be required concerning data in order to set the appropriate label weights. Alternatively, a label weight learning procedure could be used to compute weights automatically from data sets beforehand. In this respect, for instance, weight could be given to labels according to their occurrences in the data set. In this particular case, a label weight would be obtained by counting the total number of occurrences and averaging it through all instances.

The selection is carried out on the whole data set. The S-CLS score of a feature Fr, is mathematically formulated by:

S-CLSr=∑i,j(fri−frj)2Sij∑i,j(αirj−βirj)2Dii
(2)
This objective function needs to be minimized.

S(N×N) represents the pairwise similarity between instances. Elements Sij are defined by Eq. (3). If two instances xi and xj belong to XL, Sij is evaluated to e−∥xi−xj∥2λPij. If neither one is labeled and xi is among the k-nearest neighbors of xj (or vice-versa), Sij = e−∥xi−xj∥2λ. Otherwise Sij=0.

Sij=⎧⎩⎨⎪⎪⎪⎪⎪⎪⎪⎪e−∥xi−xj∥2λe−∥xi−xj∥2λPij0 if  xi  and  xj  are  neighbors  if (xi∈XL)∧(xj∈XL) otherwise 
(3)
λ is a regularization parameter that needs to be tuned to meet the data set at hand, while the terms αri and βrj are defined by Eqs. (4) and (5), respectively.

αirj={fri×(1−Pij)fri if (xi∈XL)∧(xj∈XL) otherwise 
(4)
βirj={frj×(1−Pij)μr if (xi∈XL)∧(xj∈XL) otherwise 
(5)
Where μr is the weighted data mean of the feature Fr:

μr=∑i=1N(friDii∑iDii)
(6)
D is the density matrix defined by:

Dii=∑j=1NSij
(7)
S-CLS is self-explanatory. In essence, most discriminative features must have close values in instances highly related by a soft constraint Pij, i.e. share a great number of important labels, or are geometrically located in the neighborhood of one another. On the other hand, they have large values for instances with a weak soft constraint or which are not bound by a vicinity relationship. Consequently, these features will get smaller S-CLS scores, and eventually will be selected as most relevant. Inversely, features where pairs of instances are far apart while they have a large soft constraint, i.e. have a lot of important labels in common, or which are neighbors, should be penalized. They will consequently get larger scores and will thus finally be discarded.

If two instances xi and xj share all labels in the label space, that is yi=(1,1,…,1) and yj=(1,1,…,1), respectively, their corresponding Pij will be equal to 1 (the pair (xi,xj) belongs to the Must-link set, in the context of single-label learning). In this case, in the objective function, the numerator, Sij is maximized, and one expects (fri−frj) to be smaller, to select the feature Fr. In contrast, if xi∈XL and xj∈XL, and there is no common label between them, then Pij=0 (which is equivalent to a Cannot-link constraint). In the denominator, (fri−frj) should be larger, for Fr to be a good feature. Finally, if two instances belong to the unlabeled part of the data set XU, feature variance μr will be used to retain the most informative features, assuming that labels have the same importance.

figure d
Algorithm 1 summarizes all steps in the Soft-Constrained Laplacian Score. The algorithm takes as input a semi-supervised multi-label data X to produce a ranking of features according to the S-CLS score. The algorithm starts by constructing the matrix of soft constraints from the label space; it then builds the similarity and density matrices. Subsequently, for each feature in the feature space, it calculates the corresponding S-CLS score and eventually the features are sorted accordingly in an ascending order. S-CLS runs in a time O(M×max(N2,logM)). Indeed, the first step of the algorithm requires L2 operations. In Step 2, the matrices are computed, requiring N2 operations. Step 3 computes the scores for M features requiring MN2 operations. The last step sorts features according to their scores in a time estimated by MLog(M).

The following section explains the S-CLS score function in the context of spectral graph theory [17]. In fact, a reasonable criterion for choosing a relevant feature is to minimize the objective function represented by S-CLS. Thus, the problem is to optimize both terms T1=∑i,j(fri−frj)2Sij and T2=∑i,j(αirj−βirj)2Dii, by minimizing the former and maximizing the latter.

To resolve these two optimization problems, we propose to construct their pre-defined graphs, respectively. Thus, the relevant features are those that respect theses graphs. The first one is a k-neighborhood graph, Gk, using the data set X, while the second one, GL, is constructed from the label space.

Given a data set X, let G(V, E) be the complete undirected graph constructed from X, where V is its node set and E is its edge set. The ith node vi of G corresponds to xi∈X and there is an edge between each node pair (vi,vj), the weight of which wij=e−∥∥xi−xj∥∥2λ is the similarity between xi and xj.

Let Gk(V,Ek) be a subgraph which can be constructed from G, where Ek is the edge set {ei,j} from E such that ei,j∈Ek if (xi∈XL)∧(xj∈XL) or xi is one of the k-neighbors of xj. Let GL(VL,EL) be a subgraph constructed from G, where VL its node set and {ei,j} its edge set such that ei,j∈EL if (xi∈XL)∧(xj∈XL).

Once the graphs Gk and GL are constructed, their weight matrices, denoted by Sk and SL respectively, can be defined as:

SSkij=⎧⎩⎨⎪⎪wijwijPij0ifxiandxjareneighborsif(xi∈XL)∧(xj∈XL)otherwise
(8)
and

SSLij={(1−Pij)20if(xi∈XL)∧(xj∈XL)otherwise
(9)
Then, we can define:

For each feature Fr, its vector fr=(fr1,…,frN)T.

Diagonal matrices DDkii=∑jSSkij and DDLii=∑jSSLij.

Laplacian matrices LLk=DDk−Sk and LLL=DDL−SSL.

We can easily develop the first term of S-CLSr as follows:

T1=====∑i,j(fri−frj)2SSkij∑i,j(f2ri+f2rj−2frifrj)SSkij2(∑i,jf2riSSkij−∑i,jfriSSkijfrj)2(fTrDDkfr−fTrSSkijfr)2fTrLLkfr
(10)
Note that the graph-structures are satisfied according to αirj in Eq. (4) and βirj in Eq. (5). In fact, when the label space does not exist (Y=∅), we should maximize the variance of fr which would be estimated as:

var(fr)=∑i(fri−μr)2DDkii
(11)
Optimization of (11) has already been undertaken by He et al. [27]. In this case, S-CLSr=fTrLLkfrfTrDDkfr. Otherwise, αirj=fri(1−Pij) and βirj=frj(1−Pij). Thus, we develop the second term (T2) as follows:

T2========∑i,j(αirj−βirj)2DDkii∑i,j(fri−frj)2(1−Pij)2DDkii∑i,j(fri−frj)2SSLijDDkii∑i,j(f2ri+f2rj−2frifrj)SSLijDDkii∑i,jf2riSSLijDDkii+∑i,jfrjSSLijDDkii−2∑i,jfriSSLijDDkiifrj2(∑i,jf2riSSLijDDkii−∑i,jfriSSLijDDkiifrj)2(fTrDDLDDkfr−fTrSSLDDkfr)2fTrLLLDDkfr
(12)
Subsequently, S-CLSr=fTrLLkfrfTrLLLDDkfr seeks those features that respect Gk and GL, meaning that Algorithm 1 can be rewritten by a graph formulation in Algorithm 2. Algorithm 2 takes the same steps as Algorithm 1. The main difference is that the S-CLS score formula is expressed in terms of the spectral graph theory. For this reason, an additional step, which builds the graphs Gk and GL, is added. The output, as for the initial algorithm is a sorted list of features.

figure e
Ensemble S-CLS: 3-3FS
The success of ensemble methods is highly dependent on the degree of diversity among individual members. This claim is supported by various research studies, which show that performance is definitely commensurate with diversity within the ensemble.

In this regard, we propose to apply simultaneously three ensemble techniques to different aspects of a data set. First, RSM performs a subspace selection to feature space, whereas RSL applies a random subset selection to label space. Afterwards, bagging applies a random sub-sampling with replacement to instance space. Finally, each data replicate will be a subsample obtained by bagging, projected on a feature subset selected by RSM, and associated with a label subspace chosen by RSL. These perturbed versions of the original data will be separately fed into the base selector (S-CLS) to generate different feature selectors, the combination of which is expected to guarantee more stable results.

This method allows us to ensure a large scale of diversity among components, helping both to alleviate the curse of dimensionality and treating label correlation. More specifically, the diversity is definitely ensured by the application of the three ensemble methods. Each will, through simple data manipulation, lead to the yielding of new data (reflecting a partial view of the original one) for each member in the committee of learners, and will thus ultimately generate diverse feature selectors. Hence, by varying the instance, the feature and the label subsets, it is possible to promote diversity and produce a more effective ensemble. On the other hand, the projection from the high dimensional feature space into the low dimensional space can avoid the problems caused by high dimensionality, while the selection of smaller subsets of labels for each subsample deals efficiently with label correlation.The Algorithm 3 outlines the proposed approach.

figure f
The Algorithm  3 has as input the different parts of a semi-supervised multi-label data set and aims to rank features according to their S-CLS scores. To this end, the algorithm iteratively constructs an ensemble of feature selectors. It starts by initializing the scores and occurrences of all features to zero. The occurrence variable is used to compute the average score of a feature, since it may appear many times throughout the ensemble. To construct a feature selector, at each iteration, we randomly select from F, the original input space, M features to build the new feature space denoted by RSMi. Meanwhile, we randomly select L labels from the label space Y: RSLi will be the new label space for this iteration. The former selections are carried out without replacement. In a second step, bootstrap sampling is performed in the supervised part of the data set XL. The new sample XiL is then projected on the corresponding feature subspace RSMi and assigned to RSLi as well. On the other hand, the unsupervised part XU of the original data set, is also projected on the same feature subspace RSMi and the result of this projection is denoted by XiU. By the end of the iteration, the resulting parts XiL and XiU are submitted to S-CLS. The constructed individual feature selector will compute the scores of each feature in the related feature subspace RSMi. Finally, the results of all feature selectors will be aggregated. According to their occurrences in the different subspaces, each feature will be assigned a final score corresponding to the averaging of its scores in each iteration.

Experiments
This section describes the general layout of the experiments, including descriptions of data sets, methods, and measure metrics used in the comparisons.

Data sets and methods
Multi-label data sets are commonly reported with certain statistics regarding different aspects about the instance, feature, and label distributions. These metrics play a key role in building learning algorithms and consequently affect their related results. Specifically, there is a general consensus amongst researchers that cardinality and density are likely to bias multi-label classifier behaviors. In fact, two data sets that have more or less the same cardinality and different densities will produce different results. Cardinality is defined as the average number of labels per instance, whereas density is the normalization of cardinality by the total number of labels in the label space.

In this example, D is defined as the whole data set formed by the data space X and the label space Y. Cardinality and density of labels can be calculated as follows:

LC(D)=1N∑i=1N∥yi∥1
(13)
LD(D)=1N∑i=1N∥yi∥1C
(14)
where ∥.∥1 is the l1-norm.

Experiments depicted in this section were carried out on various benchmarks that were drawn from the following domains:

“Emotions”, a data set in the field of audio annotation, which is comprised of 593 music tracks form various genres associated with different emotional reactions [53];

“Scene”, a data set on semantic scene annotation, which contains 2407 image-related instances associated with different tags according to the content [13];

“Corel16k”, consists of 13811 instances, 500 features and 161 labels [4];

“LangLog”, drawn from text categorization [43], including articles posted on the Language Log Forum related to 75 different topics;

“Delicious”, encompassing a total of 16105 instances, 500 features, and 983 labels [55];

“Imdb”, contains 120900 observations, 1001 features and 28 labels [45];

“Yeast” in gene functional categorization, a data set in the field of biology, including a collection of 2417 gene descriptions related to various gene functions [22];

“Plant” is composed of sequences of plant protein, with 440 features for each protein sequence: there are 12 locations representing the different class labels;

“Human” is a collection of sequences of human protein, with 440 features for each protein sequence: there are 14 locations.

The last two data sets focus on the prediction of sub-cellular locations of proteins according to their sequences [60]. Further information about these data sets is provided in Table 1.

Table 1 Description of the data sets used in the experiments
Full size table
In addition to the baseline (without feature selection) and S-CLS [1], the performance of the proposed algorithm is compared with five other state-of-the-art methods: ML-MI, PPT+MI, Memetic [33], NeuralFS [29], TSFS [38]. ML-MI and PPT+MI are both supervised multi-label feature selection methods based on Mutual Information. The latter utilizes Pruned Problem Transformation PPT, a variant of LP, to transform the original data set into a multi-class data set before carrying out a search strategy with Mutual Information to filter out irrelevant features, whereas ML-MI searches directly in the original data set using a multi-label variant of MI to get the best feature subset. Memetic is a wrapper algorithm for feature selection. Memetic feature selection makes use of a new generation of genetic algorithms called Memetic along with a performance measure, as a fitness function, to select the best subset of features. NeuralFS and TSFS are recent supervised single-label feature selection methods, based on Deep Learning, that we adapt to multi-label setting by binary relevance (BR) transformation.

Experimental settings
Data sets used in this work are originally supervised. However, to be usable in semi-supervised context, the experiments used a random selection with a rate of 10% of supervision, that is to say that 90% of data are selected as unlabeled, and the rest is taken as multi-labeled. S-CLS and 3-3FS perform on semi-supervised data, while other methods are totally supervised.

Besides, in the different equations, where the parameter λ is used, it should be carefully tuned according to the problem at hand. If overestimated, the function in Eq (3) will behave almost linearly and lose its non-linear power. If underestimated, it will lack regularization and the decision boundary will be highly sensitive to noise. In the experiments, λ is defined as follows:

λ=percentile({∥xi−xj∥2;i,j=1..N};20).
(15)
The choice of the number of neighbors k Eq (3) varies according to data sets. After empirically testing all possible values in the range from 1 to 10 for each data set, the optimal value of k is adopted.

In these experiments, parameters of the ensemble method are set as follows: The ensemble size is determined by equation 16

N=10×ceil(log(0.01)log(1−1/m−−√)).
(16)
This formula ensures that each feature is drawn ten times at a confidence level of 0.01. The number of features in each member is M=m−−√, where m is the size of the feature space, and the size L of each label subset is L=C−−√, where C is the cardinality of the original label space.

Classification performance of the different feature selection methods is evaluated using ML-kNN, a multi-label version of the traditional kNN algorithm [64]. This classifier is trained on the whole labeled training data and is tuned via 3-fold cross-validation.

Evaluation metrics
Performance evaluation of multi-label learning algorithms is more challenging in comparison with the traditional assessment of binary or multi-class classifiers [2, 3, 35]. As explained above, multi-label classifiers are asked to predict subsets of labels instead of isolated labels, making them more complicated to evaluate. Generally, multi-label evaluation metrics fall into two categories. The first one is example-based, whereby the performance of the learning algorithm on each example is evaluated separately. We then average over the entire test set. The second category consists of label-based measures, which evaluate the learning algorithm on each class label separately, and then average over the whole label space. These two categories can be further categorized into classification-related or ranking-related tasks. In the experiments, we used five of the most used measures from different categories to ensure a thorough comparison between methods.

Let h:X→Y be a classification hypothesis, which for each instance xi∈X, predicts its vector of labels h(xi)∈Y.

1.:
Hamming loss: Indicates how many times an instance-label pair is misclassified [49].

HammingLoss(h,D)=1N∑i=1N∥h(xi)⊕yi∥1C
(17)
h(xi)⊕yi stands for the XOR operation between the predicted label-vector h(xi) and the true label-vector yi.

2.:
Micro-F: Calculates the F-measure on the predictions of different labels as a whole.

Micro−F(h,D)=2∑Ni=1∥h(xi)∧yi∥1∑Ni=1∥yi∥1+∑Ni=1∥h(xi)∥1
(18)
3.:
Average Precision: Computes the average fraction of labels ranked above a particular label y∈Yi [48].

Avgprec(f,D)=1N∑i=1N1|Yi|×∑y∈Yi|{y′∈Yi|rankf(xi,y′)≤rankf(xi,y)}|rankf(xi,y)
(19)
|.| denotes the cardinality of a set.

4.:
Ranking loss: Evaluates the average fraction of label pairs that are not correctly ordered.

Ranking Loss(f,D)=1N∑i=1N1|Yi||Yi¯¯¯¯¯||{(ys,yt)∈Yi×Yi¯¯¯¯¯;f(xi,ys)≤f(xi,yt)}|
(20)
where Yi¯¯¯¯¯ denotes the complementary set of Yi in Y.

5.:
AUC: Area under ROC (stands for Receiver Operating Characteristic) curve is a criterion for measuring the quality of ranking, specifically the probability that a random positive example is ranked before a random negative one.

AUCtotal=1C∑j=1CAUCj
(21)
Results and discussion
Results reported in this section interpret the impact of feature selection on the performance of the base classifier ML-kNN. First, 3-3FS along with other feature selection algorithms are used to pick out most relevant features in relation to each data set. Subsequently, using the selected features, ML-kNN is tested by means of different evaluation measures, and results are reported in Fig. 1 and Table 2.

Figure 1 shows classification performance of ML-kNN using increasingly the 30 top-ranked features according to different feature selection methods. Performance is illustrated in terms of Area under ROC (AUC). Key findings are as follows:

When we increase the ratio of selected features, the classification performance first tends to increase and then remains stable or even degrades.

Our ensemble method 3-3FS outperforms the other algorithms in all data sets, while NeuralFS and S-CLS come second.

3-3FS does outstandingly well in the “scene” data set. We recall that “scene” has a strong correlation between labels.

3-3FS clearly works well in the “Imdb” data set.

3-3FS captures efficiently the correlation between labels.

Deep feature selection methods (NeuralFS and TSFS) are equally effective due to the many computational layers.

Features selected by 3-3FS are relevant and helped improve the ranking performance of learning.

Fig. 1
figure 1
AUC versus #numbers of selected features

Full size image
In Table 2, we introduced a baseline using ML-kNN with the entire feature space, that is without selection. The baseline will serve as a ground truth to show the gain/loss in performance when applying feature selection. Table 2 summarizes performance in terms of the most used and powerful evaluation measures. Values are obtained by using exactly 30 top-ranked features. For all evaluation measures, 3-3FS has the best values and shows the best performance in all data sets. More specifically, in terms of classification performance, represented by hamming loss and Micro-F we see a neat improvement of S-CLS when using the proposed ensemble technique. Moreover, ranking performance quantified by average precision and ranking loss shows a good gain when employing 3-3FS as a combining strategy to S-CLS.

Table 2 Results (mean ± SD) on all data sets used, over all measures
Full size table
Usage of S-CLS on diverse subsamples obtained by applying a 3-pass combining technique has led to very interesting results when compared to the basic S-CLS and other state-of-the-art algorithms. Features selected by our ensemble method helped gain in both classification and ranking accuracy of the chosen base classifier (ML-kNN).

Fig. 2
figure 2
Hamming loss (↘) versus percentage of labeled data

Full size image
In the following, we study the impact of changing the percentage of labeled data on the performance of S-CLS and 3-3FS. We increased the percentage of labeled data from 20 % to 100 % for the nine data sets. The percentage of selected features is set at 30%. Figures 2 and 3 show the performance comparison.

Key findings:

Whatever the percentage of labeled data, the corresponding metrics results (Hamming loss and Micro-F) of S-CLS and 3-3FS remained practically stable. i.e. better performance can be easily achieved with a reduced number of labeled data.

3-3FS consistently outperforms S-CLS in terms of Micro-F on all nine data sets.

3-3FS clearly outperforms S-CLS in terms of Hamming loss on eight data sets.

Some degradation in performance are reported with the “Yeast” data set where 3-3FS is ranked second after S-CLS.

Fig. 3
figure 3
Micro-F (↗) versus percentage of labeled data

Full size image
Statistical validation
In this section, we conduct a performance analysis among the compared algorithms systematically, according to Dems̆ar methodology [19]. We use the non-parametric Friedman test to prove that the methods do not have the same outcome and thus identify which algorithm is more performant. First, the Friedman test compares the average ranks of algorithms under the null-hypothesis, which states that all algorithms are equivalent. After calculating the Friedman statistic, we compare it to a critical value for a given level of significance in order to accept or reject the initial null hypothesis. Friedman statistic XF2 is calculated by Eq. (22) and is distributed according to the distribution of the chi-square with γ-1 degrees of freedom.

XF2=12θγ(γ+1)[∑j=1γR2j−γ(γ+1)24]
(22)
where γ the number of algorithms, θ the number of data sets (in our case, γ=6, θ=6), Rj=1θ∑jrji where rji is the rank of the j-th of γ algorithms on the i-th of θ data sets. If there is a significant difference between the algorithms, i.e. the null-hypothesis is rejected, we use the Nemenyi test as a post-hoc test to perform a pairwise comparison between the algorithms. The performances of two compared algorithms are considerably different if the difference between their corresponding average ranks is larger than or equal to a critical distance (CD), which is calculated by Eq. (23).

CD=qαγ(γ+1)6θ−−−−−−−√.
(23)
The critical value q0.05=2.84 at significance level α=0.05. The tests were performed on the Micro-F evaluation metric (Table 2). Results showed that the Friedman test rejected the null hypothesis and affirmed considerable differences between feature selection algorithms when applied as a preprocessing step to ML-kNN (XF2=49.31 where p value=1.9705e−8<0.05). The smaller the p-value, the stronger the alternative hypothesis compared to the null hypothesis. Figure 4 presents the results for the Nemenyi test, illustrating the CD diagram on the Micro-F evaluation metric. Any comparing algorithm whose average rank is within a CD to that of S-CLS and/or 3-3FS is interconnected with a thick line (in our case, CD=3.49). Otherwise, any algorithm not connected to S-CLS and/or 3-3FS is considered to have significantly different performance from each other. As can be seen, S-CLS and 3-3FS top the ranking, but their performances are not statistically distinguishable from performances of the other feature selection methods. T-test is employed to further support these rank comparisons. Table 3 includes a comparison of each data set and, for each pair of methods, the Micro-F values. Table 2 summarises the results of t-tests. Each cell in Table 3 has three values indicating the win/tie/loss counts of all pairs of methods, respectively. According to the results, the 3-3FS method is better than all the other methods in the sign test on the number of wins/ties/losses (nine wins out of nine compared to any of the other methods). S-CLS is ranked second and confirms its power in terms of selection precision.

Fig. 4
figure 4
Average ranks diagram comparing algorithms in terms of Micro-F

Full size image
Table 3 Pairwise t-test comparisons of feature selection methods over the values in Table 2 (Micro-F)
Full size table
Runtime comparison
As a next step, we empirically verify the efficiency of our proposal in terms of runtime in order to capture the full cost of learning with FS vs. learning without FS as well as deep learning methods. Table 4 lists the computational time (in seconds) required for all these methods across nine high-dimensional data sets. The major observations resulting from analysis of the table are as follows:

The running time varies obviously according to the data set size.

Learning over features selected by 3-3FS exhibits the lower runtime performance on all multi-label data sets compared to the baseline, without feature selection.

NeuralFS and TSFS are the most time-consuming compared to the 3-3FS method. This is due to the independence of our proposal from any learning process when selecting features.

Table 4 (a) Performance comparison in terms of running time of learning classifier “without” versus “with” feature selection by 3-3FS (measured in s), (b) performance comparison in terms of running time of feature selection methods: 3-3FS versus Deep Learning methods (measured in s)
Full size table
Additional experiment
In this section we test the effectiveness of our proposal with a practical application to E-mail filtering. To comprehensively evaluate our model, a real-world benchmark data set is employed. In the following sections, we give a little background on email filtering process, then we provide descriptions of the data set, methods and measures used in the comparisons.

Background
E-mail filtering is a very important process in distinguishing messages of interest from the huge amount of unsolicited messages we receive. Typically, an email consists of two essential pieces: the body/email text and the email header. The body is the heart of the email, which can contain web page, audio, video, images, files, and so on. The header is the area that precedes the body of an email and contains routing information of the message such as the “sender name”, “recipient”, “date”, “subject”, etc. Specifically, the headers such “From”, “To” and “Date” are mandatory. “Subject” and “CC” are optional, but very commonly used. In a nutshell, all the information contained in both the email pieces (header and body) represent features with potential for a more precise classification.

Feature selection is a very important step in the E-mail filtering process, especially when the message size is large. It chooses the smallest subset of attributes in the email that has the greatest impact on classifier performance. Thus, computational time and cost are reduced. In the following, we apply our 3-3FS on a real email data set in order to select the most useful features in the same data set.

Data set and methods
In this experiment, we used the EnronFootnote1 email data set in Read et al. [44] that contains the emails from 150 senior Enron officials. The data set encompasses 1702 email documents (instances) and 1001 features. Each email belongs to at least one of the 53 labels. The 53 labels are organized into 4 categories: coarse genre, included/forwarded information, primary topics, and emotional tone. Table 5, presents the different categories. Remember that all labels provided are inspired by the body/email text and/or the email header. More precisely, the label “Empty message” in Coarse genre in the Table 5 means that the verification is done at the email body level.

The performance of the proposed algorithm is compared with the six aforementioned state-of-the-art methods, including, ML-MI, PPT+MI, Memetic, S-CLS, TSFS and NeuralFS (see Sect. 5.1).

Table 5 The different categories of the enron data set
Full size table
Experimental settings
The enron data set is primarily supervised. However, 3-3FS exploits small-labeled-sample data sets to perform feature selection. In order to simulate this context, we randomly select 10% of instances from original data set as labeled data, while the rest is taken as unlabeled data. The parameters λ, k, N, M and L were set as given in Sect. 5.2. Performance is evaluated using ML-kNN and experimental results are averaged over 10 runs.

Evaluation metrics
In the experiments, we adopt the five mentioned evaluation metrics for comparison, including: Hamming loss, Micro-F, Average Precision, Ranking loss and Area Under the Receiver Operating Characteristic curve metric (AUC) (see Sect. 5.3).

Results and discussion
This section reports the results of our exhaustive experimentation on enron data set. Similar to the experiments in Sect. 5, we applied simultaneously the three ensemble techniques, RSM, RSL and bagging on different aspects of the data set. At the end, each data replicate will be a subsample obtained by bagging, projected on a feature subset selected by RSM, and associated with a label subspace chosen by RSL. The performances of the proposed 3-3FS approache and baseline models are shown in Table 6 and Figs. 5, 6 and 7.

Table 6 Results (mean ± SD) on enron data set used, over all measures
Full size table
Fig. 5
figure 5
AUC versus #numbers of selected features

Full size image
Table 6 and Figure 5 show the classification performance of ML-kNN using increasingly the 30 top-ranked features according to different feature selection methods. Performance is illustrated in terms of the most used and powerful evaluation measures. As shown in the table and the figure, our ensemble method 3-3FS outstandingly do well in the enron data set and outperforms other algorithms.

Fig. 6
figure 6
Micro-F (↗) versus percentage of labeled data

Full size image
Figure 6 shows the impact of changing the percentage of labeled data on the performance of S-CLS and 3-3FS. Note that the percentage of labeled data varies from 20 % to 100 % on the enron data set, and the percentage of selected features is set at 30%. From the figure, we can easily see that 3-3FS consistently outperforms S-CLS in terms of Micro-F, and also whatever the percentage of labeled data the results of S-CLS and 3-3FS keep practically stable.

Fig. 7
figure 7
The impact of using the features “All”, “Body”, “Subject”, “From”, “To” and/or “CC” on the classification performance of 3-3FS, PPT+MI, ML+MI, Memetic, S-CLS, NeuralFS, and TSFS

Full size image
Figure 7 shows the impact of using a specific features extracted from the email message on the classification performance. The features tested are, “Body”, “From”, “To”, “CC”, “Subject” and “All” meaning the combination of the five features above. According to the figure, the most useful feature on average is “Body” followed by “From”. They are the best performing representations for email classification. The least useful feature is clearly “To” and “CC” followed by “Subject”. It is very understandable that the feature “To” is not a very discriminative feature so that most messages sent to a user have the same address. “All” gives the best classification performance, but it is very computationally intensive.

Conclusion
In this paper, we analyze the performance of ensemble methods for semi-supervised multi-label feature selection, combining a small amount of multi-labeled data with a large number of unlabeled data. In this particular setting, we develop an ensemble framework to enhance the stability of feature selection algorithms and improve their performance. More specifically, we propose applying three ensemble methods simultaneously, one for each aspect of a data set. We use bagging for samples, random subspace method for input space, and random subspace labeling for the label space. This results in a committee of feature selectors working on different perturbed versions of the original data. Their outputs are subsequently combined to generate a more stable feature subset capable of guaranteeing better predictive performance for the problem at hand. Besides, issues related to correlation between labels and label importance were addressed and integrated in a variant of the S-CLS feature score. Results on ten real-world data sets from various domains show the relevance of the proposed methods in comparison with different state-of-the-art algorithms both in classification and label ranking.

Looking forward, we plan to enlarge the scope of the comparison to cover a large number of data sets and apply other methods, including different ensemble methods. Furthermore, it would be interesting to scale up the proposed ensemble framework in order to deal efficiently with large-scale data sets.