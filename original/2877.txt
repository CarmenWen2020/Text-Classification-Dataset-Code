Automated planning for problems without an explicit model is an elusive research challenge. However, if tackled, it could provide a general approach to problems in real-world unstructured environments. There are currently two strong research directions in the area of artificial intelligence (AI), namely machine learning and symbolic AI. The former provides techniques to learn models of unstructured data but does not provide further problem solving capabilities on such models. The latter provides efficient algorithms for general problem solving, but requires a model to work with. Creating the model can itself be a bottleneck of many problem domains. Complicated problems require an explicit description that can be very costly or even impossible to create. In this paper, we propose a combination of the two areas, namely deep learning and classical planning, to form a planning system that works without a human-encoded model for variably scaled problems. The deep learning part extracts the model in the form of a transition system and a goal-distance heuristic estimator; the classical planning part uses such a model to efficiently solve the planning problem. Both networks in the planning system, we introduced, work with a problem in its graphic form and there is no need for any additional information to create the state transition system or to estimate a heuristic value. We proposed three different architectures for the heuristic estimator to compare different characteristics of well- known deep learning techniques. Besides the design of such planning systems, we provide experimental evaluation comparing the implemented techniques to classical model-based methods.

Access provided by University of Auckland Library

Introduction
Classical planning is a powerful tool that can solve many well-modeled real-world problems in logistics, navigation, production and many other domains. The deep learning comes to the scene once we find ourselves modeling a problem that is too difficult to grasp and describe. Modeling every process that happens in a distribution warehouse, for example, can be too much for a human to encode. Such warehouse can comprise of a robotic team of forklifts, which ought to transport pallets with goods, in boxes of various sizes. Modeling such problem is a tidy work and needs expert knowledge of all the parameters of the logistic problem. Deep learning mechanisms can help to obtain such model by processing the visual input of said warehouse which eliminates the need to create a perfectly defined model by hand. Taking the human input out of this equation allows us to model complicated issues that would otherwise be impossible to solve with classical planning.

Creating symbolic representations of real-world problems is a well-known bottleneck in classical planning that puts constraints on types of problems we can solve. By taking advantage of the deep learning and using its ability to generalize, we can access more real-world domains and solve more complex problems when using scalable domain-free architectures.

The main focus of this work is to analyze the possibilities and limitations of using deep learning in combination with classical planning. Instead of replacing the planning process as a whole and trying to make the network learn a search algorithm, we decided to focus on partial replacement of two components involved in many standard planning algorithms, namely the transition system and heuristic functions.

Classical planning provides great methods for general problem solving. Unfortunately, these methods can struggle in large unstructured domains. On the other hand, deep learning methods have been demonstrated to work well on many domains without a clear structure. Therefore, combining both of these methods may remove the need for an explicit planning model.

Asai and Fukunaga in [2, 3] connected deep learning and classical planning by creating LatPlan, which is a system that takes in an initial and a goal state of a problem instance and returns a visualized plan execution. The image on the input is transformed and processed in order to generate a standardized problem representation, which can then be solved by classical planning methods. In [9], Garret et al. use machine learning techniques to create heuristic functions that improve a search algorithm. In [11, 12], the authors propose learning of policies for search algorithms. Such policies are used as heuristics and provide preferred operators for expansion.

Our work can be understood as a follow-up work of Asai and Fukunaga’s architecture. In contrast to their work, we use maze-like problems and images of these mazes are the input to our algorithm. The solution is then produced as a sequential plan navigating through the learned transition system, which in turn is generated by our learned model. To increase efficiency of the search, we use heuristic principle used by Garret et al., Groshev et al., and Gomoluch et al. In contrast to these approaches, we provide a solution able to learn the model both for the expansion part of the search and for the heuristic part of the search. Additionally, our approach is scale-free, which is not the case for the prior state of the art.

Related work
We work with multiple different problem domains which overlap with different disciplines. The Single-agent maze domain can be introduced as a simplified robotic path-planning problem [20]. The same holds for the Multi-agent maze which requires generating plans without collisions for multiple agents. The Multi-agent Path Finding [27] tackles this very challenge.

A key element of classical planning, a tool for general problem solving in Artificial Intelligence, is domain-independent heuristic computation. Many of such heuristics are a great inspiration for this work. For example relaxations [4], abstractions [7], landmarks [18], or potential heuristic [25] are all among the best tools for computing heuristic functions which are used in planners. Landmarks represent important parts of the problem which have to be visited in every plan. In the case of simple problem domains located on a grid, e.g., a maze, certain crossroads represent landmarks, because they have to be a part of every generated plan. Potential heuristic provides great information about the problem but not by extracting information from the graphic form. It creates a linear program which defines features of the problem and constraints that restrict the solution.

Besides classical planning, our work draws from another broadly developed research direction—Deep Learning. Stochastic Gradient Descent [5] is one of the most frequently used methods for training of deep neural networks. This principle hinges on correctly defined loss function. It is similar to the mentioned potential heuristic computation. In our case, the loss is focused on restricting output values of the network according to the monotonicity requirement of systematic search used in classical planning.

We aim to create a system without any model so we work with the problems in their graphic representation. To process this type of input we use common deep learning techniques which involve convolutional networks [22]. Creating a state-transition system involves residual connection from the ResNet [14] architecture which runs the input through the neural network and concatenates it with the original input. By using the residual connection we build the state-transition system using both the processed input and the original input so we avoid losing the information about the problem’s structure.

The Transformer [29] architecture introduced the attention mechanism. Attention allows a network to locate interesting parts of the input that can, in our case, be seen as landmarks. Similarly we can look this way at abstractions which aim to simplify the problem and solve its less complex version. Similar behavior can be achieved by using convolutional kernel to abstract information from a certain part of the problem. Since neural networks have a limited information capacity caused by their parameters, the problem has to be simplified in order to fit into the available memory. That forces the network to perform some form of simplification which is an analogy to the process of problem relaxation.

Another possible direction is creating a recurrent structure that iterates over a simplified encoding of the problem and emulates the search algorithm to obtain the heuristic estimate. Recurrent cells such as LSTM [16] and GRU [6] are widely used building blocks of recurrent neural networks that can emulate such behavior. There are many different recurrent cells such as MAC cell [19], which performs reasoning over the input encoded in a vector form.

The search algorithm emulation can be achieved not only by using a specific network architecture but also by using a different form of learning such as reinforcement learning [28]. This method also resembles search algorithm as it searches for the best policy to solve a problem. The reinforcement learning techniques have also been combined with deep learning and were introduced as deep reinforcement learning; however the absence of model building commonly causes poor scaling in problems with sparse goals [23].

Background
Classical planning
First, we focus on STRIPS [8], which provides a symbolic representation to a model-based planning problem instance.

Definition 1
(STRIPS Planning Task) A STRIPS planning task Π is a tuple

Π=⟨F,O,si,sg,c⟩
where F={f1,f2,...,fn} is a set of facts, which can hold in the world. A state of the world is defined by facts which hold in the state s⊆F . The set O={o1,o2,...,om} contains all operators which are transforming the world, si⊆F is the initial state, which consists of facts that hold in the initial state, sg⊆F is a goal state condition, which contains facts that hold in every goal state and c is a cost function c(o):o→R+ which gives each operator a positive cost.

Every operator o∈O is a tuple where o=⟨pre(o),add(o),del(o)⟩, pre(o)⊆F is a set of preconditions, which are facts, that have to hold in a state for the operator o to be applicable in that state, add(o)⊆F is a set of facts, which are added to the state after applying the operator o in s and del(o)⊆F are delete effects, which are facts that are no longer true after using the operator o.

We say that operator o is applicable in state s⊆F, if pre(o)⊆s. After applying the operator o in state s, we get state to s′, which is defined as s′=(s∖del(o))∪add(o)).

To create a STRIPS representation of a problem, we have to be able to construct the facts and the operators from the problem definition. To find a solution to such a problem, we construct a transition system, in which we look for a path to the goal state from the initial state.

Such a path is a sequence of states and operators, which starts in the initial state and ends in one of the goal states. Each operator in the sequence has to be applicable in the state that is in front of it in the sequence.

Definition 2
(Transition System) A transition system is a tuple Σ=⟨S,A,γ,c⟩, where

S is a finite set of states

A is a finite set of actions

γ:S×A→S is a state-transition function. γ(s,a) is defined iff a is applicable in s, with γ(s,a) being the outcome of action application.

cost:A→[0,∞) is a cost function assigning a value to each action. The cost value can have various meanings, for example time, price or anything we want to optimize.

Any problem Π defined as STRIPS by Definition 1 can be translated to a transition system Σ and solved by the means of path-searching algorithms. To do so, we create the set of actions A of Σ by taking the set of all operators O from Π. The set of states S contains all subsets of the facts set F from Π. The cost function remains and the state-transition function γ is defined based on actions in A.

In order to find a solution to a planning problem, we need to find a path through the induced transition system, which is typically done by one of the heuristic state-space search algorithms.

Definition 3
(State-Space Search) State-space search algorithm performs search over a graph G=(N,E), where N is a set of nodes and E is a set of edges. Having a planning problem Π=⟨F,O,si,sg,c⟩ and its induced transition system Σ=⟨S,A,γ,c⟩, N corresponds to S and E corresponds to A. The search starts in si, expanding each found state with γ, until a goal state is reached. In that case, plan π can be returned as a sequence of actions applied at each expansion of the search in order to reach the goal state.

State space of many problems can be very large and exhaustive search may not be the most efficient way to look for the solution. Generally speaking, state-space search can be done blindly; however, additional information can greatly improve its performance. This additional information is added in the form of heuristic functions.

Heuristic function h(s):s→R+ maps any state s∈S to a positive value. Heuristic function gives us an estimate of path length from the current state s to a goal state. A function which always maps h(s) to the length of shortest possible path is called perfect or optimal heuristic and is denoted as h∗.

Neural networks
Neural networks have proved to be a very powerful tool in many different domains. Here, we use the current state-of-the-art approach which uses feed-forward neural networks that learn through back-propagation using Stochastic Gradient Descent [26].

Our primary aim lies in creating two networks, each one to substitute a different part of the state-space search algorithm. First network is used to replace the state-transition function γ in order to generate possible successor states in the state-transition system as defined in Definition 2. The second network is used to replace a heuristic function h(s), which returns a number representing an estimate of the distance from state s to a goal state.

By replacing these parts of the state-space search, we avoid the need for creating a symbolic representation of the states which would have been necessary if we were to adhere to classical architectures. Thus, we make our problem domains model free. Creating a model would be necessary in classical architecture in order to obtain new successor states and to compute a heuristic value.

To implement the two mentioned models we used primarily convolutional neural networks (CNNs) as described in [21] and recurrent neural networks in combination with convolutional layers. The problem domains, in our case, have image-like structures which make CNNs a viable choice in trying to extract information from the visual representations. When using the recurrent neural networks we took advantage of the recurrent mechanism and added convolutional layers in order to process the image-like input.

Attention for neural networks
In the recent past, by introducing the Transformer architecture, attention networks have proven to be a great success as shown in [29]. In general, attention allows the network to focus only on subsets of inputs and requires the creation of attention masks.

In this work, we use soft attention in which the network focuses on input values that are between 0 and 1 as opposed to hard attention where the network focuses on either zeroes or ones.

Masks are generated using convolutional layer and a softmax layer of the same width and height as the input, with all values summing up to one. On having such a mask, we can then multiply the input features, resulting in a modified input with some of the features “emphasized” by the attention mask. The layers which generate the attention masks are also trained with the whole network architecture.

Fig. 1
figure 1
Example of generated attention masks for Sokoban instance

Full size image
In Fig. 1, we see an instance of a Sokoban puzzle (on the far left). In this case, there are ten attention masks in total. The ten images on the right show multiplication of the original Sokoban map by every generated attention mask. From the images it is clear that some of the masks focus on the map structure, while some of them focus on the positions of goals or boxes and other places in the image.

Reasoning recurrent network
The other tested approach includes recurrent neural networks, namely the MAC cell as introduced in [19]). Since our task is very different from the original task described in [19] we had to adjust the input and output processing parts of the network to suit our needs. The structure of the introduced recurrent cell has been preserved the same.

The MAC cell is a recurrent cell with similar properties to any recurrent cell like GRU [6] or LSTM [16] which are both frequently used. It runs for a given number of iterations and it uses output of the previous iteration as its input for the following one. In this case, the structure of the cell contains multiple modules where each module modifies the hidden state of the cell.

The networks which contain the MAC cell also contain an input and output module. Here we had to make slight modifications to the modules’ structures. The input module creates two more compact representations of the input tensor which are further processed through the network using the MAC recurrent cell. That is because the network is designed to take two inputs; therefore we create two different representations of the input and feed it to the network.

The output module originally ended with a softmax layer which was used to pick an answer from a fixed set of possible answers. In our case we need the output to be a single value which is then used as a heuristic value. Therefore, we modified this module to give a single value on the output.

Data domains
One of the key challenges of implementing the proposed approach is obtaining a good quality data sets in order to train the networks. That is one of the bottlenecks of this approach since there has to be a reliable dataset for each selected domain.

We use four domains in total. For each one of the domains we had to create our own data generator to create solvable problem instances in order to train all the neural networks. Three of the domains are instances of mazes with varying difficulty and the last one is a well-known game Sokoban. Examples of all four domains are shown in Fig. 2.

First domain is a maze with one agent and one goal (Single-agent maze). The agent can move only in its 4-neighborhood and every free cell in the maze is accessible by the agent.

The second domain is conceptually the same as the previous one, but there are multiple goal positions in the map (Multi-goal maze). An instance of such maze is solved by moving the agent to any of the defined goals.

The third domain is a Multi-agent maze where the same rules apply, but we have the number of agents greater than one and the same number of goal positions in the maze. All the agents have to move at the same time and the goals are not assigned to a specific agent. The goal state is reached when every agent in the map stands on a different goal position.

The last domain we use is the Sokoban puzzle. It is similar to the Single-agent maze domain in terms of movement, but it contains boxes additionally. To solve the puzzle, the boxes have to be pushed by the agent to occupy all the goal positions. There is no specification of which box has to occupy which goal position. Agent can push a box if there is a free space in front of it. It is not possible to push multiple boxes at the same time and it is not possible to pull boxes.

All introduced domains are defined on a square grid of the same width and height. Each problem instance is a grid that consist of cells where each cell contains one entity. There are four entity types for the maze-based domains (wall, floor, agent, goal) and five entity types for the Sokoban domain (wall, floor, agent, goal, box).

Fig. 2
figure 2
Examples of all four problem domains

Full size image
Expansion network
In the state-space search (Definition 3), transition function takes in the current state of the search and returns all its successors. The expansion network is used to generate these successors.

By observing pairs of states (in form of images of the mazes with the agent) without knowledge about the actions that connect them, we want to learn possible actions for the problem. Even for the most simple maze domain, the size of the data set is important in order to train the network. The task does not only lay in locating the free spaces around the agent. We need to make sure that the maze structure remains the same and no rules are broken on performing the learned actions.

We work with problems represented as images; therefore as mentioned earlier, it is convenient to use a CNN for this task. In the case of Single-agent maze, Multi-goal maze and Multi-agent maze we want to focus on the 4-neighborhood of the agent. Therefore, we choose the kernel as a 3×3 window. We preserve the size of the input through the whole network so we use the same padding.

In the case of Sokoban the rules of movement are very similar and we also want to focus on the 4-neighborhood of the agent but since we can also push the boxes, we need to look one step further. Therefore, we use 5×5 window. Such window allows us to see the free spaces around the agent but in the case when there is a box next to the agent it allows us to decide if it is possible to push it.

To additionally improve the network, we use residual connections. A residual connection is an architecture modification, which is often used in deep learning and has achieved great results in learning an identity function, for example, in ResNet image classification network [14]. Furthermore, residual connections resulted in a reduction in the complexity of the network and an improvement in the results.

The expansion network uses one residual connection. As we can see in Fig. 3, the residual connection concatenates the original input of the network with result of multiple convolutional layers that process the exact input. Except for the residual connection, the network architecture is a convolutional network that uses same padding through the whole process. We also use dropout equal to 0.2 between the convolutional layers as a form of regularization.

Just like every other architecture we use in this work the expansion network contains several parameters we altered and tested to get the best possible configurations, namely

the number of channels (denoted as k in Fig. 3)

the size of the convolutional window

the size of the padding

We experimented with the number of channels k in architectures for all the problem domains. The size of the convolutional kernel and size of the padding were mostly parameters tested for the Sokoban domain.

Fig. 3
figure 3
Expansion network architecture for the 3×3 instances (mazes) with 4 entity types. The 5×5 case with 5 entity types for Sokoban is analogical

Full size image
All expansion network configurations were trained on 10000 batches, each having 50 samples (problem instances).

Input and output
As we mentioned earlier, the input of this network is a visual representation of the problem represented by a grid. Each domain has a fixed number of possible entities which can be located in the grid and each position can contain only one entity. Therefore, we can create a one-hot representation of the problem by giving each cell on the grid a vector with marked entity type. An example of this representation on a simple maze problem is shown in Fig. 4. That leads to a one-hot tensor representing the problem. Its dimensions are width and height of the problem instance and the third dimension marks the number of possible entities in the problem domain.

Fig. 4
figure 4
Example of one-hot encoding of the problem

Full size image
In Single-agent maze, Multi-goal maze and Multi-agent maze the number of entities is four in total. We have the agent, the goal, walls and empty spaces. In Sokoban puzzle domain we have the agent, boxes, goals for the boxes, walls and empty spaces. Therefore, the size of the third dimension in this case is equal to five.

We already stated that the input of this network is a representation which contains information about every position in the grid. The output of the network has the same size as the input and contains probability distribution over possible entities on the grid. As we can see in Fig. 5 the regular transition function receives the current state and returns a set of all possible successor states. The expansion network receives the same input in the one-hot encoding and returns one state that contains information about all possible successor states.

Fig. 5
figure 5
Comparison of the classical transition function and the output of the proposed expansion network. The transition function in classical planning returns a set of successor states. Our expansion network returns all expansions in form of a probability distribution over the one-hot encoded states. It is possible for the agent to execute two moves as it can either go to the right or up. Both of these cells will then have encoding that reflects 50% probability of agent and 50% probability of free space being on the cell

Full size image
If there are two empty cells where the agent can move (just like in our example in Fig. 5) the probability of an agent occurring at each of the cells is going to be 50%. Each of those cells has 50% chance of having an agent entity on it and 50% chance of being empty. That leads to the encoding [0 0.5 0.5 0] where second value represents probability of a free space and the third value represents probability of an agent occurring on the cell. There is also a zero probability of agent on the starting position and probability equal to one for the free space on the starting position, because the agent cannot stay on one spot and it can stand only on free spaces.

Since a classical transition function is deterministic, but our expansion network is probabilistic, we need a way of determinization [31] to be able to use the expansion network in a discrete deterministic search later. Therefore, from the cell entity distribution on the output of the network, we derive the actual successor states using a determinization threshold δ. We set δ=0.15 so every cell with probability at least 15% of the agent being there gets selected as a possible next state.

All positions selected as agent’s placement in the successor states are further processed so that they can be inserted into the priority queue that is used by the state-space search algorithm. Every state is also checked for validness so if there is an invalid agent’s position suggested by the expansion network, it will not be permitted into the priority queue and the planner will report invalid state and fail.

Loss function
To train a neural network it is necessary to define a loss function. A loss function receives output generated by a network and output that a network is expected to generate. The reason of the loss function is to quantify how successful the network is and its partial derivatives with respect to network’s parameters are used to adjust weights in the network.

In this network, we want to learn probabilistic distributions of the possible successor states of a current state. Our data is one-hot encoded, which means that there is a vector for each cell in the problem representation grid. According to the entity type placed on that cell, we put 1 to the corresponding index in the vector.

Heuristic network
Another function in the state-space search (Definition 3) is the heuristic function which aids the search process. In this work, we compute a heuristic value based on non-simplified visual representation of the problem without using an explicit latent representation. During the experiments we explore three different network architecture types. All architectures were trained on the same data with 10000 batches, each having 50 samples (problem instances).

Since the input data is still based on the visual representation of the problem, convolutional networks are a good direction to explore. One of the explored approaches is creating a purely convolutional neural network. Then, inspired by the classical planning approaches for computing heuristics, we use attention to simulate simplification of the problem. Such simplification is usually used in classical planning heuristics in the form of relaxations or abstractions [10].

The third and final approach is using recurrent neural networks. The intuition behind them is that the recurrence works as a simplified search emulator. Therefore, the network can be capable of reasoning over the problem. For this approach we use the MAC recurrent reasoning cell [19].

Input and output
Even though we explore multiple approaches and architectures for the heuristic network the input and output of the network stays the same. The heuristic network receives a one-hot encoded visual representation of an input state. The label of the state is the value of the h∗ heuristic which is the length of the shortest path from the input state to the closest goal state. Therefore, we want the network to produce a single value for each input as well. This generated value is then used as a heuristic in the planning algorithm.

Although learning a heuristic estimator from optimal plans sounds rather nonsensical (why to learn something we know the ground truth for), it acts in this work more as a proof of concept. As a future work, we want to use bootstrap learning technique [1], which address specially this problem.

Dataset structure and loss function
In order to train the heuristic network it is important to say what do we want to learn. Since neural network represents a black-box approximation scheme, we cannot ensure any properties of the generated values. Therefore, we use satisficing planning for our experiments instead of optimal planning.

One property of the heuristic estimates which can influence the performance of the planning algorithm is monotonicity of the values. Having monotonic heuristic values for all the states provides us with a possibility of selecting the best states on just following the descent of the state heuristic values. To get as close as possible to this property, we implemented a custom loss function, which measures how far off is the monotonicity of the learned values when compared to the h∗ values.

We already stated that we use initial states and length of their optimal solutions to train the network. Since we aim to learn monotonicity which is a relationship between values, it is not enough to provide just random state-value pairs for the network. To determine the monotonicity of different values, we must provide the values for the same problem instance. That is why we create a data set that contains various problem instances but each instance is in the data set multiple times with a different agent placement and therefore a different h∗ value as its label. That way the network can learn a connection between the values which are related to the same problem instance.

In order to train the heuristic network we have to define a loss function which returns a scalar value for any valid given input. Our main task is optimization so we aim to converge to a minimal possible error regarding the outputs of the heuristic network. To do so, we use the Stochastic Gradient Descent (SGD) algorithm [5] which is one of the most used optimization algorithms in deep learning.

Our loss function takes the labels of the data (h∗ values) and the values returned by the network. We are not looking into learning the same values, but we are aiming at learning values which will keep the monotonicity property of the learned heuristic function. Therefore, the loss computes how far off is each returned value from being correct, in terms of being monotonic in the context of all the other values for the particular maze instance:

dij=M=loss(x,y)=max(0,τ−sign(yi−yj)⋅(xi−xj))⎡⎣⎢⎢d11⋮dn1…⋱…d1n⋮dnn⎤⎦⎥⎥∑i=1n∑j=1n(M−Iτ)
The symbol n denotes the number of problem instances on the input of the heuristic network. During training it denotes the number of samples in a batch. The x is output of the heuristic network where xi denotes its elements. Its size is equal to number of problem instances on the network’s input. The y, of the same size as x, contains labels for the problem instances on the input that are denoted as yi. In the case of heuristic network we use optimal solution lengths as the labels.

The dij denotes distance between the ith and jth value in the output vector x multiplied by signum of the ith and jth value in the vector of labels y. We aim to compute distance of these values since our main goal is to learn monotonicity of the values. The parameter τ denotes tolerance to small noise in the distances. The dij values make up the matrix M which is always symmetrical. We subtract identity matrix Iτ from M to create zeros on the diagonal since the diagonal represents the relationship between ith and jth value, where i=j, which always has to be zero. The resulting loss value is a scalar that is obtained by summing the result of said subtraction over both dimensions.

Example. Let us compute the loss for a simple 3×3 maze example problem as displayed in Fig. 6. In the training data set we have three samples where each one contains the agent on a different cell. Labels of these data samples are lengths of the optimal solutions of these agent placements, namely y=[4,2,3]. After evaluating a current state of the neural network (with the initial randomized weights) on these data samples, we obtain a vector x=[10,7,5]. The parameter τ=1. Next step in the training process is computing the loss for these two input vectors.

Fig. 6
figure 6
Example data samples used for demonstrating the loss function computation

Full size image
d11=d12=d13=d21=d22=d23=d31=d32=d33=M=loss(x,y)=max(0,1−sign(4−4)⋅(10−10))max(0,1−sign(4−2)⋅(10−7))=d21max(0,1−sign(4−3)⋅(10−5))=d31max(0,1−sign(2−4)⋅(7−10))=d12max(0,1−sign(2−2)⋅(7−7))max(0,1−sign(2−3)⋅(7−5))=d32max(0,1−sign(3−4)⋅(5−10))=d13max(0,1−sign(3−2)⋅(5−7))=d23max(0,1−sign(3−3)⋅(5−5))⎡⎣⎢max(0,1)max(0,−2)max(0,−4)max(0,−2)max(0,1)max(0,3)max(0,−4)max(0,3)max(0,1)⎤⎦⎥=⎡⎣⎢100013031⎤⎦⎥∑i=13∑j=13⎛⎝⎜⎡⎣⎢100013031⎤⎦⎥−I⎞⎠⎟=∑i=13∑j=13⎡⎣⎢000003030⎤⎦⎥=6
We subtract the tolerance matrix Iτ in order to create zero elements on the diagonal, because on the diagonal we see the relationship between each sample and itself. From the very last step in the loss computation we can see that the matrix is symmetrical (which is always the case) and the positions [2,3] and [3,2] contain the same value which is greater than 0. That tells us that the relationship between the second and third value generated by the network in x is incorrect and the monotonicity of the values is not the same as in the case of the labels in y. In the label vector y we see that y[2]<y[3] but the values returned by the neural network show that x[2]>x[3].

Since we are using SGD to train the neural network we aim to minimize this loss function. The minimal possible loss is loss(x,y)=0 which means that there is no discrepancy in the relationship between all the heuristic values in the data set that were returned by the neural network. Using such values to follow a heuristic function would in a GBFS algorithm lead to the same results as following the h∗ heuristic.

After loss computation we use the resulting value in the backpropagation algorithm to compute partial derivatives with respect to all the parameters of the network. Value of the derivative for each weight in the neural network denotes how much it contributes to the output of the network. The weights are adjusted during the backward pass through the network. That concludes the training process of the network.

CNN network
The most simple architecture for the heuristic network we use is a simple convolutional network. It processes the input state through a set of convolutional layers which results in a vector representation of the state. The vector representation is then processed with linear layers and the output value is the resulting heuristic value. The whole architecture is displayed in Fig. 7.

First convolutional layer of this network extends the number of channels of the input with 1x1 window and then processed the input through multiple convolutional layers with same padding in order to keep the input size through the whole network. It then performs aggregation over the width and height, which creates a vector which is then processed through linear layers resulting in one output value.

This architecture is the simplest one of the ones we have trained but it still has different possible configurations which can be compared. In order to find the best network configurations we experimented with:

the parameter h (size of the linear layers in the architecture in Fig. 7)

the addition of coordinate channels

Parameter h represents size of the linear layers in the network. Addition to the input in form of coordination channels requires adding two new channels to the input data. One channel contains x coordinate values for each grid cell and the other contains y coordinate values for each grid cell. By adding these, the input’s number of channels increases by two so the first convolutional layer processes 6 channels instead of the original 4 so there is a small modification required.

Fig. 7
figure 7
Heuristic network—CNN architecture

Full size image
CNN attention network
The second architecture of the heuristic network also relies heavily on the convolutional layers. One of the most notable features in this architecture is the usage of attention as described in section Attention for Neural Networks. It is analogous to the problem simplification in classical planning (e.g., relaxation [10] or abstraction [15]). If we imagine looking at a maze and identifying interesting parts of it, such as crossroads or long straight paths, we might simplify the problem enough to obtain a distance estimate from the agent to the goal.

Implementation of the attention was done by using convolutional layers and softmax over the first two dimensions of the input. This means that at the end, we received the attention mask, which has the same width and height as the input and all its values sum up to one.

The first part of the architecture is denoted as “Attention block” in Fig. 8. In this block we use the mentioned convolutional layers to create attention masks and multiply the input by each of the masks as displayed in the schema. We also concatenate coordinate channels with the output of the attention block. There is one channel with x coordinates for each of the grid cells and a second channel with y coordinates for each of the grid cells.

The second part of the architecture is a convolutional neutral network which has a very similar structure to the previously described expansion network in Sect. 4.

This network has several parameters we experimented with to find the best configurations:

the number of the attention masks

the number of used attention blocks

One problem which comes up when using attention is the problem of deciding the number of attention masks required to find enough attention-worthy places in the data. We experimented with different numbers of attention masks in the architecture while selecting the networks that are further used in the planning experiments. Another architecture modification is the use of multiple attention blocks through the network. One option is using just one block at the beginning which is displayed in Fig. 8. The other option we tested is placing an attention block between every two convolutional layers in the network which results in using five blocks in total. The increased number of attention blocks also leads to much slower computation.

Fig. 8
figure 8
Heuristic network—Attention CNN architecture

Full size image
The first step in the proposed architecture in Fig. 8 is the processing of the input through the attention block. In this block, we create the attention masks, multiply each mask with each channel of the input and concatenate all the results. At the very end, we add the coordinate channels.

The output of the attention block is then processed by multiple convolutional layers with the same padding. Then, we take the output of the last convolutional layer and sum it over its width and height. Thanks to this operation we create a vector which does not rely on the problem’s width and height and we can use the network for problems of any size. The vector is then processed with a linear layer which returns a single value.

RNN network
This model’s architecture is based on the MAC recurrent cell described in [19]. We have modified the introduced MAC cell to meet our requirements for learning the heuristic function.

The first part of the network we altered is the input module. The original network has two inputs, an image and a question in text. Each of these inputs is processed differently. In our case, the only input we have is the image representation of the problem, so the same input processing does not apply to our data. We created two convolutional networks to create two different embeddings of the input image which is further processed in the network.

The output is different, as originally the network learned to pick an answer from a fixed set of possible answers. In our setup, we learn a single numeric value. Therefore, the last layer of the output processing module has to be modified to return one scalar value.

The authors of the [19] architecture train the network with a fixed number of iterations of the MAC cell, which are completed every time the network runs. This appeared to be a bottleneck for our problem, since we aim to create a solution that is capable of scaling with an increasing problem size. The network is capable of processing any problem instances with arbitrary sizes. However, we assume that for larger problems the reasoning has to take longer and therefore we wanted to avoid using a fixed number of iterations. To solve this problem we added a self-stopping module into the network.

The self-stopping module gets a maximal number of iterations after which needs to stop. After each iteration computed by the MAC cell it takes its outputs, concatenates them and passes them through a linear layer and a sigmoid layer. The output of this processing is a single value for each sample which determines if the sample gets processed again the next iteration or if its output gets saved and the sample gets labeled as stopped. The MAC cell stops when it reaches the maximal number of iterations or when all the samples get labeled as stopped.

In this particular architecture of the heuristic network displayed in Fig. 9, there are several parameters which can be altered:

the addition of the coordinate channels

the maximum number of iterations of the recurrent cell

the usage of self-attention

the usage of gating

the size of the embedding (parameter h giving size of embeddings e1 and e2 in Fig. 9).

The addition of coordinate channels is the same as in the case of all the other architectures. There is a possibility of adding one channel with x coordinates for every cell in the grid and one channel with y coordinates for every cell in the grid. The maximum number of iterations is required by the self-stopping module and we selected either 100 or 200 iterations. The usage of self-attention and gating is already mentioned in [19]. There are possible additions to the architecture which appear in the write module. We trained the network both with and without them. And the last parameter is the size of embedding. Both embeddings use this parameter to determine size of their outputs and this value is also used to determine size of different layers inside the architecture.

Fig. 9
figure 9
Heuristic network—RNN architecture c is output of the Control module, r is output of the Read module, m is output of the Write module which represents memory

Full size image
Experiments
We have trained the proposed network architectures with varying hyper-parameters to obtain the best possible networks. These selected networks were then integrated to our implemented planning algorithms to be compared against techniques used in classical planning. We compared the expansion network with classical transition function as described in Definition 3. We also compared the heuristic network with blind heuristic, Euclidean distance (ED), and textbook implementation of HFF [17] and LM-cut heuristics [24].

Our main goal is to verify the proof of concept and show that the expansion network and heuristic network architectures can be used in classical planning search algorithms. We provide results of the other heuristics and state-transition function in order to give context to results of the neural networks.

Expansion network evaluation
From all the trained expansion network architectures we had to choose the best ones to integrate them into the planners. To do so we created an evaluation function to determine which of the expansion network configurations are the best.

Evaluation function for the expansion network is used to check how accurately it can generate the possible successor states while also checking whether the problem structure stays the same. We want the network to give us a distribution over the entity types which contains only valid and reachable states. However, we also want it to keep structure of the problem the same and not to move any entities where they are not supposed to be (e.g., walls). The same goes the other way—it is not desired to obtain any unreachable states in the output distribution.

Wall difference (wall diff) denotes the largest value assigned to a cell which is not supposed to be a wall. This means that we aim for the lowest possible values. In evaluation table 1, it is obvious that learning the wall placement identity is not a big problem for any of the networks.

Minimal probability of correct expansion (min corr) denotes the smallest probability of a cell containing a correct entity. The smaller the values, the less probable it is that agent can be located on the cell. In the case of exp-net-maze in Table 1, we can see that the value is 0.23, which means that the least probable correct agent placement has this value (note that maximum is 0.25 when there are 4 possible new placements of the agent).

Maximal probability of incorrect expansion (max wr) is analogous to the previous case. Here, small probabilities denote probability of incorrect expansion. In the case of exp-net-maze, the value is negligible, so there are no invalid successor states generated.

Since the three maze domains are similar in structure, we were able to train one generalized expansion network for all Single-agent maze, Multi-goal maze and Multi-agent maze domains. We expected that Single-agent maze and Multi-goal maze will be able to share the expansion network, because their rules of movement are exactly the same as there is only one agent in the map. However, the Multi-agent maze differs, because we have to move all the available agents at once so we are no longer just looking for a local state of environment around one agent but we have to locate all of the agents and compute the successor states for all of them at the same time.

In Table 1 we can see performance of two expansion networks, exp-net-maze which was trained on Singe-agent maze data and exp-net-ma-maze which was trained on Multi-agent maze data. We see that exp-net-maze did not perform well on Multi-agent maze data and did not find all valid reachable actions (see the min corr values). On the other hand, the exp-net-ma-maze performed just as well on the Single-agent maze data (maze data) as it did on the Multi-agent maze data (ma-maze data) and therefore we selected it as the one expansion network that is further used in the planning experiments.

Table 1 Expansion network evaluation for maze-based domains
Full size table
For the Sokoban domain, we have trained a different network, as the structure of the problem contains an additional entity type (boxes), which required to extend the structure of the network accordingly. With the 3×3 training window, similarly as in the three maze domains, the expansion network was not able to learn the transition function, i.e., wall diff was <0.01, min corr was <0.01 and max wr was 0.09. Since the rules of Sokoban require pushing boxes, not only moving the agent in the 4-neighborhood, we also trained the network with the window of size 5×5. This led to similar results as with the smaller window, particularly: wall diff <0.01, min corr <0.01 and max wr >0.10. Such results represented various errors as missing valid steps completely, misplacing the walls or suggesting invalid movements for both agent and the boxes.

The most plausible reason with the 5×5 windows is the pure combinatorial complexity of the learning problem, where there is 55×5⋍3×1017 possible permutations of the input window; therefore the training set covers only minuscule portion of all the situations the network faces during planning. The 3×3 case is combinatorially not so hard (53×3⋍2×106), on the other hand, misses substantial structural principle of the Sokoban mazes and that is the box pushing; therefore it does not represent the domain correctly.

Heuristic network evaluation
To show that deep learning can be used to provide a satisfactory method for automatically deriving a heuristic function, we created a planner framework for every discussed domain. We compare the learned heuristic with two baseline heuristics: blind and Euclidean distance (ED) heuristic and two classical planning heuristics HFF and LM-cut. As we previously stated, the heuristic values we are able to obtain from the heuristic network are values which do not have any guaranteed properties. Therefore, the resulting planning is satisficing. LM-cut, as an admissible heuristic, assures optimal planning (if used with optimal search algorithm as A*) and is therefore included only to complete the results. However, head-to-head comparison is not possible.

For each domain we implemented a state-space search algorithms. Namely the greedy best-first search (GBFS) which uses the h values to guide the search. In every planner we integrated the proposed expansion network and heuristic network which can be arbitrarily combined with all the other implemented methods.

Hyperparameter selection
Training neural networks also requires selecting the right hyperparameters. In Sects. 4 and 5 we stated parameters for each of the proposed architectures. We trained models with all various combinations of available parameters and evaluated their results to select the best ones. The evaluation was performed using metrics which are evaluated in our experiments. Namely we focused on coverage (percentage of problems from the data set that were solved), average length of found solutions and average number of expanded states during the search for the heuristic network. For the expansion network we followed evaluation methods introduced in Sect. 6.1 which tell us how well did the network learn the state transition system.

Parameters of the expansion network we use in the planning experiments are:

the number of channels = 64

the size of the convolutional window = 3

the size of the padding = 1.

Selection of the size of the convolutional window is connected with the padding size because of the network’s architecture. Using padding equal to 1 with convolutional window with size 3×3 makes the convolutional layer produce output of the same width and height as the input. That allows us to make this architecture scale-free as it always produces output that has the same width and height as the input. The other option is using padding equal to 2 with convolutional window with size 5×5.

The number of channels which is used inside of the expansion network architecture is the smallest from our selection that was capable of learning the state transition system of the Multi-agent maze domain. We use one trained expansion network for three domains so we selected parameters that were sufficient to train the network for the most complex domain and which were capable of evaluating the rest of the maze-based domain perfectly. The selected number of channels is 64 and it has been selected from the options 64, 128, and 256.

In Sect. 5 we introduced three architectures with various parameters. The CNN architecture proposed in Sect. 5.3 has two parameters which influence size of the linear layers in the architecture and decide addition of coordinates into the input data.

The second proposed architecture of the CNN_att network in Sect. 5.4 also has two parameters. The first one defines the number of the attention masks and the second one defines the number of used attention blocks.

The RNN architecture, which is the final one introduced in Sect. 5.5, has five parameters. The self-attention and gating parameter is connected to functionalities available in the recurrent cell. One parameter stands for adding the coordinate channels. One parameter is used to set the maximum number of iteration that the recurrent cell can make before it is stopped. The last parameter defines the size of the embedding that is created from the input data.

Table 2 Selection of hyperparameters for all heuristic network architectures
Full size table
Table 2 shows parameters of all heuristic network architectures that were the best according to our evaluations for each domain and problem size when using the GBFS algorithm. Parameters which achieved the best results for each domain and domain size are emphasized.

There are a few things to note regarding the parameters. The CNN architecture rarely achieves the best results with the maximal available h (linear layer size). Only in the case of Multi-agent maze which is the most complex maze domain we use. The architecture also does not benefit from the added coordinates.

The CNN_att architecture has one dominating parameter which is att_masks (number of the attention masks) equal to 10. Three out of four domains achieve better results with number of attention masks equal to the higher available value. The only domain which uses 5 attention masks is Multi-goal maze. In the case of Multi-goal maze domain we have two agents and one goal so the area of the maze that has to be covered per agent is smaller than in the other domains.

The parameters for the RNN architecture also repeat in the table. One interesting thing to notice is that we never achieved the best results by combining the gating and self-attention with adding the coordinates into the input. The coordinates on the input have a certain advantage but when they are used together with the enriched module in the recurrent cell they do not improve the results.

To demonstrate how the selection of parameters changes the network’s performance we took the RNN architecture that is used for the Sokoban domain with the following parameters:

the addition of the coordinate channels - YES

the maximum number of iterations for the recurrent cell = 100

the usage of self-attention - YES

the usage of gating - YES

the size of embedding = 32.

If we fix some of these parameters we can clearly see the influence of the other parameters on the network’s performance in Fig. 10. For instance, if we fix the embedding size and number of iterations as we displayed in the left column, we can see that the selection of the remaining parameters influences all of the metrics. Coverage is influenced only slightly in the case of using only gating and self-attention. That supports the idea of coordinate addition which has been used according to [30] that provides us with a better coverage. We can also see that the coverage is equal to one when we do not add coordinates, gating or self-attention at all. That can be partially caused by simplification of the network’s evaluation which makes it evaluate faster with worse heuristic estimates. That is supported by the average plan length values which are longer compared to all the other parameter combinations.

Another option is to fix number of iterations and usage of coordinates, gating and self-attention to use them all and change the size of embedding. That is the case displayed in the right column in Fig. 10. In this case we see that the largest embedding which is 64 causes lower coverage and also increases the number of expanded states significantly. That can be caused by a higher resource demand that is necessary when using a larger embedding in the recurrent network that slows down its evaluation.

From these comparisons we can see that the selection of the parameters is important not only to achieve the greatest results but it also influences the time required for the network’s evaluation. Since we use the networks in a planning system, evaluation time also plays a great role. In some cases it is necessary to choose which metric is going to be the leading one. In this case we mostly focused on coverage and the quality of the solutions when selecting which configuration of parameters is going to be selected in the final planning experiments.

Fig. 10
figure 10
Influence of selected hyperparameters on the results of the RNN heuristic network. In the left column we can see the influence of different parameters when fixing the embedding size as well as the maximum number of iterations. In the right column we can see results after using gating, self-attention and adding coordinates but choosing different embedding size. The evaluation was performed on the 8×8 Sokoban domain

Full size image
Planning experiment data
Data for the planning experiments consist of unique problem instances that were not used to train any of the networks. For the Single-agent maze, Multi-goal maze and Multi-agent maze domains there are four data sets in total, each one is corresponding to a different size. Each data set contains 50 problem instances. The sizes are 8×8, 16×16, 32×32 and 64×64. As we already mentioned the expansion network was trained on 3×3 instances and can be used on any instance of any size. And the heuristic networks were trained on 8×8 instances and can be used on arbitrarily large instances. All of our solutions are designed to be scale-free.

In the case of the Sokoban domain we have a data set of size 50 with unseen problem instances. The sizes we use in the planning experiments are 8×8 and 16×16. We also performed experiments on the Boxoban data set [13] which contains 1000 hard problems. To keep the number of problem instances the same through the experiments, we selected 50 random problems from the Boxoban data set which were used for all the Boxoban experiments.

Evaluation of the experiments
To evaluate the experiments and compare performance of the planners using different techniques we decide to use several metrics. The first of them is the solved problem coverage (the percentage of problems from the data set that were solved). The second metric is the average number of the expanded states during the search and the last metric is average path length of the found solutions. We are in the field of satisficing planning; however, the solution quality is also a valuable information which can contribute to better comparison of provided techniques.

All experiments were performed on the same machine with 64 CPU cores, 346 GB of memory and a GPU for computation of the neural networks required in the planning process. Each planning instance had a time limit equal to ten minutes.

Algorithms in the experiments using the traditional state-transition function are denoted as GBFS. The ones which use the expansion network are denoted as GBFSnn.

Each of the planning experiments was performed on 50 problem instances that were not present in any of the training or testing data sets. The experiments were evaluated for the regular state-transition function and the expansion network, for GBFS also for 4 heuristics (blind, ED, HFF, LM-cut) and three types of heuristic networks (CNN, CNN_att, RNN).

Coverage analysis
The Single-agent maze domain which uses the traditional state-transition function has 100% coverage for all problem sizes and all used heuristics and heuristic networks. The difference in coverage starts to show in usage of the expansion network. Problem instances of sizes 8×8 and 16×16 have 100% coverage of all instance, but as the problem size grows, the coverage starts to drop as we can see in Table 3.

At 32×32, the CNN attention network has coverage only 68% which is caused by its high time requirements to compute the heuristic value. At 64×64 we can see even bigger drop in coverage. It is partially caused by the problem size and also by the slower generation of the successor states. Computational requirement of the expansion network is certainly higher than of the state-transition function.

The Multi-goal maze domain shows a similar trend which is to be expected since it is very similar to the Single-agent maze domain. The coverage starts to go down at size 32×32 again and it also happens first to the CNN attention network as shown in 3. At 64×64 we can see a drop of coverage through the whole table but the RNN network is still providing second-best or best coverage.

The Multi-agent maze domain is a more complex problem since we have to move both agent simultaneously so right from the size 8×8 we can see that LM-cut suffers from low coverage due to high computation time. Again, at size 32×32 we can see the coverage going down. The CNN_att network gets the coverage drop first from the provided heuristic networks. The 64×64 instances show the limits for all the compared heuristics.

In the case of the Sokoban domain, we used search with classical transition function (denoted as GBFS instead of GBFSnn), as the expansion network did not learn a usable function, see Sect. 6.1. Right from the start we can see that HFF and LM-cut show near to no coverage which is caused by computationally heavy heuristics on Sokoban problems. From the heuristic networks, the CNN network performs the best in terms of coverage and outperforms all other heuristics.

Plan length analysis
The average plan lengths are evaluated on the same experiment setting as in Sect. 6.5.1 Coverage analysis. To provide only significant plan length averages, we did not present results with coverage of solved problems lower than 0.33 (see Table 3).

For the Single-agent maze and the Multi-goal maze domains, we can see that for sizes 8×8 and 16×16 all the heuristic networks perform as well as the classical heuristic functions in terms of quality of the found solutions. From size 32×32, we can see that the average plan length of CNN attention heuristic network decreases, which is caused by its lower coverage. We can also see that the configurations using expansion network perform worse because of the time requirements, just as we saw in the coverage analysis. We can also see that RNN heuristic network has the second best results after the HFF heuristic.

The Multi-goal maze domain shows very similar results in the smaller data sets as the Single-agent maze. CNN and RNN heuristic networks give as good results as for example HFF or LM-cut up to size 32×32.

The Multi-agent maze domain has a lot more variation in the coverage so the average generated plan lengths results are harder to interpret. However, in the smallest 8×8 data set we can see that the heuristic networks in general return longer solutions than the HFF or LM-cut heuristics. Same goes for the 16×16 where we can compare HFF with CNN and RNN networks due to their full coverage. In this case, we can see that both heuristic networks lead to generating longer paths.

Expanded states analysis
The number of expanded states during search is the main factor that determines which heuristic navigates the search in a better direction. In this analysis, we also established a similar coverage threshold of 0.33 which denotes minimal coverage that needs to be achieved in order to include results in analysis.

In both Single-agent maze and Multi-goal maze domains we can see that the heuristic networks expand more states than HFF or LM-cut. From the provided heuristic networks it is clear that the CNN_att network expands the highest amount of states. On small problem instances, the difference is close to none, but as the problem instances grow, the number of expanded states is rising. For example in the case of Single-agent maze domain we can see the difference between RNN network and HFF where RNN network expanded nearly ten times more states. In Sokoban domain, all the heuristic networks outperform the classical heuristics.

Comparison of computational time
We discovered differences in the described metrics which were influenced only by using a different state-transition function. Therefore, we wanted to measure all the state-transition functions and all the heuristic functions to see if there are any differences in the run time that could also influence the performance of the search algorithm. All presented values are an average computed from 5 measurements over the same problem instance. Time limit for each measurement was 10 minutes.

First, we compared the state-transition function with the expansion network. As we suspected, the expansion network evaluates slower than the state-transition function. As expected, the greatest difference appears on very large instances of very complex problems like Multi-agent maze of size 64×64. The second comparison was computed for all heuristic functions and the heuristic networks. It is no surprise that blind and Euclidean heuristic are very fast to compute but they are also not very informative. HFF and LM-cut are both very informative heuristics but they are also more costly to compute. HFF heuristic performs well for all maze domains and its computational time does not increase even with larger problem sizes. However, on Sokoban the time necessary to compute the heuristic value for size 16×16 exceeded the limit for the measurement. LM-cut exceeded the measurement time even for the Multi-agent maze domain for sizes 32×32 and 64×64.

The heuristic networks all compute the value very fast and there does not seem to be a problem with increasing problem size. The CNN attention network is slower for the Multi-goal maze, Multi-agent maze and Sokoban than the other two heuristic networks. That is also a trend we saw in the heuristic experiment results as coverage decreases. The CNN network has lowest computational times which is caused by its architecture, which is very simple compared to the other two networks. However, even though RNN heuristic network has a complex recurrent architecture, it does outperform HFF in Multi-goal maze, Multi-agent maze and Sokoban.

Table 3 Coverage for Single-agent maze, Multi-goal maze and Multi-agent maze domains using GBFSnn and Sokoban domain using GBFS
Full size table
Table 4 Plan length analysis for Single-agent maze, Multi-goal maze and Multi-agent maze domains using GBFSnn and Sokoban domain using GBFS
Full size table
Table 5 Analysis of expanded states for Single-agent maze, Multi-goal maze and Multi-agent maze domains using GBFSnn and Sokoban domain using GBFS
Full size table
Table 6 Comparison of computational time for all state-transition functions and heuristic functions
Full size table
Discussion
In Sect. 6 we provided results of all planning experiments performed with state-transition function and the expansion network as well as the 4 implemented heuristic functions (blind, ED, HFF, LM-cut) and 3 architectures of the heuristic networks (CNN, CNN_att, RNN). We evaluated all the experiments with respect to the proposed metrics. Now we explain the outcomes even further and discuss their contributions in theory and also in real-world scenarios.

Coverage discussion
The results provide several insights into the problem of learning heuristic information. First, with growing size of the problems, the performance of the heuristics and therefore the coverage degrades. This is not a surprising result (for the classical heuristics as well), since the increasing size of the problem increases complexity of the problem. Moreover, we can see that although the heuristic networks were trained only on the 8×8 problems, they perform nearly perfectly on the 16×16 maze domains as well and are on par with the classical heuristics in Sokoban. They have a good performance on 32×32 mazes as well with exception of the hardest Multi-agent maze. This suggests the networks generalize (abstract in the classical planning terminology) the problem and provide a scale-free solution, similarly as the best classical heuristics.

Second, on the 64×64 Multi-goal maze and 32×32 Multi-agent maze, the simpler CNN network exhibits better performance over the more complex RNN network; however in the 64×64 Single-agent maze the RNN network strongly outperforms the CNN network. The explanation is that the Single-agent maze needs better counting of distance to the goal cell, which the RNN should be more capable of (provided that it is able to learn the problem at all, which in the case of the simplest maze it was). The CNN, however, has to propagate the distance information only indirectly through the overlapping convolutional windows from the agent position in the maze to the only goal position. This phenomenon is not a problem in the mazes to the size 32×32, since statistically the initial position of the agent and the goal are close enough to allow the search per se mitigate this weakness. Nevertheless, in the largest 64×64 Single-agent maze this manifested strongly and the performance span between CNN and RNN is nearly 100%. In the Multi-goal and Multi-agent mazes, the goals are statistically closer (as there is more of them); therefore the precise distance counting is not so important.

Third, the information learned by the attention modules is valuable in navigating the search. In theory, the attention should provide an alternative way of distance learning. The structure of the network is complex which causes its training and evaluation to be costly and it cannot be outbalanced by the added information value.

Plan length discussion
Based on the plan length analysis results, we can say that all of the heuristic networks perform as well as the classical heuristics on less complex domains (Single-agent maze, Multi-goal maze). Even on the large instances such as 64×64 Single-agent maze we can see that HFF and RNN are on par w.r.t. the plan length. We can see a similar trend in Multi-agent maze domain as well. In the Sokoban 8×8 domain, the CNN_att network outperforms the other networks and classical heuristics, but it is not keeping up with the results in the other map sizes.

The high quality of solutions in smaller instances could be caused by the heuristic networks being trained with optimal path lengths. However, we can see that the solution quality does not decrease even for larger instances in Single-agent maze and Multi-goal maze domains. Since the networks were trained on 8×8 data we can see that the computed heuristic values actually provide a reliable value estimate that can lead the search and generalized the problem of navigating in maze problems.

Expanded states discussion
Based on the analysis of expanded states, the heuristic networks tend to expand more states in more complex domains which is expected as the heuristic estimate gets more complicated to compute. From size 32×32 we see the number of expanded states exceeds the classical heuristics in most cases. Interesting case is the CNN_att network which expands more states than the other heuristics in most simple domains but does very well on the Multi-agent domain of size 8×8. Attention might be powerful for extracting information about global and not only local environment as it focuses on multiple locations in the grid, but it seemingly does not generalize well for the bigger problems.

Real-world overlap
Our results show that both the expansion network and the heuristic network can be used in combination with the classical planning algorithms. As we discussed, the results show the ability to generalize to larger and more complex problem instances. That gives us a set of tools which can be used to apply classical planning algorithms to any problem which can be properly learned.

The expansion network did successfully substitute state-transition function in the planning algorithm. Its high computational requirements cause it to evaluate less states during the search and it can happen that not enough states are expanded in order to find the solution. This can be a problem in smaller problems with easily hand-encoded domain definitions. However, a distribution warehouse with multiple machines, routines, arbitrarily large cargo and many details could still benefit from a slower state-transition system with better learned rules.

The heuristic network proved to be able to guide a search and generalize to larger problem instances. In contrast to the expansion network, the heuristic network evaluates faster than some of the heuristic functions we used in the experiments. The time necessary to train any of the heuristic networks is then balanced by the faster evaluation during the search. The heuristic estimate of very complex problems can be more informative when using the classical planning techniques; however the time requirements of heuristic network architectures are usually lower for arbitrarily large problems. In the case of complex real-world scenarios like the distribution warehouse or a forklift fleet, the time requirements might outbalance the heuristic estimate quality.

Said forklift fleet can be abstracted as the Multi-agent maze problem we use in our experiments. The learned heuristic can be used as a centralized heuristic function for any robotic fleet, drones or different group of devices with well-defined behavior.

Conclusion
In this work, we have proposed replacement of the two key parts of search-based automated planning algorithm by deep neural networks. One network learns the planning model from an image representation of state transitions. The other network learns the heuristic function from an image representations of states and their distances to a goal. Such architecture allows for the use of automated planning for model-free problems. Experimentally, we have shown the efficiency of such search is on par with the classical planning heuristics and therefore is a viable direction for future research.

Our results show that in the terms of coverage, the heuristic networks are on par with the classical heuristics and they also respond the same or better to increasing problem complexity. They provide scale-free solutions without the need for an explicitly defined model and their time requirements for computation are significantly lower than for well-informed classical heuristics (LM-cut, HFF).

The scale-free property is very important since the proposed architectures learn from smaller data samples (8×8) and are able to evaluate arbitrarily large problem instances. The results of our experiments also show that the networks are able to generalize. The heuristic networks proved to provide an informed heuristic to guide the search even in the case of much larger instances than 8×8.

The expansion network was able to learn rules of the state-transition system for all maze domains; however the time requirements of the network’s evaluation can cause the search to expand not enough states to find a solution in a given time limit.

The quality of the generated solutions is the same for the heuristic networks and classical heuristics for simple problems such as Single-agent maze and Multi-goal maze. The solutions get longer as the complexity increases in Multi-agent maze and Sokoban domains. The same trend appears in the analysis of the expanded states where the heuristic networks expand ten times more states than HFF heuristic in the case of the Multi-agent maze domain. As we assumed, the heuristic networks perform worse in domains with global information necessary to solve the problem. That is caused by multiple moving entities located in a single problem instance.

One interesting direction for future work is bootstrapping which can even strengthen the message of the work that machine learning can be used to allow model-based planning techniques with model-free definition of problems. Another direction which would generalize our architectures is the addition of a problem encoder which would create suitable representation from image or video inputs. Such integration is one of our future goals as it requires another system (possibly neural network) to be implemented.

This field of research also meets many limitations. One of them is resources as the network architectures become more and more complex and their training requires time and also computational power. Another current limitation is the usage of the system only on problems represented by a grid. Data generation is a problem in general. In the domains we used in this work, we were able to create our own data generators. However, if we were to focus on more complex domains, obtaining data becomes more complicated. That could also be tackled by an image-processing unit to create data for training from real-world problems. Such extensions would help to deploy proposed techniques to the real-world problems as automated modeling and planning for robotized distribution warehouses or a forklift fleets.