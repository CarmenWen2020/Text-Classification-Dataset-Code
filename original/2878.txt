In order to realize globalization of cloud computing, joint use of different services of different cloud providers has become an inevitable trend. The geo-distributed cloud consists of several different clouds, providing a general environment for cloud computing. In data placement, many recently proposed data placement algorithms unilaterally use a single performance index to evaluate the performance of the algorithm. In task scheduling, when tasks are allocated with excess cloud resources, resources are wasted. When little cloud resources are allocated to the complex task, cause the overall progress of the system to stagnate, the overall progress of the system is stalled. For solving the above problems, the data placement method and the task scheduling method are proposed. In the proposed data placement scheme, multiple performance indicators are considered. The detection of the straggling nodes and the reasonable allocation of cloud resources are taken into account when the task is scheduled. For proving the superiority of the proposed methods, extensive experiments are conducted. In terms of the data placement, when the number of files is set as 800, the safety level of the proposed data placement algorithm is 7.0, which is 27.3% higher than that of the IDP algorithm, 45.8% higher than that of the GA-DPSO algorithm and 16.7% higher than that of the H2DP algorithm. As for the task scheduling, the percentage improvement in the time overhead of the proposed task scheduling method is the lowest, which implies that the time overhead of the proposed task scheduling algorithm is closest to the optimal time and is the shortest.

Access provided by University of Auckland Library

Introduction
As a new business computing model, to provide users with cheap on-demand services, Cloud computing combines a large number of physical units to provide users with rich and diverse computer resources. In cloud computing, Quality of Service (QoS), Service Level Agreement (SLA), security and privacy requirements are emphasized under the business model [1]. To meet the various needs of users, cloud service providers deploy data centers in different locations. For example, Netflix, a well-known video media company, uses Amazon's cloud computing resources to deploy tasks in data centers in more than 20 countries to provide better video streaming services to its more than 100 million customers worldwide [2].

As cloud storage is the storage choice for individuals or enterprises, cloud storage contains a large amount of sensitive information. To deal with a malicious network environment, sufficient security performance is required. Due to the rapid upgrading of hardware, the configuration of new nodes and old nodes is often different. Then the cluster environment becomes heterogeneous [3]. In the cluster, the presence of the straggling nodes can seriously affect the whole system.

In data placement, for dealing with the bad network environment and satisfying the actual needs of users, some evaluation indicators are applied for evaluating the merits of its performance. Due to the different business scales, business volume and complexity of each tenant, the phenomenon of load imbalance often appears. So, load balancing cannot be ignored in the data placement model. Other performance indicators should be optimized under the premise of ensuring the security of stored data. The data placement problem can be regarded as a Multi-objective Optimization Problem (MOP). When the task is scheduled, firstly, the anomaly detection method is used to identify the straggling nodes. Then, the tasks are no longer assigned to the straggling nodes and the tasks in the straggling nodes are assigned to the normal nodes with free task slots. Finally, according to the task priority and resource service capability, the task is scheduled in the optimal way.

In current studies, the existing works cannot solve the above problems comprehensively and efficiently. As for the research problem of data placement, some existing researches fail to consider the perfection of the optimization variables of multi-objective optimization, and some fail to consider information security. In task scheduling, the node state and the task attributes are ignored. The task is stuck on inefficient nodes, which affects the efficiency of the cluster.

For solving the above problems, the research highlights are listed as follows.

(1)
In order to ensure the safety of data placement, the safe distance between nodes is defined and used as an index of MOP. In the meanwhile, the minimum time of data placement, the total overhead and the load balancing are also taken into account in MOP.

(2)
For improving the task scheduling method, according to our anomaly detection method, the nodes are identified as normal nodes and straggling nodes. The priorities of tasks and resources are calculated based on the various performance metrics, and the corresponding mapping relationship is established. Then, tasks are then properly scheduled.

(3)
In the experiment of evaluating the efficiency of the proposed algorithms, many experiments are conducted, the data placement method based on the transmission time, the safety level, the total cost and the load balancing (TSCL) has low transmission time and cost on the basis of ensuring the safety of data. In addition, the load balancing of the task scheduling algorithm based on the anomaly detection and the priorities of tasks and resources (AD-PTR) is the best among the benchmark algorithms.

The structure of the whole paper is shown as follows: The related work is presented in Sect. 2. The data placement and task scheduling models are shown in Sect. 3. Section 4 shows the pseudocodes of the proposed methods. The effectiveness of our proposed methods is evaluated in Sect. 5. In Sect. 6, the conclusion is described.

Related work
For unified management and utilization, the distributed computing system integrates resources of different regions, types and performance into a virtual resource pool. When the required data and the task are not on the same node, data transmission is an inevitable problem. For improving the fault tolerance, a simple replica creation approach is often adopted to guarantee system security in the cloud computing system [4]. In the past, the number of replicas is determined as a static value. In this case, frequently accessed replicas may not be enough, and then the access requirements can only be met by data transmission. In addition, in the measurement of data transmission standards, unreasonable measurement standards do not really reduce the data transmission time, which seriously affects the execution efficiency of the cloud computing system. Data placement is closely related to task scheduling. The unreasonable data placement and task scheduling schemes create more time. On the one hand, the data transmission time and the overhead increase. On the other hand, the throughput of the system reduces and the efficiency of the system is affected. Therefore, for reducing the transmission time, improving the efficiency, the research topics are important.

Data placement
At present, for reducing the time of data placement and the total overhead in the process of data placement, making the node load more stable and ensuring the security of data, many researchers study how to optimize the various performance indicators of data placement. Some existing data placement techniques are analyzed as follows.

Some existing works [5,6,7] optimizes a single goal, focusing on optimizing a single performance parameter in data placement. Of course, multiple performance metrics in data placement is considered in some studies [8, 9]. These studies optimize the multiple performance parameters comprehensively. However, the research objects in these papers are also not perfect, the data security is not taken into account.

In addition, some researches [10,11,12] focus on the security of data placement, but do not optimize other performance parameters on the basis of ensuring data security. In their proposed model, the data security is considered. However, few parameters are optimized, the total retrieval time is the key parameter to be optimized [11]. R V et al. proposed a placement strategy to reduce the time while ensuring data security [12].

In conclusion, the recent researches do not consider the various evaluation indexes of data placement. Some previous data placement models optimize for a single goal. And some data placement models do not take the data security into account. Compared with the existing state-of-the-art data placement techniques, the novel findings of our proposed data placement method are shown as follows.

(1)
The performance metrics for many aspects of data placement are considered. Firstly, the load balancing degree, the data security, the transmission time and the total overhead are defined. Then, the solution algorithm is used for solving the MOP problem.

(2)
For ensuring the data security, in a distributed method, many data blocks of varying sizes from the same data are placed on nodes at different geographic locations. After one or some data nodes of the system are attacked by malicious attack, hackers cannot guess all the sensitive information in the whole data.

Task scheduling
Nowadays, for improving the time overhead of task scheduling and optimizing the allocation of resources, many studies focused on the task scheduling. Some state-of-the-art task scheduling techniques are described as follows.

Some existing works do not take into account the impact of slow nodes on cluster efficiency [13,14,15,16,17]. In this paper, these nodes that execute too slowly are called straggling nodes. In this case, tasks may be assigned to the straggling nodes, resulting in a timing exception for task scheduling. So, if the straggling nodes are not found, the overall response time of the system will become longer, and the user's data transmission will have a high delay, so the QoS cannot be satisfied [18, 19]. Kumar et al. created a task queue [13]. Dubey et al. balanced the workload of nodes and improved the time overhead [14]. Jana et al. used multiple parameter indicators [15]. Panda et al. focused on the number of the task and the resource [16]. Sreenu et al. improved multiple parameter indicators [17].

Some current studies ignored the attributes of the tasks and the resources [20,21,22]. In this case, resources may be misused and the demand for complex resources are not be met. If tasks with high execution time are scheduled first, the scheduling time will be longer, the system delay will increase and the QoS is not satisfied [20, 23,24,25,26].

In addition, some current studies note the importance of multidimensional attributes of tasks. However, in their proposed model, the troubleshooting of the straggling nodes is not combined with the multidimensional attributes of the task. And the attributes of the resource are not taken into account. Specifically, Ali et al. strived to satisfy the QoS. In the studies above, tasks are distributed into 5 species according to the multiple attributes of tasks [27].

To sum up, the detection of abnormal nodes and the attributes of tasks and resources are not considered simultaneously in the existing works. The main contributions of our proposed task scheduling method are summarized as follows.

(1)
According to the various performance indicators of the nodes and the information about the historical state of the nodes, the current state of the node is represented. Then, according to the theory of statistics, the node state threshold is calculated. The threshold and the current node state are used to determine whether the node is a straggling node. Finally, tasks are no longer assigned to the straggling nodes, and tasks in the straggling nodes are assigned to other normal nodes.

(2)
In order to allocate proper resources to tasks, tasks and resources are prioritized according to their properties and the proposed algorithm. Then, the mapping of tasks and resources is established on the basis of expelling the straggling nodes. Finally, tasks are scheduling according to the mapping.

Data placement optimization model and task scheduling model
The system architecture is shown as follows. The architecture includes the model based on MOP and the model of task scheduling based on anomaly detection. A central cloud and several edge clouds exist in the system. The performance of the physical machines of clouds varies greatly. The system architecture is shown in Fig. 1.

Fig. 1
figure 1
The system architecture

Full size image
From the original data, the safety level, the transmission time, the total cost and the load balancing are calculated. Then, by using the Non-dominated Sorting Genetic Algorithm II (NSGA-II), the data placement strategy is obtained.

As for the task scheduling method, firstly, nodes are divided into the straggling nodes and the normal nodes, according to the proposed anomaly detection algorithm, and then tasks are no longer scheduled to the straggling nodes. Next, considering the multidimensional attributes of tasks and resources, tasks and cloud resources are sorted. Finally, according to the table of normal nodes, priority-based task queue and priority-based resource queue, the mapping between tasks and cloud resources is established.

Data placement model based on the transmission time, the safety level, the total cost and the load balancing (TSCL)
The establishments of the objective functions
The amount of the data node is defined as M, each data node is defined as nodei(1≤i≤M)(1≤i≤M), and then the set of the data node is represented as Node=[node1,node2,…,nodeM]. The graph G(Node,Line) represents the whole system. Line=(l1,l2,…,lx) is the set of connections between data nodes, where 0<x<M(M−1)/2. The storage space of nodei is denoted as nodestoi. The communication bandwidth between nodei and nodej is denoted as nodecommi,j. The bandwidth is denoted as a matrix B, and Bm1,m2=Bm2,m1. Bm1,m2=0 represents that no data connection exists between nodem1 and nodem2. Bm1,m2≠0 represents that data connection exists between nodem1 and nodem2.

Assuming that a size of K bit of data Data=[d1,d2,…,dk,…,dK] needs to be stored in the system. These data contain sensitive information Sensitive=[s1,s2,…,sL]. L is the size of the sensitive information, and L<K. The data block is represented as datan=[dn,1,dn,2,…,dn,kn]. The size of each data block is kn, n=1,2,…,N, N<M, ∑Nn=1kn=K. Each data block contains some sensitive information sensn=[sn,1,sn,2,…,sn,ln]. The size of the sensitive information is ln. The segmented data blocks are stored in different places, ln<L, ∑Nn=1ln=L.pn,m is a decision variable. When the nth data block is stored to the mth data node, pn,m=1, otherwise pn,m=0. So, pn,m∈{0,1}. The decision set is represented as P, P={p1,1,p1,2,…p1,m,p2,1,…,pn,m}.

Since data blocks of different sizes from the same data are placed on nodes in a distributed method, after one or some data nodes of the system are attacked by malicious attack, hackers cannot guess all the sensitive information in the whole data. Therefore, for a malicious attacker, the sensitive information of a single data node has no specific meaning, unless most of the data blocks containing the sensitive information are stored in several data nodes with close links. For example, data nodes are stored in adjacent data nodes, and hackers can easily retrieve other sensitive information by traversing it item by item. On the other hand, for ensuring the data security, the segmented data blocks are placed too far away that the user's normal data retrieval time will increase. Our goal is to ensure that even if a single data node is attacked by hackers, the specific locations of the other nodes are not detected.

Firstly, the security of stored data should be ensured. Therefore, the distributed dependency is introduced to define the security level of the system. The Pearson correlation coefficient represents the correlation between nodem1 and nodem2 [28].

ρm1,m2=cov(sensm1,sensm2)σm1σm2
(1)
where cov(sensi,sensj) is the covariance of sensi and sensj. Formula (1) can be further deduced to the following formula.

ρm1,m2=E[sensm1sensm2]−E[sensm1]E[sensm2]E[(sensm1)2]−(E[sensm2])2−−−−−−−−−−−−−−−−−−−−−−√E[(sensm1)2]−(E[sensm2])2−−−−−−−−−−−−−−−−−−−−−−√
(2)
where E[sensm1] is the expectation of data sequence sensm1, and 0≤|ρm1,m2|≤1. If ρm1,m2>0, sensm1 is positively correlated with sensm2. If ρm1,m2<0, sensm1 is negatively correlated with sensm2.

For ensuring the safety of data storage, if the correlation between sensm1 and sensj is high and the logical distance between these two data nodes, when one data node is malicious attacks, the probability that a hacker gains access to information on another data node will greatly increase. Therefore, data blocks with high correlation are as far apart as possible. Thus, the safety level between nodem1 and nodem2 can be defined as follows.

SLm1,m2=distancem1,m2|ρm1,m2|
(3)
where distancem1,m2 is the Euclidean distance between nodei and nodej.

distancem1,m2=∥nodem1−nodem2∥2−−−−−−−−−−−−−−−−√
(4)
When no data link exists between nodem1 and nodem2, distance=+∞. The higher the value of SLm1,m2, the more secure the storage of the data.

The safety level of the whole system is defined as the mean of the safety level of N(N−1)/2 data node pairs.

SL=∑m1M∑m2M(∑nNpn,m1)(∑nNpn,m2)2N(N−1)∥nodem1−nodem2∥2−−−−−−−−−−−−−−−−√|ρm1,m2(sensm1,sensm2)|
(5)
Assume that the data block datan is transferred to the nodem. For minimizing the transmission time, the shortest path between the starting point and nodem needs to be found. According to the reference [29], the shortest path between starting point and nodem can be obtained, and the data node number in the shortest path is set as Im. Then the minimum transmission time of all the data blocks can be represented as follows.

T=∑n=1N∑m=1Mpn,m∑i=1Im−1knBi,i+1
(6)
where Bi,i+1 is the bandwidth of each link of the shortest path. Then the retrieval time can be obtained as follow. The data blocks are restricted as follows.

∑n=1Nkn=K
(7)
In formula (7), the total size of all data blocks is equal to original data size.

The total cost of storing replicas of user data can be represented as follow.

Cost=∑n=1N∑m=1Mpricem⋅timen⋅kn⋅pn,m
(8)
where pricem is the storage price for the mth data node to store 1 GB of data per hour. timen refers to the storage time of the nth data block, and its unit is hours. kn is the size of the data.

The utilization of each node can be expressed as follows.

Um=stousedm+∑n=1Nkn⋅pn,mUtotalm
(9)
where stousedm is the used storage capacity of the mth data node. stototalm is the total storage capacity of the mth data node. The average utilization of nodes can be expressed as follows.

U¯¯¯¯=∑m=1M1MUm
(10)
Then load balancing degree is shown as follows.

lb=∑m=1M1M(Um−U¯¯¯¯)
(11)
When the value of lb is small, the load is balanced well in the system.

The NSGA-II algorithm is used for solving the MOP
Based on the formulas above, the problem can be regarded as a MOP. The MOP is shown as follows.

minPPf1=T=∑n=1N∑m=1Mpn,m∑i=1Im−1knBi,i+1
(12)
maxPPf2=SL−1=⎛⎝⎜∑m1M∑m2M(∑nNpn,m1)(∑nNpn,m2)2N(N−1)∥nodem1−nodem2∥2−−−−−−−−−−−−−−−−√|ρm1,m2(sensm1,sensm2)|⎞⎠⎟−1
(13)
minPPf3=Cost=∑n=1N∑m=1Mpricem⋅timen⋅kn⋅pn,m
(14)
minPPf4=lb=∑m=1M1M(Um−U¯¯¯¯)
(15)
s.t.∑n=1Nkn=K
(16)
pn,m∈{0,1}
(17)
stousedm+∑n=1Nkn⋅pn,m≤Utotalm
(18)
Um=stousedm+∑n=1Nkn⋅pn,mUtotalm
(19)
U¯¯¯¯=∑m=1M1MUm
(20)
The constraint in (16) guarantees that the total size of all data blocks is equal to initial data size. The constraint in (17) means for the nth data block and the mth data node, when the nth data block is stored to the mth data node, pn,m=1, otherwise pn,m=0. The constraint in (18) guarantees that the size of all data blocks stored to the mth data node is less than the remaining capacity of the node. The constraints in (19) and (20) are the calculation formulas of the utilization of each node and the average utilization of nodes. The MOP problem can be further denoted as follows.

{minPPF(p)=[f1(p),f2(p),f3(p),f4(p)]s.t.p∈X
(21)
where X is the solution set.

As for the MOP above, the Pareto optimal solution can be obtained. NSGA-II is an elitist multi-objective evolutionary algorithm based on non-dominant ranking [30]. NSGA-II is a classical method that can find an extension of a better solution and better converge near a true Pareto optimal front. The relevant algorithm is shown in Algorithm 1 in Sect. 4.

①
Chromosome coding and fitness function

In Fig. 2, because pn,m∈{0,1}, the binary encoding is selected as the coding method of this paper. The length of the chromosome code can be defined as MN. If the jth position of the chromosome is 1, the data block with the serial number ⌊j/M⌋+1 is transferred to the data node with the serial number jmodeM for storage. Because each data block is transferred into a data node and each data node can only store one data block at most, the number of places is ANM, that is, the size of the population is ANM.

Fig. 2
figure 2
The chromosome coding scheme

Full size image
②
The fast non-dominant sort

The steps of the fast non-dominant sort are described as follows. (1) The collection of all solutions is set as S, and the non-dominated solution collection is found and denoted as Y1. (2) S=S−Y1 [31], and then the non-dominated solution collection is found and denoted as Y2. (3) The second step is repeated until S is an empty set. Finally, the non-dominated solutions found each time are sorted as {Y1,Y2,…,Yn}.

③
The calculation of the crowding distance

For making the obtained solutions more uniform in the target space, the crowding degree nd is introduced. The crowding distance is calculated as the following orders. (1) The crowding distance of each solution is initialized to 0, cdr=0,r∈{1,2,3…MN}. (2) For each objective function fq,q∈Q,Q={1,2,3,4}, firstly, the individuals in the same rank are sorted according to the objective function. Next, the crowding degrees of the two boundaries are set to infinity. cd1=∞,cdMN=∞. Finally, the crowding degrees are updated as follows.

cdr=1Q∑q=1Qfq(r+1)−fq(r−1)fmaxq−fminq
(22)
where fmaxq is the maximum value of fq, fminq is the minimum value of fq. fq(r+1)−fq(r−1) is the difference between the values of the qth objective function of the (r+1)th individual and the (i−1)th individual.

④
The process of solving the MOP by NSGA-II

(1) By the coding method, the initial population z is generated. By using the binary tournament method [32], the individuals in z are compared for fast non-dominated ranking and crowding distance, and then the parent population Z is generated. (2) The progeny population V is obtained through crossover and mutation operations. The parent population Z and the progeny population V are merged into W. The crossover method is simulated binary crossover, and the crossover distribution index is set to 1. The single point mutation is adopted for mutation, and the mutation operator is set at 0.1. (3) The elite strategy is used to generate the next generation population. The individuals in W are quickly non-dominated ranked. The appropriate individuals are selected to form the next generation population. Then, the number of iterations in this paper is set to 500. Finally, the pareto optimal solution is obtained.

Task scheduling model based on the anomaly detection and the priorities of tasks and resources (AD-PTR)
Anomaly detection
In the MapReduce framework, the execution speeds of some nodes are slow. In this paper, these nodes are called straggling nodes [33]. The node processing capacity is the node performance. In a heterogeneous cluster, the node performance varies. The current performance of the node is restricted by the data processing capacity, the memory size, the bandwidth throughput rate, the CPU utilization, the maximum disk read and write speed and other factors. The amount of nodes is set as v. The nodes are represented as node1,node2,…,nodev. The performance vector of the ith nodes r={node1i,node2i,…,nodepi}. p is the number of the impact factors. nodepi is the impact factors.

In our proposed speculative execution scheme, firstly, the straggling nodes is found, and then the tasks are not transferred to the straggling nodes to be performed. In the following content, the steps of finding the straggling nodes are described.

Firstly, the historical record of all the performance impact factors of the node i in the last week is collected in the log information. The first historical record is denoted as Xi,1, and the n records are denoted in the following matrix. In the matrix, m is the amount of performance impact factors.

⎡⎣⎢⎢⎢⎢⎢⎢Xi,11Xi,21⋮Xi,n1Xi,12Xi,22⋮Xi,n2⋯⋯⋯Xi,1mXi,2m⋮Xi,nm⎤⎦⎥⎥⎥⎥⎥⎥
Next, the mean values of the n records of each impact factor in node i are obtained.

μi1=1n∑j=1j=nXi,j1
(23)
μi2=1n∑j=1j=nXi,j2
(24)
μim=1n∑j=1j=nXi,jm
(25)
Then, the variances of the n records of each impact factor in the node i are obtained.

(σi1)2=1n∑j=1j=n(Xi,j1−μi1)
(26)
(σi2)2=1n∑j=1j=n(Xi,j2−μi2)
(27)
(σim)2=1n∑j=1j=n(Xi,jm−μim)
(28)
Finally, according to the mean values, variances and the latest logging information, the state of the current is obtained.

Pi(n)=∏j=1mp[Xi,nj;μij,(σij)2]=∏j=1m12πσij−−−−√exp⎛⎝⎜⎜−(xi,nj−μij)22(σij)2⎞⎠⎟⎟
(29)
ε is the threshold value. If Pi(n)<ε, the ith node is marked as a straggling node and added to the straggling node list. Otherwise, the ith nodes are added to the normal node list. In this paper, for getting the optimal value of ε, the classic precision and recall rate are used to calculate the corresponding F-score. The relevant details are given in Algorithm 2 in Sect. 4.

Task scheduling based on the priorities of tasks and resources
In this paper, only meta-tasks are studied, and all tasks are assumed to be computational tasks. The task set is represented as T(n)={t1,t2,⋯,ti,⋯,tn},i∈[1,n]. ti is the ith task. In addition, the multidimensional properties of the task is represented as the one-dimensional vector ti=(tID,tuser,tauthority,turgency,tlen,tdemand). tID is the unique number of the task. tuser is the unique identity of the task creator. tauthority is the permission category of the task creator, and tauthority∈{A,B,C}. turgency is the urgency of the task and turgency∈{urgent,high,middle,ignored}. tlength is the computational cost of the task. The greater the tlength is, the greater the calculation of the task. tdemand represents the resource requirements of the task. The resource requirements include the computing power, the communication power and the storage power, which are, respectively, represented by tcomp, tcomm and tsto.

The resource set is represented as R(m)={r1,r2,⋯,rj,⋯rm},j∈[1,m]. rj is the jth resource. The different characteristics of the resource is denoted as the one-dimensional vector rj=(rID,rprovider,rservice,rcap,rcost). rID is the unique number of resources. rprovider is the identities of the resource providers. rservice is the name of the service provided by the resource. rcap={rcomp,rcomm,rsto} is the capacity of the resource, and contains the computing power, communication power and storage power of the resource. rcost={a,b,c} is the resource charging standard, and contains the unit service price in the task calculation, the bandwidth allocation and the storage allocation.

For reflecting the importance degree of the task, according to the multifaceted attributes of the task, the task is prioritized. The permission category of the task indicates whether the task can be prioritized. The urgency of the task expresses its own importance. The length of the task indicates the workload of the task. Therefore, by considering the user permission category, urgency and length of the task, the task priority is calculated.

To realize the multi-attribute priority calculation of the task, the related data are standardized in advance. If the indicators of different properties are summed directly, the comprehensive result cannot be reflected correctly. Therefore, the data property of the inverse indicator is changed to make the forces of all indicators syntactic with the evaluation scheme, and then the correct results can be obtained by summing. Dimensionless data processing mainly solves the comparability of data. Each index value is in the same quantity level and can be evaluated and analyzed comprehensively.

The normalization processing result for the jth attribute of the task ti is represented as Ni,j. The Ni,j is calculated as follows.

Ni,j=(ai,j−a¯¯¯j)/Oj
(30)
In formula (30), ai,j is the jth attribute of the task ti. a¯¯¯j is the average value of the jth attribute of all the tasks and is calculated as follows.

a¯¯¯j=1n∑i=1nai,j
(31)
Oj is the mean absolute deviation of the jth attribute of all the tasks and is obtained by the following formula.

Oj=1n∑i=1n|ai,j−a¯¯¯j|
(32)
The priority of the task ti is denoted as P(ti). tnorauthority, tnorurgency and tnorlen represent the normalization processing results of tauthority, turgency and tlen, respectively. The task priority is calculated by weighting.

P(ti)=αtnorauthority+βtnorurgency+γtnorlen
(33)
In formula (33), α,β,γ∈[0,1][0,1], and α+β+γ=1.

The task priority is calculated, and then the task is sorted. High-priority tasks are assigned to better resources, optimized resources are allocated, and the quality of service is guaranteed. The total service capacity is calculated as follows.

S(rj)=(rnorcomp)2+(rnorcomm)2+(rnorsto)2−−−−−−−−−−−−−−−−−−−−−−√
(34)
In formula (34), rnorcomp, rnorcomm and rnorstorage represent the normalization processing results of rcomp, rcomm and rsto, respectively.

When a single service executes a single task under no load, the expected execution time is defined. For the task ti and the resource rj, timeexei,j is the expected execution time of.

timeexei,j=tilen/rjcomp
(35)
In the formula above, tilen is the length of the task ti, rjcomp is the computation power of the resource rj. The amount of the resources is set as m, and the average expected execution time is denoted as time¯¯¯¯¯¯¯¯¯¯exei,j and calculated in the following formula.

time¯¯¯¯¯¯¯¯¯¯exei,j=1m∑j=1mtimeexei,j
(36)
The expected time of task scheduling is the expected time of the execution of the assigned resource. timei,j is the expected completion time of the task ti on the resource rj. The earliest available time of the resource rj is denoted as timeavaj. Then, the expected completion time can be expressed as follows.

timei,j=timeexei,j+timeavaj
(37)
The expected cost of the task is the cost of executing the task ti on the resource rj, and contains the costs of computation, communication and storage. The expected cost is denoted as Costi,j, and calculated as follows.

Costi,j=α2⋅rjcom⋅timeexei,j+β2⋅tjcomm+γ2⋅tjsto
(38)
In formula (38), α2, β2 and γ2 are the unit service prices of the task computation, bandwidth allocation and storage allocation, respectively. rjcom is the computation power of the resource j. ticomm and tisto are the communication bandwidth and storage space that the resource is allocated to the task.

The expected average cost of the task is the average cost of executing the task ti on all resources separately. The number of resources is set as m, and the expected average cost is expressed as follows.

Cost¯¯¯¯¯¯¯¯¯¯i=1m∑j=1mCosti,j
(39)
The completion time and cost of the task ti on the resource rj are balanced. Bali,j is used to represent the tradeoff.

Bali,j=k1⋅timei,jtime¯¯¯¯¯¯¯¯¯¯exei,j+k2⋅Costi,jCost¯¯¯¯¯¯¯¯¯¯i
(40)
In formula (40), k1 and k2 are the weighted factors, and k1+k2=1. The larger Bali,j is, the higher the time and cost of completing the task on the resource are. The relevant details and algorithms are described in Algorithm 3 in Sect. 4.

Privacy policy
In the privacy policy, the ability of individuals to control their private information is emphasized. The privacy protection mechanism, users formulate the access control policy for private information, is effective. Because the system behavior only changes with the change of policies, this is conducive to providing users with flexible and adaptive privacy information control interface. Policies are embodied as authorization rules, and the execution mechanism of policies can be realized through rule-based systems. Users make their own access control policies for privacy information, and then the privacy protection will of them can be realized.

In this paper, the information granularity of the privacy policy is analyzed. The privacy policy can not only control whether the requester can access the privacy object, but also control the information access granularity of the privacy object when the request is allowed. So, the fuzzy access control of privacy information can be realized. The fuzzy privacy policy can greatly meet the diversified and personalized needs of users for privacy protection.

As shown in Fig. 3, the owner formulates three privacy policies for the same privacy object (PO), and implements hierarchical control of information exposure for different requesters. In Fig. 3, the requester 1 satisfies the constraints of the privacy policy 1, so the most accurate information can be returned to the requester 1, while the requester 3 gets the most ambiguous description of the privacy information. In general, when the inclusion relation exists in the information granularity, when the privacy object can be represented as information description with different accuracy through a certain binary relation, the privacy policy can be used to control the granularity of the information exposure.

Fig. 3
figure 3
The information granularity control of privacy policy

Full size image
The algorithm of data placement optimization and task scheduling
The data placement algorithm based on NSGA-II in geo-distributed cloud is described in the following pseuocode. For the pseucode, the input is the decision variable pn,m. The algorithm is executed as follows. First, the safety level of the whole system is calculated (Algorithm 1 Line 1–6). Next, the parameter indicators of the method are calculated (Algorithm 1 Line 7–13). Finally, according to the objective functions and the constraints, the MOP is solved by the NSGA-II. Then the optimal scheme is obtained according to the set of decision variables P.

figure e
In Algorithm 1, two for-loops exist in line 1–6, the time expense of these lines is O(n2), the time expense of line 7–13 is O(n2). As for the NSGA-II, the population size is size=MN and the number of the objective functions is four, so the time expense of the NSGA-II is O(4(size)2) [34]. To sum up, for the proposed algorithm, the time complexity is calculated as O(n2).

The threshold selection algorithm of the node state is described in the following pseucode. The calculation flow is shown as follows. First, in the cluster, the log information is obtained at a fixed time each day for the first n days of node i. The log information is denoted as {(Xi,1,yi,1),(Xi,1,yi,1),⋯,(Xi,n,yi,n)}. Xi,n is the nth record information. yi,n is the nth state information. If yi,n=1, the ith node is the straggling node. If yi,n=0, the ith node is normal node (Algorithm 2 Line 3–10). Next, according to formula (29) and the value of Xi,n, the current state is obtained (Algorithm 2 Line 11–18). ε with the highest value of F-score is obtained and taken as the current threshold value (Algorithm 2 Line 18–22).

figure f
In Algorithm 2, three for-loops exist, and two of them are contained in one for-loop side by side. The time expense of each for-loops is O(n). So, as for the threshold selection for the node state, the time complexity is O(n(n+n))=O(n2).

When the straggling node is found, the task scheduling algorithm will be called, and no new tasks will be assigned to the straggling node.

First, the specified attributes of the task are normalized, and the priority of the task is calculated (Algorithm 3 Line 1–2). According to the priority, tasks are sorted and grouped (Algorithm 3 Line 3). The total service capacity of resources is calculated and grouped (Algorithm 3 Line 4–5). The scheduling constraint relationship between task group and resource group is established (Algorithm 3 Line 6). Second, the time expection and the expected cost of each task on different resources in the associated resource group are calculated, respectively, and then the average time expection and the average cost of each task are obtained (Algorithm 3 Line 7–13). Third, the time–cost tradeoff value of each task on different resources in the associated resource group is calculated (Algorithm 3 Line 14–20). Next, according to the time–cost tradeoff values from small to large, the mapping relationships of tasks and resources from good to bad are obtained (Algorithm 3 Line 21–28). Finally, from the task queue, the task is allocated to the optimal normal node in the normal node table.

figure g
In Algorithm 3, the time expenses of the line 1 and 2 are O(n). The amount of tasks is set as n. For the line 3, the time expense is calculated as O(nlogn). The time expense of the line 4 is O(m). For the line 5, the time expense is O(mlogm). The time expenses of the line 6–14 are O(m⋅n). In line 15–27, two for-loops exist, and the time expenses are O(m⋅n). Hence, the time complexity of the task scheduling algorithm is obtained: O(n+n⋅logn+m+m⋅logm+9⋅m⋅n+2⋅m⋅n)=O(n2).

Experiments
Experimental environment
Spark and Hadoop are the platforms of the experitment [35]. MyEclipse is taken as the compiler. The version numbers of Apache Spark and Hadoop are 2.4.5 and 2.7.0, respectively. The clusters of local servers and the central cloud compose the experiment framework. In local clusters, 11 virtual machines exist. In addition, some virtual machines are tenanted from the central cloud. Figure 4 shows the experiment environment. From Fig. 4 the local server and the central cloud are from different geographic locations. In each cluster, the master and slave nodes are in a one-to-many relationship. The central controller is used to perform the data placement operation. The configurations of the servers and the tenanted instances are listed in Tables 1 and 2.

Fig. 4
figure 4
The experimental setup

Full size image
Table 1 The hardware configurations of the servers
Full size table
Table 2 The configurations of the tenanted instances
Full size table
In the geo-distributed cloud environment, for evaluating the effectiveness of the TSCL method, many related works [36,37,38] take the online social network Facebook as the dataset. In the experiment of the data placement, the dataset is obtained from Facebook, and the time of the dataset is September 20, 2019 [39]. More than 1000 data blocks ranging in [16 MB, 128 MB] are contained in this data set. In each experiment, some data blocks are chosen to be transmitted to the whole system.

For verifying the performance of the AD-PTR algorithm, the author of [40] use PageRank and Inverted-Index. In the experiment of [40], the Google undirected graph data and the Wikipedia data are taken as the test data. The PageRank and the Inverted-Index are used for obtaining the algorithm performances. In Page Rank, in the Google undirected graph data, the range of vertices is [82,424, 284,581], and the range of the edge numbers is [139,081, 316,312] [41, 42]. For the Inverted-Index, 2–128 GB of the Wikipedia data is used, and the range of the data size is [2 GB,128 GB] [43].

For placing the data, the retrieval time, the data node utilization, the total cost and the safety level are the performance metrics.

The total retrieval time of a file retrieval operation is denoted as ti. In Sect. 3, the amount of the data block is N. Assuming that the amount of the file retrieval operations is m. Then the retrieval time can be expressed as follow.

t=∑i=1mti/Nm
(41)
In the data migration scheme, for the selection of N data blocks, when multiple files are stored, the actual used node ratio among M data nodes is checked, and the minimum value of this ratio is N/M.

According to the definition of the total cost in formula (8), the total cost of storing user data can be obtained. According to the definition of the safety level in formula (5), the safety level of the whole system is calculated.

As for the task scheduling method based on anomaly detection, the load balance degree, the ratio of the number of processors to the total load of the task cluster and the percentage improvement in completion time are taken as the performance metrics. The relative standard deviation is used as the load balance degree and expressed as follows.

LBD=1M∑i=1M(li−l¯)−−−−−−−−−−√l¯
(42)
where li is the workload of data node nodei. l¯ is the mean of the load values. M is the amount of the data nodes. The smaller LBD is, the better the load balancing of the nodes.

According to the reference [44], the ratio of the amount of processors to the task cluster load is expressed as M/L, where M is the number of the virtual machine nodes. L is the total workload. M/L represents the number of processors required per unit load. The smaller M/L is, the higher the scheduling performance of the algorithm is.

For n different tasks assigned to m different nodes, the maximum processing time can be expressed as follow [45].

tmax=Maxj=1M(tmaxj)
(43)
The optimal time overhead of the total task is the sum of the total instruction length of N tasks divided by the execution speed of M virtual machine instructions and can be formulated as follows.

toptimal=∑ni=1MIi/∑mj=1MIPSj
(44)
where MIi, Million Instructions, refers to the instruction length of each task, MIPSj, Million Instructions Per Second, refers to the execution speed per virtual machine.

Then the percentage of difference between maximum processing time and optimal completion time can be called the percentage improvement in completion time.

δ=(tmax−toptimal)/toptimal
(45)
In order to verify the performance of our proposed data placement method, IDP [10], GA-DPSO [7] and H2DP [46] are taken as the benchmark algorithms. The references [7, 10, 46] have the same optimized parameters as our paper. The author of [10] optimizes a number of other performance metrics while preserve the privacy of users. The author of [7] considers the impact of transmission delay. The author of [46] considers the heterogeneity of servers. Therefore, it is meaningful for the TSCL algorithm to compare with algorithms of references [7, 10, 46].

In the comparative experiment of fault-tolerant scheduling, our proposed fault-tolerant scheduling algorithm is compared with NLS [21], CHEFT [47] and DRAM [48]. The references [21, 47, 48] have similar ideas to our proposed algorithm. The author of [21] also considers the priorities of tasks. In references [47, 48], the stragglers are considered, and the author of [48] also considers the utilization of resources.

Results
Data placement algorithm based on the TSCL strategy
In the experiment of data migration scheme, the amount of nodes is 100, and the amount of data blocks is 10. For each data, the data size is 100 MB. The experiment is conducted eight times, 100 raw data are stored each time, and then the average time to retrieve 100 raw data is calculated (Fig. 5).

Fig. 5
figure 5
The retrieval time varies from the number of files

Full size image
The experiment above shows that the retrieval efficiency of the TSCL has obvious advantage. The proposed method and GA-DPSO have better stability than IDP and H2DP (Fig. 6).

Fig. 6
figure 6
The actual used node ratio varies from the number of files

Full size image
As the amount of original data stored increases, the utilization changes. As the amount of original data stored increases, the utilization rates of data nodes of the proposed algorithm and IDP are significantly improved. The higher the actual used node ratio, the higher degree of data dispersion. When data blocks are distributed in the system, the security and load balance are guaranteed (Fig. 7).

Fig. 7
figure 7
The safety level varies from the number of files

Full size image
As shown in this figure, with the increase in the amount of original data, some storage nodes are fully loaded. Because of the smaller set of optional nodes, the safety level of data is affected. For the data security, the TSCL algorithm has the best stability. When the number of files is 800, the safety levels of the TSCL algorithm, IDP, GA-DPSO and H2DP are approximately 7.0, 5.5, 4.8 and 6.0, respectively. The safety level of the TSCL is 27.3% higher than that of the IDP, 45.8% higher than that of the GA-DPSO algorithm and 16.7% higher than that of the H2DP algorithm. Because the safety level is used as one of the optimized indexes for the MOP, compared with the benchmark algorithms, the TSCL data placement algorithm has advantages in data security.

In Fig. 8, the impact of changes in the security level on the retrieval time is depicted. In this experiment, the amount of data nodes is set as 100, and the amount of data blocks is set as 10. The retrieval time of the TSCL algorithm increases, when the safety level is improved. Since the improvement of the security performance directly increases the distance between nodes storing data blocks, the length of retrieval path increases.

Fig. 8
figure 8
The retrieval time varies from the safety level

Full size image
Figure 9 depicts the impact of changes in data nodes on the retrieval time. The number of the data blocks is set as 10. From the result, the retrieval time increases with the increase in the number of data nodes. Because the larger the network topology, the greater the physical distance between nodes storing data blocks. The TSCL algorithm shows the slower growth and the better performance than the other methods.

Fig. 9
figure 9
The retrieval time changes with the number of data nodes

Full size image
Figure 10 describes the impact of the change in the number of data blocks on the retrieval time. The number of data nodes is set as 200. When the amount of data blocks increases to a certain value, for the four algorithms, the retrieval time is similar. Because the number of data blocks increases, these data blocks are evenly distributed in every part of the topology. Predictably, the greater the difference between the amounts of data blocks and nodes, the better the TSCL will be. So, the TSCL algorithm has better stability than the benchmark algorithms.

Fig. 10
figure 10
The retrieval time varies from the data blocks

Full size image
From the experimental result of Fig. 11, the TSCL algorithm gets better operating cost than the other three benchmark algorithms. When the experiment is conducted for 15 days, the cost of the proposed algorithm, IDP, GA-DPSO and H2DP is 21, 72, 75 and 36. The overhead of the TSCL algorithm is the lowest.

Fig. 11
figure 11
The total cost varies from the time

Full size image
Proposed task scheduling algorithm
According to the importance of different attributes of tasks, the α, β and γ are given as 0.5, 0.3 and 0.2.

From Fig. 12, when the number of tasks is set as 400, 800, 1200 or 1600, among all the experimental task scheduling algorithms, the load balancing degree of the AD-PTR algorithm is the smallest. In addition to resource selection according to the time overhead, the proposed task scheduling algorithm also groups tasks and resources, and sets the scheduling constraint association between task group and resource group. In our proposed task scheduling scheme, while resource allocation is optimized, tasks are more evenly scheduled to different resources.

Fig. 12
figure 12
The load balancing degree varies from the number of tasks when k1=0.5 and k2=0.5

Full size image
From Fig. 13, the AD-PTR algorithm performs better than the other three algorithms in the aspect of service cost, because in the process of task scheduling, the cost is taken as the basis of task scheduling efficiency in the proposed algorithm. In addition, in Fig. 14, by adjusting the weight of the time overhead and cost, increasing the proportion of the cost k2 and decreasing the proportion of completion time k2, the total cost of the AD-PTR is further reduced.

Fig. 13
figure 13
The total service cost varies from the number of tasks when k1=0.5 and k2=0.5

Full size image
Fig. 14
figure 14
The total service cost of the AD-PTR algorithm when k2=0.5 or k2=0.6

Full size image
For Fig. 15, when the task quantity in the experiment is set to any of the six conditions, compared with the other algorithms, the value of M/L of the AD-PTR is lower.

Fig. 15
figure 15
The M/U of different algorithms varies from the task quantity

Full size image
In Fig. 16, when the task amount is set as 500, 1000, 1500, 2000 or 2500, the percentage improvement in the completion time of the AD-PTR is smaller than that of the other algorithms. Therefore, the task completion time of the AD-PTR algorithm is closest to the optimal time overhead. In addition, as can be seen intuitively from Table 3, compared with the benchmark algorithms, the AD-PTR algorithm has the shortest completion time.

Fig. 16
figure 16
The percentage improvement in completion time varies from the number of tasks

Full size image
Table 3 The latest completion time under different number of tasks
Full size table
Conclusion and Future Work
On the one hand, considering the data storage security and other parameter indicators, the data placement problem is regarded as a MOP problem. Then, by solving the MOP problem, the specific data placement scheme is obtained. On the other hand, before tasks are scheduled, the straggling nodes are detected. Then, resources are no longer assigned to the straggling nodes. Next, considering the multiple attributes of tasks and resources, the priority of tasks and resources are calculated according to the task scheduling model. Finally, the mapping between tasks and resources is established, and tasks are scheduled according to this mapping.

For evaluating the performance of these two proposed algorithms, experiments are conducted. In terms of the data placement, the retrieval time of the TSCL algorithm is the shortest among all of the algorithm in the experiment, and the TSCL algorithm has the lowest cost. When the number of files is set as 800, the safety level of the proposed algorithm is 7.0, which is 27.3% higher than that of the IDP algorithm, 45.8% higher than that of the GA-DPSO algorithm and 16.7% higher than that of the H2DP algorithm. In terms of the task scheduling, the total overhead of the AD-PTR algorithm is the lowest. The values of load balance degree and M/L of the AD-PTR are the smallest, which implies that the AD-PTR algorithm can schedule tasks with fewer resources, and the load on multiple nodes is more balanced. The percentage improvement in the time overhead of the AD-PTR is the lowest, which implies that the time overhead of the AD-PTR is closest to the optimal time and is the shortest.

Admittedly, the proposed models can still be further improved. In the data placement model, more performance metrics can be used as optimized parameters and more efficient solutions can be developed to obtain the optimal scheme of the MOP. In the task scheduling method, additional experiments can be done to obtain better weighting coefficients.