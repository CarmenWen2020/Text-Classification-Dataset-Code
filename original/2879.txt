Graph learning, such as node classification, is typically carried out in a closed-world setting. A number of nodes are labeled, and the learning goal is to correctly classify remaining (unlabeled) nodes into classes, represented by the labeled nodes. In reality, due to limited labeling capability or dynamic evolving nature of networks, some nodes in the networks may not belong to any existing/seen classes and therefore cannot be correctly classified by closed-world learning algorithms. In this paper, we propose a new open-world graph learning paradigm, where the learning goal is to correctly classify nodes belonging to labeled classes into correct categories and also classify nodes not belonging to labeled classes to an unseen class. Open-world graph learning has three major challenges: (1) Graphs do not have features to represent nodes for learning; (2) unseen class nodes do not have labels and may exist in an arbitrary form different from labeled classes; and (3) graph learning should differentiate whether a node belongs to an existing/seen class or an unseen class. To tackle the challenges, we propose an uncertain node representation learning principle to use multiple versions of node feature representation to test a classifier’s response on a node, through which we can differentiate whether a node belongs to the unseen class. Technical wise, we propose constrained variational graph autoencoder, using label loss and class uncertainty loss constraints, to ensure that node representation learning is sensitive to the unseen class. As a result, node embedding features are denoted by distributions, instead of deterministic feature vectors. In order to test the certainty of a node belonging to seen classes, a sampling process is proposed to generate multiple versions of feature vectors to represent each node, using automatic thresholding to reject nodes not belonging to seen classes as unseen class nodes. Experiments, using graph convolutional networks and graph attention networks on four real-world networks, demonstrate the algorithm performance. Case studies and ablation analysis also show the advantage of the uncertain representation learning and automatic threshold selection for open-world graph learning.

Access provided by University of Auckland Library

Introduction
Networks/graphs are convenient tools to model interactions and interdependencies between large-scale data. Graph learning, such as node classificationFootnote1, attempts to categorize nodes of graphs into several groups. Such learning tasks are fundamental, but challenging, and have received continuous attention in the research field. Many research efforts have been made to develop reliable and efficient algorithms for different types of node classification tasks. However, existing methods mainly carry out the learning in a “closed-world” setting, where classes in the test set must be consistent to the classes used in the training set. In other words, nodes in the test data must belong to classes already seen in the training set. As a result, when a new/unseen class node appears in the test set, classifiers cannot detect the new/unseen class and will erroneously classify the node to seen/learned classes in the training data.

In reality, data collection and labeling may be continuously evolving. New trends emerge constantly and a model that cannot detect these new/unseen trends can hardly work well in a dynamic environment. This problem/phenomenon is referred to as the open-world classification or open classification problem [1]. The new “open-world” learning (OWL) [2,3,4] paradigm is to not only recognize objects belonging to the classes already seen/learned before, but also detect new class samples which are previously unseen.

Several approaches, such as one-class SVM [5], can be adjusted to address open-world learning by treating all seen classes as the positive class, but they cannot differentiate instances in seen classes and often have poor performance to find unseen class, because no negative data are used. Alternatively, a similar problem called covariate shift [6] has also been studied in social media text classification, where covariate shift means that training data are not fully representative of the test data. To address the problem, a center-based similarity (CBS) space learning method [6] firstly computes a center for each class and converts a document to a vector of similarities to the center. The transformed data are then used to build a binary classifier for each class.

To date, open-world learning has already attracted many interests from natural language processing (NLP) [1, 2] and computer vision fields [7,8,9]. In NLP, Shu et al. [1] proposed the solution to open-world learning by setting thresholds before the sigmoid function to reject data from the unseen class. In computer vision, Scheirer et al. [7] studied the problem of recognizing unseen images that are not in the training data by reducing the half-space of a binary SVM classifier with a positive region. However, to the best of our knowledge, the open-world classification problem has not been previously investigated in graph structure data and graph learning tasks.

Given a graph consisting of seen class and unseen class nodes, the objective of open-world graph learning is to build a classifier to classify nodes belonging to seen classes into correct categories and also detect nodes not belonging to any seen class as unseen class. An example of open-world learning for graph node classification is illustrated in Fig. 1.

Fig. 1
figure 1
An example of open-world learning for network node classification. Nodes are either labeled or unlabeled. Given a graph with some labeled nodes and unlabeled nodes (left panel), open-world graph learning aims to learn a classifier to classify unlabeled nodes belonging to seen classes into its own class and also detects unlabeled nodes not belonging to any seen class as unseen class nodes (denoted by green-colored nodes in the right panel)

Full size image
Currently, existing solutions to open-world learning are mainly focused on documents or images and cannot be directly applied to graph structured data and graph learning tasks because they cannot model graph structural information, which is the core of node classification.

The challenge of graph learning is that graphs have node content and structure information where nodes are connected with edges representing their relations. Furthermore, existing solutions to node classification task are built on the closed-world assumption, in which the classes appeared in the testing data must have appeared in training. For example, the basic idea of graph convolutional networks (GCNs) is to develop a convolutional layer to exploit the graph structure information and use a classification loss function to guide the classification task. However, they directly use softmax as the final output layer, which does not have the rejection capability to unseen class nodes because the prediction probability of each class is normalized across all training/seen classes. In addition, in representation learning level, most existing graph learning methods employ feature engineering or deep learning to extract feature vectors. However, these models can only generate deterministic mappings to capture latent features of nodes. A major limitation of them is their inability to represent uncertainty caused by incomplete or finite available data.

In this paper, we propose to study open-world learning for graph data. Considering the complicated graph data structure and the node classification task, we summarize the main challenges as follows:

Challenge 1 How to design an end-to-end framework for open-world graph learning in graphs where the unseen class has no labeled samples, and may exist in an arbitrary form different from seen classes. Existing graph neural networks (GNNs) are typical built based on closed-world assumption and cannot detect unseen class.

Challenge 2 How can we model the uncertainty of node representations and promote robustness in graphs. Many existing GNN-based approaches only generate deterministic mappings to capture latent features of nodes.

Fig. 2
figure 2
A visualization of classification probability on seen (blue) and unseen (orange) class test instances for Cora dataset. The x-axis denotes the index of test instances (first 500 instances belong to seen classes and the last 100 instances belong to unseen class). The y-axis denotes the maximum probability output of each instance through the softmax classifier. a denotes the classification probabilities using only label loss, and b denotes the classification probabilities combining both label loss and class uncertainty loss

Full size image
To overcome the above challenges, we propose a novel open-world graph leaning paradigm (OpenWGL) for the node classification task. For Challenge 1, we employ two loss constraints (a label loss and a class uncertainty loss) to ensure that the node representation learning is sensitive to unseen class and assist in our model to differentiate whether a node belongs to an existing/seen class or the unseen class. We visualize a testing dataset in our experiment in Fig. 2, which can illustrate the effectiveness of our method. In Fig. 2a, we only use the label loss (the cross-entropy loss), which has a good performance on existing/seen class nodes, but unseen class nodes cannot be differentiated and will be classified to seen classes randomly. In Fig. 2b, we introduce a class uncertainty loss constraint, which can reduce the probability of unseen class nodes being classified as the seen class, and therefore help detect unseen class nodes without reducing the classification performance for nodes in seen classes. For Challenge 2, instead of learning a deterministic node feature vector, we utilize a graph variational autoencoder module to learn a latent distribution to represent each node. During the classification phase, a novel sampling process is used to generate multiple versions of feature vectors to test the certainty of a node belonging to seen classes, and automatically determine a threshold to reject nodes not belonging to seen classes as unseen class nodes.

Our contributions can be summarized as follows:

We formulate a new open-world learning problem for graph data and present a novel deep learning model OpenWGL as a solution.

We propose an uncertain node representation learning approach, by using label loss and class uncertainty loss to constrain variational graph autoencoder to learn node representation sensitive to unseen class.

We propose to use sampling process to test the certainty of a node belonging to seen classes, and automatically determine a threshold to reject nodes not belonging to seen classes as unseen class nodes.

Experiments on benchmark graph datasets demonstrate that our approach outperforms the baseline methods.

Related work
This work is closely related to open-world learning, emerging class and outlier detection, and graph neural networks, which are briefly reviewed in the following.

Open-world learning
Open-world learning aims to recognize the classes the learner has seen/learned before and also detect a new class it has never seen before. There are some early explorations of open-world learning. Scholkopf et al. [5] employ the one-class SVM as the classifier, which shows poor performance since no negative data is used. Fei and Liu [6] propose a center-based similarity (CBS) space learning method, which first computes a center for each class and converts each document to a vector of similarities to the center. The transformed data are then used to build a binary classifier for each class. Fei et al. [3] then extend their work by adding the capability of incrementally or cumulatively learning new classes.

Recently, open-world learning has been studied in natural language processing [1, 2] and computer vision (where it is called open-set recognition) [7,8,9]. In NLP, Shu et al. [1] propose the deep learning solution to open-world learning by setting thresholds before the sigmoid function to reject unseen classes. Xu et al. [2] propose a new open-world learning model based on meta-learning, which allows new classes to be added or deleted with no need for model re-training. In computer vision, Scheirer et al. [7] study the problem of recognizing unseen images that are not in the training data by reducing the half-space of a binary SVM classifier with a positive region. In [8] and [9], Scheirer et al. utilize the probability threshold to detect new classes, while their models are weak because of lacking prior knowledge.

Most existing open-world learning approaches are primarily focused on NLP and CV domains and cannot model graph structural data. In our research [10], we proposed to advance the open-world learning principle to graph data and designed a graph learning framework to classify network nodes in an open-world setting.

Emerging class and outlier detection
Our research is also related to emerging/new class detection in supervised learning, such as stream data mining [11, 12] and multi-instance learning [13], and outlier detection [14].

In supervised learning, instances are assumed to belong to at least one of the predefined classes, and a classifier is trained to learn discriminative patterns to separate samples into known classes. In reality, all data patterns may not be known when the data are collected, or new classes may emerge over time. When a class is unknown or unavailable at the time of training a classifier, in the test stage, an ideal classifier is expected to be able to detect the emerging/new class [15]. A common solution of detecting new class samples is to use a decision threshold to give a confidence score [16,17,18], including multilayer neural network [19] to increase the threshold, and samples with low confidence below threshold are recognized as the new class. Unfortunately, as we show in Fig. 2, simply increasing the threshold will make existing class samples being misclassified.

Outlier detection, on the other hand, aims to detect data instances which abnormally deviate from the underlying data [20]. Akoglu et al.[21] provide a comprehensive overview of graph-based techniques for anomaly, event, and fraud detection, as well as their use for post-analysis and sense-making in explaining the detected abnormalities. Some distance-based outlier detection methods such as one-class SVM have been proposed, in which the normal data domain is obtained by finding a hyper-sphere enclosing the normal data samples [5, 22]. For all methods, there is a trade-off between the number of true outliers and false outliers (samples being detected as outliers but come from the same distribution as the training data)[15]. A recent proposed method called StrOUD utilizes transduction and statistical tests to measure the fitness of cluster structures[23]. A recent method [14] proposes to detect outliers from data stream, but new class detection by outliers is not addressed.

In summary, our research not only advances the emerging (new) class detection to networked data settings, but also proposes a new way of automatically determining threshold for open-world learning.

Graph neural networks
Graph neural networks (GNNs), introduced in [24] and [25] as a generalization of recursive neural networks to directly deal with a more general class of graphs, e.g., cyclic, directed and undirected graphs, are a powerful tool for machine learning on graphs. GNNs have attracted attention all around the world, which are designed to use deep learning architectures on graph-structured data [26,27,28]. Many solutions are proposed to generalize well-established neural network models that work on regular grid structure to deal with graphs with arbitrary structures [29,30,31].

Among these methods, the most classic model is graph convolutional network (GCN), which is a deep convolutional learning paradigm for graph-structured data integrating local node features and graph topology structure in convolutional layers [32]. GraphSAGE [33] is a variant of GCN which designs different aggregation methods for feature extraction. GAT [34] improves GCN by leveraging attention mechanism to aggregate features from the neighbors of a node with discrimination. Although GCNs have shown great performance in graph-structured data for semi-supervised learning tasks such as node classification, the variational graph autoencoder (VGAE) [35] extends it to unsupervised scenarios. Specifically, VGAE integrates GCN into the variational encoder framework [36] by using a graph convolutional encoder and a simple inner product decoder.

For existing GCN-based graph learning models, they are built on the closed-world assumption, in which the classes appeared in the test data must have shown in training. In this paper, We employ two loss constrains to ensure that the node representation learning is sensitive to unseen class and assist in our model differentiating whether a node may belong to an existing/seen class or an unseen class.

To the best of our knowledge, the open-world learning problem has not been previously investigated in graph structure data and graph learning tasks. We are the first to study the open-world graph learning and propose an novel uncertain node representation learning approach, based on a variant of GCN (i.e., variational graph autoencoder networks) to differentiate whether a node belongs to an existing (seen) class or an unseen class.

Fig. 3
figure 3
The overall architecture of the proposed open-world graph learning (OpenWGL) model for unseen class node classification. The input consists of a graph with labeled and unlabeled nodes. The learning objective of OpenWGL, defined by Eq. (12), is constrained by \textcircled1 label loss (LL) defined by Eq. (13), \textcircled2 class uncertainty loss (LC) defined by Eq. (14), and \textcircled3 the KL divergence loss and network reconstruction loss (LS) defined by Eq. (11). As a result, OpenWGL can learn uncertain node representation sensitive to the class labels and unseen class. More specifically, OpenWGL first uses uncertain node representation learning to generate a latent distribution of each node, which consists of a graph encoder model and a graph decoder model. After that, a sampling process is employed to the latent distribution to learn solutions to an objective function which combines the three loss constraints (structure loss, label loss, and class uncertainty loss). More details are given in Sect. 4

Full size image
Problem definition and overall framework
This section defines the problem to be addressed in our paper and then presents our overall framework for the problem.

Problem statement
Node classification on graphs In this paper, we focus on node classification on graphs. A graph is represented as G=(V,E,X,Y), where V={vi}i=1,…,N is a vertex set representing nodes in a graph, and ei,j=(vi,vj)∈E is an edge indicating the relationship between two nodes. The topological structure of a graph G can be represented by an adjacency matrix A, where Ai,j=1 if (vi,vj)∈E; otherwise, Ai,j=0. xi∈X indicates content features associated with each node vi. Y∈RN×C is a label matrix of G, where N is the number of nodes in G and C is the number of node categories (classes) already known/seen. If a node vi∈V is associated with label l , Yl(i)=1; otherwise, Yl(i)=0.

Open-world graph learning Given a graph G=(V,E,X,Y), X=Xtrain⋃Xtest, where Xtrain denotes training data (labeled nodes) and Xtest denotes testing nodes (unlabeled nodes). Assume Xtest=S⋃U, where S are the set of nodes belonging to seen classes already appeared in Xtrain and U are the set of nodes not belonging to any seen class (i.e., unseen class nodes). Open-World Learning on Graphs aims to learn a (C+1)-class classifier model, f(Xtest)↦Y, (Y∈{1,⋯,C,rejection}) to classify each test node S to one of the training/seen classes in Y and reject U to indicate that it does not belong to any training/seen class (i.e., it belongs to the unseen class).

Overall framework
In order to learn a classifier for open-world graph learning, we propose an uncertain node representation learning approach called constrained variational graph autoencoder network to classify each seen node to its accurate category and reject the unseen nodes. Our framework for open-world graph learning, as shown in Fig. 3, mainly consists of the following two components:

Node uncertainty representation learning Most GCN models generate deterministic mappings to capture latent features of nodes. A major limitation of these models is their inability to represent uncertainty caused by incomplete or finite available data. In order to learn a better representation of each node, we employ a variational graph autoencoder network to obtain a latent distribution of each node, which enables to represent uncertainty and promote robustness.

Open-world classifier learning In order to classify seen class nodes to their own groups and detect unseen class nodes, we introduce two constraints, label loss and class uncertainty loss, to differentiate whether a node belongs to an existing class or an unseen class.

Open-world classification and rejection To perform inference during the testing phase (i.e., perform classification or rejection of an example), we propose a novel sampling process to generate multiple versions of feature vectors to test the certainty of a node belonging to seen classes and automatically determine a threshold to reject nodes not belonging to seen classes as unseen nodes. Our inference framework is given in Fig. 4 with detailed discussion given in Sect. 4.2.

Fig. 4
figure 4
The classification and rejection process (assuming seen class set has four classes). For nodes in the testing set, node uncertainty representation learning generates M different versions of feature vectors for each node by a sampling process. The M different representations are fed into a softmax layer to obtain M probability outputs Si. The probabilities of each class are averaged to obtain a vector si,a, and the largest average is denoted by max(si,a). Finally, Eq. (15) is used to decide whether a node belongs to the seen or unseen classes

Full size image
Methodology
Node uncertainty representation learning
In order to encode latent feature information of each node and obtain an effective representation of uncertainty, we employ variational graph autoencoder network (VGAE) to generate a latent distribution based on extracted node features. This allows our method to leverage uncertainty for robust representation learning.

Graph encoder model Given a graph G=(X,A), in order to represent both node content X and graph structure A in a unified framework, our approach firstly utilizes two-layer GNNs to map the feature matrix. Several classical GNNs, such as GCN [32] and GAT [34], are tested as the backbone of the two-layer GNNs. Given the input feature matrix X and adjacency matrix A, the first GCN layer generates a lower-dimensional feature matrix, which is defined as follows:

Z(1)=GNN(X,A).
(1)
For the second-layer GNN model, instead of generating a deterministic representation, we assume that the output Z is continuous and follows a multivariate Gaussian distribution. Hence, we follow an inference model proposed by [35]:

q(Z|X,A)=∏i=1Nq(zzi|X,A),
(2)
q(zzi|X,A)=N(zzi|μμi,diag(σσ2i)).
(3)
Here, μμ=GNNμ(X,A) is the matrix of mean vectors μi and σσ is the standard variance matrix of the distribution, logσσ=GNNσ(X,A). Then, we can calculate Z using a parameterization trick:

Z=μμ+σσ⋅ζ,ζ∼N(00,II),
(4)
where 00 is a vector of zeros and II is the identity matrix. By making use of the latent variable Z, our model is able to capture complex noisy patterns in the data.

For each layer of GNNs, the calculation process is as follows:

(1) Graph convolutional networks The lth GCN layer inputs a feature matrix Zl∈Rc×d(l) and outputs a higher-order feature matrix Zl+1∈Rc×d(l+1), which can be written as a nonlinear function:

Z0Zl+1=X,=σ(D−12AD−12ZlWl),
(5)
where the degree matrix Dij=∑jAij is a diagonal matrix, Wl∈Rd(l)×d(l+1) is the transformation matrix for the lth layer and σ(⋅) is a nonlinear activation function, which is acted by ReLU in our experiments.

(2) Graph attention networks The lth GAT layer with single head inputs a feature matrix Zl∈Rc×d(l) and apply a shared linear transformation, parametrized by a weight matrix Wl∈Rd(l+1)×d(l), to each node. Then, a shared attention mechanism is leveraged to compute attention coefficients of the pairs of connected nodes:

elij=al(WlZli,WlZlj)=LeakyRelu(al[WlZli||WlZlj]T),
(6)
where al(⋅,⋅):Rd(l+1)×Rd(l+1)→R is a single-layer feed-forward neural network which is parametrized by a weight vector al∈R2d(l+1), and applying the LeakyRelu nonlinearity (with negative input slope α=0.2). Then, attention coefficients are normalized across all choices of j using the softmax function:

ααlij=softmaxj(eij)=exp(elij)∑k∈Niexp(elik),
(7)
where Ni is the set containing node i and neighbors of node i. Once obtained, the normalized attention coefficients are used to compute a linear combination of the features corresponding to them, to serve as the final output features for each node:

Z0Zl+1i=X,=σ(∑j∈NiααlijWlZlj),
(8)
where σ(⋅) is a nonlinear activation function, which is acted by exponential linear unit (ELU) [37] in our experiments. In particular, GAT applies the multi-head attention mechanism, which learns the embedding via Eq. (8) multiple times and concatenates the embedding into a new representation.

Graph decoder model After we get the latent variable Z, we use a decoder model to reconstruct the graph structure A to better learn the relationship between two nodes. Here, the graph decoder model is defined by a generative model [35]:

p(A|Z)=∏i=1N∏j=1Np(Ai,j|zzi,zzj),
(9)
p(Aij=1|zzi,zzj)=σ(zzTizzj),
(10)
where Aij are the elements of A and σ(⋅) denotes the logistic sigmoid function.

Optimization To better learn class discriminative node representations, we optimize the variational graph autoencoder module via two losses as follows:

LS=Eq(Z|X,A)[logp(A|Z)]−KL[q(Z|X,A)||p(Z)],
(11)
where the first term is the reconstruction loss between the input adjacent matrix and the reconstructed adjacent matrix. The second term KL[q(Z|X, A)||p(Z)] is the Kullback–Leibler divergence between q(Z|X, A) and p(Z), here p(Z)=N(00,II).

Open-world classifier learning
After the variational graph autoencoder network, we obtain the uncertainty embeddings for each node through Eq. (4), which consists of two parts: uncertainty embeddings for labeled/training nodes Zlabeled and uncertainty embeddings for unlabeled/test nodes Zunlabeled. To better learn an accurate classifier for classifying both seen and unseen nodes in testing data, our proposed model consists of a cooperative module, a label loss as well as a class uncertainty loss working together to differentiate whether a node belongs to an existing class or an unseen class. The overall objective function is as follows:

LOpenWGL=γ1LL+γ2LC+LS.
(12)
The γ1, γ2 are the balance parameters. The LS is the loss function of the variational graph autoencoder network mentioned above. The LL and LC represent the label loss and the class uncertainty loss, respectively. The details are introduced as follows.

Label loss The label loss LL is to minimize the cross-entropy loss for the labeled data:

LL(fs(Zlabeled),Y)=−1Nl∑i=1Nl∑c=1Cyi,clog(y^i,c).
(13)
In the above equation, fs(⋅) denotes a full-connected layer with softmax activation function, where the full-connected layer is a linear transformation, transforming Zunlabeled into probabilities that sum to one. Nl is the number of labeled nodes. C denotes the number of seen classes, and yi,c denotes the ground truth of the ith node in the labeled data and y^i,c is the classification prediction score for the ith labeled node vi in the c class, respectively.

Class uncertainty loss Since we do not have the class information in the test data and there exist a considerable number of unseen nodes, we need to find a way to differentiate the seen class and unseen class. Unlike the label loss LL, which can utilize the abundant training data and have a good performance on the seen class by the cross-entropy loss, the class uncertainty loss is proposed to balance the classification output for each node and have superior effects on the unseen nodes. In our paper, an entropy loss is placed as the class uncertainty loss and our goal is to maximize this entropy loss to make the normalized output of each node balanced. The formula is as follows:

LC(fs(Zunlabeled))=1Nu∑i=1Nu∑c=1Cy^i,clog(y^i,c),
(14)
where Nu is the number of unlabeled nodes. y^i,c is the classification prediction score for the ith unlabeled node vi in the c class. Note that we do not put a negative sign in front of the formula as usual because we need to maximize the entropy loss. In addition, we will not use all the unlabeled data to maximize the entropy loss. We first sort all the unlabeled data output probability values (choosing the maximum probability for each node) after the softmax layer and then discard the largest 10% (nodes with large probability values are easily classified into seen classes since their output is discriminative) and the smallest 10% nodes (nodes with small probability values means that the node’s output is balanced over each seen class which can be easily detected as the unseen class). Finally, the remaining nodes are utilized to maximize their entropy.

The training for label loss and class uncertainty loss acts like an adversarial process. On the one hand, we want the label loss to influence the classifier to make the output of each node to be more discriminative and classify each seen node into correct classes via minimizing Eq. (13). On the other hand, we would like that the class uncertainty loss can make the output of each node to be more balanced to assist the detection of unseen class through the maximization of the entropy loss.

LL, LC, and LS are jointly optimized via objective function defined in Eq. (12), and all parameters are optimized using the standard backpropagation algorithms.

Fig. 5
figure 5
A visualization of determining the threshold using a validation set (only contains seen class instances). a Determining the threshold using validation set. b Applying determined threshold to the test set (contain both seen class and unseen class instances)

Full size image
Open-world classification and rejection
After performing the node uncertainty representation learning, we obtain a distribution (i.e., the Gaussian distribution) of the node embeddings. Therefore, M different versions of feature vectors (zz1i,…,zzMi) are generated for each node vi from this distribution via Eq. (4), where this process is called a reparametrization trick. Then, the M different representations are fed into the softmax layer to turn them into probabilities over C classes, respectively. (Each zzmi can obtain an output vector ssmi∈R1×C.)

After this process, the M outputs are concatenated to obtain a sampling matrix Si∈RM×C. In Si, each column denotes M different probabilities of a specific class and we average these probabilities for each class to obtain a vector ssi,a∈R1×C. For the vector ssi,a with C different probabilities, we choose the largest one max(ssi,a). To recognize whether each node vi is the seen or unseen classes for testing data, we have:

y^={Rejection,argmaxc∈C p(c|xi),if maxc∈C p(c|xi)≤totherwise,
(15)
where p(c|xi) is obtained from the softmax layer output of fs(⋅). If none of existing seen classes probability p(c|xi) value is above the threshold t, we reject xi as a sample from the unseen class; otherwise, its predicted class is the one with the highest probability. The prediction process of each testing sample is illustrated in Fig. 4.

Automatic rejection threshold selection
In open-world graph learning, a key problem is how to determine of the threshold t in Eq. (15) which can be used to reject a node from seen classes. In our paper, we propose a selection approach to automatically determine a threshold to reject nodes not belonging to seen classes. Specifically, we use a validation set Xvaltrain, which is separated from the training set Xtrain for threshold selection. For nodes in the validation set, we perform the node uncertainty representation learning and conduct the same sampling process and choose the largest posterior probability. Then, we average these chosen largest probabilities of all the nodes and obtain avg_seen. Because unseen class instances are assumed not appearing in the training set (including the validation set), we choose 10% nodes with the largest class distribution entropy, defined in Eq. (16), as the “expected unseen class nodes” (ΔXvaltrain), and their average posterior probability is denoted by avg_E_unseen.

H(xi)=−∑c∈Cp(c|xi)logp(c|xi).
(16)
The final threshold is calculated by averaging the probabilities as follows:

t=avg_seen+avg_E_unseen2.
(17)
Figure 5a shows an example of the determining process in the validation set. We use this determined threshold to classify seen and unseen nodes in the test set in Fig. 5b, and the result shows that the threshold is a good distinction between seen and unseen classes.

As a result of the above design, the node embedding features are denoted by distributions, instead of deterministic feature vectors. By using a sampling process to generate multiple versions of feature vectors, we are able to test the confidence of a node belonging to seen classes, and automatically determine a threshold to reject nodes not belonging to seen classes as unseen class nodes.

Algorithm description
Our algorithm is illustrated in Algorithm 1. Given a graph G=(V,E,X,Y), our goal is to obtain the node representations and classify the seen nodes and detect the unseen nodes, respectively. Firstly, we employ a variational graph autoencoder network to model the uncertainty of each node (Steps 2–10). Here, the output Z is a distribution and we optimize the network through the KL loss and the reconstruction loss (Step 12). Then, we propose two loss constraints LL and LC to make our model capable of classifying seen and unseen classes (Step 13 and 14). Finally, by jointly considering the label loss, class uncertainty loss and the VGAE loss (the KL divergence loss and network reconstruction loss), our model can better differentiate whether a node belongs to a seen class or an unseen class and capture the uncertainty representations for open-world graph learning.

figure d
Time complexity analysis
Given a graph G=(V,E,X,Y) with N nodes (vertices), the proposed open-world graph learning (OpenWGL) consists of two parts: graph encoder model and graph decoder model.

For GCN and GAT, the time complexity is asymptotically bounded by the number of edges of the network [29], i.e., O(|E|). This is mainly because that both methods rely on message passing between each node and its neighbors to learn node representation. Ei denotes edges incident to node vi; for all nodes in the network, the total number of message passing is ∑Ni=1|Ei|=2×|E|=O(|E|). Because OpenWGL replies on graph encoder module, the time complexity of the graph encoder model is O(|E|). The time complexity of the process of reconstructing the original graph is O(dN2), where d is the dimension of the latent space of matrix Z.

In order to test whether a node belongs to unseen class or not, OpenWGL needs to sample uncertain node embedding M times. Supposing the graph decoder model is sampled M times, the time complexity of the graph decoder model is O(dMN2). As a result, the time complexity of OpenWGL is asymptotically bounded by O(|E|+dMN2).

For sparse networks, the number of edges m is far less than the number of node pairs N2, and then, the complex of OpenWGL is quadratic to the number of nodes O(|E|+dMN2)=O(dMN2)=O(N2). For generic networks, the number of edges is less than the square of the number of nodes (the complete graph), i.e.,|E|≤N2, so we have O(|E|+dMN2)=O(N2+dMN2)=O((dM+1)N2)=O(N2). In summary, the complexity of OpenWGL is O(N2), which is mainly attributed to the graph encoder and decoder steps.

Experiments
Experimental setup
Benchmark datasets We employ four widely used citation network datasets (Cora, Citeseer, DBLP, PubMed) for node classification[38, 39]. The details of the experimental datasets are reported in Table 1.

Table 1 Statistics of the benchmark datasets
Full size table
Table 2 Statistics of the number of nodes and number of classes of the benchmark data
Full size table
Test settings and evaluation metrics For each dataset, we hold out some classes as the unseen class for testing and the remaining classes as the seen classes. In Table 2, we report the statistics of the benchmark data, with respect to the number of nodes and number of classes. In the experiments, when the number of unseen classes is set as |U|=1, |U|=2, and |U|=3, for Cora, Citeseer, and DBLP dataset, we select the last class, the last two classes, and the last three classes as the unseen class and the remaining classes as the seen classes of each dataset, respectively. For PubMed dataset (with one unseen class |U|=1), we select the first class as the unseen class and the remaining classes as the seen classes. We randomly sample 70% of nodes for training, 10% for validation and 20% for testing. Note that the nodes of unseen class only appear in the testing set. We use the validation set to determine the threshold for rejecting the unseen class. Like the traditional semi-supervised node classification, for each dataset, we feed the whole graph into our model. We vary the number of unseen classes to verify the performance of our model at different unseen class proportion. We use the Macro F1 score and Accuracy for evaluation [1].

Baselines We employ following methods as baselines.

GCN [32]: GCN is a deep convolutional network for graph-structured data. GCN employs a convolution layer to exploit the graph structure information and uses a classification loss function to guide the classification task. In GCN, it directly uses softmax as the final output layer. GCN does not have the rejection capability to the unseen class.

GCN_Sigmod: In GCN_Sigmod, we use multiple 1-vs-rest of sigmoids rather than softmax as the final output layer of the GCN model, which also does not have the rejection capability to the unseen class.

GCN_Sigmod_Thre: Based on GCN_Sigmod, we use the default probability threshold of ti=0.5 for classification of each class i, which means if all predicted probabilities are less than the threshold 0.5, we will reject it as the unseen class. Otherwise, its predicted class is the one with the highest probability.

MLP_DOC: DOC[1] is the state-of-the-art open-world classification method for text classification. We use a two-layer perceptron to obtain the node representation.

GCN_DOC: We utilize the rich node relationships and combine the GCN with DOC to compare with our model. In DOC, it uses multiple 1-vs-rest of sigmoids rather than softmax as the final output layer and defines an automatic threshold setting mechanism.

Proposed method In order to validate the performance of the proposed OpenWGL learning algorithm, we implement OpenWGL using two types of graph neural networks, including graph convolutional network (GCN) and graph attention network (GAT).

OpenWGL_GCN OpenWGL_GCN employs a two-layer GCN as the graph encoder model to aggregate node features.

OpenWGL_GAT OpenWGL_GAT employs a two-layer GAT as the graph encoder model to aggregate node features.

All deep learning algorithms are implemented using Tensorflow [40, 41] and are trained with Adam optimizer. We follow the evaluation protocol in open-world learning [1, 2], evaluate all approaches through grid search on the hyperparameter space, and report the best results of each approach. We feed the whole graph into our model when training. For all baseline methods, we use the same set of parameter configurations unless otherwise specified. For each deep approach, we use a fixed learning rate 1e−3. For each method, the GCNs contain two hidden layers (L=2) with structure as 32−16. The balance parameters γ1 and γ2 are set to 1 and 0.8, respectively. The dropout rate for each GCN layer is set to 0.3. M is set to 100. In addition, we choose two layers for OpenWGL_GAT, where the first GAT layer contains 32 hidden units and the second layer contains 16 hidden units.

Table 3 Experimental results on Cora with different numbers of unseen classes |U|
Full size table
Table 4 Experimental results on Citeseer with different numbers of unseen classes |U|
Full size table
Table 5 Experimental results on DBLP with different numbers of unseen classes |U|
Full size table
Table 6 Experimental results on PubMed with different numbers of unseen classes |U|
Full size table
Table 7 The Macro F1 score and Accuracy on three datasets for closed-world settings (without unseen classes)
Full size table
Open-world graph learning classification results
Tables 3, 4, 5, and 6 list the Macro F1 score and Accuracy of different methods on open-world node classification task. From the results, we have following observations:

(1)
The GCN and GCN_Sigmoid obtain the worst performance among these baselines in all datasets since they do not have the rejection capability to the unseen class. Therefore, all the unseen nodes will be misclassified and their performance becomes worse when the number of unseen nodes increases.

(2)
GCN_Sigmoid_Thre and GCN_DOC have better performances than GCN and GCN_Sigmoid, which shows that the threshold can improve the performance of detecting the unseen nodes. In addition, when the number of unseen nodes increases, GCN_Sigmoid_Thre and GCN_DOC become more competitive.

(3)
GCN_DOC has better performance than GCN_Sigmoid_Thre in most cases, confirming that the threshold is not a fixed value and it varies with different datasets and the ratio of unseen class. DOC’s automatic threshold setting mechanism can effectively improve the classification results of unseen class.

(4)
The proposed open-world graph learning model (OpenWGL_GCN and OpenWGL_GAT) consistently outperforms all baselines on three datasets with different numbers of unseen classes. It demonstrates that the proposed constrained graph variational encoder network can better differentiate whether a node belongs to a seen class or an unseen class and capture the uncertainty representation of each node by jointly considering the label loss, class uncertainty loss, and the node uncertainty representation learning as a unified learning framework. In addition, the proposed OpenWGL_GAT outperforms OpenWGL_GCN, which shows that assigning different weights to nodes of a same neighborhood can be more beneficial for node representation learning.

(5)
We also report closed-world learning setting results (without unseen class) in Table 7. The results show that when networks do not have unseen class, OpenWGL (OpenWGL_GCN and OpenWGL_GAT) has comparable performance as GCN, confirming its effectiveness and generalization for node classification. Overall, as the number of unseen classes increases, the performance of all methods, including our proposed methods, will decrease on Cora, DBLP, and PubMed but increase on the Citeseer dataset. This is mainly because that as more nodes are being assigned as unseen class nodes, the network will have less label information, resulting in deterioration in performance. On the other hand, as more classes being treated as unseen class (e.g., from |U|=1 to |U|=3), the whole network will have less number of classes, resulting in a slightly higher random prediction accuracy (e.g., random prediction accuracy on a binary classification task is 50%, which is higher than 33.3%, the random prediction accuracy on a three class classification task). This possibly leads to the performance increase on the Citeseer dataset. Meanwhile, as the number of unseen classes increases, using thresholds to detect unseen class nodes has clear benefits. However, in the absence of unseen classes, the performance of our method may be lower than rival methods, as shown in Table 7.

Ablation analysis of OpenWGL components
Because OpenWGL contains two key constraints, in this subsection, we compare variants of OpenWGL with respect to the following aspects to demonstrate: (1) the effect of the class uncertainty loss and (2) the impact of the VGAE module (KL loss and reconstruction loss). Note that we adopt GCN-based module in OpenWGL.

The following OpenWGL variants are designed for comparison.

OpenWGL¬C: A variant of OpenWGL with only the class uncertainty loss being removed.

OpenWGL¬V: A variant of OpenWGL with the KL loss and reconstruction loss being removed.

Tables 8, 9, and 10 report the ablation study results.

The effect of the class uncertainty loss
In order to show the superiority of the class uncertainty loss, we design a variant model OpenWGL¬C. As mentioned before, the class uncertainty loss is a constraint on the unlabeled nodes. The ablation study results show that the performances of the node classification task on both datasets are improved when the class uncertainty loss is used, indicating its effectiveness of detecting unseen nodes.

The impact of the VGAE module (KL loss and reconstruction loss)
In order to verify the impact of the VGAE module which can model the uncertainty node representations, we compare OpenWGL model and OpenWGL¬V. From the results, we can easily observe the OpenWGL model performs significantly better than OpenWGL¬V. This confirms that the usage of KL loss can model the uncertainty to better capture the latent representation of each node, and reconstruction loss can preserve node relationships which will assist in the node representation.

Table 8 The Macro F1 score and Accuracy between OpenWGL variants on Cora
Full size table
Table 9 The Macro F1 score and Accuracy between OpenWGL variants on Citeseer
Full size table
Table 10 The Macro F1 score and Accuracy between OpenWGL variants on DBLP
Full size table
Fig. 6
figure 6
Impact of feature dimensions of node output embeddings for the Accuracy and Macro F1 score on three datasets

Full size image
Parameter analysis
Impact of the feature dimensions of node output embeddings ZZ As mentioned in Method section, the output of node embeddings is represented as Z. OpenWGL uses two-layer GCNs with structure as 32−16, and feature dimensions d of node output embeddings is 16. We vary d from 4 to 64 and report the results on three datasets, respectively, in Fig. 6. On Citeseer and DBLP datasets, as d increases from 4 to 64, the performance grows gradually to reach a plateau. The performance of Cora dataset is stable with d increasing from 4 to 32 and has a slight decrease at 64. When d further increases to 128, the accuracy of target domain is improved on both tasks. After that, both the Macro_F1 score and Accuracy remain steady and no obvious difference is observed with different d. Therefore, only slight differences can be observed with different d values. The increase of d, from 4 to 64, does not necessarily result in performance improvements. The results show that with sufficient feature dimensions (d≥16), OpenWGL is stable with the increasing number of feature dimensions.

Fig. 7
figure 7
A case study of the OpenWGL sampling results with two randomly selected nodes from seen classes and unseen class on Cora, respectively. Each row denotes one node, “True” denotes the class to which the node genuinely belongs, and “False” means that the node does not belong to this class. a two nodes randomly selected from seen classes, and b two nodes randomly selected from unseen class. The x-axis denotes the probability output of each node through the softmax classifier, and the y-axis denotes the frequency appearing in each class

Full size image
Fig. 8
figure 8
A case study of the OpenWGL sampling results using statistics of all nodes of each seen class and the unseen class on Cora, respectively. Each row denotes all nodes of each seen class, “True” denotes the class to which the node belongs, and “False” means that the node does not belong to this class. a Statistics of all nodes of each seen class, and b statistics of all nodes of unseen class. The x-axis denotes the probability output of each node through the softmax classifier, and the y-axis denotes the frequency of all nodes of per class appearing in different classes

Full size image
Case study
Visualization of the OpenWGL sampling results
In order to verify the effectiveness of the sampling process of our model, we randomly choose two testing nodes from Cora dataset for seen and unseen classes (we choose one class as unseen, i.e., |U|=1), respectively. After performing the node uncertainty representation learning, we obtain a distribution of the node embeddings. Then, we generate 100 different versions of feature vectors for each node from this distribution and feed them into the softmax layer to turn them into probabilities over six classes, respectively. Therefore, after this process, for each node we obtain a 6×100 sampling matrix. In the sampling matrix, each column denotes 100 different probabilities of a specific class. We visualize the sampling matrices of these four nodes through histogram charts with seen and unseen classes in Fig. 7a and b. In Fig. 7, each row represents one node and in each row, there are six subfigures indicating the 100 different probabilities of each class, respectively. From Fig. 7, we can observe that the sampling process has superior performance in differentiating the seen classes and the unseen class, and it is very helpful for determining the threshold. For example, as shown in the first row in Fig. 7a, only in class 2, most of the 100 different probabilities are distributed on far right side of the histogram (i.e., large probability), while all the other classes (0,1,3,4,5) are distributed on the far left side (i.e., small probability). Thus, through the softmax layer, we can classify this node to class 2 and the ground truth is also class 2. However, if we just use a deterministic feature vector instead of this sampling method, this node may not be classified to class 2, since class 2 also has cases with small probability values. Similarly, for the unseen nodes as shown in Fig. 7b, in each seen class, most of the probability values are concentrated on the left side of the histogram (i.e., small probability), so we can easily detect them and classify them into unseen class. However, if we only obtain one probability output and do not have the sampling process, the unseen node might be misclassified randomly.

In order to show the average results of all nodes, Fig. 8 reports the sampling results using statistics of all nodes in each of the seen classes and unseen class on Cora, respectively. The difference between Figs. 7 and 8 is that the latter is obtained using the average of all nodes, whereas Fig. 7 is based on results from two randomly selected nodes. The results show that seen class nodes and unseen class nodes share different patterns. For seen class nodes, its average probability with respect to its genuine class is flat, with high probability values toward the 1.0 side, and its average probability values with respect to other classes have high probabilities toward the 0.0 side. For unseen class nodes, its average probability values to all classes have high probabilities toward the 0.0 side, perfectly explaining that the node does not belong to these classes.

The confusion matrix
In order to verify the effectiveness of OpenWGL in differentiating seen class nodes versus unseen class nodes, Fig. 9 reports the confusion matrix of OpenWGL on Cora network, where “−1” denotes unseen class. The results show that OpenWGL correctly identifies 87% of unseen class nodes and also remains a high accuracy in classifying seen class nodes.

Fig. 9
figure 9
The confusion matrix of OpenWGL on Cora. “−1” denotes the unseen class and “0,1,2,3,4,5” are seen classes. The (i, j) value of the matrix shows that the percentage value of the ith class is classified to the jth category

Full size image
Discussion
Graph learning in an open-world setting is a significant challenge, because it involves feature learning, prediction loss, and classification confidence. In the proposed design, we combine multiple loss terms as objective function to learn embedding features to represent node for classification. This novel learning task has many interesting topics for future study.

In order to decide whether a node belongs to the unseen class, we use a thresholding approach, in Eq. (15), to reject a node from seen classes, if its posterior probability p(c|xi) is less than a threshold t. Although the threshold value t is automatically determined by Eq. (17), it is solely based on the posterior probability values p(c|xi),c∈C. Alternatively, because rejecting xi as seen class or not is a binary decision, one can design a binary classification task, by using features to learn whether instance xi belongs to seen classes or not [2].

In this paper, we address the open-world graph learning in a static network setting, where nodes and edges do not change. In many applications, networks are continuously evolving with new nodes/edges [42], and node content may also change. Carrying out open-world graph learning in a dynamic network setting is another significant challenge. This is mainly because changes in node/edge distributions may impact on the unseen class detection, and some seen class nodes may also be misclassified as unseen class if they are undergoing an evolving or concept drifting [43]. Finding good representation for nodes in dynamic networks, with capability to differentiate nodes in seen vs. unseen classes, is another topic for future study.

Currently, our method aims to attribute all unknown classes to an unseen class (a single class), and it cannot distinguish each unknown class. In the future, we will study to better distinguish different unknown classes through post-processing and other unsupervised methods, such as clustering.

In addition, we use two GNN variants (i.e., GAT and GCN) as graph encoder model and compare them empirically. Admittedly, there are other alternatives instead of GCN and GAT. However, in this paper, our goal is not to propose a novel graph representation learning model, but rather to focus on a new open-world graph learning paradigm, where the learning goal is to not only classify nodes belonging to seen classes into correct groups, but also classify nodes not belonging to existing classes to an unseen class. We also observe that OpenWGL_GAT outperforms OpenWGL_GCN, which shows that new variant GNN method can be more beneficial for node representation learning and can improve the performance of the model. We will try to apply some new GNN models, such as GIN [44], GIL [45], APPNP [46], and FiLMConv [47], for the open-world graph learning task in the future.

Conclusions
Traditional graph learning tasks are based on the closed-world setting, where unlabeled nodes (i.e., test set) should have the same class space as the labeled nodes (i.e., training set). The learning goal is to classify nodes into classes already known. In the paper, we advocated an open-world graph learning paradigm which not only classifies nodes belonging to seen classes into correct groups, but also classifies nodes not belonging to existing classes to an unseen class. To achieve the goal, we proposed an open-world graph learning (OpenWGL) framework with two major components: (1) node uncertainty representation learning and (2) open-world classifier learning. The former uses label loss and class uncertainty loss to guide graph variational autoencoder to learn node embedding as distributions, and the latter automatically learns a threshold to detect unseen class nodes. The former learns a distribution for each node embedding via a graph variational autoencoder to capture the uncertainty, and the latter minimizes the label loss and class uncertainty loss simultaneously to distinguish seen and unseen class nodes, using automatically determined threshold. The threshold to reject the unseen class is further automatically determined in our framework. Experiments showed that when unseen class presents in test data, OpenWGL significantly outperforms baselines in classifying both seen and unseen class nodes. When networks do not have unseen class nodes (only contain nodes from seen classes), OpenWGL has a comparable performance to the baseline.