Naive Bayes (NB) was once awarded as one of the top 10 data mining algorithms, but the unreliable probability estimation and the unrealistic attribute conditional independence assumption limit its performance. To alleviate these two primary weaknesses simultaneously, instance and attribute weighting has been recently proposed. However, the existing approach learns instance and attribute weights separately, without considering their interactions at all, which restricts the performance of the learned model. Therefore, in this study, we propose a novel approach to learning instance and attribute weights collaboratively and call the resulting model collaboratively weighted naive Bayes (CWNB). In CWNB, we first learn the weight of each training instance iteratively based on its estimated posterior probability loss to make the prior and conditional probabilities more accurate, then we incorporate these two probabilities into the conditional log-likelihood (CLL) formula, and at last we search the optimal weight of each attribute by maximizing the CLL. Extensive experimental results show that CWNB significantly outperforms the standard NB and all the other existing state-of-the-art competitors.

Access provided by University of Auckland Library

Introduction
Over the past few decades, supervised classification has always been one of the most important tasks in data mining [1, 28]. Compared with other classifiers, Bayesian network classifiers (BNC) have always received much attention [27, 31]. Even in today’s big data era where deep learning has a wide impact [41], BNC still have some obvious advantages such as the explicit interpretability and the high robustness [3, 34]. Among numerous BNC, naive Bayes (NB) is the simplest and the most basic classifier, which assumes that all attributes are fully independent given the class. Given a test instance x represented by an attribute value vector <a1,a2,…,am>, NB uses Eqs. (1) and (2) to predict its class label.

P^(c|x)=πc∏mj=1θaj|c∑c∈Cπc∏mj=1θaj|c,
(1)
c(x)=argmaxc∈CP^(c|x),
(2)
where m is the number of attributes, aj is the jth attribute value of x, C is the collection of all possible class labels c, πc is the prior probability of the class c and θaj|c is the conditional probability of aj given the class c, which can be estimated by:

πc=∑ni=1δ(ci,c)+1n+q,
(3)
θaj|c=∑ni=1δ(aij,aj)δ(ci,c)+1∑ni=1δ(ci,c)+nj,
(4)
where n is the number of training instances, q is the number of classes, ci is the class label of the ith training instance, the indicator function δ(x,y) is one if x=y and zero otherwise, nj is the number of values for the jth attribute Aj, and aij is the jth attribute value of the ith training instance. Here, we only consider discrete attributes.

Although the structure is indeed quite simple, NB has already exhibited surprising classification performance and was once awarded as one of the top 10 algorithms in data mining [36]. However, there are still two primary weaknesses remained in NB: the unreliable probability estimation and the unrealistic attribute conditional independence assumption. To alleviate the former weakness, the proposed improvements consist of instance selection [22, 32], instance weighting [11, 18, 37], and fine-tuning [8, 16, 17]. To alleviate the latter weakness, the related enhancements include structure extension [13, 19, 30, 33], attribute selection [4, 5, 24] and attribute weighting [15, 20, 39].

Recently, to alleviate these two primary weaknesses simultaneously, instance and attribute weighting has been proposed [43]. In this model, instance weights are incorporated into the formula of the prior and conditional probabilities to get more accurate probability estimations. At the same time, attribute weights are incorporated into the exponential part of the conditional probabilities in naive Bayesian classification formula to alleviate the attribute conditional independence assumption. However, this model learns instance and attribute weights separately, without considering their interactions at all. But we argue that the interactions between them should not be ignored, because when we use this model to classify a test instance, the power function of conditional probabilities is collaboratively dependent on the instance and attribute weights. If we just use general data characteristics to calculate instance and attribute weights separately, the interactions between them are totally neglected, which is sub-optimal and greatly restricts the performance of the learned model. Therefore, in this study, we propose to learn instance and attribute weights collaboratively, and call the resulting model collaboratively weighted naive Bayes (CWNB). In CWNB, we first learn the weight of each training instance iteratively based on its estimated posterior probability loss to make the prior and conditional probabilities more accurate, then we incorporate these two probabilities into the conditional log-likelihood (CLL) formula, and at last we conduct a gradient descent search to find the optimal weight of each attribute by maximizing the CLL. Besides, we also propose three variants of CWNB and analyze the consistency of CWNB. The experimental results on a collection of 60 benchmark datasets from the University of California at Irvine (UCI) repository validate the effectiveness of our proposed CWNB.

To sum up, the main contributions of our work include:

We analyze the disadvantages of the existing instance and attribute weighting approach and argue that the interactions between the instance and attribute weights should not be ignored because the power function of conditional probabilities is collaboratively dependent on both of them.

We propose a novel model called collaboratively weighted naive Bayes (CWNB), which first learns the weight of each training instance based on its estimated posterior probability loss, then the learned instance weights are used to collaboratively learn attribute weights by maximizing the CLL. Meanwhile, we also propose three variants of CWNB.

We conduct comprehensive experiments on a collection of 60 UCI benchmark datasets to validate the effectiveness of CWNB. Besides, we also compare the performance of CWNB with its three variants and analyze the consistency of CWNB.

The paper is organized as follows. Section 2 provides a brief survey of related work about this study. Section 3 describes our proposed CWNB model and its variants. Section 4 presents the experimental setup and results. Section 5 concludes this study and outlines the main directions for future work.

Related work
Among numerous enhancements to NB, instance weighting is a commonly used approach to alleviating the unreliable probability estimation, and attribute weighting is a representative approach to relaxing the unrealistic attribute conditional independence assumption [43]. More concretely, instance weighting assigns a specific weight to each instance based on its importance. Likewise, attribute weighting assigns a continuous weight to each attribute according to its predictive ability. Both of them could be further divided into two broad categories: filters and wrappers [25, 26]. Filters employ general data characteristics to calculate the corresponding weights, regarding it as a preprocessing step before building the model. Contrarily, wrappers optimize the corresponding weights iteratively according to the performance feedback from the built model. Generally speaking, filters are much faster than wrappers in training phase, but wrappers usually could get higher classification accuracy.

For instance weighting filters, the easiest way to weight each training instance is to estimate the distance between it and the test instance, then the training instance nearest to the test instance is assigned the highest weight and vice versa. Among numerous algorithms, locally weighted NB (LWNB) [11] is a well-known algorithm. In LWNB, k-nearest neighbor algorithm is firstly utilized to find the neighbors of the test instance, then each neighbor is weighted based on its distance from the test instance. Finally, an instance weighted NB model is built based on the locally weighted training instances. Unfortunately, LWNB is a typical lazy learning model, which spends the main computational costs at classification phase and thus incurs very high time complexity. To obtain a more efficient model, recently, Xu et al. [37] proposed a simple but effective instance weighting model called attribute value frequency-based instance weighted naive Bayes (AVFWNB). In AVFWNB, the weight of each training instance is defined as the inner product of its attribute value frequency vector and the attribute value number vector. Extensive experimental results validate that AVFWNB could obtain better classification performance than NB yet maintain the simplicity of the model.

For instance weighting wrappers, Elhan [9] proposed boosted naive Bayes (BNB), which applies boosting [12] to NB. In BNB, a series of instance weighted NB models are learned successively, where each model pays more attention to the instances misclassified by its predecessor. Specifically, after building one instance weighted NB model, each training instance will be classified by the model, and the weight of those misclassified instances will be increased. Then, another instance weighted NB model is built using the re-weighted training instances. This process will be repeated for several rounds, and at last the output of the final BNB is the weighted sum of the results of all the base models, with each base model weighted according to its classification accuracy on the training instances. Different from BNB, Jiang et al. [18] presented another instance weighting wrapper called discriminatively weighted naive Bayes (DWNB). In each iteration of DWNB, training instances are discriminatively increased different weights associated with the estimated posterior probability loss, and then a new instance weighted NB model is built from the re-weighted training instances in the next iteration.

For attribute weighting filters, how to calculate the weight of each attribute is the most important problem. To solve this problem, Zhang and Sheng [40] first proposed a gain ratio-based attribute weighted NB (GRAWNB) model. In GRAWNB, the weight of an attribute is assumed relevant to its gain ratio, i.e., an attribute with a higher gain ratio will be assigned a larger weight and vice versa. Apart from this model, Hall [15] proposed a decision tree-based attribute weighted NB (DTAWNB) model. In DTAWNB, an ensemble of unpruned decision tree is first built from the training instances with random sample, then each attribute weight is assigned inversely proportional to its minimum depth in the decision tree. However, both of them only consider the attribute–class relevance. Recently, Jiang et al. [20] proposed a correlation-based attribute weighted NB model, which considers both the attribute–class relevance and the attribute–attribute redundancy. Extensive experimental results have demonstrated that all these models are simple and efficient. Meanwhile, they are really effective to improve NB for the classification problems.

For attribute weighting wrappers, how to search for an optimal weight vector is the most significant problem. Here, we just introduce two typical models among them. The first model is proposed by Zaidi et al. [39] called weighting attributes to alleviate NB’ independence assumption (WANBIA). In WANBIA, gradient descent searches are used to optimize the weight vector. As for the objective function, WANBIA chooses to maximize the conditional log likelihood (CLL) or minimize the mean squared error (MSE), and thus two different versions are created, which are denoted by WANBIACLL and WANBIAMSE, respectively. Recently, Jiang et al. [21] expanded this model to class-specific attribute weighting to discriminatively assign each attribute a specific weight for each class. Furthermore, Zhang et al. [42] simultaneously paid attention to attribute values and class labels. They expanded the weight vector to the weight matrix, and thus assigned a specific weight to each attribute value for each class. Different from these gradient descent searches models, Yu et al. [38] proposed correlation-based weight-adjusted NB (CWANB), which could be regarded as a filter–wrapper hybrid attribute weighting model. In CWANB, the attribute weights are first initialized by a correlation-based attribute weighting filter, and then optimized by a weight-adjusted wrapper.

Recently, to alleviate the two primary weaknesses simultaneously, Zhang et al. [43] combines attribute weighting with instance weighting into one uniform framework, and call their model attribute and instance weighted NB (AIWNB). In AIWNB, the weight of each attribute is calculated by correlation-based attribute weighting, and the weight of each instance is estimated by eager attribute value frequency-based instance weighting or lazy similarity-based instance weighting, thus two different versions are created, which are denoted as AIWNBE and AIWNBL, respectively. Extensive experiments have proven that their models could perform better than only instance weighting or just attribute weighting.

Collaboratively weighted naive Bayes
Motivation
As discussed above, instance and attribute weighting could obtain better performance than only instance weighting or just attribute weighting [43]. However, the existing approach [43] is a filter, which learns instance and attribute weights separately without considering their interactions at all. To be more specific, the existing approach first calculates the weight of each training instance wi (i=1,2,…,n) according to general data characteristics, and then the instance weights are incorporated into the formula of the prior and conditional probabilities, which can be represented by Eqs. (5) and (6), respectively.

πc=∑ni=1wiδ(ci,c)+1∑ni=1wi+q,
(5)
θaj|c=∑ni=1wiδ(aij,aj)δ(ci,c)+1∑ni=1wiδ(ci,c)+nj.
(6)
At the same time, the existing approach also uses general data characteristics to calculate the weight of each attribute wj (j=1,2,…,m) and then incorporates them into the exponential part of conditional probabilities in the classification formula, which can be represented by Eqs. (7) and (8).

P^(c|x)=πc∏mj=1θwjaj|c∑c∈Cπc∏mj=1θwjaj|c,
(7)
c(x)=argmaxc∈CP^(c|x).
(8)
However, in this study, we argue that the interactions between instance and attribute weights should not be ignored because when we utilize Eq. (7) to estimate the posterior probability P^(c|x) for a test instance x, the power function of conditional probability θwjaj|c is collaboratively dependent on instance and attribute weights. To explain it more clearly, the weight of each instance wi is incorporated into the conditional probability formula by Eq. (6), and the weight of each attribute wj is incorporated into the exponential part of the conditional probability by Eq. (7). If we just use general data characteristics to calculate their weights separately, the single conditional probability θaj|c and the single attribute weight wj both appear to be optimal themselves, but the interactions between them are totally neglected, so the overall value of θwjaj|c might be sub-optimal, which greatly restricts the performance of the learned model. Therefore, in this study, we propose to collaboratively learn instance and attribute weights, and call the resulting model collaboratively weighted naive Bayes (CWNB). We expect CWNB to obtain better performance because this model pays attention to the interactions between instance and attribute weights and learns their weights collaboratively, thus the overall value of θwjaj|c will be more accurate.

Proposed approach
The overall framework of CWNB is described in Fig. 1. In CWNB, we first initialize the weight of each training instance to 1.0. Then, we can build an instance weighted NB model using current weights. To be more specific, the current weights are utilized to estimate the prior probability πc and conditional probability θaj|c by Eqs. (5) and (6). Next, we use the built instance weighted NB model to estimate the posterior probability P^(ci|yi) for each training instance yi belonging to its true class ci by Eq. (1). Intuitively, we hope that the predicted posterior probability P^(ci|yi) and the true posterior probability P(ci|yi) are as close as possible. Therefore, we can use the information how “far-off” P^(ci|yi) is from P(ci|yi) to update the weight of training instance yi in the next iteration. Specifically, in each iteration, we add the estimated posterior probability loss of the ith training instance to its current weight wi. Here, we assume that the true posterior probability P(ci|yi) is 1.0 because the true class label of yi is exactly ci. Now, the detailed weight update equation is:

w(t+1)i=w(t)i+(1.0−P^(t)(ci|yi)),
(9)
where w(t+1)i represents the weight of the ith training instance in the (t+1)th iteration, P^(t)(ci|yi) represents the estimated posterior probability of the ith training instance in the tth iteration.

Fig. 1
figure 1
Overall framework of CWNB

Full size image
To make the instance weights more accurate, the process will repeat for T iterations. Extensive experiments have found that a majority of datasets will converge when T=15 [18]. Therefore, we also set T=15 in our experiments. After running for 15 iterations, we could calculate the final prior probability πc and conditional probability θaj|c using the learned instance weights by Eqs. (5) and (6).

Next we will discuss how to use the obtained πc and θaj|c to collaboratively learn attribute weights. As previously mentioned, Zaidi et al. [39] proposed an attribute weighted NB algorithm to adjust the weight vector by the L-BFGS-M optimization procedure [44], and their objective functions are either maximizing the CLL or minimizing the MSE. Motivated by this research, in our proposed CWNB, we also use the L-BFGS-M optimization procedure to solve the attribute weight vector optimization problem. But different from their approach, we take the instance weights into consideration when we calculate the value of the objective function. In our approach, prior probability πc and conditional probability θaj|c are obtained by Eqs. (5) and (6) instead of Eqs. (3) and (4). The difference might appear subtle, but this is in fact significant enough, because only in this way the instance and attribute weights could be learned collaboratively, and thus the overall value of θwjaj|c could be estimated more accurately. As for the objective function, in this paper we only choose to maximize the CLL. But we can get similar conclusions when we change the objective function to minimize the MSE.

Now, we will describe the remaining processes of CWNB in detail. As we can see in Fig. 1, the value of CLL could be calculated by prior probability πc, conditional probability θaj|c and attribute weight wj. First, we just initialize the weight of each attribute wj to 1.0, then we take the obtained πc, θaj|c and wj into the CLL formula, next we exploit gradient descent searches to iteratively update attribute weights by maximizing the CLL. For detail, the CLL objective function is defined by Eq. (10).

CLL(w)===logP^(C|D,w)∑i=1nlogP^(ci|yi,w)∑i=1nlogγcyi(w)∑c′γc′yi(w),
(10)
where w represents the attribute weight vector, and

γcyi(w)=πc∏j=1mθwjaj|c,
(11)
where πc and θaj|c are obtained by Eqs. (5) and (6).

Before calculating the gradient of CLL(w) with respect to wj, we could first calculate the gradient of γcyi(w) with respect to wj by Eq. (12).

∂∂wjγcyi(w)===⎛⎝πc∏j′≠jθwj′aj′|c⎞⎠∂∂wjθwjaj|c⎛⎝πc∏j′≠jθwj′aj′|c⎞⎠θwjaj|clog(θaj|c)γcyi(w)log(θaj|c).
(12)
Then, the gradient of CLL(w) with respect to wj can be represented by Eq. (13).

∂∂wjCLL(w)===∂∂wj∑i=1n(log(γcyi(w))−log(∑c′γc′yi(w)))∑i=1n(γcyi(w)log(θaj|c)γcyi(w)−∑c′γc′yi(w)log(θaj|c′)∑c′γc′yi(w))∑i=1n(log(θaj|c)−∑c′P^(c′|yi,w)log(θaj|c′)).
(13)
figure d
figure e
In summary, the entire learning algorithm for our CWNB can be partitioned into training (CWNB-training) and classification (CWNB-classification) algorithms. They are briefly depicted in Algorithms 1 and 2, respectively.

Three variants
Apart from CWNB that we described above, there are also three other variants for collaboratively weighted NB: 1) Reversing the order of weighting. Specifically, we could first learn attribute weights by maximizing the CLL and then build an attribute weighted NB model, next we utilize this model to estimate the posterior probability loss for each training instance and learn instance weights. Different from CWNB, in this approach, the posterior probability of each training instance is estimated by Eq. (7) instead of Eq. (1). For convenience, we denote this approach as CWNB-R. 2) Interactive weighting. Same as CWNB, this approach first learns instance weights and then learns attribute weights. But different from CWNB, in this approach, we evaluate the training classification accuracy (i.e., classification accuracy computed using the training data) of the current model after each round, and the weight learning processes will be repeated until the training classification accuracy no longer improves. Besides, to avoid the instance weights are updated so fast that the model cannot converge, in each round we only update the instance weights 1 iteration rather than 15 iterations in CWNB. Here, we denote this approach as CWNB-I. 3) Reversing the order of weighting and interactive learning. This approach first learns attribute weights and then learns instance weights, next we repeat the weight learning processes until the training classification accuracy no longer improves. Same as CWNB-I, in each round we update the instance weights only 1 iteration. We denote this approach as CWNB-RI. In Sect. 4.3, we will show the detailed comparison results for CWNB versus CWNB-R, CWNB-I, and CWNB-RI.

Experiments and results
Experimental setting and benchmark data
To evaluate the effectiveness of CWNB, we conduct experiments on the Waikato Environment for Knowledge Analysis (WEKA) platform [35] and compare it with several state-of-the-art competitors including AIWNBE [43], DWNB [18], WANBIACLL [39], NB [23] and SVM [6]. We implemented CWNB, AIWNBE, and DWNB on the WEKA platform. In addition, we used the existing implementation of NB and SVM (LibLINEAR with L2-regularized L2-loss support vector classification (primal) [10]) on the WEKA platform. Besides, the source code of WANBIACLL was kindly provided by the original authors. All the algorithms and their abbreviations are listed as follows.

CWNB: Collaboratively Weighted NB.

AIWNBE: Eager Attribute and Instance Weighted NB [43].

DWNB: Discriminatively Weighted NB [18].

WANBIACLL: CLL-based Attribute Weighted NB [39].

NB: Standard NB [23].

SVM: Support Vector Machine [10].

Table 1 Descriptions of 60 UCI datasets used in the experiments
Full size table
Table 2 Classification accuracy comparisons for CWNB versus AIWNBE, DWNB, WANBIACLL, NB and SVM
Full size table
Table 3 Classification accuracy comparisons of Wilcoxon tests
Full size table
Our experiments were conducted on a collection of 60 benchmark classification datasets from the University of California at Irvine (UCI) repository, which are the same as those of the recent peer work [42] and represent a wide range of domains and data characteristics. Table 1 lists the properties of these datasets in ascending order of instance number. In our experiments, all missing attribute values were replaced with the modes of the nominal attribute values and the means of the numerical attribute values from the available data. Besides, we discretized all numerical attributes into nominal attributes with equal-width ten bins.

Experimental results and analysis
In our experiments, all classification accuracy estimates are obtained by averaging the results from 10 separate runs of stratified tenfold cross-validation. Table 2 shows the detailed classification accuracy of each algorithm on each dataset. For each row, a field marked with ∙ and ∘ signifies that the classification accuracy of CWNB is statistically and significantly better or worse compared to the algorithm shown in the corresponding column, when corrected paired two-tailed t tests at 95% significance level are conducted [29]. Besides, the average classification accuracy of each algorithm on 60 datasets is summarized at the bottom of Table 2. Then, based on the classification accuracy results, we conducted a Friedman test [7] to obtain the average rankings of each algorithm, whose results are also summarized at the bottom of Table 2.

Furthermore, to thoroughly compare each pair of algorithms, we used the well-known KEEL Data-Mining Software Tool [2] to conduct the Wilcoxon signed-ranks test [7]. Table 3 summarizes the detailed comparison results. In Table 3, ∙ indicates that the algorithm in the column improves the algorithm in the corresponding row, and ∘ indicates that the algorithm in the row improves the algorithm in the corresponding column. The lower diagonal level of significance is α=0.05, and the upper diagonal level of significance is α=0.1.

Observed from all these comparison results, we can draw the conclusion that CWNB is generally the best among all the competitors including AIWNBE, DWNB, WANBIACLL, NB and SVM. This fully verifies the universal applicability of CWNB for a wide range of domains and data characteristics. Now, we summarize the highlights as:

Fig. 2
figure 2
Scatter plot of classification accuracy (%) comparisons on large datasets (instance number > 1000) for CWNB versus AIWNBE, DWNB, WANBIACLL, NB and SVM

Full size image
The improvement of averaged classification accuracy: The averaged classification accuracy of CWNB on 60 datasets is 83.03%, which is remarkably higher than that of AIWNBE (81.78%), DWNB (82.60%), WANBIACLL (81.77%), NB (79.91%), and SVM (79.53%).

The superiority of averaged rankings: The averaged ranking of CWNB is 2.6500, which is the lowest and much better than that of AIWNBE (3.2083), DWNB (3.2667), WANBIACLL (3.3417), NB (4.9250) and SVM (3.6083).

The superiority than other state-of-the-art attribute and instance weighted NB algorithm: Compared to AIWNBE, CWNB improves the classification accuracy on 35 datasets, and 20 of them are statistically better. Besides, CWNB only statistically loses on 3 datasets. Therefore, the performance of CWNB is much better than the existing state-of-the-art AIWNBE. These results also validate that the interactions between instance and attribute weights should not be ignored.

The effectiveness of attribute weighting (ablation study): Compared to DWNB, CWNB improves the classification accuracy on 33 datasets, and 7 of them are statistically better. More important, CWNB surprisingly statistically loses on zero dataset. Therefore, from the perspective of ablation study, the process of attribute weighting in CWNB is very effective to improve the classification performance.

The effectiveness of instance weighting (ablation study): Compared to WANBIACLL, CWNB improves the classification accuracy on 37 datasets, and 18 of them are statistically better. Meanwhile, CWNB statistically loses on zero dataset. In summary, the process of instance weighting in CWNB is also necessary to improve the classification performance.

The superiority than the standard NB and the SVM: Compared to the standard NB, CWNB statistically improves the classification accuracy on 30 datasets, and statistically loses on zero dataset. Compared to the SVM, CWNB statistically improves the classification accuracy on 20 datasets, and statistically loses on 9 datasets. These results demonstrate that CWNB can perform better than the SVM and much better than the standard NB.

The significance on the Wilcoxon signed-ranks test: Based on the Wilcoxon signed-ranks test results in Table 3, we could easily find that whatever the significance level is α=0.05 or α=0.1, our proposed CWNB significantly better than all the other competitors including AIWNBE, DWNB, WANBIACLL, NB and SVM.

Moreover, in order to show the relationship between the performance of CWNB and the instance number, we divide the datasets into two categories: small datasets (instance number ≤ 1000) and large datasets (instance number > 1000). Since the detailed classification accuracy results in Table 2 are in ascending order of instance number, and the dataset “credit-g” exactly has 1000 instances, from the results in Table 2, we can get a preliminary conclusion that CWNB is more appropriate to solve large dataset classification problems. To show the conclusion more intuitively and more clearly, we present the scatter plot for classification accuracy comparisons on large datasets, where the Y-axis and X-axis represent the classification accuracy (%) results of CWNB and its competitors, respectively. In Fig. 2, the blue and orange circle symbols indicate that the classification accuracy of CWNB is better and significantly better than its competitors, respectively. Meanwhile, the gray and yellow circle symbols indicate that the classification accuracy of the related competitors is better and significantly better than CWNB, respectively. From the results in Fig. 2, we could easily find that the classification accuracy of CWNB is better than the other five competitors on 17, 17, 14, 20, and 10 datasets, which accounts for about 85%, 85%, 70%, 100%, and 50% of all the large datasets, respectively. Furthermore, CWNB is statistically better than the other five competitors on 14, 6, 11, 16, and 8 datasets, which still accounts for as high as 70%, 30%, 55%, 80%, and 40% of all the large datasets, respectively. In a word, all these results demonstrate that CWNB can achieve comparable results with SVM. Moreover, compared to AIWNBE, DWNB, WANBIACLL and NB, our proposed CWNB performs much better on large dataset classification problems, which makes CWNB very attractive in the era of big data.

Table 4 Classification accuracy comparisons for CWNB versus CWNB-R, CWNB-I, and CWNB-RI
Full size table
Table 5 The average number of rounds required for CWNB-I and CWNB-RI on each dataset
Full size table
Discussion
Furthermore, to make a thorough analysis on collaborative weighting, we also compared the performance of CWNB with its three variants mentioned in Sect. 3.3. Table 4 shows the detailed results of each algorithm on each dataset. From these results, we can draw the conclusion that CWNB is almost comparative with CWNB-R, CWNB-I, and CWNB-RI in terms of classification accuracy. Meanwhile, since CWNB-I and CWNB-RI are both interactive weighting, we show their average number of rounds required for learning weights in Table 5. Therefore, we can find that CWNB is much faster than CWNB-I and CWNB-RI. Now, we summarize the highlights as:

The comparison results in terms of classification accuracy: Compared to CWNB-R and CWNB-RI, CWNB wins on 2 datasets and loses on 3 datasets. Compared to CWNB-I, CWNB wins on 3 datasets and also loses on 3 datasets. In summary, CWNB is almost comparative with CWNB-R, CWNB-I and CWNB-RI in terms of classification accuracy.

The comparison results in terms of training time complexity: From Table 5, the average number of rounds required for CWNB-I and CWNB-RI is 3.70 and 3.74, respectively. That is to say, CWNB is approximately three times faster than CWNB-I and CWNB-RI. Besides, the median number of rounds for CWNB-I and CWNB-RI is 2.50 and 2.61, respectively, i.e., about half datasets need more than 2 rounds for learning weights. Moreover, the maximum number of rounds can even up to 22.04 and 22.64, respectively. Unfortunately, more iterations still cannot improve the classification accuracy greatly, such as the dataset “optdigits.” Therefore, generally speaking, CWNB is more promising than its three variants.

Consistency
Finally, to observe whether the proposed CWNB is statistically consistent, we generate an artificial toy dataset to compare the performance of our proposed CWNB with the standard NB. In other words, we would like to verify when the NB assumptions, e.g., attribute conditional independence, are satisfied, whether our proposed CWNB will converge to the optimal NB classifier as instance number goes to infinity. To this end, we generated a binary classification dataset artificially, which contains 6 independent attributes and 1 class label. In this dataset, each attribute value is generated randomly from a finite set {0,1,2,3}. Moreover, for each instance, if the sum of its attribute values is ≥ 9, the class label is set to 1, else the class label is set to 0. Please note that, the only purpose for us to choose 9 as the split point is to keep the dataset balanced. From the description above, we can see that the total possible attribute space of this dataset is 46=4096. Therefore, to fully observe the performance of CWNB and NB under different instance number settings, we generate multiple groups of dataset whose instance number in the range of [25, 20,000].

Fig. 3
figure 3
Classification accuracy comparisons for CWNB versus NB under different instance number settings

Full size image
Here, we use the same experimental platform and the same experimental settings as we described in Sects. 4.1 and 4.2. Figure 3 shows the detailed classification accuracy comparison results. From the results, we find that when instance number is very small, NB can obtain higher classification accuracy than CWNB, but as instance number increases, CWNB gradually outperforms NB. Finally, when the instance number goes to infinity, both of them can converge to similar classification accuracy. Now, we summarize the highlights as:

The relationship between classification accuracy and instance number: In general, as the instance number increases, the classification accuracy of CWNB and NB can both be improved. One of the most possible reasons is that more instances can usually provide more useful information for modeling.

The comparisons of classification accuracy between CWNB and NB: When instance number is very small (<75 in Fig. 3), NB can achieve better performance than CWNB. Maybe this is because that the standard NB is robust and can have good performance even when instance number is very small, but CWNB needs to learn instance and attribute weights, when instance number is very small, CWNB has the potential risk of overfitting. As instance number increases (≥75 in Fig. 3), CWNB can always obtain better performance than NB. Specifically, when instance number ranges from 75 to 500, the classification accuracy of CWNB is about 5% higher than that of NB, and when instance number ranges from 500 to 10,000, CWNB usually has 3% higher classification accuracy than NB.

The statistical consistency of our proposed CWNB: From the results in Fig. 3 where instance number ≥10,000, we can draw a conclusion that CWNB is statistically consistent because CWNB will converge to the optimal NB classifier as instance number goes to infinity. As we described above, the total possible attribute space of this dataset is 46=4096, so when instance number ≥10,000, we can assume that in this situation we have obtained enough data for building an accurate model. In Fig. 3, as instance number goes to infinity, CWNB and NB can really converge to similar classification accuracy. Therefore, CWNB is statistically consistent.

Conclusions and future work
In this study, we argue that the interactions between instance and attribute weights should not be ignored, and propose a novel model called collaboratively weighted naive Bayes (CWNB). In CWNB, we first learn the weight of each training instance iteratively based on its estimated posterior probability loss, then we incorporate the prior and conditional probabilities into the CLL formula, at last we conduct a gradient descent search to find the optimal weight of each attribute by maximizing the CLL. Extensive experimental results on a collection of 60 UCI datasets show that CWNB significantly outperforms NB and all the other existing state-of-the-art competitors, especially on large datasets. Besides, we propose other three variants related to CWNB, and extensive experimental results show that their classification accuracy is comparative with CWNB but they usually need to take more time to train the model. Finally, we analyze the consistency of CWNB and find that when the attribute conditional independence assumptions are satisfied, CWNB will converge to the optimal NB classifier gradually as instance number goes to infinity.

Although our approach is collaboratively weighting, we still employ two different approaches to learn instance and attribute weights. We believe that employing more sophisticated optimizing search approaches, such as the differential-evolution-based approach [14], to learn weights for both instance and attribute simultaneously could further improve the performance of CWNB. In addition, applying this collaboratively weighting idea to improve other classification models is another interesting topic for our future work.