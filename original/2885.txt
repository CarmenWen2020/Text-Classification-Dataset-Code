Multi-label classification is the task of inferring the set of unseen instances using the knowledge obtained through the analysis of a set of training examples with known label sets. In this paper, a multi-label classifier fusion ensemble approach named decision templates for ensemble of classifier chains is presented, which is derived from the decision templates method. The proposed method estimates two decision templates per class, one representing the presence of the class and the other representing its absence, based on the same examples used for training the set of classifiers. For each unseen instance, a new decision profile is created and the similarity between the decision templates and the decision profile determines the resulting label set. The method is incorporated into a traditional multi-label classifier algorithm: the ensemble of classifier chains. Empirical evidence indicates that the use of the proposed decision templates adaptation can improve the performance over the traditionally used combining schemes, especially for domains with a large number of instances available, improving the performance of an already high-performing multi-label learning method.

Access provided by University of Auckland Library

Introduction
Multi-label classification is a generalization of the traditional single-label classification problem where instances are associated with a label set instead of a single label. Several real-world domains are better represented as multi-label problems. In the movie genre classification, for instance, a movie can belong to any combination of genres, such as drama, action and romance.

Many of the proposed multi-label classification algorithms, such as ensemble of classifier chains (ECC) [1], are based upon the concept of ensembles of classifiers. In those methods, an ensemble is composed of multiple multi-label classifiers. In order to classify an unseen instance, the output of each member of the ensemble for that instance is collected and then a predefined fusion function determines the final classification result. The choice of this function has fundamental importance for the operation of the ensemble and directly influences its performance.

This paper focuses on the decision templates (DT) method of classifier fusion [2], which organizes the outputs of the members of the ensemble into matrices called decision profiles (DP). Those DP are then used to compute the DT of each class. A DT can be understood as the expected DP for instances that belong to that class. During the classification process of an unknown instance, the class with the most similar DT is chosen. Since this method was created to work with single-label ensembles, DT cannot be applied directly in multi-label problems.

In a review study evaluating state-of-the-art multi-label classifiers based on ensembles, the ECC method stood out as the one with the best overall performance and, therefore, was chosen to be used in this work [3]. In order to adapt the DT method to the ECC multi-label classifier, this work proposes the decision templates for ensemble of classifier chains (DTECC) method. Two DP matrices are defined for each label, one representing the instances that are assigned to that label and a complementary DP representing the instances that are not assigned to that label. For an unseen instance, the task of classification for each label becomes the task of selecting the most suitable of the two DP matrices computed for that label. This procedure enables the assignment of multiple labels to each instance and, thus, adapts the DT method to multi-label problems.

The performance of the DTECC  is empirically evaluated and compared with two known fusion methods often present in the literature: majority vote and mean ensemble. The empirical results show that replacing the fusion scheme of the ECC method with the DT can improve the performance for accuracy and F-measure in most data sets, while remaining competitive in terms of other metrics.

This paper is structured as follows: Sect. 2 presents the ensemble learning approach and defines the classification tasks, reviews the ensemble of classifier chains method and introduces DT. Section 3 presents the decision templates for ECC (DTECC) method. Section 4 details the data sets, metrics, parameters and results of the experiments. Finally, Sect. 5 presents the conclusions and future work.

Ensemble learning
This section briefly presents the background required for presenting the DTECC method. It starts describing the classification tasks. Then, classifiers ensembles and fusion functions are introduced. Special attention is given to the DT method. The section closes with a review of the main multi-label classifiers ensemble methods.

Classification tasks
The single-label classification task associates a feature vector xx from domain X with exactly one out of m possible labels ℓj,j=1,…,m. A training set S={(xx1,y1),…,(xxn,yn)} consists of feature–label pairs (xxk,yk) where the desired output yk is one of the possible m labels. A common practice is to code the class membership of the features as a vector yyk from domain Y of length m where the components are binary values. This is called one-out-of-m coding. Only one of the binary values has value one, i.e., yk,j=1, the remaining (m−1) values are zero; hence, the associated class information of the feature vector xxk becomes yyk=(yk,1…yk,j−1,yj,k,yk,j+1…yk,m). The single-label classification problem may be binary (two possible label values) or multiclass (three or more label values).

Multi-label classification is a generalization of single-label classification where a feature vector can be associated with more than one label simultaneously. In the multi-label classification task, let X⊆Rn be the feature space and L={ℓ1,ℓ2,ℓ3,…,ℓm} be the set of possible labels. A classifier can be defined as a function that maps the feature space X to the label space Y={0,1}m or h:X→Y, xx↦yy. Many real-world problems such as image recognition [4], subcellular locations of proteins [5] and text classification [6] can be best represented as multi-label problems.

Classifiers ensemble
The technique of combining the predictions of multiple classifiers to produce a single classifier is known as ensemble of classifiers [7, 8]. In the last years, classifiers ensemble techniques have become a popular approach to improve the performance of individual classifiers [9,10,11,12]. Typically, a classifiers ensemble achieves better performance than individual classifiers . Good ensembles are generally composed of individual classifiers which are both accurate and make their errors in different parts of the input space.

Ensemble methods can be divided into two categories: classifier selection and classifier fusion [2, 13,14,15]. Classifier selection methods train classifiers to be local experts in some area of the feature space. The importance given to the prediction of a specific classifier is inversely proportional to the distance of the data used to train this classifier. One or more classifiers can be selected to give the final decision [13, 16,17,18]. In classifier fusion, classifiers are trained on the entire feature space. In those techniques, the output of the weaker classifiers must be merged to create the stronger output. Traditional methods such as Bagging [19] and Boosting [20, 21] are based on this approach. These methods are based on resampling techniques for obtaining different training sets for each of the individual classifiers. This paper focuses on classifier fusion methods. Thus, further explanation on the operation of classifier fusion methods is presented.

Let H={h1,h2,…,hc} be the set of classifiers of a classifiers ensemble.

The output of each classifier can be scaled to the [0,1] interval without loss of generality. For a feature vector xx∈X, the output of the ith classifier is represented as hi(xx)=[di,1(xx),…,di,m(xx)],di,j(xx)∈[0,1],1≤j≤m, where di,j(xx) can be generally interpreted as the degree of support outputted by the hi classifier to the hypothesis that xx is labeled as ℓj. The output of all the classifiers that compose an ensemble composed by c classifiers and m labels can be organized as a matrix known as the decision profile (DP) of xx:

DP(xx)=⎛⎝⎜⎜⎜⎜⎜⎜d1,1(xx)…di,1(xx)…dc,1(xx)………d1,j(xx)di,j(xx)dc,j(xx)………d1,m(xx)di,m(xx)dc,m(xx)⎞⎠⎟⎟⎟⎟⎟⎟.
The output of the resulting classifier hˆ is defined by a fusion function F that uses the classifiers outputs to create a combined output:

hˆ(xx)=F(h1(xx),…,hc(xx))=F(DP(xx)).
(1)
Fusion functions can be divided into two groups based on whether they require additional training once the classifiers of the ensemble are trained [22]. Popular combination functions that do not require further training include the majority vote, maximum, minimum, average and product. With the exception of the majority vote, which uses DP(xx) for computing the label that is most voted by the classifiers, all these functions compute the combined support value for each label as

dj(xx)=Φ(d1,j(xx),…,dc,j(xx)),
(2)
where Φ represents the corresponding mathematical function (maximum, minimum, average or product). For single-label classification, the resulting label is calculated using Eq. 3:

hˆ(xx)=ℓ⇔dℓ(xx)=maxj=1,…,m{dj(xx)}.
(3)
DT is an example of fusion function that requires additional training. The following subsection describes the DT fusion function in detail.

Decision templates
The decision templates method [2] is a single-label classifier fusion function. The main idea consists of using the DP matrix in its entirety thus avoiding missing information that can improve the overall performance of the ensemble. Let S={(xx1,y1),…,(xxn,yn)} be the single-label training set. An ensemble of classifiers is trained using S. For each label ℓj∈L a DT matrix is computed:

DTj=∑nk=1f(yk,ℓj)⋅DP(xxk)∑nk=1f(yk,ℓj),
(4)
where f(yk,ℓj) is a function that assumes the value 1 if the label of yk is ℓj. Otherwise, the function is 0.

DTj can be defined as the expected value of DP(xx) for the ℓj label. The final support value ϕj to ℓj is directly proportional to the level of similarity between the matrices DP(xx) and DTj.

The similarity function based on the Euclidean distance recommended in [23] is calculated as

ϕj(xx)=1−||DP(xx)−DTj||22∑mi=1(1−||DP(xx)−DTi||22).
(5)
where ||.||2 is the entry-wise matrix norm, or equivalently the Euclidean distance between the linearized DP and DT. (All lines have been sequentially assembled into a c×m vector.) After the support value ϕj has been obtained for each class, a further evaluation takes the final decision. Usually, in a single-label crisp classification task, the highest score defines the assigned class. In a binary classification task, the first class is chosen, if ϕ1(xx)≥ϕ2(xx). In a multi-label problem, this strategy is not applicable. Figure 1 illustrates the application of the DT method for defining the classification of feature vector xx.

Fig. 1
figure 1
Decision templates fusion process for evaluating feature vector xx. An ensemble of c classifiers outputs the values that compose the decision profile (DP(xx)). A similarity function computes the similarity values between DP(xx) and DTj for all m labels. The resulting label is selected by the largest similarity value

Full size image
Algorithm 1 describes the process of composing a decision profile matrix. Predict is a function that has the unseen instance and a classifier as inputs, and outputs a |L|-dimensional vector containing the support value for each label j∈L. For a given instance, the support vector of each classifier is collected, and its values are used to compose the DP matrix.

figure d
Algorithm 2 shows how to compute single-label decision templates (DT) matrices. ComputeDT computes a decision profile (DP) matrix for each training instance. A DT matrix for a given label is obtained by averaging the values of all the DP matrices for the training instances associated with that label. Each DP is added to the DT matrix of the label to which the instance belongs. Then, the DT matrix is divided by the number of training instances belonging to the respective label. (Counting is performed by function countByLabel.)

figure e
Algorithm 3 combines the classifier outputs in order to assign a label to an instance. The ClassifyDT function computes a DP for the test instance. According to the similarity function, the label ℓ associated with the most similar DT of the test instance DP is chosen as output.

figure f
Ensemble of classifier chains
Ensemble methods have also been effective in multi-label classification tasks. This section presents the ensemble of classifier chains, a widely used multi-label ensemble method.

The ECC method is a traditional multi-label classification method that gathers several different classifier chain (CC) classifiers [1]. The results of an extensive experimental comparison study showed ECC as the best overall performance algorithm for the most common multi-label metrics [3].

The main idea of the CC method is the exploration of dependencies between labels through a threading strategy that increases successively the number of labels used to predict the next label in the chain of classifiers. The first classifier of the string hCC1:X→{0,1}, xx↦y1 works as one of m single-label binary classifiers. The prediction value of the first label is used in conjunction with the feature vector xx to perform the prediction of the second label. hCC2:X×{0,1}→{0,1}, (xx,y1)↦y2. The classifiers that compose the CC are trained using the real labels y1, whereas in the prediction phase the estimated label values y^1 are used in the mapping, such that (xx,y1^)↦y2^. Therefore, CC is composed of m classifiers that can use from zero to (m−1) labels as additional features, such that in the training phase hCCj:X×{0,1}j−1→{0,1}, (xx,y1,…,yj−1)↦yj,j=1,…,m and in the prediction phase hCCj:X×{0,1}j−1→{0,1}, (xx,y^1,…,y^j−1)↦yj^,j=1,…,m. The order in which the labels are chained has a great influence on the final result, but the method does not define the ordering to be followed.

ECC creates an ensemble composed of c CC classifiers:

ECC={CCCC1,CCCC2,…,CCCCc},

where each CCCCk is trained using a random classifier chaining order and a random subset of D. The purpose of such a procedure is to construct a varied ensemble of classifiers, where each CCCC model is unique and can perform predictions different from the others [1].

Multi-label ensemble fusion functions
ECC typically uses the most common fusion functions proposed to combine the output values of ensembles of multi-label methods: MajorityVoteEnsemble (MV) and MeanEnsemble (ME) [11].

MajorityVoteEnsemble sums the positive and negative votes of all classifiers for each problem class. The fraction of positive votes relative to the total number of classifiers is calculated, and only labels with a value above a defined threshold t are assigned to the example. Traditionally, only classes with a majority of positive votes (i.e., t≥0.5) are assigned to the example and ties are broken randomly. This approach is exemplified with an example from the Image data set in Table 1.

Table 1 Example of MajorityVoteEnsemble fusion function for classifying an instance of the image data set with an ensemble consisting of 5 multi-label classifiers
Full size table
MeanEnsemble computes the average support value for each class and only includes in the final result classes with an average support value greater than a threshold t. The ith classifier hhi∈H predicts a vector hhi(xx)=[di,1(xx),…,di,m(xx)]∈[0,1]m. The MeanEnsemble method stores the sums of the predictions of each individual classifier in a vector W=(λ1,…,λm)∈Im such that λj=∑ci=1di,j. W is then averaged to Wavg∈Rm, which represents a distribution of votes for each label in the [0, 1] interval. Using a score-based method [24], a threshold t is used to choose the final multi-label set Y such that ℓj∈Y when λjc≥t. Hence, the relevant labels in Y represent the final multi-label prediction. This scheme is represented in Algorithm 4 and exemplified in Table 2 for the Image data set. The sum of support values of all classifiers is stored in a vector and then averaged according to the number of classifiers. If the average support value for the label is greater than the threshold value, the label is assigned to the instance.

figure g
Table 2 Example of MeanEnsemble fusion function for classifying an instance of the image data set with an ensemble consisting of five multi-label classifiers
Full size table
Decision templates for ensemble of classifier chains
This paper presents the decision templates for ensemble of classifier chains (DTECC) method, an adaptation of the DT fusion scheme to the multi-label classification method. In DTECC, given an ensemble composed by c multi-label classifiers, the problem is transformed into m binary classification problems. The DTECC method condenses the information of the original c×m into c×2 DP matrices. Using the previously labeled multi-label training set D={(xx1,yy1),…,(xxn,yyn)}, the DP for each label ℓj∈L,j=1,…,m becomes

DPj(xx)=⎛⎝⎜d1,ℓj(xx)…dn,ℓj(xx)⎞⎠⎟,j=1,…,m.
(6)
Using the previously labeled multi-label training set D={(xx1,yy1),…,(xxn,yyn)}, two corresponding matrices are computed for each label ℓj∈L,j=1,…,m:

DTj=∑ni=1f(yyi,ℓj)DP(xxi)∑ni=1f(yyi,ℓj),
(7)
DTj¯=|∑ni=1(1−f(yyi,ℓj))DP(xxi)|∑ni=1|1−f(yyi,ℓj)|,
(8)
where f(yyi,ℓj) is a function that takes value 1 if ℓj∈yyi. Otherwise, its value is 0.

DTj represents the average DP matrices of the training instances associated with the label ℓj, while DTj¯ represents the average DP of the instances with the complementary labels. The classification of each label is done separately according to the level of similarity.

This work adopts a similarity function in the fusion process based on Eq. 5 but removes the normalization:

μj(xx)=1−||DP(xx)−DTj||22=1−∑i=1c∑ℓ=1m[di,l(xx)−DTj(i,ℓ)]2.
(9)
Normalization is removed only to reduce computational cost and does not change the final result, as the ordering of labels with respect to their similarities remains unchanged, regardless of normalization.

Thus, the level of similarity measured between DP(xx) and both DTj and DTj¯ is computed using Eq. 9. Finally, an unseen instance (xx,yy) is labeled as ℓj if μj(xx)>μj¯(xx). The DTECC  architecture is represented in Fig. 2.

Fig. 2
figure 2
(DTECC) Ensemble of classifier chains decision templates fusion architecture. The output of c CC classifiers is combined into one final classification output

Full size image
Algorithm 5 shows how to compute the ensemble of classifier chains decision templates. For each training instance, the results of the classifiers are compiled into a DP matrix, which is added to one of the two possible DT matrices for each label, according to the actual label of the instance. At the end of the process, the DT matrices are averaged. Function ComputeDTECC outputs two decision templates for each label instead of just one: a DT matrix for the positive training instances and another for the negative training instances. Algorithm 6 shows how to assign a label set yy to an instance. The function ClassifyDTECC checks for each label whether the DP of instance x is more similar to DT of positive training instances or the DT of negative training instances. Figure 3 shows an example of using the DTECC  method in an instance of the Image data set for evaluating if the label Desert should be assigned to the multi-label set of the instance.

Fig. 3
figure 3
Example of classifier fusion using the DTECC  method on an instance of the Image data set for the label Desert. The similarity level of the DPDesert matrix with DTDesert and DTDesert¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ is measured, as μ1>μ1¯¯¯ the label is assigned to the instance. This procedure is performed for all labels in the problem: Desert, Mountains, Sea, Sunset and Trees. At the end of this procedure, the Label set associated with the instance is determined

Full size image
figure h
figure i
Experiments
The classifiers are evaluated using a tenfold cross-validation technique. To ensure an unbiased evaluation of the methods, all the experiments use c=50 classifiers in the ensemble, a threshold value of t=0.5 and naive Bayes as a single-label base classifier. While working on the multi-label feature ranking based on ensembles, [25] showed that ensembles of 50 classifiers are enough to maximize most metrics. It has been also shown that ensemble learning works well when naive Bayes is used as the base classifier [26]. Additional ECC parameters were set as the default values defined by the authors, e.g., size of each bag sample as a percentage of the training size is set to 100, and sampling with replacement is used.

Mulan [27] provides implementations for the ensemble of classifier chains, and for all evaluated metrics, those implementations were used to perform the experiments. Reproducibility and verifiability are very important issues when it comes to experimental results [28]; in order to make the experiments in this article reproducible, the code and the results of the experiments are made available in the public repository https://github.com/vfrocha/dtecc.

Data sets from several domains were used in the experiments; some of their properties are given in Table 3. The average number of labels for example of the data set is called cardinality. High cardinality values indicate data sets in which many labels occur simultaneously, while values close to 1 indicate data sets with characteristics that resemble single-label data sets.

Table 3 Properties of the multi-label data sets used in the experiments: number of features, label cardinality, number of instances and number of labels
Full size table
Evaluation metrics
This section presents the measures used to compare the algorithms in the experiments. Multi-label evaluation metrics are usually divided into two groups according to the way they are calculated: example-based metrics which are calculated for each test example separately, and then returning the mean value across the test set, and label-based which are calculated with respect to each class label separately, and then returning the macro-/micro-averaged value across all class labels [29]. The metrics used in this work are all example-based.

The example-based accuracy measure is defined as the proportion of correctly classified labels on the total number of labels of each test sample.

Accuracy(yy,hh(xx))=|yy∩hh(xx)||yy∪hh(xx)|.
(10)
Hamming loss [30] computes the fraction of incorrectly predicted labels by the classifier hh. It is a loss metric, i.e., a classifier that only produces correct outputs would achieve a value of zero for this metric.

HammingLoss(yy,hh(xx))=1m∑i=1m[yi≠hi(xx)].
(11)
Subset accuracy, or Subset0/1 accuracy, [31] compares the label set predicted by hh to the real label set yy associated with the example. The set of labels predicted for the example must exactly match the corresponding set of labels in yy.

Subset0/1(yy,hh(xx))=[yy=hh(xx)].
(12)
It should be noted that subset accuracy is a very strict metric since it penalizes nearly correct and completely wrong predictions in the same way. This metric may be useful, however, in certain applications where an exact classifier performance is of high priority.

Example-based precision is defined as the ratio of correct predicted labels, i.e., the ratio of labels predicted by hh that actually occurs in yy:

Precision(yy,hh(xx))=|yy∩hh(xx)||hh(xx)|.
(13)
Example-based recall represents the ratio of actual labels effectively predicted, i.e., the ratio of labels occurring in yy that were actually predicted by hh:

Recall(yy,hh(xx))=|yy∩hh(xx)||yy|.
(14)
A classifier that has a high recall, but low precision values provide many labels, yet mostly incorrect. A classifier with high precision, but low recall values provides few correct labels, yet most actual labels are omitted.

Example-based F-measure is a commonly applied multi-label metric that binds well with unbalanced data sets. F-measure is a function of two other multi-labels metrics: example-based precision and example-based recall.

The example-based F-measure, or F1, is defined as the harmonic mean of the precision and recall values. The value of the F1 measure metric is high only when recall and precision values are high.

F1(yy,hh(xx))=2⋅Precision×RecallPrecision+Recall.
(15)
Results
This section presents the empirical study of the performance of the DTECC method and compares them with the MEECC and MVECC methods. The methods are evaluated against each other according to their average performance for all iterations of the tenfold cross-validation of each data set.

Tables 4, 5, 6, 7, 8 and 9 show the results obtained by each method using accuracy, precision, recall, F-measure, subset accuracy and Hamming loss metrics, respectively. Altogether with the average performance, the rank achieved by the method is also presented between parentheses. The best results are highlighted in bold face. At first glance, one may see that there is no method that is the best in all data sets when evaluated by one of the metrics. Indeed, every method is the best for at least two data sets in each metric.

Table 4 Results for accuracy for the ECC using all the fusion schemes and data sets
Full size table
Table 5 Results for precision for the ECC using all the fusion schemes and data sets
Full size table
Table 6 Results for recall for the ECC using all the fusion schemes and data sets
Full size table
Table 7 Results for F-measure for the ECC using all the fusion schemes and data sets
Full size table
Table 8 Results for subset accuracy for the ECC using all the fusion schemes and data sets
Full size table
Table 9 Results for Hamming loss for the ECC using all the fusion schemes and data sets
Full size table
Tables 4, 5, 7 and 9 show that the DTECC method presents the largest number of wins for accuracy, precision, F-measure and Hamming loss on the evaluated data sets. Table 6 shows that the MVECC method achieves the best results for the recall metric, winning in 11 of the data sets, while the DTECC achieves almost the same performance, winning in 10 data sets. The only metric that the DTECC actually achieves an inferior performance is subset accuracy, shown in Table 8, even though it was still able to win in 6 data sets.

Arguably the most important result obtained in the experiments comes from comparing the average ranking results for the metrics. Table 10 shows the average rank of all methods for each evaluated metric. While the DTECC fusion method version was superior on average for the accuracy and F-measure metrics, MEECC was the best for the Hamming loss, precision and subset accuracy metrics. MVECC only gives the best recall results. However, among the three fusion methods evaluated, the DTECC method was the most consistent in its evaluation metrics results, not being among the two best metrics only for subset accuracy. The MEECC method, on the other hand, despite the good results for the precision, subset accuracy and Hamming loss, presented the worst results for all other metrics. Similarly, MVECC has the best average rankings for recall, but the worst for precision and Hamming loss. In addition, while the MEECC method was ranked best in most metrics, one may argue that results obtained in accuracy and F-measure are the most relevant since they are two of the three most commonly used example-based metrics for evaluating multi-label classifiers [32].

Table 10 Average rank of the ECC methods using the evaluated fusion schemes for all data sets
Full size table
DTECC results are particularly positive in data sets with a reliably large number of instances. A possible explanation for the good results of the DTECC for large data sets is that a larger number of instances allow the creation of more accurate decision profiles compared with data sets with fewer instances. As the decision profiles are calculated through the training instances, it is desirable that there are a reasonable number of instances for the creation of a reliable decision profile. Table 11 shows the average rank of all three methods considering only data sets with more than a thousand instances: 20NG, Enron, GnegativeGO, HumanGO, Image, Langlog, Reuters-K500, Scene, Water-quality, Yeast and Yelp. As one may see, DTECC now wins in three out of six metrics.

Table 11 Average rank of the ECC methods using the evaluated fusion schemes for the largest data sets in terms of number of instances
Full size table
The positive results on data sets with a larger number of instances were confirmed by statistical tests. Whenever performed using all data sets, they showed no statistically significant difference on the overall results. However, these differences were found when analyzing only the results on data sets with more than a thousand instances. The Friedman–Nemenyi test was applied. First, the Friedman test is performed to reject the null hypothesis and then proceeded with a post hoc analysis based on the Nemenyi method. For the Nemenyi test, it was used α=0.1. Figure 4 shows the critical difference diagrams. Wherever methods are connected by a horizontal line, it means they are not significantly different. It can be seen from the diagrams in Fig. 4a and b that the DTECC  method is significantly better on the accuracy and F-measure metrics in the data sets evaluated. The DTECC  method is also the best on precision, but it is not statistically better than the MEECC method, as shown in Fig. 4d. For the metrics Hamming loss, recall and subset accuracy, respectively, shown in Fig. 4c, e, f, no statistically significant difference was found between the DTECC  method and the other methods.

Fig. 4
figure 4
Critical difference (CD) diagrams obtained from the average ranking comparison of the methods over the 11 data sets with more than 1000 instances: 20NG, Enron, GnegativeGO, HumanGO, Image, Langlog, Reuters-K500, Scene, Water-quality, Yeast and Yelp

Full size image
The fusion methods running times were also compared. Since the running times of the fusion methods depend mainly on the number of labels, the CAL500 data set, which has the highest number of labels, was chosen to perform the runtime test. Figure 5 reports the average running times for each cross-validation round in seconds. One may observe that there is virtually no difference in terms of execution time between the evaluated methods.

Fig. 5
figure 5
Average of test runtime in seconds for each cross-validation round on the CAL500 data set

Full size image
Conclusions
This work focuses on fusion functions in the context of multi-label ensemble classification methods, a topic little explored in the current literature. It extends the decision templates approach applied to single-label ensemble classification methods, to be applied on multi-label problems using the ensemble of classifier chains method.

The proposed decision templates for ensemble of classifier chains was compared with two other traditional fusion functions: the majority vote ensemble and mean ensemble methods. Each of these fusion functions was incorporated in the ECC classifier: one of the state-of-the-art multi-label classification algorithms based on ensembles most discussed in the literature. Some of the most well-known example-based multi-label metrics were used for obtaining the experimental results: accuracy, precision, recall, F-measure, subset accuracy and Hamming loss.

The proposed method produced the best overall results in the experiments for both accuracy and F-measure. The results for the DTECC were stronger in large data sets (over 1000 instances). This may be linked with the creation of more accurate decision templates in those large data sets. Statistical tests conducted on such data sets found a significant difference on both metrics. In all evaluated metrics, the DTECC was ranked among the two best showing more consistent results than MEECC and MVECC.

An experimental comparison of the fusion methods running times showed no difference between them. While the time complexity of the DTECC fusion function is O(|ℓ|2), while MVECC and MEECC are both O(|ℓ|), this difference is often negligible in practice. In an ensemble of classifier chains, most of the execution time is determined by the more complex computations performed by its member classifiers.

This work made some simplifying choices for making the experiments lighter. The naive Bayes classifier was used for not requiring hyperparameter tuning. In addition, the threshold value of 0.5 was used in all experiments with different data sets. In general, it is better to customize the threshold value for each data set. Therefore, future work should expand the methodology of experiments including other classifiers beyond naive Bayes and performing hyperparameter optimization using a nested cross-validation model selection procedure. Furthermore, the threshold value should also be considered as a hyperparameter to be tuned. The incorporation of the DT method in other multi-label classification methods, such as ensemble of pruned sets (EPS) [33], ensemble of subset learners (ESL) [34] and RAndom k-labELsets (RAkEL) [35], should also be tested. Finally, an analysis of variations of the DTECC method that include more information in each matrix can be proposed and evaluated.