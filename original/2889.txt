Entity typing (ET) is the process of identifying the semantic types of every entity within a corpus. ET involves labelling each entity mention with one or more class labels. As a multi-class, multi-label task, it is considerably more challenging than named entity recognition. This means existing entity typing models require pre-identified mentions and cannot operate directly on plain text. Pipeline-based approaches are therefore used to join a mention extraction model and an entity typing model to process raw text. Another key limiting factor is that these mention-level ET models are trained on fixed context windows, which makes the entity typing results sensitive to window size selection. In light of these drawbacks, we propose an end-to-end entity typing model (E2EET) using a Bi-GRU to remove the dependency on window size. To demonstrate the effectiveness of our E2EET model, we created a stronger baseline mention-level model by incorporating the latest contextualised transformer-based embeddings (BERT). Extensive ablative studies demonstrate the competitiveness and simplicity of our end-to-end model for entity typing.

Access provided by University of Auckland Library

Introduction
Entity typing (ET) is the process of identifying the semantic types of every entity within a corpus. In contrast to named entity recognition, where each token in a sentence is labelled with zero or one class label, ET involves labelling each entity mention with one or more class labels, which are typically arranged in a hierarchy [7]. These fine-grained class labels encapsulate more semantic information than singular labels and allow for entities to be labelled with mutually exclusive types. The results obtained by ET are therefore highly valuable for many downstream natural language processing tasks such as information extraction [12], knowledge graph construction [16], and text mining [15].

Fig. 1
figure 1
Three examples of sentences that have been labelled for multi-label entity typing. The first sentence is from the BBN dataset, while the second and third are from the Ontonotes dataset, discussed in Sect. 4

Full size image
Despite the widespread success of entity recognition, research into effective entity typing is still ongoing. End-to-end entity typing, whereby every token is labelled with zero or more type(s) as shown in Fig. 1, is considerably more challenging than entity recognition as it is a multi-class, multi-label task [17]. This difficulty has resulted in the majority of state-of-the-art entity typing systems [8, 19, 21] assuming that the entities are already known, hence operating only at the mention level. In other words, each entity mention in the dataset has already been identified and is labelled with one or more semantic classes.

There are a number of limitations that arise from performing entity typing at the mention level, as opposed to end-to-end, however. Firstly, the state-of-the-art technique for mention-level entity typing is to train on fixed windows of tokens centred around the entity mention using a three-part Bidirectional LSTM [3], in an attempt to prevent the model from training on irrelevant information. This not only results in the model being highly sensitive to the size of the context window, but also means that it will never be able to incorporate the contextual information outside the window when it is actually important. An end-to-end model employing a bidirectional gated recurrent unit (GRU) would be capable of learning to harness this context effectively via the forget gate [1]. Secondly, to perform entity typing from scratch with a mention-level system one must also train a segmentation model and combine both systems, which is slower and a more complicated pipeline than training one end-to-end system. We show that our proposed end-to-end model (E2EET) is capable of outperforming a mention-level model given the right architecture, while also simplifying the task.

Existing models are also hindered by their input representations. The input representation of each word in state-of-the-art mention-level typing models is generated using context-independent embedding models, such as GloVe [9]. The effectiveness of state-of-the-art context-dependent embedding models, such as BERT [2], in entity typing has not yet been investigated despite its proven success in many other NLP tasks such as named entity recognition. In addition to explicitly learning a context representation using an end-to-end model, we also investigate the effectiveness of contextualised word embeddings on mention-level entity typing.

We therefore carry out extensive ablative studies to demonstrate effectiveness of contextualised embeddings for mention-level entity typing, and show the competitiveness of end-to-end entity typing despite it being a more challenging learning task. We accomplish this by introducing a stronger baseline attention-based mention-level model which embeds the left, right, and mention contexts using BERT, to compare with our end-to-end entity typing (E2EET) model that determines the type(s) of all tokens in a sentence.

We begin this paper with a review of prior work in entity typing in Sect. 2. We then describe our two modelsFootnote1 in detail in Sect. 3. In Sect. 4, we then provide an overview of our experiments. In Sect. 5, we evaluate our stronger baseline mention-level model and show that it outperforms state-of-the-art mention-level entity typing models. We also show that E2EET is effective on clean datasets and is capable of outperforming mention-level entity typing models despite not knowing which tokens in each sentence are entities apriori.

Related work
Fig. 2
figure 2
A taxonomy of entity extraction, which shows the current state of the literature in entity typing. Note that NFGEC appears under two categories as it is both a three-part Bi-LSTM model as well as a hierarchy-aware model. Our two models are emphasised in bold

Full size image
In this review, we focus our attention on state-of-the-art methods for entity typing. In Fig. 2, we present a taxonomy of entity extraction, showing the distinction between named entity recognition, mention-level entity typing, and end-to-end entity typing. Each of the following models reviewed in this section has been categorised within this taxonomy.

End-to-end entity typing
Initial entity typing research treated entity typing as a multi-class, multi-label classification problem, i.e. identify the type(s) of every entity within a document. The fine-grained entity recognition (FIGER) [7] model follows a pipeline-based approach to perform end-to-end entity typing: it first identifies the entity mentions via a segmentation step and then predicts a list of class labels (types) associated with each mention. The segmentation is performed using a conditional random field (CRF) trained on a variety of handcrafted features such as token length and contextual bi-grams. Label prediction is performed using a multi-layer perceptron. While it differs from other state-of-the-art entity typing systems in that it treats entity typing as an end-to-end task, FIGER’s reliance on handcrafted features to perform segmentation makes it unfeasible for domains and applications where these features are not readily available.

Mention-level entity typing
More recent research focuses on mention-level entity typing. In contrast to the two-staged pipeline of FIGER, which performs entity segmentation (i.e. entity recognition) followed by label prediction, mention-level models are trained on already-segmented data [8] and aim to predict the type(s) of each entity mention given its context.

State-of-the-art entity typing models typically employ a three-part bi-directional long short-term memory model (Bi-LSTM). The left, right, and mention contexts are each fed through a bi-LSTM layer to obtain an encoded representation, which is then decoded and fed through a linear layer to obtain a set of labels for the corresponding mention. This architecture was introduced as part of the hybrid neural model (HNM) [3], which comprises two components: a recurrent-based mention model that obtains a vector representation of the entity mention given its context and a context model that generates a single vector for both the left and right contexts of the mention. The output from the mention model and context model is concatenated and fed through a softmax layer to obtain a probability distribution over all possible types. The type with the highest probability is selected as the prediction.

Multi-instance entity typing from corpus (METIC) [19] also employs a three-part Bi-LSTM. Once the input has been embedded via GloVe [9] and passed through the Bi-LSTM layers, the output is concatenated and two constraints form the basis of an integer linear programming model which is applied to the output of the final dense layer. The type disjointness constraint ensures that an entity is not labelled as two mutually exclusive types. The mutual exclusivity of each type are determined via an external knowledge base. The type hierarchy constraint ensures that an entity is not labelled as a certain type if it is not also labelled as that type’s parent category.

In contrast to other mention-level entity typing systems, automatic fine-grained entity typing (AFET)  [11] does not employ a bi-LSTM. It instead introduces a novel, heuristic-based method to separate clean and noisy mentions, while also introducing hierarchy-based partial label embeddings to improve performance. AFET takes advantage of the noisy labels in the dataset; mentions are separated into clean and noisy sets depending on whether their ground truth labels form a single path in the category hierarchy, i.e. are not mutually exclusive. The loss function of the model differs for each training example depending on whether it is from the clean or noisy set. AFET notably relies on handcrafted features (such as POS tag and Brown Cluster), unlike other systems, which limits its functionality on datasets that have not been labelled by hand.

Recent work has investigated the effectiveness of incorporating the category hierarchy into the classification stage. Neural architectures for fine-grained entity type classification (NFGEC) [14] also uses a three-part bi-LSTM model and introduces a hierarchical label encoding that improves classification performance. The final weights of the network are multiplied by an automatically built one-hot hierarchy matrix. This provides the facility to share weights between commonly seen parent classes (such as person) and their less commonly seen children (e.g. person/artist/actor), improving F1 scores on rare subclasses.

Murty et al.  [8] extend the incorporation of the entity hierarchy by combining a CNN-based entity typing model along with a hierarchical loss function. The sentence encoder model takes positional embeddings and word embeddings as input, and the output is concatenated with the mean of the mention embeddings. This vector is fed through a multi-layer perception to obtain the predictions vector. The hierarchical loss function employs knowledge graph embedding in the form of real and complex bilinear maps to embed the hierarchy. The loss function aims to minimise the binary cross entropy across both predicted class labels and hierarchy embeddings.

Another line of research in entity typing aims to capitalise on the wide availability of external knowledge graphs to more accurately predict entity types. ERNIE (enhanced language representation with informative entities) [21] identifies a single entity mention within each input sentence using TAGME [4] and then determines the type(s) of that entity via a transformer-based information fusion model. The training objective of the underlying language model is similar to the transformer-based language model, BERT [2], which aims to predict a randomly masked word in a given input sequence as well as the next sentence following the given input sentence. Rather than masking any word at random, only the entities are masked in the input sequence, which helps the model to learn the underlying language representation.

Summary
In summary, FIGER is a pipeline-based entity typing system that is heavily reliant on feature collection. HNM, METIC, AFET, ERNIE and the system proposed by Murty et al. are all deep learning-based mention-level entity typing systems. HNM, METIC and NFGEC use a three-part bi-LSTM for representation learning, while ERNIE is based upon a language model using a transformer architecture. AFET, NFGEC and the system proposed by Murty et al. integrate the entity hierarchy into the loss function. ERNIE requires access to a knowledge graph in order to identify the entity mentions, while AFET relies on handcrafted features. The most notable omission from existing state-of-the-art entity typing systems is the inability to perform true end-to-end entity typing—they all operate at the mention level and often require external knowledge or handcrafted features.

Entity typing models
This paper introduces two entity typing models: a mention-level model, which determines the type(s) of an entity given an entity mention and its surrounding context, and an end-to-end model (E2EET), which determines the type(s) (if any) of all tokens in a sentence. We begin this section by providing an overview of the embedding layer that is common between both models, and then explain each model in detail.

The embedding layer plays a crucial role in our models, allowing for a context-dependent, deep representation of input tokens. To facilitate such a representation, we use BERT [2]. BERT is based upon the encoder stack of the bidirectional transformer model [18], an encoder–decoder model structure that combines feed-forward layers with a multi-headed attention mechanism.

The input sequences to BERT are arbitrary spans of contiguous text that have been padded with [CLS] and [SEP] tokens to denote the start and end of a linguistic sentence, respectively. The input representations of each item in a sequence are the sum of the positional embeddings, segmentation embeddings, and token embeddings. In contrast to other state-of-the-art context-dependent embedding models, such as ELMo [10], BERT learns deep bidirectional representations by performing a procedure known as the “masked language model”. For every input sentence to the model, some number of terms are masked at random. The training objective of the model is to predict the vocabulary index of the original terms that were masked.

BERT embeddings allow our models to incorporate valuable contextual information in the embedding layer, as opposed to being learned via a recurrent layer as is common in existing entity typing systems. The embeddings generated by BERT and fed into our model are context-dependent, meaning the embeddings of each word are generated with respect to its surrounding context. Polysemous words are embedded according to their canonical meaning, providing a richer representation than the context-independent embedding models that are currently used in many state-of-the-art entity typing models.

An important distinction between BERT and other embedding models is that it is trained at the “wordpiece” level (also known as Byte Pair Encoding) [13], as opposed to the word level. A single unknown token must first be tokenised into pieces prior to being fed through the BERT model. For example, “Johanson” becomes “Johan” “##son”.

Fig. 3
figure 3
The architecture of the mention-level model. Circles with horizontal lines through them denote that the weights are averaged, while the crossed circle indicates concatenation

Full size image
Mention-level model
The attention-based mention-level model, as shown in Fig. 3, predicts the label(s) of a given entity mention and its surrounding context. It accomplishes this using a three-part context model, consisting of the left, right, and mention context, inspired by the state-of-the-art mention-level systems discussed in Sect. 2. However, our model does not employ a recurrent neural network and instead uses two feed forward layers: one to learn an encoded representation of the combined left, right, and mention contexts, and another to map this encoded representation back to the label space.

Context vectors
We first build the left and right context windows by taking W wordpieces to the left and right of the mention, respectively, where W is a fixed context window size. For the mention context window, we take the first W wordpieces of the mention. If any vector is not of length W, it is padded with [PAD] tokens. If its length is greater than W, excess wordpieces are trimmed. Each of the left, right, and mention context windows is then encoded via a pre-trained BERT model to obtain three embedding matrices of size W×d, where d is the embedding dimension. We then take the average across each of these matrices, yielding three vectors of size d. These three vectors are hereby denoted as the left, right, and mention context vectors cl, cr, and cm.

Attention mechanisms
The mention-level model may be augmented with one of two attention mechanisms: scalar and dynamic.

The scalar attention mechanism learns the extent to which each context (left, right, and mention) is important when predicting the labels of each mention. The weights of each context are then multiplied according to their relevance to the task. To do this, a scalar value ai˜ is learned for each context vector cl, cr, and cm. We normalise the attention weights using the softmax operation so that they sum to 1 (here, C represents the contexts {l,r,m}). The weights of each layer are multiplied by the corresponding attention value and are then concatenated (⊙) to form cc:

ai˜=eai∑Ci=1eai
(1a)
cl=cl⋅a1˜
(1b)
cr=cr⋅a2˜
(1c)
cm=cm⋅a3˜
(1d)
cc=cl⊙cr⊙cm
(1e)
The three attention weights are applied to every mention regardless of the mention itself. This results in low complexity but has the downside of assuming that every mention will benefit from the same attention weights. For example, if the attention value is high for the left context and low for the mention and right contexts, and a mention has no left context (i.e. it is at the start of the sentence), the predictions for this particular mention may be adversely affected by the attention mechanism.

Fig. 4
figure 4
The relevant components of the mention-level model that are changed when using the dynamic attention mechanism

Full size image
In light of this issue, we propose a dynamic attention mechanism, as illustrated in Fig. 4. Rather than learning one weight per context layer, the dynamic variant uses a much simpler feed-forward network to assign weights to each context layer based upon the mention context. The inputs to this layer are the averaged embeddings across each wordpiece in the mention context. The output is a vector of three weights corresponding to the left, right and mention context, which are softmaxed and applied to each layer hl, hr and hm.

The dynamic attention mechanism allows for the predictions of polysemous mentions to be more heavily influenced by their surrounding context, while also reducing irrelevant contextual information for non-polysemous mentions. For example, given the mention “Apple” (which could be a company or fruit depending on the context), the normalised weights of the attention layer’s output might be [0.45, 0.45, 0.1] for the left, right, and mention contexts, respectively. Given another example, where the entity types may be easily inferred from the mention itself (such as “Barrack Obama”), the normalised weights might be [0.1, 0.8, 0.1].

Hidden and output layers
After being multiplied by the attention weights, the three context vectors are concatenated to form the combined representation cc. This vector is fed through a linear layer, followed by a ReLU activation function. The outputs of this layer are fed through one final layer to obtain the output vector, x, which contains one weight corresponding to each label ∈N, where N is the set of all labels.

Loss function
After performing the sigmoid function to normalise the weights to between 0 and 1, the loss of the model is calculated using binary cross entropy :

ln=−ynlog(y′n)−(1−yn)log(1−y′n)
(2a)
loss=∑n∈Nln|N|
(2b)
Here, yn∈{0,1} is the correct label of the class of index n, {y′n∈R|0≤y′n≤1} is the prediction score associated with the class of index n, and N is the set of labels.

Prediction layer
Given a set of prediction weights y′ across each label n, the prediction layer outputs 1 when y′n>0.5 and 0 when y′n≤0.5.

One minor difference between the mention-level model and the end-to-end model is that in mention-level typing, each entity mention is guaranteed to have at least one label. To address this issue we adjust the prediction layer of our mention-level model to output its highest-weighted label in the event that no prediction weights are >0.5.

Fig. 5
figure 5
The architecture of the end-to-end model (E2EET). The remaining wordpieces at either side of the four example wordpieces are not included in the diagram for brevity

Full size image
End-to-end model (E2EET)
The end-to-end entity typing model (E2EET), as shown in Fig. 5, predicts the type(s) of every token in a sentence. In contrast to the mention-level model, it does not require the entities to be segmented and can operate on a dataset composed purely of raw text. Rather than taking an entity mention and its context as input, it takes an entire sentence as input and outputs a set of zero or more labels for each token in the sentence.

E2EET is novel when compared to existing entity typing approaches as it is the first entity typing model to operate completely end-to-end. This allows for the context of the entire sentence to be taken into account when predicting the label(s) of each token. It also prevents the need for a separate entity segmentation step prior to performing entity typing.

The model is similar in architecture to our mention-level model, but uses a bidirectional gated recurrent unit (GRU) [1] instead of a feed-forward network. This allows for both forward and backward contexts (independent of window size) to be taken into account when predicting the label(s) of each token.

Gated recurrent units are a similar recurrent-based architecture to the long short-term memory model (LSTM) [5]; however, they are typically faster to train as they only contain two gates. The first is the update gate, which functions similarly to the LSTM’s memory cell. This gate controls how much information from the previous hidden state is allowed into the current state, by multiplying all information by a value between 0 and 1. The reset gate, on the other hand, allows the hidden state to discard information found to be irrelevant.

The update gate z and reset gate r are calculated as follows, where U and W are weight matrices which are learned [6]:

zt=σ(Wzit+Uzht−1+bz)
(3a)
rt=σ(Writ+Urht−1+br)
(3b)
The activation of the unit is then calculated as follows:

h′t=ϕ(Wit+U(rt⊙ht−1))
(4a)
ht=zt⊙ht−1+(1−zt)⊙h′t
(4b)
Loss function
The loss of E2EET is calculated using binary cross-entropy in the same manner as the mention-level model as per Eq. 2. However, rather than averaging across a single set of label predictions, the loss of E2EET is averaged across the predictions of all wordpiece tokens T in the current batch:

loss=∑Tt=1loss(t)|T|
(5a)
Concatenation layer
As opposed to many embedding models, BERT operates at the wordpiece level. This is not an issue for the mention-level model because it predicts only one set of labels per mention, regardless of how it is tokenised. It presents complications for an end-to-end model, however, whose aim is to classify each token in a sentence into a set of labels. The outputs of the model must be a set of labels per token, not per wordpiece. In order to accommodate this, we incorporate a special concatenation layer at the end of our model.

During the data loading stage, we construct a map between token indices and their corresponding wordpiece indexes in each sentence as described by [2] in the BERT documentation.Footnote2 For example, given that the sequence of tokens ["John", "Johanson", "’s", "house"] is tokenised to ["[CLS]", "john", "johan", "##son", "’", "s", "house", "[SEP]"], the map, hereby denoted as m, for this sentence is constructed as [1, 2, 4, 6].

The concatenation layer works as follows. Given a set of predictions y′ for each wordpiece in a sentence, the layer takes the average of every prediction that corresponds to the same token according to m. In the above example, the four token predictions are based on the predictions corresponding to the first wordpiece ("john"), the average of the second and third wordpiece ("johan" and "##son"), the average of the fourth and fifth wordpiece ("’" and "s"), and the sixth wordpiece ("house"), respectively. This allows for the model to output token-level predictions despite being trained at the wordpiece level.

Entity segmentation
The output of E2EET is a set of entity labels for each token in the input sequence. The set of entity mentions appearing in the sentence can be obtained via contiguous sequences of tokens with the same label prediction(s). For example, given the sentence Barrack Obama spoke to Michelle and corresponding labels [person, person/president], [person, person/president], [], [person], the entity mentions will be Barrack Obama and Michelle.

Fig. 6
figure 6
Five examples of sentences in the Ontonotes dataset containing contiguous entity mentions of the same type. These mentions are outlined with a border

Full size image
E2EET is not capable of segmenting contiguous entity mentions of the same class, as shown in the examples in Fig. 6. For example, if there was another person and president immediately following Barrack Obama, it is not possible to identify it as a separate entity and the entire span would be labelled as person. In named entity recognition, models predict Beginning (B-) and Inside (I-) tags to resolve this issue, but for our task of multi-label end-to-end entity typing we found that the trade-off of doubling the size of the label set was not worth the benefit of being able to segment contiguous, same-class entities. Doubling the size of the label set leads to decreased accuracy and increased training time as it substantially increases the complexity of the model. Furthermore, contiguous, same-label entity mentions are typically uncommon; in the three datasets evaluated in Sect. 5, it was found that only 0.92% of all documents contained at least one instance of contiguous, same-class entity mentions.

Experiments
Datasets
Table 1 The number of sentences in the original datasets (used to evaluate our mention-level model) and modified datasets (used to evaluate the end-to-end model)
Full size table
We evaluate our mention-level model on the benchmark datasets provided by [11]: Wiki, Ontonotes, and BBNFootnote3. We use a portion of the training datasets as validation sets (434 for Wiki, and 10% for Ontonotes and BBN). These datasets are summarised in Table 1 in the Original column.

Initial data exploration of the training sets of the Ontonotes and BBN datasets found that the data contained a high proportion of incorrect labels. The testing sets, however, appear to be free from error as a result of being manually annotated [14]. We therefore created our own versions of these datasets, hereby known as the “modified” datasets and prefixed with an “M”. The datasets are summarised in Table 1 in the Modified column,

The training set of the Wiki dataset is relatively clean when compared with the training sets of the original Ontonotes and BBN datasets. However, we found that due to the complexity of our end-to-end model, it was necessary to trim the large Wiki dataset in order for it to fit in memory. We therefore constructed the M-Wiki dataset by taking the first 50,000 documents of the original  1.5 million document training set as the new training set and the following 434 as the new validation set. The test set is the same as in the original dataset. For the M-Ontonotes and M-BBN datasets, the training sets comprise the first 80% of the test data. The validation sets comprise the following 10%, and the test sets comprise the remaining 10%.

Model parameters
After parameter tuning, we found that the best performance on the development set for both of our models was achieved with a learning rate of 0.0001, a hidden dimension size of 768, and 0.5 dropout prior to the final layer. The models were optimised using ADAM. The batch size was 100 for the mention-level model and 10 for the end-to-end model. For the mention-level model, we used a context window size of 10 for the left, right, and mention contexts. The end-to-end model was trained with a max sequence length of 100, allowing for 99% of the data to be included without dramatically increasing training time.

Embedding techniques
In order to evaluate the effectiveness of the BERT embeddings in our models, we evaluate our end-to-end model with four different embedding techniques. Uniform, the baseline, assigns a uniform distribution of embedding weights for each token. The GloVe [9] embeddings are pre-trained on Wikipedia 2014 + Gigaword 5.Footnote4 The Word2Vec embeddings are pre-trained on the Wikipedia corpus.Footnote5 The embedding dimension of each of these techniques was 300. The BERT embeddings are generated from the pre-trained BERTBASE,Cased model,Footnote6 which provides embeddings of dimension 768. We used Bert-as-serviceFootnote7 to embed the sentences per-batch. We did not fine-tune BERT on our datasets as we found it did not provide a performance improvement.

Evaluation metrics
We evaluate our model using three standard metrics for entity typing systems: strict accuracy, Loose Macro, and Loose Micro score. The formulas for these metrics, as first noted by [7], are as follows.

We denote T as the set of all entities, P as the set of predicted labels for each entity, te as the ground truth labels for entity e, and te^ as the prediction set for e. When evaluating the end-to-end model baseline in Sect. 5.2.1, T is the set of all tokens, and P is the set of predicted labels for each token and te and te^ are the ground truth labels and prediction set for token e, respectively.

Strict Accuracy:

precision=(∑e∈P∩Tδ(te^=te))/|P|
(6a)
recall=(∑e∈P∩Tδ(te^=te))/|T|
(6b)
Loose Macro:

precision=1|P|(∑e∈P|te^∩te||te^|
(7a)
recall=1|T|(∑e∈T|te^∩te||te^|
(7b)
Loose Micro:

precision=∑e∈P|te∩te^|∑e∈P|te^|
(8a)
recall=∑e∈T|te∩te^|∑e∈T|te^|
(8b)
The F1 scores of each metric are then calculated as follows:

F1=2×precision×recallprecision+recall
(9)
Strict Accuracy only considers the prediction of a token correct when the set of predicted classes matches the set of ground truth classes exactly. Loose Macro calculates the scores for matching subsets at the entity level, individually for each entity mention, whereas Loose Micro computes the score at the corpus level, and the score is averaged across all entities. Loose Macro tends to be penalised when new unseen categories appear in the test set and is therefore sensitive in unbalanced datasets. In such cases, Loose Micro is a fairer metric.

Baseline systems
We compare our mention-level model to the state-of-the-art systems evaluated in [19]:

AFET [11]: A neural mention typing model that uses handcrafted features.

HNM [3]: A hybrid neural model for mention typing.

HNM-ML: As above but adapted for multi-label typing.

METIC [19]: A neural model that uses a combination of bi-LSTMs and integer linear programming.

Table 2 A comparison of the effects of attention on our mention-level (ML) model against state-of-the-art mention-level typing systems
Full size table
Results
Mention-level model performance—a stronger baseline
Our first set of investigations is to determine how our mention-level model’s performance compares to existing systems. We also investigate the effectiveness of the proposed attention mechanisms.

Table 2 shows the results of our model when compared to state-of-the-art mention-level models, with results for existing systems supplied by [19]. AFET, which relies on handcrafted features, is highlighted in grey.

Our mention-level model outperforms METIC, the top-performing system that does not rely on handcrafted features, in every experiment except the micro-F1 metric on the Wiki dataset. We attribute the success to the combination of the context-dependent BERT embedding vectors and the three-part context model. In contrast to existing state-of-the-art systems, our model is able to encapsulate contextual information via the context-dependent embeddings provided by BERT’s transformer-based architecture.

Despite not relying on handcrafted features like AFET does, our mention-level model mostly outperforms AFET on the Wiki and BBN datasets. However, it performs substantially worse on the Ontonotes dataset. This is most likely due to the high quality of the handcrafted features present in Ontonotes which help to boost AFET’s performance.

Table 3 The results of our E2EET model on the modified Wiki, Ontonotes and BBN datasets using various embedding techniques
Full size table
Table 4 A comparison between our mention-level (ML) model (with three different attention mechanisms), and E2EET on the modified Wiki, Ontonotes and BBN datasets. Top scores (prior to rounding) are in bold. Misleading metrics are in grey. “Attn.” stands for “attention”, and “Dyn.” stands for “dynamic”
Full size table
The attention mechanisms (scalar and dynamic) generally had very little impact on performance. Additionally, despite its complexity, the dynamic attention mechanism was often outperformed by the scalar variant. It was found during training that the dynamic attention mechanism performs best on the validation sets (which comprise 10% of the training data), but there was a significant difference (0.2) in F1 scores between the validation and test sets. The most likely cause of this phenomenon is that the training and testing sets are too distinct from one another, leading to rapid overfitting in more complex models such as attention-based models. This is supported by the fact that the training sets of BBN and Ontonotes were automatically labelled, whereas the testing sets were manually labelled [14].

The results clearly show that our mention-level entity typing model outperforms all current state-of-the-art techniques that do not rely on handcrafted features. It is also highly competitive with AFET, a top-performing system that uses handcrafted features. Our model consistently outperforms AFET and other entity typing systems on the M-BBN dataset for two criteria (macro-F1 and micro-F1).

End-to-end entity typing (E2EET) performance
Baseline performance
To evaluate E2EET, we test the model after its embedding layer has been initialised with four different embedding techniques. Table 3 shows the results of E2EET on the modified Wiki, Ontonotes and BBN datasets. Here, the strict accuracy, macro-, and micro-F1 scores are calculated with respect to the model’s predictions of every single token in the corpus.

The BERT embeddings significantly outperform the other embedding techniques on every dataset. It is clear that the context-dependent embeddings provided by the BERT model are highly effective when used to support an entity typing model.

The results indicate that the accuracy and macro-F1 metrics used for mention-level models do not accurately reflect the performance of an end-to-end model. The accuracy is extremely high because the vast majority of tokens are not entities, and the model successfully predicts no labels for these tokens. Macro-F1 is similarly misleading, being consistently low due to the scores being divided by the total number of tokens. The only useful metric for evaluating E2EET appears to be micro-F1, which disregards non-entities by dividing by the sum of ground truth labels for each token.

Disregarding the misleading accuracy and macro-F1 scores, E2EET performs well on the M-Ontonotes and M-BBN datasets. It did not fare well on the M-Wiki dataset, which is considerably noisier. Overall, the results indicate that the model is capable of performing end-to-end entity typing, but future research regarding end-to-end models should investigate and devise more suitable evaluation metrics.

Comparison with mention-level model
Table 4 shows a comparison between our mention-level and E2EET when trained and evaluated upon the modified datasets. Here, E2EET is using BERT embeddings and is evaluated using the same F1 calculations as the mention-level model, i.e. the scores are only calculated across entities as opposed to across all tokens. This means that any non-entities that are incorrectly labelled as entities by E2EET are ignored.

E2EET is competitive with the mention-level model on the relatively clean M-Ontonotes and M-BBN datasets, even outperforming the mention-level model on M-BBN. This is surprising considering E2EET has a vastly more difficult training objective and does not know which tokens are entity mentions. The most likely explanation for this result is that the context of the entire sequence plays a pivotal role in the model’s success, particularly in M-BBN. It allows E2EET to more effectively classify each token than the mention-level model which was trained on smaller context windows.

Another noticeable result is that, for the mention-level typing models, there is a clear relationship between the complexity of the attention model and the overall performance. This is in contrast to the results on the original datasets (Table 2), where the attention mechanism had little impact on performance. The dynamic attention model in particular excels on the modified datasets, which are significantly cleaner than the original datasets as a result of being taken from the hand-labelled test set of their respective original dataset. The dynamic attention mechanism is clearly most effective when the dataset is clean and when there is consistency between the training and testing sets.

Overall the results of the comparison show that E2EET is highly competitive with the mention-level model as a result of its ability to incorporate document-level context. It performs well on clean datasets where there is little disparity between the training and evaluation sets and provides a strong foundation for future research into entity typing.

Conclusion
In this paper, we have carried extensive ablative studies on three datasets demonstrating the effectiveness of contextualised embeddings for mention-level entity typing and have shown the competitiveness of our proposed end-to-end system for entity typing. Our attention-based mention-level model embeds the left, right, and mention contexts using BERT and employs two novel attention mechanisms in order to predict the labels associated with each entity mention. Our end-to-end model (E2EET), on the other hand, effectively determines the type(s) of all tokens in a sentence. Results show that our mention-level model outperforms state-of-the-art mention-level entity typing models. Our end-to-end model performs well on clean datasets and is capable of outperforming the mention-level model despite not knowing which tokens in each sentence are entities.

In future, we plan to run tenfold cross-validation to ensure statistical significance in the experiments. It would be interesting to investigate the effectiveness of other recent context-dependent embeddings such as XLNet [20], transformers to replace Bi-GRU for context representation learning, and to incorporate hierarchical encoding techniques to improve prediction accuracy. Finally, it would be useful to further investigate ways to allow E2EET to deal with contiguous entity mentions of the same type without the trade-off of decreased accuracy and increased training time.