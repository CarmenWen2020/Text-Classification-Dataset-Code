Due to introduction and increased availability of Electronic Health Records (EHRs), disease prediction has recently gained immense research attention and achieved impressive progress. Existing methods are based on RNN-like architectures, which treat every disease equally, and learn the representations from medical knowledge. However, strong structural information among diseases is ignored in these methods. In this paper, we introduce a novel Path-based reasoning model for self-AttentIonal Disease prediction (ProAID), which utilizes medical paths extracted from patient EHR and external medical knowledge bases to augment the latent interaction between diseases and learn highly representative patient embeddings. By explicitly incorporating medical paths, ProAID effectively generates embeddings that capture the hierarchical information of diseases and learn effective representations of a patient based on the historical patient admission sequences in her/his EHRs to allow accurate disease prediction for the next hospital admission. Extensive experiments on public medical datasets show that ProAID achieves better performance than the compared methods, which indicates the effectiveness of the proposed model.

Access provided by University of Auckland Library

Introduction
Electronic Health Record (EHR) retains a significant importance in the development of data-driven healthcare research, which belongs to knowledge engineering [1]. Due to the prosperous advances in information technology [2, 3] and machine learning [4,5,6], the sheer volume of EHRs is becoming manageable, and analyzing EHRs with machine learning and data mining techniques is becoming an emerging research direction to fulfill the goal of improving health care services [7]. For example, many studies focus on the phenotyping task which aims to identify phenotypes from the patient EHR corpus [8, 9]. Some studies leverage deep learning approaches for representation learning of medical codes to improve the performance of various healthcare domain tasks [10, 11].

Predicting the future health conditions of patients based on their historical EHR is one of the most important research tasks in this domain. Machine learning techniques have proved effective and successful for disease diagnosis prediction [12,13,14]. Generally, EHR data consist of sequences of admissions over a specific period of time, and each admission record contains multiple medical codes, such as diagnosis, medication, and procedure codes Ma et al. [13]. To predict the future diagnosis, existing studies usually embed each admission information into a low-dimensional space and represent the patients’ medical history as a sequence of latent vectors. Afterward, the sequences are fed into RNNs [15] to predict the diagnosis for the next admission. Apart from this, knowledge-discovery-based methods are also proposed to discover novel patterns in EHR data, which resort to external medical knowledge to enhance the representation of medical codes. These techniques leverage RNN-based methods to handle the sequences of admissions over time to improve the prediction accuracy [16,17,18], which integrate available patient data with clinical guidelines to assist the physician at the point of care [19].

However, the above-mentioned RNN-like architectures suffer from several problems: First, the RNN-like architectures require a large amount of EHRs to effectively optimize all model parameters and thoroughly capture the complex patterns within the data, hindering existing models from generating accurate predictions when there is no sufficient EHRs. Moreover, most existing methods apply attention mechanisms to learn the hierarchical information of medical codes and pre-train the medical codes based on skip-gram or CBOW [20], whilst ignoring the strong structural information among diseases. Last but not least, these methods have achieved high accuracy but they suffer from complicated interpretation. Many studies [16,17,18] manage to import the medical knowledge to improve the interpretability, but the process of modeling the temporal sequences is still hard to be trusted.

Inspired by recent advanced self-attention mechanisms that have successfully been applied in natural language processing tasks and have gained impressive performance [21,22,23], we model mutual effects among the admissions instead of relying on the costly RNN-based architectures. As a result, we learn the temporal dependencies from patient EHR with better prediction performance and interpretability.

To further model the interactions between diseases in a complex medical information graph, we design a path-based reasoning mechanism over the medical information graph inspired by the notion of meta-path. A meta-path is a sequence of relations between object types, which defines a new composite relation between its starting type and ending type [24, 25]. The meta-path has been applied in item recommendation tasks [26,27,28]. Conveniently, using meta-paths, we define the medical paths, linking different diseases and medical codes through paths in medical knowledge graphs. The model learns from these paths, captures the complex relationships between the most relevant diseases in the medical knowledge and fuses this information with patient EHR to improve the accuracy and interpretability of prediction. Some examples of the medical paths are given in Fig. 1.

Fig. 1
figure 1
Example of Medical Paths. The left is a disease taxonomy. The dotted circles (C4, C6, C7, C8, C9, C10) are ancestor nodes in the disease taxonomy, and the solid circles (C1, C2, C3, and C5) are leaf nodes, which are the medical codes matched with patient EHRs. The right is medical paths sampled from the disease taxonomy. All medical paths start from a leaf node, through the tree structure of disease taxonomy, ending at another leaf node

Full size image
In a nutshell, in this paper, we introduce the Path-based reasoning model for self-AttentIonal Disease prediction (ProAID), an innovative model based on a self-attentional mechanism for disease prediction, which utilizes medical paths extracted from external medical knowledge bases to augment the latent interactions among diseases and learn expressive patient embeddings. The self-attentional reasoning network is also able to learn effective representations of a patient based on the historical patient admission sequences in her/his EHRs. By explicitly incorporating medical paths extracted for diseases in each admission, the proposed path encoder effectively generates embeddings that capture the hierarchical information of diseases, which is fused into admission representations to allow for accurate disease prediction for the next hospital admission.

The contributions of this paper are summarized as follows:

A novel method, ProAID, is proposed to exploit medical paths, which are sampled from disease taxonomy, to enrich the representations of admissions of a patient with the hierarchy of diseases.

Extensive experiments on two real-world EHR datasets have been conducted, and the results show that the proposed model outperforms all baseline models in the disease prediction task, which indicates the effectiveness of the proposed method.

The rest of this paper is organized as follows. The related work is introduced in Sect. 2. Sections 3 and 4 present the preliminaries and the proposed model, respectively. The experimental results and discussion are presented in Sect. 5. Finally, Sect. 6 concludes this paper.

Related work
Electronic Health Record (EHR) is one of the significant basis for the development of data-driven healthcare research. Patient EHR data consist of health information, such as patient demographics and medical codes. Predicting the future health information of patients from the historical EHR is a core research task in the healthcare domain. The key challenge is to model the temporality and high dimensionality for accurate prediction and interpretability.

The performance of classic machine learning methods may degrade in the presence of complex and nonlinear relationships among observed measurements. For instance, Che et al. [29] formulate a prior-based regularization framework for discovery and detection of different length patterns in clinical time series [30]. Choi et al. [31] propose a multivariate context-sensitive Hawkes process for prediction of a possible related disease for a patient in the future. However, these methods are computationally expensive and make strong assumptions about the data generation process which might not be valid for EHR data.

To leverage large historical data in EHR, Choi et al. [32] develop DoctorAI, a temporal RNN model assessing the history of patients to make predictions. However, DoctorAI represents the medical codes as multi-hot label vectors, while medical concepts contain rich latent relationships that cannot simply be represented by one-hot coding [33]. For this reason, Choi et al. [10] further explore multi-layer representation learning for medical concepts and propose Med2Vec, wherein the predictive performance is improved and clinical interpretation is facilitated.

RNN-based models have been successfully applied in diagnoses prediction; however, the high-quality prediction is achieved at the cost of complicated interpretation. such RNN-based approaches also suffer from a severe performance drop when the length of the sequences becomes too large to learn long-range dependencies. To address this issue, some research studies leverage attention mechanism to learn medical representations for sequential diagnoses prediction tasks [12, 16]. [13] propose Dipole employing bidirectional RNNs to predict future health information for patients. [17] further propose the KAME, which learns reasonable embeddings for both diseases and general knowledge to improve the prediction accuracy. [18] propose CAMP using co-attention memory networks for diagnosis prediction. Besides, most existing works focus on learning disease patterns from longitudinal patient data but pay little attention to the disease progression stage itself. To fill the gap, [34] propose a stage-aware neural network (StageNet) model to extract disease stage information from patient data and integrate it into risk prediction.

Conclusively, existing studies mostly focus on learning from the sequences of patient admissions and modeling the temporal features from EHR. Insufficient data and complicated interpretability are the key issues in these models. Although some studies use attention mechanism to learn disease representations from the medical knowledge, however, in these models, every disease is treated equally and strong structural information among diseases is ignored. The issues pointed above, if resolved, could result in improvement in prediction performance and interpretability of these learning models.

Preliminaries
In this section, we define key notations and term used throughout this paper. We also describe the self-attention block [21] exploited in the proposed model, At the end of the section, we depict the problem of disease prediction upon upcoming admission of a patient.

Definition 1 (Patient Historical Records) We denote the set of medical codes from the EHR data by {c1,c2,..c|C|}, where |C| is the number of medical codes. In this paper, |P| denotes the number of patients in the EHR data. In the EHR data, a patient instance u is represented as a sequence of hospital admission records T(u)=[V1,V2,…,V|T(u)|], where |T(u)| is the number of hospital admissions (a.k.a. visits) of the patient u. Each hospital admission Vt is denoted by a |C|-dimensional binary vector xt∈{0,1}|C|, where the i-th element is 1 if Vt contains the medical code ci.

Definition 2 (Disease Taxonomy) We denote disease taxonomy as GD to represent the hierarchy of medical concepts in the form of hypernym–hyponym relationship, where concrete diseases in C form the leaf nodes. Disease taxonomy GD is represented as a tree having nodes from a set D=C+C′, where C′ represents the set of all non-leaf nodes and C′={c|C|+1,c|C|+2,…,c|C|+|C′|}. A parent node in GD represents a related but more general concept over its child nodes. We build GD by using well-organized taxonomies of diseases (e.g., ICDFootnote1 or CCSFootnote2).

Definition 3 (Medical Path) Inspired by the path-based reasoning networks in the recommendation systems [26,27,28], we introduce the medical paths for diseases. Generally speaking, a medical path ρ is defined as a path in the form A1−→R1A2−→R2⋯−→RlAl+1, which describes a composite relation R1∘R2∘⋯∘Rl between objects A1 and Al+1, where “∘” denotes the composition operator on relations. In the context of this paper, a medical path starts from a disease (a leaf node in the disease taxonomy GD), through the tree structure of disease taxonomy, ending at another disease (another leaf node). The ending disease of a medical path is randomly sampled from the most related diseases of the beginning node. Some medical paths are shown as examples in Fig. 1. The ending disease of a medical path is randomly sampled from the most related disease of the beginning node.

Fig. 2
figure 2
The workflow of our medical path based self-attentional reasoning model for disease prediction

Full size image
Self-Attention Block. Recently, the attention mechanisms [21, 22] have been applied successfully in natural language processing tasks with impressive performance. We borrow this idea to model mutual effects among the admissions and learn the temporal dependencies from patient EHR to achieve better prediction performance and interpretability. In this paper, we develop medical path encoder and temporal admission reasoning network by stacking multiple self-attention blocks. A self-attention block consists of two components: a multi-head self-attention layer and a position-wise feed-forward network. Specifically, a multi-head self-attention layer is defined as:

MH(Xl)=[head1,head2,…,headh]WO,
(1)
headi=Attention(XlWQi,XlWKi,XlWVi),
(2)
where Xl is the input for the l-th layer. The initial input embedding of the first layer is denoted as X0. The WO∈Rd×d, WQi∈Rd×dh, WKi∈Rd×dh, and WVi∈Rd×dh are the learnable parameters for each attention head. The attention function is defined as follows:

Attention(Q,K,V)=softmax(QKTd/h−−−√)V,
(3)
where d is the dimension of input vector and h is the number of attention heads.

In addition to the attention layer, each self-attention block contains a fully connected feed-forward network, which is applied to each position separately and identically. The network contains two linear transformations with a ReLU activation in between. The network is defined below:

FFN(z)=(ReLU(zW1+b1))W2+b2,
(4)
where z∈Rd is a position vector in the output of the previous attention layer. W1∈Rd×da, b1∈Rda, W2∈Rda×d, and b2∈Rd are trainable parameters, and da is the dimension of intermediate embeddings of the FFN layer. In addition, each component (self-attention layer, FFN layer) in a self-attention block has a residual connection around it and is followed by a layer-normalization step.

Problem: Prediction of Diseases in the Next Admission. Given historical admission records [V1,V2,…V|T(u)|] for patient u and a disease taxonomy GD, the goal is to learn a medical path-based self-attentional disease prediction model, which can predict the diseases in the next admission for patients.

The proposed model
In this section, the proposed Path-based reasoning model for self-Attentional Disease prediction (ProAID) model is introduced in detail. The list of notations, used throughout this paper, is given in Table 1.

For the patient u who has |T(u)| hospital admission records, we firstly sample diseases from each admission, and extract medical paths for these diseases based on the disease taxonomy GD. Each medical path starts from a disease in an admission, through the tree structure of disease taxonomy, ending at a highly frequent co-occurring disease of the beginning disease. The set of medical paths, which are built for diseases sampled from the same admission Vt, is denoted as ρ(set)t. In this way, we obtain medical paths [ρ(set)1,ρ(set)2,…,ρ(set)|T(u)|] for the hospital admission records [V1,V2,…,V|T(u)|], respectively. At the same time, we pretrain medical codes to capture the hierarchy structure information behind the disease taxonomy and generate embeddings for medical codes. The model embeds the admission information into a low-dimensional representation to develop patient admission histories. The embedding for each set of paths is firstly learned by the path encoder, and secondly combined with each admission representation. Finally, a self-attentional reasoning network takes the temporal information from a sequence that fuses both admission information and the medical paths to learn the representation for the patient, which can be fed into a fully connected layer to predict the diseases in the next admission for patients. Figure 2 shows the workflow of our proposed model.

Table 1 Notations in this work
Full size table
Disease taxonomy pre-training
In this paper, we leverage two different pre-training methods for medical codes. We use GRAM [16], which employs the graph embedding method, to learn the reasonable representation of medical codes. We first initialize the medical code embeddings ei by using co-occurrence information. Afterwards, the GRAM model combines the basic embedding ei and its ancestors via a graph-based attention mechanism to generate the final embedding of medical code ci.

In order to learn reasonable and correct representations of medical codes, we also employ the hyperbolic graph neural networks (HGNN) [35], which learn representations on Riemannian manifold with differentiable exponential and logarithmic maps. It has been shown recently that hyperbolic geometry lends itself particularly well for learning hierarchical representations of symbolic data and can lead to substantial gains in representational efficiency and generalization performance [36], which suits our disease taxonomy structure very well. We leverage a hyperbolic graph neural network on the disease taxonomy DG and take the medical code embeddings initialized by co-occurrence information as input node feature to learn the representational embeddings for medical codes in the disease taxonomy DG.

The final embedding vector of i-th medical code is denoted as mi. By concatenating the vector representation m1,m2,…,m|C| of all the medical codes, we generate the medical code embedding matrix M∈Rd×|C|, where d is the dimensionality size and mi is the i-th column of M.

Hospital admission encoding
Given the t-th hospital admission Vt and its |C|-dimensional binary vector xt∈{0,1}|C|, the vector representation vt∈R|d| is obtained by multiplying medical code embeddings M with xt as follows:

vt=tanh(Mxt),
(5)
where vt is the summarized admission embedding and d is its dimension.

Path encoding
The medical paths are already introduced in Sect. 3. In practice, we consider an instance of medical path by a sequence of medical codes. To embed a medical code sequence into a low-dimensional vector, we stack self-attention blocks to deal with sequences of variable lengths. Given a path ρ from a medical path set ρ(set)t, let Xρ∈RL×d denote the embedding matrix formed by concatenating medical code embeddings, where L is the number of medical codes in the path instance and d is the embedding dimension for each medical code. In addition, we incorporate a learnable position matrix P(path)∈RL×d to enhance the representation of the path. This implies that the medical path representation X′ρ can be obtained by summing two embedding matrices: X′ρ=Xρ+P(path).

We feed the medical path embedding matrix X′ρ into a path encoder, which consists of multiple self-attention blocks described in Sect. 3.

Hρ=PathEncoder(X′ρ).
(6)
The final embedding of the path ρ is hρL, which is the L-th row of the Hρ as well as the learned representation for the L-th position of medical code in the path ρ using the path encoder.

Finally, we apply the average pooling operation to derive the embedding for the medical path set of the t-th admission,

ht=1|ρ(set)t|∑ρ∈ρ(set)thρLρ.
(7)
Patient historical information learning
Having modeled the hospital admission embeddings and the medical path embeddings, we consider fusing this information to capture the patient temporal health information and predict the diagnoses in the next hospital admission. Specifically, we sum the two embeddings and adopt similar self-attention blocks to effectively learn the admission-level representation for the patient. We also incorporate a learnable position matrix P(adm)∈Rt×d to enhance the temporal information for the admission representations. The process is defined as follows:

ct= vt+ht,
(8)
[c′1;c′2;…;c′t]T= [c1;c2;…;ct]T+P(adm),
(9)
[o1;o2;…;ot]= TemporalEncoder([c′1;c′2;…;c′t]),
(10)
where the TemporalEncoder is also the stacking of self-attention blocks in Sect. 3. We take ot as the final representation for the patient.

Diagnosis prediction for next admission
Given the enhanced patient representations ot, it is fed through the activation layer to produce the next admission information defined as:

y^t+1=σ(Wpot+bp),
(11)
where Wp∈R|C|×d and bp∈R|C| are the learnable parameters, and σ is the sigmoid activation function. The prediction for next admission is treated as a multi-class task [37, 38]. To train our model, we quantify the prediction error via the following binary cross-entropy:

L=−1|T(u)|−1∑|T(u)|−1t=1ℓt,
(12)
ℓt= yTtlog(y^t)+(1−yTt)log(1−y^t).
(13)
Experiments
In what follows, we present detailed experimental analysis on our proposed disease prediction model.

Dataset
We conduct experiments on two public datasets collected from real world: the MIMIC-III and the MIMIC-IV.

MIMIC-III is a freely available database comprising deidentified health-related data associated with over 40,000 patients who stayed in critical care units of the Beth Israel Deaconess Medical Center between 2001 and 2012 [39].

MIMIC-IV builds upon the success of MIMIC-III, and incorporates numerous improvements over MIMIC-III. Patients data were extracted from the respective hospital databases, which contained patients admitted to an ICU or the emergency department between 2008 and 2019 [40].

Statistical summary of these datasets is given in Table 2. The average number of patient admissions in the MIMIC-III is short, so we use this dataset to evaluate the performance with insufficient training data. Considering the time range of MIMIC-IV is wide, we select the patients with over 10 admissions to evaluate the performance with long records. In experiments, we take the original ICD-9 codes as input features and predict the first 3 digit ICD-9 code as categories. Here we use CCS-multi-level diagnosis hierarchyFootnote3 as the taxonomy of diseases.

Table 2 Major statistics of the datasets
Full size table
Table 3 Disease prediction performance on two datasets. Entries in bold face are the best results
Full size table
Baselines
We compare our model with a well-established recurrent neural network as well as two state-of-the-art disease prediction models GRAM and CAMP, which are introduced below.

GRU. The admission sequence x1,x2,…xT is embedded by the global co-occurrence matrix of medical codes and fed into a GRU module [41].

GRAM. GRAM is a graph-based attention model using both a knowledge graph and EHR to learn accurate and interpretable medical representations for sequential diagnoses prediction tasks [16].

CAMP. CAMP uses co-attention memory networks for diagnosis prediction to integrate historical records, fine-grained patient conditions, and demographics with a three-way interaction architecture built on co-attention [18].

Disease prediction
For the proposed ProAID, we set the number of the self-attention blocks to 2, the attention heads to 4, and the hidden embedding size to 128. For the MIMIC-III dataset, the learning rate is set to 0.0006. The maximum patient admission number is limited up to 10. For the MIMIC-IV dataset, we set the learning rate to 0.0002. We select the patients with more than 10 admissions and the maximum patient admission number is limited up to 15. For each disease, we randomly sample 10 related diseases from the most 200 frequent co-occurring diseases. We randomly select 50 paths for each admission on MIMIC-III datasets and 20 paths on MIMIC-IV datasets. We randomly split the patients in the both two datasets with a ratio of 7:1:2 for training, evaluation and testing, respectively. We utilize three widely used metrics, namely recall and precision of the top-K diseases in each patient’s prediction result. Recall reflects how accurately a model can predict the right disease for a patient, precision indicates how well a model distinguishes the true diseases from the false ones. Here we choose K = {2, 5, 10, 20} based on the average and maximum number of diagnosed diseases in datasets. All experiments are conducted on a GPU server equipped with two Nvidia Titan RTX graphic cards.

The comparative results are given in Table 3. Apparently, in most cases (K={2,5,10}), ProAID outperforms all state-of-the-art baselines by a significant margin. ProAID (GRU, GRAM, CAMP) achieves 0.506 (0.436, 0.485, 0.435) precision and 0.36 (0.29, 3.16, 0.27) recall on the average of multiple settings (K = {2,5,10,20}) on MIMIC-III. Compared with GRU (GRAM, CAMP), ProAID has increased precision by 15.9% (4.4%, 17.0%) and increased recall by 24.1% (13.9%, 33.6%) on MIMIC-III. Similarly, ProAID increases precision by 7.5% (3.3%, 10.7%) and recall by 7.0% (3.4%, 17.6%) compared with GRU (GRAM, CAMP) on MIMIC-IV.

This verifies the effectiveness of ProAID in capturing the temporal information and learning the representations of a patient. With an increasing K, the general trend shows an increasing recall and a decreasing precision. It is reasonable since, when more possible diseases are predicted, more actually diagnosed diseases will be covered in the result, but the percentage of correct results also drops in the prediction. All models obtain higher scores on the MIMIC-IV dataset. It is possible that we select the patients with over 10 hospital admissions in the dataset. Longer admission sequences would help the model to learn more temporal information and make a precise prediction.

For K={2,5,10}, the GRAM obtains similar results with ProAID on both two datasets. It is possibly due to the fact that GRAM also pre-trains the medical codes using co-occurrence information and leverages attention mechanism to learn the relation between the medical codes and ancestors. But the attention mechanism in the GRAM ignores the structural relationship in the disease taxonomy. We use a hyperbolic graph neural network to capture this information and obtain the best performance. Besides, the performance of CAMP is degraded in all datasets, since CAMP is designed for training on ICD categories rather than full ICD code, which is very sparse.

In the case K=20, ProAID beats all the baselines except GRAM. Interestingly, with a higher value of K, the performance gap between GRAM and ProAID gets smaller. One possible reason is that our ProAID only learns the most related medical information but neglects some tiny but important facts. For example, the sampling of the medical paths might miss a few strong related medical information.

Ablation study
To verify the effectiveness of each component and for a more detailed analysis, we conduct the ablation study on both two datasets. We remove the medical codes pre-training process and the path encoder to evaluate the contributions of these components, denoted as ProAID− and ProAID∗, respectively. Besides, we replace our pre-trained medical code embeddings with a co-occurrence matrix trained by Glove and test the performance, the result is denoted as ProAIDG.

The results of ablation study are shown in Table 3. It is pertinent to note that removing any components of the model would lead to a performance drop. ProAID- (ProAID*) achieves 0.473 (0.503) precision and 0.315 (0.333) recall on the average of multiple settings (K=2,5,10,20) on MIMIC-III. Compared with ProAID- (ProAID*), ProAID has increased precision by 6.9% (0.6%) and increased recall by 14.2% (8.0%) on MIMIC-III. Similarly, ProAID increases precision by 1.2% (0.7%) and recall by 1.3% (1.1%) compared with ProAID- (ProAID*) on MIMIC-IV. These experimental results consistently indicate the effectiveness of both the medical codes pre-training process and the path encoder. It indicates that both medical codes pre-training and medical path encoder are useful to improve the prediction performance. However, the importance of these components is different. We notice that removing the medical codes pre-training processing yields a larger drop in prediction performance. One possible reason is that the pre-training process in the hyperbolic space can capture the structural information inside the disease taxonomy, which helps a lot to prediction tasks. On the other hand, we randomly select the medical paths for each disease, the less related information might be added to the model. The path sampling method is a meaningful research topic in the future.

In addition, the ProAIDG performs similar to ProAID, which suggests that the medical code pre-training process can learn the effective representations for medical codes.

Conclusions
In this paper, we present ProAID, a novel model based on a self-attentional reasoning network that utilizes medical paths to augment the latent interaction between diseases and learn highly representative patient embeddings to enable accurate disease prediction. The experimental results on two datasets show promising effectiveness of the proposed model in multi-label disease classification. To conclude, ProAID offers an intuitive, accurate solution to disease prediction, generating embeddings that capture the hierarchical information of diseases and learning effective representations of a patient based on the historical patient admission sequences. In the future, we plan to adopt medical path sampling and encoding to improve prediction performance and interpretability of ProAID.