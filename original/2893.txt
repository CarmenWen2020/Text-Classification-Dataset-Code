The rapid, increasing adoption of businesses to deliver their services in Software as a Service (SaaS) products in the marketplace presents selection challenges to users. Recently, major cloud service providers such as Amazon Web Services and Microsoft Azure have introduced well-architected frameworks that assess SaaS products in different pillars (also referred to herein as features). Furthermore, customers leave feedback on these features after using SaaS products. However, they do not provide feedback on all the features of a product, which renders the reviews unusable to prospective users needing to assess a product’s quality before committing to it. Our study addresses this drawback by imputing or inferring the intensity of the customer’s feedback on features that they do not mention in their reviews. Specifically, we propose threshold-based nearest neighbors (T-NN) as an extension of the conventional k-nearest neighbor approach to determine the missing sentiment intensity score of a feature from the values of its other features. We evaluate the proposed approach in two different systems and compare our results with seven other data imputation techniques. The results show that the proposed T-NN approach performs better than the other imputation approaches on the SaaS sentiment dataset.

Access provided by University of Auckland Library

Introduction
Software as a Service (SaaS) has become a popular choice in current software delivery models. In such a model, service providers employ cloud infrastructure to develop and deploy their services to the users. To ensure that the delivered service meets the required expectations, service providers are increasingly developing criteria for performing structural audits and assessments of their service delivery [1]. For example, Amazon Web Services (AWS) and Microsoft Azure are the two major cloud-computing vendors that have designed frameworks that define the levels key areas of services running on the cloud should achieve in software quality assessment. As shown in Table 1, the well-architected framework designed by AWS [2] comprises five pillars (also known as features in this paper)—security, reliability, performance efficiency, cost optimization, and operational excellence. Microsoft Azure in its Azure architecture framework also considers five features—cost, DevOps (previously labeled ‘management’ aspect), resiliency, scalability, and security [3]. In its previous version, Azure included the feature ‘availability’ in its architectural framework, referring to the proportion of time for which the software system is functional and working on the cloud. We consider the ‘availability’ feature in this paper, along with others.

Table 1 Standard application/service architectural features
Full size table
These features assist both service providers and users. To the service developers/providers, the features provide the requirements the service should meet in terms of stable design and implementation. To the service users, the features provide the criteria for assessing the performance and quality of a service in the form of a textual review. Such assessments greatly assist potential users in making an informed decision when choosing a service or product. However, before such benefits can be realized, a framework is needed that (a) identifies and links which part of the textual review relates to which service quality aspect from AWS or Azure, and (b) ascertains the sentiment intensity score of each service quality aspect which can be used for decision-making.

In our previous work [4], using machine learning methods under a multiclass text-classification setting, we proposed an approach that identifies to which service quality aspect/s the textual assessments relate. This determines the user’s sentiment intensity value toward that service quality aspect. However, a common problem in users’ assessments is that they often do not provide their opinion on all the service quality aspects. This leads to scenarios where the sentiment intensity of some features is missing. To address this shortcoming, we need an approach that infers the sentiment intensity of the missing features by utilizing information from similar SaaS services. Our focus in this paper is to address this problem—inferring or imputing the sentiment intensity values of the features which are not mentioned in the users’ assessments. We propose here a threshold-based nearest neighbors (T-NN) approach, which is an extension of the conventional K-nearest neighbor approach, to determine the missing sentiment intensity score of a feature from the values of other features. We then evaluate the proposed approach in two different schemes and compare the results of T-NN against seven other data imputation techniques. Our results show that the proposed T-NN approach performs better than the other imputation approaches on the SaaS sentiment dataset. The rest of the paper is organized as follows. In Sect. 2, we briefly present the importance of sentiment intensity values for customers to make informed decisions during service selection. We also present the background and core concepts related to inference, imputation, and the well-known imputation approaches applied in this study. Sections 3 and 4 detail our proposed T-NN approach. Section 5 details the results from the experiments. Section 6 concludes the paper.

Background
Several different approaches have been used for missing value imputation in various domains. From classification to regression problems, these imputation approaches have proven useful in preventing datasets with missing values from being discarded. In this section, we present the literature review in two broad categories. The first is on the importance of knowing the quality of service (QoS) aspects that will assist customers to make an informed decision during SaaS product selection, and the second is on the imputation approaches in the literature for inferring the values of missing variables.

Importance of service quality during SaaS product selection
Customer satisfaction is closely linked to SaaS service quality [5]. The level of this quality, which can be inferred from customer satisfaction [6, 7], can affect customer trust in the SaaS product. Several frameworks have been proposed to measure service quality by considering a variety of QoS factors. One such scale to measure SaaS quality is SaaS-Qual [8, 9]. This metric considers six service quality factors to assess SaaS quality concerning customer satisfaction, with the added advantage of creating a measure providers can exploit for continuous service usage. In choosing an SaaS service, the selection criteria play an important role, helping to select both SaaS services and their providers. One source of selecting the criteria is expert interviews [10], where suggestions from domain experts are used to finalize the key factors in selecting SaaS services. In their work, Badidi et al. [11] consider QoS as non-functional attributes. They proposed an algorithm that selects and ranks SaaS service providers. Using an aggregate utility function, their algorithm matches the service consumer requirement with the offerings of SaaS service providers. Their proposed framework works on the service-level agreement (SLA) brokered between SaaS consumers and providers to facilitate negotiation. To select SaaS service providers, they suggest considering functional attributes such as service functionality, integration with on-premises services, SLA negotiation and compliance monitoring, software change management, data access, data security, and a non-functional category that includes QoS-related indicators such as availability, response time, reliability and throughput. However, their algorithm only considers the non-functional categories, i.e., the QoS attributes. SaaS providers can exhibit a different QoS for each of their services.

The importance of QoS factors can vary based on user requirements and can be ranked and weighted accordingly. Godse et al. [12] propose an approach that uses the analytical hierarchy process (AHP) to prioritize SaaS product features. From expert interviews and experience, a set of factors is determined to select software products. These factors include functionality, architecture, usability, vendor reputation, and cost. The parameters used to select SaaS products are assigned weights using AHP and are considered as quantitative opinions, rather than subjective. The hierarchy of the service selection parameters is created from surveys from the domain experts. As part of their methodology, they compare parameters in pairwise settings where each parameter results in a value between one and nine. The SaaS products receive a weighted score from the AHP ranking and weighting process. Selecting a service on the cloud by considering multiple criteria is not limited to SaaS. Cloud service selection has been studied under the multi-criteria decision-making problem [13, 14]. Based on multiple service quality criteria, Rehman et al. [14] propose a cloud service selection and ranking methodology using the technique for order preference by similarity to ideal solution (TOPSIS) and ELimination Et Choix Traduisant la REalité (Elimination and Choice Expressing Reality or ELECTRE), which are both multi-criteria decision-making (MCDM) approaches. Their approach selects and ranks IaaS services and considers the service’s QoS history in terms of criteria that include CPU, memory, I/O performance, and price per hour. The ranking is determined by user criteria for different timeslots. Another study by Ezenwoke et al. [15] combines quantitative and qualitative QoS attributes for SaaS ranking and selection using a heterogeneous similarity model (HSM). Their work grades the ranking performance of five HSM metrics using a synthetically generated cloud dataset. They highlight the importance of both qualitative and quantitative QoS attributes in selecting SaaS applications. They use six QoS attributes of which three are quantitative and three are qualitative. These attributes are service response time, availability, cost, usability, security management, and flexibility.

Quality-based SaaS service selection introduces more complexities such as multi-tenancy. In their work, Wang et al. [16] propose an approach for multi-tenant SaaS products to improve the QoS aware service selection process. Their approach is a service recommendation based on the service’s QoS values and the tenant’s QoS requirements. They use a k-means clustering approach to group similar tenants based on their common QoS features, reducing the search space. Their experiments include publicly available web services datasets. The quality parameters used in their approach include cost, response time, availability, and throughput. Another, similar work by He et al. [17] proposes a multi-tenant SaaS optimizer (MSSOptimiser) to capture the difference in end users’ QoS constraints and the quality of the candidate service. Their method helps to deal with multi-tenant SaaS products rather than single-tenant providers. SaaS deployment and multi-tenant SaaS products have many stakeholders with different QoS requirements. The quality parameters used in their method include cost, response time, availability, and throughput. They evaluate their approach using synthetically generated datasets for a real-world web service.

It is evident from the existing literature that QoS plays an important role in SaaS service selection and each QoS factor and its associated values are crucial. Unobserved or missing values for these QoS factors or service quality aspects can affect the overall interpretation of QoS values for SaaS products. In different domains, the missing values are dealt with using several approaches discussed in the following sections.

Missing data and missing data mechanisms
The missingness of data can be categorized under three mechanisms [18, 19], i.e., missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR). The missingness of data is assumed to be MCAR when the missing value of a given variable does not depend on the values (observed or missing) of other variables. In the MAR assumption, the missingness of the values in one variable might be dependent on the observed values of other variables. MCAR and MAR are also known as ignorable missingness. When the missingness of values is dependent on the unobserved data, it is assumed to be MNAR. The MNAR mechanism is also called on-ignorable missingness. The data can be missing in one of many patterns which include univariate, multivariate, monotone, and general [19]. The pattern of missingness in the dataset used in this study follows a general pattern, i.e., the missing values are spread randomly across all the variables.

For most of the predictive models, missing values in the datasets can be problematic. The methods that deal with datasets with missing values generally take two different approaches. In the first, the instances with missing values are discarded from the dataset in a process also known as complete case analysis. In the second approach, the missing values are imputed using different techniques. Discarding the instances with missing values can lead to several challenges. It shrinks the dataset because different variables could have missing values on different units, leaving only those complete units that are not representative of the whole population (thus adding significant bias) [20]. On the other hand, imputing the missing values in instances helps to retain those instances which can assist in reducing the resulting bias regarding population representation. Missing data imputation can be achieved through univariate imputation methods, i.e., imputing a single variable by exploiting information from the variable itself. Alternatively, it can be accomplished via multivariate imputation methods that impute missing data in more than one variable using a multivariate model.

Mean and median
Mean imputation is frequently used for imputation due to its simplicity. In mean imputation, the missing values in a given variable (feature) vi are imputed by calculating the arithmetic mean of all the values in vi. Although this method takes less effort to implement, it can have a negative effect on the frequency distribution of the given variable vi when a large sample of values is to be imputed. In addition to the frequency distribution, the mean imputation can also affect the correlation between the variable with missing values and other variables in the dataset. This can result in underestimation and a misleading interpretation of the variance [19]. The same is the case with median imputation. For the data points with a missing value in the jth variable, the mean imputed value is calculated as:

xj(imp)=1Nj(obs)∑i=1Nxij(obs)
(1)
where xj(imp) is the mean imputed value for the jth variable, xij(obs) is the observed value in jth variable of the ith instance, and Nj(obs) is the total number of observed values in the jth variable.

K-Nearest neighbors (kNN)
kNN is one of the nonparametric algorithms and can be considered the simplest to implement and interpret. There is no model learning involved with the kNN predictions (it follows instance-based learning for each prediction); rather, it stores and uses the entire available dataset [21]. Predictions for missing values in a candidate instance are made by searching the entire training dataset for the K most similar donor instances. The mean or median value of the target variable from the selected K donor instances is then used to impute the missing value in the candidate instance. When the value of K reaches the maximum, i.e., the total number of instances in the dataset (n − 1), the kNN imputation becomes equivalent to the mean imputation. The K most similar instances are selected using a distance measure. The most used distance measure is Euclidean distance which is calculated as the square root of the sum of the squared differences between a candidate instance and donor instance. It can be written as:

d(x,y)=∑i=1i(xi−yi)2−−−−−−−−−−−⎷
(2)
where x is the candidate instance, y is a donor instance and i represents the number of variables (features) in the dataset. In addition to Euclidean distance, there are several other distance measures such as Manhattan distance, hamming, cosine, Tanimoto, Jaccard, and Mahalanobis [21]. When used with large datasets (with higher dimensions), kNN creates challenges in terms of computational cost. When the standard Euclidean distance is used as a distance measure, it assumes equal weights for all the variables in the dataset. This can be a wrong assumption in many real-life datasets as variables tend to relate to each other at certain levels. Two of the most common aspects of kNN that can be adjusted are the number of k-nearest neighbors and the distance measures used to calculate the distance between neighbors.

kNN Extensions
There are several extensions of the kNN that can be applied to different problems. A weighted KNN (W-KNN) is proposed by Fan et al. [22] to improve short-term load forecasting accuracy. Using the electricity load dataset from the National Electricity Market Australia, their study uses the reciprocal of the calculated Euclidean distance as weights for each of the forecasted data points. The results show that their proposed W-KNN approach has a better ability when it comes to fitting than the autoregressive moving average model (ARMA) and the back-propagation neural network (BPNN) methods. The sparse learning KNN (S-KNN) [23] is an extension of kNN that considers the correlation between the testing and training samples to obtain the optimal k parameter.

The kNN imputation approach is also considered to be one of the simplest and best known methods for missing value imputation [24,25,26]. In the domain of knowledge discovery from databases [24, 26], the weighted K-nearest neighbor approach shows better performance compared to methods such as C4.5. To estimate the missing values in DNA micro-arrays, the weighted kNN is more robust where weights are assigned to the genes based on the similarity of their expressions [25]. Other extensions of the kNN for imputing gene micro-arrays include Sequential KNN (SKNN) [27] and Iterative KNN (IKNN) [28]. SKNN, which is a cluster-based approach, initially selects genes with the least number of missing values for imputation and later reuses the same imputed genes for further imputations of other genes with missing values. Their results indicate that the SKNN has higher performance compared with the conventional kNN method. The IKNN is introduced to improve the prediction capability of the cluster-based estimation approaches which iteratively estimate missing values.

The weighted nearest-neighbor algorithms can be categorized into three groups—global, local, and partial-local weighting [29]. In global weighting, the weights are assigned to each variable based on its relative salience and remain the same throughout the dataset for all instances. In local weighting, the weights for the features are context-dependent and can change from one instance to another. The last group of weighting methods, partial-local, uses relatively less contextual information than local weighting, but remains context-sensitive, relative to the global weighting method. The most common approach for weighting the neighbors uses the distances between the instances (specifically the reciprocal of the Euclidean distance) as weights [22, 24, 30,31,32], where the weights decrease with the increase in the distance value from a given instance. A similar distance-based weighting approach is also proposed in the ordinal classification problem [33], where a kernel function is then used to transform the normalized distances into weights for the instances. In industry, W-KNN, which uses the variance-based Euclidean distance evaluation technique, is successfully applied in two-stage feature selection and weighting to identify and detect gear damage in rotating machines [34]. In a classification setting on unbalanced text corpus, Tan et al. [35] apply a different weighting approach using kNN. In their work, larger weights are assigned to the neighbors with small class memberships and smaller weights are allocated to the neighbors with large class memberships. Recently, Martin et al. show the benefit of applying W-KNN in reinforcement learning [36]. The weights are calculated as the inverse of the distances from a current observation, and this is used for the approximation of a linear function. Bhattacharya et al. [37] use AHP to assign weights to the features obtained dynamically. Their approach gives a different preference to each feature when used with kNN and outperforms the conventional kNN. Other approaches such as information gain (IG) [38] and gain ratio (GR) [39] have been used to assign weights to features. Biswas et al. [40] use the fuzzy membership function to assign weights to classes in a classification setup. In addition to the weights assigned to features and instances separately, Mateos-Garcia et al. [41] use an evolutionary method that considers the contribution of each neighbor, along with the importance of each feature simultaneously. Another evolutionary approach using the distance-weighted kNN by Mateos-Garcia et al. [42] calculates the optimal value of k along with the set of class-dependent local weights. AlSukker et al. [43], in their study, use a differential evolution optimization technique that enhances the performance of kNN. They use four types of weighting mechanisms with kNN—feature, neighbor, class, and hybrid weighting. One of the main challenges with kNN is selecting the size of K and the introduction of outliers within the selected neighborhood size. To overcome this, Gou et al. [44] present a dual-weighted voting [44, 45] scheme which achieves lower error rates than conventional kNN. kNN is widely used in classification problems, and some of the shortcomings faced in that scenario include the effect of voting on prediction accuracy, the optimal value of k and the inclusion of irrelevant attributes [46]. The dynamic kNN naïve Bayes with an attribute-weighted algorithm as Jiang et al. [46] propose employs local naïve Bayes on the kNN of test instances and uses the mutual information between the attributes to assign weights to each attribute based on their contribution. Similar work by Wu et al. [47] also recommends the use of mutual information to assign weights based on the contribution of each attribute. Yan et al. [48] present a weighted kNN using a genetic algorithm for text classification. In their work, the set of features is initially reduced using a hybrid rough set theory with Bee Colony Optimization while a genetic algorithm is used later in combination with kNN for classification. Weighted kNN also has applications in advanced domains such as big data. In their study, Talavera-Llames et al. [49] use distance-based weights for each neighbor to predict energy consumption in a distributed computing setup. For energy-price forecasting, the weighted kNN combined with a genetic algorithm also achieves lower forecast errors compared with artificial neural networks [50].

SVD
Singular-value decomposition (SVD) is another well-known approach for missing data imputation [25, 51, 52]. SVD is a matrix factorization method [53, 54] used to decompose matrices to facilitate matrix calculations. The SVD of a given matrix A is given by:

A=UΣVT
(3)
where A is the actual m × n matrix, U is an orthonormal matrix with m x m dimensions, V is another orthogonal matrix with n × n dimensions, VT is the transpose of V, and Σ is a m × n diagonal matrix. The columns of the matrices U and V represent the left singular and right singular vectors of matrix A, respectively. The diagonal matrix Σ defines the singular values of A that consist of non-negative real numbers. In terms of missing values imputation, SOFT-IMPUTE [55] is one of the well-known algorithms that use soft-thresholded SVD to impute missing values in datasets during an iterative process. Parameters such as the maximum iterations on the dataset and the threshold value can be adjusted for the decomposition process. The threshold is set to values less than one and the algorithm uses the threshold value during the iterative updating of the estimated values for imputation.

Decision tree (DT)
A decision tree (DT), also known as a classification and regression tree (CART) [56], is a nonparametric supervised learning method. A decision tree is used to create predictive models by learning decision rules from the available features in the datasets. In both classification and regression problems, the core idea of the DT is to divide the input space recursively into partitions. During the partitioning of the input space, cuts or split points on selected variables are used to create the trees. Using a greedy algorithm, a variable with a potential split point that has the lowest cost is selected. A predictive model is fitted on each node, i.e., a classification or a regression model [57]. Using CART in classification problems, the prediction error is measured as the misclassification cost and the Gini index as the impurity function whereas, in regression problems, the prediction error is measured as the squared difference between the observed and the predicted values. In other words, the heterogeneity in the response (target) variable at any node is evaluated by an impurity measure [58].

For data pre-processing in classification, a hybrid imputation approach using a decision tree and expectation maximization [59, 60] proves to be very effective. The work in [59] uses similarities among the instances and the correlations among the variables. DT is also used to impute categorical data [61]. A tree-based missing data imputation approach called the ITree [62] captures missing data patterns using a binary classification tree, and then, to estimate the missing values on the terminal nodes, a regression tree is constructed. [61] introduces a probability rule with DT for the stochastic imputation of categorical data. [63] uses sequential trees in an iterative setting for imputing complex missing data structures. Sequential trees are tree models used in a sequential procedure. Their evaluation results show that their proposed iterative sequential tree-based model outperforms the non-iterated sequential trees. Tree-based incremental approaches [64, 65] using a boosting algorithm have also proven to be better in performance in large datasets compared to incremental nonparametric imputation [58]. The implementation of DT can be adjusted with parameters such as a strategy of choosing a split at nodes, criteria to measure the quality of the split, maximum depth of the tree, and the minimum number of samples intended to be at the leaf nodes.

Stochastic gradient descent (SGD)
Originally introduced by Robbins et al. [66], SGD is an efficient iterative algorithm for optimizing an objective function. Unlike the general gradient descent approach, the SGD randomly selects fewer samples rather than the whole dataset.

θ=θ−α∇θJ(θ;x(i);y(i))
(4)
For a given training sample i, i.e., (x(i), y(i)), the parameter θ of the objective J(θ) will be updated with each iteration. SGD shows promising results when used in datasets with missing values. In this study, the SGD is implemented with least squares as the loss function. With linear models, the SGD is implemented with many adjusted parameters and has proven useful for large datasets with missing values [67,68,69]. The SGD with a linear model can work with several loss functions as the parameter that measures mode fit, such as ordinary least squares, Huber loss, and epsilon-insensitive. In addition to the loss function, a regularization term determines and penalizes model complexity. The two well-known regularization terms used are L1 and L2 norms. The ordinary least square loss function, along with L2 regularization, is most suitable for non-sparse solutions. Other parameters that are generally tuned include alpha, which controls the strength of the regularization term, the number of epochs on the training data, and the learning rate.

Expectation maximization (EM)
The EM algorithm is another well-known approach [70] used to impute missing values in datasets [59, 71]. According to the specifications in [70], an incomplete-data specification with sample densities g(y|θ) is derived from complete data with sample densities f(x|θ). The EM algorithm aims to find the value of the parameter θ. There could be many possible complete-data specifications needed to achieve the above derivation. The θ parameter exists with the data but is not directly observable. Similarly speaking, the EM algorithm finds the maximum likelihood estimates for model parameters. The likelihood function is approximated in an iterative manner consisting of two steps. The first step is called the E-step (estimation step) in which the missing values in a variable (log likelihood) are estimated. Formally:

Q(θ|θt)=E[l(θ|X,Y)|X,θt]
(5)
where X represents the observed data, l(θ|X,Y) is the log-likelihood, θ is the unknown parameter, and θt is the parameter estimate for the sth iteration. The second step is called the M-step (maximization step) in which the model parameters that maximize the log-likelihood are optimized. Formally:

θt+1=maxθQ(θ|θt)
(6)
Both the steps are repeated until the convergence of θt+1.

T-NN approach to impute the missing sentiment intensity values of features
As shown in Fig. 1, we propose a solution comprising four stages. Pre-processing is the first stage where, using our previous work [4], we determine which service quality aspects are mentioned in the textual reviews from the users. Sentiment intensity computation is the second stage where the sentiment intensity in the users’ reviews linked to a feature in the first stage is determined. The inference stage is the third stage of the framework and aims to infer the sentiment intensity values of features for which no user reviews are present. We present a modified version of the traditional kNN approach—T-NN, a threshold-based nearest neighbor with weighted Euclidean distance to infer the values of the missing aspects. The testing stage is the last stage of the framework in which the performance of T -NN in inferring the missing sentiment intensity values is compared with other similar techniques.

Fig. 1
figure 1
Stages in the T-NN approach for imputing the sentiment intensity values of missing features

Full size image
In the next sub-sections, we explain in detail the workings of each stage of the framework.

Pre-processing stage
The objective of this stage, which is to collect the reviews and pre-process them, was achieved in three sub-stages as follows:

Collecting customer reviews
The pre-processing stage is responsible for collecting customer reviews, processing and segmenting them, and determining to which SaaS quality aspect they relate. In employing the approach proposed in [4], we used three different sources, i.e., AWS marketplace, GetApp.com, and Serchen.com, and collected 29,361 reviews on 3215 SaaS products. Mathematically, the collected reviews (RT) are the combination of all the reviews for all the products from n number of review sources considered (RSn) (which in this case is 3) and are shown in Eq. 7.

RT=∪{RS1,..,RSn}
(7)
The reviews for product Pi are generally provided by multiple reviewers and can be represented as:

RPi={r|r∈RT}
(8)
Processing and segmentation
All the collected reviews were pre-processed and cleansed for further analysis. Each review was then split into a different number of segments. As outlined in [4], this step is very important because a single review might tend to address multiple features. The review segments for product Pi can be represented as:

SPi={s|s∈RPi}
(9)
The total review segments in the whole dataset can be represented as:

ST={s|s∈SPi|i∈PT}
(10)
where PT represents the set of SaaS products.

SaaS quality aspect identification
Each review segment was labeled using the trained ensemble classifier [4]. As a result, each of the text segments had a label associated with it, as shown in Table 2. The label for each segment either identifies the SaaS quality feature to which it refers or an unidentified segment “N” if it does not refer to any of the service aspects. Mathematically, the label set is shown in Eq. 11, and the labeled dataset XL with SaaS features can be represented as shown in Eq. 12.

Y={A,C,E,L,M,N,O,P,R,S}
(11)
XL={(s1,y1),(s2,y2),…,(sn,yn)|s∈ST|y∈Y}
(12)
Table 2 SaaS service quality features
Full size table
Sentiment intensity computation stage
The objective of this stage was to compute the sentiment score for each review segment. Sentiment analysis is a widely researched area that comes under the umbrella of affective computing. Researchers have developed techniques that determine the polarity or sentiment at various levels of granularity—at document level [72], sentence level [73], and aspect or feature level [74,75,76]. Recent work has focused on determining the sentiment of text in a language other than English [77] and using deep learning combined with attention-based mechanisms for sentiment determination [78]. The output of sentiment analysis approaches varies from simply classifying the overall emotion of the input as either positive, negative or neutral, to identifying the strength of the determined emotion [79]. As our focus in this paper is on imputing the missing values of features rather than on sentiment analysis, here, we utilized an existing sentiment intensity determination approach to determine the sentiment intensity of each review segment. We used Valence Aware Dictionary for sEntiment Reasoning (VADER) [80] which uses a reliable valence-based sentiment lexicon for the sentiment analysis of online product reviews, social media, and other online communities [81,82,83,84]. As its output, VADER provides positive and negative sentiment valence (polarity + intensity) scores for text segments. Each text segment is associated with a compound score (which is a normalized valence score) ranging between − 1 and + 1. A value of − 1 indicates the most negative or worst sentiment intensity score, and a value of + 1 indicates a positive or best sentiment intensity score. VADER computes the compound sentiment score for a text segment by summing up the valence scores for all the words in the text segment and then normalizing the sum between − 1 and + 1. The compound sentiment score for a given segment was calculated as follows:

=VSVS⋅VS+α−−−−−−−−−√
(13)
where α approximates the maximum expected value and Vs represents the sum of valance scores for n number of words in the text segment T, and is calculated as:

VS=∑j=1nVj
(14)
At the end of this stage, the sentiment intensity value for each feature j of the SaaS product i is a value between -1 and 1 as shown in Eq. 15.

xij∈[−11](i=1,2,…,n;j=1,2,…,m)
(15)
At the end of this stage, for a given product and for each of the identified features which the customer mentions in their reviews, the corresponding sentiment intensity scores are aggregated. For instance, if product A has 10 text segments associated with it and each of the 10 segments have identified the security aspect (S), then the sentiment intensity score for feature S of product A is the aggregated sentiment scores over all the 10 segments, as shown in Eq. 16.

As=∑nj=1sjn
(16)
As a result of this analysis, we created a dataset that includes the aggregated sentiment intensity scores for the service aspects associated with each SaaS product. The analysis indicated that, of 3215 products, only 729 had the aggregated sentiment intensity scores for all nine features while the remaining products were missing the aggregated sentiment intensity scores for one or more features. If the review segments did not mention any of the features, then they were assigned with the level ‘N’ and the sentiment scores for those segments aggregated. This value was needed in the inference stage where the overall sentiment intensity score is needed (regardless of the sentiment intensity scores for each aspect).

Inference stage
The objective of this stage was to infer or impute the sentiment intensity values in the quality features (features) that do not have user reviews. In our framework, we achieved this by using the T-NN approach which is an enhanced version of the kNN approach. In the traditional kNN approach, the k most similar features, based on their distances to the candidate feature being studied, are used to decide to which class it should belong. When used in classification problems, the classes of the selected k instances are used to find the class label with higher votes (most frequent) as the selected class of the candidate feature. When used in regression, the aggregated value (mean) of the selected k instances is used to determine/infer the value of the candidate feature. However, the traditional implementation of the kNN approach has two main drawbacks. The first is that it assumes equal weights for all the features in the dataset when calculating the distances between the candidate feature and the other features. This assumption can affect the prediction results negatively, as features in real datasets tend to correlate at different levels. Thus, assigning all the features, the same weight undermines the correct estimate of the values of the candidate feature. The second drawback is that selecting k instances based on their distances from the candidate feature can also lead to unrealistic estimations. For example, if the distance among the k selected features is too large, it can increase the error when inferring the values of the candidate feature. This is especially relevant in cases when the aggregated values from the k features are used to determine the value of the candidate feature, which is the nature of the problem discussed in this paper. To address the first drawback, in T-NN, we used a weighted Euclidean measure to calculate the distances among the different features. The weight to be applied to any two features is determined by Pearson’s correlation coefficient that measures the strength and direction of the relationship between them. The rationale here is that if a feature is strongly (either positively or negatively) related to the candidate feature (which has the missing value), then a higher weight should be placed on the value of the feature while inferring the value of the candidate feature. To address the second drawback, we introduced a threshold parameter (T) to select the donor instances to be considered in inferring the value of the candidate feature. The parameter T replaces the traditional selection approach based on the k closest instances and selects ‘the most relevant’ donor features that will improve the accuracy of the inferred values rather than increasing the prediction error. Figure 2 shows the five steps in our proposed T-NN approach to impute the sentiment intensity value in a feature of a SaaS product (instance).

Fig. 2
figure 2
Steps to impute the sentiment intensity value of an instance’s feature

Full size image
Step 1: find the donor instance/s to the candidate instance
A candidate instance refers to that SaaS product in which the sentiment intensity value of a feature is to be imputed. A donor instance refers to that instance which can be used to impute the missing sentiment value in the feature of the candidate instance. A condition for an instance to be a donor is that it should have the sentiment intensity value in the feature for which it needs to be imputed in the candidate instance. In other words, the donor instance should have the sentiment intensity value in the feature to be imputed in the candidate instance.

For example, Table 3 shows a sample of the dataset whose rows have the service instance, the columns show the features and the intersecting cell of the feature with the instance shows the sentiment intensity value for that instance in that feature. Let us assume that service S002 is the candidate instance whose sentiment intensity value in feature R needs to be imputed. The donor instances in this case are all service instances except S489, as it does not have the value in the feature R. Representing this mathematically, if X is the dataset with n instances {x1, x2, x3,….,xn} and m features {k1, k2, k3,….,km}, then the sentiment intensity value of the ith instance in jth feature can be represented as xij. If the value in xij needs to be imputed, then those instances that have a value in the jth feature are donor instances.

Table 3 Sample dataset with the sentiment intensity value of SaaS features
Full size table
Step 2: determining Pearson’s correlation coefficient between features
Pearson’s correlation coefficient is one of the most commonly used measures of the strength of the linear relationship between two features. Correlation, also known as the bivariate correlation, is used to measure the strength and direction of the relationship between two features [85]. The values of the correlation vary between + 1 and − 1. A positive correlation between two features indicates that the value of a feature tends to increase (positive) with an increase in the value of the other feature and vice versa. A correlation value of + 1 means that there is a perfect positive relationship between the two features. On the other hand, a negative correlation between two features indicates that the value of the feature tends to decrease (negative) with the increase in the other feature and vice versa. A correlation value of -1 means that there is a perfect negative relationship between the two features. A correlation value of zero means that there is no relationship between the two features. The correlation coefficient for two features was calculated as:

ρxy=Cov(x,y)σxσy
(17)
where Cov(x, y) represents the covariance of the two features x and y, calculated as:

Cov(x,y)=∑ni=1(xi−x¯¯¯)(yi−y¯¯¯)n−1
(18)
where xi and yi are the ith values (data points) of the features x and y, and x¯¯¯ and y¯¯¯ are the mean values of the features x and y.

σx represents the standard deviation of x and σy represents the standard deviation of y and was calculated as follows:

σx=∑ni=1(xi−x¯¯¯)2n−1−−−−−−−−−−−−√
(19)
Similarly, the standard deviation of y can be calculated using Eq. 19 with values related to y. Table 4 shows the correlation coefficients among the features calculated from a dataset that has no missing values. These coefficients are used when determining the distance between the features of the candidate and the donor instances in the next step. It should be noted that the correlation values of Table 4 are from the dataset that has no missing values. These values will change if there are missing values present, as shown in Table 3. In such a case, the correlation values between the features need to be computed according to the specifics of the dataset.

Table 4 Correlation values among the features
Full size table
Step 3: determine the distance between the donor instance/s and the candidate instance
In this step, the distance between the donor instance and the candidate instance was computed. The distance between any two features can be computed using one of many techniques such as Euclidean, Manhattan, or cosine. Of these techniques, the Euclidean distance measure is the most commonly used as it is a good choice when all the input features contain similar types of data [21]. This fitted the input data of our framework that consisted of sentiment intensity scores scaled between − 1 and + 1. The distance between a candidate instance xi and donor instance xj. was computed across all the features m that have sentiment intensity values in both instances. Mathematically, the distance between the two instances xi and xj was calculated as follows:

d(xi,xj)=∑k=1m(xik−xjk)2−−−−−−−−−−−−√
(20)
where xi and xj represent the candidate and the donor instance, respectively, and m represents the number of features that have sentiment values in the features of both the instances.

As discussed earlier, the T-NN approach uses a weighted version of Euclidean distance to accurately determine the distance between the donor and the candidate instances. This is done by using weight wk from the perspective of the feature whose value needs to be imputed. This weight is applied while computing the distance between any two features of the candidate and donor instances. wk is calculated by determining the correlation between the candidate feature and other features. Once the wk values between the candidate and each other feature are determined, the distance between xi and xj is calculated as shown in Eq. 21:

d(xi,xj)=∑k=1mwk(xik−xjk)2−−−−−−−−−−−−−−√
(21)
where wk represents the weight for the feature k and is calculated as:

wk=1−|ρkl|
(22)
Here, ρkl represents the Pearson’s correlation coefficient between the features k and l.

Step 4: find the threshold parameter (T) to determine which donor instances will be used to impute the value of the required feature in the candidate instance
The threshold parameter (T) is one of the core components of the T-NN approach. As explained in the previous section, using the threshold range to select the donor instances is more suitable compared to selecting instances based on counts. Assuming a given dataset X and a set of donor instances D, where D ⊆ X, the threshold value T helps to select a subset D¯¯¯¯ from the donor instances from D. If Sdc is the distance of the closest donor instance dc to the candidate instance, then the members of D¯¯¯¯ are selected as:

{d∣d∈D,Sd−Sdc≤T}
(23)
The threshold value defines the closeness between the selected donor instances. Column d(xi,xj) in Table 5 shows the distance between candidate instance S002 and the other donor instances. Donor instance S489 is the most relevant donor with the closest distance to the candidate instance. The next column computes the distance between S489 and the other donor instances. If the T being considered is 1.5, then only S489 and S784 from the donor instances will be used to impute the value of feature R of S002. For a given dataset, T needs to be determined according to the understanding of the features within it and the count with missing values. This helped us to reduce errors in the estimation process compared to only randomly considering the value of T. In Sect. 4, we explain in detail the process of determining the value of T according to the underlying dataset. When the value of T is set to 0, only the donor instance that has the least distance from the candidate instance is selected.

Table 5 Forming a subset D¯¯¯¯ from the donor instances
Full size table
Step 5: imputing the value of the feature in the candidate instance from the donor instances within the threshold
The observed values in each feature j from D¯¯¯¯ were then averaged to determine the value of the candidate feature of instance xi where xi ϵ X.

xij=∑nk=1dkjn,d∈D¯¯¯¯
(24)
where d represents the donor instances in D¯¯¯¯ and n represents the total number of instances in D¯¯¯¯.

Figure 3 shows the process of imputing the sentiment intensity value of the missing features in a dataset. To summarize, the process of T-NN in imputing the value of a feature in a candidate instance from donor instances was as follows:

(1)
Identify all missing value locations in X.

(2)
For each missing value in the candidate instance xi on a specific feature kj, select the donor instances D based on the following conditions – (a) donor instances must have the value for that specific feature kj, (b) donor instances must at least have values for the features that have observed values in the candidate instance xi. The donor instances D = {d1, d2, d3 …. dm} are a subset of X (i.e., D ⊆ X).

(3)
Calculate the Pearson’s correlation coefficients among the features in X using Eq. 18.

(4)
Calculate the weighted Euclidean distance (Eq. 17) between donor instances in D and candidate instance xi. The weights for each feature are calculated using the PCC between all the features in step 3. Assuming that the data point xil has a value missing, where l represents a given feature with a missing value, then the weight for feature k is calculated as wk=1−|ρkl|. ρkl is the PCC between l and k. For example, with the missing value in xil, when calculating the distance between xi and dj, the weights for feature k are assigned based on the PCC value between feature l and k.

(5)
Sort the donor instances according to their distances

(6)
Select donors D¯¯¯¯ (where, D¯¯¯¯⊆D) falling within the threshold range. In this step, the donor instance with the least distance dc is selected first. Other donor instances are then selected if their distance from instance dc falls within the given threshold range.

(7)
The values from feature j of the selected donor instance D¯¯¯¯ are aggregated and assigned to feature l of the candidate instance xi (the datapoint with the missing value)

(8)
Steps 2 to 7 are repeated to impute all the missing values in X.

Fig. 3
figure 3
Sequence of steps in imputing the missing sentiment intensity values in a dataset

Full size image
Testing stage
The objective of this stage was to test the accuracy of the sentiment intensity values imputed in the previous step. The testing was done in two schemes, scheme A and scheme B. Each scheme uses a different dataset for the experimentation and the evaluation procedure and comparison to ascertain the accuracy of T-NN imputation. The details of the testing process of each scheme are explained in the next sub-sections.

Testing specifics in scheme A
Objective of scheme A
As mentioned in Sect. 3.2, in this study, we considered a dataset that had reviews for 3215 SaaS products. From these reviews, only 729 products were found to have an aggregated sentiment intensity score for all nine features. For scheme A, we created a dataset of 542 product instances from the 729 available which had the aggregated sentiment intensity values of all the features. The dataset for the experiment was created under the MCAR assumption by randomly inserting missing values into the features.

The core idea in scheme A is to replace the actual sentiment intensity values of random features and randomly selected instances with missing values. By using T-NN, the missing values were imputed and then compared with the actual values to ascertain their accuracy. This process was repeated 10 times with varying percentages of missing instances in each iteration. The values of missing percentages were 5, 10, 15, 20, 25, 30, 35, 40, 45 and 50. As shown in Table 6, the missing instances in the dataset varied according to the different values of the percentage considered. Furthermore, Table 6 also shows that, in the missing instances, a random number of features were marked as having missing values within each instance of the subsets. The features to have missing values in an instance were selected randomly but they should satisfy the condition that there are at least two features with observed values (sentiment intensity scores) in an instance. The other values can be missing. This condition was chosen to allow the imputation approaches used in this study to utilize the value from the existing feature to impute the missing values.

Table 6 Datasets description (scheme A)
Full size table
As shown in Table 6, for scheme A, the initial complete dataset consisted of 542 instances (each instance representing a SaaS product and its related sentiment intensity scores for the features). Table 6 illustrates that, for each of the generated datasets, the number of instances with missing values is increased by 5 percent. In each of the generated datasets, the number of missing values in each of the features is different as well. Figure 4 shows the steps in the testing process of scheme A.

Fig. 4
figure 4
Steps in the testing process of Scheme A

Full size image
Evaluation of the accuracy of the imputation values in Scheme A
To determine the accuracy of the imputed values in scheme A by T-NN, they were compared with the observed values. Other approaches from the literature, such as SGD, K-Neighbors, Mean, Median, SVD, DT, and EM are regularly used to impute the values in the missing instances. The performance of the approaches is quantified and compared by determining their root-mean-square error (RMSE) and the adjusted R-squared (Adj-R2).

Root-mean-square error (RMSE)
RMSE is a common metric used in regression analysis where the values to be compared are continuous. The RMSE is calculated as the standard deviation of the difference between the true and imputed values (residuals).

RMSE=∑ni=1(Xtruei−Xpredictedi)2n−−−−−−−−−−−−−−−−−−−−−⎷
(25)
where Xtruei represents the ith observed value of the feature X and Xpredictedi represents the ith imputed value of the feature X.

A smaller value of RMSE (closer to zero) indicates a better performance by an estimator on the imputed data. In other words, the RMSE values on an imputed dataset translate to the performance of the imputation (or prediction) approaches.

Adjusted R-squared (Adj-R 2)
R-squared computes the coefficient of determination. It measures the proportion of variance in the dependent feature according to the independent features. R-squared is calculated as the ratio of the sum of the squares of residuals to the total sum of squares, written as follows:

R2(y,y^)=1−∑ni=1(yi−y^i)2∑ni=1(yi−y¯¯¯)2
(26)
where yi is the true value for the ith instance in y, y^i is the corresponding predicted value and y¯¯¯ is the mean of y over n instances.

An R-squared value of 1 is the best outcome. One weakness of R-squared is that its value tends to increase with an increase in the number of features. Adj-R2(adjusted coefficient of determination) can be used instead, which penalizes the addition of features that do not bring significant improvement to the performance of the model.

AdjustedR2=1−(1−R2)n−1n−(p+1)
(27)
where n represents the number of instances and p the total number of features (independent features).

In addition to the above standard performance tests, a nonparametric significance test was also conducted to compare the difference in the performance of the methods. The Friedman [86] test statistics F is shown as:

F=12Nk(k+1)[∑j=1kR2j−k(k+1)24]
(28)
where k is the number of models being compared, N is the number of datasets and Rj represents the ranks obtained for the jth model.

Testing specifics in scheme B
Objective of scheme B
As mentioned in Sect. 3.2, in this study, there were only 729 products that had an aggregated sentiment intensity score for all nine features. For scheme B, we started with a dataset of 542 product instances from the 729 available ones. The remaining 187 instances were kept for testing purposes as explained later. The core idea for scheme B was to introduce missing values to the dataset by adding new data instances that contain missing values in random features and test the accuracy of T-NN in imputing them. Scheme B differs from scheme A in two ways:

Dataset Scheme B added additional data instances with missing values to the initial dataset. As seen in Table 6, in scheme A, all the datasets had the same number of total instances (542) with varying levels of instances with missing features. Scheme B, as shown in Table 7, starts with an initial dataset of 542 instances with no missing features in any of them. It then created 10 more datasets by adding new instances to the initial dataset. Adding new instances and features with missing values represented an online scenario where the incoming data can be incomplete to different degrees. The objective now was to test how effective and accurate the proposed T-NN approach is for imputing the missing values in such scenarios. The instances added to the initial dataset were randomly picked from the core SaaS dataset of 29,361 reviews from 3215 SaaS products [4]. The added instances might have had missing sentiment intensity values in one or more features, but they were thought to satisfy the condition of having at least two features with observed sentiment intensity values. So, of the 10 datasets created, the total number of missing values varied among the features. For example, as shown in Table 7, feature E representing ‘Resiliency,’ had the most missing values compared to the other datasets where feature M representing ‘Management or DevOps,’ had the least number of missing values among the datasets. The missing values in a feature across the datasets are a representation of the review dataset. A trained classifier, developed in [4], assigned each text segment of the review to the feature to which it related. On our considered dataset of 29,361 reviews, the output of the classifier suggested that most of the text segments are highly indicative of service feature M and least indicative of service feature E. This resulted in some service features of the SaaS products having relatively more missing sentiment intensity scores than the others, as the customers did not mention them in their reviews.

Testing approach Scheme B uses a separate machine learning-based approach for testing the accuracy of T-NN for imputing missing values. It consists of the following steps:

After the missing values in each dataset were imputed, the dataset was considered to be the training dataset. The 187 instances which had service intensity values in all the features from the initial dataset were termed the testing dataset.

In both the training and testing dataset, feature M which had the least number of missing values (from Table 7) was considered the output feature.

For the training dataset, SVM was used to train and determine the sentiment intensity value of the feature with label M based on the values of the other features. The training dataset was divided into 10 folds × 3 repetitions, and the accuracy of the prediction of each was evaluated using RMSE and R-squared.

Once the SVR model had been trained, it was applied to the unseen testing dataset that had 187 instances with a sentiment intensity value in each feature. While testing, the objective was to predict the values of feature M and compare them with the observed values of M in the testing dataset. The predicted output was then compared with the actual output and the accuracy of the approach was determined using RMSE and Adj-R2.

The SVR model was also compared with the baseline performance model using the RMSE and Adj-R2 techniques.

Apart from the dataset imputed from T-NN, the above process was also applied on the datasets imputed from SGD, K-Neighbors, Mean, Median, SVD, Decision Tree, and EM approaches. The value in feature M was predicted in each approach and compared with its actual value.

Table 7 Datasets description (scheme B)
Full size table
Performance evaluation with support vector for regression (SVR)
The support vector machine was initially introduced by Vapnik [87] to solve binary classification problems [87, 88]. Beyond that, the idea of support vector machines has been extended and successfully used in regression problems as SVR [89, 90]. A support vector machine belongs to the family of margin-based classification (and regression) approaches where the hyperplane that separates the instances is constructed using the maximum margin in the feature space. The generic estimation function for SVR can be written as:

f(x)=w⋅x+b
(29)
SVR is implemented using different kernels. These kernels can be linear or nonlinear. Nonlinear kernels such as radial basis function (RBF) are more useful when the training data cannot be separated by a straight line (linearly). A linear SVR kernel function takes the form:

k(x,xi)=x⋅xi
(30)
whereas the RBF function kernel is as follows:

K(x,xi)=exp(−γ||x−xi||2),γ>0
(31)
In scheme B, SVR was used during cross-validation and on the test dataset. We also used SVR in tuning the T-NN for the optimal threshold value in both schemes (from Figs. 2 and 3). The evaluation was performed with both the linear and RBF kernels. All the performance results by SVR in scheme B were compared to a baseline estimator implemented using a zero-rule algorithm with the median as a central tendency [91]. Figure 5 shows the steps taken in the testing process of scheme B.

Fig. 5
figure 5
Steps in the testing process of Scheme B

Full size image
Determining the optimal value for the threshold parameter (T)
Tables 6 and 7 show the different level/s of missing instances and features used in schemes A & B, respectively. For each dataset, we needed to determine the threshold parameter T. We did this by tuning and determining the optimal threshold value for parameter T according to the following:

Step 1 We considered each dataset that has different percentages of missing values i.e., 0, 5, 10, 15, 20, 25, 30, 35, 40, 45 and 50.

Step 2 We defined a possible set of T as {0, 0.02, 0.04, 0.06, 0.08, 0.10, 0.12, 0.14, 0.16, 0.18, 0.20, 0.22, 0.24, 0.26, 0.28, 0.30, 0.32, 0.34, 0.36, 0.38 and 0.40}. In other words, the possible set represents the range from which the value of T was chosen.

Step 3 For each value of the possible set of T, we applied the proposed T-NN approach to impute the missing values in each of the datasets from step 1. This resulted in 11 x 21 imputed datasets.

Step 4 Using SVR, we ran the 10 x 3 folds cross-validation on each of the imputed datasets from step 3 (using RMSE as the evaluation criteria).

Step 5 We selected the T value that results in the lowest RMSE score aggregated over all the datasets with different missing percentages.

Tables 8 and 9 depict the RMSE scores on each set of the imputed datasets used in schemes A and B. For scheme A, the aggregated RMSE score tends to drop with an increase in the threshold value before rising again as shown in Fig. 6. The lowest aggregated RMSE score of 0.12902 is observed at the threshold value (T) of 0.22. For scheme B, the RMSE value also decreases with an increase in the T value before increasing again, as shown in Fig. 7. From Table 9, it can be seen that the lowest aggregated RMSE score of 0.15543 is observed at the threshold of 0.16 before rising again. The RMSE scores in scheme A are relatively lower than the corresponding scores observed in scheme B. This difference in the RMSE scores is because the datasets in scheme B contain more instances with missing values compared to the datasets in scheme A. Hence, based on the number of missing feature values and instances in the dataset, it is important to ascertain the value of T created as it impacts performance accuracy. Using the lowest values of T in each scheme, the performance results of the proposed T-NN approach in imputing the missing features are compared with the results of other approaches in the next section.

Table 8 RMSE values by increasing the threshold values (scheme A)
Full size table
Table 9 RMSE values by increasing the threshold values (scheme B)
Full size table
Fig. 6
figure 6
RMSE values averaged on all the datasets with different missing percentages of Scheme A

Full size image
Fig. 7
figure 7
RMSE values averaged on all the datasets with different missing percentages of Scheme B

Full size image
Comparing the performance of T-NN with other approaches while imputing the sentiment intensity score of missing features
Scheme A
Figure 8 plots the performance of the proposed T-NN approach against SGD, K-Neighbors, Mean, Median, SVD, Decision Tree, and EM approaches in terms of their RMSE scores on different datasets. Table 10 shows the RMSE values of each approach on different datasets that have varying levels of missing values. Results shown in the table illustrate that T-NN delivers lower RMSE scores on each dataset, outperforming the other imputation approaches. The performance of EM is the worst of all the other techniques, including the univariate mean and median imputation approaches. The performance of SGD, although it is close to the proposed T-NN approach, is not better. As depicted in Fig. 8, for all the methods, the RMSE scores are different for different missing value percentages in the dataset. This is because each of the datasets is generated with missing values in random rows and columns making them different from the others. Figure 9 and Table 11 show the performances of the approaches in terms of the Adj-R2 metric. From the results, we can see that, although there are approaches depicting lower values (i.e., a lower proportion of variance is explained by the fits), T-NN fits relatively better than the others. The negative values for the Adj-R2 in the cases of EM, DT, and SVD indicate that these models have the worst fit on the datasets. The result of the Friedman test implemented under the significance level α = 0.05 shows the statistics of 68.267 with p < 0.001. This is enough evidence to reject the null hypothesis, hence indicating the differences in the performances of the methods.

Fig. 8
figure 8
RMSE score comparison on all imputed datasets with different missing percentages

Full size image
Table 10 RMSE scores with different missing percentages (scheme A)
Full size table
Fig. 9
figure 9
Adj-R2 score comparison on all imputed 40 datasets with different missing percentages

Full size image
Table 11 Adj-R2 values with different missing percentages (scheme A)
Full size table
Scheme B
As previously discussed, in scheme B, the datasets were generated by adding instances with missing values to the complete dataset. Each generated dataset had different percentages of missing values with a different total number of instances. The datasets were then imputed using the imputation approaches selected in this study. In scheme B, as the actual values for the missing data points were unknown, the datasets imputed by the chosen approaches were evaluated using cross-validation and test dataset. The cross-validation and test performance on the imputed datasets were used to highlight the effectiveness of the imputation approaches on unseen data. As with the procedure applied in threshold tuning, the cross-validation in scheme B was also performed using SVR. All the experiments were performed using SVR with a linear kernel and a nonlinear (RBF) kernel and compared to a baseline implementation using a zero rule. The zero rule was implemented using the central tendency (mean values) of each feature. However, the results from our experiments show that the RBF kernel performs better than the linear kernel with lower error scores as presented in Figs. 10 and 11. Therefore, we report the results from the SVR with the RBF kernel only. In the next subsections, we show the results from performing cross-validation and test results.

Fig. 10
figure 10
Cross-validation results from Linear and RBF kernels (RMSE) on T-NN imputed datasets

Full size image
Fig. 11
figure 11
Cross-validation results from Linear and RBF kernels (Adj-R2) on T-NN imputed datasets

Full size image
Cross-validation results performed on the testing dataset
As mentioned in Sect. 3.4.2, the datasets imputed by all the selected approaches in this study were evaluated with 10 × 3 folds cross-validation. Table 12 shows the RMSE scores for all the approaches during cross-validation. The results show that the proposed T-NN outperforms all the other imputation approaches with the lowest RMSE scores on the datasets with different missing value percentages. Similar to the results in scheme A, the performance of the EM approach is the worst of all the other imputation approaches. Overall, the RMSE error increases with the gradual addition of missing values, as seen in Fig. 12. The Adj-R2 from Table 13 also shows that the proposed T-NN consistently fits better when compared to the other approaches across the datasets (although a lower proportion of variance is explained by the fit). Figure 13 shows a decrease in the Adj-R2 scores as the number of missing values in the datasets increases.

Table 12 Cross-validation results with RMSE values using SVR on imputed datasets (scheme B)
Full size table
Fig. 12
figure 12
RMSE scores comparison on all imputed datasets with different missing percentages during cross-validation

Full size image
Table 13 Cross-validation results with Adj-R2 values using SVR on imputed datasets (scheme B)
Full size table
Fig. 13
figure 13
Adj-R2 scores comparison on all imputed datasets with different missing percentages during cross-validation

Full size image
In scheme B, it must be noted that, in each dataset, the imputed values are estimates which means that these values have associated corresponding errors. Any predictions made using the imputed datasets will propagate these errors. The interpretation and adjustment of this additional error are not within the scope of this study.

Validation on the test dataset
Once trained on the imputed datasets, the approaches in scheme B were tested on a validation dataset. This validation step is similar to scheme A in that the actual target values for which the predictions are made are known. The difference between these schemes, as discussed earlier, is that the models are trained on complete data in scheme A whereas, in scheme B, the models are trained on partially imputed datasets.

The RMSE scores on the test dataset imputed by all selected approaches are depicted in Table 14 and Fig. 14. The SVR model trained on the datasets imputed by the proposed T-NN approach has a lower RMSE score, compared to the models trained on the imputed datasets by the other approaches. The RMSE scores from the datasets imputed by the EM approach are the highest. Similarly, the Adj-R2 scores from Table 15 and Fig. 15 show that the SVR model fits better on the datasets imputed with T-NN, compared to the datasets imputed by the other approaches (although a lower proportion of variance is explained by the fit). The performance comparison of the SVR model with the baseline is depicted in Table 16. Table 16 shows the results from the experiments on datasets imputed by the T-NN approach. SVR performs better than the baseline with lower RMSE scores as shown in Fig. 16 and higher Adj-R2 scores presented in Fig. 17. A comparison of the performance of the SVR model from the training and test results is depicted in Figs. 18 and 19. From the experiments, it can be concluded that the proposed T-NN approach outperforms other selected imputation approaches such as EM, SGD, SVD, DT, kNN, and univariate mean and median imputations. The evaluation using both the schemes shows T-NN achieves lower RMSE scores and higher Adj-R2 scores compared to the others. However, in all cases, a lower proportion of variance is explained by fitting the SVR on the datasets imputed by all the selected imputation approaches. This can be attributed to the nature of the dataset used in this study.

Table 14 Test results with RMSE values using SVR on imputed datasets (scheme B)
Full size table
Fig. 14
figure 14
RMSE scores comparison on all imputed datasets with different missing percentages on the test dataset

Full size image
Table 15 Test results with Adj-R2 using SVR on imputed datasets (scheme B)
Full size table
Fig. 15
figure 15
Adj-R2 scores comparison on all imputed datasets with different missing percentages on the test dataset

Full size image
Table 16 SVR vs Baseline performance with RMSE and Adj-R2
Full size table
Fig. 16
figure 16
RMSE score comparison on imputed datasets using T-NN

Full size image
Fig. 17
figure 17
Adj-R2 score comparison on imputed datasets using T-NN

Full size image
Fig. 18
figure 18
SVR and Baseline evaluation comparison on T-NN imputed and test datasets

Full size image
Fig. 19
figure 19
SVR and Baseline evaluation comparison on T-NN imputed and test datasets

Full size image
Conclusion
Selecting the most suitable SaaS product is one of the main challenges businesses face. In this study, we utilized the well-architected frameworks offered by AWS and Microsoft Azure to identify the sentiment intensity scores of key SaaS features. The main contribution of this study is to infer the missing sentiment scores for SaaS quality aspects from the known ones. We conducted experiments to implement the proposed T-NN approach and evaluate it in two different schemes. In both schemes, the T-NN approach performs better than well-known imputation methods such as Traditional kNN, SVD, SGD, DT, EM, mean and median. The dataset that we used to test the performance of these techniques focuses on the SaaS reviews. As mentioned in Sect. 3.2, the sentiment intensity for the SaaS products has been normalized within the range of -1 to 1. In addition, there is no other requirement to make the application of the proposed T-NN approach agnostic to the underlying data. Sections 5.1 and 5.2 detail the process of testing the performance and accuracy of the different methods in two schemes. So, even though the data are taken from the domain of SaaS reviews, due to the agnostic nature of the way it is represented, the same level of performance accuracy can be observed if it is taken from a different domain, such as hotel reviews. Using the SaaS sentiment dataset, the approach proposed in this paper will be evaluated against other imputation approaches as part of the future work of this study.