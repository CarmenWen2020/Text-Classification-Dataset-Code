Fisher’s Linear Discriminant Analysis (LDA) has been widely used for linear classification, feature selection, and metrics learning in multivariate data analytics. To ensure high classification accuracy while optimally discovering predictive features from the data, this paper studied CDA, namely Combinatorial Discriminant Analysis that intends to combinatorially select a subset of features and assign weights to them optimally. CDA extents the Truncated Rayleigh Flow algorithm (Tan et al. in J R Stat Soc: Ser B (Stat Methodol) 80(5):1057–1086, 2018) and improves LDA estimation under k-sparsity constraint. The experimental results based on the synthesized and real-world datasets demonstrate that our algorithm outperforms other LDA baselines and downstream classifiers. The empirical analysis shows that our algorithm can recover the combinatorial structure of optimal LDA with empirical consistency.

Access provided by University of Auckland Library

Introduction
The Fisher’s Linear Discriminant Analysis (LDA) [2, 3] is a well-known technique for dimension reduction and metric learning [4, 5]. It has been widely used in many applications [6] such as face recognition, image retrieval, and etc. Typically, LDA finds the projection directions such that for the projected data, the between-class variance is maximized relative to the within-class variance, thus achieving the optimal discrimination. An intrinsic limitation of LDA is that its decision function in Eq. (1) relies on a well-estimated projection vector (a.k.a β) for (optimal) linear classification.

For example, let X and Y be two p-dimensional random vectors following two Gaussian distributions, X∼N(μ∗+,Σ∗) and Y∼N(μ∗−,Σ∗), which is with the same covariance matrix Σ∗ but with distinct mean vectors μ∗+ and μ∗−. For a new observation Z that is drawn from the two Gaussian distributions with equal prior probabilities, the Fisher’s linear discriminant analysis classifies z using the rule

f¯(z)=sign((z−μ)⊤β),
(1)
where sign(⋅)∈{−1,+1}, the optimal estimation of global mean μ is μ∗=(μ∗++μ∗−)/2, the optimal estimation of projection vector β should be β∗=Θ∗(μ∗+−μ∗−) and Θ∗=Σ∗−1. Note that above classification rule is optimal, if the data of two classes are drawn from two Gaussian distributions with the same covariance matrix but different means.

Though the optimal classification rule exists under the Gaussian distribution assumptions, the estimation of β using the given training data sets is somehow difficult. A straightforward solution is to use sample estimators to first calculate sample covariance matrix and sample mean vectors separately, then estimate β compositely via matrix inverse. However, For many applications, such as the micro-array data analysis and biomedical data analysis, the number of dimensions (denoted as p) is significantly larger than the number of samples (denoted as n), i.e., p≫n. In this case, the sample estimation of the covariance matrix is usually singular and non-invertible. Hence, the estimation of linear projection vector β is ill-posed under high dimension but low sample size settings (HDLSS) [7].

Our contributions. Inspired by the recent progress in combinatorial regression [8], this work studies a novel combinatorial Fisher’s discriminant analysis problem incorporating new sparsity assumptions. The existing work [9,10,11,12,13] makes specific sparsity assumptions on β or Σ−1, then estimates sparse LDA using either ℓ1-norm sparse inverse covariance estimator or direct ℓ1-norm sparse LDA estimators. Our work however assumes the true LDA projection vector β∗ is k-sparse [14], and proposes a new algorithm CDA for ℓ0-norm sparse LDA estimation with improved covariance matrix estimation under mild covariance structure conditions.

More specific, we derive CDA algorithm from the Truncated Rayleigh Flow (TRifle) Optimization Framework [1], which was originally designed to solve generalized eigenvalue problems under k-sparse constraints. To enhance TRifle, we (1) precondition TRifle with a better initialization to reduce the optimization error, (2) replace sample covariance matrix estimators with the shrunken one to marginalize the statistical error, and (3) correct the direction of gradient ascent with de-biased estimator [15, 16] for faster statistical convergence. The enhanced optimization process is named as Conditioned Rayleigh Flow. We evaluate CDA using both a synthesized dataset and real-world datasets. The experimental results using the synthesized dataset show that CDA outperforms common combinatorial linear models, including Truncated Rayleigh Flow (TRifle) [1] and Orthogonal Matching Pursuit (OMP) [17], for combinatorial structure recovery (retrieving nonzero elements in β), with higher precision, recall and F1-score.

Backgrounds and related work
Recently, many efforts have been devoted to bear on such HDLSS problem using shrunken estimators [6, 9,10,11,12,13, 18, 19] Generally, these studies assume that the data for training and classification is randomly drawn from two (unknown) Gaussian distributions N(μ+,Σ) and N(μ−,Σ) with certain priors. Given n training samples, LDA needs to estimate the linear projection vector β, which is structured as β=Σ−1(μ+−μ−) [3]. However, when the number of dimensions (denoted as p) is significantly larger than the number of samples (denoted as n), i.e., p≫n, the sample estimation of the covariance matrix Σ is usually singular and non-invertible. In this case, it is impossible to use the inverse of sample covariance matrix to estimate β. Related work which intends to solve this problem can be categorized into two folders as follow.

Covariance-regularized Estimation To estimate β, these studies [6, 9, 18] proposed to first estimate the covariance matrix Σ (or the inverse of covariance matrix Σ−1) using shrunken covariance/precision matrix estimators, such as the empirical shrinkage estimator or Graphical Lasso [20], to obtain an estimation of inverse covariance matrix (frequently denoted as Θ), and then multiply it with the estimated mean vectors to compute β in an indirect manner. This type of estimators used to be not favored by the community, as the estimation of inverse covariance matrices is more computational expensive [10] while relying on additional assumptions to the sparsity of covariance matrices.

Direct Sparse LDA Estimation Instead of estimating Σ and μ+,μ− separately, this line of research [10,11,12,13, 19] proposed to estimate β directly from the data (sometimes, with sample estimators [10, 11]). Specifically, [11] studied the algorithm to estimate β through maximizing fisher’s information; [12] used thresholding technique to pursue a sparse representation of β; [10] leveraged Dantzig Selector to estimate a sparse approximation of β under ℓ1-norm sparsity; most recently, [13] studied using Lasso-like estimator to compute the sparse approximation of β with ℓ1-norm sparsity and least-square error [19].

Preliminaries and problem formulation
In this section, we first review the state of the art of LDA, then formulate the problem of the proposed research.

Sample LDA for binary classification
With the labeled data pairs (x1,y1)⋯(xn,yn) (where xi∈Rp is a p-dimensional vector and yi∈{−1,+1} with 1≤i≤n) as inputs, LDA first estimates the sample covariance matrix Σ¯ using the pooled sample covariance matrix estimator with respect to the two classes [3], as well as the mean vectors μ¯+ and μ¯− for the two classes. Given all estimated parameters Σ¯ (and Θ¯=Σ¯−1 as the inverse of sample covariance), μ¯+ and μ¯−, the LDA model classifies a new data vector x as the result of

f¯(x)=sign((x−μ¯++μ¯−2)⊤β¯+logπ+π−),
(2)
where β¯=Θ¯(μ¯+−μ¯−) relies on the accurate estimation of inverse covariance matrix Θ¯=Σ¯−1. The function sign(⋅)∈{−1,+1}, π+ and π− refer to the (foreknown) frequencies of positive and negative samples in the whole population respectively.

Combinatorial discriminant analysis problem
Given labeled data pairs (x1,y1)⋯(xn,yn) for training, we assume each tuple (xi,yi) (1≤i≤n) is i.i.d drawn from a joint distribution denoted as (X,Y). Specifically the p-dimension vector xi is drawn from two (unknown) Gaussian distributions N(μ∗+,Σ∗) and N(μ∗−,Σ∗) with equal priors. While label yi=+1 when xi is drawn from N(μ∗+,Σ∗), corresponding label yi=−1 when xi is drawn from N(μ∗−,Σ∗).

Problem. Given an integer k≤p, our research intends to find the sparse LDA estimator βˆ with k nonzero coefficient that can recover the combinatorial structure of the optimal LDA estimate β∗. More specific, let us denote supp(β) as a function that maps a vector β to the set of the nonzero coefficients in the vector (i.e., support set of the vector). Thus, we formulate the problem of combinatorial discriminant analysis as:

βˆ←argmax∀β∈Rp|supp(β∗)∩supp(β)|s.t. R(β)≤εand|supp(β)|=k,
(3)
where R(β)≤ε refers to the bound of the expected error rate of β for sparse LDA classification [21] and R(β) has been defined in Sect. 3 of [10]. This problem intends to search a subset of k features from the overall p features and assign each feature a weight, for optimal linear classification with generalizability ensured. Considering the combinatorics in the subproblem [22], the overall problem should be NP-hard. Note that we study binary LDA problem in this work, while multiclass LDA could be adopted through one-vs.-rest transformation.

The proposed algorithm
In this section, we introduce CDAwith two steps: Covariance Matrices Estimation and Conditioned Rayleigh Flow. CDA outputs an near-optimal estimation of the projection vector denoted as βˆt, where t refers to the number of iterations for Conditioned Rayleigh Flow. CDA replaces β¯ with βˆt in Eq. (2) as the classification rule.

Covariance matrices estimation
Given the training data and the tuning parameter λ>0, this step estimates three covariance matrices as the input of the Conditioned Rayleigh Flow.

Between-class covariance matrix estimation
Given the labeled data pairs (x1,ℓ1)⋯(xn,ℓn) for training, the algorithm first sorts the the training data, using the label, into the two sets X+ and X− for the data with positive and negative label respectively. It estimates the mean vectors μ¯+=1|X+|∑xi∈X+xi, and  μ¯−=1|X−|∑xi∈X−xi for the two classes separately. Based on the overall mean μ¯ and the mean vectors of the two classes μ¯+, μ¯−, the algorithm uses the sample estimator as the Between-Class Covariance Matrix [23] such that:

Σˆb=∑ℓ∈{±1}|Xℓ|n(μ¯ℓ−μ¯)(μ¯ℓ−μ¯)⊤.
(4)
Shrunken within-class covariance matrix estimation
Given the training data and the mean estimation, the algorithm first estimates the sample within-class covariance matrix:

Σ¯w=1n⎧⎩⎨∑ℓ∈{±1} ∑xi∈Xℓ(xi−μ¯ℓ)(xi−μ¯ℓ)⊤⎫⎭⎬.
(5)
It is obvious that, when p≫n (under HDLSS settings), the sample covariance matrix Σ¯w is usually singular [24]. To obtain on a robust estimation of within-class covariance matrix, CDA proposes to use the inversion of Graphical Lasso estimator [9]. Such that Sˆw=Θˆ−1w and

Θˆw=argminΘ≥0(tr(Σ¯wΘ)−logdet(Θ)+λ∑j≠k|Θjk|),
(6)
where Θˆw is the Graphical Lasso estimator, Θ≥0 refers to the positive-semidefinite constraint, |Θi,j| refers to the absolute value of the matrix element Θi,j, and the tuning parameter λ is used to control the sparsity. Note that Θˆw is sparse and Sˆw is biased (i.e., Sˆ−1wΣ¯w≠I) due to the induced sparsity.

De-biasing covariance estimation
As was mentioned, we set covariance structure on mild conditions and thus need to de-bias/de-sparse the within-class covariance estimator [16]. Given the sample estimator Σ¯w, we measure the bias of Sˆw (or Θˆw) as Bˆw and correct the bias with Cˆw as follow.

Bˆw=(ΘˆwΣ¯w−I), and Cˆw=((I−Bˆw)Θˆw)−1Bˆw.
(7)
CDA lowers the bias of Sˆw via Sˆw+Cˆw. In fact, Sˆw+Cˆw=(2Θˆw−ΘˆwΣ¯wΘˆw)−1 is equivalent to the inverse matrix of the de-biased graphical Lasso estimator  [16] with a faster statistical convergence rate.

Statistical convergence of covariance
In above sections, we present three within-class covariance matrix estimators Σ¯w, Sˆw and Σˆw that improve the estimation of Σ∗ step-by-step. The sample estimator Σ¯w is an unbiased estimator of Σ∗ with statistical errors due to the finite sample estimation, however Σ¯ is often singular under high-dimensional and low sample size (p≫n) settings.

Remark 1
Let’s denote Θ∗=Σ∗−1 and d as the maximal node degree of the graph of Θ∗. The estimator Sˆw is the inverse matrix of graphical Lasso estimator Θˆw (i.e., Sˆw=Θˆ−1w) which provides an improved non-singular estimator of Σ∗ with a statistical convergence rate ∥Θˆw−Θ∗∥2F=O((p+d)logp/n) under mild conditions on the eigenvalues of Θ∗ and sparsity assumptions [25, 26]. The de-biased estimator (Sˆw+Cˆw) further improves Sˆw while its inverse matrix (Sˆw+Cˆw)−1=(2Θˆw−ΘˆwΣ¯wΘˆw) converging to Θ∗ with a sharper ℓ∞-norm statistical convergence rate O(logp/n−−−−−−√) under milder conditions [16].

With improved estimation of covariance matrices, CDA is expected to outperform the existing solution to the problem.

figure h
figure i
Conditioned Rayleigh flow
Given the desired number of nonzero coefficients k, CDA adopts a Conditioned Rayleigh Flow listed in Algorithm 1 to estimate a k-sparse discriminant projection vector βˆt. Conditioned Rayleigh Flow is a gradient ascent derived from [1], which maximizes the Fisher’s information for optimal discrimination [2] under ℓ0-norm constraint. The step-size is η=1.0×10−4, the maximal tolerated perturbation is tol=1.0×10−3, and the maximal number of iterations is set to T=200 in our research.

Specifically, the proposed Conditioned Rayleigh Flow algorithm consists of two major steps:

Step 1. Preconditioning with Initialized βˆ0—In this step, given the shrunken within-class covariance matrix (Sˆw+Cˆw) and the mean vectors μ¯+,μ¯−, the algorithm first leverages a Scout [9] estimator (in Line 3 of Algorithm 1) to compute βˆd, where the de-biased linear model [15, 16] has been used. Further, given the desired number of nonzero elements k, hard thresholding and normalization (in Lines 4-5 of Algorithm 1) are used to project βˆd into the desired region to initialize βˆ0 (i.e., |βˆ0|0≤k and |βˆ0|2=1), for gradient ascent. Note that the HardThreshold function used in Algorithm 1 is introduced in Algorithm 2. The further theoretical analysis on the preconditioned βˆ0 is presented in Theorem 2.

Step 2. Conditioned Gradient ascent with Projection—Given the de-biased within-class covariance matrix (Sˆw+Cˆw) and between-class covariance matrix Σˆb, the proposed algorithm CDA indeed intends to find the βˆ that maximize the Fisher’s information:

	(8)
where βˆ⊤Σˆbβˆβˆ⊤(Sˆw+Cˆw)βˆ refers to the generalized Rayleigh quotient based on the within/between-classes covariance matrices that have been used for Fisher’s information maximization. In order to achieve the goal, a gradient ascent algorithm is proposed to minimize β⊤(Sˆw+Cˆw)βˆ/βˆ⊤Σˆbβˆ under the same constraint. Instead of minimizing the quotient of βˆ⊤(Sˆw+Cˆw)βˆ and βˆ⊤Σˆbβˆ, the algorithm indeed minimizes the gap between them (Rayleigh flow). Starting with βˆ0, the algorithm iteratively update βˆt (t=1,2,…) using the gradient ∇t−1 (estimated in Lines 10–12 of Algorithm 1). Specifically, the gradient is estimated (in Line 12 of Algorithm 1) as ∇t−1=Σˆbβˆt−1−(Sˆw+Cˆw)βˆt−1/ρt−1, where ρt−1 (estimated in Line 10 of Algorithm 1) is an adjusting parameter that adjusts gap to approximate the quotient minimization. Through projection, the algorithm diffuses βˆt in the desired region after each iteration, so as to meet sparsity constraints.

Note that the major contributions made in our research on top of [1] is that we propose to leverage preconditioning initialization and conditioned gradient estimation to improve the performance of optimization and estimation, so as to advance the state-of-the-art (which was based on the sample-based estimators) with significant performance enhancement.

The CDA Classifier
After obtaining the output of Algorithm 1, i.e., βˆt, given any p-dimensional vector ∀x∈Rp for classification, the proposed algorithm classify x using the following linear classification rule:

sign(x⊤βˆt−(μ¯++μ¯−)⊤2βˆt+log π+π−),
(9)
where μ¯+ and μ¯− refer to the sample mean vector of the two classes.

Algorithm analysis
Here we introduce the main result of analysis, where we first introduce the key assumptions and main results of the algorithm analysis, then present theoretical results of [1] and two key lemmas that help obtain our main result by reusing the results in  [1], finally the sketched proofs of the two lemmas are given.

Main results: assumptions and theorems
Suppose n samples are randomly drawn from (unknown) Gaussian distributions N(μ∗+,Σ∗) and N(μ∗−,Σ∗) with equal priors, then the optimal projection vector β∗ should be β∗=Σ∗−1(μ∗+−μ∗−). Let β¯∗ denote the normalized optimal projection vector β¯∗=β∗/|β∗|2. We focus on the Euclid distance between the estimated βˆt and β¯∗ i.e., |βˆt−β¯∗|2 and how it converges with the number of dimensions p and the number of training samples n, under the structural assumption as follow.

Assumption 1
Let’s denote Θ∗=Σ∗−1. The optimal LDA [2] estimator β∗=Θ∗(μ∗+−μ∗−) should be s-sparse i.e., |β∗|0=s and 1≤s<p.

Assumption 2
We assume all training/testing samples are realized from a random vector X, where |X|2≤L. Thus, there has |μ∗+−μ∗−|2≤2L, |μ¯+−μ¯−|2≤2L, |μ∗−−μ¯−|2≤2L, and |μ∗+−μ¯+|2≤2L.

Assumption 3
[25] There exists a positive constant B that bounds the eigenvalues of Σ∗ such as 1/B≤λmin(Σ∗)≤λmax(Σ∗)≤B.

Assumption 4
With appropriate setting of k and k≥s, we assume that supp(β∗)⊆supp(βˆh), where βˆh is defined in Algorithm 1. Note that βˆh is the hard-thresholding result of βˆd with top-k elements preserved in Algorithm 1, there thus has supp(βˆh)⊆supp(βˆd).

Assumption 5
To simplify our research, we suppose the data samples are well normalized. In this way, for ∀1≤i,j≤p and i≠j, there has |Σ∗i,j|≤|Σ∗i,i|.

Based on above assumptions, we make the main theorem as follow.

Theorem 1
(Main Result) The ℓ2-norm convergence rate of βˆt (CDA after t iterations) in high probability should be:

|βˆt−β¯∗|2≤O((νt|β∗|2+1)(2k+s)logpn−−−−−−−−−−−√).
(10)
where β¯∗=β∗/|β∗|2 normalizes the optimal LDA, k=|βˆt|0 refers to the desired number of nonzero elements, s=|β∗|0 refers to the number of true nonzero elements. Moreover, ν∈(0,1) referring to the rate of error reduction per iteration.

The analytical framework based on truncated Rayleigh flow
The convergence rate of Truncated Rayleigh Flow [1] is sensitive to (1) the initial setting βˆ0 in our algorithm, (2) the estimation error of Within-Class and Between-Class covariance matrices, and (3) how does s and k match. Thus, the earlier result can be summarized as:

|βˆt−β¯∗|2≤νt⋅|βˆ0−β¯∗|2+ξ∗ρ(Σ∗−(Sˆw+Cˆw),2k+s)2+ρ(Σ∗b−Σˆb,2k+s)2−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−√,
(11)
where ξ∗ denotes as a scalar that depends on Σ∗, μ∗+ and μ∗−, ρ(E,2k+s)=supv∈Rp,|v|0≤2k+s,|v|2=1|vTEv| and E is a p×p matrix here. There has ρ(Σ∗b−Σˆb,2k+s)≤O((2k+s)⋅logp/n−−−−−−−−−−−−−√), according to [27].

To obtain our results in Theorem 1, our work introduce and prove two lemmas as follow.

Lemma 1
Let denote β¯∗=β∗/|β∗|2 and βˆ0 refers to the normalized vector in Line 6 of Algorithm 1 (and |β¯∗|2=|βˆ0|=1). There has

|βˆ0−β¯∗|2≤O(|β∗|−12klogpn−−−−−−√),
(12)
in high probability.

Lemma 2
Suppose s refers to the number of the nonzero elements in β∗, i.e., |β∗|0=s. Given any integer k and 2k+s≤p, there has:

	(13)
in high probability.

Considering the two constants ν and ξ∗ already defined in the earlier work [1], we can easily combine the results of Lemmas 1 and 2 with the results listed in Eq. (11). The proofs of above two lemmas are in the following section.

Experiments
In this section, we report our experiments using both synthesized and real-world datasets.

Evaluation using the synthesized dataset
In this experiment, we evaluate CDA on the multivariate Gaussian data under HDLSS settings. Specifically, we intend to (1) study the performance of CDA to recover the combinatorial structure of the sparse projection vector β, and (2) understand the performance of CDA for classification task.

Data synthesis
The synthetic data are generated by two predefined Gaussian distributions N(μ∗+,Σ∗) and N(μ∗−,Σ∗) with equal priors. The settings of μ∗+, μ∗− and Σ∗ are as follows: Σ∗ is a p×p symmetric and positive-definite matrix, where each element Σ∗i,j=0.8|i−j|, 1≤i≤p and 1≤j≤p. μ∗+ and μ∗− are both p-dimensional vectors, where μ∗+=⟨1,1,…,1,0,0,…,0⟩T (the first 10 elements are all 1’s, while the rest p−10 elements are 0’s) and μ∗−=0. (Settings of the two Gaussian distributions first appear in [10].) In our experiment, we set p=200. Note that, under this setting, the optimal projection vector should be β∗=Σ∗−1(μ∗+−μ∗−) and the normalized vector β¯∗=β∗/|β∗|2, which is with totally 11 nonzero elements—i.e., 11 variables/features are selected into the optimal discriminant analysis.

Baseline algorithms and settings
To understand the performance of the proposed algorithm, we compare CDA with following baselines:

TRifle and Rifle—TRifle is directly brought from [1] using the sample estimation of covariance matrices. Pseudo-inverse of the sample within-class covariance matrix is used for the algorithm initialization, when the inverse of the sample covariance matrix doesn’t exist. Note that the parameter k is also used here to control the number of nonzero elements. Rifle leverages the common Rayleigh Flow optimization process to compute the projection vector using the sample covariance matrix estimation and without any sparsity constraint.

OMP and SDA—The OMP algorithm is derived from Orthogonal Matching Pursuit [17], which approximates a sparse projection vector for discrimination under ℓ0-norm sparsity constraint with a least-square loss [13, 19]. The desired number of nonzero elements k is used here. In addition to ℓ0-norm sparsity, SDA is derived from Scout [9] using Graphical Lasso estimator [20] (using ℓ1-norm sparsity). βˆ0 in the Line 3 of Algorithm 1 should perform the same function as this algorithm. Note that the tuning parameter λ is used here for shrunken covariance matrix estimation,

All these algorithms first estimate the projection vector βˆ, then classify the new vector using the same rule in Eq. (2). To simulate the HDLSS settings, we train CDA and baseline algorithms, with 10 to 100 samples randomly drawn from the distributions with equal priors, and test the two algorithms with 500 samples. For each settings, we repeat the experiments for 100 times and take the averaged results. Further, a grid search algorithm is used to evaluate the algorithms under various parameter (i.e., k and λ) settings.

Fig. 1
figure 1
Performance on ℓ2-norm estimation error

Full size image
Estimation error and consistency
First of all, to verify the consistency of proposed estimators, we present the ℓ2-norm error of the estimated projection vector (i.e., |βˆt−β¯∗|2 for CDA) and compare it to other algorithms including OMP, SDA, TRifle and Rifle. In Fig. 1a, the overall ℓ2-norm error of all algorithms under varying training dataset size is presented, where all algorithms are tuned with the best parameters (through grid search and three-fold cross validation) in each setting. For fair comparison, vectors estimated by OMP, SDA, TRifle and Rifle are normalized,and compare to β¯∗. It is obvious that CDA is with lower ℓ2-norm error than all baseline algorithms, when sample size is between 30 and 70. Figure 1b illustrates the ℓ2-norm estimation error using CDA with varying desired number of nonzero elements k (while λ is set to the best), where we can see CDA gets its lowest ℓ2-norm error when k=11. The descending trend of |βˆt−β¯∗|2 shown in Fig. 1b demonstrates the potentials of empirical consistency.

Fig. 2
figure 2
Performance on combinatorial structure recovery

Full size image
Combinatorial structure recovery
Further, Fig. 2 demonstrates the performance of CDA and baseline algorithms for combinatorial structure recovery. In Fig. 2a, the overall F1-score for combinatorial structure recovery of all algorithms under varying training dataset size is presented, where all algorithms are tuned with the best parameters (through grid search) in each setting. Compared to the optimal β∗, F1-score of the estimated βˆ takes both precision and recall of nonzero element retrieval into consideration, and it is calculated as

Precision=|supp(βˆ)∩supp(β∗)||supp(βˆ)|,
(14)
Recall=|supp(βˆ)∩supp(β∗)||supp(β∗)|,
(15)
F1-Score(βˆ,β∗)=2⋅|supp(β∗)|⋅|supp(βˆ)||supp(β∗)|+|supp(βˆ)|.
(16)
It demonstrates that CDA outperforms all baseline algorithms including OMP, TRifle, SDA and Rifle with higher F1-score, when sample size is larger than 30. Figure 2b, c illustrate the F1-score, Precision and Recall of nonzero element retrieval applying CDA with varying desired number of nonzero elements k (while λ is set to the best). Obviously, CDA is with its best F1-score when k=11, it also gets its best precision and recall when k=5 and k=11 respectively. The comparison shows the potential of CDA to select features optimally under HDLSS settings.

Fig. 3
figure 3
Performance comparison on classification

Full size image
Classification accuracy. We also compare the classification performance of CDA with baseline algorithms. Figure 3a presents the accuracy of each algorithm using the same parameters/settings that was used in Fig. 2a. Apparently, CDA outperforms all baseline algorithms. In Fig. 3b, we demonstrate the classification of CDA with varying k. We can also observe that, in our experiments, the performance of CDA is not significantly susceptible to the choice of k.

In summary, compared CDA to SDA, the performance advantage of CDA is contributed by the Conditioned Rayleigh Flow gradient ascent to refine βˆ0 (which is equivalent to SDA in our experiment); compared CDA to TRifle, the performance advantage of CDA is due to that we condition the truncated Rayleigh Flow gradient ascent with shrunken covariance matrix estimator and bias reduction.

Classification with benchmark datasets
In this experiment, we evaluate CDA on real-world datasets. We evaluate our algorithm using binary classification benchmarks—“Adult” and “Web” datasets imported from [28], where Adult dataset is with 123 dimensions (p=123) and Web is with 300 dimensions (p=300), respectively. To evaluate the algorithms under HDLSS settings, we train the algorithms using 20 to 100 samples randomly drawn from the datasets with equal priors (of the two classes).

Figure 4 presents the classification accuracy of CDA, baseline algorithms and downstream classifiers, including Support Vector Machine (SVM), the least-square discriminant analysis (Ye-LDA) proposed by [19], k-Nearest Neighbor (KNN), Decision Tree (DTree), Random Forest classifier (RFC) and Kernel SVM with Gaussian Kernel (NLSVM) for nonlinear classification. All algorithms are tuned with the best parameter/settings through grid search (e.g., searching the best from 1-NN, 3-NN, 5-NN...). It is obvious that CDA outperforms all other algorithms using both datasets under such HDLSS settings. The performance of LDA is not quite stable with the increasing number of training samples, since all these methods use Pseudo-inverse when the sample covariance matrix is singular.

Fig. 4
figure 4
Performance on adult and web datasets

Full size image
HDLSS classification with biomedical data
Two ultra-high dimensional biomedical datasets—Leukemia cancer datasets (p=7,128) [29] and Colon cancer datasets (p=2,000) [28] are used to further evaluate our algorithms. We compare CDA to Decision Tree, Random Forest, and SVM. With respect to HDLSS settings, we train these classifiers using 20 samples randomly drawn from the datasets with equal priors (n=20), and test the classifiers through cross-validation.

Table 1 demonstrates the averaged accuracy and F1-score with standard deviation of these algorithms on the two datasets after 100 rounds of cross-validation, where all algorithms are tuned with the best parameters. compared to [29], our experiment takes fewer samples for training, the performance however is still comparable to the previous work. Result shows CDA delivers one of the best performance with decent accuracy and F1-score in all experiments for the both datasets. While OMP delivers slightly better performance in Colon datasets, CDA outperforms OMP in Leukemia datasets. Similar patterns could be also found in the comparison between CDA and SDA. Though CDA only selects a small subset of features for classification, it could deliver comparable performance against the baselines.

Table 1 Accuracy and F1-score comparison between CDA and other baselines
Full size table
Combinatorial inference for tobacco-related behaviors
The frequent tobacco use is one of the leading causes of “preventable illness and death” in the world. This experiment proposed to use CDA to understand the tobacco-related behaviors is United States.

Data. We collected Foursquare check-in data of 120,853 United States Residence for 2-years (2011–2013) [30]. These human subjects are homed in 32 states of United States, where 16 states are with more frequent smokers than the national average (positive) and the rest are with less frequent smokers than the national average (negative). In this experiment, each state was first labeled according to its frequent smokers (as was mentioned). Then, the behavior pattern of each state was characterized using an averaged check-in frequency vector, where each dimension of the vector refers to the averaged frequency of check-ins on a certain type of venue (e.g., Bar, Gym and Church) per human subject per year in the state. There are totally 428 types of venues are considered in this experiments (i.e., p=428 and n=32).

Methodologies. In this experiment, we intend to analyze the correlation between check-ins and the tobacco use through CDA path—an combinatorial inference tool based on proposed algorithm. To achieve the goal, we first estimated the projection vectors βˆ using the increasing k (i.e., number of nonzero elements in the vector) from 1 to 428. We vary k from 1 to 428. For each k, to ensure the stability of CDA path, we repeat the βˆ estimation with sub-sampling for 100 times, and use averaging-and-truncation strategy to estimate the robust projection vectors (denoted as βˆk and k=1,2,…428).

We sort these vectors βˆk with k=1,2,…428, then track of the trends of each coefficient. With a specific setting of k=k′, suppose the jth element equals to 0—i.e., (βˆk′)j=0, it refers to that the corresponding behavior (such as visiting Gym) is not selected, when any k′ behaviors were considered. Intuitively, with a small k′, the selected behaviors are more important than unselected behaviors, in terms of discriminating frequent smoking or infrequent smoking populations. In this case, we defined that, when (βˆk′−1)j=0 but (βˆk′)j≠0, we call the corresponding behavior of (the jth coefficient) was picked-up by CDAat k′. On the other hand, when (βˆk′−1)j≠0 but (βˆk′)j=0, we call the corresponding behavior was rejected by CDA at k′. We, thus, record the last time that each behavior is picked-up by CDA and will be never rejected with increasing k.

Results. Figure 5 illustrates the CDA path of top 6 most significant behaviors with the largest absolute coefficients (when k=428). It shows the coefficient path based on CDA, when 1≤k≤50. When k=1, a single behavior of “Home.private” is firstly picked-up by CDA, but then rejected when k=2. In the same time, the behavior of “bar” is selected when k=2 without further rejection with increasing k (i.e., “Bar” behaviors is always selected for k≥3). The “Home.private” is picked-up again when k=12 with no further rejection. Other behaviors such as “Gym”, “Coffee Shop”, “Church” and “Train Station” are picked-up by CDA when k is relevantly small and are not rejected with increasing k. The result indicates that a state with more normalized check-ins (per human subject per year) in “Bar”, “Church”, and “Home.private” would have more frequent smokers. On the other hand, if a state is with more check-ins in “Gym”, “Train Station” and “Gym”, such state would have a smaller number of frequent smokers.

Fig. 5
figure 5
CDA Path of 6 most significant behaviors/venue types (i.e. Home.private, Bar, Church, Coffee Shop, Train Station, Gym selected from 484 Behaviors/Venue Types) as parameter k increases

Full size image
Note that this figure only includes the coefficient path of these six behaviors from k=1 to 50. They are all picked-up at a small k and not rejected (again) with increasing k. Many other behaviors are also picked-up at small k, however, they are all latterly rejected when k increases. We believe these 6 behaviors are most relevant to the level of frequent smokers in a state. All in all, it is quite obvious that the behaviors featured as “Home.private”, “Bar” and “Church” are positively related to the level of frequent smokers, while “Coffee Shop”, “Train Station” and “Gym” are negatively related to the level of frequent smokers. Intuitively, it is quite easy to understand the causation of “Bar” and “Gym” to smoking. We are very glad to observe some new patterns that link “Home.private”, “Coffee Shop”, “Church” and “Train Station” to smoking. In our future work, we will work with psychologists to understand the relevance of observed patterns.

Conclusion
In this paper, we studied the novel combinatorial discriminant analysis problem, and proposed CDA algorithm that can approximate the sparse estimation of the projection vector with desired number (denoted as k) of nonzero elements. The proposed CDA operates on top of a Conditioned Rayleigh Flow gradient ascent algorithm that leverages covariance-regularized initialization (βˆ0) and de-biased shrunken covariance estimators [16] to further improve TRifle [1]. The experiment results show that CDA outperforms a brunch of the existing sparse LDA estimators with lower error for projection vector estimation under HDLSS settings while enjoying better accuracy for combinatorial structure recovery; CDA also performs better than the sparse LDA algorithms and downstream classifiers with higher accuracy for HDLSS data classification. Note that though binary LDA problem was studied in this work, the multiclass LDA could be easily adopted through one-vs.-rest transformation.