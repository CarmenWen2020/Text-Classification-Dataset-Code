Recommender systems that predict the preference of users have attracted more and more attention in decades. One of the most popular methods in this field is collaborative filtering, which employs explicit or implicit feedback to model the user–item connections. Most methods of collaborative filtering are based on matrix completion techniques which recover the missing values of user–item interaction matrices. The low-rank assumption is a critical premise for matrix completion in recommender systems, which speculates that most information in interaction matrices is redundant. Based on this assumption, a large number of methods have been developed, including matrix factorization models, rank optimization models, and frameworks based on neural networks. In this paper, we first provide a brief description of recommender systems based on matrix completion. Next, several classical and state-of-the-art algorithms related to matrix completion for collaborative filtering are introduced, most of which are based on the assumption of low-rank property. Moreover, the performance of these algorithms is evaluated and discussed by conducting substantial experiments on different real-world datasets. Finally, we provide open research issues for future exploration of matrix completion on recommender systems.

Access provided by University of Auckland Library

Introduction
The primary target of recommender systems is making recommendations for users to meet their needs or tastes based on their past behavior, thereby significantly saving the time for users to find useful information [1,2,3]. Rating is a typical user explicit feedback that visually reflects how much a user likes a related item. A multitude of ratings that users left on the Internet come into being a vast matrix, which is employed by recommender systems to make predictive recommendations. Since the number of items is huge, users tend to rate only a small part of the items, which leads to the sparsity of matrices [4,5,6]. This problem brings great difficulty in profiling users and items and has been a hot research issue for decades. Huang et al. [7] dealt with this problem by applying an associative retrieval framework and related spreading activation algorithms to explore transitive associations among users through their past feedback. Moshfeghi et al. [8] presented a framework taking item-related emotions and semantic data into concern to handle the sparsity problem in recommender systems. Li et al. [9] proposed a cross-domain framework that transferred user–item rating patterns from a dense auxiliary rating matrix in other domains to a sparse rating matrix in the target domain, so that two datasets in different domains worked together to solve the problem of sparsity. Implicit feedback is another useful information for recommender systems [10,11,12]. Although this type of feedback is not as clear as the explicit feedback, it is more common on the Internet. For example, click records, shopping carts of shopping sites, and favorites or forwardings of the articles are part of the implicit feedback [13]. Such feedback does not necessarily reflect the preference of users; however, it promotes the modeling of users or items and reduces the sparsity of matrix [14,15,16].

In general, there are correlations between different items or users, which can be measured by user–item interaction matrices generated from explicit or implicit feedback [17]. Collaborative filtering (CF) [18,19,20,21] discovers these correlations in a data-driven manner to make accurate recommendations and solve the sparsity issues. Matrix completion is extensively applied in CF [22,23,24,25], which attempts to recover missing values in interaction matrices. There are mainly two types of methods for CF with matrix completion: neighborhood-based models (NBMs) and latent factor models (LFMs). NBMs compute similarities between users to recommend items for a specific user according to ratings from other similar users. It is also possible to recommend items similar to a known favorite item for the user by computing the similarity between items. LFMs focus on profiling features of users and items and then project them into low-dimensional vectors, which are also called latent factors. It is a common method to reap feature matrices by matrix factorization or singular value decomposition (SVD) [26]. In some state-of-the-art methods, latent factors are obtained via neural networks [27]. For example, Cheng et al. [28] proposed wide and deep learning which jointly trained wide linear models and deep neural networks to improve recommender systems. Covington et al. [29] applied deep neural network (DNN) on Youtube website recommendations. He et al. [30] presented the neural network-based collaborative filtering (NCF) to express and generalize matrix factorization.

Fig. 1
figure 1
Singular values of rating matrices generated from two widely used datasets for recommender systems: a MovieLens-100K and b MovieLens-1M, where top 20% singular values account for 51.10% and 55.97% of the sum of all singular values, respectively

Full size image
Low-rank matrix is an essential assumption for matrix completion with LFMs, which considers that most information in interaction matrices is redundant and can be compressed. As shown in Fig. 1, only top 20% singular values record the most information of original matrices in recommender systems. This motivates us to enforce low-rank property on interaction matrices. Existing methods have also proved the effectiveness of low-rank property in solving sparsity issues [31,32,33]. Many approaches have been investigated for low-rank matrix completion, including low-rank matrix factorization [34,35,36,37], nuclear norm heuristic, singular value thresholding (SVT) [38] and robust principal component analysis (Robust PCA) [39, 40], etc. In this paper, we provide a review on recommender systems with matrix completion techniques, most of which follow the low-rank assumption. It is noted that although these methods are developed with the low-rank assumption, they do not necessarily minimize the rank of the matrix explicitly. Therefore, the review is divided into several aspects, i.e., matrix factorization models, neural network models and rank minimization models. Only rank minimization methods optimize the rank function explicitly. The main contributions of this paper are listed as follows:

We look into the matrix completion approaches used in recommender systems, including matrix factorization models, neural network models and rank minimization models, which cover primary techniques of matrix completion-based recommender systems. The advantages and drawbacks of these models are also analyzed.

Substantial experiments on rating and top-k recommendation prediction that are typical applications of recommender systems are conducted for the selected well-known and state-of-the-art algorithms.

According to the experimental observation, we analyze and summarize existing models of recommender systems with matrix completion, and discuss the challenges and potential research directions, which may bring insights for readers.

For the rest of this paper, we introduce algorithms for matrix completion methods used in recommender systems in Sect. 2, including matrix factorization models, neural network models and rank minimization models. In Sect. 3, we conduct substantial experiments with these different types of models to compare the performance of various algorithms. Future directions of recommender systems based on matrix completion and insights for researchers are summarized and discussed in Sect. 4.

Matrix completion methods
Problem formulation
In general, models of recommendation systems in the real world can be divided into two types: rating prediction models and top-k recommendation models. Because recommender systems are feedback-driven and most user feedback can be transformed into one or multiple incomplete matrices, optimization methods on matrix completion can address recommender system problems effectively. Consequently, most approaches preprocess the feedback data at the beginning of algorithms so that matrix completion methods can be applied to recommender systems. First of all, we discuss rating tasks in recommender systems which predict missing values of a rating matrix. Given a set of users u∈{1,…,m} and items i∈{1,…,n}, the rating of user u on item i is denoted by yui. Massive ratings are transformed into a matrix M∈Rm×n which corresponds to ratings of items rated by users. If all observed user–item pairs are stored in the set Ω={(u,i)|yuiisobserved} , the interaction matrix M is defined by

Mui={yui,null,(u,i)∈Ω,(u,i)∉Ω,
(1)
or we can replace the unknown ratings with 0, that is,

Mui={yui,0,(u,i)∈Ω,(u,i)∉Ω.
(2)
In some top-k recommendation tasks, yui may be other values ranging in [0, 1] representing affinities of users. Matrix completion methods fill the missing values in a user–item interaction matrix and provide top-k recommendation list according to the predicting values in the matrix. Usually, most entries of the interaction matrix M are unknown because most users only rated a tiny part of items. In this section, we look into some traditional and popular methods for matrix completion in recommender systems. In the beginning of this section, we first provide a table explaining commonly used mathematical symbols for better readability. Other method-specific mathematical symbols, e.g., different regularization coefficients, are explained when they first appear.

Table 1 Explanations for commonly used mathematical symbols
Full size table
Matrix factorization models
NMF
Matrix factorization is a popular and classical technique of CF for rating problems. It aims to decompose the user–item rating matrix M into the user latent factors and item latent factors that profile users and items accurately. In this subsection, we start from the well-known nonnegative matrix factorization (NMF) algorithm which has proved to be effective for learning a partial representation of the data [34, 35]. Given a rating matrix M∈Rm×n, it considers the problem by finding nonnegative matrices P∈Rm×r and Q∈Rr×n that follow M≈PQ, as illustrated in Fig. 2. Usually, the parameter r is set much smaller than min(m,n) to promise low-rank property, so that the model learns compressed representations from the original matrix.

Fig. 2
figure 2
A simple example for NMF. The incomplete user–item interaction matrix is approximated by the multiplication of two nonnegative matrices, one profiles the user latent features while the other one profiles the item latent features

Full size image
To measure the quality of the approximation, loss functions are defined to compute the distance between two arbitrary nonnegative matrices A and B. One widely used loss function is

L(A,B)=∑ij(Aij−Bij)2,
(3)
which computes the square of the Euclidean distance [41]. The gradient descent-based algorithm is applied to update two learnable matrices P and Q [42]. The multiplicative updating rules to minimize this loss function are

Qαμ←Qαμ(PTM)αμ(PTPQ)αμ,Piα←Piα(MQT)iα(PQQT)iα.
(4)
Because matrices P and Q are nonnegative, they are explained as user latent factors and item latent factors, respectively. This constraint enhances the interpretability for low-rank matrix factorization, and values of predictive ratings are directly computed by multiplying nonnegative matrices P and Q.

SVD++
Before the introduction to SVD++, we first review some methods for matrix completion which are based on NBMs. An appropriate similarity measure for the item-oriented NBMs is described by

sij=nijnij+λ1ρij,
(5)
where variable nij represents the number of users who have rated both items i and j. The variable ρij is the Pearson correlation coefficient [43] which measures the chances if the user will rate items i and j similarly. Equation (5) can be regarded as a shrunk correlation coefficient controlled by the hyperparameter λ1. The typical value of λ1 is 100. Theoretically, if the number of users that have co-rated both items i and j is higher, the value of sij should be more close to ρij. Namely, similarity values computed with more existing ratings are far more convincing. On the contrary, similarity values computed with few ratings should be shrunk considerably. Using this similarity measure, the prediction for an unknown rating is

y^ui=bui+∑j∈Sk(i;u)sij(yuj−buj)∑j∈Sk(i;u)sij,
(6)
where bui=μ+bu+bi is the baseline estimation for the unknown rating yui and explains for the user and item effect (denoted by bu and bi). In Eq. (6), Sk(i;u) denotes a set consisting of k items that are most similar to the item i, all of which are rated by the user u. Koren [44] improved the basic neighborhood model by exploiting implicit feedback. If the item set rated by user u is denoted by R(u) and the set containing all items which the user u has provided implicit feedback is denoted by N(u), the improved model is computed with

y^ui=bui+∣∣Rk(i;u)∣∣−12∑j∈Rk(i;u)(yuj−buj)wij+∣∣Nk(i;u)∣∣−12∑j∈Nk(i;u)cij,
(7)
where Rk(i;u)=defR(u)∩Sk(i) and Nk(i;u)=defN(u)∩Sk(i). Rk(i;u) and Nk(i;u) are sets containing k items most similar to the item i, and all of these items are rated by the user u. The model assumes that users who have provided more ratings should generate more significant deviations from baseline estimations.

The LFM proposed by Koren [44] is named SVD++, which considers the user implicit feedback, as shown in Eq. (8). Similar examples of such model have been proposed in other literature [45, 46].

y^ui=bui+qTi⎛⎝pu+|N(u)|−12∑j∈N(u)yj⎞⎠.
(8)
In Eq. (8), a user u is modeled as pu+|N(u)|−12∑j∈N(u)yj. The sum |N(u)|−12∑j∈N(u)yj denotes the perspective of implicit feedback. Learnable variables of the model are learned by minimizing the squared error function through the gradient descent-based method.

Koren finally integrated the SVD++ model with the neighborhood model by adding the results of Eqs. (7) and (8) directly:

y^ui=μ+bu+bi+qTi⎛⎝pu+|N(u)|−12∑j∈N(u)yj⎞⎠+∣∣Rk(i;u)∣∣−12∑j∈Rk(i;u)(yuj−buj)wij+∣∣Nk(i;u)∣∣−12∑j∈Nk(i;u)cij.
(9)
The equation above is a three-tier model for matrix completion of CF. In the first tier, μ+bu+bi is a baseline estimation for yui, without taking any other interactions into account. In the second tier, term qTi(pu+|N(u)|−12∑j∈N(u)yj) predicts the interactions between the user profile and the item profile. The final tier works as a ‘neighborhood tier’ which explains the influence of implicit feedback. This three-tier framework significantly improves the accuracy of the matrix completion model by considering both neighbor information and latent embeddings of users or items.

SLIM and FISM
Inspired by the item-based k-nearest neighbor (Item-KNN) method, which is another important model of NBMs, Ning and Karypis [47] proposed a sparse linear model (SLIM) that learns an item–item similarity matrix S∈Rn×n by solving the optimization problem

argminS12∥M−MS∥2F+β2∥S∥2F+λ∥S∥1,s.t.S⩾0,diag(S)=0,
(10)
where matrix S is constrained to be sparse because generally k≪n. Especially, M∈Rm×n is the binary matrix for implicit feedback. The value of yui equals one if the user u has provided feedback (such as likes and click history) on item i and 0 otherwise. Though SLIM learns the latent features through S from the existing data, it only discovers relationships between items that have been co-rated, which leads to missing the transitive nature of such relations.

To solve this problem and overcome the sparsity inherent in SLIM, Kabbur et al. [48] provided an improved algorithm dubbed factored item similarity model (FISM) which takes matrix factorization model into concern:

y^ui=bu+bi+(n+u)−α∑j∈R(u)pjqTi,
(11)
where pjqTi computes the similarity between items i and j. Variables pj and qi are the corresponding vectors from latent matrices P and Q. The set of items rated by user u is denoted by R(u). Constant n+u is the number of items rated by user u, and α ranges in [0, 1]. Consequently, the term (n+u)−α represents the degree of agreement between items rated by the user u according to their similarities.

Kabbur et al. [48] introduced two variations for FISM which are different in optimization targets. For the model computed by the root mean squared error (RMSE), the model aims to address the optimization problem

argminP,Q12∑u,i∈Ω∥yui−y^ui∥2F+β2(∥P∥2F+∥Q∥2F)+λ2∥bu∥22+γ2∥bi∥22,
(12)
where P and Q are updated via stochastic gradient descent (SGD) [49]. For the prediction of the item i, the estimated rating yui is computed by excluding the current item i. Namely,

y^ui=bu+bi+(n+u−1)−α∑j∈R(u)∖{i}pjqTi.
(13)
FISM takes advantage of SVD-based models to project feedback into latent spaces, and then uses the product of low-rank latent matrices to learn item similarities. This promotes the model to learn transitive relationships implicitly in top-k recommendation tasks. However, the improvement also complicates the computation and requires more time.

LLORMA
For most matrix factorization models, there is an assumption that the observed matrix should be low rank. Because most rating records in the user–item matrix are similar and related in rows or columns, the information within the matrix is redundant. With such an assumption, hidden information is mined by decomposing a matrix into low-rank matrices, as we have discussed before. However, instead of considering the entire matrix M to be low rank, Lee et al. [50] presented a new model with the assumption that M should be a low-rank matrix in the vicinity of certain row-column combinations. The algorithm makes a smooth convex combination of local low-rank matrices, each of which approximates the original matrix M in its local region.

Lee et al. [50] first developed local extensions of incomplete SVD model in the vicinity of (a,b)∈[m]×[n]. The local incomplete SVD model is

T^(a,b)=argminX∥Kh(a,b)⊙M−X∥2Fs.t.rank(X)=r,
(14)
where T(⋅,⋅) denotes the operators of form T:[m]×[n]→Rm×n. The rank of optimal X is subjected to r. Here Kh(a,b) is a smoothing kernel parameterized by a bandwidth parameter h>0. Lee et al. [50] applied Epanechnikov kernel in their experiments, as shown below:

Kh(s1,s2)∝(1−d(s1,s2)2)1[d(s1,s2)<h],
(15)
where the item similarity or user similarity is applied to measure the distance d(s1,s2).

In order to obtain an efficient estimation T^^(s) for all s∈[m]×[n] with t local models T^(s1), ⋯, T^(st), st∈[m]×[n], Nadaraya–Watson regression [50, 51] is applied to obtain a more precise global approximation by calculating

T^^(s)=∑i=1tKh(si,s)∑tj=1Kh(sj,s)T^(si),
(16)
where the sum of weights equals one. Namely, it is a weighted average of T^(s1),…,T^(st). The weights indicate that T^ close to s at indices is more important than that further away from s. The final matrix approximation is conducted by Y^^ab=T^^ab(a,b),(a,b)∈[m]×[n].

As for the choosing of s1,…,st, anchor points are sampled from the whole set [m]×[n] or training set evenly. With the increase in the number of local models t and the degree of continuity of T^, the accuracy of T^^ improves. Accordingly, the accuracy of the local models T^(s1),…,T^(st) directly controls the accuracy of global estimation T^^a,b. If T^(s1),…,T^(st) are precise enough and t is large enough, the prediction error between T^^ and T should be small [41]. This algorithm is termed as the local low-rank matrix approximation (LLORMA) model.

In addition, because the t iterations of this algorithm are independent from each other, they can be computed parallelly and work with high efficiency. The idea of solving local matrix with lower dimensions also helps speed up the computation. As a result, the time cost of LLORMA is t times solving a single regularized SVD problem. Due to the time-consuming issue of conducting SVD over the whole matrix, it is a common solution to divide the matrix completion problem into several subproblems. Multi-Schatten-p norm surrogate (MSS) also handles rank optimization problems with several subproblems like LLROMA. Both of them accelerate the computation speed, and we will discuss MSS later in Sect. 2.4.3.

Cofactor
Word embedding models [52,53,54] have been widely investigated in natural language processing and obtained great success. Word2vec [55] developed by Google is one of the well-known models. These algorithms project words or phrases from the real world into low-dimensional vectors which are applied as the inputs of other models. Inspired by word embedding models, Liang et al. [56] proposed the Cofactor to improve the quality of matrix factorization models. In this model, apart from the user–item matrix, an item co-occurrence matrix across all users is built to learn an item embedding, with the assumption that the pairs of items preferred by different users should be similar. This is very similar to the word embedding models which transform the documents into a set of co-occurring words.

The whole framework is made of two parts: the matrix factorization model and the item embedding model. In the matrix factorization part, the model attempts to factorize the given implicit feedback matrix M∈Rm×n into the user latent vector pu∈Rr(u=1,…,m) and the item latent vector qi∈Rr(i=1,…,n) with low-rank assumption. The objective function for this model is defined as

L(y^ui,pu,qi)=∑u,i∈Ωcui(y^ui−pTuqi)2+λp∑u=1m∥pu∥22+λq∑i=1n∥qi∥22,
(17)
where cui is a hyperparameter that is usually set to be cy=1>cy=0. This scaling parameter is applied to balance the missing ratings (y=0) which are far more than the existing ratings (y=1) in most click-based data. The optimization of L is considered as maximizing a posteriori estimate of the probabilistic Gaussian matrix factorization model [45, 56].

As for the item embedding model, because sequences of items are similar to sequences of words, the idea of word embedding models is analogously applied in the item embedding. For a text document, the context words of word i are surrounding words within a fixed window. Point-wise mutual information (PMI) [57] matrix between a word i and its context word j is defined by

PMI(i,j)=log#(i,j)⋅D#(i)#(j),
(18)
where #(i,j) represents the frequency that word j appears in the context of word i and D denotes the sum of word-context pairs. Levy and Goldberg [58] have proved the equivalence between skip-gram word2vec trained with negative sampling value of k and implicit decomposing the PMI matrix shifted by logk. They also recommended implementing the word embedding by spectral dimensionality reduction on the (sparse) shifted positive PMI (SPPMI) matrix defined as follows:

SSPMI(i,j)=max{PMI(i,j)−logk,0},
(19)
where k becomes the hyperparameter controlling the sparsity of SSPMI matrix. In the item embedding model, if matrix H is the co-occurrence SPPMI matrix, the item embedding can be obtained by decomposing H. Given a rated item i from a specific user, its context j is represented as all other items in the click history. The click history refers to items that the user has rated or consumed. The value of hij is obtained by the empirical estimates of PMI(i, j) defined in Eq. (18), where particularly #(i,j) is the total number of users that rated both items i and j. The combination of the matrix factorization model and item embedding model is

L(y^ui,pu,qi,hij)=∑u,i∈Ωcui(yui−pTuqi)2+∑hij≠0(hij−qTiγj−wi−cj)2+λp∑u∥pu∥22+λq∑i∥qi∥22+λγ∑j∥γj∥22.
(20)
In this objective function, the first line is the matrix factorization model and the second line is the item embedding model, the regularization term that avoids overfitting is defined in the third line. Because the matrix factorization model encodes item vectors to represent the latent features of items, while the item embedding has to explain item co-occurrence patterns, matrix factorization part and item embedding part share the same item latent factor qi in Eq. (20). As a result, qi explains both user–item interactions and item–item co-occurrence. Besides, γ is introduced as an additional model parameter for PMI matrix factorization. Here, cui is not only a scaling parameter to balance the observed and unobserved information in the click matrix, but also works on balancing the matrix factorization part and item embedding part of the model.

Neural network models
AutoRec
The autoencoder is a neural network which aims to learn a representation for a set of data by projecting original data into low-dimensional space and rebuilding it, which achieves desired performance in many fields like natural language processing [59,60,61] and computer vision [62,63,64]. Because matrices in recommender systems are generally low-rank, low-dimensional vectors in hidden layers also contain compressed information as latent factors. Therefore, Sedhain et al. [65] proposed a new CF model based on the autoencoder framework, dubbed AutoRec, which works effectively in finding compressed representation for user–item rating matrices.

In this model, each user u∈{1,…,m} is represented by an observed vector yu=[yu1,…,yun]∈Rn. In the same way, each item is represented by an observed vector yi=[y1i,…,ymi]∈Rm. The AutoRec builds an item-based or user-based autoencoder model which takes each observed yi or yu as input, and then compresses the inputs onto low-dimension vectors. Finally, the model reconstructs yi or yu in the output layer to predict unknown ratings of the user–item rating matrix. The model is illustrated in Fig. 3.

Fig. 3
figure 3
The structure of an item-based AutoRec model

Full size image
Given a set of rating vectors S∈Rd, the autoencoder aims to solve the optimization problem

argminθ∑y∈S∥y−h(y;θ))∥22,
(21)
where h(y;θ) rebuilds the input y∈Rd from hidden layers with

h(y;θ)=f(WDg(WEy+μ)+b).
(22)
Functions f(⋅) and g(⋅) are arbitrary activation functions. The parameter set θ={WE,WD,μ,b} is updated with gradient descent and back propagation. The optimization for item-based AutoRec is defined as

argminθ∑i=1n∥yi−h(yi;θ)∥2F+λ2(∥WE∥2F+∥WD∥2F),
(23)
where only existing ratings in the training set are considered due to partial observation on ratings. This optimization target corresponds to a neural network with a single hidden layer which has k hidden units. Term λ2(∥WE∥2F+∥WD∥2F) is the regularization term that avoids overfitting, and λ>0 controls the regularization strength. After training, the prediction for yui is computed by y^ui=(h(yi;θ))u. A main shortcoming of AutoRec is that it only projects data with linear layers. This makes the projection become an identity function, which may lead to inadequate feature learning.

CDAE
AutoRec is a simple but efficient application for matrix completion via autoencoders. On the basis of it, Wu et al. [66] further presented a model for top-k recommender dubbed collaborative denoising autoencoder (CDAE) with denoising autoencoder (DAE) framework. DAE is widely used in many fields [67,68,69], which learns latent representations from the original and corrupted features of the training set and then trains the model to rebuild the original values.

Fig. 4
figure 4
The structure of CDAE model for a specific user u

Full size image
CDAE model is a neural network with one hidden layer, as shown in Fig. 4. In the input layer, there are n item input nodes and a specific user input node. The user node is a k-dimensional vector that is learned during training. The item nodes yu={yu1,…,yun} denote the n-dimensional implicit feedback vector of user u on all items, where yui=1 if the item i is preferred by the user u in the training set. DAE generates corrupted y~ui and then attempts to reconstruct it into the original yui, for the purpose of making the hidden layer discover more robust features and preventing from merely learning the identity function [70]. Wu et al. [66] applied multiplicative mask-out/drop-out noise to reconstruct each dimension of yui with 0 via a probability of q, formulated as

P(y~ui=δyui)P(y~ui=0)=1−q,=q.
(24)
In order to make the corruption unbiased, uncorrupted values are recomputed via multiplying original values by δ=1(1−q).

In the hidden layer, there are k nodes fully connecting to the nodes of the input layer, as well as an additional node representing the bias effect. WE∈Rn×k is a weight matrix between item input nodes and hidden layer. Notice that Vu∈Rk is a user-specific node in the input layer, and each user has their own user vector. In the hidden layer, the model first projects the input into a latent vector zu by

zu=h(WTEy~u+Vu+bE),
(25)
where h(⋅) is the activation function, e.g., the sigmoid function. The output layer rebuilds the input vector via

y^u=f(WTDzu+bD),
(26)
where WD∈Rn×k and bD are the weight matrix and bias vector between the hidden layer and the output layer. f(⋅) is also an activation function. Finally, parameters are learned by minimizing the average reconstruction error over all m users:

argminWE,WD,V,bE,bD1m∑u=1mEp(y~u|yu)[l(y~u,y^u)]+T(WE,WD,V,bE,bD),
(27)
where T(⋅) is the L2 regularization that avoids overfitting, as shown below:

T(⋅)=λ2(∥WE∥22+∥WD∥22+∥V∥22+∥bE∥22+∥bD∥22).
(28)
All trainable parameters are learned by SGD. In order to accelerate the speed of this model, the framework only computes parameters with part of the rating set. If set R(u) is the collection of items in the training set rated by user u and R¯(u) is a set of unrated items for user u, the model samples a subset of negative items S(u) from R¯(u) randomly for parameter updating.

Meanwhile, the model employs AdaGrad [71] to automatically adapt the step size during the training procedure:

θ(t+1)=θ(t)−ηg(t)θβ+∑ts=1g(s)θ−−−−−−−−−−−√,
(29)
where θ(t) and g(t)θ are values of θ and gradient at the tth SGD step, respectively. When the model makes predictions, the first k items with the largest values in the output layer are recommended to the specific user.

DMF
CDAE model mentioned above only illustrates the preference of users by addressing on the implicit feedback. Xue et al. [72] presented a neural network-based model dubbed deep matrix factorization (DMF) for top-k recommendations with both explicit ratings and implicit feedback. DMF is also a matrix factorization model, but it is implemented with deep neural networks. As shown in Fig. 5, there are two parallel multilayer networks to transform the representation of user u and item i, respectively. Both user u and item i are projected onto low-dimensional vectors by

pu=fθUN(⋯fθU3(WU2fθU2(yuWU1))⋯),qi=fθIN(⋯fθI3(WI2fθI2(yiWI1))⋯),
(30)
where WUk and WIk are the kth weight matrices for extracting hidden information of users and items. The similarity between the user u and the item i is measured by

y^ui=FDMF(u,i|θ)=cosine(pu,qi)=pTuqi∥pu∥∥qi∥.
(31)
Because the predicted value in Eq. (31) may be negative, mapping y^oui=max(μ,y^ui) is applied to transform the prediction to a nonnegative value.

Fig. 5
figure 5
The basic architecture of the DMF model

Full size image
A new loss function is employed to consider both explicit and implicit information in this model, so that both two types of feedback are used together for optimization. The new loss function is dubbed normalized cross-entropy (NCE) loss, defined by

L(y^ui,yui)=−∑(u,i)∈Ω(yuimax(R)logy^ui+(1−yuimax(R))log(1−y^ui)),
(32)
where max(R) represents the maximum value among all ratings. If it is a classical 5-star rating recommender, max(R) equals 5. As a result, different values of yij lead to different influence on the loss function.

Graph-based methods
Owing to the powerful ability of integrating connecting patterns between nodes in the non-Euclidean domain, graph-based methods have achieved significant performance in recent years. Spectral graph convolution is a typical method of graph-based approaches, which has been widely applied in recommender systems. As an example, SpectralCF [73] was proposed to address the cold-start problem in recommender systems, exploiting the bipartite user–item relationship graph and a new convolution operation to estimate recommendations in the spectral domain. It can also be regarded as a variant of graph convolutional network (GCN). GCN was developed by Kipf et al. [74] to perform convolution operations on graph-structured data, formulated as

H(l)=σ(D~−12A~D~−12H(l−1)W(l)),
(33)
where A~=A+I denotes the adjacency matrix considering the self-connections, and [D~]ii=∑j[A~]ij. The weight matrix in the lth layer is denoted by W(l). It is a first-order approximation of truncated Chebyshev polynomial, which is deduced from spectral convolutions on graphs, that is,

gθ⋆x=UgθU⊤x,
(34)
where U denotes the eigenvalue matrix of the normalized graph Laplacian matrix, x∈Rm is the input feature and gθ=diag(θ) is the parameterized filter. GCN aims to learn the embedding H(l) from network typology, because of which it is extensively applied in recommender systems to recover missing values of an interaction matrix via exploring the latent relationship between users and items. Numerous methods based on GCN have been developed recently. For example, Berg et al. [75] employed graph autoencoders derived from GCN to retrieval the missing values in an incomplete matrix, where the matrix completion task was converted into the link prediction problem on graphs. Monti et al. [76] combined GCN with recurrent neural networks (RNN) to explore the underlying graph-structured patterns between users. Wang et al. [77] presented a neural graph collaborative filtering (NGCF) framework which integrates user–item interactions into the GCN framework and explicitly leverages the collaborative signal.

Rank minimization models
IRNN
In this section, we take a look into rank minimization models for low-rank optimization problems. Different from the low-rank matrix factorization described before, rank optimization methods focus on minimizing the rank function with

argminMh(M)=f(M)+rank(M),
(35)
where f(⋅) is a differentiable loss function and rank(⋅) is the rank function on the matrix M. For matrix completion problems in recommender systems, f(⋅) is generally defined as

f(M)=∥MΩ−M∗Ω∥2F,
(36)
where M∗Ω is the reconstructed matrix. Because rank(⋅) is exactly the sum of nonzero singular values of the input matrix, it is nondifferentiable. Sometimes, it becomes an NP-hard problem which is difficult to solve. To tackle this problem, we can relax rank(⋅) to some other surrogate functions g(⋅) like ℓp norm, nuclear norm or Schatten-p norm. Accordingly, Eq. (35) is transformed into

argminMh(M)=f(M)+λg(M),
(37)
where λ is the regularization coefficient. Generally, the surrogate function g(⋅) and loss function f(⋅) should satisfy the following assumptions.

Assumption 1
g(⋅) is continuous, nonconvex and monotonically increasing on [0,∞). It is possibly nonsmooth.

Assumption 2
f(⋅) is a differentiable and smooth function whose gradient is Lipschitz continuous as

∥∇f(A)−∇f(B)∥F≤L(f)∥A−B∥F,
(38)
for any A,B∈Rm×n. Here L(f)>0 is the Lipschitz constant. Notice that f(⋅) is possibly nonconvex.

Assumption 3
h(M)→∞ if and only if ∥M∥F→∞, which guarantees the convergence.

Table 2 Several specified nonconvex definitions of functions g(θ)
Full size table
Table 2 shows some specified nonconvex surrogates of g(⋅). Applying theory of supergradient [83], Lu et al. [84] proposed iteratively reweighted nuclear norm (IRNN) to optimize rank minimization problems with surrogates of g(⋅). IRNN updates M at the kth iteration by solving the minimization problem

Mk+1=argminMf(M)+∑i=1rwkiσi,
(39)
where σi is the ith singular value of M and wki is the supergradient computed via

wki∈∂gλ(σi).
(40)
Because of the antimonotone property of surpergradient, a significant singular value has a smaller weight wi. The iterative updating rule of the proximal gradient method for solving Equation (39) is derived from

M(k+1)==argminMf(M(k))+⟨∇f(M(k)),M−M(k)⟩+L2∥∥M−M(k)∥∥2F+g(M)argminML2∥∥∥M−M(k)+1L∇f(M(k))∥∥∥2F+∑i=1rwkiσi,
(41)
where L is the Lipschitz constant. Equation (41) has a closed-form solution given by weighted singular value thresholding

Mk+1=Uηλw(Σ)VT,
(42)
where Y=M(k)−1L∇f(M(k)), UΣVT=Y is the SVD of Y, and ηλw=diag{(Σii−λwi)+}. Σ is the diagonal singular value matrix, which means that Σii denotes the ith singular value of Y. Each iteration of IRNN is a two-step learning scheme that updates wki with Eq. (40) and Mk+1 with Eq. (41), respectively.

DNNR
Inspired by IRNN and properties of supergradient, Zhang et al. [85] improved IRNN with the weighted singular value function (WSVF), which was formulated as

ρw(σ(M))=∑i=1rwiρ(σi),
(43)
where ρ(⋅) is a nonconvex and lower semicontinuous function on [0,∞). The same as IRNN, a lower weight indicates a more significant singular value. When ρ(σi) is the identity function, Eq. (43) is degraded to g(⋅) in IRNN. With aforementioned notations, the rank minimization problem can be solved by

argminMh(M)=f(M)+λρw(σ(M)).
(44)
Because the optimization target can be derived from

argminMh(M)=f(M)+λ∑i=1rρ1(ρ(σi))
(45)
with concepts of supergradient and reweighted strategies, where ρ1(⋅)=ρ(⋅), it is also dubbed double nonconvex nonsmooth rank (DNNR) minimization problem. By linearizing Equation (44), the optimal solution is achieved by

argminM12∥Mk−Y∥2F+λρw(σ(Mk)),
(46)
which has a closed form solution Mk+1=Udiag(δ∗(Y))V, termed as WSVF thresholding operator. The ith operator solves the problem with

δ∗i∈proxρ(σi)=argminδi≥0λwiρ(δi)+12(δi−σi)2.
(47)
Existing works [86,87,88] have derived the closed-form solutions when ρ(⋅) is the ℓp-norm with p=12 or p=23. For simplicity, we denote λwi as ξ, σi as σ and δi as δ, respectively. When p=12, we have

δ∗={23σ(1+cos(2π3−2ϕ(σ)3)),0,σ>φ(ξ),otherwise,
(48)
where ϕ(σ)=arccos(ξ/4(σ/3)−3/2) and φ(ξ)=32–√3/4(2ξ)2/3. Similarly, when p=23, the optimal solution is computed by

δ∗={((ϖ+2σ/ϖ−ϖ2−−−−−−−−−√)/2)3,0,σ>φ(ξ),otherwise,
(49)
where ϖ=2/31/2(2ξ)1/4cosh(arccosh(27σ2/16(2ξ)−3/2)/3)1/2 and φ(ξ)=2/3(3(2ξ)3)1/4. Analogous to IRNN, the updating rules for DNNR include 2 steps at each iteration. The model first computes the weight wki with supergradient

wki∈∂ρ(ρ(σi(Mk))),
(50)
and then updates Mk+1 by solving optimization problem (46). Distinct from the aforementioned IRNN, DNNR method is more general than IRNN because of double nonconvex constraint functions on singular values. Due to this reason, the updating rules of DNNR for Xk+1 and wk are based on the singular value function ρ(σi(⋅)) instead of depending on σi(⋅) directly. However, there is a critical problem that IRNN and DNNR face the time-consuming issue because of conducting SVD on the whole interaction matrix M, which brings difficulties for applying them to large-scale datasets.

MSS
To avoid conducting SVD of the entire matrix and reduce time consumption, Xu et al. [89] proposed a unified convex surrogate for the Schatten-p norm minimization problem, where the optimization problem was decomposed into several subproblems so that SVD could be conducted over a matrix with lower dimensions.

Inspired by low-rank matrix factorization, the low-rank minimization problem defined by Eq. (37) can be rewritten as

argminU,Vh(U,V)=f(U,V)+λ(g(U)+g(V)),
(51)
where U∈Rm×d and V∈Rn×d are the unknown latent factor matrices that M=UVT holds. Recent related works have attempted to find surrogates for specific p values when g(X) is denoted by the Schatten-p norm

∥X∥Sp=(∑i=1min{m,n}σi(X)p)1p=(Tr((XTX)p2))1p
(52)
for any factors X. It is a well-known unitarily invariant norm. When p=1, Schatten-p norm becomes the widely used nuclear norm or trace norm. Srebro et al. [90] has investigated the bi-Frobenius norm surrogate for the nuclear norm as

∥M∥∗=argminU,V:M=UVT12∥U∥2F+12∥V∥2F.
(53)
Moreover, Shang et al. [91, 92] proved the following equalities when p=12 and p=23:

2∥M∥1/2S1/232∥M∥2/3S2/3=argminU,V:M=UVT∥U∥∗+∥V∥∗,=argminU,V:M=UVT∥U∥∗+12∥V∥2F.
(54)
Summarized from these existing work, Xu et al. [89] speculated and proved that the bilinear surrogate for Schatten-p norm could be extended to a more general problem, dubbed multi-Schatten-p norm surrogate (MSS) optimization problem:

1p∥M∥pSp=argminXi∑i=1I1pi∥Xi∥piSpi,
(55)
where any pi>0 satisfies 1p=∑Ii=11pi, M=∏Ii=1Xi with X1∈Rm×d1, Xi∈Rdi×di,i=2,…,I−1 and XI∈RdI×n. The optimization problem for MSS can be solved by block coordinate descent (BCD) [93] which minimizes each Xi at a single iteration by fixing the remaining blocks. Each subproblem is solved by the proximal alternating linearized minimization (PALM) algorithm. Specifically, the proximal gradient method for each factor Xi at each iteration is computed by

X(k+1)i==argminXif(X(k)i)+⟨∇f(X(k)i),Xi−X(k)⟩+L(k−1)i2∥∥Xi−X(k)i∥∥2F+1pi∥Xi∥piSpiargminXiL(k−1)i2∥Xi−Y∥2F+1pi∥Xi∥piSpi,
(56)
which can be solved by closed-form solutions for a specific pi value. Furthermore, the acceleration technique [94] is adopted, where X^(k)i is updated via

X^(k)i=X(k)i+wki(Xki−Xk−1i),
(57)
and wki is computed by

wki=min{tk−1tk,0.9999Lk−1iLki−−−−−⎷}
(58)
with t1=1 and tk+1=(1+1+4t2k−−−−−−√)/2.

Generally, the learning scheme for MSS of each subproblem is similar to IRNN and DNNR. However, compared with IRNN and DNNR, MSS solves the time-consuming issue by transforming the original problem into subproblems that are easier to solve. This avoids conducting SVD of the whole matrix and further speeds up the computation on large-scale datasets. In addition, rather than only specific to some p values of DNNR (p=12 and p=23), the unified model can solve more p values flexibly by considering different combinations of subproblems.

ISVTA
Distinct from the convex and nonconvex rank relaxations we have introduced before, Zhang et al. [95] presented a modified Schatten-p norm as a surrogate of the rank function, denoted as

minX{Hλ(X)=12∥A(X)−b∥2F+λ∥X∥pSp,ϵ},
(59)
where the objective Hλ(X) is nonconvex and cannot be optimized directly. However, it can be solved by the linearized strategy or adding more variables (e.g., alternating direction method of multipliers), which may guarantee that each subproblem has the closed-form solution. In order to gain the closed-form solution directly, Zhang et al. optimized the surrogate function of Hλ(X) by adding several quadratic terms, as shown below:

minX,Y{Hλ,μ(X,Y)=μ[12∥A(X)−b∥2F+λ∑iσi(X)(σi(Y)+ϵi)1−p]−μ2∥A(X)−A(Y)∥2F+12∥X−Y∥2F},
(60)
which can be optimized more efficiently. Furthermore, Zhang et al. devised the iterative singular value thresholding algorithm (ISVTA) to solve Eq. (60). Each iteration of ISVTA can be summarized as follows:

Bμ(Xk)=Xk−μA∗(A(Xk)−b),
(61)
λk+1=κkλ0≤λt,0<κ<1,
(62)
Xk+1=Gλ,μ(Bμ(Xk))=UkSτw(ΣkBμ)(Vk)T,
(63)
where ΣkBμ is the singular values of Bμ(Xk) and the soft thresholding operator is denoted as Sτw(ΣkBμ)=Diag{(ΣkBμ,ii−τwi)+} for i=1,2,…,r. In particular, we can set τ=λμ and wi=1(σi(X∗)+ϵi)1−p with 0<p<1. In theory, this method can reduce the number of iterations to improve the computational consumption and provide a better global convergence guarantee compared to other methods introduced before [95].

Experiments
In this section, we conduct substantial experiments on the models mentioned in Sect. 2 with different real-world datasets. Because some methods are designed specially for top-k recommendations, and some methods focusing on rating prediction are not suitable for top-k tasks, experiments are divided into rating (NMF [34], SVD++ [44], LLORMA [50], AutoRec [65], GCMC [75], sRGCNN [76], IRNN [84], DNNR [85], MSS [89], ISVTA [95]) and ranking (Item-KNN, SLIM [47], FISM [48], CDAE [66], Cofactor [56], DMF [72], SpectralCF [73], NGCF [77]) tasks for a fair comparison. Different evaluation measurements are utilized to compare the performance of different algorithms, as well as the time cost for predicting.

Table 3 Statistics of the real-world datasets in our experiments
Full size table
Datasets description
In this paper, comparing experiments are conducted over several real-world datasets, including movie recommendation datasets, joke rating datasets and shopping recommendation datasets, etc. The textual descriptions of these datasets are listed below:

FilmTrustFootnote1 dataset is a small movie recommender dataset crawled from the FilmTrust website in 2011, which contains 35,497 rating records over 1508 users and 2071 items.

MovielensFootnote2 is provided by GroupLens Research from the MovieLens website and has many versions of datasets collected at different times. In this paper, MovieLens-100K (ML-100K) and MovieLens-1M (ML-1M) are selected to conduct experiments.

NetflixFootnote3 is a popular online movies and TVs website. Its dataset contains about 100 million ratings and is used in the Netflix Prize competition. We only extract part of the data with 1,500 users and 2,000 items.

EpinionsFootnote4 was collected from the Epinions website where people have provided ratings for different types of products. The dataset in our experiments contains 81,513 ratings over 3586 users and 12,000 items.

JesterFootnote5 is a benchmark dataset for joke recommender systems which contains substantial ratings of users on different jokes. Different from other datasets, its ratings range in [−10.0,10.0].

The detailed statistics of these datasets are shown in Table 3, including dimensions, numbers of ratings, rating scales, and density.

Performance evaluation
For rating problems, we use root mean squared error (RMSE) and mean absolute error (MAE) [96] to evaluate the performance of recommendation models for rating. Both RMSE and MAE measure the deviation between the observed data and the real data, and smaller values of these two metrics indicate better performance of models. Given t testing entries with their real values y1,…,yt and predictive values y^1,…,y^t, Eqs. (64) and (65) are used to compute RMSE and MAE:

RMSE=1t∑i=1t(yi−y^i)−−−−−−−−−−−⎷,
(64)
MAE=1t∑i=1t∥yi−y^i∥.
(65)
As for top-k problems, we adopt two metrics designed for ranking known as normalized discounted cumulative gain (NDCG) [97] and RECALL [98] to evaluate the list of k recommended items. NDCG emphasizes the ranks of the estimating results and takes the variation between real ranks and predicted ranks into concern. When an item of high preference for a specific user appears in the high ranking, the value of NDCG would be higher. To explain the definition of NDCG, we need to introduce the discounted cumulative gain (DCG)

DCG=∑i=1k2reli−1log2(i+1),
(66)
where reli is the relationship that measures the importance of item i. In our experiments, we use the real score of item i to represent reli, because a higher rating value indicates a higher preference of a user. Ideal DCG (iDCG) is the ideal value of DCG, which is also computed by Eq. (66). With DCG and iDCG computed, NDCG is computed with NDCG=DCGIDCG.

RECALL is different from NDCG, which holds the view that all items in the recommended list are equivalent without considering predicted ranking. It is defined by

RECALL=#TP#TP+#FN,
(67)
where #TP is the number of true positive items and #FN is the number of false negative items.

Performance comparison
For all algorithms, we follow the settings in original papers or codes if feasible. Some parameter settings that we selected via our substantial experiments are clarified in advance to gain more credible results. Learning rates of all algorithms are selected in {0.1,0.001,0.005,0.0001}. The dimensions of latent factors for matrix factorization models range in [50, 300] with step size 50. Other specific parameters of some algorithms are listed as follows: SVD++: fix regularization coefficients λp, λq and λb =0.01; LLORMA: fix number of anchor points t=55, set local λP, λQ =0.01, global λP, λQ =0.1, and apply Epanechnikov kernel with h1=h2=0.8; AutoRec: fix λ=0.001; Item KNN and SLIM: adopt numbers of KNN neighbors ranging in [10,20,…,100]. FISM: set α and β ranging in [0.1, 1.0] with step size 0.1, and fix λ=γ=0.1; CDAE: set sigmoid as the activation function and set λ=0.01; DMF: set ReLU as the activation function, and fix λ=0.001. The number of hidden layers is set N=8; Cofactor: set λU=λV=0.000001, the ratio cy=0=0.1 and cy=1=1; IRNN, DNNR and ISVTA: initialize λ0=α∥PΩ(M)∥∞, where α ranges in [1,100,200,…,1000]; MSS: fix η=0.1 and λ=200, and the factor number is set 4 or 5 with pi=1,i=1,…,I, that is, p=0.25 or 0.2. For top-k recommendation tasks, we set k=100 to generate top 100 recommendation lists.

Fig. 6
figure 6
Effect of factor number on a ML-100K and b Jester

Full size image
In order to discover the effect of varying numbers of latent factors, we run comparison experiments for all algorithms based on learning user or item embedding, including AutoRec, CDAE, and all matrix factorization models. We keep the other parameters as constants and then consider the number of factors as a variable. Experiments are conducted over ML-100K and Jester datasets. For AutoRec and CDAE, we define the dimension of the hidden layer as the number of latent factors. Figure 6 records the results of experiments. On the whole, the performance of most algorithms increases as the factors increase, while may decline when the number of factors is too large. From the figures, we find that the performance of FISM is stable, so better recommendations can be obtained with a small computational cost. The performance of AutoRec and CDAE fluctuates greatly in all tested datasets; CDAE especially demands for more factors to rebuild the corrupted data for obtaining a higher value of NDCG. Contrary to other algorithms, Cofactor reaches its best performance when the number of factors is small compared with other models, and performs poorly with too many factors. For rank minimization methods, we compare the performance across different p values, as shown in Fig. 7. Generally, smaller p values lead to lower RMSE for IRNN. On the contrary, MSS gains better performance when p is close to 0. Although DNNR is developed from IRNN, it only works when p=12 and 23. The best p value for DNNR method is uncertain.

Fig. 7
figure 7
Effect of varying p values on a ML-100K and b Jester datasets. Because DNNR only allows p=12 and 23, we plot it with two lines across different p values

Full size image
Furthermore, we make comparison over all tested datasets for all selected algorithms. Fivefold cross-validation is applied to all experiments, and we record the average as well as the standard deviation of all metrics. The performance of rating and ranking tasks is listed in Tables 4 to 7. From these tables, we have the following observations: First of all, it can be found from the experimental results that SVD++ has excellent performance on most datasets among all tested methods except graph-based models. This is because that SVD++ makes full use of implicit feedback and explicit feedback, and considers both matrix factorization methods and neighborhood methods. Graph-based models (i.e., GCMC and sRGCNN) achieve pleasurable performance in rating prediction tasks, because generated graph-structured features can better depict the relationships between users and items, and graphs are able to propagate information more effectively. Secondly, on the whole, rank minimization methods perform worse than other types of models, which may be attributed to the fact that these methods concentrate more on recovering existing values of the matrix instead of predicting missing values in test sets. As to the top-k experiments, methods based on KNN (Item-KNN and SLIM) achieve superior performance, which indicates that ideas of KNN are still effective. It is obvious that SLIM and CDAE gain extremely high NDCG values in Jester dataset. This is probably because that both of them learn meaningful user or item embedding when the number of items is further smaller than the number of users. Besides, FISM and DMF both gain acceptable performance, while CDAE performs poorly on some sparse datasets. The experimental results figure out that neural network methods are not always better than traditional machine learning models, and even sometimes achieve worse performance. This motivates us to continue developing traditional optimization methods. Last but not the least, it is clear that all algorithms perform poorly with sparse datasets, which shows that an essential challenge for recommender systems is sparsity. Because the scale of ratings in Jester is wide, the results of RMSE and MAE are extraordinarily larger than other datasets.

Table 4 The performance (MAE±std%) comparison for rating tasks on all tested data. The lower the better
Full size table
Table 5 The performance (RMSE±std%) comparison for rating tasks on all tested data. The lower the better
Full size table
Table 6 The performance (NDCG±std%) comparison for ranking tasks on all tested data. The higher the better
Full size table
Table 7 The performance (RECALL±std%) comparison for ranking tasks on all tested data. The higher the better
Full size table
Table 8 records runtime comparison of various algorithms on different tested datasets. It is evident that neural network-based models such as AutoRec, CDAE, DMF, and all graph-based methods, cost more time for estimations than other models. Because item-KNN has no iterative updating procedure, it is the fastest method among all tested models. Apart from item-KNN, LLORMA and MSS work swiftly in most datasets, followed by SVD++. This is because that both LLORMA and MSS consider localized subproblems with lower dimensions. The runtime of IRNN and DNNR is extremely high among machine learning methods, due to the inefficient SVD of the entire matrix at each iteration. It is noted that the computational cost of some gradient-based methods is not related to the dimension of the feedback matrix, because they may converge slowly on some datasets.

Table 8 Average runtime (seconds) of matrix completion algorithms on all tested datasets
Full size table
Insights and discussions
Advantages of matrix completion-based methods
In this paper, we start the survey from the concept of low-rank matrix completion problems. Although the real-world applications are diverse, most techniques of recommender systems can be formulated as matrix completion problems that attempt to recover the missing values in incomplete feedback matrices. This is because that the large amount of user feedback data naturally come into being various huge incomplete matrices. It is beneficial for researchers to look into the recommender systems by starting from a well-defined optimization problem, so that researchers that are new in this field can easily follow previous works. Consequently, in real-world applications, we first need to consider how to transform the problems of recommender systems into matrix completion optimization. Besides, as the rank optimization problems we have discussed in Sect. 2.4, a method developed via the concept of matrix completion tends to have better interpretability and can be solved by traditional iterative optimization methods, not limited to the deep learning methods that may lack theoretical explanation. Despite the fact that deep learning has achieved promising improvements in recommender systems, we encourage researchers to explore related algorithms from the concept of traditional matrix completion algorithms, because it is still a vital learning problem and may inspire us to develop new neural network structures with better interpretability. We will discuss these in the next subsection.

Table 9 The brief comparison for all algorithms in our experiments
Full size table
Besides, in real-world applications, although some recommender systems do not need to predict the missing values in user–item interaction matrices, techniques of matrix completion still play an essential role in many scenarios. For example, matrix factorization-based methods which decompose the observed user–item interaction matrices into latent factors of users and items for extracting underlying features, are widely utilized in context-aware [99,100,101] and sequential recommender systems [102,103,104]. Therefore, a more in-depth study of matrix completion can help the development of other related techniques for recommender systems.

Challenges and potential future directions
Traditional Machine Learning Methods First of all, we conclude the methods tested in our experiments. Table 9 shows the brief comparison for all experimented models. As we have analyzed in detail before, traditional convex optimization models like matrix factorization models and rank minimization models still play important roles in recommender systems, and achieve performance that is competitive with or even superior to the deep learning-based models. For rating prediction, SVD++ which exploits both latent factor information and neighborhood coefficients has higher accuracy on most datasets. KNN-based models such as SLIM and Item-KNN gain excellent performance in ranking tasks with smaller time cost. This points out that KNN-based matrix completion methods generally gain pleasurable accuracy in top-k recommendation tasks, owing to the fact that KNN methods are virtually designed for recommending top-k items. These experimental results indicate that traditional ideas of machine learning are still effective in recommender systems. The observation also reveals that conducting matrix completion via exploring neighborhood relationships is profitable to both rating and ranking tasks. We may discover more interpretable and effective models based on machine learning techniques, such as kernel learning, Bayesian learning and clustering. However, the time complexity for some of these methods is high, especially for rank optimization methods. Most rank minimization methods require conducting SVD (O(min(m,n)mn)) of the original matrix at each iteration, and runtime for most similarity measures is O(n2) and O(m2) or higher. The time complexity of most popular neural networks is at least O(max(n,m)3). As a result, these methods are not suitable for large-scale recommender system datasets. In light of this, how to reduce the computational complexity of recommendation algorithms requires further study. As a matter of fact, LLORMA and MSS that we discussed have attempted to reduce the runtime by avoiding conducting computation on the entire matrix, and decomposing the original optimization problem into several subproblems. The idea of the divide and conquer algorithm can be considered to accelerate the speed of models.

Deep learning methods Due to the rapid development of deep learning, many methods implemented with deep learning have appeared [105]. Most neural networks are applied for extracting features or generating user/item profiles. Not limited to rating matrices, networks like CNN and RNN are widely used for feature engineering on image, audio or text inputs. The extracted features are either used in content-based CF methods or applied as side information. However, most effective deep learning methods seem to need profound understandings of neural networks and substantial trials of experiments, which are tough for researchers to develop a model with theoretical guarantees. Recent study also points out that recommender systems built via neural networks may not perform well compared with traditional iterative machine learning algorithms [106], and we have also found this phenomenon in our experiments.

Consequently, how to integrate traditional machine learning methods into state-of-the-art techniques like deep learning becomes an interesting direction. Some deep learning models are associated with traditional matrix factorization models. For example, DMF that we have discussed conducts matrix factorization with deep neural networks and computes the preference of users with cosine similarity measure. Hence, it is a valuable research direction that we may transform traditional models into deep learning frameworks, because deep learning frameworks inspired by traditional iterative optimization problems generally have better theoretical guarantees. However, most matrix completion optimization problems are based on nonconvex and nondifferentiable optimization objectives, and deep learning methods have difficulties in dealing with these problems. l1 norm which promotes sparse solutions and nuclear norm which generates low-rank solutions in rank minimization problems are typical examples. The nonsmooth and nondifferentiable properties of these constraints make the gradient descent and backpropagation algorithms not applicable. Although some works have investigated on transforming traditional optimization algorithms into deep learning frameworks [107,108,109,110], to our knowledge, there is limited study on handling rank constraints with neural networks. Therefore, how to construct a deep learning framework following the spirits of traditional iterative optimization methods is also a potential direction.

Cold start issues: The performance of most compared models declines due to the sparsity of the user–item feedback matrix, as we have analyzed in experiments. This phenomenon can be regarded as the cold start issue, which has become the primary problem since recommender systems appeared. Generally, it is impossible for users to provide feedback on most items in the database. In real-world applications, it is common to find that a user only has rated a few (even one or two) of millions of items. Thus, the cold start challenges in practical applications are far more severe than experiments on benchmark datasets. Inspired by the excellent performance of SVD++, we may consider exploring more models adopting implicit feedback or side information for matrix completion of recommender systems, which are useful for generating user or item profiles to address the cold start issues. Side information can also be obtained from social relationships, geographic locations, user shopping history, and even time sequences. More embedding methods for these side information are also important for building a more interpretable model, so that unknown values in an incomplete matrix can be predicted more accurately.

Conclusion
In this paper, we looked into different types of matrix completion algorithms, including matrix factorization models, neural network models, and rank minimization models. We investigated these methods and discussed the characteristics and improvements. Finally, we introduced different evaluation measurements for recommender systems and used them to evaluate the performance of different algorithms on varying datasets. Some shared hyperparameters were experimented and discussed for investigation. Inspired by experiments and existing research, we further provided insights and potential directions of matrix completion on recommender systems for readers. We believe that a combination of traditional optimization problems in machine learning and popular neural networks will further improve the accuracy of matrix completion. Nowadays, recommender systems are playing more and more critical roles in data mining to discover useful messages and provide suggestions for people. We will explore more efficient algorithms for recommender systems with regard to matrix completion in the future.