Support vector domain description (SVDD) is a data description method inspired by support vector machine (SVM). This classifier describes a set of data points with a sphere that encloses the majority of them and has a minimal volume. The boundary of this sphere is used to classify new samples. SVDD has been successfully applied to many challenging classification problems and has shown a good generalization capability. However, this classifier still has some major weaknesses. This paper focuses on two of them: The first regards the large amount of memory and computational time required by SVDD in the training step. This problem manifests most strongly when dealing with large-size datasets and can hinder or prevent its use. This paper presents an approximate solution to this problem that permits to apply SVDD to large-scale datasets. This new version is based on divide-and-conquer strategy and it processes in two steps: It begins by dividing the whole large-size dataset into random subsets that each can be described efficiently with a small sphere using SVDD. Then, it applies our new algorithm that can find the smallest sphere that encloses the minimal spheres built in the previous step. The second weak point of standard SVDD concerns its static learning process. This classifier must be re-trained with the whole dataset each time when new training samples are available. This paper proposes a new dynamic approach that only trains the new samples with SVDD and incorporates the resulting minimal sphere with the previous one (s) to construct the smallest sphere that encloses all the samples. Like Support Vector Domain Description, the proposed approach can be extended to non-linear classification cases by using kernel functions. Experimental results on artificial and real datasets have successfully validated the performance of our approach.

Access provided by University of Auckland Library

Introduction
Support Vector Domain Description is a well-known kernel method widely used for novelty detection and classification tasks. Inspired by SVM of Vapnik et al. [5, 26], Tax and Duin have developed SVDD as a data description method [22, 23]. The key idea behind this classifier is to describe a class of objects through a minimal sphere that encloses most of them. The boundary of this latter is used to decide whether or not a new sample belongs to a learned class. This classifier has many attractive proprieties, including it’s based on structural risk minimization principle that governs the trade-off between the empirical risk and Vapnik-Chervonenkis dimension [25]. Additionally, the description of a dataset with SVDD implies solving a convex quadratic programming problem (QP) that has a unique global minimum which avoids the risk of falling into local minima during the training. Also, it can be extended easily to describe non-linearly separable data using kernel functions. These latter can be incorporated in the mathematical formulation of SVDD implicitly by replacing the inner products with a suitable kernel function. Another advantage of this classification method is that its decision function depends just on a few training samples known as support vectors (SVs). This propriety reduces the computational time and the storage resources required to determine the membership of new instances. Furthermore, SVDD has a small number of parameters to adjust during the training which facilitates its use. This classifier has shown good classification accuracy on a variety of applications. Including, credit assessment [6], image and video segmentation [20], computer and network security [3, 17], face recognition [32], features extraction [18], fault diagnosis [21], and diseases diagnosis [11].

This paper focuses on two problems that standard Support Vector Domain Description encounters. The first concerns the excessive computational cost required to train a large-size dataset with this classifier. As known, SVDD tries to minimize a constrained quadratic programming problem that needs large amounts of computational time and memory, especially for large-scale datasets. Suppose that we have N records to train, the space complexity of this classifier is O(N2) and its time complexity is O(N3) [4, 14]. With a big corpus of data to classify the training process becomes excessively slow or even impossible to achieve. While the second problem concerns the architecture of standard SVDD. As an offline classifier, it builds a static model after training a given dataset. This model must be re-built entirely each time when new data points are available for training which is time, memory, and effort consuming.

This paper presents a new version of Support Vector Domain Description that aims to deal with both problems described above. Concerning the first one, the proposed approach processes as follows: At first, the large-size dataset will be divided randomly into M subsets that can be trained each with standard SVDD. The result is a set of M minimal spheres, each of which encloses the samples of a specific subset. Then the proposed algorithm will be applied to build a global minimal sphere that encloses the minimal spheres found previously. Figure 1 describes this process in four steps. Concerning the second problem, our method processes as follows: Suppose that an initial class of objects exists and it’s enclosed by the minimal sphere using SVDD. If a new set of objects belonging to the same class appears, SVDD will be applied to enclose them with the minimal sphere. Then, our algorithm will be applied to enclose the set of spheres with one minimal sphere that represents the whole dataset. This process will be applied recurrently when new samples are arrived at by considering the last sphere as the initial one. Figure 2 displays this solution in five steps.

Fig. 1
figure 1
Steps to enclose a large dataset with the proposed approach

Full size image
Fig. 2
figure 2
Steps to apply our approach to a dynamically increased dataset

Full size image
The rest of this paper is organized as follows: Sect. 2 presents Support Vector Domain Description; Sect. 3 gives an overview of some major improvements of this classifier; and Sect. 4 presents our approach and the last section gives an experimental evaluation of our method using artificial and real datasets.

Support vector domain description
Suppose we are given a set of objects A={x1,x2,xN} that contains N instances xi with i=1,…,N. SVDD attempts to find the smallest ball represented with the center a and the radius R, that encloses most of the patterns in A. The problem is described by the following equation:

Minimize:R2Subjectto:||xi−a||2≤R2∀i=1,2,…,N
(1)
To allow the presence of some instances outside the minimal sphere, the constraints are relaxed by introducing slack variables εi>0 with i=1,2,…,N as follows:

Minimize:R2Subjectto:||xi−a||2≤R2+εi∀i=1,2,…,N
(2)
The optimization problem Eq. (2) can be expressed using Lagrange multipliers α=(α1,α2,…,αN) and μ=(μ1,μ2,…,μN) as follows:

Minimize:L=R2−∑i=1N(R2+εi−||xi−a||2)αi−∑i=1Nεiμi+C∑i=1NεiSubjectto:αi≥0andμi≥0∀i=1,2,…,N
(3)
where C is a positive and non-null parameter that gives the trade-off between the volume of the minimal sphere and the number of classification errors. The annulment of the three partial derivatives of L with respect to R,a and εi yields the following equations:

∂L∂R=0⇒∑i=1Nαi=1
(4)
∂L∂a=0⇒a=∑i=1Nαixi
(5)
∂L∂εi=0⇒αi=C−μi
(6)
By replacing the equations Eqs. (4), (5), (6) in Eq. (3) we obtain the dual maximization problem that can be written as follows:

Maximize:W=∑i=1Nx2iαi−∑i=1N∑j=1NαiαjxixjSubjectto:0≤αi≤C∀i=1,2,…,Nand∑i=1Nai=1
(7)
The maximization of the equation Eq. (7) gives the values of the Lagrange multipliers α=α∗1,α∗2,…,α∗N. These latter will be used to find the center a∗ of the minimal sphere using Eq. (5). The radius of this sphere is calculated by choosing a support vector point xs, the latter has a Lagrange multiplier 0<αs<C  and is located on the minimal sphere.

R2=x2s−2∑i=1Nαixixs+∑i=1N∑j=1Nαiαjxixj
(8)
In real world classification problems, the set of examples to learn with SVDD has not usually had a convex shape, which complicates the description. As inherited from SVM, SVDD can deal with this problem by mapping the examples into a high-dimensional feature space where they can be described by a sphere. The mapping is performed implicitly by using a suitable kernel function that replaces the inner product in SVDD formulations. Table 1 presents some commonly kernel functions and their mathematical formulation.

Table 1 Some kernel functions and their Mathematical formulation
Full size table
Support Vector Domain Description can be generalized to describe K classes. In this problem, the samples belonging to each class j with j=1,..,K will be enclosed separately by a minimal sphere of a center aj and a radius Rj using SVDD. Then, to classify a new unknown sample xz a decision function must be evaluated.

In the literature, many decision functions of SVDD were presented. In this work, we will use the one introduced by Wu et al. [31]. Their proposed function has the advantage of being more precise because it takes into consideration the three possible positions of xz with respect to the minimal spheres. Following each position, they proposed a mathematical formulation of a decision function. Their approach was formulated as follows:

First case: When the unknown instance xz is outside all of the minimal spheres, they assign this sample to the class represented by the nearest sphere to xz:

classofxz=argmaxj=1,…,K(Rj−∥xz−aj∥).
(9)
Second case: When the test pot xz is located within only one sphere and is outside the remaining spheres, they assign xz to the class corresponding to the aforementioned sphere.

Third case: When the test point xz is located in the space between a set of M spheres. They obtain the class of xz by comparing the distance between this point and the center of each sphere. To eliminate the effect of different spherical radii, a relative distance was applied:

classofxz=argmaxj=1,…,M(−∥xz−aj∥Rj).
(10)
Related work
There have been several important researches that aim to improve Support Vector Domain Description, especially in terms of classification accuracy. These improvements concern both one-class and two-class versions. In this latter case, a label yi=±1 was introduced to differentiate between both classes. Among these improved versions of SVDD, we can highlight the following.

Wang et al. [28] introduced a new method to construct a minimal sphere (a,R) that separates data points of opposite classes with the maximum separation ratio represented by a parameter d. Their method provides a series of solutions ranging from spherical to linear decision boundaries and thus encompasses both SVM and SVDD classifiers. The primal problem of this approach was formulated as follows:

Minimize:R2−Kd2+C∑i=1NεiSubjectto:yi(R2−∥xi−a∥2)≥d2−εiεi≥0∀i=1,2,…N
(11)
where K≥1 is a constant that controls the ratio of the radius to the separation margin d.εi with i=1,2,…,N are slack variables, C is a regularization parameter, yi=±1 represents the label of the class to which belongs a training sample xi.

Jinglong et al. [10] proposed a new SVDD method that applies two minimal and distant spheres (a1,R1) and (a2,R2) separates two different classes of objects. The primal problem was formulated in such a way that the minimization of the radii R1 and R2, and the maximization of the distance between the centers a1 and a2 are applied simultaneously.

Minimize:R21+R22−Kd2+C∑i=1NεiSubjectto:∥xi−a1∥2−R21≤0+εi,yi=+1,εi≥0∀i=1,2,…N∥xi−a2∥2−R22≤0+εi,yi=−1,εi≥0∀i=1,2,…N
(12)
where d2=∥a1−a2∥2 and K is a strictly positive constant used to control the proportion between separation margin and separation radius of the sphere. The parameters εi,yi and C,∀i=1,2,..N have the same meaning as before.

Wu and Ye [30] presented a new version of SVDD. They searched the smallest sphere (a,R) that encloses most of the normal examples with yi=+1, while at the same time the margin d between the surface of this sphere and the outlier training data is as large as possible. This can result in a closed and tight boundary around the normal data. The primal problem of this approach was expressed as follows.

Minimize:R2−vd2+1l1v1∑yi=1εi+1l2v2∑yi=−1εiSubjectto:∥xi−a∥2≤R2+εi,yi=+1,εi≥0,∀i=1,2,…N∥xi−a∥2≥R2+d2−εi,yi=−1,εi≥0,∀i=1,2,…N
(13)
where ν, ν1 and ν2 are three positive constants, l1 is the number of normal data points with the label yi=+1, l2=s−m1 is the number of abnormal data points with the label yi=−1.εi,…,εl1+l2 are slack variables.

Mu and Nandi [16] proposed a variation of SVDD with negative samples that learns a closed spherically shaped boundary around a set of samples in the target class by involving the square of the slack variables εi. Their method requires solving the following optimization problem:

Minimize:R2+C12∑yi=+1ε2i+C22∑yi=−1ε2iSubjectto:yi(R2−∥xi−a∥2)≥−εi,εi≥0,∀i=1,2,…l
(14)
where C1 and C2 are regularization parameters corresponding, respectively to the positive and the negative training samples. The positive samples must be within the smallest sphere while the negative ones have to be outside this sphere.

Lee et al. [12] proposed a new SVDD that takes into consideration the characteristics of the target data. They associated new measurements based on the notion of a relative density degree to each sample in order to reflect the distribution of a given dataset. The primal problem of their new approach is formulated as follows:

Minimize:R2+C1∑i=1Nεi+C2∑l=1MεlSubjectto:ρi∥xi−a∥2≤R2+ε∀i=1,2,…,Nand1ρl∥xl−a∥2<R2−εl∀i=1,2,…,M
(15)
where N and M are respectively the total number of the target and negative samples. The parameters C1>0 and C2>0 are control parameters corresponding respectively to the positive and negative samples, ρi and ρl are the relative density degree defined by the exponentially weighted Parzen-window density.εi and εl have the same meaning as previously.

Ghasemigol et al. [8] modified the structure of the SVDD by using an ellipse with minimal volume to specify the boundary of the target objects instead of a sphere used by the standard SVDD. Their approach suffers from a major weakness because it can’t be applied in a feature space using kernels. The primal problem of their method was expressed as follows:

Minimize:∑j=1dR2j+C∑i=1NεiSubjectto:∑j=1d(xi,j−ajRj)2≤1+εi,εi≥0,∀i=1,…,n
(16)
where C is a strictly positive parameter that controls the trade-off between the volume of the minimal ellipse and the data description errors’.aj and Rj with j=1,…,d represent respectively the coordinates of the center and the radius of the ellipse, N is the total number of samples represented in d dimensions.

El boujnouni et al. [7] presented a new SVDD approach that improves the description of Multi-classes problem. They proposed to minimize the volume of the sphere (ak,Rk) that encloses the target dataset of the kth class, while at the same time maximize the distance between this sphere and the previous ones having the radii Rj and the centers aj with 1≤j<k. The primal problem can be written as follows:

Minimize:R2k−v⋅ρ2+C1∑i=1Nεi+C2∑j=1k−1δjSubjectto:∥xi−ak∥2≤R2k+εii∈{1,…,N}withyi=+1∥xi−ak∥2≥R2k−εii∈{1,..,N}withyi=−1∥ak−aj∥2≥ρ2−δjj∈{1,…,k−1}
(17)
where C1, C2 are constants real strictly positives, and C1∑Ni=1εi, C2∑k−1j=1δj are penalty terms,ρ2 is the margin between the center of the sphere (ak,Rk) and all previous ones (aj,Rj) with 1≤j<k.

Hao and Lin [9] proposed an extension of SVDD to solve K-classes problem in one formal step and considered the distribution of each class. The primal problem can be written as:

Minimize:∑m=1KR2m+C∑m=1K∑i=1SmεmiSubjectto:cmi(∥xi−am∥2−R2m)−εmi≤0andεmi≥0,∀m=1,…,Kandi=1,…,Sm
(18)
where Rm and am are the center and radius of the mth sphere, and εmi are the slack variables corresponding to a training data point xi in the mth class having Sm samples. cmi=1 if xi∈mth class and cmi=−1 if xi∉mth class.

Le et al. [13] proposed a new classification model that unites SVM and SVDD in one formulation and is called unified support vector machine (USVM). Their model takes into consideration the principles of maximum margin of SVM and minimal sphere of SVDD through two optimal points a and b. They introduced a new parameter called k that represents the curving degree. When k varies in its domain, USVM can provide a wider class of decision boundaries than either Support Vector Machine or Support Vector Domain Description. The formulation of this new version is expressed as follows:

Minimize:∥b−a∥2+C∑i=1NεiSubjectto:(∥xi−a∥2−k∥xi−b∥2)yi≥1−εi,εi≥0,∀i=1,…N
(19)
where a and b are two distinct points in the affine space Rd. Constructing the fixed point p such that the vectors pa−→ and pb→ satisfy the condition pa−→=kpb→ with (k≠1).C and εi has the same effect as previously.

Allahyari and Sadoghi-Yazdi [1] have proposed a new version of SVDD called Quasi Support Vector Data Description (QSVDD). This method aims at directing the center of the minimal sphere towards the gravity center of training samples and, by consequence, reducing the effect of noisy or outlier samples. The formulation of this new method can be expressed as follows:

Minimize:R2+BN2∑i=1N∑j=1N(xi−a)′⋅(xj−a)+C∑i=1NεiSubjectto:(xi−a)′⋅(xj−a)≤R2+εi,εi≥0∀i=1,…,N
(20)
The notation ′ represents the transpose of a vector or matrix and the parameter B regulates the importance of the gravity center of the training samples. The parameters C and εi have the same meaning as in SVDD mathematical formulation.

Liu and Zheng [15] integrated discrimination and description in a single classification model. They proposed minimum enclosing and maximum excluding machine. Based on SVDD classifier, they model the support of a target class by a minimal sphere (description), but unlike SVDD they seek an additional sphere having the same center as the first but with a large volume that excludes the negative samples (discrimination). The formulation of their method can be expressed as follows:

Minimize:γR2−ΔR2+C∑i=1NεiSubjectto:yi(R2−∥xi−a∥2)≥ΔR2−εi,εi≥0,ΔR2≥0∀i=1,…,N
(21)
The parameter γ is used to adjust the ratio between description and discrimination. ΔR2 is the margin between the smallest sphere that encloses target samples and the large sphere that excludes outliers.

Wang et al. [27] proposed a work entitled “dynamic hypersphere support vector data description without describing boundary for one-class classification”. They built a new method that checks dynamic variations of support vectors (SVs) and can classify a testing sample through capturing the changes in SVs. Wang and Lan [29] proposed a new version of SVDD to deal with the sensitivity of conventional SVDD to contamination (e.g., outliers or mislabeled objects) in the target training set. Their method assigns weights to every object according to its outlyingness in the kernel space to expose the outliers and withstand a large fraction of contamination. Qu et al. [19] proposed a new method to reduce the cubic time complexity of SVDD, which limits its efficiency. To achieve this goal, they suggest a new method to select a reduced-subset as the training set of SVDD while guaranteeing the classification quality. Chaudhuri et al. [2] proposed a new unsupervised method to select an efficient value for the bandwidth parameter of the Gaussian kernel commonly used by SVDD. Their method exploits a low-rank representation of the kernel matrix to suggest a kernel bandwidth value. Zheng [33] proposed a simple and easy-to-implement iterative algorithm to solve the minimization problem of SVDD. The author proves that his algorithm converges r-linearly to the unique minimum point.

Our approach
Taken the dataset A described in the second section, the key idea behind our approach is to divide randomly A into M disjoint subsets Aki,Nk={xi,xi+1,…,xi+Nk−1}. The index k enumerates a subset and lies in the interval {1,…,M} and i is the index of the first sample in a given subset. Nk is the number of samples in the kth subset with N=∑Mk=1Nk. The application of SVDD on these subsets gives a set of M minimal spheres S={S1,S2,…,SM} where each Sk has a center ak and a radius Rk and encloses the samples of the kth subset Aki,Nk. Our aim is to find the smallest global sphere S with the center a and the radius R that encloses all of these small spheres. Figure 3 shows the condition that must satisfy a large sphere S(a,R) to enclose another small sphere Ss(as,Rs). This condition can be expressed mathematically as follows:

(R−Rs)2≥∥a−as∥2
(22)
Fig. 3
figure 3
The condition that must satisfy Ss(as,Rs) to be inside S(a,R)

Full size image
The condition described above can be generalized to surround M spheres {(a1,R1),(a2,R2),…,(aM,RM)} with one enclosing sphere (a,R). Our goal is to keep the idea behind SVDD classifier by minimizing the volume of this latter. This constrained optimization problem can be written as follows:

Minimize:R2Subjectto:(R−Rk)2≥∥a−ak∥2∀k=1,2,…,M
(23)
The primal formulation of this problem can be expressed based on Lagrange multipliers βk with k=1,…,M. as follows:

Minimize:L=R2−∑k=1Mβk((R−Rk)2−∥a−ak∥2)Subjectto:βk≥0∀k=1,2,…,M
(24)
The Karush–Kuhn–Tucker conditions or KKT conditions are:

Parallel gradients conditions:

∂L∂R=0⇒R=−∑Mk=1βkRk(1−∑Mk=1βk)
(25)
∂L∂a=0⇒a=∑Mk=1βkak∑Mk=1βk
(26)
Orthogonality conditions:

βk((R−Rk)2−∥a−ak∥2)=0∀k=1,2,…,M
(27)
Satisfaction of original constraints:

((R−Rk)2−∥a−ak∥2)≥0∀k=1,2,…,M
(28)
Lagrange multiplier non-negativity:

βk≥0∀k=1,2,…,M
(29)
By analyzing the derivatives above, it can be seen that two conditions concerning the Lagrange multipliers must be satisfied, namely: ∑Mk=kβk≠0 and ∑Mk=1βk>1.

The dual problem can be written as follows:

W=−∑Mk=1∑Ml=1βkβlakal∑Mk=kβk−∑Mk=1∑Ml=1βkβlRkRl(1−∑Mk=kβk)+∑k=1Mβk(a2k−R2k)
(30)
To check whether a sphere (Rk,ak) is inside the global sphere (R,a), it’s sufficient to evaluate the sign of the decision function described by the equation below:

sign((R−Rk)2−∥a−ak∥2)
(31)
Our new approach can be applied in feature space by using kernel functions that can be introduced in the optimization problem described by Eq. (30) as follows:

The equation Eq. (5) in the second section gives the expression of the center a of the smallest sphere that encloses a set of points A={x1,x2,…,xN}. This expression can be rewritten for a specific subset Aki,Nk={xk1,xk2,…,xkNk} as follows:

ak=∑i=1Nkαkixki
(32)
where αki is the Lagrange multiplier related to the ith example xki of the kth subset. By incorporating this formulation in the Eq. (30), the expression of the equation W in the feature space becomes:

W=−∑Mk=1∑Ml=1βkβl∑Nki=1∑Nlj=1αliαkjxli.xkj∑Mk=kβk−∑Mk=1∑Ml=1βkβlRkRl(1−∑Mk=kβk)+∑k=1Mβk⎛⎝(∑i=1Nkαkixki)2−R2k⎞⎠
(33)
W=−∑Mk=1∑Ml=1βkβl∑Nki=1∑Nlj=1αljαkiK(xlj,xki)∑Mk=kβk−∑Mk=1∑Ml=1βkβlRkRl(1−∑Mk=kβk)+∑k=1Mβk(∑i=1Nk∑j=1NkαkiαkjK(xki,xkj)−R2k)
(34)
Also, the decision function expressed by the equation Eq. (31) can be rewritten in the feature space as follows:

sign⎛⎝⎜⎜(R−Rk)2−∑Mi=1∑Mj=1βiβj(∑Mi=1βi)2∑l=1Ni∑l=1NjαilαjlK(xil,xjl)+2∑Mi=1βi∑Mi=1βi∑l=1Ni∑l=1NkαilαklK(xil,xkl)−∑l=1Nk∑l=1Nkα2klK(xkl,xkl)⎞⎠⎟⎟
(35)
The optimization problem expressed by the Eqs. (30) and (34) has M variables β=(β1,β2,…,βM) and can be solved through an improved Newton–Raphson method. The update rule of this method can be expressed as:

βn+1=βn−γ[H(βn)]−1∇(βn)
(36)
where H is the Hussein matrix, ∇ is the gradient operator, and βn is the value of the vector β=(β1,β2,…,βM) at the nth step of the resolution process. ∇ and H are evaluated from the equations Eqs. (38) and (40), respectively.

The first derivative of W with respect to the lth Lagrange multiplier βl is:

∂W∂βl=−∑Mk=1βkak(∑Mj=1βj(2al−aj))(∑Mk=1βk)2−∑Mk=1βkRk(2Rl−∑Ml=1βl(2Rl−Rl))(1−∑Mk=1βk)2+a2l−R2l
(37)
Thus, the gradient of W can be expressed as:

∇W=⎡⎣⎢⎢⎢⎢∂W∂β1⋮∂W∂βM⎤⎦⎥⎥⎥⎥
(38)
The second derivative of W with respect to the lth and the mth Lagrange multipliers is:

∂2W∂βl∂βm=−2(∑Mk=1βk)2∑Ml=1βl(al(am+al)−amal)−∑Mk=1βk∑Ml=1βlal∑Mk=1βk(2al−ak)(∑Mk=1βk)4−2(1−∑Mk=1βk)2(RmRl−∑Ml=1βl(Rl(Rm+Rl)−RmRl))−(1−∑Mk=1βk)∑Ml=1βlRl(2Rl−∑Mk=1βk(2Rl−Rk))(1−∑Mk=1βk)4
(39)
Thus, the Hussein matrix of W can be expressed as:

H=⎡⎣⎢⎢⎢⎢⎢∂2W∂β21⋮∂2W∂βM∂β1…⋱…∂2W∂β1∂βM⋮∂2W∂β2M⎤⎦⎥⎥⎥⎥⎥
(40)
The optimization algorithm proposed to find the smallest sphere that encloses a set of spheres using Newton–Raphson method is as follows:

figure b
As said before, our objective is to solve two problems: The first one concerns classifying a big corpus of data points, the second regards classifying a set of samples dynamically. Below the expression of two algorithms to solve these problems:

figure c
figure d
As said before, the proposed method begins by dividing a large dataset of size N into M small disjoint subsets of random sizes N1,N2,…,NM then applying SVDD to enclose each subset Ni with the smallest sphere Si , and finally applying the proposed algorithm to find the smallest sphere S that encloses the M spheres found previously. The time complexity of SVDD is O(N3) [4, 14] where N represents the number of samples in the training dataset. Consequently, the overall computational complexity of our method can be expressed as the summation of the complexities of M applications of SVDD in addition to the complexity required by our algorithm that uses Newton–Raphson method to minimize a function of M variables where M<<<N that can be ignored by SVDD complexity.

To compare the complexity of the proposed method and SVDD, we will begin by searching for the relation between N3 and N31,N32,…,N3M. Equation (41) yields this information.

N3=(N1+N2+⋯+Nl)3=∑i=1MN3i+∑i=1M∑j≠iM3N2iNj+∑i=1M∑j=iM∑k=jM6NiNjNk
(41)
By introducing the complexity, the Eq. (41) becomes Eq. (42). The latter expresses the difference between the complexity of standard SVDD which is O(N3) and the one resulting from the application of SVDD on each subset ∑Mi=1O(N3i). It can be seen that this difference is very significant and increases with the growth of M (M<<<N) which reduces the computational complexity of our method compared to standard SVDD.

O(N3)−∑i=1MO(N3i)=O(∑i=1M∑j≠iM3N2iNj+∑i=1M∑j=iM∑k=jM6NiNjNk)
(42)
Experimental evaluations
The first experiment aims to show graphically the classification accuracy of our new approach illustrated in Figs. 4, 5, and 6 against the application direct of Support Vector Domain Description shown in Fig. 7. The experimental dataset contains 28 random points in two dimensions. In this experiment, the regularization parameter C of SVDD is fixed at 100. The parameters of our algorithm N°1 δ and p are equal to 0.01 and 0.0001, respectively, and the parameter Max that represents the maximum of iterations is equal to 1000.

Fig. 4
figure 4
The classification accuracy of our new algorithm using dot product

Full size image
Fig. 5
figure 5
The classification accuracy of our new algorithm using Gaussian kernel

Full size image
Fig. 6
figure 6
The classification accuracy of our new algorithm using polynomial kernel

Full size image
Fig. 7
figure 7
The classification accuracy using directly standard SVDD with dot product, Gaussian, and polynomial kernels

Full size image
Figures 4, 5, and 6 correspond respectively to the use of dot product, Gaussian kernel with σ=50, and Polynomial kernel with (d=2,α=1,c=0) (Table 1). Each Figure is divided into three parts that represent the steps of applying the proposed approach: The left side displays the distribution of the initial dataset. The middle shows the decomposition of the initial dataset into four subsets and the application of standard SVDD on each subset. The right displays the application of our new approach (Algorithm N°1) on the four spheres found previously and the resulting global minimal sphere. Figure 7 shows the classification accuracy by applying Standard Support Vector Domain Description directly on the aforementioned dataset. In this last test, we used the same kernel functions with the same values of their parameters as previously. Figure 7, from left to right, shows the minimal spheres found using dot product, Gaussian, and polynomial kernels, respectively.

By comparing Fig. 4 and the left side of Fig. 7 that corresponds to the use of dot product, we remark that the classification performance is exactly the same. When applying Gaussian kernel (Fig. 5), our method finds a global hyper-sphere that surrounds, successfully, the four minimal hyper-spheres and has the same form and curvature as that found by a direct application of SVDD (The middle part of Fig. 7). However, the volume of our hyper-sphere is little bigger than the smallest one found by Support Vector Domain Description. When using a polynomial kernel (Fig. 6), the global hyper-sphere is approximately the same (from the point of view of curvature and form) as the one found by standard SVDD (The right part of Fig. 7), but is quite bigger. In conclusion, Gaussian kernels describe more precisely a dataset than polynomial ones.

Let us now perform another experiment that aims to compare the performance of the conventional Support Vector Domain Description and our method. This comparison concerns three levels: The recognition rate in the training step, the recognition rate in the testing step, and the duration required to find the minimal hyper-spheres (maximizing the constrained QPs). The experiment is conducted on four classification problems taken from UCI repository of machine learning databases [24]. The first one is called optical recognition of handwritten digits (ORHD) and contains 5620 instances divided into ten classes; each sample has 64 integer attributes in the range {0, 1, …, 16}. This dataset is stored into two files: the first one is named “optdigits.tra” and contains 3823 of training records, and the second one is named “optdigits.tes” and has 1797 of testing samples. The second one is landsat satellite (LS) and consists of 6435 instances divided into six classes; each sample has 36 numerical attributes in the range 0—255. This dataset is stored into two files: the first one is named “sat.trn” and contains 4435 of training records, and the second one is named “sat.tst” and has 2000 of testing samples. The third one is called pen-based recognition of handwritten digits (PRHD). It contains 10,992 instances divided into ten classes; each sample has 16 integer attributes in the range {0, 1, …, 100}. Also, this database is stored into two files: the first one is named “pendigits.tra” and contains 7494 of training records, and the second one is named “pendigits.tes” and has 3498 of testing samples. And the last one is letter recognition (LR) and consists of 20,000 instances divided into 26 classes; each sample has 16 attributes in the range {0, 1, …, 15}. This dataset is stored in one file named “letter-recognition.data” that contains the training and testing instances. In the experiment, 80% (16,000) of instances will be used for training and 20% (4000) of instances for testing.

The kernel used in this experiment is a Gaussian since it is widely used in classification tasks and has a good generalization capability. The width σ of the Gaussian will be varied in the intervals {2, 4, …, 40} (ORHD), {2, 4, …, 60} (PRHD), {5, 10, …, 75} (LS), and {0.5, 1.00, …, 5.00} (LR) the parameters C,δ, p and Max equals to 100, 0.01, 1000, respectively. The results of this experiment are shown in Figs. 8, 9, and 10. Our experiments are conducted on a computer with an Intel(R) Core(TM) i5-7200U CPU @ 2.50 GHz (4CPUs) having 12 GB of RAM.

Fig. 8
figure 8
The variation of the classification accuracy of standard SVDD and our method in training step with respect to the Gaussian kernel parameter (σ) and the number of subsets (M) on which the datasets was divided

Full size image
Fig. 9
figure 9
The variation of the classification accuracy of standard SVDD and our method in testing step with respect to the Gaussian kernel parameter (σ) and the number of subsets (M) on which the datasets was divided

Full size image
Fig. 10
figure 10
The variation of the duration to maximize the QP problems in the training using standard SVDD and our method with respect to the Gaussian kernel parameter (σ) and the number of subsets (M) on which the datasets was divided

Full size image
Figure 8 shows the variation of the recognition rates of standard SVDD and the proposed method in the training step with respect to the number of decompositions of the initial database into 2, 3, 4, and 5 subsets, and the values taken by σ. Also, this figure illustrates the influence of the growth of the number of instances used in the training (ORHD → LS → PRHD → LR) on the recognition rates. Generally, it can be seen that the recognition rate of standard SVDD is higher than our method, and the rates given by this latter decrease as the number of subsets is increased. These results do not allow deciding which method is better because high rates in the training step generally correspond to an overfitting. This phenomenon can occur where a classifier learns the detail in the training data, to the extent that it negatively impacts the performance of the model in the testing (see in Fig. 9). Furthermore, it can be seen that the majority of curves have the same behavior and converge in tight range with the growth of σ.

Figure 9 shows the variation of the recognition rates of standard SVDD, and the proposed method with respect to the values that takes the width of the Gaussian kernel and also the number of decompositions of the initial database into 2, 3, 4, and 5 subsets. It can be seen that the recognition rates begin with minimal values near to 10% and grow with the increase of the Gaussian width σ till maximum values then they become approximately stable or decrease. The curves of a specific dataset have the same general form and tend to converge in tight range. Concerning ORHD, standard SVDD achieves its maximum value of recognition rate with 92.99% and our method gives 92.32%, 88.31%, 88.65%, and 88.20% when the number of division of the initial dataset is equal to 2, 3, 4, and 5, respectively. Concerning LS, standard SVDD achieves its maximum value of recognition rate with 83.25% and our method gives 81.25%, 80.75%, 77.00%, and 78.05% when the number of division of the initial dataset is equal to 2, 3, 4, and 5, respectively. Concerning PRHD, standard SVDD reaches its maximum value of recognition rate with 82.42% and our approach gives 82.30%, 82.25%, 82.33%, and 79.56% when the number of division of the initial dataset is equal to 2, 3, 4, and 5, respectively. Concerning LR, standard SVDD achieves its maximum value of recognition rate with 84.59% and our method gives 76.61%, 76.74%, 74.45%, and 74.40% when the number of division of the initial dataset is equal to 2, 3, 4, and 5, respectively. To summarize, the numerical results have shown that our algorithm has a lower and acceptable (in some cases) generalization capability compared to the standard SVDD.

Figure 10 shows the variation of the duration to solve the QP problems of standard SVDD, and the proposed method with respect to the values that takes the width of the Gaussian kernel and also to the number of decompositions of the initial database {2, 3, 4, and 5} subsets. It can be seen that, generally, the duration to maximize the QPs increases with the growth of σ. Also, the increase rate of this duration is accelerated in the case of standard SVDD compared to the proposed method. Furthermore, for a fixed value of σ when the number of subsets on which the initial dataset was divided grows from 2 to 5, the duration to maximize the QPs decreases. By comparing ORHD (3823 of training instances), LS (4435 of training instances), PRHD (7494 of training instances), and LR (16,000 of training instances), we remark that the duration required to maximize the QPs of standard SVDD increases drastically with the growth of the number of the training examples contrary to our method that still gives lower and tolerable durations. To resume, numerical results have shown that our algorithm requires a minimum of CPU time to train a large corpus of data and is quasi-indifferent with the variation of σ compared to the standard SVDD.

Conclusion
In this work, we have introduced a new scheme to use Support Vector Domain Description classifier in cases where the dataset to describe is either too large to be trained with the conventional SVDD or its size increases with time. Our proposed method is based on divide-and-conquer strategy and can find approximately a small sphere that surrounds a large corpus of data in a reasonable duration. Also, it can train a dataset that grows dynamically without the need to rebuild entirely the model of SVDD (minimal enclosing sphere) each time when new instances will appear.

After formulating mathematically our method, we proposed an algorithm that permits to solve it numerically based on Newton–Raphson method. Experimental evaluation of this new approach using four benchmark datasets having different number of attributes and sizes has shown that the recognition rate given by our method is comparable to the standard SVDD in term of generalization capability. Contrary to our method, experimental results prove that conventional SVDD consumes considerably CPU time caused by solving a constrained quadratic optimization problem and obviously consumes also memory resources needed to store Gram matrix.

Our future work will be oriented to the application of the proposed method in a big data environment, where the quantity of information generated from different resources increases continuously, and conventional classifiers cannot deal with this huge amount of data in a limited time. Also, to use this new method in devices with limited CPU because it reduces the amount of time required to train a given dataset.