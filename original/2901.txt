Uncertainty about data appears in many real-world applications and an important issue is how to manage, analyze and solve optimization problems over such data. An important tool for data analysis is clustering. When the data set is uncertain, we can model them as a set of probabilistic points each formalized as a probability distribution function which describes the possible locations of the points. In this paper, we study k-center problem for probabilistic points in a general metric space. First, we present a fast greedy approximation algorithm that builds k centers using a farthest-first traversal in k iterations. This algorithm improves the previous approximation factor of the unrestricted assigned k-center problem from 10 (see [1]) to 6. Next, we restrict the centers to be selected from all the probabilistic locations of the given points and we show that an optimal solution for this restricted setting is a 2-approximation factor solution for an optimal solution of the assigned k-center problem with expected distance assignment. Using this idea, we improve the approximation factor of the unrestricted assigned k-center problem to 4 by increasing the running time. The algorithm also runs in polynomial time when k is a constant. Additionally, we implement our algorithms on three real data sets. The experimental results show that in practice the approximation factors of our algorithms are better than in theory for these data sets. Also we compare the results of our algorithm with the previous works and discuss about the achieved results. At the end, we present our theoretical results for probabilistic k-median clustering.

Access provided by University of Auckland Library

Introduction
Uncertainty about data appears in many real-world applications and an important issue is how to manage, analyze and solve optimization problems over such data which attracted attention from several research communities including theoretical computer science, data science, databases and wireless networks. Most of the time, we need to deal with optimization problems in databases, such as data integration, streaming, cluster computing and sensor network applications that involve parameters and inputs whose values are known only with some uncertainty[2].

For example consider a typical sensor monitoring application, where sensors are used to monitor the identities, locations and other features of a number of objects (such as people, vehicles, animals). However, the sensor data can be very noisy and even conflicting with each other (e.g., the same object appeared in two different locations at the same time). A major way to capture such uncertain sensor readings is to use probabilistic models. Another example is about people in a city that are located in various locations (home, work, etc). Here the data set is the location of people and the goal is to solve an optimization problem on this data set. This case can also be modeled by probabilistic data.

Here, we focus on k-center problem for uncertain data. In the k-center problem for certain points, a set of n clients are given that are located in a metric space. The goal is to build k servers so that the maximum distance of each client to its closest server is minimized. In the real world, the location of clients is not fixed, they can be at home, at work, or at some other locations and each of these possible locations have certain probability. This leads to a definition of uncertain k-center problem, where the goal is to minimize the expected travel cost.

This expected cost can be defined according to the practical needs in different ways. One way is to find the cost of each realization of our uncertain clients and then take the weighted average of these costs according to the probability of each realization. This is called the unassigned uncertain k-center problem. Another method is to assign to each client (regardless of its stochastic location) a fixed server by to a certain rule. One rule (but not the only one) is to assign the server with minimum expected distance to that client. Now, for each realization we calculate the cost to be the maximum distance of clients with their assigned server and take a weighted average as before. This is the assigned uncertain k-center problem. An optimal solution that minimizes the cost with a given assignment or among all possible assignments is the solution of the restricted or unrestricted assigned uncertain k-center problem, respectively.

The k-center problem is NP-hard [3] and there are approximation algorithms for both certain and uncertain versions. Therefore, our goal is to present fast and robust algorithms that improve the approximation factor of the previous algorithms. We also study k-median problem for uncertain data which is also NP-hard.

Problem statement
k-center
For a set of (certain) points {P1,…,Pn} in a metric space X with metric d, the classical k-center problem asks for k center points C={c1,…,ck} in X that minimize the following cost

cost(c1,…,ck)=maxi=1,…,n d(Pi,C),

where d(Pi,C)=minc∈Cd(Pi,c) and d(x, y) is the distance between x and y.

In the uncertain k-center problem each point has a finite number of possible locations independently from the other points with given probabilities. Precisely, we are given a set D={D1,…,Dn} of n discrete and independent probability distributions.

The i-th distribution, Di is defined over a set of zi possible locations Pi,1,…,Pi,zi∈X. A probability pi,j is associated with each location such that ∑zij=1pi,j=1 for every i∈[n]={1,…,n}. Thus, the probabilistic points can be considered to be independent random variables Xi. The locations together with the probabilities specify their distributions Pr[Xi=Pi,j]=pi,j for every i∈[n] and j∈[zi]. A probabilistic set Y, consisting of the probabilistic points, is therefore a random variable. Let z=max{z1,…,zn}. For simplicity, we use the notation P^i for a realization of an uncertain point Pi and prob(P^i) for its probability. We define Ω as the probability space of all realizations R={P1,j1,…Pn,jn} with prob(R)=∏ni=1prob(Pi,ji).

There are three known versions of the k-center problem for uncertain points based on the definition of the cost function. To motivate, imagine a city where its citizens are randomly located in several locations (home, work, etc.) and our goal is to build k hospitals to make the expected travel cost of the citizens minimum. There are two plausible scenarios. First, any citizen can go to a closest hospital, depending on his location, this corresponds to the unassigned version of the k-center problem. Second, that a citizen based on his insurance policy or health issue has to go to an assigned hospital regardless of his location. This is corresponding to the assigned version of the k-center problem. If the assignment is an output of problem, then we have the unrestricted assigned version and if there is a assignment rule given as an input of the problem, we have the restricted assigned version. These are made precise below.

Unassigned version: The goal is to find k centers C={c1,…,ck} that minimize Ecost(c1,c2,…,ck)=∑R∈Ωprob(R)maxi=1,…,nd(P^i,C).

Unrestricted assigned version: All realizations of an uncertain point Pi are assigned to a center denoted by A(Pi). In fact, all realizations of an uncertain point Pi in the assigned version are in the cluster of the same center. The goal is to find k centers {c1,⋯,ck} and an assignment A:{P1,⋯,Pn}→{c1,⋯,ck} that minimize EcostA(c1,c2,…,ck)=∑R∈Ωprob(R)maxi=1,…,nd(P^i,A(Pi)).

Restricted assigned version: For any set of uncertain points {P1,…,Pn} and k centers {c1,…,ck} an assignment rule A:{P1,…,Pn}→{c1,…,ck} is given. Which means each Pi is assigned to a center cPi based on a defined rule. The goal is to find {c1,…,ck} that minimizes the value of EcostA(c1,…,ck). For example we consider the expected distance assignment defined in [4]. In the expected distance assignment, each uncertain point Pi is assigned to

ED(Pi)=argminQ∈{c1…,ck}∑P^i∈Diprob(P^i)d(P^i,Q).
If several centers minimize the above expected distance, we arbitrarily assign one of them to Pi. This comment applies to other cases where we take argmin. There are other assignments such as expected point assignment and 1-center assignment that have been introduced in [1].

k-median
For a set of (certain) points {P1,…,Pn} in a metric space X with metric d, the classical k-median problem asks for k center points C={c1,…,ck} in X that minimize the following cost

cost(c1,…,ck)=∑i=1,…,n d(Pi,C),

In the uncertain k-median problem for a given set D={D1,…,Dn} of n discrete and independent probability distributions. All realizations of an uncertain point Pi are assigned to a center denoted by A(Pi). In fact, all realizations of an uncertain point Pi in the assigned version are in the cluster of the same center. The goal is to find k centers {c1,⋯,ck} that minimize

EcostA(c1,c2,…,ck)=∑R∈Ωprob(R)∑i=1,…,nd(P^i,A(Pi)).

And we have

EcostA(c1,c2,…,ck)=∑i=1n∑Pi^∈Dprob(Pi^)d(Pi^,cpi).
Note that in the assigned version the minimum cost happens when we assign the points to the centers by expected distance assignment, i.e., each uncertain point Pi is assigned to

ED(Pi)=argminQ∈{c1…,ck}∑P^i∈Diprob(P^i)d(Pi^,Q).
So, we do not need to consider the restricted assigned version in this problem since the optimal assignments happens in the expected distance assignment.

Due to the linear nature of k-median problem, the unassigned case can be efficiently reduced to its non-probabilistic counterparts. But in the assigned version, some more work is needed in order to give required guarantees.

Related works
The deterministic k-center problem is a classical problem that has been extensively studied. It is well known that the k-center problem is NP-hard even in the plane [3] and approximation algorithms have been proposed (e.g., see [5,6,7]). Efficient algorithms were also given for some special cases, e.g., the smallest enclosing circle and its weighed version and discrete version [8,9,10], the Fermat–Weber problem [11], k-center on trees [12,13,14]. Refer to [15] for other variations of facility location problems. The deterministic k-center in one-dimensional space is solvable in O(nlogn) time [16]. An elegant approximation algorithms for k-center clustering is the 2-factor approximation algorithm by Gonzalez [17] which can be made to run in O(nlogk) time [18]. And one of the fastest methods for k-center clustering in 2 and 3 dimensions is by Agarwal and Procopiuc [19] which uses a dynamic programming approach to k-center clustering with the running time of O(nlogk)+(kϵ)O(k1−1d). Another elegant solution to the k-center problem was given by Badoiu et al. [6]. This algorithm gives a (1+ϵ)-approximation factor algorithm which runs in 2O((klogk)/ϵ2)dn in Rd. Another algorithm based on coresets runs in O(kn) [20]. Their algorithm runs much faster in practice than its worst-case time.

Several recent works have dealt with clustering problems on probabilistic data. One approach was to generalize well-known heuristic algorithms to the uncertain setting. For example, a clustering algorithm called DBSCAN [21] was also modified to handle probabilistic data by Kriegel and Pfeifle [22, 23] and Xu and Li [24]. Refer to [25] for a survey on data mining of uncertain data.

Cormode and McGregor [26] introduced the study of probabilistic clustering problems. They developed approximation algorithms for the probabilistic settings of k-means, k-median as well as k-center clustering. They described a pair of bicriteria approximation algorithms, for inputs of a particular form; one of which achieves a (1+ϵ)-approximation with a large blow up in the number of centers, and the other which achieves a constant factor approximation with only 2k centers. Guha and Munagala [2] improved upon the previous work. In finite metric space, they achieved O(1)-approximations, while preserving the number of centers both for assigned and unassigned version of the k-center problem. More precisely, the approximation factor of their algorithm for unrestricted assigned version is 15(1+2ϵ) and the running time of their algorithm is polynomial in input size and 1ϵ. Munteanu and et al. [27] presented the first polynomial time (1 +ϵ)-approximation algorithm for the probabilistic smallest enclosing ball problem with extensions to the streaming setting. Wang and Zhang [4] introduced the restricted assigned version under the expected distance assignment. They solved the one-dimensional k-center problem, in O(znlogzn+nlogklogn) time. If the dimension is one and the z locations of each uncertain point are sorted, they reduced the problem to a linear programming problem and thus solved the problem in O(zn) time by applying a linear time algorithm. Haung and Li [28] gave the first PTAS (Polynomial Time Approximation Scheme) for unassigned version of the probabilistic k-center problem in Rd, when both k and d are constants. However for the assigned version, no such PTAS solutions are found and it seems to be a challenging problem. Alipour and Jafari [1] presented a 10-approximation factor algorithm for the unrestricted k-center problem. They also provide fast algorithms for the restricted k-center problem.

Clustering relative to the k-median objective is also known to be NP-hard [3]. And the best known approximation algorithm guarantees a (3+ϵ)-approximate solution, with time cost exponential in ϵ [5]. See [29,30,31,32] for other approximation algorithm for certain version of k-median problem.

In [26], they proved that given an algorithm for weighted k-median clustering with approximation factor α, they can find a (2α+1)-approximation for the uncertain k-median problem. By [32], this gives a (3+ϵ) approximate solution for uncertain k-median in Euclidean space.

Table 1 Our results for various versions of uncertain k-center compared to [1]
Full size table
Our results
In this paper we present some algorithms for both unrestricted and restricted version of the problem. Our results are summarized in Table 1. The main approaches of the paper are as follows. We present a greedy algorithm that is an extension of the well known greedy algorithm for the classical k-center problem presented in [17] and the algorithms presented in [1]. The greedy algorithm for the deterministic k-center problem simply builds the k centers among the given points by choosing the point farthest away from the current set of centers in each iteration as the new center. We use the same idea for the uncertain k-center problem but the definition of the farthest point and the location of the centers are different. Then we analyze the approximation factor of this algorithm. Note that the approximation factor of the new algorithm is improved but the running time of previous algorithm is better than ours. However when k is a constant, the running times is the same which happens in real applications most of the times.

We also restrict the centers to be selected from a specific set of points and show that an optimal solution for the unrestricted assigned k-center problem by this restriction is a 2-approximation factor solution for the original unrestricted assigned k-center problem. This enables us to improve the approximation factor for the unrestricted assigned k-center problem significantly. When k is a constant the running time is polynomial on n and z. We implement our algorithms on real data sets and show that the approximated solutions in practice are better than in theory. Also we implement the proposed algorithm in [1] to compare our results with them. We conclude that the cost of our algorithm is better than [1] both in theory and practice. As far as we know this is the first time that real problems are modeled as uncertain k-center problem and experimental results are given for them. At the end we present an approximation algorithm for probabilistic k-median problem in Euclidean space.

Note that the preliminary conference version of this paper is appeared in [33]. In this journal version, in addition to the k-center problem, we also study the k-median problem and present some improved theoretical results for the k-median problem in Sect. 6.

Expected distance assignment in Rd
We propose our algorithm for the expected distance assignment in Rd. The expected point of each uncertain point Pi is defined as

Pi¯=∑Pi^∈Diprob(Pi^)Pi^.
The distance of an uncertain point P from a given set of points C is defined as d(C,P)=minc∈C∑P^∈Dprob(P^)d(P^,c).

Lemma 2.1
[1] For an uncertain point P in a Euclidean space and any point Q, we have

d(P¯,Q)≤ED(P,Q)=∑P^∈Dprob(P^)d(P^,Q).
Now we describe our algorithm (See Algorithm 2.1).

figure b
Theorem 2.1
The k-centers, C={c1,…,ck}, chosen by Algorithm 2.1 give a 4-approximation solution for the expected distance assignment of uncertain k-center problem.

Proof
Let c∗1,…,c∗k be an optimal solution. So, we have OPT=EcostED(c∗1,…,c∗k)=∑R∈Ωprob(R)maxni=1d(Pi^,c∗pi) where Pi is assigned to c∗Pi∈{c∗1,…,c∗k} under the expected distance assignment. Two cases can happen.

Case 1: suppose that for each c∗i, there is exactly one uncertain point Pj such that Pj¯=ct∈C and Pj is assigned to c∗i in the optimal solution. In this case we have

EcostED(c1,…ck)=∑R∈Ωprob(R)maxi=1nd(Pi^,cPi)≤∑R∈Ωprob(R)maxi=1n(d(Pi^,c∗Pi)+d(c∗Pi,cPi))
Suppose that d(c∗P1,cP1)=maxni=1d(c∗Pi,cPi)

≤OPT+d(c∗P1,cP1)
So, it is enough to show that d(c∗P1,cP1)≤3OPT. We have

d(c∗P1,cP1)≤∑P^1∈D1prob(P^1)(d(P^1,c∗P1)+d(P^1,cP1))≤OPT+∑P^1∈D1prob(P^1)d(P^1,Pj¯)
By our assumption, there is a point Pj such that Pj¯∈C and Pj is assigned to c∗P1. Since P1 is assigned to cP1, the expected distance of P1 from cP1 is less than the expected distance of P1 from Pj¯. So,

≤OPT+∑P^1∈D1prob(P^1)d(P^1,c∗P1)+d(c∗P1,Pj¯)≤OPT+OPT+d(c∗P1,Pj¯)
By Lemma 2.1,

≤2OPT+∑Pj^∈Djprob(Pj^)d(Pj^,c∗P1)≤3OPT
Case 2: Suppose that there are two points Pu and Pv such that Pu¯ and Pv¯ are in C and both Pu and Pv are assigned to the same center (for example c∗2) in the optimal solution. Suppose that Pu¯ is added to C before Pv¯ and Pv¯ is added in the ithe step. So, we have

EcostED(c1,…,ck)≤EcostED(c1,…,ci−1)=∑R∈Ωprob(R)maxj=1nd(Pj^,cPj)≤∑R∈Ωprob(R)maxj=1n(d(Pj^,c∗Pj)+d(c∗Pj,cPj))
Note that cPj∈{c1,…,ci−1}. Suppose that d(c∗P1,cP1)=maxni=1d(c∗Pi,cPi), then we have

≤OPT+d(c∗P1,cP1)≤OPT+∑P^1∈D1prob(P^1)(d(P^1,c∗P1)+d(P^1,cP1))≤OPT+OPT+∑P^1∈D1prob(P^1)d(P^1,cP1)
Now we show that ∑P^1∈D1prob(P^1)d(P^1,cP1)≤2OPT. According to the algorithm, the farthest point to c1,…,ci−1 is Pv, so

∑P^1∈D1prob(P^1)d(P^1,cP1)≤∑Pv^∈Dvprob(Pv^)d(Pv^,cPv)
Also we know that the expected distance of Pv from Pu¯ is greater than the expected distance of Pv from cPv which means

≤∑Pv^∈Dvprob(Pv^)d(Pv^,Pu¯)≤∑Pv^∈Dvprob(Pv^)(d(Pv^,c∗Pv)+d(c∗Pv,Pu¯))≤OPT+d(c∗Pv,Pu¯)
Be Lemma 2.1,

≤OPT+∑Pu^∈Duprob(Pu^)d(Pu^,c∗Pv)
Since Pu and Pv are assigned to the same center we have

≤2OPT
□

Now we analyze the running time of Algorithm 2.1. First, we compute the expected point of each uncertain point which takes O(z) for each uncertain point and O(nz) for all of them. In each iteration, we compute the expected distance of each point from C. There are at most k centers and computing the expected distance of each uncertain point from each center takes O(z) time. Since there are k centers and n uncertain points, each iteration takes O(nzk) time. So, the overall running time is O(nzk2).

Unrestricted k-center in a general metric
In this section, we propose our algorithm for unrestricted k-center problem in a general metric space.

Theorem 3.1
[1] Let D be the set of possible locations of an uncertain point P and let

O′=argminQ∈D∑P^∈Dprob(P^)d(P^,Q)
then O′ is a 2-approximation for the 1-center of P, i.e., for any point Q,

∑P^∈Dprob(P^)d(P^,O′)≤2∑P^∈Dprob(P^)d(P^,Q).
The distance of Pj from a point set C is:

d(C,Pj)=minO′i∈C∑prob(Pj^)d(Pj^,O′i).
Now we present our algorithm (See Algorithm 3.1).

figure c
Theorem 3.2
If there is a (1+α)-approximation algorithm for the 1-center of an uncertain point P. Then, the k centers chosen by Algorithm 3.1, C={c1,…ck}, under the expected distance assignment give a (5+α)-approximation solution for the unrestricted assigned k-center problem.

Proof
Let c∗1,…,c∗k and assignment A be an optimal solution for the unrestricted k-center problem. So, OPT=EcostA(c∗1,…,c∗k)=∑R∈Ωprob(R)maxni=1d(Pi^,c∗pi). Where Pi is assigned to c∗Pi∈{c∗1,…,c∗k} in assignment A. We have two cases.

Case 1: Suppose that for each c∗i, there is exactly one uncertain point Pj such that O′j=ct∈C and Pj is assigned to c∗i in the optimal solution. In this case we have

EcostED(c1,…,ck)=∑R∈Ωprob(R)maxi=1nd(Pi^,cpi)≤∑R∈Ωprob(R)maxi=1n(d(Pi^,c∗pi)+d(c∗pi,cpi))
Without loss of generality, we can assume that d(c∗p1,cp1)=maxni=1d(c∗pi,cpi).

≤OPT+d(c∗p1,cp1)≤OPT+∑P^1∈D1prob(P^1)(d(P^1,cP1)+d(P^1,c∗P1))≤OPT+OPT+∑P^1∈D1prob(P^1)d(P^1,cP1)
The expected distance of P1 from cp1 is less than the expected distance of P1 from any other point c∈C. According to our assumption, there is a point Pj such that O′pj∈C and Pj is assigned to cP1 in the optimal solution, so we have

∑P^1∈D1prob(P^1)d(P^1,cP1)≤∑P1∈D1^prob(P^1)d(P^1,O′pj)≤∑P^1∈D1prob(P^1)(d(P^1,c∗p1)+d(c∗p1,O′pj))≤OPT+∑Pj^∈Djprob(Pj)(d(Pj^,c∗p1)+d(Pj^,O′pj))≤OPT+OPT+(1+α)OPT
So, we have

∑R∈Ωprob(R)maxi=1nd(Pi^,cpi)≤(5+α)OPT
Case 2: Suppose that there are two points Pu and Pv such that O′u and O′v are in C and both Pu and Pv are assigned to the same center (for example c∗2) in the optimal solution. Suppose that O′u is added to C before O′v and O′v is added in the ithe step. So, we have

EcostED(c1,…ck)≤EcostED(c1,…ci−1)=∑R∈Ωprob(R)maxj=1nd(Pj^,cPj)
Note that cPj∈{c1…ci−1} and cPu=O′u.

≤∑R∈Ωprob(R)maxj=1n(d(Pj^,c∗Pj)+d(c∗Pj,cPj))
Without loss of generality let d(c∗P1,cP1)=maxnj=1d(c∗Pj,cPj).

≤OPT+d(c∗p1,cp1)≤OPT+∑p1^∈D1prob(p1^)(d(p1^,c∗p1)+d(p1^,cp1))≤2OPT+∑p1^∈D1prob(p1^)d(p1^,cp1)
We know that ∑pv^∈Dvprob(pv^)d(pv^,cpv) has the maximum distance from {c1,…ci−1}, so

≤2OPT+∑pv^∈Dvprob(pv^)d(pv^,cpv)≤2OPT+∑pv^∈Dvprob(pv^)d(pv^,cpu)≤2OPT+∑pv^∈Dvprob(pv^)(d(pv^,c∗pv)+d(c∗pv,cpu))≤2OPT+OPT+d(c∗pv,cpu)≤3OPT+∑Pu^∈Duprob(Pu^)(d(c∗Pv,Pu^)+d(pu^,cPu))
Since Pu and Pv are assigned to the same center and cPu is a (1+α)-approximation solution for the 1-center of Pu,

≤4OPT+(1+α)OPT=(5+α)OPT.
□

By Theorem 3.1, a 2-approximation solution for the 1-center of each uncertain point can be computed in O(z2) time. In each iteration the expected distance of each uncertain point from the centers can be computed in O(kz). There are n uncertain points so each iteration takes O(nzk) and for k iterations the running time is O(nzk2). So the overall running time is O(nzk2)+O(nz2).

Another approach to the k-center problem
Here we define restricted center problem where the centers should be selected from the expected points of the uncertain points in Rd or the 1-center of the uncertain points in a general metric or the possible locations of the uncertain points, Pi,j’s, in a general metric. We denote this version of the problem by restricted centers.

Theorem 4.1
If the centers are restricted to be selected from the expected points of the uncertain points in Rd, then an optimal solution of unrestricted assigned k-center problem is a 2-approximation factor solution for the original unrestricted assigned k-center.

Proof
Let {c′1,…,c′k}⊆{P1¯,…,Pn¯} and assignment A′ be an optimal solution in the unrestricted k-center problem when the centers are restricted to be selected from the expected points of the centers. Suppose that {c∗1,…,c∗k}⊆Rd and assignment A are an optimal solution for the unrestricted k-center problem. Here, Pi is assigned to c′Pi and c∗Pi in the restricted centers and original unrestricted k-center problem, respectively. For each point c∗i let f(c∗i)∈{P1¯,…,Pn¯} be the closest point to c∗i. We have

EcostA′(c′1,…,c′k)=∑R∈Ωprob(R)maxi=1nd(Pi^,c′Pi)
Note that A′ and {c′1,c′2…,c′k} are the optimal solution for the case where the centers are selected from the expected points of Pi’s. So, if we choose {f(c∗1),f(c∗2),…,f(c∗k)} as new centers and for each Pi we assign it to f(c∗Pi), then the expected cost will be greater than EcostA′(c′1,…,c′k), so

≤∑R∈Ωprob(R)maxi=1nd(Pi^,f(c∗Pi))≤∑R∈Ωprob(R)maxi=1n(d(Pi^,c∗Pi)+d(c∗Pi,f(c∗Pi)))
Assume that d(c∗P1,f(c∗P1))=maxni=1d(c∗Pi,f(c∗Pi)).

≤OPT+d(c∗P1,f(c∗P1))
The closest point to c∗P1 is f(c∗P1)∈{P1¯,…,Pn¯}, so d(c∗P1,f(c∗P1)≤d(c∗P1,P1¯),

≤OPT+d(c∗P1,P1¯)≤OPT+∑P^1∈D1prob(P^1)d(P^1,c∗P1)≤2OPT
□

Theorem 4.2
In a general metric space, if the centers are restricted to be selected from the possible locations of Pi’s, Pi,j’s, then an optimal solution of unrestricted assigned k-center problem in this setting is a 2-approximation factor solution for the original unrestricted assigned k-center problem.

The proof is similar to the proof of Theorem 4.1.

Theorem 4.3
In a general metric space, if the centers are restricted to be selected from the possible locations of Pi’s, Pi,j’s, then an optimal solution of expected distance assignment in this setting is a 4-approximation factor solution for the original unrestricted assigned k-center problem.

Proof
Suppose that {c′1,c′2,…,c′k}⊆{P1,1,P1,2,…,Pn,z} is an optimal solution for expected distance assignment when the centers are restricted to be selected from Pi,j’s. Let c∗1,c∗2,…,c∗k and assignment A be an optimal solution for the unrestricted assigned k-center problem. For each point c∗i, let c′′i∈{P1,1,P1,2,…,Pn,z} be the closest point to c∗i. Since {c′1,c′2,…,c′k}⊆{P1,1,P1,2,…,Pn,z} is an optimal solution for expected distance assignment when the centers are restricted to be selected from Pi,j’s then we have

EcostED(c′1,…,c′k)≤EcostED(c′′1,…,c′′k)=∑R∈Ωprob(R)maxi=1nd(Pi^,c′′Pi)≤∑R∈Ωprob(R)maxi=1n(d(Pi^,c∗Pi)+d(c∗Pi,c′′Pi)).
Where Pi is assigned to c′′Pi in the expected distance assignment. Suppose that maxni=1d(c∗Pi,c′′Pi)=d(c∗P1,c′′P1).

≤OPT+d(c∗P1,c′′P1)≤OPT+∑P^1∈D1prob(P^1)(d(P^1,c∗P1)+d(P^1,c′′P1))
Suppose that the closest point to c∗P1 is c′′j. Since each point is assigned to the center that minimizes its expected distance from its center, so the expected distance of P1 from any center c′′i, is greater than the expected distance of P1 from c′′P1.

≤OPT+OPT+∑P^1∈D1prob(P^1)d(P^1,c′′j)≤2OPT+∑P^1∈D1prob(P^1)(d(P^1,c∗P1)+d(c∗P1,c′′j))≤3OPT+∑P^1∈D1prob(P^1)d(c∗P1,c′′j)
Since c′′j is the closest point to c∗P1, so for any P^1∈D1 we have d(c∗P1,c′′j)≤d(P^1,c∗P1).

≤3OPT+∑P^1∈D1prob(P^1)d(P^1,c∗P1)≤4OPT
□

Now, we show that for a given assignment A and a given set of centers {c1,…,ck} how to compute EcostA(c1,…,ck) in O(n2z2) time. Let prob(Pi,j=max) be the sum of the probabilities of all realizations R∈Ω such that Pi^=Pi,j and d(Pi,j,cPi)=maxnl=1d(Pl^,cPl).

Let Xl,m=1 if d(Pl,m,cPl)<d(Pi,j,cPi,j) and zero otherwise.

Xl=∑m=1zprob(Pl^=Pl,m)Xl,m.
The probability that d(Pl^,cPl)<d(Pi,j,cPi,j) is equal to Xl. Thus,

prob(Pi,j=max)=prob(Pi^=Pi,j)⊓nl=1,l≠iXl.
For each Pl we compute Xl which takes O(z) time. So, the overall running time for computing prob(Pi,j=max) is O(nz).

EcostA(c1,…,ck)=∑R∈Ωprob(R)maxi=1nd(Pi^,cPi)
which is equal to the following:

∑i=1n∑j=1zprob(Pi,j=max)d(Pi,j,cPi).
So, we can compute EcostA(c1,…,ck) in O(n2z2) time, which leads us to a 4-approximation algorithm for the unrestricted assigned k-center problem (See Algorithm 4.1) that computes the minimum of costs of all (nzk) subsets.

figure d
Corollary 4.1
In a general metric space, we can compute a 4-approximation solution for unrestricted assigned k-center problem in O((nzk)n2z2).

Corollary 4.2
In a general metric space, By Theorem 4.2, we can compute a 2-approximation solution for the uncertain 1-center problem in O(n3z3) time.

If we want to give a 2-approximation factor for unrestricted assigned k-center problem, by Theorem 4.2, for each k-subset as centers we have to find an optimal assignment A. So, we need to consider all the kn possible assignments. Each point can be assigned to any of the k centers which takes to much time.

Experimental results
We have implemented and run our algorithms on three real data sets. The first data set is related to a famous video game called Pokemon where there are some types of Pokemons that one should hit. In this data set, the locations of appearance of 140 Pokemon Go types are given which is available in the internet.Footnote1 Suppose that we want to play as a team of k people such that each of us is responsible of some types of Pokemons. To compute our locations and the types of Pokemons that each of us should hit, so that we can hit the Pokemons as quickly as possible, and we can use our algorithms to compute the k-centers as our locations to minimize the expected cost to reach faster to the targets.

Table 2 Experimental results of Algorithm 3.1 and Algorithm 4.1 for unrestricted k-center problem for various values of n, k and z for the Pokemon data set
Full size table
Assume that there is a bounding box that contains all the locations. We have partitioned the bounding box into a z=q∗q grid. We have enumerated the Pokemons by their types. For each Pokemon i, in each cell j of the grid we count the number of times that Pokemon i has appeared. The probability of appearance of Pokemon i in cell j, pi,j, is equal to this number divided by the total number of its appearances. We combine all the locations of the Pokemon i in cell j by their average (i.e., taking the average of their x-coordinates and the average of their y-coordinates) and call it Pi,j, so each random point has at most z locations. In this way, we have 140 probabilistic points with the probability of their appearance in each cell. We have solved the unassigned k-center problem using our proposed algorithms. The results are shown in Table 2.

The second data set is related to 192M taxi trips in Chicago available in the internetFootnote2. Each row of the data is the information of one trip which consists of 23 data about that trip. The number of taxis is 7000. Suppose that we want to build k stations such that each taxi is assigned to one of them. The goal is to minimize the expected cost of traveling distance of taxis to their assigned stations. We model the problem as follows. For each trip we extract the taxi ID, pick-up location and drop-off location. Each taxi is considered as a probabilistic point. The same as previous data set we partition the city into a z=q∗q grid. Then in each cell j of grid, for each taxi i we enumerate the number of pickup and drop-off locations of that taxi, ni,j. So for each taxi i, the probability that is assigned to it in cell j is ni,j∑zj′=1ni,j′. The results of our algorithms are shown in Table 3. Note that for each n, we choose the first n taxis from the data sheet and compute all the trips of that taxi among the 192M trips.

Table 3 Experimental results of Algorithm 3.1 and Algorithm 4.1 for unrestricted k-center problem for various values of n, k and z for Chicago taxi trips
Full size table
Also in Table 4, we can see the results of algorithm 3.1 and the proposed algorithm in [1] for Chicago taxi trip data set and compare the results. Due to computational restriction, we did not compute the exact solutions.

Table 4 Experimental results of Algorithm 3.1 and [1] for unrestricted k-center problem for Chicago taxi trips
Full size table
The third data set is related to Salt Lake city police call reports available in the internetFootnote3. The data is related to the crime cases which are reported by phone to the police. We have extracted the location and description of each call from the data set. The call description includes cases such as traffic stop, burglary, loud party/music, suicide, vehicle theft, alarm information and so on.

Now suppose that we want to establish k police stations or determine the location of k police cars in various locations of the city such that the police can reach the cases as soon as possible. We can consider a scenario where each police station or police car is responsible of some special type of cases. For example for the suicide case we might need police with ambulance or for the case of burglary the police might need special equipments. Or even some polices have abilities that match some special cases. So in this scenario we can consider each type of case as a probabilistic point. The same as previous two data sets we assign the probability of each type crime. This can be a reasonable partitioning since in practice the type of crime is related to the location. For example in some parts the city vehicle theft is more common or in some other parts loud party is more common. In Table 5, we have the results of running Algorithm 3.1 and also the algorithm of [1] for this data set.

Table 5 Experimental results of Algorithm 3.1 and [1] for unrestricted k-center problem for Salt Lake city police call reports
Full size table
Due to the computational limitations, we were not able to compute the exact optimal costs of these data sets. So, it was not possible to compare our results with the optimal costs, but we were able to compare the results of Algorithms 3.1 and Algorithm 4.1 in Tables 2 and 3 for the first and second data sets. As we can see from the results, the cost of Algorithm 3.1 is similar to the cost of Algorithm 4.1 and even in some cases is better than the cost of Algorithm 4.1. We were not able to run Algorithm 4.1 on large values of n, z and k in our system. Since Algorithm 3.1 is really fast, we can use it in practice for large values even in a desktop computer.

In Tables 4 and 5 we compared the results of [1] with our proposed algorithm for Chicago taxi trip data set and also Salt Lake city police call report data set . As it can seen the running time of our algorithm is slower than [1] (this was proved theoretically) but the cost of our algorithm in all the cases is better than [1]. This shows that the approximation factor of our algorithm is better than [1] also in practice.

The programming language for Algorithm 3.1 and Algorithm of [1] was Python and Algorithm 4.1 was Openmp. We used two Intel(R) Xeon(R) CPU E5-2630L v3 @ 1.80GHz CPUs and 64GB DDR4 2133 MHZ ECC.REG RAM for running the algorithms in Tables 2 and 3 and two Intel(R) Xeon(R) CPU E5-2630L v3 @ 1.80GHz CPUs and 64GB DDR4 2133 MHZ ECC.REG RAM for running the algorithms in Tables 4 and 5.

Assigned uncertain k-median
In this section we present some theoretical results for the k-median problem.

First we present our algorithm for k-median problem which is similar to the algorithms presented in [1] for uncertain k-center problem and the algorithm presented in [26] for uncertain k-median problem.

figure e
Note that the algorithm is similar to the algorithm presented in [26]. The difference is that in the first step they compute a near optimal 1-median for each uncertain point and then they compute the k-medians of these near optimal 1-medians. But in our algorithm we compute the expected point of each uncertain point and then we compute the k-medians of these expected points. The expected point of each uncertain point has a property (See Lemma 2.1) that helps us to improve the approximation factor from 2α+1 to α+2 (where α is the approximation factor of the algorithm for k-median clustering).

Now we compute the approximation factor of the proposed algorithm.

Theorem 6.1
Given an algorithm for k-median clustering with approximation factor α we can find a (2+α)-approximation for the uncertain k-median problem in Rd.

Proof
Let c∗1,…,c∗k be an optimal solution. So, we have

OPT=EcostED(c∗1,…,c∗k)=∑R∈Ωprob(R)∑i=1nd(Pi^,c∗Pi)
where Pi is assigned to c∗Pi∈{c∗1,…,c∗k} under the expected distance assignment. Now let c1,…,ck be an α-approximation of k-medians of {P1¯,…,Pn¯}. Suppose that the uncertain point Pi is assigned to cPi∈{c1,…,ck} in the expected distance assignment and the certain point Pi¯ is assigned to cPi¯∈{c1,…,ck} when we compute the cost of k-median for P1¯,…Pn¯.

EcostED(c1,…,ck)=∑R∈Ωprob(R)∑i=1nd(Pi^,cPi)=∑i=1n∑Pi^∈Dprob(Pi^)d(Pi^,cPi)≤∑i=1n∑Pi^∈Dprob(Pi^)d(Pi^,cPi¯)≤∑i=1n∑Pi^∈Dprob(Pi^)(d(Pi^,Pi¯)+d(Pi¯,cPi¯))=∑i=1n∑Pi^∈Dprob(Pi^)d(Pi^,Pi¯)+∑i=1nd(Pi¯,cPi¯)≤
Note that ∑ni=1∑Pi^∈Dprob(Pi^)d(Pi^,Pi¯)≤2OPT because by Lemma 2.1 we have

∑i=1n∑Pi^∈Dprob(Pi^)d(Pi^,Pi¯)≤∑i=1n∑Pi^∈Dprob(Pi^)(d(Pi^,c∗Pi)+d(c∗Pi,Pi¯))
On the other hand by Lemma 2.1,

d(Pi¯,cP∗i)≤∑Pi^∈Dprob(Pi^)d(Pi^,cP∗i).
And also since c1,…,ck are an α approximation for the k-medians of cPi¯∈{c1,…,ck}, so we have

∑i=1nd(Pi¯,cPi¯)≤α∑i=1nd(Pi¯,cP∗i)≤α∑i=1n∑Pi^∈Dprob(Pi^)(d(Pi^,c∗Pi)).
So,

EcostED(c1,…,ck)≤(2+α)OPT.
□

Conclusion
In this paper we studied the k-center problem for uncertain data points in a general metric space. We gave improved constant approximation factor algorithms for both restricted and unrestricted assigned versions of the problem. We restricted the centers to be chosen from some certain points and show that an optimal solution with this restriction gives a 2-approximation solution for the general problem. This led us to give an improved approximation factor algorithms for the unrestricted assigned k-center problem. As a future work, this idea can help us to propose PTAS for the problem. Next, we presented some experimental results on real data sets. As far as we know this is the first time that experimental results are given for this problem. We also study the assigned version of uncertain k-median problem in Euclidean space. The proposed algorithms are fast and easy to implement and can run on large data sets even in a desktop computer.