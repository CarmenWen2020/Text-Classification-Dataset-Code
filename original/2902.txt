Querying a search engine is one of the most frequent activities performed by Internet users. As queries are submitted, the server collects and aggregates them to build detailed user profiles. While user profiles are used to offer personalized search services, they may also be employed in behavioral targeting or, even worse, be transferred to third parties. Proactive protection of users' privacy in front of search engines has been tackled by submitting fake queries that aim at distorting the users' real profile. However, most approaches submit either random queries (which do not allow controlling the profile distortion) or queries constructed by following deterministic algorithms (which may be detected by aware search engines). In this paper, we propose a semantically grounded method to generate fake queries that (i) is driven by the privacy requirements of the user, (ii) submits the least number of fake queries needed to fulfill the requirements and (iii) creates queries in a non-deterministic way. Unlike related works, we accurately analyze and exploit the semantics underlying to user queries and their influence in the resulting profile. As a result, our approach offers more control—because users can tailor how their profile should be protected—and greater efficiency—because the desired protection is achieved with fewer fake queries. The experimental results on real query logs illustrate the benefits of our approach.

Access provided by University of Auckland Library

Introduction
The users of online services may expose their (private) personal data in several ways, either explicitly—e.g., by posting their interests or opinions in social networks—or implicitly—e.g., by installing and using software that collects usage data and telemetry from the user’s device. Leakage of private data may be avoided, or at least limited, by refraining from publishing personal information online or by employing applications that are respectful with the privacy of their users. However, users can hardly avoid the privacy leakage that occurs when they use online search engines.

Querying a search engine is one of the most frequent activities performed by Internet users. Most users employ web search engines (WSE) such as Google as the entry point for navigating the Web. Each time the users submit a query, the server learns their interests, preferences or even demography, such as location, sex or approximate age. Queries can often be associated with user identities via login into services or by performing “vanity queries” (that is, queries that include the name of the user). Moreover, user queries can be aggregated through time and across different services in order to build detailed user profiles by matching the user’s IP or by using cookies deployed by the search engine [1]. Even privacy-enhancing measures such as disabling cookies or hiding IPs can be overcome by exploiting browser’s or device’s fingerprints [2]. On the positive side, user profiles can be employed by the search engines to offer a better service—e.g., to provide personalized search [3], implement query disambiguation [4], or offer query suggestions or expansion [5, 6]. However, in many occasions, detailed user profiles are employed in Behavioral Targeting, which aims at increasing the effectiveness of advertisements [7], or, even worse, profiles are sold to marketing companies and third parties, which may use them for discriminatory purposes—e.g., in health insurance or credit applications [8].

One solution would be to resort on privacy-preserving search engines (such as DuckDuckGo, Searx or Qwant). However, these systems expect the user to trust the search engine provider to be respectful with the privacy of the users. On the other hand, these search engines are still a minority and “big” players such as Google are so integrated into existing ecosystems (such as Android smartphones) that it is difficult to avoid their use.

An alternative is to empower users in the protection of their privacy w.r.t. to any search provider. This has been traditionally tackled in the literature by employing fake queries [9]. The idea is to create and submit fake queries to the search engine on behalf of the user together with the user’s real queries; because it is not possible to distinguish between real and fake queries, the legitimate interests of the user would not be unequivocally identified. Moreover, if the fake queries refer to interests different to the user’s, they will contribute to distort the user profile built by the search engine. According to the criteria employed to create the fake queries, the resulting profile may still be useful for personalized search services [1] but will prevent from building unequivocal profiles with detailed users’ interests. Even though the most naïve approaches pick fake queries randomly (thereby resulting in random profiles at the server side that are of no use for delivering personalized services), more sophisticated solutions aim at balancing the trade-off between quality of service and privacy. However, while query semantics is the key to building user profiles [1], the solutions proposed so far (discussed in Sect. 2) have neglected or poorly considered the semantics underlying to the user queries when constructing fake queries and distorting the user profile. Also, current solutions offer poor control on which and how many fake queries are actually submitted and on how the user profile is distorted as a result of them.

Contributions and plan of this paper
In this work, we accurately analyze and exploit the semantics of the user queries in order to offer more control on the creation of fake queries and to achieve a more efficient distortion of the user profile built by the search engine. Regarding control, our solution allows users to specify the kind of privacy protection/utility preservation they desire. This goes from a slight protection that will contribute to hide their detailed interests but that will not significantly distort their profiles (thereby maintaining the accuracy of the personalized web services), to a full protection in which the users will apparently have no dominant interests (and therefore no personalization will be possible). We also offer the possibility of distorting profiles toward specific fake interests, in case the users want to make the search engine believe they have a plausible profile—i.e., with dominant interests, but privacy-preserving. Regarding efficiency, our approach is able to achieve the desired degree of protection while submitting the least number of fake queries, which is relevant in scenarios with limited bandwidth. Queries are also created in a non-deterministic way via semantically consistent randomization, which makes them more difficult to detect by aware search engines. These benefits are possible because, unlike most related works, we consider query semantics and their influence in the resulting profile through the whole life cycle—i.e., from the analysis of the users’ real queries to the creation of the fake queries that better contribute to fulfilling the users’ privacy requirements.

The rest of the paper is organized as follows. Section 2 discusses related works on privacy-preserving web search. Section 3 presents our method in detail. Section 4 reports empirical results and compares the performance of our method against related works. The final section presents the conclusions and proposes several lines of future research.

Related work
The use of fake queries to preserve the privacy of web search engine’s users has been a common practice in the literature and it has resulted in proposals that use a variety of strategies to generate them. In general, works can be classified into two main groups depending on the approach being followed [1, 10]: multi-party protocols and single-party protocols.

In a multi-party protocol, a group of users is created; next, a member of this group willing to submit a query asks another group member to submit her query on her behalf to the web search engine; finally, the user who has submitted the query (and for which the query is a fake one, since it is not hers) sends back the query results to the initiator. Within this general approach, the different strategies found in the literature deal with the way in which the users are effectively grouped. There are solutions that use static groups, where the same members participate in every execution of the protocol [11]. Other approaches use dynamic groups, where a user is grouped with different members every time she runs the protocol [12,13,14,15]. Finally, in some schemes the users only contact a trusted central node that acts as a proxy and is in charge of simulating a group of users by means of their past queries [16].

Multi-party protocols bring a relevant advantage: The fake queries the users submit are human-generated and, hence, they are assumed to be semantically and grammatically correct in most cases. However, due to their distributed nature, multi-party protocols suffer from slow response time and system availability. Both issues are caused by the fact that they require the collaboration of external and remote entities that must be online and available at any time. Moreover, while users rely on other users to protect their data w.r.t. the search engine, they also compromise their privacy w.r.t. to the other members of the group; this could make these solutions unsafe if attackers are capable of sneaking into the formed groups [17].

Single-party protocols work in the computer of the user and do not require interactions with external parties. Instead, they build and submit machine-generated fake queries in a stand-alone way. This approach provides two important advantages over multi-party protocols: (i) It is suited to provide very fast response times, and (ii) users may get control over the contents of the queries they are submitting to the WSE.

The first and most renowned single-party schemes in the literature are TrackMeNot [9] and GooPIR [18]. Both systems are based on submitting random queries to the WSE. Specifically, TrackMeNot periodically submits fake queries that are randomly gathered from blog entries and news headlines, while GooPIR submits a unique query that contains legitimate terms together with fake ones randomly obtained from a thesaurus. In both cases, fake queries are generated in a random way that, in the long term, will tend to generate a random (and therefore useless) profile at the search engine. However, if the user’s real queries are biased toward a specific topic, the uniform random queries may be ineffective in hiding this interest. Moreover, the quality of the fake queries generated by those two solutions are so low that they can be easily detected by an aware WSE. Specifically, in [19] it is shown that it is possible to distinguish real queries from fake queries generated by TrackMeNot with a mean of misclassification around 0.02%.

In contrast to TrackMeNot and GooPIR, other single-party solutions have tried to preserve the profile usefulness while protecting the users’ privacy by considering the semantics underlying to the submitted queries. In [20], the authors use a proxy at the user side that submits the legitimate query together with a generalized user profile that may enable the WSE to tailor personalized search services while ensuring privacy. This proposal is not practical because it requires the collaboration of the web search engine in order to understand and use the generalized user profile being transmitted. More realistic approaches are presented in [21,22,23,24]. These four proposals generate fake queries by retrieving random terms that are semantically distant to the user’s real queries from knowledge bases like WordNet or the Open Directory Project (ODP). They mainly differ in the criteria used to select the category to whom the contents of the fake query should belong. Specifically, [24] proposes a very straightforward approach in which user queries are generalized using WordNet, and generalizations are retrieved according to a fixed depth percentage. Shapira et al. [21] build fake queries by retrieving terms related to the past user queries and a local database related to the user's interests, whereas [22, 23] only consider the past legitimate queries. In all cases, fake queries are selected from the categories in the knowledge bases being at a certain semantic distance from the legitimate queries. The criteria employed by these approaches are, however, quite deterministic, because each fake query is created as a function of each real query according to a predefined to a fixed criterion or a semantic distance set by the user. Moreover, setting this distance can be unintuitive, since users will only have a vague idea on how the resulting fake queries may distort their real profiles, which is the ultimate goal when it comes to protecting the users’ privacy.

The work presented in [25] also leverages a knowledge base (Wikipedia) to build fake queries, but the authors focus on the indistinguishability of the fake queries, rather than their meaning. For this, they create fake queries with feature distributions—such as specificity, synonymy and polysemy—close to those of the legitimate queries. With these fake queries the authors aim at reducing the significance of user query topics, but there is neither certainty nor control on how the distorted profile will result.

A different way of gathering the users’ real interests, while focusing on preserving the profile usefulness, is presented in [1]. The authors consider that the interests reflected by the users in their social networks may be more accurate and stable than those that can be inferred from past search queries, which may be more circumstantial. They therefore employ the users’ publications in social networks to derive semantically distant queries that may contribute to hide the real profiles. This is the only proposal considering the profile built by the search engine, and creating fake queries that make that profile converge to the users’ interests in their social networks. Even though this approach may bring some privacy due to the queries created by the system being artificial, as it has been already mentioned, it is more focused on retaining the utility of the profile for deploying personalized search services.

Our proposal
In this section, we present our solution to protect the user profile that search engines may learn from submitted queries. On the contrary to the related works depicted above, our approach empowers users to take their own decisions about how their search profiles should be protected. Specifically, the users may select which profile they want the search engine to learn from the submitted queries, which can go from being similar to their real profiles (therefore being still useful for deploying personalized search services), to being completely different or uniform (therefore offering maximum privacy). To distort the real profiles according to users’ privacy requirements, our solution relies on what we call semantic randomization, a mechanism capable of creating the minimum number of fake queries needed in a semantically consistent and non-deterministic manner.

Figure 1 depicts an overview of the proposed solution. Each time the users create and submit a query to the search engine, the semantic randomizer creates in the background a set of fake queries with the aim of diverting the users’ search profile toward the interest category the users desire the search engine learn from them, i.e., to a given target profile. On the contrary to the related works discussed in Sect. 2 that systematically create a number of fake queries from the real ones, our system decides the number of fake queries to yield at each iteration according to how similar the user current profile is with respect to the target profile; in this way, we minimize the overhead caused by submitting fake queries and, also, make their number and submission pattern non-deterministic.

Fig. 1
figure 1
Overview of the proposed solution

Full size image
The target profile is set by the users and captures their privacy requirements. Generic target profiles lead to scattered user profiles with non-dominant topics of interest, thus offering maximum privacy since the search engine will not be able to associate the user profile with any specific interest. As an example, if a user with a real profile focused on heart diseases sets the target to the abstract category thing, the semantic randomizer will make the user profile converge on that target by generating fake queries about widely dispersed topics of interest. The dispersion of the user’s fake behavior contributes to maximize the user’s privacy against automatic profiling because no dominant interests are exposed but, as a consequence, personalization of web services will hardly be possible. On the other hand, specific target profiles can also contribute to hide the user’s real profile. In this case, the more divergent from the user's real profile the specific target is, the more protected the user profile will be. Following the above example, if the target is set to the category photography, which is a specific topic that is quite different from the interest category that represents the real profile (heart diseases), the semantic randomizer will make the user profile converge that target by generating fake queries on topics predominantly associated with photography. In this way, we achieve privacy (because the interest category of the real user profile is not dominant anymore) while biasing personalized services toward the desired target category, which may still be useful for the user. A somewhat lighter protection could also be possible by setting the target to a topic close to the real profile, e.g., arterial pressure. This will also contribute to hide the user’s detailed interests but maintaining the accuracy of the personalized web services.

Setting the target profile for the well-defined scenarios described above can also be automatized. In this way, the user can be provided with a predefined set of possible target profiles ranging from a fixed set of generic ones with very broad categories (which would not depend on the user’s real interests), to detailed but distant/similar targets (which would be calculated from the user’s real interests).

Because the users’ search queries are of nominal nature, the semantic randomizer should capture their semantics in order to yield semantically consistent fake queries. In this way, we aim to overcome the limitations of non-semantic approaches [9, 18], whose random queries can be easily detected by aware search engines [19]. We exploit ontologies to capture and manage the semantics underlying to (nominal) queries. Ontologies are rigorous and exhaustive organizations of knowledge domains, which model concepts and their interrelations [26]. In our solution, we use ontologies to map the user’s queries with conceptual abstractions, which allow us to analyze and properly manage the semantics underlying to them during the generation of fake queries.

In the following, we first detail how we capture and manage the semantics underlying nominal data by exploiting the formal semantics modeled in an ontology and how we compare nominal values through the notion of semantic distance. Afterward, we describe the algorithm of the semantic randomizer.

Semantic management of queries
To manage queries from a semantic perspective, we first need to map them with their conceptual abstractions. For this, we first apply a syntactic analysis on each query in order to extract its semantics units [22], which are pieces of text (nouns or noun phrases) that refer to unique concepts. This consists of a pipeline of analyses performing sentence detection, tokenization, part-of-speech tagging and chunking. For example, the query “new documentary about extreme water sports” will result in two semantic units: “new documentary” and “extreme water sports”. Afterward, we map each semantic unit to the concept it refers to in an ontology by terminologically matching their labels. In this process, stops words are removed and words are stemmed to remove derivational affixes, such as plurals (e.g., “extreme water sports” → “extreme water sport”). If a semantic unit (e.g., “extreme water sport”) cannot be directly matched with a concept in the ontology, we try simpler forms of the noun phrase by progressively removing adjectives and nouns starting from the one most on the left (e.g., “extreme water sport” → “water sport”). The simpler the noun phrase, the more likely it is to match it with a concept in the ontology, while maintaining the core semantics conveyed by the noun most on the right. Notice that if a query contains several semantic units, it may be mapped to several concepts in the ontology (e.g., “documentary” and “water sport”, that is, one concept for each semantic unit in the previous example).

Many of the operations carried out by the semantic randomizer require comparing two nominal values, for example, for assessing how far the fake queries must be from the profile queries according to the magnitudes of the components of an error sequence randomly drawn from a given distribution. Thanks to the conceptual mapping described above, this comparison can be done in the semantic domain by evaluating the semantic distance of the concepts the queries refer to. Semantic distance, sd: ci × cj → ℝ, is a function mapping a pair of concepts to a real number that quantifies the differences between their meanings according to the semantic evidences gathered from one or several knowledge sources [27]. A suitable semantic distance to be applied in semantic randomization should (i) be computationally efficient, due to the number of distance calculations that are needed during the semantic randomization process, (ii) provide values normalized in the range [0..1], where 0 represents the minimum distance, i.e., both concepts are the same, and 1 represents the maximum distance, and (iii) perform the calculation of the semantic distance consistently with the distribution of the random error sequence. For random error sequences that are normally distributed, sd(.,.) should perform a neither logarithmic nor exponential calculation so that distances are well spread in the range [0..1]. In this way, it would be more likely to find fake queries that are as semantically distant from the user profile as defined by the error magnitude. According to the discussion above, we propose to use the well-known semantic similarity measure proposed by Wu and Palmer simwp(ci,cj) [28] because it fulfills the above requirements.

simwp(ci,cj)=2×nodes(root,LCS(ci,cj))2×nodes(root,LCS(ci,cj))+links(ci,LCS(ci,cj))+links(cj,LCS(ci,cj))
where LCS(ci,cj) is the most specific concept in taxonomy subsuming both ci and cj; nodes(root, LCS(ci,cj)) is the number of nodes in the longest taxonomic path between the node LCS(ci,cj) and the node root of the taxonomy, including both LCS(ci,cj) and root; links(ci, LCS(ci,cj)) is the number of links in the shortest taxonomic path between ci and LCS(ci,cj), similarly for links(cj, LCS(ci,cj)).

It should be noted that if concepts are mapped into different ontologies, the semantic similarity calculation above can be adjusted—as proposed in [29, 30]—to take into account the different sizes and granularities of each ontology so that concept similarities can be fairly compared.

Distorting user profiles via semantic randomization
The semantic randomizer is deployed as a web browser plugin and it is iteratively executed each time the user submits a query to the search engine. At each iteration, a new set of fake queries is generated in the background, which is later submitted to the search engine. The actions carried out at each iteration are divided into two phases, as introduced in Fig. 1 and detailed in below subsections.

First phase
On the contrary to related works, our approach does not generate a fixed number of fake queries for each real one (a pattern that could be easily detected by the search engine). Instead, at each iteration, the semantic randomizer calculates the number of fake queries required to approximate the current user profile, that is, the profile that the search engine may build from queries submitted from the user’s device, to the category defined by the user as the target profile. The number of fake queries is calculated in a first phase according to the divergence between the current and target profiles. In this way, the algorithm would generate more queries when the different is large (therefore speeding up the convergence toward the target profile) and less queries when the difference is small (therefore minimizing the number of fake queries). The algorithm is also capable of detecting when the current profile has converged to the target, therefore avoiding submitting unnecessary fake queries that may waste bandwidth.

We define the current user profile as follows.

Definition 1
The current user profile at iteration t, denoted by P, is defined as the accumulated set of search queries submitted from the user’s device until iteration t. This consists of the history of each of individual real queries qreal submitted by the user, denoted by {qreal1,…,qrealt−1,qrealt}, plus all sets of fake queries Qfake yielded by the user’s semantic randomizer in previous iterations, denoted by {Qfake1,…,Qfaket−1}.

P={qreal1,…,qrealt−1,qrealt,Qfake1,…,Qfaket−1}
The current user profile is intended to keep in the semantic randomizer an updated record of the query history that the search engine may have built of the user, thus containing all queries sent from the user’s device, both real and fake. This profile will be used by the semantic randomizer to estimate the representative topic of interest that the search engine may derive of the user’s profile.

The categorized version of P, denoted by CP, is obtained by mapping queries to ontological concepts as detailed in Sect. 3.1:

CP={Creal1,…,Crealt−1,Crealt,Cfake1,…,Cfaket−1}
where Creali and Cfakei are sets of concepts in the ontology O that categorize the queries qreali and Qfakei, respectively. Note that, as introduced in Sect. 3.1., a query qreal may be mapped to several concepts ci if the query contains several semantic units, such that Creal=∪ci. All concepts ci are individually considered by the semantic randomizer and, thus, added to CP as if each one came from a different query. The history of queries (or categories) that are considered in P (or CP) can be limited by setting a window size that omits queries prior to a certain iteration.

Then, we calculate the dominant category of the profile similarly to how clustering algorithms derive representatives of clusters. Specifically, clustering algorithms like k-means calculate cluster representatives as the most central element or centroid, which is the element that minimizes the aggregated distances to all the elements in the cluster [31]. Using this notion in our setting, we calculate the centroid of the profile as follows:

Definition 2
The centroid of CP, denoted by cid(CP), is defined as the concept c from the ontology O that minimizes the sum of the semantic distances sd(.,.) (calculated as detailed in Sect. 3.1) with respect to all concepts ci in CP.

cid(Cp)=argminc∈O⎛⎝∑ci∈Cpsd(c,ci)⎞⎠
In contrast with the usual notion of centroid, with our definition, any concept in O can be the centroid of the categorized profile, regardless of whether it was present in CP or not. In this way, we expand the set of centroid candidates to, e.g., generalizations of concepts in the profile, which may constitute more accurate representatives. When more than one candidate minimizes the distance, this means that all of them are equally representative, and any of them can be selected as the centroid of the profile. Notice that our notion of centroid captures the semantics of the profile more accurately than non-semantic averages, such as the mode. The centroid of CP computed by the semantic randomizer is an estimate of the representative topic of interest that the search engine may derive of the user’s profile, which will allow the randomizer to generate the appropriate fake queries to make the current user profile converge toward the target profile.

Consistently with Definition 2, the target profile is defined as the category from the ontology O the user selects to act as the centroid of the profile that she desires the search engine learns from her. This target defines the privacy requirement of the user (as introduced in Sect. 3) and, being a linguistic category, it is much more intuitive to set than the abstract numerical parameters employed by related works [22, 23]. Notice that, even though our method also uses numerical parameters (η, γ and α, as shown in Fig. 1), those control low-level aspects of our algorithm and are meant for advanced users. For regular users, it suffices to set them to default values and just select the desired target profile.

Then, we measure the degree of convergence (or, inversely, the convergence error) between the current and target profiles from a semantic perspective as follows.

Definition 3
The semantic convergence error, denoted by SCE, is calculated as the semantic distance sd(.,.) between the centroid of CP, cid(CP), and the target profile, target, defined by the user.

SCE=sd(cid(Cp),target),0≤SCE≤1
where target is the category in the ontology O that represents the desired target profile. Because SCE is a semantic distance, its value ranges between 0 and 1, where 0 represents the maximum convergence—i.e., cid(CP) = target—and 1 represents maximum divergence.

Because perfect convergence between profiles can be very hard to achieve (and may require a large number of fake queries to compensate very small differences), convergence is deemed to be achieved as long as the convergence error is below or equal a given small tolerance value γ (0 < γ < 1); that is, SCE ≤ γ.

The semantic randomizer calculates the number of fake queries that must be generated at iteration t according to the SCE. Specifically, if the SCE is less than or equal to γ, no fake queries will be generated because it is considered that the user profile has reached the target profile. Otherwise, the number of fake queries to be generated, denoted by n, is computed as a function proportional to the SCE and a parameter η that limits the maximum number of fake queries the method is allowed to generate at each iteration (this may be used to limit bandwidth usage). The result is mapped to the least succeeding integer through a ceiling function ⌈⋅⌉, as follows

n={⌈η(SCE−γ)1−γ⌉,0,SCE>γSCE≤γ},0≤n≤η
The sequence of steps carried out in the first phase of the semantic randomizer is shown in Fig. 2.

Fig. 2
figure 2
Steps of the first phase of the semantic randomizer at iteration t

Full size image
Second phase
In this phase, the semantic randomizer creates the n fake queries calculated in the previous phase. Semantically grounded related works [22, 23] generate fake queries by retrieving categories from an ontology that are at a certain semantic distance from the user’s query categories. In those approaches, the distance value is set by the user and captures her privacy requirements: A small (large) distance results in similar (distant) fake queries that produce a small (large) distortion in the user profile. Given that this distance value is a fixed parameter, fake queries will follow a deterministic pattern because real queries are systematically followed by fake queries at a fixed distance, which can be exploited by the aware search engines to filter out fake queries. Unlike those proposals, our approach does not follow a deterministic pattern because fake queries are built at each iteration according to the actual divergence between the current and target profiles. Moreover, we introduce randomization in the selection of the categories of the fake queries from the ontology via what we call semantic randomization. In a nutshell, semantic randomization consists of controllable random distortion around a reference concept of the ontology that acts as a distortion “epicenter”. With this, we are capable of selecting a set of random categories from the ontology with the following property: The centroid of such set of categories tends to the distortion epicenter. Thus, if the distortion epicenter is established toward the convergence of the target profile, we can generate a sequence of (apparently random) n fake queries that, in aggregate, will achieve the desired convergence.

The procedure works as follows. First, to minimize the convergence error, we need to determine the optimal distortion epicenter in the ontology so that the categories derived from it bring the user profile as close as possible to the target. In line with Definition 3, the convergence error would be minimized if the centroid of the user profile after submitting the fake queries generated at iteration t (which we denote cid’) minimizes the semantic distance of the profile and the target. To obtain the optimal cid’ we need to look for the concept c in the ontology that, when used as distortion epicenter for generating fake queries, will make cid’ to minimize sd(cid’, target). We calculate cid’ as the centroid between cid(CP) and each distortion epicenter candidate c; that is, we consider cid’ as the center between the mass of categories of the current profile CP and the mass of categories of the fake queries that will be generated at this iteration if c is chosen as distortion epicenter. The optimal distortion epicenter, which we denote co, will be the concept c that makes cid’ to minimize sd(cid’, target), as follows:

co=argminc∈O{sd(cid′,target)|cid′=cid({cid(Cp),c})}
The procedure followed to choose co is graphically depicted in Fig. 3a, b.

Fig. 3
figure 3
Generating fake categories through semantic randomization. a Profile at the beginning of the iteration t, categories of CP in light grey. b Obtaining optimal distortion epicenter co. c Deriving fake categories from co. d Profile after generating the fake queries

Full size image
Once co is obtained, the semantic randomizer yields a set of random fake categories Cfake=(cfake1,…,cfakei,…,cfaken) around co with as many categories as number of fake queries (n) to generate. In this way, instead of choosing n fake categories of “type” co, which would minimize the convergence error but will follow a deterministic pattern, we randomize the construction of Cfake while achieving similar convergence. Figures 3c, d illustrate the process of generating Cfake through semantic randomization.

Numerically, semantic randomization is based on generating an error sequence ε=(ϵ1,…,ϵi,…,ϵn)∼N(0,α) randomly drawn from a normal distribution with mean zero and variance α, such that the fake categories are derived by adding the error components єi to co

Cfake=(co)n+ε
(co)n being a vector of n categories co.

Because the mean of the random error sequence is equal to 0, the centroid (or semantic mean) of the set of fake categories cid(Cfake) will match co, as shown below

cid(Cfake)=cid((co)n)+μ(ε)=co+0=co
Due to the random character of the error sequence, even if the semantic randomizer is initialized again with the same co, this will produce different sequences of fake categories. The parameter α determines the amount of error to be added and, thus, the dispersion of these fake categories. The greater the α, the greater the variance of the random error sequence and, thus, the dispersion degree of the fake categories. Notice that the centroid of Cfake is maintained for any value of α, which is our goal to fulfill with the desired convergence.

To add the error values єi to co and, thus, obtain the set of fake categories, it is necessary to interpret the error magnitudes and their sign in the semantic domain. The magnitudes of the error components can intuitively be perceived as distances that designate how semantically far the fake categories should be from co. On this basis, the fake categories are those concepts in the ontology that are as semantically distant from co as defined by the error magnitude, i.e., sd(co, cfakei) =|ϵi|. Because the semantic domain is discrete, if there is no concept at the exact required distance, we select the concept that exceeds and best approximates the error magnitude, such that

cfakei=argminc∈O{sd(c,co)|sd(c,co)≥|ϵi|}
Due to the need to discretize error values in the semantic domain, the accuracy of cid(Cfake) = co will be limited by the size and granularity of the underlying ontology.

Finally, we propose to use the error sign to balance derivations of fake categories w.r.t. co. In particular, if the error is positive, the fake category cfakei will be the descendant category of co that best approximates the error magnitude. Instead, if the error is negative, the fake category will be the descendant category of a co-hyponym of co that best approximates the error magnitude. Because the accumulated mass of positive and negative errors in the normal random error sequence tends to be equivalent, this strategy ensures that cid(Cfake) tends to match co.

Once the fake categories have been selected from O, these are appended to the categorized profile CP, so that it gets updated for the next iteration. A straightforward way to generate the queries to be submitted from the n fake categories consists of employing the category labels. However, it is advisable to use a more sophisticated approach to derive queries from categories, such as the one described in [23], which creates syntactically consistent n-grams from a given category label. Also, our method is not linked to any particular query submission protocol. In fact, fake queries should not be submitted as soon as they are created, because this will result in a repetitive pattern that will be easy to detect. A submission protocol that follows human querying behavior will be desirable [9].

Figure 4 depicts and overview of the sequence of steps carried out in the second phase of our approach.

Fig. 4
figure 4
Steps of the second phase of the semantic randomizer at iteration t

Full size image
In order to determine the computational cost of the semantic randomizer, we analyze below the most costly operation in each phase. In the first phase, this operation is the calculation of the centroid of the profile, cid(CP), with O(n × m), n and m being the cardinality of the profile and the ontology, respectively. In the second phase, the most costly operation is the calculation of the optimal distortion epicenter, co, since it is necessary to calculate m times cid' (an operation O(2 m) that computes the centroid between cid(CP) and a given candidate epicenter), the cost of co being therefore equal to O(m2). Nevertheless, the complexity of cid and cid’ may be minimized without impairing their accuracy by restricting their calculation to the taxonomy of the profile (i.e., the taxonomy extracted from the ontology whose root node subsumes all the concepts of the profile involved in cid) or to the taxonomy of the distortion (i.e., the taxonomy extracted from the ontology whose root node subsumes the two concepts involved in cid’), thus improving the computational costs of the first and second phase to O(n × s) and O(m × s), s being the cardinality of the taxonomy extracted from O, such that s ≪ m in medium and large ontologies. It is therefore concluded that the computational cost of the semantic randomizer in a given iteration is O(m × s), that is, lineal to the cardinality of the ontology.

Empirical experiments
As evaluation data, we used random real queries taken from the AOL dataset [32]. To evaluate more clearly the effects of user profiling, we selected queries that belong to four general topics typically employed in user profiling: Health, Science, Sports and Society [1, 33]. In order to evaluate our method with well-differentiated profiles, we built the query logs of two simulated users. As shown in Fig. 5, the first query log (named Query log 1) consists of 110 queries on topics focusing mainly Health, whereas the second one (named Query log 2) consists of 105 queries with two predominant interests: Health and Science. According to [1], 100 queries are enough to build well-characterized user profiles.

Fig. 5
figure 5
Query logs employed in our empirical experiments

Full size image
To semantically interpret queries and to map them to appropriate categories (and profile topics), we used WordNet as ontology. WordNet [34] is a general-purpose semantic electronic repository that taxonomically models cross-domain concepts and their interrelationships. Queries have been mapped to concepts in WordNet by following the procedure detailed in Sect. 3.1.

Our proposal has been compared with three privacy-preserving baseline methods, which cover the approaches usually employed in the literature:

Random 1:n. A naïve method that adds n random fake queries per each real query, as done in [18]. In our experiments, the categories of the fake queries were randomly taken from WordNet concepts.

Random of Target 1:n. A variant of the naïve approach where the spectrum of categories for fake queries is restricted to specializations of the target category in WordNet. As a result, the user profile is expected to converge to the target profile better than with the previous method.

Semantic Approach 1:n. A method that uses an ontology to semantically categorize each real query and to generate n fake queries at a fixed semantic distance sd, as done in [22]. To carry out a fair evaluation, we set sd to the value of the semantic distance that between the centroid of the real queries and the target, thereby facilitating the convergence of the user profile.

We considered—as evaluation metrics—(i) the degree of convergence toward the desired target profile, and (ii) the efficiency of the method, measured as the number of fakes required to achieve such convergence. The degree of convergence has been measured as the semantic convergence error (SCE) between the current and target profiles after each iteration (i.e., after each new real query submitted by the user). A value equal to 0 or less than the convergence tolerance γ (which we set to γ = 0.05 for all tests) indicates that target profile has been reached and, therefore, that the privacy requirement of the user has been fulfilled.

In the following, we report the evaluation results of the different methods varying the target. In a first experiment, the target has been set to a category different than the dominant one(s) in the users’ query logs. In this way, we achieve privacy (because the dominant interests in the user profile are hidden) while biasing personalized services toward the desired target category, which may still be useful for the user. Specifically, the target has been set to footrace, which is a specific concept that is quite different to the dominant interests in the real query logs. Specifically, the semantic distance of this target to the centroids of Query log 1 (cid = Disease) and Query log 2 (cid = Scientific discipline) is 0.68 and 0.56, respectively. This test allows us to examine the behavior of the methods when a large divergence between the real profile and the target should be overtaken. For baseline methods (Random, Random of Target and Semantic Approach), we evaluated to executions with n = 1 and n = 4, so that either 1 or 4 fake queries are respectively created for each legitimate one. Consistently, we set the maximum number of fake queries our method is allowed to generate at each iteration to η = 1 and η = 4. As detailed above, the semantic distance (sd) of fake queries for the Semantic Approach was set to the distances between the centroid of the real queries and the target: 0.68 for Query log 1 and 0.56 for Query log 2. Finally, the variance of the random error sequence was set to α = 0.1.

Figure 6 reports the SCE between the target and the current profile obtained after each iteration, that is, after the submission of each legitimate query by the user. This results in 110 iterations for Query log 1 and 105 for Query log 2. Figure 7 depicts the cumulative number of fake queries introduced by the different methods.

Fig. 6
figure 6
SCE for the Random, Random of Target, Semantic Approach and Semantic Randomizer methods on Query Log 1 (left) and Query Log 2 (right) using footrace as target with n = η = 1 (top) and n = η = 4 (bottom)

Full size image
Fig. 7
figure 7
Cumulative number of fake queries for the Random, Random of Target, Semantic Approach and Semantic Randomizer methods on Query Log 1 (left) and Query Log 2 (right) using footrace as target with n = η = 1 (top) and n = η = 4 (bottom)

Full size image
Our method reaches the target (i.e., the SCE falls within the tolerance range) from the very first beginning and regardless of the type of profile (i.e., for both Query log 1 and Query log 2). In contrast, distortion strategies adopted by related works (Random and Semantic Approach) fail to provide any control on how the profile is distorted. The method Random of Target is able to achieve a similar convergence level to our method. This is due to the small spectrum of descendant categories of the target footrace (9 specializations in WordNet), which configures a very favorable scenario for this type of technique. When the weight of all false queries is concentrated on a very small sub-domain of WordNet, it is understandable that the centroid moves accurately toward the target. In any case, our approach is able to reach the convergence while generating a significantly smaller number of queries than baseline methods. Also, whereas the number of fake queries introduced by baseline methods is linearly proportional to n (i.e., 440 and 420 for n = 4 and 110 and 105 for n = 1), our method is able to self-regulate the production of fake queries to the actual degree convergence rather than to the maximum number of allowed queries (i.e., we generate 54 and 55 fake queries for η = 1 and 53 and 47 for η = 4). This results in much lower overhead w.r.t. the only baseline that was able to converge (Random of Target).

In a second experiment, the target has been set to the more general concept Event, which is closely related to the Society profile class. The semantic distance of this target to the centroids of Query log 1 (cid = Disease) and Query log 2 (cid = Scientific discipline) is 0.6 and 0.43, respectively, which indicate that now the target is more similar to the real profiles. Results for the different methods and metrics are reported in Figs. 8 and 9 for n = η = 4.

Fig. 8
figure 8
SCE for the Random, Random of Target, Semantic Approach and Semantic Randomizer methods on Query Log 1 (left) and Query Log 2 (right) using event as target with n = η = 4

Full size image
Fig. 9
figure 9
Cumulative number of fake queries for the Random, Random of Target, Semantic Approach and Semantic Randomizer methods on Query Log 1 (left) and Query Log 2 (right) using event as target with n = η = 4

Full size image
Our method easily reaches the new target while employing a number of fake queries that is still much smaller than the allowed maximum: 65 for Query Log 1 and 60 for Query Log 2. In contrast, none of the baseline methods is able to converge even though they generate as many queries as they can: 440 for Query Log 1 and 420 for Query Log 2. Only the Random of Target method approaches the convergence, but the generality of the target results in random fake queries that are too diverse to accurately distort the profile. In contrast, our method not only is able to accurately reach the target, but also to quickly respond to changes in the dominant categories of the real profile (which happens with Query Log 2 in iterations 17, 23 and 84). This can be also seen in the cumulative number of queries for Query Log 2: The number of fake queries increases until reaching a stable convergence around iteration 37 and stays stationary until the change observed in iteration 84, when it grows again.

In a third experiment we set the target to a very general concept that subsumes all the categories considered in the user profile: Abstraction. Making the user profile converge that target provides maximum privacy, because no dominant interests are exposed. The results for the different methods are shown in Figs. 10 and 11 for n = η = 4.

Fig. 10
figure 10
SCE for the Random, Random of Target, Semantic Approach and Semantic Randomizer methods on Query Log 1 (left) and Query Log 2 (right) using abstraction as target with n = η = 4

Full size image
Fig. 11
figure 11
Cumulative number of fake queries for the Random, Random of Target, Semantic Approach and Semantic Randomizer methods on Query Log 1 (left) and Query Log 2 (right) using abstraction as target with n = η = 4

Full size image
Even though convergence is not achieved by any method, ours is the one that decreases the SCE the most. Also, note that the method Random of Target no longer provides good convergence results because the spectrum of descendant categories of Abstraction matches the spectrum of categories used in the method Random. As a result, all baseline methods provide extremely similar (or even identical) poor results because, in practice, they all behave like the Random method. Examining the cumulative number of queries, we can see that our method generated much more queries than in the former experiments (138 for Query Log 1 and 123 for Query Log 2), which is normal due to the fact that convergence is not achieved. The number of queries is, however, much lower than the allowed maximum (and also of the number queries employed by baseline methods) because the actual number of fake queries generated at each iteration is proportional to the convergence error, which decreased as the number of iterations increased.

Conclusions and future work
We presented an automatic method to protect user profiles in front of search engines by submitting fake queries on behalf of the user. In comparison with related works, our approach introduces three main advantages. First, it allows users to specify their privacy requirements by stating the degree of profile distortion they desire. This distortion can encompass mild protection, which will retain the utility of the profile (e.g., for personalized search), full protection—in which the profile will be completely generic and will not exhibit dominant interests, and any intermediate scenario. The second advantage regards the number of fake queries needed to distortion the user profile. As we have shown in the empirical experiments, our method is able to self-regulate in order to use the least number of fakes that make the profile converge toward the desired target. This will minimize the overhead of the protection and avoid wasting network bandwidth. Finally, we have put special care in avoiding fake queries to follow a deterministic pattern (e.g., regarding their number and category) by employing a semantically consistent randomization.

For our method to be applicable in practice, several points should be taken into consideration. First, the ontology or ontologies used as background knowledge should cover most of the concepts the user may query. For this to be possible, general and broad ontologies should be employed. WordNet is a good candidate, but other larger ontologies such as YAGO (which, in addition of WordNet’s general concepts, also covers a variety of named entities and instances) could also facilitate matching queries to their corresponding categories. Our method also requires language-dependent tools to perform the required linguistic analyses. Because queries may be written in different languages, a variety of tools supporting such languages and a mechanism to automatically detect the query language would also be needed. Finally, fake queries should be submitted to the search engine without following a deterministic pattern that may highlight their synthetic nature. Other systems [9] have already proposed strategies to perform such submission in a human-like way by considering when the user is online and by randomized timing.

As future line of research, we plan to empirically study the impact that the introduced fake queries have in the personalized search services offered by commercial web search engines. With this, we will measure the utility of the protected profile and also the actual effectiveness of the protection (because search personalization is a direct function of the profile built by the search engine). For this, we will leverage linguistic mechanisms (such as those described in [23]) to build detailed queries from the fake categories provided by our method.