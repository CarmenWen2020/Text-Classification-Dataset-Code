In this research paper, we consider the subspace clustering problem which aims at finding a low-dimensional representation of a high-dimensional data set. In particular, our central focus is upon the least squares regression based on which we elaborate an adaptive weighted least squares regression for subspace clustering. Compared to the least squares regression, we consider the data locality to adaptively select relevant and close samples and discard irrelevant and faraway ones. Additionally, we impose a weight matrix on the representation errors to adaptively highlight the meaningful features and minimize the effect of redundant/noisy ones. Finally, we also add a non-negativity constraint on the representation coefficients to enhance the graph interpretability. These interesting properties allow to build up a more informative and quality graph, thereby yielding very promising clustering results. Extensive experiments on synthetic and real databases demonstrated that our clustering method achieves consistently optimal results, compared to multiple clustering methods.

Access provided by University of Auckland Library

Introduction
Over the past few years, we have been witnessing a blooming explosion of a tremendous amount of high-dimensional and heterogeneous data, such as images and videos [1, 9]. However, their labels have been challenging to be obtained [10, 17]. How to organize a high-dimensional data set into its corresponding natural groups in an automatic manner is still a thorny and challenging issue. In several real-world problems, the data samples can be considered as data points drawn from a union of multiple low-dimensional subspaces, e.g., the face images of multiple subjects under different conditions [20]. This accounts for the task of segmenting the high-dimensional data points into a union of multiple low-dimensional subspaces from which they are drawn, known as subspace clustering [31]. Exiting subspace clustering methods can be classified into three categories: algebraic, statistical, and spectral clustering based on the applied mathematical model. Particularly, owing to its efficiency and distinguished performance, the spectral clustering has whetted the interest and drawn the attention of numerous researchers and multiple algorithms have been developed over the past few years [31]. The spectral clustering rests upon two main steps: (1) learning an affinity matrix of data points and (2) applying traditional clustering methods such as k-means [16] and Normalized Cuts (NCut) [28] in order to generate the final clustering results [20]. The potential of spectral clustering is essentially determined by the graph construction scheme, i.e., modeling the relationships among data points using a block-diagonal affinity matrix [5, 11, 21]. Additionally, the affinity matrix should reveal the intrinsic structures of data so that a more robust and informative graph can be obtained.

Early spectral clustering methods usually built the affinity matrix through connecting each sample with its k-nearest neighbors using the Euclidean distance [3, 4]. Although efficient, this method exhibits certain drawbacks. First, the distance metric can neither capture the real data distribution nor preserve the distinctive neighborhood structure of each datum since it uses a global pre-fixed parameter “k” to denote the number of neighbors of all samples. Second, the parameter “k” should be optimally tuned for each data sample. Third, this neighborhood strategy is very sensitive to noise and corruption owing to the use of the Euclidean distance [25, 32]. To overcome these limitations, multiple researchers opted for efficiently representing each data sample as a linear combination of remaining samples [31]. Adopting this strategy, the intrinsic representation structure of data can be adaptively captured. Basically, researchers refer to these methods as the representation-based subspace clustering. For instance, in [7], the authors introduced the sparse subspace clustering (SSC) method, in which the sparse representation coefficients are used to build the affinity matrix. Afterwards, Peng et. al. elaborated the scalable sparse subspace clustering (SSSC) as an out-of-sample extension of the SSC algorithm [26]. The sparse representation-based methods usually capture the local structure of data uniquely [19, 20]. Moreover, they require a heavy computational cost [4]. In contrast to SSC algorithm and its extensions, authors in [19] set forward the low rank representation LRR that constrains the representation matrix by a nuclear norm penalty to uncover the global data structure. Compared to distance-based, the representation-based spectral clustering methods (e.g., SSC and LRR) achieve higher clustering performance [6]. However, with the presence of highly correlated samples, they fail to reveal the grouping information effect [35]. Authors in [21] addressed this issue and introduced a subspace clustering algorithm based on the least squares regression (LSR). The proposed algorithm encourages the grouping effect by grouping the highly correlated samples together [21]. Compared to other representation techniques, the LSR is simpler, more efficient and robust to noisy data. In [14], Hu et al. invested the smooth representation (SMR) to further explore the grouping information effect. Peng et al. [24] proposed a novel subspace clustering method based on ridge regression. They applied a hard threshold operator to efficiently handle any type of error, thereby improving the robustness of subspace clustering.

Inspired from the progress fulfilled in terms of regression-based subspace clustering, our central focus in this research is upon the least squares regression and we propose an adaptive weighted nonnegative least square regression (AWLSR). Unlike previous representation techniques, the AWLSR model can adaptively highlight the important samples while reducing the effect of redundant/noisy ones. Moreover, a nonnegativity constraint is incorporated in such a way that the representation coefficients can directly reveal the similarity among samples, which in turn enhances the graph interpretability. Finally, a locality constraint is integrated in the LSR model to insure that close representation coefficients are assigned to close samples. These reliable properties stand for a strong impetus inciting us to build up a more robust and informative graph. Extensive experiments on both synthetic and real databases revealed that our subspace clustering method achieves better clustering performance and robustness to errors than other methods in almost all cases. The rest of the paper is structured as follows. Section 2 provides an overview of some related works. Section 3 describes in details our proposed subspace clustering method. Experiments and discussions are displayed in Sect. 5. Eventually, Sect. 6 exhibits some outstanding remarks and provides new perspectives for future works.

Related works
Spectral clustering has emerged as a fundamental and effective subspace clustering method especially for high-dimensional data [7, 20, 21, 32]. It has the ability to discover the local data structure and find a low-dimensional representation from the high-dimensional data in a flexible and elegant way [2, 22]. Particularly, the representation-based spectral clustering has been extensively investigated thanks to its outstanding ability in adaptively capturing the real data structure [7, 19, 29, 30].

Mathematically, we can unify the representation-based subspace clustering into the following framework [32]:

minC,EΦ(C)+λΨ(E) s.t X=XC+E,
(1)
where E denotes the reconstruction error and C indicates the representation coefficient matrix to be learned.

The basic objective of the above self-expressive framework is to pick up the “best” representation that can better capture the underlying structures embedded in the original high-dimensional space. More precisely, Ψ(E) models different types of errors (e.g., noise, corruption, outliers) through applying different penalties. Φ(Z) represents the regularization function with respect to the matrix C. Obviously, a multitude of representation models can be learned by varying the two terms Ψ(E) and Φ(C).

In whats follows, we provide a detailed review of three popular representation-based clustering algorithms, namely the Sparse Subspace Clustering, the low-rank representation, and the least squares regression.

Sparse subspace clustering
Rather than using the Euclidean distance, the Sparse Subspace Clustering (SSC) algorithm [7] uses the ℓ1-penalty to represent a sample as a linear combination of few samples from the same subspace.

Mathematically, SSC aims to solve the following objective function:

minC∥C∥1+λ2∥E∥2Fs.tX=XC+E,diag(C)=0
(2)
According to this formulation, the SSC algorithm supposes that the data are self-expressive [7]. Besides, it introduces the error E into the learning framework to handle noises and outliers. However, in view of the sparsity penalty, some important correlations may be lost [14]. Moreover, the global structure of data is not explicitly considered [32].

Low rank representation
One of the most popular subspace clustering methods is the Low Rank Representation (LRR) [19]. Considering the self-expressiveness property of data, the authors constrained the representation matrix by a nuclear norm penalty so that the global data structure is well captured. In other words, the LRR tends to represent the data structure using its lowest rank representation [19].

Mathematically, LRR aims to optimize the following problem:

minC∥C∥∗+λ∥E∥p,s.tX=XC+E
(3)
where ∥C∥∗=∑nj=1δi is the nuclear norm of C , δi is the ith singular value of C, and p represents either the ℓ1 or ℓ2,1 norm.

Here, the locality is not guaranteed since the obtained representation matrix is very dense.

Least squares regression
Data in real-world problems are often highly correlated. Nevertheless, with the presence of a group of highly correlated samples, the sparse representation (we can assume that the low-rank as a two-dimensional sparsity [18]) tends to arbitrarily select any data sample and neglects the remaining ones [35]. Hence, it’s unable to reveal important grouping information in the data. Recently, many researchers have investigated the regression models for subspace clustering [14, 21, 24].

For instance, in 2012, the Least Squares Regression (LSR) was leveraged for subspace segmentation by solving the following regression problem [21]:

minC∥C∥2F+λ∥X−XC∥2F,s.t.diag(C)=0
(4)
Another formulation of the LSR can be obtained by removing the constraint diag(C)=0 :

minC∥C∥2F+λ∥X−XC∥2F
(5)
Indeed, problem (5) is the Tikhonov regularization or the ridge regression [13].

According to both formulations, LSR tends to encourage the grouping effect by making full use of the data and grouping highly correlated data together. Moreover, LSR provides an analytical solution which makes it very efficient and feasible compared to sparse representation-based methods.

Many extensions of the Least Square Regression were reported in the literature. For example, authors in [14] put emphasis on the grouping effect and proposed the Smooth Representation (SMR) to further enhance the subspace clustering performance:

minC∥C∥2F+tr(CL˜CT),
(6)
where L˜ is a graph Laplacian matrix. Here, the SMR model explicitly reinforces the grouping effect through its second term [14].

In [24], a novel subspace clustering algorithm called Thresholding Ridge Regression (TRR) was highlighted. Basically, TRR performs a hard thresholding operator to efficiently and robustly eliminate the errors without any prior knowledge on their structures.

The proposed method
Model of the proposed method
In this section, we introduce the adaptive weighted least squares regression (AWLSR) framework for data representation. First, we clarify important notations used throughout this paper. The matrix X=[x1,...,xN]∈Rd×N is a collection of N data samples where xi∈Rd represents the i-th sample. 1 and 1 are a matrix and a vector whose elements are equal to 1. ⊙ denotes the element-wise multiplication. For a scalar v, we define (v)+ as (v)+=max(v,0) [23].

The least squares regression aims to optimize the following problem [21]:

minC∥X−XC∥22+λ∥C∥22 \; s.t \; diag(C)=0
(7)
Here, the ℓ2-regularizer ensures not only the stability of the solution but also a certain amount of sparsity. Nevertheless, all samples are equally treated regardless their nature ( whether relevant or noisy).

Basically, the representation error of redundant/noisy samples is large. Thus, we opt for regularizing the representation errors by a non-negative weight matrix as follows:

minC,W∥W1/2⊙(X−XC)∥2F+β2∥W∥2F+λ∥C∥2F \; s.t \; W≥0,WT1=1,diag(C)=0
(8)
The constraint WT1=1 and the regularization term ∥W∥2F are incorporated to avoid the trivial solution of W [33].

According to this formulation, we adaptively highlight the important samples while reducing the effect of redundant/noisy ones. The representation matrix is hence more robust to noisy data. However, the present representation does not consider the locality of data which proved to be critical for various machine learning problems such as classification, clustering and dimensionality reduction, etc [20]. For this reason, a locality-preserving term is incorporated in the least squares regression model as follows:

argminC,W∥W1/2⊙(X−XC)∥2F+β2∥W∥2F+λ∥C∥2F+γ∑i,j=1N||xi−xj||22cij \; s.t \;W≥0,WT1=1,diag(C)=0
(9)
The locality-preserving term ensures that close samples will have close representation coefficients.

Let Q be a matrix whose element qij=||xi−xj||22. Then, Eq. 9 becomes:

argminC,W∥W1/2⊙(X−XC)∥2F+β2∥W∥2F+λ∥C∥2F+γtr(QTC) \;s.t \; W≥0,WT1=1,diag(C)=0
(10)
Finally, the following constraints are added to the representation matrix C:

C≥0: Many recent works have demonstrated that the non-negativity constraint often helps obtain a better representation of visual data [32, 34]. In the light of this observation, we constrain the representation matrix with a non-negative penalty in such a way that each representation coefficient cij directly reveals the similarity between the samples xi and xj.

C1=1: the sum of each row of C is set to be equal to 1, which ensures that all samples are selected in the joint representation.

Overall, the adaptive weighted least squares regression model is expressed as:

argminC,W∥W1/2⊙(X−XC)∥2F+β2∥W∥2F+λ∥C∥2F+γtr(QTC) \;s.t \; W≥0,WT1=1,C≥0,diag(C)=0,C1=1
(11)
Solution to the proposed method
There are two unknown variables in the problem (11) that are needed to be optimized, e.g., C and W. Obviously, problem (11) is not jointly convex for C and W. Subsequently, it is challenging to obtain a closed-form solution and optimize the two variables simultaneously. Recently, the alternating direction method of multipliers (ADMM) has been a hot area of research in many fields owing to its simple form and decoupling of variables [20]. Therefore, in this work, we adopt the ADMM method to optimize the proposed model.

First, to make problem (11) separable, certain auxiliary variables are integrated as follows:

argminC,W∥W1/2⊙E∥2F+β2∥W∥2F+λ∥J∥2F+γtr(QTC) \;s.t \;W≥0,WT1=1,C≥0,diag(C)=0,C1=1,E=X−XC,J=C
(12)
Considering problem (12) as a two-block optimization problem, the augmented Lagrangian function can be formulated as:

L(C,W,E,J,C1,C2)=∥W1/2⊙E∥2F+β2∥W∥2F+λ∥J∥2F+γtr(QTC)+μ2(∥∥∥X−XC−E+C1μ∥∥∥2F+∥∥∥C−J+C2μ∥∥∥2F),
(13)
where C1, C2 denote the Lagrangian multipliers and μ indicates the penalty parameter.

The basic idea of the ADMM method rests on solving each unknown variable while alternately fixing the other variables as follows.

Step 1 The variable W is obtained by minimizing the following problem while fixing the other variables:

minW∥W1/2⊙E∥2F+β2||W||2F \;s.t \;W≥0,WT1=1
(14)
Since E is fixed, solving problem (14) is equivalent to solve:

minwij≥0,∑jwij=1∑i,j(wije2ij+β2w2ij)⇔minwij≥0,∑jwij=1∑i,j(wij+e2ijβ)2
(15)
Problem (15) can be separately solved for each row of W since it is independent for different i [23]. Thus, we re-write problem (15) in the vector form as follows:

minwi≥0,wTi1=1∥∥∥wi+hiβ∥∥∥22,
(16)
where H=E⊙E

The associated Lagrangian function corresponds to :

L(wi,c,mi)=12∥∥∥wi+hiβ∥∥∥22−c(wTi1−1)−mTiwi
(17)
where c and mi are the Lagrangian multipliers associated with the boundary constraints on wi.

According to KKT condition, we have mijwij=0 [33]. As a matter of fact,

wij=(c−hijβ)+
(18)
Eventually, we update the Lagrangian multiplier c according to the constraint wTi1=1 in problem (16) as follows:

∑i=1N(c−hijβ)=1⇒c=1N+1Nβ∑j=1Nhij
(19)
Step 2 By fixing the variables C, W and the Lagrangian multiplier C1 and C2, we obtain the error matrix E by solving the following problem:

minE||W1/2⊙E||2F+μ2||E−G||2F,
(20)
where G=X−XC+C1μ

Problem (20) is equivalent to :

mineij∑i,j(eij−μgijμ+2wij)2
(21)
Therefore, the optimal solution of each element eij is

eij=μgijμ+2wij
(22)
Step 3 The optimal solution of J is obtained by solving the following minimization problem:

minJλ∥J∥2F+μ2||C−J+C2μ||2F,
(23)
The analytical solution of J can be obtained by equating the derivative of (23) w.r.t J to zero:

J∗=μGμ+2λ
(24)
where G=C+C2μ

Step 4 The variable C can be optimized by solving the following minimization problem while fixing the variables J, W, E, C1 and C2:

minZ γtr(QTC)+μ2(||M1−XC||2F+||C−M2||2F) s.t C≥0,diag(C)=0,C1=1
(25)
where M1=X−E+C1μ and M2=J−C2μ

Considering the following unconstrained problem:

argminC γtr(QTC)+μ2(||M1−XC||2F+||C−M2||2F)
(26)
Problem (26) has an analytical solution obtained by setting its derivative equal to zero:

Cˆ=(XTX+I)−1(XTM1+M2−γμQ)
(27)
Hence, the optimal solution C of problem (25) can be obtained more efficiently by solving the following problem:

minC≥0,diag(C)=0,C1=1 ||C−Cˆ||2F,
(28)
We obtain the optimal solution for each row ci as follows:

ci=(ηiITf+ci¯)+
(29)
where If is a column vector whose elements are equal to one except the i−th element which is set equal to zero. ci¯ is defined as :

ci¯={cijˆ0i≠jotherwise
(30)
The Lagrangian multiplier ηi can be calculated as:

ηi=1+ci¯1N−1
(31)
Step 5 The Lagrangian multipliers and the penalty parameter are updated by:

C1=C1+μ(X−XC−E)
(32)
C2=C2μ(C−J)
(33)
μ=min(μmax,μρ)
(34)
The aforementioned ADMM-based optimization procedure is summarized in Algorithm 2.

Spectral clustering based on AWLSR
Broadly speaking, the subspace clustering seeks to partition data into several dissimilar groups (or clusters) while maximizing the similarity within each cluster. Particularly, spectral clustering is an effective method that aims at preserving the local data structure via constructing a graph and treating the clustering as a graph partition. Similar to conventional subspace clustering methods, the final clustering results in this work are obtained through applying the Normalized cut (Ncut) algorithm to the affinity matrix [28].

The spectral clustering steps based on the AWLSR model are summarized in Algorithm 1.

figure a
Analysis of the proposed method
Computational complexity analysis
figure b
This section provides a computational complexity analysis of the AWLSR model. The most computationally-demanding step in Algorithm 2 stands for the fourth step which includes matrix multiplication and matrix inverse operations. It costs O(N3) for N×N matrix. Fortunately, the term (XTX+I)−1 can be pre-calculated before the iteration loop since it is independent of all variables. The first two steps are efficiently calculated as they can be considered as element-wise operations. The third step mainly involves matrix addition operation. From this perspective, their computational complexities can be ignored compared to the fourth step.

Convergence analysis
The classical ADMM is guaranteed to converge when the objective function contains, at most, two sets of variables as proven in numerous works [8, 32]. In our problem, however, the Lagrangian function (13) involves more than two variables. Therefore, it is difficult to prove theoretically its convergence [33]. In practice, it has been proven in several recent studies that the ADMM algorithm exhibits good convergence properties when the optimality gap decreases in a monotonous way at each iteration [19]. Departing from this finding, we aim to experimentally demonstrate, in this section, the convergence of our method. Figure 1 reveals the values of the objective function w.r.t the number of iterations on Coil-20 and ExYaleB databases, respectively. One can obviously notice that the objective function is decreasing in a monotonous way and converges to a stable point within a few iterations. Moreover, the convergence curve demonstrates that our ADMM-based optimization procedure is efficient and feasible.

Fig. 1
figure 1
Objective function values w.r.t the number of iterations for AWLSR

Full size image
Experiments
In this section, we attempt to validate the clustering performance of our AWLSR model through conducting experiments on both synthetic and real databases.

Experimental settings
In all experiments, several state-of-the-art clustering algorithms including k-means, Ncut [28], SSC [7], LRR [19], LSR [21], TRR [25] and SMR [14] were selected for comparison. To assess the clustering performance of all methods, we adopted two popular metrics, namely, the clustering accuracy (Acc) and the normalized mutual information (NMI) to carry out experiments [15].

For fair comparison, all experiments were conducted on MATLAB platform running on Windows7, with an Intel (R)-Core(TM) i7-4500U 3.40GHz processor and 8GB memory. Moreover, the parameters of all other methods were manually tuned to reach optimal results.

Experiments on the synthetic data
Table 1 Clustering results on the synthetic data
Full size table
To quantitatively evaluate the clustering performance of all methods, a synthetic dataset whose ground truth is perfectly known in details was created. Proceeding in the same manner as [20], we generated a synthetic database X∈R300×200 which includes 200 data points that are separated into 5 clusters. Each data point has 300 features. To further check the robustness of these clustering methods, the synthetic data matrix X was corrupted with ’salt-and-pepper’ noise with different degrees. Table 1 reports the performance of the compared methods on the noisy synthetic data. The following conclusions can be drawn from the obtained results.

All clustering methods successively assign the data points to their respective groups when the data are clean (i.e., the noise density is equal to zero).

The accuracies and NMIs of all methods tend to drop when the noise density increases.

Particularly, the performance of distance-based clustering methods (e.g., k-means and Ncut) drops dramatically with the noise density. This demonstrates that the distance metric fails to capture the real data structure in the presence of noise.

In almost all cases, the AWLSR method outperforms other clustering methods. For instance, the accuracy gain of our method over the second best method amounts to 13% when the noise degree is equal to 0.3.

The performance achieved by the proposed method under the noisy synthetic data reveals its potential ability at the level of handling noises and obtaining a more robust graph.

Experiments on real databases
In this subsection, we aim to validate the performance of our method in dealing with three real-world problems: clustering images of human faces, clustering images of objects, clustering images of handwritten digits clustering. All databases used in these experiments are publicly available and are displayed in Table 2.

Table 2 Description of real databases
Full size table
Fig. 2
figure 2
Typical images from the ExYaleB database and the ORL database, respectively

Full size image
Face images clustering
Table 3 The clustering performance in terms of ACC and NMI of all methods on the face databases
Full size table
All methods were assisted on two well-known face databasesFootnote1, including ORL [27] and Extended YaleB [12]. The ORL database contains 400 images of 40 distinct subjects. The images of some subjects are taken at various lighting conditions, different moments, various facial expressions (e.g., smiling/not smiling) and facial details (e.g., glasses/no glasses) [27]. The ExYaleB database has 2414 images for 38 subjects under different illuminations per individual. Figure 2 illustrates some images from the aforementioned databases. All face images are of the size 32×32. Experimental results on the face databases are outlined in Table 3. It can be inferred that our method obtains the best results on the ExYaleB database and the second best results on the ORL database. For instance, the accuracy of the AWLSR is 13.565% higher than that of the LSR, 18.11% higher than that of SMR, 15.22% higher than that of the SSC and 10.61% higher than that of the LRR on the ExYaleB database. This corroborates the robustness of our method in terms of handling varying illuminations. Nevertheless, on the ORL database, the TRR model is more robust in handling different types of errors presented in the samples ( e.g., images with various facial expressions and facial details).

Object images clustering
Fig. 3
figure 3
Typical samples of the Coil-20 and Coil-100 databases

Full size image
Table 4 The clustering performance in terms of ACC and NMI of all methods on the object databases
Full size table
This subsection centers around experimenting two popular object databasesFootnote2, namely the Coil-20 and the Coil-100 databases. The former consists of 1440 images of 20 objects. The latter corresponds to 7200 images of 100 objects. For computational efficiency, all images are normalized to 32×32 pixels. Figure 3 illustrates some examples from the Coil-20 and the Coil-100 databases, respectively.

Table 4 depicts the clustering performance of different algorithms on the Coil-20 and the Coil-100 object databases. It can be detected that our AWLSR model achieves the best clustering performance on the Coil-20 database and the second best results on the Coil-100 database. Concerning the object databases, in particular, it is noteworthy that the performance achieved by the Ncut algorithm is higher than many representation-based clustering methods such as SSC and LRR. This implies that the distance-based methods can accurately capture the real data structure on the object databases.

Handwritten digital images clustering
For Handwritten Digital Images Clustering, USPSFootnote3 and MNISTFootnote4 handwritten digit databases are selected for performance evaluation. The two databases consist of images from “0” to “9.” For the USPS database, a subset of 1000 16×16 gray digit images is used. For the MNIST database, a subset of 6996 28×28 gray digit images is invested. Some sample images from the above databases are incorporated in Fig. 4. The experimental results on the USPS database and MNIST database are presented in Table 5. It is clear to observe that the clustering performance of different methods is relatively low compared to that on other databases. This can be attributed to the fact that, unlike other images, the handwritten images are difficult to be distinguished and clustered ( i.e., images are binary, less aligned and contain less information compared to face or object images). Moreover, it is clear to notice that our method consistently and significantly outperforms almost all compared spectral clustering methods. For example, the clustering accuracy gains of AWLSR over TRR are 17.8%, 26.93% on USPS and MNIST databases, respectively. Similarly, such gains of AWLSR in terms of NMI are 5.91% and 31.91%.

Fig. 4
figure 4
Typical images from the MNIST database and the USPS database, respectively

Full size image
Table 5 The clustering performance in terms of ACC and NMI of all methods on the handwritten digit databases
Full size table
Basically, departing from the above experiments, we can further draw the following conclusions. First, the experimental results also reveal that the AWLSR achieves considerable improvement over the least squares regression LSR in terms of both ACC and NMI on all databases. AWLSR is mainly superior in view of its ability to highlight the useful features and intensify their effects during graph learning. Additionally, the locality-regularization of the AWLSR enables to construct a more discriminative graph and allows to ultimately obtain much better clustering performance. Second, on all databases, the K-means method performs the worst in comparison with the spectral clustering methods. This demonstrates that performing clustering on a low-dimensional representation is more effective than using the original high-dimensional data directly. Third, in almost all cases, the representation-based methods always outperform the distance-based spectral clustering methods. This demonstrates (1) the power of the representation techniques in discovering the real neighborhood structures of data and (2) the robustness of representation techniques in revealing the intrinsic structure of data in presence of noise and outliers. Finally, it is noteworthy that, in almost all cases, the ℓ2-representations methods, i.e., LSR, TRR, SMR , and AWLSR exhibit the best performance compared to the sparse representation, i.e., LRR and SSC. This can be assigned to the fact that the ℓ2-regularization offers good discriminative power. Besides, it encourages the grouping effect to model the data correlations.

Parameter settings
This experiment was carried out to explore the effect of different parameters on the performance of our proposed method. Basically, the AWLSR model involves three regularization parameters i.e., β, λ and γ which require an optimal tuning. The first parameter controls the regularization term on the weighted matrix W. To analyze its influence, we first fixed the parameters λ and γ and plotted the accuracies of AWLSR as a function of β within the range of [10−3,103]. Obtained results are highlighted in Fig. 5a and Fig. 6a on the Coil-20 and ExYaleB databases, respectively. Departing from these figures, it is clear to record that the AWLSR performance is quite insensitive to the value of β.

Next, we fixed the parameter β and tested the influence of the two parameters λ and γ. Referring to Fig. 5b and b, one can notice the high impact of these two parameters on the overall clustering performance. One plausible interpretation is that those parameters directly determine the effect of the ℓ2-regularizer and the locality-preserving term on the representation matrix Z. Accordingly, tuning optimally these three parameters is the guarantee of satisfactory performance.

Fig. 5
figure 5
Clustering accuracy of AWLSR as a function of a parameter β when λ and γ are fixed, b parameters λ and γ with β=10 on the Coil-20 database

Full size image
Fig. 6
figure 6
Clustering accuracy of AWLSR as a function of a parameter β when λ and γ are fixed, b parameters λ and γ with β=10 on the ExYaleB database. Note that bold numbers denote the best results

Full size image
Conclusion
In this research, the adaptive weighted nonnegative least squares regression (AWLSR) is elaborated to build a more informative graph model. The AWLSR shares the efficiency of the least squares regression while being more effective for graph learning. By constraining the self-representation term with a weighted matrix, the effect of those redundant and useless features can be adaptively minimized so that a more robust graph can be constructed. In addition, our proposed model exhibits the merit of simultaneously capturing both global and local data structures through introducing a locality-preserving term to the LSR model.

Extensive experiments on both synthetic and real databases yielded that our method fulfills consistently superior results compared to numerous state-of-the-art subspace clustering methods. At this stage of analysis, we would assert that although the obtained results can be regarded as worthwhile findings, our study remains a step that may be extended and built upon as it opens further fruitful lines of investigation and offers promising future research directions. Indeed, in future research we may explore unknown territories and undeciphered areas which deserve deeper investigation like the issue of handling images with occlusion and various facial expressions.