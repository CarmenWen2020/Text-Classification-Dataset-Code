Graph attention networks are effective graph neural networks that perform graph embedding for semi-supervised learning, which considers the neighbors of a node when learning its features. This paper presents a novel attention-based graph neural network that introduces an attention mechanism in the word-represented features of a node together incorporating the neighbors’ attention in the embedding process. Instead of using a vector as the feature of a node in the traditional graph attention networks, the proposed method uses a 2D matrix to represent a node, where each row in the matrix stands for a different attention distribution against the original word-represented features of a node. Then, the compressed features are fed into a graph attention layer that aggregates the matrix representation of the node and its neighbor nodes with different attention weights as a new representation. By stacking several graph attention layers, it obtains the final representation of nodes as matrices, which considers both that the neighbors of a node have different importance and that the words also have different importance in their original features. Experimental results on three citation network datasets show that the proposed method significantly outperforms eight state-of-the-art methods in semi-supervised classification tasks.

Access provided by University of Auckland Library

Introduction
The graph is a data structure with strong expressive ability, which can describe the relationship between objects and the strength of the relationship. The data in many real-world applications are organized as graphs, such as citation networks, social networks, knowledge graphs, and so on. With the growth of data, the efficient representation of a graph becomes a fundamental issue that can affect the success of subsequent tasks, which has attracted more attention in recent years. Basically, a graph has two kinds of elements, i.e., nodes and edges. Since a node itself is often described using natural languages in many applications (e.g., papers in citation networks, people in social networks, and entities in knowledge graphs), a straightforward method is to represent a node as a one-hot vector, known as the bag-of-word representation. Edges in a graph are often represented as an adjacency matrix. Using a one-hot matrix (for all nodes in a graph) and an adjacency matrix to represent a graph suffers from two defects: First, the one-hot vector of a node is sparse representation. When the vectors become very huge, this representation will seriously lower the processing efficiency of subsequent tasks. Second, the separation of two different sized matrices (i.e., one-shot node matrix and adjacency matrix) will increase the complexity of subsequent tasks. A new train of thought arisen in recent years is to embed a graph (ideally, including features of nodes and information of edges) into a low-dimensional space. Consequently, two similar nodes from the joint perspective of their original features and structural information (i.e., attached edges and connecting nodes) have approximate vector representation. With the graph embedding, the subsequent tasks, such as node classification [15, 28], link prediction [40, 41], and network visualization [34, 39], could be facilitated in a uniform space.

The early graph embedding methods only utilize the adjacency matrix. For example, GF [1] first performs the factorization of the adjacency matrix to obtain a preliminary embedding matrix for all nodes and then minimizes the L2-norm of the matrix to obtain its final form. LINE [34] first initializes nodes using vectors with random elemental values. For each pair of nodes, LINE defines two joint probability distributions. One is calculated from the adjacency matrix, and the other is calculated from the second-order adjacency matrix (considering the nodes shared between these two nodes) by the embedding vector. Then, the embedding vectors of the nodes are obtained by minimizing the Kullback–Leibler divergence of these two distributions. DeepWalk [28] uses a random walk to generate a node sequence. Then, for each node sequence, it applies the Word2Vec model [27] to obtain the embedding vector for the node by treating the sequence as a sentence. All those above can be classified as shallow models. Recently, a series of methods based on deep learning, namely graph neural networks (GNNs) [38], have attracted more interest, which adds the original features of nodes (as one-hot bag-of-words representation) into the embedding process. Some typical methods include graph convolutional networks (GCNs) [21], GraphSAGE [17], and graph attention networks (GATs) [38].

Compared with other GNNs (such as GCNs and GraphSAGE) that also aggregate neighbor information, GATs assign the neighbors of a node different attention weights, which has an easily understandable reality interpretation. For example, in social networks, an opinion leader as a neighbor probably has a greater impact. In citation networks, a paper cited by a world-famous professor should demonstrate more importance. By this means, in a GAT model, not only a node and its neighbors will get closer in low-dimensional space but also and the neighbors with higher attention play a more important role in the aggregation. By stacking several attention layers, each node can gather its k-hop neighbors’ information with different attention weights so that the structure information can be sufficiently preserved. This graph attention mechanism has demonstrated good performance in several graph-oriented tasks [42].

However, it must be noticed that in a GAT model, the features of nodes are still represented as flat one-hot vectors. For example, in a citation network, a sparse bag-of-words representation of node features treats each element in the vector as the presentation of a word. If a word appears in the article, the corresponding element has the value 1. Otherwise, it is 0. This scheme means that all the words are equally important for the article, which obviously violates common sense. If we want to determine the category of a machine learning article, the word “reinforcement” should be more important than some other words. Moreover, the importance of a word may spread along with the network. An article with several neighbors with the word “Q-Learning” is highly probably in the category “reinforcement learning”. Thus, it would be better if the importance of words can be preserved in the embedding.

To address this issue, we propose novel Word and Graph AttenTion networks (WGATs), which introduce the word-level attention mechanism and stacked graph attention operations. The WGAT model first uses a vector to represent each word. Then, all the word vectors belonging to a node are fed into a gated recurrent unit (GRU) [9] model, and the hidden state of each GRU unit is taken as a new representation of each word. Then, it performs a self-attention operation to these word vectors to obtain a 2D matrix as a node representation, which considers the attention weights of different words on the node. Then, this matrix is fed into a stacked graph attention layer to aggregate its neighbors’ matrices with different attention weights, which generates a new more compressed matrix. After performing two rounds of stacked graph attention operation, WGATs can obtain the final node embedding vector for each node by the concatenation of its last 2D compressed matrix representation. We evaluate the proposed WGAT model in the semi-supervised classification tasks on three citation network datasets Cora, Citeseer, and DBLP. Experimental results show that our WGATs significantly outperform eight state-of-the-art methods.

The remainder of the paper is organized as follows: Sect. 2 reviews the related work. Section 3 presents the proposed WGAT model in detail. Section 4 presents the experimental settings and results. Section 5 concludes the paper.

Related work
Graph embedding can provide an effective and efficient representation of a graph for graph analytics and learning. It converts a graph into a set of vectors in a low-dimensional space, where graph information is preserved. Many traditional methods, such as multidimensional scaling (MDS) [11], IsoMap [36], LLE [29], and Laplacian eigenmaps [4], construct an affinity graph [29] using feature vectors, where the nodes originally close together are also in the close distances in the new graph. Because these methods involve solving the leading eigenvectors of the affinity matrix, the computational complexity is a critical bottleneck, which makes them inefficient in real-world applications. In recent years, there come several deep learning-based graph embedding methods, namely graph neural networks (GNNs). These methods aim to learn an embedding representation of a graph that preserves both structural and feature information for the nodes. GNNs can be divided into two categories: spectral-based and spatial-based methods. The spectral-based methods [6, 16, 18] introduce convolution operations by employing filters from the perspective of graph signal processing [30], where the convolution on a graph can be interpreted as removing noises from graph signals and passing messages in the spectral domain. The spatial-based methods [14, 21] formulate convolution operations on a node as aggregating features derived from its neighbors and the information passing through it. The principle behind the GNN models is “neural message passing” proposed by Gilmer et al. [14]. All the current GNN models can be viewed as message passing algorithms, where the representation of a node is iteratively computed from the features of its neighbors using differentiable aggregation functions. By this means, two nodes directly linked by an edge or sharing a set of common neighbors tend to have similar embedding vectors in low-dimensional space. That is, the graph structural information can be preserved by such aggregation. As a deep learning-based embedding method, the proposed model also follows the same hypothesis.

Another line of related work is attention mechanism, which can help the model concentrate on the most important parts of the input in decision making, which has been widely used in natural language processing (NLP) tasks for several years. Machine translation [3, 13] first takes the advantage of attention mechanism, which considers attention scores for each input word against all the output words to weigh the importance of the input word. Besides machine translation, attention mechanism is also widely used in some other NLP tasks such as question answering [25, 32], sentence embedding [24], word embedding [8, 12], item recommendation [33], and sentiment analysis [2, 31]. The attention mechanism also can be applied to graph-structured datasets. It allows models to assign relevance scores to different elements in a graph. For example, for a node and its neighbors, a model can assign different weights to the neighbors, which better describes the relation between two nodes [38]. The attention mechanism also allows models to avoid or ignore noisy parts of a graph, which consequently improves the models’ signal-to-noise ratios [22]. Besides the attention derived from the neighbors, the proposed model also introduces the attention mechanism in nodes’ original word-represented features.

The proposed method
In this section, we first formalize the problem. Then, we describe how to obtain an embedding vector for a node in a single epoch in training. Finally, we present the whole training process as an algorithm.

Problem statement
In this study, we assume that a graph G=(V,E,X,Y) is an undirected citation network. Node set V=(Vl∪Vu) contains nl labeled nodes (Vl) and nu unlabeled nodes (Vu). The total number of nodes is n=nl+nu. E is the edge set, where each element denotes a link two nodes (articles) that has citation relationship. X=(xx1,xx2,…,xxn)T,X∈Rn×d is a 0-1 sparse bag-of-words feature matrix for all n nodes. Here, the word bag has d words in total. The known labels of the nodes in Vl form a label matrix YL=(yy1,yy2,…,yynl)T. That is, YL∈Rnl×c is a 0–1 matrix that denotes the labels of nl labeled nodes, where c is the number of concept classes in a graph. We treat the citation relationship as undirected edges and construct a binary symmetric matrix as the adjacency matrix of the graph, denoted by L=[lij]∈Rn×n, where lij (lji) is 1 if nodes i and j have citation relationship, otherwise, lij (lji) is 0. We add a self-connected link to each node itself. Thus, the self-connected adjacency matrix is L¯=L+I, where I∈Rn×n is the identity matrix. Our goal is to build an end-to-end graph embedding model, where each node is assigned a compressed vector representation and the labels of unlabeled nodes in the graph can be predicted.

Fig. 1
figure 1
The architecture of the proposed WGAT model. A node passes through the network as an example

Full size image
Word attention
We propose a novel Word and Graph AttenTion (WGAT) model. Since WGAT is an end-to-end model, in training and prediction, each node represented by the original bag-of-words model will pass through the network and finally obtains an estimated label. The network can be divided into three sequential parts: word attention part, graph attention part, and rear linear part. Our innovation lies in the first two parts, where the bag-of-words presentation of a node is converted into a compressed embedding vector. The architecture of the WGAT model is illustrated in Fig. 1, where we use a node xxi to show how it changes when passing through the network.

Our model starts with a word attention structure. The input of this structure is a 0–1 sparse bag-of-words vector of a node. The output is a new matrix represented features of the node, which considers the attention weights of words on this node.

First, for all the d words in the word bag (corpus), WGAT generates an embedding matrix W∈Rd×dw, where each row is randomly initialized as a dw-dimensional vector, representing a word. For a node xx, if its 0–1 bag-of-words feature vector has m presented words, WGAT uses the embedding vectors of these words to form a word embedding matrix for this node, denoted by

S=(ww1,ww2,…,wwm)T,
(1)
where wwi(1≤i≤m) is the ith words that the node contains. Note that for each node, the value of m can be different.

To clarify the correct order of the abstract words or the title words in different articles, we take dataset Cora as an example. Considering RNN can adapt to the length of a sequence, we select the first 100 words of the abstract of an article. If the abstract is less than 100 words, then default values will be appended to the rear, which will ensure that the dimensions of the tensors are the same. Suppose the abstract of ab article is “I like pets” and we have already built a word bag (corpus) containing 12,691 words. All the words are numbered from 0 to 12,690. The three words in above sentence are numbered 4838, 5900, and 7615, respectively. When the article node is visited, the word vector of the three words is (4838, 5900, 7615). Because we set GRU to bidirectional, the vector (7615, 5900, 4838) is also implemented. Different datasets have different word bag sizes and word numbers, but the way of ordering is the same.

Our proposed word attention mechanism is based on the contextual relevance between the current word and previous words. To catch this information, WGAT utilizes a bi-directional gated recurrent unit (GRU) model [9], which is similar to long short-term memory (LSTM) [19] that can model the sequence data but performs better on smaller and less frequent datasets [10]. GRU contains a series of gated units, each of which takes the output of the previous unit as the input of itself. That is, use the information of previous words to calculate the representation of the current word. WGAT feeds all the word vectors (ww1,ww2,…,wwm)T into a GRU model and sets the number of units of GRU to be m (the same as the number of words on the node). Thus, for a different node, m could be different. As Fig. 1 shows, WGAT feeds the tth word vector wwt(1≤t≤m) into the tth unit in the GRU model. The calculation of the tth unit is as follows:

zt=sigmoid(Wz⋅(hht−1∥wwt)),
(2)
rt=sigmoid(Wr⋅(hht−1∥wwt)),
(3)
hh~t=tanh(Wh⋅(rt∗(hht−1∥wwt))),
(4)
hht=(1−zt)∗hht−1+zt∗hh~t.
(5)
Here, zt is the ratio of update gate and rt is the ratio of reset gate. The operator ∥ is the concatenation. hht−1 is the output of the (t−1)th unit in GRU. At unit t, WGAT first concatenates the output of the immediately preceding GRU unit hht−1 and word vector wt, and then calculates the ratios of update gate and reset gate using Eqs. (2) and (3), respectively. Wz and Wr are the parameters for the update gate and reset gate. Then, WGAT calculates the candidate activation vector hh~t by Eq. (4), where Wh a parameter matrix and tanh(x)=(e2x−1)/(e2x+1) is the hyperbolic tangent. Finally, the output of unit t hht can be obtained by Eq. (5). For simplicity, we encapsulate the above calculations into a function GRU(). Since the calculations can go either from the first unit to the last one (forward) or form the last unit to the first one (backward), WGAT conducts this bidirectional computation, denoted by

hh→t=GRU−→−−(wwt,hh→t−1),
(6)
hh←t=GRU←−−−(wwt,hh→t−1),
(7)
where the top arrows stand for the direction of calculation. Finally, WGAT re-defines the output of unit t as the concatenation of two directional output vectors, i.e., hht=hh→t∥hh←t. Therefore, the entire output of the GRU model forms a new matrix representation for the features of a node as follows:

H=(hh1,hh2,…,hhm)T,H∈Rm×2du,
(8)
where du is the dimension of the output vector of a GRU unit.

To better represent a node with word vectors, WGAT further introduces a self-attention mechanism, which assigns word vectors with different attention weights, defined as follows:

aa=softmax(wwatanh(W1HT)),
(9)
where W1∈Rda×2du is parameter matrix and wwa is a parameter vector with size da. Thus, aa is also a vector with size m, where element ai represents the attention weight of hhi. In this study, we use the standard sotfmax function: RK↦RK, defined as follows:

softmax(zz)i=ezi∑Kj=1ezj for i=1,…,K and zz=(z1,…,zK)∈RK.
(10)
Then, the compressed vector representation of a node can be obtained by summing up the word vectors (hh1,hh2,…,hhm) with their attention weights (a1,a2,…,am), i.e., mm=aaH=∑mi=1aihhi. However, such an attention vector aa can only reflect a particular importance distribution. Actually, there could be multiple importance distributions inhabited in the words on a node. Therefore, the WGAT model requires to obtain several sets of attention weights, namely multiple hops of attention. For this sake, the WGAT model extends the above wwa to a matrix Wa∈Rr×da, where r is the number of attention hops. By this means the model can get r different attention vectors, formally,

A=softmax(Watanh(W1HT)),
(11)
where the softmax function performs on each row vector of its input matrix and A∈Rr×m is the word attention matrix. A new 2D matrix representation of a node can be obtained by the multiplication of the word attention matrix and the GRU output matrix as follows:

M=AH,M∈Rr×2du.
(12)
Graph attention
In graph learning, the importance of neighbors is another key point. Since the graph attention weights of a node are calculated based on the node and its neighbors represented by the above 2D matrix, we first compute all the word attention matrices for a node set V, i.e., M=(M1,M2,…,Mn). Note that Mi(1≤i≤n) has r rows representing different word attention distributions. Thus, WGAT also has r different graph attention operations for a node, each of which takes place on the particular counterpart rows of the matrices of the node and its neighbors. In this manner, as Fig. 1 shows, it forms a stacked graph attention layer. Each graph attention operation on this layer will aggregate the neighbor information of a node. Now, we describe the details of a graph attention operation.

First, we denote each row vector in Mi as mm(k)i, i.e., Mi=(mm(1)i,mm(2)i,…,mm(r)i)T. To represent a node after the neighbor aggregation, we first impose a linear layer upon each row vector in Mi, denoted as mm(k)iW(k)g(1≤k≤r), where W(k)g∈R2du×du′ is a weight matrix applied on mmki shared by all neighbors of the node. For two nodes i and j that link together, WGAT calculates their coefficient against each row by introducing a learnable shared attention vector bb with size 2du′ as follows:

α(k)ij=exp(LeakyReLU(bb[mm(k)iW(k)g∥mm(k)jW(k)g]T))∑h∈Niexp(LeakyReLU(bb[mm(k)iW(k)g∥mm(k)hW(k)g]T)),
(13)
where Ni is the neighbor set of node i (including node i itself), calculated from the adjacency matrix L¯. Here, αij is the attention weight of the neighbor j connected with node i. In this study, the leaky rectified linear unit (LeakyReLU) activation function is defined on a vector xx=(x1,…,xK)∈RK as follows:

LeakyReLU(xx)i={0.2xixi if xi<0 if xi≥0 for i=1,…,K.
(14)
Having the graph attention weights, the new word-and-graph attention representation for the k row of Mi (i.e., node i) can be calculated by aggregating its neighbors as follows:

mm(k)i′=LeakyReLU⎛⎝∑j∈Niα(k)ijm(k)j⎞⎠.
(15)
For total r rows of Mi, we have a new word-and-graph attention representation matrix of a node as follows:

Mi′=(mm(1)i′,mm2(2)′,…,mm(r)i′)T,
(16)
where Mi′∈Rr×du′ and usually du′<2du.

If we want to continue compressing the representation matrix Mi′ to a lower-dimensional space, we can use Mi′ as the input for another stacked graph attention layer. That is, we can adopt multiple stacked graph attention layers one after another. Our implementation of WGAT only uses two stacked graph attention layers, which means the final 2D word-and-graph attention representation matrix of node i will be Mi′′=(mm(1)i′′,mm(2)i′′,…,mm(r)i′′)T, where Mi′′∈Rr×du′′ and du′′<du′. To generate the input vector for the rear part of WGAT, we concatenate all the row vectors of Mi′′ as a new vector to preserve the information of multiple attention hops, resulting in a final embedding vector of a node:

uui=∥k=1rmm(k)i′′.
(17)
Therefore, the length of uui is rdu′′.

Rear Part of WGAT
The word-and-graph attention feature representation vector uui will be finally fed into the rear part of the WGAT model. The structure of the rear part of the network is shown in Fig. 2. uui is first fed into a liner layer, whose output layer contains c (the number of concept classes) neurons. Immediately after the linear layer is the softmax activation function layer, whose output is the estimated probability of each concept class of the current node i. If node i is unlabeled, these probabilities are its predicted class distribution. If node i is labeled, in the training phase WGAT calculates the cross-entropy loss against its true labels as follows:

loss=yyi(softmax(WuuuTi+bbu)),
(18)
where yyi is the label vector of node i, Wu∈Rc×(rdu′′) is the linear weight matrix, and bbu is the bias vector in linear layer with length c. The training process utilizes the Adam optimizer [20] to minimize the loss.

Fig. 2
figure 2
The rear part structure of the WGAT model

Full size image
Training Algorithm
The proposed WGAT is end-to-end. In each training epoch, all the labeled nodes pass through the network. We summarize the training process in Algorithm 3.1. First, the algorithm initializes the graph to generate the neighbors of each labeled node. Here, the neighbors of a node could include some unlabeled nodes. Therefore, the training process can be viewed as semi-supervised learning. Then, for each labeled node, it passes through the network one after another. The process was described clearly.

figure f
Experiments
To evaluate the proposed model, we conducted extensive experiments on three citation networks. In this section, we first present the experimental settings and then show the experimental results and provide some analysis.

Experimental settings
Datasets
We use citation networks to evaluate our model, where each node is an article and if two articles have a citation relation they are connected by an edge. The three citation network datasets are as follows:

Cora [26]. In Cora, all the articles can be divided into 7 categories, which are taken as the values of node labels. The abstract of each article is used as the feature of this article. In order to ensure the accuracy and applicability of the feature, we remove some low-frequency words, such as some inflectional auxiliaries and punctuation marks. Meanwhile, we set the size of the word bag to 12619. Finally, this dataset retains 2211 nodes and 5214 edges.

Citeseer [23]. In Citeseer, all the articles can be divided into 10 categories. We use the title of each article as its feature. This dataset contains a number of invalid articles which have an impact on the results of the experiment. In the meantime, this dataset retains 4610 nodes and 5923 edges. Finally, we set the size of the word bag to 5523.

DBLP [35]. In DBLP, each node represents an article coming from 4 different research areas. This dataset contains 13404 nodes and 39861 edges. The abstracts of the articles serve as their features. In the end, we set the size of the word bag to 8501.

The edges in all the datasets are unweighted. More characteristics of the datasets are listed in Table 1. Network Cora is denser than networks Citeseer and DBLP. Citeseer has more classes and the distribution of classes is more imbalanced. DBLP has the largest network scale.

Table 1 The characteristics of the datasets used in the experiments
Full size table
Methods in comparison
We compared our proposed WGAT model with eight state-of-the-art methods. We simply introduce these compared methods as follows:

DeepWalk [28] is a graph embedding method, which performs the random walk on the graph and treats the walking path as the equivalent of sentences. After parameter tuning, we set the number of walks starting at each node to be 80, the length of paths to be 40, the window size to be 10, and the dimension of the node embedding to be 100.

Node2vec [15] is a method based on DeepWalk, which performs the biased random walk on the graph. We set the number of walks, the length of paths, the window size, and the dimension of node embedding the same as they are in DeepWalk. The biased random walk parameter p and q is set to be in the range of [0.1,0.9].

LINE [34] makes two-node embedding vectors close or apart based on their first and second-order proximity. In our experiments, we only use the second-order proximity and set the embedding dimension of nodes to be 200 and the number of negative sampling to be 5.

GraRep [7] is a method that uses the matrix factorization and high-order adjacency matrix to obtain the node embedding. We set the highest-order adjacency matrix to be 7 and the dimension of the node embedding to be 100.

MMDW [37] is a unified network representation learning framework that jointly optimizes the max-margin classifier and the aimed social representation learning model. The embedding not only contains the structure information but also has the characteristic of discrimination. The length of random walks is set to 3, and the balancing parameter is set to 0.001.

GCN [21] is a method that uses the variants of convolutional neural networks on the graph and encodes both local graph structure and features of nodes. We set the dimension of node embedding to be 64 and use a 2-layer structure.

GraphSAGE [17] is a general inductive framework that leverages node features to efficiently generate node embedding for previously unseen data. There are four different aggregators. We use the GCN aggregator in our experiment. The embedding dimension is set to 64.

GAT [38] leverages node features by different attention weights. We use a 2-layer GAT model. The multi-head attention is set to 8 and the dimension of output in the last GAT layer is also set to 8.

For our WGAT model, the dimension of the word embedding dw is set to be 500, the hidden dimension of GRU du is set to be 100. The parameters of word attention r and da are set to be 8 and 100, respectively. Two parameters du′ and du”are set to be 100 and 8, respectively. These settings are the same as those of GAT. Finally, the learning rate of the Adam optimizer is set to be 0.001.

Evaluation metric
We conducted the experiments in a semi-supervised manner. First, the whole dataset was randomly divided into a training set and a test set. The test set contains 1000 nodes. For the training set, we randomly hid the labels of a portion of nodes. That is, we used a portion of labeled nodes and their neighbors as the training sets. The datasets preparation and the training process repeated 10 times, and the average performance is reported. Since the evaluation is a multi-class classification, we used the Micro-F1 score as the evaluation metric.

Experimental results
Overall evaluation
First, we let 10% proportion of the nodes in the training sets of all three datasets have labels and used micro-F1 to evaluate the experimental result. The experimental result is shown in Table 2, where the best performance is marked in bold. Second, compared with the three GNN-based methods, we further evaluated our WGAT model on different training set settings. We increased the percentages of the labeled nodes in all three datasets from 20% to 50%. Third, considering that only using the micro-F1 score as the evaluation metric could be potentially biased, we added the macro-F1 score as the second metric to investigate whether the WGAT would still have the best performance among the mentioned GNN-based classification models.

Table 2 Experimental results in terms of the micro-F1 score (in percentage)
Full size table
Refer to Table 2, it can be found that our WGAT model significantly outperforms the other methods in all datasets. Particularly, compared with GAT which uses the one-hot vectors as the node features, our WGAT model further captures the word attention information hidden in the node features and obtains a better performance in semi-supervised classification tasks. Compared with the results on Cora and Citeseer, those on DBLP show a greater improvement, which indicates that our WGAT has more advantages on large datasets. Further, we found that the traditional nonGNN-based embedding methods (DeepWalk, Node2Vec, LINE, and MMDW) generally perform worse than the GNN-based methods (GCN, GraphSAGE, GAT, and WGAT) on Cora and Citeseer but perform slightly better (except our WGAT) on DBLP. Both GAT and our WGAT have a graph attention mechanism, which makes them better than the other two GNN-based methods (GCN and GraphSAGE). In short, our WGAT takes both advantages of the word and graph attention information.

Considering that the performance of the nonGNN-based models like DeepWalk is far lower than the GNN-based models, our next experiment only evaluates the GNN-based models. Since our model is semi-supervised, we let the labeled node rates vary from 20% to 50%. To make an unbiased comparison, we use both the micro-F1 score and macro-F1 score as evaluation metrics. The experimental results of GNN-based models are listed in Tables 3 and 4.

It is easy to found that whether we use micro-F1 or macro-F1 to evaluate experimental results, our WGAT model consistently outperforms the other GNN-based models in most training set settings. Again, we also noticed that on the large dataset DBLP, our WGAT is more advantageous with the maximum increment of 5.4 (20% labeled nodes in micro-F1 score). Another interesting phenomenon in the experiment is that as the percentages of the labeled nodes increase, the performance of all models consistently increases, which indicates that the percentage of the labeled nodes will help the performance improvement of models. However, we found that the advantage of our model gradually weakened during this process. This may be because a large number of labeled nodes have made the information available for classification in the network sufficient. This phenomenon also in turn shows that our WGAT model has a more significant advantage at low label percentages. In real-world applications, the low label percentage networks are more common than the sufficient labeled ones.

Parameter tuning
To verify the robustness of our WGAT model, we adjusted the hyperparameter settings to investigate the impact of hyperparameter values on the performance of the model. In the experiment, we tuned the attention hops r, the dimension of word embedding dw, and the hidden dimension of GRU du. Still, we evaluate WGAT on the above three datasets and let 10% of the nodes have labels. When we adjusted one hyperparameter, we keep the other two the same as they are in Sect. 4.1.2. That is, when we changed the attention hops, we kept the dimension of word embedding dw as 500 and the hidden dimension of GRU du as 100; when we changed the dimension of word embedding, we kept attention hops as 8 and the hidden dimension of GRU du as 100; when we changed the hidden dimension of GRU, we kept attention hops as 8 and the dimension of word embedding dw as 500. The experimental results are listed in Table 5.

Table 3 Experimental results with different labeled node rates (micro-F1 in percentage)
Full size table
Table 4 Experimental results with different labeled node rates (macro-F1 in percentage)
Full size table
Table 5 Experimental results under different hyperparameter settings in terms of micro-F1 score (in percentage)
Full size table
The number of attention hops defines how many vectors are used for a node when constructing its 2D matrix representation in WGAT. It is supposed to have more information if the number of attention hops is greater. Thus, to investigate its influence, we set its values as 1, 4, 8, 12, and 16. From Table 5 we can find that on dataset Cora, the model obtains the best performance when attention hops are 4 and 8. After that, as the number of attention hops increases, the performance no longer increases even slightly declines when the number is too large. It indicates that too many representation vectors probably result in the redundancy of the information, which impairs the degree of discrimination of the nodes in the classification. But compared to the result when it is set to 1 (i.e., do not use the attention hop), the result under 16 is still promoted 4 points. The same phenomenon can be observed on datasets Citeseer and DBLP, but the decline of the performance is not significant when the numbers of attention hops are set to 12 and 16. The maximum difference is only 1.1 points.

The dimension of word embedding dw is another important hyperparameter in our model, which determines the length of word representation vectors (that will be optimized in the training) in the initial transformation. To investigate the impact of the hyperparameter, we set the dimension of word embedding to be 200, 250, 500, 750, and 1000. For datasets Cora, Citeseer, and DBLP, the best performance is obtained at 1000, 500, and 250. It seems that the small dimension of word embedding (here is 200) will result in a mediocre performance. However, the differences in the performance under different settings are not significant. Because the larger dimension will result in a longer training time, we usually set it to 500.

The hyperparameter of the hidden dimension of GRU (du) controls the length of the output vectors of GRU units. In the experiment, we set the hidden dimension to be 50, 100, 150, 200, and 250. Thus, the length of the output vectors of GRU units is twice the setting values because the model uses the bi-directional GRU, which combines the two-directional output vectors together. The best performance in the micro-F1 score on datasets Cora and Citeseer is consistently obtained at the hidden dimension setting of 100, and on DBLP, the best performance is obtained at the hidden dimension setting of 150. Again, under different settings, the differences in the performance are not significant. To sum up, the proposed WGAT model is robust to different parameter settings. We can obtain good performance according to the suggested values in this paper.

Conclusion
In this paper, we present novel word and graph attention neural networks, which jointly consider the word and neighbor attention mechanism for each node. By assigning different attention weights to the features of nodes, the model can obtain better representation for nodes in a 2D matrix form. The following graph attention operations aggregate the neighbor information with different attention weights to form a 2D matrix representation for each node, which can preserve the structure information. Finally, the concatenation of rows of 2D matrix presentation is fed into the linear layer to optimize. Experimental results on three article citation network datasets consistently show that our proposed model significantly outperforms eight state-of-the-art methods in semi-supervised classification tasks. Thus, the word and graph attention mechanism does benefit the model.

Although our model has achieved good performance, in future research, we still expect better progress in the proximity of graph nodes. At present, mainstream graph embedding models still use the first-order or second-order proximity to define the similarity of two nodes in the graph. Nodes should also be close to each other in the embedding space. However, in addition to the two proximity, there are still some special structures that have not been fully considered. For example, how to integrate motifs [5], one of the most common high-level structures in the network, into the network embedding, is still an open problem. In addition, we can consider using a more complex node-local structure to define the proximity of two nodes. The current assumptions of the first-order proximity and second-order proximity are based on the assumption of the node–pair relationship. This assumption can be well applied to certain applications, such as link prediction. However, it cannot retain the centrality information of nodes because the centrality of nodes is usually related to more complex local structures. These problems still need to be addressed by hard work.