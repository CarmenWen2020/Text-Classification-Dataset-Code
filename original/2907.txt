Provenance is recognized as a central challenge to establish the reliability and provide security in computational systems. In scientific workflows, provenance is considered essential to support experiments’ reproducibility, interpretation of results, and problem diagnosis. We consider that these requirements can also be used in new application domains, such as software processes and IoT. However, for a better understanding and use of provenance data, efficient and user-friendly mechanisms are needed. Ontology, complex networks, and software visualization can help in this process by generating new data insights and strategic information for decision-making. This paper presents the Visionary framework, designed to assist in the understanding and use of provenance data through ontologies, complex network analysis, and software visualization techniques. The framework captures the provenance data and generates new information using ontologies and structural analysis of the provenance graph. The visualization presents and highlights inferences and results obtained with the data analysis. Visionary is an application domain-free framework adapted to any system that uses the PROV provenance model. Evaluations were carried out, and some evidence was found that the framework assists in the understanding and analysis of provenance data when decision-making is needed.

Access provided by University of Auckland Library

Introduction
Provenance data can be considered the information about the parts involved in the production of an object [1]. It is recognized as a central challenge to establish reliability and provide security in computational systems, particularly on the Web [2]. For scientific workflows, provenance is considered essential to support both the reproducibility of experiments and the interpretation of results and diagnosis of problems [3]. For the same reason, several research domains have studied provenance data and its benefits, such as system files [4], software development [5], cloud computing [6], semantic web [7], database [8], security [9] and IoT [10]. Besides, with the use of provenance models, such as OPM [11] or PROV [1], the interoperability of these data is facilitated, aiding reuse and cooperation among working groups.

Understanding provenance data and the processes by which it is captured is a difficult task [12, 13]. Software visualization techniques can help by intuitively communicating the main aspects of complex data to the user [14]. However, only using visualization mechanisms does not guarantee an adequate understanding of provenance data since the volume of information is vast and valuable. New techniques are needed to assist in its investigation and understanding [15]. Inference mechanisms, ontologies, and complex network analysis can help identify important information derived from raw data. Using these techniques together, we believe that it is possible to improve the understanding of the information, the reliability of the results, and their reuse.

This work presents a framework to visualize and analyse provenance data to stimulate the use of provenance and aid in its comprehension and analysis. We use ontologies and complex network approaches to support data analysis and present the results through visualization mechanisms. The framework, named Visionary, is designed to be flexible and adapt to different application domains, such as software development, business transactions, or scientific workflows. An earlier version of our approach was presented in [16]. This paper presents its evolution (including improvements in analysis and visualization features) and its evaluation.

Although many researchers deal with provenance data [17], most of them focus on the collection and semantic analysis of provenance information [18, 19]. Today, many tools use provenance data [20], but few are concerned with the ease of use and understanding for a non-specialist audience or casual users [21]. This paper details the following aspects: (1) discusses proposals in the literature that deal with provenance analysis and visualization; (2) presents an approach that analyses and visualizes provenance information to facilitate its understanding; and (3) Details how to generate new knowledge from provenance data.

To conduct this investigation, we used a Design Science Research methodology [22]. In the design science paradigm, the knowledge and understanding of a problem domain and its solution are achieved in the building and applying the designed artefact. The artefact evaluation then provides feedback information and a better understanding of the problem to improve both the product's quality and the design process.

Therefore, considering the design science research guidelines [22], our approach proposes the creation of a framework (creation of an artefact), named Visionary, to assist users in decision-making as it facilitates the understanding of provenance data (problem domain). It uses provenance data principlesFootnote1 to analyse data and processes used to derive these data. Visionary includes an architecture that encompasses provenance and visualization layers and analysis mechanisms, favouring understanding the data and their analysis. The framework is designed to be flexible and capable of adapting to different application domains. Our approach is innovative because it uses implicit information, i.e. information discovered from inference algorithm processing, structural analysis of complex networks, and visualization techniques to help in data understanding and decision-making using these data (the creation of an innovative purposeful artefact). We analysed proposals in the literature that deal with provenance analysis and visualization. We did not find any that presents an approach that analyses and visualizes provenance information to facilitate its understanding and emphasizing the discovery of implicit information about provenance data (a problem space and a mechanism posed or enacted to find an effective solution). To collect evidence of our approach's feasibility, an evaluation was conducted with data from scientific projects (an evaluation of the artefact is crucial), and a pilot study was conducted with software process data. The results point to the feasibility of the proposal in both domains. It is important to emphasize that one of our primary concerns is also the reproducibility of our findings. Therefore, all data used in the evaluations are also available at.Footnote2

The following research question was formulated from this scope definition: “How can the Visionary framework support decision-making through the analysis and understanding of provenance data, considering implicit information?”.

This article is divided into five sections, including Introduction. Section 2 presents the main concepts related to the proposal. Section 3 describes the methods and materials, including two systematic literature reviews and the Visionary framework detailing. Section 4 presents the evaluation. Section 5 concludes the paper, presenting final considerations and future works.

Research background
This section presents the main concepts used to detail the Visionary framework. One of the first authors to define data provenance was Buneman et al. [12]. They defined it as the description of a data object's origin or the process by which it arrived in a database. A more recent and precise definition was presented by Moreau et al. [23]: the documentation of the life cycle processes of a digital object.

We can distinguish two types of data provenance: retrospective and prospective [24]. Retrospective provenance models the execution of workflows (processes) and data derivation information, such as the tasks performed and how the data artefacts were derived. Prospective provenance models an abstract specification of workflows (processes) as a recipe for the future derivation of data. The Visionary framework deals with both types.

Several provenance models have been proposed in the literature [1, 25,26,27], but the interoperability between these models is considered deficient [28]. Two of these models stand out and have become a standard for provenance data capture and are widely used: the OPM (Open Provenance Model) model [11] and the PROV model [1]. Both are generic models. Figure 1 shows the main elements of the PROV model. The OPM is simpler, focused on controlling execution flows and capturing its provenance, an essential point in scientific experiments, indicating that a specific process was initiated by another. In the meantime, PROV focuses on responsibility and data history issues, having several relationships between agents and other types (activities and entities). Therefore, PROV is more detailed, based on the assumption that it has more relations than OPM and provides ontology and specific constraints.

Fig. 1
figure 1
PROV model with basic types and their relations [29]

Full size image
The PROV ontology,Footnote3 named PROV-O, expresses the PROV Data Model (PROV-DM) using OWL2 (Web Ontology Language) [30]. PROV-O provides a set of classes, properties, and constraints that can be used to represent and to exchange provenance information generated in different systems and different domains.

A PROV model can also be considered a graph, especially a complex network, considering that it has a set of connected nodes. Complex networks, also known as graphs, are structures formed by a set of nodes, also called vertices, and a set of connections between them, called edges [31]. Graphs are weighted when the edges have values. In the context of provenance data, the generated graphs are oriented,Footnote4 multi-relational,Footnote5 and not valued since the relations do not have weight but have different types (qualities).

Over the years, scientists have developed an extensive set of tools for analysing, modelling, and understanding complex networks. These tools can be used to, with some calculations, provide useful information. In this context, the calculations performed in the networks are obtained from some measurements and metrics. One of the metrics in node analysis is the node degree, which is often defined as local centrality. The centrality defines the most important node in the network. There are other ways to calculate centrality, such as the closeness and betweenness metrics [32].

Another essential concept is the similarity between vertices. There are two fundamental approaches in constructing graph similarity measures, called structural equivalence and regular equivalence. Two vertices in a graph are structurally equivalent if they share many of the same neighbours. Regular equivalence is harder to determine. Two vertices that have regular equivalence do not necessarily share the same neighbours. Nevertheless, they have similar neighbours, i.e. they are of the same type, or they have other characteristics that are similar between them [32].

Methods and materials
To conduct our research, as stated before, we used a design science methodology approach [22]. First, we identified the problem relevance as ‘Understanding the provenance and the processes by which it is captured is a difficult task to accomplish”. As such, we investigated the domain area in order to conduct a Search Process. Systematic literature reviews were conducted to find proposals in the literature that deal with provenance analysis and visualization, i.e. identifying the problem domain and the existing solutions on the field (Sect. 3.1). Then we proposed the Visionary framework (Sect. 3.2). Therefore, our research contributions can be summarized as the proposal of an approach that analyses and visualizes provenance information to facilitate the understanding of provenance data and generate new scientific knowledge from these data.

To collect evidence of our approach's feasibility, an evaluation was conducted with data from scientific projects. The evaluation results are presented in Sect. 4. The results point to the feasibility of the proposal.

Systematic literature review: provenance visualization and provenance analysis
Systematic Literature Reviews (SLRs) are essential pillars when employing an evidence-based paradigm in software engineering. In addition to SLR, snowballing has recently arisen as a potentially efficient alternative or complementary solution. A hybrid search strategy combining searching in specific digital libraries with backward and forward snowballing can be interesting. This section presents the related works found from two SLRsFootnote6 [34]. The first review focused on the visualization of provenance data, while the second one focused on provenance analysis. Next, we present a summary of the main findings. The complete reviews can be found at.Footnote7

Provenance visualization systematic review
Considering the visualization of provenance data, after the publication of the OPM [11] in 2007 and PROV [1] in 2013, these models' use was expected in most of the papers found. Nevertheless, many of them did not use these provenance models, only proprietary models. Only [35] use the OPM model, and [15, 18, 36], and [16] work with the PROV model. Thus, PROV is the most used model in 4 of 15 publications.

In this direction, Ragan et al. [37] present an organizational framework of the different types of provenance information and purposes for why they are desired in visual analytics. Our work can be classified as rationale and meta-analysis, according to [37] work. Ragan et al. [37] stated that few projects focus on using provenance to support meta-analysis as we do in our work.

Stitz et al. [38] present an approach that retrieves analysis states that are structured as provenance graphs containing automatically recorded user interactions and visualizations. Stitz et al.’s [38] research operates on interaction provenance, according to [37], which contains the history of user interactions with the system and the tightly coupled visualization provenance, which comprises the history of visualization states. Our work has as its main differential the emphasis on inferred provenance data, derived from the processing of inference algorithms and complex networks structural analysis.

As a summary of the SLR, Table 1 summarizes the main functional requirements found in the principal related papers.

Table 1 Functional Requirements found in publications listed in the systematic review
Full size table
Besides the functional requirements described in Table 1, other elements must also be considered, such as the temporal visualization generated by PROV-O-Viz [15] and AVOCADO [46]. They are specific to the analysis of information flow, which restricts the analysis and application context. The PROV model's use is also fundamental for the visualization system's flexibility, which was found in a few works.

Although the SLR approach results are essential and reach major publishing vehicles in computer science, many important works related to provenance visualization were published in specialized workshops that were not returned by digital libraries' search engines. Thus, from the snowballing technique, it was possible to find essential works that deserve to be discussed. Macko and Margo [47] present a tool for interactively exploring large provenance graphs using graph summarization and semantic zoom. Although it provides a semantic zoom, it does not use semantic information or inference algorithms to discover and present new provenance data. The work on provenance data visualization from Vistrails group [48] and other visualization tools related to Workflows Management Systems [49, 50] brought important advances in the area, allowing easy integration between provenance collection and analysis. However, they do not support provenance data generated by other workflow management systems or standalone provenance gathering or other tools. Comparing the visualizations provided by Vistrails and Visionary, our work emphasizes the semantic processing of provenance information, using inference algorithms, and presenting new information. Vistrails has other functionalities and focuses on its specific workflow provenance data.

Since the PROV model has already established symbols for encoding elements, coding in other visual patterns is unnecessary. In the case of AVOCADO [46] and PROV Viewer [18], the user must know the PROV model. The Visionary framework was developed to improve the usability of the PROV provenance model for new users. The framework includes all the functional requirements for visualization presented in Table 1, except the adjustment and maintenance of previous visualizations (Automatic Layout). However, these adjustments are not the goal of Visionary, which was built to aid in data analysis and decision-making support.

Provenance analysis systematic review
Considering the second review, related to provenance data analysis, the search string used in the SLR [34] was generated with the concatenation of the search terms: (\provenance") AND (\analy*" OR \inference*" OR \assessment*") AND (\tool*" OR \software*" OR \program*" OR \system*" OR \model*" OR \process*"OR \framework*" OR \method*" OR \technique*" OR \approach*"). Customizing the search string to specific digital libraries, 24 publications were found. From the quality assessment activity, we obtained 9 articles. PROV stands out as the most used model, it is used in 4 of the 9 publications, and OPM is the second most used in 2 of the 9 publications.

Ceolin et al. [51] present a combination of reputation and provenance to determine reliability. Analyses performed by Visionary cover more aspects than reliability, such as the importance of the graph elements, which provides more information for decision-making.

Cheah and Plale’s [35] proposal presents a method to evaluate the quality of provenance graphs generated with the OPM model. This proposal is complementary to our work since it acts in the quality of the provenance itself and the Visionary framework acts in the quality of the artefacts represented by the provenance. McGrath and Futrelle [52] combine rules written in SWRL with an OWL representation of provenance modelled with OPM. Similar work was detailed in [53], which presents a PROV graph codification, rules, and restrictions validated by a DLV (Disjunction Logic Validation) machine. Different analyses can be used in provenance data at the ontological level or considering complex networks. The Visionary framework presents the two approaches. It explores the inferences of the ontology (OWL2) and complex network analyses.

Prat and Madnick [54] present a provenance model and an approach to compute credibility based on provenance metadata. The provenance data credibility is computed with simple distance and similarity metrics used to generate other metrics and results. The Visionary approach is based on a provenance standard model (PROV), which encourages reuse and allows integration with other techniques. Graph analysis is also based on complex network metrics and can be exploited outside a specific context.

The work presented by [55] implements inference rules based on provenance to reduce the amount of provenance information and to aid quality control. Visionary focuses on the generation of new knowledge from provenance data.

Therefore, from the approaches identified, some analyses are domain-specific, and others use specific provenance models. Some analyses deal with the quality of provenance metadata, but do not provide the user with information to improve the application that generates the metadata.

The results of the two systematic literature reviews show that none of the works meet all the requirements found in the SLRs. Therefore, we considered it necessary to specify a framework to provide provenance analysis with the support of a visualization mechanism, i.e. the creation of an innovative purposeful artefact according to the design science methodology.

Thus, our work proposes the Visionary framework that attempts to meet the requirements listed in SLRs (Table 1). It also meets the following requirements:

Acts on the PROV model, which is a popular and extensible model.

Provides strategic information to improve the analysed processes.

Provides results without dependence on data for training.

Operates in different application domains.

Visionary framework
The framework usage has five distinct steps, as illustrated in Fig. 2: (1) Capture, (2) Ontology, (3) Data Transformation, (4) Data Analysis, and (5) Visualization.

Fig. 2
figure 2
Visionary framework steps. The data capture from repository (1), ontology processing (2), the data transformation for the visualization (3), data analysis through complex networks (4), and finally the visualization step for user exploration

Full size image
Visionary was designed to be flexible and capable of adapting to different application domains. The Visionary framework captures the provenance data from executed processes, manipulates, and analyses the data to visually present information to the user. Through the visualizations, users can explore the data to understand and analyse the processes that originated the provenance data. Thus, the user gains more knowledge and support to extend, correct, and modify the original processes.

Step 1: Data capture
In the data capture, the analysed application domain (management processes, software processes, scientific workflow, among others) must be previously adapted to capture provenance data. The application to be used to execute the process must be “instrumentalized”; that is, components to capture specific provenance data must be developed and coupled to the application to capture and store the provenance data in a specific repository. Costa et al. [5] present an example of this step of the Visionary framework. The framework provides native support for provenance data defined according to PROV-DM specifications and can be adapted to use other PROV-specific extensions, such as ProvONE [56] or others [57, 58].

The PROV's extensions that specify the applications' domain are diverse and, therefore, vary according to the type of application used. The author of the extension must adjust the steps whenever necessary. In this way, the framework is open to receive other application-specific extensions.

Suppose the provenance model is not modified for a specific domain. In that case, Visionary will continue to obtain results of the analysis, even if in a more general way, i.e. results are generic considering generic provenance types (agent, activity, and entity). If the provenance is captured considering specific extensions, the model will provide results considering specific types (like workflow, task, among others, considering scientific workflow domain, and ProvONE model extension, for example). Therefore, adapting Visionary will allow results that are also specific to the domain used.

Step 2: Ontology
In this step, provenance data are analysed to help users to find new information. The provenance data are processed in two stages of analysis: the PROV-O ontology and complex network analysis algorithms. In the first analysis, data modelled with PROV-DM or domain-specific extensions are loaded into the PROV-O ontology that provides a set of classes, properties, and constraints to aid in analysing provenance data. All the inferences generated with the ontology are used in step 5 (Visualization) and are highlighted.

Therefore, the inferences are related to the classes, object properties, and rules of the PROV-O ontology in general and the classes, object properties, and rules of the specific application domain (i.e. software process or scientific workflows). Pellet reasoner [59] and OWLAPI framework were used to process the inferences. In addition to generating new knowledge, this step also prepares the provenance data for the next analysis stage.

An important point to highlight is the use of the “influence” relationship. As Moreau and Missier [29] argue, all relations contained in PROV are those that generate some level of influence. The influence between the elements is fundamental during the analysis of provenance graphs. Therefore, it is essential to map the chain of influence generated from the relationships between the elements defined in PROV.

Thus, in addition to PROV-O's basic relations, Visionary uses the “influence” relationship to define the influence of the relations. This relationship tracks influence during the analysis phase. The domain and the range of this relationship can be any entity, activity, or agent. The relationship “influenced” has the inverse relationship called “wasInfluencedBy”. The relation, or property, “wasInfluencedBy” is a wide-ranging relationship and often is replaced by one of its subproperties. These associations in the ontology are illustrated in Table 2.

Table 2 Generation of the “influenced” relationship from the basic relations of the PROV
Full size table
The importance of the “influenced” relationship in the analysis phase allows any PROV user who made PROV extensions to a specific domain to apply this relation whenever necessary to create new relations of influence. For example, if the management relationship (wasManagedBy) is created to map the management of a group or organization by a specific person, we must associate the influenced relationship as an inverse one to wasManagedBy. Thus, the provenance graph analysis can track the wasManagedBy relationship and achieve more accurate results in the specific domain context.

An example of an ontology adapted to a specific context is the PROV-Process ontology [60]. This ontology is an extension of PROV-O, adapted to the software processes domain, and uses the Visionary Framework for analysis on the provenance graph. In PROV-Process, three property chains are defined over the basic relationships that can be used within Visionary. The three property chains generate the wasAssociatedWith relationship, defined as:

1.
used o wasAttributedTo SubPropertyOf: wasAssociatedWith

2.
wasStartedBy o wasAttributedTo SubPropertyOf: wasAssociatedWith

3.
wasEndedBy o wasAttributedTo SubPropertyOf: wasAssociatedWith

The wasAssociatedWith relation is one of the subproperties of the wasInfluencedBy relation and is, therefore, an opposite relation to the influenced relation, as shown in Table 2. Therefore, PROV-Process can be used together with the Visionary framework without modifications. This is possible because the base relation in Visionary is the influenced relationship, allowing that Visionary can analyse influence specializations.

The same occurs with the ontology presented in [58], a version of the PROV-O extended and adapted to the context of scientific experiments and workflow configuration management. In addition to creating new classes, two relationships were created (evolutionOf and evolutionTo) and are inferred from other existing relationships (wasDerivedFrom, specializationOf, and alternateOf). The wasDerivedFrom relation is a subproperty of wasInfluencedBy, inverse to influenced relation. The specializationOf relation is defined as a subproperty of alternateOf, and to define the inverse relation to the influenced relation, we define alternateOf as inverse. Thus, the two inferred relationships would be linked to the two influencing relationships. The Visionary Framework can be used in this context to expand the analyses carried out for the maintenance and evolution of scientific workflows and associated experiments.

The new relations and classes do not change Visionary's operation, as long as the three primary classes defined in the PROV model (entity, activity, and agent) are maintained. Also, the new relations must be specializations of influence relationship.

All inferences generated with the ontology are used in step 5, being highlighted in the visualization. In addition to generating new knowledge, this step also prepares the data for the next analysis stage. Considering the domain of software process and scientific applications, extracted from the evaluations conducted, we present some examples of inferences processing results.

Software Process Considering an inference rule (Rule 1), specified in SWRL [61], that can be applied to Software Process Provenance Data, an example of its operation is presented. This inference states that if an activity ac was associated with a stakeholder sta and this activity ac generated an artefact art, the relation between the stakeholder and the artefact art can be inferred. Figure 3 shows an example to explain the inferences' processing (the inferred associations appear in red). Even if there is no explicit and direct relation in the provenance data between Mary and Payment_Test_Cases, we can infer, using the rule presented by Inference 1, that Mary created Payment_Test_Cases.

Fig. 3
figure 3
Software process inferences example

Full size image
Rule 1

prov:wasAssociatedWith(?ac, ?sta) ^ prov:generated(?ac, ?art) -> provswprocess:created(?sta, ?art)	 
The same mechanism occurs for the domain of the scientific experiment, i.e. through inference algorithms, implicit knowledge can be derived, such as experiments and workflows similar to each other or derived from each other; SWMS used for the workflow's execution (when they are not explicitly informed); researchers, institutions and research groups involved or influencing the experiment; workflows or external services used in the experimentation process; data consumed and generated by the experiment, among others.

Step 3: Data transformation
In step 3, the ontology data are loaded and stored in a graph format to proceed to the framework's network analysis. With provenance data in graph format, it is possible to extract characteristics generated by the PROV model to describe its structure. This description can be used to identify similarities between nodes and the influence range of each node.

Step 4: Data analysis
With the provenance data in graph format, it is possible to extract characteristics generated by the PROV model to describe its structure. This description can be used to identify similarities between nodes and the influence range of each node.

Ebden et al. [62] present several characteristics of the provenance graphs to analyse graphs' evolution, considering time. Four different metrics were selected by [63], such as (1) number of nodes, (2) number of edges, (3) diameter, and (4) maximum finite distance (MFD) [62]. Huynh et al. [63] use these metrics to characterize each node's influence subgraph and infer the quality of other nodes from predetermined quality information.

The Visionary framework specifies an algorithm that uses these metrics but does not need previous information to determine the nodes' quality. Visionary improves Huynh algorithms. Visionary's algorithm determines the regularity similarity between the nodes by analysing the subgraph that influences the node. The graph is divided into several subgraphs related to each node, and through its characteristics, it is possible to relate the generated subgraphs and, consequently, the related nodes. Each edge of a PROV graph represents the influence between the origin node and the destination node [29].

The user can then identify an element or process that deserves to be highlighted, positive or negative, and thus relate the identified node with the same dependency structure through the graph analysis. As an example, to understand the importance of such an analysis, we can consider that when identifying an element or process in the application subject to an error, this analysis allows us to find nodes that represent the same types of elements or processes that may have the same error due to the similarity of the dependency graph since provenance deals with data quality and reliability [29]. The same can happen with elements with the high quality confirmed by the user since the framework finds similar elements, and the user can increase the importance of these elements in the application, consequently increasing the importance of the respective nodes in the provenance graph.

With the same principle as the dependency subgraph, we can determine an influence subgraph containing only the nodes potentially influenced by a given node a. This analysis goes beyond the proposals of Huynh et al. [63] and Ebden et al. [62] and presents a new way of determining each node's importance within the provenance graph. The importance of node a is determined by the node's ability to influence the other nodes in the graph, and the metrics are used to calculate the degree of influence.

To assist users in understanding the application, the subgraph analysis is highlighted in the visualization. With the graph representing the network of influence of node a, the user can identify the impact of deleting or modifying the element or process represented by that node. An option in the view allows the nodes' highlight (changing their size) according to their influence on the network. This feature allows the user to quickly identify the most influential nodes in the network and make decisions.

The application of these analyses varies according to the specific domain. Thus, while in one domain application, the user may need to increase the influence of a node in the graph for better results, in another domain application, it may need to decrease the influence of the nodes on the graph, leaving the graph with fewer connections to obtain better results. In this way, the framework and its resources can improve understanding of the application, enabling better reuse and cooperation between partners and reliability. A detailed description of the metrics used in Visionary can be found at.Footnote8

To highlight the functionality of the analyses, we present three examples of the use of these analyses:

In a comprehensive scientific workflow, the data appear to have been modified incorrectly. The scientist, analysing some of the services used, discovers an error in one service and believes that other services are also unreliable. Analysing the provenance data, the scientist identifies the mapped service as an activity, and through the Visionary Framework, finds activities of similar quality. Finally, the scientist focuses his efforts on checking errors in the activities highlighted by the framework to find other poor quality services.

In order to optimize work, one of a company's processes must be replaced. The responsible manager checks the provenance data to identify the impact of the change on other processes. Using the influence graph analysis, the manager discovers that many processes and data artefacts are impacted by the change and decides to review the plans or consider the change with the other managers.

Unable to check the entire system due to the stipulated deadline, an analyst decides to use the Pareto principle (principle 80/20) and check only 20% of the system that is most important. To identify this group of elements, the analyst goes to the provenance data and discovers, through the analysis of the nodes' influence, the most critical elements involved in the system and checks them.

Step 5: Visualization
Provenance data are presented in a graph format with visual resources for navigation and comprehension. This last step presents the information generated by the ontology and data analysis steps in a user-friendly and understandable way. Figure 4 shows a visualization generated by the framework. At the top of Fig. 4, the visualization option is highlighted, as indicated by the number 1; on the left-hand side are the symbols and analysis controls, as indicated by the number 2; filter controls are indicated by the number 3; and the search for nodes is indicated by the number 4. On the right-hand side, in the larger area, the provenance data in graph format is indicated by the number 5. The visualization option allows the user to switch between source code (Source Visualization), general metrics of provenance data (Data Chart Visualization), and graph visualization (Graph Visualization, using PROV or BPMN symbols).

Fig. 4
figure 4
Visualization generated by the framework

Full size image
Figure 5 presents the source code (Fig. 5a) and metrics (Fig. 5b) views provided by the framework. The source code view (Fig. 5a) displays the data as a list of nodes and edges. This option allows a direct search for the name of the elements and helps solve syntax problems and character encoding. The metrics option (Fig. 5b) displays the total number of nodes and edges broken down by type. Two sets of symbols are used to encode the type of each node. The first set uses the PROV notation. The second set uses the same symbols used in BPMN (Business Process Model and Notation) [64]. However, other symbol sets can be added. This feature allows the visualization to be understood more quickly by the user without knowing or learning new symbols to interpret the data. A filter control allows the user to filter the information displayed on the screen. This function helps to reduce information overload, primarily when visualizing dense graphs in a small space. The search for nodes option highlights the chosen node by name. Up to two nodes and their direct relationships can be highlighted with this option. The search filters out all other nodes and relationships and compares two elements or visually analyses the relationship between two nodes.

Fig. 5
figure 5
Two visualization options present in the framework

Full size image
The viewing area can be zoomed in to focus on a specific area of interest and panned to assist in graph provenance navigation. The icons have different levels of colour, according to the degree of each node. Figure 6a presents three different tones to represent tasks with different degrees. The smaller the degree, the lighter the node's tone is. By clicking on the node, a panel with available information about the node and its connections is displayed (Fig. 6b). The thickness of the edge represents the number of its internal links, and the colour of the edge represents the types of links. These are green, yellow, or red for the links affirmed, affirmed and inferred, and only inferred, respectively. The inferences generated by the ontology are highlighted to attract the user's attention to new information and possible important discoveries about the data.

Fig. 6
figure 6
Visualization features used to aid in the understanding and exploration of provenance data

Full size image
The network analysis (step 4) is presented in two different ways in view.Footnote9 The subgraph of the influence is presented by modifying the size of the nodes. The larger the node, the greater its influence on the provenance graph, as shown in Fig. 7a. The user can activate this option to identify critical points of the application, and it displays the influence subgraph of the node allowing the user to analyse the impacts of modification, maintenance, and replacement of the objects represented by the node. The dependence subgraph analysis identifies nodes with a similar structure and presents percentages in the view. These values represent similarity and help the user to identify nodes similar to the analysed one. As provenance represents data quality and reliability, this feature presents nodes with similar quality and reliability. Figure 7b shows the similarity feature used in the visualization.

Fig. 7
figure 7
Visualization resources provided by the analyses of influence subgraph and the dependence of each node

Full size image
Evaluation
The Design Science Methodology emphasizes the importance of proper evaluation. Hevner et al. [22] state that the selection of evaluation methods must be matched appropriately with the designed artefact and the selected evaluation metrics. A case study in Software Engineering is an empirical inquiry that draws on multiple sources of evidence to investigate one instance (or a small number of instances) of a contemporary software engineering phenomenon within its real-life context, especially when the boundary between phenomenon and context cannot be specified [34].

Considering Visionary Framework, we consider that a case study is the best instrument because the evaluation aims to verify if the framework offers an adequate mechanism for the analysis and understanding of provenance data and if it supports the decision-making process.

Section 4.1 presents the results of a pilot case study that helped improve the Visionary framework, carrying out a case study. Section 4.2 presents the case study, describing its planning, the study subjects, how the data collection was done, and the results analysed. Threats to validity are also presented.

Pilot study
A pilot study was carried out using the Visionary framework, which was integrated with the PROV-Process architecture [60]. PROV-Process is an architecture that aims to identify improvements in software processes and present them to the project manager through a service-oriented application. The PROV-Process architecture [60] used the Visionary framework to process, analyse, and visualize provenance data. In this vein, PROV's activities are related to software processes' tasks, entities in PROV are related to process data that are processed/used by the tasks, and agents in PROV are software developer/managers or software applications.

The evaluation scope was defined based on the GQM (Goal/Question/ Metric) method [65] as follows: “Analyse the PROV-Process architecture in order to characterize it with respect to the feasibility of supporting decision-making considering software development processes, using software process provenance data from the point of view of project managers and software developers in the context of software processes”.

The information used in this pilot case study is related to the software development process executions. The company evaluated deals with creation/maintenance of accounting systems, and it has been in the market for more than 25 years. The process evaluated in this pilot case study considers the development of a specific subproduct of an Enterprise Resource Planning (ERP) System, using the SCRUM method [66]. For this ERP subproduct creation, five different sprints were executed. Data from other activities involved in SCRUM (such as sprint planning, daily meetings, sprint review) were not provided by the company and therefore are not included in the analysis.

Planning
The study was based on real data collected from development projects conducted in the company. The name of the company was omitted and is treated as company I for confidentiality reasons. The subjects were software developers with at least 5 years' experience.

The data were collected using questionnaires and interviews. The research had direct contact with the subjects involved, and the collection was also done in real time. Semi-structured interviews were used, considering that the focus is on analysing company processes and Visionary use.

To conduct the study, the following instruments were selected: (1) consent from the subjects to enable the publication of the collected data in this work; (2) profile background questionnaire; (3) training material to support the training session performed with the subjects before the study began; (4) a document containing the explanation of the task that was established for the subjects; and (5) a follow-up questionnaire in order to collect data after the subjects had accomplished the task.

Visionary was made available for subjects to accomplish the task. The study was carried out with ten subjects. Two were project managers, four software developers of company I; one Ph.D. student with software process experience; and three computer science postgraduate students who are also software developers. We consider that knowledge in the area, together with the experience in software development, facilitates understanding the information presented by Visionary.

The analysed data are from a process that deals with error handling and implementation of new features in an ERP Project. It is performed by six different roles (client, test team, support, support manager, development manager, and programmer).

From the data requested from the company, they did not provide the procedures and resources used. Stakeholder's names have been hidden to preserve their privacy. Figure 8 shows the used process flow model with its activities and roles. This model was used to capture process prospective provenance. From this process model, we only obtained data about five specific activities: System Error Report, New Feature Request, Case Registration, Case Resolution, and Close the Case.

Fig. 8
figure 8
Flow model with activities and roles

Full size image
Execution
The generated provenance graph with the SDP data from the ten process instances is shown in Fig. 9. The orange pentagons represent the reported stakeholders, executed activities are the blue rectangles, and the artefacts and roles correspond to the yellow ellipses. After generating the visualization presented in Fig. 9, filters were applied to this graph to facilitate its interpretation.

Fig. 9
figure 9
Ten instances overview

Full size image
Results
Through the results obtained from the PROV-Process evaluation, it was possible to identify some specific aspects considering the use of the Visionary approach. These results are summarized in Fig. 10, and the questions that generated these results are detailed in.Footnote10

Fig. 10
figure 10
Answer assertiveness for the process analysis

Full size image
Some of the results are presented below:

Regarding whether the (inferred) information presented to subjects about a particular activity presents strategic information about this activity, 10% of participants disagreed partially, 10% indicated indifference, 40% partially agreed, and 40% fully agreed. This result indicates that there is an agreement that the inferences improve the information available for the analysis of the activity (regarding Question 5).

Based on the question that inferred information in an agent detailing presents new information about the agent’s participation in the process, 10% of participants disagreed partially, 40% indicated indifference, 20% partially agreed, and 30% fully agreed (regarding Question 6).

Results show that 10% of the participants disagree partially, 40% agree partially, and 50% agree totally, considering that it is possible to identify activities, agents, and entities more easily using graphic visualization. These rates show that the visualization helped the subjects. Therefore, it should be reiterated that, due to many instances used, many relations between the nodes were displayed, which, to a certain extent, delayed the location of the desired information (regarding Question 7).

Regarding the statement that the graphical visualization allows faster analysis of software development processes' execution data, 10% of the participants disagreed partially, 10% indicated indifference, 70% partially agreed, and 10% agreed totally. The greater percentage indicating partial agreement can be justified by many relations, which makes it difficult to see the bigger picture (regarding Question 9).

At the end of the evaluation, open questions about the approach and the PROV-Process tool were presented. Analysing the answers to the question—"What did you like most about the PROV-Process approach?", 50% of the subjects mentioned the discovery of new information through inferences (regarding Question 13).

Finally, when they were asked about what they would change in the PROV-Process tool, considering only visualization problems, 90% of the subjects indicated the addition of some filter and/or search, 10% suggested the insertion of subtitles, and 10% the implementation of the zooming function. The complete study is available in [57].

Based on these results, flaws pointed out by the subjects were considered in the improvement of the Visionary approach and will be evaluated in a case study. This pilot study served to demonstrate the technical feasibility of the model, concepts, and technologies involved in the approach and adjustments in the source of data collection. As an initial study, it helped unveil areas for improvement and errors that should be corrected in a regular case study.

Case study
After the pilot study, some aspects of the approach have been modified, and other features have been added. This case study was used to evaluate specific aspects of the Visionary framework, such as (1) the assistance in the comprehension and analysis of provenance data, (2) the generation of new information, (3) the use of the interface and the use of visualization resources.

Planning
The scope of the evaluation was defined based on the GQM (Goal/Question/Metric) method [65], which is: “Analyse the Visionary approach with the purpose of verifying its ability to assist in the understanding and analysis of provenance data with respect to the support in decision-making from the point of view of users and developers in the context of scientific applications.”

A specific research question (RQ) was formulated:

RQ How can the Visionary framework support decision-making through the analysis and understanding of provenance data?

This major research question generated three supporting secondary research questions (SQ):

SQ1 Is the Visionary framework easy to use?

SQ2 Does the Visionary Framework aid in provenance data understanding?

SQ3 Does the Visionary framework assist in provenance data analysis?

The Visionary connection service was coupled with the E-SECO architecture [67]. E-SECO is a scientific software ecosystem [67] that allows the prototyping, maintenance, and execution of scientific workflows. During the execution of workflows and their modifications, E-SECO captures the provenance data, and the Visionary framework accesses these data.

Therefore, the subjects were instructed to run scripts for pre-established activities using Visionary. Four different provenance data setsFootnote11 with their respective activitiesFootnote12 were made available to each subject.

Five subjects participated in the regular case study, signing the Term of Free ConsentFootnote13 and the characterization questionnaire.Footnote14 Subjects were scientists who work in a renowned scientific agricultural institution, with experience in using scientific workflows and scientific applications.

Execution
The group's participation was to evaluate the understanding and the analysis of provenance data through Visionary. The characterization questionnaire results show subjects with different levels of knowledge in the areas and provenance data.

To better understand the study, it is essential to use different data collection methods [68]. Three methods of data collection were established during this study. The first method was of a first-order and semi-structured type, where subjects completed four activity scripts, named RoadMap 1, 2 3, and 4 in.Footnote15 Each roadmap had 12 different and specific activities for the first data set. Data setsFootnote16 are provenance data related to scientific software, with the first set with 56 nodes, the second with 64 nodes, and the third and fourth data set with 410 and 327 nodes, respectively. This data collection method was intended to verify the accuracy of the subjects' responses to each activity type.

The second method was of a second-order and unstructured type, where the researcher made an observational analysis of the subjects taking note of relevant events. This method was intended to verify the quality of the user interface, the use of visualization resources, and the identification of difficulties in locating functions and information. The third method was of a first-order and semi-structured type, where participants answered an evaluation questionnaireFootnote17 after the activities were carried out. This method was used to verify the subjects' perceptions about the Visionary’s support in the proposed activities. Besides, the questionnaire also accepted answers to discursive questions.

The four activity scripts had three groups. Activities 1, 2, and 3 of each roadmap were general information used to analyse the interface and display features. Activities 4, 5, 6, and 12 were for investigation and understanding of the data and helped to verify whether the results were presented clearly and objectively. Activities 7, 8, 9, 10, and 11 supported data analysis and to verify whether the new information presented was useful and helped in decision-making. Activity scripts are available at.Footnote18

Results analysis
In this section, we present the evaluation results, using the three collection methods, separately. The precision metric was used to evaluate the activities conducted along with the execution time of the scripts. The Precision metric [69] deals with the accuracy of the information returned by the participant, which is calculated by Eq. (1).

Precision=Number of correct answersTotal answers given
(1)
All responses were compiled, and the precision of the obtained results is summarized in Table 3. Initially, we can highlight the average precision of the activities. Activities that stood out positively for accuracy were activities 1, 3, and 10. Activities 1 and 2 responses are shown directly at the interface, which justifies its high accuracy, but this contrasts with the low precision (unexpected) of Activity 2. Activities 3 and 10 can be executed with a visual analysis of the data (observing the Visionary’s interface; the answer can be found). Even without training in the use of the framework, the subjects executed the activities successfully. Figure 11 shows the precision of the subjects' responses to each activity.

Table 3 Precision of subjects’ responses in each activity for each activity roadmap5
Full size table
Fig. 11
figure 11
Accuracy of activities' responses. The red line shows the average of values, the dashed line the deviation (color figure online)

Full size image
The activities were also analysed according to the type of the questions: general information (activities 1, 2, and 3), investigation and comprehension (activities 4, 5, 6, and 12), and analysis and decision support (activities 7, 8, 9, 10 and 11). Table 4 presents the precision obtained for each type of question.

Table 4 Precision of subjects' responses in each type of activity of the roadmap
Full size table
Considering the different types of activities, the general information activities has the highest precision with a value of 0.93. These results reflect the visualization features, indicating a good result, but there is a need for minor adjustments, such as more information about the displayed data and subtitles, to some of the visualization resources.

The investigation and comprehension activities were an essential part of the evaluation scope and obtained the worst results with a precision of 0.75. Activity 12 had an unexpected result, as shown in Fig. 11. It can be considered an outlier; its value is beyond a standard deviation (highlighted by the broken line in Fig. 11) of the average of all the results. It also indicates interference in the result of the activity since an error in the elaboration allows double interpretation of the activity, leading to diverse responses from the subjects. Even disregarding the results of activity 12, investigation and comprehension activities obtained 0.85 in precision. Although it is the smallest precision in the scope of this type of activity, the result is more significant than a random response with two options (precision 0.5), also considering that the responses were open. Therefore, we can consider that there is some evidence that Visionary assists in the investigation and understanding of the provenance data, although we cannot generalize the results.

The analysis and decision support activities obtained a precision of 0.92. It was close to the first group of activities, i.e. a good result. One of the possibilities for this high precision is that these activities have more comprehensive answers than the others. In any case, the questions' results indicate that Visionary assists in the analysis and support of decision-making.

The execution time of the roadmaps was also recorded. Table 5 presents a summary of the times. Of all the subjects, the time spent on roadmap 2 was significantly lower than on roadmap 1. The reduction varies between 37.5 and 64.29%. This variation reflects Visionary's learning curve since no instruction was given for the subjects to understand the Visionary resources. The time spent increases between the second and third roadmaps, reflecting the difference in the provenance data size from 64 nodes from roadmap 2 to 410 nodes in roadmap 3. Another reduction occurs from roadmap 3 to roadmap 4; this timeless expressive reduce 0% to 43.48%. After dealing with a larger number of nodes, the reduction in time spent on roadmap 4 was predictable.

Table 5 Time of accomplishment of activity roadmaps by each participant
Full size table
As a summary of this result's analysis, we can consider that the preliminary assessment of activity precision provides evidence that the framework helped understand and analyse provenance data and supports the decision-making process. A need for adaptation has been identified, such as presenting additional information to the user on demand and user interface modifications.

Other suggestions and needs considered the extension of some functionalities, including creating a help system to facilitate understanding the resources. The improvement of the similarity feature considering structural equivalence can improve this functionality's results and needs to be tested. The presentation of numeric values for “importance” computation was also suggested by subjects and can be easily added in the Data Chart view.

Evaluation of subjects
The evaluation questionnaireFootnote19 answers registered subjects’ perception of Visionary's importance in carrying out the activities. The questionnaire has 31 questions, where 28 are objective, and 3 are discursive. The following analysis can be done in light of data sources triangulation and data collection methods. These analyses are based on interviews and direct observations.

Focusing on the Visionary’s objective, subjects were asked if Visionary assists in understanding provenance data and inquires whether the framework assisted in each activity’s execution. Figure 12 shows the comparison of the direct and indirect answers about the understanding of the provenance data.

Fig. 12
figure 12
Comparison between the evaluation questionnaire's direct and indirect answers on the understanding of provenance data

Full size image
Considering direct questions, 80% of the subjects stated that they fully agree that Visionary helps understand provenance data, and 20% agree partially, as shown in Fig. 12. Similar results occurred when the question was about help in the understanding and exploitation of data. In this case, 75% totally agreed, and 25% partially agreed. We can therefore consider that there is some evidence that the approach assists in activities related to the understanding of provenance data.

Answers related to the analysis and decision support questions reveal more significant differences, as shown in Fig. 13. Considering direct questions, 80% of the subjects stated that they fully agree that Visionary helped analyse provenance data, and 20% agree partially, as shown in Fig. 13. In indirect questions, 4% totally disagreed, 8% were indifferent, 28% partially agreed, and the vast majority, 60%, totally agreed that Visionary helped in provenance analysis. Therefore, the subjects considered that, in general, Visionary assisted in the analysis and decision support. This indication reveals that the framework adequately supports the provenance data analysis and supports decision-making based on these data, although modifications are necessary.

Fig. 13
figure 13
Comparison between the evaluation questionnaire's direct and indirect answers to data analysis and decision support

Full size image
Therefore, considering these findings, we have evidence that Visionary can assist in the comprehension and analysis of provenance data, with the help of the visualization mechanisms, and the generation of new information, with the support of inference mechanisms and complex network analysis.

The generation of new information is possible considering the inference mechanism that acts on provenance data, generating new connections (relations on the provenance graph) between data. This new information is generated based on the connections between data that did not exist before inference processing. Besides, the complex network metrics can also provide new insights on provenance data, improving its comprehension.

Results discussion
This study was specified, focusing on the research questions presented in Sect. 4.2.1. Based on the obtained results and from the triangulation of data sources, the following analyses can also be performed considering the secondary research questions:

We can answer positively SQ1 (Is the Visionary framework easy to use?), considering the evidence found in the methods of data collection, such as (1) despite the lack of training of the subjects, they answered positively to most of the activities (first and second methods of data collection) and (2) in answer to the question “Do you consider Visionary to be an easy-to-use tool?”, a total of 60% of subjects agreed with the question (third method of data collection).

SQ2 (Does the Visionary Framework aid in provenance data understanding?) could also be answered positively based on (1) successful completion of comprehension activities, as Table 4 shows with 0.75 precision in these activities (first method of data collection) and (2) in the direct and indirect questions of the evaluation questionnaire, as shown in Fig. 9, with more than 75% of the answers fully agreeing with the affirmative answer to the research question (third method of data collection).

The last secondary research question (SQ3) can be answered positively, considering (1) the successful conclusion of the analysis activities, as presented in Table 4 with a precision of 0.92 in these activities (first method of data collection), and (2) in the direct and indirect questions of the evaluation questionnaire, as shown in Fig. 13, with more than 88% of the answers agreeing partially or totally with the affirmation that Visionary framework assists in the analysis and decision support (third method of data collection).

In this way, we have evidence to answer the main research question, considering that the Visionary framework supports decision-making by analysing and understanding provenance data.

Threats to validity
The study's validity is related to the reliability of the results, considering whether the results are biased or not, from the researchers' subjective point of view [70].

Construct validity The proposed activities may not have been comprehensive enough to address all the needs of users and developers who need to understand and analyse provenance data. A more comprehensive study would aid in the evolution of the framework and provide more sustainability to the relationships established in this study.

Internal validity The results obtained are still preliminary, and although they indicate a positive conclusion, a more detailed study, including statistical methods, is valuable to present additional findings.

External validity The study was presented in the PROV-Process architecture context, Scientific Workflow Management Systems context and E_SECO scientific software ecosystem platform and the results cannot be generalized. They are limited to the context and data sets used.

Reliability Complete details of the study execution were not presented, but the documentation guarantees the evaluation's relative reproducibility.

Conclusions
This work presented the Visionary framework, which aims to understand and analyse provenance data using ontologies, complex networks, and visualization resources. A Design Science Research Methodology was used to conduct the research, and two systematic literature reviews were carried out. The first was related to the topic "Visualization of Provenance Data", and the second was related to "Analysis of Provenance Data". The results revealed deficiencies in the literature approaches and opened an opportunity for research in this domain.

The Visionary framework was developed considering the PROV model. Visionary encompasses the PROV model's characteristics and is generic and flexible to be adapted to different application domains. Analyses using ontologies and complex networks can be used in any extension made in the model, considering that its basic characteristics are maintained.

A preliminary evaluation of Visionary provided evidence of its ability to assist in the understanding and analysing provenance data, supporting users in decision-making, and improving the processes that generated the data. Nevertheless, the framework needs to be further explored, expanded, and evaluated.

The main contributions of the work are highlighted as follows:

Systematic Literature Reviews considering the two main research areas involved in the approach, i.e. visualization of provenance data and provenance data analysis. From these reviews, it was possible to identify the foremost works in the area and present these works' main contributions. It was also possible to identify specific requirements not yet addressed by the literature and, from these, identify new functional requirements that gave rise to the Visionary framework's proposal.

The specification and implementation of functionalities related to the analysis of the provenance graph. They can be used in any context and extension of the PROV model, capable of providing strategic knowledge considering provenance data.

An open-source and web-based approach designed to visualize provenance data to simplify the understanding and exploration of data.Footnote20

An evaluation in the software development processes (pilot study) and in the scientific workflow domain (case study).

Some limitations of the approach and the implementation of its resources must also be highlighted.

An evaluation with more subjects should be conducted to support better the results obtained with the evaluation study.

Information visualization is overwhelming, with a vast data set. Despite the filtering, highlighting, and localization functionalities having already been implemented, it is convenient to specify a semantic zooming system capable of selecting the data.

Analyses of the graph also present a delay with larger data sets. In this case, it is essential to optimize the algorithm and/or strategies for analysis.

The Visionary framework can be expanded in several ways:

Specification of new algorithms and rules for graph analysis allows the generation of new information to amplify the analyses currently carried out.

New views can be specified. Multiple views will offer different points of view of the data, facilitating their exploration and understanding.

A semi-automatic adaptation and installation system can be developed, allowing PROV users to quickly and easily configure their system's framework.