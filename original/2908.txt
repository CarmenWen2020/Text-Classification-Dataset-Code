Question answering over knowledge graph (KGQA), which automatically answers natural language questions by querying the facts in knowledge graph (KG), has drawn significant attention in recent years. In this paper, we focus on single-relation questions, which can be answered through a single fact in KG. This task is a non-trivial problem since capturing the meaning of questions and selecting the golden fact from billions of facts in KG are both challengeable. We propose a pipeline framework for KGQA, which consists of three cascaded components: (1) an entity detection model, which can label the entity mention in the question; (2) a novel entity linking model, which considers the contextual information of candidate entities in KG and builds a question pattern classifier according to the correlations between question patterns and relation types to mitigate entity ambiguity problem; and (3) a simple yet effective relation detection model, which is used to match the semantic similarity between the question and relation candidates. Substantial experiments on the SimpleQuestions benchmark dataset show that our proposed method could achieve better performance than many existing state-of-the-art methods on accuracy, top-N recall and other evaluation metrics.

Access provided by University of Auckland Library

Introduction
With the rise of several large-scale knowledge graphs such as YAGO [34], Freebase [4], NELL [8] and DBpedia [25], people are seeking effective approaches to access the substantial and valuable knowledge in KG. Even though some formal query languages, such as SPARQL [37] and GraphQL [15], are designed for querying KGs, it is difficult for end-users to understand the syntax of the formal query language. Alternatively, question answering over knowledge graph (KGQA) is proposed. KGQA system can return the concise answer generated by querying KGs when users ask questions in natural language.

In this paper, we mainly focus on handling single-relation questions, which can be answered by just one fact in KG. Single-relation question answering task can be regarded as the foundation of handling much more complex questions, e.g., multi-relation questions [9] and conversational questions [32]. Single-relation task is still far from being solved, and it continues to face four major challenges: polysemy problem, ambiguity problem, lexical gap problem and same name problem.

Polysemy problem: The same word or phrase appears in different context may express different meanings, e.g., the word “Apple” in the question “Who is the CEO of Apple?” refers to the “Apple Corporation” instead of the fruit “apple.”

Ambiguity problem: A word or phrase can be linked to multiple entities in KG, e.g., the word “Chicago” can be linked to “City of Chicago” or “Chicago Bulls.”

Lexical gap problem: Relations expressed in the question can be quite different from those used in KG, e.g., the relation “place_of_birth” can be expressed as “Where was ... born?” or “What is the birthplace of ...?”.

Same name problem: As the size of KG keeps growing, many entities share the same names, which makes it impossible to distinguish them in literal.

To overcome above challenges, previous efforts apply semantic parsing-based methods, where the natural language question is converted into a logic query over the KG. These models [17, 18] obtain competitive performance at the cost of manual annotations and hand-crafted features, but these methods are difficult to be generalized to other domains. In recent years, with the development of deep learning, several approaches develop increasingly complex end-to-end neural network frameworks [19, 23, 44] for KGQA task. However, as pointed out by Mohammed et al. [29], the gains from sophisticated deep learning techniques are quite modest and that some previous models exhibit unnecessary complexity. Moreover, such end-to-end frameworks take long execution time to converge when training the networks and it is difficult to conduct elaborately stepwise performance analysis in end-to-end setting. Therefore, some work [6, 27, 30] explores simple yet effective pipeline architectures, which mostly decomposes the KGQA task into three subtasks, i.e., entity detection, entity linking and relation prediction.

Pipeline-based methods enable a comprehensive analysis that provides insights about each component of the training models. Thus, our work follows the line of pipeline approaches.

In this paper, we aim to answer the following research questions. (1) How to extract the entity mention from the question? (2) How to distinguish the correct entity from plausible wrong entities? (3) How to calculate the correlation between the relation and question? Following these questions, we propose a pipeline framework for KGQA, which consists of an entity detection module, an entity linking module and a relation detection module. Specifically, for entity detection, we train a sequence labeling model to label the entity mention span in the question. For entity linking, we first adopt the string matching-based entity linking method as a foundation. To mitigate entity ambiguity problem, we take the contextual information of candidate entities into consideration and build a question pattern classifier according to the correlations between question patterns and relation types, and then, a novel entity linking method is proposed to get the top-N ranked entity candidates. For relation detection, we propose a simple yet effective attention-based model to detect the relation. If both the entity and relation are correctly predicted, we will get the answer by querying the KG.

In summary, our key contributions are presented as follows:

To capture the correlations between question patterns and relation types, we formulate it as a multi-label classification task and examine several basic text classification models.

We consider the contextual information of each candidate entity in KG, in which extra information can help to distinguish the entities with same names or aliases and improve the performance of entity linking.

We propose a simple but effective attention-based model to detect the semantic similarities between the question and relation candidates.

Our proposed method achieves competitive results on the SimpleQuestions benchmark dataset with two subsets of Freebase. Moreover, we present an empirical error analysis to gain insights into the mistakes of our model.

The rest of the paper is organized as follows. Section 2 lays out the research scope and introduces related work. Section 3 formally formulates the problem and presents the overall architecture of the proposed solution. Section 4 describes experimental settings and presents the results to illustrate the effectiveness of our proposed method. Finally, Sect. 5 concludes the paper.

Related work
There are two mainstream research routes for the KGQA task: semantic parsing-based methods [3, 18, 35] and neural network-based methods [12, 22]. Semantic parsing-based methods first map questions to logical forms and then execute the structured queries within KGs. For instance, Yih et al. [39] define a query graph as the meaning representation and the query graph generation process is formulated as a staged search problem. Zheng et al. [43] utilize binary templates to understand questions and then devise an index to facilitate the online template decomposition. Hu et al. [18] propose relation-first and node-first frameworks to build the semantic query graph. These approaches are effective in question semantic parsing, but need complex grammars, rules or manual annotations.

Neural network-based methods first represent questions as low-dimensional vectors and then score the similarities between questions and the candidate facts to find the best match. More specifically, these methods can be divided into pipeline frameworks and end-to-end frameworks.

In pipeline frameworks, Dai et al. [10] employ a conditional factoid factorization by inferring the target relation first and then the target subject associated with the candidate relations. Yin et al. [40] use the char-CNN and word-CNN to perform entity matching and relation matching, respectively. Mohammed et al. [29] decompose this problem into entity detection, entity linking, relation prediction and evidence combination. They also evaluate various strong baselines with simple models. Qu et al. [31] divide the task into entity detection and relation detection, a heuristic method is used to get entity mention in the question, and then, an attentive RNN with similarity matrix-based CNN model is utilized to capture the semantic-level and literal-level similarities. Huang et al. [19] propose a knowledge embedding based framework, which trains a predicate learning model to get the predicted predicate representation, and then, a head entity learning model is used to predict the question’s head entity representation. Zhao et al. [42] improve subgraph selection through subgraph ranking method and propose a joint-scoring CNN model to capture the subject-relation dependency. [24] divide their method into topic unit generation, topic unit scoring and relation path ranking steps. Pipeline frameworks usually combine the entity detection and relation detection methods to solve KGQA tasks. Although pipeline frameworks face the problem of error propagation, these methods achieve high accuracy on the SimpleQuestions dataset.

In end-to-end frameworks, Golub et al. [16] use a new character-level, attention-based encoder–decoder model for question answering. The model is robust for unseen entities because of character-level modeling. Hao et al. [14] adopt an end-to-end neural network model to represent the questions and their corresponding scores dynamically via cross-attention mechanism. Lukovnikov et al. [28] use an end-to-end neural network, which merges word-level and character-level representations of words to exploit the advantages of both. Wang et al. [38] propose the APVA-TUBRO approach, which includes a verification mechanism responsible for checking the correctness of predicted relations. Zhang et al. [41] build a probabilistic modeling framework for end-to-end KGQA system, which could handle uncertain topic entity and multi-relation reasoning. Most of these models consider multi-level representations of words and apply attention mechanism to enhance the performance.

Our proposed pipeline framework for KGQA
In this section, we will describe the proposed pipeline framework for KGQA, which includes three cascaded components: (1) An entity detection module for identifying the entity mention in the question. (2) An entity linking module for generating entity candidates in KG that the question refers to. (3) A relation detection module for measuring the semantic similarity between question and relation candidates. The overall process is illustrated in Fig. 1.

Fig. 1
figure 1
The overview of our proposed pipeline framework for KGQA

Full size image
Freebase is utilized as our background KG. The knowledge graph is depicted on the left-hand side of Fig. 1, where each circle represents an entity and each edge is the corresponding relation between two entities. In Freebase, each entity has a unique ID, which is referred to as the Machine Identifier, or MID. For example, the MID “fb:m:06qtn” is named as “Sub-Saharan Africa.”

The right-hand side of Fig. 1 illustrates the entire process, which can be decomposed into three stages. Given the question “What is the time zone in Sub-Saharan Africa?”, the entity detection module first identifies the entity mention “Sub-Saharan Africa,” then we replace the span “Sub-Saharan Africa” with a special token “⟨head⟩” to generate the question pattern “What is the time zone in ⟨head⟩?”. Next, entity linking module links the identified entity mention to the corresponding nodes in KG by calculating the literal-level and structural-level matching degree. For instance, the entity mention “Sub-Saharan Africa” is linked to two entities in KG, e.g., “fb:m:04whzt2” and “fb:m:06qtn.” Afterward, relation detection module is employed to compute the semantic similarity between question pattern and relation candidates. It is obvious that the relation “time_zone” has the highest score with respect to the question pattern “What is the time zone in ⟨head⟩?”. Finally, based on the predicted entity and relation, we form a query to retrieve the KG for the correct answer “Western European Summer Time.”

Task definition
The goal of KGQA is to find a single fact that could answer the question. To facilitate this, we define some notations used in this paper. Let G be a knowledge graph that consists of a large number of triples to describe the facts in the real world. We use E and R to represent the set of entities and the set of relations in G, respectively. Then, a triple ⟨s,r,o⟩ corresponds to a fact, where s,o∈E and r∈R.

For each single-relation question (simple question), a common assumption is that it can be answered by just one triple. Hence, only the tuple ⟨s,r⟩ is needed to match the question. As long as its head entity s and relation r are both correctly identified, then we can take the tail entity as the answer. Formally, given a question q, which is a sequence of words w1,w2,…,wn, we aim to design a pipeline framework that takes the question q as input and returns a triple ⟨s^,r^,o^⟩ as the answer. According to the above definitions, this task can be solved by the following three steps:

Given a question q, we need to find out the entity mention m in the question.

The entity candidate set Ce is comprised of all the entities in KG that associate with the entity mention m, and all the relations connected to the entity candidates make up the relation candidate set Cr.

We score the semantic similarities between the question pattern p and relation candidates Cr. Then, the entity e and relation r with the highest score will be considered as the final answer.

Entity detection
Given a question, the goal of entity detection is to identify the consecutive tokens of mention span which refers to the topic entity in the question. This problem can be treated as a sequence labeling task which assigns a class or label to each token in a given input sequence. To this end, we train a BiGRU–CRF model to label the entity mention span in the question. In addition, begin–inside–outside (BIO) tagging scheme is used to represent a label attached to each word, where “B” denotes the beginning of an entity, “I” represents the inside of an entity and “O” is non-entity regions. For instance, the gold label sequence “{O, O, O, O, O, O, B, I}” is assigned to the given question “What is the time zone in Sub-Saharan Africa?”. Next, we will present the details of the BiGRU–CRF model and its constituent components, i.e., BiGRU and CRF.

BiGRU
Gated recurrent units (GRUs) are a special type of recurrent neural networks (RNNs) and are designed to overcome the gradient vanishing problem of RNNs. In particular, GRUs have additional memory cells, which store memory from long-distance terms. To capture the flow of information both ways, we apply BiGRU which can obtain the sequence information from past and future tokens. Specifically, we derive a forward hidden state ht→ and backward hidden state ht←. Then, we combine above hidden states using a concatenation operation ht=[ht→;ht←]. The sequence of hidden states obtained by BiGRU is briefly denoted as:

H=BiGRU(x)=(h1,h2,…,hn)
(1)
where x={x1,…,xn} denotes the input sequence, n is the length of x and H is the matrix of hidden states.

CRF
Conditional random field (CRF) has been successfully used in many sequence labeling tasks. Given an input sequence x, the conditional probability distribution over the label sequence y={y1,…,yn} given x defined by CRF is calculated as follows:

p(y∣x;W,b)∝exp(∑i=1nWTyi−1,yixi+byi−1,yi)
(2)
where Wyi−1,yi and byi−1,yi are learnable parameters corresponding to the neighboring labels (yi−1,yi). The parameters of a CRF model are updated by maximizing the log likelihood on the training dataset:

L(W,b)=∑j=1Nlogp(y(j)∣x(j);W,b).
(3)
To find the best labeling sequence during decoding, the optimal sequence y is computed using the Viterbi algorithm:

y∗=argmaxy∈Yp(y∣x;W,b).
(4)
BiGRU–CRF
We combine a BiGRU network and a CRF network to incorporate long-distance information over a sequence of input as well as information on the output sequence. The network architecture is shown in Fig. 2. The bottom layer of Fig. 2 is the embedding layer, which acquires corresponding word vector. The middle layer is a BiGRU network with the purpose of capturing the semantic information of the input text sequence. The output of BiGRU layer is passed to a CRF layer that calculates the probability distribution using the dependencies among labels of the entire sequence.

Entity linking
The output of entity detection is a sequence of tokens representing an entity mention, and it still needs to be linked to actual entities in KG. In entity linking, the goal is to identify the correct entity node in KG according to the entity mention. We adopt the approach of Mohammed et al. [29] as a foundation, which solves this problem via string matching. First, we will introduce the string matching-based entity linking method in detail.

String matching-based entity linking method
This method can be divided into the following four steps: (1) pre-building an inverted index that maps all entity n-grams to the entity’s name; (2) extracting all n-grams from the entity mention; (3) generating an entity candidate set by finding matches in the inverted index; and (4) ranking entity candidates by computing Levenshtein distance.

Fig. 2
figure 2
The architecture of a BiGRU–CRF network

Full size image
More specifically, for each entity in E, we generate an inverted index that maps all entity n-grams where n∈{1,2,3} to the name of entity. For example, given the entity “Sub-Saharan Africa” (fb:m:06qtn), the corresponding n-grams should be {“Sub-Saharan,” “Africa,” “Sub-Saharan Africa”}. Then, we extract all corresponding n-grams from the entity mention and look them up in the inverted index to find all matches. For example, the mention “Sub-Saharan Africa” in question “What is the time zone in Sub-Saharan Africa?” will be matched to KG entities “Sub-Saharan Africa” (fb:m:06qtn) and “Francophone Sub-Saharan Africa, 1880-1995” (fb:m:04whzt2). We also apply an early termination heuristic method to reduce the number of entity candidates. Concretely, we begin with n=3; if we get an exact match for an entity, we ignore lower-order n-grams, backing off otherwise. For instance, the query “Sub-Saharan Africa” for n=2 has found candidate entities, and then, the search is terminated. It is beneficial to prune entities that would match to “Sub-Saharan” or “Africa” for n=1. Once all entity candidates are gathered, they are ranked by Levenshtein Distance. Levenshtein distance or edit distance counts the minimal number of elementary editing operations (deletions, insertions, or replacements) needed to transform the one string into the other. If two strings are similar, they have small edit distance. Finally, top-N ranked entities are kept in the candidate pool. The process is illustrated in Algorithm 1.

figure f
However, the above method will introduce the issue of ambiguity, which means that candidate entities can only capture literal relevance but not semantic relevance. There are two major reasons that cause the issue. First, as the scale of KG grows, many entities share the same names. For instance, more than one thousand entities in Freebase are called “California,” and it is impossible to distinguish which entity that the mention “California” refers to. Second, partial names may occur in the utterance, which makes it difficult to find the correct entity. For example, given the question “Where was Obama born?”, only part of the name “Barack Obama” is indicated. For these reasons, we identify that entity linking appears to be a bottleneck in KGQA.

Fig. 3
figure 3
The contextual structure of three entities in KG

Full size image
Identification of entity ambiguity
To bridge the gap, our idea is to take the contextual information of candidate entities into consideration to alleviate entity ambiguity problem, because each entity has unique contextual structure (i.e., outgoing relations) in KG. For example, as shown in Fig. 3, we demonstrate the contextual structure of three entities (i.e., fb:m:0td30, fb:m:0c1v2qp, fb:m:0f9fs65), which has the same entity name “California.” Although it is hard to distinguish these entities by text surface matching, they are attached with different relations. These relations can help to distinguish the entities with the same name.

To capture such contextual information, we construct a question pattern classifier according to the correlations between question patterns and relation types. For example, given the question pattern “Who wrote the book ⟨head⟩?”, this pattern is obviously related to the relation type “author.” Thus, if an entity is attached with the relation type “author,” it will be assigned an extra relation score to re-rank the given candidate entities.

Formally, we denote the set of relation types as St. Given a question pattern p, a neural text classifier represents p as a vector vvp. Then, for each relation type t∈St, the probability P(y=t|vvp) that the type of p equals to t is calculated as:

P(y=t∣vp)=softmax(WWvvp+bb)
(5)
where WW and bb are parameters of the classifier and softmax(.) refers to the softmax function.

In practice, we construct a question pattern classification dataset based on the training set of SimpleQuestions benchmark, and the details will be described in next section. It should be noted that no extra information sources are introduced in the entire process of constructing the dataset. Furthermore, we observe that there exist one-to-many phenomena between the question pattern and relation types. For example, “Who was the director of ⟨head⟩?” has two corresponding relation types [“dirctor,” “directed_by”]. Therefore, we convert this task to a multi-label classification task. To solve the above task, we examine several basic text classification models, such as TextRNN [26], TextCNN [20] and TextRCNN [21]. Then, we adopt cross-entropy loss to train these models.

Fig. 4
figure 4
The overview of our proposed entity linking procedure

Full size image
Entity linking procedure enhanced by question pattern classification
In the rest of this section, we will formulate our proposed entity linking procedure. Given a question pattern p and an entity mention m, the set of all the relation types is denoted as St. Our proposed procedure achieves entity linking via three steps:

Training a question pattern classifier M, which takes p as input and returns the probabilities that p belongs to each type t in St.

Utilizing the basis string matching-based entity linking method to generate entity candidates Ce which corresponds to the entity mention m.

Re-ranking the entity candidates by extra relation scores from question pattern classifier.

The entire procedure is shown in Fig. 4. The left-hand side of Fig. 4 illustrates the string matching-based entity linking Method, which is elaborated in Sect. 3.3.1. The right-hand side shows the question pattern classification model with TextCNN as example. The architecture of TextCNN includes convolution, max-pooling and fully connected layer.

Formally, for each entity ei∈Ce, and its outgoing relation types Rei, where Rei⊂St. We compute the final entity linking score as follows, which considers not only edit distance score but also classification score:

SEL(ei,p,m)=Sed(ei,m)+maxr∈Rei(M(p)r)
(6)
where Sed(ei,m) represents the edit distance score between entity ei and entity mention m and maxr∈Rei(M(p)r) represents that we select the maximum value from the probabilities that p belongs to each type r∈Rei. For example, the entity “fb:m:0c1v2qp” has three relation types [“name,” “notable_type,” “author”], each relation is assigned to a probability and the maximum value is viewed as the classification score of entity “fb:m:0c1v2qp.” Finally, top-N ranked entities are kept as entity candidates. The overall procedure is summarized in Algorithm 2.

figure g
Relation detection
The output of entity linking is a set of top-N ranked entities, and all the relations connected to the entity candidates are considered as relation candidates. The goal of relation detection is to match the semantic similarities between the question and relation candidates. Formally, given a question pattern p , for each relation ri in relation candidates Cr, we calculate the matching score SRD(p,ri) that reflects the correlation degree between them. The final predicted relation r^ is given by:

r^=argmaxri∈CrSRD(p,ri).
(7)
We propose a simple but effective attention-based model to detect the correlation. The overall structure is depicted in Fig. 5. Our model has three components: (1) relation encoder; (2) question pattern encoder; and (3) similarity measure. We will introduce all the components, respectively.

Fig. 5
figure 5
Our proposed attention-based relation detection model

Full size image
Relation encoder: A relation embedding matrix EER∈R|nr|×dr is employed to obtain the embedding vector for each relation ri∈Cr. Here, |nr| is the number of relations, and dr means the dimension of relation embeddings. The embedding vector vvr is viewed as the representation of the relation.

Question pattern encoder: A word embedding matrix EEp∈R|nw|×dp is applied to transform each word in p to its corresponding word embedding. Here, |nw| is the vocabulary size and dp means the dimension of word embeddings. Then, the word embeddings are fed into a bidirectional GRUs network to get hidden representations [hh1;hh2;…;hhL] where hh1,hh2,…,hhL∈R1×dh. Each hidden representation is a concatenation of the hidden states from the forward and backward passes. Then, we use attention mechanism [1] to generate the representation of question pattern vvp, and vvp is calculated based on the following formulas:

vvp=∑i=1Lαihhi
(8)
αi=exp(wi)∑Lj=1exp(wj)
(9)
wi=vvTtanh(WWvvr+UUhhi)
(10)
where vv∈Rm×1, WW∈Rm×dr and UU∈Rm×dh are the trainable parameters, L is the length of the question and αi is the attention weight of the ith word in the question.

Similarity measure: We utilize a cosine similarity function to calculate the correlation degree between vvrandvvp, which can be written as:

SRD(vvr,vvp)=cosine(vvr,vvp)
(11)
Training: At the training stage, we utilize the hinge loss with negative samples as the training objective. Specifically, the loss function is calculated as:

L(θ)=∑i=1N∑j=1Mmax[0,γ−S(i)RD+S(j)RD]
(12)
where N is the number of positive samples, M is the number of negative samples for each ground truth relation, γ is a hyper-parameter, S(i)RD is the correction degree of positive relation and S(j)RD is the correction degree of negative relation.

Experiments and results
In the experiment, we use the SimpleQuestions dataset [5], which is a widely adopted single-relation question answering benchmark. It consists of 108,442 single-relation questions and their corresponding triples in Freebase. This dataset is split into train (75,910), valid (10,845) and test (21,687) sets. Two subsets of Freebase are given, i.e., FB2M and FB5M. The former includes 2M entities, while the latter contains 5M entities. In order to fully illustrate the effectiveness of the proposed method, we conduct experiments on both FB2M and FB5M.

The evaluation metric is accuracy. If both the entity and relation that we predict match the ground truth, these data are counted as correct. The accuracy metric is calculated as:

Accuracy=∑Ni=11[(s^i,r^i),(si,ri)]N
(13)
1[(s^i,r^i),(si,ri)]={1, if s^i=si and r^i=ri0, if s^i≠si or r^i≠ri
(14)
where 1[.] is the indicator function, si^ is the golden subject entity, ri^ indicates the golden relation, si is the predicted entity and ri corresponds to the predicted relation.

Experimental settings
There are three neural network-based models used in our proposed pipeline framework, i.e., a sequence labeling model in entity detection stage, a multi-label classification model in entity linking stage and a matching model in relation detection stage. Our models are implemented in PyTorch running on an NVDIA GeForce GTX 1080 GPU, and we list all the basis settings of these models in Table 1.

Additionally, for entity detection, we use the names file released by Dai et al. [10] to map the entity name back to each question to label the entity mention span. With the aid of annotated topic entities, we directly generate question patterns by replacing the topic entities in the questions with a mark “⟨head⟩.” This operation helps the model better distinguish topic entity from other words in the question. For example, given a question “What is the time zone in Sub-Saharan Africa?” and its corresponding topic entity “Sub-Saharan Africa,” we could get “What is the time zone in ⟨head⟩?” as the question pattern.

For entity linking, we construct a multi-label classification dataset based on the training set of SimpleQuestions for each piece of data, and we derive the question pattern from original question by the entity detection module. And we use the last part of golden relation as the corresponding relation type. As Qu et al. [31] point out that the relations in Freebase contain two aspects of information, one is identical to subject’s type, while the other describes the genuine relation, e.g., the question pattern “What is the time zone in ⟨e⟩ ?” corresponds to the relation “location.location.time_zones” . The first two parts of relation “location.location” represent the type of subject, while the last part “time_zones” represents the genuine relation. It is natural to regard the genuine relation as the corresponding relation type. At the same time, for each entity in Freebase, we collect all the outgoing relations of each entity for further calculations. After constructing the dataset, we accomplish minimal preprocessing: deduplicating and lowercasing. Finally, this classification dataset consists of more than 30,000 correlations between question patterns and relation types, and the number of relation types is 1240. Moreover, we keep top-20 ranked entities in the candidate pool.

For relation detection, the hyper-parameter γ is set to 0.5, and negative samples number is 50.

Table 1 The basis settings of neural network-based models
Full size table
Baselines
We compare our model with the following baselines:

Mohammed et al. [29] explore simple yet strong baselines and decompose this problem into entity detection, entity linking, relation prediction and evidence combination.

Qu et al. [31] propose an attentive recurrent neural network with similarity matrix-based convolutional neural network (AR-SMCNN) to capture the semantic-level and literal-level similarities between question and relations.

Hao et al. [13] use a BiLSTM with CRF-tagging-based model to conduct entity extraction and introduce a pattern-revising procedure to improve the performance.

Petrochuk et al. [30] introduce a baseline which consists of top-K subject recognition and relation classification. In addition, they conclude that about 33.9% of the questions are unanswerable due to the ambiguity in the data, so the performance is bound to 83.4%.

Huang et al. [19] utilize knowledge graph embedding to enhance the quality of topic entity and predicate representation in the matching model.

Lukovnikov et al. [27] leverage a pretrained transformer network (BERT) for simple KGQA. This paper focuses on two subtasks: entity span detection and relation classification.

Lan et al. [23] formulate question answering over knowledge graph as a sequence matching problem. Specifically, they use a “matching-aggregation” framework to match candidate answers with questions.

Buzaaba et al. [7] decompose the question answering problem in a three-step pipeline of entity detection, entity linking and relation prediction and solve each component separately.

Zhou et al. [44] propose a deep fused model that combines subject detection and predicate matching under a unified framework. These models share parameters and can be trained jointly.

Table 2 Comparison with baselines on SimpleQuestions dataset
Full size table
Overall results
First of all, we compare our model with baselines to demonstrate the effectiveness of our proposed method. Table 2 shows that our proposed model outperforms all the pipeline and end-to-end baselines on SimpleQuestion dataset. Compared with pipeline methods, our model benefits from the novel entity linking module, which takes the contextual information of candidate entities into consideration to alleviate entity ambiguity problem. However, Mohammed et al. [29], Petrochuk et al. [30] and Lukovnikov et al. [27] generate entity candidate set by calculating string similarity, which cannot distinguish the entities with same name. Although Qu et al. [31] utilize heuristic rules to improve the recall of entity candidates, we will reveal that our model achieves better entity linking results than their method in Sect. 4.5. Furthermore, we notice that our model and Hao et al. [13] achieve a similar performance, but our model samples 50 negative samples for each ground truth, which is much smaller than their approach (they sample 200 negative facts). In addition, our method surpasses Buzaaba et al. [7] perhaps due to the attention mechanism, in which we calculate attention scores to learn the alignments between relation and question tokens, while their approach applies simple baseline methods without attention mechanisms.

Even though we do not adopt complex neural networks, our approach still achieves competitive performance compared with end-to-end methods. End-to-end approaches generally exploit increasingly complex deep learning techniques, e.g., Zhou et al. [44] utilize complex convolutional layers and gate mechanism to consider both shallow- and deep-level semantic information. Lan et al. [23] first match the token representations from two sequences, and then, these matching vectors are further aggregated. Moreover, we observe that Huang et al. [19] have relatively poor performance, and the reason is that instead of directly inferring head entity and predicate, they take an indirect way to jointly generate the question’s head entity, predicate and tail entity representations in KG embedding spaces, which may mislead the task of KGQA, especially in a large-scale KG.

The performance of our proposed method decreases 0.6% when conducted on FB5M. It is because all the ground-truth facts belong to FB2M, and FB5M has 26.1% more facts than FB2M, which significantly increases the difficulty of selecting the golden fact.

Table 3 The sequence labeling results on validation set
Full size table
Entity detection results
For entity detection, we compute the precision, recall and F1 of sequence labeling on the validation set. Then, we compare BiGRU–CRF model with BiGRU model and CRF model, respectively. The results are shown in Table 3. From the results, we have two observations. First, the BiGRU model outperforms the CRF model, and we could see a 2.48% improvement on F1. Second, the BiGRU–CRF model could help boost the performance of sequence labeling compared with the other two models. Due to the high accuracy of entity detection when using the BiGRU–CRF model, we perform no extra process [13, 31] to revise the wrong labeled mention span.

Table 4 Top-N recall with different classifiers on validation set
Full size table
Entity linking results
For entity linking, we first examine the performances of different text classifiers used in question pattern classification on the validation set. The evaluation metric is the recall of top-N entity candidates, and it is calculated as the percentage of questions for which the top-N entity candidates include the ground-truth entity. Table 4 shows the overall results.

In Table 4, SM corresponds to the basis string matching-based entity linking method, other methods, e.g., SM-RNN, SM-CNN and SM-RCNN, correspond to our proposed entity linking method with TextRNN [26], TextCNN [20] and TextRCNN [21], respectively.

From the results in Table 4, we have two observations. (1) Our proposed entity linking methods outperform the basis string matching-based approach by a large margin, and it confirms the effectiveness of our methods. Let us take SM-CNN as an example, for top-1, 5, 10 and 20 recall, SM-CNN surpasses the baseline by 12.86%, 6.96%, 5.03% and 3.65%, respectively. Note that the top-N recall result is not monotonically increasing with the number N. The reason is that the difficulty that golden entity appears at the first position in the entity candidate set is much higher than it ranked in top-20. (2) The proposed methods have similar performances when using different text classifiers, and it demonstrates the generalizability of our methods. We only examine basic text classification models, and the entity linking results could be better if we select much more advanced text classifiers.

Table 5 Top-N recall with different baselines on the test set
Full size table
We choose SM-CNN as the final model, which has the best result at top-20. Then, we compare SM-CNN with several baselines on the test set, including the entity linking approaches by Lukovnikov et al. [28], Yin et al. [40] and Qu et al. [31]. Table 5 shows that our method surpasses other baselines from top-1 to top-20 on FB2M. Therefore, it is beneficial to take the contextual information into account to distinguish the correct entity from plausible wrong entities. Moreover, the entity linking results degrade slightly on FB5M due to the larger scale of candidate entities.

Table 6 Ablation results of attention mechanism
Full size table
Relation detection results
For relation detection, ablation analysis is conducted to evaluate the effectiveness of attention mechanism. From the results in Table 6, we have two observations. (1) Attention mechanism can give a 0.4% performance boost in both overall accuracy and relation accuracy, which indicates that the alignment between the question and relation has been learned and attention mechanism facilitates the semantic similarity matching. (2) The relation accuracy is slightly higher than the overall accuracy, which seems clear that relation detection process is easier than entity linking process and entity linking appears to be the bottleneck in KGQA.

Fig. 6
figure 6
Visualized attention weights. The darker represents the higher weight

Full size image
Effectiveness of attention
According to the analysis in Sect. 4.6, the attention mechanism plays a critical role in achieving the best performance. To gain a deeper understanding of its effectiveness, we visualize the attention weights in Fig. 6.

In the first example of Fig. 6, the relation “soccer.football_player.position_s” pays attention to the words “position” and “known.” Although this model does not assign a higher attention weight to the word “football,” the result is still correct. And in the second example, the relation “cvg.computer_videogame.gameplay_modes” accurately pays attention to the words “gameplay” and “mode.” Above examples confirm that attention mechanism learns a reasonable alignment between question pattern and relation.

Error analysis
In order to analyze the limitations of our approach, we randomly sample 100 wrongly predicted questions from the test set. Then, we list the major categories of errors as follows:

1.
The golden entity does not appear in the entity candidate set: Top-N ranked entities are kept as entity candidates in the entity linking process, and if the golden subject entity is not selected, we cannot find the answer. In detail, there are three main reasons. The first is that many entities share the identical names in KG, e.g., for the question “What format is fearless?”, more than hundreds of entities in Freebase are called “fearless.” The second is that the spelling of entity mention is inconsistent with the corresponding spelling in KG, e.g., for the question “What position does Carlos Gomez play?”, the corresponding entity mention is “Carlos Gomez,” but the entity in KG is named as “Carlos Gómez.” The last is that the sequence labeling model wrongly labels the entity mention span, e.g., for the question “What’s the name of an environmental disaster in Italy?”, the golden entity mention is “environmental disaster,” but the BiGRU–CRF model return “Italy” as the corresponding entity mention.

2.
Incorrect entity prediction: Our method selects the wrong entity from the entity candidates. Concretely, there are two major causes. The first is that the question pattern is too general, e.g., for the question “Name a history film,” the corresponding question pattern is “Name a<head>.” Since we adopt the entity linking procedure enhanced by question pattern classification to distinguish the correct entity from plausible wrong entities, we need informative question patterns. The second is that partial names occur in the question, e.g., for the question “What to people of the Blackfeet tribe speak?”, the entity name in KG is “Piegan Blackfeet,” and it is clear that only the word “Blackfeet” appears in the question.

3.
Incorrect relation prediction: Relation detection model is applied to match the semantic similarity between the question and relation candidates, but this model returns the wrong relation. Specifically, there are three main reasons. The first is that the difference between the golden relation and predicted relation is very tiny, e.g., for the question “Who produced the film Woodstock Villa?”, the golden relation is “film.film.executive_produced_by,” while the relation predicted by our model is “film.film.produced_by.” The second is that the golden relation is not common, e.g., for the question “Which military was involved in the Second Battle of Fort Fisher?”, the corresponding golden relation is “base.culturalevent.event.entity_involved.” The last is that the semantic similarity between the question and relation is poor, e.g., for the question “What ’s the name of an Australian rock and roll?”, but the corresponding golden relation is “music.genre.artists.”

Conclusion
In this paper, we propose a pipeline framework for KGQA, which consists of three cascaded components: (1) a sequence labeling model, which could label the entity mention in the question; (2) a novel entity linking model, which first trains a multi-label classification model based on the correlations between question patterns and relation types and then above classification model is combined with the string matching-based entity linking method to link the correct entity and (3) a simple yet effective relation detection model, which is used to match the semantic similarity between the question and relation candidates. The overall process is that we first train a BiGRU–CRF model to label the entity mention span in the question and then present a novel question pattern classification model to enhance the entity linking process and finally leverage an attention-based model to detect the relation. Our proposed framework achieves competitive results on the SimpleQuestions dataset, and it indicates the effectiveness of our model.

Even though the proposed method is restricted to single-relation questions, this work can serve as a foundation for further exploration. In the future, we will explore advanced multi-label classification models as well as different network structures for relation detection. Also, we will focus on designing more advanced neural KGQA approaches to handle complex questions, e.g., GraphQuestions [33], ComplexQuestions [2], LC-QuAD [36] and LC-QuAD 2 [11].