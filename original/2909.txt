This article presents a novel framework for the semantic enrichment of documents, exploiting the hierarchical ontological knowledge of a domain in conjunction with classification techniques. The main contributions of this work are fourfold: (a) a well-defined theoretical model for the semantic representation and enrichment of documents is defined, (b) a method for dealing with the problem of class imbalance is outlined, based on the transformation of the document representations into more balanced ones, (c) a methodology is proposed for assigning semantic labels in those cases where it is hard to decide which label fits best and (d) a set of novel metrics for evaluating the performance of the suggested framework are introduced. The extensive experimental procedure that follows, conducted on two popular datasets, exhibits promising results and constitutes a proof of the robustness of the overall approach.

Access provided by University of Auckland Library

Introduction
A constantly growing interest in semantic document representation in recent years has led to a vast increase in related research [10, 40, 47, 62]. Indeed, semantic technologies nowadays provide benefits to a wide range of areas such as the Semantic Web [62], concept-based search engines [40] and many other applications (see for example [47] and [10]) that try to use the knowledge of a domain in order to provide useful services and improve aspects like usability, correctness, completeness and searching.

The performance of semantic applications depends on the quantity and quality of the underlying data [6]. For example, an increase in the amount of available useful metadata that describe the knowledge of a domain has been observed in the area of Semantic Web. Yet, an even larger amount of unstructured information has still to be incorporated into the semantic content. Since user interaction with the Web is primarily in free-text form, most of the generated data are unstructured and thus it would be desirable to have them enhanced with semantic information [1, 12].

Traditional approaches of enriching unstructured documents with structured information have been mainly based on manual techniques [26]. This task is obviously laborious and therefore it is necessary to develop methodologies that would eventually automate this procedure. In this direction, automatic annotators that classify documents to additional concepts can be useful, especially if they are based on ontologies which represent the knowledge of specific domains. In particular, the process of automatic classification requires an ontology and an unstructured part of a document in order to be classified into further concepts of the given ontology [30, 46, 61]. This process is predominantly based on machine learning classification techniques [3, 25, 61], which are employed in the proposed framework.

A difficulty in the procedure of mapping documents to the concepts of an ontology is that the latter is usually comprised of a considerable number of concepts. Therefore, a classification technique would require a lot of data to cover all concepts with related training usage examples. Furthermore, training datasets are often highly imbalanced; many concepts are only found in very few documents, which means that the provided instances for training the classifiers are insufficient. A straightforward solution to this problem would be to balance the training data, taking into consideration the hierarchical structure of ontologies [13, 29, 75]. Another, equally important, problem is that rich ontologies usually contain concepts that are semantically close to each other, making the discrimination in-between them hard. This may constitute a difficulty for a system to determine which concept, out of a set of candidate concepts, fits best to a given document. The aforementioned issues may result in many documents being mapped to incorrect concepts [17, 44, 67] and it is exactly for those cases that the framework proposed in this work provides valuable insight.

Within the above framework, the contribution of this work is the presentation of a novel methodology which improves the semantic representation of documents, especially in cases of imbalanced data. This is achieved through their enrichment with semantic information, obtained via knowledge-based resources and classification techniques that take advantage of ontological knowledge expressed in Description Logics [9]. The overall architecture includes a series of algorithms that improve classification performance, reduce class imbalance and assign labels in those cases where the classification algorithms might fail because the representative classes are hard to be decided upon.

In essence, this work presents a well-defined framework along with a novel methodology which focuses mainly on imbalanced semantic descriptions, enriching them with new concepts (labels). In particular, the proposed methodology includes two main components; (i) semantification and (ii) semantic enrichment. The proposed architecture is illustrated in more detail in Fig. 1 (Sect. 5). The input to the first component is a set of documents along with a set of terminologies. After the steps of semantic annotation and ontological document representation, the respective semantic description is returned and is subsequently provided to the second component, which returns its enriched version (i.e., a semantic description with additional semantic labels). The purpose of this step is to transform the given semantic description into a concise and balanced one, in order to deal with the class imbalance problem. Semantic description enrichment with fresh semantic labels is achieved using classification techniques along with a novel algorithm which assigns the label that best fits each document.

Finally, the presented framework is tested on two different datasets, using various classifiers under different features and thresholds. The performance is assessed on a set of novel metrics, proposed for this task, that estimate the effect of adding semantic concepts to the documents ranging from a general level down to a specific and strict level of accuracy. Overall, the obtained results are promising, as they assess the robustness of the described solution.

The rest of this paper is organized as follows; Sect. 2 discusses related work, while Sect. 3 provides insight into the theoretical background and the used notation. Section 4 introduces the proposed framework for the semantic enrichment and Sect. 5 presents a novel system architecture of semantic document enrichment. Section 6 include the experimental procedure where a series of evaluation measures are defined along with the presentation of the experimental results. Lastly, the paper is concluded in Sect. 7, where some final remarks are made and future work directions are suggested.

Related work
Classifying or annotating documents using ontological knowledge of a domain is a subject that has been extensively studied in literature [30, 36, 46, 60, 61, 64, 70]. Most commonly, a hierarchical ontological structure is used; in [70], the documents are represented by features (concepts) selected from a hierarchical ontology. Similarly, in [36] the input data is preprocessed through the application of an ontology-based heuristic for feature selection and feature aggregation. Then, a number of alternative text representations is constructed and based on these representations, multiple clustering results are produced using the k-means algorithm. An ontology-based classification of semi-structured documents is presented in [64], where documents are classified into ontology classes. In [60], an automated text categorization technique for document classification is proposed which is based on term frequency-inverse document frequency (TF-IDF) and on a dependency graph provided by the domain-based ontology. In contrast to the above approaches which focus mainly on text categorization and text clustering, our solution bears an essential difference as it provides an integrated framework (theoretical model and methodology) for enhancing the semantic description of a document.

In order to bridge the gap between natural language processing and semantic annotation for feature extraction, an ontology-based parser which incorporates a method for converting text in natural language into the predicate-argument format is introduced in [18]. The ontology-based parser can be used in a variety of applications (e.g., search engines and applications of summarization and categorization); it contains functional components that receive and tokenize the documents, extract particular concepts according to the employed ontology and finally assemble the resulting concepts into predicates. The ontological parser has two major functional elements, a sentence lexer and a parser. The sentence lexer takes a sentence and converts it into a sequence of ontological entities that are tagged with part-of-speech information. The parser converts the sequence of the ontological entities into predicate structures using a two-stage process that analyzes the grammatical structure of the sentence by applying rules to it and by binding arguments into predicates. Similarly to [18], our framework try to cover the gap between natural language and semantic annotation extending the above approach not only by extracting labels from the current document but also enriching a document with new (fresh) labels (i.e., labels that fit best to the document without the restriction of extracting them only from this document).

In [2], a methodology for the automatic classification of text documents into a dynamically defined set of topics of interest is outlined. The method is based on measuring the semantic similarity of the thematic graph that is created from a text document, considering the ontology sub-graphs resulting from the projection of the defined contexts. The domain ontology effectively becomes the classifier, where the classification topics are expressed using the defined ontological contexts. In contrast to the traditional supervised categorization methods, the proposed system [2] does not require a training set of documents. Different from [2], our approach is a supervised system which is capable of learning through usage examples optimizing its performance.

In the biomedical domain, the enhancement of relevant literature via automatic multi-label classification is investigated in [37]. More specifically, the structure of the graph derived from the Gene Ontology [19] is utilized in graph-based multi-label classification methods. The obtained results indicate a significant improvement in the prediction accuracy of terms implied by the analyzed text.

In conclusion, even though most of the approaches mentioned above bear some similarity to the proposed framework in this work, they do not deal with the two fundamental problems presented in Sect. 1; imbalanced data and assigning semantic labels in hard cases (as it has been discussed in the Introduction).

Preliminaries
In the current section, the Web Ontology Language (OWL) [11] is briefly described, along with some properties of the Description Logics [8].

Ontologies
An ontology [68] typically consists of a hierarchical description of important concepts in a domain, including properties (roles) of instances of each concept and constraints among concepts and properties. Following the Web Ontology Language (OWL) [4, 49] guidelines proposed by the W3CFootnote1 [52], we consider Description Logic-based ontologies [8] for the representation of the knowledge in a domain.

Description logics
Description Logics (DLs) [7] are a family of class-based (concept-based) knowledge representation formalisms, equipped with well-defined model-theoretic semantics, which are used for representing the knowledge of an application domain. In DLs, an application domain is defined in terms of concepts which are used to describe the entities of the domain. The main reason for the popularity of DLs in knowledge representation systems is their reasoning capabilities, which allow the methods to infer implicit knowledge from explicitly defined descriptions.

In DLs, three main types of syntactic categories exist; (i) the atomic concepts, (ii) the atomic roles and (iii) the individuals. The atomic concepts are unary predicates which represent entities, types or sets of objects (e.g., Glass, Color, Material, etc.). The atomic roles are binary predicates that may represent relations (i) between instances of atomic concepts, (ii) between instances of concepts and individuals or (iii) between individuals (e.g., isMadeOf or OriginatesFrom). Finally, the individuals are constants which represent actual objects or persons in a domain (e.g., John or Golden Fleece). The vocabulary of a description logic language can be defined as a tuple of three sets (CN,RN,IN), where CN is the set of atomic concept names, RN is the set of atomic roles and IN is the set of individuals (mutually disjoint sets of names for individuals, concepts and roles of the world, respectively).

According to DLs [9], the semantics of concept descriptions are defined in terms of interpretations. An interpretation I consists of both the domain of the description (a non-empty set ΔI) and an interpretation function (.I). The interpretation function assigns: (i) a subset of the domain (AI⊆ΔI) to each atomic concept (A∈CN), (ii) a two-member relationship of the domain (RI⊆ΔI×ΔI) to each atomic role (R∈RN) and (iii) an object of the domain (aI⊆ΔI) to each individual name (a∈IN). An ontology (knowledge base) K in DLs is an ordered pair K=⟨T, A⟩ comprised of a terminological component (TBox) T and an assertional component (ABox) A. The former contains a finite set of terminological axioms, i.e., statements about the relationship among concepts and roles, while the later contains a set of assertions that classify object to concepts and define relations between objects. Formally, let L=⟨IN,CN,RN⟩ be a vocabulary. We call the statement a≈b individual equality assertion, the statement a≉b individual inequality assertion, the statement A(a) concept assertion and the statement r(a, b) role assertion, where a,b∈IN, A∈CN and r∈RN. A set of (equality, inequality, concept or role) assertions is called assertion box or simply ABox. A role r∈RN is a named role expression or atomic role. Let r be an atomic role. The expressions r− recursively defined using the role constructors − (inverse role constructor) is a role expression or a complex role or simply a role.

Concerning the TBox, in this paper we consider only a cluster of tractable DL ontologies, the EL (underpinning the OWL EL tractable fragment). A concept C∈CN is a named concept expression or atomic concept. Let C, D be atomic concepts, r an atomic role, a an individual name and n a natural number. The expressions C⊓D, ∃r.C (i.e., the dot operation relates an atomic role with an individual, e.g., ∃hasMaterial.Wool constitutes a concept), recursively defined using the concept constructors ⊓ (conjunction), and ∃ (existential), are called concept expressions or complex concepts or simply concepts. Moreover, ⊤ (named Top) and ⊥ (named Bottom) are concepts. An expression of the form C⊑D (C≡D) is a concept subsumption axiom (concept equivalence axiom). Similarly, an expression of the form r⊑s (r≡s) is a role subsumption axiom (role equivalence axiom).

A set of concept or role subsumption or equivalence axioms is a terminological box or TBox. In this paper, we consider only acyclic TBoxes (for more details see [7]). The concept and role names used in the axioms of a TBox T is the signature of T, written as Sig(T).

Framework
In this section, a novel semantic description model for document representation and enrichment is defined. This model is based on concepts of hierarchical ontologies, assuming subordinate and superordinate relations between them. In particular, this framework assumes a domain ontology (or a set of ontologies) used to extract particular concepts from a set of documents, in order to create a semantic description S={S1, S2,…,Si}, where Si is a set of concepts that represent a semantic description of a document. Therefore, each Si is comprised of concepts that are included in the respective document. Additionally, each Si represents a semantic description of a document and the concepts that it contains are extracted using a particular ontology (i.e., extracting concepts that are included in the particular ontology). Additionally, this framework tries to enrich a semantic description by assigning new concepts (that are not included in the description, yet) or labels to it, drawn from a domain ontology. The definitions that follow describe the proposed model in detail.

First of all, the definition of the semantic description of a set of documents (Definition 1) is introduced, which plays a key role in our model, mapping the content of documents to a set of concepts with respect to a given terminology. The set of concepts, in turn, contains the semantic labels (Definition 2), which are assigned to documents in order to (semantically) describe them. Since the proposed framework aims at balancing the semantic description (as it will be described in more detail below), the occurrences of a term in the set of documents defining the semantic label usage need to be measured (Definition 3).

Definition 1
(Semantic description) A semantic description σ:℘(D)→℘(CN), in terms of a terminology T, is a mapping of each document di∈D to a set of concepts CNdi∈CN, where ℘(D) and ℘(CN) represent the powersets of D and CN, respectively (i.e., the set of all subsets of D and CN). The domain (scope) and the range (reference) of a semantic description σ are expressed as sco(σ) and ref(σ), respectively.

Definition 2
(Semantic label) A semantic label A of a document d is a term of the set of concepts CN, where A∈CN and A∈ref(σ(d)).

Definition 3
(Semantic label usage) The usage of a semantic label A in a set of documents D is the set of documents U which contain A as a semantic label. This is expressed as use(A)=U, where U⊆D.

Definition 1 may be further simplified if the set of semantic labels L⊆CN of a document di∈D is to be considered as the semantic description of di. Slightly abusing notation, σ may be re-written as S={S1,…,Sn}; a set of the sets of semantic labels of all documents. In this context, Si∈S is the semantic description of document di or equivalently S|di. Finally, ref(S) contains all semantic labels of every document of sco(S).

Additionally, each semantic label A may be interpreted as a concept assertion A(di) of di and therefore σ may be regarded as an ABox AS that contains all the assertions of di. The labels are assumed to be those retrieved from a text or a set of documents, according to a given ontology containing concepts from the original text. Thus, the ABox AS is the ontological representation of the semantic description σ. Definition 4 provides an obvious way of checking the consistency of semantic descriptions. This concept of ontological representation is better illustrated in Example  1.

Definition 4
(Consistent semantic description) Let d be a document, S a semantic description of d in terms of a TBox T and AS its ontological representation. S is consistent w.r.t. T if and only if AS is consistent w.r.t. T.

Example 1
(Ontological representation) Let O be an ontology and D={d1,d2, d3} be a set of documents whose semantic description is the sets of labels S={S1, S2, S3}, respectively, where

S1={A, B}S2={A, C, D, E} S3={H, I, J}
The semantic descriptions may be retrieved using an annotator, which extracts all semantic labels (A1, A2,... An∈CN) identified in each document. Consequently, the ontological representations AS1, AS2 and AS3, in terms of the TBox T of the ontology O, are the following:

AS1={A(d1), B(d1)}AS2={A(d2), C(d2), D(d2), E(d2)}AS3={H(d3), I(d3), J(d3)}.
The ontological representation of semantic descriptions may result in further analysis and enrichment, especially in conjunction with the reasoning services of DLs. Since the aim of this framework is to semantically enrich documents with additional (fresh) semantic labels, an additional semantic label for a specific document is considered to be more “interesting” if it is either fresh or does not contradict existing semantic descriptions, w.r.t T. For instance, a semantic label may contradict a semantic description S if A∉T. Definition 5 proposes an interesting semantic enrichment technique of the ontological representations. This point of assigning fresh semantic labels is further illustrated in Example 2.

Definition 5
(Fresh semantic label) Let d be a document, S a semantic description of d in terms of a TBox T, AS the ontological representation of d and A∈CN an atomic concept of  T. A is fresh for S w.r.t T if and only if A∉AS and ∄A′∈AS such that A′⊑A.

Example 2
(Fresh semantic label) Let the ABox AS1 of document d1∈D and the TBox T be:

AS1T={A(d1), B(d1)}={A⊑C, D⊑B, E⊑C}.
Then, the semantic label E is a fresh label for S1 because E∉AS1 and ∄E′∈AS1 such that E′⊑E. On the other hand, the semantic label C is not a fresh semantic label because, in this case, C is a logical entailment of A as A⊑C and A(d1)∈AS1.

Since a semantic label may entail other labels that could also be part of the same description, it is expected that the removal of the redundant labels will lead to a more concise semantic description. The latter should retain the most informative labels that describe the same semantic content. In this direction, the proposed framework tries to extract the most concise semantic description, called laconic semantic description. More specifically, Definition 6 determines when two semantic descriptions are equivalent, while Definition 7 retrieves the most concise of two semantically equivalent descriptions. Definition 8 decides if a semantic description is laconic (or which is the laconic semantic description). Laconic semantic descriptions are further reasoned upon in Example 3.

Definition 6
(Semantic description equivalence) Two semantic descriptions S1 and S2 of a document d are (semantically) equivalent (S1∼S2) if and only if

∀A∈S1, A(d)∈AS2  and   ∀A′∈S2, A′(d)∈AS1
where AS1 and AS2 are the respective ontological representations.

Two semantic descriptions S1 and S2 of a set of documents D are semantically equivalent (S1∼S2) if and only if

∀d∈D,S1|d∼S2|d
that is, the two semantic descriptions are semantically equivalent w.r.t. every document in the collection.

Corollary 1
(Equivalent descriptions’ label match) If two semantic descriptions S1 & S2 of a document d are semantically equivalent (S1∼S2), then their respective sets of labels contain the same elements.

Corollary 1 is easily deducible if we observe that:

S1∼S2⇔∀A∈S1, A∈S2  and   ∀A′∈S2, A′∈S1⇔S1=S2.
A similar conclusion may also be derived for two semantic descriptions that describe a set of documents D, according to Definition 6.

Definition 7
(Semantic description conciseness) A semantic description S1 is more concise than S2 (S1⊲S2) if and only if |S1|<|S2| and S1∼S2, where |Si| represents the cardinality of set Si.

Given two semantic descriptions S1 and S2 of a set of documents D in terms of a TBox T, S1 is said to be more concise than S2 (S1⊲S2) if and only if  ∀d∈D, S1|d⊲S2|d.

Definition 8
(Laconic semantic description) A semantic description S of a document d in terms of a TBox T is laconic if and only if there is no other semantic description S′ of d more concise than S.

A semantic description S of a set of documents D in terms of a TBox T is laconic for D if and only if it is laconic ∀d∈D.

Example 3
(Laconic semantic description) Let

S1S′1={A(d1), B(d1), C(d1)}  and ={A(d1), B(d1), C(d1), D(d1)}
be two semantic descriptions of document d1 in terms of a TBox T, where C⊑D. The two semantic descriptions are equivalent. Additionally, S1 is laconic because there exists no other semantic description more concise for d1. On the contrary, S′1 is a non-laconic semantic description because there exists another one (S1) which is more concise than S′1.

On the grounds of the above definition of semantic description conciseness between two equivalent semantic descriptions, one more definition is required in order to determine which of them is the smallest when they are not equivalent. Bearing this in mind, a smaller semantic description includes less semantic labels and does not entail the semantic labels of the larger one. Therefore, the smaller semantic description is less informative. Definition 9 specifies when a semantic description is considered to be smaller than another one.

Definition 9
(Smaller semantic description) A semantic description S1 is smaller than S2 (S1≺S2) if and only if

∀A∃A′∈S1A(d)∈AS2, and ∈S2 s.t A′(d)∉AS1
where AS1,AS2 are the ontological representations of  S1 and S2, respectively.

Given two semantic descriptions S1 and S2 of a set of documents D in terms of a TBox T, S1 is smaller than S2 (S1≺S2) if and only if  ∀d∈D, S1 with respect to d is smaller than S2 with respect to d (S1|d≺S2|d).

Corollary 2
(Fewer Labels) If S1,S2 are two semantic descriptions of a document d and S1≺S2, then S1 contains fewer labels than S2.

Corollary 2 is easily deducible if we observe that:

S1≺S2⇔∀A∈S1, A∈S2,   and   ∃A′∈S2  s.t  A′∉S1⇔|S1|<|S2|
A similar conclusion may also be derived for two semantic descriptions that describe a set of documents D, according to Definition 9.

Since the proposed framework relies on classification techniques to assign fresh labels to a semantic description, the imbalanced usage of semantic labels may greatly affect the performance of the classification algorithms. To balance a semantic description, the level of balancing in data needs to be determined. Prior to defining the balancing level k of a semantic description, the semantic label pairs need to be specified. The semantic labels are retrieved from documents and each label A, along with its predecessor label B, is considered to be a label pair (Definition 10). According to this definition, the semantic label pairs are formed by the retrieved labels that are related directly with an is−A relation, assuming that such labels create semantically consistent pairs. The number of occurrences of each label of semantic pairs is used to measure the balancing level k (Definition 11), where k lies in the (0, 1] range. In particular, k→0 is an indication of imbalanced data. On the other hand, when k=1, data instances are perfectly balanced (uniformly distributed among the classes). For example, if k=0.5, it may be said that the semantic description is 50%-balanced.

Definition 10
(Semantic label pairs) Let E be a set of semantic label pairs. Then two labels A and B drawn from a set of labels L belong to the set of pairs of semantic labels E((A,B)∈E) if and only if:

A,B∈L,A⊑B  in terms of  TBoxT and ∄Cs.t.A⊑B  and  C⊑B  in terms of  TBoxT
Definition 11
(k-balanced data) Let D be a set of documents and S a semantic description of D in terms of a TBox T. S is said to be k-balanced if and only if

∀(A,B)∈E,k≥1−||use(A)|−|use(B)||max(|use(A)|,|use(B)|)
(1)
Alternatively,

k=min∀(A,B)∈E(1−||use(A)|−|use(B)||max(|use(A)|,|use(B)|))
(2)
where k is the ratio which represents the degree of balancing.

To extract a k-balanced semantic description S′ from an initial semantic description S, S′ is assumed to be the smallest k-balanced semantic description, according to Definition 12.

Definition 12
(k-Balanced semantic description) Let D be a set of documents and S, S′ two semantic descriptions of D in terms of a TBox T. S′ is a k-balancing of S if and only if:

S′⪯S,S′  is  k-balanced  and ∄ (k-balanced S′′)  s.t.  S′≺S′′≺S.
Semantic document enrichment
System architecture
Figure 1 illustrates the architecture of the proposed system, whose input is a set of documents D and a set of Terminologies T and whose output is an enriched semantic description. The main components of this system are two; (i) semantification and (ii) semantic enrichment, which are described in detail below.

Fig. 1
figure 1
The architecture of the proposed system

Full size image
System input
The proposed framework requires as input a set of documents D and a set of Terminologies T. As input terminologies, we assume hierarchical ontologies as those that have been described in Sect. 3.1.

The input documents may be unstructured or Semi-structured. The unstructured documents consist of unstructured text in natural language, while semi-structured documents consist of both structured and unstructured information. The latter is usually in the form of free-text in natural language that may also contain data such as dates, numbers and facts. The main disadvantages of processing this type of information are the resultant irregularities and ambiguities. In general, it is more difficult to understand the meaning of unstructured information when compared to structured, such as the data stored in tabular databases or annotated (semantically tagged) documents.

Semi-structured documents contain two types of descriptions; textual, which includes information in natural language and semantic, which is in the form of an ABox. The semantic part describes the individuals, using a defined set of concepts. An example of a semi-structured document is illustrated in Fig. 2; it contains a title and a description (textual part) along with a classification of the document to three different concepts (semantic part). The description of the document of Fig. 2 in OWL ontology form may be as follows:

d hasTitle ‘‘Baby′sLibertybodice",

d hasDescription ‘‘Machine−knittedcreamcolouredwool...",

d hasConcept Garment,

d hasConcept Material,

d hasConcept ∃hasMaterial.wooland

d isDescribedBy O.

Fig. 2
figure 2
A semi-structured document containing a textual part in natural language and a semantic part in ABox form

Full size image
Semantification
The input to semantification (Fig. 1) is a set of documents D and a set of Terminologies T and the output is the respective semantic description S of D. This process is completed in two steps; the first one is the task of the semantic annotation and the second one is the ontological document representation.

Semantic annotation
Since the proposed framework processes semi-structured documents that contain both structured and unstructured information (Sect. 5.2), a method should be proposed for the extraction of their semantic descriptions. Initially, all concepts from the structured part of the document are retrieved (which are usually based on an ontology knowledge) and constitute the features that characterize the document. Additionally, in those cases where the number of roles is limited (like in the datasets of Sect. 6), then they are regarded as atomic concepts and therefore become part of the feature space as well.

Atomic concepts can also be obtained from the unstructured part of the document using an annotator like GATE [14, 23], which is able of extracting semantic labels out of free-text. Annotator input is usually a set of documents written in natural language along with an ontology and the output is a list of concepts derived from the document, in accordance with the supplied ontology. Obviously, the extracted concepts become part of the document features.

In a final augmentation step, the semantic descriptions may be enriched through the use of the large lexical database of WordNet [27, 53]. WordNet groups English words into sets of synonyms, called synsets, expressing distinct concepts. WordNet may be viewed as a hierarchical lexical ontology whose nodes are the synsets and where every node represents a particular meaning or sense. Figure 3 demonstrates an example of a WordNet hierarchy.

In our approach, pattern matching techniques are used to extract WordNet synsets from the textual part of a semi-structured document. In particular, we apply part-of-speech (POS) tagging and lemmatization on the text and then, for each word, we try to identify the respective synset of a particular POS (e.g., noun). Moreover, we use the WordNet first sense for selecting the appropriate synset, as it has been proven to be a very hard baseline in knowledge-based word sense disambiguation approaches [58]. The synsets are then treated as semantic labels and the synonyms of each synset are merged into one semantic label, as these represent the same sense.

Fig. 3
figure 3
An example of a WordNet hierarchy

Full size image
Ontological document representation
Here, we assume that knowledge is expressed in acyclic DL ontologies and the ontological document representation is actually based on the concepts of an ontology. It should be noted that this framework has been designed for acyclic ontologies, presuming a hierarchical structure of concepts that are related with broader–narrower relationships. Therefore, an ontology that includes cycles among concepts would be inconsistent and incompatible with the provided framework.

Additionally, it should be clarified that the proposed framework is appropriate not only for hierarchical ontologies with isA type of relations, but also for other types of broader–narrower hierarchies, such as semantic relations in thesauri [41]. In particular, hierarchical relations in thesauri mainly include generic relations (e.g., isA), part–whole relations (e.g., parts of a car, parts of a body, geographic locations, etc.) and instance relations (i.e., relations between a general category and individual instances like “fruit” and “apple”) [16, 50]. Moreover, in the relevant literature, the semantic relations in thesauri are also discriminated to hyponym–hyperonym (e.g., isA, kindOf, taxonomic, superordinate–subordinate, genus–species, topic–subtopic and class–subclass relations [22]—for example, “animal” is a hypernym of “dog” and “Rottweiler” is a hyponym of “dog”), meronymy (i.e., part–whole relation between an entity and its parts) [45] and troponymy relations (i.e., broader–narrower relations between verbs) [28]. Consequently, this framework can be combined with ontology or thesauri semantic relations, provided that these relations are structured hierarchically (e.g., broader–narrower, part–whole or topic–subtopic type of relations). The key concept is that the employed hierarchy (or hierarchies) should allow the concepts to be generalized semantically in more general meanings, as the overall framework requires.

As it has been mentioned before, in the proposed framework, each document d is represented by its respective semantic description S. Semantic descriptions are encoded as “one-hot” binary vectors, whose elements are the concepts of the ontology. Therefore, the dimensionality of the vector space is initially equal to the cardinality of the set of concepts (|CN|).

Table 1 Binary vector representation of a document based on the extracted features
Full size table
Table 1 illustrates the overall approach of document representation as a vector, for the document depicted in Fig. 2. Finally, as the obtained binary vectors are of a large size and sparse, dimensionality reduction techniques [63, 69, 72] may be applied in order to obtain a document representation in a lower-dimensional feature space, while at the same time minimizing the incurred information loss.

Semantic enrichment
The input to semantic enrichment (Fig. 1) is a semantic description S and the output is the respective enriched semantic description SE. This process includes four steps; (i) the task of extracting a laconic semantic description, (ii) the process of balancing, (iii) the classification technique and (iv) the execution of class labeling algorithm. The aforementioned steps are described in detail below.

Extracting a laconic semantic description
Algorithm 1 is used for the extraction of a laconic semantic description. This algorithm requires as input a Terminology T, a set of documents D and a non-laconic semantic description S and it returns the respective laconic semantic description Slaconinc of S. The explanation of the usage of this algorithm is illustrated in Example 4.

figure g
Fig. 4
figure 4
Example of the terminology of an ontology O in directed acyclic graph (DAG) format

Full size image
Example 4
(GetLaconic) Let O be an ontology, whose TBox T is the directed acyclic graph (DAG) of Fig. 4. More precisely, O can be described as follows:

T = {B⊑A, C⊑A, D⊑B, E⊑B, D⊑C, E⊑C}.
Let D={d1, d2} be a set of two documents whose semantic description S is:

S={D(d1), E(d1), B(d2), D(d2)}.
Initially, Slaconic, the laconic semantic description of the D, is an empty set (line 2). Then, the semantic descriptions S1 and S2 of the documents d1, d2∈D are retrieved (line 4):

S1={D, E},S2={D, B}.
The nested for-loop (lines 6–10) examines all semantic label pairs (Definition 10) of each semantic description to find superordinate concepts in concept pairs, removing these concepts from the current semantic description Stemp (lines 7–9). This step aims at retrieving the most concise semantic descriptions of each document:

Stemp1={D, E},Stemp2={D}.
Finally, the laconic semantic description Slaconic becomes

Slaconic={D(d1), E(d1), D(d2)}.
Balancing semantic description
Many practical applications involve imbalanced data, where instances are not distributed evenly among the available classes [54]. Since this phenomenon greatly affects the performance of classification algorithms, a novel algorithm for alleviating this effect through the creation of more “balanced” data instances (more equally distributed in the available classes) is proposed in this section.

In many cases, the ontology which describes the knowledge of a domain is not entirely representative for a given set of documents D. This is due to the fact that many of its concepts may appear in very few documents or none at all. Since the proposed framework for the semantic enrichment of documents is based on classification techniques, the imbalanced number of classes (concepts) has an impact on the accuracy of the classification, as there might not exist a sufficient number of training documents for each class.

In order to overcome the problem of class imbalance, the KBalancing algorithm (Algorithm 2) is applied to the semantic description S of D. The resulting semantic description S′ is smaller than S, according to Definition 9.

The KBalancing algorithm (Algorithm 2) extracts a k-balanced semantic description S′ from a given S, according to threshold θk (desired level of balance), as explained in Definition 12. In particular, the algorithm repeatedly removes child concepts in semantic label pairs, measuring the level of balance until it reaches the desired one. This process, finally, returns a k-balanced semantic description. It should be noted that θk (k-balancing threshold) is not determined by the algorithm itself, but it is a parameter of the procedure. In the experiments that follow (Sect. 6), the effect of k-balancing in semantic descriptions is examined in more detail. Example 5 demonstrates an application of Algorithm 2, where a 100%-balanced semantic description (S′) is extracted from a 50%-balanced semantic description (S).

figure h
Example 5
(Extracting k-Balanced semantic descriptions) Let D={d1, d2, d3, d4} be a set of documents and S={S1, S2, S3, S4} the set of the respective semantic descriptions in terms of a TBox T. More specifically, let:

S1={A, C}, S2={B}, S3={D}, S4={C, E}.
In order to extract an 100%-balanced semantic description S′ from S, the KBalancing algorithm uses the semantic label pairs of each document in order to remove child concepts, until the desired level of balance is reached. According to the semantic description S and the employed ontology of Fig. 4, the semantic label pairs are:

E={(A, C), (C, E)}.
The algorithm, iterating on the semantic label pairs, removes concept C, as it is a child concept A in the first pair. This concept is also removed from the semantic description and the level of balance (k) is computed according to Definition 12. In particular, C is removed from S1,S4 and k becomes equal to 1.0 which is the desired level of balancing. Therefore, the algorithm terminates returning the balanced semantic description.

Finally, the 100%-balanced semantic description S′ of S is

S′={A(d1), B(d2), D(d3), E(d4)},
as Definition 12 is satisfied.

Initially, the KBalancing algorithm computes the current level of balance k (line 2). Then, as long as it remains below the desired threshold (θk), the main loop is executed (lines 3-25). According to Definition 10, for each pair of semantic labels ej∈E (lines 4–16), every new kj, which results from the merging of the two semantic labels, is determined (line 10). Naturally, it stems from a set of candidate ABox ASj and, obviously, a set of candidate semantic descriptions (lines 7–9). In particular, when kj assumes a value greater than k, then kj is set as the new k, updating the semantic description S′ with the corresponding candidate semantic description Sj (lines 11–15). In the end, the label which maximizes k is removed and the new candidate semantic description S′ is updated (line 13). Subsequently, the semantic description of each document (Sdi) is updated by removing the Lremove label (lines 17–19) and E is updated by removing those pairs that contain the label Lremove (lines 20–24). This procedure is repeated until k≥θk, returning the k-balanced semantic description S′.

Classification
Document enrichment with fresh semantic labels is achieved through the application of classification techniques that map documents to the appropriate labels. In particular, this framework performs multi-class classification that returns a probability for each class (i.e., a semantic label) in order to determine which is the most representative class for each document. Moreover, it should be noted that the confidence level of the candidate classes, which is computed by the classification task, should have a sum equal to one, so as to designate a probability distribution over all available classes. Cases of imbalanced semantic descriptions are handled though the proposition of the methodologies of ontology balancing (i.e., k-balancing methodology of the previous paragraph) and class labeling which described in more detail in the next paragraph.

The purpose of the classification techniques is to semantically enrich documents through their assignment to ontology concepts. More specifically, each semantic label assigned to a document is considered to be a fresh label of the said document, with respect to TBox T.

Class labeling
figure i
figure j
figure k
The output of the classification algorithms is most commonly a probability value for each semantic label of description Si, which determines the most appropriate label for document di. However, there are cases where an appropriate label cannot be found; either because it has not been possible to identify any classes with a sufficient level of confidence or more than one classes have been identified with similar confidence values. In those extreme cases, class assignment is achieved via the proposed ClassLabeler algorithm (Algorithm 3), which decides which of the available class labels fits best the document in question.

Initially, Algorithm 3 checks for the existence of at least one semantic label Aj∈CN with a high confidence value for the given document di. If such a label is found (confdi,Aj>θH), a clear decision can be made and Aj is added to the semantic description Si of di (lines 7–8). On the other hand, if none of the candidate semantic labels has a confidence value greater than θH (lines 8–23), then the labels with confdi,Aj>θL are added to Kdi; the set of candidate labels for di (lines 10–14). Additionally, each semantic label A∈CN which is higher in the ontology than the currently inspected Am (Am⊑A) is also added to Kdi (lines 16–20). In the next step (line 21), the new confidence vector confdi for each document di is computed, according to the proposed NewConfidence algorithm (Algorithm 4). Subsequently (line 22), the new semantic label Anew is retrieved by the proposed SelectLabel algorithm (Algorithm 5). Finally (line 23), the current semantic description of document di is augmented (enhanced) through the addition of label Anew to its semantic description.

Algorithm 4 returns a vector with new confidence values confnew for each candidate label of document d. Intuitively, the new confidence value of a semantic label Am is equal to its previous value, increased by the sum of the confidence values of every label Aj above Am in the hierarchical ontology (line 6). In particular, the algorithm computes a new confidence level for each class, increasing their confidence value by accumulating the confidence level of its subclasses. This assumption is based on the intuition that a superclass includes semantically all the senses of its subclasses because the subordinate classes semantically entail their superordinate one.

Algorithm 5 returns the new semantic label Anew in a bottom-up direction; in the beginning, the value of the maximum depth of the hierarchical representation of the terminology is set as the current depth (line 2). Then, for each candidate label (Aj∈K) whose depth in the hierarchical ontology is equal to the current depth, its confidence value is compared to the current threshold. If it is found to be greater, then the label Aj is assigned to Anew and the current threshold is amended accordingly; otherwise, the label Aj is removed from the set Ki (lines 6–13). Subsequently, the algorithm returns the new label Anew with the maximum confidence value in its level of hierarchy or searches for a semantic label in a higher level of the hierarchal ontology, reducing the depth (lines 14–18).

Example 6 illustrates an application of the ClassLabeler algorithm where a fresh semantic label is assigned to the semantic description.

Fig. 5
figure 5
The hierarchical terminology (T) of Example 6

Full size image
Example 6
(Extracting a fresh semantic label using the ClassLabeler algorithm) Let TBox T be the following hierarchical terminology (also depicted in Fig. 5):

T=⎧⎩⎨A8⊑A4,A4⊑A2,A7⊑A3,   A9⊑A4,   A5⊑A2,   A2⊑A1,A10⊑A5,A6⊑A2, A3⊑A1 ⎫⎭⎬.
Table 2 contains the sample confidence values (conf) for the candidate semantic labels of a document d. Finally, let the higher and lower threshold values be 0.6 and 0.05, respectively.

Table 2 Confidence values of document di per semantic label Aj
Full size table
The objective is to enhance the semantic description S of d through the addition of a fresh semantic label. Since there exists no label with a confidence value greater than 0.6 (θH), the ClassLabeler algorithm (lines 10-14) will consider A6,A8,A9 and A10 as candidate labels, because their confidence values are bigger than 0.05 (θL). The second for-loop in lines 16-20 will also consider A1, A2, A4 and A5 as candidate labels due to the fact that A8⊑A4, A9⊑A4, A10⊑A5, A6⊑A2 and A2⊑A1. Therefore, the set K of candidate labels becomes: K={A1, A2, A4, A5, A6, A8, A9, A10}. Table 3 summarizes the new confidence values per semantic label, as estimated by the NewConfidence algorithm.

Table 3 New confidence values of the candidate semantic labels (Ki)
Full size table
Finally, the SelectLabel algorithm retrieves the optimum semantic label. Initially, currdepth is equal the maximum depth (3). At this depth, three semantic labels are found (A8, A9 and A10) but none of them has a confidence value over θH, so they are removed from the set of candidate labels. Then, currdepth is reduced to 2, where the semantic labels A4, A5 and A6 are found. A4 has a confidence value greater than θH and as a result, this label is added to the semantic description of document d.

System output
The overall methodology aims at extracting the enriched semantic description SE. The semantic enrichment component of Fig. 1, enriches S with fresh semantic labels. As it has been mentioned before, the overall semantic description S={S1, S…Sn} contains the semantic descriptions for each of the corresponding documents of D={d1, d2,…dn}. Respectively, the enriched semantic description SE={SE1, SE2,…SEn}, which is the output of the system, contains the enriched semantic description for each document di∈D, where SEi is the semantic description of document di.

Experiments and results
Setting
Datasets
The proposed framework has been tested on two distinct datasets; the Europeana Fashion dataset, retrieved from the Europeana Fashion Research ProjectFootnote2 [65] and the DMOZ dataset [32], retrieved from the Open Directory Project (ODP)Footnote3, also known as the Netscape DMOZ or the Yahoo! Directory.

Europeana Fashion dataset The Europeana Fashion dataset contains a fashion ontology, based on the Europeana Fashion Thesaurus, that consists of classes describing fashion objects, such as “costumes” or “accessories.” Additional classes represent materials such as “leather” or “silk,” fabrication techniques (e.g., “knitted”) and fashion events (e.g., “gala”) which describe the objects. The total number of classes is 1, 084, with 693 of them being fashion objects, 234 materials and 132 fabrication techniques. The hierarchy that is taken into account is that of isA, class–subclass (e.g., Trousers - Jeans) or part–whole (e.g., isPartOf, hasPart) type of relations among concepts, in order to be in line with the present framework. An example of this hierarchy is illustrated in Fig. 6a.

The total number of documents in the Europeana Fashion dataset is 5278. In the experiments that follow, they were randomly split into a training (4234) and a test set ( 1044), according to a 80%–20% ratio.

DMOZ dataset The DMOZ dataset [32] is maintained by a global community of volunteer editors and has been used widely in the research area of text mining in order to evaluate hierarchical classification models [20, 42, 55, 56, 74]. It contains 3,699,385 documents which belong to 592,555 different topics. A part of the DMOZ hierarchy is illustrated in Fig. 6b. In the following experiments, 10,000 documents belonging to different topics have been randomly selected from this dataset, according to a 80%–20% ratio, with 8000 documents forming the training set and 2000 the test set.

The DMOZ hierarchy is known as a topic ontology [33, 34, 59, 73] where each term in the DMOZ dataset is identified by a URL (e.g., http://dmoz.org/Arts/Literature/Genres/Romance), containing a root category or class (e.g., arts) and successive sub-categories or sub-classes (e.g., Literature). This URLs ends to a unique display name (e.g., Romance). With the exception of the leaf concepts, we assume that the hierarchically structured concepts are semantically related to each other with a broader–narrower, topic–subtopic or class–subclass type of relationship (e.g., Literature is a broader category than its subcategory Genres). In the case of the leaf concepts (i.e., concepts of leaf nodes in the hierarchy), we have instance relations among categories and individuals (e.g., Romance is an individual instance of Genres). Additionally, in the representation of the DMOZ taxonomy, the URLs are used for avoiding ambiguity among classes with the same name. For example, the semantic label History in /Reference/Museums/History has a different semantic meaning than the History label in /Games/Video_Games/History. Therefore, in the DMOZ hierarchy, to discriminate the particular meaning of each term (i.e., node of the DMOZ hierarchy tree), we should assume the hierarchical order the term belongs to (e.g., History is a subclass of Games/Video_Games/, having a different semantic meaning than the label History under Reference/Museums/). Therefore, the characteristics of the DMOZ taxonomy outlined above make it suitable for use in the framework introduced in this work.

Fig. 6
figure 6
Examples of hierarchical ontologies

Full size image
In contrast to the Europeana Fashion dataset, DMOZ documents have only a textual part. For this reason, WordNet [27, 53], the well-known lexical database discussed in Sect. 5.2, has been used in order to extract the semantic features of each document. More specifically, the display string of each DMOZ document is matched to a Wordnet synset, which contains the same sense. Disambiguation problems are handled easily since both hierarchical schemes are known to us. Finally, the ABox consists of the synsets of each topic.

Evaluation metrics
The performance of the proposed approach is evaluated on a set of four distinct metrics. These evaluation measures range from strict or inflexible assessment (i.e., considering that the predicted label must be identical with the golden one) to less strict metrics or flexible assessment (i.e., allowing predicted labels, which have a relationship type with the golden one, to contribute to the accuracy score). More specifically, the less strict measures examine the semantic closeness among the predicted and golden labels. In the definitions that follow, we assume that there exists a golden set of labels, which are expected to be assigned to the respective documents and a predicted set of labels that are assigned to documents by the classification task. In this manner, the labels of the golden and predicted sets are referred to as the golden and predicted labels, respectively. To evaluate this framework, firstly, a set of labels which correspond to ontology concepts have been annotated as golden concepts in the datasets. Then, the framework tries to assign particular concepts as predicted ones to the documents in order to enrich them. The evaluation metrics, which are described in detail below, compare the predicted concepts with the corresponding golden ones, specifying a score according to the employed metric.

Precision The ratio of correctly labeled documents (hits), with respect to the golden set of labels (N), as in Eq. 3:

Precision=hitsN
(3)
Some of the following metrics rely on the taxonomy depth, which is the depth of a concept c (depth(c)) in a hierarchical ontology of concepts. The latter is defined as the number of concepts that are inserted in the path from the root concept up to c. The root concept is assumed to have a taxonomy depth equal to 0, the children of the root have a depth of 1 and so forth. Consequently, the taxonomy depth is increased by one for each concept in the taxonomy path from the root to the leaf concepts.

Average labeling accuracy (ALA) The Labeling Accuracy (LAi) of a document i is defined as the ratio of the depth of the hierarchical ontology representation of the predicted label (depth(predictedi)) to the golden one (depth(goldeni)), if the predicted semantic label is higher in the ontology representation than the golden label. Otherwise, it is equal to zero (Eq. 4)

LAi={depth(predictedi)depth(goldeni),0,if goldeni⊑predictediotherwise
(4)
The Average Labeling Accuracy is the average of the labeling accuracy across all documents in the collection (Eq. 5)

ALA=1N∑i=1NLAi
(5)
ALA assumes its highest value (one) if the system manages to precisely find the golden concept of all documents in the collection.

Average binary labeling accuracy (ABLA) The Binary Labeling Accuracy (BLAi) of a document i is the binary version of the labeling accuracy (Eq. 4). More formally, BLAi assumes the value of 1 if the golden concept is as sub-concept of the predicted label and 0 otherwise (Eq. 6)

BLAi={1,0,if goldeni⊑predictediotherwise
(6)
Similar to the LA measure, the BLA metric is also a strict measure that takes into account a concept that is either identical to the golden one or the golden one semantically entails the predicted concept. It should be noted that the presented metrics provide a range of assessments from strict (inflexible) to less restrict (flexible) measures, so as to evaluate the respective aspects of the presented framework. MALA, a flexible metric, takes into consideration the semantic closeness among concepts, as it is described in detail below.

In a similar fashion to ALA (Eq. 5), the Average Binary Labeling Accuracy is the average of the binary labeling accuracy across all documents in the collection (Eq. 7)

ABLA=1N∑i=1NBLAi
(7)
ABLA assumes its highest value (one) if each golden concept is a sub-concept for each retrieved concept for all documents.

Modified average labeling accuracy (MALA) The Modified Labeling Accuracy MLAi of a document i is a modified version of ALA (Eq. 5) for those rare cases where the predicted label is a sub-concept of the golden label (Eq. 8)

MLAi=⎧⎩⎨⎪⎪⎪⎪⎪⎪depth(predictedi)depth(goldeni),if goldeni⊑predictedi1−dis(goldeni, predictedi)wdis(goldeni)+1, if predictedi⊏goldeni0, otherwise
(8)
The difference between Eqs. 5 and 8 is the addition of an extra clause for the aforementioned case, where MLAi is equal to the ratio of the distance of the ontological hierarchy of concepts from the golden to the predicted label (dis(goldeni,predictedi)), to the (worst) distance of the golden concept from the leaf of the ontology (wdis(goldeni)), subtracted from 1. Indeed, for the same distance between the golden and predicted concept, the second branch of Eq. (8) gives a better score if the worst distance from the golden label to the leaf is long (i.e., significant margin for error) than that of a short distance. In particular, the MALA metric is a more flexible measure than ALA, allowing a predicted concept that is lower in the taxonomy path than its golden counterpart (i.e., a predicted concept that semantically entails the golden one) to increase the metric score. Therefore, this metric takes into consideration the labels that are either hypernyms of hyponyms of the golden concept in the ontology, modifying ALA. Additionally, MALA considers concepts that are in the same semantic path with the golden concept. The said concepts are assumed to fit better to the target concept than other concepts that are not included in the same semantic path.

The Modified Average Labeling Accuracy is the average of the modified labeling accuracy across all documents in the collection (Eq. 9)

MALA=1n∑i=1NMLAi
(9)
The metrics defined above aim at measuring different aspects of the performance of the proposed model, from a specific, or strict, level of accuracy to a more general, less strict, one. The strictest metric is Precision, as the system must precisely retrieve the correct fresh label. ALA, ABLA and MALA are less strict metrics, taking into consideration the relative position of the golden and predicted concepts in the hierarchy of the ontology. For example, if the golden concept for a document is “T-Shirt” and the system returns the more generic parent concept “Shirt,” then Precision will assume a value of 0 but ALA and MALA will assume values near 1 and the most general metric, ABLA, will be exactly 1. Moreover, ALA and ABLA require the golden concept to be a sub-concept of the predicted one while the more general metric, MALA, takes into account the relative distance in-between them. In conclusion, all four metrics examine the performance of the labeling methodology under a wide range of required efficiency.

Moreover, in order to achieve a trade-off between a strict metric (Precision) and a “soft” one (ALA), their harmonic mean (F-measure) is introduced as a final metric (Eq. 10)

F−measure=2⋅Precision⋅ALAPrecision+ALA
(10)
The aforementioned metrics consider one predicted label for each document, which constitutes a limitation for those cases when more than one label is assigned to a document. In an effort to cover this specific case, the presented metrics may be extended to compute the average score according to the total number of labels J that are assigned to documents. In particular, Eqs. 11–15 define Precision(J),ALA(J),ABLA(J),MALA(J) and F-measure(J), considering more than one predicted label, where j and J represent the current predicted label and the total number of predicted labels for each document, respectively.

Precision(J)=1J∑j=1JPrecisionj
(11)
ALA(J)=1J∑j=1JALAj
(12)
ABLA(J)=1J∑j=1JABLAj
(13)
MALA(J)=1J∑j=1JMALAj
(14)
F−measure(J)=1J∑j=1JF−measurej.
(15)
Experiments
The experimental procedure includes six sets of experiments. Firstly, various classifiers have been tested on the task of assigning a fresh semantic label to each document, so as to enhance their semantic description. In order to thoroughly assess the effect of the proposed methodologies of k-balancing and class labeling, the performance of the examined classifiers on the imbalanced semantic description is regarded as the baseline. Out of the examined classifiers, two achieved the best results; the Stanford linear classifier [48] and a Support Vector Machines (SVM) classifier with a Radial-Basis kernel function (RBF) [15]. The optimal hyper-parameters for the SVM classifierFootnote4 have been found to be γ=2−5, C=8 for the Europeana Fashion dataset and γ=−5, C=3 for the DMOZ dataset. In these experiments, the performance of classification techniques on semantic descriptions ranging from imbalanced (k<0.01) to balanced (k≈1) is examined.

Secondly, the effect of the k-balancing methodology on the number of semantic labels is examined on both datasets (Europeana Fashion and DMOZ).

Thirdly, the effect of the ClassLabeler algorithm is assessed. In these experiments, the classification performance of the proposed methodology is measured before and after applying the ClassLabeler algorithm for various k thresholds (i.e., varying level of balancing).

In the fourth set of experiments, the effect of the number of features which are used for document representation is studied. In Sect. 5.3.2, it has been pointed out that the obtained semantic description representations are sparse and consequently the need for the use of dimensionality reduction techniques has been stressed out. Various relevant methodologies have been examined and the best results have been obtained for the chi-square feature selection [43] method. This technique, derived from the chi-square test in statistics, evaluates the dependence between a feature and a classification label. If these two are found to be dependent, then the occurrence of the examined feature makes the presence of the class more likely and consequently this class should remain in the feature space. On the other hand, if a feature is found to be independent of a classification label, it can be omitted, thereby reducing the size of the feature space.

Moreover, in the case of Europeana Fashion dataset, we extract features from its ontology and from WordNet. Specifically, the performance of using only the features of the Europeana Fashion ontology versus using features from both the Europeana Fashion ontology and WordNet (i.e., further enrichment of document representation), is examined. In particular, in Sect. 5.3.1, the lexical database of WordNet has been introduced, in an effort to further improve the labeling accuracy of the proposed framework. The synsets extracted from the textual part of each document expand its semantic description, resulting in a new vector representation. Table 4 outlines the number of extracted features from the fashion ontology before and after WordNet expansion. Feature expansion can only be applied to the Europeana Fashion dataset since the use of WordNet is the only method for extracting ontological knowledge in the case of the DMOZ dataset.

Table 4 Number of features of the Fashion ontology before and after the WordNet feature expansion (Europeana Fashion dataset)
Full size table
Finally, the performance of ClassLabeler algorithm is assessed by varying its θL hyper-parameter (Sect. 5.4.4) . More specifically, the performance of the SVM-RBF classifier with fixed k-balancing and 47 different labels is studied, for various θL thresholds.

Competitive approaches—baselines
As it has been mentioned above, two popular classifiers are used in the proposed framework, that predict new concepts to be assigned to semantic descriptions for their eventual enrichment. In order to assess the influence of balancing semantic descriptions and of assigning labels to difficult cases, two baseline classifiers are considered; (i) SVM, which is regarded as the state-of-the-art approach in the relevant literature [31, 35, 38, 39] and (ii) the Stanford linear classifier [48] that has been described in the previous section. Of course, in the case of the baseline classifiers, the process of k-balancing is not applied. More specifically, when the performance of the framework for varying levels of balancing is outlined in Sect. 6.2, the baseline approach corresponds to the minimum value of k (i.e., the original level of balancing).

Results
Fig. 7
figure 7
Performance of the Stanford linear classifier and SVM with an RBF kernel function in the Europeana Fashion dataset

Full size image
The comparison of the Stanford linear classifier and a SVM classifier, which is illustrated in Fig. 7, reveals that the SVM classifier exhibits an average performance improvement of 3% w.r.t. Precision and over 2% w.r.t. the other three metrics. Consequently, the SVM classifier with an RBF kernel has been solely used the experiments that follow. Finally, Fig. 7 illustrates that both classifiers (Stanford linear classifier and SVM) improve the performance of classification as the semantic description is getting more and more balanced (i.e., increasing the value of k).

Figure 8 depicts, in log–log scale, the exponential reduction of the number of labels when k increases (i.e., balancing the classes of the semantic description) for both datasets.

The combined effect of k-balancing and the ClassLabeler algorithm in document classification is outlined in Fig. 9, for the Europeana Fashion dataset, and in Fig. 10, for the DMOZ dataset. Both figures summarize the results before and after the application of the ClassLabeler algorithm.

The effect of the number of features on the selected classification technique (SVM classifier with a RBF kernel) is illustrated in Fig. 11, where the score of each evaluation metric for a varying number of features on the Europeana Fashion dataset, is summarized.

Figure 12 depicts the performance of the SVM classifier before and after WordNet expansion on the Europeana Fashion dataset. The 300 most important features have been retrieved, using the chi-square selection method.

Finally, Fig. 13 illustrates how each metric is affected by the threshold θL of ClassLabeler algorithm.

Fig. 8
figure 8
The effect of k-balancing on the number of semantic labels for the Europeana Fashion and DMOZ datasets

Full size image
Fig. 9
figure 9
Classification performance before and after applying the ClassLabeler algorithm on the Europeana Fashion dataset

Full size image
Fig. 10
figure 10
Classification performance before and after applying the ClassLabeler algorithm on the DMOZ dataset

Full size image
Fig. 11
figure 11
Metrics versus number of features on the Europeana Fashion dataset

Full size image
Fig. 12
figure 12
Classification performance for the 300 most important features extracted from the ontology only versus the features extracted from the ontology and WordNet for the Europeana Fashion dataset

Full size image
Fig. 13
figure 13
Classification performance for varying levels of the θL hyper-parameter of the ClassLabeler algorithm (Europeana Fashion dataset)

Full size image
Discussion
The effect of k-balancing on the number of labels
As discussed in Sect. 5.4.2, the classification accuracy can be improved through the balancing of the number of labels of a semantic description. However, this activity has the side-effect of reducing the number of available semantic classes (labels), thereby resulting in a less detailed semantic description as we have seen in Fig. 8.

The effect of k-balancing and the ClassLabeler algorithm
As it has been expected, the classification task becomes easier when the semantic description becomes more and more concise (Figs. 9 and 10). In other words, the increased values of k are translated to a more balanced semantic description and an improved classification accuracy.

In the Europeana Fashion dataset (Fig. 9), the ALA metric is clearly improved after the application of the ClassLabeler algorithm, while for the ABLA and MALA metrics, the classification accuracy is alleviated for small values of k. In case of Precision, which is the most “strict” metric, the performance before and after the use of ClassLabeler algorithm seems to be almost the same. Even though in the three out of the four metrics discussed before, the ClassLabeler algorithm does not seem to have a big impact, a more careful observation, especially for small values of k (greatly imbalanced data) and for the ABLA and MALA metrics, reveals that the ClassLabeler algorithm significantly enhances system performance. Apparently, for unbalanced semantic descriptions, when there exist many classes which are hard to be assigned to a document, the employment of the aforementioned algorithm is beneficial.

In the DMOZ dataset (Fig. 10), the ClassLabeler algorithm has an effect on the Precision, ALA and ABLA metrics. Especially in the case of the latter metric, the use of the said algorithm results in a great performance improvement. This is attributed to the fact that the golden concept may be a sub-concept of the predicted one for most of the assigned labels. It should be noted that the ABLA metric does not take into account the distance between the golden and predicted label, like the ALA and MALA metrics do. In conclusion, in the DMOZ dataset, the ClassLabeler algorithm may be useful as it can improve the accuracy of labeling, especially in unbalanced semantic descriptions.

The effect of feature space size
Since the obtained semantic description representations are sparse and consequently the dimensionality reduction techniques are required.

The effect of the number of features on the selected classification technique (SVM classifier with a RBF kernel) is illustrated in Fig. 11, where the score of each evaluation metric for a varying number of features on the Europeana Fashion dataset, is summarized.

According to the results of Fig. 11, it is evident that no further improvement is observed when the size of the feature space exceeds 300. The respective threshold for the DMOZ dataset has been found to be equal to 250. Therefore, assuming that a certain reduction in the number of features does not impact the classification accuracy, a pruning strategy may be applied in order to improve classification.

Feature selection using the WordNet database
The obtained results (Fig. 12) indicate that the additional feature expansion step positively affects the performance of the proposed framework. Or, in other words, the concatenation of the semantic labels extracted from the initial ontology, with the semantic labels derived from other well-populated ontologies may lead to a significant performance improvement.

The effect of the ClassLabeler threshold
As we have seen in Fig. 13, Precision, a very strict measure by definition, decreases when θL rises. On the other hand, ALA increases when θL rises. Indeed, there is a trade-off between the two measures. In order to decide which value is optimal, F-measure, their harmonic mean, is employed. In the given example, F-measure assumes its highest value when θL=0.5.

Summarizing the evaluation results
Overall, the obtained evaluation results indicate that the proposed methodology exhibits a satisfactory performance on at least two distinct datasets. In particular, comparing the framework with the baselines on the imbalanced data, prior to the application of the KBalancing and ClassLabeler algorithms, confirms the robustness of the approach. More specifically, the performance lead is more than 20% (on average) on the Europeana Fashion dataset and more than 70% on the DMOZ dataset. Hence, the outlined approach provides a way for k-balancing the semantic representation of the documents, better adapting them for the classification task. Additionally, an accurate classification may enhance the semantic description with valuable information.

Finally, the results indicate that ontology pruning is a necessary step, since it provides more balanced semantic representations. Another important conclusion is the efficiency of the ClassLabeler algorithm, which gives more accurate results. Even though the newly generated classes are less descriptive than the original semantic labels, the proposed framework improves the classification accuracy via the balancing of the semantic descriptions. Lastly, the F-measure can be used to determine the desired trade-off between Precision and ALA, further optimizing performance.

Conclusions and future work
In this work, a methodology for the semantic enrichment of documents through the addition of labels to their semantic description has been proposed. Initially, a well-defined theoretical model of semantic descriptions has been presented, followed by a binary vector representation of documents, taking into account all of their semantic labels. More specifically, semantic labels may be present in the structured representation of a document or they can be extracted from free-text by an annotator, with respect to a hierarchical ontology which represents the knowledge of a domain. The semantic description of documents is extended by adding tokens with a high semantic weight, using a general lexical-based hierarchical ontology (i.e., WordNet). In order to build a more balanced dataset, with respect to the actual data, the KBalancing algorithm is proposed, which balances the distribution of concepts of the ontology. The documents are classified into semantic labels through the use of classifiers such as the Stanford Linear classifier and SVM with a RBF kernel. Moreover, the ClassLabeler algorithm is proposed, which assigns labels in hard-to-decide cases. Overall, the proposed framework deals with the problem of imbalanced data by adding the semantic labels which best fit the given documents.

In order to more thoroughly evaluate the proposed approach, a set of metrics examining the accuracy of the addition of semantic concepts that match the documents, have been introduced. These metrics estimate the performance of the framework, ranging from a specific and strict level, to a more general level of accuracy. An extensive experimental procedure has been conducted by applying the methodology to two datasets. The obtained results exhibit a satisfactory performance.

The outlined framework may be extended through the extraction of more concepts and features from the documents, using more complicated methods. For example, the semantic labels may also originate from roles, apart from concepts, in order to create richer semantic descriptions for the documents. Another interesting research direction would be to consider more complex, non-hierarchical ontologies. In particular, since this work is mainly based on ontologies that consist of taxonomical (hierarchical) relations among concepts, methods of non-taxonomical relations could be combined with the proposed framework (i.e., investigating non-hierarchical relations among concepts) [57, 71], that could lead to further improvement. Additionally, since this framework is capable of enabling more than one ontologies, a process of ontology alignment and data integration for their efficient combination may be an interesting extension [5, 24, 51]. Finally, the incorporation of continuous vector-based embeddings (as in [21, 66]) into the proposed framework might have a positive impact on its performance.