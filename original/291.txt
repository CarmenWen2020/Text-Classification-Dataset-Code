Energy is one of the most critical resources for sensor devices that decides the network lifetime of the wireless sensor networks. In many circumstances, sensor devices consume more energy for data transmission, reception, and forwarding operations. The major challenge is to increase the network lifetime by implementing the latest research models to reduce the deployment and operational cost. Many existing methods address the application of the static sink with multi-hop routing. But most of them suffer from energy-hole issues and inefficient data collection due to the early death of sensor nodes. Most of the existing methods of learning require massive data with feature engineering which eventually increases the learning complexity. In order to avoid these issues, a robust reinforcement learning-based mobile sink model is proposed for dynamic routing with efficient data collection. In addition, the Q-Learning approach is implemented to induce automatic learning through the shortest route. Combining these strategies preserves network stability and efficiently improves the routing performance as well as the reward. The simulation results reveal that the proposed reinforcement learning-based mobile sink model extends the network lifetime, provides an improved learning time with more reward, and results in high efficiency when compared with existing methods.

Previous
Next 
Keywords
Wireless sensor networks

Internet of Things

Clustering

Reinforcement learning

Q-Learning

Mobile sink

Routing

1. Introduction
Wireless sensor network (WSN) is a bunch of sensor nodes which is used for continuous monitoring and recording of various physical or environmental changes in the form of data for different real-time applications (Zhang and Chen, 2020, Alsheikh et al., 2014). Through the Internet, a group of sensors and other electronic devices are interconnected, allowing for bidirectional exchange of data called the internet of things (IoT) (Li et al., 2018b). In most real-time cases, the operating time of the sensors is confined due to the constraints of their energy sources (Bandyopadhyay and Sen, 2011). When the sensor network is turned on, recharging or replacing battery sources is a difficult task. Therefore, it is an important area of research to understand the problems as well as provide perfect automated solutions to enhance the network lifetime (Kone et al., 2015).

In practical situations, the energy consumption among the sensors may change depending upon their role and responsibility. For instance, nodes that are closer to sink node consume more energy than other nodes because they involve more data transmission and reception operations (Fu et al., 2019). In the existing approaches, sinks are static and data communication is handled by multi-hop routing. During the data transmission, traffic received by cluster head and sink node consumes more energy when compared to other sensor nodes. It thus creates issues such as data loss, premature death of sensor nodes accompanied by energy-hole problem in WSNs (Watfa et al., 2013). These circumstances make the mechanism of transferring data to the base station (BS) complex and further leads to the network partition problem (Dong et al., 2015). Due to the above-mentioned issues, the lifetime of the network is reduced, and quality of service is often degraded.

Improving the network lifetime of WSNs is a crucial task and it can be achieved through various mechanisms such as an automated deployment of sensors (Liu et al., 2013), clustering of sensor nodes (Fanian and Kuchaki Rafsanjani, 2019), mobile sink (MS) implementation, data aggregation, efficient routing, and the efficient use of battery units (Chen et al., 2016). These strategies are implemented in collaborative ways to improve the network lifetime. However, in an asynchronous WSN system, performing the routing and scheduling operation through MS is quite challenging due to the change in number of active nodes over time. Therefore, routing and scheduling of MS should be automated in accordance with the asynchronous behavior of the IoT network environment.

Optimization is one of the fundamental techniques applied to solve several practical applications especially in the areas of WSN based IoT (Wang et al., 2018), communication infrastructure planning (He et al., 2017), and transportation with a good solution (Meng et al., 2018). One of the typical optimization problems is to find the shortest path between the source and destination in WSN based IoT networks, which is similar to the traveling salesman problem (TSP) and a type of NP-Hard problem. Nowadays, to address the key challenges in the combination of WSN and IoT, researchers pay more attention to applying machine learning techniques such as supervised, unsupervised, and reinforcement learning (Wei et al., 2020, Zaheer et al., 2018).

SVM and KNN are supervised algorithms that can be used to solve classification tasks such as security, detection of interferences, classification of images and spectrum sensing (Alsheikh et al., 2014, Jiang et al., 2017). Regression algorithms (Linear regression, Support vector regression, and Gaussian process regression) are under the category of supervised learning method which is used in channel estimation, cross-layer handover, and mobility prediction (Alsheikh et al., 2014, Charrada and Samet, 2016). K-Means, principle component analysis and isometric mapping is the type of unsupervised algorithms used in clustering, localization, and service segmentation (Wang et al., 2018). Reinforcement learning algorithms (Markov decision process, Q-Learning, policy gradient, actor critic, and deep Q-network) are used in packet transmission, decision making, spectrum access, energy harvesting, network sharing, routing, and gaming (He et al., 2017, Jagodnik et al., 2017, A.M. et al., 2018). More specifically, reinforcement learning (RL) approach plays a major role in finding the shortest route automatically (Al-Rawi et al., 2015).

In the near future, the advantages that RL brings to routing are highly necessary to encourage appropriate research interests such as automated gaming, robotics, WSN, IoT, etc., (Bello et al., 2016, Zhang et al., 2018). It can also be applied to WSN-based IoT applications to monitor the environments and perform the data routing. RL algorithms can be classified into two types: model based and model free algorithms. The model based algorithm estimates the optimal policy by using the transitions and reward functions, where as the model free algorithm incorporates a learning method based on values. The value function is updated on the basis of an equation (Bellman) or greedy policy.

1.1. Motivations
There are obvious drawbacks to traditional methods, which are presented here. The majority of the existing cluster head node selection methods focus exclusively on residual energy and not on other factors (Li et al., 2018a, Wang et al., 2018, Gupta and Jana, 2015). In WSNs and IoT environment, most data needs to be forwarded by cluster heads that are closer to the sink node. As a result, these cluster head nodes have created the energy hole problem by losing their energy faster than other cluster heads that are also far from the sink (Watfa et al., 2013, Liu et al., 2013, Al-Sodairi and Ouni, 2018). Most of the existing works did not achieved an efficient data transfer method due to multi-path routing (Khalily-Dermany and Nadjafi-Arani, 2017). Nevertheless, the data transmission mechanism can be automatically improved to increase the life of the network. The optimal route selection approach for the mobile devices is not fully explored in the latest methods (Kallapur and V., 2011, Gu et al., 2013). Subsequently, the operational scope of data collection methods for WSNs and IoTs must be extended using reinforcement learning techniques.

1.2. Contributions
Data collection using mobile devices in WSNs and IoT has received a great deal of industry interest (Zhang and Chen, 2020, Khalily-Dermany and Nadjafi-Arani, 2017, Yun and Xia, 2010, Liu et al., 2017). When the mobile device reaches the cluster head node, the data is transmitted directly to the mobile device. Multi-path routing or data forwarding is completely avoided in this situation, and MS delivers data efficiently to the base station (Krishnan et al., 2019). Adopting the RL algorithm, the mobile sink nodes traverse path intelligently and hence prolongs the network lifetime in WSNs and IoT environments. As a result, manual intervention is entirely ignored and the environment gets transformed into a smart environment. This research aims to adopt the model free RL mechanism that enables an MS node to observe its traveling environment and then performs automated routing for data collection. The purpose of this research is to extend the network lifetime while retaining the highest level of dynamic WSN environment management using the Q-Learning method. In order to extend the network lifetime, novel processes such as enhanced cluster creation, dynamic route selection, and data collection are being developed and their performance are tested.

•
Cluster formation: By evaluating the remaining energy, the CH building phase chooses the cluster’s head node dynamically. The CH node of each cluster should be dynamically elected since the remaining energy of each sensor node depletes over time (each iteration). The network’s energy consumption is balanced with the dynamic CH election to ensure that all sensors can continue to function effectively.

•
Dynamic routing using mobile sink: A model free Q-Learning approach coupled with a RL algorithm is adopted for finding the shortest route while performing automated data collection. In order to perform effective data collection and to avoid hot spot problems, the Q-learning approach and the mobile sink method are used. When the mobile sink moves in the predetermined trajectory of the network region, it seeks out the nearest available CH for gathering the data. It then proceeds to the next closest CH, and so on until all CHs in the network have been visited.

•
Performance evaluation: A detailed analysis and comparisons are performed based on the proposed Q-Learning approach in order to improve the network lifetime and reward. The performance of the Q-Learning protocol is compared against the traditional (leach), heuristic (genetic algorithm), and machine learning (k-means) algorithms on several criteria, such as, network lifetime, first node death, final node death, energy consumption, average packet loss, standard deviation, and average tour cost. In addition, performance evaluation is made by considering the state-of-the-art method. Furthermore, in order to comprehend the core of mobile sink with RL routing in the network, the reward performance is also examined.

The remaining part of this work is organized as follows: Section 2 explores the recent research work based on clustering and MS based data collection schemes by considering various algorithms. Section 3 presents the system model which includes energy and network model. In Section 4, a modified leach algorithm is employed to form the clusters and Q-Learning mechanism is adopted for finding the shortest route. The experimental results are presented in Section 5, and the significant findings are summarized in Section 6.

2. Related works
With comparison tables, the existing algorithms (LEACH, GA, and K-Means) and state-of-the-art methods are explained in this section.

2.1. Existing algorithms
LEACH is a simple WSN protocol used to cluster the nodes in the network and to route the data from source nodes to sink (Heinzelman et al., 2002). For each cluster, a node is randomly selected as the cluster head and a random rotation strategy is employed to pick CH from all active sensor nodes. However, the residual energy, the node density, and the node locations are not taken into account when choosing the CH. Hence, the performance of LEACH continually suffers from energy hole and hotspot issues (Liu et al., 2021).

The GA is a well-known meta-heuristic method for dealing with optimization issues (Wang et al., 2018, Gupta and Jana, 2015). It begins with the development of random feasible solutions known as the initial population. Each individual solution is known as a chromosome, and it evolves through successive iterations called generations. Chromosomes are represented by a single string or by an array of genes, and the lengths of the chromosomes in the population are equal. Each individual solution is evaluated using the fitness function in order to estimate its performance. The fitness function is designed such that an individual solution gives an outcome near the optimal solution. Any two chromosomes are selected randomly as parents after generating the initial population, and they exchange their genetic information to produce two child chromosomes. This process is called crossover. To produce better solutions, the child chromosomes undergo the mutation process. The lost genetic values are restored in this process of mutation. After the fitness of the child’s chromosomes is calculated through crossover and mutation, it is evaluated and compared with that of all the chromosomes of the previous generation. Finally, the two chromosomes of the previous generation with the poorest fitness are replaced with newly generated child chromosomes to produce better results.

K-means is one of the most popular algorithms that can be implemented easily (Khan and Ahmad, 2004, Ray, 2016). In K-means, the data points are divided into many clusters that minimize the sum of squared distance between the center of the clusters and the data points. It is not ideal for massive quantities of data since the time complexity is high. Backtracking is one method that has been suggested to reduce the computational complexity of K-Means. Initially,  data points are chosen at random from the n data points to serve as clustering centers. After that, the algorithm determines the distance between each data point and each of the  clustering centers and assigns the data point to the cluster of the closest center. As soon as all the computations are completed,  new clusters are created. Following that, the algorithm updates the mean value of each new cluster and creates a new clustering node. According to the procedure, it demonstrates that the K-Means method updates the distance calculation further and iterates until the result converges.

Algorithms comparison
Table 1 demonstrates that existing methods have been compared with different parameters. We considered the traditional LEACH, heuristic GA, and machine learning K-Means algorithms to gain a better understanding of the methodologies.


Table 1. Algorithm/Protocol comparison.

Features	LEACH (Heinzelman et al., 2002)	GA (Wang et al., 2018)	K-Means (Khan and Ahmad, 2004)
Clustering	Random	Heuristic	Machine learning
CH selection	Random	Node energy, location	Random
Load balancing	Not achieved	Not achieved	Not achieved
Communication	Multi-hop	One-hop	One-hop
MS routing	Not considered	Considered	Not considered
Data loss	Yes	Yes	Yes
Pros.	Less computation	Near optimal	Fast, easy to implement
Cons.	Hot-spot problem	Local optima	Local optima
2.2. State-of-the-art methods
More recently, the diverse innovations of WSN and IoT applications have drastically altered the strategies for data collection. The authors of Praveen Kumar et al. (2018) proposed the Ant Colony Optimization-based MS path determination (ACO-MSPD) algorithm to select a set of rendezvous points (RPs) and find the shortest route to collect data from them. By choosing dynamic RPs, ACO-MSPD uniformizes node energy consumption and extends the lifetime of WSNs. But, the computational complexity of the ACO-MSPD is high, and the optimality depends on the ACO parameters. In Gupta and Saha (2020), the authors used Artificial Bee Colony (ABC) and differential evolution to implement the load balanced CH formation. Delay, average energy, and intra-cluster distance parameters were taken into consideration, but failed to consider the tour cost, resulting in degradation of network performance. In Vijayashree and Dhas (2019), an algorithm for clustering and data processing is proposed using the ABC algorithm. The position of CHs is taken into account, and the MS mobility is determined by a random walk. The use of more mobile sinks at the same time makes the network more difficult to manage.

Selection of RPs on the basis of entropy weight is implemented based on node density, residual energy, and average delay in Zhang et al. (2019) and to find the best access route for a mobile sink, an ACO is adopted. The RP selection process is not optimized and hence it results in high data collection latency. Wang et al. (2020) built an efficient routing algorithm based on an elite-hybrid algorithm for the optimization of WSNs, to extend their life span using a sink node. The elite subset of three optimization algorithms is used to determine the optimal path for a MS. For the larger networks, the suggested method becomes incapable of finding viable solutions.

The authors of Wen et al. (2018) suggested the EAPC algorithm to increase the lifetime of WSNs using a MS. Primarily, it consists of three phases: the construction of MST using Prim’s technique, the selection of center points, and the construction of paths using the convex polygon technique. Due to the convex polygon method, the EAPC constructs a longer travel path for MS. This resulted in high data loss for a larger network because of the increased delay of MS. In Habib et al. (2020), the authors formulated an optimal data collection schedule for a WSN as a MILP (mixed integer linear programming). This method discovers an optimal group of rendezvous nodes over a preformed starfish routing algorithm, and finds the corresponding sojourn time duration, to optimize network lifetime. Because of the limitations of MILP and the computational complexity of the starfish system, it suffers in performance for larger WSN networks.

VD-PSO algorithm for determining the MS path for reliable data collection over a WSN is proposed in Wang et al. (2017). The method identifies certain locations that are one-hop away from the RP node, and the MS goes to the RP location to gather data directly through sensor nodes. It makes the best tour among the RPs, but it is unsuitable for large networks due to automatic selection of multiple RPs, which increases the delay of MS. In Redhu and Hegde (2019), energy-efficient data aggregation technique is being implemented with the use of landmark-assisted MS scheduling. A method for identifying landmark nodes and clustering is achieved by performing random walks over the graphs. The locations of landmark nodes are used to aggregate the sensor data by applying an extended Kalman filtering method. Since the entire process requires more computation, it is not recommended to use larger networks.

To address the combined energy and load balance problem, a Genetic Algorithm (GA) based clustering and routing algorithm is proposed (Gupta and Jana, 2015). Depending on the transfer distance and number of hops from a CH node, the fitness function is set for travel. Only the amount of cluster member nodes can make up the load on each CH. Besides data collection from cluster members, CHs in the network must also relay data from previous hop CHs. As a result, the CHs load is not evenly distributed. In order to reduce energy usage and improve QoS, K-Means algorithms were employed in the Low-Energy Adaptive Clustering Hierarchy (LEACH) protocol to elect the CH (Gantassi et al., 2020). The findings indicated reduced consumption and delay, as well as enhanced system stability and throughput. This methodology excluded the use of the MS and the evaluation of the LSWSN.

The authors suggested a query-driven virtual wheel-based path-finding technique (QWRP) for WSNs with MS (Jain et al., 2019). QWRP network configuration goes through four distinct phases: network initialization, advertising MS location, forwarding of queries to MS, and forwarding the data to MS. Adopting QWRP paradigm, the communication costs associated with finding the MS position in the network are high, while also making traffic costs incurred by query messages larger. In Donta et al. (2020), RPs in wireless sensor networks are identified using an unsupervised learning-based hierarchical agglomerative clustering technique, and routes between RPs are identified using an mobile sink tour planning (MSTP) strategy. Clustering using an unsupervised learning technique necessitates substantially more data for the learning process. However, their suggested method operates well in smaller networks, while the supplied MSTP method has a higher computational cost in larger networks.

An effective energy conservation scheme based on tree building to minimize the transmission distance of data is implemented in Miao et al. (2018). The MS only collects data from the root node, and the sensor nodes closer to the root node die quickly during this operation due to the high data forwarding. In Zhao et al. (2015), the authors proposed a tree-based mathematical model for increasing network lifetime when collecting data with MS. In this analysis, the authors used the MLS algorithm to locate the tree’s root node, which is the position where the MS goes to collect data. Both approaches have an effect on the issue of hot-spot (Miao et al., 2018, Zhao et al., 2015). The firefly methodology has also been used in other MS-based tour construction and data gathering investigations (Krishnan et al., 2018, Yogarajan and Revathi, 2018). Table 2 summarizes the various WSN techniques on which related works are based.

RL is a subset of ML technique whose aim is to create self-learning models through the use of learning techniques. It is concerned with decision sequences in which each decision has an effect on the subsequent decisions. This is not the case for optimization or other algorithms, which concentrate instead on individual decisions.


Table 2. Comparative analysis of existing research techniques.

Method type	(Wen et al., 2018)	(Wang et al., 2020)	(Heinzelman et al., 2002)	(Zhang et al., 2019)	(Praveen Kumar et al., 2018)	(Khan and Ahmad, 2004)	(Miao et al., 2018)	(Ray, 2016)	(Vijayashree and Dhas, 2019)	(Zhao et al., 2015)	(Krishnan et al., 2018)	(Liu et al., 2021)	(Habib et al., 2020)	(Gupta and Saha, 2020)	(Yogarajan and Revathi, 2018)	(Redhu and Hegde, 2019)	(Gantassi et al., 2020)	(Jain et al., 2019)	(Donta et al., 2020)
Clustering	✓	✗	✓	✓	✓	✓	✓	✓	✓	✗	✓	✓	✗	✓	✗	✓	✓	✓	✓
Collection points	✓	✓	✗	✓	✓	✗	✓	✗	✓	✓	✓	✓	✓	✓	✗	✓	✗	✓	✓
Mobile sink	✓	✓	✗	✓	✓	✗	✓	✗	✓	✓	✓	✗	✓	✓	✓	✓	✗	✓	✓
Tree topology	✓	✗	✗	✗	✓	✗	✗	✗	✗	✓	✗	✗	✗	✗	✓	✗	✗	✗	✗
Path cost	✓	✓	✗	✓	✓	✗	✓	✗	✓	✓	✓	✗	✓	✓	✓	✓	✗	✗	✓
Homogeneous n/w	✓	✓	✓	✓	✓	✓	✓	✓	✓	✓	✓	✓	✓	✓	✓	✓	✓	✓	✓
Heterogeneous n/w	✗	✗	✗	✗	✗	✗	✗	✗	✗	✗	✗	✗	✗	✗	✗	✗	✗	✗	✗
Static senors	✓	✓	✓	✓	✓	✓	✓	✓	✓	✓	✓	✗	✓	✓	✓	✓	✓	✓	✓
Mobile sensors	✗	✗	✗	✗	✗	✗	✗	✗	✗	✗	✗	✓	✗	✗	✗	✗	✗	✗	✗
Optimization	✗	✓	✗	✓	✓	✗	✗	✗	✓	✗	✓	✗	✗	✓	✓	✗	✗	✓	✓
M.L	✗	✗	✗	✗	✗	✓	✗	✗	✗	✗	✗	✗	✗	✗	✗	✗	✓	✗	✓
Routing the data to the CHs from the sensor nodes in the dynamic WSN environment using an RL-based method is not considered in any of the above existing works. As a result, sensor node energy consumption is not distributed uniformly across the network. Either data is relayed across additional hops or a greater power level is used when transmitting directly, which consumes more energy. Most of the applications that handle these activities manually encountered a number of difficulties during the transition from traditional wired and manually built networks to more adaptive and dynamic networks. The majority of existing networks have grown so complex that even in-depth manual administration is not possible and the management of most existing WSN networks has progressed beyond the capabilities of manual administration. RL technology plays a major role in replacing manual networks with fully adaptive and dynamically configured networks. Therefore, the proposed Q-Learning-based MS implementation discovers and makes use of dynamic pathways to the MS, and it assists the MS in traversing the entire network region for data collection while selecting the most efficient route for each available CH, thereby extending the lifetime of the WSN.

3. System model
Due to the exponential growth of electronic devices, the modern environment requires a communication infrastructure with an efficient quality of service. Latest advancements in Information Communication Technologies (ICT) have been highly expected to monitor real-time applications. In the current scenario, most of the environmental monitoring tasks are achieved using the combination of WSNs and IoTs (a smart way to monitor the event-driven environments). The list of symbols and their definitions used in the proposed RL method are described in Table 3.

In general, the sensors are deployed randomly in a particular region, and the whole region is divided into several clusters. CH is chosen according to the energy level of the sensors in each cluster. Mostly, the sensor with the highest energy often serves as a CH and the rest of the sensors act as members of the cluster. All members of the cluster will send their data to the CH node and the CH node takes the responsibility of sending the data to the base station by implementing a data transmission mechanism. In the proposed approach, the data forwarding mechanism is replaced with mobile sinks (Cheng and Wu, 2009). Consider the following to implement WSN: (i) all the sensor nodes are homogeneous and possess the same operable capability such as sensing, processing and transmitting or receiving the data, (ii) there are no obstacles present in the region.

3.1. Energy model
In basic context, the sensors will consume energy in any kind of data communication system. A typical radio model is adopted for data transmission and reception (Heinzelman et al., 2002). For transmission, energy can be consumed by operating the radio unit and power amplifier. Energy consumption for transmitting  bits of data to a distance  can be calculated as: (1)
 where 
 is the transmission power needed to transmit data through the wireless medium, 
 represents the energy consumption in the free-space model, and 
 represents the energy consumption in the multi-path model. The energy required by the receiver to obtain  bits of data is provided by: (2)
where 
 is the energy required to receive data over a wireless medium.

3.2. Network model
Fig. 1 depicts the network model containing sensor nodes and MS. MS finds the best route (red trajectory) to reach all the CH for data collection. The entire network is clustered by implementing the enhanced leach algorithm. MS is employed to perform routing with efficient data collection by using the Q-Learning technique. Assume that the MS has unlimited energy and it can travel anywhere in the network region. CH locations are supplied to MS from the BS, and BS is also responsible for executing the Q-Learning algorithm to find the shortest route. While routing, MS collects the data from all the CHs (Yu et al., 2014, Gu et al., 2016). MS is provided with enough buffer, stores the collected data from CHs temporarily and delivers the data to BS. In the WSN literature, there are a variety of definitions available to predict the network lifetime (Dietrich and Dressler, 2009). In this proposed approach, the network lifetime is defined as the time at which the last node dies in the sensor network due to the implementation of MS.

Fig. 1. Network model for mobile sink.


Table 3. List of symbols.

Notation	Description
Set of sensor nodes, 
.
Set of cluster heads, 
, .
Cluster head senor node.
Energy consumption in the free-space model.
Energy consumption in the multipath model.
Energy required for the radio unit.
Energy required for transmission.
Energy required for reception.
Number of bits transferred or received per second.
Data transfer distance over a wireless medium.
Distance between the cluster head 
 and 
.
Residual energy of the sensor node.
Initial energy of the sensor node.
Energy required to receive the data.
Desired percentage of CH.
Current iteration (round) number.
Threshold energy of the sensor to be chosen as CH.
Number of cluster heads.
Set of sensors as graph.
Random value for leach algorithm.
Set of vertices.
Set of edges.
Set of rewards in Q-Learning.
Set of states in Q-Learning.
Set of actions in Q-Learning.
Total gain in the Q-Learning.
Learning rate.
Discount factor.
Control parameter.
State action pair.
Q-value for a state .
Expected discount cumulative reward for a  pair.
Distance matrix for the storage of distance values.
Immediate reward.
Delayed reward.
Queue matrix for the storage of Q-values.
4. Proposed approach
The purpose of RL is to optimize an agent reward by taking a series of actions in response to an environment. In the Q-Learning approach, the environment refers to the cluster heads (CHs) or objects that the agent (MS) is in action, while the agent represents the Q-Learning algorithm or a software. Routing environment starts by sending a state (next CH location) to the agent (MS), from the state information the required action is taken, and the corresponding reward will be given. Fig. 2 illustrates the fundamental RL model for the IoT-based WSN environment monitoring.

Define the collection of sensors as a network , with each node (sensor or CH) as a vertex 
 and every 
 edge can conduct a two-way communication between the 
 and 
 node pairs. Assume that  represents a single source node and  represents a group of target nodes. The minimum cost path from the source vertex  to all of the destination vertices  is known as routing to multiple destinations. With vertices including the source and destination, the Q-Learning 
 algorithm will actually determine the shortest path. The cost of a route  would be lower if it(MS) covered all destinations, resulting in less travel distance and more benefits.


Fig. 2. Reinforcement Learning.

4.1. Clustering
In order to perform the clustering, the enhanced leach algorithm is proposed and it comprises two stages, namely setup state and steady state. In the setup state, sensors are deployed in the environment, clusters are formed and their corresponding CH’s are created. For environmental monitoring and data transfer, each node spends some energy depending on their location, responsibility, and action. Most of the time, the energy dissipation is based on the travel distance of data. CH selection for the initial round can be expressed as: (3) 
 
 where  denotes the probability measure signifying the desired percentage of CH,  is the current round number and  is a set of nodes that are not selected as CH in  last rounds. In the LEACH algorithm, CHs are selected randomly from sensor nodes and all sensors get an equal chance to become a CH. In order to improve the network lifetime, residual energy and the initial energy of each sensor are also considered by the modified clustering method. Therefore, CH selection for the rest of the rounds are given as: (4) 
 
 
 where 
 and 
 denote the remaining energy level of the sensor node and the initial assigned energy level, respectively.

During the steady state, each CH collects data from their cluster members, aggregates the collected data and transmits it directly to the destination based on time division multiple access (TDMA). In TDMA, the time is divided into equal intervals called rounds and the clustering algorithm is executed using Eq. (4). A random number between 0 and 1 is assigned to each sensor. If the number is smaller than the threshold value of Eq. (4), then the node will be CH. The clustering process is continuously performed until the death of the last node in the sensor network and the Algorithm 1 describes in detail the process of cluster head formation.

4.2. Routing
Upon successful cluster formation the base station receives all CH coordinates and there upon RL approach is implemented while performing the data collection to find the shortest route. Calculated route information is updated in MS and as per the shortest distance, MS reaches all the CHs and collects the data. CH adopts a basic data fusion strategy that reduces packet length, minimizes energy consumption and reduces delays when transmitting data to the MS. Once the MS detects it is in range, it broadcasts the Ready-to-Receive signal to the CH. CH sends an ACK message to MS, indicating that the data gathered has been sent to MS. MS advances to the next CH after the current CHs data is collected. Based on RL terminology, CHs are known as states. Therefore, the agent (a software program running in an electronic system that helps to find the shortest route) is seeking the shortest path to be followed by a sequence of actions  taken by states  and getting rewards . During the early stages, the RL software or algorithm does not know the best action to take in a specific state. In the later stages, seeking the shortest route is the most essential learning goal.

Reinforcement learning
RL is a process of mapping states, rewards, and actions to a set of CHs. Given a set of routing distances between CHs, the routing problem is to find the shortest path to visit every CH. In this case, MS will start from the initial CH (source), visit the rest of CHs (nodes) in the network, and finish at the same initial location of CH (source), which makes it a perfect round trip. For example,  represents the initial location of the MS (CH
) and  represents the action taken by the agent, i.e. MS is able to visit the next possible CHs (
) from its current location. Each time-step, the state and action is changed to a new state and new action, the appropriate reward will be earned until the optimal path is found.

The goal of the agent is to maximize the sum of the rewards over the long term. However, the agent is not entitled to receive the highest immediate reward for every action. In some instances, some of the rewards may be delayed. More specifically, the agent will learn through experience to find the best strategy that generates the highest reward over the long term. Therefore, the learning objective is to maximize the expected reward, which can be expressed as (5)
where  denotes the discount factor whose value is maintained between [0–1] and  denotes the reward value. If  value is closer to 0, the agent puts more emphasis on the immediate reward . The agent tends to focus on delayed reward  if the discount factor is closer to 1. Hence, (6)

Q-Learning is one of the most popular model free RL algorithm which is employed in the present study. The goal of the Q-Learning algorithm is to estimate the reward by following a policy whenever the agent takes an action for a particular state. These identified values are referred to as Q-values, which estimate the quality and encourage the agent to take the best possible action in a given state. In addition, the learning algorithm allows the agent to understand the values of  pairs by continuous updates that guarantees an optimal policy. The update rule for  pairs is represented as follows: (7)
where 
 denotes the current state, 
 denotes the current action, 
 denotes the reward, 
 is the next state, and  denotes the learning rate. Note that the agent observes 
, 
, 
, and 
, during  pair update, Q-Learning considers the best possible action 
 in the next state, irrespective of the action taken by the current policy.

The detailed Q-function (Q-Matrix) initialization and update mechanism follows: -function uses the Bellman Equation and takes two inputs: state  and action  described by (8)
where 
 is the -value for a given state and 
 is the expected discounted cumulative reward for a given state and action.

 value is updated using the following equation, (9)
where 
 is the new -matrix (value),  is the learning rate, 
 is the old  value, 
 is the reward,  is the discount factor and 
 is the estimate of optimal future value. Agent detailed learning process for a mobile sink to find the shortest route includes: finding the distance between cluster head nodes 
 
 and 
 
 within a sub-region is given by (10)
where 
 denotes the coordinates of cluster head 
 and 
 are the coordinates of cluster head 
.

State  is the current location of the MS at time . The possible routes are the next available CHs that the mobile sink can visit. Now, we need to find the immediate reward for going from cluster head (
) to the cluster head (
). The main objective is to gain the reward using a monotonic descendant function with respect to the travel distance, i.e., lesser the travel distance, higher the reward be. An immediate reward can be calculated as: (11)
 
where  is the traveling distance between the cluster head (
) and cluster head (
). We are neglecting that there is a  distance (travel) between the two cluster heads. The obtained delayed reward can be expressed as (12)Update the Q-Matrix () using the below equation: (13)In Q-Learning approach,  represents quality, and in this study, quality represents how useful a given action is in gaining some delayed reward. Q-Matrix () becomes a reference matrix for the agent  to select the best action based on the Q-value. Using Q-Learning, the process of finding the shortest path formation for MS follows:

i. Initialize Q-matrix (Q-table) values to 0.

ii. Set the  value, to identify the best route to perform the travel.

iii. During the learning approach, generate a random value between 0 and 1, compare the random value with . If the random value is less than  perform the exploration (select a random path), else perform the exploitation (select the best well known path with future reward).

iv. At each epoch, update Q-values by using the (13) to gain more future rewards.

v. Adjust the Q-values  depending on the difference between the current discounted values and the previous values.

vi. Use  parameter to discount the new values, and  parameter to deciding the learning rate.

The route formation process is continuously carried out until the stopping condition is reached. The pseudo code of the proposed RL based Q-Learning algorithm is given in Algorithm 2, Algorithm 3 describes the process of finding the next feasible CH to travel by the MS, and Algorithm 4 is for updating Q-value.


5. Results
The numerical simulations are carried out using Matlab 2020a and the corresponding pertinent parameters are displayed in Table 4. In order to evaluate the performance of the proposed clustering and routing approach, we consider three different cases: the first case accounts for the traditional clustering and routing in WSN and IoT; the second case represents the MS-based approach described in 4.1 Clustering, 4.2 Routing for clustering and routing in WSN and IoT; the third case investigates how the proposed technique compares to the state-of-the-art methods when applied to various network metrics. The performance of Q-Learning is described in Section 4.2 to understand the reward. Further, the performance analysis is evaluated for first node death, last node death, number of dead nodes in each round, average energy consumption, packet loss, standard deviation, and tour length for varying CHs from 10 to 30. Thereupon, employing the network lifetime defined in Section 3.2 the numerical simulations are performed.

In order to understand the efficiency of the proposed Q-Learning algorithm, we require a sample range of various important parameters of interest. In this regard, Q values are initially set to 0 and the varying range of other parameters such as nCH (number of cluster heads), number of epochs,  (learning rate),  (discount factor), and  (control parameter) are displayed in Table 5. The deployment of sensor nodes and the construction of cluster heads is depicted in Fig. 3.


Table 4. WSN details.

Parameters	Value
Area of network	200 . 200 m
Sensors	100–500
Deployment	Static
Sensor initial energy	0.5 J
Number of rounds	2000–5000
Clustering technique	Dynamic
Probability of CH	0.05
Data collection method	MS
4.602 μJ/bit
2.34 μJ/bit
5.1. Traditional approach
The network lifetime due to the death status of the initial final node are shown in Fig. 4, Fig. 5, respectively. From Fig. 4, Fig. 5, we infer that the proposed solution maintains a greater network lifetime as compared to other conventional systems. The reason behind the enhanced network lifetime is the replacement of conventional data transmission techniques by the MS strategy. In order to transfer the data, most of the sensor nodes select one-hop communication to transfer their data to the corresponding CH. Furthermore, the proposed Q-Learning approach dynamically seeks the shortest route compared with all the available CHs to perform data collection. Hence, we observe that the combination of MS and Q-Learning methods increases the lifetime of the network.

5.2. Existing approach
The iteration at which the death of the first and last node for the proposed Q-Learning routing method is illustrated in Fig. 8, Fig. 9. The proposed approach considers that the residual energy of nodes for clustering (CH selection) and MS based on Q-Learning performs optimal routing with efficient data collection and thus enhances network lifetime. Fig. 10 signifies that as the number of rounds increases, the number of dead nodes for the proposed model is less than the traditional model (Fig. 6). Therefore, the proposed approach consumes less energy because most of the data forwarding operations are reduced in CH nodes, which in turn saves the energy utilized by the sensors significantly. Hence, the proposed approach surpasses the issues faced by traditional WSN and effectively improves the network lifetime.

The average packet loss is defined as the percentage of data packets received by the MS to the number of data packets generated by the sensor nodes. Fig. 7, Fig. 11 demonstrate that the suggested data collection approach using Q-Learning achieves lower average packet loss than existing approaches due to the complete avoidance of data forwarding technique. Since the lower value of average packet loss represents the reliability of the data routing over the RL implementation, we see that the reliability of the data routing across Q-Learning implementations is significant. The graphs further illustrate that for the smaller and medium-sized network, the suggested strategy works efficiently. In contrast, the number of sensor nodes and the rate at which packets are produced are increasing and the need to send more control packets to the MS for data collection, thus increases the operational overhead of the network.

Fig. 8. Death of initial node.


Fig. 9. Death of last node.

Table 6, Table 7 show the traditional and proposed strategy of average energy consumption with different sensor nodes when a first node is dead in the WSN. Sensor density is increased from low (100) to high (500). According to the results of Table 6, Table 7, it is apparent that the proposed learning technique consumes less energy than the existing algorithms. For instance, where the number of the sensors is 100 in both Table 6, Table 7, the proposed method with Q-Learning MS reduces the average power consumption from 12.3606 Joules to 11.2481 Joules, and similar findings are observed for the rest of the values in Table 7. Therefore, the use of Q-Learning based MS helps to increase the balance of energy use and lower energy consumption for the sensor nodes.

Fig. 10. Number of dead nodes.


Table 7. Avg. energy consumption in joules.

No. of nodes	Leach	ga	k-means	Proposed
100	15.2844	14.7892	12.3591	11.2481
200	28.2989	25.8192	24.3313	23.2997
300	42.6319	39.9248	36.6309	34.9463
400	51.6185	49.7438	48.6924	46.7300
500	63.5915	62.0334	59.7322	58.5006


Fig. 11. Average packet loss in (%).

5.3. State-of-the-art methods
This subsection compares the performance of the proposed RL algorithm to existing state-of-the-art methods EAPC (Wen et al., 2018), QWRP (Jain et al., 2019), and HACDC (Donta et al., 2020) in terms of first and last node death conditions, average energy consumption, number of dead nodes on each round, average packet loss, standard deviation, and tour length. The EAPC determined the data gathering path based on the weight assigned to each sensor. EAPC compared the path length from the current position to the next position in the route selection process, whereas HACDC only accounted for the distance from the current position to the end of the path. Using convex polygons, the moving path of the MS is established in EAPC. When compared to the suggested RL technique, HACDC necessitates more data for learning, and QWRP has an impact on the communication cost of query processing and locating the MS for collecting data.

The comparison of the state-of-the-art techniques to the proposed RL algorithm’s initial and last node death conditions is shown in Fig. 12, Fig. 13. When it comes to network lifetimes, all four techniques follow a similar pattern, with lifetimes increasing in direct proportion to the number of sensors. In comparison to existing routing strategies, even when routes advised by RL outperform in both cases, it is due to the energy balanced CH selection and the dynamic routing technique combined with MS data collection, which results in superior performance.

The standard deviation (stdDev), which is a measurement of how much energy a sensor node uses to complete a data transfer or reception operation, is also utilized in WSN to determine the level of energy balance among the nodes. The number of sensor nodes in this investigation ranges from 50 to 500. A definition (expression) of  is given below. (14)
 
where 
 and 
 denote the energy utilization of node 
 and the average energy usage of all sensor nodes, respectively. A low  value implies that all nodes’ energy consumption is balanced, which enhances the WSN’s network lifetime. In Fig. 16, when discussing state-of-the-art algorithms, when assessing the path length between the present data collection point and the next data collection point, the EAPC has a lower overall performance than the QWRP. The proposed method evaluates the optimal route and dynamically employs the shortest route construction with Q-Learning policies to collect data efficiently. When compared to previous approaches (refer to Fig. 15), similar observations are obtained, but in the previous approaches  values are increased due to the insufficient energy balancing mechanism.

The graphic shown in Fig. 18 compares the number of dead sensors with the number of rounds. When comparing the suggested approach to the state-of-the-art methods, we observed that it has a lower number of dead nodes. The CHs are rotated in each iteration based on the residual energy of the node in the proposed approach, ensuring that the energy of all sensors is utilized uniformly across the network.


Fig. 17. Average packet loss in (%).

Distance traveled by the MS in all trips divided by number of tours gives the MS’s average tour cost. The comparison of the MS travel distance for the suggested method with existing methods and state-of-the-art techniques is shown in Fig. 19, Fig. 20. When it comes to performance, the proposed RL traversal technique exceeds the existing findings in both scenarios (refer Fig. 19, Fig. 20). There is additional evidence to suggest that, for continuous domains, the suggested RL technique is faster than others because of its exploration and exploitation properties. A speedier optimal solution (shortest path) is provided both locally and globally via the exploration property.

Fig. 18. Number of dead nodes.

5.4. Q-learning
The number of cluster heads () and discount factor  values are taken into account to explain the reward behavior of the proposed system. For testing purpose, CH values are changed from 10 to 30, and the discount factor () values have been considered between 0.1 and 0.9. In training, the parameter  (values between 0 and 1) is used to regulate the ratio of exploration to exploitation. If  is closer to 1, random decisions will be made by the agent  and new routing possibilities will be explored. Low  (0.8) value means that the agent  will take the best well known route, taking advantage of the already obtained knowledge (exploitation). As the model gets better understanding of the routing problem, a well-known policy is to lower this  parameter. The agent takes random actions at the beginning and explores various route paths, and it is more likely to take the path consisting of greater reward as the -matrix gains more valuable knowledge. The agent begins at the starting CH and runs according to the Q-matrix through all CHs, attempting to minimize the route’s total distance.

The obtained reward values are shown in Fig. 21, Fig. 22, Fig. 23, in which the green line denotes immediate reward and the blue line denotes future reward. From Fig. 21, we observed that the increasing number of epoch increases the immediate reward value. Further, from all the sub-figures of Fig. 21 we infer that the increase in  value decreases the immediate reward which further leads to the delayed reward. Similar observations can be remarked from the remaining Fig. 22, Fig. 23. Hence, by fine tuning the  parameter we can adjust the reward (immediate and delayed).


6. Conclusion
Reinforcement learning based Q-Learning methodology is proposed in order to perform automated routing using MS for data collection. Firstly, to find a set of cluster heads in WSN-based IoTs, an improved clustering technique has been used. Secondly, in order to find the shortest path to perform data collection, a Q-Learning algorithm has been proposed. More precisely, the traditional data forwarding method is replaced by MS and data is transferred to BS with one hop communication, thus reducing the energy consumption of the sensor module significantly. In addition, the combination of these techniques ensures an automated WSN environment to be efficient in terms of improving the network lifetime. In order to evaluate the effectiveness of the suggested approach, different scenarios with existing methods are considered. Finally, the suggested approach is evaluated under different learning factor  values to obtain the efficient reward function. Mobility, obstacles, and the impact of noise on the communication channel will be considered in future work. In addition, multiple mobile devices should be integrated and implemented with other Q-Learning algorithms to determine the relative importance of reinforcement learning.