A graph embedding algorithm embeds a graph into a low-dimensional space such that the embedding preserves the inherent properties of the graph. While graph embedding is fundamentally related to graph visualization, prior work did not exploit this connection explicitly. We develop Force2Vec that uses force-directed graph layout models in a graph embedding setting with an aim to excel in both machine learning (ML) and visualization tasks. We make Force2Vec highly parallel by mapping its core computations to linear algebra and utilizing multiple levels of parallelism available in modern processors. The resultant algorithm is an order of magnitude faster than existing methods (43× faster than DeepWalk, on average) and can generate embeddings from graphs with billions of edges in a few hours. In comparison to existing methods, Force2Vec is better in graph visualization and performs comparably or better in ML tasks such as link prediction, node classification, and clustering. Source code is available at https://github.com/HipGraph/Force2Vec.This paper is an extension of a conference paper by Rahman et al. (in: 20th IEEE international conference on data mining, IEEE ICDM, 2020b) published in IEEE ICDM 2020.

Access provided by University of Auckland Library

Introduction
Graphs are powerful models for relational data arisen in social, information, chemical, and biological domains. The sparsity, irregularity, and especially high dimensionality of real-world graphs, made machine learning (ML) on graphs much harder than image and text data. According to the definition of Erdös et al., a graph with n vertices may represent an n-dimensional feature space [10]. To learn from such high-dimensional data (n can be billions or more), ML techniques would require enormous amounts of training data to build faithful models. To tackle this problem, graphs are often embedded into d-dimensional vector space, where 2≤d≪n. Thus, graph embedding enables standard ML techniques for classification and regression easily applicable on graphs.

ML methods on graphs can be broadly divided into transductive methods where all test-time queries are restricted to the set of nodes given in the training data, and inductive methods where test-time queries may be defined over new graphs not available during training. Inductive algorithms such as GCN [16] and GraphSAGE [13] use neural networks to generalize for unseen data and are more suitable for dynamic graphs. By contrast, transductive algorithms such as DeepWalk [23], node2vec [12], Verse [30], and HARP [8] directly learn the embedding of graphs in an unsupervised manner. Transductive algorithms are often followed by another ML algorithm and are better applicable for static graphs. This paper only considers transductive graph embedding algorithms.

Even though graph embedding received more attention in ML research in recent years, this problem is well studied by the graph drawing and visualization community for more than 60 years, such as the force-directed algorithm discussed by Tutte in 1963 [31]. In graph drawing, vertices are embedded in 2D or 3D space so that an “aesthetically pleasing” layout of the graph can be visualized on a 2D or 3D screen. Hence, graph drawing and node embedding both solve the same underlying problem: find a function that maps every vertex v in a graph to a d-dimensional vector zv∈Rd. The primary difference is the value of d: graph drawing uses d = 2 or 3, whereas ML tasks use d≈100. Despite this clear connection, recent graph embedding studies are primarily based on techniques from language models such as word2vec [21]. This paper establishes a connection between graph embedding and force-directed graph drawing, thereby bringing ML and visualization lines of research under the same umbrella.

Force-directed layout algorithms consider spring-like attractive forces (e.g., based on Hooke’s law) among adjacent vertices and repulsive forces (e.g., based on Coulomb’s law between electrically charged particles) among non-adjacent vertices. By comparison, random walk-based algorithms such as node2vec perform random walks from a vertex v, and vertices reached in these walks form v’s context. Then, these methods form two subsets of vertices based on vertices inside and outside of v’s context and maximize a likelihood function. Here, we model the likelihood computation within v’s context by attractive forces and outside of v’s context by repulsive forces. We develop a novel similarity-based optimization function that combines attractive and repulsive forces to learn a low-dimensional representation. This expressive force-directed framework named as Force2Vec opens up the door for all popular graph drawing models applicable to graph embedding. Even though a visible layout is not needed for many downstream ML tasks, the concept of force-directed embedding adapted from the graph visualization literature generally helps various ML tasks such as node classification and link prediction. We experimentally demonstrate that Force2Vec performs comparably or better for node classification, link prediction, and visualization tasks.

Graph embedding algorithms are computationally expensive. Most existing algorithms including Force2Vec minimize a loss function by using an optimization method such as Stochastic Gradient Descent (SGD). Finding embedding of a graph with millions of vertices and hundreds of millions of edges may need hours or even days. For example, parallel DeepWalk needs about a day to find embedding of the Orkut graph (3M vertices and 117M edges) using a 48-core Intel Skylake processor (see Table 7). This is a severe impediment in analyzing large-scale social and biological networks. In this paper, we develop a parallel Force2Vec algorithm that runs an order of magnitude faster than existing methods. This speedup comes from two sources: (a) the mathematical formulation of Force2Vec maps the gradient computation to linear algebra operations that are computationally similar to sparse matrix-dense matrix multiplication and (b) we design efficient algorithms for the low-level kernels by employing multiple levels of parallelism and regular data accesses. Hence, Force2Vec allows us to generate high-quality embedding of large-scale graphs quickly. The main contributions of this paper are the following:

We merge visualization and ML approaches for graph embedding in the Force2Vec algorithm. Force2Vec achieves state-of-the-art results for various ML tasks such as node classification and link prediction, and generates high-quality visualization.

Force2Vec provides an expressive framework for graph embedding with mathematical formulation based on linear algebra. Other graph embedding and visualization algorithms can be implemented in this framework.

We present a highly parallel algorithm that uses multicore processors and memory efficiently. Force2Vec is 22× to 56× faster than DeepWalk and 10.5× to 45.4× faster than Verse for various large graphs.

Force2Vec can generate embedding of a graph with billions of edges. Most existing algorithms fail to generate embedding for such large graphs.

Background and related work
The general structure of the node embedding problem
Let G(V, E) denote a graph with a set of n vertices V and a set of m edges E. A∈Rn×n denotes the sparse adjacency matrix of the graph where Aij=1 if {vi,vj}∈E, otherwise Aij=0. The node embedding problem aims to learn a d-dimensional embedding matrix Z∈Rn×d such that every vertex v can be embedded into d-dimensional vector space using a function f: v→zv∈Rd, where zv is the embedding of v in Rd and d≪n. The embedding function f is also called an encoder as it encodes a vertex into a vector in Rd.

Let δ:Rd×Rd→R be a function that computes the similarity between embedding vectors of two vertices. In this paper, we consider δ to be a probability distribution function with δ(zu,zv) producing a value between 0 and 1. Let δG(u,v) be some proximity between u and v in the original graph. For example, δG(u,v) can simply represent vertex connectivity: δG(u,v)=1, when (u,v)∈E. The goal of a graph embedding algorithm is to find an embedding so that δ(zu,zv)≈δG(u,v). Thus, a good embedding can be found by defining a loss function L(δ(zu,zv),δG(u,v)) to measure the discrepancy between the embedded and true proximity values and then minimizing this loss function by using SGD. This general structure of embedding algorithms falls into the encoder–decoder paradigm that is very expressive to capture most unsupervised graph embedding techniques. Note that unlike semi-supervised approaches such as GNNs, unsupervised embedding methods need another algorithm such as logistic regression to classify nodes or to predict links.

Table 1 A summary of state-of-the-art unsupervised models including time and memory complexity, dependency on any specific model in corresponding programming language, and respective parallelization technique
Full size table
Previous work
Graph embedding is a well-studied problem in graph mining and machine learning literature. We refer readers to a recent survey [6] for a comprehensive view of the field. Researchers have previously developed various encoding and decoding schemes, graph proximity measures, and loss functions [12, 23, 29, 30]. We can broadly categorize them based on the encoding schemes and the learning strategies. We provide a brief summary of some embedding methods in Table 1.

Early methods of graph mining perform manual feature extraction from graphs such as degree, clustering co-efficient, etc. which can not fully capture the inherent structure of the graph [2, 14]. Later, matrix factorization-based methods [1, 7] have been introduced to decompose graphs (represented by various matrices) using Singular Value Decomposition (SVD) or Non-negative Matrix Factorization (NMF). Lack of parallelization and high runtime/memory complexity restrict them to be applied on bigger graphs.

A plethora of random walk-based methods have been introduced for graph representation learning. DeepWalk [23] performs random walk on the graph to sample a set of paths for each vertex. Then, the problem is formulated using the word2vec model [21] to generate embedding of nodes. LINE captures information from the graph for first order and second order proximities i.e, 1-hop and 2-hop neighbors [29]. The node2vec method samples walks based on Depth First Search (DFS) and Breadth First Search (BFS) traversal [12]. Tsitsulin et al. introduce a versatile graph embedding method, called Verse, that uses various approaches to instantiate the embedding [30]. Verse shows that the stationary distribution of random walk eventually converges to personalized pagerank [22]. HARP [8] developed multi-level algorithms for three state-of-the-art methods [32]. Even though HARP made some embedding methods faster, it consumes a significant amount of memory for storing several intermediate graphs.

Generally, a general-purpose graph embedding method should run fast, consume less memory, and generate high-quality embeddings that perform well in different prediction tasks. To accomplish these goals, we introduce an unsupervised parallel algorithm called Force2Vec.

The Force2Vec algorithm
A general framework for node embedding
A good embedding of a graph preserves structural information from the original graph in the embedding space. Thus, the neighboring vertices usually have “similar” embeddings than non-neighboring vertices. Let N(u) denote the “neighbors” of u in a particular context. Here, N(u) can simply denote vertices connected with u by edges or vertices reached by random walks from u. Using the similarity distribution function δ, we can define the loss function for vertex u as the negative log likelihood with respect to all other vertices in the graph:

L(u)=−∑v∈N(u)logδ(zu,zv)−∑w∉N(u)log(1−δ(zu,zw)).
(1)
We can then optimize Eq. 1 by minimizing the loss function using the Stochastic Gradient Descent (SGD) algorithm. To this end, the gradient of L(u) with respect to zu is computed as

∂L(u)∂zu=−∑v∈N(u)  1δ(zu,zv)∂δ(zu,zv)∂zu+∑w∉N(u)  11−δ(zu,zw)∂δ(zu,zw)∂zu.
(2)
We update the embedding of vertex u in each iteration of the SGD as follows:

zu=zu−η∂L(u)∂zu,
(3)
where η is the learning rate or step size.

The force model of the gradient
Equation 2 reveals two fundamental aspects of this general-purpose technique for embedding vertices of a graph: (a) the first component of the gradient (∇fa(u,v)=1δ(zu,zv)) is inversely proportional to the similarity between a pair of neighboring vertices and (b) the second component of the gradient (∇fr(u,w)=11−δ(zu,zw)) is proportional to the similarity between a pair of non-neighboring vertices. Table 2 elaborates these relations. Interestingly, these two components of the gradient behave as if ∇fa(u,v) attracts neighbors when they move away from one another and ∇fr(u,w) repulses non-neighbors when they come closer in the embedding space. These type of attractive and repulsive forces are at the heart of force-directed graph layout algorithms [15, 20, 24] that are popular to generate aesthetically pleasing visualizations of graphs. We use this apparent connection between graph embedding and force-directed layout methods to design a family of algorithms called Force2Vec for robust representation learning and visualization of graphs.

Table 2 Relationships between the components of the gradient and the similarity function
Full size table
With an aim to recast the graph embedding problem to force-directed model, we focus mostly on the spring-electrical model where two types of forces are calculated based on the connectivity of the vertices, namely an attractive force when two vertices are connected by an edge and a repulsive force when there is no connection between a pair of vertices [11]. In the original setting of the spring-electrical model, adjacent vertices attract each other by a spring-like attractive force dictated by Hooke’s law and non-adjacent vertices repulse each other based on Coulomb’s law between electrically charged particles. Researchers have developed many variants of this model by changing the extent of the force with respect to similarities (or distances) among vertices. In this paper, we explored two algorithmic variants: (1) we considered several choices for the similarity function δ in Eq. 1 and 3 we plug in several widely used force models directly into the gradient. In both cases, the first term in the computed gradient can be considered as the attractive force between a pair of neighboring vertices and the second term can be considered as the repulsive force between a pair of non-neighboring vertices. Thus, we denote the attractive component by ∇fa(u,v) where u and v are neighbors and denote repulsive components by ∇fr(u,w), where u and w are not neighbors.

Similar to prior work [12, 30], we use negative samples to reduce the cost of the second term in Eq. 1. Let S(u) denote a subset of vertices considered as negative samples for u. Then, we use the following equation throughout the paper:

L(u)=−∑v∈N(u)logδ(zu,zv)−∑w∈S(u)log(1−δ(zu,zw)).
(4)
Variants of the similarity function and force models
Our overall goal is to generate embeddings that are effective for visualization and other downstream machine learning tasks. To this end, we consider two similarity functions and three force models.

The sigmoid function. Our first similarity function is based on the sigmoid function applied on the dot product of two embedding vectors [12, 23, 30]. Using δ(zu,zv)=σ(zuzv)=11+e−zuzv in Eq. 4, we get the following loss function:

L(u)=∑v∈N(u)log(1+e−zuzv)−∑w∈S(u)loge−zuzw1+e−zuzw.
(5)
We then compute the gradient of Eq. 5 as follows:

∇L(u)=−∑v∈N(u)(1−σ(zuzv)).zv+∑w∈S(u)σ(zuzw).zw,
(6)
where ∇fa(u,v)=(σ(zuzv)−1).zv denotes the attractive force between u and v, and ∇fr(u,w)=σ(zuzw).zw denotes the repulsive force between u and w. Then, the total attractive force acting on u is ∇fa(u)=∑v∈N(u)∇fa(u,v), and the total repulsive force acting on u is ∇fr(u)=∑w∈S(u)∇fr(u,w).

Student’s t-distribution. Our second similarity function is t-distribution which has been found effective in high-dimensional data visualization [19, 28]. Note that t-distribution with one degree of freedom approximates a standard Cauchy distribution [18]. Putting the value of δ(zu,zv)=11+t2, where t=∥zu−zv∥, in Eq. 4, we get the following:

L(u)=∑v∈N(u)log(1+t2)−∑w∈S(u)logt21+t2.
(7)
We can compute the gradient of the loss function as follows:

∇L(u)=∑v∈N(u)2t1+t2+∑w∈S(u)−2t(1+t2),
(8)
where, similar to the sigmoid function, the first term denotes the attractive force ∇fa(u) acting on u and the second term ∇fr(u) denotes the repulsive force acting on u. Note that ∇fa(u) and ∇fr(u) in Eq. 8 represent scalar values for the corresponding gradients. To get the direction of the gradient, we multiply them with a unit vector as follows:

∇fa(u)=∑v∈N(u)2∥zu−zv∥1+∥zu−zv∥2×zu−zv∥zu−zv∥
(9)
∇fr(u)=∑w∈S(u)−2∥zu−zw∥(1+∥zu−zw∥2)×zu−zw∥zu−zw∥
(10)
Using force-directed models directly in gradients. In addition to the above similarity functions, we have also employed several popular force-directed models directly in the gradient calculations. Table 3 summarizes these models along with the attractive and repulsive forces. These models are commonly used to obtain aesthetically pleasing layouts of graphs, and the authors of ForceAtlas2 provide precise descriptions of these models [15].

Table 3 Different spring-electrical model with corresponding gradients of attractive and repulsive forces
Full size table
Optimization
After obtaining the gradient, we can optimize the embedding using the SGD algorithm using Eq. 3. A vanilla SGD that processes just one vertex in each update in Eq. 3 does not offer enough parallelism for multiple threads. In a minibatch SGD, we compute the gradient for a minibatch of vertices and update the embedding of the minibatch in parallel. Here, we only implement the synchronous version of minibatch-SGD that processes each vertex in a minibatch independently and thus provides deterministic results, unlike asynchronous parallelization of SGD [26].

figure d
figure e
The Force2Vec algorithm
Algorithm 1 provides a generic description of the Force2Vec algorithm based on negative sampling. This algorithm can be easily adapted based on different force models. Line 3 creates minibatches of equal sizes (randomly without replacement). Each iteration (line 4-9) of Algorithm 1 computes the gradient of a minibatch of b vertices Vb and then updates the embedding of vertices in Vb. For a minibatch Vb, we identify a set Nb of neighboring vertices following a given strategy. For example, Nb can be vertices adjacent to Vb or vertices discovered by random walks from Vb. Hence, the attractive forces are computed between Vb and Nb. When computing repulsive forces, we use a negative sampling approach [12, 21]. A set of negative samples for a vertex is formed with a subset of non-neighboring vertices that are chosen randomly from a uniform distribution. In line 6 of Algorithm 1, we choose a set of negative sample Sb for the current minibatch. Sb contains s vertices that are used by every vertex in the minibatch to compute repulsive forces. After Nb and Sb are formed, we can compute gradients (line 7) based on our force model as discussed in the next section. Lines 8 and 9 update the embedding of vertices in Vb using the computed gradient.

Gradient computation based on force models. Algorithm 2 describes how we compute gradients for each vertex in the current minibatch Vb. Algorithm 2 also takes Nb (neighbors of Vb) and Sb (negative samples for Vb) as inputs. We use Nb to compute attractive forces and Sb to compute repulsive forces, both with respect to Vb. Each iteration of the outer for loop at line 2 computes the gradient ∇f(u) for a vertex u∈Vb. Since gradients for vertices in Vb are independently computed, several threads can process vertices in parallel. Lines 4 to 5 of Algorithm 2 show the gradient computation based on the attractive force with respect to vertices in Nb. Specifically, line 5 computes ∇fa(u,v) between u and its neighbor v in Nb. This gradient computation (∇fa(u,v)) depends on the force model such as based on Eqs. 6 or 8. Similarly, lines 6 to 7 of Algorithm 2 show the gradient computation based on the repulsive force with respect to vertices in Sb. Specifically, line 6 computes ∇fr(u,w) between u and its negative sample w in Sb. Line 8 adds the gradient components from attractive and repulsive forces. Finally, the function returns a b×d matrix storing gradients for each vertex in Vb.

Initialization and hyper parameters. Algorithm 1 starts with a random embedding zu for every vertex u, an initial learning rate η, size of a minibatch b, and the number of iterations nepoch. These parameters can be tuned empirically as shown in the result section.

Computational complexity. Each gradient component ∇fa(u,v) and ∇fr(u,w) can be computed by operations on the embedding vectors zu, zv, and zw. Hence, the complexity of computing ∇fa(u,v) and ∇fr(u,w) is O(d). If we consider just 1-hop neighbors in Nb, then ∇fa(u,v) is needed to be computed for every edge in the graph, giving us O(md) complexity for the attractive force computation. This complexity can change if multi-hop neighbors or random walks are used. Each vertex computes repulsive forces with respect to s negative samples, giving us an overall cost of O(nsd) for the repulsive force computations. Hence, per-iteration complexity of Force2Vec is O(md+nsd). Here, the relative cost of attractive and repulsive forces depends on the neighborhood formation and negative sampling strategies.

figure f
rForce2Vec: Random walk-based Force2Vec. Algorithm 1 provides a generic framework for Force2Vec and can be easily adapted for other force and embedding models. For example, we can form neighborhood of a vertex based on random walks as was used in DeepWalk and node2vec. To capture this model, we need to create Nb by random walks from the current minibatch Vb as shown in Algorithm 3. For each vertex u (lines 4 to 7), we select k vertices from its subsequent k-hop neighbors for calculating attractive forces. After Nb is created using Algorithm 3, we can pass it to Algorithm 1 without changing the gradient computations. We call this approach rForce2Vec that may perform better for heterogeneous networks by extracting information from multi-hop neighbors. The complexity of computing the attractive force is O(nkd). The complexity of computing the repulsive force in rForce2Vec remains the same.

Fig. 1
figure 1
Gradient computations in linear-algebraic format. The minibatch consists of four vertices. ∇fa and ∇fr denote gradients with respect to attractive and repulsive forces. Red arrows denote computations guided by the adjacency matrix

Full size image
Parallel Force2Vec
A linear algebra model. The gradient computations based on attractive and repulsive forces (that is, Algorithm 2) dominate the runtime of Force2Vec. To effectively optimize gradient computations, we model them as a series of linear algebra operations. Let Vb be the set of b vertices in the current minibatch and Zb∈Rb×d be the slice of the embedding matrix corresponding to Vb. Also assume that Ab∈Rb×n denotes the slice of the adjacency matrix storing the edges corresponding to Vb. Figure 1 shows an example of this setting where the minibatch consists of the first four vertices.

To compute the attractive force based on Eq. 6, we need to operate on the embeddings of Vb and the embeddings of their neighbors Nb. For example, in Fig. 1, vertex v0 in row0 of the adjacency matrix has three non-zero elements at positions 1, 3, and 7, which means Nb(v0)={v1,v3,v7}. Hence, we access z1, z3, and z7 (shown in red arrows in Fig. 1), perform some computations based on force equations (e.g., Eq. 6), and then sum them up to compute the gradient ∇fa(v0) for vertex v0. This computation follows the pattern of a sparse-dense matrix multiplication (SpMM). Similarly, the repulsive force computation based on Eq. 6 accesses the embeddings of a subset of non-adjacent vertices (v2 and v5 in Fig. 1). This repulsive force computation can also be mapped to a general SpMM operation.

Fig. 2
figure 2
Example shows gradient computation of attractive force for a single vertex using the sigmoid function

Full size image
While Fig. 1 provides a high-level concept of using SpMM in gradient computation, we cannot directly use an off-the-shelf implementation of SpMM because of the delicacy of various force models. For example, Fig. 2 shows how we actually compute ∇fa(v0) based on Eq. 6. At first, we compute dot products between z0 and z1, z3, and z7. These dot products produce scalar values denoted by a1, a2, and a7 that are passed through a sigmoid function generating s1, s2, and s7. We broadcast these scalar values to form vectors by copying it in each lanes of the vectors that are element-wise multiplied with their corresponding embedding vectors. The resultant vectors are summed up to finally output ∇fa(v0). While this computation is conceptually similar to SpMM, the use of sigmoid makes it harder to use standard libraries. This mapping allows us to optimally parallelize gradient computation as described below.

Thread-level parallelization. All vertices in a minibatch are independent and compute gradients in parallel. For example, different threads can parallelly compute gradients for four vertices (vertices corresponding to Zb) in Fig. 1. Hence, each thread can sequentially compute ∇fa(v) and ∇fr(v) following the flow shown in Fig. 2. When vertices in a minibatch have similar number of neighbors, we use static scheduling where each thread processes equal number of vertices from the minibatch. However, if a vertex has very high degree (commonly seen in scale-free networks), the computation on a high-degree vertex can become the bottleneck. To address this issue, we compute the degree distribution of a minibatch and partition the minibatch with respect to the number of threads such that each thread processes almost equal number of neighbors. The latter approach results in a superior load-balancing for scale-free graphs.

SIMD vectorization. While each thread computes ∇fa(v) and ∇fr(v) in a core, these computations can be further optimized by employing Single Instruction Multiple Data (SIMD) parallelism available in each core. To this end, we implemented the computations in Fig. 2 using hardware intrinsics. Our library includes a code generator that generates intrinsic codes for different hardware architectures (similar as [33]). In our implementation, we employed extensive register blocking to perform all intermediate computations for a vertex on SIMD registers reducing the write accesses on the output. Note that we need to use the full capacity of the SIMD registers of any hardware architecture to effectively implement this technique and general-purpose compilers often are not very good at this level of auto-vectorization.

Experiments
Experiment setup
Experiment overview. We conduct three types of experiments to show the effectiveness of Force2Vec: (a) runtime and scalability experiments show that Force2Vec is an order of magnitude faster than other methods considered, (b) visualization experiments show that Force2Vec generates qualitatively and quantitatively better visualizations, and (c) node classification, link prediction, and modularity experiments show that Force2Vec performs similar or better than previous graph embedding algorithms. We also provide insights behind the observed performance of Force2Vec.

Experiment platform. We implemented Force2Vec in C++ with multithreading support from OpenMP. We used intrinsic functions to exploit SIMD vectorization available on a single core. All our experiments were conducted on a server with a dual-socket Intel Xeon Platinum 8160 processors (2.10GHz). The server has 256GB memory, 48 cores (24 cores/socket), and 32MB L3 cache/socket.

Table 4 Parameter settings for other methods
Full size table
Algorithm settings. We used three variants of Force2Vec in different experiments: Algorithm 1 with sigmoid function (sForce2Vec), Algorithm 1 with t-distribution (tForce2Vec), and Algorithms 1 and 3 with sigmoid function (rForce2Vec). We empirically set the minibatch size b to 384 so that each thread gets 384/48=8 vertices within a minibatch. Changing the minibatch size slightly does not impact the convergence. In all experiments, we set the number of epochs to 1200, the number of negative samples to 6, and the learning rate to 0.02. We run all experiments 10 times and report average results for all performance measures. Some of these hyper-parameters (e.g., η) are obtained by using a grid search discussed in Sect. 4.5. As Algorithm 3 performs better for heterogeneous networks, we report results of Algorithm 3 for multi-label classification. We compared Force2Vec with four other embedding methods: DeepWalk [23], HARP [8], struct2vec [27], and Verse [30]. We selected these methods because they have different underlying models and support multi-threading. Table 4 reports various parameters used with these methods. Unless otherwise mentioned explicitly, we generate 128-dimensional embedding and set the number of workers/threads to 48. In all of our experiments, we use default values for other parameters that have not been stated directly.

Datasets for experiments. Table 5 reports a set of graphs of various sizes used in our experiments. All these graphs have multiple labels. Most of these graphs have been widely used in previous studies [16, 23, 30, 34]. These graphs include networks from scientific publication domain (Cora, Citeseer, Pubmed) and social photo/video sharing networks (Flickr and Youtube). To conduct experiments with large graphs, we have used a social interaction network (Orkut) and a web-crawler network (uk-2005). Note that uk-2005 is the biggest graph that we can solve with 256 GB memory in our server. The ground truth node labels are not available for Orkut and uk-2005 graphs. Thus, we perform only link prediction task for these graphs. Cora, Citeseer, Pubmed, and Flickr networks are homogeneous i.e., each node is assigned to a single label. Thus, node classification task on those networks automatically becomes a multi-class classification problem. On the other hand, Youtube graph is heterogeneous where one node can have more than one labels. So, node classification on Youtube is a multi-label classification problem.

Table 5 Datasets used for graph embedding experiments
Full size table
Table 6 Time (s) taken by different tools to generate embeddings using 48 threads. tForce2Vec, sForce2Vec, and rForce2Vec represent Algorithm 1 with t-distribution, Algorithm 1 with sigmoid function, and Algorithms 1 with random walks from Algorithms 3 and sigmoid function, respectively
Full size table
Table 7 Time (sec.) taken by different tools to generate embeddings using 48 threads
Full size table
Runtime and scalability experiments
Runtime on 48 threads. Table 7 reports the total runtime using 48 threads for different methods to generate 128-dimensional embeddings. The numbers within the parentheses of other methods denote the speed-up of tForce2Vec over the corresponding method. Overall, Force2Vec is on average 43× and 28× faster than DeepWalk and Verse, respectively. Force2Vec is more than 100× faster than struc2vec and more than 10× faster than HARP for large graphs. We observe that HARP is usually fast for smaller graphs, but it becomes increasingly slower as the graph becomes bigger. We also found that DeepWalk, struc2vec and HARP cannot process larger graphs because of their high computational and/or memory requirement.

Runtime for a billion-edge graph. To conduct experiments with a very large graph, we select uk-2005 web-crawler graph with 39 million vertices and nearly one billion edges. For uk-2005, we generated 64-dimensional embedding so that it can be stored in the memory. DeepWalk, HARP, and struc2vec fail for uk-2005 due to memory error. Verse takes nearly 5 days (413375.62 seconds) to generate embedding. By contrast, sForce2Vec takes 2.57 hours to generate embedding, which is around 45× faster than Verse. Thus, Force2Vec advances the state-of-the-art in big graph analytics by enabling the embedding of large-scale graphs that are becoming more and more common in various science and business applications.

Scalability. Figure 3(a) shows the strong scaling results for the Flickr graph (the biggest graph where all methods are successful). We observe that both tForce2Vec and Verse scales well as we increase the number of threads. However, tForce2Vec runs an order of magnitude faster on all thread counts. The HARP code obtained from the repository provided by authors does not scale well.

Why does Force2Vec perform so well? The faster runtime of Force2Vec originates from three aspects of our algorithm and implementation. First, the mapping of core gradient computation to linear algebra operation shown in Fig. 1 helps us simplify and parallelize the core computations which dominate the runtime of the algorithm. We completely eliminated synchronization and false sharing that can happen due to asynchronous or dependent read-write operations in memory [4]. We avoid this dependency by employing batch processing of vertices. Second, we fully utilized SIMD parallelization using low-level intrinsics codes available for modern processors to get the full capacity of the hardware. Our implementation provides up to 2.7× speedup over the implementation which does not use intrinsic and solely relies on the auto-vectorization of compiler. Third, Force2Vec utilizes cache hierarchies as much as possible. Our linear algebra kernels stream data to processor caches and registers and utilize in-cache data as much as possible. In our intrinsic implementation, we also use extensive register-blocking to eliminate the intermediate write accesses of the output data. Hence, we are able to eliminate unnecessary data accesses. For example, the L1 cache miss rate in Force2Vec is never greater than 2%, whereas the L1 cache miss rate can be as high as 16.5% in Verse. Since memory accesses (in terms of memory bandwidth and latency) are the primary bottleneck in graph analysis, Force2Vec performs well by utilizing spatial and temporal data locality.

Memory requirements. Figure 3b shows the memory consumption by different methods. Force2Vec stores the graph in the Compressed Sparse Row (CSR) format and uses 4 bytes for vertex indices and edge weights. We also use single-precision floating point numbers to store 128-dimensional embedding. Thus, the estimated memory cost for Force2Vec is 4n+8m+512n bytes or 516n+8m bytes. We observe that Verse and tForce2Vec consume less memory than DeepWalk and HARP. For example, in Fig. 3b, DeepWalk, HARP, Verse, and tForce2Vec empirically consume peak memory of around 60 GB, 13.4 GB, 587 MB, and 727 MB for the Youtube graph, respectively.

Fig. 3
figure 3
a Strong scaling results for the Flickr graph with different number of threads. b Memory consumption by different methods for five different benchmark datasets

Full size image
Fig. 4
figure 4
Visualization of 2D projections for Pubmed dataset by t-SNE from 128 dimensional embeddings generated by a tForce2Vec, b DeepWalk, c HARP, and d Verse. Colors represent respective classes in the dataset

Full size image
Visualization quality
A primary motivation of this work is to generate an embedding that leads us to an aesthetically pleasing visualization. Graph visualization is especially important for exploratory science and interpretation of results. Here, we evaluate visualization quality both qualitatively and quantitatively using 128D and 2D embeddings.

Visualizing 128D embedding using t-SNE. To visualize graphs, we use t-SNE [19] to generate 2D projections from 128D embeddings obtained by various embedding algorithms. Figure 4 shows visualizations of the Pubmed graph, where different colors denote different classes of vertices. A good embedding should group vertices of the same class together to form a cluster while separating one class from others as much as possible. Figure 4 shows that tForce2Vec preserves the class structures better than other methods as it can make different classes well separated. The next best visualization is provided by DeepWalk. Figure 4 shows that most methods are able to form small clusters from vertices of a particular class; that is, they preserve local structures of clusters reasonably well. However, tForce2Vec and to some extent DeepWalk preserve the global structure better than other methods.

Quantitative evaluation of the visualization. To evaluate these visualizations quantitatively, we use Qlocal and Qglobal quality measures [17] for all lower dimensional projections by t-SNE using dimRed R package. Qlocal reflects the preservation of local structure in clusters i.e., how one cluster of a class is separated from other clusters of classes. Qglobal represents the global structure preservation of clusters where data points of a class are expected to form one cluster. For both of these measures, higher values mean better results. We report the quantitative values in Table 8. The quantitative measures also support that tForce2Vec preserves more global structure than other methods. This is due to the robustness and quality of the embedding generated by tForce2Vec.

Table 8 Quantitative measures for visualization by t-SNE shown in Fig. 4
Full size table
Fig. 5
figure 5
Visualization of 2D embeddings for Pubmed dataset generated by tForce2Vec (left) and DeepWalk (right). Colors represent respective classes in the dataset

Full size image
Visualizations from direct 2D embeddings. Algorithm 1 computes attractive and repulsive forces based on neighbors and non-neighbors, respectively. As mentioned earlier, this force-directed model is widely used to generate 2D or 3D graph layouts. Figure 5 shows the visualization of the Pubmed graph obtained from direct 2D embeddings from tForce2Vec and DeepWalk. While tForce2Vec can still generate a good embedding, DeepWalk fails to visualize a graph (without the help of t-SNE). Thus, tForce2Vec has a clear advantage in visual graph analytics over other graph embedding methods.

The effectiveness of embeddings in ML tasks
Now we discuss the effectiveness of graph embedding methods in traditional prediction tasks such as node classification, link prediction, and community detection. Specifically, we use F1-micro and F1-macro scores to assess the quality of link prediction and node classification tasks. To compare clusterings, we use the Modularity score.

Fig. 6
figure 6
a Comparative performance of link prediction task by different methods on different datasets. b Comparative results of clustering task by different methods

Full size image
Link prediction. To predict links (edges) from the embedding, we reconstruct an edge by performing an element-wise vector operation between the embeddings of a pair of neighboring vertices in the graph. Following the guidance of prior work [12, 30], we use a dHadamard vector operator that performs element-wise multiplication of d-dimensional vectors. Basically, we treat a pair of adjacent vertices as a positive sample and a pair of non-adjacent vertices as a negative sample. This formulation converts the link predication task into a binary classification problem. In any type of binary classification, we define accuracy to be TP+TNTP+TN+FP+FN, where TP, FP, TN, and FN represent the absolute number of true positive, false positive, true negative, and false negative predictions, respectively. We prepare an evenly distributed dataset having equal number of positive and negative samples. We shuffle them and use 50% of the samples to train a logistic regression model while rest of the samples for testing. We report the results in Fig. 6a. We observe that sForce2Vec and Verse perform better than other methods. DeepWalk also performs generally well, but the embeddings from HARP and struc2vec cannot predict links with high accuracy. Overall, sForce2Vec takes much less time to generate an embedding that are either better or competitive for the link prediction task compared to other graph embedding algorithms.

For uk-2005 with about 1 billion edges, we could not conduct a link prediction experiment using all edges due to memory limitation. Hence, we create an induced subgraph from a randomly selected subset of 1% vertices and edges adjacent to these sampled vertices. Then, we predict links from this induced subgraph using sForce2Vec and Verse (other methods failed to generate embeddings for this graph). We observe that sForce2Vec and Verse achieve accuracy of 95% and 97%, respectively. Note that sForce2Vec attains this competitive accuracy significantly faster than Verse.

Clustering. To extract the community structure from the embedding of a graph, we follow the technique used in Verse [30] and use the k-means algorithm to cluster 128-dimensional embeddings. If an embedding captures the community structure effectively, each cluster identified by k-means should represent a community in the original graph. We evaluate the quality of a k-means clustering by the modularity score [3] that computes the fraction of the edges that are within a given cluster minus the expected fraction, if edges are distributed randomly. To identify the best clustering solution, we run k-means for the number of clusters from 2 to 50 and report the best clustering solution measured by the modularity score (higher is better). Figure 6b shows that tForce2Vec achieves the best or near best modularity score. DeepWalk also performs well for all graphs, but the performance of other methods fluctuates across graphs. For example, Verse performs the best for Youtube, but does not perform well for other graphs. In summary, tForce2Vec is robust in preserving community structures in the embedding space while providing embeddings much faster than other methods.

Fig. 7
figure 7
F1-micro scores of node classification task on embeddings generated from a Cora, b Citeseer, c Pubmed, and d Flickr datasets by different graph embedding methods

Full size image
Node classification. In a node classification task, we aim to predict node labels from the embedding of nodes. We use a random subset of vertices and their embeddings to train a logistic regression model and then predict the labels of the rest of the vertices (that are not used in training). We evaluate the quality of the multi-class classification by the F1-micro score that aggregates the contributions of all classes to calculate the average value of the final F1 score. For this case, the precision and recall for a set of classes C are defined as follows: P=∑c∈CTPc∑c∈C(TPc+FPc), R=∑c∈CTPc∑c∈C(TPc+FNc), where TPc, FPc, and FNc denote the number of true positive, false positive, and false negative predictions of class c, respectively. Then, the F1-micro score is computed by 2∗P∗RP+R.

We train a logistic regression (one vs. rest) model by taking 5%, 10%, 15%, 20%, and 25% of the nodes in training set, respectively, and report the F1-micro score in Fig. 7 by making prediction on rest of the nodes as the testing set. We observe that tForce2Vec and DeepWalk generally perform better than other methods. For Citeseer, tForce2Vec performs much better than other approaches. The accuracy of struc2vec and Verse is not competitive, while HARP performs reasonably well. We also measured the performance of node classification using F1-macro scores and observed that the relative performance of various methods is similar to the performance observed under F1-micro scores.

For some heterogeneous graph such as Youtube, rForce2Vec may perform better in the node classification task. For these graphs, multi-hop neighbors reached by random walks are able to capture complex relationships better. We tested rForce2Vec for Youtube with walk length set to 5. We observe that rForce2Vec, DeepWalk, HARP, and Verse achieve an F1-micro score of 42%, 45%, 44%, and 42%, respectively, for 25% training samples. DeepWalk performs better than other tools for this graph, and rForce2Vec shows competitive performance.

Parameter sensitivity
We demonstrate parameter sensitivity of tForce2Vec using the Pubmed graph in Fig. 8.

Fig. 8
figure 8
a Loss curves based on different learning rates for Pubmed dataset. b Effect of the number of negative samples on node classification task for Pubmed dataset. c Effect of dimensions for node classification task on Pubmed dataset

Full size image
Convergence. Figure 8a shows loss curves of Eq. 4 for different values of the learning rate η. A higher value of η usually expedites the convergence, but the solution may get stuck to a local optima. By contrast, a small value of η may lead to a better solution with a slow convergence rate. Hence, we keep η as a hyper-parameters in Force2Vec. We empirically observed that η=0.02 provides the best convergence-optima balance for Force2Vec, and thus use it as a default value.

Negative samples. Figure 8b shows F1-micro scores of node classification for different number of negative samples per vertex. We observe that Force2Vec shows better performance when we set s to 5. This parameter is highly sensitive because a larger value of s can create false negative samples that are also part of the same class. Thus far, negative samples are taken randomly from all vertices of the graph. To explore more on negative sampling approach, we also used a pure version of negative samples where samples are forced to take from non-neighbors. This version has a significant runtime overhead due to the explicit checking of non-neighbors after sampling each vertex. We show the results of runtime and corresponding accuracy for Pubmed dataset in Figs. 9a and 9b. In Fig. 9a, we observe that the runtime for pure negative sampling approach is very high compared to random sampling approach. We also notice that the runtime gap increases when we increase the number of negative samples. On the other hand, in Fig. 9b, we do not see any advantage of using pure negative sampling approach. Instead, we see a slight less accuracy for pure negative sampling approach. Although we guarantee of sampling non-neighbors in pure negative sampling approach, we cannot guarantee the selection of samples that belong to a different class due to the unsupervised nature of the algorithm. As a result, we may observe such performance for some graphs.

Embedding dimensions. Figure 8c shows the results of node classification for different values of the embedding dimension d. As we increase d from 2 to 256, the F1-micro score consistently increases up to d=128 and then starts decreasing. Hence, we used d=128 for all previous experiments in the paper. We also observe that F1-micro scores are remarkably stable for different percentages of training samples.

Fig. 9
figure 9
Comparison of Pure negative sampling and Random negative sampling approaches. a Runtime in seconds and b F1-micro scores for different number of negative samples on Pubmed graph

Full size image
Fig. 10
figure 10
F1-micro scores when classifying nodes from embeddings generated using different force models (shown in Table 3) for a Cora and b Pubmed datasets

Full size image
Comparison among different force models within Force2Vec
In addition to three variants of Force2Vec discussed thus far, we also implemented and experimented with other force-directed models mentioned in Table 3, namely Fruchterman Reingold (FR), ForceAtlas (FA), and LinLog (LL). Since these models are directly used inside the gradient computation in Algorithm 2, their runtimes are similar to tForce2Vec. The performance of different force models in ML and visualization tasks varies from one graph to another.

Figure 10 shows the performance of different force models in the node classification task. We observe that the Force2Vec with the t-distribution force model always performs better than other models. The superior performance of t-distribution is not surprising because it manipulates the similarity function directly for a loss function that should excel in classifying nodes. Hence, we recommend t-distribution to be the default model when performing downstream ML tasks. t-distribution is also the default model in our software.

Fig. 11
figure 11
Visualization of 2D embedding generated by t-SNE from 128D embedding of the Pubmed graph using different force-directed models of Force2Vec: a t-distribution, b LinLog, c Fruchterman Reingold, and d ForceAtlas. Nodes with the same labels are shown in the same color. Edges are not shown to better visualize clusters of nodes

Full size image
Table 9 Qlocal and Qglobal measures for different force models based on 128D embeddings
Full size table
Fig. 12
figure 12
Direct 2D embedding of the Pubmed graph using different force-directed models of Force2Vec: a t-distribution, b LinLog, c Fruchterman Reingold, and d ForceAtlas. Nodes with the same labels are shown in the same color. Edges are not shown to better visualize clusters of nodes

Full size image
Visualization of embeddings. Figure 11 shows 2D visualizations of the Pubmed graph generated by t-SNE from 128D embeddings. We observed that all force models (available in the Force2Vec framework) perform well in separating different classes of nodes. By contrast, traditional graph embedding methods may not generate good visualizations as shown in Fig. 4. We also quantitatively measure the quality of embeddings and compare them in Table 9 for Cora, Citeseer, and Pubmed datasets. As discussed in Sect. 4.3, higher values of Qlocal and Qglobal denote better visualizations for clustered data. Table 9 shows that all force models generally perform well, but there is no clear winner. For Cora, the FA model achieves the best Qlocal score, whereas the LL model attains the best Qglobal score. F2V (with t-distribution) achieves the best Qlocal for Citeseer, whereas the FR model attains the best Qglobal score. However, despite their differences, all force models in Force2Vec generate better visualizations than other graph embedding methods as discussed in Sect. 4.3.

As Force2Vec method is inspired by the force-directed graph visualization model, we generate direct 2D layouts of graphs to assess the quality of the visualization. Figure 12 shows the 2D embeddings of Pubmed graph generated by different force models. We see t-distribution and LinLog models cluster nodes with the same labels better than other models. Hence, these two models are better for visualizing clusters of nodes in labeled graphs. In the embedding space, t-distribution and LinLog models form clusters of nodes based on their labels, which results in better accuracy in the node classification task as shown in Fig. 10.

Table 10 2D layouts of four representative graphs with each graph shown in a column of the table
Full size table
Visualization of graph layouts. In earlier visualization experiments, we did not draw edges so that we could emphasize clusters consisting of vertices with the same labels. However, force-directed algorithms are well known for generating layouts in which edge lengths are small while vertices are well separated. To explore the quality of graph layouts generated by Force2Vec and other related algorithms, we show 2D layouts along with edges for four representative graphs in Table 10. In addition to Force2Vec with four force models, we show 2D layouts obtained from two graph layout algorithms BathLayout [24] and OpenOrd [20] and also from DeepWalk. These two methods are directly used for graph visualization. Table 10 shows that some force models (e.g., LinLog and Fruchterman Reingold) inside Force2Vec can generate aesthetically pleasing visualizations compared to BatchLayout and OpenOrd. Note that BatchLayout and OpenOrd are solely designed to generate better visualizations and hence do not perform well for downstream ML tasks such as classifying nodes. On the other hand, DeepWalk does not generate good layouts because of the use of cosine similarity function, but it performs reasonably well in ML tasks. Hence, Force2Vec exhibits clear benefits in both ML and visualization tasks, which cannot be achieved by other methods considered.

Table 10 reports qualitative results that are subject to human interpretation. Next, we measure the qualities of different graph layouts quantitatively using Neighborhood Preservation (NP) and Stress (ST)—two widely used metrics for measuring the aesthetic quality of graph layouts. Neighborhood Preservation measures the number of neighbors that are close to a vertex in a graph as well as in the layout. Stress computes the difference between geometric distance and graph theoretic distance for any pair of vertices in a graph [5, 9]. Table 11 shows that various force models in Force2Vec achieve competitive neighborhood preservation and stress measures relative to specialized methods such as BatchLayout and OpenOrd. Thus, Force2Vec generates good layouts both qualitatively (Table 10 ) and quantitatively (Table 11).

Table 11 Visualization measures to assess the quality of 2D layouts by different force models
Full size table
Table 12 Recommendations for using similarity functions or force models with Force2Vec for different machine learning and visualization tasks
Full size table
Conclusions
This paper presents Force2Vec, a family of parallel graph embedding algorithms based on the force-directed graph layout models. Force2Vec advances the graph embedding field in the following ways: (a) by incorporating proven visualization models into ML optimizations, Force2Vec provides high quality visualizations of graphs; (b) by using multiple levels of parallelism, Force2Vec runs at least an order of magnitude faster than previous graph embedding algorithms; (c) Force2Vec is uniformly good at node classification, link predictions, and clustering compared to other embedding methods; (d) Force2Vec provides a generic and parallel framework that can be easily used with various force-directed and random walk-based methods. Thus, Force2Vec enables large-scale graph learning and visualization in various scientific domains.

Force2Vec excels in both ML and visualization tasks by incorporating several similarity functions and force models. Our extensive evaluations show that different variants of Force2Vec perform well for different machine learning and visualization tasks. Based on our observations, Table 12 summarizes recommended models for common tasks. For example, Force2Vec with t-distribution usually performs the best for classifying nodes in a multi-class classification setting. Force2Vec coupled with t-distribution also performs well for the clustering task. By contrast, Force2Vec with the sigmoid similarity function performs the best for the multi-label node classification task. Force2Vec coupled with sigmoid also performs better for link predictions. To generate better layouts for visualization, Force2Vec with the LinLog model usually performs better than other models. However, if visualizing a graph is the only objective, one can just use traditional graph layout algorithms such as BatchLayout [24] because the latter algorithms are expected to generate better layouts for graph visualization.