The Web provides many data that are encoded using HTML tables. This facilitates rendering them, but obfuscates their structure and makes it difficult for automated business processes to leverage them. This has motivated many authors to work on proposals to extract them as automatically as possible. In this article, we present a new unsupervised proposal that uses a hybrid approach in which a standard computer is used to perform pre- and post-processing tasks and a quantum computer is used to perform the core task: guessing whether the cells have labels or values. The problem is addressed using a clustering approach that is known to be NP using standard computers, but our proposal can solve it in polynomial time, which implies a significant performance improvement. It is novel in that it relies on an entropy-preservation metaphor that has proven to work very well on two large collections of real-world tables from the Wikipedia and the Dresden Web Table Corpus. Our experiments prove that our proposal can beat the state-of-the-art proposal in terms of both effectiveness and efficiency; the key difference is that our proposal is totally unsupervised, whereas the state-of-the-art proposal is supervised.

Access provided by University of Auckland Library

Introduction
Automated business processes commonly integrate multiple data sources to produce more useful information than the sources can yield individually. We are interested in developing tools that help feed them with data that are provided by web sites across the world. Those data sources are particularly problematic in cases in which the data are encoded using HTML, since this encoding is intended to help render user-friendly documents, not to help machines understand the data therein. Particularly, we focus on analysing data tables, which are HTML tables that are used to display data, not to position other HTML elements on the screen. Data tables are very popular according to many studies; for instance, Cafarella et al. [8] found 154 million data tables in a crawl with 14.10 billion tables, Crestan and Pantel [15] found 1.30 billion data tables in a crawl with 12 billion tables, Pimplikar and Sarawagi [53] found 25 million data tables in a crawl with 500 million HTML documents, and the recent Web Table Corpora initiative found 233 million data tables in a crawl with 1.78 billion HTML documents.

Unfortunately, it is not easy to use the data in a typical HTML table to feed an automated business process. The problem is that such data are not structured: they are encoded using a variety of HTML tags that facilitate rendering them, but obfuscate extracting them using programmatic procedures. HTML5 provides a number of tags that are intended to encode some semantics and facilitate the extraction, but they are biased towards encoding horizontal tables, they do not provide a means to delimit the data tuples or the headers, and they cannot make it explicit the relationship between the headers and the data in the tuples; furthermore, it is very common to find tables in which the tags are not used properly (for instance, in our experimental repositories, roughly 34.96% of the label cells and 11.74% of the value cells are incorrectly encoded.) Meta-data tags are intended to enhance the ability of plain HTML to encode the semantics of the data [35]. Unfortunately, a recent analysis of the 32.04 million domains in the November 2019 Common Crawl has revealed that only 11.92 million domains provide such tags [5], which means that there are roughly 20.12 million domains that do not facilitate at all extracting their data. Major knowledge bases have built-in data extractors that can deal with many HTML documents, but they are far from universal. For instance, Oulabi and Bizer [52] analysed the tables in a few domains that are well supported by DBpedia and extracted 206,690 data records with no matches in the knowledge base. And WikiData has a number of data extractors that can only work with tables that are generated using templates like infobox, navbox, or sistersitebox. It is then not surprising that many sites that provide data tables remain out of the reach of automated business processes [46].

The previous problems clearly argue for a method to extract data from HTML tables. In the literature, there are many general proposals to extract data from HTML documents [12, 23, 55, 58, 61]. Some of them are based on analysing visual features [47], text alignment [57, 60], neural networks [59], matching trees [38], learning first-order rules [33], and also inferring propositio-relational rules [34], to mention a few. Unfortunately, they are not appropriate to extract the underlying relationships between the cells in a data table [9], which motivated many researchers to devise proposals that are specifically tailored to understand the intrinsic relational nature of HTML tables [14, 20, 31, 48, 54, 70, 71].

In this article, we present LuperQ, which is our proposal to feed automated business processes with data that are encoded using HTML tables. It deviates from the other proposals in the literature in that it relies on a quantum clustering approach that builds on an entropy-preservation metaphor that helps make the label cells apart from the value cells very effectively and efficiently, which is the cornerstone to understand the tables automatically. Our approach can find the optimum solution in polynomial time, which constitutes a huge speedup for a problem that is known to be NP using standard computers. We performed our experiments on two large repositories with tables from the Wikipedia and the Dresden Web Table Corpus. Our conclusion was that LuperQ can outperform the state-of-the-art proposal by Nishida et al. [51] in terms of both effectiveness and efficiency; the key here is that LuperQ can attain these results in a totally unsupervised manner, whereas the state-of-the-art proposal is supervised. The differences in performance were confirmed to be statistically significant using statistical hypothesis testing at the standard significance level.

The rest of the article is organised as follows: Sect. 2 describes and compares the related work; Sect. 3 presents the details of our proposal; Sect. 4 analyses its computational complexity; Sect. 5 reports on our experimental results; finally, Sect. 6 presents our conclusions and some future work.

Related work
In this section, we first summarise the literature on extracting data from HTML tables; then, we provide a short introduction to quantum computing; next, we summarise the literature on quantum clustering; finally, we discuss on our contributions.

Data extraction from HTML tables
Extracting data from HTML tables is a hot research topic. The two recent surveys by Roldán et al. [54] and Zhang and Balog [71] make it clear that there are a variety of approaches to the problem. Most of them are modelled as pipelines that are composed of the following tasks: locating the tables in the input documents, segmenting them into cells, discriminating the non-data tables, analysing the function of the cells, analysing the structure of the tables, and interpreting them. The functional analysis task is a cornerstone in this pipeline since the overall quality of the data extracted heavily depends on the ability of a proposal to make the label cells apart from the value cells.

Some of the proposals to implement the functional analysis task are very naive, namely: Braunschweig et al. [7] assumed that the label cells are located on the first row and Wu et al. [66] assumed that they are encoded using th tags only. Unfortunately, many real-world HTML tables are far more complex.

The literature provides better approaches. Chen et al. [13] devised a proposal that identifies the label cells by measuring their similarity according to some pre-defined features; Yoshida et al. [69] used the Expectation Maximisation method to learn the probability that a cell provides a label or a value using a large learning corpus; Yang and Luk [68] devised a proposal that is specifically tailored to work with numeric tables, since it exploits the fact that the data in such tables have well-known patterns; Kim and Lee [42] devised a proposal that first uses some heuristics that are based on the dimensions of the input table to determine the regions in which the label or the value cells are and then performs some pattern-based coherency checks; Jung and Kwon [36] developed another heuristic-based proposal that attempts to identify the regions with label or value cells by analysing some stylistic features; Gatterbauer et al. [25] used some visual features to match the structure of a table onto a number of common table types in which it is easy to identify the functions of the cells; Cafarella et al. [8] presented a supervised proposal that learns a classifier from a training set that provides many cells with structural and content-based features plus additional user-provided annotations; Embley et al. [21] developed a method that relies on identifying some so-called critical cells that allow to divide the input table into four regions, some of which can index the others; Milošević et al. [49] developed a heuristic-based proposal to extract data from the tables in the PubMed Central repository; and Nishida et al. [51] devised the current state-of-the-art proposal, which uses a recurrent neural network with an embedding layer, a convolution layer, a filter layer, a fully-connected layer, and a SoftMax layer.

Quantum computing in a nutshell
Quantum computing is nowadays a key topic in computing research. Unfortunately, quantum computers are far from being sold at major retail stores, but the hype behind them is motivating many companies and researchers to become early adopters and to publish many introductory tutorials [37, 41, 43]. Basically, quantum computing is about taking advantage of some phenomena that occur at the (sub-)atomic levels to perform computations. In the last three decades, many physicists have been struggling to transform the idea into real-world computing devices [10, 26]. It is now the time for computer scientists to leverage them to boost machine learning. In this field, it is common to address NP problems by searching a space of solutions using (meta-)heuristics that allow to find good-enough solutions that are not necessarily optimal. Quantum computers might be the key to find the optimal solutions very efficiently [18, 65].

Describing some fundamental differences amongst classical and quantum computers may help understand quantum computing better. They both build on encoding data using strings of zeroes and ones. In classical computers, the zeros and ones are referred to as bits and they are implemented using voltages, current pulses, or latch circuits. In quantum computers, they are referred to as qubits and they are implemented using ions, electrons, or other (sub-)atomic particles; such particles oscillate between the two states (including many intermediate states) almost instantaneously unless they are somewhat constrained. Commonly, researchers say that unconstrained particles are in superposition and constrained particles are entangled. In classical computers, computations are performed by moving the bits through a circuit that consists of Boolean gates, e.g., “not”, “and”, “or”, or “xor”. In quantum computers, there are two mainstream models: the circuit model and the adiabatic model. The circuit model is similar to the electronic counterpart since it relies on circuits in which the qubits are entangled using quantum gates, e.g., “Pauli-X”, “Hadamard”, “Phase”, or “Toffoli”. The adiabatic model arranges the qubits in a network, e.g., a Chimera or a Pegasus network, and then uses annealing to perform computations. Fortunately, standard programmers may use high-level programming languages that are compiled into machine code that is interpreted by a processor that ultimately relies on Boolean gates. Unfortunately, quantum programmers must use a standard computer to encode the data and to set up a circuit or a network, then wait for the qubits to stabilise, and then use a standard computer to observe their state and to decode the results. Simply put: classical computers may be programmed at a very high abstraction level in which Boolean gates constitute just a theoretical foundation; in quantum computers, the qubits, the gates, or the networks constitute the highest abstraction level achieved so far.

As of the time of writing this article, it remains unclear which model is superior to the other or if they are equivalent. The quantum circuit model is attractive in that it is the closest to the electronic counterpart; unfortunately, the lack of quantum memory renders most current proposals of theoretical interest only. The adiabatic approach seems very appropriate to deal with problems that can be naturally expressed as optimisation problems and the current technology provides many more qubits than the circuit model; furthermore, there are some research results that allow to decompose arbitrary-size problems into smaller problems whose individual solutions can be computed using an adiabatic computer and then combined using a standard computer [6].

The key is that quantum computers work almost instantaneously. That is obviously a strong point in favour of quantum computing; unfortunately, the problems are manyfold, namely: the number of qubits is very limited (e.g., Google’s Bristlecone computer relies on the circuit model and provides 72 qubits, whereas D-Wave’s Advantage computer relies on the adiabatic model and provides 5000 qubits); developers must work at the qubit level because there are not any “quantum compilers” that can transform arbitrary algorithms that are expressed in high-level languages into quantum circuits or adiabatic networks; quantum computers are very sensitive to noise, which means that their current error rate is far above the error rate in standard computers; finally, quantum computers require to sample the solution to a problem many times in order to determine which the most-probable one is.

Approaches to quantum clustering
Regarding the circuit model, there are some well-known proposals by Aïmeur et al. [1], Wittek [64], and Kerenidis et al [40]. They leveraged some key ideas from algorithmic proposals like k-means, k-medians, or hierarchical clustering and attempted to implement them using quantum circuits. The key problem is to compute the distance between any two points in the input dataset, which has been addressed using oracles (associative memories), Grover’s search circuit, or the improvement by Dürr and Høyer. Unfortunately, these solutions require quantum memory circuits, which are still on the design table. Giovannetti et al. [27] or Kerenidis and Prakash [39] have recently presented some proposals to implement quantum memory circuits that might be used to implement an oracle by pre-loading the distance matrix using a standard computer. Unfortunately, as far as we know, this kind of memory has not been implemented yet, which renders the previous proposals of theoretical interest only. Neither is it clear how floating point operations can be implemented using quantum circuits since the number of qubits required to implement a single double precision floating point number typically exceeds the total number of qubits that are available in many quantum computers. Chakraborty et al. [11] presented some building blocks that might be useful to implement a quantum version of k-means. Unfortunately, their proposal has many limitations: their formulation works for k=2 only and it is not clear how it can be extended; they require a standard computer to compute the seed clusters; they can implement only an iteration of the k-means algorithm since current quantum circuits cannot loop; and it cannot deal with real data since their circuits work on positive integers only (neither is it clear how they react when a subtraction results in a negative integer). Kerenidis et al. [40] have recently presented another proposal to implement k-means using a quantum circuit. It can deal with real data, but it also requires a quantum memory circuit to store intermediate data. The authors also devised a proposal to implement such a circuit [39], but, as far as we know, it has not been materialised yet. They analysed the performance of their proposal in the case of so-called well-clusterable datasets, but how it works with other kinds of datasets is not known. (A dataset is well-clusterable if most of the data are close to the centroids, the centroids are sufficiently far from each other, and the clusters are approximately the same size.)

The adiabatic model seems to be more appropriate to address clustering problems. The reason is that clustering can be naturally modelled as an optimisation problem in which the goal is to maximise a fitness function that measures how compact and isolated the clusters are subject to restrictions like hard vs soft clustering, partitioning vs overlapping clustering, flat vs hierarchical clustering, or single-way vs multi-way clustering. The key difficulty is how to map clustering onto a problem that an adiabatic computer can solve. Currently, they can only deal with quadratic unconstrained binary optimisation problems (aka QUBO problems), using either spin variables, whose values are +1 or −1, or Boolean variables, whose values are 0 or 1. Several authors have devised different approaches to perform the mapping. Neukart et al. [50] first use a standard computer to generate a number of random seed clusters that are distributed homogeneously across the input dataset; the number of data in each seed cluster (ν) and the shift of each seed cluster with regard to the previous one (ϵ) are hyper-parameters that must be fine-tuned for each particular dataset. They require a total of kdν qubits to cluster a dataset with d-dimensional data into k clusters; basically, they associate dν qubits with each cluster and formulate a QUBO problem per datum in the input dataset; they assign the datum to the cluster that results in a larger number of qubits whose state is one. Bauckhage et al. [4] presented a proposal to cluster a dataset into two clusters of approximately the same size. They require the input dataset to be standardised so that its mean is zero. Their proposal builds on the fact that the k-means algorithm actually implements a number of heuristics whose goal is to find a good-enough minimum of the within-cluster scatter function; they realised that finding that minimum is equivalent to maximising the between-cluster scatter function, which sounds intuitive but requires a non-trivial mathematical proof. Assuming that k=2 and the two clusters are similar in size, they introduced a clever trick to compute the QUBO problem without relying on explicit distance functions. Kumar et al. [44] presented two proposals. The first one uses one-hot encoding and can cluster a dataset into an arbitrary number of clusters; basically, it assigns an array of k Boolean variables to each datum; each variable determines whether the datum belongs to the corresponding cluster or not. The idea was to compute an energy matrix from the Euclidean distance matrix, but their encoding scheme requires to set n Lagrange multipliers whose exact values are not easy to compute. Unfortunately, the authors concluded that the quality of the clustering worsens as n≫k, which is very common in practice. Their formulation is also very dependent on the floating point precision that can be implemented using the quantum computer, which is six bits in the current state of the art according to the authors. This is the reason why they developed an ad-hoc proposal for the common case in which k=2. Wereszczyński et al. [62] presented another approach that targets finding two clusters only; unfortunately, it was only simulated on a standard computer using small datasets with 50 data only. Their proposal assigns a qubit to every datum and creates a QUBO problem in which data that are less than half the diameter of the dataset are assigned positive weights and data that are further apart are assigned negative weights.

There are a few more proposals that got inspiration from quantum mechanics. They rely on algorithms that are intended to be run on a standard computer, not a quantum computer. The goal is to map the clustering problem onto some phenomena that are formalised using Schrödinger’s equation, which relies on a wave function that can be analysed using some complex, but well-known results in numerical calculus. Horn and Gottlieb [30] range amongst the pioneers in this field. They devised a quantum clustering algorithm that basically constructs a wave function from the input dataset and then searches for the clusters in its equi-potential regions. It relies on a hyper-parameter that determines how large the clusters can be. Unfortunately, the authors presented a solution for two-dimensional data only and simply sketched how to extend it to the multi-dimensional case; an inherent problem of this extension is that the evaluation of the potential function requires O(n2) time, where n denotes the number of data, independently from their dimensionality. Li et al. [45] followed up on Horn and Gottlieb’s [30] proposal and devised a new one that applies Kernel Entropy Component Analysis [32] to the input dataset prior to clustering; they also devised a statistical approach to compute the potential function that is more effective and efficient than the original one, but still requires to compute the k-nearest neighbourhood of every datum, which is an inherently complex operation. Decheng et al. [16] presented an improvement that uses a new weighted distance that does not assume that all of the attributes of the input data are equally important. Unfortunately, the method to compute the weight heavily depends on the characteristics of the datasets, which hinders its practical applicability. Eslava et al. [22] presented an approach that improves on Horn and Gottlieb’s [30] and Li et al’s [45] proposals since they use a Bayesian framework that provides an objective measure of goodness-of-fit that helps them optimise their hyper-parameters in a totally unsupervised manner.

Discussion
Extracting data from HTML tables is an important problem nowadays. The sprout of data-hungry services that rely on machine learning is motivating the need for many datasets on a variety of topics. Unfortunately, many such datasets are available in HTML tables only, which argues for a solution to extract them automatically. In the literature, there are many proposals to extract data from HTML tables, but our analysis reveals that none of them has ever attempted to use a clustering approach to implement the functional analysis task, which we think is one of the cornerstones in the process.

We guess that many authors may have thought that the clustering approach would not result in an efficient solution to the problem. Assuming that the cells are somewhat projected onto d-dimensional feature vectors (d≥1), the size of the space that must be searched to find which of those features are actually relevant is O(2d); furthermore, the size of the space that must be searched to find the clusters is 1k!∑i=0k(−1)k−i(ki)in, where k denotes the number of clusters and n denotes the number of cells (n≥1) [67]. Finding the optimum in such a space is not feasible using standard computers; even in cases like ours in which k=2. This has motivated many researchers to devise algorithms whose goal is to find good-enough clusterings that are not necessarily optimal. Furthermore, it is not trivial to determine which of the clusters correspond to which kind of cells; the clustering is a clue, but not the solution to the problem.

Quantum computing has changed the vision of NP problems since many of them have been proven to become polynomial or even constant-time using that technology. Clustering is one such problem. In the literature, there are many proposals: we discard the ones that rely on the circuit model because there are not any quantum memories available, which renders such proposals of theoretical interest only; we also discard the quantum-inspired proposals, because they are standard algorithms that were devised to run on standard computers, not quantum computers; thus, our focus is on the adiabatic model, which seems more appropriate to address clustering.

Our proposal differs from the existing ones, namely: it relies on an entropy preservation metaphor in which the input dataset is interpreted as a collection of particles, which deviates from the k-means-inspired approaches by Neukart et al. [50] or Bauckhage et al. [4]; it has proven to work well in a context in which the size of the clusters is clearly unbalanced, but the proposal by Bauckhage et al. [4] is biased towards finding clusters that are similar in size; it has proven to work well with datasets in which the number of data is much larger than the number of clusters, but the first proposal by Kumar et al. [44] worsens as the difference increases; it requires floating point computations during its preprocessing step only, which is carried out using a standard computer, whereas Kumar et al.’s [44] first proposal depends on the limited precision that can be achieved using the quantum computers; it can be easily generalised to an arbitrary number of clusters, although we only need two, whereas the proposals by Bauckhage et al. [4], Wereszczyński et al. [62], and the second proposal by Kumar et al. [44] can only find two clusters; furthermore, it has been tested on two large repositories of real-world tables, whereas Wereszczyński et al.’s [62] proposal was only simulated and tested on small datasets.

Our proposal
In this section, we first present some preliminaries and then describe the three steps in which we have grouped the tasks of the data extraction pipeline: pre-processing, identifying cell functions, and post-processing.

Preliminaries
Definition 1
(Mathematical concepts) A d-dimensional vector V is a tuple of the form (v1,v2,…,vd) (d≥1); its dimensionality is denoted as dimV=d. Given vector V and a natural number i, its component at position i is denoted as V[i] (1≤i≤dimV). A (p, q)-dimensional matrix M is a tuple of the form ((v1,1,v1,2,…,v1,q),…,(vp,1,vp,2,…,vp,q)) (p≥1,q≥1); its dimensionality is denoted as dimM=(p,q). Given matrix M and two natural numbers i and j, its component at position (i, j) is denoted as M[i, j] (1≤i≤p,1≤j≤q).

Definition 2
(Documents, tables, data records) A document is a text file whose contents are encoded using the HTML mark-up language, which allows to represent it using a DOM tree. A table is a grid that is used to display data or to position other elements on the screen. The former are called data tables and the latter are called non-data tables. Every piece of HTML whose root is a node with a table tag is considered a table. Rows are encoded using tr tags and cells are encoded using th or td tags. The function of a cell can be either label, which means that it provides a semantic hint, or value, which means that it provides a datum. Data tables are commonly laid out as horizontal listings, in which the label cells are at the top rows (if any) and the value cells are in the rows below, vertical listings, in which the label cells (if any) are on the leftmost columns and the value cells are in the columns to the right, and matrices, which have label cells both at the topmost rows and the leftmost columns and the value cells occupy the bottom-right area of the table. A data record is a map of the form {hi:vi}ki=1 (k≥1) that represents the data that have been extracted from a table. Each hi is a header and each vi is a value (1≤i≤k); the headers result from catenating one or more labels in the same row/column to form a descriptor that provides a semantic hint for the corresponding value.

Table 1 Taxonomy of attributes
Full size table
Definition 3
(Features, clusterings) A feature is a property of a cell. The features of a cell are represented by means of feature vectors; the features of the cells of a table are represented by means of a collection of feature vectors. The features can be categorised as intra-cell features or inter-cell features. An intra-cell feature is computed from the attributes of a single cell. (Table 1 provides the catalogue of attributes from which we computed useful intra-cell features in our experimental study.) It can be a visual feature, which is computed from the rendering, a structural feature, which is computed from the structure of the DOM tree, a lexical feature, which is computed from the contents of the cells, or a miscellaneous feature, which is computed from several sources. An inter-cell feature is computed from the deviation of the intra-cell features of a cell with respect to the intra-cell features of the cells in the same row, column, or table. A clustering is a Boolean matrix whose components indicate whether the corresponding cells in a table are label cells or value cells.

Definition 4
(Quantum foundations) A quantum system is a set of (sub-)atomic particles that interact with each other through couplers. (Note that is not generally possible to connect every two particles with a coupler due to physical constraints.) Both the particles and the couplers may have some energy; the particles interact with each other by exchanging energy through the couplers. The energy level of a particle induces a property that can be probabilistically mapped onto a Boolean state, e.g., a spin or a spatial position. A particle is said to be in superposition or entangled depending on whether its state is not constrained or constrained by its interaction with other particles, respectively. The state of a system is determined by the state of its particles. A Hamiltonian is a function that models the total amount of energy of a quantum system in terms of its state. A process in a quantum system is said to be adiabatic if it does not transfer any energy or mass to the environment, that is: if it happens in total isolation. A quantum computer is a device that traps a number of particles, sets their initial states, controls their interactions through the couplers, and reads their final states.

Definition 5
(Ising systems) An Ising system is a quantum system whose Hamiltonian is modelled as follows:

∑i=1nH[i]S[i]+∑i,j=1∣i<jn,nJ[i,j]S[i]S[j],
where n denotes the number of particles, H[i] denotes the energy of the ith particle, J[i, j] denotes the energy of the coupler between the ith and the jth particle, and S[i] and S[j] are variables that denote their spin (1≤i≤n,1≤j≤n). The range of energy depends on the quantum computer used, so it is commonly normalised in real interval [−1.00,+1.00]. The spin variables range in set {−1,+1}.

For the sake of simplicity, this Hamiltonian is commonly rewritten as follows:

BTEB,
where B denotes the following vector of Boolean variables:

B[i]=1/2(S[i]+1)(1≤i≤n),
and E=diag(H)+J is the so-called energy matrix, where diag(H) denotes an n×n diagonal matrix with the elements of vector H.

Definition 6
(QUBO problems, adiabatic optimisation) A QUBO problem is a quadratic unconstrained binary optimisation problem of the following form:

argminBBTEB
where E is an energy matrix and B is a vector of Boolean variables. Adiabatic optimisation is a quantum method that helps finding its optimal solutions. This is a well-known NP-hard problem using standard computers [50], but it can be addressed using adiabatic computers as follows: assume that there is a quantum computer that implements a system whose Hamiltonian is h; let ht denote the exact value of the Hamiltonian at time t (t≥0); assume that the system has a particular Hamiltonian h∗ whose state encodes the solution to the problem and that h0 and h∗ do not commute; then, the adiabatic theorem [28] states that there is a time s such that if the Hamiltonian of the system changes adiabatically according to equation

ht=(1−t/s)h0+t/sh∗,
then the system will remain in a state with Hamiltonian h∗ after time s. That is, it suffices to wait for s units of time so that the system stabilises in a state that represents the solution to the problem. The value of s is known to be in the order of O(eαnβ), where α and β are positive constants and n is the size of the problem. Determining whether constants α and β are small enough to render the original problem tractable or not in a particular computer is a matter of experimentation [3].

Fig. 1
figure 1
Sample tables and data records

Full size image
Fig. 2
figure 2
Running example: sample document

Full size image
Example 1
Figure 1 shows some real-world sample HTML tables with different layouts and their corresponding data records. Figure 2 shows a fictitious sample document that we use as a running example; the details regarding the CSS style used are omitted since they are cumbersome and provide little information to illustrate the proposal. The header of the document consists of a menu with options and a navigation bar; the body consists of a grid that shows the marks attained by several students (per column) on their assignments (per row); the footer consists of a copyright message. Note that the author of the document decided to encode both the menu and the grid using the table HTML tag, but the menu is a non-data table and the grid is a data table in this context. The grid is a matrix table because it has labels both on the first row and column. We have attached some tags to a few elements in the document to facilitate locating them in the HTML excerpt; we have also added a small number at the upper right corner of each cell in the grid to facilitate references. Realise that the encoding focuses on describing how the HTML elements must be rendered, not on helping a computer understand the data.

Step 1: pre-processing
This step implements the location, the discrimination, and the segmentation tasks of the data extraction pipeline. It gets a document as input and returns its collection of data tables plus their corresponding collections of feature vectors.

The first sub-step computes the collection of tables in the input document, namely:

1.
It loads the input document and transforms it into a DOM tree using an HTML parser. Next, it uses a headless browser to render the DOM tree on a virtual canvas and makes it explicit all of the attributes of the DOM nodes using a simple custom script. Finally, it uses a CSS selector to locate the DOM nodes with a table tag.

2.
It then discards the tables whose width or height attributes are 0px or the tables whose display attribute is none because they are not visible to the user; it also discards the tables that have one single row/column because they are typically used to display listings that do not provide any data in tabular form; it further discards the tables that are nested within other tables because they are typically used to show menus, paginators, and other structures without any relevant data.

3.
It then sets the maximum cell span to 200 cells, which helps avoid overhead when processing tables that are incorrectly encoded. The cells whose span is greater than one are replicated accordingly and their span is set to one. The rows that are shorter than the largest row are padded to the right using empty cells, which prevents outputting ragged tables. If the input table or any of its ancestors in the DOM tree has attribute dir set to rtl, then the table is flipped horizontally so that the first column is always the leftmost column. Duplicated rows or columns are removed, except for the topmost and the leftmost ones. If all of the cells in a row or a column are empty, then they are removed. (A cell is considered empty if it consists exclusively of blanks, dashes, question marks, or language-dependent symbols like “N/A”.)

The second sub-step computes the collection of feature vectors that correspond to the cells of the input table. It iterates over the DOM nodes that correspond to the cells and computes their intra-cell features as the weighted average of their attributes and the intra-cell features of their children, where the weight is computed as the relative area of the bounding box in which each DOM node is rendered. We transform the attributes of the DOM nodes as follows: in the case of composite attributes, e.g., the colours, we split them into their components; in the case of enumerated attributes, e.g., the font, we use one-hot encoding; in the case of numeric attributes (be them original or split ones), we normalise their values in interval [0.00,+1.00] using global min-max normalisation. This way, all of the attributes are numeric and range in the same interval, which facilitates computing the features. Then, we compute three inter-cell features that measure the deviations of the intra-cell features with respect to the intra-cell features of the cells in the same row, column, and table. The result is a collection of (4κ)-dimensional vectors, where κ denotes the number of intra-cell features.

Fig. 3
figure 3
Running example: pre-processing

Full size image
Example 2
Figure 3 illustrates the pre-processing step. The input is the document in our running example. The first sub-step converts it into a DOM tree and cleans it: first, the document is parsed and the table elements are selected; second, the table element that represents the main menu is discarded because it consists of a single row, which is not common at all for data tables; third, the table element that represents the grid needs not any more cleaning because it does not have any excessively-spanned cells, the document is written using the common “lrt” direction, and no duplicated or empty rows/columns exists. The second sub-step transforms it into a collection of vectors that represent the cells in the grid, in the order indicated by the small numbers at the upper-right corner of each cell. Unfortunately, this collection is far too large to be shown in a figure because it has 16 feature vectors with 316 dimensions each; thus, we decided to illustrate how it looks only. The first group of columns refer to the intra-cell features that are computed from the attributes of the DOM tree, cf. Table 1; basically, the attributes are decomposed and transformed into numeric ones, if necessary, and then normalised to [0.00,+1.00]. The following three groups of columns refer to the inter-cell features that measure the deviation of the previous features with respect to features of the cells in the same row, column, and table, respectively.

Step 2: identifying cell functions
This step implements the functional analysis task in the pipeline. It works on the tables and the collections of feature vectors returned by the previous step as follows: it first clusters the cells and then uses some heuristics to determine the function of the cells in each cluster.

Clustering the cells
Our idea is to map the clustering problem using an entropy-preservation metaphor. It is well-known that the entropy of the Universe increases monotonically. The net effect is that the Universe expands because its particles tend to go further apart as time goes by (thus increasing the entropy) unless some energy is used to keep them together (thus preserving the entropy). Our idea is to interpret a dataset as a collection of particles in a multi-dimensional Universe. For these particles to be stable in that space, some energy must be injected in the system so that its total amount of entropy remains constant; otherwise, the particles would tend to move apart and the entropy would increase. The idea is then to assume that the cells in a table are particles that are represented using their feature vectors; if the table “does not disintegrate”, it is because there is some energy that keeps the entropy constant. The goal is then to find a clustering that requires a minimum amount of energy to keep the entropy constant.

The first sub-step maps the clustering problem onto a standard QUBO problem of the following form:

argminBBTEB,
where B is a Boolean variable vector and E is the energy matrix.

First, we have to define the Boolean variables. We introduce a matrix K with n×2 Boolean variables such that variable K[i, k] denotes whether the ith cell belongs to cluster k or not and n denotes the number of cells (1≤i≤n,k∈{1,2}). The relationship between the variables in K and the variables in B are the following:

B[i]=K[i,k]=K[(i−1) mod n+1,⌈i/n⌉] and B[i+n(k−1)].
Although the formulation is a bit involved, it is conceptually simple since the goal is to map the two-dimensional structure of matrix K onto the one-dimensional structure of vector B and vice versa. (Note that this idea can be easily extended to an arbitrary number of clusters; we focus on two clusters because we have two kinds of cells only.)

Now, we have to define the energy matrix. We know that E[i, j] is the coefficient that corresponds to variables B[i] and B[j] in the QUBO problem (1≤i,j≤2n). We also know that variable B[i] corresponds to variable K[p, q] and variable B[j] corresponds to variable K[u, v], where

p=q=u=v=(i−1) mod n+1,⌈i/n⌉,(j−1) mod n+1, and ⌈j/n⌉.
We now need to introduce two ancillary functions, namely: a distance function D, which works on two indices that represent two cells and returns the distance between their corresponding feature vectors, and an energy function E, which works on a distance value and returns the amount of energy required to keep two particles at that distance so that the entropy is preserved. Without any loss of generality, we assume that function D is normalised to interval [0.00,+1.00], where the lower bound indicates the distance between a cell and itself and the upper bound indicates the maximum distance between any two cells in the same table; we also assume that function E is normalised to interval [−1.00,+1.00], where the lower bound represents the amount of energy required to keep the farthest particles apart and the upper bound represents the amount of energy required to keep the closest particles together.

There are several cases regarding the energy matrix:

Case p=u and q=v::
in this case, B[i] and B[j] refer to the same cell and the same cluster. It makes sense to set E[i,j]=−1.00 so that the system is biased towards setting B[i]=B[j]=1, which helps minimise the Hamiltonian; that is, the system is biased towards setting K[p,q]=K[u,v]=1 so that both cells are assigned to the same cluster.

Case p=u and q≠v::
in this case, B[i] and B[j] refer to the same cell, but different clusters. It makes sense to set E[i,j]=+1.00 so that the system is biased towards setting B[i]=0 or B[j]=0, which helps minimise the Hamiltonian; that is, the system is biased towards setting K[p,q]=0 or K[u,v]=0, which prevents the same cell from being assigned to two different clusters. Note that this case works co-ordinately with the previous one, which seeks to assign each cell to a cluster.

Case p≠u and q=v::
in this case, B[i] and B[j] refer to different cells in the same cluster. It makes sense to set E[i,j]=−E(D(p,u)) so that the system is biased towards setting B[i]=1,B[j]=1, or both depending on how distant the cells are. Note that the maximum energy level returned by function E is +1.00, which happens when two cells have the same feature vectors and their distance is then 0.00; such cells should clearly be in the same cluster, which is biased by setting E[i, j] to the minimum possible energy level. Note, too, that the minimum energy level returned by E is −1.00, which corresponds to the farthest apart cells; such cells should clearly not be in the same cluster, which is biased by setting E[i, j] to the maximum possible energy level. The energy level that corresponds to the other pairs of cells depends completely on the model provided by function E.

Case p≠u and q≠v::
in this case, B[i] and B[j] refer to different cells in different clusters. It is the complementary of the previous case, so it makes sense to set E[i,j]=−E(D(p,u)).

Note that the exact formulation regarding how the energy matrix is computed depends completely on the definitions of the distance function D and the energy function E. In the literature, there are a variety of choices to implement both functions [2, 17, 29] and none of them is clearly superior to the others in every case. Thus, we defer making a decision to the experimental analysis section.

The second sub-step uses a quantum computer to find the values of the B variables that minimise BTEB. Once the values of the B variables are computed, it is very simple to map them onto the corresponding values of the K variables that determine the cluster to which each cell belongs. Hopefully, this step will result in a perfect clustering almost instantaneously, which is the key to attain the best possible effectiveness very efficiently.

Fig. 4
figure 4
Running example: clustering cells

Full size image
Example 3
Figure 4 illustrates the previous procedure. The first sub-step consists in transforming the collection of feature vectors that was computed previously into a QUBO problem that helps compute a clustering of the input cells. That requires to introduce 32 Boolean variables of the form K[i, k] because we have 16 cells and need to group them into two clusters; intuitively, variable K[i, k] must be one in cases in which the ith cell belongs to cluster k and zero otherwise (1≤i≤16,k∈{1,2}). The K variables are very intuitive, but their two-dimensional structure makes it impossible to use them in a QUBO problem unless they are mapped onto a one-dimensional vector. That is the role of the 32 Boolean variables of the form B[j], for 1≤j≤2n; intuitively, variables B[1],B[2],…,B[16] correspond to variables K[1,1],K[2,1],…,K[16,1] and variables B[17],B[18],…,B[32] correspond to variables K[1,2],K[2,2],…,K[16,2]. Once the variables are mapped, it is relatively easy to compute the corresponding energy matrix by finding the cells to which each component of the matrix refers to, computing their distances, and the associated energy level. The energy matrix is a 32×32 real matrix whose components range in interval [−1.00,+1.00], where the lower bound corresponds to the closest cells and the upper bound corresponds to the most distant cells in terms of their feature vectors. The second sub-step finds the values of the B variables using a quantum computer, then maps them onto the corresponding K variables, and outputs a clustering, which is a Boolean matrix that indicates the cluster to which each cell belongs.

Finding the function of each cluster
The idea is to use some heuristics that help determine which cluster has the label cells and which one has the data cells.

The first sub-step consists in removing the noise in the clustering that was computed before. Our experiments prove that the clusterings are not generally perfect because they are totally independent from the actual layout of the input tables. We decided to include the row and the column of the cells as additional intra-cell features, which definitely helped in the process, but it is not generally expected to produce perfect clusterings because there are cases in which a few label or data cells have features that deviate largely from the other cells of the same kind. Thus, it is necessary to resort to a heuristic to remove the noise from the resulting clusterings and the majority vote has proven to work very well in our context. That is, if there is a subset of cells with a given function in a row/column in which the majority of cells have the opposite function, we then correct the function of that minority of cells. In case of ties, the majority vote around the cells is used. If the ties persists, then a random choice is made. This approach proved to correct the noise very well.

The second sub-step determines which cluster corresponds to label cells and which one corresponds to value cells. We use a heuristic that builds on the following variables:

rk=sk=ck=1/2(ak/n+bk/m),1−|Kk|/mn, and 1−∑d∈Dkd/(|Kk|maxDk),
where k∈{1,2},m and n denote the number of rows and columns, ak and bk denote the number of cells in the first row or column that are in cluster Kk, and Dk is a set with the distance of the cells in cluster Kk to the top-left position in the table. Set Dk is defined as follows: Dk={(p−1)2+(q−1)2−−−−−−−−−−−−−−−√∣⟨p,q⟩∈Kk}, where notation ⟨p,q⟩∈Kk gets the indices of the cells from the feature vectors in cluster Kk.

Variable rk measures the ratio of cells in cluster Kk that are in the first row and column; thus, the higher rk, the higher the chances that cluster Kk consists of label cells since such cells are typically placed in the first row or column (k∈{1,2}). Variable sk measures the one-complement of the relative size of cluster Kk; thus, the higher sk, the higher the chances that cluster Kk consists of label cells since these cells are typically a minority (k∈{1,2}); finally, variable ck measures the one-complement of the average normalised distance of cluster Kk to the top-left corner of a table; thus, the smaller ck, the higher the chances that cluster Kk consists of label cells since such cells are typically near the top-left corner of the tables (k∈{1,2}).

Our heuristic is now straightforward using the previous formulation: we assume that the label cells are in cluster K1 and the value cells in cluster K2 if 1/3(r1+s1+c1)≥1/3(r2+s2+c2); otherwise, we assume that the value cells are in cluster K1 and the label cells are in cluster K2.

Fig. 5
figure 5
Running example: finding functions

Full size image
Example 4
Figure 5 illustrates how the function of the clusters is found. The first sub-step cleans the input clustering. Note that it identifies the empty cell at the upper left corner as belonging to cluster K2. This cell deviates largely from the others both in style and contents, but the majority of cells in its row and column were identified as belonging to cluster K1. Thus, it makes sense to correct the clustering and flag that cell as belonging to cluster K1. The second sub-step computes the rk,sk, and ck heuristic variables (k∈{1,2}) in order to determine the functions of the cells in each cluster. The rk variables measure the proportion of cells in each cluster that are on the first row and/or column (k∈{1,2}); realise that r1=1.00 and r2=0.00 because all of the cells of cluster K1 are in the first row and column, which provides a strong indication that the cells in cluster K1 are label cells. The sk variables measure the one-complement of the relative size of the clusters (k∈{1,2}); realise that s1=0.56 and s2=0.44, which indicates that cluster K1 has the minority of cells and is thus likely to contain the label cells. The ck variables somewhat measure the distance of cells to the top-left corner (k∈{1,2}); realise that c1=0.43 and c2=0.31, which again indicates that cluster K1 is the closest to the upper-left corner and then more likely to have label cells. Summing up: the three heuristic variables indicate that cluster K1 is likely to provide the label cells and cluster K2 is then likely to provide the data cells.

Step 3: post-processing
This step implements the structural analysis and the interpretation tasks in the pipeline. It works on an input table, the collection of feature vectors computed in the first step, and the clustering computed in the second step; it returns a set of records that provide the data in the input table in a structured format that is amenable for further processing.

The first sub-step classifies the input table as a horizontal listing, a vertical listing, or a matrix. This is simple in the case in which none of the clusters in the input clustering is empty; it is a bit more involved in the other case. Sometimes, the input table does not have any label cells because the semantics are implicit in the context or the data themselves. These tables result in a clustering in which one of the clusters is empty. In such cases, we guess the orientation according to the group of inter-cell features whose average deviation is the smallest one, namely: it is horizontally-oriented if the average of the per-column inter-cell features is the smallest one; it is vertically-oriented, if the average of the per-row inter-cell features is the smallest one; otherwise, it is both horizontally- and vertically-oriented if the smallest average corresponds to the per-table inter-cell features. Our proposal is to add an artificial row and/or column of computer-generated label cells before the first actual row and/or column in the input table. Classifying the input table is now straightforward: if the first topmost rows consists of label cells, then it is a horizontal listing; if the leftmost columns consists of label cells, then it is a vertical listing; if both the topmost and the leftmost columns have label cells, then it is a matrix.

The second sub-step consists in creating the data records themselves. If the input table is a horizontal/vertical listing, then the label cells in the first few rows/columns index the data on a per-column/per-row basis. In this case, the records are generated per row/column by catenating the contents of the label cells vertically/horizontally to form a header and mapping them onto the contents of the corresponding value cells. In cases in which the catenation of the labels results in two identical headers, we disambiguate them by means of a sequential index; in cases in which two consecutive labels are the same, one of them is ignored since this is typically the result of replicating a spanned cell. If the input table is a matrix, then each individual value cell results in a record in which its unique component has two headers that correspond to catenating the label cells in the corresponding row and column as we mentioned before.

Fig. 6
figure 6
Running example: post-processing

Full size image
Example 5
Figure 6 illustrates how the records of the original table are computed. The first sub-step takes the input clustering and guesses that the input table is a matrix because it has label cells in both the first row and column. The second sub-step then creates a data record in which the headers provided by the label cells are mapped onto the values provided by the corresponding data cells. Note that we used the JSON notation to represent the data records and a slash to separate the headers, but this is completely customisable in our implementation.

Complexity analysis
In this section, we analyse the complexity of our proposal. Our goal is not to characterise its exact time complexity, but an upper bound that makes it clear that it can break the NP complexity that is inherent to finding an optimal clustering using a standard computer. For instance: we use the number of DOM nodes in a document as an upper bound to the number of cells in any of its tables, which is perfectly valid, but very likely far above the actual figure. We first present three supporting lemmata and then the theorem that analyses the complexity of LuperQ; there is a corollary that instantiates the previous complexity result in the context of the adiabatic quantum computer used in our experimentation. Below, we use ν to denote the number of DOM nodes in the input document and κ to denote the number of intra-cell features; α and β denote two constants that depend on the adiabatic computer used.

Lemma 1
(Step 1: Pre-processing) This step does not require more than O(νκ) time to process an input document.

Proof
The pre-processing step involves the following operations: a) locating the tables: this involves parsing, rendering, and selecting the table nodes; these operations can be implemented using industrial components that are not expected to require more than O(ν) time. b) Discriminating the tables: this requires to check a few conditions on a subset of DOM nodes; thus, this operation may not require more than O(ν) time. c) Segmenting the tables, which requires to perform some simple operations on their cells; then, this operation may not require more than O(ν) time. d) Computing the features; this operation requires to normalise the attributes (which may not require more than O(νκ) time), to compute the intra-cell features (which may not require more than O(νκ) time), and to compute the inter-cell features (which may not require more than O(νκ) time to compute the intra-cell feature averages plus O(3νκ) time to compute the deviations). Summing up: the pre-processing step may not require more than O(3ν+3νκ+3νκ)⊆O(νκ) time. □

Lemma 2
(Step 2: Identifying cell functions) This step does not require more than O(ν2κ+eανβ) time to process a table.

Proof
This step involves the following operations: a) clustering the cells: this operation requires to compute the distance between every two cells in the input table and the corresponding energy to preserve the entropy of the dataset; since there cannot be more than ν cells, then this operation may not require more than O(ν2κ) time (note that the time to compute the distance amongst any two κ-dimensional vectors is not expected to require more than O(κ) time, but computing the associated energy level involves computing a real-valued function on a distance only, which may be safely assumed to require O(1) time); finding the optimal clustering requires a time in the order of O(eανβ) [28], where α and β are constants that depend on the quantum computer used. b) Correcting the clustering: this operation may not require more than O(ν) time to compute the majority vote per row/column plus O(ν) time to correct the function of each cell. c) Finding the function of each cluster: this time is dominated by the computation of variables ci, which requires to iterate over every pair of cells; that is, it may not require more than O(ν2) time. Summing up: identifying cell functions may not require more than O(ν2κ+eανβ+2ν+ν2)⊆O(ν2κ+eανβ) time.

□

Lemma 3
(Step 3: Post-processing) This step does not require more than O(νκ) time to process a table.

Proof
This step involves the following operations: a) Classifying the input table: if none of the clusters identified is empty, then the operation must determine if the label cells are arranged at the topmost rows, the leftmost columns or both, which may not require more than O(ν) time; otherwise, it must check the coherency of the rows and columns, which may not require more than O(3νκ) time to iterate over the cells and compute the average deviation of their features; since O(ν)⊆O(3νκ), then classifying the input table is not expected to take more than O(3νκ) time. b) Computing the records: this operation iterates over the cells and creates the records by associating headers to the value cells, which may not require more than O(ν) time. Summing up: the post-processing step may not require more than O(3νκ+ν)⊆O(νκ) time. □

Fig. 7
figure 7
Distribution of number of DOM nodes

Full size image
Theorem 1
(Complexity of LuperQ) Our proposal does not require more than O(ν2+eανβ) time to process a table.

Proof
According to the previous lemmata, LuperQ does not take more than O(νκ+ν2κ+eανβ+νκ) time to process a table. Realise that ν is generally expected to be larger than κ since typical HTML documents have thousands of DOM nodes, whereas the total number of features is comparatively smaller. Figure 7 shows the distribution of the number of DOM nodes in the HTML documents in our experimental repositories; the average is 973.97 DOM nodes, with a minimum of 6 nodes and a maximum of 39,552; note that the number of features is a constant in our experiments, cf. Table 1. Thus, the previous upper bound is a subset of O(ν+ν2+eανβ+ν)⊆O(ν2+eανβ). Recall that α and β are constants that depend on the quantum computer used [28], so the actual time complexity depends completely on it.

□

Corollary 1
(Our implementation) We implemented LuperQ on a hybrid system: we used a standard computer to run the pre-processing step, the computation of the QUBO problem, and the post-processing step; we used an adiabatic quantum computer to solve the QUBO problem. The quantum computer was a D-Wave 2000Q system, which allows to configure the annealing time from 1 up to 2000 μs. We used the default value of 20 μs, as suggested by the manufacturer, which means that eανβ≤20μs. Unfortunately, this does not imply that we can solve every clustering problem in a maximum of 20 μs because there are other factors that have an impact on the total time. These factors include the time required to set the computer up, which usually requires some milliseconds, and the time that the computer must be idle in between two consecutive problems, which is in the order of the annealing time. Furthermore, the number of qubits available is 2048 and they are organised in a Chimera network that allows for a maximum of 65 fully-connected qubits. This implies that every QUBO problem must be mapped onto the available qubits taking their physical connections into account; furthermore, some large problems must be partitioned into smaller problems whose solutions must be combined. The previous problems can be addressed using a standard computer thanks to D-Wave’s qsolver algorithm [6]. There is not a complexity analysis available, but the results of the performance studies that were carried out on complex, large synthetic problems show that the delay ranges in the order of seconds. Our experiments confirm the previous results since most of the problems were mapped, partitioned, and solved in a matter of seconds or less. Summing up: we can safely conclude that O(ν2) is a sensible characterisation of the upper limit to the amount of time that LuperQ requires to process a table when it is implemented using the D-Wave 2000Q system.

Note 1
In the previous discussion, we intentionally omitted the time that a problem must wait for D-Wave’s computer to become available. The D-Wave 2000Q system can only work on a problem at a time, which means that the problems that are submitted to it must wait in a queue until the previous problems are solved. In our experience, the waiting time ranged from an instant to up to 10–15 s. It was not considered because it is not inherent to our proposal, but something that derives from the fact that the computer is shared by many scientists around the world.

Experimental analysis
In this section, we first describe our experimental setup, next describe how we configured our proposal and the competitors, then present and analyse the results of our empirical comparison, and, finally, report on the results attained using other clustering approaches. The experimental repositories as well as the implementation of our proposal and the competitors are available at http://dx.doi.org.ezproxy.auckland.ac.nz/10.17632/g4tz8m5p6k.2 so that other scientists can repeat our results and work on new proposals. The tool that we used to create the ground truth is available at http://tomatera.tdg-seville.info.

Experimental setup
We implemented our proposal using Python 3.7.6 and many ancillary components. We used BeautifulSoup 4.9.0 to parse the HTML documents; we computed their attributes by rendering them on a virtual canvas using the Selenium 3.141.0 headless browser with Firefox 80.0.1 and Geckodriver 0.27.0. We used SciKit Learn 0.24.3 to leverage the implementation of some common distance functions and to compute effectiveness measures, Pandas 1.0.3 to implement the datasets, and NumPy 1.18.2 to implement some vector and matrix operations. The Ocean 2.2.0 toolkit was used to map QUBO problems onto the quantum computer used. The statistical tests were performed using the SCMAMP 0.2.55 library from the R project.

We confronted it with the proposals by Yoshida et al. [69], Jung and Kwon [36], and Embley et al. [21], which are well-known unsupervised techniques, as well as Nishida et al.’s [51] proposal, which is the most advanced supervised proposal of which we are aware. The unsupervised proposals rely on algorithmic approaches that build on the Expectation Maximisation method, custom heuristics, or a custom search algorithm, respectively; the supervised one relies on a deep neural network.

The experiments were run on a hybrid system that consisted of a standard computer and a quantum computer. The former was a commodity Windows 10 computer that was equipped with an AMD Ryzen 7 2700X processor with eight two-threaded cores and 16 GiB of DDR4 RAM memory; the latter was a D-Wave 2000Q computer that provides 2048 qubits that are connected using a Chimera network.

We assembled two experimental repositories with data tables from the Wikipedia [63] and the Dresden Web Table Corpus (DWTC) [19]. They were annotated by four independent judges who achieved a Krippendorff Alpha coefficient as high as 96.11% on a random subset of 400 tables. The high degree of inter-agreement makes the previous repositories a good ground truth to perform experimentation. The Wikipedia repository provides a total of 1353 tables that are distributed as follows: 1089 horizontal listings, 75 vertical listings, and 189 matrices; they have 90717 value cells and 8734 label cells. The Dresden repository provides a total of 1191 tables that are distributed as follows: 709 horizontal listings, 424 vertical listings, and 58 matrices; they have 49820 value cells and 6294 label cells.

We computed the F1 score and the Accuracy score as the effectiveness measures and the prediction time as the efficiency measure. The F1 score was computed as the one-harmonic mean of precision and recall. (Precision was computed as the ratio of the number of cells that are correctly predicted to be label/value cells to the total number of cells that are predicted as label/value cells; recall was computed as the ratio of the number of cells that are correctly predicted to be label/value cells to the actual number of label/value cells). We computed the F1 score at the class level and globally; in the former case, the score was computed regarding the value cells and the label cells independently and they were then averaged independently, too; in the latter case, it was computed by averaging all of the previous results using the number of cells of each kind as their relative weights. In both cases, the goal was to measure the extent to which the imbalance of value cells (the majority kind of cells) and label cells (the minority kind of cells) may have an impact on the results. We also measured the Accuracy score, which is defined as the ratio of true positives plus true negatives to the total number of cells. Note that it was computed globally since it coincides with the per-class score in the case of binary problems like ours in which there are only two kinds of data. Both measures provide a good overview of how good a proposal is at classifying cells properly: the F1 score provides a view in terms of precision and recall, whereas the Accuracy score provides a global view in terms of how good it is at not producing wrong predictions.

We collected the prediction time as the efficiency measure. It was computed as the average number of CPU seconds required to predict the function of the cells in a table; note that CPU time was used instead of wall time because it is far more stable across different experimentations and discards the IO time required to load the datasets or the time consumed by other concurrent processes. In the case of the supervised proposal, the time required to learn the model was apportioned across all of the tables.

We used the well-known threefold cross validation method to strengthen the validity of our results. The experimental repositories were partitioned into three equal-size random splits, all of which were used to compute the performance measures; the other two were used for learning purposes or ignored depending on the proposal. That is: all of the experimental data were used for learning purposes (when necessary) and for validation.

To confirm that the experimental ranks computed were sound, we performed hypothesis testing [24, 56], namely: we first computed the empirical ranking for each performance measure; we then performed Hommel’s test to compare the best-ranking proposal according to the empirical ranking to the others. The differences in rank are assumed to be statistically significant if the resulting p-value is smaller than the standard significance level (α=0.05) and not significant otherwise.

Configuring the proposals
Our proposal has two hyper-parameters whose values must be set before it is confronted with other proposals, namely: the function used to compute the distance between the cells and the function used to transform the distances into energy levels.

The literature provides many proposals to implement the distance function [17]. We selected the distances that can be applied to multi-dimensional real data for which Scikit-Learn provides an implementation, namely: Euclidean, Standardized Euclidean, Squared Euclidean, Correlation, L1, L2, Cosine, Bray-Curtis, Chebyshev, Manhattan, and Canberra.

Fig. 8
figure 8
Energy functions

Full size image
The literature also provides many proposals to implement the energy function [2, 29]. We experimented with the following ones:

linearτ(d)=sinusoidτ(d)=tangentτ(d)=exponentialτ(d)=1.00−2.00d,cos(τπd),1.00−tanh(τd−−√), and 2.00−cosh(−τd−−√2τ)
In the previous formulation, τ denotes a parameter that allows to compute the specific functions in each family (The linear case is a baseline in which τ is ignored). We experimented with both the regular formulations above and their reflections. The results of the functions were normalised to interval [−1.00,+1.00] using the following formula:

N(f,d)=2.00f(d)−f(1.00)f(0.00)−f(1.00)−1.00,
where f denotes an energy function and d denotes a distance. Figure 8 illustrates the normalised energy functions. The illustration makes it explicit which bunch of curves correspond to the regular or reflected functions and also whether the inner curves correspond to increasing values of the τ parameter or vice versa.

Table 2 Grid search (energy function)
Full size table
Table 3 Grid search (distance function)
Full size table
We randomly selected 100 tables from our repositories and performed a stratified grid search on the hyper-parameter space. We first set the distance function to the standard Euclidean distance and experimented with many different energy functions, cf. Table 2. The best results in terms of the F1 score were attained using the reflected sinusoid with parameter τ=0.60 or τ=0.70; we took the second choice because the time was smaller in that case. Then, we experimented with the distance functions, cf. Table 3. The best results in terms of the F1 score were attained using the correlation distance function.

We configured the competitors using the guidelines provided by the authors [21, 36, 51, 69]. Unfortunately, Yoshida et al.’s [69] guideline was incomplete, so we made some decisions that are in accordance with the common practices in the literature, namely: we initialised the probabilities of their Expectation-Maximisation method with random values, we adjusted them in 10 iterations, we repeated the process 100 times, and we kept the best result only.

Fig. 9
figure 9
Comparison of competitors

Full size image
Empirical comparison
Figure 9 shows our experimental results using boxplots that help understand their distribution; note that the chart regarding the prediction times is in 10-logarithmic scale because this measure ranges in very different intervals depending on the proposal. Table 4 shows the results of our statistical analysis using tables with the following columns: the first column provides the name of a proposal; the second column presents its empirical rank; the third column shows the average of several performance measures plus/minus their corresponding standard deviations; the last column shows the p-values computed by Hommel’s test when comparing the best ranking proposal to the others (the cells in boldface indicate significant differences in rank; the cells with “N/A” correspond to the comparison of the best-ranking proposal to itself).

Regarding the F1 score on the value cells, LuperQ attains the highest average and median and has the smallest and highest inter-quartile range; it is closely followed by the proposals by Nishida et al. [51], Jung and Kwon [36], Yoshida et al. [69], and Embley et al. [21]. All of the proposals attain relatively high F1 scores since the first quartile is above 0.65 and the median is above 0.80 in all cases. This is not surprising at all because value cells constitute the majority of cells in our experimental repositories. It is regarding its ability to identify label cells that LuperQ shines when compared to the other proposals. All of them attain F1 scores in the full range, but LuperQ is the one with the highest average and median and the smallest and highest inter-quartile range; note that 75% of the distribution of the F1 score on the label cells is above 0.75 (the first quartile and the median coincides in this distribution) and the average is a bit above 0.80. It is followed by the proposals by Nishida et al. [51], Embley et al. [21], Jung and Kwon [36], and Yoshida et al. [69]. Regarding the global F1 score LuperQ seems to rank the first one since it attains the highest average and median and has the smallest and highest inter-quartile range; it is followed by the proposals by Nishida et al. [51], Embley et al. [21], Jung and Kwon [36], and Yoshida et al. [69]. Note that the Accuracy score reflects the imbalance much better. According to it, LuperQ ranks the first because it has the highest average and median and the smallest and highest inter-quartile range again; it is closely followed by the proposal by Nishida et al. [51]; then come the proposals by Embley et al. [21] and Jung and Kwon [36]; finally, comes the proposal by Yoshida et al. [69], which has the smallest average and median and the largest inter-quartile range.

Regarding the prediction time, Embley et al.’s [21] and Jung and Kwon’s [36] proposals are clearly the fastest ones since they were able to process all of the tables in our experimental repositories in a few hundredths of a CPU second. Unfortunately, their effectiveness is not very good, which means that their extra efficiency is not so appealing in a real-world context. LuperQ ranks at the third position and it is followed by Yoshida et al.’s [69] proposal and Nishida et al.’s [51] proposal, which is clearly the most inefficient one due to its deep neural approach.

Table 4 Statistical analysis of competitors
Full size table
The previous intuitive conclusions regarding performance were confirmed by our statistical analysis. Note that LuperQ attains the first position in the empirical ranks regarding the effectiveness measures. In the case of the class-level F1 scores, the p-values returned by Hommel’s test confirm that the differences in rank are statistically significant when comparing LuperQ to every other competitor, but Nishida et al.’s [51] proposal. In the case of the global F1 score or the Accuracy score, the p-value computed by Hommel’s test is nearly zero in all cases, which is a strong indication that the differences in rank are statistically significant. Therefore, the conclusion supported by the statistical analysis is that LuperQ ranks the first regarding effectiveness, even though its results are statistically indistinguishable from the proposal by Nishida et al. [51] at the class level. This result is very important since it proves that LuperQ can be as effective as the state-of-the-art proposal, but in a totally unsupervised manner. Regarding the efficiency measure, note that Hommel’s test returns a zero p-value for every comparison, which means that the differences in rank are then statistically significant.

Clearly, all of the proposals have difficulties to identify label cells; generally speaking, the greater the imbalance, the more difficulties to identify them properly. Yoshida et al.’s [69] proposal seems to be the one that has more difficulties to deal the imbalance. It did not attain very good results because it builds on the assumption that the label cells usually have the same common contents across different tables, which is a difficult-to-meet assumption when working at Web scale; furthermore, its frequency-based approach also has problems when processing cells with numeric contents, since many of them occur only once in the datasets. Neither was Jung and Kwon’s [36] proposal very good at dealing with the imbalance. It seems to have trouble with tables in which the user highlights some cells that provide data using a style that is similar to the style of the label cells. That was somewhat common in our experimental repositories because we have found out that many authors use the th HTML tag to highlight some data cells. Embley et al.’s [21] proposal is very fast, but has many problems when dealing with listings; we found that tables with no repeated contents are interpreted as matrices with one row and one column of headers, which is not usually true. The proposal was devised to work in the context of spreadsheets, where we assume that matrices are far more common than in the Web. Nishida et al.’s [51] proposal achieves very good results because it is supervised and learns patterns from a training set in which a person must provide as many tables as possible with annotations regarding the function of their cells; unfortunately, it tends to get in trouble when it is confronted with a table that deviates largely from the tables in the training set. It can deal with label cells much better than the previous proposals, but the imbalance of the datasets still has a negative impact on its results. We found out that the cases in which LuperQ cannot identify them properly correspond to tables in which the difference amongst the label and the data cells is very vague, many of which are matrices from which it is not easy to extract data unless one can really understand their semantics. In some cases, the problem was that the ratio of label to data cells was heavily skewed in large tables with less than 1% label cells. We also found out that LuperQ sometimes had trouble with horizontal listings in which the first columns are somewhat highlighted; in such cases, it tends to mistake horizontal listings for matrices. We profiled LuperQ and we found that the cases in which it requires more than 1.00 second correspond to cases in which the algorithm to map the QUBO problems onto the quantum computer had difficulties to find the optimum mapping [6].

Fig. 10
figure 10
Comparison using other clusterers

Full size image
Replacing the clusterer
We set up several variants of LuperQ in which we replaced the component that implements our quantum clustering approach by the classical k-means algorithm with the mini-batch extension to deal with large datasets, as well as the quantum approaches by Neukart et al. [50], Bauckhage et al. [4], Kumar et al. [44], and Wereszczyński et al. [62]. Figure 10 and Table 5 summarise our results.

Regarding the F1 score on the value cells, LuperQ attains the highest average value and median and it has the smallest inter-quartile range, which indicates that it ranks the first. It is followed by the variant that uses k-means, Bauckhage et al.’s [4] clusterer, and Kumar et al.’s [44] clusterer; then comes the variant that uses Wereszczyński et al.s [62] clusterer; the variant that uses Neukart et al.’s [50] seems to clearly deviate from the others since it attains the worst average and median and has the lowest inter-quartile range. The results regarding the F1 score on the label cells makes the variants further apart: the results regarding LuperQ are very similar in both cases because it has proven to deal well with the imbalance of value and label cells. The other variants are clearly worse at identifying the label cells since they are the minority in our experimental repositories. The global F1 score makes the differences between LuperQ and its variants very clear in the case of the variants that use quantum approaches to cluster the cells since LuperQ attains the highest average and median score and has the smallest and highest inter-quartile range; it is followed by the variant that uses k-means. The Accuracy score also makes a clear difference between LuperQ and the variants that use Neukart et al.’s [50] or Wereszczyński et al.’s [62] clusterers; it is followed by the variants that use k-means, Kumar et al.’s [44] clusterer, and Bauckhage et al.’s [4] clusterers.

Table 5 Statistical analysis of other clusterers
Full size table
Regarding the prediction time, LuperQ and its variants seem to behave very similarly, except for the variant that uses Neukart et al.’s [50] clusterer. That result was not surprising because all of the quantum proposals take approximately the same time to prepare and solve the underlying QUBO problems; the reason why the variant that uses Neukart et al.’s [50] clusterer deviates significantly is that it requires to prepare and solve a different QUBO problem for each datum in the input dataset, that is: each cell is clustered independently from the others by solving its own QUBO problem, which results in extra effort as the number of cells in a table increases. Unfortunately, the extra effort was not worth neither in terms of effectiveness nor efficiency.

The previous conclusions were clearly confirmed by our statistical analysis. Note that Hommel’s test returns a zero p-value regarding the comparisons that involve effectiveness measures, which means that the experimental data support the hypothesis that LuperQ ranks at the first position and the differences are statistically significant. It also returns zero p-values for the comparisons regarding the prediction time, except for the case of the comparison with the variant that uses k-means. Summing up: the variant that used LuperQ’s original approach to clustering seems to attain the best effectiveness results without degrading efficiency when compared to a variant that builds solely on classical computing algorithms. It is expected that it can clearly beat it in future, when quantum computers provide enough qubits to deal with larger problems than they can do today.

Conclusions
This article presents LuperQ, which is a new proposal to extract data from HTML tables. It differentiates from the existing ones in that it relies on an adiabatic quantum clustering approach to identify the function of the cells. This approach has proven to be very effective and efficient in practice and supports the idea that adiabatic quantum computing is a technology that helps solve some classical NP problems in polynomial time. Whether a particular problem can attain such a tremendous speedup or not depends completely on the problem and requires experimentation. Our empirical analysis makes it clear that extracting data from HTML tables is one of the problems that may benefit from this speedup. Future work includes researching how to use a quantum computer to select the most informative features automatically. In our experimentation, we used all of the cells in the input tables and all of the features, but we realised that some of them do not actually contribute to the results in some cases; detecting them automatically might help make our proposal more effective and efficient. Generalising our clustering approach to deal with arbitrary datasets and distributing large problems across several quantum computers will also be paid much attention.