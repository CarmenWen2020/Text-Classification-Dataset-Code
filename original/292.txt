Protecting web applications is becoming challenging every passing day, primarily because of attack sophistication, omnipresence of web applications and over-reliance on traditional Web Application Firewalls (WAFs). Advanced Persistent Threats (APTs) make overwhelming use of web attacks during infiltration and expansion phase. Noteworthy research has been carried out to detect web attacks using deep learning because traditional approaches fail against complicated attacks having crafted payloads, scripts and cookie manipulations. This paper proposes a framework based on an enhanced hybrid approach where Deep Learning model is nested with a Cookie Analysis Engine for web attacks detection, mitigation and attacker profiling in real time. We first generated a huge dataset over a period of time and trained our Convolution Neural Network (CNN) based deep learning model using Hypertext Transfer Protocol (HTTP) request parameters like Type, Content length, Data and Requested URL etc. We also developed a Cookie Analysis Engine that checks all incoming cookie(s) for integrity, mutations and failed sanitization checks and informs the user about probable privacy infringement by third party cookies. The framework analyzes the cascading output from the classifier and cookie analysis engine and takes the final decision. We performed rigorous testing of the proposed framework wherein it was first validated on our own custom dataset giving an accuracy of 99.94%. It was also validated on a publicly available benchmark dataset and gave an accuracy of 98.74%. When deployed in a controlled real time environment, the attacker profiling feature enabled the framework to save useful processing time as the deep learning classifier does not get triggered for every incoming request. This makes it easy to deploy in any environment to protect web applications in real time.

Previous
Next 
Keywords
Web Security

Web Application Security

Deep learning

Attacker profiling

Deception

Cookies

HTTP

1. Introduction
The ubiquity of internet has resulted in a momentous growth in its usage all across the globe resulting in roughly half of humanity having an internet connection (Max Roser and Ortiz-Ospina, 2020). This perpetual digital transformation in human lives has intrigued end users, corporates, small businesses, large organizations and in fact everyone to share their data and information via websites. The number of websites in the world have surpassed 1.7 Billion and are still growing (Liedke, 2020). These websites are connected to backend databases in real time where the later, although not visible or directly accessible to the user are updated, parsed and accessed by the web application(s) as a result of some user activity (Kandpal et al., 2021). Besides catalyzing this massive e-transformation, the overwhelming use of webpages and reliance on websites has also stirred the attackers to compromise these sites in order to get hold of important data that might be of some financial or reputational gain. According to 2019 Verizon Data Breach Investigations Report, web attacks are the most prevalent sort of cyber-attacks, happening once every 39 s (Cukier, 2007, Paine, 2019). These attacks often result in massive data, financial and reputational loss and are further aggravated due to the absence of cybercrime regulations in many parts of the world.

Security experts have developed ways and means to tackle the grave threat of chronic web attacks. On the other hand, attackers have equipped themselves with more sophisticated approaches to circumvent these defensive mechanisms (Hamam and Derhab, 2021). OWASP (Open Web Application Security Project) has listed the top 10 web attacks (The Open Web Application Security Project, 2017) that pose a great deal of challenge to security experts (Shoel et al., 2018). These attacks have accentuated the need to not only go for secure coding in web applications (Meng et al., 2018) but to also rely on security mechanisms that can help thwart such attacks in real time.

One obvious choice is to use a WAF (Web Application Firewall (Clincy and Shahriar, 2018)), as shown in Fig. 1, that acts as a layer of defense standing between the attacker and the website. WAFs analyze user requests in order to filter the illicit traffic out of the routine one. This analysis of the HTTP requests (Wittern et al., 2017) is based on a set of predefined rules designed to discriminate the malicious traffic.

One of the most effective means to assess the security of web applications is to make use of WAVs (Web Application Vulnerability Scanners) (Sarkar, 2021). These tools analyze web applications against common web related vulnerabilities and launch all sorts of attack packets using their built-in malicious payloads to compromise the target website‚Äôs confidentiality, integrity or availability (Bitzer et al., 2021). WAFs perform better against these scanners because of known signatures and payloads, but often fail against experienced hackers who prefer writing their own pieces of code. Even the most popular WAFs such as ModSecurity (Trustwave, 2020) fails in detecting such HTTP requests. These issues with WAFs are either because of misconfigurations, their rule-based nature, lack of semantic analysis and behavioral detection, failure against permutation of known attacks and no protection against zero-day attacks . Moreover, tired of false positives, many security administrators keep their WAFs on the Alert Mode, in which attacks are never blocked in real time. According to a survey conducted by Ponemon institute (Vicente, 2019), only 9 percent of the WAF users say that their solutions have never been compromised, whereas, 65 percent complain that attackers can bypass their solutions without much of an effort.




Fig. 1. Web Application Firewall (Nair, 2019).

These immense problems associated with traditional rule-based WAFs accentuate the need for incorporating machine learning for automated real-time attack detection. Many research works use traditional machine learning classifiers like Naive Bayesian, Decision Tree, K-NN and SVM (Betarte et al., 2018, Dong and Zhang, 2017, Althubiti et al., 2017, Zhou and Wang, 2019), but these approaches fail to provide better accuracy and precision against unknown data. Deep Learning based approaches have gained much importance as they outperform traditional machine learning classifiers in terms of accuracy and precision but there are certain issues associated with the use of deep learning based methods while dealing with web attacks. Firstly, their strength depends on the data they are trained on and are unable to give same performance on anything that appears different and would require re-training. Many research works as discussed in Section 2 have not cross-validated their proposed techniques either on publicly available benchmark datasets or by real-time deployment. Another major issue with deep learning methods is the huge processing power they require. Almost all related research works ignore this issue and route every incoming HTTP request to the deep learning classifier. This phenomenon incurs a lot of latency and performance decadence in real-time systems.

Therefore, development of any web attack detection system must consider achieving certain security goals. Firstly, the system should have very high accuracy and precision values. This would only happen if it is based on deep learning and not traditional classifiers or rule-based techniques. Performance and efficiency are other key security goals because they would make any detection system suitable for real-time deployment. Since applying deep learning would incur a lot of performance decadence and latency, therefore, smarter techniques need to be adopted which help in optimizing attack detection. Validation is another vital security goal which would guarantee any attack detection system‚Äôs performance on unknown data.

The proposed web attack detection framework achieves all these security goals by addressing all performance and optimization issues related to Deep Learning based classifiers. The main research contributions of this research work are listed as follows:

1.
A novel framework has been proposed which is based on a hybrid approach which nests Deep Learning model with a Cookie Analysis Engine for web attacks detection, mitigation and attacker profiling in real time.

2.
The Convolutional Neural Network based deep learning classifier is trained using HTTP request parameters like Content Type, Length, Requested URL and Data etc. on a large dataset specifically generated for this purpose.

3.
A Cookie Analysis Engine that has been designed to check all incoming cookie(s) for integrity failures, mutations and failed sanitization checks and informs the user about probable privacy infringement by third party cookies.

4.
An efficient framework which saves useful processing time when deployed in real time as the attacker profiling feature limits the execution of deep learning classifier for every incoming HTTP request without degrading attack detection capability. The proposed framework gives an accuracy of 99.94% on our testing dataset and 98.74% on a publicly available benchmark dataset.

This paper is divided into five sections. Introduction section is followed by Section 2 which consists of relevant research works related to detection and mitigation of web attacks. These techniques have been compared and evaluated on the basis of their strengths and weaknesses which necessitates the idea to perform further research in this area. Section 3 discusses dataset generation comprising of both benign and malicious HTTP requests, along with the capturing of POST HTTP requests. The proposed framework comprising of the Cookie Analysis Engine and the Deep Learning classifier has been discussed in Section 3.4.1 and Section 3.4.2 respectively. Section 4 provides the complete testing, performance evaluation, results and comparative analysis of the proposed framework with related research works. The research work is concluded in Section 5 where future work related to development of a web deception system has also been discussed.

2. Related work
Several research works have discussed the use of deep learning based solutions for detecting and mitigating web attacks and scanning probes.

Luo et al. (2020) proposed an ensemble classification model based on three deep learning models where the final decision is based on results extracted out of the three classifiers. The research gives high accuracy and low false positive values. Niu and Li (2020) proposed a web attacks detection framework by combining Convolutional Neural Network (CNN) with Gated Recurrent Unit (GRU) approach. This model gives high accuracy. Both research works made use of the CSIC data (Gim√©nez et al., 2012) for training and testing their models. Kim and Cho (2018) presented a C-LSTM neural network based approach to model incoming web traffic. They demonstrated that C-LSTM analyzes the web traffic deeply as it combines both CNN and LSTM to extract temporal and spatial features of web data. The proposed work achieves an accuracy of 98.6% and a recall value of 89.7% on Yahoo‚Äôs Webscope S5 dataset. Liang et al. (2017) make use of Recurrent Neural Network (RNN) to train their model only on normal HTTP GET requests from the CSIC dataset (Gim√©nez et al., 2012) and malicious HTTP GET requests from ModSecurity (Trustwave, 2020) logs. The proposed methodology yields an accuracy of 98.42% in comparison to ModSecurity. These proposed approaches have nested different deep learning models in order to achieve higher accuracy but at the cost of processing power as they trigger all models for every incoming HTTP request thereby incurring latency and computational issues when protecting web applications in real time.

Tekerek (2021) also presented a CNN based approach for detection of web attacks with high accuracy on CSIC dataset (Gim√©nez et al., 2012). Pan et al. (2019) came up with an end-to-end deep learning approach for detecting SQLi, XSS and deserialization attacks and evaluated their model on a synthetic dataset as well as production applications with known vulnerabilities. Mahfoudhi (2021) and Jemal et al. (2021) presented a CNN based code level and ASCII embedding technique respectively. The later detects web server attacks with an accuracy of 98.24% on CSIC dataset (Gim√©nez et al., 2012). Mokbal et al. (2019) presented an artificial neural network based on multi-layer perceptron approach (MLP) for detecting XSS attacks by dynamically extracting features from web traffic. It achieved an accuracy of 98.97%. Liu et al. (2019) presented a Locate-Then-Detect (LTD) system for detecting malicious payloads exploiting XSS and SQLi attacks by using attention based deep neural networks. The proposed work uses limited features for training purpose and yields good performance on both CSIC (Gim√©nez et al., 2012) and real-world data.

Fang et al. (2018) presented an LSTM based deep learning based approach to counter XSS attacks. It was tested on real-time dataset with an accuracy of 99.98%. Abaimov and Bianchi (2019) detected code injection attacks like SQLi and XSS using convolutional deep neural network resulting in high levels of accuracy on real-world datasets. The proposed research work only uses operators, escape symbols and expressions as features for training the CNN which is not sufficient for detecting advanced payloads. Although few of these proposed research works have generated their custom datasets for classifier training but most of them have made use of the same publicly available dataset for both training and testing. Moreover, these proposed approaches only address limited number of attacks like XXS and SQLi and do not detect all of the OWASP top ten attacks.

Zhang et al. (2017) proposed a CNN based technique for detecting SQLi, buffer overflow, CRLF injection, XSS and parameter tampering attacks. The system was not trained or tested on real-time data but rather relied on CSIC dataset (Gim√©nez et al., 2012). The proposed approach studies the HTTP requests to form word sequences for the data pre-processing stage. It only detects web attacks hidden in URLs and does not analyze the complete HTTP request message. The framework achieves the accuracy of 96.49% after 10 epochs of training which is not efficient enough for deployment and testing in real time. Tian et al. (2020) proposed a deep-learning based mechanism for detection of web attacks from the analysis of URLs. It also makes use of word2vec (Wen et al., 2016) in order to look for words in the URL. Since the proposed approach only caters for detecting attacks from URLs, it does not detect web attacks based on HTTP POST requests. The proposed work has a detection accuracy of 99.41%.

Some research works have also added behavioral detection in WAFs to enhance their functionality. Moradi Vartouni et al. (2019) presented an anomaly based WAF using deep neural networks. The system gives an accuracy of 89.24% when tested on CSIC (Gim√©nez et al., 2012) dataset. Khan et al. (2017) presented a machine learning driven approach to detect malicious JavaScripts based on an interceptor designed to protect the client side as it performs static analysis on server‚Äôs response to client‚Äôs HTTP GET request for investigating potential attacks in the source code to be executed by the client-side browser. A major drawback of the proposed approach is that it would fail to detect zero-day attacks as behavioral analysis of malicious code has not been taken care off. It achieves an accuracy of 97.14% by using the KNN approach. Birkinshaw et al. (2019) presented an SDN (Software Defined Networking) based approach that counters port scanning and DDoS attacks using Reinforcement Learning algorithm (Recht, 2019). The proposed work does not address a wider range of attacks, specifically the top web attacks.

Dong and Zhang (2017) presented an adaptive learning based approach, named SVM hybrid to tackle malicious queries in web attacks. They use an ensemble classifier based on meta-learning for underlying detection of malicious queries and outperform other malicious query detection approaches in performance. The proposed technique achieves an F score of 94.79%.

Few research works have particularly focused on detecting WAVs and web crawlers instead of attack detection. Stevanovic et al. (2011) came up with a couple of features in order to detect web crawlers using data mining classification algorithms. The research makes use of Sequential Request Ratio and Standard Deviation of page requests in order to differentiate web crawlers from normal users accessing a university website. The proposed scheme appears to stand weak against modern web scanners which are good at mimicking the behavior of normal human users. Moreover, the research work only targets web crawlers and does not address scans probing against any of the OWASP top 10 vulnerabilities. Dong and Zhang (2017) presented an adaptive learning based approach, named SVM hybrid to tackle malicious queries in web attacks. They use an ensemble classifier based on meta-learning for underlying detection of malicious queries and outperform other malicious query detection approaches in performance. The proposed technique achieves an F score of 94.79%.

Yang et al. (2019) proposed a hierarchical correlation-based web application scan detection technique to figure out coordinated attacks from branded malicious sources. The model makes use of semantic correlation analysis and temporal‚Äìspatial correlation analysis to distinguish the coordinated web scanners. Later, Bro/Zeek network monitor is used to collect web traffic logs from some web hosting service provider and the proposed scheme is evaluated on those logs. The system achieves 97% accuracy. Zhang et al. (2019) presented an approach that assimilates attack indications from web requests and their associated responses in order to detect XSS vulnerabilities with an accuracy of 93.98%. The proposed model extracts attack features from existing attack datasets using the word2vec (Wen et al., 2016) algorithm and compares them with normal traffic in order to train the GMM (Gaussian Mixture Model).

Table 1 contains a summary of related work that has been carried out as discussed in this section.


Table 1. Existing research works on using deep learning to detect web attacks.

Research Work.	Year	Technique	POST HTTP
Analysis	Attacks Mitigated	Dataset used
Tekerek (2021)	2021	CNN	NA	Web Attacks	CSIC dataset (Gim√©nez et al., 2012)
Mahfoudhi (2021)	2021	Code Embedding using CNN	NA	Basic Web Attacks	CSIC dataset (Gim√©nez et al., 2012)
Luo et al. (2020)	2020	Ensemble approach (Deep Learning)	Yes	Web Attacks	CSIC dataset (Gim√©nez et al., 2012)
Niu and Li (2020)	2020	CNN + GRU	Yes	Web Attacks	CSIC dataset (Gim√©nez et al., 2012)
Mokbal et al. (2019)	2019	ANN based MLP	Yes	XSS	Custom Dataset
Yang et al. (2019)	2019	Hierarchical correlation, Temporal‚Äìspatial correlation	No	Web scans	Real-world dataset
Moradi Vartouni et al. (2019)	2019	Deep Neural Network	No	OWASP Top-10	CSIC dataset (Gim√©nez et al., 2012), PKDD2007
Abaimov and Bianchi (2019)	2019	CNN	No	SQLi, XSS	Real-time dataset
Liu et al. (2019)	2019	Attention based DNN	No	SQLi, XSS	CSIC dataset (Gim√©nez et al., 2012), Real-time dataset
Zhou and Wang (2019)	2019	Ensemble learning approach	Yes	XSS attacks	Real-world dataset
Tian et al. (2020)	2019	Deep learning	No	OWASP Top-10	CSIC dataset (Gim√©nez et al., 2012), FWAF, HttpParams Dataset
Kim and Cho (2018)	2018	CNN + LSTM	NA	All	CSIC dataset (Gim√©nez et al., 2012)
Fang et al. (2018)	2018	word2vec, LSTM	Yes	XSS	Real-time dataset
Liang et al. (2017)	2017	RNN	No	OWASP Top-10	CSIC dataset (Gim√©nez et al., 2012)
Zhang et al. (2017)	2017	CNN	No	SQLi, Buffer overflow, CRLFi, XSS	CSIC dataset (Gim√©nez et al., 2012)
3. Proposed framework for web attacks detection mitigation and attacker profiling
The proposed framework detects and mitigates web attacks in real time along with attacker profiling. The incoming HTTP request is first examined by the profiler that is maintained and updated over time. If the user profile associated with the incoming request is benign and a static page has been requested, the page will be opened without triggering either the deep learning classifier or the cookie analysis engine. Moreover, requests by malicious users will be blocked straight away while all other requests will be forwarded for further analysis. This feature of the proposed framework improves efficiency and saves processing time. The request forwarded by the profiler is first passed through a bot and scanning detection engine which analyzes it with respect to time, requested URL and socket information of the requester. If a bot or scanner is detected, the request will be turned down and the user will be profiled malicious. Otherwise, the request will be split in a way that the cookie(s) are analyzed by the Cookie Analysis Engine while remaining HTTP parameters go to the deep learning classifier for further analysis. The final decision is based on their outcome, and the profiler is updated accordingly as shown in Fig. 2.

3.1. Creation of dataset
For generating our dataset we set up the environment by deploying a vulnerable PHP/MySQL web application that contained pages susceptible to a large variety of web attacks including the OWASP top ten. We first targeted our website with a large number of custom attack scripts and payloads for SQLi, XSS, CSRF, XXE, directory traversal and broken authentication etc. along with their permutations. Apart from that, we used open-source WAVs like OWASP-ZAP (OWASP, 2020), Nikto (Chris Sullo, 2020) and Arachni (Arachni, 2020) and a commercially licensed scanner Burpsuite (Portswigger Web Security, 2020) for generating attack traffic. It was made sure that no benign traffic was targeted towards the website during this period. This traffic was later logged and parsed as shown in Fig. 3 for later use.

For generating benign dataset, the target website having all features and contents of a normal real world website was accessed over a period of one month from various machines at different intervals and the logs were captured as shown in Fig. 3. As expected, these logs had no abnormality in any of the header fields. Unlike the attack packets, benign requests only had GET and POST request types, web browser as the user-agent and accessed limited URLs in given time, unlike many of the attack packets.


  
Fig. 3. Process of malicious and benign logs generation.

Our dataset contains a large number of attacks as compared to publicly available benchmark datasets. These were generated by using crafted payloads, their mutations and different vulnerability scanners. Our dataset also contains a larger number of benign HTTP requests.

In our case, the number of benign HTTP samples exceeded their malicious counterparts. We therefore used SMOTE (Chawla et al., 2002) over-sampling technique for balancing both class labels. Malicious samples were augmented by selecting those which are relatively closer in the feature space. A line was then drawn between these selected samples in the feature space and additional samples were synthetically created along that line. For instance, let M = (m1, m2, m3, ‚Ä¶.., mn) be the set of all malicious HTTP requests generated, whereas n are the total number of samples belonging to the malicious class.


3.2. Analysis of attack logs
Malicious logs reveal key information about various attack methodologies, crafted scripts, payloads and WAVs. For instance, attackers launch spidering probes on the website to enumerate as much as they can for exploitation purpose. The attack logs happen to be so random that a certain opinion can never be formed about their malicious intent by simply analyzing the request parameters in detail. Fig. 4 shows detail of HTTP Request Type parameter in the attack dataset. Moreover, attackers often tamper verbs in the HTTP Request field in order to analyze the generated response. Scanners also conceal their name so that the target application believes the request to be coming from a normal browser. Attackers can also leave this field blank in order to analyze the generated response. Details of how the Request Type and User Agent fields are forged in our attack dataset are shown in Fig. 5.

3.3. Bot and scanner detection
Novice attackers prefer using bots and popular scanners to attack a website as they often bombard a particular URL with multiple requests in a very short time span, whereas normal users take some time on web pages and respond with appropriate headers and parameters to move with the website flow. The proposed bot and scanner detection module deals with such HTTP requests by queuing up a few packets and analyze them on the basis of time, requested URL and sender‚Äôs identity and socket information as explained in algorithm 2. If found malicious, the request is dropped and feedback is sent to the profiler who marks the requester as a malicious user. Otherwise, the request is forwarded to the cookie analysis engine and deep learning classifier.


3.4. Classifier training
From an attack point of view, all fields of an incoming HTTP request are inter-related and, neither in isolation nor in any combination, are they helpful in determining the type and intent of the request. For instance, the HTTP request shown in Fig. 7 appears benign as it is a simple GET request with no injection in language or encoding fields, no cookie manipulation, and a simple requested URL. Any rule based security application will label this HTTP request as benign but in fact it is not because it is a type of directory bruteforcing attack where the attacker is checking for /operador directory, as highlighted. ModSecurity (Trustwave, 2020) lets this HTTP request pass safely.

Similarly, Fig. 8 looks malicious because any rule-based security solution will block IP addresses in user submissions, as highlighted but in this special case, it is required by the application itself and is not malicious in any way. This request is also blocked by ModSecurity (Trustwave, 2020). Experienced attackers can misuse any parameter of the HTTP request like Type, User Agent, Data or even Language as explained in Table 2. Cookie mutations and injections are also important but are specific to the target website. It is hard to determine as to which HTTP parameter alone helps in determining about the nature of request. Therefore, in this research, we split the incoming HTTP request so that the cookie(s) go to the Cookie Analysis Engine while the remaining parameters of HTTP request go the deep learning classifier.



Fig. 7. A Benign looking Malicious HTTP request.



Fig. 8. A Malicious looking Benign HTTP request.


Table 2. HTTP features and their attack scenarios.

Features	Attack scenarios
Request Type	Attackers often change this field deliberately when requesting a particular resource by tampering HTTP verbs. For instance, OPTIONS request type helps in analyzing the response headers that will show all the allowed HTTP methods.
User Agent	This field is usually different in benign and attack traffic, as the later might reveal the scanner‚Äôs name, if not forged or obfuscated. If the attacker has somehow forged this field as shown in Fig. 7, then this feature alone will fail to determine the attack as other features like detecting payload and timing analysis will serve the purpose.
Content Length	Content length in attack probes do not provide true representation of the data being sent. Few attack packets in our dataset had 0 as the content length value despite the fact that data was being sent. Similarly, few attack packets had large Content Length value with no data being sent in GET requests. This important feature alone provides limited help because the user application might not send content length in the first place.
URL	This parameter must also be looked upon in relation to sender‚Äôs profile and authorization. Fig. 7 is a brilliant example of the fact that an HTTP request which looks perfectly benign is in fact malicious because of the directory brute-forcing attack, where the attacker requests a particular URL.
3.4.1. Cookie(s) analysis engine
The Cookie Analysis Engine as shown in Fig. 6 takes all cookies of an HTTP request as input. CAE would first detect presence of third party cookies and notify the user about probable privacy infringement as these tracking cookies are mostly used by advertisers (Gomer et al., 2013). It would then launch integrity checks on local cookie(s) to see if they have been modified by the sender. If the integrity checks are successful the cookie(s) are marked as benign. Cookies where modifications are not malicious will also fail integrity checks. Therefore, such cookies along with all third party cookies (if present) are passed through sanitization checks aimed at finding unwanted characters, symbols, trackers, script parameters and values etc. in any field. Failed sanitization checks for any cookie as shown in Fig. 9 result in marking the incoming cookie(s) as malicious.

This separate analysis of cookie(s) in the CAE also help us in settling somewhere between malicious and benign, we call suspicious. The details of how the Cookie Analysis Engine decides about the fate of incoming cookie(s) is mentioned in Table 3.


Table 3. Cookie analysis engine working.

Third Party Cookie(s) Found	x	x	x	‚úì	‚úì	‚úì	‚úì	‚úì	‚úì
Sanitization Checks 	‚Äì	‚Äì	‚Äì	‚úì	‚úì	‚úì	x	x	x
Local Cookies(s) Integrity Check	x	x	‚úì	‚úì	x	x	x	x	‚úì
Sanitization Checks 	‚úì	x	‚Äì	‚Äì	‚úì	x	x	‚úì	‚Äì
CAE Decision	S	M	B	S	S	M	M	M	M
3.4.2. Deep learning based classifier
Convolutional Neural Networks (CNNs) considerably enhance the efficiency of neural networks as the convolution operation performs matrix multiplication on small regions of data. In this research, we are dealing with HTTP request logs which are represented by a single sequence of multiple features. Therefore, two-dimensional CNN is not required. This would also result in low computational complexity as the forward and backward propagation only requires simple array operations. We have already consumed cookie(s) in the CAE, so the remaining HTTP request parameters are used for training the 1D-CNN. Firstly, the training dataset Dtrg is fed into the system which is passed through layers of embedding, dropout, convolution, pooling and densing before ending up at the fully connected layer as shown in Fig. 11.

Our HTTP requests vector V can be represented as a set of features (f1, f2, ‚Ä¶.., fn), where n holds the number of possible values in the window, fi represents the feature‚Äôs value in normalized form, i represents the index of the feature value and fm represents the index of feature map for each window of the input vector. We then move on to defining initializing parameters for our CNN as listed below:

1.
W represents the maximum number of distinct words in the training dataset Dtrg

2.
S represents the size of the embedding vectors

3.
L is fixed and represents the number of HTTP Request parameters being used as our feature set

4.
Kernel size w defines the size of the sliding window to be convolved over layer input

5.
Filters parameter Z, that shows the total number of sliding windows of size w

6.
R representing the number of strides

Since the output of Eq. (1) contains maximum of the total parameters on which convolution is going to take place, therefore, we want to avoid overfitting at this stage by adding a dropout layer to the model as shown in Eq. (2) (2)
œ∂

We now add the convolution layer to our CNN model with a kernel size w and filters Z along with the activation function and strides R as shown in Eq. (3). (3)
ùï†ùïüùïß

At the next stage, Eq. (4) shows output Oim1 of the first convolution layer. Here, bias for the th feature map is represented as Bfm0 and K represents the kernel weight. (4)

Pooling layer is then added to our CNN model to reduce dimensionality, avoid over-fitting and only highlight the prominent features. We then add the fully connected layer to the model so that the layer has n neurons as shown in Eq. (5). We again add a dropout and activation layer as mentioned in Eqs. (2) and (3) respectively. (5)

We project our model onto an output layer that has two neurons for two classes by using the Softmax activation function, as shown in Fig. 11. This very combination of the fully connected layer and Softmax function are used for finding anomalies in the incoming HTTP request packets as it would classify the unknown test data into benign and malicious classes.

Finally, the model, with all its parameters is compiled and evaluated on both Dtest and CSIC dataset (Gim√©nez et al., 2012) to compute accuracy and binary cross-entropy loss for both the epochs.

3.5. Analyzing nested output of CAE and deep learning classifier
The outcomes of both Cookie Analysis engine and the Deep Learning based classifier are analyzed in order to decide about the nature of incoming HTTP request. The classifier‚Äôs decision always supersedes the decision of CAE. One odd exception occurs when the, classifier categorizes a request as benign but the CAE considers it suspicious. Such a request is marked as suspicious as mentioned in Table 4 and the user is profiled accordingly. If a suspicious user again performs cookie mutation in the very next request, he is profiled malicious and the request is blocked without triggering the CAE or the classifier. This profiling of a user as suspicious is therefore helpful in both attacker profiling and reduction of processing time.


Table 4. Merging CAE with deep learning classifier.

CAE‚Äôs Decision	‚Äì	‚Äì	B	B	M	M	S	S
DL Classifier‚Äôs Decision	B	M	B	M	B	M	B	M
Final Decision	B	M	B	M	M	M	S	M
4. Framework analysis, results and discussion
There are certain metrics which help in theoretically analyzing the proposed framework before its implementation evaluation. Our proposed attack detection system would receive both malicious and benign HTTP requests which it needs to correctly determine with high accuracy. Let  be a set of requests and  and  are malicious and benign requests respectively. Let 
 and 
 be the correctly classified malicious and benign requests. The system will only yield high accuracy if Eqs. (6) and (7) are satisfied. (6)
 (7)

Moreover, optimizing the deep learning classifier by enhancing its performance was one of our key objectives. Suppose, a user  generates a total of  requests that includes both  and  queries at different time intervals ti. Suppose  denotes the time deep learning classifier takes to correctly classify one incoming request. Now, with simple application of the classifier, for all  requests by the user, total execution time of the classifier 
 is mentioned in Eq. (8). (8)

Since we have optimized the classifier by introducing attacker profiling through the cookie analysis engine, therefore the classifier is not triggered once the attacker has been successfully profiled. Suppose the user  was sending benign requests but suddenly sent a malicious payload in his request pi, such that . This time, due to attacker profiling the optimized system results in lesser number of classifier executions as mentioned in Eq. (9). Thorough testing and evaluation under normal circumstances will help us give percentage decrease in the number of classifier executions which would further validate the proposed optimization. (9)

Based on the theoretical analysis, the proposed web attack detection, mitigation and attacker profiling framework was carefully analyzed and tested using a two-fold approach where we first tested our classifier on our own testing dataset and then on a publicly available benchmark dataset. We later tested our framework by deploying it in a live environment to analyze its performance and the benefits of attacker profiling. Moreover, the custom dataset we generated for both training and testing was also analyzed in order to validate our feature engineering.

Different HTTP parameters were found to actually correlate with others which reveals their interdependence. Correlation details of features User agent and Content length are shown in Fig. 12 while correlation details of features Request Type and Data are shown in Fig. 13.

4.1. Performance on our own dataset
The proposed deep learning classifier was first tested against Dtest. It successfully classifies almost all malicious and benign requests (overall accuracy: 99.94%) with very few exceptions where a large number of permutations are performed on different request fields. A comparison of loss and accuracy on Dtest is shown in Fig. 14 while other performance parameters are shown in Fig. 15.

4.2. Cross-dataset testing and validation
The proposed framework was further tested against a completely unknown publicly available benchmark CSIC dataset (Gim√©nez et al., 2012). On this totally unknown data, a high accuracy would eliminate all chances of overfitting in the system. The proposed deep learning based classifier gives very high values of True Positive and True Negative as shown in Fig. 16 while accuracy, recall, precision and F-score on this public benchmark dataset are shown in Fig. 17.

4.3. Discussion on cookies
While analyzing cookies in both datasets, it was observed that most of the malicious requests had normal benign cookie fields. Similarly, a few requests which were categorized as benign by the classifier had cookies which failed sanitization checks as shown in Fig. 20.

4.4. Deployment and testing of proposed framework in real time
The proposed framework has been designed to work efficiently in real time with high accuracy and minimized latency. As the CAE helps in maintaining state in case of a stateless HTTP protocol, testing the proposed framework against offline datasets would not offer complete benefits of the system. Nevertheless, evaluation of individual components (Deep Learning classifier and CAE) of the framework on these datasets in a silo has already hinted about very good performance of the proposed framework when deployed in real time.

4.4.1. Environment setup
We deployed the proposed framework in front of a sample website in a controlled environment. There are five groups of users G=(G1, G2, G3, G4, G5) who accessed the website under different scenarios by sending fifty requests each in a phase wise manner.

Initially G1, G2 and G3 accessed the website normally while G4 and G5 used a scanner which were blocked by the bot and scanner detection engine. In the next phase, G4 and G5 accessed the site normally, G1 and G2 requested static pages, while G3 contaminated their HTTP request by inserting a script in data field. In the next phase, G2, G4 and G5 sent benign requests where the later requested a dynamic page, G1 modified the cookie which still passed the sanitization checks and G3 added third party cookies to their HTTP requests. In the last phase, G1 again performed cookie mutation, cookies of G5 failed sanitization checks while benign requests are sent by G2, G4 and G3, with the later requesting a dynamic page. These request packets were generated to test and validate all modules of the proposed framework in a live environment. A Summary of User profiles at the end of each case is depicted in Table 5.


Table 5. Summarization of user profiling.

Phase	Benign Groups	Malicious Groups	Suspicious Groups
1	
, 
, 
, 
0
2	
, 
, 
, 
0
3	
, 
, 
4	
, 
, 
4.4.2. Comparison with existing literature
A major drawback of deep learning classifiers discussed in Section 1 is consumption of processing time which incurs latency in the system. Many research works mentioned in Section 2 employ deep learning without much concern about the processing time. This issue becomes more serious in case of real time systems where user behavior is changing rapidly. Our proposed framework has also made use of the deep learning based classifier but in an optimized way so that it does not get triggered in all cases as shown in Fig. 25, which saves both access time and processing power without any degradation in attack detection capability. In our phase wise testing, we executed, a total of 1000 requests as shown in Fig. 24. If we were to pass all incoming requests to the deep learning classifier, it would get executed every time. Looking individually at classifier execution for certain groups, we realized that by maintaining maximum possible accuracy, the classifier‚Äôs execution dropped to 50% in the case of G1 for all phases as mentioned in Fig. 22. Since G3 turned malicious at 51st request and duly profiled then, all later requests did not trigger the classifier resulting in optimal performance without hurting accuracy and precision. Executions for G3 resume at 151st request as it returns back to normalcy as shown in Fig. 23. In this case the proposed optimization enables the deep learning classifier to get triggered for only 51% of requests by G3.

Fig. 27 provides comparative analysis in terms of accuracy of the proposed framework (for all three testing scenarios) with exiting approaches. It is pertinent to mention here that none of these approaches address the key issue of processing delays which has been duly addressed and catered for in our proposed framework.



Fig. 26. Performance comparison: Proposed framework Vs. ModSecurity WAF.

Moreover, most approaches in Section 2 focus on a limited number of web attacks. For instance, Althubiti et al. (2017) only detects SQLi, XSS and object de-serialization, Jacob et al. (2012) focuses on URL, cookie and user-agent infection and Birkinshaw et al. (2019) only detects SQLi and XSS attacks, Pham et al., 2016, Recht, 2019, Pan et al., 2019, Dewhurst, 2020 just cater for XSS attacks. Moreover, Simos et al. (2019) focuses on port scanning and DDoS, Nguyen and Hwang (2016) only detects website defacement while Appelt et al., 2018, Seyyar et al., 2017, Yang et al., 2019, Zhou and Wang, 2019, Stevanovic et al., 2011, The Apache Software Foundation, 2020 detect the OWASP top ten attacks. In comparison, our proposed framework detects all major kinds of web attacks and saves useful time and processing power because of the attacker profiling feature.


Fig. 27. 1D Convolutional Neural Network in detail.

5. Conclusion and future work
As websites carry huge amount of critical data, therefore the urge to maintain their security and privacy is of paramount importance. This research work proposes a framework where Deep Learning based classifier is nested with a Cookie Analysis Engine in order to protect a website from a wide variety of web attacks along with attacker profiling. For that purpose, a custom dataset was created for training while testing of the system was performed on a publicly available benchmark dataset completely unknown for the trained system. The proposed framework was also deployed and tested in a controlled real-time environment.

In our research we have demonstrated that the proposed Deep Learning classifier yields very high values of accuracy on all testing datasets. The Cookie Analysis engine whose prime objective is to differentiate suspicious HTTP requests from the malicious ones, complements the functionality of our classifier by working in cohesion. The attacker profiling feature limits the execution of our classifier and cookie analysis engine, thereby saving processing resources in real time. The framework also caters for POST HTTP requests, unlike many other research works in the same domain. To summarize, we have presented a web attack detection, mitigation and attacker profiling framework, that detects web attacks with a very high accuracy and can be deployed in any scenario to safeguard a website. This paper also presents a detailed comparison of existing deep learning based techniques for detecting web attacks.

The main task we intend doing in near future is to develop a complete Web Deception system which inhibits key characteristics of the actual website, having a variety of deceptive lures. Later we will join our proposed framework with the deception system in such a way that anomalous requests are routed not to the actual website but the deception system where the attacker‚Äôs engagement is enhanced in order to study and analyze the attack methodology, tactics and our weaknesses.