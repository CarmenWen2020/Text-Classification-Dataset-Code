Mastery assessments are used in many digital learning environments to control learning progression and support low-achieving students relearning important content. But research on personalized feedback following a mastery assessment is scarce and yielded ambiguous effects. Therefore, this study seeks to contribute to this area of research by investigating the effect of two different feedback messages in short online courses for German and English language grammar and spelling in secondary classrooms (grades 6–10). The study compares a reward-based feedback message to a self-referenced feedback message. The reward-based message indicates if the learner reaches the mastery criterion and how many points the learning app awarded for completing the test. The self-referenced feedback message applies motivation theory to strengthen students' internal attribution of causes for the test performance. The web-app MasteryX (www.masteryx.de) randomly assigned students to either the reward-based or the self-referenced feedback message. The study sampled 620 students (309 female, 311 male) in 53 classrooms from 27 secondary schools. It analyzed the effect of the two types of feedback messages on the level of test-retest-sequences (n = 2450). Results indicate small though significant positive effects of the self-referenced feedback message on subsequent learning behavior (reading elaborated item feedback, training behavior). However, a multilevel regression model showed small to medium effects of the reward-based feedback message on the higher course levels' follow-up mastery assessment score. The findings emphasize the complexity of designing personalized feedback strategies in online learning environments with mastery assessments.

Previous
Next 
Keywords
Mastery learning

Personalized feedback

Mastery assessment

Online courses

Language learning

Formative assessment

1. Introduction
Mastery assessments are formative assessments that have to be passed with a high percentage of correct answers and are used to control learning progression in highly structured content domains (Zimmerman & Dibenedetto, 2008; Lineberry, Soo Park, Cook, & Yudkowsky, 2015). For instance, learning management systems offer item formats for creating formative mastery assessments (Gamage, Ayres, Behrend, & Smith, 2019). And many online courses integrate short questions to evaluate student understanding of the materials and control for learner progression throughout the course (Monllaó Olivé, Huynh, Reynolds, Dougiamas, & Wiese, 2020). However, recent studies show that the repetition of a mastery assessment in e-learning environments does not necessarily improve student understanding (e.g., Pelkola, Rasila, & Sangwin, 2018). Moreover, feedback research implies that mastery assessments are only beneficial for learning when students process the formative feedback information to enhance their motivation and the effectiveness of the subsequent learning activities (Hattie & Timperley, 2007).

Digital technologies offer new solutions to tackle this problem. For example, recent studies showed beneficial learning effects of personalized and motivating feedback messages following formative tests (De Sixte, Mana, Clemente, & Sanchez, 2019). There is also evidence of positive achievement and motivation effects of self-referenced feedback messages over norm-referenced feedback on follow-up tests (McColskey & Leary, 1985; Chan & Lam, 2010; Shih & Alexander, 2000). But research on gamification design principles in e-learning also shows that simple feedback strategies such as rewarding students with points or badges can increase motivation, learning, and achievement (Attali & Arieli-Attali, 2015). However, research on both feedback strategies is scarce and often ambiguous. Moreover, most studies were conducted in postsecondary contexts and did not apply to formative mastery assessments in online courses.

Therefore, this paper investigates the effect of two different feedback strategies in short online courses for German and English language grammar and spelling in secondary classrooms. What kind of feedback strategy following a formative mastery assessment with a passing criterion of 90 % correct answers is more effective? A simple reward-based feedback strategy (Type A) or a more advanced personalized, self-referenced feedback strategy (Type B). The reward-based feedback message informs the learners about passing the assessment or not and how many points the app awarded, even if they did not pass the mastery assessment. The self-referenced feedback message comments on a student's learning progression to strengthen internal attributions, and self-efficacy gives recommendations on what to do next and motivates them to learn in the course. The paper contributes to the existing literature by comparing the effect of two personalized feedback strategies on elaborated feedback reception, learning activities, and student achievement in a digital learning context in secondary classrooms.

2. Background
2.1. Formative assessment and mastery assessment
Formative assessment is a widely recognized principle of effective teaching. Black and Wiliam (2009, p. 9) wrote that “practice in a classroom is formative to the extent that evidence about student achievement is elicited, interpreted, and used by teachers, learners, or their peers, to make decisions about the next steps in instruction that are likely to be better, or better founded, than the decisions they would have taken in the absence of the evidence that was elicited.” A recent meta-analysis revealed a moderate effect size (d = 0.29) of formative assessment interventions on student achievement in K-12 education and even larger effect sizes for student-initiated self-assessment, written feedback on quizzes, and medium-cycle feedback loops within or between instructional units (Lee, Chung, Zhang, Abedi, & Warschauer, 2020). Lee et al. (2020) found equal effects sizes for formative assessment interventions in mathematics (d = 0.34) and literacy (d = 0.33), whereas Kingston and Nash (2011) reported higher effects of formative assessment in English learning arts (d = 0.32) compared to math (d = 0.17). Formative assessment is also a frequently used component in many e-learning scenarios (Gikandi, Morrow, & Davis, 2011) with various types depending on content, goals, technology, and classroom implementation.

Mastery assessment is a specific type of formative assessment that goes back to the concept of mastery learning (Bloom, 1974; Guskey, 2010). It is frequently used in digital learning environments, although only a few studies explicitly draw on this concept's roots (Rae & Samuels, 2011). For example, intelligent tutoring systems (Paiva, Ferreira, & Frade, 2017; Sales & Pane, 2019), technology-enhanced writing programs (Birhan, 2018), online algebra courses (Pelkola et al., 2018), or game-based systems (Cui, Chu, & Chen, 2019) use formative mastery assessments. Mastery learning stands for an individualized learning approach based on a hierarchy of learning goals (Kulik, Kulik, & Bangert-Drowns, 1990; Lineberry, SooPark, Cook, & Yudkowsky, 2015; Zimmerman & Dibenedetto, 2008). Learning materials are arranged in subunits, and students have to pass frequent formative mastery assessments with a high success criterion of 70 %–90 % correct items. The mastery criterion ensures that each learner is proficient in the core concepts before progressing to the next learning unit. In case of failure, the mastery assessments provide corrective feedback to motivate and inform subsequent learning behavior.

Even though the concept of formative mastery assessments is widely used in many different e-learning scenarios, only a few studies explicitly examined the effects of formative mastery assessments on student achievement. For example, Rae and Samuels (2011) found high positive achievement effects of an online learning environment applying mastery learning principles on at-risk students in an undergraduate Computer Science program. Paiva et al. (2017) showed in a quasi-experimental approach that students who had to pass mastery assessments with a criterion of 60 % before progressing to the next unit of an online algebra course outperformed students in the control condition without any restrictions on course progression. Petrovic-Dzerdz (2019) found a high correlation between the number of mastery assessment repetitions and long-term memory (end-of-course exam) in a postsecondary Moodle course.

But other studies indicate problems of integrating mastery assessments in e-learning. There is evidence that a failed formative mastery assessment does not necessarily result in meaningful remedial instructional activities. Bokhove and Drijvers (2012) showed for a digital algebra learning course in Dutch secondary schools that the time students spent in training modules following a mastery assessment did not explain post-test variance. Pelkola et al. (2018) found that undergraduate engineering students in an online algebra course with mastery assessment (mastery criterion of 80 %) hardly used the correctives (e.g., watching a tutorial video) that the system displayed in the feedback message following a failed mastery assessment. Cui et al. (2019) described the same problem for the repetition of a not correctly solved “storm-chaser”-task in a game-based formative assessment system for learning weather concepts in geography education (Grade 5). And although Dutch third graders using Snappet, a mobile formative assessment tool for math and spelling, outperformed students in schools without access to the digital learning system (Faber, Luyten, & Visscher, 2017), no positive effect of the ratio of practice time divided by test repetitions appeared. This result implies that the number of repetitions mattered and not the learning activities in response to a failed mastery assessment. And finally, Sales and Pane (2019) found that student post-test performance in the field trial of Cognitive Tutor Algebra did not depend on course progression mode. Both modes, progression by exhaustion (working through all assignments in a subunit) and progression by mastery (demonstrating mastery in all relevant concepts in a subunit), were equally effective.

2.2. Feedback
Authors in the field of formative assessment widely agree that students' seeking and processing feedback information is critical to formative assessments' effectiveness (Hattie & Timperley, 2007; Timmers, Braber-van den Broek, & van den Berg, 2013). But what are the characteristics of effective feedback? Feedback research has a long tradition, and several meta-analyses and research overviews concluded that feedback intervention effects are highly mixed (Bangert-Drowns et al., 1991; Azevedo & Bernard, 1995; Kluger & DeNisi, 1996; Mory, 2004; Shute, 2008). For example, Kluger and DeNisi (1996) showed that feedback effectiveness depends on how the feedback message cues direct student attention. Normative feedback cues, i.e., unspecific praise or grades, lead to disengagement or negative subsequent task performance. But velocity feedback interventions (comparing task performance to previous trials) yielded average effect sizes of d = 0.55 compared to non-velocity feedback with d = 0.28. Feedback cues that direct attention to the task detail level (item-based feedback) may also improve performance (correct solution) but divert attention from the task level if too detailed.

It is also widely recognized in feedback research that feedback effects depend on student characteristics (Kluger & DeNisi, 1996, Mory, 2004; Narciss et al., 2014). Building on learning motivation and learning emotion theories, scholars in the field of computer-based feedback developed a theoretical framework for analyzing the interplay between motivation and feedback processing (Timmers et al., 2013; De Sixte, Mañá, Clemente, & Sánchez, 2019). First, students make attributional judgments to explain the feedback-performance gap (internal vs. external, controllable vs. non-controllable, stable vs. non-stable). Second, this attributional explanation results in emotional states (appraisal judgments). For example, shame and hopelessness will follow negative feedback if a student perceives the cause of the feedback as not controllable and stable. If the student attributes the cause of negative performance as internal, controllable, and non-stable, it is more likely that the feedback will increase task motivation and hope.

Feedback effects also interact with the domain. Research on item-based feedback effects in digital learning found larger effect sizes in math and smaller effect sizes in science, social sciences, or language instruction (Van der Kleij, Feskens, & Eggen, 2015). Eventually, feedback intervention effects depend on task and feedback complexity (Kluger & DeNisi, 1996; Mory, 2004; Shute, 2008; Van der Kleij et al., 2015). Several meta-analyses found that elaborated feedback (EF) is more effective than knowledge of results (KR, item correct or not correct) or knowledge of correct result (KCR, providing a correct answer). But the effect of feedback elaboration is moderated by task complexity. Kluger and DeNisi (1996) found in their meta-analysis that simple feedback messages following lower-order tasks increased motivation and subsequent task performance, and elaborated feedback is more beneficial for learning complex tasks. But some studies on feedback effects in more complex learning domains also show that elaborated feedback messages do not necessarily outperform more simple feedback (Maier, Wolf, & Randler, 2016). Van der Kleij, Eggen, Timmers, and Veldkamp (2012) found no significant effect of elaborated feedback on post-test scores. However, elaborated feedback was perceived most valuable, and students with higher motivation spent more time reading the elaborated feedback.

2.3. Feedback in online courses
Since general feedback research refers to all kinds of learning situations, this study focuses on feedback effects in online courses with formative mastery assessments. This means that elaborated feedback does not only refer to a student's performance in one task or specific assessment. Personalized or elaborated feedback messages in online courses should convey additional information about the student's progress towards the course's learning goals. Referencing the learner's performance to previous test results is one possible solution to this problem. The previous section mentioned that meta-analyses found positive achievement and motivation effects of velocity feedback or self-referenced feedback, which compares actual performance with prior measures of an individual's ability. Compared to norm-referenced feedback (comparing an individual's performance to the performance of other students), self-referenced feedback increases internal attributions (McColskey & Leary, 1985) and achievement gains (Shih & Alexander, 2000).

However, research also revealed ambiguous effects. Chan and Lam (2010) described that self-referenced feedback caused a lower self-efficacy in English vocabulary acquisition in a Chinese secondary classroom compared to norm-referenced feedback. But students in both feedback conditions did not differ in performance. In an experimental study, De Sixte et al. (2019) found that warm elaborated feedback (prompting internal attributions, motivating statements) in a computer-based reading course in secondary education resulted in better post-feedback learning behavior than elaborated feedback only and no feedback. But again, the three experimental groups did not differ in overall performance.

Another feedback strategy in online learning is awarding points for learning activities throughout the course (Hew, Huang, Chu, & Chiu, 2016). This strategy roots in the concept of reinforcement learning (Bangert-Drowns et al., 1991; Thorndike, 1913). It is also one design principle of gamification in e-learning (applying game design principles to non-game learning contexts). But research on the effects of awarding points in online courses is also scarce and mixed. For example, Attali and Arieli-Attali (2015) found positive effects of point rewards on the speed of responses, likeability ratings, perceived effort, but no effects on the accuracy of responses in a computerized math assessment system in middle schools. Two studies on the effects of reward-based feedback in university courses reported increases in some course activities (e.g., activity in discussion forums) and increased motivation to engage with challenging tasks (Hew et al., 2016; Ortega-Arranz et al., 2019). However, the online course's reward-based feedback strategy did not affect student retention and other learning activities such as page views, task submissions, and student activity time.

3. Research context and research questions
This paper's research context is MasteryX, a University of Education Schwäbisch Gmünd (Germany) project that examines the interplay between mastery assessments, feedback usage, subsequent learning behavior, and achievement growth in online courses. The project launched a web application (www.masteryx.de) with short, curriculum-based courses for German and English language grammar and spelling in secondary education (e.g., punctuation, sentence structure, times, classification of words). The main focus of the project is German language grammar (9 courses). Meanwhile, the web app also offers two English language grammar courses (tenses, conditionals). The web app structures course content along a hierarchy of learning goals. Each course involves three course levels with ascending difficulty and a well-defined knowledge progression based on state curricula (Fig. 1a). Students have to pass the mastery assessment at each level before moving forward to the next course level (Fig. 1b). Mastery assessments are concise formative tests with a highly demanding mastery criterion of 90 %. Classroom observations and teacher feedback in the precursor studies clearly showed that short formative tests (5–10 min) keep up student motivation and facilitate course implementation in ongoing lessons (Maier, 2020; Maier, Ramsteck, & Hofmann, 2017). Hence, a mastery criterion of 90 % ensured that only students with a good command of the grammar rules progress to the next level.

Fig. 1
Download : Download high-res image (1MB)
Download : Download full-size image
Fig. 1
Download : Download high-res image (386KB)
Download : Download full-size image
Fig. 1
Download : Download high-res image (409KB)
Download : Download full-size image
Fig. 1
Download : Download high-res image (315KB)
Download : Download full-size image
Fig. 1
Download : Download high-res image (463KB)
Download : Download full-size image
Fig. 1
Download : Download high-res image (768KB)
Download : Download full-size image
Fig. 1. a: Dashboard for the course tenses (Zeitformen). The rectangles display all relevant rules in the course, and the rectangle's color indicates the level. Students can see if they mastered the rule in the formative assessment and if they completed the training. b: Test item in the course tenses (Zeitformen). Students have to click on the correct tense of the verb that is marked in the sentence. c: Item feedback for the formative mastery assessment in the course tenses on level 1. The web app only displays incorrect items in combination with the distractors (tenses). d: Rule feedback for the formative mastery assessment in the course tenses on level 1. The web app displays that the student only correctly applied the present tense (Gegenwart). e: Example of the rule section for the tense “Vorvergangenheit” on level 2. f: Part of the training section of the web app. Students have to insert commas in the text by clicking on the word right before the comma. The check button (Prüfen) displays immediate feedback (only available for training items, not for test items), e.g., the item is partially correct in this case. (For interpretation of the references to color in this figure legend, the reader is referred to the Web version of this article.)

After finishing the mastery assessment, the web app displays a written feedback message that says if the level was mastered or not (for more details see 4.3). The program also displays two additional buttons with item feedback and rule feedback. The item feedback lists all faulty items (sentences) and the associated grammar rule in brackets (Fig. 1c). The web app did not display correct solutions to engage students in thinking about their misunderstandings. The rule feedback displays an overview of the grammar rules tested in the mastery assessment and the percentage of correct items of this rule (Fig. 1d). The web app highlighted rules with more than 80 % correct items in green. All grammar rules with less than 80 % correct answers were automatically subject to revision and displayed as “Training needed” in the course overview. By clicking on the rules, the course offers grammar explanations and worked examples (Fig. 1e). Time spent reading explanations and examples and clicking back and forth between explanations and examples are recorded and used as an active revision indicator. Before repeating the mastery assessment, students had to work on one set of assignments with relatively simple sentences that correctly applied one specific grammar rule (Fig. 1f). As students are allowed to repeat the training assignments as often as needed, this study interprets the ratio between failed training sets and overall training sets to indicate rule-focused learning.

Fig. 2 shows the sequential order of repeated formative assessments. The follow-up mastery assessment examines if feedback reception and compulsory training positively affect students’ understanding of the grammar rules. Therefore, the unit of analysis for this study is a test-retest-sequence, encompassing test, feedback reception, learning activities, and follow-up mastery test. The web app offers elaborated feedback information on different levels (Kluger & DeNisi, 1996; Shute, 2008): task-level (items), process level (rules, elaborated examples, training). The elaborated feedback messages aim at directing student attention to relearning rules that a student had not mastered yet. However, it is not clear to what extent students think about their incorrect items and revisit the grammar rules.

Fig. 2
Download : Download high-res image (370KB)
Download : Download full-size image
Fig. 2. Sequential order of repeated formative assessments and test-retest-sequences.

Furthermore, the evaluation of a Moodle precursor of the web app showed that not all students used the item-based feedback and that achievement gains were explained by test-repetition rather than efficiently relearning the grammar rules (Maier, 2020). Hence, what kind of feedback message displayed right after completing the mastery assessment is best suited to direct student attention to the rule and item feedback components and the training materials? There is evidence from the reviewed literature that personalized feedback messages with motivating statements might enhance learners' engagement in an online course and result in positive achievement gains. Self-referenced feedback messages, for example, can increase students’ attributional judgments to perceive the cause of failing a mastery assessment as internal and controllable. But research also showed that a more simple and easy-to-apply point reward feedback system reinforces student engagement following a failed course assessment.

This paper, therefore, seeks to compare the effectiveness of two different feedback messages on learning gains of students who failed the mastery assessment: reward-based feedback (Type A) vs. self-referenced feedback (Type B). Both feedback messages are displayed right after a student finished the mastery assessment. Type A informs the learner about passing the assessment or not and how many points the web-app rewards, even if the student did not pass the mastery assessment. Type A draws on the theory of reinforcement learning and gamification research. Type B feedback comments on a student's learning progression to strengthen internal attributions and self-efficacy, gives recommendations on what to do next and motivates them to learn in the course. This paper assumes that a self-referenced feedback message can increase a student's motivation and performance gains when it attributes the causes for the mastery test performance as internal and controllable.

Since previous research on both feedback types yielded mixed effects and none of the studies to date compared both feedback types, it is not yet possible to formulate research hypotheses in one or the other direction. However, it is assumed that the feedback types might have a differential impact on learning activities and achievement gains since studies on reward-based feedback showed an impact on learning behavior but not on achievement (e.g., Ortega-Arranz et al., 2019). It is also hypothesized that course level interacts with the feedback effects since elaborated feedback is more beneficial for complex tasks (e.g., Kluger & DeNisi, 1996). The paper, therefore, aims at answering the following research questions:

Q1: What is the impact of reward-based feedback vs. self-referenced feedback on the reception of the rule and item feedback? This means that the study manipulates only one more minor part of the elaborated feedback and seeks to investigate the effect on the reception of the other feedback parts.

Q2: What is the impact of reward-based feedback vs. self-referenced feedback on the learning activities between the failed mastery assessment and the follow-up mastery assessment? Previous research implies that both feedback strategies yield some effects on the course behavior. Log Data is used to create three indicators for learning activities in this web app:

-
the time learners spent reading the rules and examples (Q2a)

-
the clicks between rules and examples (Q2b)

-
the failed training ratio (Q2c)

More reading time does not necessarily indicate a deeper understanding of the rules and examples. However, little or no reading time indicates that a student did not process the rules and examples. The click rates between rules and examples indicate that students actively connect rules and their respective examples by switching back and forth. The failed-trainings ratio is high when the student repeats the training until it is correct. Consequently, a low failed-trainings ratio implies that the student does not rely on trial-and-error (Q2c).

Q3: What is the impact of reward-based feedback vs. self-referenced feedback on the follow-up mastery assessment score? This question cannot be answered independently of Q1 and Q2 since feedback reception and subsequent learning activities might affect the learning gain in a test-retest-sequence.

4. Method
4.1. Study setting
The web-app masteryx. de, a research project of the University of Education Schwäbisch Gmünd is online since January 2019 and builds on a precursor Moodle course (Maier, Ramsteck, & Hoffmann, 2017). Teachers and students in the teacher training program developed the course materials. The project team invited schools in the Southwest of Germany to participate in the evaluation study. Before using the web app in the classroom, teachers read through and agree with the data processing regulations. After that, the teacher generated his virtual classroom with anonymous student aliases and received login data for the students via email. The students logged in to the system with their alias and subscribed to courses. The teacher had access to a classroom roster with all relevant learning progress data from the students.

In sum, 72 teachers used the web app from January 2019 until February 2021 in 98 secondary classrooms (43 schools). Teachers were free to decide how to use the web app with their ongoing curriculum. Some teachers decided to implement the web app as compulsory learning material in their free study time. Others only used one or two courses in their regular lessons. And some teachers offered online courses as voluntary homework. The amount of course usage within the classroom varies between one course (19.4%) and all eleven courses (6.1%), with a mean of 4.8 courses per classroom (sd = 3.1). In 9 out of 98 classrooms, the students only worked within one lesson with MasteryX. On average, the web-app usage was spread out over 9.8 days (sd = 12.9 days, skew = 1.97 days) on the classroom level with a maximum of 58 days.

The overall summed active learning time in one course varied due to implementation conditions (mean: 45 min, median: 33 min, lower-quartile: 15.1 min, upper-quartile: 75.0 min). Active learning time was computed as the sum of the duration of subsequent learning activities in one course. If the period between two learning activities exceeded 10 min, I assumed that the student paused or stopped working in the course. Course content also affected students overall summed active learning time (e.g. punctuation: mean = 80.5 min, Sentence structure: mean = 94 min, upper- and lower-case writing: 43.9 min).

Table 1 summarizes course achievement data from January 2019 up to February 2021. It shows the mean of the highest level reached (range from 0 to 3, see Fig. 2) by course content, school type, and grade. Highest course level 0 was assigned if a student did not master the formative assessment on level 1. For reasons of clarity, the table aggregates grades in two major groups and only displays the mean for cells with more than five course completions (in brackets). Data show that older students and students in the higher-track secondary schools (Gymnasien) achieve higher mean course levels than younger students and students in lower-track secondary schools. This result proves the curricular validity of the formative assessments and the course levels. However, course achievement also depends on course content which reflects the curricular alignment of the courses. For instance, sentence analysis and upper-lower case writing are well-positioned topics in the first years of secondary education curricula and textbooks, whereas secondary school curricula less emphasized punctuation.


Table 1. Mean of course level reached and course completions by course content, school type, and grade (n = 2801).

Course	Lower-track schools	Higher-track schools
Grades 5-7	Grades 8-10	Grades 5-7	Grades 8-10
Conditionals (engl.)	–	1.0 (64)	–	–
Conjunctions	0.6 (9)	1.4 (278)	2.0 (49)	2.1 (54)
Spelling	–	1.5 (82)	1.6 (53)	2.2 (13)
Sentence analysis	1.5 (47)	1.7 (561)	1.1 (46)	2.2 (69)
Word combinations	–	1.1 (106)	1.5 (26)	1.9 (34)
Upper-lower cases	1.18 (79)	1.6 (375)	1.8 (126)	2.4 (105)
Punctuation	–	0.9 (276)	1.0 (53)	1.6 (64)
Sentence parts	0.9 (6)	1.2 (244)	1.4 (27)	–
Tenses	0.5 (24)	0.7 (95)	–	–
Word classification	0.3 (19)	0.7 (142)	0.7 (33)	1.5 (18)
Tenses (engl.)	0.8 (54)	1.1 (161)	0.8 (11)	1.6 (20)
Note: Course completions in brackets; highest course level ranges from 0 to 3; course level 0 was assigned if a student did not master the formative assessment on level 1; mean only provided if subgroup includes more than 5 course completions.

4.2. Sample
The experimental manipulation of the feedback was online from January 2020 until February 2021. Students who learned with the web app in this time provided data for the subsequent analyses (n = 1053 students, see Appendix A). However, not all classrooms and students who used the courses in this period were eligible for data analyses in this paper. First, the study excluded classrooms with less than ten test-retest-sequences in one course, because these teachers only tested the web app and then stopped using it. Second, all incomplete test-retest-sequences were excluded, for instance, if a student mastered the first attempt of the mastery assessment and then progressed to the next level. This exclusion is necessary since the research questions can only be answered within complete test-retest-sequences. The final sample encompasses 620 students. The exclusion of test-retest-sequences does not result in a systematic bias of the final sample in terms of gender and school type. However, more students in the lower Grades 5–7 have been excluded (see Appendix A). One plausible reason for this exclusion might be that lower Grade students faced more difficulties in higher course levels and did not try to complete the level after one failed formative assessment. The web app could not collect other relevant student characteristics except gender because of strict data processing regulations.

The schools in the sample are spread over the Southwest of Germany, with most of them located in smaller cities, suburban or rural areas. The socio-demographic structure of the school system in Germany implies that students in the lower-track secondary schools are rather heterogeneous in terms of ethnicity, migration background, and socioeconomic status. Therefore, although the school sampling procedure is regional and non-probabilistic, the classrooms most likely cover a wide range of prior knowledge in German and English language grammar and spelling. This variance is in line with the project's main goal to provide a formative assessment tool that covers basic grammar knowledge throughout secondary education. Additionally, secondary teachers in both types of the school system (Gymnasium, secondary school) have to deal with a growing diversity of student achievement in their classrooms. Hence, the Grade 8 and higher teachers used the web app to repeat basic grammar knowledge of the Grade 5/6 German or English language curriculum.

4.3. Experimental conditions and randomization procedure
A randomized controlled trial was conducted within the web app MasteryX to answer the research questions. The assignment to one of the two experimental feedback conditions happened on the student-course level. After subscription to a course, the system randomly assigned this course to either the reward-based feedback message (Type A) or the self-referenced feedback message (Type B). Consequently, students who used more than one course in the web app by chance worked with some of their courses in condition A and other courses in condition B. Randomization on the student-course-level instead of on the student-level has advantages and disadvantages. The randomization procedure results in more balanced feedback conditions regarding the number of observations or course content since student engagement in the web app differs significantly. However, the dataset structure is nested on three levels: classrooms, individuals, and student-course level. Therefore, a three-level regression model will control the classroom and student grouping structure (Level 3: classrooms, Level 2: student, Level 1: student.

-course). The sampled students started 2199 courses during the time of the experimental manipulation of the feedback. The courses are equally distributed by feedback condition (Type A: 50.3%, Type B: 49.7%). The randomization procedure also implies that all courses (Chi-squared = 5.88, df = 10, p = 0.82) were equally distributed across both experimental conditions. Chi-square and t-testing also showed that there are no person-related differences between experimental groups (Gender: Chi-squared = 0.75, df = 1, p = 0.38; School type: Chi-squared = 0.38, df = 1, p = 0.53; Grade: t = 1.119, df = 465.58, p = 0.26). And the prior knowledge, operationalized as the first assessment on level 1, is also equally distributed across conditions (t = −0.23, df = 3060.1, p = 0.81).

The experimental groups differed in the feedback message the system displayed right after completing a mastery assessment (Fig. 3). In the reward-based feedback condition (Type A), the test feedback message told the student the percentage of correct answers and if he mastered this course level or not. In addition, the second sentence of the feedback message informed the student about the overall course score's added points. But the system only awarded points for failed mastery assessments with 50 % and more correct answers. Hence, students who just guessed or clicked through the formative test did not earn any points. However, students who tried to do their best but had some problems with the rules got the reward points. Example for the reward-based feedback message are:

“You passed this test with 95% correct answers. As a result, your score will increase by 95 points!"

“Unfortunately, you failed to pass this test with 60% correct answers. Therefore, your score will increase by 60 points!"

Fig. 3
Download : Download high-res image (233KB)
Download : Download full-size image
Fig. 3. Experimental modification in the test-retest-sequences.

Students got points for mastery assessments in both feedback conditions. Still, the reward-based feedback message signaled the gain of points and triggered the learner to connect points as a reinforcer to the completed mastery assessment.

The self-referenced feedback message (Type B) refers to a students' learning progression (if it is the first, second, or third attempt), the mastery assessment score (0–59 %: no mastery and little understanding of the rule; 60–89 %: almost mastery and some understanding; 90–100 % high level of mastery), and if a student made progress by comparing the score with previous attempts. Based on this information, the message gives directive cues on what to do next in the course and ends with a motivating statement to keep going. The self-referenced feedback message's overall goal is to enhance students' attribution of their course performance as controllable and non-stable. If, for example, a student just finished his follow-up mastery assessment with 70 % correct answers and he did not pass his first mastery assessment with 40 % correct answers, the displayed message is as follows:

“You, unfortunately, did not pass the test, but it was tight, and you improved your test score. Keep on going! Read the item feedback and practice the rules that you require training!"

The feedback message for a student who passed the first mastery assessment:

“Well done! You immediately passed the test. Go on to the next level of the course!”

If a student had less than 60 % correct answers in the first and second mastery assessment, the feedback message said:

“You had lots of mistakes in the quiz again! Did you thoroughly read the rules and examples? Ask your teacher for additional support and exercises!"

A student with 80 % correct in the first test and only 50 % correct in the follow-up mastery assessment saw this message:

“Oops! Now you have even fewer correct answers compared to your first trial. Keep focused and go through all the rules you require training!"

The self-referenced feedback messages had a maximum of three short sentences to ensure that students thoroughly read the message and guarantee that feedback message length does not confound the differential feedback effect.

4.4. Measures
Classroom and student characteristics: Since the project had to adopt rigorous data processing standards, only limited information about student characteristics is available for this study. The web app only asked for school type (higher or lower-track secondary school), grade, and gender for each student.

Student learning behavior: This study used log-data (timestamps, number of clicks) and training scores to measure feedback usage (rules feedback reading time, item feedback reading time) and student learning behavior (examples and rules reading times, switching between rules and examples, the ratio between successful and overall training) within the test-retest-sequences. However, the feedback reading times were positively skewed. A high proportion of students did not click on one of the two feedback buttons (item feedback: 41.1%, feedback on rules: 84.7%). Therefore, I transformed the reading times into a binary variable: read vs. not read. The number of clicks within the rules and examples module is also significantly skewed, with 66.8 % test-retest-sequences without any clicks in this module (M = 1.14 clicks; SD = 2.52; Max = 34). The ratio between the number of failed training divided by the number of completed training describes a student's accuracy in applying the rules to the training items' examples. The ratio was zero in 57.4% of the test-retest-sequences since learners completed compulsory training with more than 80 % correct items. In at least one failed training (42.6% of the sequences), the failed training ratio had a mean of 0.43 (SD = 0.17).

Mastery assessments: The web app is a practical and easy-to-use formative assessment tool that is well-aligned with the German and English language grammar and spelling curricula in secondary education. The mean achievement levels displayed in Table 1 correspond with the curricular alignment of the mastery assessments and the level structure of the courses. The mastery assessments are as short as possible (median test time duration: 180 s, lower-quartile: 116 s, upper-quartile: 282 s). Students should have enough time to do all the training and the follow-up assessment within a 45-min lesson. Therefore, the number of items in the mastery assessments varied between 15 and 20 (mean = 17 items), depending on course content and level. The level-1 mastery assessment in the course word classification, for example, included 20 sentences with one word highlighted for classification. The mastery assessment on level 2 in the course “punctuation” included three short texts with eight commas to insert per text resulting in 24 items. Students clicked on the word, after which the app inserted a comma.

The web app randomly generated the formative assessment from an item bank. The item bank's volume was about three to four times larger than the number of items for one mastery assessment (e.g., Pelkola et al., 2018). Hence, all mastery assessments at one level in one course showed different items in a different sequence. This was important for teachers who started a course to prevent any cheating during the mastery assessments. A second reason was that follow-up mastery assessments displayed more than 50 % unseen items. However, students do not get a completely new, parallel test for their follow-up-mastery assessment. Some items are familiar. But students did not get the correct item solutions in the item feedback. Hence, it might indicate learning gains if they correctly answer a familiar test item in the follow-up mastery assessment.

The limited number of test items and frequent retesting may threaten the mastery assessments' reliability and validity (Lineberry et al., 2015; Way, Dolan, & Nichols, 2010). And unfortunately, the random item selection does not allow to compute internal consistency values (Cronbach's Alpha). This study used two alternative strategies to gauge and improve mastery assessment reliability. A pilot study computed an internal consistency value for at least one mastery assessment. The pilot offered three level-2 texts with 25 items (commas to insert) from the item base for “punctuation” as a demo-test in the web-app information tab. Data from 140 persons who completed this demo test were analyzed and showed a good reliability score (alpha = 0.86; M = 80.1 % correct answers; SD = 18 %). The item-total correlation of the 25 items ranged from 0.12 to 0.70 (M = 0.49; SD = 0.14). Most items are within an acceptable or even good range of item-total correlation (De Vaus, 2002). Only two out of 25 items had an item-total correlation under 0.30, which indicated severe reliability problems. The domain experts revised the items. The second strategy to ensure high test reliability is an ongoing item revision based on item-total correlation and item difficulty. The majority of test items came already from the precursor Moodle project and underwent revision several times. In most cases, items with extremely low or high item difficulty or item-total correlations under 0.30 were erroneous or did not fit as a good measure for the respective grammar rule. These items were either deleted or revised.

4.5. Analytic strategy and descriptive statistics
Since the research questions refer to learning behavior and follow-up test scores following a failed mastery assessment, the analytical unit is at the level of test-retest-sequences (see Fig. 2). One test-retest-sequence involves a failed mastery assessment (test), feedback, subsequent learning behavior, and the follow-up mastery assessment (retest). The final dataset included 4144 test-retest-sequences (unit of analyses). In the next step, this dataset was split into two parts: The first test-retest-sequence on a course level (Dataset 1: n = 1694) and the second and higher test-retest-sequence on a course level (Dataset 2: n = 2450). Since it is assumed that the feedback messages aim at motivating struggling students who fail to succeed in the mastery assessment two times and more, the effects of the feedback messages should mainly occur in dataset 2. In dataset 1, for example, the self-referenced feedback message has no velocity feedback component because no information on learning gains is available.

Table 2 provides descriptive statistics for both datasets and all features. The number of observations in both datasets is equally distributed across the levels, and all classrooms contribute cases. The proportion of observations in the experimental groups are also equally distributed in both datasets. Type A feedback condition slightly exceeds the 50% from randomization at the course level. Subsequent analyses will shed light on this detail. The Chi-square tests show that both datasets significantly differ in school type and gender. The proportion of test-retest-sequences from higher track students and females significantly decreases in dataset 2, meaning that male and lower-track students are more likely to repeat the formative assessments several times. Test scores on all three levels also significantly differ between datasets (t-Tests) since dataset 1 encompasses the prior knowledge at one level, and tests in dataset 2 reflect at least some learning gains from previous test-retest-sequences. However, test scores and retest scores are not skewed by level in datasets 1 and 2.


Table 2. Descriptive statistics for dataset 1 and 2.

Dataset 1	Dataset 2	Significance of difference between datasets
Number of observations	1694 (100%)	2450 (100%)	
Level 1	46%	42%
Level 2	34%	35%
Level 3	20%	23%
Order of test-retest-sequences (n = 4144 equals 100 %)	First: 40.9%	Second: 21.8%	
Third: 12.3%
Fourth: 7.5%
Fifth: 5.0%
Number of classrooms	53	53	
Exp. Groups: Type A/Type B	52.7%/47.3%	53.5%/46.5%	n.s.
School type: lower track/higher track	85.5%/14.5%	92.0%/8.0%	p < 0.001
Gender: male/female	46.6%/53.4%	53.4%/46.6%	p < 0.001
Grades: 5–7/8–10	11.0%/89.0%	9.7%/90.3%	n.s.
Test score (0 %–100%)
Level 1	M = 69%, SD = 17%	M = 71%, SD = 14%	p < 0.01
Level 2	M = 71%, SD = 16%	M = 76%, SD = 12%	p < 0.001
Level 3	M = 70%, SD = 16%	M = 75%, SD = 12%	p < 0.001
Retest score (0%–100 %)
Level 1	M = 82%, SD = 17%	M = 79%, SD = 17%	p < 0.001
Level 2	M = 82%, SD = 14%	M = 81%, SD = 12%	n.s.
Level 3	M = 80%, SD = 13%	M = 79%, SD = 13%	n.s.
Reading rule feedback: no/yes
Level 1	78.5%/21.5%	86.8%/13.2%	p < 0.001
Level 2	82.1%/17.9%	83.0%/17.0%	n.s.
Level 3	84.4%/15.6%	84.6%/15.4%	n.s.
Reading item feedback: no/yes
Level 1	35.6%/64.4%	45.4%/54.6%	p < 0.001
Level 2	41.4%/58.6%	45.5%/54.5%	n.s.
Level 3	34.2%/65.8%	35.5%/64.5%	n.s.
Reading rules and examples (sec.)
Level 1	M = 82.4, SD = 128	M = 43.6, SD = 98	p < 0.001
Level 2	M = 59.8, SD = 117	M = 37.0, SD = 151	p < 0.001
Level 3	M = 94.7, SD = 138	M = 29.8, SD = 61	p < 0.001
Clicks between rules and examples
Level 1	M = 2.07, SD = 3.65	M = 0.71, SD = 1.87	p < 0.001
Level 2	M = 1.47, SD = 2.71	M = 0.68, SD = 2.08	p < 0.001
Level 3	M = 1.69, SD = 2.69	M = 0.54, SD = 1.34	p < 0.001
Failed trainings ratio
Level 1	M = 0.27, SD = 0.27	M = 0.16, SD = 0.24	p < 0.001
Level 2	M = 0.18, SD = 0.23	M = 0.12, SD = 0.20	p < 0.001
Level 3	M = 0.28, SD = 0.25	M = 0.15, SD = 0.22	p < 0.001
Chi-square tests and T-testing also show that the feedback and learning variables mainly differ between both datasets. This finding corresponds with the distinction between the first and subsequent test-retest-sequences. The struggling students in dataset 2 are likely to skip feedback information, ignore the rules and examples section, and switch between rules and examples. However, the failed training ratio decreases in dataset two, which might indicate that struggling students learned from their first test-retest sequence that a trial-and-error training strategy results in more training time before being allowed to repeat the formative assessment.

The following steps in data analyses were exclusively performed with dataset 2. The result section will first present T-tests and Chi-square tests to establish the effect of the feedback message conditions on the item and rule feedback reading time, rules and examples reading time, click-rates within the rules and examples-component, the failed training ratio, and the follow-up mastery assessment (retest). Since descriptive statistics indicate different means and frequencies of feedback and learning variables by level, all comparisons will consider this stratification in the dataset.

This study computed a series of random intercept-only linear-mixed effects models with the R-package lme4 (Bates, Mächler, Bolker, & Walker, 2015) to investigate the effect of feedback conditions on learning gains. Strictly speaking, data are nested in four layers: Classrooms, students, courses, and test-retest-sequences. However, fitting a hierarchical model with four levels would result in highly imbalanced group sizes across levels. For example, some students provide only one test-retest-sequence, whereas others used multiple courses with multiple test repetitions. I, therefore, decided to fit a three-level model with the classrooms and students as the grouping variables. The result section presents four consecutive random intercept-only multilevel models. The null model showed a 19% level 2 variance (students) and a 7% level 3 variance (classrooms) in the intercept. Model 1 and 2 subsequently add student characteristics, course content, test score, level, learning variables, and experimental condition as fixed effects. Since descriptive statistics show that most variables differ by level and feedback research implies that feedback effects depend on task difficulty, model 3 added interactions between levels and feedback conditions. Furthermore, it is assumed that the feedback conditions affect the learning variables. Model 3, therefore, also tests interactions between feedback type and the learning variables on level 1.

5. Results
Table 3 summarizes the T-tests and Chi-Square tests by experimental group and level for formative assessment scores, feedback reception, and learning behaviors. The upper part of the table already gives one answer to research question 3 (Q3). No differences between experimental conditions occurred for both achievement variables (test score, retest score) on level 1. However, T-testing shows significantly higher retest scores for the reward-based feedback condition (Type A) on levels 2 and 3 and higher test scores on level 3. Thus, it seems that students in some way benefit from the reward-based feedback and that this effect accumulates from level 1 through 3. The multilevel analysis will further investigate this effect.


Table 3. Student learning behavior by feedback groups and level (Dataset 2: Second test-retest-sequence and higher).

Variable	Metric	Level	Type A n = 1311	Type B n = 1139	Test	Sign. Of difference	Effect size (Cramer's V, Cohen's d)
Test score		1	70.4%	71.6%	T-test	n.s.	0.08
2	76.5%	75.5%	n.s.	0.09
3	76.1%	73.3%	p < 0.01	0.24
Retest score		1	78.0%	79.6%	T-test	n.s.	0.10
2	82.0%	80.3%	p < 0.05	0.14
3	80.6%	76.3%	p < 0.001	0.35
Item feedback (Q1a)	Student clicked on feedback	1	49.2%	61.7%	Chi-square test	p < 0.001	V = .12
2	56.0%	52.9%	n.s.	V = .03
3	61.6%	67.5%	n.s.	V = .06
Rules feedback (Q1b)	Student clicked on feedback	1	9.9%	17.5%	Chi-square test	p < 0.001	V = .11
2	12.8%	21.5%	p < 0.01	V = .12
3	13.5%	17.3%	n.s.	V = .05
Reading rules and examples (Q2a)	Time in seconds	1	47.0 s	39.1 s	T-test	n.s.	d = .08
2	35.2 s	38.8 s	n.s.	d = .02
3	27.8 s	31.9 s	n.s.	d = .07
Clicks between rules and examples (Q2b)	Number of clicks	1	.67	.78	T-test	n.s.	d = .04
2	.71	.65	n.s.	d = .03
3	.55	.53	n.s.	d = .02
Failed-trainings ratio (Q2c)	Failed trainings/number of trainings	1	18.2%	14.1%	T-test	p < 0.01	d = -.17
2	13.6%	10.4%	p < 0.05	d = -.16
3	14.9%	15.1%	n.s.	d = −.01
Item feedback (Q1a) significantly differs between the two experimental groups on level 1 and rule feedback (Q1b) on levels 1 and 2, with small effect sizes. Contrary to test and retest scores, the self-referenced feedback positively affects the elaborated feedback reception. Although the proportion of test-retest-sequences where students read the rules feedback remains low, the self-referenced feedback message considerably increases to roughly 20 %. The time learners spent reading the rules and examples (Q2a) and the number of clicks between rules and examples (Q2b) are not affected by both feedback conditions. But a significant difference in the failed training ratio (Q2c) appeared between the two groups on levels 1 and 2, again in favor of the self-referenced feedback message. The small effect size indicates that the self-referenced feedback message slightly results in a more thorough training behavior with less trial-and-error.

Eventually, this study computed three-level mixed-effects linear regression models with random intercept-only to evaluate experimental conditions' effect on follow-up mastery assessment scores and control student characteristics, test scores, and learning behavior. The density plots in Appendix A (file 2) show a visual examination of the dependent variable's distribution. Between and within groups (classroom, student) retest scores are plotted against a normal distribution. The classroom results show no outliers, and the dependent variable is normally distributed on the group level. Likewise, there are only a few outliers (low retest scores) on the student level, and the ICC residuals (within classroom variance, within student variance) are close to the normal distribution.

Another prerequisite of multilevel modeling is a certain percentage of variance on the grouping level. The null model's ICC (intra-class correlation coefficient) indicated that classroom as a second grouping variable accounts for 7% of the total variance (Table 4). This variance is at the lower point of the 10–25% interval of ICC values usually reported for nested educational datasets (Hedges & Hedberg, 2007). Adding student characteristics, course content, test score, and level as fixed effects (Table 4, model 1) reduced the classroom grouping variance from 7% to 2%. Adding other variables (models 2 and 3) does not change the ICC value. This means that classroom influences on retest scores decrease by controlling for school type and retest score on level 1 of the models. The ICC of the student grouping variable is higher and only decreases from 19% to 12% from the null model to model 3.


Table 4. Three-levels multilevel regression models (random intercept-only) with retest score as dependent variable.

Null model	Model 1	Model 2	Model 3
(Intercept)	81.27 *** (0.75)	53.39 *** (4.52)	55.90 *** (4.55)	54.95 *** (4.52)
Fixed main effects
School type (lower)		−4.86 ** (1.60)	−4.92 ** (1.59)	−4.68 ** (1.57)
Grade		.49 (.45)	.47 (.45)	.53 (.45)
Gender (female)		1.21 (.80)	1.10 (.80)	1.08 (.80)
Courses (a)		−3.39 (1.93) to 6.12 (2.45)	−2.98 (1.92) to 6.18 (2.44)	−2.98 (1.93) to 5.96 (2.43)
Test score		.31 *** (.02)	.29 *** (.02)	.29 *** (.02)
Level 2		−1.14 (.68)	−1.12 (.68)	-.06 (.89)
Level 3		−5.17 *** (.81)	−5.02 *** (.80)	−3.41 *** (1.05)
Feedback (Type B)			−1.19 (.63)	.98 (1.20)
Item feedback			1.12 (.60)	1.05 (.79)
Rule feedback			.54 (.82)	.62 (1.20)
Rules and examples			.01 ** (.00)	.01 ** (.00)
Clicks between rules and examples			-.68 *** (.18)	-.66 *** (.18)
Failed-trainings ratio			-.69 *** (.17)	-.41 * (.20)
Fixed interaction effects
Feedback (Type B) X Level 2				−2.39 (1.29)
Feedback (Type B) X Level 3				−3.30 * (1.46)
Feedback (Type B) X Item feedback				-.08 (1.15)
Feedback (Type B) X Rule feedback				-.18 (1.57)
Feedback (Type B) X Failed trainings ratio				-.88 ** (.34)
Random effects
Classrooms (Intercept)	14.54	3.13	2.98	2.73
Students (Intercept)	40.07	20.98	21.61	21.51
Residual	161.83	148.91	145.96	145.24
Overall model
N (L3: Classrooms)	53	53	53	53
N (L2: Students)	457	457	457	457
N (L1: Sequences)	2450	2450	2450	2450
ICC (Classroom)	.07	.02	.02	.02
ICC (Students in classrooms)	.19	.12	.13	.13
AIC	19,776.44	19,481.89	19,451.80	19,448.76
BIC	19,799.66	19,597.96	19,602.70	19,628.68
logLik	−9884.22	−9720.94	−9699.90	−9693.38
Pseudo R2 (fixed)	.00	.15	.17	.17
Pseudo R2 (total)	.25	.27	.29	.29
Notes: Unstandardized coefficients for fixed effects with standard errors in brackets; standard maximum likelihood estimation; levels of significance: * … p < 0.05, ** … p < 0.01, *** … p < 0.001; reference category for course estimates is the course “conditionals”; (a) For lack of space the range of estimates is displayed in the table instead of the estimate for each course.

It is important to note that the dependent variable's (follow-up mastery assessment in the test-retest-sequence) score ranges from 0 to 100. For example, the school type (lower vs. higher track of the secondary school system) significantly impacts 4 points on the follow-up mastery assessment. Given that the short courses in the web app are highly aligned to the curriculum and teachers only selected courses that match the enacted curriculum in their classroom, it is reasonable that grade does not matter. Model 1 also shows that course content and the prior mastery assessment score explain the dependent variable variance. The estimates for the courses vary due to their difficulty level. As expected, the mastery assessment score has the highest regression estimate since it is an indicator for prior knowledge in the test-retest-sequence. One point increase in the prior mastery assessment translates to a 0.29 point increase in the follow-up mastery assessment.

Model 2 includes all variables as fixed main effects. The significant coefficients for school type, test score, and level 3 remain stable. Item feedback, reading rules and examples, and the failed training ratio show the hypothesized associations with learning gains in the test-retest-sequence. However, model 2 reveals a negative impact of clicks between rules and examples and student achievement. It seems that switching between rules and examples rather indicates a student's confusion or helplessness.

The feedback condition has no significant main effect on the follow-up mastery assessment score (model 2). However, Table 3 showed that the retest score means significantly differs by feedback condition on level 3 of the web app. For this reason, model 3 included fixed interaction effects between the experimental conditions and level. Furthermore, model 3 also includes fixed interaction effects between feedback conditions and other learning variables that differ by level and condition (Table 3) to control mediation effects. The results of model 3 confirm the negative impact of the self-referenced feedback condition (Type B) on retest scores at level 3 of the courses. In sum, the results presented in Table 3 and Table 4 show that the reward-based feedback condition outperformed the self-referenced feedback message with small though increasing effect sizes from level 2 to level 3.

6. Discussion
This paper presented an experimental study on the effects of reward-based vs. self-referenced feedback messages on learning behavior and achievement gains in sequences of mastery assessments and follow-up assessments. Before discussing the results, this section first evaluates the impact of this study's methodological strengths and weaknesses on the results' internal and external validity.

An argument for the internal validity of the findings is the randomization procedure and the number of observations. However, only the two feedback conditions likely explain the above-presented differences. The experimental setting excludes any personal or situational characteristics as causes. However, the effects are low, and the regression model explains only about a quarter of the variance in follow-up mastery assessments. The low explained variance indicates that many relevant aspects are not included, such as person variables (e.g., the student's socioeconomic status), learning variables, classroom content, and the web-app implementation. For example, one major limitation of the findings' internal validity is that reading times do not necessarily reflect understanding. However, this is a general problem discussed in the learning analytics community (e.g., Tempelaar, Rienties, & Giesbers, 2015). On the one hand, log-data are an objective and precise measure of what students in the web app might pay attention to. They are widely used in learning analytics studies to capture student learning behavior and feedback usage in e-learning environments. For example, Timmers and Veldkamp (2011) used the number of feedback pages opened following correctly or incorrectly answered questions to measure the attention paid to elaborated feedback. On the other hand, time spent with digital learning materials or clicking behavior might overestimate student engagement and the quality of student learning processes.

The study's external validity determines how the results apply to similar formative mastery assessment applications. An argument for high external validity is that the teachers implemented the web app in their authentic classroom context. Although this study did not measure any implementation conditions, the number of classrooms and teachers is sufficient to guarantee some natural implementation variance. One threat to the external validity is the web app's uniqueness, which served as a container for investigating the feedback effects. However, this study's feedback situation is very similar to feedback situations in digital learning environments with short, formative tests that result in feedback on mastery. Thus, the findings presented in this paper might have practical implications for designing feedback strategies in online courses.

Moreover, the sampled schools and classrooms in this study determine the results’ external validity. Although this study includes schools from different school types and the sampled schools are widely spread, classrooms from the lower track of the German secondary school system are overrepresented. Consequently, the results are more likely to reflect the learning behaviors of a low-achieving student population, and the results might not be valid for high-achievers and students with effective learning strategies. Lastly, the external validity of the results is restricted since teachers voluntarily participated in the study. Consequently, teachers with higher digital teaching experiences and classrooms with a sufficient digital infrastructure are likely to be overrepresented.

Considering the above-discussed strengths and limitations of the research design, the findings contribute to our understanding of the feedback effects in learning environments with formative assessments. This paper showed that reward-based feedback outperformed the self-referenced feedback in the advanced level of the online courses. And it is noteworthy that the effect size of the reward-based feedback on level 3 of the courses has to be considered medium even though the experimental variation in this study was relatively tiny. The difference between both conditions is a rather short feedback message following the failed mastery assessment. All the other components (item feedback, rule feedback, course overview, training messages, etc.) were equal for both experimental conditions. Against this background and considering the randomization procedure, the effect of the reward-based feedback seems to be a robust finding. Moreover, it contributes to recent studies on gamification design principles showing that more simple feedback strategies are adequate to increase student performance in e-learning (Petrovic-Dzerdz, 2019; Ortega-Arranz et al., 2019).

Interestingly, the self-referenced feedback had some impact on learning behavior within the test-retest-sequence. Students in this experimental condition were more inclined to read the elaborated feedback messages. They had lower failed training attempts which might indicate that they were more inclined to work through compulsory training thoughtfully (e.g., making less use of the check button, less trial-and-error). Therefore, it is likely that the self-referenced feedback message directed student attention to relearning the rules that a learner not correctly applied in the mastery assessment. This might affect the attributional judgments triggered by the feedback message (Timmers et al., 2013). But unfortunately, this did not result in positive achievement gains measured by the follow-up mastery assessment. The result is similar to the study of De Sixte et al. (2019). They found that the warm elaborated feedback increased post-feedback learning behavior (reading texts) but did not affect performance in subsequent assessments.

An explanation might be that the web-app components for relearning (elaborated item feedback, rules, examples, training) were not very helpful for struggling students. Furthermore, the self-referenced feedback message's motivational effect declines when the difficulty of rules and assignments increases in level 2 or 3 of the courses. If a struggling student experiences multiple failed test repetitions, this student might perceive the self-referenced feedback message as a discouraging statement. Hence, the well-meant motivating feedback message diverts attention from the task and process level (e.g., Kluger & DeNisi, 1996; Shute, 2008).

The effect of the reward-based feedback message in more advanced online course levels is in line with some of the field's empirical findings. For example, several studies showed that struggling students benefited more from test repetitions than learning activities that followed a failed mastery assessment (Bokhove & Drijvers, 2012; Pelkola et al., 2018; Faber et al., 2017). The reward-based feedback motivates learners to repeat the tests since this will increase their overall score in the course. Previous studies on reward-based feedback also found that awarding points increased students' perceived effort in the course (Attali & Arieli-Attali, 2015) and their engagement with complex tasks (Hew et al., 2016; Ortega-Arranz et al., 2019).

The results confirm the mixed effects of personalized feedback studies to date (e.g., Chan & Lam, 2010; De Sixte et al., 2019). The design of efficient feedback strategies for online courses with formative mastery assessments remains challenging. Nonetheless, the findings will result in some practical recommendations for revising the web app. For example, only a few students read the rules and examples module after a failed mastery assessment. This result is in line with previous research that described inefficient student learning behavior in response to failed mastery assessments (Bokhove & Drijvers, 2012; Pelkola et al., 2018). Consequently, future revisions of the web app should continuously improve the learning activities after a failed mastery assessment (e.g., video tutorials). The findings also indicate revising the feedback strategies. One suggestion is combining reward-based and self-referenced feedback messages to gain more significant effects on learning behavior and achievement. Lastly, I will also think about the available information for generating the personalized feedback message. A revised version of the web app should include student characteristics (e.g., self-efficacy), learning behavior measures (e.g., reading vs. not reading instructions in the course), and previous achievement gains to generate personalized feedback messages to keep students motivated and on track.

