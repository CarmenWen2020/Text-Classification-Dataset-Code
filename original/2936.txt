Immersive virtual reality learning environments (VRLEs) have great potential but they do not guarantee effective or in-depth learning. Activating learners by including prompts, has been reported to increase learning in VR. Based on their promising effects in conventional settings, elaboration prompts should increase learning outcomes by activating learners to explain the learning content in their own words or to contrast it to prior knowledge. In our study, we investigated the effect of an elaboration prompt before entering the VRLE on learning robotics. Our between-subject study (prompt vs. no prompt) included N = 64 participants. We applied a multi-method approach (functional and structural) to measure mental models. We used a differentiated learning outcome measurement (knowledge, comprehension, and application) as a functional measurement. By using the structural assessment of knowledge technique, we deduce the structural accuracy of the developed mental model. We found an increase by the elaboration prompt on knowledge level but not on comprehension or application level. The interplay of pre-accuracy and prompt had a significant impact on structural accuracy after learning. Application level and structural accuracy after learning were positively correlated. Our multi-method approach delivered insights on different aspects of the developed mental model when prompting before entering the VRLE. For future research, further approaches to explore in-depth learning in VRLEs are recommended.

Previous
Next 
Keywords
Virtual reality

Multimedia learning

Elaboration prompt

Mental models

Structural assessment of knowledge

1. Introduction
Learning scientific content or technical concepts is often abstract, not intuitive or easy to access. Using virtual reality learning environments (VRLEs) is one promising approach to depict technical concepts for higher education learning (Wu, Yu, & Gu, 2020). Virtual reality (VR) is defined as 'a computer-mediated simulation that is three-dimensional, multisensory, and interactive, so that the user's experience is ‘as if’ inhabiting and acting within an external environment' (Burbules, 2006, p. 37). For instance, VRLEs have been used to create situated and realistic learning environments where learners can experience micro or macro phenomena otherwise hard or impossible to include in an everyday university setting (Radianti, Majchrzak, Fromm, & Wohlgenannt, 2020; Calvert & Abadia, 2020). When learning with simulations in VR, this allows learners to experience the learning content in a multisensory way. Particularly, by using immersive VR, including a head-mounted display with a stereoscopic view, the VRLE is experienced from the inside and offers the possibility to fully immerse oneself into the learning environment (Cummings & Bailenson, 2016).

Many previous studies aimed to explore the potential of VRLEs and thus compared learning in immersive VR with desktop VR or conventional learning material (Howard & Lee, 2020; Klingenberg et al., 2020; Parong & Mayer, 2020). Heterogeneous effects were reported: The meta-analysis of Wu et al. (2020) summarized 35 studies investigating the effects of immersive VR. The overall effect indicated that learning in immersive VR was more effective than other learning settings. However, they reported that 34% of the studies showed no or even adverse effects. For instance, Makransky, Andreasen, Baceviciute, and Mayer (2020) described the beneficial effects of learning in immersive VR on liking but not on learning outcome. Moreover, Parong and Mayer (2020) describe that learning with powerpoint slides was more effective than learning in immersive VR. In addition, Baceviciute, Mottelson, Terkildsen, & Makransky (2020), describe that reading in VR led to a higher retention than listening to an auditory text. Based on prior findings, one can conclude that there are many influencing factors for learning successfully in VR: the learning content and its presentation, the instructional design, and the learners' prior knowledge and abilities.

Hence, VRLEs do not per se guarantee successful learning. Therefore, the question arises how to support learners to turn the potential of VRLEs into an actual in-depth learning approach. Generally, there are two possible ways for such an approach: on the one hand, learners might be supported by providing relevant information as pre-training before learning, i.e. the support specifies what to learn (Meyer, Omdahl, & Makransky, 2019). On the other hand, learners might be supported on a strategic level, i.e. the support specifies how to learn. Parong and Mayer (2018) investigated one approach of strategic support: They asked learners to summarize the learning content after each VR learning session, which resulted in a beneficial effect on learning outcome. However, summary prompts means that learners must have an additional workstation and may have to switch back and forth between immersive VR (which takes a larger space to interact) and their desk. Depending on the implementation and the given learning situation, this can be an organizational challenge and rather costly.

Therefore, we aimed to investigate an approach that includes the advantages of both approaches by including an elaboration prompt, before entering the immersive VRLE. Thus, we offer strategic support, which can be easily generalized to different learning contexts and environments, but which is nevertheless provided beforehand not to interrupt the learning process in the VRLE. But, what is an elaboration prompt? In general, prompts aim to activate and stimulate learners either on a meta-cognitive or cognitive level to foster related learning processes (Bannert & Reimann, 2012; Gentner & Seufert, 2020). More precisely, cognitive prompts can be used to activate cognitive learning processes in a targeted manner. This can then result in a more in-depth elaboration of the learning content. As this is what we are aiming to, we use cognitive prompts that particularly foster in-depth elaboration of the learning content. These prompts are also referred to as elaboration prompts (e.g. Endres, Carpenter, Matrin, & Renkl, 2017; Loibl & Leuders, 2019; further details see 3.3).

We used a multi-method approach to explore the positive effects of our elaboration prompt on learning. Particularly, we investigated the prompt's effect on the development of a mental model containing abstract technological concepts. On the one hand, our focus was on learners' performance or more precisely on the extent to which learners are able to answer questions on different levels of learning outcome (knowledge, comprehension, application; Bloom, 1956). This can be described as a functional approach to measure mental models. Furthermore, we measured which knowledge structure was given before and after the learning unit. By using the structural assessment of knowledge technique, we determined learners' structural accuracy compared with the respective counterpart, based on expert ratings (Trumpower & Vanapalli, 2016; for further details see appendix A). Furthermore, as the development of a mental model also depends on learners' aptitudes, we also considered these in our study.

2. Mental models and learning in virtual reality
To display the learning content in VR, different information sources are often combined (e.g. Parong & Mayer, 2018). The beneficial effect of combining pictorial and textual components (multimedia effect) has been well researched and established when learning with conventional material (Mayer, 2009). Therefore, using multiple and different representations might also be promising in VRLEs. When learning successfully, this information is built up to an accurate, coherent mental model (Mayer, 2014; Scheiter, Schüler, & Eitel, 2017). On the one hand, VRLEs offer substantial learning opportunities because learners are able to choose their preferred position to process the textual and pictorial representations. Furthermore, learners are able to explore the learning content by interacting with the given objects in the virtual world. On the other hand, this might result in simply watching the learning material superficially. Hence, to understand potential challenges while learning in VR, the underlying cognitive processes need to be considered.

The Cognitive Theory of Multimedia Learning (CTML) by Mayer (2014) offers insight into the perception, processing, and integration of learning content. This theory describes processing depending on different codes (text or picture) or different modalities (visual or auditory; Paivio, 1990). If learning content is presented in different codes, this goes along with processing via both channels and the probability of successful memory retrieval is increased (Paivio, 1990). For instance, while learning in VR, spoken text, animations, or pictures of the learning content are combined to trigger this beneficial effect (e.g., Parong & Mayer, 2018; Radianti et al., 2020). While processing the learning content, first, two separate models, the verbal and the pictorial model, are built from the different information sources (Mayer, 2014). At the end of the learning process, these two mental models are integrated into one coherent mental model. While processing both verbal and pictorial information building one coherent mental model is the main goal. Johnson-Laird (1981) described mental models as internal models of the world which are analog in nature and are set up based on perception and information processing. Based on these developed mental models, learners are able to answer questions on different skill levels.

To understand the integration process of texts and pictures into the mental model, the Integrated Model of Text and Picture Comprehension (IMTPC) provides more details (Schnotz & Bannert, 2003). Because pictures are analog in nature, no additional translation process is required after processing the information semantically to include them into their analog counterpart, the mental model (Schnotz, Böckheler, & Grzondziel, 1999). In line with other text comprehension models (e.g. Newmeyer, 1996) the IMTPC describes an additional translation process when including verbal information (written or spoken) into the mental model. After processing sub-semantically, an internal representation results based on the given text syntax. When verbal information is then semantically processed, this results in propositional representations (Schnotz & Bannert, 2003). To include these propositional representations into the analog mental model, these need to be translated. Hence, there is a fundamental distinction between propositional representations and mental models. Propositional representations are described as strings of symbols with an arbitrary syntactic structure while mental models are analog in nature (Johnson-Laird, 1981). Furthermore, the IMTPC describes a possible shortcut between the internal text surface representation and the mental model. Hence, it assumes this superficially processed textual information can also be stored and integrated into the mental model without being further processed semantically (Schnotz & Bannert, 2003).

2.1. Measuring mental models
To answer the question of how to measure a mental model, these need to be first characterized in more detail. As outlined earlier, mental models represent an external world's internal model (Johnson-Laird, 1981). Although there is a basic consensus about mental models' analog nature, different definitions emphasize the mental model's more specific characteristics (Hong & O'Neil, 1992). For instance, Rouse and Morris (1986) described mental models as a specific type of knowledge in the respective field of interest in contrast to a more general view of knowledge. White and Frederiksen (1990) defined mental models as knowledge structures that incorporate different types of knowledge: declarative, procedural, and a control structure. Another definition described the term mental model to be connected with the organization and structure of the processed knowledge (Kraiger, Ford, & Salas, 1993). Hence, based on these mental models, certain functions of concepts can be deduced and future events anticipated (Rouse & Morris, 1986). Consequently, when learning in VR different aspects of the concept mental model can be used as indicators for learning success. The mental model's resulting structure represents a more abstract part of the mental model, giving information about relationships and distances between the included concepts without relying on direction or describing concrete consequences. When measuring mental models by a multi-method approach, this might uncover different aspects of mental models: On the one hand different levels of information processing of declarative knowledge (semantic or sub-semantic) might be measured by a functional approach. On the other hand, the resulting mental model's configuration or organization structure including distances and relationships between different concepts might be uncovered using a structural approach (Kraiger et al., 1993).

Therefore, both functional and structural approaches were chosen to assess the quality of learners' mental models in our study. To measure mental models' functional aspects, performance tests with questions differentiating sub-sematic and semantic processing are used. Bloom's (1956) first level (knowledge) reflects sub-semantic processing and thus learners' performance on simple recall of terms or definitions. In contrast, higher levels (comprehension and application) require semantic processing (Bloom, 1956). To answer comprehension questions, learners must predict the effect of manipulating one component onto another. For application, a more global understanding is necessary. Different components, their relating cause-and-effect chains, and their connection to already stored knowledge must be correctly combined and applied to answer these questions. Using this taxonomy leads to insights into the connection of the concepts on a functional level but does not deliver insights into the structure of the mental model itself.

To analyze the mental models' structure, the structural assessment of knowledge technique (SAK; Kraiger et al., 1993) can be used. SAK is a summative assessment to investigate the resulting knowledge network's accuracy and it's undirected connections and distance between included concepts (Jonassen, 1995). The SAK and the resulting structural accuracy are specified based on the rating of pairs of concepts for their undirected connectedness (Trumpower & Vanapalli, 2016). These ratings can be compared to experts' knowledge networks and accuracy can be deduced (Acton, Johnson, & Goldsmith, 1994). For instance, the expert might rate the relationship between the concepts actuator technology and sensor technology as with a 2, standing for a rather weak relationship and a high distance between these concepts. When participants rate these concepts to be strongly related in contrast to those of the experts, this is reflected in a low accuracy score (for further details see appendix A). This technique has been successfully used in the field of learning science to examine the structure of the resulting mental model (Trumpower & Vanapalli, 2016; Wouters, van der Spek, & van Oostendorp, 2011).

2.2. Fostering in-depth learning processes by using prompts
The theory of generative learning activities described by Fiorella and Mayer (2016) refers to an active learning process, including making sense of the learning content by integrating new information into the pre-existing mental model. Prompts can support generative learning activities, for instance, when they ask learners to learn by summarizing (e.g. Parong & Mayer, 2018). Prompts are described as strategy activators because they support specific cognitive strategies (Reigeluth, 1983; Bannert & Reimann, 2012). Hence, they increase learning outcomes and support the development of a mental model (Bannert & Reimann, 2012; Berthold, Eysink, & Renkl, 2009; Berthold, Nückles, & Renkl, 2007). Additionally, prompts might enable learners to transfer what they have learned to new contexts.

Studies have described different realizations of prompts in the learning context. For instance, learners were asked to take notes, think of examples of their everyday life, or conduct short summaries (e.g., Bannert & Reimann, 2012; Parong & Mayer, 2018; Endres et al., 2017). All these prompts aim to foster in-depth in contrast to superficial and non-semantic learning processes and they activate the elaboration of the learning content. Therefore, these are also named elaboration prompts. In one study, learners were activated to connect and compare relevant concepts of the learning content. Learners in the elaboration prompt group outperformed the learners in the control group (Endres et al., 2017). Including explanatory components, which activate the learners to process the content deeply and rephrase the relevant concepts in their own words, is particularly effective (Berthold et al., 2009; for further details see 3.3).

Few studies have examined the effects of prompting in VRLEs. In this context, learners were encouraged to interact with a VRLE in which learning content was repeated and important concepts were outlined by the prompts (Kamarainen et al., 2018), or learners were asked to interrupt the VR experience to write short summaries (Parong & Mayer, 2018). The beneficial effects of these prompts on learning outcome were described and uncovered in simple recall and higher learning outcomes. These prompts have in common that they were included in the learning unit and require specific adaption to the respective learning unit. Reflecting on these findings, one promising and very flexible approach to activate learners is to use elaboration prompts, including an explanatory component before entering the VRLE. As outlined before, when adding the elaboration prompt into the instruction, these do not have to be adapted to the specific VR learning content and do not interrupt the learning process and are very flexible to implement.

Our study investigated this beneficial effect of using an elaboration prompt before entering the immersive VRLE. To provide insights into the resulting mental model, we used the aforementioned functional and structural approaches: learning outcome measures on different levels and structural accuracy measured by SAK. Furthermore, we considered specific aptitudes of the learners to control their impact on developing a mental model. Pre-existing prior knowledge influenced the mental model's development and learning outcome (e.g., learning outcome based on Blooms' taxonomy: Seufert, 2019; Lehmann & Seufert, 2018, and structural: Riemer & Schrader, 2016; Trumpower & Vanapalli, 2016). Therefore, prior knowledge might impact the beneficial effect of the prompt. Furthermore, learners' abilities, namely verbal ability, was described as crucial for the verbalization of the developed mental model: The influence of verbal ability on different learning outcome levels (knowledge, comprehension, and application) is well proven (Wallen, Plass, & Brünken, 2005). As verbal ability has been described to impact accuracy scores significantly, we also considered this aptitude in relation to our structural approach to measuring mental models (Jonassen, 1995).

2.3. Present study
While learning in VR, learners are challenged by processing and integrating all relevant information in one coherent mental model. Therefore, we implemented an elaboration prompt to foster the semantic processing of the learning content (Bannert & Reimann, 2012; Endres et al., 2017). Hence, we expected the elaboration prompt to particularly impact the learning outcome's higher levels because these reflect semantic processing. Our expectations were as follows:

-
We hypothesized that a significantly higher learning outcome would be observed for the group with prompt than in the group without prompt for all three levels of Bloom (1956; knowledge, comprehension, application; H1a). Additionally, we expected the elaboration prompt to cause a larger beneficial effect on the higher learning outcomes, namely, comprehension and application, compared to the knowledge level (H1b).

Furthermore, we investigated whether the elaboration prompt had beneficial effects on the structural accuracy of the resulting mental model (Kraiger et al., 1993; Trumpower & Vanapalli, 2016). As building connections is highly dependent of the level and sophistication of pre-existing knowledge networks, we assume that the beneficial effect of prompting on the post-accuracy score depends on the pre-accuracy score (Kraiger et al., 1993; Scheiter et al., 2017; Trumpower & Vanapalli, 2016). Hence, we expect the following effect:

-
We hypothesized that the elaboration prompt's beneficial effect on structural post-accuracy of learners' mental model depends on the pre-accuracy (H2).

Previous findings outlined, that for both, high structural accuracy and answering questions on the application level require a more global understanding of the connectedness of the different components, compared to the knowledge and comprehension level, these two measurements are expected to be related (Bloom, 1956; Davis, Curtis, & Tschetter, 2003; Kraiger et al., 1993; Trumpower & Vanapalli, 2016). For instance, Goldsmith, Johnson, and Acton (1991) described a positive relationship between structural accuracy and learning outcome. Thus, based on theoretical assumptions and prior findings we assume as follows:

-
We hypothesized a significant positive correlation between the mental model's accuracy and learning outcome based on the application level (H3).

To gain further insights into the processes while learning in VR, we investigate as an aditional exploratory research question, how the elaboration prompt affects learners' distribution of attention in the VRLE. To quantify the learners' attention, we explored the time the robot was focussed in the learners’ field of view. As our analysis was based on a very brief measurement of attention based on head-tracking data, no specific hypothesis was postulated.

3. Material and method
3.1. A priori power analysis
To determine the necessary sample size, an a priori power analysis was performed. To estimate the effect size, we based our analysis on the study of Endres et al. (2017). They uncovered a large beneficial effect of an elaboration prompt on higher learning outcomes (d = 0.62). Based on our power analysis (d = 0.62; α = 0.05; power = .80), the necessary sample size was N = 66 (G*Power 3.1.9.4; Faul, Erdfelder, Buchner, & Lang, 2009).

3.2. Participants and design
In this study, we tested an initial sample of N = 67 participants. We had to exclude three participants due to technical problems. The remaining 64 participants were mainly university students studying psychology. The majority of the participants were female (70%) and ranged from 18 to 48 years of age (Mage = 24.05; SDage = 6.05). The design of our immersive VR-study was a between-subject design: Participants were randomly assigned to one of the two experimental conditions based on a computer-generated randomization list (RandList 1.2; Ellwanger & Luedtke, 2001) with (n = 31) or without (n = 33) an elaboration prompt. As dependent variables, we measured the learning outcome on the three levels: knowledge, comprehension, and application. Additionally, accuracy scores based on the mental model structure were measured by the SAK, both before and after learning. As covariates, we considered learners' aptitudes, namely, their prior knowledge (functional and structural) and verbal ability. For further exploration of the VR learning experience, we assessed learners' perceived presence in VR and attention distribution. Our analysis was based on head-tracking data. This brief measurement of attention was used to gain insights into attention distribution and interaction with the VRLE.

3.3. Learning material and the elaboration prompt
The auditory learning unit consisted of two main topics: On the one hand, it contained details on the robot, for instance, its technical specification; on the other hand, rather abstract content such as basic kinematic definitions were included. To foster learners' engagement in the topic, we chose the robot as the narrator (‘Hello, I am a service robot for home use …’). The robot described its height, weight and the potential use of the platform on its back. Additionally, it provided more details about the general distinction between industrial and service robots. The robot gave insights into its main function, which was cleaning the floor. It described the necessary technical components to conduct its cleaning task. For instance, it described its sensors and safety protocols (‘First, I capture my environment through various sensors and a camera … ’). This explanation also included details about data processing, its fusion and its transformation into appropriate reactions. Furthermore, the robot provided information about its general moving trajectory and its relation to the more abstract concept of kinematic (‘I can move in a straight line, which is also described by the term … ’). Next, the robot provided more details about its locomotive system and the underlying control circuitry. For instance, it described the particular controller (PID controller) and its connection to movements and driving speed. At the end of the learning unit, the robot explained differences in local and global navigation and how this was related to its cleaning task (‘ … in global navigation, I have a more comprehensive environment model and try to reach my cleaning goal without driving into unnecessary dead ends … ’).

While listening to the robot, the learners saw the robot as a VR animation, allowing them to visually perceive some aspects of the learning content (see Fig. 1). This animation contained information on the robot, its component, and its moving trajectory, and its purpose was to stimulate the learners to elaborate on the learning content deeply. Hence, the learning unit focused on different types of knowledge. Both, the robot's narration and VR animation of the robot contributed to the knowledge acquisition. For instance, included facts (e.g. height and weight of the robot) reflected the learning outcome's knowledge level (Bloom, 1956). The distinction between related concepts reflected the comprehension level (e.g. Which aspects are taken into account for global instead of local navigation?). The application level included a more general understanding of different components (e.g. What happens if the distance between the current pulses in the drive is shortened?).

Fig. 1
Download : Download high-res image (388KB)
Download : Download full-size image
Fig. 1. Virtual reality learning environment displaying the service robot.

The animation was a VR simulation programmed with Unity (version 2019.2.0f1). The learners were able to choose their exploration perspective and move in the VR. By using the controls, they were able to interact with the surrounding environment. To get to know the learning environment, learners had to perform a short sorting task of dishes in the kitchen environment. However, the learners were not able to manipulate the robot. A 3D model of the humanoid robot REEM from the manufacturer PalRobotics was used to display the virtual robot. REEM was placed in a kitchen environment and automatically entered the room. The task of the learners was to listen to the robot and observe its behavior. Because the study contained a second and separate part investigating human–robot interaction, the robot model was slightly adapted to be slightly larger than its real counterpart (1.75 m instead of 1.6 m). In addition, a mouth was added to the robot's face to make it appear friendlier in the VR. The VRLE was displayed by using an HTC Vive Pro. Previous findings outline that stereoscopic view and wide fields of view of visual displays are essential to ensure the participants feel immersed in the VR environment (Cummings & Bailenson, 2016).

To activate learners, we included a prompt in our study before learners entered the VRLE. Prompts might trigger different cognitive learning processes by inducting generative learning strategies (Fiorella & Mayer, 2016; Bannert, 2009). The chosen prompt was developed based on theoretical considerations and adapted based on prompts that were used in previous publications (Bannert & Reimann, 2012; Berthold et al., 2009; Endres et al., 2017):

'You will now get to know the robot better. You will also learn some details about robotics. Pay particular attention to concepts and terms that are new to you and how they are related to each other. Imagine that you have to explain them to someone else who has no prior knowledge in this field.'

The developed prompt included different elements, that aimed at stimulating the elaboration of the learning content. First, we aimed to activate learners to recall the most important keywords and definitions with the chosen elaboration prompt. For that purpose, we included an explanatory component (Bannert, 2009). The publication of Berthold et al. (2009) outlines the effectiveness of including such an explanatory component into a prompt on both, procedural knowledge and conceptual understanding. Based on this finding, we asked our learners to prepare a peer-explanation at the end of the learning unit. For the peer-explanation, learners are required to recall the most important concepts and definitions and hence this might trigger learning outcome on the knowledge level (Bloom, 1956). As claimed before, our aim was not simply to induce superficial learning but to stimulate in-depth processing of learning content. Hence, we included a second aspect into the developed prompt: Learners received a hint, that the presented concepts are also related in a meaningful way. This part of the prompt was based on the study of Endres et al. (2017). They used one elaboration prompt to stimulate learners to think of potential connections or conflicts of the new information from the learning unit and their pre-existing knowledge. Knowing about meaningful connections and understanding the relationships between concepts, or deduce potential conflicts, reflects learning outcome on the comprehension level (Bloom, 1956). Hence, by including such a component into our elaboration prompt, learners were stimulated to go beyond superficial processing and to process the learning content semantically (Schnotz & Bannert, 2003). The third aspect of our developed prompt was that the peer-explanation does not only require rephrasing the basic concepts but is challenging as the peer does not have any prior knowledge in the field of robotics. Hence, learners can not simply rely on repeating the phrases of the learning unit but need to translate the learning content in their own words. This requires processing the learning content semantically and for instance search for synonyms in their mental network of prior knowledge or contrasting the new concepts to concepts that they already know (Schnotz & Bannert, 2003; Endres et al., 2017). This third aspect of our elaboration prompt reflects learning outcome on the application level (Bloom, 1956). Furthermore, the fact that learners are triggered to build meaningful connections, which requires translating and integrating the new concepts into their prior knowledge network, should stimulate them to build a more accurate structural knowledge network (Kraiger et al., 1993; Bannert & Reimann, 2012).

3.4. Questionnaires
In a short online questionnaire, participants were asked for their gender, age, educational level, the field of study, experiences with VR head-mounted displays and applications, prior contact with robots, and whether they own a specific type of robot. To measure verbal ability, we used the subcategory of the basic module A (similarities) of the IST2000-R (Liepmann, Beauducel, Brocke, & Amthauer, 2007). This test comprised 20 items (items 41–60) for which participants were instructed to mark two corresponding words in a list of six words. The decision should have been based on an obvious generic term suitable for two words of this list only. For instance, for the fictitious list ‘fork, butter, necklace, book, biscuit, and cigarette’ the correct answer is ‘butter and biscuit’ because both are food products.

The pre-test questionnaire included a measurement of prior knowledge to control different domain-specific prior knowledge levels between the experimental groups. This test aimed to measure domain-specific knowledge in science with a particular focus on relevant technical aspects related to robotics and further questions related to robots' use in real-life settings. The test comprised seven open questions (e.g., ‘What is the difference between control and regulation in the field of robotics?’ or ‘Name three different types of sensors used in the field of robotics’). To determine the rigor of the chosen questions, we determined the inter-rater reliability, which revealed a very high consistency between the two raters (r = 0.93, CI = 0.89-0.96). To further explore the structural pre-existing mental model of robotics' relevant concepts, we used the SAK technique (Trumpower & Vanapalli, 2016). SAK is based on a pairwise rating of relevant concepts for their connectedness without considering their relation's causality or direction. For each pair of concepts, learners had to rate the level of connectedness on a 7-point Likert scale. In the literature, many concepts ranging between 6 and 20 have been commended (Wouters et al., 2011). Based on these expert ratings, we selected the following 6 relevant concepts of robotics: 0: PID controller, 1: transition, 2: kinematics, 3: actuator technology, 4: sensor technology, and 5: environment model. Thus, overall, learners made 36 ratings (for further details see appendix A). As recommended in literature, we referred to experts to find appropriate SAK rating concepts (Riemer & Schrader, 2016). Additionally, we ensured that rating all concepts as highly connected did not result in a higher accuracy value by choosing the concepts respectively. Two researchers of learning and instruction that were experts at extracting relevant concepts from learning materials were recruited to choose appropriate concepts for the given learning material. They first analyzed the given learning content for potential concepts. Again, an expert of research on robotics and computer science provided feedback on the different concepts.

Furthermore, we developed a post-test for measuring learning outcomes on a functional level. The post-test comprised 12 open questions measuring the different learning outcomes: knowledge, comprehension, and application. To measure a learning outcome on the knowledge level, 4 recall questions were developed (e.g., ‘Which term is appropriate to describe that the robot is heading straight toward you?’). For the comprehension level, 4 questions were included (e.g., ‘What can a service robot do in contrast to an industrial robot?’), and for application, we developed 4 questions (e.g., ‘What happens if the distance between the current pulses in the actuator is shortened?’). Again, we determined the inter-rater reliability, which revealed a very high consistency between the two raters (r = 0.88, CI = 0.81-0.93).

As we used a head-mounted display enabling the learners to perceive the VRLE in 3 D and stereoscopic, we claimed our VRLE to be immersive. Hence, we included a manipulation check on how real the participants experienced the VR and their subjective presence. We used the Technology Usage Inventory (TUI) developed by Kothgassner et al. (2012). The scale comprised four items rated on a 7-point Likert scale (e.g., ‘Through the virtual simulation, I had the feeling of really experiencing the situation.’). Internal consistency was high, with Cronbach's α = 0.84 (CI = 0.82-0.85).

3.5. Procedure
This study started with an online questionnaire, which included demographic questions and assessed learners' verbal ability. The questionnaire was filled in at home by the participants before the testing session. At the beginning of the testing session, which was conducted in a laboratory at the university, each participant was informed of the study's procedure and signed an informed consent form before participating. The participants were informed that their data was handled confidentially and that they could withdraw their data or participation at any point in the study without having any disadvantage. Afterward, the participants filled out the pre-test questionnaire, namely, questions to assess their prior knowledge and the structure of their pre-existing mental model (SAK). This step was followed by a brief introduction to the VR equipment. Depending on the experimental condition, the participant received the elaboration prompt (see Fig. 2). To ensure that the participants understood and remembered the elaboration prompt, they had to repeat it in their own words shortly. After this, a short exploration phase in the VRLE was included. The participants conducted a short introduction task including sorting dishes to know the virtual world better as recommended in literature (e.g. Wu et al., 2020). This lasted about 2 min. Then the learning unit started as the robot entered the kitchen. After the learning unit, the participants filled in the post-test questionnaire, including the learning performance test, the post rating for the SAK, and the subjective rating or presence. Overall, the duration of the study was 2 hours. An additional part of investigating human-robot interaction was conducted after the described study and used one of the 2 hours.

Fig. 2
Download : Download high-res image (434KB)
Download : Download full-size image
Fig. 2. Procedure for both experimental groups, displaying researcher (R) and learner (L).

3.6. Data preparation
All questionnaires were presented by using the online survey tool Unipark. Data preparation was performed by using R 3.5.1 and RStudio 1.1.463. To analyze the effect of the elaboration prompt, the two experimental groups were dummy coded (0 = without, 1 = with prompt), and for calculating models, variables were z-transformed when necessary. We measured the learners' SAK before (pre) and after (post) the learning session. To determine the accuracy of ratings, four experts rated the relatedness of the concepts. The average similarity score between individual experts' models was very high (0.91; Acton et al., 1994; Wouters et al., 2011). To analyze the accuracy values, we used the software Jpathfinder and python code to prepare the data. The averaged relatedness ratings of the experts (median) provided the reference standard. The software Jpathfinder and its scaling algorithm were used to derive similarity measures of structural knowledge quality by comparing the expert network to the learners' individual network. The learners' accuracy scores, ranging from 0 to 1, were calculated based on the calculations. Higher values reflected a higher accuracy of the network than the expert network (for further details see appendix A). To explore the data quality of the accuracy ratings, these were analyzed for fixed patterns and outliers. One participant was excluded because the SAK ratings were an outlier.

4. Results
4.1. Descriptive results
Learners' domain-specific prior knowledge was on a medium level in both experimental conditions (Table 1). The learning outcome was on a medium level, both overall and on the knowledge and comprehension level. The learning outcome values for application were low (Table 1).


Table 1. Means and standard deviations of the experimental conditions.

With prompt n = 31
M (SD)	Without prompt n = 33
M (SD)
Prior knowledge (%)	51.44 (12.40)	47.21 (14.01)
Verbal ability (%)	58.01 (13.07)	53.00 (15.00)
Pre-accuracy (SAK, Max = 1)
Age (years)
Male (%)	0.42 (0.12)
24.10 (6.62)
30.30	0.40 (0.10)
24.21 (5.29)
38.71
To ensure that the two experimental groups did not differ significantly in their relevant characteristics, a MANOVA was conducted with age, prior knowledge, verbal ability, presence, and pre-accuracy for the rating of the SAK as dependent variables. No significant differences between the groups could be found (F(5,55) = 0.69, p = .694). Moreover, the groups did not differ in their gender distribution (X2(1, N = 64) = 0.01, p > .050). As previously mentioned, learners' prior knowledge and verbal ability play an important role in learning outcome. Therefore, these factors were included as covariates in the analyses. Because of missing data, six cases were excluded in these analyses when the verbal ability was included. Based on a conducted Shapiro-Wilk test, the multivariate normal distribution can be assumed for the variables learning outcome, prior knowledge, and accuracy (pre, post) for each experimental subgroup (p > .056), and the variances can be classified as homogenous based on Bartlett's test (p > .367).

4.2. Effect of the prompt on learning outcome
To test our first hypothesis, we analyzed the effect of the elaboration prompt on the functional level by differentiating the learning outcome on the knowledge, comprehension, and application level (Bloom, 1956). We expected a significantly higher learning outcome in the prompt group than in the control group on all three levels of the learning outcome (H1a).

The prompt group's means were higher for overall learning outcome, knowledge, and application compared to the control group, as can be seen in Table 2. For comprehension, the group without prompt showed a descriptively higher learning outcome. To test the prompt's main effect on overall learning outcome, we conducted an ANCOVA including verbal ability and prior knowledge as covariates. We found no significant effect of the elaboration prompt on overall learning outcome (F(1,55) = 0.19, p = .661). Furthermore, to analyze the elaboration prompt effects on each of the three levels of learning outcome, we conducted a MANCOVA (F(3,55) = 1.14, p = .172, η2partial = .059). When controlling for variance because of verbal ability and prior knowledge the results show a significant, beneficial effect of the prompt on learning outcome on the knowledge level, only (p = .036; see Table 3). No significant effects were found on comprehension and application level. Additionally, we expected the largest beneficial effect on the higher learning outcome levels (comprehension and application; H1b). The descriptive pattern is already not in line with our previous assumption because for comprehension the prompt had no positive effect. Therefore, this hypothesis is rejected.


Table 2. Means and standard deviations of learning outcome depending on the experimental condition.

With prompt
M (SD)	Without prompt
M (SD)
Overall learning outcome (%)	39.93 (12.62)	37.54 (12.78)
Knowledge (%)	66.41 (22.55)	58.87 (23.76)
Comprehension (%)
Application (%)	32.39 (16.61)
21.09 (15.48)	34.54 (16.66)
19.22 (17.00)

Table 3. Results of the ANCOVA depending on the level of learning outcome.

F(1,55)	p	η2partial
Knowledge			
Prompt	3.42	.036*	.042
Verbal ability	0.29	.296	.013
Prior knowledge	4.49	.019*	.075
Comprehension			
Prompt	0.03	.432	.006
Verbal ability	1.42	.119	.018
Prior knowledge	1.36	.124	.024
Application			
Prompt	0.03	.436	.007
Verbal ability	8.82	.002**	.120
Prior knowledge	1.97	.083	.035
Note. **p < .01, *p < .05.

4.3. Effect of the prompt on structural accuracy
To uncover the elaboration prompt's effects on the structural level, we measured the structural accuracy deduced from the SAK ratings after the learning session. In our second hypothesis, we expected that learners in the prompt group had a significantly higher post-accuracy score than the control group after accounting for pre-accuracy. In detail, we expected that the beneficial effect of the elaboration prompt depends on the pre-accuracy score (H2). Looking at the visualization of the post networks of the different groups, the configuration of experts, prompt group and control group seem to differ substantially based on post-accuracy ratings. The prompt group ratings' indicate that more connections and closer relationships between the individual concepts were assumed than in the expert or control group (see Fig. 3). For instance, concept 0 (PID controller) and 5 (environment model) were not connected based on experts rating while concept 2 (kinematics) was rated as highly connected with 3 (actuator technology). In contrast, learners in the control group rated the concepts 0 and 5 to be closer connected compared to 2 and 3. This results in lower post-accuracy ratings compared to the learners in the prompt group who rated 2 and 3 as closer connected compared to 0 and 5. To be able to interpret these differences, we calculated accuracy scores for each experimental group before and after learning in VR (see appendix A).

Fig. 3
Download : Download high-res image (195KB)
Download : Download full-size image
Fig. 3. Knowledge networks, before and after learning in VR contrasted to the experts' knowledge network; from left to right with increasing accuracy scores.

The accuracy means compared to the experts’ ratings were in both groups higher after the learning unit than the baseline measurement (see Table 4). Descriptively, the highest post-accuracy mean was found for the prompt group.


Table 4. Means and standard deviations of learning outcome depending on the experimental condition.

With prompt
M (SD)	Without prompt
M (SD)
Pre-accuracy	0.42 (0.12)	0.40 (0.10)
Post-accuracy	0.51 (0.13)	0.47 (0.14)
Overall, there was a significant increase from pre-to post-accuracy (t(62) = −3.31, p < .001, r = 0.79). As our research question aimed at answering the question of how the effect of the prompt on post-accuracy depends on the pre-accuracy score, we chose an ANCOVA including an interaction term as recommended by Wan (2019) for pre-accuracy and experimental condition (prompt vs. no prompt) as well as the covariate verbal ability (see Table 5).


Table 5. Results of the ANCOVA with the experimental condition, verbal ability, and pre-accuracy on post-accuracy.

F(1,52)	p	η2partial
Prompt	0.77	.192	.017
Verbal ability	2.05	.079	.027
Pre-accuracy	0.33	.285	.006
Prompt*pre-accuracy	3.11	.042*	.056
Note. *p < .05.

We found the expected significant interaction of the experimental condition and pre-accuracy on post-accuracy (F(1,52) = 3.11, p = .042, η2partial = .056). Therefore, the effect of the elaboration prompt on post-accuracy depends on the pre-accuracy. For better understanding this interaction, we visualized the effects of pre-accuracy by dividing learners into hypothetic groups with three distinct levels of pre-accuracy in Fig. 4.

Fig. 4
Download : Download high-res image (133KB)
Download : Download full-size image
Fig. 4. Post-accuracy depending on experimental group and pre-accuracy. Note: Pre-accuracy defined in low <mean – 1 SD: n = 4 with and n = 5 without prompt; medium = mean ± 1 SD: n = 21 with and n = 23 without prompt; high >mean + 1 SD: n = 6 with and n = 4 without prompt; the covariate verbal ability is constant with βverbal = 0.56.

Learners with low pre-accuracy benefit from the elaboration prompt compared to the group without prompt. For learners with medium and high pre-accuracy, this beneficial effect was not found.

4.4. Relationship of learning outcome based on performance and structural accuracy
Both dependent variables, learning outcome and SAK, aimed to measure learners' mental models by considering different perspectives. In our third hypothesis, we expected a significant positive correlation of the learning outcome with the application level and post-accuracy values (H3). In line with our expectations, application and post-accuracy showed a moderate, significant, positive correlation (r = 0.36, p < .010; Table 6). Neither knowledge nor comprehension level showed a significant correlation with post-accuracy scores.


Table 6. Relationship between learning outcome and structural assessment of knowledge (SAK).

SAKr
p
Knowledge	.19	.141
Comprehension	.09	.460
Application	.36	.004**
Note. **p < .01.

4.5. Effect of the prompt on attention distribution
To gain further insights into the VR experience of the learners, we measured their perceived presence and explored their behavior via head-tracking. The perceived presence was rather high in both groups (Mprompt = 5.52, SDprompt = 0.87, Mnoprompt = 5.14, SDnoprompt = 1.10). To ensure that learners focused on their learning task, we recorded their field of view while learning in VR. To quantify whether the learners were distracted by the VR environment, we analyzed the time when the robot was in their field of view completely, indicating that it was focused on by the learner. Due to technical problems, while recording the data, only videos of n = 42 learners were available. Based on the descriptive means, learners with prompt focused on the robot longer (MEG = 88%, SDEG = 18) than the learners without prompt (MCG = 79%, SDCG = 35). This difference was not significant (t(40) = −1.2, p = .117, d = 0.32).

5. Discussion
In this study, we investigated the effects of an elaboration prompt that aimed to activate learners to process the learning content deeply and thus semantically when learning in VR. The chosen prompt included elements to foster the elaboration of the learning content and pointed out that the concepts should not only be considered in isolation but that the deduction of possible connections between the concepts were also part of the learning process. Additionally, the prompt contained an explanatory component as learners were asked to prepare a peer explanation at the end of the learning session. To examine the prompt's effect on the resulting mental model, we used a multi-method approach: one functional approach with a differentiated learning outcome measurement (knowledge, comprehension, and application) and one structural approach by using the SAK technique. By this, we aimed to gain further insights into the developmental process of mental models in VRLEs.

5.1. Impact of the elaboration prompt on the functional and structural level
In our first hypothesis, we expected a beneficial effect of the prompt on the learning outcome, particularly for the higher levels of learning outcome: comprehension and application. In contrast to this expectation, the prompt led to beneficial effects on the knowledge level, only. Hence, the used elaboration prompt did not foster in-depth and semantic processing. As comprehension and application scores were low in both experimental groups, overall, very little semantic processing occurred. Thus, based on the present findings, learners were unable to build meaningful connections between the different concepts and were therefore unable to predict the consequences of manipulating one component onto another. Other studies have also described limited cognitive information processing when learning in VRLEs (e.g. Makransky et al., 2020; Makransky, Terkildsen, & Mayer, 2019; Wu et al., 2020). Whether VRLEs were effective learning environments depended largely on the boundary conditions such as the subject or the chosen sample (Wu et al., 2020). In our study, the limited semantic processing might be because the learning unit included many technical and abstract concepts. Consequently, the learners might have had insufficient resources to develop a causal mental model that would answer the questions on these levels (Bloom, 1956; Mayer, Mathias, & Wetzell, 2002). Hence, VRLE settings do not per se guarantee adequate semantic processing even when an elaboration prompt stimulated learners to process the content in-depth.

What about the structural level? Does the prompt enable the learners to have a basic understanding of the relationships between the concepts? We addressed this question in our second hypothesis. As expected, we found a significant interaction of prompt and pre-accuracy on structural accuracy after learning. Hence, learners' structural accuracy depended on both the quality of their pre-existing knowledge network and whether or not they received the elaboration prompt. Learners whose pre-accuracy was in the low or medium range, benefit from the elaboration prompt. An expertise reversal effect was found for learners with higher pre-accuracy when they were prompted. In line with prior findings, supporting learners with higher prior knowledge might interfere with their strategies or their pre-existing knowledge network (Kalyuga, 2009).

5.2. Coherence of the functional and structural approach
In our third hypothesis, we expected a positive relationship between the application level and structural accuracy. As hypothesized, we found a moderate positive correlation between application and structural accuracy. In line with our expectation, no significant correlation was found for knowledge or comprehension. Answering application questions includes deducing correct causal or directional connections of the components (Bloom, 1956; Schnotz & Bannert, 2003). By contrast, it is sufficient for the structural accuracy to recognize these concepts' pure connectedness (Kraiger et al., 1993). Therefore, both approaches require a more global understanding of the relevant concepts' connection in a more or less sophisticated mental model. Although the concepts have this in concordance, they differ significantly in the different prerequisites for the specificity of the different components' connections. The two first hypotheses' results reflected this: The elaboration prompt had beneficial effects on the knowledge level and on structural accuracy. As outlined before, high structural accuracy after learning reflects information on the relatedness of the different concepts without considering the details on the cause-and-effect relations. This information processing is also reflected in the IMTPC as a shortcut between the text surface representation and the analog mental model (Schnotz & Bannert, 2003). Our findings indicate that this shortcut was stimulated by the current learning setting and allows to deduce relations on a superficial level only. For instance, the information presented in the same sentence might be stored as more closely related than information with a greater syntactical distance. Based on our findings, the processing of this relational information was stimulated successfully by the elaboration prompt, but only by the shortcut because the elaboration prompt did not increase performance on higher learning outcomes and thus no semantic processing occurred.

5.3. Limitations and recommendations for further research
This study's results indicate a limited effect of the elaboration prompt on semantic processing of the learning content; thus, VRLEs and the learning processes in such environments must be further explored. In our study, we used one prompt in the instruction before entering the VRLE. As outlined earlier, different concepts of how and when to support learners by prompting exist. The chosen prompt did simply outline the necessity to learn the concepts and their connectedness by semantic processing but did not include further strategic support for cognitive or metacognitive strategies. For instance, to support cognitive elaboration, learners could be asked to connect the learned concepts to their lives or think of further examples (e.g., Endres et al., 2017). However, when learning abstract concepts combined with very low prior knowledge, finding adequate strategy activators remains a challenge. Even when learners are motivated to use the proposed strategies, they might not have the required skills or the provided strategic activators might interfere with their existing strategies. Further possibilities to support learners include using metacognitive prompts, which might activate and support learners' self-regulation processes and their metacognitive strategy use. By this, learners might be supported on semantic processing of the learning content (Gentner & Seufert, 2020). Additionally, studies have outlined the benefits of using a combination of cognitive and metacognitive prompts on learning outcomes (Berthold et al., 2007).

In our VRLE, we aimed to display the robot's natural behavior, for instance, its moving trajectory. To avoid the robot's intimidating effect, we implemented a security protocol that ensured a minimal distance of 1.5 m between the learner and the robot. Therefore, learners could manipulate the surrounding objects in the room and choose their position to observe the robot individually but were not able to directly manipulate the robot. The possibility to compose and decompose the different parts of the robot flexibly might foster a deeper understanding of the interplay of its technical components (Kamarainen et al., 2018). Hence, direct manipulation of the robot might be implemented in future studies.

Furthermore, future studies might gain further insight into the perceived load while learning in VR. The chosen learning content in this study was complex and rather abstract. Thus, a high overall cognitive load could have occurred (Sweller, 2011). Although the learners were activated to elaborate on the learning content, they might not have had the required cognitive resources. To provide insights into the perceived cognitive load while learning, further research should consider a differentiated cognitive load measurement (Klepsch, Schmitz, & Seufert, 2017). Additionally, the perceptual load could be considered for analyzing cognitive learning processes in VRLEs (Huang et al., 2019).

Moreover, because the score on comprehension levels and the application level were low, the respective questions should be revised critically. Different questions with more variance in their difficulty might offer further insights in the resulting mental model. Furthermore, SAK is described as a valid method to gain insight into the structural components of the relevant mental models (Kraiger et al., 1993; Trumpower & Vanapalli, 2016); nevertheless, its limitations must be considered. The process of choosing the concepts must be discussed. These concepts have been chosen based on expert ratings, a standard method to deduce relevant concepts. However, choosing these concepts included to some degree arbitrary decisions. Different concepts might result in different ratings and effects. Therefore, further research might include different concepts or different subsets of concepts to investigate whether our findings are replicable. One additional challenge using the SAK is to choose the concepts in a way that rating concepts as highly connected do not automatically result in high accuracy values. In our study, we ensured that this effect did not bias the results as based on theoretical consideration, developing a knowledge network reflects building a more organized and elaborated structure. Hence, learners need to be able to differentiate what is and what is not closely connected and not just simply describe a structure that includes more connections and is, therefore, more complex (Trumpower & Vanapalli, 2016, Haiyue & Yoong, 2010; Ifenthaler, Masduki, & Seel, 2011). To assure the expert ratings were of good quality, we analyzed the consistency of the expert rating, which was very high compared to that of other studies and more extensive than the required value (Riemer & Schrader, 2016). For structural accuracy, learners only needed to rate the given concepts ‘passively’ and were not asked to name or verbalize the concepts. This gap between the passive knowledge of concepts and the active retrieval of information from the mental model and the successful use has often been described. Publications have outlined the importance of using adequate learning strategies to overcome this gap and thus this might be considered when designing future studies (Fan, 2000).

Moreover, different approaches to investigate the structure and the developmental process of mental models might be used. For instance, the network elaboration technique of Eckert (2000) might deliver further insights into the semantic and sub-semantic processing of learning content. When using this technique, learners create their individual concept maps and specify the connections between the included concepts. Extending this essential measurement of the meaningful connection, other techniques that include a more detailed description of these meaningful connections would be used in the model of explanatory coherence approach (e.g., Read & Marcus-Newhall, 1993). Participants are asked to describe relevant scenarios and explanatory hypotheses related to the learning content. This enables the researcher to uncover underlying principles and gain deeper insights into learners' mental models (Thagard, 1989). However, these methods are very effortful and require very motivated learners to be able to interpret the results.

By assessing individual learners' aptitudes, other concepts such as motivation and the prompt's use could be investigated (Nückles, Roelle, Glogger-Frey, Waldeyer, & Renkl, 2020). Additionally, further cognitive abilities such as logical abilities or field-independence might explain individual differences while learning in VRLEs. Finally, our chosen sample size was based on our a priori power analysis following the recommendations of Faul et al. (2009). However, as our effect size was relatively small, we recommend a higher sample size in future studies and to conduct a power analysis including a higher test power. To gain further insights into the interaction of prompting and pre-accuracy on post-accuracy, we used three hypothetical groups. This approach involved an extreme group comparison and therefore contained few learners in some groups. Therefore, the described findings need to be further validated by higher sample sizes in future studies (Preacher, Rucker, Mac Callum, & Nicewander, 2005).

5.4. Conclusion
Before learning in VR, prompting the learners to elaborate on the learning content showed beneficial effects. Although, this manipulation of including a short elaboration prompt was a very brief intervention, it showed a substantial effect on the knowledge level and the structural level. Hence, based on our findings, elaboration prompts are recommended for future VRLEs, particularly for learners with lower to medium prior knowledge levels. Although the chosen elaboration prompt showed beneficial effects on the recall or knowledge level only, we outlined this level as a critical precondition for further semantic processing and a deeper understanding of the learning content. Using the multi-method approach to measure mental model enables further deduction of details on what learners processed semantically and how they organized this knowledge structurally. In our study, learners were not able to answer the application questions correctly. Hence, they could not build correct meaningful connections, even when being activated by the elaboration prompt. However, they were supported by the prompt in building a structural mental model with unspecific connections reflected by a less sophisticated mental model. Many challenges to developing adequate instructional designs and supportive elements for VRLEs remain for further research. This study outlines the importance of investigating the boundary conditions of learning in VRLEs to compare their effectiveness to conventional learning settings.