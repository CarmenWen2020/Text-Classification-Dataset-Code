Several gaps exist in the literature on coding. First, little exploration has focused on early elementary school students. In addition, close description of the overall context of coding tasks at this level is rare. Further, there is a need for both teacher and student voices around coding experiences to be heard. Moreover, a task engagement framework has not been used to evaluate the process or outcomes of early elementary coding tasks. Therefore, an exploratory holistic case study design was used to investigate student and teacher processes and outcomes of coding lessons in order to fill gaps in the literature. In this study, forty-six 2nd grade students, two teachers, and four researchers completed two one-week units on basic coding. Multiple descriptive and numeric data sources were employed to describe the process and outcomes of learning coding. Conclusions include: (1) teachers should start learning about coding first with short awareness sessions and then move to their own classrooms with knowledge brokers and other forms of assistance; (2) a focus on content and process, including problem-solving, is effective for coding with young children; (3) there can be a high level of engagement for teachers and students with the use of robots and welldesigned, age-appropriate coding tasks, and; (4) multiple data sources and the inclusion of both teacher and student data are essential in exploring coding in classrooms.

Keywords
Elementary education
Robotics
Coding
Teacher learning
Computational thinking 

1. Introduction
“Coding” generally refers to creating instructions that a computer-based entity follows (Buckley, 2019). With the current focus on STEAM (science, technology, engineering, art, and math) and a huge push from governments around the world, a focus on coding has proliferated in schools (Bers et al., 2019). In addition, education standards have begun to include coding as a required topic in school curricula to address computational thinking (see, e.g., Arfe et al., 2020; Mason& Rich, 2019), and an abundance of resources is available for coding support (e.g., code.org Hour of Code, Lego's Mindstorms, free coding platforms and websites such as Scratch, Blockly, and GirlsWhoCode.com). While the extant literature clearly addresses the need for coding instruction (see, e.g., Popat & Starkey, 2019) and anecdotes from educators about classroom coding abound on the Internet, less is known from a research standpoint about what the process looks like in classrooms and what the outcomes of classroom coding are (Bers et al., 2019); this is particularly true at the lower elementary levels (K-2 grade) (Arfe et al., 2020; Grover & Pea, 2013; Rich et al., 2019). In other words, research examining how young students learn to code and the results of their experiences is needed.

In addition, Egbert (2009) and Ozturk et al. (2018) note that teachers are essential in the effective integration of technology in classrooms, and their perspectives are an important aspect of determining what works. A small number of studies address primary level teachers and robotics (see, for example, Kim et al., 2015), but they also suggest that these educators need ideas and support for integrating concepts of computational thinking, coding, and robotics into their classrooms (Bers et al., 2019; Rich et al., 2019). For these reasons, the voices of teachers regarding their own and their students’ learning should be included in research on coding in classrooms with young students.

Further, because student task engagement is crucial for learning (Christenson et al., 2012), the level of student task engagement and their perceptions of what engages them during coding lessons may shed additional light on both the process and outcomes (Fessakis et al., 2013). In other words, studies of coding with teachers and young learners might benefit from a foundation of task engagement.

The problem that this study addresses, therefore, is the lack of research on coding with early primary students that includes the voices of both students and teachers. The purpose of this study is to use student and teacher data to explore and describe both the process and outcomes of coding in primary classrooms, using student task engagement to ground the work. To do so, this paper first describes relevant literature. It then describes the study methodology, explains findings and their interpretations, and finally presents conclusions and implications for teaching and future research.

2. Literature review
This literature review defines coding and computational thinking, describes current research on coding with primary students and teachers, and then summarizes the elements of task engagement. This literature review helps to establish the gaps in the literature and provide a framework for the study.

2.1. Coding and computational thinking
Although research with young students using the Logo coding application was conducted as long ago as the 1970s (Popat & Starkey, 2019), the number of studies decreased as interest in classroom coding declined. This level has increased in the last 10 years as additional programming environments have become available and interest in the benefits of coding for student of all ages has grown. However, the majority of studies of coding still focus on participants at the undergraduate level (e.g., Mingoc & Sala, 2019) and languages such as Python and JavaScript. Recently, however, literature that emphasizes coding with elementary school students has become more prevalent.

Research on coding at the primary level, although relatively sparse (Bers et al., 2019), indicates that there are many benefits for young children of learning about coding. For example, in addition to learning about coding concepts and understanding what makes things work, students can gain a positive attitude toward coding, work on collaboration and cooperation skills, use and develop creativity and other higher order thinking skills, learn to be resilient in the face of problems or mistakes, and gain procedural, math, language, and digital literacies (Fokides & Atsikpasi, 2017; Popat & Starkey, 2019; Wakil et al., 2019). Further, Kim et al. (2015) found that the use of robotics could lead students to think critically and employ other problem-solving skills such as trial and error. Another argument for coding with young children is provided by Ozturk et al. (2018), who note that primary aged children are “more often exposed to using technology rather than making technology work” (p. 150); they assert that students who understand coding can become creators and makers themselves and gain a sense of ownership over technology.

In much of the literature, coding is taught to address students’ need for computational thinking (CT). According to Bers et al., 2019, CT at its most basic is the “skill to solve problems algorithmically” (p. 131); this means to explicate and solve problems in a systematic way. Grover and Pea (2013) contend that “those in possession of computational competencies will be better positioned to take advantage of a world with ubiquitous computing” (p. 40). More advanced or older learners may learn CT processes that include concepts such as creating models and simulations and using advanced logic, but for younger students, the simpler process of becoming aware of problems, reflecting on them, and attempting solutions is typically seen in the literature as more appropriate to their cognitive levels. Using coding through robotics is one way to teach CT that appears to engage students across cultures, ages, and genders (Wakil et al., 2019); in part, this is because programming a robot allows users to see their result immediately and also receive feedback so that they can see and solve any problems.

2.2. Coding in elementary education
Upper elementary (4–6th grades) and junior high students (Wakil et al., 2019) have been the focus of a number of coding studies (e.g., Cakiroglu et al., 2018). Some studies have found that there is no difference among attitudes, aptitudes, or ability between female and male students toward coding in elementary schools (c.f.a., Cheng, 2019; Luo et al., 2020; Papavlasopoulou et al., 2020). Others have focused on theoretical support for teaching coding; for example, Papavlasopoulou et al. (2019) concluded that constructionism is a valid approach for teaching and learning coding and support the development of effective instructional design principles.

In one study with younger children, Fessakis et al. (2013) explored kindergarten children's problem-solving during basic programming tasks for which they learned four commands. The lessons were conducted by the teacher using a whiteboard. Although the ten participants found the programming engaging in general, only half of them completed the programming problems. The researchers suggest that exploring the role of the teacher, interviewing the students, and studying the impact of engagement on learning are important elements for future studies. Further, more explicit data about student engagement, with additional participants, are needed.

Bers et al. (2019) is another of the few studies conducted with lower primary school students. In this study, data from 172 3–5 year old children and teachers confirmed that very young children can benefit from introducing coding and computational thinking into the curriculum through robotics. The researchers described the format and process of lessons in the authentic classroom context and used an observation checklist to identify engaged positive behaviors; this helped to provide a well-rounded explanation of the context. However, only the first and final sessions with the robots were observed, leaving out part of the process that could be included in future research.

Additional studies focusing on young children include Keren and Fridin (2014), Lopez et al. (2016), and Fokides and Atsikpasi (2017). These studies provide evidence that young learners are generally very interested in coding/robotics. However, Mason and Rich (2019) state that there are no validated measures of elementary students’ CT and a systematic measure for their attitudes toward aspects of computer science has rarely been used. These are additional gaps in the literature.

While studies of lower primary students are fewer than at other levels, more attention has been paid to teacher preparation in research around coding at the elementary level. Much of this literature claims, as Ozturk et al. (2018) do, that teachers are crucial to the integration of problem-solving in coding lessons, and other studies show that teachers are essential to the successful integration of coding/robotics in general (e.g., Fokides & Atsikpasi, 2017). Fessakis et al.‘s case study (2013) with kindergarteners provides additional evidence for this argument. On the other hand, Mason and Rich (2019) claim that teachers need more practice in their own classrooms, and they need to both practice coding and practice teaching coding; these processes can be the focus of future research. In fact, Kim et al. (2015) contend that there is insufficient research describing and measuring teacher professional development with robotics. Rich et al. (2019) also note that teacher classroom practices around coding need more examination.

2.3. Engagement in coding tasks
While the literature agrees that there are benefits when young children learn to code, these benefits may not accrue if students are not engaged in their learning (Kim et al., 2015). Current research suggests that engagement facilitates academic achievement; Oga-Baldwin (2019) asserts that engagement is “perhaps one of the most crucial steps in predicting how students succeed” in schools (p.4). This implies that for students to learn computational thinking and other aspects of coding, they must be engaged in them. Task engagement, simply defined as optimal task involvement, has been identified in a variety of studies as consisting of the facilitators and indicators presented in Fig. 1 (see Christenson et al., 2012, for seminal studies).

Fig. 1
Download : Download high-res image (92KB)
Download : Download full-size image
Fig. 1. Task engagement facilitators and indicators (adapted from Egbert et al., 2019).

These constructs also find support in the literature on coding. For example, authenticity can be broadly defined as learner perceptions of the relevance and value of a task; in other words, they see the task as worthy of doing. Brown et al. (1989), addresses the idea that students learn most effectively when they can learn while embedded in authentic contexts and study topics that are meaningful to them. Fessakis et al. (2013) agree that programming tasks for young children must be designed to be meaningful.

In addition, social interaction plays an essential role in supporting learners’ engagement, although with whom and how much interaction can vary by age. For example, Arfe et al. (2020) note that young children tend not to work together well, being distracted by peers and not sharing the work equally; they are more willing to interact with the teacher around their questions. This should be considered when designing coding tasks for young children.

A third facilitator, learning support, consists of teacher, peer, and/or expert support about the students’ process and progress in the task. Support includes feedback, direct instruction, scaffolds and structure (see, e.g., Núñez & León, 2019; Reeve, 2013; Shernoff et al., 2016; van Uden et al., 2014).

Further, learner interest, both personal and situational, can also support active task engagement (Kim et al., 2015). Bers et al. (2019) note that learning to code, particularly robots, can be fun for students and allow them to play and be creative, piquing students’ interest in coding tasks. In addition, Kim et al. (2015) found that interest played a major role in the positive outcomes in their study.

Autonomy as a facilitator of task engagement has been well researched as a construct that includes students' ability to make choices during the task and to exert an optimal level of control/responsibility over the task and their participation in it (e.g., Núñez & León, 2019; Reeve, 2013; Shernoff et al., 2016. Although student age can have an influence on how much autonomy students want/need, approaching an optimal level means greater engagement in the task (Mozgalina, 2015). Ozturk, Dooley, and Welch's (2018) teachers saw greater levels of student autonomy and were able to provide them opportunities to choose during computer tasks; they attributed students' positive attitudes to their resulting sense of ownership.

Finally, Csikszentmihályi (2009) posited that the challenge level of a task should be just beyond learners’ skill levels, so that they are pushed to make an effort and engage in the task. Additional studies provide evidence that this challenge/skills balance is an important facilitator of task engagement. For example, Fessakis et al. (2013) agreed that programming activities for young children must be both challenging and achievable so that students do not become apathetic or discouraged during the tasks.

According to the literature, when students perceive the facilitators that are important to them, they evidence a number of engagement indicators. For example, they may participate often, clearly pay attention, make an effort and persist even when challenged (behavioral indicators); likewise, they may exhibit curiosity, feel that time is flying, work on their explanations and communications, and express a sense of control (cognitive indicators). Further, they may indicate their emotional engagement through positive affect in the form of laughter, smiling, fidgeting, and even jumping up and down; Wakil et al. (2019) note that most studies of children and computing found evidence of such positive attitudes. Agentive indicators can be seen as students take ownership of their work and reflect on both their process and progress (Núñez & León, 2019). Finally, social indicators of task engagement include students’ willingness to communicate with others in the class around the content of the task. Although specific levels of facilitators and indicators are not known and have not yet been measured across all of the elements in Fig. 1, observational, interview, and survey data together can provide useful view of levels of overall task engagement (see, e.g., studies by Henrie et al., 2015; Pintrich & de Groot, 1990; van Uden et al., 2014).

In addition, Pedler et al. (2020) provide clear evidence that teachers are essential for supporting student engagement by explicitly linking tasks to authentic student needs and goals, providing opportunities, designing tasks that are interesting to students, and creating participation structures that allow all students to learn in ways that engage them. However, few if any coding studies address these engagement facilitators or record indicators of task engagement, and none at the early elementary level do so. The studies noted above provide evidence that these facilitators might be essential for young learners studying coding. Because of the importance of engagement to learning, this task engagement framework serves as the foundation for the current study.

Overall, a review of the literature indicates the need for research into coding at the early elementary levels that includes both teacher and student voices. It also suggests the use of an engagement framework for this research. Therefore, this study works to fill these gaps.

3. Research questions
In order to explore the process and outcomes of integrating coding/robotics into early primary classrooms and to investigate participants’ engagement and learning, the guiding questions for this research are:

1.
What processes do teachers follow in learning about and teaching coding?

2.
What processes do students follow in learning about and using coding?

3.
What are the task engagement and learning outcomes for teachers and students?

4. Methodology
4.1. Research design
Following a qualitative methodological framework, this study uses a case study method to explore the research foci within authentic contexts (Baxter & Jacks, 2008); the study mixes descriptive and numeric data to produce deep descriptions of the case and enhance the data reliability. The boundaries of the focal case are the participating teacher and students in each of two natural classrooms and the participant observers who made up the research team. This case approach allowed for “close collaboration between the researcher and the participants, while enabling participants to tell their stories” (Baxter & Jacks, 2008, p. 545). The study background, participants, context, data sources and analysis, and methodological limitations are described below.

4.2. Study background
Teachers in a rural school district in the Pacific Northwest had attended a fall semester technology workshop at a local university, which included 1.5 h around the use of a small educational robot called an “Ozobot” (ozobot.com; called “bots” in this paper) that can be programmed to follow instructions based on color and/or block programming codes. (See Fig. 2, Ozobots on a color-coded track; see Fig. 3 for a view of Ozoblockly block programming). As Plair (2008) notes, this type of workshop typically only supports teacher awareness and beginning “how-to” knowledge. Moving, one of the teachers, Ms. N., later approached the workshop facilitators to inquire about working with her and the bots in her classroom as a request for on-going support. The facilitators agreed and applied for funding to support the project, while the teacher asked her grade-level colleagues to join the project. Both of her grade-level colleagues initially agreed to participate, but one had to drop out before the project started. Ms. N and Mr. R indicated their willingness to learn about and implement the bots by initially each setting aside a week of 1-h daily periods (originally a total of 5 h each) toward the end of the spring semester for in-class lessons. Ms. N's class participated in the first week and Mr. R's the second. Mr. R had no previous experience with the Ozobots, although he was familiar with the Hour of Code website that provides beginning coding experiences for students (code.org).

Fig. 2
Download : Download high-res image (155KB)
Download : Download full-size image
Fig. 2. Ozobots on a color-coded track. (For interpretation of the references to color in this figure legend, the reader is referred to the Web version of this article.)

Fig. 3
Download : Download high-res image (337KB)
Download : Download full-size image
Fig. 3. A student calibrating the bot using the Ozoblockly website.

Funding for the project was awarded by a foundation based in the state and a small grant from the university, supporting the purchase of the bots and other materials. Ms. N served as the corresponding teacher for pre-project preparations. The teachers explained the project to the students and collected consents and parent assents, and all students were given permission to participate.

4.3. Participants and context
Both participating classes were at the 2nd grade level in the same public school. Ms. N's class had 22 students (7 girls and 15 boys) and Mr. R's class had 24 (8 girls and 16 boys). One student in Mr. R's class was in the school's English as a Second Language program; none of Ms. N's were. All of the students had participated in an “Hour of Code” (code.org) the previous fall (over 6 months earlier), but Ms. N said the students had just solved puzzles and did not have the opportunity to do their own coding. Ms. N also noted that her class had done some other short coding activities prior to the current study that were led by one of her students who practiced coding with her father. Mr. R said that his students had worked on the Hour of Code only three times for a half hour to an hour each time. Both teachers had been teaching in public elementary schools for many years. Ms. N had 17 years of experience, whereas Mr. R had 7 years at the time the study was conducted.

The participant research team initially consisted of one university professor, one postdoctoral researcher, and one graduate student assistant (GSA). There was evidence that the students felt comfortable with the research team's presence, including asking them to tie their shoes and to give them help the following day, joking with the team, and even telling one member, “I like your name” on the first day. Mr. R confirmed that his students were used to having guests and being filmed, and he did not think that the research team was distracting.

After the first day in the school, two additional GSAs were asked to help with data collection and other project needs so that data could be captured thoroughly and student and teacher questions addressed in a timely manner. One GSA mostly videotaped and rarely interacted with the students. The professor, postdoc, and one GSA conducted interviews, described below. The classroom teachers provided student instruction at the start of each session, and everyone on the research team except the GSA videographer helped the teachers and students as needed during the “lab” time to answer questions, give hints, listen to student explanations, set up and put away materials, and problem-solve; in other words, the researchers participated both as knowledge brokers (Plair, 2008) and as technology assistants. Part of the last session of each week was used by the research team to create a “surprise” coding product for the teachers, who returned to class at the end of that session for a student demonstration.

Before the study started, Ms. N. asked the research team to design the tasks for the week to provide her with an instructional foundation. The team asked Ms. N to provide information about what the teachers needed and wanted to do. She wrote in a vision statement, “I am hoping that coding can offer a way for students to confront frustration in a setting that helps them practice thinking through new situations and attempting a variety of alternatives until they solve their problem and move on …”. She added that she believed that coding could help students practice paying attention to detail and being accurate, and she continued that, “Very interesting coding activities attract all students, even those who succumb to frustration quickly and balk at learning new things.” In other words, Ms. N expected her students to be engaged in the coding tasks. Ms. N included in her statement that she needed help with framing lessons that dealt with technology and not getting “bogged down in the logistics.” Finally, she emphasized that, “It is very important to me to keep time used for teaching to a minimum and time for students to try new things at a maximum.” Mr. R noted that he wanted to participate in the study because “it was a chance for [his students] to learn new skills that are used a lot in the world” and that he would follow Ms. N's lead. Therefore, before the first week of “stand-alone” coding instruction (Ozturk et al., 2018, p. 152) for the students, the researchers created simple step-by-step tasks for the students grounded in vocabulary, engagement, problem-solving, and use of attributes such as precision and communication. Also considered were students' lack of previous exposure to the bots and the handouts in the bot starter packs that were purchased for this purpose. Appendix B provides a general overview of the objectives and actions in each day's lesson.

In general, students were to learn to use color codes for their bot to follow (such as spinning, turning, moving a certain way, or going faster) and then to create their own tracks for the bots. They then would have the option (if ready) to move to using blocks of instructions included in the coding language Ozoblockly to make the bots perform more advanced maneuvers, such as dancing or racing. Along the way, they would learn vocabulary, coding skills such as debugging, and working in teams. The teachers were provided with lesson outlines from the research team.

4.4. Data sources and analysis
The researchers employed six data sources, described below. The study data were coded and sorted according to their relevance to the elements of the task engagement framework and to other themes and patterns found in the data. Data collection helped the researchers to identify students’ achievement based on their initial knowledge, establish levels of engagement, determine lesson goals, and take formative notice each day of student progress, which we shared with the teachers.

4.4.1. Background survey
A brief initial survey that inquired about the students' previous experiences/levels of coding and their perceptions of coding was conducted the first day of the session. Students answered seven multiple choice questions by circling the “smiley” icon that best represented their answer. These data functioned as a general needs analysis for both the researchers and the teacher (who already knew some but not all of the information) and allowed the research team to understand students’ backgrounds and ideas about coding and working in pairs. Student background data are presented in Table 1.


Table 1. Student background data for the combined classes. (N = 45).

Statement	Never/Not at all	A little	Medium amount	A lot
I like to use computers.	0	3	5	37
I have a computer at home.	2	9	9	24
I use a computer at home.	1	16	19	9
I use computers at school.	2	16	10	15
I know how to code.	3	7	11	22
I have coded before.	3	9	12	20
I like to work with a partner.	2	8	16	19
Overall, the data show that most of the students liked to use computers, and that the majority perceived that they had had some experience with them. Some of the data seemed contradictory or illogical (e.g., students who claimed that they had never used computers at school), so they were used only as a general indicator.

4.4.2. Pre-test and post-test
A content quiz designed by the researchers was based on the developed tasks and goals for learning. It included seven multiple choice vocabulary questions, recognition matching, a problem-solving question, and two questions that asked students to transfer their knowledge of coding the bots to two different problems (see Appendix A for the quiz). A final, open-ended question was added to the post-test to assess students' problem-solving. The short test was administered by the teachers before the first day and on either the last day of the study or the following class session. The teachers read each question and answer out loud to their class, making sure that the students knew to answer individually and that it was okay to mark “I don't know.” The teacher explained that the quiz was not part of the students' course grade and that the purpose was to help the teacher to see what to teach. The teachers answered any questions students had about test instructions but did not give them any of the answers. The purpose of collecting these data was to measure student gains in coding knowledge and higher order thinking and to provide the teachers with outcomes/feedback data.

Answers to the numeric questions were input as numbers into a spreadsheet, and the open-ended questions were coded by two members of the research team and added to the spreadsheet. Anecdotes were highlighted to support the statistical results. An independent-samples t-test was conducted using SPSS (ver. 22) to compare the two classes’ pre-tests. The results suggested that there was no significant difference between the first (M = 7.70, SD = 2.53) and second (M = 6.59, SD = 3.34) groups; t(43) = 1.25, p = .21; hence, the two classes could be collapsed into one case for statistical analyses. With the two groups collapsed, a paired-samples t-test was conducted on the test data at the end of the study to explore student content and knowledge gains. These data are further described in the Findings section of this paper.

4.4.3. Task engagement survey
A task engagement survey based on Egbert (2003) was taken by the students halfway through the week and at the end of the last day of the coding class to gather their perceptions of whether the coding project in general was engaging to them and how they perceived facilitators of engaged learning. The four levels of survey answers used both text (i.e., from “Not at all” to “Very”) and “smileys” with different expressions for ease of understanding by the students. The teachers walked the students through the surveys, reading each question and answer. Copies of the completed surveys were made for the research team and the originals given to the teachers. The responses were input into a spreadsheet as numeric data (1–4) and descriptive statistics were conducted. Responses were also compared qualitatively, using student comments from interviews to verify or explain the answers. The survey questions and outcomes are embedded in the Findings section below.

4.4.4. Interviews
Researchers interviewed each teacher on the second and last days of their coding week, using structured and semi-structured questions such as “What are you trying to accomplish?,” “What do you think has gone well so far and why?,” “What have you learned about technology?,” and “Why did you do ____________?” Additional questions were based on classroom observations.

These interviews were recorded with digital recorders and notes were taken, then the interviews were transcribed and checked. Further, informal interviews with the teachers took place both during the class time and after class as needed to explain and complete the lessons and the data collection. At times these consisted of one question, and at other times teachers were asked for explanations of their own or their students’ behaviors or their perceptions of a variety of aspects of the context and learning activities. Responses to these interviews were taken as field notes by the interviewers.

In addition, specific students were interviewed each day by one or two researchers. Twenty-one students overall were chosen based on the observations of their conduct and outcomes of the class on specific days; all were interviewed individually except for one group of three students and one group of two because of time constraints on the day those students were interviewed. Semi- and unstructured questions included “What do you like about using the bots?,” What don't you like?,” “Will you keep using bots after this class?,” “Why or why not?,” and “On your survey you said ___________. Can you explain?” These interviews were recorded using digital audio recorders, and notes were also taken during the interviews. The interviews were transcribed and checked.

All interview transcriptions were read and reread, and responses were categorized according to the engaged learning framework using a constant comparison procedure (Corbin & Strauss, 2014). Anecdotes were highlighted to explicate observational and other data.

4.4.5. Class observation field notes
Three members of the research team took field notes during every session based on teacher and student comments and actions, looking in particular for indications of task engagement and learning. Many notes were taken verbatim, while others briefly indicated actions and linked them to the task engagement framework (the class sessions were also videotaped for backup but were not used in the data analysis). In addition, the teachers recorded changes they made in the lessons for future use, and the research team also observed and recorded the changes. The field notes were read for comparability and then re-read for applicability to the research questions. Salient anecdotes were pulled out to support other data.

4.4.6. Student work
Video and digital photos of student printed work and screens were taken, and all student artifacts were collected, copied, and scanned. These included the required tracks they were assigned to create for the bots, the individual tracks they produced when allowed, and tracks that both worked and did not. The originals were returned to the teachers. Students’ coding products were used as process and outcomes measures and analyzed in two ways: (1) as a member check, by asking students what they were doing/thinking when they created the task, what problems they had, etc., and (2) holistically by the research team to examine their use of the learning opportunities and tools that their teachers provided.

4.5. Methodological validation and limitations
The researcher team collaborated on data analysis in order to triangulate data and make sure that central patterns and independent data items were both understood in the same ways and integrated well into the overall findings. In addition to the triangulation, data sources such as surveys and observations converged to support multiple views of classroom phenomena and allowed for member checks.

There are limitations to working in any natural education setting, one of them being, as McKinley (2019) notes, the “messiness of the real world” (p. 2) and the related “methodological messiness” (p. 6). However, as he notes, there are also benefits to reflecting on the “complex issues that teachers deal with in their daily practices” (p. 2). First, each class studied coding for only one week. Although this is a short time to collect data (and to study coding), it is the way that coding is incorporated into many classrooms (i.e., as an afterthought or add-on). In other words, the time limit provides a view of a real process. Further, data were collected from only two classes; because of the limitations of such a small group of participants, no overarching generalizations were made. In addition, the tests and surveys were not validated except by content experts (researchers and teachers). This means that the results may not represent statistically reliable measures of students’ perceptions or knowledge; however, they directly reflect typical classroom assessment processes and, along with the interviews and observations, provide useful data.

5. Findings
The findings in this section are presented in order of the research questions. Within the report for each question, patterns and themes are noted. All quotes are verbatim unless otherwise noted.

5.1. Processes that teachers and learners follow in learning about and implementing coding
In this section, data from observations, documents such as drawings and tracks, and interviews are used to describe how teaching and learning were operationalized during the study period. Because both teachers clearly used a “preview-do-review” format, results have been organized in this same way.

5.1.1. Previewing the tasks
Both teachers started each coding session with a group lesson; Ms. N had all students on the rug in the front of the class, while Mr. R varied between the rug and student desks. Ms. N asked the students questions, made sure they were ready to listen, foreshadowed upcoming tasks, reviewed and summarized what had come before and what they had just learned, and she always had the goals written on the board. As soon as they thought they understood what to do, the students started to wiggle and whisper to each other, but Ms. N waited until they had settled down and finished what she wanted to say.

Mr. R also previewed the week, had the goals written on the board, and he regularly asked what the students had learned that day. Like Ms. N, Mr. R made mistakes and had the students help him to solve them, but in a less orderly and more involved way than Ms. N. did, with students raising their hands wildly and shouting out. During his “rug time,” Mr. R told more than asked, the opposite of what Ms. N did, but he was also clearly cognizant of how long the kids would sit and listen and how much he could tell them at one time. Both teachers focused on the vocabulary that they saw on the pretest, reviewing words already learned and introducing only the additional words needed for that day.

More specifically, the teachers modelled/provided discovery type questions for each of the students' tasks in a whole class group before students were allowed to work with their bots. About her own modelling for her students, Ms. N stated,

I am the lead learner, so I need to model for them what it looks like to make a mistake, to think about it, to choose. The more I can make a mistake and show them what it was like for me, the more they will understand that they can make the mistake and do it too … So, I try to show I am doing all the things you do, I do it differently because I'm an adult and because I've done this before. I have experience but I want to show you what it is like.

One student even expressed appreciation for her teacher's modelling, noting that after copying her teacher's model, she could do things on her own. With her modelling, Ms. N explained that she was hoping to
show them more possibilities and show them that it's not that hard, maybe just teeny tiny little blocks of code and I could explain it, ‘Oh, this is why it went forward this far, and this is why it stopped, and this is why it turned a light, and this is what the light looks like’.

As part of his modelling, Mr. R asked more technical questions than Ms. N, asking about how precise the codes had to be colored in and why the bots sometimes acted unpredictably.
5.1.2. Doing the tasks
In both classes, the initial tasks for each day were assigned to small groups (2–4 students) across the whole class, although some students chose and were allowed to work by themselves or in parallel play. Ms. N made her coding groups by using a random group generator application and then changing groups that interested her or she did not think would work. Mr. R employed the groups that the students had already been assigned for the month. In both classes, students who chose to work alone were observed participating in plenty of social interaction––they asked questions whenever they wanted to, showed others their work, and copied others’ work to compare with them.

Ms. N generally followed the original lessons, but she added more detail, particularly in the instructions that she wanted to remember, and she reworded or highlighted some of the items to include more specifics addressed to her classroom context. For example, she inserted many additional “rug times,” when the students would gather as a group to discuss their progress, thus providing learning support throughout the lessons. Knowing her students also allowed her to delete the use of some of the handouts that the initial lessons suggested, keeping their interest going. In her notes for Mr. R, she included ideas that she had found useful, such as not having the students waste time decorating the bots.

On the other hand, Mr. R followed the research team's lessons almost exactly, but he modified how much time he allowed for each task to fit what he knew about his students. In addition, Mr. R sometimes asked members of the research team to explain aspects of the bots and the lessons. Mr. R. also encouraged the students to use other resources, for example, saying, “check your worksheets and box––what can help you?,” thereby helping the students to focus on their ownership and responsibility.

Student interaction and collaboration was supported specifically by both teachers in many ways. For example, two students in Mr. R's class had missed two days of coding and were quite unhappy about it. Mr. R facilitated another student in the class spending 15 min of his lunch time helping them to catch up with the class, and they did so very quickly. Further, during the tasks, both teachers made it a point to overtly encourage students to share and model. For example, one student was trying to help another to fix her code, and Ms. N asked them to show the rest of the class; when asked by a researcher, “why share it?,” Ms. N said,

partly because it was so explicit ––‘this is a mistake and this is how to fix it.’ But also it sort of valued that mistake and also it acknowledged her generosity in her doing that. So, it seemed like it brought everything together. This is how we deal with mistakes.

When Ms. N shared her own coding mistakes with the class and asked for input, the students were very attentive. In fact, one student had a bloody nose during a segment when Ms. N was demonstrating, and the student refused to stop watching the teacher's explanation. During her sharing, Ms. N encouraged interaction by asking, “What was I thinking?” She constantly emphasized the benefits of collaboration with her students, saying, “You taught each other a lot, you taught me a lot.”

Mr. R. also encouraged students to share, especially in helping him make his bot work during the rug sessions. When students were not quite correct, however, he did not always correct them. For example, when a student was trying to use a “jump left” code but had the track the wrong way to show the idea, Mr. R did not comment. In both classrooms, students were allowed to check with any peer about what was going on and to compare their work.

Further, Ms. N provided students with a lot of additional scaffolding. For example, if a student could not answer a question, Ms. N would ask, “Would you like me to give you choices of answers? Which one would you like to pick?” Mr. R did not always provide an example for students, noting that, “It's like math––sometimes they need to fill in the information.” When he found a problem that a number of students were having, for example, calibrating in Ozoblockly, he asked the students to come to the front of the room for a demonstration. Mr. R also consistently reminded students of how much time they had left for specific tasks, which seemed to help students to stay on task.

Ms. N gave students the option whether to use Ozoblockly, while Mr. R had all of the students move together to use it. However, Mr. R said the students in his class were not as focused on the task he gave them and wanted to do their own thing more. They often did, and they made excuses about why they were not on the specific task he had shown them. If they were working on something that would meet the goal, Mr. R continued to let them work, or if they had finished his assignment and moved onto something else, he allowed them. However, if they were completely off task (e.g., decorating their bots), he asked them to get back on task and waited by them until they did so.

5.1.3. Review
Both teachers focused on debriefing the whole group after each session so that students could listen to each other's answers and experiences, although Ms. N noted that she wished she had done so more often. If Ms. N did not have time after students had put their materials away to debrief the day's activities, she did it at the start of the next coding lesson. The debriefings included reiterating the vocabulary focus for that and previous days, talking about the new ideas learned, and reinforcing ideas such as problem-solving and collaboration.

As noted in the previous section, time was an issue for both classes, in general because the students wanted to continue working with the bots. Ms. N often allowed students who asked to stay in from recess to continue creating and testing their tracks. After the second day, Mr. R decided that he wanted to start the lessons a bit earlier so that students had time both to work and to put everything away neatly. In fact, organizing and putting the materials away each day was quite messy until the teachers decided to use the research team, who devised a system for putting the bots away so they would be ready for the next day.

5.2. What are the task engagement and learning outcomes for teachers and students?
Data from observations, surveys, and interviews provide evidence of participants’ perceptions of their task engagement and learning. Further, data from the pretest/posttest and interviews provide additional information about student engagement and learning.

5.2.1. Teacher engagement
Overall, the teachers appeared to be invested in both learning about the bots and teaching their students in effective ways; this indicates task engagement on their parts. The most salient patterns in the data are discussed below.

5.2.1.1. Authenticity
The authenticity of learning about coding for the teachers was based in the use of the bots in their actual classrooms. This was underscored by comments like the one Ms. N made in her consent letter to parents, where she stated, “I am excited to learn how to teach these lessons.” Underlying this excitement, in part, was that she had seen other students coding and noted that “one of the things frankly I liked last year was that I saw a girl succeeding in ways that sometimes they didn't in other math.” Mr. R said that the problem-solving aspect engaged him as a teacher, too, and this is important because teacher enthusiasm and engagement have been cited as facilitators of student engagement (see, for example, Patrick et al., 2000).

Ms. N. experienced authenticity in an additional way. At the time of the study, she was working on better understanding student agency with a professor at a different local university. She noted how this work and the current context worked together to help her to learn, saying, “I've noticed how in building these lessons I've intentionally put in times where [the students] have more agency and they are making more decisions.”

5.2.1.2. Autonomy
Further, autonomy within structure was present in the form of teacher choice. Unlike in many workshop experiences, the ultimate goals for the week and each day, although based on the research team's task plans, were created by the teacher, with no specific outcomes required by the research team. Teachers had the single complex task of helping the students learn to code the bots and a week to get to wherever they wanted goal-wise.

5.2.1.3. Learning support
In addition to authenticity and interest, learning support appeared to be the most important facilitator for the teachers. The teachers could tap the researchers as knowledge brokers, check online videos, use instructions made by the research team or included in the Ozobot package, and Mr. R could ask Ms. N if he had questions about the lessons that she passed on to him. Ms. N made use of most of these resources, including, for example, asking team members to ask a female student a question we had about her work and to let her know what the student said. During the first interview, Ms. N expressed a bit of stress in changing to Ozoblockly, so the researchers/interviewers explained the process to her. When she needed help with small administrative tasks, she asked for it; for example, she wanted all the decorations taken out of the Ozobot boxes because the students were distracted by decorating their bots, and she asked the team to do it before the following class.

Informal interactions with the teachers allowed the teachers to ask the researchers questions just-in-time, and they often did, even in the middle of class. For example, during rug time when the students and Mr. R could not figure out why the bot was not reacting as it should, he asked the research team. Ms. N sometimes interrupted what she was saying to ask the research team for support, demonstrating to the students that it was fine to ask questions and request help. Mr. R said that he wished he had also had a teacher's guide or “cheat sheet” to save time in learning the technology. This suggests that the one-shot bot workshop that Ms. N attended might have been a useful lead-in to the classroom instruction.

Ms. N also served as learning support by passing on her comments to Mr. R and telling him how the lessons were progressing to help him to prepare for his week of coding. Mr. R noted that he initially wanted to do the lessons the way the research team had laid them out, so that the team could see some of the issues with the tasks, in that way, he was working to provide new knowledge for the team. He also used Ms. N's plans and felt like he understood the process. Mr. R noted that having learning support in the classroom was helpful to him.

In addition, the researchers provided learning support in the form of feedback to the teachers. One exchange demonstrates the social interaction:

Interviewer: So, between Monday and Friday, have you noticed any difference between students' engagement? Did it increase or decrease?

Ms. N: I think it's increasing. But you guys probably actually might be a better measure than I am. Because you're able to sit back and watch. What do you think?

The presence of learning support can be summarized by Ms. N's comment that, to teach technology, what was most important was to try it out; she added, however, that she had “delayed until you guys came along.” This underscores the importance that assistance of different kinds can have in motivating teachers to try something new in their classrooms. However, Ms. N also noted that she wished she had a community of other teachers who were doing it with her. She said, “that's actually one of the most motivating and the way to grow the fastest if other people were doing it too.”

Overall, the task engagement facilitators, and the indicators of such in the data, help to explain teacher involvement in the process and provide evidence that authenticity and learning support can play an essential role in professional development around coding.

5.2.1.4. Teacher learning through experience
The teachers learned more than just how the bot worked by being supported to use the technology with their own students. For example, when students shared their tracks/coding with the class, the teachers had the opportunity to consider new ideas, express their views, ask the experts for theirs, learn about their students, and try out new tasks with student support.

One important example of teacher learning involved a whole-class discussion early in the week when Ms. N. asked students what they thought a computer was. The class made a list of examples and then came up with the definition that computers “make decisions.” Ms. N was then asked whether a Kindle (a hand-held book reader and app displayer from Amazon.com) was a computer; she decided that it did not meet the definition of a computer (being instead an information retrieval device that “you can't manipulate it, you can't write code with it, you can't search with it”). She also stated her belief that an iPad is only a computer if it is “on the internet,” and that a television was not a computer. During a conversation following this, the researcher shared with her the general definition that a computer is anything with a chip. She replied, “Okay, so Kindle would be? Okay … And anything that we would consider a smart machine is a computer.” The following day she expressed this mistake to the students, reinforcing the idea that it was okay to make mistakes and sharing her new learning with her students.

Clearly the teachers learned about what it takes to teach about and with the bot. For example, Ms. N commented that the start of the week was “more stressful that I expected” and that “management of different parts was off.” She was glad that she had left additional time (beyond the original 1-h limit given) and said that she “didn't realize how much organization it required” to teach coding. She also learned about using the other parts of the Ozobot kit, such as stickers and envelopes, to help students who were not able to draw the lines precisely; when reminded about them, she said “I didn't think about that … I'm thinking I'm going to use it tomorrow.” Ms. N also related that she had learned how to focus the lessons, explaining, “I like the two focuses that I've kind of landed on … now that they're actually using the code to make something new, I think that's a really motivating thing.” The limited time period, however, meant the teachers still had more to learn. For example, in her final interview, Ms. N said that one of her students had made a picture that became a “code,” when she should have said that the picture became a “program” (a set of related codes).

5.2.1.5. Teacher learning through reflection
Both teachers noted that this experience helped them identify what they should change when using Ozobots again. Ms. N said that her most crucial change would be to explore the Ozobots more herself. She also stated that when she did it again, she would be sure to have partners like the research team. She noted that until we offered to work with her, she “did not think it was really worth doing it.” She also said that, in working with Ozobots in the future, she would use more of the time for debriefing, where the students would show how they had solved problems in their coding. Additionally, she noted that in future iterations she would provide fewer worksheets and do a more “stripped down” version of the coding so she could be more thoughtful about the focus. Mr. R agreed that less paper and fewer distractions would be more effective, as he struggled with asking students to look for papers in the bot kits and using time effectively.

Mr. R learned that more free play with the bots should have been planned in every other day for students to really finish the tasks and to get the desire to do what they wanted “out of their system.” He discussed with the research team about how he might rework the lessons, using the posttest to think about how to partner (or not) the students. He also learned that using the bots gave him the opportunity to do more whole class learning than the previous coding experience had (the students had been at different levels in that experience). He felt he had a better chance to teach the vocabulary to the whole group and provide them with reminders. He also learned that the hands-on bots were more engaging to his students than other kinds of coding. He realized that even when they moved on to Ozoblockly and had some issues with the computers, students were still engaged because they got to continue to use the bot.

The teachers also had the chance to reflect on other aspects of their learning during the project. For example, Ms. N related that, “I think my understanding of problem solving now has kind of evolved in that I see it as debugging; debugging is the heart of problem solving.” She also realized,

I need to keep my instructions short and not long. And I know if I don't stop I'll have continuing questions. It's guaranteed. But I try to train [the students] so that if they need a question, they can ask me later and they can stand a book upon their desk and I'll catch up. That's instead of raising your hand. So, you work while you wait and you're not sitting there …

However, she also learned that the students were so invested in the coding tasks that they did not want to wait, and they usually asked questions to anyone close by.

Finally, the teachers learned from making their own mistakes and watching their students’ behaviors and reactions. The teachers explored the bots at home to prepare for each day, as they would with any new concept. However, because of the short time frame of the coding sessions and the total absorption of the students in their tasks, both teachers and students might have lacked opportunities for sufficient reflection. With more time, the teachers may have been able to have a deeper understanding not only of the bots and coding but of how technology may be integrated most effectively into their instruction.

She displayed her ongoing interest, stating,

We now have a set of Ozobots that will be used by our STEAM teacher this year. I am curious to see what her lessons will look like.

5.2.1.6. Teacher social learning
Both teachers learned from their students and the research team. For example, Ms. N noted that the students thought of things she did not. She also said that having other eyes in her class helped her to get feedback on students that she did not have time to work with. For example, she put together a pair of students who she thought might have power struggles “to see how they work it out and to support them as much as I can.” However, the first two days she did not have time to work with them. During the interview when she was asked about that particular group, the interviewers noted what they had observed, and the teacher was interested to get more information. She made it a point to check with that group first the next day. As to another student, she asked the research team, “I don't know how [he] is doing. How's [he] doing?”

Moreover, the teachers commented that coding for the Ozobots specifically enabled them to see another side of some of their students. For example, Ms. N explained, “[Student name] is one of the people that … he struggles to listen in school, to find meaning for doing and he's all over this. I know that he has the potential, it's really nice seeing it coming out.” Ms. N was also surprised by the initial behavior of one team: “[A student team], couldn't believe they did nothing but sort of decorated and I would have expected those two would've been all over it exploring it and getting it to go places.” She noted that two of her students usually worked very well together, but when it came to the coding and the use of the bots, she was surprised that their partnership did not work as well as she had hoped. Ms. N perceived her students differently in other ways, too. For example, on the second day of the Ozobot session, Ms. N said,

I did not expect them to go as far as they did. Tomorrow I wanna have more kids showing ‘this is the problem, this is what we found, this is what we tried, and this is how we solved it … ’ So I was surprised to see that starting yesterday.

Mr. R also said that he was surprised how some of his students reacted positively to bot use; for example, he noted that one student “can't sit still to save his life,” yet he did for the bot explanations and use. He also had positive comments about other students that normally acted differently. He noted that “trying to get [student name] to do his best can be challenging,” but that he was one of the most focused during the bot tasks. In other words, the use of the bots engaged some students to behave in more learning-oriented ways than they usually did and gave the teachers new perspectives on those students.

When asked if they would use the bots again in the coming school year, both teachers said yes. Ms. N added that other teachers in her school would be interested in learning to use the bots and that she hoped to share her knowledge and experience with other teachers in the future. This implies that one of the outcomes of this project was greater teacher self-efficacy in the use of the bots and a perception of the bots as useful for learning.

5.2.2. Student task engagement
The data show that students were very involved in learning about and using not only the bots but the vocabulary and ideas that were part of the lessons. Table 2 presents the numeric outcomes of the engagement surveys; the task facilitators were clearly indicated throughout the data, as described below.


Table 2. Students’ perceptions of interest on the task engagement surveys.

Interest	Not at all	A little	Medium	Very
T1	T2	T1	T2	T1	T2	T1	T2
1. This task was interesting to me.	0	0	3	0	5	2	35	38
2. This task was about something I like.	1	0	11	4	10	4	31	32
*Note: For the first survey (T1), N = 43; second survey (T2), N = 40.

5.2.2.1. Interest
Observation, interview, and survey data (below in Table 2) show that most of the students were very interested in learning about and using the bots. For example, when Ms. N showed her students a bot for the first time, they made exclamations such as “It's so cute!” and “It's like a baby!” Ms. N related that one of the girls was so excited that she “asked to call her mom at work to ask when they're getting another Ozobot.” Further, when Mr. R announced that the lesson was done for the day and it was time for recess, a collective “aaaaaw” came from the students, who were not ready to be done. Mr. R. said that he could tell that the students were highly interested, because even though there was a mess of things on each desk, the students were on-task. He explained that, “if it was writing we were doing, they would use the messiness as an excuse [not to do it].” During interviews, students all said some variation of this student's comment: “There's not really much things I don't like about it. I really like it.” Mr. R added that some of the parents had expressed excitement that their students were going to study coding; this may have also had an impact on the students' interest.

Further, in their interviews, all of the students said that learning to code with the bots was “fun” and “interesting”; some even used words such as “amazing” and “fantastic.” Mr. R implied that students even might have been in some way over-engaged in using the bots; he said that they wanted to check their work so badly that some of them coded sloppily in a rush to test it. Fig. 4 shows an example of a student getting ready to test an individual track that was not created precisely or with the correct codes.

Fig. 4
Download : Download high-res image (467KB)
Download : Download full-size image
Fig. 4. A student getting ready to test an individual track.

Ms. N, when asked about student task engagement, stated that

even as frustration levels stay pretty high, they're working through it … And that to me says at the very least engagement is still really high. And I think also by this time the novelty has worn off, so it has to be interesting for itself. It can't be interesting just because it's new, which is what it was the first day. And the Ozobots are becoming a bit ordinary right now, but they're still really, really interesting. And they're interesting not because they're a little machine with lights, they're interesting because I can make it do something.

5.2.2.2. Authenticity
The results also show that the majority of students perceived the tasks as connected to their lives (see Table 3 below). Ms. N helped students to understand the authenticity of the learning tasks by discussing with them why they needed to learn about coding at school and what it might mean for their futures; on the other hand, Mr. R noted that he was not sure whether his students knew at first why they were coding. Both teachers focused on the coding experience as a useful part of learning itself. Ms. N explained, “what I really wanted to do with that is them link that idea of mistakes … as the most powerful part of learning.” However, one student in Ms. N's class had a different idea about authenticity. She noted that, when given the choice to keep using the bots and markers or move on to programming with Ozoblockly, she chose the markers because, “I don't like doing computer at school, because it's a little different than Minecraft and Animal Jam at home.” At the end of the coding week, however, a number of students were still not sure how/whether they would use coding in their “real lives,” even though the focus on making and correcting mistakes was made clear as an authentic life context by the teachers.


Table 3. Students’ perceptions of authenticity on the task engagement surveys.

Authenticity	Not at all	A little	Medium	Very
T1	T2	T1	T2	T1	T2	T1	T2
3. I will use the things I learned in this task outside of the classroom.	7	4	5	5	17	14	13	17
4. The content of this task was meaningful to me.	4	3	5	3	15	9	19	25
5.2.2.3. Autonomy and social interaction
Further, the teachers gave the students choices in most of the tasks (e.g., to code a pre-made track or create their own track, to work in teams or alone) and students had the opportunity to define their specific goals, an option not always on offer for 2nd graders. Ms. N demonstrated her understanding of students’ need for autonomy when she noted:

Whenever I can I put them in a position where they need to make choices, I think that's one thing you need to practice doing a lot. But also, it allows them to choose the path that is most interesting to them. And it's more interesting to them it's more motivating, it's more engaging.

Many of the students said that their favorite part of learning about the bots was getting to create their own programs; some students commented, however, that their partner (not the task per se), prevented them from expressing their opinions and making decisions. This may have led to those students perceiving a lack of autonomy (see Table 4). Some students did find their social interaction with their partner explored through interviews and observations, to be useful; one student suggested collaborating by dividing the tasks in an attempt to solve coding problems, and noted, “my partner kept telling me don't worry about the Ozobots, just worry about the paths.”


Table 4. Students’ perceptions of autonomy on the task engagement surveys.

Autonomy	Not at all	A little	Medium	Very
T1	T2	T1	T2	T1	T2	T1	T2
13. During this task I could make my own decisions about what to do.	7	4	7	6	10	8	15	22
14. I could express myself freely during this task.	3	3	6	5	12	7	21	25
15. I understand the instructions for this task.	1	2	3	2	11	4	28	32
The choice to work in teams or individually provided students with opportunities for social interaction, and both teachers also checked regularly with students to assess how much more time they needed and to go over the allotted time. A couple of students disliked the “rug time” when the teachers would demonstrate, model, and ask questions; one student said, “I'd rather actually experience things than actually have to hear everything.”

5.2.2.4. Learning support
Student learning support included the teacher, peers, members of the research team, handouts in the Ozobot kits, and the teacher's laptop, which could be used for checking ideas. The results suggest that the majority of students perceived the resources available to them as ample, and that this perception increased between administrations of the survey; see Table 5 for the survey results. The bot itself also provided learning support. Along with a focus on debugging, or fixing errors, when the bot did not work as planned, students had instant, contextualized feedback and were aware that they needed to review their coding.


Table 5. Students’ perception of learning support on the task engagement surveys.

Learning Support	Not at all	A little	Medium	Very
T1	T2	T1	T2	T1	T2	T1	T2
7. I got the help I needed to this task.	1	0	4	6	11	8	25	26
8. I had the information I needed to succeed at this task.	3	0	3	6	12	5	25	29
In addition, some of the study data sources provided support. For example, the pre- and post-tests served as formative assessment to help the students to see how far they had come from their initial knowledge and where they might need to focus next. Student interviews and observations also allowed the students to reflect on their experiences and think about what they were doing and how they felt about it.

5.2.2.5. Challenge
Many of the students claimed that the biggest challenge for them was being very precise drawing their codes. Observations showed, and teachers confirmed, that students had the most trouble with drawing their lines and codes precisely (see Fig. 5 for an example of an imprecisely drawn track that did not work). One teacher suggested that smaller markers would be better for the students’ small hands. In addition, while all students but one liked to make the paths with the markers, the majority of interviewed students disliked the use of Ozoblockly and found it difficult because they had trouble loading the programs from the computers onto their bots. Mr. R commented that during the previous coding experience some of the students in his class did not engage because it was too challenging, but he felt that the Ozobot use was less frustrating for the students and enabled him to offer more support. When the bots seemed to follow some of the paths randomly or did not follow them at all, Mr. R noted that this kind of frustration could teach the students to approach their path differently. Student perceptions of task challenge are presented in Table 6.

Fig. 5
Download : Download high-res image (352KB)
Download : Download full-size image
Fig. 5. Example of a student's imprecise track.


Table 6. Students’ perceptions of challenge on the task engagement surveys.

Challenge	Not at all	A little	Medium	Very
5. This task was difficult for me.	20	21	12	9	5	4	5	6
6. I knew how to do this task.	7	3	8	6	10	7	17	24
Another challenge that was observed was that some students appeared distracted. Students provided explanations for their perceived distraction. For example, one student said she was distracted because she was thinking “maybe I could use some of this information instead from school at my house … on my Ozobot.” Another established that she was “good distracted” rather than “bad distracted” because she was thinking about the bot. Another was discouraged at first because he felt that the bot “makes its own choices” when it did not do what he thought he had asked it. He kept on trying to problem-solve, however, and was happy when he finally figured out what he was doing wrong.

Although the teachers emphasized one or two vocabulary words each day and recycled them constantly, one student said that her challenge came from “all these different words that I don't really know” and was the thing she disliked most about the tasks. A few students had technical issues that challenged and discouraged them – for example, one student in Mr. R's class said that she did not like when the bot had to recharge. When asked what she could do about that, she decided that she could charge it when she was not using it. Another student in Mr. R's class felt challenged when she had trouble calibrating her bot. A couple of students wished for more help because they felt that they did not know how to debug their mistakes; however, this did not keep them from perceiving the tasks as fun. One student stated that if the bot could talk, she could better understand her mistakes; apparently, she wanted more or different resources to support her learning. It appears from these data that most of the students had the appropriate level of challenge while learning to code with the Ozobots.

5.2.2.6. Overall engagement
Overall, the students did not have many problems with the coding focus or the bots but rather with technical glitches or instruction that they did not care for. This implies that the use of the bots themselves and coding were inherently engaging to these young students. The survey data support this assertion, with students reporting that they generally were quite focused on the tasks and were paying attention (see Table 7 for these data).


Table 7. Students’ perceptions of focus on the task engagement surveys.

Focus	Not at all	A little	Medium	Very
T1	T2	T1	T2	T1	T2	T1	T2
9. This task kept me busy and focused.	3	3	4	0	9	3	27	34
10. During the coding task I thought about other things.	21	15	13	12	4	2	4	11
11. During this task I was distracted.	22	22	11	11	6	2	3	5
12. During this task I was paying attention and the time passed quickly.	0	1	4	3	10	6	27	30
Asked about her students’ interest and progress, Ms. N said,

I think they are doing really well. I think they are trying a lot of things and several of them are taking risks and fixing them. And there is couple of kids that I've been watching that are super engaged. They are super attentive to what's going on, attentive to the details of what they see when they try something. And they are starting to mentally check through ‘did you try to do this? Did you try that?’

5.2.2.7. Learning outcomes
Overall, the outcomes of this study support the literature, cited previously, that states that young children can benefit from coding and robotics, and that they show a high level of engagement during coding tasks. When asked what they had learned about coding and the bots, students' answers generally indicated that the bots worked with colors and used tracks and required precision, but there were other answers, too. For example, one student in Mr. R's class noted that “I learned that robots don't usually have … a mind of their own and don't have feelings … that means it doesn't have self-discipline.” A number of the students indicated that they discovered that mistakes were okay.

In more concrete terms, during that one week of instruction, students learned not only to draw intricate designs using correct technique but also to use codes that would send the bot the way they wanted it to go. This indicates a grasp of the principles of color coding. However, actual individual coding performance varied. Mr. R commented that some of his students were struggling with some of the more technical aspects of using the bots’ color codes, such as where to put them in relation to corners and before turns.

However, most of the students' final tracks did actually demonstrate an understanding of trial and error/correcting mistakes, as their documents and interviews recorded. For example, one student explained that sometimes the robot did not “listen.” He then illustrated his problem-solving process by saying “if I said turn or go forward, I want it to do that, but it doesn't because it makes its own choices.” He explained how he finally overcame the issue, noting that, “but then I figured out that I'm supposed I had to press the start button twice like boom boom and then it worked.” Examples of two students' creative, personal, working tracks are presented in Fig. 6, Fig. 7.

Fig. 6
Download : Download high-res image (243KB)
Download : Download full-size image
Fig. 6. Example of a student's individual track.

Fig. 7
Download : Download high-res image (355KB)
Download : Download full-size image
Fig. 7. Example of a student's individual track.

Several students reflected on their learning experience with Ozoblockly, stating that it was more difficult than the paper color coding as it took longer to build the required code on the computer, to solve coding issues, and to load the codes to the robot. As one of Mr. R's students put it, working to create a code commanding the robot to move in a rectangular motion, “Yeah, that was kind of difficult for like a few parts and then I finally made it in a few tries and that made me feel good.” The students expressed satisfaction, describing successfully creating a working code after multiple attempts as “awesome,” “fun,” “great,” and “cool.”

The results of the posttest provide additional evidence that students developed knowledge of coding vocabulary and processes during the study. There was a significant difference in the scores for pre-test (N = 44, M = 7.16, SD = 2.97) and post-test (N = 44, M = 13.07, SD = 1.724) conditions; t(44) = -10.65, p = .000. Even on the open-ended question asked only on the posttest (i.e., “What would you do if the puppy didn't get to the bone?”), almost all students could state the ideas that 1) the question asked about debugging, and 2) that they would need to find and correct their mistakes.

Many students could explain the target coding vocabulary well, and observations noted that the students used the vocabulary consistently and with apparent pride when they remembered the words. A few of the students could not express themselves using the actual vocabulary, but they could provide an example or process for the term; for example, one student described “calibration” as, “getting your Ozobot and then try and push the button until it's light and then put it in that little circle and then when it moves and it's green. That means you calibrated it.”

Mr. R said that he was surprised how well the students did on the vocabulary part of the quiz. He thought that it had helped that they had practiced the vocabulary in other contexts. For example, the class defined the word “calibrate” as “to get something ready.” During the study week, Mr. R was getting his students ready to go to the library, and he told them that they needed “to calibrate,” using this term with a breathing exercise to help his students relax. He told them at other times to “calibrate yourselves.”

About what she thinks her students learned, Ms. N said,

I think most of the skills that they are most likely to use continue with the one that I talked about at the beginning, the idea that one mistake is not painful. One mistake is how you learn. I don't think the kids realize that that's one of the lessons that they're learning, but I can just really nicely reinforce this as an extension of the idea in a really concrete way. The teacher is not telling them it's ok to make a mistake and coaching them on doing that themselves by working it through and trying to figure it out. They have that chance to learn that for themselves.

Ms. N felt that her students would be motivated to continue learning to code because they were “using the code to make something new.” She added that she thought that the students would have a better understanding after this week about what code can be good for. Students confirmed that they had many things that they wanted to both do with and learn about the bots in the future. These ranged from speaking, jumping, flying, dancing, “climbing up me,” and going to recess, to learning how the bot was constructed and what else it could do. One student expressed the desire to use the bots in class every year.

6. Discussion and conclusions
This study contributes to the literature in several ways: First, it examines coding with young children, which to date has received less attention than it should. Second, it views teacher learning in authentic classroom contexts and includes their voices, noted as essential by Rich et al. (2019). In addition, this study is grounded in a task engagement framework that not only has not previously been used to study coding with young children but provides a way for both teachers and researchers to conceptualize classrooms and learning.

This study confirms previous research that found that, without going through the study experience, the teachers could have learned about problems teaching coding but might not have been able to solve them and reflect upon their practices (see, for example, Plair, 2008; Sentence & Humphreys, 2018). During the coding lessons, Ms. N realized that her students could do more than she thought. She may not have encouraged them to do this if she were not learning in her classroom, because she may not have thought they could do it if she just heard about it in an external workshop. In other words, having professional development experiences in which teachers are asked about and can work with their students directly can help teachers to articulate their thoughts and learn about how to work with students in ways that they had not previously.

In the current study context, the findings indicate that Ms. N had an easier time implementing the bot lessons because of her previous experience with both the bot and the team. This indicates that a workshop, meeting, or other kind of orientation session might be useful before classroom implementation begins. Overall, there was perhaps not enough time for teachers to learn and reflect or for all the participants to share their stories. Ms. N noted that time was an issue, noting about the overall goal of her bot lessons, “I think [the students] were inclined to do it. I don't think they were prepared, and especially with the limited time, to actually achieve it.”

However, as Fessakis et al. (2013) suggest, the bot learning was engaging for the teachers and students, and the engagement framework provided a useful lens for both observing and describing these classrooms. The authenticity of the tasks and contexts were clear to all participants, and the research team was able to provide support for and perspectives on students with whom the teachers did not have time to work. The lesson structure, combined with opportunities for both teachers and students to exercise some autonomy, provided scaffolding for all participants’ learning. Although the teachers approached the lesson tasks in slightly different ways, they noted that in both classes the students appeared to be okay with risk-taking, mistake making, and explaining what they did and why.

Different occasions for meaningful interaction between the teachers and the students were also documented, as tasks on solving a coding problem (debugging) created a positive atmosphere of knowledge exchange, boosting student's self-confidence and task engagement levels. In some sense, both teachers and students were polishing their learning and becoming experts as they embarked on the tasks and activities together with limited experience with the focal content (coding with the Ozobot). Although the teachers in this study said that they wished that more of their colleagues had participated, the findings show that teachers and researchers can benefit from collaborating with students, too. This implies that researchers/outside experts can serve effectively as knowledge brokers in professional development for teachers.

Further, by considering how to provide students with both structure and autonomy, the teachers kept their students on task for most of each class session. The combination of direct instruction in small doses and hands-on learning in large doses (which Ozobot use motivates) allowed students to test each of the concepts presented thoroughly. Giving students the opportunity to move forward as they wanted to, to receive assistance when they needed to, and to experiment as they saw fit kept student focus on coding and testing their bots. This appeared to be essential both in how the students perceived their learning and the outcomes of it. While the students developed knowledge about vocabulary and programing the robots to operate, they also developed less specialized knowledge in problem-solving. In addition, during the coding lessons, some students achieved the status of experts, demonstrated by other students noticeably calling their names or asking them for help.

Although students varied in their responses to the surveys, the data indicate that the students perceived the bots as interesting and doable and the instruction mostly supporting autonomy. Observations and field notes concur that the students were deeply engaged in bot use and even in teacher instruction around its use. Overall, perceptions of authenticity, a sense of autonomy, appropriate challenge, and most of all, sufficient learning support appear to be facilitators of teacher and student focus and interest in their classroom tasks around coding. Coding tasks and tools, like the Ozobots, that offer these opportunities may be more effective for facilitating teacher and student interest and effort than those that do not.

Finally, when coding is taught with a focus on problem-solving and referred to in multiple contexts, it can help students to engage with and transfer their knowledge to similar contexts. Using new technology requires flexibility, understanding of the skills participants have, and choices so that students and teachers do not disengage. However, over-engagement in what participants perceive as exciting and “awesome” technology use may also be an issue; this indicates that some free play time with the new tool might benefit learning from it. These conclusions lead to implications for both teaching and research.

7. Implications for classroom practice
Many ideas for classroom practice around coding are presented throughout this paper; the teachers’ successes and challenges can serve as guidelines for others in their classrooms. Of importance, as Pedler et al. (2020) suggest, the findings of this study support the idea that teachers are central to student engagement. Therefore, teachers might pay close attention to the engagement facilitators as they design tasks, and they can also assess student engagement during tasks. In addition, allowing students choices in reaching task and curricular goals can ensure that all students are engaged. The study findings also suggest that both students and teachers can use support when learning about classroom innovations; for the teacher this may be a parent or researcher serving as a knowledge broker, and students may get help from a close working relationship with the teacher and/or expert peers. Support can be designed into the task from the beginning to help ensure its success. Perhaps the clearest outcome is that teachers must come to know their students in many ways in order to provide the most effective learning experiences.

8. Implications for future research
Several ideas for future research arise from the outcomes of the current study. First, although teachers helped students complete the surveys, the use of tools that better measure young students' perceptions and outcomes might allow a more specific idea of the influence of coding study and how it might be conducted most effectively. Further, more rigor in measuring teachers' initial perceptions and knowledge might offer additional explanatory power for the outcomes. In addition, transfer, both teacher and student, could be the focus of closer inspection in future research. Students showed the ability to near-transfer their new knowledge after only a week of working with the bots, which leads to additional questions: Do the skills and knowledge that the teachers gain move to other areas, or are they already transferring their skills and knowledge to the new situation? What happens to students’ coding and cognitive skills from learning coding? How do teachers use their professional development experience in the short and long terms? Also, to account for the methodological limitations of the current study, a study sample with equal numbers of girls and boys (or one or the other) may provide additional insight into any gender effects. Finally, longer-term studies in contexts where teachers implement coding over a month, a semester, or even a school year may elicit additional information about all of these issues. Future research can address these questions and whether the use of a task engagement framework can be effective for research around the processes and outcomes of coding.