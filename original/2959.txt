Despite efforts for defining best practices for assessing Digital Game-Based Learning (DGBL) effectiveness, lack of scientific rigor is yet to be established. This has led to academics and educational practitioners doubting the quality of evidence and practical value of scientific research in educational settings. Hence, the present manuscript aims to test the feasibility of previously defined best practices by means of 3 feasibility studies: one in formal, one in health and one in corporate education. Results firstly show a more nuanced view on previously defined best practices regarding control groups: a) inclusion of an educational activity is not always desirable and depends on whether absolute or relative effectiveness is assessed and b) keeping instructional time equal in experimental and control group does not comply with the time efficiency outcome of DGBL. Secondly, several non-intervention related elements jeopardizing internal validity have been established: a) failed randomization, which can be tackled with blocked randomization and b) pre-test effects, which can be tackled with carefully piloted parallel versions of tests. Lastly, additional indicators for motivational and efficiency outcomes in a self-paced distance learning context have been established: a) motivation during/after the training should be expanded with motivation to start playing and b) time required to finish the training should be expended with time required to follow-up on learners. Recommendations in the present manuscript are not exhaustive or generalizable to all contexts, but do provide preliminary insights into feasible experimental designs for DGBL effectiveness studies.


Keywords
Digital game-based learning
Effectiveness assessment
Best practices
Feasibility studies

1. Introduction
Interest in and support for using games as instructional media or Digital Game-Based Learning (DGBL) keeps growing, with resources increasingly being invested in a wide array of contexts. In light of current events, we have entered an epoch where distant education and informal, self-directed learning are becoming the norm during certain periods. Accordingly, the need to know which games lead towards their intended learning goals is growing. Recent literature reviews have aimed to investigate the effectiveness of DGBL in several areas, finding mixed results (Gorbanev et al., 2018, Zhonggen, 2019; Hussein, Ow, Cheong, Hong, & Ebrahim, 2019; Acquah & Katz, 2020). This is related to the idea that some games are simply well developed and some games are not (Gee, 2005), similar to ‘good’ and ‘badly’ written text books. Hence, there is an increasing need to know what gaming and/or contextual elements foster effectiveness of DGBL in order to inform game design. Game characteristics and context of learning are indeed increasingly being taken into account in recent meta-analyses (Xu et al., 2019, Zhonggen, 2019; Acquah & Katz, 2020). However, quality of research design is not taken into consideration while it has been known to impact effect sizes. More precisely, high quality research designs are negatively correlated with positive effectiveness outcomes (Clark, Tanner-Smith, & Killingsworth, 2016). This can potentially lead to incorrect conclusions on which game characteristics positively influence effectiveness, providing a non-solid base for recommendations regarding game design and implementation of DGBL.

Despite several authors establishing lack of scientific rigor in the effectiveness assessment of DGBL in the previous decades (Randel, Morris, Wetzel, & Whitehill, 1992; Hays, 2005; Clark et al., 2016; Sitzmann, 2011) and several efforts for defining best practices (Mayer et al., 2012, Serrano-Laguna, Torrente, Manero, Blanco, & Borro-EscribanoFernández-Manjón, 2013; Mayer, 2015), the same issues are still brought forward: undetailed information regarding research design (Hussein, Ow, Cheong, Thong, & Ebrahim, 2019; Acquah & Katz, 2020), wrongly described research designs (Acquah & Katz, 2020) and non-rigorous research designs (Petri & von Wangenheim, 2017; Gorbanov et al., 2018; Xu et al., 2019; Hussein et al., 2019). All these elements point toward a lack of scientific rigor regarding conducting and reporting on DGBL effectiveness studies.

The lack of scientific rigor can be partly explained by the shortfall of resources afforded to evaluation, with game development using the largest share (Education Endowment Foundation, 2020). There are also 2 possible non-resource related explanations why recent systematic literature reviews still establish a lack of scientific rigor, despite formulations of guidelines and best practices for DGBL effectiveness assessment in academic literature. Firstly, more rigorous designs might lead to non-significant results (Clark, 2007; Clark et al., 2016; Hays, 2005) and do not get published as a result of publication bias. Publication bias is indeed an established issue in the context of DGBL effectiveness research (Chiu, Kao, & Reynolds, 2012; Xu et al., 2019). A second explanation is that defined best practices do not comply with the practical context where the games are being implemented (Pauline-Graf & Mandel, 2019). Randomized controlled trials, presumed as the gold standard, require too much effort and resources (Pauline-Graf & Mandel, 2019). Possibly, the provided best practices in literature are thus not a feasible approach.

The practical use and value of scientific research in educational settings has indeed been skeptically received by educational practitioners (Pauline-Graf & Mandel, 2019). On top of this, next to academics, educational practitioners doubt the quality of evidence of DGBL effectiveness studies as well (Gorbanev et al., 2018). There is, however, a need for evidence-based decisions in education and scientific research is an indicator for this evidence (Pauline-Graf & Mandel, 2019; Hussein et al., 2019).

There seems to be a mismatch between what is considered ‘methodologically sound’ and what is of practical value for educational practitioners. Hence, the present manuscript aims to reduce this gap by testing the feasibility of previously defined best practices (see Table 1) in several real-life contexts where educational developers and practitioners aimed to get insight in the effectiveness of the products they had invested in or developed.


Table 1. Best practices for effectiveness assessment of DGBL aimed at cognitive learning outcomes.

Best practices	Advantages	Disadvantages
Research design: general
1. Control group
1.1. Control group 1: ‘Business as usual’	Control if positive results are not result of mere lapse of time
Compare motivational aspects	Larger sample required
1.2. Control group 2: game without educational content
(optional)	Compare motivational aspects	Larger sample required
2. Pre-test	
•
Control for pre-existing differences between experimental and control group(s)

•
Determine progress/learning gains as a result of the intervention

•
Make it possible to control for characteristics of drop-outs (i.e., random or non-random attrition)

Practice effects
3. Similarity between experimental and control group should		
a) Randomization of subjects
or	Balanced groups in terms of observed and unobserved variables	Larger sample required
b) Randomization of classrooms/schools
or	Often more practical in educational research	Classroom/teacher/school influences
c) Matching (blocked randomized design)	Control for similarity between conditions	Unmatched latent variables can influence results
4. Follow-up study
(Min. 2 weeks after intervention has finished)	
•
Control for novelty effect

•
Control for positive results as a result of intensive training

•
Control for longer term effects

Attrition
Intervention
5. Training session	
•
Might reduce the need for procedural help during the intervention

•
Less cognitive load is used up for learning to work with the game-environment

Might bias result
6. DGBL as stand-alone intervention	Potential confounds are reduced	Ecological validity might be reduced
6.1. No adding of elements that contain substantive information	
6.2. Instructor role reduced to procedural help	
7. Instructor type
a) Researcher
Or	More experimental control	Ecological validity is jeopardized
b) Familiar person (current teacher)	Increases ecological validity	
•
Less experimental control

•
Teacher influences

8. Similarity between interventions should be assured by:	Potential confounds are reduced	Ecological validity might be reduced
8.1. Time of exposure		
8.2. Content		
8.3. Support received		
8.4. Environment		
8.5. Awareness of testing moment		
8.6. Reward for participation		
Participants
9. Min. 20 participants per condition	Determine differences in dependent variables between groups	More sophisticated analyses are not possible (e.g., covariance adjustment)
Measures
10. Instrument validity
10.1. Standardized tests
OR	
•
Have been validated

•
Provide suggestions with regard to timespan required between pre- and post-test for administering the same test

•
Might not exactly cover what has been discussed in the game/traditional class

10.2. Ad hoc test developed by researcher	
•
More closely aligned to content treated in game/traditional class

•
Pilot study required

•
Content might be too closely aligned to content treated in game/class

11. Student achievement (e.g., exam scores)	
•
Ecological validity measure

•
Can be influenced by other factors than the game/control intervention

•
Pupils should come from the same school (or even class)

•
Only relevant for longer term interventions (e.g., a whole semester)

Data analysis
12. Repeated measures	
•
Analyze interaction between condition and time

•
Differences with regard to the dependent variable(s) can exist between groups before the intervention

13. Add random effects if observed	More precise estimate of the treatment effect	Larger sample size required
14. Add participant characteristics as covariates
(e.g., game experience, computer skills, ability)	Take into account individual differences in order to determine for whom DGBL interventions are more beneficial	Larger sample size required
1.1. Feasibility studies
A feasibility study refers to preliminary research that aims to ‘trial-run’ a certain study, aiming to test the viability of research design aspects instead of pre-testing hypotheses, which are considered to be pilot studies (Pauline-Graf & Mandel, 2019). Pilot studies can also be feasibility studies but not all feasibility studies can be considered pilot studies (Whitehead& Campbell, 2014). Feasibility studies can also be conducted by means of other research methods, such as survey studies or meta analyses (Whitehead& Campbell, 2014). Both feasibility and pilot studies have smaller sample sizes (Pauline-Graf & Mandel, 2019; Whitehead& Campbell, 2014). Most published feasibility studies in the context of DGBL investigate the feasibility of implementing DGBL to reach a certain learning goal or the feasibility of implementation of DGBL in a certain context (Neville, 2010; Vate-U-Lan, 2015; Fernández-González, Carratalá-Tejada, Monge-Pereira, Collado-Vázquez, Baeza, Cuesta-Gómez, & Miangolarra-Page, 2019; Görgen, R., Huemer, Schulte-Körne, & Moll, 2020). Few feasibility studies investigate feasibility of methodology for DGBL effectiveness evaluation. Sabben et al. (2019) tested the feasibility of a study aimed towards the effectiveness assessment of a smartphone game to prevent HIV among youngsters in Kenya. Their study focused on the acceptability of study aspects among adolescents (i.e., the target group of the game) and their parents by means of gameplay log files, a questionnaire and focus groups. Results of the feasibility study showed that the intervention was generally accepted by both adolescents and parents. One lesson learned for the actual implementation of the study was that communication to parents regarding parent-child interaction should be optimized, in order to improve ecological validity of the intervention. More specifically, a majority of the parents were under the impression they could not interact with their children around the game and its contents. Di Palma et al. (2017) conducted a feasibility study to investigate whether wearable technology, in combination with serious games provide a valid methodology for monitoring autonomic response to socio-cognitive tasks for children with autism (the term ‘serious games’ is used here as this encompasses games that have applications outside the field of education such as therapy in this case). More specially, the aim of their feasibility study was to test the tolerability of the wearable sensors by youngsters with autism, and evaluate the possibility to detect modifications in physiological parameters (i.e., ECG) induced by socio-cognitive tasks mediated by serious games. Results of this feasibility study showed that this was a promising methodology to monitor autonomous response of children with ASD during treatment as the children did not show any issues with wearing the devices and completing all the tasks. Acceptability of the wearables improved throughout the treatment sessions. Moreover, the wearable technology could provide insights in longitudinal changes in the physiological response during the treatment, which can in turn increase efficacy of the treatment (i.e., empower tasks that can increase engagement in therapy and alter those that result in disengagement/stress).

To our knowledge, feasibility studies evaluating research design components for DGBL effectiveness assessment have not yet been published. This also does not seem to be a common practice in educational research. In health research, more published feasibility studies on research design components can be found, but the focus here lies on acceptance of the interventions, compliance or outcome measures (Arain, Campbell, Cooper, & Lancaster, 2010; Mengoni et al., 2017; Shoesmith, Charura, & Surr, 2020) rather than research design characteristics, such as design of experimental and control group or design of the interventions. Feasibility studies where research design components are evaluated and optimized are often not published as they are considered ‘simple piloting that is never meant to be published’ (Green et al., 2019, p. 5). Green and colleagues note, however, that publication of methodological tweaks as a result of this piloting could be of great value for the broader field of efficacy and effectiveness studies.

Feasibility studies on research design components of DGBL effectiveness studies can add value to the scientific community and educational practitioners for 2 reasons. Firstly, it supports development of validated recommendations for assessing effectiveness of DGBL, unveiling less feasible theoretical best practices and providing alternatives that are considered more feasible in a practical context. Secondly, feasibility studies can provide a more nuanced definition of ‘quality of research design’ that often serve as inclusion or evaluation criteria in systematic reviews and meta-analyses. Definition of ‘quality of research design’ is often limited to whether one has conducted a randomized trial and/or included an educational activity in the control group.

Hence, the focus of this manuscript is on the feasibility of research design components in order to further guide design of effectiveness studies of DGBL. More specifically, the present study will investigate the feasibility of previously defined best practices for effectiveness of DGBL by means of 3 feasibility studies in 3 main areas where DGBL is implemented: formal education, corporate education and health education. Since these studies can also be considered as pilot trials as outcome measures are evaluated, a second aim of this paper is to establish whether a more rigorous research design leads to less positive results.

2. Methodology
Three feasibility studies were conducted in 3 different contexts. Inclusion criteria for the games to be included in a feasibility study were a) that they had to be aimed towards cognitive learning outcomes, b) that they are implemented in a real-life context (and not developed for research purposes) and c) come from different application fields. In the present manuscript, the focus lies on effectiveness assessment of DGBL that primarily aim towards cognitive learning outcomes, considering that different types of learning outcomes require different types of assessment and thus resist categorization in one research taxonomy (Kraiger, Ford, & Salas, 1993). As a result, the previously defined best practices by the researchers (see Table 1) only focused on games that primarily aim towards cognitive learning outcomes. DGBL that primarily aim towards skill development or attitudinal/behavioral change were thus excluded. The criterion related to implementation in different application fields was important in order to develop an overarching methodology for assessing effectiveness of games targeted at cognitive learning outcomes that can be flexibly used across different sectors. More specifically, effectiveness of a game aimed at teaching English vocabulary to primary school children (i.e., formal education with a focus on declarative knowledge), a game aimed at teaching fire safety procedures to hospital staff (i.e., education for health professionals with a focus on procedural knowledge) and a game aimed at teaching client-oriented principles for bank employees (i.e., corporate education, with a focus on strategic knowledge) were assessed.

To design the experiment, previously published best practices by the authors All, Nuñez Castellar, & Van Looy (2016) were used as a guideline (see Table 1). These best practices were the result of a systematic literature review and expert interviews. In all studies, learning, motivational and efficiency outcomes have been taken into account (All, Nuñez Castellar, & Van Looy, 2015).

The aim of the current manuscript is to provide a reflection on lessons learned for further design of DGBL effectiveness studies using an experimental approach. Considering the feasibility studies included in this manuscript are also published as separate studies, the methodology section below provides a summarized version of the methodology implemented per study. More detailed information on methodology and results can be found in the separate publications.

2.1. Feasibility study 1: English vocabulary learning in a school context (declarative knowledge)
In the first feasibility study, the effectiveness of an English vocabulary learning game (All, Nunez Castellar, Meesschaert, & Van Looy, 2015); has been assessed (N = 71). In this study, DGBL (n = 23) was compared to a traditional class (n = 24) and a game + debriefing group (n = 23), to investigate whether adding a debriefing session would lead to additional learning.

2.1.1. Participants
Pupils of the fourth and fifth grade participated in the study. Each grade consisted of 2 class groups. Randomization occurred on subject level, to avoid influences of age, gender, previous English vocabulary knowledge and game experience. No significant differences could be found between the experimental conditions in terms of gender distribution, previous game experience or previous knowledge of English vocabulary or grade the participants belong to.

2.1.2. Materials
The DGBL learning platform can be played on tablets or on a computer in a web browser. For the current study, the browser-based version was used and logins were provided by the developer. The game is a virtual world called Mingoville where the Pinkelton family lives. The game itself consists of 10 thematic missions. Every mission consists of several activities where they can gather coins for correct answers. Once an activity is successfully completed, the pupil earns a ‘feather’ and once a pupil has gathered 3 feathers, he/she reaches another level. The game developer makes a distinction between 3 types of activities in the missions: interactive stories, games and creative lab activities (see Table 2 for examples). Every mission starts with an interactive story to let the pupils get acquainted with the vocabulary to be practiced in the mission. After this introduction, children practice the vocabulary in these games, interactive stories and creative lab activities (which all can be considered minigames using play to practice vocabulary). Mingoville can thus be considered as an extrinsic game type (Ang & Zaphiris, 2006), i.e. “a structured series of puzzles or tasks embedded in a game or narrative structure with which they have only the most slender connection” (p. 10). For the purpose of this study, pupils played the first mission of the game.


Table 2. Activities of mission 1 of (name game anonymized for blind review purposes) pupils had to complete for the study.

Activity	Type	Description
Meet my family	Interactive story	Recognizing and indicating Candy's family members
Color my family	Game	Coloring family members in the colors instructed
Play Memory	Game	Linking images to written words, spoken words to written words and spoken words to images in the form of a memory game
When Ryan met Martha	Interactive story	Listening exercise accompanied by text, where the students need to illustrate the story by linking the images to the storyline
Make a family picture	Creative lab	Kids have to make their own picture by dragging the picture elements from the mission on to the screen.
Pacman	Game	Spelling words by eating the correct letters in the context of a pacman game
In mission 1, the family is about to take the family photo, but little sister Andrea is missing. The family photo cannot be taken before she is found. An overview of the activities can be found in Table 2.

2.1.3. Procedure
Table 3 provides an overview of the research design.


Table 3. Overview of procedure followed in the 3 conditions.

Condition	Pre-test	Intervention	Post-test	Instructor	Role instructor	Follow-up test
Game
(n=23)	English vocabulary test, game frequency, age	Game, in class, played individually (40 min)	English Vocabulary test + relative enjoment scale	ICT Coördinator & Researcher	Procedural help (related to technology/gameplay)	After 3 weeks
Traditional class
(n=24)	English vocabulary test, game experience, age	Class, classical (40 min)	English Vocabulary test + relative enjoment scale	Teacher	Content instruction	After 3 weeks
Game + debriefing
(n=23)	English vocabulary test, game experience, age	Game, in class, played individually (20 min) + debriefing session, collectively (20 min)	English Vocabulary test + relative enjoment scale	ICT Coördinator & Researcher	Procedural help & teacher leading debriefing session	After 3 weeks
After 20 min, the pupils in the game + debrief condition were requested to end the game because 20 min of debriefing would follow. We have chosen for an expert-led oral debriefing session, which was structured according to the conceptual model proposed by Van Der Meij and colleagues (2013) presented in Table 4. The debriefing session was led by the teacher asking questions to the class group such as ‘What do you think was the aim of this game?’, ‘Where there words present in the game that you already knew?’ ‘Which words were new to you?’, ‘Do you think that the words you have learned are useful? Where would you be able to use what you have learned today?’. In appendix A, notes taken during the debriefing can be found.


Table 4. Conceptual model for debriefing with phases, topics and leading questions.

Phases	Topics	Leading questions
Concrete experience	Events	What happened?
Emotions	How did you feel?
Reflective
observation	Empathy	How do you value this experience?
Abstract
conceptualization	Explanations	What did you learn?
What would have happened if … ?
Active
experimentation	Every day	How are the game events and reality connected.
Employment
Evaluation	How do you go on from here?
What would you do differently?
Adapted from Van Der Meij, H., Leemkuil, H., & Li, J.-L. (2013). Does individual or collaborative self-debriefing better enhance learning from games? Computers in Human Behavior, 29(6), 2471–2479. p. 2473.

For the traditional class, the teacher was provided with a lesson schedule to follow when giving the class. The lesson was designed in such a way, that it covered exactly the same content that was treated in the game. For this, a content analysis of the DGBL platform was conducted. A large drawing with family members was also provided to be presented on the blackboard, to keep the instructional method as similar as possible. Also, paper and pencil exercises were provided for the pupils in the traditional class, which served as support material during class and were jointly filled out. This was added to the lesson, because this is the way pupils are used to being instructed in the classroom and deviation from the current way the lessons are designed was minimized.

2.1.4. Measures
a)
Learning Outcomes- Considering that English is not part of the school curriculum for the pupils who participated in this study, validated English tests were not available at the moment of testing. Hence, a test was developed by the researchers in cooperation with one of the teachers based on an analysis of the game content. Three tests were developed: a pre-test, a post-test and a second post-test which was administered 3 weeks after the end of the intervention. The pre- and post-tests were not identical, but fully interchangeable tests (i.e., same types of questions and same type of difficulty level), in order to reduce the risks of a practice effect, considering that the pre-test and post-test were administered the same day. The pre- and first post-test were piloted (N = 15) to see if undesirable effects could be observed (i.e., ceiling effect, non-normal distribution of the data or non-parallelism of the pre- and post-test). An analysis of variance indicated that the pre-test (M = 16.37, SD = 1.59) and post-test (M = 15.69, SD = 2.12) could be considered as interchangeable versions (F = 0.49, p = .50). Also, we failed to find deviations from normality and homogeneity of variances. The follow-up was developed according to the exact same protocol as the pre- and the first post-test.

b)
Motivational outcomes-the validated Relative Enjoyment Scale (RES) was used as an indicator for motivational outcomes (Van Looy, 2016; Nuñez Castellar & Houttekiet, 2016).

c)
Efficiency outcomes – time-on-task was kept equal for all groups

2.2. Feasibility study 2: fire safety training in a health context (procedural knowledge)
In our second feasibility study (All, Nunez Castellar, & Van Looy, 2017) a game developed for a hospital in order to provide staff with the yearly required fire safety training was compared to the slide-based lecture that was ‘business as usual’ (N = 133). In this study, we also investigated pre-test influences by means of a Solomon 4-group design, in order to provide recommendations regarding implementation of a pre-test of knowledge in DGBL effectiveness studies. This means that one experimental group received a pre-test before the DGBL training (n = 34) and one experimental group did not receive a pre-test before the DGBL training (n = 33). Similarly, one control group received a pre-test before the slide-based lecture (n = 39) and one group did not (n = 29).

2.2.1. Participants
This study was conducted in collaboration with the hospital for whom the game was developed. Participants were recruited on the 4 campuses of the hospital by subscribing for the yearly fire safety training. Randomization in this experiment occurred on a group level (i.e., a group was composed of people that subscribed for a safety training on the same date).

2.2.2. Materials
All hospital personnel (i.e., doctors, nurses, cleaning personnel, administrative staff, technical staff, etc.) is required to complete the fire safety training every year. Because the hospital has expanded over the years, is still expanding and personnel works in different shifts, it is becoming increasingly difficult to organize traditional training for everyone. Hence the decision to develop a digital game in cooperation with DAE studios (i.e., the research and development cell of the Digital Arts & Entertainment study program at HOWEST college). The researchers of the present manuscript were not involved in development of the game, but were solely asked for evaluation of the final product. The game consists of three mini-games or courses: ‘small fire’; ‘smoke’ and ‘blaze’. After participants have completed these courses, they can also play a random ‘fire safety’ scenario, during which elements learned in the course can be practiced. In total, 6 different scenarios are available. The game can be freely played on the following website: http://sggo.howest.be/het-serious-game.

Table 5 provides an overview of the game.


Table 5. Overview of the fire safety training game.

Minigame	Type	Description
Small fire	Interactive simulation where player can earn coins by providing the right answers regarding actions to take	A small fire has arised in a hospital room. The player has to take several steps when this happens (e.g., internal alarm and different options, extinguish fire and different options, etc.)
Smoke	Interactive simulation where player can earn coins by providing the right answers regarding actions to take	Smoke is coming from a hospital room. The player has to take several steps when this happens (e.g., feel heat at the door, right position to open the door, etc.)
Blaze	Interactive simulation where player can earn coins by providing the right answers regarding actions to take	There is a blaze in a hospital room. The player has to take several steps when this happens (e.g., internal alarm, external alarm, evacuation in right order, etc.)
2.2.3. Procedure
Table 6 provides an overview of the research design.


Table 6. Overview procedures followed in the 4 conditions.

Condition	Pre-test	Intervention	Instructor	Instructor role	Post-test
Game 1
(n = 34)	Fire safety knowledge test, game skills, game frequency, age, gender	Game, in conference room, individually	Two researchers	Supervision, provide procedural help if necessary	Knowledge test (same as pre-test), Intructional Material Motivation Survey, Instructional time
Game 2
(n = 31)	Game skills, game frequency, age, gender
Control 1
(n = 39)	Fire safety knowledge test, game skills, game frequency, age, gender	Slide-based lecture, conference room, classical	Two researchers and one prevention manager	Prevention manager: teach class
Researchers: observation
Control 2
(n = 29)	Game skills, game frequency, age, gender
The control groups received the slide-based lecture in a conference room on one of the four campuses. The slide-based lecture was given by the prevention manager responsible for the fire safety training. The subjects were instructed in groups of minimum 8 and maximum 20 people. During every slide-based lecture, the same two researchers who were present during the DGBL intervention were present during the slide-based lectures to check whether all topics discussed in the game were also discussed in the slide-based lecture using a topic list.

2.2.4. Measures
Learning outcomes - In order to assess performance, a test was developed by researchers of DAE in cooperation with the prevention staff responsible for the fire safety training – the same staff who provided the slide-based lectures. Using a standardized test was not possible, as procedures are hospital-specific. The test had previously been implemented in a pilot study (N = 52) in an initial phase of development of the game. The test consisted of 18 open-ended questions, covering all topics that are treated in the interventions allowing for a maximum score of 40. The tests were corrected by two researchers. For this purpose, an evaluation form was developed in order to guarantee a standardized manner of correcting the tests. If there was uncertainty regarding the correctness of certain answers, the correctors discussed the response and agreed upon a score.

Motivational outcomes – Validated questionnaire Instructional Materials Motivation Survey (IMMS, Keller 1987) was used to assess motivation towards the instruction method. We based ourselves on Huang and colleagues (2010) for the game version of the IMMS. The IMMS consists of 36 items, divided in 4 subscales: attention (i.e., gaining and keeping the learner's attention), relevance (i.e., activities must relate to current situation or to them personally), confidence/challenge (i.e., activities cannot be perceived as too hard or too easy, which is also a prerequisite for an optimal game experience or game flow) and satisfaction/success (i.e., learners must attain some type of satisfaction or reward from the learning experience). The items were scored on a 5-point Likert scale, with 1 being ‘not true’ to 5 ‘very true’. The total score represents motivation towards the instructional material. The scores on the subscales give an indication as to the sub dimensions on which the intervention was either more or less successful (Keller 2010).

Efficiency outcomes – time-on-task was assessed by timing every separate slide-based lecture and retrieved individual information on total time spent on the DGBL intervention based on automated logging.

2.3. Feasibility study 3: client-oriented principles in a corporate context (strategic knowledge)
In our third feasibility study (All, Plovie, Nunez Castellar, & Van Looy, 2017) which took place in a corporate context (N = 64), we compared a game (n = 20) developed for a bank to teach employees client-oriented principles (e.g., make eye contact within 3 s) with a group that received a more passive e-learning approach by means of an instructional video (n = 21). This way, the added value of interactivity of the game could be investigated. Both instructional groups were compared to a no-activity control group (n = 23).

2.3.1. Participants
Participants were recruited by an e-mail sent by the training manager (N = 89). More specifically, e-mails for participation were sent to employees that had started working for the bank between 1 and 12 months before the start of the study and had not yet played the game. The DGBL training is compulsory. Blocked random assignment (i.e., ‘matching’) was used to assign participants to conditions. Blocks were created based on age, number of months working at the bank and gender.

2.3.2. Materials
The game that was tested has been developed for a large bank – who requested anonymity when publishing results - by a commercial e-learning developer in order to teach new employees the bank's basic principles of customer-friendliness in order to improve their loyalty to the bank. The game consists of 5 minigames. The first minigame focusses on client-oriented rules that should be applied before clients are received (e.g., clean office); the second on client-oriented principles that should be applied at the reception (e.g., make eye contact with entering customers); the third on client-oriented principles to be applied when dealing with a client (e.g., empathize with the environment of the customer); the fourth on client-oriented goodbye (e.g., accompany the client to the exit) and the last minigame on client-oriented organization during a day at the office (e.g., follow up on the bank's general mailbox). The game is available to all employees via the online learning platform of the bank, accessible only via the intranet of the bank. The minigames can therefore only be played in the workplace. Table 7 provides a description of the game.


Table 7. Overview of the bank game aimed towards client-friendliness principles.

Mini-game	Goal	Play-time	Gameplay
1	Client-oriented principles to be applied before clients are received (e.g., clean office, briefing)	±5 min	The player gets 5 min to get everything that needs to be done before opening the office in order. At the end of the simulation he gets an overview of what he has done and what he has forgotten, linked to a score.
2	Client-oriented principles that should be applied at the reception (e.g., make eye contact with entering customers)	±10–15 min	Minigame with 9 levels where one has to drag and click images in the right order in a grid below (e.g., picture of a broken ATM should come before a picture of an entering costumer). Some activities also need to occur within a certain time (e.g., 3 s to make eye contact with an entering costumer). Every level contains more images to be sorted.
3	Client-oriented principles to be applied when dealing with a client (e.g., empathize with the environment of the customer)	±10–15 min	Minigame based on the format of the TV show Who wants to be a millionaire?, which was a quiz where the player can choose from 4 answers to a question and can get several helpline options.
4	Client-oriented goodbye (e.g., accompany the client to the exit)	±5 min	A graphic novel which the player has to complete by choosing from several options to fill in the blanks in the story.
5	Client-oriented organization during a day at the office; This final game consists of all client-oriented principles learned in the previous minigames.	±10–15 min	The player is in charge of a certain dashboard, in charge of a day at the office. At the top of the dashboard there is a customer satisfaction meter that can turn red. To know what to do to when the customer satisfaction meter reaches the red zone, the player can look at several meters that correspond to tasks that need to happen during the day. These can also turn red, so the player needs to know where actions should be taken. The player can also assign tasks to two colleagues during this game.
For the purpose of this study, an instructional video was developed, using the game and game play as a basis. For this purpose, the screen of the game was captured while being played by the researcher. To make it look more like an instructional video and less like a game, in game actions were accompanied by texts boxes, explaining why a certain decision is taken or why a certain action is being carried out. The instructional video training was also subdivided into 5 separate video's corresponding the 5 topics in the minigames. This way, the same training format could be applied: the employees could spread the training over several days, to their own time convenience. It takes 35 min and 15 s to view all instructional videos.

2.3.3. Procedure
After filling out the pre-test online, participants received 6 weeks to complete the training (game or instructional video). It was not necessary to play/watch all five games/video's consecutively, but they could choose to spread them over several days/weeks. The game was thus played in the context in which it was meant to be played: during working hours and at the employee's convenience.

The training manager could retrieve weekly reports on who participated in each mini game/video and provided them to the researcher. One week before the six-week intervention period had passed, the researcher sent a reminder to those who did not finish the game yet, asking them to complete the training considering they would receive a post-test a week later. If they still not had finished the training 6 weeks after the pre-test, the researcher contacted the employees by phone. Once the employees had finished the training, the researcher sent them an e-mail with the link to the post-test. Table 8 provides an overview of the study design. The control group simply received a post-test 6 weeks after they had filled in the pre-test (Table 9).


Table 8. Procedures followed in the 3 conditions.

Condition	Pre-test	Intervention	Instructor	Instructor role	Post-test
Game	Client-oriented principles, game skills, previous bank experience	5 minigames, during office hours, at the office, individually	N/A	N/A	Client-oriented principles, Instructional Material Motivation Survey, Enjoyment
Instructional video	5 video's, during office hours, at the office, individually	N/A	N/A
Control	No intervention	N/A	N/A
2.3.4. Measures
Learning outcomes -Two parallel versions (i.e., same types of questions and difficulty level) of knowledge tests were developed based on the content treated in the games, in cooperation with the training manager of the bank. We choose for administrating parallel versions pre- and post-intervention, to reduce pre-test influences (Crawford, Stewart, & Moore, 1989; Randel et al., 1992). Tests development consisted of 3 iterations: a first version of the test was piloted among a convenience sample of 18 participants (9 received version A, 9 received version B) who have no prior experience with working in a bank and are not currently working at a bank to test whether or not the test was too easy (e.g., too obvious what the correct answer would be) and whether the parallel versions of the tests could be considered equal regarding difficulty level. Results showed that participants receiving version A scored significantly higher than participants receiving version B, F(1,16) = 5,36, p = .03. Consequently, the tests could not be considered as parallel versions. Based on the average correct answers per question, a new version of the test was created. This new version was piloted among 14 employees at the bank who have been working there for several years, of which 6 received version A and 8 version B. Results of this final pilot showed no significant difference on the total score between the versions, F(1,12) = 0.31, p = .59. Example questions from the knowledge test are: ‘When you enter the office, which 5 tasks are important to finish before opening for clients? (open question)’/‘Rank the following tasks based in importance (with 1 being the most important task and 3 the least important ask): a) Fix a broken ATM, b) Administration, c) A client who walks in/A client walks in the bank, what is the first thing you need to do? a) Stop what you are doing and immediately address the client b) Make eye contact to let the client know you have noticed him/her, c) Quickly finish what you were doing and then address the client.

Motivational Outcomes - Validated questionnaire Instructional Materials Motivation Survey (IMMS, Keller 1987) was used to assess motivation towards the instruction method. We based ourselves on Huang and colleagues (2010) for the game version of the IMMS (see section 2.2.4).

Efficiency outcomes – this exercise had previously been done by the training manager and formed the base for the decision to go for a game-based training instead of an oral class (see section 3.3).

3. Results
3.1. Feasibility study 1: English vocabulary learning in a school context (declarative knowledge)
The defined best practices described in Table 1 worked well in a formal school context: by implementing them we did not run in to elements that were considered ‘unfeasible’. Randomization on subject level instead on group level occurred, as several grades were involved (2 third and 2 fourth grade classes). Randomization on subject level was thus the only way to increase our chances of equal distribution regarding relevant participant variables over the 3 conditions, as we can assume that pupils from the fourth grade might have a better knowledge of English than pupils from the third grade.

A repeated measures MANOVA (see Fig. 3) showed a significant main effect of time with a large effect size, F(2,136) = 82.23, p < .001, r = 0.47. Pairwise comparisons showed that there is a significant difference between the pre-test and the post-test (p < .001), between the pre-test and the follow-up test (p < .001), but not between the post-test and the follow-up test (p = .14). The interaction between time and condition was significant, with a small effect size, F(4,136) = 5.63, p < 001, r = 0.17. Analyses of variance on the gain scores were conducted to follow up on the significant interaction. Results show that on the short term no significant differences could be found regarding vocabulary retention between the DGBL group and traditional class group (p = .16). The game + debriefing group scored significantly lower than the traditional classroom group at post-test 1 (p < .001). The difference between the game group and game + debriefing group was not significant (p = 0.10). This was rather unexpected. On the longer term (after 3 weeks), the traditional class outperformed both the DGBL group (p = 0.03) and game + debriefing group (p < .001). Remarkably, on both short and long term, the debriefing session did not add value to the game-only condition. A possible explanation for this is that concentration of the game + debriefing group was lower as they received the instruction in the second lesson hour (10h15-11h30). The game only and traditional classroom instruction occurred in the first lesson hour (9 h–10h15). This was for practical reasons as there was only one computer class in the school. While the interventions were all implemented on the same day at the same primary school, even a difference in moment in time can thus possibly confound results.

Fig. 3
Download : Download high-res image (158KB)
Download : Download full-size image
Fig. 3. Line plot of scores on pre-, post and follow-up test.

As the assumption of homogeneity of variance was not met for the Relative Enjoyment Scale, we have conducted the non-parametric Kruskal-Wallis test to analyze the RES data. Motivation for the both the game (p = 0.053) and game + debrief group (p = 0.01), however, was significantly higher than the traditional class group. No differences were found between the game and game + debrief condition. Fig. 4 provides an error bar graph of the RES scores for separate conditions.

Fig. 4
Download : Download high-res image (62KB)
Download : Download full-size image
Fig. 4. Error bar graph of mean scores on Relative Enjoyment Scale.

We can draw three lessons from this feasibility study for DGBL effectiveness assessment. Firstly, a follow-up test is indispensable for conducting effectiveness research on DGBL in order to determine whether initial effects can be sustained over time. Secondly, similarity between interventions regarding time of day is also important to control for. An alternative declaration is that considering that Mingoville is an extrinsic game type (this is now also discussed in section 2.1.2), learning content is rather simple (memorizing words) and made explicit in the game-based activities, making the connection between game content and classroom content -which is the aim of a debriefing session-possibly unnecessary. Another alternative explanation is that the debriefing led to less in-game exercise time distracting them from the content, focusing on their own experiences at the expense of study time. Nevertheless, we have established that adding extra elements during the implementation of DGBL, such as a debriefing session, can yield different results and makes it impossible for researchers to get an indication of the educational game as such, without the addition of the extra elements. Consequently, in the context of DGBL effectiveness research, DGBL should be implemented as a stand-alone intervention in order to establish the effectiveness of the game as such. If one does add elements to the intervention, effectiveness claims can only be made regarding the intervention as a whole.

3.2. Study 2: feasibility study 2: fire safety training in a health context (procedural knowledge)
In the second feasibility study, keeping exposure time to instructional intervention equal proved to be not only impossible, but undesirable. After the first sessions, it became clear that it took less time to complete the DGBL training. This lead to a discussion with the training manager of the hospital, with researchers suggesting to repeat playing the game until same time-on-task as the traditional class was achieved. The training manager stated that requiring less time to finish the training was a positive thing. Hence, we decided to add time-on-task as an outcome variable.

Considering that we could distinguish two designs (pre-test post-test design and post-only) in our data (van Engelenburg, 1999) we conducted an analysis on two datasets: one containing the participants receiving both a pre- and a post-test and one on the participants only receiving a post-test for assessing the effectiveness of the intervention. A problem we ran into when analyzing the data of the pre-test post-test design, is one of failed randomization. More specifically, the DGBL group scored significantly higher on the pre-test than the lecture group. In a pre-test, post-test control group design, recommendations on how to handle this are clear: conduct an analysis of covariance, including the pre-test score as covariate or conduct an analysis of variance on the gain scores (Dimitrov & Rumrill, 2003; Knapp & Schafer, 2009). Considering that there is no agreement on which one to use and that the aim of the present study is to explore the disadvantages and advantages of adding a pre-test in your study design, we have conducted both. Results of the ANCOVA (N = 73) show that, after controlling for initial differences on the knowledge test, the game group scores significantly higher on the post-test than the slide-based group (1,71) = 18,357, p < .01, r = 0.35. An ANOVA on the change scores, however, does not show a difference F(1,71) = 0.22, p = .88, r = 0.02. An ANOVA on the IMMS shows a significantly higher score for the groups who received the game-based intervention F(1,66) = 8.64, p = .01, r = 0.34. Finally, it takes significantly less time to complete the game-based training F(1,71) = 54,61, p < .001, r = 0.66. Table 9 provides an overview of descriptive statistics for the pre-test –post-test design.


Table 9. Descriptive statistics of pre-test post-test design (N = 73).

Group	N	Pre-test score (M/SD)	Post-test score (M/SD)	Minimum score (pre-test/post-test)	Maximum score
(pre-test/post-test)	Adjusted post-test score (M/SD)	Gain score (M/SD)	Time spent on the intervention	Total score IMMS (M/SD)
Game group	34	16.19/7.8	34.44/5.64	3.5/17	33/39.5	33.75/.84	18.03/5.45	35,08 min	4.21/.48
Lecture group	39	9.27/5.2	27.29/5.31	1/18	14.5/35	28.63/.91	18.25/7.30	25,18 min	3.86/.48
When analyzing the group that did not receive pre-test (N = 60), a treatment effect in favor of the DGBL intervention can be found F(1,58) = 104,23, p < .001, r = 0.80. No difference could be found for the IMMS, F(1,54) = 2,606, p = .11, r = 0.2. When we look at the subscales, however, a difference can be found for satisfaction in favor of the DGBL training, F(1,56) = 5,021, p = .03, r = 0.29. In this sample, it also takes significantly less time to complete the game-based training F(1,55) = 46,42, p < .001, r = 0.68. Table 10 provides an overview of descriptive statistics for the post-test only design.


Table 10. Descriptive statistics of the post-test only design (N = 60).

Group	N	Post-test score (M/SD)	Minimum score post-test	Maximum score
Post-test	Time spent on the intervention	Total score on IMMS
(M/SD)
Game group	31	35.05/3.78	25.50	39.5	35 min	4.14/.36
Lecture group	29	21.60/6.65	7	31.5	24,59 min	3.98/.41
For the analysis of the Solomon 4-group design, including the whole sample, there are no guidelines on how to handle pre-existing differences. We have handled this by conducting the analysis twice: once on the whole sample and once on a smaller sample, that ‘matched’ participants from the experimental and control group on their pre-test scores. The latter excluding participants for whom no match was found.

Results of the 2 (pre-test or no pre-test) x2 (slide based lecture or DGBL) ANOVA on the whole sample (N = 133) showed is a very large main effect of instruction type in favor of the game-based training F(1,129) = 136.67, p < .01, r = 0.71 and a small effect of the pre-test F(1,129) = 5.22, p = .02, r = 0.14. An interaction effect between pre-test and instruction type is also found F(1,129) = 7.46, p = .01, r = 0.17. More specifically, participants that received a pre-test before the slide-based lecture had significantly higher post-test scores than the slide-based group that did not receive a pre-test. In the game group, no such differences were found. Fig. 5 provides a line plot of mean post-test scores on the whole sample.

Fig. 5
Download : Download high-res image (147KB)
Download : Download full-size image
Fig. 5. Line plot of mean post-test scores on the whole sample (N = 133).

For the matched sample (N = 102) we find similar results: F(1,98) = 46,29, p < .01, r = 0.59 effect of instruction type in favor of the game-based training F(1,129) = 136.67, p < .01, r = 0.71 and an interaction effect between pre-test and instruction type is also found F(1,98) = 16.42, p < .01, r = 0.28. The main effect of pre-test, however disappears F(1,98) = 2.7, p = .1, r = 0.12. An interaction between instruction type and pre-test administration is still detected, showing a small to medium effect F(1,98) = 16.42, p < .01, r = 0.28. Again, participants from the slide-based lecture receiving a pre-test score significantly higher than participants from this group that did not receive a pre-test. In the game group, no such differences are found. Fig. 6 provides a line plot of mean post-test scores of matched sample.

Fig. 6
Download : Download high-res image (136KB)
Download : Download full-size image
Fig. 6. Line plot of mean post-test scores of matched sample (N = 102).

The fact that pre-test sensitization does not occur in the DGBL group also confirms the effectiveness of DGBL, as the interactivity of the game required them to be attentive, regardless of whether they received a pre-test or not before the DGBL intervention.

However, this approach still leaves insecurity as to what extent the groups that did not receive a pre-test had the same baseline knowledge regarding fire safety. The Solomon design seems to be a good design if one has perfect data, but in the social sciences, data is often not textbook perfect and researchers do not have the opportunity to gather over thousands of data points assuring successful randomization. Nevertheless, the Solomon design has proven its added value, as it provides us with a more nuanced view of our data. For instance, we can make a more supported claim on the DGBL effectiveness regarding learning outcomes and know that we have to be careful with the interpretation of our results regarding motivational outcomes. We have also established the advantages of adding a pre-test, indicating pre-existing differences between experimental and control group. Consequently, when looking into the effectiveness of the DGBL treatment, we could control for these initial differences by adding pre-test scores as a covariate and thus have a more precise estimate of our treatment effect.

A final issue we bumped in to in this feasibility study is that we planned a follow-up tests 6 months after the training. This proved to be very challenging for 2 reasons: either people did not work at the hospital anymore or people needed to be excluded from the analysis, as they had decided to revise the fire safety training for the follow-up test (even if we had specifically asked them not to). As a result, there were not enough eligible participants to conduct the analysis of the follow-up test.

3.3. Feasibility study 3: client-oriented principles in a corporate context (strategic knowledge)
In this study, a first discussion we had with the training manager of the bank was the activity to be implemented in the control group. There was no ‘traditional instruction’ method to compare the game to, as the decision for a DGBL training was made based on cost-efficiency reasons. The game was developed, because the bank's costumers' loyalty to the bank had decreased. Hence, they decided to develop some client-friendly principles to be applied at the office in order to improve this loyalty. They had chosen for a game-based format for cost-efficiency reasons. A cost-benefit analysis where the game was compared with hypothetical oral classes of 15 people, the game proved to be more cost-efficient after 50 sessions. The training manager was simply interested in knowing whether the game achieved in increasing knowledge regarding the client-friendly principles. Hence, together with the training manager, we decided to include a no-activity control group. In the case that there was a significant difference from pre-to post-test, this would allow us to investigate whether this was the result as a mere lapse of time (by gaining on-the job experience, for instance) or whether this was the result of the DGBL training. The addition of the passive e-learning condition using an instructional video was thus an interesting addition for the researchers and not upon the request of the training manager.

Four participants from the instructional video group did not complete all four video's and three participants from the game group did not complete all mini games when filling out the post-test. Hence, we have conducted the analyses twice: once on the complete dataset (n = 64) and once only including the participants that have fully completed the training (n = 57). Results show a significant gain from pre-to post-test (p < .01) with a large effect size for both the game and instructional video group (r = 0.57 for the complete game group, r = .51 for the complete instructional video group, r = .54 for those who fully completed the game training and r = 0.57 for those who fully completed the instructional video training). The control group shows no significant difference between pre and post-test (p = .14). For the complete dataset, the biggest gain from pre-to post-test can be found in the game group (M = 5.5, SD = 4.93), followed by the instructional video group (M = 4.86, SD = 4.84). The control group slightly declined (M = −0.1.07, SD = 3.38). An ANOVA on the gain scores shows a main effect of treatment with a large effect size F(2,61) = 14.90, p < .001, r = 0.57. Post hoc Scheffé tests show that the gain of the game and video group is significantly larger than the control group (p < .001). No significant differences can be found regarding progress on the knowledge test between the game and instructional video group (p = .90). Fig. 7 provides an overview of pre- and post-test scores for all groups.

Fig. 7
Download : Download high-res image (108KB)
Download : Download full-size image
Fig. 7. Pre- and post-test scores for all groups.

For the IMMS, no difference could be found between both instructional groups F(1,38) = 0.27, p = .61. The motivation rationale behind DGBL did not hold true in this case pointing to the need for careful consideration as to where to use interactive content.

An issue we encountered during the study was that the decision to choose for an ecologically valid context of implementation in turn threatened ecological validity. While almost everyone filled out the pre-test, a major issue in the present study was motivating the employees to start playing the game, even though it was compulsory. The researcher had to track activity of every individual participant, following up on whether or not they had already started playing the minigames/watching the instructional videos. Subsequent e-mailing and calling participants several times to finish the training reduced external validity as this is not common practice in the corporation.

4. Discussion and conclusion on best practices
Based on these 3 feasibility studies, implementing previously defined best practices defined by the researchers (Table 1), we can conclude that certain best practices are not only unfeasible, but also not desirable. More specifically, adding an educational activity in the control group and keeping time-on-task equal between experimental and control group are not always desirable. This manuscript also established several non-intervention related elements that can jeopardize internal validity: failed randomization and pre-test effects. The feasibility study in the corporate context also pointed out additional indicators to take into account when assessing motivational and efficiency outcomes in a self-paced distance learning context. Feasible practices for keeping experimental and control group equal (both on participant characteristic as intervention level) were also brought forward in the present manuscript. Finally, a tradeoff should be made between gathering long term data and the possibility to find enough eligible participants. A more detailed description of these considerations can be found below.

4.1. Rethinking best practices regarding control groups
4.1.1. No activity in the control group is meaningful when assessing absolute effectiveness
Results show that inclusion of an educational activity - which we previously defined a best practice - in the control group is not always desirable and meaningful comparisons should be made. What became clear during these feasibility studies is that effectiveness of DGBL is a complex construct. An important distinction that comes forward in this manuscript is the difference between absolute effectiveness and relative effectiveness. This is somewhat in line with Mayer's (2015) cognitive consequences and media comparison approach. The only difference is that the cognitive consequences approach also applies for all DGBL and is not limited to commercial-off-the-shelf games. What type of activity one should implement in the control group will ultimately depend on whether one wants to assess absolute effectiveness (i.e., does DGBL succeed in achieving its predefined goals?) or relative effectiveness (i.e., is DGBL similar or better compared to the other instructional media?). The former primarily refers to learning outcomes and refers to the investigation of progress regarding those learning outcomes as a result of the game. Hence, this requires an analysis from pre-to post-test. It is still recommended to also have a control group, to investigate whether differences between pre- and post are a result of the mere lapse of time (Campbell, Stanley, & Gage, 1963). Interpretation of motivational outcomes is more difficult as this is a post-only intervention measure. Here, only descriptive analysis of the scores is possible. For the latter (i.e., relative effectiveness), preferably, the media that are currently implemented to teach a certain subject matter are implemented in the control group. With relative effectiveness, all dimensions of DGBL effectiveness (i.e., learning outcomes, motivational outcomes and efficiency outcomes) are considered relevant. Note that when using the relative effectiveness approach all parameters concern a judgment of relative worth, comparing the outcomes to the current instructional medium used for teaching a particular content matter, implying the need for a control group where another educational activity is implemented.

The distinction between absolute and relative effectiveness is an important one to be made, as DGBL effectiveness studies that do not implement an educational activity in the control group, are often criticized as not being rigorous (Clark, 2007; Clark et al., 2016; Hays, 2005). In some cases, however, there is no control group available to which the content matter is taught in a more traditional way. For instance, a training manager can choose to develop a new training immediately using DGBL for cost-efficiency reasons, based on a cost-benefit analysis. This was the case in our third feasibility study (see section 2.3 Feasibility study 3: client-oriented principles in a corporate context (strategic knowledge), 3.3 Feasibility study 3: client-oriented principles in a corporate context (strategic knowledge)). Here, the training manager simply desired to know whether the game they had invested in actually succeeded in achieving its goal to guide further investment decisions regarding DGBL trainings. In this case, there is no use in creating a more traditional training just for research purposes. Hence, we would like to refute the necessity of a control group where another educational activity is implemented if there is no other current method to compare it to. We would however, suggest, to make meaningful comparisons. In the bank study (section 2.3), the question the training manager had was simply ‘does the game help new staff gain insight in client friendly principles?’ In this case, comparing to a group that does not receive extra instruction is not meaningless, as it looks at the added value the game provides compared to on the job experience. In this case, ‘business as usual’ could thus simply be no extra instruction.

4.1.2. Equal time-on-task is not required when increased time-efficiency is the goal
One of the main critiques regarding research rigor in DGBL effectiveness studies is a difference in instructional time between the experimental (game) and control (traditional education) groups (Clark, 2007; Randel et al., 1992). However, in our second feasibility study (see section 2.2.) it has become clear that this is not always desirable. In a context where learners are paid employees, keeping instructional time equal for the experimental and control group is often difficult, as a reduction of training time and as a result, higher cost-efficiency is often a desired outcome in these contexts. Hence, keeping instructional time equal is incompatible with the efficiency outcomes of DGBL. In such cases, instructional time should be treated as an outcome variable and research should focus on investigating whether learners learn as much or more in less time using DGBL.

4.2. Non-intervention related threats to internal validity and solutions
4.2.1. Failed randomization and blocked randomization as a solution
While in experimental research, control of as many elements as possible that might influence results is an important aspect, it can also be problematic in the context of DGBL. A first reason for this is the complex environments in which DGBL is often being implemented, such as natural collectives in which one does not always have control over observed and unobserved variables. In our first feasibility study (see section 2.1.), for instance, we had assigned participants to conditions by randomizing on the subject level. However, this implied that pupils from different classes were divided into new groups (2 classes from the fourth and 2 classes from the fifth grade were divided over 3 conditions), potentially threatening ecological validity. This, however, resulted in successful randomization, increasing internal validity as there were no pre-existing differences on the pre-test scores of the English vocabulary test. In our second feasibility study (see section 2.2.), we randomized on a group level. However, in this study randomization failed, resulting in pre-existing differences in the pre-test scores of the fire safety knowledge test, threatening internal validity. Moreover, it resulted in some issues for analyzing our Solomon 4-group design. Hence, in our third feasibility study, we have used blocked random assignment to assure similarity between conditions (Gerber & Green, 2012). Blocks were created based on age, number of months working at the bank and gender. This has resulted in a successful randomization and thus no pre-existing differences on the pre-test scores regarding client-oriented principles of the bank. We thus recommend -when possible-to use blocked randomized design among smaller samples, in order to ensure successful randomization beforehand, reducing the changes on pre-existing differences and consequently, increasing internal validity.

4.2.2. Pre-test effects and parallel versions of pre- and post-test as a solution
Increasing internal validity is also the result of reducing pre-test effects. For instance, in our first (see section 2.1.4) and third feasibility study (see section 2.3.4), we have developed parallel versions (i.e., same types of questions and same difficulty level) of a knowledge test that we implemented pre- and post-intervention, in order to reduce the confounding practice and pre-test sensitization effects of implementing the same test pre- and post-intervention. As became clear in our second feasibility study (see section 3.2), a pretest can bias learning outcomes. More specifically, this study showed that pre-test sensitization took place in the control group that received the fire safety training by means of a slide-based lecture. In the game group, no such effect was found. This is a relevant methodological issue in the field of DGBL as the learning outcomes in control groups receiving more traditional lectures might be positively biased, while in DGBL these probably represent more ‘true’ scores, as interactivity of the game requires them to process the content in order to finish the training. When receptivity to an intervention is altered due to the pre-test in one group and not in the group to which it is compared to, bias is introduced in the design (McCambridge et al., 2011). Hence, it becomes tricky to compare these groups in a pre-test post-test design; especially considering that often non-significant differences are found between DGBL and traditional classes (Clark et al., 2016).

4.3. Additional indicators for motivational and efficiency outcomes in a self-paced remote learning context
4.3.1. Motivation to start playing in self-paced learning context
Considering that the main rationale behind implementing games as instructional tools is one of motivation (Garris, Ahlers, & Driskell, 2002), experimental control is not always desirable. Implementing a game in a controlled lab setting would provide us with limited insight in motivational outcomes as this is a highly artificial environment. Giving in on control in order to increase ecological validity, however, can in turn get in the way of ecological validity. For instance, in our third feasibility study (see section 2.3.), the game we tested was meant to be played by the bank employees at their own convenience, during office hours. Hence, the decision to provide the employees with 6 weeks to finish the training at their own convenience, as this is the way it would occur in real life. This way, motivation for the instructional material was more ecologically valid, as they would play when they would feel like it or when they had some time available. However, we had to contact the employees several times to start or finish the training, that this in turn effected ecological validity; since in real life there would be no one nudging the employees. In hindsight, we still believe this was the best way to conduct this study as it gave us valuable insights in providing meaningful contexts for DGBL that resulted in recommendations for the bank to further optimize their DGBL trainings in the future (see section 4.4). Another interesting conclusion regarding motivational outcomes related to this study, is that success of games as instructional medium in a distance self-paced learning context is not only related to the question ‘If learners play the game, does it improve motivation, learning outcomes and/or cost-efficiency?’ but also ‘does the game succeed in motivating learners to actually start playing?’.

4.3.2. Time required to follow-up on learners
Related to the issue described above, if we would take the cost of monitoring whether or not the employees followed the training into account and following up on those who did not, we can put the efficiency rationale behind technology delivered instruction and game-based learning -in this case-in doubt. Time required to follow-up on learners should thus also be included when assessing cost-efficiency.

4.4. Feasible ways to keep experimental and control group comparable
Internal validity can be increased by keeping potential sources of variability other than the experimental variance equal. Based on the 3 feasibility studies discussed in this manuscript, keeping the following variables equal between experimental and control group are considered feasible and desirable practices are: date and time of the intervention, amount of support provided and content. Implementing the same content and amount of support provided was possible in all 3 studies. Time of the intervention might possibly have an impact on outcomes. In our first feasibility study, the game + debriefing group scored lowest on learning outcomes. This group differed from the other two conditions (traditional and DGBL group) in time of day when the instruction was delivered (second lesson hour vs first lesson hour). Due to practical reasons, it was not possible to simultaneously run all 3 conditions. In hindsight, we would have better let one game group play the game in the first lesson hour one day and another game group plat the game in the first lesson hour on another day, instead of running all conditions on the same day as suggested in the defined best practices. Again, making meaningful comparisons is of importance: we had not considered concentration levels before the break when designing the experiment. It also proved feasible to keep experimental and control group as similar as possible regarding relevant observable participant characteristics, such as game experience, age and gender. Hence, researcher are suggested to report on how similarity between these participant and intervention characteristics between experimental and control group was attained, but also on which characteristics it was not attained.

4.5. Follow-up study and eligible data
In our first feasibility study we have shown that the addition of a follow-up test is necessary to get a better indication of learning gain, even if it is already after three weeks. Longer term follow-up studies can certainly be interesting, especially for interventions that have been implemented for a longer period of time. However, in practice, it is often difficult to gather this type of data. For instance, in our second feasibility study (see section 2.2), we implemented the follow-up study after 6 months. This was difficult for two reasons. Firstly, not everyone that had participated in the initial study still worked at the hospital. Secondly, since the end of the initial study a year before, the game was available on the hospital's intranet, for employees to play in order to revise the fire safety training. Consequently, the following things happened: a) people that were in the control condition and did not get to play the game, had now played the game between the end of the study and the follow-up; b) people from the game-based group replayed the game during the end of the study and the follow-up and c) people from both groups decided to revise the safety training using the game before they came to fill out the test at follow-up, even if we had stressed not to revise the course in order for us to investigate longer term effects. Consequently, we did not have enough participants left that were eligible for the data-analysis of the follow-up study. In this case, it would have been better to have a quicker follow-up study, so the participants did not feel the need to ‘revise’ the content.

5. Discussion and conclusion on DGBL effectiveness
While the main focus if this manuscript was to evaluate the feasibility of suggested best practices for effectiveness assessment of DGBL, we can also provide some -careful- insights regarding DGBL effectiveness as relevant outcome measures were evaluated and sample sizes met the minimal requirements of 20 participants per condition (All, Nuñez Castellar, & Van Looy, 2016, Simmons, Nelson, & Simonsohn, 2011). These are, however, pilot studies and replication would be required to validate these statements.

In sum, in two of the three feasibility studies, DGBL did not prove to more effective than more traditional, ‘less engaging’ instructional media. Does this mean that this supports critics' statement that positive findings in favor of DGBL might be attributed to less rigorous research design elements? (Clark, 2007; Hays, 2005). This is something that needs to be further investigated in the future and is increasingly being integrated in meta-analyses. Based on our findings, it is possible that some studies jump to conclusions or do not provide enough information in order to provide readers with a nuanced view of possible alternative explanations for their outcomes. This has also been broached by other authors (Clark et al., 2016; Sitzmann, 2011).

Another question related to the ‘disappointing’ outcomes of our effectiveness studies is linked with the ‘can media influencing learning? discussion referring the Kozma vs. Clark media debate (Clark, 1994; Kozma, 1994). In the context of DGBL we do believe game-characteristics can influence learning outcomes. For instance, the interactivity of the fire safety training game in our second feasibility study increased attention and consequently, learning outcomes. Also, in our first feasibility study, both gaming groups (with and without debriefing) found the game more fun; which is also a desired outcome of DGBL related to the game as a medium. However, we do believe that often, game mechanics are not used to their fullest potential and DGBL does not always succeed in finding the right balance between entertainment and instruction. This is also confirmed by equal motivation for learning through the instructional media in our second (see section 3.2) and third feasibility study (see section 3.3). For instance, in the bank study the players get a final score for each minigame, but no meaning is attributed to this score. This shows that merely making a training ‘game-based’ will not automatically result in higher learning or motivation and that game design should be carefully thought out, investigating suitable and meaningful game mechanics. Gamification elements could have provided a solution here, such as a leaderboard creating competition between colleagues or the earning of badges when a certain score is achieved to stimulate engagement.

The lack of motivation to start the DGBL bank training is also detected on a broader scale within the company, as only 200 of 8000 employees have already played the game. This lack of motivation to start playing is unlikely to be related to individual underlying reasons such as technology skills, game skills or attitudes towards games considering it was as difficult to motivate the participants in the instructional video group, which did not require any of these skills. Hence, a more plausible explanation might be related to the format of the training. Making the training only accessible at the office and consequently, during working hours may have impeded employees to play the game. Time management has indeed previously proven to be an issue for employees to actually use e-learning programs (Joo et al., 2011). Other impeding factors are the lack of social interaction on the platform (Short, 2014), the lack of supervisory support and, related to this, lack of incentive to engage in e-learning programs (Joo et al., 2011). The non-engagement to start the training might thus be related to the lack of a meaningful learning context (De Freitas, 2006).

Further research should thus not only focus on whether DGBL is effective and which in-game elements make DGBL effective, but also on which implementation methods or context variables motivate employees to actually start the game-based training. All of the above, however, require a sound methodological strategy to properly inform design.

6. Implications & limitations
While the results of only 3 feasibility studies are certainly not generalizable to all contexts, the results of these feasibility studies did result in preliminary recommendations that are not solely based on academic expertise but also take into account constraints that can occur in real life. Moreover, possible solutions for those constraints have been described. Furthermore, by implementing best practices in different sectors rather than focusing on formal, school education, increases the practical value of these recommendations. These results can help reduce the gap between academic research and evidence-based implementation of DGBL in learning contexts. The extensive reflection on several study design characteristics and the impact these have on results, can help researchers and game-developers make informed, evidence-based decisions when working out their study design when aiming to assess effectiveness. More studies mapping best practices and testing their feasibility, can lead towards a more generally accepted and validated experimental protocol that fit the reality of the contexts of play. When game developers can let their games be tested by independently developed research guidelines, more credibility can be attributed to their products (as opposed to testing the games themselves using a methodology they deem appropriate). Demonstrating effectiveness is an important factor in the decision making process for adopting or implementing DGBL (Bardon & Josserand, 2009). Hence, this could positively influence uptake of their games. This can in turn have as a result that more game developers let their educational games be tested, resulting in higher quality DGBL products and in turn, a higher quality DGBL industry. Ideally, the games would be tested by an independent research organization not involved in development, but this would of course require more resources for evaluation, which is already considered one of the reasons why less rigorous studies are being conducted.

This manuscript also provides a preliminary base for defining quality of research design in a more nuanced way: focus on reducing confounds as a result of different participant characteristics in experimental and control group, differential treatment during the interventions and pre-test effects are more relevant to take into account rather than implementation of a randomized controlled trial or the inclusion of another educational activity in the control group. The latter is typically how quality of research design is currently being evaluated, either by adding it as a control variable in meta-analyses or by excluding studies that do not implement such an approach from systematic literature reviews.

A limitation of this manuscript is that two of the three studies have been conducted in a semi-lab context, providing results that will be mainly applicable for a controlled setting. More feasibility studies need to be conducted in field experiments, similar to our third feasibility study. Especially considering the COVID-19 crisis, DGBL will move more towards self-paced distance education the coming years. A second limitation of this study is that we were not able to test all aspects of the defined best practices by (All et al., 2016) . Elements that have not been tested for feasibility are: using standardized tests for learning outcomes (because of the nature of the learning content in the specific contexts), assessing student achievement as an ecological measure for learning outcomes (because English vocabulary is not thought in primary education, which was the target group in feasibility study 1) and adding random effects to data-analysis (because of experimental control in the semi-lab environments in feasibility study 1 and 2 and because of the small sample size in feasibility 3). A third limitation is that the intervention periods were rather small, with 2 feasibility studies limited to one lessen hour and 1 study implementing a period of 6 weeks (but a small intervention of 5 mini games). Testing feasibility of best practices on longer intervention periods is required. Lastly, these best practices did not take into account the rising field of stealth assessment, a technique that aims at accurately and dynamically measuring the player's progress, analyzing the player's competencies at various levels during game play (Shute, Rieber, & Van Eck, 2011); nor do they provide recommendations on how to assess effectiveness of game design characteristics. Best practices regarding those two elements should be further investigated.

Finally, regarding DGBL effectiveness (section 5), we should note that the outcomes of the feasibility studies represented in 4.5. are not representative of the potential effectiveness of DGBL within their given contexts. No selection process regarding game design quality preceded inclusion in the studies, as the focus lied on investigating experimental procedure rather than evaluating effectiveness of the games.