Information-Centric Networking (ICN) uses content name to replace traditional IP address where data becomes independent from location, application, storage, and means of transportation. Due to the complex structure and variable length of content names, designing efficient content name lookup algorithms becomes a new challenge. In this paper, we propose an efficient name lookup structure for ICN, called Learned Bloom-Filter Lookup, which combines Recurrent Neural Networks (RNN) with standard Bloom filter to improve lookup efficiency. In our scheme, RNN trains the element set and non-element set, which are used to obtain the pre-filtering of names. Moreover, we look up the contents by using the backup Bloom filter to reduce the false negatives generated by the learned model. In addition, we evaluate the performance of the proposed algorithm using experimental simulations. Compared with the Bloom-Hash and NameFilter methods, the results show that our method can reduce the false positive rate and improve the accuracy of the search. Furthermore, the memory required by our method is less than the Bloom-Hash and NameFilter methods.

Previous
Next 
Keywords
Information-centric networking

Learned Bloom-filter

Name lookup

Recurrent neural networks

1. Introduction
With the rapid increase of network information contents, users are mainly concerned with the content rather than the location of the content. The main information flow on the Internet has undergone tremendous changes. The current host-centric work model is not conducive to the development of information publish/subscribe, which will affect the efficiency of information acquisition. In order to solve this problems, a new network architecture Information-Centric Network (ICN) is proposed (Ahlgren et al., 2012; Vasilakos et al., 2015). Compared with traditional IP networks, ICN transmits data packets based on the content names which can improve transmission efficiency, and directly protects the content and improve the security of the information.

The issues of the ICN architecture include naming, routing, caching, congestion and so on (Bari et al., 2012; Liu et al., 2018; Song et al., 2017; Abdullahi et al., 2015). Due to the increase of users’ demand for information, improving the routing efficiency of information can greatly satisfy the needs of users. Hence, routing problem becomes an urgent problem to be solved. Unlike IP-based networks, ICN uses names to process data, called content, requesting content by publishing content names, and the network is responsible for finding and returning the requested content. Each router has three components in ICN: Content Store (CS), Forwarding Information Base (FIB) and Pending Interest Table (PIT) (Jacobson et al., 2012). A routing node retrieves content by sending an Interest package, which contains the name of the desired content. As with IP address, the router can identify the interface for forwarding the content by using the longest prefix match searching FIB. Current routing technology (Zhang et al., 2017; Yao et al., 2019; Torres et al., 2019) cannot meet the needs of ICN. The structure of the name is complicated due to the diversity of naming methods, and the length of the name varies compared with the fixed length of the IP address, which makes it difficult to look up. Therefore, how to lookup names quickly and accurately becomes a challenging problem.

In order to improve the efficiency of the lookup and reduce the memory usage, Bloom (1970) first proposed a binary data structure named Bloom filter which has good efficiency in time and space. Mitzenmacher (2002) used the Bloom filter to build an efficient name lookup scheme, where the Bloom filter uses a hash function method to detect whether an element is a member of the set. However, the standard Bloom filter cannot delete the old entries. To this end, Dai et al. (2017) proposed a CBF-based name lookup strategy, which can delete and insert new entries dynamically, reducing the memory consumption while increasing the rate. Wang et al. (2013) proposed NameFilter, a two-stage name search method for counting Bloom filter (CBF), which reduces the memory access speed from the time complexity. Quan et al. (2014a) proposed a strategy which combines with Tree-Bitmap and Bloom filter, and this strategy needs less memory than that of the Name Prefix-Trie. Athanassoulis and Ailamaki (2014) proposed a Tree-based lookup strategy, which has a higher efficiency and a small memory footprint than that of the Bloom filter. Kraska et al. (2017) proposed a new architecture called learned Bloom filter, which uses machine learning model to improve the query efficiency of names, but this structure produces significant false negatives. The above methods do not solve the conflict between elements caused by using the hash function in the Bloom filter and the false negative problem caused by using machine learning.

In this paper, we use learned Bloom filter to construct the process of name lookup in ICN, which leverages layered learned model and Bloom filter solutions. We obtain the pre-filtering of names through the learned model of the first layer, and then through the second layer for an accurate lookup. Since the name of an object is a string, the effect of using recurrent neural networks (RNNs) (Graves, 2013) is very significant. For this reason, we propose a name lookup algorithm by using RNN as the learned model. This algorithm is extended from (Wang et al., 2019). Because the elements will generate certain false negatives through the first layer structure, we use a backup Bloom filter as the second layer of the lookup, and propose a name insertion algorithm. Compared to (Wang et al., 2019), we add the whole lookup structure overview and describe the process of name lookup by using traditional Bloom filter, analyze the false positive probability of Bloom filter, then add the analysis of the lookup process and the mathematical analysis of the false positive rate. Moreover, we also add multiple sets of comparative experiments. Our contributions are summarized as follows:

●
We design a novel name lookup engine, learned Bloom filter, which is divided into two parts: learned model and backup Bloom filter.

●
We construct learned model and analyze the false positive probability generated by the structure.

●
We construct a learned function instead of multiple hash functions to reduce the conflict between keys.

●
We propose two algorithms, name insertion and name lookup, which can reduce the false positive probability.

●
We validate the performance of the proposed method by various of simulation experiments.

The rest of this paper is organized as follows. We discuss related work in Section 2. In Section 3, we outline the whole lookup structure and describe the process of name lookup by using traditional Bloom filter, analyze the false positive probability of Bloom filter, then provide the analysis of the lookup process and the mathematical analysis of the false positive rate. In Section 4, we devise an insertion algorithm and a lookup algorithm. Simulation results are provided in Section 5. In Section 6, we conclude and outlook this paper.

2. Related work
Due to the important role of name lookup in ICN routing, it has become a hot topic of research. The method used in this paper is to combine machine learning with the standard Bloom filter to build learned Bloom filter to optimize the lookup efficiency.

2.1. Trie based schemes
IP based routing is based on the longest prefix match where an IP packet will be routed through the interface associated with the longest prefix matched target IP address. The content names in ICT are different from IP address due to their variable lengths and content variation. Therefore, its longest prefix match scheme is different. The longest prefix match in ICN first sorts the content names by the prefix length, and then determines the content name to be queried from the ordered prefix set. One of the most common methods of fast LPM is Trie (Christensen et al., 2010). Li et al. (2016) proposed a trie-based lookup method named P-trie. It speeds up the lookup of packets in NDN: if a packet is not in the routing table, P-trie can be used to analyze the existing entries and help forward the packet to a most promising port. This method reduces the memory footprint while improving the speed of the lookup. Athanassoulis and Ailamaki (2014) proposed an architecture named BF-Trees. It uses the existing data sorting to provide better lookup performance, which reduces the precision of the lookup to a certain extent and saves the lookup space when searching for order. But as the depth of the tree increases, its performance will drop greatly. In ICN, due to the indefinite length of the name structure, the depth of the tree increases, which will affect the performance of the lookup and cannot be well applied in the network environment.

2.2. Bloom filter and other variants
Compared with the trie-based lookup method, another quick name lookup way is to use the Bloom filter. Dharmapurikar et al. (2006) first proposed using the Bloom filter for the longest prefix match. The Bloom filter is a data structure, which can adjust the accuracy of finding members by adjusting the size of the false positive rate, so as to find more efficient and accurate matching contents. Lee et al. (2016) proposed using Bloom filter pre-searching to match the name prefix to improve matching efficiency. And the probability of its false positive depends on the amount of content stored in the filter, the array length of the filter and the number of hash functions used for the calculation. In order to improve the speed and accuracy of name lookup, researchers developed a variety of standard Bloom filter based solutions for ICN. Quan et al. (2014b) proposed a new name lookup method for adaptive prefix Bloom filter. It decomposes each NDN name prefix and matches it by the Bloom filter, and the decomposed prefix lengths speed up the lookup by dynamically adjusting their popularity, reducing the memory when it is occupied. Since the standard Bloom filter cannot delete the old contents, the memory footprint increases with the addition of new contents, hence the counting Bloom filter (CBF) is proposed to add a counter to delete the elements (Dai et al., 2017). Wang et al. (Wu et al., 2019) proposed a two-stage CBF-based search strategy to optimize the data structure of the Bloom filter through two CBFs to reduce memory access time. However, all existing methods mix these lookup strategies to improve the performance, and they have failed to resolving the conflicts between elements.

2.3. Neural network based schemes
In order to optimize the search type, Kraska et al. (2017) proposed a learning index based on the traditional search type. The paper used the deep learning model (Goodfellow et al., 2014; Abadi et al., 2016; Priya et al., 2019) to learn the search content and used this signal to effectively predict the existence of records. If we know the exact data distribution, then we can highly optimize the content lookup. But in the real environment, the patterns of data are not fully known, and the workload of creating a dedicated solution for each data can be very large. However, using machine learning to learn the correlation between data can build an efficient content lookup architecture at a lower cost. Neural networks, a powerful machine learning model (Sutskever et al., 2014) can largely replace the traditional lookup architectures. For example, the Bloom filter can be replaced by a model that predicts whether a content exists in the content set based on a content. Our work is built directly on the existing Bloom filter, using the learned model as a special hash function to optimize the search efficiency.

3. Problem statement and our model: Learned Bloom filter
In ICN, a learned Bloom filter is used to lookup name content. It replaces multiple hash functions into a machine learning model on the basis of the standard Bloom filter, so that all contents can be found through the same learned model. Next, we will first show the lookup structure and then describe the content lookup process of the standard Bloom filter, finally propose data structure of a learned Bloom filter and query process.

3.1. Lookup structure overview
The bottleneck of the standard Bloom filter technology is that it needs to use multiple hash functions to improve the lookup accuracy and reduce the false positive rate caused by the lookup, thus it will generate a large memory footprint. For example, using the Bloom filter to find one billion name contents, controlling the false positive rate to be 0.1%, then about 1.76 Gigabytes is needed. If the false positive rate is 0.01%, then about 2.23 Gigabytes is required (Kraska et al., 2017). Therefore, we propose a structure to reduce the memory usage.

Due to the continuous development of machine learning technology and flexible application scenarios, we use machine learning in ICN to improve the efficiency of content name lookup and forwarding between nodes and reduce the memory consumption. In this paper, our learned Bloom filter lookup structure is shown in Fig. 1, we use RNN to build a learned model instead of the standard Bloom filter, and use the learned model to find the name contents. However, this learned model has some defects. We have found through experimentation that each lookup will produce a certain number of false negatives. Therefore, we build a backup Bloom filter to reduce the false negatives generated by the learned model. As the learned model increases, the data structure will becomes more stable and the accuracy of the lookup will also increases.

Fig. 1

Fig. 1. Overview of the Learned Bloom filter lookup structure.

3.2. Query process of name content using Bloom filter
The IP lookup using a Bloom filter was proposed by Dharmapurikar et al. (2006), which is also applicable to the routing process of named look up in ICN. In the processing of membership inquire, the performance of a Bloom filter is controlled by a false positive probability that the probability of an element is not in the set but wrongly reported as being in the set. A Bloom filter is an array length of m bits with range {0, …, m − 1}. Given a set S = {a1, a2, …, an} with n elements and the Bloom filter use k hash functions to compute the elements in S and map to the corresponding m-bit array. Initially all value of array bits are 0, for each element ai ∈ S, it calculates the hash value through k independent hash functions, and sets the corresponding hash value bit to 1, a bit value address can be set to 1 multiple times. To look up a content y, the Bloom filter uses the same k hash functions to compute, and check whether all corresponding bits are set to 1. If we find at least one bit is 0, the element y ∉ S. If all the bits are set to 1, y is a member of S but this may be exist a false positive.

In ICN, the content is named as a URL, and the Bloom filter is used for name lookup and routing forwarding. Now we take an example to show the process of inserting three URLs into a Bloom filter in Fig. 2. As shown in Fig. 2, a content name contains a full name (e.g.,/com/baidu/images) and two name prefixes (e.g.,/com/baidu/and/com/) and three URLs insert into the Bloom filter. In Fig. 2, the parameter set is {n = 3, k = 3, m = 15}, each URL compute the hash values through three hash functions and set the corresponding three array bits to 1, the bit will not be changed if this position is already set to 1.

Fig. 2

Fig. 2. An example for content insertion into a Bloom filter.

According to (Christensen et al., 2010; Dharmapurikar et al., 2006; Bose et al., 2004), when an element y ∉ S but all the bits hi(y) are found to be 1, then the Bloom filter has false positive probability fp, which is given by(1)
 
 

Through probabilistic analysis, we get the following relationship:(2)
 
(3)
 

From (2), (3), we can see that the false positive probability, fp, is independent of the name content, and the size of array bits and hash functions can affect the false positive probability. We provide an example to illustrate this conclusion. Given 100 content objects and set n = 100 to represent these objects. We first control the false positive probability at 1%, thus the server will need a Bloom filter with number k = 6.65 hash functions and m = 959.41 bits. For a false positive probability of 0.1% we will require m = 1437.76 bits and k = 9.97. In order to reduce the conflict and improve the query accuracy, the filter should increase the value of k and m for a given content set, n. Therefore, using a Bloom filter in ICN for naming lookups and routing forwarding still takes up a lot of memory.

3.3. Problem statement
In ICN, if you want to perform a lookup in the off-chip memory of the FIB, the maximum speed of the interest packet is 15 Mpck/s, assuming that the off-chip memory is 140 MBytes, then the maximum allowable LPM for about 20 million prefixes (Perino, 2011). Thus, how to perform efficient name lookup in ICN is a key issue. For this problem, we have a formal description. Table 1 shows the summary of notations in this paper. Let us define Q = {q1, q2, q3, …, qu} as the set of names requested during the routing time interval t, and K = {a1, a2, a3, …, an} is a set of all prefix elements pre-stored in the FIB. We assume that a router receives u interest packets within a time interval t, each interest packet requests a content name, then the request rate is u/t. The memory footprint generated by the entire architecture is .


Table 1. Summary of notations and symbols.

Notation	Representation of the symbol
Q	Set of requesting names
K	Set of prefixes entries stored in forwarding table (FIB)
U	Set of non-elements for model training
D	Set of data structure
A	Lookup algorithm for K
Memory space
F(⋅)	False positive rate
t	Request time interval
τ	Threshold
By the above definition, we can formalize the entire lookup process. Suppose we request an interest packet, the interest packet is the content name c and c ∈ Q. Then at the routing node, the content name c is searched in the forwarding table K by the longest prefix matching algorithm. If the match is successful in the forwarding table, then the corresponding next hop is sent and a time interval shorter than u/t is generated. If no prefix matching, return the default interface. However, the algorithm will generate a certain false positive rate, and we define the false positive rate generated by this lookup process as F(Q).

Our design aims to minimize the memory space occupied by the lookup, optimize the lookup algorithm to improve the accuracy of the lookup, and reduce the false positive rate generated by the lookup process. The problem can be expressed in the following form:(4) 
 	
 where σ is the acceptable threshold on false positive probability.

3.4. Structure of learned Bloom filter
In this section, we describe a novel structure for ICN name lookup to improve search efficiency. To minimize the space of name lookup and the number of false positives produced, Kraska et al. (2017) presents a learned index structure, using machine learning in the standard Bloom filter to reduce the space of the index.

We use recurrent neural networks (RNNs) as the learned model, and a query x can be predicted whether an element in the learned model. Kraska et al. (2017) offer the construction of the learned Bloom filter. They use a binary classification task to frame the index. We denote a set of elements K that corresponds to the set S of the Bloom filter in the previous section. We also denote a set of non-elements U, and then training set K and set U. We now train a neural network with , and use a logistic sigmoid function to produce a probability(5)
 
Note that the range of f(x) belongs to [0, 1], we can make f(x) as a probability estimate, therefore, the probability of predicting the current sample output to be 1 is(6)and the probability of predicting the current sample output to be 0 is(7)According to the maximum likelihood estimation, we combine the above two cases to get(8)and we get the joint probability density(9)then the loss function L is(10)

We represent our structure in Fig. 3. For an ICN name x, Fig. 3 shows the process of naming lookup and our design is to replace the standard Bloom filter with a learned model. Firstly, check whether the name is in the query set by the learned index model, if x belongs to this set, output the element and match the prefix, then forward it through the next hop. If x does not belong to this set, the learned model will send the element to the backup Bloom filter and the Bloom filter will check the element. If the name can be found in the backup Bloom filter, match the prefix and forward it through the next hop, otherwise drop the name data. The purpose of using a backup Bloom filter is to avoid the false negatives from the learned model.

Fig. 3

Fig. 3. Framework for name lookups using the learned Bloom filter.

Due to the use of trained model, a certain false negative rate is generated. In order to ensure that the false negative rate is 0, we set a threshold τ to determine if x is in the element set and use a backup Bloom filter to reduce the false negative rate to 0. As shown in (Athanassoulis and Ailamaki, 2014), if f(x) ≥ τ, the element x exists in the set, and if x ∈ K and f(x) < τ, the process may provide a false negative and check the backup Bloom filter. Because the Bloom filter has no false negative rate but the learned model has false negative rate, we can use the learned model f as a hash function to map the key to a m-bit array. The sigmod function f(x) is mapped into the range of [0, 1], so we define a function g by g(f(x)) = ⌊mf(x)⌋ as a hash function in the Bloom filter, and create a set C = {x ∈ K|f(x) < τ} for the backup Bloom filter from the false negatives of model f. As shown in Fig. 4, the element bi in C through model hash function g to map to the farther range of a bit position, otherwise map to the closer range of bit positions. We will prove the effects of this structure in the experimental section.

Fig. 4

Fig. 4. Backup Bloom filter insertion.

3.5. Analyze the false positive probability
False positive probability is an important indicator to measure the accuracy of name lookup. In this section, we will analyze the false positive probability of the learned model f and the backup Bloom filter. Since our architecture is composed of two parts, the learned model trains a set of elements K and a set of non-elements U, the false positive rate is largely close to its expected value, and the false positives of the backup Bloom filter is related to the set C. The following is the analysis and formal definition.

The false positive probability generated by the name lookup through our structure consists of the learned model and the Bloom filter. Learned model f trains an element set K and a non-element set U. We give a query set Q if a query element q ∉ K and q ∈ Q satisfies the condition f(q) ≥ τ and it is a similar element of K, it will have a large f(q) value and it judge as b ∈ K, so it produces a false positive in model f. Choose an element q ∉ K but via model f returns q ∈ K and f(q) < τ, then it produces a false negative in f and a false positive in the backup Bloom filter. It can be seen that the false positive probability depends on the data set and query data. Now we formally define the false positive probability on the set D, the learned Bloom filter has an element set K and |K| = m, the false positive probability of query set Q in model f is F(p) and false negative probability generated is Fn. The false positive probability of the backup Bloom filter is F(B). The set size of the backup Bloom filter is mFn which depends on the false negatives number from K.

Definition 1

A false positive probability of a learned Bloom filter structure is given by

(11) 
 
For the backup Bloom filter, its false positive rate F(B) is a random variable, and the size varies with |B|, while the size of |B| depends on the number of false negatives generated by the learned model. Therefore, we consider using expectation to represent the false positive rate produced by the size of the backup Bloom filter for a given size |B|(12)

We assume a budget of bm bits for backup Bloom filter and the false positive probability falls as αi, where i is the bits of each element stored in a Bloom filter ((Graves, 2013) described α = 1/2 when using a perfect hash function in the Bloom filter). So in this way, we get the bits of each element stored in the backup Bloom filter as b/Fn, then(13)then, derivatives for b(14)
 
this formula has a minimum when α = 1/2.

A learned Bloom filter by using RNN is able to predict the false positive probability of a test set with sufficient data. In the naming query process, the test set and the query set under the same set. Since the false positive probability is concentrated on the expectation, the false positive probability predicted by experience will be infinitely close to the true false positive rate.

Definition 2

Denote a test set R and R⋂K = ∅, then the false positive rate generated by the set R in learned Bloom filter is the number of false positives divided by |R|.

According to the above definition, combined with the Chernoff bound proposed by Motwani and Raghavan (1995) [Theorem 4.18] (see also Azuma-Hoeffding inequality in (Broder and Mitzenmacher, 2004)), we can obtain the bound of the false positive rate generated by the whole structure.

Theorem 1

For a learned Bloom filter, given a test set R and a query set Q, both R and Q have a same set D. Let X is a random variable corresponding to the false positive rate on the test set R, and Y is a random variable corresponding to the false positive rate on the query set Q. Then for any ε > 0,

(15)
Proof

Let F(Q) be the false positive rate for the learned Bloom filter, and(16)then we first show the probability for R and X(17)Since R and Q have the same set distribution D, we will get a bound similar to the above formula for Q and Y(18)then(19) 
 

□

In summary, it is important to choose a suitable value for τ and the selected value of τ will determine the probability such that the false positive probability is close to the expectation.

4. Name lookup process design
In this section, we propose the algorithm to describe in detail the process of using a learned Bloom filter in ICN for name lookup.

4.1. Name insertion algorithm
Since our architecture consists of two layers, the first layer of the learned model will generate false negative, we need to build a backup Bloom filter to reduce the false negatives to 0. We train two sets in the learned model structure, and only insert the elements in the element set in the backup Bloom filter to improve the lookup accuracy. We first present the name insertion algorithm that builds a complete backup Bloom filter to facilitate name lookup.


Algorithm 1. The insertion of an entry in the backup Bloom filter

Input: Element set K, Threshold τ, Size m, Element x
1: for x ∈ K do
2: Calculate f(x) according to (5);
3: end for
4: if x ∈ K and f(x) < τ then
5: Calculate g(x) according to g(x) = ⌊mf(x)⌋;
6: Locate in backup Bloom filter: i ← g(x);
7: Set bits in backup Bloom filter: b[i] ← 1;
8: else
9: Set bits in backup Bloom filter: b[i] ← 0;
10: end if
To apply the name insertion algorithm for a learned Bloom filter, we treat the process of inserting elements into a backup Bloom filter as a standard Bloom filter inserting elements. For each element xi ∈ K, in order to reduce the conflict between a large number of keys in the standard Bloom filter, we combine multiple hash functions into a learned hash function to map the element to its corresponding position in the backup Bloom filter and the value of bit is set to 1 as shown in lines 4–7 of Algorithm 1. The hash function is shown in Line 5, by using learned function f(x) as a variable to build a hash function g(x), the elements are mapped to higher bit position and the non-elements are mapped to the lower bit position. This approach greatly reduces the memory footprint and conflicts between elements.


Algorithm 2. The search of an entry in the learned Bloom filter

Input: Element set K, Non-element set U, Threshold τ
Output: Element x
1: Invoke RNN construction use set K and U to get a set D
2: for (x, y) ∈ D do
3: Calculate f(x) according to (5);
4: end for
5: if x ∈ K and f(x) ≥ τ then
6: Output x and match the Hash Table;
7: Send x to next hop;
8: else if x ∈ K and f(x) < τ then
9: Search x in backup Bloom filter;
10: while Search b[i] = 1 in backup Bloom filter do
11: Output x and match the Hash Table;
12: Send x to next hop;
13: end while
14: else
15: Drop the element x;
16: end if
4.2. Name lookup algorithm
After constructing the backup Bloom filter, we present the name lookup algorithm to search the name in the learned Bloom filter structure. Denote an element set K, a non-element set U, and train a neural network by using RNNs with D = {(xi, yi = 1)|xi ∈ K}⋃{(xi, yi = 0)|xi ∈ U}.

In Algorithm 2, we give an element x to lookup via the learned Bloom filter structure. First, the element through the learned model f and determine if x is in the element set K, f(x) means whether or not an element x is in the set. Thus, we set a threshold τ to demarcate this range. As shown in Line 5–7, if x passes through model f and satisfies the condition f(x) ≥ τ, the x is determined as an element in the database, then output x to match the prefix and forward through the corresponding next hop. If f(x) < τ, in order to avoid the false negatives, we find x in backup Bloom filter and if x in the filter, match the prefix and forwarding through the next hop, otherwise drop the data package. In this process, using the learned model to preprocess the query data when new entries arrive in the FIB can reduces the memory and speeds up the search efficiency.

Since the learned model uses RNN to preprocess the elements, RNN has a memory function and can predict the next event based on the occurrence of the previous event. Therefore, it can improve the lookup efficiency and avoid unnecessary memory consumption in certain cases. The experimental findings of large data sets indicate that the accuracy of training and the names lookup are limited by the trained data sets. Therefore, by using the appropriate data set to control the accuracy of the names lookup, the resulting false negatives are transferred to the backup Bloom filter for further analysis to improve the accuracy. In the next section, all algorithms will be experimentally demonstrated to clearly emphasize the relevance of the learned Bloom filter to the ICN scenario.

5. Simulation
In ICN, the length of the content name is different because the content requested by the user is different. The actual forwarding table study found that the distribution of name prefixes on the prefix length set is not uniform, the forwarding table is dynamically changing. Our data set comes from the blacklist domain information and the urls obtained from the network. We randomly extracted 6M data from our test data set and calculated the distribution information of the prefix length. As shown in Fig. 5, a large number of content names are concentrated in 15–25 characters, and characters with a length greater than 45 have less content prefixes.

Fig. 5

Fig. 5. Prefix length distribution.

To evaluate the efficiency of our learned model structure, we select Bloom-Hash (Dharmapurikar et al., 2006) and train a character-level RNN (Cho et al., 2014) for comparison experiments. In the experiment, we mainly compare our solutions through three metrics: (a) false negative probability, (b) false positive probability, (c) memory cost. We get a lot of URLs from the web to experiment, all algorithms are implemented in the software using Python language, the device is configured with a 8G RAM and an Intel(R) Core(TM) 2 Cores 2.27GHz CPU. We divide the crawled URLs into two sets: a positive set and a negative set, and randomly extract data from the positive set as a test set. We insert the positive set into the FIB, then randomly assign the forwarding interface and conduct the name query to simulate the name lookup process of ICN.

False negative probability: Our learned Bloom filter lookup contains a two-layer architecture, in the learned model of the first layer, we control the false negatives of the first layer by setting a reasonable threshold τ to reduce the false positive probability of the entire model. We selected the train set and the test set data under the same false positive rate for comparison. As shown in Fig. 6, the test set results are similar to the train set results. We compare the hidden layer size of the neural network with 0–8, respectively, as shown in Fig. 7, when τ = 0.1, the false negative probability generated by the five situations is almost equal. The larger the τ value, the greater false negative probability generated by the learned model, while the lower the total false positive probability. When the value of τ approaches 0.9, the false negative rate of hidden size is equal to 6 is the lowest, while the value of τ approaches 0.5, the false negative rate of hidden size is equal to 4 is the lowest. The reason is f(x) ∈ [0, 1] and f(x) ≥ τ, the closer the value of τ is set to 1, the more the value produced by f(x) tends to 1, and the more accurate the output value results of our model in a smaller false positive rate. This result verifies our previous analysis.

Fig. 6

Fig. 6. Comparison between train set and test set.

Fig. 7

Fig. 7. The influence of threshold and learning rate selection on false negative rate and false positive rate.

False positive probability: We also compared the hidden layer size of the neural network with 0–8, as shown in Fig. 7, when τ is less than 0.1 and τ is greater than 0.7, the false positive rate generated by the five situations is almost equal. When τ = 0.5, the false positive rate of hidden size is equal to 8 is the highest. We selected different learning rates and epoch times to compare with Bloom filter and NameFilter (Wang et al., 2013). Fig. 8 (a) shows as the false positive rate increases, the memory footprint decreases. For learned Bloom filter, we choose different learning rate and epoch to compare. Under the condition of the same false positive rate, when the false positive rate equal to 0.005, the memory footprint by the NameFilter is 50% higher than that of the learned model and the Bloom filter is 30% higher than the learned model. When the false positive rate is lower, the memory footprint generated by the learned model is the least. For the learned model, memory footprint and false positive rates are reduced as learning rate and data training times decrease. The reason is that the lower learning rate in RNN, the higher false negative rate generated by the learned model, thus reducing the false positive rate. This result also verifies our previous analysis.

Fig. 8

Fig. 8. Memory cost(lr: RNN learning rate, E: train epoch).

Memory cost: We selected 1M–8M names from the database in this experiment, and insert them into the FIB. We experimented by increasing the number of names in order to see the corresponding memory footprint and selected NameFilter (Wang et al., 2013) and Bloom filter for comparison. Fig. 8 (b) shows the memory footprint generated using different structures, which increases as the number of names increases. We select different learning rate and epoch of the learned model to compare. Learned model (lr = 0.01, E = 40) has the fastest memory footprint with the increase of the number of names. The learned model (lr = 0.005, E = 20) has the least amount of memory and the number of names is larger, its memory footprint increases less. When the number of names is less than 4M, the memory footprint generated by NameFilter is the largest. When the number of names is around 1M, the learned model's memory footprint is close to that of Bloom filter.

Lookup speed: In terms of time consumption in the lookup process, three search methods of Hash check structure, Bloom filter and NameFilter are selected to compare with the learned model. As shown in Fig. 9, the lookup time increases with the number of prefixes. In terms of the total time, the more the number of prefixes, the greater the time consumption generated by Bloom filter. This is because using the Bloom filter to lookup, multiple hash functions are used to calculate each content name, thus generating a large number of conflicts. Therefore, it takes more time to handle these conflicts. NameFilter lookup structure has less time consumption, the average lookup time is stable. The average time of Hash check method is also stable, but with the increase of search content, the total time consumption increases rapidly. Since the learned model needs to train the sets, the more the number of content names, the less time the lookup will consume, which implies that the lookup for more content is efficient.

Fig. 9

Fig. 9. The components of processing time for each lookup:(a) average time; (b) total time.

In summary, through the above experiments, it can be seen that the learned model performs data preprocessing through RNN and then searches through the backup Bloom filter, which reduces the false positive rate of the Bloom filter. Although it is not as fast as NameFilter in terms of search time, it has obvious advantages in terms of memory usage and false positive rate. As the number of lookup names increases, the average time consumption becomes closer to the Hash check method, and the growth rate of the total time consumption also decreases. By controlling some parameters of RNN internal structure, better efficiency is achieved, and in terms of processing large amounts of data, a lower memory footprint can be achieved.

6. Conclusion and outlook
In this paper, a name lookup architecture based on ICN has been proposed. Learned Bloom filter lookup contains a two-layer structure of learned model and a backup Bloom filter. Firstly, the predictability of RNN is used to preprocess the name of ICN, and then use the Bloom filter to achieve more accurate and efficient lookup. Through a lot of experiments, the learned Bloom filter architecture has less memory usage and a lower false positive rate than the standard Bloom filter structure. This architecture is expected to provide new directions in the deployment of actual ICN and ICN router deployments. In our future work, the learned model will be further optimized to improve the performance and efficiency of the training data set process, and more tests will be conducted to find a better architecture for name lookups.