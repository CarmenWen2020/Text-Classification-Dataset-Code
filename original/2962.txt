Advances in research on educational technologies and increasing access to computers offer teacher education providers numerous tools and opportunities for supporting student teachers. However, systematic investigations of online interventions using complex classroom scenarios (scenario-based learning activities) are currently lacking. This study tested whether an online scenario-based learning activity has a positive impact on student teachers’ self-efficacy and emotional, motivational, and cognitive classroom readiness before they start their first teaching practicum. In order to draw differentiated conclusions, we explored whether the effectiveness of the intervention depends on the inclusion of automatized expert teacher feedback and the opportunity to reflect on the scenarios. A total of 238 Australian student teachers (64.3% females, mean age = 23.84 years, SD = 6.64) participated in the study. The student teachers were randomly assigned to one of three experimental conditions:control group (online scenario-based learning activity), intervention group 1 (online scenario-based learning activity and feedback), and intervention group 2 (online scenario-based learning, feedback, and reflection). The findings indicated that, compared to the control group, both intervention conditions had a significant positive effect on cognitive classroom readiness. A significant positive effect on self-efficacy was found for intervention group 2. Overall, our research demonstrates the potential of an easy-to-implement online intervention in enhancing self-efficacy and classroom readiness and points towards the importance of combining feedback and reflection within online scenario-based learning activities.

Previous
Next 
Keywords
Online intervention

Scenario-based learning

Student teachers

Self-efficacy

Classroom readiness

The teaching practicum constitutes the most influential experience in teacher education (e.g., Bullough et al., 2002) and has been labelled as the cornerstone of teaching preparation (Ronfeldt, 2015). During the practicum, student teachers can apply acquired pedagogical knowledge in real classrooms within complex school settings, and thus, in a context that most closely resembles the workplace environment they will encounter as practicing teachers. Accordingly, there is a strong need for teacher education providers to find ways to adequately prepare their students for the challenges of the practicum, and to increase their confidence as teachers. However, classical approaches, such as face-to-face coaching (e.g., Kraft, Blazar, & Hogan, 2018) are time-consuming and largely depend on the availability of personal resources, i.e., experienced coaches. Moreover, in light of the coronavirus (COVID-19) pandemic, access to “real classrooms” and to face-to-face coaching has become difficult, further reinforcing the need for teacher education providers to search for alternatives that can replace more traditional face-to-face approaches (e.g., Kim, 2020).

The present study presents such an alternative. We tested a scenario-based learning activity designed to increase student teachers’ self-efficacy and classroom readiness before the practicum. The intervention can be delivered in an online environment, which does not depend on the real-time presence of classrooms or coaches, overcomes time and space constraints, and can be accessed by large numbers of student teachers (see also e.g., Gossman, Stewart, Jaspers, & Bruceman, 2007; Prilop, Weber, & Kleinknecht, 2019; Prilop, Weber, & Kleinknecht, 2020). Comparing a control group in which student teachers worked on online scenario-based learning tasks comprising complex classroom situations with two intervention groups including additional feedback (intervention group 1) and additional feedback and a reflection exercise (intervention group 2), allowed us to gain insights into the effectiveness of different components of the online intervention as well as their combinations. All in all, by shedding light on the potential of a brief and easy-to-implement online scenario-based intervention and specifically, the role of integrated feedback and reflection in enhancing self-efficacy and classroom readiness, the present study contributes to our research knowledge and yields information that is useful for practice (e.g., the re-design of teacher education curricula to consider online-based practicum preparation).

1. Self-efficacy and multi-dimensional classroom readiness
Student teachers need numerous skills and profound knowledge in multiple areas, such as content knowledge and pedagogical content knowledge (Kunter, Kleickmann, Klusmann, & Richter, 2013) to succeed in the practicum and as practicing teachers. However, from a socio-cognitive perspective (Bandura, 1997), it also seems necessary that they themselves are convinced that they can be successful as teachers, i.e., possess sufficiently high levels of teaching self-efficacy (e.g., Chesnut & Burley, 2015; Dellinger, Bobbett, Olivier, & Ellett, 2008). Self-efficacy has, for example, been found to augment positive emotions and counteract negative emotions during the teaching practicum (Hascher & Hagenauer, 2016). Furthermore, meta-analyses have linked self-efficacy to (beginning) teachers’ decision to remain in the profession (Chesnut & Burley, 2015) as well as to lower burn-out (e.g., Aloe, Amo, & Shanahan, 2014) and higher teaching performance (e.g., Klassen & Tze, 2014), making self-efficacy a particularly promising target for (online) interventions (see also e.g., Weber, Prilop, & Kleinknecht, 2019).

In addition to self-efficacy, a further relevant factor worth targeting in interventions preceding the practicum is student teachers’ perceived “classroom readiness”. Different definitions of, and approaches to, measure classroom readiness exist in the literature on teachers and teacher education (see e.g., Craven et al., 2014; Darling-Hammond, Newton, & Wei, 2013; Haigh, Ell, & Mackisack, 2013; Larsen, 2017). For the purpose of this study, we relied on a multi-dimensional conceptualization with three tangible dimensions reflecting the core psychological concepts of emotions, motivation, and cognition in the teaching domain: Emotional readiness (positive feelings about teaching), motivational readiness (wanting to teach), and cognitive readiness (having the knowledge and skills required to teach).

First, emotions are ubiquitous aspects of teachers' lives (e.g., Frenzel, 2014) and positive emotions have been found to impact on teachers' performance in the practicum (e.g., Chen, 2019). In a similar vein, the recent research synthesis of Keller and colleagues (Keller, Hoy, Goetz, & Frenzel, 2016) underlines the value of teachers' experienced enthusiasm in promoting adaptive student outcomes, such as achievement and motivation, as well as teacher outcomes, such as well-being. Accordingly, emotional readiness in terms of general positive emotions towards teaching and feeling enthusiastic about teaching covered the affective space of classroom readiness. Second, a student teacher arguably cannot be called “classroom ready” if he or she does not (yet) want to teach, and fails to see any personal value in being a teacher (see also e.g., Watt & Richardson, 2007, for the overlapping component of “intrinsic career value”). Therefore, motivational readiness was introduced as a broader drive to start teaching, including the personal importance attached to becoming a teacher. Third, complementing self-efficacy and its more narrowly defined future-oriented focus on specific teaching tasks, student teachers' general impression of their capabilities in the teaching domain represent pivotal prerequisites of their effective teaching and well-being (e.g., Yeung, Craven, & Kaur, 2014). For this reason, cognitive classroom readiness was included as a third facet to capture student teachers’ perceptions of the extent to which they possess the knowledge and skills required to be a good teacher.

2. Online scenario-based learning activities
Scenario-based learning (SBL), also known as case-based learning or problem-based learning (e.g., Errington, 2011; Smith & Ragan, 2005), represents a promising approach to prepare student teachers for the practicum and to boost their self-efficacy and classroom readiness. SBL relies on principles of situated learning theory (e.g., Lave & Wenger, 1991) and situated cognition (e.g., Brown, Collins, & Duguid, 1989), stating that learning is maximized if it can be embedded in situations that mirror the context in which learners later have to apply their acquired knowledge. SBL and related activities may even help to bridge the theory-practice gap in teacher education (e.g., De Coninck, Valcke, Ophalvens, & Vanderlinde, 2019). The theory-practice gap refers to the discrepancy novice teachers encounter between the nature of their (declarative) knowledge-focused teacher preparation program and the experiences they make as practicing teachers (e.g., Korthagen et al., 2007). As SBL provides authentic and complex learning experiences that prompt student teachers to consider different ways of acting and problem solving in everyday teaching situations, it facilitates the transfer of knowledge into professional action and can thus contribute to closing the theory-practice divide (e.g., De Coninck et al., 2019; Dotger, 2013; Sheridan & Kelly, 2012). SBL has been found to be effective in promoting students' learning in different fields, such as medical education and training programs for police officers (e.g., McLean, 2016; Werth, 2011). In addition, scarce research in teacher education suggests that SBL approaches can enhance student teachers’ self-efficacy (Goodin, Bartos, Caukin, & Dillard, 2014).

Delivering SBL in an online environment may be particularly advantageous. First, if SBL can be provided within an online environment, such activities are independent of the real-time presence of coaches or instructors and can overcome time and space limitations (see also e.g., Prilop et al., 2019; Prilop et al., 2020). They can furthermore be delivered at scale, which makes them attractive for both teacher education providers and educational researchers across the world. Second, SBL in online environments offers a “safe space” (e.g., Badiee & Kaufman, 2015) with opportunities to experience conflicts and challenges that will become part of a student teacher's future professional life, whereas provoking such situations with actual students would be ethically problematic (Nordvall, Arvola, & Samulesson, 2014; see also; McGarr, 2020). Third, as online SBL can rely on commonly available technologies, they offer a low-cost alternative to extending teaching practice in the field (Badiee & Kaufman, 2015). They maximize implementation fidelity and allow for flexible use, e.g., by being embedded in teacher education programs or offered as free-standing professional development activity (see e.g., Daniels, Goegan, Radil, & Dueck, 2020, for another online intervention). Fourth, online SBL allows providers to seamlessly and automatically integrate two core components (feedback and reflection) within the scenario tasks that could be key factors driving changes in self-efficacy and classroom readiness.

2.1. The power of feedback and reflection
It is widely acknowledged that feedback can be highly informative as it points towards gaps between one's current limited understanding and a more complete understanding (Hattie & Timperley, 2007). Online SBL activities allow for an integration of principles of good feedback practices, arguing that feedback needs to be timely, specific, accessible, and able to ‘feed-forward’ in that learners should be able to apply what they have learnt (e.g., Gibbs & Simpson, 2004; Hounsell, 2007; Smith, Starratt, McCrink, & Whitford, 2019). In an online environment, personalized feedback provided by experts (i.e., experienced teachers) to student teachers is useful to identify learning and pedagogical gaps, foster self-awareness about their own learning, reflection on behaviours, and self-assessment (e.g., Karaoglan Yilmaz, Olpak, & Yilmaz, 2018; Karaoglan Yilmaz & Yilmaz, 2020a; Wang, Yuan, Kirschner, Kushniruk, & Peng, 2018). Expert feedback in the online environment is a kind of metacognitive support that encourages self-reflection and monitoring, leading to changes in future behaviours (Karaoglan Yilmaz & Yilmaz, 2020b; Karaoglan Yilmaz, 2020).

Of particular relevance, feedback in online environments may be effective in increasing motivation as it individualizes teaching and provides support and guidance to the student in the learning environment (e.g., Karaoglan Yilmaz & Yilmaz, 2020b). With regard to self-efficacy, “social persuasion”, i.e., evaluative performance-related comments one receives, constitutes one of the sources of self-efficacy highlighted by Bandura (1997; see also e.g., Morris, Usher, & Chen, 2017; Oh, 2011; Van Rooij, Fokkens-Bruinsma, & Goedhart, 2019). Accordingly, appropriate feedback should (and has been found to) impact on self-efficacy (e.g., Smith et al., 2019). Feedback within the online SBL activity could conceivably be critical in shaping classroom readiness too. Without guiding feedback, student teachers working on an online SBL activity might easily feel lost, whereas additional feedback should increase motivation for teaching by helping student teachers to thoroughly understand how to act professionally in challenging situations (e.g., De Coninck et al., 2019). As feedback assists them to make sense of complex classroom situations, it could enhance their self-perceived knowledge and elicit more positive emotions towards teaching. With regard to the providers of feedback, prior research indicates that both expert and peer feedback can be valuable (e.g., Weber, Gold, Prilop, & Kleinknecht, 2018); nevertheless, experts tend to deliver higher quality feedback than peers (Prins, Sluijsmans, & Kirschner, 2006). In online SBL activities, standardized feedback from expert teachers that has been pre-recorded or pre-written can be automatically displayed, eliminating burdens in terms of required personal resources for the teacher education program.

Second, positive effects of online SBL might, in addition to feedback, depend on the opportunity for and explicit encouragement of reflection, defined as the “intellectual and affective activities in that individuals engage to explore their experiences in order to lead to new understandings and appreciations” (Boud, Keogh, & Walker, 1985, p. 19). Reflection figures prominently in theoretical models explaining the quality of individuals' learning processes and how they self-regulate their learning (e.g., Coulson & Harvey, 2013; Zimmerman & Labuhn, 2012) and reflecting on practice has long been emphasized as a central activity in teacher education (e.g., Dewey, 1933; see also e.g., Toom, Husu, & Patrikainen, 2015). As compared to solely including feedback, an online SBL combining feedback and reflection allows integrating a “reflection-feedback-cycle”, which could be particularly advantageous (see also De Coninck et al., 2019; Dieker, Rodriguez, Lignugaris/Kraft, Hynes, & Hughes, 2014): If student teachers' reflections on their responses to a complex classroom scenario are followed by feedback explaining the reasoning of experts regarding appropriate reactions to these situations, student teachers (a) gain insights into experts' reasoning and voids in their own current understandings, (b) can build on these experiences when solving and reflecting on the next classroom scenario, and (c) monitor their professional growth (see e.g., Kleinknecht & Gröschner, 2016; Tripp & Rich, 2012, in the context of video-based activities for teacher education). Prior research has demonstrated that (online) intervention studies with feedback and reflection activities can lead to an upsurge in student teachers' self-efficacy (e.g., Christensen, Knezek, Tyler-Wood, & Gibson, 2011; Prilop et al., 2019; Weber et al., 2018). Similarly, such interventions seem well-suited to support classroom readiness, given that the reflection-feedback-cycle should foster high quality learning experiences (e.g., De Coninck et al., 2019), and linked to that, more positive judgments of one's teaching competencies that are continually developed and refined throughout the activity (cognitive classroom readiness) as well as higher levels of motivation to start “real” teaching (motivational classroom readiness). They could also promote emotional classroom readiness, as the additional reflection component might aid student teachers to become even more immersed in the online activity and develop more positive feelings about their future teaching.

In light of these findings and the literature reviewed in this section, online SBL including (expert) feedback and the encouragement to reflect on taken actions augurs well as a tool for increasing self-efficacy and classroom readiness. However, several research gaps remain that the present study aims to address. To the best of the Authors' knowledge, there is a lack of research on different elements that can be included in online SBL for student teachers. Hence, despite a convincing theoretical rationale that SBL with feedback and especially, SBL with feedback and reflection, should be more beneficial for teaching practicum-relevant outcomes than online SBL missing these components, a systematic investigation comparing such different SBL conditions has not yet been carried out. Moreover, whereas research exists on online interventions similar to our proposed SBL intervention which show effects on self-efficacy, less attention has been paid to classroom readiness as a potentially critical prerequisite for a successful teaching practicum. The present study's innovative value therefore relates to ascertaining (a) the effects of an SBL activity for student teachers with varying components, and specifically the effects of online SBL with feedback (intervention group 1) as well as online SBL with feedback and reflection (intervention group 2), as compared to a SBL activity without any of these components (control group). A further highlight of the present research is that (b) both self-efficacy and multi-dimensional classroom readiness will be considered as outcomes. The study thus expands on previous research focusing on increases in self-efficacy as motivational consequence of online interventions. Due to the multi-dimensional nature of classroom readiness comprising affective, motivational, and cognitive classroom readiness-features, the study can furthermore contribute to a more fine-grained understanding by exploring whether SBL intervention conditions yield similar effects on all classroom readiness dimensions. The present study was therefore guided by the following research questions:

(1)
Does participating in one of the two types of intervention–without distinguishing between the type of interventions–have a positive effect on self-efficacy and the three domains of classroom readiness (Model 1a)? Specifically, we hypothesize that taking part in the intervention should have a positive effect on student teachers' self-efficacy (Hypothesis 1a), motivational classroom readiness (Hypothesis 1b), emotional classroom readiness (Hypothesis 1c), and cognitive classroom readiness (Hypothesis 1d). To test the robustness of the findings, we will include a set of control variables (gender, age, years of study and prior experience working as a teacher) and re-estimate the effects (Model 1b).

(2)
When considering the two interventions separately and using the control group as a reference group, does participating in intervention 1 (feedback only) or in intervention 2 (feedback and reflection) have a positive effect on the outcomes (Model 2a)? We expect that both intervention 1 and intervention 2 should have positive effects on self-efficacy (Hypothesis 2a for intervention 1 and Hypothesis 2b for intervention 2), motivational classroom readiness (Hypothesis 2c and 2d, respectively), emotional classroom readiness (Hypothesis 2e and 2f), and cognitive classroom readiness (Hypothesis 2g and 2h). In addition, does intervention 2 yield significantly stronger effects than intervention 1 for all outcomes? We hypothesize that this should be the case with regard to self-efficacy (Hypothesis 3a), motivational classroom readiness (Hypothesis 3b), emotional classroom readiness (Hypothesis 3c) and cognitive classroom readiness (Hypothesis 3d). Again, as a robustness check, Model 2b will test the same effects, while additionally accounting for the effects of control variables (gender, age, years of study and prior experience working as a teacher).

3. Method
3.1. Participants
The sample analyzed in this study consisted of 238 Australian student teachers. They were randomly assigned to three groups: 86 in the control group (SBL only), 76 in intervention group 1 (SBL and feedback), and 76 in intervention group 2 (SBL and feedback and reflection). The participating student teachers were on average 23.84 years old (SD = 6.64, ranging from 18 to 56 years) and 64.3% identified as females. The majority of the participants were recruited from two university-based teacher education providers in New South Wales (NSW), one city and one regional (150 and 54 student teachers, i.e., 63.0% and 22.7%, respectively). In addition, 34 (14.3%) current scholarship holders from a range of teacher education programs in NSW participated. Table 1 provides descriptive information separately for the three conditions. We want to mention that the original sample contained 264 student teachers who gave consent to the use of their data for research purposes. However, we excluded 10 participants as they skipped the SBL activity and/or had missing values on all outcome measures. We did not use the data of 16 further participants of one of the two intervention groups who had indicated that they did not carefully read the feedback and/or worked on the reflection exercise (“manipulation check questions”) for the main analyses. Therefore, the effects should be interpreted as effects of the intervention given that the participants appropriately completed the critical intervention components (reading feedback and reflecting). Nonetheless, we also re-ran the analyses without excluding these participants and report these findings in the Online Supplements.


Table 1. Descriptive information for the three groups assessed prior to the scenario-based learning intervention.

Condition	Control Group (SBL Only) (N = 86)	Intervention Group 1 (SBL + Feedback) (N = 76)	Intervention Group 2 (SBL + Feedback + Reflection) (N = 76)	Tests for statistically significant differences between groups
Age	M = 24.16 SD = 7.47 Range = 18 - 55	M = 23.37 SD = 5.60 Range = 18 - 46	M = 23.93 SD = 6.66 Range = 18 - 56	F(2, 235) = 0.299, p = .742
Gender				χ2 (2) = 2.126, p = .345
 Female	59	44	50	
 Male	27	32	26	
Years of Study	M = 2.27 SD = 1.31
Range = 1- 6	M = 2.27 SD = 1.24
Range = 1- 6	M = 2.18
SD = 1.18
Range = 1 - 5	F(2, 235) = 0.128, p = .880
Prior experience working in schools				χ2 (2) = 0.347, p = .841
 Yes	52	49	46	
 No	34	27	30	
Degree Program				χ2 (2) = 0.740, p = .691
 Bachelor	55	53	47	
 Master	30	23	27	
Teacher Education Provider				χ2 (4) = 3.631, p = .458
 NSW City	53	51	46	
 NSW Regional	23	16	15	
 NSW Scholarship	10	9	15	
Note. Means (M) and Standard Deviations (SD) are reported for continuous variables; categorical variables are shown in absolute numbers; One-way between-subject ANOVAs were used to compare the mean values between the three conditions and chi square tests were used to compare the distribution of the categorical variables between the three conditions. NSW = New South Wales.

3.2. Materials
The content of the online SBL activity for all three conditions was created by drawing on an extensive bank of scenarios from situational judgement tests, a vignette-based assessment method originally developed for teacher selection (for more details see e.g., Klassen, Kim, Rushby, & Bardach, 2020). Given that these situational judgment tests center on complex school-based scenarios, they lend themselves very well to be adapted for SBL (for research outside of education successfully using situational judgment test-based content for trainings, see e.g., Cox, Barron, Davis, & de la Garza, 2017; Hsu, Chang, & Hsieh, 2017). Please note that in the current study, the situational judgment tests, after their adaptation to scenario-based learning activities, were used as an intervention tool and not as a measurement tool.

In the current study, each situational judgment test scenario had three response options and student teachers were asked to rate the appropriateness of each option, from (1) appropriate to (4) inappropriate, in consideration of what a beginning teacher should do in the circumstances described in the scenario. Each response was scored; however, the scores were solely used to determine the type of feedback student teachers receive (see below). The scoring key for the situational judgment tests had been established though concordance panels with subject matter experts in the field by adopting a hybrid approach (see Bergman, Drasgow, Donovan, Henning, & Juraska, 2006 for details): Subject matter experts developed the initial scoring key which was then adapted based upon level of expert consensus, item difficulty, item-total correlations, and applicant—i.e., teacher education candidates—scoring patterns. The scoring was based on the scoring system described by Patterson, Ashowrth, and Good (2013), and thus, points were allocated based on the extent to which student teachers’ responses aligned with the established scoring key. For instance, student teachers were allocated three points if their response was in direct alignment with the scoring key, two points if their answer was one position away, one point if their answer was two positions away, and no points if three positions away. As the situational judgment tests had primarily been used in the UK, Australian expert teachers and teacher educators checked whether the content and wording were also appropriate for the Australian context. Small adaptations were made (e.g., “pupils” was changed to “students”, which is more common in Australian schools). Six situational judgment test scenarios were used for the current study.

As a next step and for the purpose of transforming the situational judgment tests from an assessment method to an SBL activity with feedback and reflection opportunities for the two intervention groups, the following adaptations were made. After each response within a scenario, student teachers were asked to elaborate on their rationale behind this choice, and therefore, to reflect on why they considered a specific response as appropriate, inappropriate etc. After reflecting, feedback was displayed. The feedback was generated based on expert teachers' explanations and thoughts on why this particular response would be appropriate, inappropriate etc. The expert explanations were then tailored to each possible option: For example, if a student teacher selected ‘appropriate’, but the experts deemed the response as “inappropriate”, the feedback was framed differently than if the student teacher would have also selected “appropriate”—even though the core message, i.e., the explanation, remained the same.

The scenario-based intervention materials (SBL with feedback and reflection) have been tested in a recent qualitative study with 40 student teachers who had not yet completed a teaching practicum (Klassen et al., 2021, anonymized). A total of 87.5% of the participating student teachers stated that they felt more confident in entering the classroom after completing the SBL activity (i.e., increased self-efficacy). Those who reported increases in self-efficacy, attributed this change to the opportunity to reflect on professional practice (35% of responses), the value of receiving feedback from experienced teachers (50% of responses), and being exposed to realistic scenarios (15% of responses). Moreover, 92.5% indicated that they felt more prepared for the teaching practicum after working on the SBL activity (a broad indicator of classroom readiness). Participants who reported an uplift in their classroom readiness believed that this was due to the opportunity to reflect on professional practice in the SBL activity (24% of responses), the value of feedback from experienced teachers (32% of responses), and the exposure to diverse, realistic scenarios (43% of responses). These first findings impart confidence in using SBL activities as a preparation strategy prior to the practicum. However, as all student teachers worked on the SBL activity that included feedback and reflection, no insights on the relative importance of different components (feedback, reflection) could be achieved—a gap that the current quantitative study aims to fill. Fig. 1 shows an example of a scenario-based learning task for intervention group 1, with the SBL task including additional feedback. Fig. 2 provides an example of one of the scenario-based learning tasks for intervention group 2 with additional feedback and the opportunity for reflection.

Fig. 1
Download : Download high-res image (1MB)
Download : Download full-size image
Fig. 1. Example of the scenario-based learning activity for intervention group 2 (SBL and feedback). The control group worked on the SBL task (displayed above) without receiving feedback (displayed below) immediately after each scenario.

Fig. 2
Download : Download high-res image (1MB)
Download : Download full-size image
Fig. 2. Example of the scenario-based learning activity for intervention group 2 (SBL and feedback and reflection).

3.3. Procedure
All student teachers were randomly assigned to one of three conditions. Prior to the activity, all groups were asked to provide socio-demographic information. In order to compare the elements of SBL, the additional feedback component was included for intervention group 1 and 2, while the reflection component was only used for intervention group 2. The control group worked on the scenarios without feedback and reflection. After the activity, the three groups filled out the outcome measures (self-efficacy, classroom readiness). For intervention group 1 and 2, an additional “manipulation check” question was included after the SBL activity and before the outcome measures asking them to rate the extent to which they had carefully read the expert teacher feedback. A further question was included only for intervention group 2, asking them to rate the extent to which they had done their best to explain the rationale behind their ratings as an indicator of how seriously they had taken the reflection exercise. A feedback report for the control group, including their chosen options in the scenarios and the feedback from expert teachers was provided after they had done the post-test measures (see Fig. 3 for a schematic diagram of the elements of the intervention). All invited student teachers received a web link to the SBL activity and completed it on their own device at home. Whether presented as a required learning activity (at one university) or not, no grade or compensation was provided. Consent for this study was sought in accordance with institutional human ethics board approval.

Fig. 3
Download : Download high-res image (439KB)
Download : Download full-size image
Fig. 3. Overview of the experimental design.

3.4. Measures
3.4.1. Independent variables
Group membership. Information on whether a participant was assigned to the control group (SBL only), intervention group 1 (SBL and feedback), or intervention group 2 (SBL and feedback and reflection) was collected.

Control variables. The following variables were used as covariates: student teachers’ age, year of study, gender (0 = female, 1 = male),1 prior experience working in schools (0 = yes, 1 = no). These variables were included to control for key sociodemographic characteristics of our sample and because prior research has documented effects on our outcomes of interest (e.g., effects of gender and teaching experience on self-efficacy, see e.g., Huang, 2013; Klassen & Chiu, 2010).

3.4.2. Dependent variables
Self-efficacy. We measured self-efficacy with items adapted from Tschannen-Moran and Woolfolk Hoy (2001), as used by Klassen and Durksen (2014) in a study with student teachers. Klassen and Durksen (2014) slightly modified the wording of the three original items after consultation with a group of teacher educators (sample item: “I am confident that I can manage student behaviour”, αControl Group = .78; α Intervention 1 (feedback) = .81; and α Intervention 2 (feedback + reflection) = .71).

Classroom readiness. We assessed student teachers’ perceived classroom readiness in three domains (see also Klassen et al., 2021, anonymized) using items adapted from existing scales (Frenzel et al., 2016; Klassen, Yerdelen, & Durksen, 2013; Schiefele, Streblow, & Retelsdorf, 2013) as well as self-developed items: emotional domain (two items, sample item: „I feel enthusiastic when thinking about becoming a teacher”, α CG = .71; α Int1 = .77; and α Int2 = .82), motivational domain (two items, sample item: “I am motivated to start teaching as soon as possible”, α CG = .70; α Int1 = .75; and α Int2 = .82), cognitive domain (two items, sample item: “I think I have the knowledge needed to be a good teacher”, α CG = .73; α Int1 = .64; and α Int2 = .68).

3.5. Analyses
In the present study data analyses were conducted in two phases. First, it was tested whether overall, the intervention was effective. In a path model (Model 1a), a dummy-coded variable (0 = control group, 1 = intervention group 1 or intervention group 2) was used to predict self-efficacy and cognitive, emotional, and motivational classroom readiness. Hence, the effects of intervention group 1 and 2 were not separately tested and instead, it was investigated whether there was an overall intervention effect. As a next step and to check the robustness of our findings, all control variables were included and the model was re-estimated (Model 1b), following suggestions in the literature to take potentially confounding variables into account even if the study was conducted using a randomized research design (Mayer, Thoemmes, Rose, Steyer, & West, 2014).

Second, to draw more differentiated conclusions regarding the different intervention components in terms of feedback (intervention group 1) and feedback and reflection (intervention group 2), another path model was set up with two dummy-coded variables indicating membership to either intervention group 1 or 2 (Model 1b). In this model, the effects of intervention 1 and 2, respectively, on the outcomes, were tested, using the control group as reference group for the effects of both interventions. In addition to the two effects of the two intervention groups, which provided insights into whether the two interventions—as compared to the control group—were effective, the differences in these two regression slopes (i.e., effect for intervention 1 vs. effect for intervention 2) were also tested for statistical significance. This made it possible to infer the differences in the effects of the two interventions and to answer the question of whether intervention 2 including both feedback and reflection had a significantly stronger effect than intervention 1 which solely included feedback (Model 2a). Finally, we entered all control variables and re-ran the analyses of Model 2a (Model 2b).

Both unstandardized and standardized regression coefficients were reported, and the latter can be interpreted according to Cohen's (1988) guidelines with values above 0.10 indicating small effects, values above 0.30 indicating moderate effects, and values above 0.50 indicating large effects. All significance testing was performed at the .05 level. The analyses were performed with Mplus Version 8.2 (Muthén & Muthén, 2017) using the robust maximum likelihood estimator (MLR) to account for non-normality of the data. In this study, the amount of missing data present at the item level was 0% for all items except for the control variable year of study with 3.4% missing values. For this control variable, list-wise deletion was used.

4. Results
4.1. Descriptive data
There were no significant differences between the three groups with respect to any of the socio-demographic variables assessed prior to the SBL learning activity (see Table 1). Table 2 shows correlations among all variables separately for the three groups. With regard to the outcome self-efficacy, the following means scores (and standard deviations) were observed for the three groups: M = 4.66 (SD = 0.68) for the control group (SBL only), M = 4.70 (SD = 0.60) for intervention group 1 (SBL and feedback), and M = 4.84 (SD = 0.69) for intervention group 2 (SBL and feedback and reflection). For emotional classroom readiness, a mean score of 5.31 (SD = 0.75) for the control group, a mean score of 5.24 (SD = 0.70) for intervention group 1, and a mean score of 5.37 (SD = 0.67) for intervention group 2 was obtained. Motivational classroom readiness in the control group had a mean of 5.08 (SD = 0.79), a mean of 5.13 (SD = 0.74) in the intervention group 1, and a mean of 5.18 (SD = 0.79) in the intervention group 2. For the scale assessing cognitive classroom readiness, the mean was 4.68 (SD = 0.75) in the control group, 4.90 (SD = 0.59) in the intervention group 1, and 4.95 (SD = 0.74) in the intervention group 2. The results from the main analyses addressing the research questions are reported in Table 3.


Table 2. Bivariate correlations among all variables analyzed in the present study separately for the three groups.

Variable	1. Self-efficacy (Outcome)	2. Emotional CR (Outcome)	3. Motivational CR (Outcome)	4. Cognitive CR (Outcome)	5. Age (Control)	6. Gender (Control)	7. Years of Study (Control)	8. Prior experience (Control)
1. Self-efficacy (Outcome)		.56	.62	.61	.17	.08	.06	-.22
2. Emotional CR (Outcome)	.49/.60		.79	.48	.17	.07	.12	-.11
3. Motivational CR (Outcome)	.39/.51	.66/.71		.45	.22	-.03	.12	-.30
4. Cognitive CR (Outcome)	.67/.71	.58/.67	.58/.62		.15	.07	-.07	-.05
5. Age (Control)	.33/-.14	.14/-.17	.26/.02	.13/-.08		-.05	.42	-.09
6. Gender (Control)	.02/-.01	.07/.08	.02/-.07	.04/.04	.11/.07		.03	.03
7. Years of Study (Control)	.20/-.07	.06/-.09	.25/-.03	.22/.04	.49/.56	.03/.09		.24
8. Prior experience (Control)	-.09/-.19	-.10/-.08	-.02/-.08	-.01/-.14	-.18/-.05	-.12/.07	-.02/.26	
Note. Correlation coefficients in the upper diagonal are for the control group (SBL only) and correlation coefficients in the lower diagonal are for intervention group 1 (SBL + feedback)/intervention group 2 (SBL + feedback + reflection); CR = Classroom Readiness; Dummy-coded variables were used for gender (0 = female, 1 = male) and prior experience working in schools (0 = prior experience, 1 = no experience); Statistically significant correlations at p < .05 are boldface.


Table 3. Results of the Regression Models: Effects of the Online Interventions on Self-efficacy, Emotional Classroom Readiness, Motivational Classroom Readiness, and Cognitive Classroom Readiness.

Predictors	Self-efficacy	Emotional CR	Motivational CR	Cognitive CR
Est. (SE)	Std. Est.	Est. (SE)	Std. Est.	Est. (SE)	Std. Est.	Est. (SE)	Std. Est.
Model 1a
 Overall Intervention Effect	0.12 (0.09)	0.17	-0.01 (0.10)	-0.02	0.07 (0.11)	0.09	0.22 (0.10)	0.32
Model 1b
 Overall Intervention Effect	0.12 (0.09)	0.18	-0.02 (0.10)	-0.03	0.08 (0.10)	0.10	0.25 (0.10)	0.35
 Controls: Age	0.01 (0.01)	0.08	0.00 (0.01)	0.04	0.01 (0.01)	0.12	0.01 (0.01)	0.06
 Controls: Gender	0.01 (0.09)	0.01	0.06 (0.09)	0.09	-0.05 (0.10)	-0.06	0.03 (0.09)	0.05
 Controls: Years of Study	-0.09 (0.03)	-0.16	-0.07 (0.04)	-0.13	-0.07 (0.04)	-0.11	-0.02 (0.04)	-0.03
 Controls: Prior experience	-0.20 (0.09)	-0.31	-0.12 (0.10)	-0.17	-0.17 (0.11)	-0.22	-0.06 (0.10)	-0.09
Model 2a
 Effect Intervention 1	0.04 (0.10)	0.06	-0.08 (0.11)	-0.11	0.04 (0.12)	0.06	0.18 (0.10)	0.25
 Effect Intervention 2	0.19 (0.11)	0.28	0.05 (0.11)	0.08	0.10 (0.12)	0.13	0.27 (0.12)	0.39
Model 2b
 Effect Intervention 1	0.04 (0.10)	0.07	-0.09 (0.12)	-0.12	0.07 (0.12)	0.09	0.19 (0.11)	0.27
 Effect Intervention 2	0.19 (0.11)	0.29	0.05 (0.11)	0.07	0.09 (0.12)	0.12	0.30 (0.12)	0.42
 Controls: Age	0.01 (0.01)	0.08	0.00 (0.01)	0.04	0.01 (0.01)	0.12	0.01 (0.01)	0.06
 Controls: Gender	0.02 (0.09)	0.03	0.07 (0.09)	0.10	-0.05 (0.10)	-0.06	0.04 (0.09)	0.05
 Controls: Years of Study	-0.08 (0.03)	-0.16	-0.07 (0.04)	-0.12	-0.07 (0.04)	-0.11	-0.02 (0.04)	-0.03
 Controls: Prior experience	-0.21 (0.09)	-0.31	-0.13 (0.10)	-0.18	-0.17 (0.10)	-0.22	-0.07(0.10)	-0.09
Note. CR = Classroom readiness; Est. = Unstandardized estimate; Std. Est. = Standardized estimate; SE = Standard Error; Dummy-coded variables were used for gender (0 = female, 1 = male) and prior experience working in schools (0 = prior experience, 1 = no prior experience); One-tailed tests were conducted for the intervention effects, whereas the results for control variables are based on two-tailed tests; Statistically significant results at p < .05 are boldface.

4.2. Overall intervention effects
First, and without distinguishing between intervention groups, we tested whether being part of one of the intervention groups predicted the measured outcomes (Model 1a). Participating in the intervention (vs. in the control condition) was, as expected, related to higher levels of self-reported cognitive classroom readiness (Hypothesis 1d, standardized β = 0.32, p < .05). We obtained no statistically significant effects for self-efficacy (Hypothesis 1a, standardized β = 0.17, p > .05), emotional classroom readiness (Hypothesis 1b, standardized β = −0.02, p > .05), and motivational classroom readiness (Hypothesis 1c, standardized β = 0.09, p > .05).

After additionally considering the control variables (Model 1b), the results indicated a significant effect for cognitive classroom readiness (standardized β = 0.35, p < .01), and, as in Model 1a without controls, non-significant effects for all other outcomes (self-efficacy: standardized β = 0.18, p > .05, emotional classroom readiness: standardized β = −0.03, p > .05, motivational classroom readiness: standardized β = 0.10, p > .05). The control variable age significantly predicted motivational classroom readiness (standardized β = 0.12, p < .01), but was not significantly related to the other outcomes (standardized β′s ranging between = 0.04 and 0.08, all p's > .05). No statistically significant effects for gender (female = 0, male = 1) were documented (standardized β′s ranging between −0.06 and 0.09, all p's > .05). Years of study was found to be significantly related to self-efficacy (standardized β = −0.16, p < .01). None of the other effects of years of study were statistically significant (standardized β′s ranging between −0.03 and −0.13, all p's > .05). Prior experiences teaching in schools (prior experience = 0, no prior experiences = 1) significantly and negatively predicted self-efficacy (standardized β = −0.31, p < .05). No significant effects occurred for the three classroom readiness domains (standardized β′s ranging between −0.22 and −0.09, all p's > .05).

4.3. Separate effects of the two interventions groups
When distinguishing between the two intervention types (Model 1b), a positive effect on self-efficacy was found for the intervention group 2 including feedback and reflection (Hypothesis 2b, standardized β = 0.28, p < .05), whereas no significant effect occurred for intervention 1 including only feedback (Hypothesis 2b, standardized β = 0.06, p > .05). None of the effects for emotional classroom readiness (intervention group 1, Hypothesis 2c, standardized β = −0.11, p > .05, intervention group 2, Hypothesis 2d, standardized β = 0.08, p > .05) and motivational classroom readiness (intervention group 1, Hypothesis 2e, standardized β = 0.06, p = >.05, intervention group 2, Hypothesis 2f, standardized β = 0.13, p > .05) attained statistical significance. For cognitive classroom readiness, significant effects emerged for both intervention groups (intervention group 1, Hypothesis 2g, standardized β = 0.25, p < .05, intervention group 2, Hypothesis 2h, standardized β = 0.39, p < .01). However, testing the regression slopes for statistical significance did not reveal a statistically significant difference between the two groups: β = −0.15, p > .05 for self-efficacy (Hypothesis 3a), β = −0.13, p > .05 for emotional classroom readiness (Hypothesis 3b), β = −0.05, p > .05 for motivational classroom readiness (Hypothesis 3c), β = −0.10, p > .05 for cognitive classroom readiness (Hypothesis 3d).

In Model 2b including the control variables, the effect of intervention 2 on self-efficacy remained significant (standardized β = 0.29, p < .05) and the effect of intervention 1 remained non-significant (standardized β = 0.07, p > .05). Similarly, all effects for emotional and motivational classroom readiness were non-significant (emotional classroom readiness: standardized β = −0.12, p > .05 for intervention group 1 and standardized β = 0.07, p > .05 for intervention group 2; motivational classroom readiness: standardized β = 0.09, p > .05 for intervention group 1 and β = 0.12, p > .05 for intervention group 2). The results furthermore yielded significant effects on cognitive classroom readiness for both intervention 1 (standardized β = 0.27, p < .05) and intervention 2 (standardized β = 0.42, p < .01). The pattern of results for all control variables was the same as in Model 1b; with some effect sizes solely differing on the second decimal place (see Table 3). As in the model without control variables, none of the regression slopes differed significantly between the two intervention groups (for self-efficacy: β = −0.15, p > .05; for emotional classroom readiness: β = −0.14, p > .05; for motivational classroom readiness, β = −0.02, p > .05; for cognitive classroom readiness: β = −0.10, p > .05). All standardized and unstandardized effects including standard deviations can be consulted in Table 3.

The results of all analyses based on the sample without excluding student teachers who had indicated that they did not carefully read the feedback or reflected on their responses are additionally reported in the Online Supplement. In these analyses, the results for the overall intervention effects remained unchanged and a statistically significant effect for cognitive classroom readiness, but not for the other outcomes was found. However, the effect of intervention 2 on self-efficacy reached statistical significance only in the model including control variables, with a p-value slightly above .05 (i.e., .054) in the model without control variables. For intervention 1, no statistically significant effect for cognitive classroom readiness was obtained (in both the model with and without covariates). Table A1 in the Online Supplement provides more details and shows all standardized and unstandardized effects including standard deviations from the additional analyses. The discussion below focuses on the results from the main analysis.

5. Discussion
The main goal of the present study was to test different components of a brief and easy-to-implement online intervention aiming to increase student teachers’ self-efficacy and classroom readiness and thus, to contribute to their practicum preparation. We therefore adopted an intervention design with random assignment to three conditions—a control group working on scenario-based content without feedback and reflection, and two intervention groups working on scenario-based content with feedback (intervention 1), and feedback and reflection (intervention 2).

Focusing on overall intervention effects, the results revealed that participating in one of the intervention groups, as compared to the control group, had a significant positive effect on cognitive classroom readiness, with a standardized medium-sized effect. Hence, taking part in one of the interventions made student teachers more likely to believe that they possessed the knowledge and the skillset needed to succeed as a teacher. There were neither significant effects for self-efficacy, nor for motivational or emotional classroom readiness, and the close-to-zero effect for emotional classroom readiness, had, unexpectedly, a negative sign. While non-significant, the results for self-efficacy and motivational classroom readiness still indicated small standardized effects in favour of the interventions (self-efficacy > motivational classroom readiness.

Considering the two types of interventions separately (i.e., feedback with and without reflection), aids in clearing up the findings and tells a more differentiated story by disentangling the functioning of different intervention components. When contrasting the effects of each intervention with the control group, several noticeable patterns emerged. First, the intervention combining feedback and reflection, but not the feedback-only intervention had a significant effect on student teachers' self-efficacy. As already pointed out by Bandura (1986), the ability to self-reflect is fundamental to the construct of self-efficacy. Aligned with this, prior studies have confirmed that engagement in reflection forecasts increases in teachers' self-efficacy (e.g., Beverborg, Sleegers, Endedijk, & Van Veen, 2015; Gabriele & Joram, 2007). Reflection helps to arrive at more satisfying solutions by facilitating a deeper understanding. As such, sustained levels of reflection can lead to mastery experiences, i.e., the achievement of goals through one's personal actions and an important source of self-efficacy (Beverborg et al., 2015; see also Bandura, 1997; Morris et al., 2017). Accordingly, the provision of feedback may not have been enough to produce an effect on self-efficacy and instead, student teachers needed to make sense of and reflect on the “why” behind their chosen actions within the online SBL activity. Students want to experience autonomy in the online learning environment (Roberts, Howell, Seaman, & Gibson, 2016; see also Karaoglan Yilmaz & Yilmaz, 2020b) and by engaging in reflection, student teachers became more active agents of their own learning, which could have also fed into their self-efficacy (e.g., Mizumoto, 2013; Walton, 2014). By solely being exposed to feedback, a more passive learning experience, the other intervention group was deprived of this more active learning opportunity.

On the other hand, both interventions showed positive effects on cognitive classroom readiness, with almost medium and medium standardized effects for intervention group 1 and 2 in the main analyses, respectively. Thus, the two interventions had effects on student teachers’ more general impressions of their competencies as teachers, whereas the intervention incorporating feedback and reflection had an additional effect on the more specific construct of teaching self-efficacy. It could be that, in order to foster self-beliefs regarding more global competencies (cognitive classroom readiness), the transmission of expert knowledge via feedback sufficed; however, only more in-depth engagement with the specific and contextualized classroom scenarios through reflection affected specific self-efficacy beliefs (e.g., Christensen et al., 2011; De Coninck et al., 2019).

Motivational classroom readiness was not significantly affected by participating in one of the two interventions. One reason underlying this finding could possibly be linked to the content of our motivational classroom readiness measure. Motivational classroom readiness was conceptualized as a desire to start teaching as soon as possible combined with the personal value attached to teaching and being a teacher. We thus suggest that the second aspect represents a rather stable, more trait-like characteristic that is formed very early in student teachers’ career (e.g., Watt & Richardson, 2007), most likely even before starting teacher education. Hence, this feature might be hard to change, especially within the limited time frame of the tested online intervention. Another possible explanation for this non-significant effect could be that the entire SBL activity, even the conditions including feedback and reflection, did not allow for sufficient flexibility. For instance, the student teachers could not choose areas they particularly wanted to focus on (e.g., challenging student behavior, parent interactions, etc.), the difficulty levels of the scenarios, or whether they wanted to engage in further reflections after the expert teacher feedback. Providing more flexibility and granting student teachers more personal responsibility to decide on own learning paths might have produced a stronger effect on their motivation (e.g., Karaoglan Yilmaz & Yilmaz, 2020b; Lee, Pate, & Cozart, 2015; Zhu, Bonk, & Doo, 2020).

Interestingly, for emotional classroom readiness, it was shown that the negative sign of the (non-significant) effect obtained in the analyses combining both interventions was driven by a negative effect for intervention group 1. Albeit non-significant too, this small standardized effect raises questions, also because intervention 2 had a positive non-significant effect on emotional classroom readiness. A potential interpretation relates to the nature of feedback in our study. Consider that the way feedback was provided in our SBL activity did not leave room for negotiations and follow-up questions due to its standardized and automatized nature. Consequently, the mere exposure to externally generated knowledge, without the opportunity for reflections on one's own approaches to solving complex SBL as internally regulated and autonomous explorations, might have sparked frustration (e.g., Reeve, Jang, Carrell, Jeon, & Barch, 2004), and as a spill-over effect, less positive feelings about teaching.

Finally, testing the regression slopes for statistical significance did not reveal a statistically significant difference between the two groups. We can thus conclude that the effects of the interventions on the considered outcomes did not differ significantly. However, the fact that the effect sizes for all outcomes were larger for the intervention group 2 than intervention group 1 let us still cautiously suggest that combining feedback and reflection could be most advantageous. In addition, it should be mentioned that the intervention effects in both analyses, i.e., the analyses for overall intervention effects as well as those for the effect for the two separate intervention types and the tests of differences between intervention types, remained largely unaffected by the inclusion of a set of control variables. Whereas gender did not predict any of the outcomes, small positive effects of age on motivational classroom readiness were noted. In addition, year of study significantly and negatively predicted self-efficacy. As student teachers who were more advanced in their program reported lower levels of self-efficacy, interventions like the one tested in this study might be particularly important for student teachers in the later stages of their program. Moreover, those who reported that they had no prior experience working in schools felt significantly less self-efficacious, indicating a need to offer student teachers high quality practical learning experiences at schools to boost their self-efficacy, supplemented by online SBL interventions with feedback and reflection in periods without teaching opportunities.

5.1. Conclusions and theoretical implications
The present study tested the effects of an online SBL intervention and its varying components (SBL only, SBL with feedback, SBL with feedback and reflection) on student teachers’ self-efficacy and their emotional, motivational, and cognitive classroom readiness--as important teacher education outcomes on their own and factors that should facilitate the navigation through the often challenging practicum period (e.g., Schwarzer & Hallum, 2008; Weber et al., 2019). To summarize, considering both interventions (SBL and feedback, SBL and feedback and reflection) together, we found overall intervention effects on cognitive classroom readiness. Disentangling effects of the two interventions revealed that, as compared to the control group, intervention 1 (SBL and feedback) had a significant effect on cognitive classroom readiness and intervention group 2 had a significant effect on self-efficacy. No significant effects for motivational and emotional classroom readiness surfaced and the regression slopes for none of the effects differed between the two intervention groups.

Theoretical and practical implications of our study are threefold. First, our study underlines the value of online SBL for student teachers which integrates multiple “reflection-feedback-cycles”. The notion that feedback and reflection within online environments is beneficial for student teachers and other populations is consistent with the literature (e.g., Karaoglan Yilmaz, 2020; Karaoglan Yilmaz & Yilmaz, 2020a; Wang et al., 2018; Wong et al., 2019). Our study adds to this body of research by systematically and repeatedly (i.e., for each scenario within the SBL activity) coupling reflection with feedback tailored to the specific responses given by the student teachers. Several models of online learning emphasize different types of interactions between students, teachers, and the content in the online environment (see e.g., online learning model developed by Anderson, 2011, see also the Community of Inquiry Model, e.g., Akyol & Garrison, 2008; Karaoglan Yilmaz, 2020). The reflection-feedback cycle leads to more intense student-content interactions; in our study “content” is represented by the online SBL activity with multiple complex scenarios. Complementing the other potential explanations for the presence (or absence) of effects in our study outlined in the Discussion, the more in-depth engagement with the content in the condition involving feedback and reflection may have been another reason for the effects found for self-efficacy and cognitive classroom readiness. For example, it has been proposed that expert feedback in online environments becomes a metacognitive support that encourages self-reflection and monitoring (Karaoglan Yilmaz & Yilmaz, 2020b; Karaoglan Yilmaz, 2020). Further supporting self-reflection by providing explicit structured opportunities for reflection immediately after the scenario—in addition to feedback helping to better understand the complex scenario and to correct misconceptions—seems to be particularly helpful for student teachers.

On the other hand, when looking at these broader online learning models (see e.g., in Anderson, 2011, Akyol & Garrison, 2008; Karaoglan Yilmaz, 2020), it also becomes apparent that our study considered solely a small part of an arguably more complex online learning process that may be necessary to fully exploit the potential of online SBL interventions. Specifically, the online SBL constituted an independent learning experience, with no potential for interactions between student teachers and teacher educators or student teachers and student teachers (e.g., in virtual learner communities, such as forums or social networking sites, see e.g., Deng & Tavares, 2013; Karaoglan Yilmaz & Yilmaz, 2019). Even though this enables us to gather differentiated insights on feedback and reflection as central components of online SBL, this restricted focus may be a reason why, for example, the intervention did not affect motivational or emotional classroom readiness. Allowing for interactions other than student-content (i.e., student-student and student-teacher educator, or student-expert teacher) could probably have further reduced transactional distance, that is, the perception of the psychological distance between the student and peers, the student and the instructor, or the student and the content (see e.g., Karaoglan Yilmaz & Yilmaz, 2019; Moore & Kearsley, 2011). For instance, student-student interactions and relatedly, reduced transactional distance, have already been shown to predict students’ positive emotions (e.g., Yu, Huang, Han, He, & Li, 2020). Offering the opportunity to become part of an online learning community thus may have also fostered emotional classroom readiness. In addition, there was a lack of interactions between student teachers and the expert teachers, as automatically generated feedback pre-written by expert teachers based on student responses was used. Nonetheless, allowing for such interactions between a mentor expert teacher and the student teacher could have further developed the quality of the online learning environment, thereby making the SBL learning activities more meaningful and potentially enhancing student teacher outcomes.

Another theoretical contribution of our work relates to the investigation of multiple outcomes relevant for the upcoming teaching practicum. Whereas some empirical evidence focusing on student teachers in online environments exists for self-efficacy as outcome (see e.g., Christensen et al., 2011), our study was the first to additionally pay attention to classroom readiness with its three dimensions. Student teachers face multi-faceted challenges during the practicum, and looking at multiple different dimensions of their readiness to teach therefore seems warranted.

Lastly, the SBL intervention in this study consisted of a single session; a format that differs from many of the previous studies on online interventions within teacher education contexts (e.g., De Coninck et al., 2019), which often rely on well-designed but relatively time-consuming series of sessions, sometimes with additional face-to-face activities. Of course, we did not compare the effects of our brief intervention with effects of other longer interventions or more intense interventions, and therefore we cannot make any claims on their relative effectiveness. Nonetheless, our study adds to current research and theorizing on online interventions by drawing attention to the potential usefulness of a brief online SBL intervention with a solid theoretical foundation rooted in research on feedback and reflection.

5.2. Limitations and future directions for research
Our study's findings should be interpreted while keeping several limitations relating to our sample, measures, design, and materials in mind. First, our sample consisted of Australian student teachers from Australian teacher education programs. It remains unclear whether the same findings would be obtained in other countries, cultural contexts, and samples of student teachers at different stages of their teacher education program. We therefore call for more research and cross-cultural comparisons of the functioning of the SBL interventions.

Second, we relied on self-reports to measure our outcomes. This makes sense, as we were interested in student teachers’ self-beliefs (self-efficacy, classroom readiness). On the other hand, self-reports have numerous limitations (e.g., social desirability, response sets). Future studies would thus do well to include other sources of data (e.g., external observers, such as mentor teachers, e.g., Bieri Buschor & Schuler Braunschweig, 2018, or students, e.g., Bardach, Oczlon, Pietschnig, & Lüftenegger, 2020) to triangulate self-reports with these other measures. A further limitation related to the measures adopted in our study is that we focused on a specific set of outcomes and consequently, further important constructs were not assessed. Relatedly, potential explanations for our findings that we offered in the Discussion have to be considered as purely speculative as we did not assess the focal constructs assumed to play a role for the effects or the lack of effects (e.g., differing perceptions of autonomy in the online environment etc.). Further research should overcome this limitation by directly assessing these constructs and putting our assumptions to the empirical test.

In addition, due to our restriction on assessing only self-efficacy and classroom, a comprehensive understanding of the effects of online SBL on a broader range of outcomes could not be achieved. We encourage future studies on online SBL to extend our findings by considering further relevant outcomes and correlates (e.g., critical thinking, transactional distance, self-regulated learning, metacognitive awareness, see e.g., Karaoglan Yilmaz, 2020; Wong et al., 2019; Yı lmaz, 2020). Moreover, even though we controlled for key socio-demographic features, there are other variables that could be relevant but were not assessed and could thus not be considered, such as students’ level of computer proficiency and the frequency of computer use. However, at all universities involved in the current research project, students have to do their course work via a university moodle site, requiring them to upload assignments, read online materials, post videos online, access resources via online platforms, engage in online discussion, and complete research tasks using online resources. Accordingly, non-contact time is spent working on a computer in most cases and very little work is done by hand. It seems likely that most students had a relatively high level of computer proficiency and regularly used computers; nonetheless, it remains a limitation of the current study that these aspects were not measured.

Third, our control group did not receive the feedback and the reflection exercise, but still worked on the complex classroom scenarios. Hence, a completely “neutral” control condition without any involvement in online SBL components could further refine the insights gained in our study. Furthermore, even though the existing literature suggests that reflection and feedback together should yield the most adaptive outcomes and that feedback stimulates reflection (e.g., Hammerness et al., 2005), future research could complement our findings by testing a reflection-only intervention group in addition to the groups introduced in our work. In addition, student teachers were not included as important stakeholders in the process (see e.g., Karaoglan Yilmaz & Yilmaz, 2020a) and stronger effects may have emerged if student teachers would have had the opportunity to participate in decisions concerning the design and specific content of the online environment, including the ways feedback and reflection were implemented.

Fourth, the scenarios used in our study relied on a text-based format. Hence, replications of our work using video-based formats of these scenarios, which have been found to be more engaging than text-based ones (Bardach, Rushby, Kim, & Klassen, 2020), and which might therefore offer an even better foundation for the online SBL activity lies ahead. Similarly, feedback in our study was solely provided in a text-based format. However, prior empirical studies suggest that the format in which feedback (e.g., text, video, audio) is delivered can make a difference (e.g., Crook et al., 2012). For instance, in a recent study comparing text-, video-, and audio-based feedback, the group that was given video-based feedback in online discussions had the lowest transactional distance perceptions (Karaoglan Yilmaz & Yilmaz, 2019). We expect that future studies will address the limitations of our study, enabling a strong research base to be built by testing such different feedback formats in the context of online SBL interventions.