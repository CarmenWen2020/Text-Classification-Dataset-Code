Social robots are a promising new technology for primary education. However, they also introduce practical and moral challenges and there is an increasing demand for guidelines for a responsible, safe introduction of social robots. In this study, we identified and compared the moral considerations associated with the introduction of social robots in primary education from the viewpoint of direct and indirect stakeholders by conducting focus group sessions (N = 118). In total, we identified and compared stakeholder considerations related to 17 moral values. Overall, each of the stakeholder groups considered social robots a potentially valuable tool for education. Many similarities and only few conflicting views across the various stakeholder groups were found. Particularly among the teachers, parents, and policymakers, there were many similarities on the issues reported and their considerations were often aligned. These insights into the moral considerations of the various stakeholders involved, provide a solid base to develop guidelines for implementing social robots in education as requested by scholars and society.

Previous
Keywords
Elementary education

Cultural and social implications

Human-computer interface

Improving classroom teaching

Media in education

1. Introduction
Social robots are a new type of robots that are increasingly studied in the educational domain. Compared to other types of robots currently used in education, social robots are less used for learning programming skills or how to build a robot, but rather to serve as a tutor or peer that helps children during their learning process. For children, social robots showed to improve cognitive (e.g., knowledge, comprehension, application, analysis, synthesis and evaluation) as well as affective outcomes (e.g., the learner being attentive, receptive, responsive, reflective, or inquisitive) (Belpaeme et al., 2018). Other benefits for children include greater enjoyment in learning (Alemi et al., 2017; Jones et al., 2014), more personalized learning (Johnson & Lester, 2016) and beyond the classroom learning (Kory Westlund et al., 2016). Social robots also introduced potential benefits for teachers, such as improved job satisfaction (Shih et al., 2007) and reduced administrative workload (J. Han, 2010; J.-H. Han et al., 2009; Lee & Lee, 2008). However, although social robots hold potential for education, they also introduce new moral challenges.

Especially in European studies, the urgent need for moral considerations and guidelines for social robots in education is voiced (Belpaeme et al., 2018; Pandey & Gelin, 2017; Serholt et al., 2017; Sharkey, 2016; Tolksdorf et al., 2020). The call for more explicit attention for the impact of technology is also reported by a recent systematic literature review on ethics in Child-Computer Interaction research, which concluded that design ethics (which focuses on the impact of technology) is underdeveloped in this field and should be addressed more explicitly (Mechelen et al., 2020). Without the proper guidelines for building and implementing social robots, the moral values upheld in education are at risk of being undermined.

Social robots are studied to assist teaching tasks for different subjects, such as teaching first and second language (Alemi et al., 2015; Chang et al., 2010; Eimler et al., 2010; Gordon et al., 2015; Kwok, 2015; Shih et al., 2007; Wang et al., 2013), teaching the times tables (Konijn & Hoorn, 2020) and sign language (Kose & Yorganci, 2011; Uluer et al., 2015). Broadly speaking, social robots have three key elements, being: physical embodiment, the robot following social norms, and having some form of autonomy (Bartneck & Forlizzi, 2004). The benefits of social robots can be traced back to these elements and set social robots apart from other types of learning technologies, subsequently creating new moral issues.

The physical embodiment of the robot makes them directly present in the same physical space as the pupil. This physical and visual presence is linked to enhanced motivation levels of learners (Belpaeme et al., 2015; Kim & Baylor, 2016). Compared to having no physical embodiment, such as in the virtual presence on a tablet, robots with physical embodiment have shown to result in greater enjoyment. Furthermore, children perceive embodied robots as friendlier, and they are also reported to accept the robot as an authority (Fridin & Belokopytov, 2014). The physical embodiment also creates opportunities for learning where physical presence is preferred, such as with handwriting (Hood et al., 2015; Salvini et al., 2016), sign language (Kose & Yorganci, 2011; Uluer et al., 2015), and dance (Ros et al., 2014; Ros & Demiris, 2013). However, the robot sharing the same physical space as a pupil also poses safety related issues. In contrast to, for example a virtual avatar, a robot can physically harm a child or physically damage the educational environment (Sitte & Winzer, 2004). Teachers specifically raised concerns about pupil's safety when they interact with a physical robot (Serholt et al., 2017).

By following social norms the robot can take on roles such as that of a tutor, a peer, or that of a naïve learner (Hood et al., 2015). These new roles can boost a child's self-confidence on a topic (Ghosh & Tanaka, 2011), relieve loneliness (Liu, 2010) and enhance learning performance (Zaga et al., 2015). However, studies show that the stakeholder perceptions on what role a robot should have are inconsistent. Children for example report on the robot being a potential servant or a friend (Lin et al., 2009), a private tutor, or even a possible rival (Shin & Kim, 2007). Similarly, parents are reported to consider robots as a potential friend for their children, while others would accept educational robots primarily as mechanical tools (Choi et al., 2008; Richards & Calvert, 2017). The robot's ability to following social norms and to take on new roles has raised questions related to the effect on children's trust (Johnson & Lester, 2016; Sharkey, 2016) and the meaning of friendship (Richards & Calvert, 2017). Children might become socially isolated as a result of a child bonding with a robot and preferring a robot over their human peers (Kennedy et al., 2016, p. 6), which could have a dehumanizing effect on children (Serholt et al., 2017).

The ability of a robot to react (autonomously) to its environment based on sensors such as cameras and microphones, allows the robot to tailor its interaction to children's individual needs, which is a cutting-edge form of personalized learning (Miliband, 2004). Even with a limited form of personalization by a robot, the performance of pupils can be increased, outperforming children who didn't get personalized support (Leyzberg et al., 2014; Kory Westlund & Breazeal, 2015). However, the robots' ability to record the behavior of children and move in the same physical space raise privacy issues (Leite et al., 2013; Sharkey, 2016). European children have been recorded to regard the storage of personal information unacceptable (Serholt & Barendregt, 2014). Although European teachers also considered privacy an issue, they mention that the privacy of children in schools is already compromised by the storage of sensitive personal information of children in educational technologies. However, these teachers considered the storage of detailed data of affective signals more intrusive compared to current technologies (Serholt et al., 2017).

As social robots and their associated benefits are being introduced in education, it is important to take into account the effect these robots might have on different stakeholder groups and what they perceive as valuable because their perceptions may differ and conflict (Ligtvoet et al., 2015). However, the existing scientific literature has focused mainly on the potential effects of social robots on children and teachers, overlooking many other stakeholders involved in implementing robots in education, such as parents, policymakers and the robot industry (Smakman & Konijn, 2020). Thus, not only do social robots pose a risk to undermine the moral values related to teachers and children, but they may also impact the other stakeholders.

This study firstly aimed at identifying and comparing the moral considerations associated with the introduction of social robots in primary education from the viewpoint of the missing perspectives. Secondly, we aimed at validating and expanding the knowledge on the already reported values of teachers and children from the extant literature. To this end, we conducted focus group sessions with children, teachers, parents, the robotic industry, and governmental policymakers/advisors.

In the following, we elaborate on our methodological approach to identify the moral considerations, following the Value Sensitive Design (VSD) methodology, which is used to account for values when designing and implementing technology (Flanagan et al., 2008; Friedman, 1997; Friedman et al., 2013a; Friedman & Kahn, 2003, pp. 1177–1201). Then, we detail the participants of our focus group sessions and describe and compare their moral considerations, thereby providing the stepping stones for the moral guidelines needed for the responsible use of social robots in education.

2. Material and methods
2.1. Values sensitive design
Our methodological approach to identify, examine, and compare the moral considerations of different stakeholders related to social robots in education is based on the Value Sensitive Design (VSD) methodology. This is a scientifically accepted methodology for the integration of (moral) values of stakeholders in the design and implementation of new technologies (Friedman et al., 2013b). In this study, value is defined as “what a person or group of people consider important in life” (Friedman et al., 2013b, p. 86), which is a common definition in the ethics of technology.

VSD has already been applied in the context of new technologies and children, such as for parental software (Nouwen et al., 2015) and online apps (Badillo-Urquiola et al., 2020), but also in the field of social robots for healthcare (van Wynsberghe, 2013). VSD provides a clear approach to systematically account for human values in the design of technology. This approach can be split into four phases: 1) value discovery, 2) value conceptualization, 3) empirical value investigation, and 4) technical values investigation (Spiekermann, 2015). The results of the four phases are requirements for the responsible design and implementation of innovations in social contexts, such as social robots in education, which can be used for moral guidelines.

The first phase starts with identifying the stakeholders related to technological innovation in a certain context (such as social robots in education), secondly, the harms (e.g., downsides, concerns, negative effects) and benefits (e.g., opportunities, positive effects) related to the technology from a stakeholder perspective are identified and later linked to moral values. In the second phase, value conceptualization, the values are broken down into norms and conflicting norms are analyzed. The empirical value investigation and technical value investigation phase, then, prioritise the norms and values and create (design) requirements which can be used for taking into account the moral values of the different stakeholders when designing and implementing technology.

Our study completed the value discovery phase, by identifying the harms and benefits related to the technology from a stakeholder perspective, and then link them to moral values related to social robots in education identified in earlier literature (Smakman & Konijn, 2020). These moral values are either potentially undermined or positively related to the introduction of social robots.

2.2. Participants
Research in ‘ethics by design’ places emphasis on the importance of including different stakeholder groups early in the design and implementation phases of a new, novel technology such as social robots in education (Friedman et al., 2017). There are multiple methods for choosing which stakeholder groups to include (Winkler & Spiekermann, 2021). However, a commonly used and accepted strategy is to focus on the potential impact of a technology, rather than on the experience stakeholders have with a technology (Friedman et al., 2017; Miller et al., 2007). Based on the potential impact of social robots in education, we therefore selected teachers, parents, representatives of the robot industry, governmental policymakers, and children.

For qualitative research in ethics and technology, such as this focus group study, participants can be selected based on their role (Miller et al., 2007), for example, being a teacher or a parent. Therefore, via purposeful sampling, participants were selected based on their role. The criterion for participants to be included in our study was: being a primary school teacher, being a parent with one or more children in primary school, being an employee of a company building or selling (social) robots, being a governmental policymaker, or being a child in primary school. Participants were recruited through newsletters of robotic companies, messages on social media, snowballing (Ghaljaie, Naderifar, & Goli, 2017) primary schools and direct e-mails. In total, 118 participants in the Netherlands agreed to participate in our study (see Table 1). The study was approved by the Institutional Ethical Review Board. All adult participants provided active, verbal consent; the parents of children participating in the focus group sessions provided informed consent for their children. The focus group sessions with children were conducted in common primary school classrooms with the children's teacher present, during a workshop on robots.


Table 1. Overview of the focus group sessions.

Stakeholder group	Teachers	Parents	Robot industry	Governmental policymakers	Children
Focus group session (N)	3	2	3	3	3
Participants (N)	18	11	13	20	56
Male/female	8/10	5/6	6/7	11/9	31/24a
Age range	26–59	33–49	22–75	19–62	9-12a
M-age	40.1	41.45	37	41	10.6a
SD	11.65	4.27	17.49	12.09	1.03
a
One child did not record her/his gender and age.

The robot experience of participants differed from no prior experience with social robots to having multiple years of experience in applying robots in education for some. To familiarize participants with social robots in education, we used a video with a general explanation of different types of robots, their current capabilities and footage of children interacting with a NAO in autonomous mode in a classroom setting. Using video footage of social robots to familiarize participants with the phenomenon is commonly used in child-robot interaction studies (Ahmad et al., 2016; Rosanda; Istenič Starčič, 2019; Serholt et al., 2017), and even has benefits over using real robots as discussed by Belpaeme (2020). In addition, we provided a presentation and a live interaction between the participants and the robot in physical space. We adapted the live robot interaction with children to the classroom setting where the interaction took place, which resulted to slightly differ from the live robot interaction with adults. The steps to familiarize participants with the abilities of social robots are presented in more detail in the next sections.

2.3. Materials
In the current study, we examined the moral conceptions of stakeholders by designing 2-h long focus group sessions. In total fifteen focus group sessions were held. Focus group sessions are group discussions aimed at exploring a specific set of issues, such as people's views on social robots in education (Kitzinger, 1994). These sessions consist of a small group of people (usually between four to six people each (Breen, 2006) with similar demographic variables, in our case being a member of the same stakeholder group. We considered focus group sessions an appropriate method for the aim of this study because focus group sessions do not just look at the perceptions of stakeholders, but also look at why or how these perceptions are formed and may address controversial points of view. Thereby, focus groups can give a deeper insight into the reasons behind participants' attitudes, as opposed to what their attitudes are (Kitzinger, 1994).

Following the VSD-methodology (Friedman et al., 2008; Spiekermann, 2016; Van Den Hoven, 2014), our focus group sessions aimed at identifying the harms (e.g., downsides, concerns, negative effects) and benefits (e.g., opportunities, positive effects) of social robots for the stakeholders to gain insight into their considerations. To analyze the data collected during the focus group sessions (see below), all harms and benefits expressed by the participants were categorized under values that were previously identified (Smakman & Konijn, 2020), summarized in Table 2. We added an extra category, named miscellaneous, for the considerations that could not be (fully) categorized under a specific value.


Table 2. Values related to robot tutors from the perspective of children and teachers (Smakman & Konijn, 2020).

Values related to robot tutors	Description
1) Psychological welfare	This value concerns the robots' capability to influence psychological or social aspects (e.g., a robot acting as a person of trust, or one that comforts a child).
2) Happiness	This value concerns the robots' capability to provide pleasure/fun.
3) Efficiency	This value concerns the usefulness and versatility of the robot.
4) Freedom from bias	This value concerns the potential bias of the robot, such as gender or racial biases.
5) Usability	This value concerns to what extent the is accessible and useable for all users.
6) Deception	This value concerns the robots' ability to make children believe something that is not true, such as pretending that the robot cares about a child or keeping information from children
7) Trust	This value concerns the issue of children's trust in robots and whether this can be violated.
8) Friendship	This value concerns the friendship bond that can develop between a child and a robot, and whether this is acceptable.
9) Attachment	This value concerns the possibility that children will get attached to the robot, and whether this is desirable.
10) Human contact	This value concerns the robots' effect on human contact of children with friends, teachers and other humans.
11) Privacy	This value concerns the effect of the robots' ability to collect personal data on children, and if this data may be shared with others.
12) Security	This value concerns the IT security of the data that the robot collects via sensors.
13) Safety	This value concerns the physical safety of children when interacting with robots
14) Accountability	This value concerns the robots‘ effect on who is accountable for the actions of robots and their effects. Someone accountable is obliged to accept the consequences of something.
2.4. The procedure of focus group sessions with adult stakeholders
The focus group sessions with the adult stakeholders opened with a short presentation on the purpose and urgency of the study, followed by a short (5 min) neutral video about the use and capabilities of social robots in primary education (available online at https://osf.io/xc5vt/). The video included a general explanation of different types of robots, their current capabilities and footage of children interacting with a NAO robot in autonomous mode in a classroom setting. By using this video, we intended to provide all participants with basic knowledge of social robots. The video was created to show a neutral view on social robots, not to influence the participants, but merely to stimulate them for the discussion.

The video was followed by a live demonstration of a NAO robot to get the participants more familiar with the topic and engaged for discussion. The physically present NAO robot performed a short calculation exercise and had a short question and answer session with the participants. During the demonstration, the NAO robot was partly teleoperated (Wizard of Oz style) by an assistant-facilitator. During the teleoperated parts of the interaction, the assistant-facilitator selected which applications or scripts the robot should run. It also enabled us to customize the introduction of the robot based on the participants. However, the calculation exercise was completely autonomous to allow participants to interact as if the robot was in a real-life classroom situation. The participants were made aware of which parts were teleoperated and which parts were autonomous interaction. By both showing footage of the NAO robot in real-life educational settings, and letting participants interact with the robots (both in autonomous mode and teleoperated), we created a narrative of the actual performance of social robots in conditions likely to be encountered in real-life classrooms.

After the demonstration, the facilitator first asked all participants to take their stakeholder perspective in mind and write down all the opportunities related to educational robots they could think of on different Post-its. These opportunities were then discussed and further elaborated on by the participants. After the opportunities were discussed, the facilitator asked the participants to do the same exercise but now for the specific stakeholder concerns.

When both exercises were finished, the discussion moved on to the final part of the session. This part consisted of a free debate wherein the participants had the opportunity to add anything that was not discussed and possibly introduce a new opportunity, issue or concern. This routine was applied for all the conducted focus group sessions with adults.

2.5. The procedure of focus group sessions with children
The focus group sessions with children were conducted in common classroom settings, with the children's teacher present. The children first got a short 10-min presentation on robots in education, followed by a demonstration of a NAO-robot. The NAO-robot introduced itself, danced, and practised arithmetic with the children. Children were also encouraged to ask questions to the NAO-robot. The robot's introduction, dance and arithmetic exercises were all fully autonomous. However, given the current state of the technology, such as the limitations of the automatic speech recognition for children's speech (Kennedy et al., 2017), the free format questions and answering part of the interaction was teleoperated.

After the general introduction, children were divided into small groups of 4–5. The groups were shown several pictures of social robots for education and were asked to pick one. Thereafter, following a procedure used by other researchers to elicit children's attitudes towards social robots (Serholt & Barendregt, 2014), we asked the children to write on a poster (see Fig. 1).

Fig. 1
Download : Download high-res image (103KB)
Download : Download full-size image
Fig. 1. Poster used in sessions with children.

The poster consisted of five questions that helped to elicit what children see as opportunities and concerns for social robots in primary education.

2.6. Analysis
After the final focus group session, all audio recordings were transcribed and combined with the notes, Post-its, and posters taken from the sessions. All transcriptions were then analyzed using an inductive and deductive coding process. To identify patterns within and across the data, we used a thematic analysis method (Braun & Clarke, 2006). Following the phases of thematic analysis (Braun & Clarke, 2006), we first familiarized ourselves with our data by reading all the transcripts. Second, we created initial codes based on all the opportunities and concerns expressed by the participants. Third, we randomly read samples of the data and created thematic codes. Fourth, we checked if the themes/issues identified worked in relation to the coded extracts of the second step. Fifth, we applied the codes onto new sample texts derived from our focus group transcriptions. Using this iterative process, we then created our final coding thematic scheme which we applied to all data collected, shown in Table 3 as ‘issues’. Finally, we categorized the identified themes with the previously identified values (cf. Table 2). For the themes/issues that could not be (fully) categorized under a specific value, we added an extra category named “miscellaneous”, which was later subdivided in newly identified moral values. Two additional, independent research assistants reviewed this process to reduce the chance of any possible bias and a coding scheme was constructed for further analysis.


Table 3. Summary of the considerations per stakeholder group.

Values	Issues: Social robots in education	Teachers	Parents	Robotic industry	Policymakers	Children
Psychological welfare and Happiness	Are fun and motivational for children	U	A	A	A	A
Can make children feel safe/sense of peace	A	A	N	A	N
Could have negative long-term social implications	U	A	N	N	N
Increased workload for teachers	A	N	N	N	N
Applicability	Are useful for simple teaching tasks, supervising, taking exams and motivating children.	A	A	A	A	A
Can build e-portfolios/collect data of children, monitor progress/development	A	U	N	A	N
Can be used outside the classroom	U	A	N	N	N
Freedom from bias	Can be (un)intended bias	N	N	A	A	A
Usability	Can cause unequal learning opportunities due to unequal access	A	A	N	A	N
Require new (IT) knowledge from teachers and can be met with the reticence of teachers	A	N	A	A	N
Friendship and Attachment	Can harm the social skills of children	A	A	N	A	N
Can be beneficial for special needs children	A	N	N	N	N
Can cause children to become too attached due to the forming of social bonds	A	A	N	A	N
Trust and Deception	May not pass on sensitive information told in confidence, this violated trust.	A	A	A	U	N
Can cause children to trust robots over humans.	A	N	N	A	N
Human Contact	Can increase the quantity and quality of time teachers spent on personal contact with children.	U	A	N	A	N
Can harm the social skills of children	A	A	A	A	N
Autonomy (new)	Harm the autonomy of the teacher	N	N	N	A	N
Flexibility (new)	Are difficult to transport	N	A	N	N	N
Privacy	Gathers data I want access to	A	A	A	A	N
May not share data with (all) others	U	A	N	A	N
Lack of clear privacy guidelines/laws	D	A	N	N	N
Security and Safety	Cause IT security issues	A	A	U	A	N
Can compromise children's physical safety	D	A	N	N	A
Responsibility (new) and Accountability	Cause ambiguities on who is responsible and accountable for the robot(s) (actions).	A	A	A	A	N
Note. (A = agree; D = disagree; U = unresolved, N = not mentioned).

3. Results
In the following, we present the values related to social robots using the considerations voiced by the key stakeholders during the focus groups sessions. The similarities and differences of the stakeholders' considerations are thereafter presented in the Discussion section.

3.1. Psychological welfare & happiness
The value of psychological welfare refers to emotional states such as mental health, comfort and sense of peace (Friedman & Kahn, 2003, pp. 1177–1201). We clustered this value with the value “happiness” (enjoyment) because they are closely related, and the considerations of the participants often overlapped. In this section we will both report views related to the robots’ impact on children, but also regarding their impact on other stakeholders, such as parents and teachers.

3.1.1. Impact on children
All stakeholder groups mentioned benefits for social robots that could increase the level of psychological welfare and happiness of children in and outside schools. The robot was considered to be fun and motivational for children by all stakeholder groups. As expressed by one of the teachers: “If I would put [the robot] in my class …, well [the children] would really enjoy that. Really!“. And: “robots are fun and motivational” as stated by a policymaker. The excitement of children for the robot could spark new interests, such as for programming, according to the teachers. The children reported that the robot should be able to support, coach and motivate them to reach higher grades. Also, the robot having infinite patience and not being judgemental was considered to make children feel more at ease.

At school, the policymakers and teachers considered the robot a potential aid for children for learning to deal with social issues and stressful situations. Teachers mostly agreed that a robot without negative emotions and with patience could make children feel safe. However, teachers preferred that social skills should be taught by humans, not robots. Children reported being concerned about robots threatening people, which according to the children, they should not do.

Although all stakeholder groups voiced potential benefits related to psychological welfare and happiness, there were also potential harms mentioned related to these values. Some parents mentioned being worried about the potential negative long-term social implications. This concern seems to be related to the uncertainty of the potential impact on children, as one parent stated: “I don't believe this should even be tested on children because we don't know what effect this will have on children, and that worries me greatly”.

Teachers seemed more concerned with the technical ability of a robot to provide children with a good educational environment. They were worried a robot would not be able to adequately interpret answers given by children. Consequently, the robot would not be able to provide children with satisfactory feedback. Parents considered it important that the data recorded and used to provide advice to children, should still be interpreted by a human. Not only to accurately interpret the data, but parents also feared that teachers could become less involved/informed with their pupil's mental well-being, as expressed by a parent: “the teacher would be less informed about a child's wellbeing and emotional level, because the robot only focusses on cognitive gains, and there's so much more to humans, skill-wise”.

The robot could also result in children being demotivated, according to some teachers. The novelty effect was a concern for some participants, teachers feared that the robot would not be able to keep children motivated and enjoyed over extended periods. Some teachers mentioned that the robot should not replace them. However, the potential of a social robot replacing a teacher was quickly dismissed among the teachers after they concluded that the robot technology was nowhere near satisfactory enough.

3.1.2. Impact on parents and teachers
Parents stated a few ways robots could provide them, as a parent, a sense of peace. This sense of peace was related to the opportunity for the robot to help with homework exercises (also see section 3.2.3 Beyond the classroom learning). Especially for parents who are not able to help their children with their homework, the knowledge that their child would get proper support from the robot would give these parents a sense of peace.

Parents indicated that a robot would save time spent by parents making sure homework was done properly, allowing them to spend their spare time differently with their children Also, parents indicated that the robot having infinite patience and not being judgemental to their children would make them feel more at ease. However, not all capabilities of the robot had a positive impact on the parents. The robot's ability to collect personal data of children worried some parents and would give them a feeling of unease. In section 3.8 on Privacy, the data collection by the robot will be further elaborated. Following the data collection, teachers voiced concerns on the potentially increased workload related to the analysis of the collected data. A teacher noted, “I can imagine that if you are going to revisit all the footage the robot records on one day … times 30 children … [] who is going to check all of that [the data collected], and when?“. This potential increase in workload, which could result in increased stress levels of teachers, and lower job satisfaction was a shared concern among teachers. Parents, however, considered the robot mainly beneficial for lowering the workload of teachers.

3.2. Applicability
We renamed the original category ‘efficiency’ to ‘applicability’, as the related harms and benefits seemed more related to the usefulness and to the quality of being relevant or appropriate than to being efficient.

3.2.1. Learning topics
Robots were considered to be potentially useful tools for teaching, supervising, taking exams, and motivating children. Policymakers specifically stated that robots could support children and teachers in achieving a higher level of digital literacy. However, teachers voiced concerns regarding the robot not being able to provide “deeper levels of education”. Deeper levels of education were referred to by the teachers as not only giving the answer to a question but to provide insight into how this answer was achieved. Children seemed to agree on this point, reporting that the robot should be able to “explain things and listen to them”. The majority of teachers argued that these deeper levels are one of the most important subjects in education. Teachers did consider the robot a great tool for learning programming.

3.2.2. Applicability of the data collection and usage
Teachers and parents discussed the possibility of a robot that would help to build e-portfolios of children by recording and analysing data, such as audio and video fragments of children and their test results. Using these data, the robot could adjust its teaching to children's specific needs, as mentioned by a policymaker: “Robots can help pupils who need extra support with exercises, but robots can also help pupils who need more challenge individually”. The collected data were also considered to be valuable by the robotic companies and policymakers. Robotic companies mainly considered the data valuable for product improvement. However, they also voiced the opportunity to invent new products and services based on the collected data. Policymakers reported that the data could be used to monitor the performance of schools. Based on this data the government could adjust its policies, which was considered a potential benefit by the policymakers.

Parents argued that the collected data could generate a more accurate and sophisticated profile of their child's educational development. Some parents even going as far as stating that this would be a better method compared to the currently used heavily weighing exams at the end of primary school. The robot was also considered to potentially be able to provide advice regarding a child's future education, an idea which some parents were open to. However, parents considered it important that the data should still be interpreted by a human. Furthermore, the majority of parents saw opportunities for a robot that gives (non-binding) advice about children's educational progress and level. Parents agreed that a robot should not make these decisions autonomously.

The data could also improve parent-teacher conversations. According to parents, the data (e.g., audio and video) collected by the robot, could be used in these conversations to provide insight into what happens at school.

However, the participants also expressed other concerns. Some parents voiced concerns about the current state of technology being inadequate for a proper two-way conversation with children, as the current state-of-the-art robots still seemed heavily reliant on human input. This reliance on human input was also considered to be a potential source for technical errors and the robots providing children with incorrect information. Because of potential misinformation, some parents and policymakers argued they would not blindly accept the robot's judgement.

3.2.3. Beyond the classroom learning
The use of the robot outside the classroom, such as at home, was also discussed in the focus group sessions. Most parents and some teachers agreed that the robot could be a possible tool for education at home. Parents reasoned that the robot could make sure the children were motivated to do their homework and that it was done properly. This was considered to improve the quality of the learning process according to parents. However, teachers also voiced strong reservations about letting children take the robot home, such as: “It's too expensive. We don't let them take laptops home either”. This opinion was shared among most teachers. They also argued that, for a robot to be useful at home, the robot should be “plug-and-play”, which they did not consider the current robots to be. Although most parents considered the robot a potential aid for learning at home, they also expressed that home should be a place where the learning process stops and where children can relax and do something other than learning.

3.3. Freedom from bias
Freedom from bias is defined in this study as the robot's ability to treat every user equally, independently of his or her characteristics. Multiple policymakers voiced benefits related to a neutrally programmed robot. “If you are capable to design a robot without prejudice, assumptions and bias, the robot will add value to education” as stated by a policymaker. This opinion was broadly shared among the other policymakers and also mentioned by the representatives of the robot industry.

Although an unbiased robot could add value to education, some policymakers were concerned about the robot's ability to correctly recognise children. This could lead to the robot being unintentionally biased due to the robot responding better to specific characteristics of children over others. This could lead to some children gaining an advantage over other children based on their characteristics. Finally, the policymakers argued that the person who programmes the robot (e.g., the teacher or the robot industry) could be (un)intentionally biased when programming a robot, resulting in a biased robot that would prefer some children over others. Some teachers did only slightly touch upon this value saying that if children would know that they would not be judged by a robot, this could be perceived by children as an opportunity to say anything a child wants.

Parents did not voice opportunities or concerns related to this value. However, some children reported that the robot should “treat everyone equally” and that the robot should not “have a favourite child”, which indicated that the robot potentially being biased is also a concern among children.

3.4. Usability
In this study, we defined the value usability as the value that makes it possible for all relevant stakeholders to become successful users of the new technology by ensuring equal access regardless of user knowledge, user diversity, or technological variety (Friedman & Kahn, 2003, pp. 1177–1201).

Although robots could provide extra attention to children, the parents, teachers, representatives of the robot industry, and policymakers, all voiced concerns related to usability. The concerns related to 1) the (un)equal distribution of robots due to the high costs of robots, and 2) the IT-knowledge and skills of the teachers.

Parents were concerned that children of low-income families could potentially be excluded from using robots due to the high cost. Parents and policymakers both discussed the option that the school would buy the robots, not the parents. However, this also led to a potential unequal distribution. If schools were responsible for investing in robots, this could lead to “a form of social segregation where there's a big difference in who gets access to the robot and who doesn't, and I believe in equal rights for a good education regardless of what school my children go to” as voiced by a parent. According to policymakers, this could be harmful to equal learning opportunities for children. Teachers all agreed that every child should have the same rights and opportunities to work or learn with social robots, regardless of school or grades.

The second concern regarding usability was the knowledge and willingness of teachers to work with robots. Most teachers considered the robot not plug-and-play, which might make it difficult for some teachers to work with a robot. As one teacher stated: “I think in practice you will always have people within your team who just can't work with it or who will not work with it”. Workshops for working with social robots, to increase the overall experience and knowledge among teachers, were considered useful and necessary by both teachers and policymakers. Using robots could be extra challenging for (older) teachers with a low level of digital literacy, according to a few of the policymakers.

Both concerns related to the cost of the robot, and the knowledge and willingness of teachers could lead to unequal opportunities for children. “There will be a difference between schools who use the knowledge, expertise and resources to implement robots in the right manner and schools who are not able to do this,” as summarized by a policymaker. According to the robotic industry participants, teachers are still reluctant, or even “very hesitant”, to buy tutor robots due to technical skills/issues.

3.5. Friendship and attachment
Friendship and attachment are values that relate to children forming friendship relations with robots, and to children becoming (emotionally) attached to the robot. All stakeholders reported harms and/or benefits related to these values, except the children. Most teachers considered a potential relationship with a robot to be similar to the relationship children have with dolls or hand puppets. One teacher said: “we give those [hand puppets] magical powers too [to entertain the children]. Toddlers believe that, and I think you can do that with a robot as well. While playing, they're discovering things.” Some teachers also considered the friendship relation potentially beneficial for special needs children (e.g., in the Autistic Spectrum Disorder). However, the teachers were concerned about older children forming bonds with a robot: If your 10-, 11-, 12-year-old are saying ‘the robot is my friend’, then I would find that somewhat worrying”. Teachers were concerned that friendships with a robot could have a negative impact on the social skills of these older, ‘regular’ children. This concern was brought up multiple times, and was also reported in the focus group sessions with parents. Policymakers also reported these concerns, however, they seemed more focused on the vulnerability of young children, the implications on the development of social skills and the potential negative impact on the function of teachers for being a role model for their pupils.

Teachers, parents and policymakers all reported concerns that the social bond between children and the robot could lead to children becoming too attached to the robot. There were, however, subtle differences in their considerations. Parents and teachers draw parallels to addictive video games, tablets and smartphones, causing children to prefer gaming over talking to their siblings, parents and/or other children. A few participants of both stakeholder groups also reported that the robot could potentially cause children to take on robotic behaviour. This will be discussed in more detail in the section “Human Contact” below. Some of the policymakers reported to be concerned about what would happen if children get attached to a robot and the robot needs to be replaced, or when the robots are suddenly breakdown.

3.6. Trust and deception
The values trust and deception are closely related, and they are therefore combined in this section. Trust relates to a child trusting a robot, and the potential impact on children when this trust is violated. Deception is associated with the robot's ability to let a child believe something that is not true and the robot being honest to children.

Policymakers discussed that children could find it easier to approach a robot with their problems than approach a human teacher. Children could share secrets or problems which they would normally not share with the human teacher. However, teachers in one session reported that if a child would only trust a robot, this might be an indicator that something is “wrong” with the child. When this would happen, the teachers agreed that the child involved should be monitored to see whether there are underlying issues. Some parents also expressed to be concerned about children disclosing sensitive information only to a robot without others finding out.

Policymakers argued that by letting children trust robots (e.g., by letting children share their secrets with robots), the educational system also instils trust into robots to respond correctly. “We have actually instilled a kind of trust in robots. Maybe my children also have the confidence that they are actually addressing their problems to the robot, but it is possible that the robot could not react adequately” as voiced by a policymaker. This view was shared among the policymakers.

Another concern was related to the robots passing on information that a child told the robot in confidence, potentially leading children to feel deceived. The policymakers were divided on this subject. They considered that the information should be passed on when the robot is not able to respond appropriately (e.g., provide a solution to the child's problem). However, they also acknowledged that passing this information on could harm the social bond between the children and the robot. The participants of the robot industry also expressed concerns related to the passing on of sensitive information told by children to the robot. As nicely illustrated by one of the industry participants: “When children are alone with the robot and they do something bad and then, afterwards, the teacher points them out on this while the teacher was not there …. [] the full trust in the robot is gone”. Some teachers also reported being concerned about the negative effect on children's trust when information would be passed on to others. According to teachers, children should be made aware which data could be retrieved by others: as a teacher said as follows: “A child should know that it [data collected by a robot] can be revisited”. This was a shared view among all teachers. Teachers considered it appropriate to explain the data collection in more detail to older children, as opposed to young toddlers. More considerations on which information should or should not be shared, and with whom is reported in the “privacy” section below.

3.7. Human contact
The value of human contact relates to the impact of social robots on both the quality and the quantity of human contact. Parents and policymakers discussed potential benefits related to human contact. Due to robots taking over administrative tasks, human teachers could potentially save time for human contact with the children according to policymakers. However, the robot costing more time than it would save, was a general concern voiced by the teachers. This concern was shared by some of the participants of the robot industry.

Parents stated that the robot could also be used to promote human contact by encouraging children to work and play together. Also, the robot's ability to promote kindness was considered a potential benefit by some parents. Related to kindness are some statements made by the children participating in this study. Some children stated it would be wrong if a robot would “swear or bully” and deemed it important that the robot should not “threaten other people”. According to other children, a teaching robot should “be nice”.

Policymakers, the robot industry, teachers and parents voiced concerns related to human contact. All four considered the potential effect of a robot's lack of social skills problematic. According to most policymakers, this could harm children's human intuition to recognise and respond to (non-)verbal human communications. Some parents and teachers feared that children would prefer working with robots to working with their classmates. This could potentially lead to “regressed social and emotional development” according to some teachers and parents. A few parents even feared that their children would adopt “robotic behaviour” due to learning with a robot. According to the teachers, emotional development is something which can't be taught by a robot. This was brought up during multiple sessions with teachers. Human contact was favoured over contact with a robot by the teachers.

3.8. Privacy
The value of privacy refers to the ability to do things without anyone else knowing, infringing, or influencing them. All stakeholders, except children, voiced concerns and opportunities related to this issue.

Parents, teachers, the robotic industry and policymakers all mentioned wanting an insight into the data collected by the robot, however, for different reasons. The robotic industry considered the data valuable for product improvement. The policymakers reported that they would like to use the data to improve their policies, by monitoring the performance of schools. Teachers considered the data valuable for teaching purposes and enhanced personalized learning. And parents reported that they should have the right to access the data that is recorded of their children. Some parents also reported that they should have the right to tell the school to delete all the data collected by the robot. Most teachers reported to agree that parents should be granted the right to access their children's data and that this is already common practice. However, they also reported that, before the data is sent to the parents, it should first be checked by a teacher. A few teachers expressed that in some cases it is best to talk to the parents before showing them the collected data, to provide context with the material. As one teacher explained: “Parents have the right to check everything. But you don't have to do that straight away. You can also say, ‘come back later’. I think you should think this through as a school, not just say ‘all right, here is the data’.” Most of the policymakers considered sharing children's data with the robotic industry to be potentially problematic because the goals of a school are not always the same as that of a commercial company. They voiced concerns that the goal of making a profit would potentially conflict the goal of schools and the ministry of education. Most teachers considered sharing data with third parties such as the robotic industry and the government to be no problem as long as all the data are anonymized.

The ownership of the data was an open issue discussed by the policymakers. They discussed whether the school, parents, children, or the manufacturer of the robot should be the owner of the data. They did not reach a consensus on this issue. The lack of clear guidelines on how to deal with the data collection of robots was a concern among parents. They voiced concerns related to the use and distribution of the data, especially for audio and video containing personal information. Some parents stated that before accepting a social robot into the school of their children, it should be clear which parties are going to use the data and for what purposes. Teachers, however, reported that there are privacy laws in place to protect children's privacy. Some teachers even reported that they “can't do anything” without the permission of parents. A few parents furthermore mentioned fearing being forced to opt-in if their school started using the data collected by robots. Some parents expressed concerns like fear that they would be forced to give up their child's privacy for them to go to school.

Children should have the right to be a child without being continually observed. This was a shared opinion among policymakers. They stated that the data collected by robots could haunt children in their later lives. Most parents and policymakers voiced concerns related to this issue, especially given the sensitive nature of the data. Finally, some teachers also expressed concerns related to the video or audio materials in which multiple children would be visible. It was unclear how this data should be shared with parents, according to the teachers.

3.9. Security and safety
The impact of social robots on the values of security and safety was discussed by parents, teachers, robotic industry participants, policymakers, and children. Whereas security focuses on IT security, safety relates to the physical safety of children and their surroundings.

The policymakers voiced two concerns related to the security of a robot. First, robots entering schools bring new security questions, making a secure IT-environment more difficult to manage for schools. Second, they considered a robot hackable which could lead to the children being spied on and, if the robot was reprogrammed by the hacker, even negatively influence children.

A majority of the parents were hesitant to accept a robot due to its security issues. One of the key issues was the lack of guarantee that all the sensitive information stored would be stored safely. Some parents also voiced concerns related to the physical safety of the children by questioning its durability. Teachers, on the other hand, did not seem very concerned with the physical safety of the children. According to some teachers, there are toys at school that are more unsafe than a social robot. They did consider the physical safety of the robot to be a potential issue. While discussing if children should be left unsupervised with a robot, the teachers agreed that some children could be left unsupervised, while others could not. They drew parallels between a laptop and a robot, stating that the children who could be left alone with a laptop could also be left alone with a robot. Opposed to teachers, a majority of the policymakers did consider the physical safety of children an issue. They considered a robot a potential risk to the physical safety of children when a robot could gain physical control over children. There were also several arguments made by the children that relate to safety, ranging from the robot beating children to unwanted sexual contact. According to children, a robot is not allowed to: “suddenly hurt someone”, “kill” or “sexually assault” someone.

During the focus group sessions with the robot industry, the participants mentioned that there is a lot of pressure on robotic companies to innovate and to keep releasing new products onto the market. Participants reported being concerned that some products, therefore, would be introduced too soon, which could impact the effectiveness and potentially also the safety and security of their products.

3.10. Responsibility and accountability (new)
The values of responsibility and accountability are closely related, however, they are different. Someone responsible should take care of something or someone, while someone accountable is obligated to bear the consequences of something or someone. The value responsibility was already identified in earlier research (see Table 2), however, our results also relate to the value of accountability. Therefore, we included this newly identified value.

The policymakers, teachers, parents, and the robotic industry all reported concerns related to these values. The robotic industry reported that it is unclear if they can be held responsible or accountable for any negative consequences from the use of social robots in education. This led to some companies being hesitant to develop robots for the social domain, according to some participants. Most teachers considered the supplier of the robots accountable for the maintenance, purchase, software updates, and security of the robot.

However, inside the classroom they considered the teachers responsible for what happens. Some parents voiced concerns related to becoming too depended on robots for parenting tasks. They considered the responsibility for raising and supporting the development of children foremost the job of parents, not robots. Some policymakers also stated that it is unclear who should be held accountable when a robot does not function properly. The participants discussed this topic and considered the school, the manufacturer and other parties, but could not come to an agreement on who should be held accountable.

3.11. Autonomy (new)
The value of autonomy concerns the teacher's ability to choose his/her learning methods and actions and is one of three newly identified values related to social robots in education. Some policymakers were concerned that robots could limit the teachers' autonomy. “If you have an artificial intelligence-driven system, that bases its answers on the answer of 10.000 teachers, do you dare to overrule this decision?” The policymakers discussed that the robot could become too prescriptive to the learning process, making the teacher an assistant to the robots, as opposed to the robots being an assistant to the teacher. The other stakeholder did not voice considerations related to this value.

3.12. Flexibility (new)
The third and last newly identify value is Flexibility. Flexibility refers to the ability to move the robot around and to be transported. Some parents expressed the concern that the robot could be too big and bulky to be brought home. Especially parents who bring their children to schools by bicycle could be limited in bringing the robot to their home. The other stakeholders did not voice concerns related to this value.

4. Discussion and conclusions
To address the moral challenges related to social robots in education, this study reports on focus group sessions among direct and indirect stakeholders regarding their perceptions on the usage of these robots. The extant literature has mostly focused on the impact that social robots would have on children and teachers, however, the other stakeholders were not involved. Filling this gap is important given the sensitive nature of education, the vulnerability of children, and design ethics being underdeveloped in Child-Computer Interaction research. More insight into the moral considerations of the various stakeholders involved provides a solid base to develop guidelines for implementing social robots in education as requested by scholars and society.

Results of the focus group sessions showed more similarities across the views of stakeholder groups than differences. In Table 3, we present a summary of the moral considerations on which stakeholder groups agreed or disagreed. Table 3 shows which issues participants within stakeholder groups generally agreed upon (A; agree), issues they disagreed with (D; disagree), issues of which no general agreement could be reached among the participants withing a stakeholder group (U; unresolved) and issues that were not discussed in specific stakeholder groups (N; not mentioned).

Important results can be found in the newly identified values, and the issues on which all stakeholder agreed, versus those on which stakeholders disagreed related to the earlier identified, theoretical values. The newly identified values were not yet reported in a systematic literature review on moral considerations and social robots in education (Smakman & Konijn, 2020). The newly added values are autonomy, flexibility, and responsibility. Our study further shows that the stakeholders considered all 14 values, that were identified previously, relevant. Thus, each of these values are relevant from the perspectives of multiple stakeholders. When adding the newly identified values to the list of values already identified in earlier research, the total number of values that might be affected by the implementation of social robots in education, comes to 17 moral values.

Each of the various stakeholder groups considered social robots fun and motivational for children. Although teachers had some concerns on the current state of technology to keep children motived, they overall agreed on this (potential) benefit. This positive impact is also mentioned in earlier research, where parents reported positive attitudes towards motivational robots for education (Oros et al., 2014). Furthermore, each of the stakeholder groups considered the robot being applicable for (simple) teaching tasks, supervision, taking exams, and motivating children. Teachers mostly considered the robot a prime tool for simple teaching tasks and for teaching programming skills. Teachers in earlier research voiced similar opinions, considering the robot mainly as an additional teaching tool for teachers, and less suited for novel concepts (Serholt et al., 2017). These results indicated that social robots do hold potential for education. However, other considerations seem to stand in the way of these benefits.

The (social) bond between a robot and a child could enhance motivation and create an enjoyable learning experience. However, teachers, parents, and policymakers all voiced concerns related to children becoming too attached to the robot caused by the social bond between child and robot. Also, they worried that this could harm the development of children's social skills. The forming of social connections between robots and humans is central to the field of social robotics (Belpaeme et al., 2013). Therefore, these fears and considerations should be taken into account by robot builders and users. If a social bond is unacceptable for crucial stakeholders, then social robots for education might be designed differently. However, the level of experience with social robots could influences people's perceptions, as pointed out by other researchers (Serholt et al., 2014). It might, therefore, be wise for robot builders and (future) users of social robots to acquaint stakeholders with this new technology before their implementation.

The various stakeholders in our focus groups also voiced strong views on data collection through a robot, related to the security of the data collected by the robot, and to privacy issues. Participants in each of the adult stakeholder groups reported concerns related to the security of the sensitive data that could be collected by the robot. Similar concerns of teachers were also found in earlier research (Serholt et al., 2017). The collection of (personal) data is needed for robots to personalise their responses to children, which in turn is reported to improve the learning outcomes compared to non-personalized interaction (Gordon et al., 2016, pp. 3951–3957). Furthermore, data collection is needed for building e-portfolios, which again is considered a benefit reported by the teachers, parents, and policymakers in our study. Although schools are used to handle personal, sensitive information of children, their current infrastructure might not be adequately secure for social robots to safely store such (large amounts of audio-visual) data.

Whereas some stakeholders voiced concerns related to privacy, all stakeholders (except the children) reported that they wanted access to the data collected by the robot. Reasons varied from increasing personalized learning, improved governmental policies, to improving social robots. These different reasons could guide the amount of access each stakeholder should get. For instance, robotic companies and policymakers could be granted rights to fully anonymized data for improving products and policies, whereas parents could be given access to a dashboard with overall data on their children. Teachers could be granted full access to the data, on which all stakeholders agreed. However, one scenario was overlooked - relating to the effect on children's trust.

Participants in each of the adult stakeholder groups expressed concerns related to children's trust being harmed when a robot would pass on information to others which was told to the robot in confidence. To autonomously detect which information was told in secrecy, robots would need to have sophisticated speech recognition. Whereas social robots rely heavily on language interactions, this still does not work reliably with children (Kennedy et al., 2017). Therefore, it might be advisable to store only aggregated data related to educational tasks, and not all the recordings of child-robot-interaction. This could solve the issue until there is an adequate technical solution.

Some methodological limitations should be noted. The current study was solely executed in the Netherlands. Therefore, the results provide insights into a Dutch (and perhaps Western European) perspective on social robots in education. Although a substantial total number of 118 people participated in our focus group sessions, we acknowledge that due to the limited number of participants per stakeholder group, the results might not be representative for the whole population within a stakeholder group. Nevertheless, the results provide important insights into the considerations of a wide range of different stakeholders and much-needed directions and pointers for further research for implementing social robots in education while keeping in mind the moral values upheld in education. Some of the categories identified in our study may not directly appear “moral” issues. However, using Friedman et al. (2008) definition of values, in our study, moral values refer to what stakeholders consider important and valuable with regard to the impact of a technology, which is a common definition in ethics of technology as discussed in the introduction section. Further research should focus on quantitative data on how the stakeholders consider the issues underlying the seventeen moral values related to social robots in education. Such a method would allow for more participants from the relevant stakeholder groups and allow quantitative comparisons across groups. Results of such a study, combined with the qualitative results of the current study, can be used as a solid basis for creating the first guidelines for the responsible use of social robots in education in view of the different perspectives at stake.

To conclude, this study aimed to identify and compare the moral conceptions of the key stakeholders related to social robots in primary education. We conclude that all stakeholders consider social robots as a potentially valuable tool for education. We identified a list of 17 values that are considered to be influenced when social robots enter education. Overall, we found many similarities and only few conflicting views across the various stakeholder groups. Particularly among the teachers, parents, and policymakers, there were many similarities on the issues reported and their considerations were often aligned. In sum, each of the stakeholder groups agreed that social robots 1) are fun and motivational for children; 2) are useful for simple teaching tasks, supervision, taking exams, and motivating children; 3) provide valuable data; and 4) cause ambiguities related to responsibility and accountability. Although many open issues still need to be addressed, stakeholders appear to agree that social robots could have great potential for education. Guidelines that address these issues are crucial for each of the stakeholder groups to accept social robots as useful in primary education. Therefore, further research is needed to start drawing up these guidelines to allow the implementation of social robots in the educational system as a justified, safe, and morally responsible new technology for children to expand their learning experiences and be prepared for the future.

