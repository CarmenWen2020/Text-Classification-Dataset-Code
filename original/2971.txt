To provide students with high-quality, tailored content, higher education institutions face the challenge of helping their instructors produce their own materials. New learning objects are usually assessed when students actually use the materials. To improve quality assurance in the design of learning objects, this paper proposes systemically analyzing the user experience of new learning objects. This involves not only considering student feedback, but also integrating other perspectives into a continuous feedback loop, through a participatory design approach. The proposed framework was tested during the implementation of a flipped classroom initiative in a large Chilean university over a period of six months, involving 6 content designers, 3 teachers, 2 teaching assistants and 98 students. We observe how elements of feedback are integrated through a systemic assessment, where the end result (the learning object) is contrasted with the elements that guided its creation (the design guidelines). In this way, the roles of designer and user are complemented so as to identify design gaps early on in the process of creating a solution. This study adds to the current literature by relating design guidelines with the principles of instruction during the design process in order to ensure the overall quality of the learning objects.

Previous
Next 
1. Introduction
Blended learning environments have been proposed as means for modifying and improving the traditional lecture style (Ngigi & Obura, 2019). In this sense, the premise is that changing the traditional roles and relationships between instructor and student may positively alter the process of knowledge construction and metacognition (DeLozier & Rhodes, 2017). In this scenario, students must assume a central role within the process of knowledge and skill acquisition, while instructors must focus on facilitating this process (Lee, Lim, & Kim, 2017). One form of blended learning is the flipped classroom (Shambaugh, 2020). This model integrates online lectures (which are viewed by the students outside of class) with face-to-face interaction between students and instructors during class time (Davies, Dean, & Ball, 2013; Strayer et al., 2015). Providing students with two main learning objects outside the classroom is central to this approach, these are: a) high-quality learning materials aligned with the goals set by the instructors and b) self-assessment materials to support self-paced learning (DeLozier & Rhodes, 2017).

Depending on the topic of study, as well as the students’ native language and educational stage, the flipped classroom can be supported by third-party materials, which have become widely available online (e.g. Khan Academy, ed-X, Coursera, among others). However, in order to provide students with high-quality, tailored content, higher education institutions face the challenge of helping their instructors produce original materials (Tolks et al., 2016). This effort is supported in two main ways: a) the institution develops new materials with the support of professional content production teams or b) instructors are provided with design guidelines and instructional-design support that allows them to advance independently towards a blended pedagogy (Ginns, Martin & Marsch, 2013; Jin, 2013).

To assist teachers in creating appropriate material for their students to learn, the learning design approach (Hernández-Leo et al., 2019) advocates a shift from a focus on content to a focus on the learning experience (Mor, Ferguson, & Wasson, 2015). Instructors are therefore faced with several instructional design challenges: understanding student needs and the context of learning beyond classroom interaction, defining learning goals, developing a new educational tool (i.e. learning materials, platform, assessment tools, etc.), implementing it and assessing its effect on a student population (Mamun, Lawrie, & Wright, 2020).

This paper aims to support the learning design process when instructors are faced with developing their own materials in the absence of professional support teams. We propose an evaluation framework that covers the user experience from design guidelines to the final product used in a flipped classroom setting.

2. Literature review
2.1. Learning objects in the flipped classroom
Learning objects are one of the most widely defined concepts in the educational sciences. Nash (2005) defined them as “a collection of content items, practice items, and assessment items that are combined based on a single learning objective”. They have also been defined as the “raw material” that is created, stored, used in teaching and administered by technologically based tools in education (Cohen & Nycz, 2006).

The flipped classroom (FC) instructional strategy includes a range of information artifacts such as readings, videos, questions, quizzes, and online tests, among many others (DeLozier & Rhodes, 2017). These artifacts correspond to a range of purposes related to the digital transformation of education and how the student approach to learning is changing, including:

•
Active engagement in the classroom: class time is focused on students producing their own learning objects, such as presentations or posters and/or discussing study topics. These activities aim to encourage peer-to-peer interaction among students, transforming the role of the instructor to one of a mediator for discussion and co-production of knowledge (Lo, Lie, & Hew, 2018).

•
Information retrieval: classroom activities, such as open-ended questions, quizzes and/or audience responses, focus on the students understanding and applying the content that has been previously accessed through the online platform (Lee et al., 2017).

•
Individual study outside of class: the goal is to “earn” classroom time for face-to-face interaction between peers and instructors. Out of class means the students interact with videos streamed through online learning platforms and/or with presentations or readings delivered through a digital medium (e.g. wikis, repositories, email, among others) (Enfield, 2013).

•
Personalized formative assessments: the goal is to allow students to guide themselves in their own out-of-class learning. This involves interacting with online quizzes, questions or any other assessment tool that allows them to move through the content at their own pace (Schroeder & Dorn, 2016).

Fulfilling each of these purposes effectively involves students and instructors performing several tasks while using learning objects. This includes accessing information, understanding the information, carrying out self-assessment and preparing for class (Handelzalts, Nieveen, & Van den Akker, 2019).

From a technology-design perspective, the interactive nature of learning objects in a flipped classroom is determined by the degree of efficiency in the two-way communication between the information artifact and the user (Merkt & Schwan, 2014). In the case of artifacts with a higher level of interactivity (such as online video platforms) the two-way communication between the artifact and the user can be greatly improved by using Human-Computer Interaction methods and guidelines during design and development phases (Law & Sun, 2012). However, despite the informational and interactive nature of the flipped classroom (Strayer, 2012) these kinds of learning objects have rarely been evaluated from the Human-Computer Interaction perspective, even though the importance of user evaluation is acknowledged in learning design and learning analytics models (Lee et al., 2017).

In this sense, learning analytics understood as “the measurement, collection, analysis and reporting of data about learners and their contexts, for purposes of understanding and optimizing learning and the environments in which it occurs” (Ferguson, 2012), have been considered a complement to the learning design process, providing valuable input that can drive change in the decision process made by instructional designers (Nguyen, Gardner, & Sheridan, 2020). Despite the powerful insights provided by learning analytics, the data analyzed is mostly derived from the end-experience of students. This type of analysis leaves in absence other important stakeholders of the process such as: experts who proposed guidelines for the creation of new learning objects, instructors who designed the learning objects, instructors who delivered and used the final learning objects and instructors who changed the learning objects.

Therefore, systemically understanding the nature of usage from the perspective of all stakeholders in a solution becomes crucial to the overall quality of the learning object. In the context of the flipped classroom framework, this implies the need to review the design decisions affecting the user experience of both the technological artifact (i.e. the video-based environment) and the assessment artifacts designed to support self-regulatory processes (Awidi & Paynter, 2019).

2.2. User experience of video-based environments
A significant amount of research has focused on the interaction with video delivered through online learning platforms. The out-of-class elements of the flipped classroom model have largely been based on interaction with multi-modal and media-rich environment (Merkt & Schwan, 2014). Given their dynamic nature, video-based environments remain an important topic of research. A significant amount of investigation has focused on understanding how students experience the content delivered through these platforms, in the form of readings, presentations and exercises, among others (Lee et al., 2017; Van der Meij & Van der Meij, 2016).

Video-based environments have been used in the context of flipped classroom for different purposes: acquiring background knowledge (Sahin, Cavlazoglu, & Zeytuncu, 2015), reviewing and summarizing concepts previously taught in class (Strayer, Hart, & Bleiler, 2015), demonstrating and giving examples (Lee & Lai, 2017) and for self-assessment (Davies et al., 2013).

Different recommendations for proper interaction with video-based environments can be taken from previous educational initiatives such as Massive Open Online Courses (MOOCs) and other methods of online education (e.g. screen-casts and video conferencing).

Table 1 includes a summary of aspects that allow for improved interaction with video-based environments, based on enhancing the dimension of user experience. Each dimension regarding the quality of the material has been based on the 3C model (content, communication and construction) proposed by Kerres and Witt (2003). According to this model any learning environment consists of three components:

•
a content component, which makes the learning material available to a learner;

•
a communication component, which offers interpersonal exchange between learners or learners and tutors; and

•
a constructive component, which facilitates and guides individual and cooperative learning activities in order to actively work on learning tasks (or assignments) with different degrees of complexity (from multiple choice to projects or problem-based learning) (p. 4)


Table 1. Recommendations for improving the use of video-based environments.

Dimension	Aspect	Recommendations	Author
Content	Length	Pre-production lesson planning is key to producing videos equal to or shorter than 6–8 min.	Guo, Kim, and Rubin (2014), Mok (2014)
Auditory	Avoid auditory distractions and ensure microphone and audio quality. Also, captions should be included in the video clips as well to facilitate hearing impaired students and foreign students who may not be accustomed to the instructor's accent	Smith and McDonald (2013), Mok (2014)
Navigation behavior	Develop feedback strategies to encourage students to engage deeply with the content, rather than just study a minimal amount in order to pass the course or year.	Guo and Reinecke (2014)
Personal feel of video production	Recording in informal settings may add a personal feel to the content (versus studio production).	Guo et al. (2014)
Communication	Personalization effect	Changing text to emphasize a conversational style in first- or second-person forms rather than third-person forms may impact on friendliness, retention and effective cognitive processing.	Ginns, Martin & Herbert (2013)
Instructor presence in slide-based videos	Post-production editing should look to show the instructor's face at opportune moments throughout the video.	Guo et al. (2014)
Immersive digital environments	A greater sense of physical presence within a virtual environment may lead to greater degrees of friendliness and helpfulness for the content.	Moreno & Mayer (2004)
Including a clear introduction and summary of main points	Online lectures should contain a basic overview of or prerequisite content for an upcoming unit.	Smith and McDonald (2013)
Construction	Formative evaluation	Inserting micro-level activities within the video lecture to check learner understanding (e.g. pop- up quizzes or self-check activities).	Lee and Lai (2017)
Scheduling	Understand student scheduling constraints in order to plan the number of flipped lessons.	Young, Bailey, Guptill, Thorp, and Thomas (2014)
Forum and participation	Provide an online forum where all questions and answers can be viewed by all participants.	Street, Gilliland, McNeil, and Royal (2015)
The following analysis can be made from the information in Table 1:

•
Interactive features (e.g., friendliness, user personalization, personal feel of narrated voice, instructor presence, navigation and interactive streaming) brings the dynamics of a human–human interaction to human–computer interactions.

•
The students' previous knowledge should be considered and assessed in terms of how the content is structured before new interactions are established.

•
The students' contextual factors, such as potential distractions, amount of time (scheduling constraints) and access to the content, should be considered in the design of video-based environments.

2.3. User experience of formative assessment tools
Flipped classroom design is based on the idea that students can benefit from self-regulated learning (Wang, 2019) by deploying effective strategies for choosing and adapting their learning strategy based on the learning setting (Nguyem Gardner & Sheridan, 2020; Winne, 2006). The process of self-paced learning and self-regulation to which the flipped classroom refers is strongly related to formative assessment milestones (Roehl, Reddy & Shannon, 2013). In terms of a key process, formative assessment is mediated by a series of artifacts, such as questionnaires, interviews and user logs.

Despite the various typologies of assessment artifacts, from the non-technological (e.g. paper-based questionnaires) to the technological (e.g. online dynamic dashboards), and their moment of application (before, during and/or after class), little is known about how design decisions can be successfully oriented towards their definition and implementation. Table 2 summarizes the relevant literature on defining formative assessment tools in terms of a better overall user experience and learning process:


Table 2. Recommendations for improving formative assessment tools.

Temporal dimension	Aspect	Strategy	Author
Pre-class	Real-time feedback on engagement	Availability of dashboard that allows students to understand their engagement in terms of content delivery though online platforms.	Jovanović, Gašević, Dawson, Pardo, and Mirriahi (2017)
Self-paced study	Students should be able to determine aspects of responsibility with regards to their own learning process through quizzes or other assessment tools. Also, instructors should develop additional videoclips and quizzes for top-performers.	Nanclares & Rodríguez (2016), Mok (2014)
Self-check quizzes or prep-questions	Multiple-choice or fill-in-the-blank questions, used as a formative assessment tool for video lectures.	Mok (2014),
Combine feedback	Provide students with three types of feedback: a) statistical feedback, with numerically-represented results; (b) textual feedback, with a personalized, written message; and (c) visual feedback with graphically-represented, statistical results.	Aljohani et al. (2019)
In-class	Short quiz at beginning of class	Questions, at the beginning of each class, from a subset of the self-check quizzes or prep-questions, to encourage students to complete the assigned lessons and to provide the instructor with information on which contents need to be reviewed in class.	Enfield (2013)
Replayed content	Teacher analysis of content most often replayed by students in order to know which content should be reviewed in class.	Nguyen et al. (2020)
After-class	Gamification of quizzes	Quiz competitions after lectures to motivate students to compete with one another and compare their work.	Zainuddin, Shujahat, Haruna, and Chu (2020)
The following analysis can be made from the information in Table 2:

•
The design of a proper user experience with formative assessment should be covered with tools at crucial moments: pre, during and after class.

•
Learning can occur during any of these moments; teachers should provide at least one instance of self-assessment delivered using online tools.

•
Student assessment can be performed with a range of tools, from personalized to automated system-based feedback.

2.4. Instructors as design agents
“Teachers may also experience problems with camera set-up, video capture, and processing—problems that may impede the successful use of the own-video condition at scale.” (Beisiegel, Mitchell, & Hill, 2018)

The way recommendations of user experience for video-based environments and formative assessment tools are considered and integrated into the end result depends significantly on the instructional design process. Therefore, we have to understand how instructional designers take into account user-centered dimensions and guidelines.

The initial adoption stages of the flipped classroom model are commonly characterized by an informal design and implementation of video-based environments. In many cases, instructors act as design agents for the entire experience delivered to students (McLaughlin et al., 2016; Davies et al., 2013). Design agents are represented by those who assume the role of designing the learning objects to be used by students and instructors. In this sense, a set of relevant barriers for design agents in video-based environments include: limited availability of time, low technological support and technological skills, and a lack of previous experience in the design of learning tools that focus on student engagement rather than test taking (Mor et al., 2015).

Consequently, design agents often rely heavily on their own previous experience in order to achieve reasonable levels of quality and adoption rates. Nevertheless, they may also need to access third-party or institutional resources so as to develop attractive and useful content (Jin, 2013; Mok, 2014). Given the development of new technological tools, user feedback (i.e. from students, instructors and/or teaching assistants) can now be incorporated into the learning design much sooner (Alonso-Fernandez, Rotaru, Freires et al., 2017). However, this feedback must be structured around the design process, from the initial guidelines to the end product, as well as the prototypes (Lee et al., 2017).

2.5. Design guidelines
Lo et al. (2018) stated that the “design of flipped classrooms is currently under-theorized”. Furthermore, the rapid development of technology for recording, storing and sharing video has led to an increase in the production of video-based environments for instructional use. There is a huge range in quality among these platforms, mainly due to the diverse contexts in which they are applied (DeLozier & Rhodes, 2017). However, four main macro-objectives can be identified for any flipped classroom initiative (Kim, Kim, Khera, & Getman, 2014):

•
Prior exposure: ensure that students can be exposed to the content before class

•
Self-study: encourage students to prepare for class

•
Self-assessment: provide mechanisms to assess student understanding

•
Prepare for class time: provide in-class activities that focus on higher-level cognitive activities

From the perspective of the learning design, achieving these goals becomes critical. The decision-making process should therefore be supported by design guidelines in the form of heuristic oriented tools (Voigt, Fredriksen, & Rasmussen, 2020). A literature review provided a set of low-level design guidelines that can relate to the macro-objectives and therefore provide design agents with a common design framework (Table 3).


Table 3. Relation of flipped classroom macro-objectives to general design guidelines.

Macro-objectives	Low-level dimension of FC guidelines	Description	Author
Prior exposure and self-assessment	Learning goals and context	The design of video-based environments should primarily take into account the learning goals of a course, content sequence, learner profile and technological environment in order to define the content and evaluation features.	Lee and Lai (2017)
Prior exposure	Activation content	The design should include content aimed at recalling the necessary background knowledge for learning about new topics (i.e. the activation principle).	Sahin et al. (2015)
Self-study	Flexibility	The design of video-based environments should allow instructors to modify content and questions online, based on an in-class assessment.	Wallace et al. (2014)
Self-study	Experience continuum	Video-based environments should encourage students to study/read before class and provide them with the opportunity to demonstrate what they have learned after materials are covered in class.	(2014)Everett, Morgan, Mallouk, & Stanzione,
Self-assessment	Simplicity	The design should provide students with short and simple questions that help them to be aware of their learning.	Kim, Kim & Khera (2014)
Prepare for class time	User integration	The design should include features that encourage students to engage in online discussion and seek help from others.	Lo et al. (2018)
Prepare for class time	Application	The design should provide students with the opportunity to tackle problem-solving exercises.	Lo et al. (2018)
Prepare for class time	Cohesive alignment	The design should be capable of tying together interaction in the video-based environment with face-to-face interaction in class regarding the content, thus making it more relevant to the students.	Buerck, Malmstrom & Peppers (2003)
While these design guidelines provide general recommendations, they can also be linked to specific design actions (such as those in Table 1, Table 2). This can impact the quality of the content, communication and creation of video-based environments and formative assessment tools across different courses.

2.6. A heuristic approach to the design of flipped classroom
As observed in Table 1, Table 2, the overall quality of student experience in flipped classroom can be linked to the utilization of user-centered recommendations related to a heuristic approach (Hermawati & Lawson, 2016)

Despite a significant amount of guidelines supporting the design of learning objects for flipped classroom environments, instructors still face significant challenges related to improving user experience with regards to flipped classroom: improve the testing of new online-materials in early stages of design, empower the connection between the online and classroom sections and foster the effectiveness of student self-paced learning (Murillo-Zamorano, López Sánchez, Godoy-Caballero, 2019).

It is possible to observe that these recommendation-efforts have centered in the evaluation of the final outcomes of the design process, focusing mostly in the student experience of final products (videos, content, self-assessments & testing tools, among others).

User-evaluation of the creative process in flipped classroom contexts should also cover the early and intermediate stages of design, with the following benefits to instructors:

•
Effectiveness: assuring that videos and formative assessment tools are impacting the intended instructional goals, thus allowing instructors to focus on the classroom experience (Voigt et al., 2020),

•
Efficiency: possibility to scale-up all content sections into flipped classrooms in shorter periods of time allocated to design, due to a standardized and proven content structure (Khanova, Roth, Rodgers, & McLaughlin, 2015)

•
Satisfaction: ensuring that the student experience of flipped classroom can meet curriculum goals, student motivations and enhanced levels of engagement (Murillo-Zamorano et al., 2019)

In Human-Computer Interaction, heuristic approaches have a long tradition in terms of enhancing usability evaluation based on expert reviews. Heuristics have been extensively used in the evaluation of interactive technology, documents and procedures in order to enhance acceptance levels of machine-person interactions (Hermawati & Lawson, 2016). A significant attribute of this type of evaluations is the consideration that the “human interaction” is covered from early stages of system developments, to regular low-level assessments when a solution is deployed (Wall et al., 2018). Despite the general use of heuristic approaches, research efforts need to be performed in order to develop domain-specific heuristics (Alonso-Virgós, Espada, Thomaschewski, & Crespo, 2020). Hence, the current theoretical gap covered in this study focuses on the lack of evaluation tools that can provide instructional designers (and institutions) with enhanced levels of readiness when facing the design of flipped classroom, through to the incorporation of feedbacks into the design process.

3. Framework for the systemic evaluation of user experience
We outline a framework, based on an heuristic approach, for the design of learning materials and user feedback in the context of flipped classroom implementation with high constraints of resources. The aim of this framework is to systematically incorporate user perceptions, from initial design of guidelines through prototypes to the final product, conceiving users in each phase as participatory agents allowing instructional designers to effectively respond to student needs (Kirschner, 2015).

3.1. Design stages, feedback instances and related agents
Learning design has improved the way learning materials are delivered and presented to students. However, most materials score poorly when it comes to instructional design principles, particularly information design (Margaryan, Bianco & Lotlejihn, 2015). Different methods have been proposed for enhancing these materials and the user experience. Lee et al. (2017) outlined an instructional design method for the flipped classroom, in which usability testing was considered to validate the model itself. These assessments were made at a midway point (between initial analysis and final implementation) by usability experts and members of an instructional design team.

In contrast, Noguera, Guerrero-Roldán, and Masó (2018) propose an agile process, with a continuous flow of improvement based on student and teacher opinions. In order to ensure that user experience assessment data positively impacts the solution under development, the data must be linked to an underlying design process (Fig. 1). Given the aim to achieve simplicity, we describe a three-level design-process based on Sim et al, (1998):

•
First design stage/Outcome design guidelines: An initial representation of the conceived solution is produced, and a design goal is set. Existing knowledge from the field is analyzed and summarized in order to produce guidelines for the design. Input knowledge of the current state of design, users, tools, constraints and external requirements (e.g. regulations) are used to produce coherent guidelines for the generation of instructional products for a given context.

•
Second design stage/Outcome prototypes: Output knowledge is produced in the form of prototypes and/or mock-ups. These artifacts or systems are the result of applying the input knowledge to a solution that has been developed based on the design goal.

•
Third design stage/Outcome final product: A design goal is achieved, and the resulting artifact/system is implemented and tested in order to address the constraints of the users and the context.

Fig. 1
Download : Download high-res image (512KB)
Download : Download full-size image
Fig. 1. Design stages, participants and dimensions assessment of user feedback.

From a participatory design perspective (Kirschner, 2015), continuous feedback from users can be integrated into each of these design stages. Therefore, three instances of user feedback can be defined:

•
Process feedback 1: This first step of assessment covers the user experience of the first design outcome (guidelines). These guidelines link the recommended design actions to the desired objectives or goals of a new learning initiative. The aim of this feedback is to understand the quality of the user experience with the design guidelines, which will heavily influence the initial creation of instructional products.

•
Process feedback 2: The second step involves assessing the results of the design activity. Output knowledge is produced from early testing of prototypes, mock-ups and/or a final product that has not yet been fully launched. The aim of this is to see how useful the end product will be with regards to the desired goals, from the designer perspective with a focus on anticipating the perception of final users.

•
Results feedback: The final step involves the knowledge that is provided by the user of the design result: is the perception of the user agents (i.e. the students) consistent with the previous perceptions and accumulated knowledge (i.e. instructional design principles, best practices and standards)?

This framework essentially depicts three main roles that are involved in providing feedback (Fig. 1):

•
Initial designers: Involved in understanding the learning context, defining (or selecting) a set of guidelines to be used for the design of activities (input knowledge), in the form of prototypical learning objects.

•
Intermediate designers: Involved in bridging the initial prototypes with the needs of the end users. In many cases the initial and intermediate designer can be the same instructor.

•
User agents: Involved in evaluating the prototypes and/or the end product and subsequently providing feedback in the learning context. Usually, these are the students, instructors and/or teaching assistants who participate in the delivery of the course material.

3.2. Dimensions of the feedback
In flipped classroom projects most often the interactive content-medium vary from videos, online pre-recorded presentations, audio-podcasts to synchronous online delivery of content. As evidenced in Table 1, Table 2, these learning objects are currently supported by a significant set of low-level guidelines (i.e heuristics), which are frequently used to assess student experience during the final phases of implementation. Hence, it is possible to find many low-level heuristics that analyze user experience around interactive elements linked to navigation, menu-design, recovery from error, learnability, satisfaction and or memorability of system operation (Antonenko, Dawson, & Sahay, 2017).

By contrast, it is necessary to complement these user experience assessments with high-level and actionable feedback over user-centered attributes of all the information artifacts created from early planning stages to those delivered at the end of the implementation.

In any designed system, the user experience is related to both the interaction patterns and the content format (Young Liu et al., 2020). Content relates directly to the informational quality of each artifact and can be treated as a dimension that allow designers to relate the initial design decisions with the quality of the final products. An information quality framework was proposed by Wang and Strong (1996), defining four key quality aspects:

•
Intrinsic quality: Information must be perceived as containing accurate and trustworthy data. Related items include completeness, clarity, believability, objectivity, reputation, traceability and variety of data sources.

•
Contextual quality: Information must be perceived as being able to resolve the issues faced by users and must be in line with their tasks. Related items include: value-added, relevance, timeliness, ease of operation, appropriate amount of data.

•
Representational quality: Information must be relevant for users and delivered within a suitable timeframe. Related items include: interpretability, ease of understanding, representational consistency, concise representation.

•
Accessibility: Information must be delivered through information systems in a safe, reliable and accessible manner. Related items include: accessibility, cost effectiveness, access security.

Based on an heuristic approach, it is possible to link each of these dimensions to the domain-specific context of any flipped classroom implementation, centering evaluation in the user experience of content patterns, from initial design guidelines (used to produce video and/or self-assessment tests), through the prototypes an into the implemented products in course-context.

Table 4 shows the specific high-level user experience dimensions on which the feedback process was based for this study considering the previous analysis on information quality. Additionally, the usability framework proposed by Nielsen (1994) was taken into account, particularly in the dimension of “ease of use” which has an impact both on performance efficiency and satisfaction (Holzinger, 2005). Nielsen and Holzinger provide a simple and reliable framework for measuring user dimensions, considering not only the final product but also initial and intermediate stages of development. Accessibility dimensions were not included due to fact that the particular experience of this study was based on the usage of a university platform to deliver videos and self-assessment tools, to which all the students had complete access during the semester. However, accessibility issues such as visual impairment, were considered due to the website following usability criteria for vision-impaired users such as: proper form layout, importance levels of elements, color of text and background, magnification at mouse-hovering, among others. Each user experience assessment dimensions is related to a flipped classroom macro objective (see previous Table 3) and then related and described with regards to each of the design stages and its corresponding stakeholder.


Table 4. Definition of user experience dimensions for systematic assessment across design phases.

Dimension of user experience based on Wang and Strong (1996); Nielsen (1994); Holzinger (2005)	Assessment roles
Initial Designer
Do guidelines cover …	Intermediate Designer
Do the prototyped learning objects …	User
Do the final video and formative assessment tools …
Completeness (intrinsic)	‥the information needs for producing videos and formative assessment tools?	… cover the aspects to be discussed in the classroom section?	… cover the aspects discussed in class?
Clarity (intrinsic)	‥prepare for producing learning objects?	… facilitate understanding of topics by students?	… allow for an understanding of the main topics covered in class?
Relevance (contextual)	… relate to critical topics for producing video and formative assessment tools?	… remain critical to the eventual learning needs of students?	… remain critical to the learning needs of the course?
Length (contextual)	… relate to the available time for producing learning objects?	… relate to students’ availability of time for the course?	… relate to students' time availability for course dedication?
Ease of use (representational)	… relate to efficiency and effectiveness in video and formative assessment production?	… relate to students’ ability to operate effectively and efficiently with regards to their previous experience and course learning demand?	… allow students to operate effectively and efficiently with regards to previous experience and course learning demand?
Fig. 1 summarizes the proposed framework for the systemic evaluation of user experience, relating design stages, instances of feedback and the high-level dimensions of assessment for each stage.

3.3. Research question of this study
The aim of this study is to propose a general framework that can guide high level assessment of user experience of educational technology in a domain-specific context (flipped classroom in higher education). Our research question therefore asks:

“How can the user experience of learning content and assessment tools be assessed systemically, from design to final product?".

4. Method
4.1. Participants and design
A study was conducted over a six-month period at a Faculty of Education at a large Chilean university. A pilot program was implemented to bring the flipped classroom model to two of its courses. The aim of both courses was for students (future teachers) to manage heterogeneous classrooms using skills to create an environment that guarantees the learning of all students. The difference between both courses was that one was focused on future high school teachers, while the other was delivered to elementary school teachers.

Students have traditionally been the main evaluators of the usability of new learning artifacts (DeLozier & Rhodes, 2017). However, the need to develop high-quality learning products drives attention towards the use of design guidelines to direct their development. From the literature, it is clear that there are guidelines in place for both developing methods and making recommendations. For instance, Plaisant and Shneiderman (2005, pp. 171–178) developed a set of ten guidelines for anyone tasked with designing video tutorials for using a new software interface, whereas Jin (2013) explored how guidelines can improve the design of digital texts for learning. Therefore, our feedback process comprises instructional designers, instructors and students.

A design-based method (Amiel & Reeves, 2008) was adopted. The study was carried out in three phases based on the design and feedback stages depicted in Fig. 1.

•
Stage 1: Design and evaluation of guidelines: A set of three design guidelines (in video format) for producing videos and formative evaluation tools were developed by the research team during a period of 4 weeks (Appendix A). The guidelines covered visual grammar (Benoît, 2016), formative assessment (Lee et al., 2017) and critical thinking (Tolks et al., 2016). The guidelines were uploaded to a YouTube channel and shared with the instructors and teaching assistants participating in the project (i.e. initial designers). Feedback 1: Course instructors were asked to answer a survey about their experience of using the guidelines for the design of their videos and formative assessment (Appendix B).

•
Stage 2: Design and evaluation of a prototype: The research team asked the instructors on the two courses to design their own video based material and formative assessment questions. A monthly meeting was held in order to discuss content structure of videos and formative assessment between instructors and the research team. Feedback 2: Instructors and teaching assistants answered a survey on user experience of their prototypes focusing in evaluating their potential learning impact and usage by the final users (i.e. students) (Appendix C)

•
Stage 3: Implementation and Evaluation of products: Flipped classroom videos (Appendix E) and formative assessment tools were designed before the start of the semester by the same instructors and teaching assistants (i.e. intermediate designers) that would deliver these learning objects later in their courses. These were implemented in the two courses that are delivered annually by the Faculty of Education to undergraduate students. Feedback 3: Students from each of the courses assessed the user experience of both the videos and the formative assessment tools (Appendix D). Surveys were sent to students 1–2 weeks after they had experienced the flipped classroom approach, and before sitting any exams.

4.2. Sample
Data was acquired at three stages during the study, with the total number of participants determined according to the framework proposed in Fig. 1. As described before all the participants were part of two courses developed in the School of Education, in which a Flipped Classroom methodology was being implemented. Therefore the “initial designers” and “intermediate designers” were part of the instructional teams behind those courses. They were involved already in a transition of their courses into a flipped classroom methodology, in consequence this study was parallel to a real implementation effort and did not have to set up a particular team of designers or instructors. Table 5 summarizes the participant in each feedback instance, instructors and teaching assistants in feedback 1 were the same as those participating on feedback 2.


Table 5. Sample participants.

Stage	Number of participants	Participants
Feedback 1: Evaluation of guidelines for video production and formative assessment/INITIAL DESIGNERS	6	Three instructors with extensive teaching experience (more than 5 years) and three teaching assistants, also with significant experience (more than 3 years).
Feedback 2: Self-evaluation of video and formative assessment prototypes/INTERMEDIATE DESIGNERS	5	Three instructors and two teaching assistants evaluated the final learning objects (videos and formative assessment tools).
Feedback 3 3: Survey of final learning materials (video and formative assessment)/USERS	98	Undergraduate students enrolled on the corresponding courses evaluated both the flipped-classroom videos and the formative assessment tools.
4.3. Survey design
A five-item questionnaire was used to survey the participants’ perception of information quality of the design guidelines and resulting learning objects (see Appendix B, C & D). This questionnaire was based on an existing instrument used to survey user experience within the context of educational learning objects (Madariaga et al., 2017). Table 6 display a pool of questions related to the development of the final questionnaire. The selection of one question to measure a variable was made due to time-restrictions related to the delivery of the survey during class-time, which was strongly limited, the specific selection of one question was made with guidance of the team of instructors.


Table 6. Set of questions to measure user experience dimensions of guidelines.

Dimension	Other possible questions	Question used
Completeness	Did the content of the learning object properly cover your learning needs?
The content of the guideline didn't allow me to develop videos and/or formative assessment tools?	Do you think the guidelines cover the main aspects of video production/formative assessment? (Barely vs adequately)
Clarity	Was the learning object difficult to understand?
The learning object was not clear in its purpose	How clear did you find the learning object? (Unclear vs very clear)
Relevance	How relevant was the guidelines to work in the prototyping?
The guidelines were not relevant for prototyping actions.	Regarding your prototyping needs, the content of the guidelines seemed to you? (not relevant vs very relevant)
Extension	The extension of the guideline were adequate with regards to my available time?
The guidelines could have been shorter	Regarding your time availability, the extension of the guidelines seems to you? (too long vs adequately)
Ease of use	The guidelines seemed properly designed to allow for video production?
The guidelines were note easy to use	Regarding your teaching experience, the guidelines seems to you? (hard to understand vs adequately)
With this questionnaire, the participants had to indicate their level of agreement with a series of statements on a scale from 1 (totally disagree) to 10 (totally agree). The values reported in Table 7, Table 8 are displayed according to the following structure:

•
Process feedback 1 to 3: Are the median scores for each of the assessment of user experience dimensions

•
Median M1 to M3: Are the median scores for the combination of the five user experience dimensions for each of the assessment roles (initial designers, intermediate designers and final users:


Table 7. Summary statistics for user experience of flipped classroom videos.

N	Median	MEDIAN
Completeness	Clarity	Relevance	Length	Ease of use
Guidelines evaluation	6	9,0	9,0	8,0	7,0	10,0	9,0
Prototypes Evaluation	5	8,0	8,0	7,0	8,0	10,0	8,0
Final product evaluation	98	9,0	9,0	9,0	7,0	9,0	9,0
Kruskal wallis test	Chi-square	χ2 = 0.1
χ2 = 2.97	χ2 = 1.35	χ2 = 1.818	χ2 = 2.273	
p	p = 0.951	p = 0.227	p = 0.508	p = 0.402	p = 0.32	
Degrees of freedom	2	2	2	2	2	
MEDIAN	9,0	9,0	8,0	7,0	10,0	
Cronbach's α was 0.72 for the survey of the video design guidelines and resulting videos, while it was 0.73 for the survey of the formative assessment design guidelines and resulting tools. Additionally, an open ended question was added to the survey in the last stage of assessment to students in order to complement the heuristic oriented approach with a qualitative input. Each response was coded with regards to the method of analysis and interpretation of social content (Miles & Huberman, 1994), relating responses to the dimension of the user experience (see Table 4) and to technical issues of the experience itself. Any other statement was coded as “other” and provided with an additional dimension.

The procedure for applying the survey was approved by the University's Institutional Review Board. Each of the participants was properly informed about the goals and methodology of the study and was asked to sign an informed consent form before answering the surveys.

4.4. Data analysis
A cross-sectional analysis of medians was performed in order to determine any statistically significant differences among the evaluations of the three stages of feedback (Table 3). Doing so allowed us to identify consistency (or inconsistency) across different stages of the design process and among the three different roles: initial designers (feedback 1), intermediate designers (feedback 2) and end users or students (feedback 3).

Due to the ordinal and nominal nature of the response data from the surveys, a non-parametrical test (Kruskal-Wallis) was performed so as to understand the change in median scores among the different roles. Analysis was performed using statistical software (STATA IC v.15). The significance level was set at alpha 0.05.

5. Results
5.1. Videos: user experience of design guidelines, prototypes and final product
The results in Table 7 display how the video design guidelines, the prototypes and the resulting videos were perceived by the three different roles described by the assessment framework (Fig. 1, initial designer, intermediate designer and user). Table 7 also provides a summary of the median, chi-square and p-value for each dimension that was evaluated.

In order to provide a general overview and foster a pragmatic interpretation of the systemic assessment, the research team defined five ranges with regards to the median for all five dimensions assessed by each role, based on the ranges covered by System Usability Scale (Bangor, Kortum, & Miller, 2009):

•
Range A: Very poor quality of user experience (median score 1–2):

•
Range B: Poor quality of user experience (median score 3–4)

•
Range C: Acceptable quality of user experience (median score 5–6)

•
Range D: Good quality of user experience (median score 7–8)

•
Range E: Very good quality of user experience (median score 9–10)

The median score for the combination of each of the five dimensions were all in the range of good to very good quality of user experience. We can observe that initial guidelines had the perception of a very high degree of quality of use (median score = 9). Subsequentially intermediate designers assessed the prototypes (resulting from those very good quality guidelines) with a good user experience quality as well (median score = 8). Finally, student's assessment of the final videos was in the range of very good quality of user experience (median score = 9).

The assessment of “length” by the students was the lowest observed median score of the five dimensions. It is also worth noticing that the assessment of “relevance” between instructors and students had the highest difference (2 points) being better assessed by students than instructors.

5.2. Formative assessment tool: user experience of design guidelines, prototypes and final product
Table 8 provides a summary of the median, chi-square and p-value for each dimension of user experience that was evaluated with regards to the design guidelines, prototypes and final formative assessment tools.


Table 8. Summary statistics for user experience of formative assessment tools.

N	Median	MEDIAN
Completeness	Clarity	Relevance	Length	Ease of use
Guidelines evaluation	6	8,0	8,0	9,0	8,0	8,5	8,0
Prototypes Evaluation	5	7,0	6,0	6,0	7,0	8,0	7,0
Final product evaluation	98	8,0	7,0	8,0	7,5	8,0	8,0
Kruskal wallis test	Chi-square	χ2 = 1.134	χ2 = 2.842	χ2 = 3.35	χ2 = 0.066	χ2 = 0.331	
p	p = 0.567	p = 0.241	p = 0.187	p = 0.967	p = 0.847	
Degrees of freedom	2	2	2	2	2	
MEDIAN	8,0	7,0	8,0	7,5	8,0	
From a pragmatic interpretation perspective, it is possible to observe that all three agents scored the quality of user experience in the “good” range, being the instructors (i.e. intermediate designers) the group that scored the lowest overall quality of user experience (median score = 7). Of the five dimensions “clarity” was the lowest scored (median score = 7) by all three agents, followed by “length” (median score = 7.5).

5.3. Open ended questions
A set of 23 student statements were retrieved from the survey that was handed to them in paper. Overall, their qualitative observations related to a decreased quality of user experience with regards to “length” and “clarity” of the videos and formative assessment tools. Table 9 displays the statements with regards to their general perception of the videos and the formative assessment tool. In the videos, students main concerns revolve around the dimension of “length” which also coincide with the lowest median score of all dimensions evaluated by the students in the survey (median score of 7 in Table 7). Same coincidence can be related with regards to the formative assessment tool, where all the statements relate to a negative user experience with regards to “clarity” (median score of 7 in Table 8).


Table 9. Statements with regards to open ended question.

Learning Object	User	Statement	Dimension
Video-based	U01	“I don't like the use of videos; I prefer face-to-face classes"	Relevance
U02	“As it is complementary, they should be shorter"	Length
U03	“(About the extension) Watching three videos each one 8 min is too much, if it was one I find would it adequate"	Length
U04	“Very extensive cases and alternatives, when there were 3 videos"	Length
U05	I think the videos have to be more didactic	Ease of use
U06	Sometimes the videos were not loaded	Technical
U07	The evaluation and the videos are very long	Length
U08	The videos did not load on the platform	Technical
Formative Assessment	U09	“The feedback failed the last classes because even marking the correct answer in green, it said incorrect"	Clarity
U10	“Review feedback"	Clarity
U11	“The wording of the questions is not always clear. In this course, many depend on the justification, so doing it with alternatives is complex"	Clarity
U12	“In most of the videos, the response written in flipped was different than the colors (red or green) that appeared"	Clarity
U13	“There are problems with the answers since they are right, and it comes out wrong"	Clarity
U14	Feedback is hard to understand, I don't understand all the criteria for improvement	Clarity
U15	The platform is clear in terms of questions and answers, but misconfigured when answering	Ease of use
U16	I do not understand the feedback from the formative evaluations	Clarity
U17	The evaluation and the videos are very long	Length
Other	U18	“Platform problems generate confusion"	Ease of use
U19	Leave written material for study support	Need of support
U20	Problems with defining correct/incorrect answers	Question definition
U21	The platform was giving me wrong answers, but the answers were in green	Technical
U22	The feedback failed, it always came out wrong even though everything was green. Then i was confused	Technical
U23	When answering the questions, many times I would skip to question 3 without answering question 2	Technical
6. Discussion
This mixed-method study was developed in order to understand “How can the user experience of learning content and assessment tools be assessed systemically, from design to final product?".

Based on previous heuristic recommendations in literature the research team developed a set of two general video guidelines aimed for: a) designing and creating video materials for an undergraduate-level course and b) develop a formative assessment tool for students to self-evaluate their learning of the video materials.

A user experience framework (see Fig. 1) was established for evaluating the user experience perception of two core sub-dimensions: a) information quality (i.e. completeness, clarity, relevance & length) and usability (i.e.ease of use).

6.1. Domain-specific design guidelines can transfer user experience quality to final learning materials
Despite an important availability of recommendations to conceive proper quality of user experience with regards to flipped classroom materials, most learning artifacts developed by instructors score poorly for overall quality (Margaryan, Bianco, & Littlejohn, 2015; Oh, Chang, & Park, 2019). Hence, the research team developed a set of domain-specific guidelines aimed for ensuring a high quality of user experience in the process of design of learning objects for flipped classroom.

In the particular case of this testing of the assessment framework, we observed that the scores assigned by the evaluators in each stage for both video and formative assessment tools didn't differ from each other. In other words, the quality of the user experience of the design guidelines was transferred to the quality of intermediate and final learning objects. This noticeable result must also be analyzed in the light of a team of instructors with low-level of experience producing both video materials and formative assessment tools but with a high level of experience (more than 8 years) with regards to producing their own materials (e.g. class presentations, study notes, exercises). The use of initial design guidelines presented as videos may also lead to better results since they serve as a real example and application of a particular learning framework (Lawson, Davis, & Son, 2019).

6.2. Formative assessment tools are perceived as more relevant by initial designers and students
With regards to the formative assessment tools in the context of flipped classroom, our results reveal a two-point difference in “relevance” between the instructors and the students. Despite the statistical non-signficant difference, this may suggest that the design guidelines could be improved in order to help instructors better understand how formative assessment can support the learning experience itself. This is in line with Lo et al. (2018), who found that formative assessment in the flipped classroom still requires effective integration between out-of-class and in-class sessions. Additionally in the open ended questions” the main concern by students is the level of “clarity” that can be added to the user experience of formative assessment tools (clarity is defined in Table 4 as the “perception of how the content of final learning objects allows understating of main aspects covered in class”). This can be connected to Shoufan (2019) who observed that content understanding seems to be the most relevant reason for liking an educational video on YouTube for students. Therefore design guidelines should be specially created to allow that “highly relevant learning objects” match a “high degree of clarity” for supporting student performance. This may sound obvious, but in practice the lack of timely assessment of user experience along the design path may lead to inconsistencies between the initial goals and the product delivered to students.

6.3. Better understating of student's time availability can lead to right sizing of content
The assessment of “length” by the students was the lowest observed median score of the five dimensions, ending up in a score that limits between the range of “Acceptable” user experience quality with “Good” quality. It is also worth noticing that the assessment of “relevance” between instructors and students had the highest difference (2 points) being better assessed by students than instructors. Hence, in terms of the design of video based materials, we observed that improvements can be made when it comes to the length of the videos (i.e. the amount of information vs. the time available for the task) as this dimension received the lowest rating from both the initial designers and the end users, being that videos are “highly relevant” for students. In the particular case of this research videos used by students had an average duration of 7.4 min. Considering guidelines such as those by Chen and Wu (2015) & Guo, Ki & Rubin (2014) which state that videos should be no longer than 6 min, we can observe that our 7.4 min did not match these studies, despite being 90 s longer. Moreover, studying the available time students have in particular learning contexts for self-study may be the cornerstone for defining proper duration of content. This emphasize the need to advance the development of co-design with students of what will be the final learning objects. This corresponds with the perspective of Xiao et al. (2019) that state that learners have still not fulfilled a role as co-creators and co-assessors in the development of learning frameworks, such as the flipped classroom.

7. Conclusions
There has been significant research into the assessment of user experience and usability in education. However, there is no report that supports the importance of developing domain-specific design guidelines and a systemic user experience assessment from that point towards the final product to be used in classroom.

Hence, this study proposes a framework with regards to shortening the “heuristical gap” that can be observed between guidelines and products in educational technology development.

We were able to observe consistency in the user experience quality, from the definition of guidelines, to the creation of prototypes and the implementation of both video based classes and formative assessment questions. Our study displays that a learning design process focused on learner experience can impact on coherence between design guidelines and the final product. Coherence can greatly impact a quality framework for educational technology (Stracke, 2019). The present study adds to the current literature, such as the study by Lo et al. (2018), in which design guidelines were related to principles of instruction (problem-centered, activation, demonstration, application and integration). However, none of these guidelines were relevant as “input knowledge” (initial design stage, Fig. 1), nor did they influence the design process. From another standpoint, this research exemplifies that the quality of the user experience of an end product (i.e. flipped classroom videos and formative assessment tools) can be traced back to the decisions related to prototypes and initial design ideas. This is in line with current quality frameworks aimed at ensuring proper achievement of standards in blended and e-learning environments considering a multidimensional participation of actors (Stracke, 2019; Han & Ellis, 2019).

8. Limitations and future work
We report on a small sample and a specific case of a flipped classroom model, limiting the extent to which the results can be generalized. Both the limited number of questions for measuring each dimension and using a ten-point Likert scale for responses also limits their dimensionality. This means that the full range of perceptions may not have been captured, even though we were able to capture a small degree of qualitative perspectives by adding a open-ended question while surveying students. We did not develop the guidelines based on the perception of how the instructors felt about the training and support they received. This would be useful to understand in the future, so as to include other quality requirements within educational communities. The platform used to deliver the videos and formative assessment did no have YouTube-style functions for students to express engagement with regards to the comments (i.e. likes/comment features), this could be useful in a future approach to understand usage. Additionally, in our case the platform had some content-accessibility issues related to technical difficulties. This affected the overall experience of students in terms of accessing some videos and receiving the adequate feedback in some instances (as can be observed in the open ended responses).

The problem of impacting the quality of learning objects goes beyond just the assessment. It also requires properly understanding and involving students “during” the design process. However, integrating different perspectives into the quality assessment can provide new insights that enhance the user experience of other learning initiatives where design guidelines or directions are involved in the early stages of design, such as Massive Open Online Course and E-learning platforms, among others.

