Predictive learning analytics (PLA) is an educational innovation that has the potential to enhance the teaching practice and facilitate student learning and success. Yet, the degree of PLA adoption across educational institutions remains limited, while teachers who make use of PLA do not engage with it in a systematic manner. Informed by the Unified Theory of Acceptance and Use of Technology (UTAUT), we conducted eleven in-depth interviews with university teachers and examined their engagement patterns with PLA for the duration of a 37-week undergraduate course. We aimed to identify (a) factors that explain the degree of using PLA in the teaching practice and (b) the impact of an intervention - sending email reminders to teachers - on facilitating systematic engagement with PLA. Findings suggested that, amongst the factors facilitating engagement with PLA were performance expectancy, effort expectancy, and social influence. Amongst the factors inhibiting engagement with PLA were performance expectancy and facilitated conditions that were related to training and a lack of understanding of predictive data. Implications for the adoption and use of PLA in higher education are discussed.

Previous
Next 
Keywords
predictive learning analytics

higher education

university teachers

technology acceptance

1. Introduction
Learning analytics refer to the “measurement, collection, analysis and reporting of data about learners and their contexts, for purposes of understanding and optimising learning and the environments in which it occurs” (LAK, 2011). In addition to describing, Predictive Learning Analytics (PLA) are about forecasting or predicting learners' future behaviour and outcomes by processing past and current student data. PLA have been seen as a means to provide real-time and actionable feedback to teachers and students that can support their learning and help them succeed (Cheng, Liang, & Tsai, 2015; Jovanović, Gašević, Dawson, Pardo, & Mirriahi, 2017). Several higher education institutions are researching the use of PLA in their practices and have developed approaches to identifying students at risk of failing or not completing their studies (e.g., Herodotou et al., 2020a; Bodily et al., 2018). Yet, most of these approaches are relatively small scale examining early adoption through, for example, single cases studies (e.g., Dawson et al., 2018; Ferguson et al., 2016; Yoo & Jin, 2020). Only few institutions have adopted PLA at an institutional level; for instance, The Open University UKis the first to implement and enact an ethics policy about learning analytics (Slade & Boroowa, 2014) and has implemented PLA across its 170,000+ students by engaging teachers with PLA and relevant student-support interventions (De Laet et al., 2020; Herodotou et al., 2020a).

Evidence of effectiveness suggests that PLA can support and enhance the teaching practice and facilitate student learning and success. PLA can identify students at risk at an early stage (Wolff, Zdrahal, Herrmannova, Kuzilek, & Hlosta, 2014) and empower teachers to effectively and proactively monitor and support their students before they fail, over and above existing practices (Herodotou et al., 2019a). What remains a challenge is the degree of teachers' engagement with PLA. There is a great variation in the uptake of PLA by teachers; some teachers were found to systematically engage and act upon student data while others used it randomly, rarely, or not at all (Herodotou et al., 2019a, Herodotou, Rienties, Boroowa, Zdrahal, & Hlosta, 2019b; Hlosta, Zdrahal, Bayer, & Herodotou, 2020; van Leeuwen, 2018). A certain degree of engagement by teachers - between 10% and 40% of the length of a course - spread throughout a course presentation was shown to relate to was better student learning outcomes (Herodotou et al., 2019a). In this study, we build on our experience of using PLA at The Open University UK over the last four years, to identify the factors that best explain teachers' adoption and use of PLA as informed by the Unified Theory of Acceptance and Use of Technology (UTAUT) (Venkatesh, Morris, Davis, & Davis, 2003; Venkatesh, Thong, & Xu, 2016), and examine whether a specific intervention could reinforce systematic engagement of teachers with PLA throughout a course presentation. The intervention consisted of six email reminders sent by ateachers' manager to each individual teacher with tailored information about PLA. Through 11 semi-structured interviews and log files analysis of teachers' actual use of PLA, we answered the following research questions (RQs):

RQ1: What are the factors explaining teachers' engagement with PLA, as informed by UTAUT?

RQ2: What is the impact of email reminders on teachers' engagement with PLA over the duration of a course presentation?

In the next sections, existing studies about the use of PLA and motivational interventions are discussed (See Section 2 Teachers' acceptance of predictive learning analytics, 3 Motivational interventions). The PLA system used in this study is presented in Section 4. Section 5 describes the methodological approach of this study and Section 6 the data analysis. Findings are discussed in Section 7 and conclusions are drawn in the last section of the paper (Section 8).

2. Teachers' acceptance of predictive learning analytics
Bringing innovation in educational practice is rather challenging. There is often resistance, or lack of willingness, to adopt any change that can alter the current status quo (Rienties et al., 2014). In higher education, this is often due to an established organisational culture and long lasting positions of staff (Chandler, 2013). An innovation can be adopted when there is support from both the macro (senior management) and micro (teachers) level of use (Piderit, 2000). Should the two levels work well together, the risk of introducing a technology which is never embedded into the actual teaching practice is minimised. In terms of PLA, some institutions follow a top-down approach to adoption, while others a bottom-up one by consulting with practitioners (Dawson et al., 2018). Yet, the adoption of PLA remains limited and on a small scale, and this is explained by several factors including a lack of evidence of effectiveness within the institution, identification of specific student support interventions, effective communication across stakeholders, inclusion of teachers in the process of adoption, allocation of managerial time to enable the process of adoption, understanding PLA as complementing rather than replacing teachers (Herodotou et al., 2019c), teachers' pedagogical conceptions and digital literacy (Herodotou, Rienties, Boroowa, Zdrahal, & Hlosta, 2019b, Herodotou et al., 2020a) and a lack of agile leadership that can transform innovations into mainstream operation (Tsai, Poquet, Gašević, Dawson, & Pardo, 2019).

The degree to which teachers accept a technological innovation can have considerable impact on the adoption and use of a new system. The Technology Acceptance Model (TAM) explains acceptance in terms of perceived ease of use and perceived usefulness (Davis. 1989). In the case of PLA, while teachers perceived a PLA dashboard as easy to use, useful and complementing their practices, they exhibited a low usage of the system over time, suggesting that perceived usefulness and ease of use were not sufficient conditions to enable systematic engagement with PLA. Authors explained this pattern by teachers' non-permanent and part time teaching contracts and holding other full time occupations that they may have influenced how they view teaching and the need for innovation (Herodotou et al. 2019b). Building on this line of work, in this study we draw from the Unified Theory of Acceptance and Use of Technology (UTAUT) (Venkatesh et al., 2003; Venkatesh et al., 2016) which builds on TAM, as a framework that could provide additional insights about whether and how teachers engage with PLA over time. UTAUT was informed by eight models and theories about technology acceptance (e.g., TAM, the theory of reasoned action, the theory of planned behaviour). It is structured on the assumption that users' reaction to technology influences their intention and actual use of it. Empirical examinations showed that UTAUT can explain 70% of the variation in the intention to use a technology - an improved variation compared to each of the eight models UTAUT was structured upon, including TAM (Venkatesh et al., 2003). The dimensions of UTAUT were used to inform the design of the interview protocol and guide the process of data analysis of this study. Yet, this was not a direct application of the model as factors such as age and gender, included in the model, and their influence on adoption have not been examined due to the qualitative character of the study, whereas social influence was examined in relation to teachers’ communication with a teacher manager and not, for example, communication with colleagues and students about PLA.

In UTAUT, three factors were shown to directly determine intention to use technology: (1) Performance Expectancy refers to whether the technology can bring up gains in job performance, (2) Effort Expectancy refers to how easy is to understand and use the technology, and (3) Social Influence refers to how significant others may view users after having used the technology. Two factors were shown to indirectly determine intention to use: (4) Self-efficacy refers to using the technology without support from others and (5) Anxiety. Self-efficacy and anxiety can influence effort expectancy (i.e., perceived ease of use) but not the intention or actual use. In addition to that, two factors were shown to determine actual usage of technology: (6) Intention to use the technology and (7) Facilitating Conditions i.e., whether organisational and technological infrastructure is in place to support the use. Age, gender, experience and voluntariness of use were shown to moderate the influence of the three determinants (see 1, 2, 3) on intention and actual use. In this paper, we examined through 11 semi-structured interviews and log files, whether the above factors could explain PLA usage, and whether social influence in particular, expressed in regular email communication between teachers and a teacher's manager could alter the degree to which teachers used PLA.

3. Motivational interventions
Motivational interventions such as emails, phone calls and texts have been successfully used to enhance student engagement and retention (Herodotou et al., 2020b; Wigfield & Wentzel, 2007). Students with a low probability of completing their studies were found to have better retention outcomes compared to a control group, after receiving a targeted intervention by the student support teams - in the form of a text followed by a phone call or email (Herodotou et al., 2020b). In this study, a text message informed students about an upcoming phone call. A phone call or email asked students the following questions: How do you feel about starting? Do you have concerns? Do you know where to look for help? Similarly, sending out emails once a fortnight was related to a 2,3% increase in student retention in the intervention group (Inkelaar & Simpson, 2015), while automated text message reminders facilitated completion of a form required to enter college and students reaching out for support (Castleman & Page, 2015). Yet, in other cases, tailored emails to at risk students had no effect on dropout rates (Borrella, Caballero-Caballero, & Ponce-Cueto, 2019), while student demographics, rather than calling students and providing support, were shown to explain student retention (Dawson, Jovanovic, Gašević, & Pardo, 2017).

In this paper, we tested an email intervention with teachers, rather than students, in particular we explored whether sending out reminder emails to teachers could prompt them to check PLA in a systematic manner. These emails were sent by the teacher's manager, the role of whom is to develop, and support the teachers to deliver a good student experience. Social influence has been shown to affect human behaviour. In UTAUT, one of the three direct determinants of technology acceptance is social influence, suggesting that how significant others may view users after they have used a technology can influence its use. Also, PLA research suggests that social influence can have an impact on the process of adoption, in particular, the involvement of Faculty representatives with PLA and teachers championing the use of PLA were shown to facilitate adoption and scale up uptake (Herodotou et al., 2020a). These insights suggest that direct interaction and communication between a teacher's manager and teachers about PLA could potentially promote systematic use of PLA throughout a course presentation. The teacher's manager may have a greater influence on the teaching practice than, for example, a teacher's colleague who is promoting the use of predictive analytics, and this is due to its role to oversee and review the teachers' performance. This is the assumption we are testing in RQ2.

4. The Early Alert Indicators dashboard
At The Open University UK, PLA have been operationalised through two systems the Student Probabilities Model (SPM) and OU Analyse (OUA). The Student Probabilities Model (SPM) produces predictions or probabilities generated through logistic regression of a set of 30 explanatory variables, grouped in: student factors (IMD area, price area, disability etc.), students' previous study (highest qualification on entry etc.), course factors (total credits studying in a year, late registration etc.), students’ previous progress at the university (best previous score, number of fails etc.), and course and qualification variables (Calvert, 2014). The SPM produces “long-term predictions” about students' course completion, pass, and returning the next academic year. It has been mostly used by student support teams to identify students with a low probability of success and target resources and interventions (Herodotou et al., 2020b).

OOUAUA OU Analyse (OUA) produces weekly “short-term predictions” as to whether a student will submit their next assignment and their banded grade, based on machine learning algorithms. These predictions are based on demographics, previous study, as well as weekly students’ interactions with the Virtual Learning Environment (VLE). They are hosted in the OUA dashboard, visualising predictive information about who is at risk for individual students, VLE engagement and assignment submission rates at the cohort level. A traffic light system showcases: (a) in red students at risk of not submitting their assignment, (b) in amber students with a moderate probability of not submitting, and (c) in green those who are likely to submit and be successful. The OUA dashboard has been mainly used by teachers to warn them about students who may need help and support and enable proactive action (Herodotou et al., 2020a).

Predictions from SPM and OUA have been recently combined into the Early Alert Indicators (EAI) dashboard, which is accessible by a range of university staff including teachers, course team members, educational managers, and student support teams. The EAI dashboard is customisable enabling university faculties to select which features of the dashboard to activate, in particular the activity of students in the VLE, Student Probabilities only, OUAO only, and the combined view. The EAI is accessible via a link embedded into the teachers' homepage. Fig. 1 shows a summary view of students attending a course, their details, and their short-term and long-term probabilities of success. Fig. 2 shows an individual view of predictions for a specific student, their engagement with the VLE, predictions for each Teacher-Marked Assignment (See TMA) and factors explaining these predictions as well as their prediction history. While in our previous work we examined the effectiveness of the two systems separately, this is the first study to examine how the combined dashboard is used and perceived by teachers.

Fig. 1
Download : Download high-res image (818KB)
Download : Download full-size image
Fig. 1. The Early Alert Indicators (EAI) dashboard combining short-term and long-term predictions about student performance (summary view; student PIsPI are not real).

Fig. 2
Download : Download high-res image (741KB)
Download : Download full-size image
Fig. 2. The student view of predictions on the Early Alert Indicators (EAI) dashboard (anonymised data). The top slider and top legend enable manipulation of the different graph elements that break down the graph and facilitate understanding.

5. Methodology
A convergent parallel design to combining qualitative and quantitative research data (Edmonds & Kennedy, 2016) has been adopted in this study. Qualitative data collected from 11 semi-structured, in-depth interviews with teachers were combined with log file data capturing teachers’ activity while using while the dashboard. This approach enabled the collection of different, yet complementary data, that informed our understanding of the phenomenon under study. All data were collected individually and analysed consecutively to answer the proposed RQs.

a.
Sample

Participants were 11 teachers (7 male and 4 female) (out of 15) teaching on a business and law course at an online and distance learning university. The tuition model at the university under study allocates one teacher to every 20 students in a course. That teacher is responsible for assessing the students' work, allocating final marks, moderating forum activity and delivering synchronous online sessions at certain points during the lifecycle of the course. These sessions are used to clarify concepts and answer questions about the course material. Participating teachers had between six to 20 students in their groups (20 is the maximum number allowed per group), with an average of 11 students in each group, and teaching in total 120 students. Nine teachers have been teaching the course for ten years, while two others for five and seven years respectively. The teachers’ manager who delivered the intervention was responsible for the overall effective delivery of the course and ensuring a great student experience. Participating teachers were reimbursed for taking part in the interviews.

b.
Course under examination

The Business and Law course under examination is a Year 2 undergraduate course that started in Sept 2018 and lasted for 37 weeks. Students were required to submit five assignments in the dates shown in Fig. 3 (see blue bars). These assignments weighted 50% of the final mark. The weight of each assignment towards that mark was as follows: assignment 1 10%, assignments 2 and 3 15% each and assignments 4 and 5 30% each. An exam weighted for the remaining 50% of the final mark.

c.
Intervention

Fig. 3
Download : Download high-res image (520KB)
Download : Download full-size image
Fig. 3. The usage patterns of participating teachers throughout the course duration. Orange lines denote the dates an email was circulated and blue boxes the respective week. Blue bars indicate the deadline for submitting an assignment. Orange boxes are Christmas and Easter holidays; low activity was expected. (For interpretation of the references to colour in this figure legend, the reader is referred to the Web version of this article.)

The teachers' manager sent out six emails to participating teachers between October 2018 and May 2019. These emails were sent a few weeks before the submission of an upcoming assignment (see blue boxes with dates in Fig. 3). The fifth email was informing teachers about the interview process and asking them for their availability to take part. The rest of the emails were providing personalised information to teachers, in particular, informing teachers about the number of students in their group under each probability band and advising them to act on that. Teachers should access the dashboard to find out who those students are and provide support accordingly (see Appendix 1).

d.
Methods of data collection

Data were collected through semi-structured interviews and log files. The interview schedule (see Appendix 2) used in this study has been informed by the UTAUT questionnaire (Venkatesh et al., 2003) and builds on the interview schedule used in previous studies with teachers (Herodotou et al., 2019b). Interview questions asked teachers about background information of the course they teach on, whether and how they used the dashboard, whether they perceive it as useful, their response and reaction to the email reminders, concerns about the dashboard and plans for future use. Interviews were conducted by the first author who had no direct involvement with the course or any relationships with teachers. Interviews took place online via Skype on a date/time of convenience to each individual teacher. In addition to that, we extracted log files from the EAI dashboard that captured the usage patterns of participating teachers throughout the course duration to identify whether there was any change in the frequency they accessed the dashboard after they received an email. This dataset complement the interview accounts.

e.
Process of data analysis

Interviews were transcribed by a professional service at the university under study and were entered in nVivo for thematic analysis (Boyatzis, 1998; Kvale, 1996). In particular, the first author analysed independently three transcripts and identified emerging themes related to the research objectives of this paper. These transcripts were also coded by the second and third authors the, to ensure inter-rater reliability. This was calculated by dividing the number of times both researchers agreed by the total number of times coding was possible (Boyatzis, 1998) reaching 90% agreement. The instances of disagreement were discussed and agreed on. The remaining of the transcripts were split and coded by the first three authors THE using the agreed coding framework (see Table 1). Data from log files were visualized into frequency graphs showing teachers' patterns of engagement with the dashboard.


Table 1. Themes emerging from the thematic analysis of 11 interviews with teachers.

Themes	Subthemes
Email reminders	
●
Effectiveness of proposed intervention

Dashboard features	
●
VLE engagement

●
Comparison between years

●
Long-term predictions

●
Short-term predictions

Data literacy	
●
Understanding of the dashboard features

Perceived usefulness of dashboard	
●
Provision of additional insights

●
Prompting proactive and timely support of students at risk

●
Systematizing existing monitoring practices

●
Confirmation of suspicions

●
Teaching approach

Training	
●
Support needed to use the dashboard

Future use	
●
Plans to use the dashboard in the future

●
Suggestions for changes to the dashboard

6. Findings
In Fig. 3 we present the usage patterns of participating teachers throughout the course duration (37 weeks). The graph shows that Participant 6 (33 weeks) and Participant 8 (24 weeks) were the most active checking the dashboard most of the weeks of the course presentation. Participants 4, 5 and 9 presented a rather average pattern of engagement ranging between 14 and 20 weeks, with the rest of the participants accessing the dashboard between 1 and 8 weeks. The degree of using the dashboard tails off the more the course progresses, with less activity observed after the first few weeks of the course. The lower degree of engagement towards the end of the course could be explained by teachers communicating with students in previous weeks and becoming more knowledgeable of their strengths and weaknesses.

In Fig. 4, a summative presentation of usage patterns is presented capturing the number of teachers accessing the dashboard on a daily basis. The dotted lines are indicating the dates emails were circulated to teachers. There are clear spikes in usage when emails were circulated. The usage was rather flat prior to sharing the last two emails. In the next sections, we provide an analysis of the interview themes that will illuminate these trends and determine whether any increase in usage could be explained by the proposed intervention (email reminders).

Fig. 4
Download : Download high-res image (372KB)
Download : Download full-size image
Fig. 4. Usage patterns of all participating teachers. Blue dotted lines indicate the dates the six emails were circulated. Orange dotted lines indicate Christmas and Easter breaks, justifying low teacher activity. (For interpretation of the references to colour in this figure legend, the reader is referred to the Web version of this article.)

6.1. Effectiveness of proposed intervention (email reminders by a teachers' manager)
Participating teachers reported that they had read the emails sent by their manager, yet their response to them varied from prompting further action and checking the dashboard to taking no action, mostly due to already using OUA. In particular, most of the teachers reported using OUA already and therefore the emails they received did not prompt any further action. As explained: “For me, who was using the Dashboard personally on a reasonably regular basis from the beginning, prompting me to have a look at it, I was already looking at it, so perhaps not so helpful from that perspective” (Participant 6). The log files analysis showed that Participant 6 was the most active teacher accessing the dashboard 33/37 weeks of the course. Another teacher explained further: “I was aware that it was there anyway […]. I tutor seven courses and I've got six groups of apprentices. So, I am on this system all day” (Participant 9). A systematic pattern of activity was observed for Participant 9, especially during the start and middle of the course presentation, yet this became less systematic towards the end of the course. In particular, the activity tailed off after week 23 of the course, with the exception of two weeks towards the end of the course.

Other teachers perceived the email reminders particularly useful in prompting them to check the dashboard frequently or even changing their own established teaching practices, showcasing the importance of reminders to get teachers into the habit of checking the dashboard or embedding it in their existing practices. As explained: “[the emails] acted [as] a reminder.” (Participant 10). The activity of Participant 10 shows to align with the email circulation in at least three occasions (first, third, fifth email). Another teacher noted that the emails were really helpful for they enabled a change to their current teaching practices and in particular how students have been monitored and supported. This teacher was active during the start and the end of the course, but rather inactive in between:

Interviewee: “With a teacher like myself who's been around for quite a while, you do get into your own way of doing things […].So, with something new, you think, well, I'm not sure what use this is to me, so I'll maybe just carry on as normal. So, the fact that [my manager] was raising the profile was helpful, I think, in getting people like me to actually interact.”

Interviewer: So, you are saying that because of the emails, you looked into the dashboard?

Interviewee: Yes, definitely. Yes.

Interviewer: So, did this make you look into it more often?

Interviewee: It definitely did. And, to be honest, I can't say I used it so much for the other courses because there was nobody doing the same thing.” (Participant 2).

Some other teachers, while they read the emails, they took no action in response to that and this was shown to relate to their perceptions about the usefulness of the dashboard. While they acknowledged the added value of the dashboard, they continued with their established teaching practice: “You can zoom down on an individual student level, but you can also have the full picture at group level […] it offers a more complete view. But the size of student groups is anyway never over 20 […] I don't think that it gives me any significant help in doing the job differently. [In response to the emails] I didn't go back to the dashboard.” (Participant 3). This insight is confirmed by the very limited usage of the dashboard by Participant 3 who accessed it in only two occasions at the start of the course. This teacher is found to proactively get in touch with students: “In the early stages of the presentation, I try to contact those who do not come live on the forum or at tutorials”. Yet, this is not always successful: “I've got students who never came live, never replied to emails, were never accessible on the phone”, suggesting that the dashboard could potentially provide insights as to how these students are engaging with the course material that could not be accessed otherwise. Finally, one other teacher replied to the emails and discussed with their manager the content, yet their usage of the dashboard was restricted to the start and end of the course: “I wrote back and confirmed what she was saying or I was saying […] So yes they were useful.” (Participant 7).

6.2. Dashboard features
Participating teachers commented on specific features of the dashboard, in particular the VLE engagement, the student comparisons to the previous year, the long term and short-term (weekly) predictions, and their perceived usefulness in terms of supporting their practices.

a)
VLE engagement

Some teachers viewed the VLE engagement graph as very useful. One teacher commented on how insights from this graph saved them time: ‘That's useful to see in a visual way, whereas I used to log those myself […] So that saves me some time.‘(Participant 7). Yet, Participant 7 had a rather limited interaction with the dashboard mostly at the start and the end of the course. Another teacher explained how it informed action: ‘I'm looking at one of my students now who I'm a bit worried about because I know he's lost his job. I can see he's been flatlining for the last couple of weeks. He peaked to get something done for his assignment and he's not done anything […]I've actually sent him stuff because I can see he's not been accessing the tutor group forum.’ (Participant 5). Participant 5 was particularly active during the start, middle and end of the course, and this more likely explains the fact that she managed to identify and support that student. Another teacher added: ‘The feature that I use is looking at the pattern of activity of an individual student versus the average […] And I use that graph then to […] inform a discussion’ (Participant 9). This participant was rather systematic in accessing the dashboard throughout the course presentation, explaining the fact that s/he developed a specific approach to interacting with dashboard features.

Yet, some other teachers questioned the usefulness of the VLE data: ‘Not really helpful for me because I was on top of what was happening so there was no surprise there […] What was interesting was the predictions.’ (Participant 4). Despite the fact that Participant 4 reported being on top of what students were doing, s/he did check on the dashboard in a rather systematic manner and throughout the course presentation. One teacher explained how they thought the VLE data worked to inform the predictions: ‘I can't say that I'm interacting with that so much, but that's actually what has helped me […] So, obviously, that needs to be done on the dashboard in order to make the red flag.’ (Participant 2). This insight stresses the need for providing explanations of how the predictions are generated that can inform teachers' understanding of the dashboard. Aligning with this insight, the most recent version of the dashboard lists the factors that contribute to a prediction for each individual student, such as low VLE activity or non-submission of an assignment.

b)
Comparisons to previous year's performance

Some teachers reported that the comparison feature between years provides insights about how a current cohort of students may compare to that of the previous year: ‘When the graph went up in 2017, it went up in 2018, so it was quite consistent. But it showed 2018 was doing better than 2017, […] that explains […] I had a difficult group last year.’ (Participant 8). Another teacher explained observed differences between years due to the marking process, rather than differences between cohorts: ‘I quite like the fact that you get the average assignment assign score for the previous year, and therefore you can […] judge in terms of your marking where you sit.’ (Participant 2). The different interpretations of the data raise the need for debriefing teachers as to how these differences could be explained, by for example, adding relevant information to the dashboard.

c)
Long-term predictions

The long term predictions were not favoured by some teachers with one participant not understanding the value of them: ‘I can't see what the value of that is really. What are they supposed to be used for?’ (Participant 9). Another teacher questioned the reliability of the long-term predictions: ‘The prediction about whether they're going to finish the course is a bit far into the future […] sometimes it's an 80%–100% chance of passing and yet their assignment scores have been consistently low, and they probably will struggle with the exam […] So, I think it's predicting quite high” (Participant 8). Others discussed the long-term predictions in relation to the short-term (weekly) predictions: “I can't really impact on [the long term predictions], whereas I can impact on the next assignment A prediction’ (Participant 2).

d)
Short-term predictions

Teachers explained the usefulness of short-term (weekly) predictions: ‘What I seem to have used is the next assignment predictions, it's almost like I'm taking one step at a time. I'm dealing with the current issues.’ (Participant 2). The colour coding of student predictions in green, amber and red, helped quickly identify students who might need support: “The reds jump out at you, the person's not contributing, there's something going on here.’ (Participant 2). Participant 2 was active at the start and towards the end of the course presentation only, with a long period of inactivity in between.

6.3. Data literacy
Some teachers appeared to be unclear about what the different parts of the dashboard were indicating even after attending some training: ‘the bit with the little line graph that goes up and down, I can't say that I really understand how to use that so much. But I think that's what's inputting to help me with the colour codes down below’ (Participant 2). Another one explained: ‘I didn't use so much of the filters down below, I did try to do that at the outset having had the training and some of that was a bit harder for me to do’ (Participant 6). This suggests that current training may not be adequate in supporting teachers' understanding of the dashboard and that explanations of the different dashboard features should be embedded into the design of the dashboard to facilitate understanding while using it. These insights are further reinforced when participants talked about their training needs (See Training section).

6.4. Perceived usefulness of the dashboard
The usefulness of the dashboard was discussed in relation to: the provision of additional insights, prompting proactive and timely support of students at risk, systematizing existing monitoring practices, confirming suspicions, and relationship to existing teaching practices.

a)
Provision of additional insights

Teachers commented on the potential of PLA to provide additional and regular information (on a weekly basis) they could not have access to otherwise and which could help to build a more complete picture about students, especially when teaching in online settings. As explained: “You have always wondered what your students are actually doing. The dashboard gave us a chance to see whether or not they were going online engaging with the course materials” (Participant 5). Others stressed the importance of being able to access historical information about a student that can inform the teaching practice and raise potential issues: “Even if the student doesn't attend the tutorial, you can check what they've done and if they're engaging or not […], then you know where you need to quickly intervene” (Participant 11). Other teachers use insights from the dashboard to tailor their support to individual students and prompt them to study throughout the course, and not only before an assignment submission: “I will say to students: you must be consistent, you must study everything […] It did inform that kind of feedback to students to say “look, you've done well in this assignment, but do make sure that you study everything because you've got an exam at the end of this” (Participant 5).

b)
Proactive and timely support of students at risk

The dashboard flagged students at risk a teacher was not aware of and prompted an intervention with a positive impact: “The dashboard said that the last assignment was going to be red, not submit. So, because of that, I wrote out to them, but I might not have noticed because this person had submitted everything else on time. […] And they had actually been struggling and were going to not bother submitting because they thought they wouldn't be able to do a good enough job. So, I had a chat with them and encouraged them to submit something rather than nothing […]. I was really pleased with myself because the dashboard had said red, this person won't submit, and then with that little bit of contact, they did” (Participant 2).

Teachers valued in particular the predictions of students they had no other information about. They perceived it as their responsibility to get in touch with a student flagged as at risk: “It gives you an early indicator something may be going wrong […]. It may be the case they are absolutely on track […] maybe they're on holiday. But since the system shows a specific student as being red or amber, it's an indication that you should explore it a bit more just to make sure that the student is on track with their studies” (Participant 9).

c)
Systematizing existing monitoring practices

Teachers noted that the dashboard made their practices more systematic and more regular; even for those teachers who had developed practices of monitoring and checking on their students, these were not as formalised or regular as accessing student information via the dashboard. The dashboard provided a structure which teachers could follow to regularly monitor their students: “It's made my analysis [more] formalised if you like […] I think it's useful to see [‥] those students are on the greens or the yellows and they've got one in the red. But I knew the one in the red from early on […] It has made a difference in my analysis, if you can see the technical part. It's made it more formative. I look at it regularly. I send through a monthly update myself and so on.” (Participant 7). Participant 7 was found to be systematic at the first few weeks of the course, yet s/he was inactive for the rest of the course with the exception of three weeks.

Also, PLA was perceived as a timely and faster way of identifying students at risk than relying on other sources of student information such as the submission (or not) of an assignment: “The teacher notices there's something wrong when that has happened [ …], when they've not submitted an assignment […]. The dashboard has helped me to see that coming and maybe just get an email or text message out to them sooner than I might have done otherwise[…] The dashboard definitely helps predict a little bit quicker than I could myself because I would have to sit and actually look at everybody's login, and really track people very closely” (Participant 2).

d)
Confirmation of suspicions

PLA were found to confirm or reassure teachers' perceptions or suspicions of students who may be at risk of failing. As stated: “I think it is a nice, sophisticated piece of kit. But I am not sure at the end of the day it tells me anything more than I didn't know” (Participant 1). Some of the teachers have developed their own ways of recording and monitoring students' progress that allow them to have an understanding of what their students are doing online. Over the years they teach, teachers have developed an awareness of who of their students may be at risk due to specific characteristics such as doing multiple courses at once or working and having a family at the same time. As explained: “And over the years you get to know that if somebody's doing two or three courses at the same time and they're not studying full-time, that they've actually got maybe a family and a job, that they could very well be in difficulty unless they're really, really, really bright. I tend to make notes of things like that […] The analytics list pretty much confirmed roughly what I knew” (Participant 4). For other teachers, this awareness was coming from contacting students frequently: “I already knew about them because I contact students quite frequently […] but it's good to actually see the system doing the predictions and predicting what I think will happen anyway.” (Participant 8). In other cases, some teachers were more knowledgeable of the dashboard as they were those allowing an extension to the submission of an assignment, information which at the moment cannot be tracked by the predictions: “The only inconsistency would be in the opposite direction, the system would say that they're not due to submit and I will know I've given them an extension” (Participant 9), or they were aware of personal issues the dashboard could not capture and that could inhibit the submission of an assignment. As explained: “Probably my brightest student, very hard-working […] Her father died […], and it hit her mother quite hard […] it affected [the student]. She […] decided to drop the course. There's no way that an analytics tool can tell you that” (Participant 4).

e)
Teaching Approach

How teachers see their role as a teacher was found to relate to how they perceive the use of the dashboard. Teachers see themselves as the content experts that can help students develop their understanding of the course material and “apply [that understanding], both to the case studies in the course but also to their practical work experience” (Participant 6). They use their expertise to “show [students] how the content they're studying lines up with their requirements for an assignment” (Participant 5). Also, they see themselves as being responsible for supporting students' participation by: “monitor[ing] the forum activity pretty much every day so I know who's contributing and who isn't” (Participant 4) or “being either online or face-to-face […] for student support” (Participant 7).

Yet, the degree to which teachers interact with students varies. Some teachers are particularly proactive: “They get reminders, they get emails, they get suggestions from me and feedback in the tutor group forums” (Participant 6) and they see that as part of their role: “I've chased them, because that's part of my job” (Participant 1). Some teachers noted the importance of building a relationship with students right from the start of the course: “so that they can contact me anyway and let me know if they're struggling rather than worrying and not being in touch” (Participant 8). Given a rather proactive approach to engaging with students, they state about the dashboard: “It's only of any use providing one shows a proactive approach to the learning experience throughout” (Participant 1).

Other teachers have adopted a more pragmatic approach in terms of their role being to help students pass the course: “Consider where they need help with the assignments […], where they need help with revision, particularly up to the exams” (Participant 8). Hence, some of them have developed their own ways of monitoring student progress and participation that enables them to “see those who are contributing, those who are not, and those who I think need more help” (Participant 7). Within this context, the dashboard is seen as an improvement to their own activities: “The analytics does make it easier for me to pinpoint what's going on” (Participant 11).

6.5. Training
Training as to how to use the dashboard was offered to teachers before the start of the course. Most of the teachers attended that training, yet not all of them found it adequate in terms of assisting them with using and understanding the dashboard. As explained: “[the training] was useful to make people know that this support, this tool would be available, and it provided an overview of the tool. As a tool, I don't think it is difficult to use […]I think it is user-friendly enough” (Participant 3). Yet, another teacher raised issues about the timing and the content of that training: “I did have training and it was very good. But then […] you end up using certain bits and maybe forgetting what the point of the other bits is. So, I think it would do no harm to have a refresher […] a group who have used it, and then you could hear from other tutors what they mostly used and why, and how it was helpful” (Participant 2). The provision of support would be more effective should this be offered after teachers had access to their student data: “at the time of the training we didn't actually have the data available to us, all very much on the dashboard. So, it would have been better if it had been a few weeks later after the system had gone live” (Participant 2). Also, training support would be more effective if it provided teachers with example case studies of how a teacher has interpreted the graphs, intervened and saved a student: “The only thing that I think would be helpful is maybe having some sort of case study information, seeing the dashboard highlighted this which led to me contacting X, which led to this. And then that student says how that helped him complete the course. “ (Participant 10). Others who did not attend the training found it easy to use the dashboard, yet certain aspects of it were not understood: “Did you attend the training […] it's all clear, really. The only thing I wasn't clear about was the graph things […] about if it was my own group and not the cohort as a whole.” (Participant 8).

6.6. Future use of the dashboard
Teachers are generally positive in terms of using the EAI dashboard in the future as part of their standard practice. Yet, some support would be needed to achieve that: ‘I think I would suggest to all teachers it should be part of their standard practice to use it […] But I do think also it needs maybe line managers to also have a more regular discussion rather than just an email to a teacher as to how they've related to it’ (Participant 7). Also, teachers commented on a number of specific benefits of using the dashboard in the future, including supporting large numbers of students: ‘If I had a very high number of students […] yes, it may help make the whole process of keeping an eye on what happens in the group easier, or more manageable’ (Participant 3). A few teachers had some reservations about using the system in the future that were related to their understanding of the dashboard features. In particular, they had concerns that some key information was missing such as not counting clicks when students use their mobile phones to access the course or that submission extensions may not be picked up stating: ‘They need to be” (Participant 9), suggesting that explanations are needed to support teachers' understanding of the dashboard features.

Participating teachers suggested that the dashboard could be improved by recording submission extensions and any student withdrawals: “the extension periods should be submitted into the dashboard so that it was telling me that they haven't submitted”(Participant 6). Also, it would be beneficial should it record information about what content students are engaging with when online: “The dashboard will tell me if they're actually using the website, but it's not giving me any information about what they're doing” (Participant 5). Finally, they raised the need for explaining how certain predictions are generated: “It would be nice to be able to click on it and perhaps see behind the reasons a little bit more” (Participant 7).

7. Discussion
In response to RQ1, several factors were found to explain teachers' engagement with the EAI dashboard. Drawing from UTAUT, performance expectancy was a major factor determining whether a teacher would make use of the dashboard. Teachers reported varied benefits from using it including (a) the provision of additional and regular information (on a weekly basis) about what their students are doing online, and which they could not have access to otherwise. This helped teachers develop a more complete picture of their students. For example, they could identify whether students are engaging with the course material on a weekly basis. This is particularly useful when students do not appear in a scheduled event, such as a synchronous online tutorial, or when teachers want to nudge inactive students to study in order to pass the course exams. (b) The provision of proactive and timely support to students at risk. Teachers highlighted cases of students the dashboard flagged as at risk. This information prompted them to make contact and provide support, enabling students to submit their assignments and succeed. (c) Systematizing existing monitoring practices. For some teachers who have developed their own ways of monitoring students, the dashboard made their practices more structured, regular, and systematic. Also, it made it easier to access information about students by checking the dashboard than other resources with student data, and faster to intervene. For example, the submission (or not) of an assignment is an indication of whether a student is progressing. Yet, this information is made available to teachers after a student has submitted an assignment, resulting in a reactive rather than a proactive response by teachers.

Yet, some other teachers perceived the dashboard as confirming or reassuring what they already knew about their students and therefore, they did not find it particularly useful. These teachers have developed their own practices of monitoring and identifying students at risk such as checking on whether students are attending multiple courses at the same time or by having frequent communication with them. They also noted that in some cases they are more knowledgeable than the dashboard as they have information the dashboard cannot capture including personal issues a student may be facing or extensions to a submission. This aligns with studies (Hlosta et al., 2020) explaining errors in predictions by factors including for example personal and financial difficulties students are facing and haven't declared to the university. Overall, the perceived usefulness of the dashboard was found to relate to the teaching approach of individual teachers. Aligning with existing studies ((Herodotou et al., 2020a, Herodotou, Rienties, Boroowa, Zdrahal, & Hlosta, 2019b)), proactive teachers were shown to recognise the value of the dashboard as a means to provide timely support to students or, as a means to improve their own monitoring practices.

In terms of effort expectancy, teachers found it easy to interpret and use dashboard insights in their practices. They commented on different dashboard features they engage with including the VLE engagement graph, the students' comparisons to the previous year, and the long-term and short-term (weekly) predictions. Despite teachers' self-efficacy and lack of anxiety in using the dashboard, issues of limited data literacy, that is understanding of how the different features of the dashboard function, were raised. These issues were related to a lack of understanding of how predictions are generated, how to interpret the between years differences in student engagement, and the interpretation of dashboard features such as the VLE engagement graph. While most of the participating teachers attended the training offered before the start of the course, they raised the need for timing that training to when they have access to actual data from their students. This would enable them to apply what is learnt directly to their practices. Echoing existing studies (Herodotou et al., 2020a), the content of the training offered should be modified to present case studies that explain how a teacher interacted with the dashboard, what information was checked, and how they intervened to save a student. In addition to that, explanations of different features could be embedded into the design of the dashboard to enable understanding while using it.

These insights suggest that, for some teachers, while facilitating conditions were in place in the form of training and in presenting data in an easy to use dashboard, these did not suffice to enable them to understand or engage with the dashboard. Amongst the major challenges in the design of dashboard visualisations are the lack of data understanding, incorrect interpretations and confusion about the outcomes (Matheus, Janssen, & Maheshwari, 2020). Facilitating conditions are, according to UTAUT, critical in determining actual use of an innovation and therefore finding ways of promoting engagement and understanding should be further explored. A user-friendly dashboard should be coupled with a deeper understanding of the computational domain that would facilitate meaning making (Gibson & Martinez-maldonado, 2017). For example, shifting away from the “one size fits all” dashboard design approach, studies are exploring the use of storytelling elements to emphasize (or de-emphasize) certain dashboard features in an effort to assist teachers construct the narrative of a student's progress and facilitate understanding (Boy, Detienne, & Fekete, 2015; Echeverria et al., 2018).

In addition to facilitating conditions, intention to use the dashboard in the future is also directly related to the use of the dashboard. While the great majority of teachers were keen to use the dashboard in the future, they raised the need for support by a teacher's manager (e.g., discuss the dashboard data) and the provision of explanations of how the different features work. Some of them intended to use it only if they had to monitor a large number of students. Some others proposed improvements to it including recording submission extensions, student withdrawals and showcasing the content students are engaging (or not) with when online.

The second research question (RQ2) of this study was to explore whether email reminders could influence systematic usage of the dashboard by teachers. Email reminders with tailored content about the performance of students as captured by PLA were composed and circulated by a teacher's manager two to three weeks before the submission of an assignment, to allow teachers time to intervene, should any of their students be flagged as at risk. Drawing from UTAUT, the direct involvement of the teacher manager could be seen as a social influence that could have a direct effect on the intention to use the dashboard, and indirect effect, on actual use. Aligning with existing studies (e.g., Wigfield & Wentzel, 2007), for some teachers, the intervention was successful in terms of altering current teaching practices and promoting engagement with the dashboard. Yet, the pattern of dashboard usage of some of those teachers did not follow the email circulation, as expected, but it rather varied. For other teachers, the email circulation had no impact on their practices as they either have already been using the dashboard in their teaching and in a rather systematic manner, or despite recognising the added value of it, they carried on using their own established ways of monitoring students, confirming previous studies pointing to academic resistance (Herodotou et al., 2019b). In the case of PLA, it is important for teachers to check on PLA in a systematic manner and particularly before the submission of an assignment as this was shown to improve student performance (Herodotou et al., 2019a, Herodotou, Rienties, Boroowa, Zdrahal, & Hlosta, 2019b). Also, any personal issues a student may be facing, such as a sickness, cannot be captured by the dashboard (Hlosta et al., 2020) and therefore, it is up to the teachers to monitor PLA systematically and intervene with students at risk in a proactive manner by, for example, allowing an extension and encouraging students to work towards a deadline and make a submission.

8. Conclusions
In this study, we identified the factors that explain teachers' adoption and use of a predictive learning analytics (PLA) dashboard at a distance learning university and explored whether an intervention in the form of email reminders, sent two to three weeks before the submission of an assignment by a teacher's manager, would facilitate teachers' systematic engagement with it. We conducted 11 semi-structured in-depth interviews with experienced teachers from a business and law undergraduate course who received the emails, and tracked their engagement with the dashboard using log files.

Aligning with the Unified Theory of Acceptance and Use of Technology (UTAUT) (Venkatesh et al., 2003; Venkatesh et al., 2016) and studies examining the adoption of technologies by adults (e.g., Lai, 2018), we identified that amongst the factors facilitating use of the dashboard were (a) the recognition of the added value of using it in terms of making their current monitoring practices more systematic and structured, identifying students at risk on a weekly basis (through short-term predictions) and enabling proactive and timely support (performance expectancy), (b) the proposed intervention (email reminders) was found to influence existing monitoring practices and getting some teachers in the habit of checking the dashboard (social influence), and (c) ease of using the dashboard that saved teachers time when monitoring students' progress (effort expectancy).

Amongst the factors inhibiting usage were (a) a lack of recognition, by some teachers of the added value of using the dashboard to improve monitoring practices and support student performance (performance expectancy), (b) facilitating conditions related to training – the timing and content of which could be improved – and a deep understanding of the dashboard features, in particular how predictions are generated and how certain visualisations work. It could be argued that one factor not currently captured by UTAUT and found to explain adoption and usage of a PLA dashboard is established or habitual practices that inhibited some teachers from engaging with the dashboard and realising its potential. This phenomenon has been linked to academic resistance and found as explaining innovation adoption in relation to predictive analytics and the teaching practice (Herodotou et al., 2019b).

In this study, teachers' actions to support students' at risk of failing their studies, as shown on the dashboard, were not directly recorded by, for example, asking teachers to keep a diary of their actions. Yet, the interview data suggested that teachers intervened with students by providing feedback in relation to how and what to study in order to pass their course, explored whether they are on track with their studies and allowed extensions to an assignment submission. Our previous published work (Hlosta, Zdrahal, Bayer, & Herodotou, 2020) showed that teachers adopted various approaches to supporting students including referring them to the student support services, sending them an email or text or giving them a phone call. These approaches align well with existing studies showing that direct communication and actionable feedback are the most commonly used interventions in response to learning analytics data (Wong & Li, 2018) and studies showing positive impact of weekly teacher feedback to students (Cobos & Ruiz-Garcia, 2020). The provision of feedback is a critical component of teaching and learning and one that can help students improve their performance. A natural language processing analysis study (Cavalcanti et al., 2020) showed that good feedback practices are related to (a) the task; they should be relevant to the student and specific as to how to solve a problem, should indicate future actions for improving the student's performance (feed-forward), and explain the effects of not completing a task on learning, (b) the processes of solving a task; they should provide information about a wider scenario and not only the specific task, should be precise and offer causal explanations, and (c) about the self; they should motivate students using positive words and informal language. As a follow-up of this study, we now work with a group of teachers who have been asked to report the actions they take to support students at risk by completing a feedback form embedded in OUAOUA. These insights are expected to provide a good understanding of teachers' current feedback practices and help us design and promote interventions that build on principles of Agood quality feedback. Also, we are considering the development of added functionality on the dashboard in the form of automated email alerts sent to teachers when a student is flagged as amber or red, as a means of engaging teachers with the dashboard especially when there is a need to act in response to a prediction.

This study has examined in detail the PLA practices and perceptions of a specific cohort of experienced teachers teaching an undergraduate business course. Insights were rich pointing to a great variation in how the dashboard is used and teachers’ perceptions of usefulness, ease of use and understanding. Future studies should seek to expand this line of work by engaging with teachers from other courses and levels, less experienced and those managing large cohorts of students in order to collect additional insights as to the factors facilitating adoption and use of predictive analytics at higher education institutions. For example, it may be the case that teachers with less established practices of monitoring and supporting students would use the dashboard in a more systematic manner. Their lack of experience with students could motivate them to check it regularly and treat it as their main source of student information. Also, this study has examined the practices of teachers at a university where a teacher is responsible for the performance of a relatively small group of students. This may mean that proactive teachers can easily monitor students through direct contact (e.g., emails, phones). Access to predictive data would enable scaling-up of the process of monitoring to large numbers of students, that could not be achieved effectively, or would be overly costly, through other approaches (such as direct communication). This would be particularly beneficial to online learning settings such as MOOCs where there is no direct teacher facilitation and the number of students is significantly large. For other universities including the campus-based ones, predictive analytics could decrease variation in the approaches adopted or developed by individual teachers, thus ensuring that all students receive the same level of proactive support. Innovations such as a PLAdashboard should inform academic professional development initiatives and be translated into teaching policy, thus enabling systematic use.

One of the limitations of this study was that UTAUT was not faithfully applied to the study mainly due to the qualitative character of the data collection and the fact that no statistical comparisons could be made to examine the moderating effects of age, gender and experience on the use of PLA. Also, social influence was examined only in relation to influences from a teacher manager and not for example, influences from colleagues who have used PLA or students’ reactions to their data being used to support their studies. Future studies should seek to collect quantitative data from a large cohort of teachers in order to explore aspects of the model related to age, gender, experience, and other factors related to social influence.

Predictive analytics implementations should pay special attention to how teachers are supported to engage, understand and use a relevant dashboard. In particular, in this paper we presented one of the few large-scale, university-wide implementations of PLA and gained insights that can inform similar implementations at other institutions. Specifically, amongst the issues that can help the process of adoption are teachers' good understanding of how dashboard features function such as how predictions are generated and how graphs can be interpreted, the timing of training offered so it relates to real student data teachers have access to, and the content of training that should draw on case studies of how teachers interacted with the dashboard, what information they checked, and how they intervened to save a student. An examination of the conditions that facilitate a deep understanding of the dashboard and help to build a clear picture of the benefits of using it is needed. This should be tailored to the needs of a given institution, in particular consider and respond to specific teachers' and students' needs. This could be achieved by working closely with teachers to design and iterate a predictive dashboard, piloting and improving any training material offered, and allocating time to teachers' managers to pursue its use in a systematic manner and in communication with teachers. UTAUT would be a useful tool for identifying enablers and inhibitors of use. More fine-grained methods of data collection such as the use of eye-tracking could provide us with additional insights as to how a dashboard could be designed to promote understanding, while an examination of the leadership related to the use of analytics could identify ways of promoting systematic engagement with a predictive analytics dashboard.

