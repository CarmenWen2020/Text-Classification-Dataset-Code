The idea of students as authors of assessment items, in particular, multiple-choice questions, is being increasingly taken up by classroom teachers, and studies examining its educational potential have largely been positive. A concomitant question regarding if and how students benefit from the use of student-authored questions with answers and explanations for learning is currently understudied. Noting that the structures, types, and amount of knowledge between a teacher and students may be distinctly different, the purpose of this study was to examine whether there are different learning effects from the use of student- versus teacher-generated explanations for answers to online multiple-choice questions. Additionally, an attempt was made to determine the ways that explanations generated by students and teachers differ. A non-equivalent pretest-posttest experimental research design was adopted. Four sixth-grade classes (n = 104) from a single primary school participated in six weekly online multiple-choice question-answering exercises. Although the results of the analysis of covariance did not show statistically significant between-group differences in academic achievement, the participants receiving student-generated explanations had significantly better attitudes toward the subject matter as compared to their counterparts receiving teacher-generated explanations. A content analysis on the explanations generated by the teacher and students, followed up by chi-square tests of independence, post-hoc pairwise tests with Bonferroni correction, and t-tests further revealed significant differences in several areas, including types of explanations, styles, and number of words.

Previous
Next 
Keywords
Applications in subject areas

Interactive learning environments

Primary education

Teaching/learning strategies

1. Introduction
1.1. Value and sources of practice and feedback in educational contexts
Scientists in the field of psychology have endeavored to identify key factors leading to effective instruction that promotes learning in order to derive corresponding principles for teaching and learning. Among these, engaging students in practice activities matched with instructional goals has been suggested to be a powerful instructive element by which to assist learning (Center for Psychology in Schools and Education, 2015; Dick, Carey, & Carey, 2015). In the meantime, feedback following practice has been shown to contribute to the regulation of the learning process and cognitive engagement in the target areas on the part of the learner (Butler & Winne, 1995), which consequently helps improve understanding, knowledge acquisition, and skill levels (Hattie & Timperley, 2007; Shute, 2008).

Although the respective pedagogical value of practice and feedback is well-recognized, questions, assignments, and tasks to be completed during practice for formative evaluation purposes, as well as feedback that is subsequently provided regarding the correctness and appropriateness of the submitted answers, completed work, and performance, often come directly from textbook publishers or teachers. In other words, the power to design and develop practice and feedback, for the most part, resides in those in authoritative positions.

1.2. The transformative power of students as authors of assessment items and feedback
By leveraging learner-centered, active, constructive elements in the learning process, the idea of enabling learners to be knowledge producers (rather than the knowledge consumers they used to be) is being embraced by educators at various educational levels (Doyle, Buckley, & McCarthy, 2020; Snowball & McKenna, 2017). Studies involving learners producing instructional materials and learning objects have evidenced the prevalence and acceptance of such practices among the student population (Hubbard, Jones, & Gallardo-Williams, 2019; Orús et al., 2016). Among the various possible forms, the learner-generated assessment items approach has received increasingly widespread attention and adoption among educators and academics (Ebersbach, Feierabend, & Nazari, 2020; Song, 2016) especially with support from freely available online learning systems bolstered with well-rounded designs and integrated functions (such as the most frequently used platform for learner-authoring of multiple-choice questions, PeerWise) (Denny, Hamer, Luxton-Reilly, & Purchase, 2008). By directing students to generate questions and their corresponding answers around instructional content assessed to be important and relevant (Yu, Wu & Huang, 2018), student-generated assessments activities are characterized by learners’ self-directed, self-reflective, and self-regulated behaviors during the learning process (Hwang, Zou, & Lin, 2020).

Serving also as an alternative assessment format, learner-generated assessment items resonate well with the contemporary view toward ‘assessment as learning,’ as opposed to ‘assessment of learning,’ where a summative evaluation purpose is stressed (Earl, 2012; Fergus, Hirani, Parkar, & Kirton, 2021; Hargreaves, Earl, & Schmidt, 2002). Empirical evidence accumulated over recent decades has typically substantiated the positive effects of learner-generated assessment questions on the enhancement of knowledge, ability, skills, and attitude (Khashaba, 2020; Rosenshine, Meister, & Chapman, 1996; Rosli, Capraro, & Capraro, 2014; Song, 2016; Zuya, 2017). Recently, students have been found to be capable of generating quality feedback1 (hereafter referred to as ‘explanations,’ denoted as information provided to explicate if and why the answer given to the question is correct or incorrect) to complement the multiple-choice questions they generate (Glassey & Bälter, 2020). Studies also confirmed that students can benefit cognitively, metacognitively, emotionally, and socially from the act of generating explanations intended for use during online multiple-choice question-answering activities (Yu et al., 2018; Yu & Wu, 2020).

While numerous studies have reported the educational potential of learner-generated assessment items, and corroborating findings have recently been obtained supporting the learning effects of student-generated explanations (Yu et al., 2018; Yu & Wu, 2020), the question as to how learners will respond to the actual use of peer-generated explanations for learning is under-examined. This unanswered question warrants investigation when considering that the uptake of student-produced artifacts that are relevant for subsequent activities or events is a major theme in computer-supported collaborative learning, which plays a central role in current approaches to education (Suthers, Dwyer, Medina, & Vatrapu, 2010). In addition, research on experts and novices suggests that explanations generated by students are different from those generated by teachers.

1.3. Teacher explanations, their effects on learning, and research on experts and novices
There is a sizable body of literature on teacher explanations. Empirical evidence from the fields of cognitive science, educational research, and linguistics generally highlights the significant role teacher explanations play on facilitating learners’ knowledge acquisition, retention, and understanding (Tharby, 2018). Besides serving the primary cognitive function (e.g., clarifying ambiguities, improving understanding, etc.), affective functions (e.g., reduced anxiety, changes in attitude, etc.) can be attained by careful pre-assessment, planning, preparation, and presentation of teacher explanations (Brown & Armstrong, 1984). Based on research on this area, a set of strategies,2 as well as techniques, key principles and characteristics of effective instructional explanations (e.g., clarity, fluency, emphasis, interest, the use of examples, summaries, etc.) have been suggested as the core skills that should be acquired by teaching professionals (Brown & Edmunds, 2019).

Although there is an abundance of literature on teacher explanations, as well as research on the positive effects of explanations on learning, it remains unclear whether and how teacher and student explanations differ and what their comparative effects on learning are when integrated into multiple-choice questions. In an attempt to better conceptualize and understand this phenomenon, research on experts and novices is drawn upon.

There is widespread recognition that there are both quantitative and qualitative differences between experts and novices in their specific domains (LaFrance, 1989). When compared to novices, experts have been found to have more declarative, procedural, and conditional knowledge accumulated from prolonged engagement in a relevant field or domain that has been used to construct an extensive, integrated base of knowledge (Chi, Glaser, & Rees, 1982; Wolff, Jarodzka, & Boshuizen, 2017). Beside differences in the amount and types of knowledge they have acquired, how such knowledge is structured and organized by experts and novices is also distinctly different (Chi, Feltovich, & Glaser, 1981). Research in cognitive psychology has shown that experts organize information in a more structurally meaningful, inter-connected manner, whereas novice knowledge has been shown to be fragmented and to occur in smaller chunks (Chi et al., 1982; LaFrance, 1989).

In addition, research done across a range of disciplines has consistently confirmed that experts and novices approach problem situations differently, attend to different targets of focus, and resort to different thought processes or plans of action (Anderson, Farrell, & Sauers, 1984; Korovin, Farrell, Hsu, White, & Ghaderi, 2020; Mangels, & Suss, & Lande, 2020; Nelms & Segura-Totten, 2019; Wolff et al., 2017). Explicitly, experts more frequently adopt an analytical approach and rely more heavily on the deep structure and underlying meaning of a problem in order to arrive at appropriate solution methods. In contrast, novices tend to depend more on the literal, surface features of a problem during problem-solving (Chi et al., 1981; LaFrance, 1989; Wolff et al., 2017; Wolff, van den Bogert, Jarodzka, & Boshuizen, 2015).

With these distinctive differences, explanations generated by teachers (who normally assume the role of experts in the educational context and are believed to have more relevant pedagogical content knowledge3) (Shulman, 1987) would be expected to be more complete, thorough, and educationally sound relative to those generated by students, who would tend to be more novice-like, with a comparatively insufficient and fragmented base of knowledge in terms of both content and pedagogy. Alternatively, since students are generally within each other's zone of proximal development (Fallows & Chandramohan, 2001), they may have a unique vantage point from which to determine or infer the reasoning processes underlying the mistakes made by or misconceptions held by their peers. In addition, with access to shared experiences and similar levels of reasoning development, explanations generated by students for their peers may be better related to their base of experience, more culturally responsive (Nava, Fahmie, & Jin, 2019), ‘speak the same language,’ and be at the same level of understanding (Hogan, Nastasi, & Pressley, 2000). As a result, explanations generated by students for their peers may be perceived as more understandable as compared to those generated by teachers, which, comparatively speaking, may be stated in a more formal and technical form.

1.4. Types and purposes of explanations
To serve multiple purposes, various types of content have been suggested as informative, constructive explanations that accompany student responses to given questions. Specifically, in order to ascertain, enhance, and extend the respondents’ understanding of the tested content as well as their question-answering ability (for cognitive purposes), provided explanations may comprise: (a) a simple verification of response accuracy or inaccuracy, (b) an announcement of the correct answer, (c) a justification for the correct answer by restating or summarizing the main ideas of the focal test item, (d) an explication of why the response is incorrect to help replace, overwrite, tune, or restructure the existing knowledge state of the respondent, (e) a provision of intra- or extra-instruction sources for further reference, or (f) a directive to a specific part of the question that may serve as a signal for deducing the correct answer (Butler & Winne, 1995; Kulhavy & Stock, 1989; Narciss & Huth, 2004; Shute, 2008).

Contrary to being explicit and direct, to achieve metacognitive purposes, provided explanations can take the form of strategic hints, implicit suggestions, or queries (e.g., Are you sure?) intending to prompt the respondent to reflect, re-examine, and re-assess the question or the submitted answer for its accuracy (Narciss & Huth, 2004; Shute, 2008).

Finally, remarks geared toward encouraging persistent engagement of the respondent in the learning task or activity may be included for motivational purposes (Butler & Winne, 1995; Narciss & Huth, 2004).

1.5. The research questions posed in this study
As previously explicated, explanations generated by students are expected to be different from those generated by a teacher. Nonetheless, questions as to how they differ and what their respective effects on learning are when used by learning community members as explanations in responses to given multiple-choice questions have yet to be made clear; thus, they serve as the focus of this study.

In summary, two research questions are examined in this study:

RQ#1: Will there be differential learning effects in terms of academic achievement and attitudes toward the studied subject matter between students receiving student-generated explanations for answers to multiple-choice questions and their counterparts receiving teacher-generated explanations?

RQ#2: In what ways will student- and teacher-generated explanations differ? For this, three sub-questions are proposed to guide data collection and analysis:

RQ#2–1: Will student- and teacher-generated explanations differ in terms of type (taking into consideration that various types of content can be provided as explanations to serve multiple purposes)?

RQ#2-2: Will student- and teacher-generated explanations differ in terms of style (in view of the fact that students and teachers are usually born in different generations)?

RQ#2–3: Will student- and teacher-generated explanations differ in terms of number of words (in light of the fact that experts and novices have different amounts of knowledge)?

2. Methods
2.1. Research design and treatment groups
For the purposes of this study, a non-equivalent pretest-posttest quasi-experimental research method was adopted. Two treatment groups were devised: Group A (the teacher-generated explanations group) and Group B (the student-generated explanations group). For Group A, explanations students received in response to their answers to online multiple-choice question-answering sessions were generated by their science teacher (i.e., the implementing teacher in this study was a woman with more than five years of formal teaching experience). For Group B, explanations were generated by student peers from other non-participating classes at the same grade level at the participating school.

All instructional elements, including the science materials, the disseminated supplemental materials, the scheduling of science lessons, and the questions used during the weekly online multiple-choice question-answering activities, were controlled to be identical for all of the classes. The main difference between the two treatment conditions lay in the source from which explanations for each of the options in the online multiple-choice questions came — the teacher or student peers.

2.2. The study context, participants, and experimental procedure
The study began right after the mid-term exam and lasted 9 weeks. The integrated online activity was introduced to support the learning of five science topics covered during the study: (1) the Earth's changing landscape: fluvial processes and earthquakes, (2) rocks, minerals and human life, (3) weathering, soil, and the topographical landscape, (4) compasses and geomagnetism, and (5) electromagnets.

Prior to the study, some essential preparation work was done (see Fig. 1). For each of the five science topics covered during the study, 40 questions were selected from the textbook publishers’ test-bank and integrated into the system adopted in this study for the weekly online multiple-choice question-answering activity for both treatment groups. Furthermore, referring to related literature on feedback (e.g., Butler & Winne, 1995; Kulhavy & Stock, 1989; Narciss & Huth, 2004; Shute, 2008), an explanation framework was developed (see Sub-section 4.2) and provided to the science teacher in order for the explanation component to be completed before the start of the study and later used by Group A. Meanwhile, four sixth-grade classes (n = 104; males: 53, females: 51; referred to as the treatment students) taught by the same science teacher from a single elementary school in Tainan, Taiwan were chosen as the participating classes and randomly assigned to the two treatment groups (Group A, n = 49; Group B, n = 55). Twenty students ranked among the top 30% in academic performance from the other five non-participating sixth-grade classes (referred to as the collaborating students) from the participating school were recommended by their home-room teachers to participate in explanation-generation for the online multiple-choice questions to be used by the students assigned to Group B. Taking into account that demand characteristics may inadvertently influence the response or behavior of the study participants, which may in turn affect the internal validity of the experimental findings (Goodwin & Goodwin, 2016), none of the participants were aware of the source of explanations used to complement the multiple-choice question-answering process (i.e., whether it was the teacher or peers in other comparable classes). The collaborating students were not informed of the later use of their generated explanations by their peers in order to avoid possible treatment diffusion effects that would threaten the validity of the study (Goodwin & Goodwin, 2016).

Fig. 1
Download : Download high-res image (556KB)
Download : Download full-size image
Fig. 1. Study flow.

The study consisted of three main phases: the 1st pretest and training phase (one week), the 2nd intervention phase (6 weeks), and the 3rd posttest phase (2 weeks). During the 1st phase, data on the student science academic performance on the school-wide mid-term test were collected, and all treatment students were directed to complete a scale to determine their attitude toward science. Subsequently, separate training sessions were held for the treatment students and the collaborating students. For the treatment students, the operational procedures for navigating the online multiple-choice question-answering space and reviewing one's question-answering performance were introduced and practiced in one instructional session. Essential information related to explanation-generation was provided to the collaborating students. It included the various types of explanations and their purposes with reference to the same framework provided to the implementing teacher (see Sub-section 4.2). Then, the operational procedures for accessing and entering explanations to complement answers in the system were explicated and practiced in two instructional sessions.

During the intervention phase, as a routine weekly activity, the collaborating students and treatment students engaged in an online explanation-generation and online multiple-choice question-answering activity, respectively4 in the participating school's computer lab. Explicitly, after three 40-min instructional sessions on science, each of the 20 collaborating students was instructed to individually generate explanations for each of the four options in the two multiple-choice questions assigned by the implementing teacher in a 20-min period during the Friday morning sessions in the preceding week. The online explanation-generation activity was introduced to the corroborating students as a supplemental activity to support their science learning and was implemented by the same science teacher. As such, questions regarding the system and science content from the students were attended to by the teacher during the session. However, the students were requested to complete explanations for the assigned multiple-choice questions themselves after receiving explication, if any, from the teacher. Alternatively, the treatment students in both groups were directed to engage in online multiple-choice question-answering activities for 20 minutes the following Monday, during which they completed a 10-item practice sheet at least three times, where a 60% accuracy rate was set as the criterion for passing.

During the 3rd phase, the students were directed to complete the same attitude scale again. The participating science teacher developed and administered the posttest assessment of the science topics covered during the study, followed by the school-wide final exam.

2.3. The online learning system
An online learning system developed by members of the authors’ research team was adopted to support the associated online learning activity (Yu, (in press)). For the treatment students, the question-answering function was used, whereas the collaborating students used the viewing function.

For each online question-answering activity, a set of 10 multiple-choice questions randomly drawn from the 40 questions integrated into the system was automatically generated. Questions were displayed and answered in a one-per-screen fashion by the treatment students (see Fig. 2). Once an answer to the focal question was submitted, an explanation accompanying the chosen option was shown on the screen (see Fig. 3). After answering the 10-item question set, the students could click on the ‘replay’ button to practice answering questions on the same topic again, where a new set of 10 multiple-choice questions would be randomly generated from the item-bank. Conversely, the students could choose to review any of their past practice performance by clicking on the review button placed next to the selected performance record, where all answered questions with answer keys and explanations could be retrieved.

Fig. 2
Download : Download high-res image (367KB)
Download : Download full-size image
Fig. 2. Online multiple-choice question-answering space.

Fig. 3
Download : Download high-res image (231KB)
Download : Download full-size image
Fig. 3. Explanations shown in response to student answers to the focal question: for the correct response (left); for incorrect responses (right).

The collaborating students simply located the questions assigned by their teacher5 in the list of questions to engage in explanation-generation for the four answer options in the focal multiple-choice question.

2.4. Measurement instruments
2.4.1. Academic performance on science
Three tests were adopted for the academic performance assessment: the school-wide mid-term exam administered prior to the study as the pretest, a teacher-developed posttest, and the school-wide final exam administered after the study, also as a posttest. Adopting the criteria proposed by Ebel and Frisbie (1991) for item discrimination (i.e., above 0.20), only items meeting the standard were included for the data analysis. As a result, the average item difficulty, item discrimination, and internal consistency reliability (using Cronbach's α) were .74, .36, and .85, respectively, for the 34-item pretest .77, .44, and .82, for the 19-item teacher-developed test, and .85, .33, and .83 for the 17-item final exam. Based on the procedures used for the item analyses, the tests were found to be of satisfactory quality.

2.4.2. Attitude toward the subject matter studied
Germann’s (1988) ‘attitude toward science in school assessment’ (14 items; 5-point Likert-scale) was adopted in this study to assess student feelings, beliefs, and values about science and learning science. The scale was originally developed and written in English, so the backward translation technique was employed, followed by an exploratory factor analysis and an assessment of internal consistency reliability to ensure validity and reliability.

Based on the data collected from this study, the exploratory factor analysis via a principal component analysis evidenced two dimensions — positive and negative, with a high loading on the loaded factor (0.65–0.88), where the total variance explained was 75.3%. The internal consistency reliability of the scale was found to be excellent (i.e., >0.90) (George & Mallery, 2003), with Cronbach's α = .93 and .91 for the pretest and posttest, respectively. Since both positive and negative statements were included, scoring on the negative statements was reversed before adding up the scores of all items. Sample statements included “Science is fascinating and fun; I don't like science, and it bothers me to have to study it.”

2.5. Data analysis
For RQ#1, the analysis of covariance (ANCOVA) technique was used to analyze the comparative effects of the two treatment groups on academic performance and attitude toward science when the assumption of the homogeneity of the regression within groups was met; otherwise, an analysis of variance (ANOVA) on gain scores (i.e., the difference between the posttest and pretest) was conducted.

For RQ#2, three sets of analyses were conducted to reveal if and how the student- and teacher-generated explanations differed. For RQ#2–1, a content analysis along the explanation-generation framework (see Table 2) on each of the generated explanations was conducted before proceeding to the chi-square test of independence to determine if the relative proportions of the various types of content were independent of the explanation-generation source. When the results of the chi-square test were significant, post-hoc tests with the Bonferroni correction were applied to control for the pairwise error rate for multiple tests.6

For RQ#2-2, another framework aiming at styles was developed (see Table 3) to analyze each of the explanations generated. Again, the chi-square test of independence was adopted, followed by post-hoc tests with the Bonferroni correction7 when a significant chi-square test value was found.

For RQ#2–3, the number of words contained in each of student- and teacher-generated explanations was counted and compared using a t-test.

3. Results
3.1. Results on RQ#1
The means and standard deviation (SD) values for the two treatment groups are listed in Table 1. Because the tests on the homogeneity of the regression coefficients within groups were insignificant for academic achievement, F (1, 100) = 0.019, p > .05, but significant for attitude toward science, F (1, 100) = 13.674, p < .05, indicating a violation of the assumption, an ANCOVA was used for the former, whereas an ANOVA was used for the latter.


Table 1. Descriptive and inferential statistics for the two treatment groups by learning outcomes.

Group Aa (n = 49)	Group Ba (n = 55)	F	p	Cohen's d
Academic achievement	Pretest	Mb (SD)	45.47 (11.38)	46.16 (9.97)	1.101	.297	
Posttest	Mc (SD)	49.98 (11.07)	48.9 (10.38)
Adjusted M.	50.240	48.668
Attitudes toward Science	Pretest	Md (SD)	54.37 (8.98)	49.38 (12.63)	4.569	.035	.427
Posttest	Md (SD)	53.71 (10.26)	53.22 (10.51)
Adjusted M.	51.36	53.89
a
Group A: the teacher-generated explanations group; Group B: the student-generated explanations group.

b
M: maximum possible score: 63.

c
M: the average score of teacher-developed posttest and final exam, for which the maximum possible scores are 76 and 50, respectively.

d
M: Possible score range: 14–70.


Table 2. Content analysis in terms of types of explanations generated by the two treatment groups.

Group Aa
f (%b), rank	Group Ba
f (%b), rank	χb
1. Verifying response accuracy or inaccuracy	394 (49.25%), 2	771 (96.38%), 1	142.06*
2. Providing encouraging comments	73 (9.13%), 4	142 (17.75%), 2	25.82*
3. Correcting the source of error(s)	179 (22.38%), 3	92 (11.50%), 3	23.69*
4. Clarifying and elaborating related concepts or frequently held misconceptions	20 (2.50%), 5	0 (0%), 7	19.00*
5. Restating the correct answer or summarizing the main concepts tested in the question	472 (59%), 1	68 (8.5%), 4	282.20*
6. Directing to specific sources of material for further reference	0 (0%), 8	7 (0.88%), 5	7.37
7. Referring to examples from life experience to enhance understanding	1 (0.13%), 7	0 (0%), 7	0.95
8. Pointing to some part in the question to prompt further thinking and re-examining of the question	8 (1%), 6	10 (1.25%), 6	0.34
a
Group A: the teacher-generated explanations group; Group B: the student-generated explanations group.

b
% is calculated by f divided by 800 (i.e., the total number of explanations generated by the teacher and students, respectively) *<.001.


Table 3. Content analysis in terms of styles of explanations generated by the two treatment groups.

Default text & no multimedia, f (%a)	Texts with font/color/size changes, f (%a)	Emoticon f (%a)	catch/trendy words
f (%1)	Multimediab f (%a)
Group A	767 (95.88%)	39 (4.89%)	0 (0%)	33 (4.13%)	0 (0%)
Group B	363 (45.38%)	109 (13.63%)	19 (2.38%)	53 (6.63%)	341 (42.64%)
χb	166.92*	29.50*	18.01*	3.65	323.28*
*<0.001.

a
f divided by 800 (i.e., the total number of explanations generated by the teacher and students, respectively).

b
Includes Emoji.

The results of the ANCOVA revealed no significant between-group differences in academic achievement, F (1,101) = 1.101, p > .05. The results of the ANOVA showed significant between-group differences in student attitude toward science, F(1, 102) = 4.569, p < .05, with the student-generated explanations group revealing better attitude toward science.

3.2. Results on RQ#2
A total of 800 explanations were generated by the teacher and the collaborating students, respectively,8 during the course of this study. The results for the three sub-questions are presented separately below.

For RQ#2–1, the results of the content analysis in terms of explanation types revealed that the most frequently used types generated by the teacher included ‘restating the correct answer or summarizing the main concepts tested in the question (59%),’ followed by ‘verifying response accuracy or inaccuracy (49.25%),’ ‘correcting the source of error(s) (22.38%),’ and ‘providing encouraging comments (9.13%),’ with ‘directing to specific sources of material for further reference’ never used. Alternatively, the collaborating group's most frequently used types included ‘verifying response accuracy or inaccuracy (96.38%),’ followed by ‘providing encouraging comments (17.75%),’ ‘correcting the source of error(s) (11.50%),’ and ‘restating the correct answer or summarizing the main concepts tested in the question (8.5%),’ with ‘clarifying and elaborating related concepts or frequently held misconceptions’ and ‘referring to examples from life experience to enhance understanding’ never used throughout the study.

A chi-square test of independence showed there to be a significant association between the explanation sources and types of explanations, X2 (7, N = 2237) = 510.42, p < .001. A further post-hoc pair-wise comparison with a Bonferroni correction found a significant relationship between the two variables in five out of the eight categories. The teacher was more likely than the students to include ‘correcting the source of error(s),’ X2 (1, N = 271) = 25.82, p < .001, ‘clarifying and elaborating related concepts or frequently held misconceptions,’ X2 (1, N = 20) = 19, p < .001, and ‘restating the correct answer or summarizing the main concepts tested in the question,’ X2 (1, N = 540) = 282.20, p < .001, as part of their generated explanations, whereas students were more likely than the teacher to include ‘verifying response accuracy or inaccuracy,’ X2 (1, N = 1165) = 142.06, p < .001, and ‘providing encouraging comments,’ X2 (1, N = 215) = 23.69, p < .001, as part of their generated explanations (see Table 2).

For RQ#2-2, the results of the content analysis of styles showed that almost all of the teacher-generated explanations were presented in the default plain text without including media of any kind (95.88%). Conversely, the majority of the student-generated explanations were embellished, using different formatting options and/or including multimedia as part of the explanations (54.62%9) (see Table 3).

A chi-square test of independence showed that the proportion of use of styles differed by the source of explanations, X2 (4, N = 1724) = 541.36, p < .001. Further post-hoc comparison tests with Bonferroni corrections found a significant relationship between the two variables in four out of the five categories. Students were more likely than the teacher to use different fonts/colors/sizes for the text, X2 (1, N = 148) = 29.50, p < .001, emoticons (e.g.,:-|,:D, \o/, O_O, ^_^), X2 (1, N = 19) = 18.01, p < .001, and multimedia (e.g.,

Image 1
,
Image 2
,
Image 3
,
Image 4
,
Image 5
), X2 (1, N = 341) = 323.28, p < .001, whereas the teacher was more likely than the students to stick with the default text setting without the inclusion of any multimedia in the generated explanations.
Finally, for RQ#2–3, the t-test conducted on the number of words showed a significant between-group difference, t(102) = 18.809, p < .001, with the teacher including more words in each generated explanation (see Table 4).


Table 4. Number of words included in the explanations generated by the two treatment groups.

Total number of words	Ma	SD	t	p	Cohen's d
Group A	11,794	14.74	10.863	18.809	.000	0.96
Group B	5151	6.44	5.605
a
Number of words per multiple-choice question option.

4. Discussion and conclusions
Educators and students alike have come to embrace the concept and practice of learner-generated content to achieve learner-centered, enriched learning (Doyle et al., 2020; Hubbard et al., 2019; Orús et al., 2016; Snowball & McKenna, 2017). Among the various possible forms, the student-generated assessment items approach has evidenced great success in terms of promoting learning (Denny et al., 2008; Ebersbach et al., 2020; Khashaba, 2020; Rosenshine et al., 1996; Rosli et al., 2014; Song, 2016; Zuya, 2017) and is regarded as a potent alternative assessment approach (Fergus et al., 2021). Capitalizing on this momentum while highlighting the ‘uptake’ concept of computer-supportive collaborative learning (Suthers et al., 2010), questions as to ‘will students receiving explanations from different sources (i.e., the teacher and students) have different learning outcomes’ and ‘how explanations generated by the two camps differ’ served as the focus of this study.

When compared to students receiving teacher-generated explanations during online multiple-choice question-answering, the following questions were contemplated in this work:

•
Will students receiving student-generated explanations have less favorable learning gains because the student population as a whole will tend to have more fragmented and less content-domain declarative, procedural, conditional knowledge (Chi et al., 1982; Wolff et al., 2017) or pedagogical content knowledge (Shulman, 1987), thus preventing them from providing explanations that are as informative and constructive as those generated by teachers?

•
Or, conversely, will student-generated explanations lead to better learning because fellow students speak the same language (Hogan et al., 2000) and are within each other's zone of proximal development (Fallows & Chandramohan, 2001), thus making them more likely to generate explanations that are more comprehensible relative to those generated by teachers?

•
Along the similar vein, will these differences affect student attitude toward the subject matter being studied to any significant extent?

Based on the results of the ANCOVA and ANOVA, receiving explanations generated by the teacher and students did not lead to different academic performance; nonetheless, students receiving explanations generated by their peers had better attitudes toward science than those receiving explanations generated by the teacher. These obtained findings may be better understood with reference to the results yielded from the content analysis, indicating that explanations generated by the teacher and students, while exhibiting similar patterns in some areas, were different in several significant ways.

First, in terms of types of explanations, both groups showed similar patterns in that the top four most frequently used types were the same. However, their occurrence and use patterns differed significantly. Specifically, elaborate explanations of science concepts were highlighted significantly more frequently in the teacher-generated explanations than they were in the student-generated explanations. Alternatively, ‘simple verification of response accuracy and inaccuracy’ and ‘providing encouraging comments’ were focused on considerably more often in the student-generated explanations. As evidence, teacher-generated explanations attended predominately to the cognitive function and purpose of explanations, whereas student-generated explanations addressed both cognitive functions and affective/motivational purposes in a more balanced way. The significantly different patterns in terms of level of elaborateness and simplicity may be due to the different quantity and quality of knowledge possessed by the teacher and students, as can be inferred from research on teachers' knowledge (Shulman, 1987) and that on experts and novices (Chi et al., 1982; LaFrance, 1989; Wolff et al., 2017). Regardless, the resultant comparative academic performance between the two groups can be understood due to both groups receiving explanations attending to the cognitive function of explanations to support understanding (Brown & Armstrong, 1984). Meanwhile, the noticeably different focus in terms of including substantially more encouraging comments by the student-generated explanations group enabled ‘support of academic motivation,’ which is suggested to be an important facilitating aspect of feedback (Shute, 2008) that shines through and may play a role in the participants receiving student-generated explanations during online multiple-choice question-answering having better attitudes toward science than their counterparts receiving teacher-generated explanations.

Secondly, in terms of styles, the teacher-generated explanations were mostly text-based with the default formatting, whereas multimedia in various forms was present in nearly half of all student-generated explanations with different text formatting embellishments. The various uses of styles by the student-generated explanations manifest the ‘appealing’ attribute of feedback highlighted by Shute (2008) that potentially helped boost interest levels toward the presented message, which has been suggested as an effective instructional explanation technique (Brown & Edmunds, 2019). Therefore, it is understandable that the participants receiving student-generated explanations during online multiple-choice question-answering were found to have better attitudes toward science as compared to those receiving teacher-generated explanations.

Finally, in terms of word quantity, the student-generated explanations were found to be more concise than the teacher-generated explanations by directly ‘verifying response accuracy or inaccuracy’ in almost all of the generated explanations. Despite the fact that, as compared to the students, teachers in their field of study are believed to have substantially more content and pedagogical knowledge and experience (Chi et al., 1982; Shulman, 1987; Wolff et al., 2017) that make it possible for them to provide more elaborate explanations intended to assist the understanding of question-answerers, this type of content may not appeal to peers during online multiple-choice question-answering activities, and they may not relate well to it. Instead, concise responses regarding the correctness of the submitted answers may be what most students would prefer to receive, as so reasoned by the collaborating students, who are more likely to be in the same zone of proximal development, with access to shared experiences (Fallows & Chandramohan, 2001; Hogan et al., 2000). Also, the brief student-generated explanations, in essence, complied with the ‘specific’ attribute of feedback accentuated by Shute (2008). Taken as a whole, this may have contributed to the more positive attitudes of those who received concise explanations generated by students as compared to others who received more elaborate explanations from the teacher.

4.1. Suggestions for instruction
In view of the findings of this study together with evidence from a study by Glassey and Bälter (2020) indicating that students can generate quality explanations for multiple-choice questions, instructors are suggested to leverage the power of the student population by inviting and empowering them to act as knowledge producers in the form of explanations for answers to multiple-choice questions, rather than relying entirely and solely on teachers as the single source of such explanations. With this instructional arrangement, students in general can play a more essential role and become more involved in the learning process. This arrangement not only is in alignment with contemporary education and assessment paradigms, but also helps actualize democratic, participatory education. Finally, as revealed by the results of the content analysis, the inclusion and use of both sources of explanations for answers to multiple-choice questions would make it possible to tap into the distinctive types and styles characteristic of both camps.

4.2. Significance and contributions of this work
This work contributes to the research on students as authors of assessments, in particular multiple-choice questions. Its contributions are explained in terms of its empirical and instructional significance below.

Empirical significance. To the best of the authors' knowledge, this is the first scientific study yielding preliminary evidence in support of student-generated explanations to complement answers to multiple-choice questions for the purpose of improving student attitudes toward subject matter and generating comparable academic performance as compared to teacher-generated explanations. In addition, up to the present, how the teacher and student explanations for multiple-choice questions differ has not been clearly known. With the results obtained in this study, knowledge on if and in what ways student- and teacher-generated explanations differ has been advanced.

Instructional significance. The learning task that the collaborative students were in charge of in this study — generating explanations for answers to multiple-choice questions, was the first ever reported in the literature on learner-generated assessment items. This work thus essentially expands the current scope of this field. In addition, the instructional sequence delineated in this study helps illuminate how different sources of questions and feedback (i.e., the explanations in this study) can be arranged to create an enriched, versatile online learning environment (e.g., student-generated explanations for self-, peer- or teacher-generated questions, integration of student-generated explanations for question-answering, among others).

4.3. Suggestions for system design
To enable seamless integration of the above-mentioned learning activities, student-generated questions (explanations) systems would need to be extended. By so doing, the figurations and uses of different sources of questions and explanations can be better supported. In addition, insights gained during this study led to awareness of another area with great potential for system enhancement, which is shared below to help promote diversified designs and use of learner-generated assessment items.

Both the instructional sequence and system design reported in this study resorted to an a priori solution; that is, explanations were generated by learners and integrated into the system before question-answering activities occurred. By design, once an answer to a question is submitted, feedback to the rendered answer can be provided instantly to attain the goal of ‘timely’ feedback. Despite its learning potential (as confirmed in this study), there is one constraint related to the current design. It is only applicable to selection type questions (e.g., true/false, multiple-choice, and matching), where finite answers/alternatives for questions are given from which students can select; thus, explanations corresponding to the respective answers/alternatives can be prepared in advance. Conversely, this approach would be infeasible for construction type questions with open-ended answers (e.g., short-answer, fill-in-the-blank, calculation questions, and so on).

To overcome the above-referenced shortcoming, it is suggested that learner-generated explanations be designed as a post-hoc learning activity. Consequently, both the delineated instructional sequence and developed system could be augmented and extended by including a ‘request for explanation’ function in the system. This built-in ‘call-out’ design would be essential when the presented answer/solution prepared a priori does not correspond to the learner's submitted answer because it would allow learners to reach out to solicit elaborate explanations from others in the learning community. By integrating a help-seeking mechanism taken up by individual learners with such needs during learning, one of the most widely emphasized and important characteristics of self-regulated learners — help-seeking (Karabenick & Gonida, 2018), would be supported.

In short, rather than taking an a priori approach, learner-generated explanations for answers to questions can be flexibly built into a system by taking both a priori and post-hoc approaches to enable its integration and use in both selection and construction question types while accommodating individual learning needs and nurturing self-regulated learners.

4.4. Limitations of this study and suggestions for future work
While this study provided encouraging results confirming the positive effects of receiving student-generated explanations corresponding to answers to multiple-choice questions on students’ attitudes toward the subject matter studied that led to comparable academic achievement in comparison to teacher-generated explanations, this study involved fifth-grade students engaged in online multiple-choice question-answering activities related to science. Additionally, it was noted that the explanations received by Group B were generated by a selective group of students who ranked among the top 30% of their peers from other classes. Limitations related to the demographics of the participants (i.e., age and grade level), the applied context (i.e., science learning), and the source of student-generated explanations should not be ignored. The external generalizability of the currently obtained findings to students with other demographic backgrounds, to other subject matter (e.g., social studies and language arts, where memorization of facts and rules takes up a considerable amount of the learning time, in contrast to the main focus of science, where understanding of concepts and clarification of misconceptions are taught and learned), or involving corroborating students composed of different levels of ability (i.e., not limited to high-achieving students, as was the case in this study) will need to be validated in future studies.

In addition, as explained in the Methods section, the treatment groups were not informed of the different sources of explanations for the multiple-choice question-answering activity. As can be expected, if the student-generated explanations group knew that the explanations they received were peer-generated, it may have evoked an elevated level of interest and excitement that could have affected their learning motivation and academic interest besides those purely influenced by the nature and quality of the explanations. This yet-to-be-understood/confirmed question would surely be a research topic worthy of further investigation.

Under the widely embraced concept of students as knowledge producers (Doyle et al., 2020; Snowball & McKenna, 2017), a logical follow-up study would be an examination of whether students who generate explanations for multiple-choice questions (i.e., the corroborating students in our study) and those who receive explanations for multiple-choice questions (i.e., the treatment B group in our study) learn differently or at similar levels. In addition, on the basis of the explanation-generation framework derived in this study (see Table 2, Table 3), measures of explanation quality could be developed for an objective assessment of the quality and accuracy of student-generated explanations in the future.

Following our discussion on instructional sequence and systems accommodating different sources of questions and explanations, the comparative effects of various feasible arrangements supported in this versatile online learning environment (e.g., student-generated explanations for self-, peer- or teacher-generated questions; further use of student-generated explanations for self-, peer- or teacher-generated questions for question-answering activities) on learning outcomes of educational importance (e.g., metacognitive skill development, argumentation skills, motivation, academic performance, attitudes, etc.) can be examined in a systematic way.

Following our suggestions on system enhancement, research questions as to if and in what ways receiving teacher- versus student-generated explanations in response to answers to construction question types affects learning can be further examined.

Finally, since student-generated explanations using a post-hoc approach (i.e., prompted by the ‘request for explanation’ function) are initiated by individual learners on an as needed basis, inevitably, some delays in receiving explanations can be expected, as compared to an a priori approach, where timely feedback is provided. Due to these differences, the comparative effects of a priori versus post-hoc approaches to learner-generated explanations for answers to questions during the learning process would be a worthwhile research topic for future exploration.

