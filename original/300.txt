Software Defined Networking (SDN) enables flexible network management with a well-defined abstraction between control and data plane. In this way, operators could issue the policies, e.g., forwarding path, flow counting and rate limiting, from the control plane, which will be enforced by the flow table rules in the data plane. However, multiple active policies with the same priority will potentially trigger conflicts among policies with overlapped flow space, causing the flow table explosion. In contrast to the local switch conflict resolution schemes proposed by previous works, this paper tackles the same problem from a different angle and resolves the policy conflict problem by coordinating all switches under a global centralized view. Specifically, we propose COnflict RAzor (CORA), which tremendously reduces the storage cost of conflicting policies leveraging the global network information obtained in the controller. The basic idea of CORA is migrating policies causing large explosions across the network if necessary, while keeping the semantics equivalence. We prove CORA’s NP hardness and propose a heuristic to efficiently search a near-optimal policy migration strategy. Our experiments demonstrate that, CORA can effectively reduce the flow table storage occupation by averagely 79.8% within less than 40 s, which is 47.9% more efficient than the state-of-the-art.

Previous
Next 
Keywords
software-defined networking

Flow table compression

Policy conflict

1. Introduction
Software Defined Network (SDN) decouples switch’s control and data plane, offering enhanced programmability via a higher-level abstraction, i.e., intent-based north-bound interface (NBI). The operator intents would be represented with native code like Python program in Ryu controller, or domain specific languages (DSL) like SNAP scripts (Arashloo et al., 2016). In either way, such intents would be firstly compiled into distributed policies, associating the packet class (i.e., flow) with the corresponding actions (e.g., forward, drop, count) for each underlying switch. The policies would be further translated into hardware-specific  when loaded into a particular device, e.g., prefix entries for Ternary Content Addressable Memory (TCAM). However, policies issued by different controllers (NBIs) may specify different actions on an overlapped flow space at the same network device. The so-called “policy conflicts” should be resolved to perform the correct combination of the actions. But, such conflict resolution will potentially incur either performance penalty or resource inefficiency for the underlying network.

We use a simple example to illustrate how the conflict happen and how to resolve it. Consider that a QoS function and a monitoring function coexist in a network, where the former has a policy at switch  to limit the bandwidth to 10 Mbps for packets with DstPort range 1–6, and the latter sets a policy to count the number of packets with DstPort range 2–7 at the same switch. Such two policies are conflicting due to the different actions on the overlapped port range 2–6. A straightforward method to cooperate these two policies in a single switch is to split them into three sub-policies with non-overlapping flow spaces: (
) 1–1limit 10 Mbps, (
) 2–6limit 10 Mbps and count, and (
) 7–7limit 10Mbps1. Intuitively, fragmented policies will inevitably be generated when resolving conflicts. In a bad case, each policy at the switch would be conflicting with all others, producing many more sub-policies on fragmented flow spaces. In addition, when it comes to multi-dimension conflicts in many flow entry fields, things will get even worse than the single-field scenario. As a result, the rule conflicting problem aggravates heavily in the context of SDN, since the switch checks more fields other than the traditional 5-tuples. Please notice that the number of extra generated policies does not reflect the complete overhead, because it may be much more costly when translating the policies into hardware-specific rules. For example, to represent a value range of packet fields like DstPort, a TCAM-based switch has to convert the range into one or more prefix entries during the so-called “rule expansion” process, e.g., at least three entries (110, 01*, 10*) are needed for the above DstPort range 2–6.

In the literature, many works have investigated the mechanisms of efficient conflict resolution. One possible way is to find a more efficient cut of the flow space by creating a new policy on the overlapped flow space with a higher priority while retaining the original ones with a lower priority (Jin et al., 2015). This may avoid producing too many flow space fragments, since the policies that are fully covered by the higher-priority ones are redundant and can be eliminated to save memory cost. However, even with this technique, extra policies would be inevitably generated as long as one flow space does not fully cover the other. Another attempt is to reduce/minimize the rule expansion specifically for TCAM-based devices (Kogan et al., 2015, Liu and Gouda, 2010, Meiners et al., 2012, Bremler-Barr and Hendler, 2012, Katta et al., 2014, Liu et al., 2010). However such low-level optimization does not address the root cause of the policy conflicts either, and in the worst case, a -bit range value (indexed by some policy) will consume  TCAM entries (Rottenstreich et al., 2013). Although the OpenFlow specification (ONF, 2015) proposes the flow table pipeline to mitigate the aftermath of policy conflicts, simply employing such pipeline will dramatically increase either the number of flow tables or the bit width for each flow table. Network slicing solutions (Al-Shabibi et al., 2014, Koponen et al., 2014, Doriguzzi Corin et al., 2012) provides individual flow spaces for each network function to isolate the conflicts, which actually disables multiple network functions to operate on the same traffic, i.e., functions can only manage the disjoint flow space. Nowadays, many high-level SDN languages offer the resource constraints in their syntaxes and compilation process, which can be adopted to generate a more efficient placement of policies to mitigate the conflicts beforehand (Arashloo et al., 2016, Prakash et al., 2015). However, this requires the global view of all operators’ intents, which violates our assumption: the intents may be issued by individual operators from different controllers that cannot cooperate at the language level (Jin et al., 2015).

We observe that the policy/rule explosion is ascribed to the flow space overlaps, and as a result, our basic idea is to move/migrate the conflicting policies from the local switch to reduce the overlaps and further decrease the #policies and entries. The prerequisite is to ensure that all network function will always hold their semantics after moving/modifying the policies, i.e., the flows should be forwarded to the original destination along the same path with the same actions applied, e.g., rewrite, count, mirror to controller, etc. SDN offers the global information of data plane policies, which can be utilized to guarantee the above requirements. Recall the aforementioned example, the conflicts can be eliminated if we move the counting policy from the switch  to an adjacent switch 
 along the routing path of the flow, since the flow forwarding behavior remains the same and the counting action can be applied at any switch along the path.

Based on the above insights, we propose COnflict RAzor (CORA) to efficiently resolve the policy conflicts in SDN, which collects policies from all network functions, and migrates the conflict-makers from the current switch to other feasible switches to relieve the conflicts while keeping the equivalent semantics. In a nutshell, two designs make CORA efficient. First, to the end of conflict elimination, CORA leverages the cross-switch information from the data plane, which would dramatically decrease the possible conflicts. Second, to the end of finding an (near-)optimal solution, CORA applies a simple yet effective heuristic to conquer such an NP-hard problem. It is worth noting that CORA focuses on efficiently eliminating the conflicts in the global network, thus can well cooperate with any existing solution that reduces/minimizes the #policies at a single switch. To be specific, the rule transferring enables much more possibilities if cooperating with the local conflict resolver, because we can find a “combination” to improve the optimization performance, e.g., transferring specific rules to a certain switch, such that the local conflict resolver can maximize its effect.

This is paper is an extension to our preliminary paper (Li et al., 2018), in which we have made the following three contributions:

•
We explore the potential benefits and technical challenges of migrating policies across the network, and propose semantics-preserving migration mechanisms to address the challenges, e.g., retaining the routing paths, slicing the endpoint actions, etc.

•
We formally define the problem of finding an optimal policy placement with many policy conflicts. After proving its NP hardness, we give some heuristics to fast generate a near-optimal placement.

•
We implement a prototype of CORA, and use synthetic policy configurations and topologies to evaluate its performance. The experimental results show that CORA can reduce at least 49% of the total conflict overhead within acceptable time, while retaining the original intents.

We further make the following contributions in this paper.

•
We check and port two typical local optimizations on CORA to demonstrate its compatibility to existing local-switch rule compression algorithms.

•
We compare CORA with existing approaches including rule packing and local resolving to demonstrate its optimality.

The reminder of this paper is organized as follows. Section 2 demonstrates the policy conflict problem and our basic idea. Section 3 proposes the semantic-preserving transferring to retain the high-level intents. Section 4 defines the problem of finding an optimal placement from the global view, and designs a heuristic algorithm to obtain a near-optimal solution within acceptable time cost. Section 5 presents two aggregation-based optimizations on local switch, and demonstrates their compatibilities with CORA. Section 6 evaluates CORA’s performance. Section 7 discusses the possible extension of CORA. And after discussing the related work in Section 8, Section 9 concludes this paper.

2. Transfer policy to raze the conflicts
2.1. Policy conflict problem
SDN high-level languages specify two categories of operators’ intents: the routing intent and the endpoint intent (Kang et al., 2013). The routing intent is to specify the paths between the ingress and egress of s packet class, driven by traffic engineering goals. The endpoint intent focuses on the end-to-end packet behaviors other than forwarding, e.g., counting, mirroring to controller, modifying header, etc. In the single-controller scenario, such intents are issued by a same NBI, and the controller would compose and compile them into distributed policies, while the policies in each switch have been properly prioritized (Arashloo et al., 2016, Prakash et al., 2015). However, it has been advocated that multiple controllers should coexist in a network with a hypervisor, which provides the ability of running any combination of controller applications (Jin et al., 2015). Therefore, the policies from different controllers can have overlaps with a same priority, triggering the policy conflicts. Notice that due to the language-barrier of different NBIs, the existing hypervisors cannot reconcile the policies at the language level, but only resolve them in the local switch.

Formally, a policy can be denoted as , where  is the switch storing the policy,  is the priority of the policy,  is a hyperspace with different fields to describe the flow space, and  is the action set that applies on . Two policies 
 and 
 conflicts, iff 
, 
, 
 and 
2. To resolve such conflict, a naive method is to fully decouple the overlapped space into continuous fragments, each of which performs the combination of actions from corresponding policies. For example, the above two conflicting policies would be decoupled into three ones: 
, 
, and 
. Notice that these three policies may expand to more because the flow space like 
 may not be continuous, which will be further transformed into two policies. In the worst case, one policy would conflict with all the other policies, and the naive decoupling process will lead to a policy increase at a complexity of 
, where  is the number of original policies. Fig. 1 illustrates a simple example of policy conflict in two dimensions DstIP and SrcIP, where 
 is a QoS policy to limit the bandwidth and 
 is to count the number of packets. The flow space would be cut into five sub-spaces to fully resolve the conflict.

The naive method is not efficient enough when the conflicts result from the inclusion of flow space. Taking the above two-dimension conflict as an example, 
 includes 
 from the perspective of  axis (SrcIP). Therefore, it is not necessary to divide the  axis into three segments, instead, the original 
 can be retained and a new policy would be added with higher priority that represents the overlapped flow space in  axis. We can apply similar analysis in  axis, and as a result, only one extra policy is needed to resolve the conflict (the solid shadowed part in Fig. 1), i.e., 
.

However, the above priority-based method is still not efficient enough to tame the explosive growth of policies. The reason lies in that this method only works for inclusion cases, which would commonly happen in IP fields due to the prefix representation. In contrast, some fields like DstPort and SrcPort are represented by ranges, which leads to overlaps rather than inclusions in most cases, and cannot be resolved by the priority-based method. For example, Fig. 2 adds a third rule 
 with the same priority to the example in Fig. 1, and they will be transformed into six policies even we sophisticatedly prioritize the overlaps, as shown in Table 1. Even worse, many switches use TCAM to implement the flow tables, which would expands the entries to represent the range values (Liu et al., 2010). Formally, each range defined over a -bit field can be encoded in  entries with the internal expansion in the worst case, and if the flow space specifies  ranges on  fields, it will consume up to 
 entries in TCAM (Rottenstreich et al., 2013). Since the policy conflicts are producing more fragments on the flow space, the overhead in the real scenarios would go far beyond the 
 complexity, and squeeze the limited TCAM resources.


Fig. 1. Two policies conflict on two fields.


Fig. 2. More conflicts caused by a third policy.


Table 1. The policies to decouple the conflicts in Fig. 2.

1	
3
2	
2
3	
2
4	
1
5	
1
6	
1
2.2. Basic idea of CORA
All the existing techniques focus on how to minimize the overhead when decoupling the conflicts. In contrast, CORA treat the same problem from a different angle, aiming to eliminate the conflicts by migrating the policies, i.e., to reduce the number of conflicts in global network instead of the number of expanded policies in a local switch.

Considering a simple linear topology with two switches 
 and 
, the policies at each switch are shown in Fig. 3. It needs 16 and 7 sub-policies to fully decouple all the conflicts in 
 with naive and priority method, respectively. However, if we transfer 
 to 
 as shown in Fig. 4, only 4 sub-policies are produced in 
 with priority method, and 
 generates no more policies other than 
 from 
, which means the overall #policies decreases from  to . The performance gain can be more significant for TCAM-based switches.

The above simple example shows the potential benefits if properly migrating policies. However, arbitrarily migrating policies may break the high-level intents from the operators. Recall the above simple network configurations, there are many prerequisites to transfer 
 to 
. First, 
 cannot be a routing policy that forwards the packet, since such transfer would produce a black hole at 
 for the packets that match the flow space of 
. Second, 
 can be transferred to 
 only when there is other routing policy that forwards all the packets in 
’s flow space to 
 (e.g., 
), or 
’s action cannot be properly triggered. These special cases need to be carefully addressed to retain the original high-level intents. The other challenge is to find an optimal policy placement with acceptable time cost. The #policy in network can easily reach 1000 with highly dependent conflicts, e.g., 
, 
 and 
 have common overlaps, while they also pairwisely conflict with each other. Simply exhausting all possible policy placements is obviously infeasible in time cost, because the #policy combinations can be up to 
 for  policies.


Fig. 3. The policy placement in two adjacent switches.


Fig. 4. The new policy placement solution if transferring 
 from 
 to 
.

In summary, to well design CORA, we need to address the following major challenges.

Semantics equivalence. The transferring operations should be semantics-preserving, which guarantees the correct fulfillment of multiple network functions.

Optimal placement. After policy transferring, rules must satisfy the constraint of the physical switch capacity, and is expected to be minimal in the network.

3. Semantics-preserving transfer
It is not trivial to correctly transfer the policies because arbitrary change of the policy placement may break the high-level intents, i.e., routing intent and endpoint intent.

3.1. Routing intent
The routing intent would be compiled into routing policies for individual switches, each of which forwards the packet to the next hop. That is, there lies strong dependencies between the those policies; if we transfer one routing policy to another switch, we have to modify the related routing policy at the previous hop, and we may need to create new routing policies to fulfill a complete routing path. Besides, we cannot guarantee the traffic engineering requirements are satisfied by the new path, because such intents are hidden in the compilation process, and cannot be reverse engineered from the policies. Therefore, the routing policies are considered as fixed in CORA to ensure the semantics equivalence of routing intent.

Please notice the conflicts between two routing policies are not resolvable in CORA, because we cannot forward a single packet to two different next hops. Such conflicts may happen if multiple controllers decide the routing paths individually, and resolving them need composing the routing intents at a higher semantics level (Li et al., 2016). In this paper, we assume the routing intents are handled by a single controller application, or the flow space is isolated for different routing applications, i.e., the routing policies are not conflicting with each other.

3.2. Endpoint intent
The endpoint intent is to perform specific actions on the packets, and such intent will hold, as long as the actions is triggered for all the packets whose headers fall in the flow space. Initially, the endpoint intent would be compiled into several endpoint policies, each of which covers partial flow space of the intent. The split of the flow space depends on the placement of the routing intent, since it is possible that not all packets in the flow space traverse a single switch. The intuitive idea is to transfer the endpoint policy along the routing path, as long as the target switch has the ability to perform the action. To be specific, we have the following principles of transferring endpoint policies.

First, the flow space of the routing policy that covers the endpoint policy should be consistent through the transferring, or the endpoint policy needs to be further divided. Considering a routing policy 
 that forwards the packets with DstPort 1–6 to port 1, and an endpoint policy 
 that counts all packets with DstPort 2–7, 
 cannot be directly transferred to the switch that connects to port 1, since it will fail to count the packets with DstPort 7. As a result, we need to divide 
 into two policies, 
 and 
, and we can transfer 
 through port 1. To this constraint, an endpoint policy (or a slice of an endpoint policy) 
 in switch  can be transferred through port , if there exists a routing policy 
 in switch , where 
 and 
 (forward transferring), or there exists a routing policy 
 in switch 
, where 
, 
 and port  in 
 connects to port  in  (backward transferring). Here we only discuss the one-step transferring to the pre or next hop, and the multi-hop transferring can be seen as a combination of multiple one-step moves.

The above principle only ensures the semantics if there is only one endpoint intent, because the dependency between endpoint intents may further constrain the placement of endpoint policy. Considering an endpoint policy 
 modifies the VLAN id to 10 for the packets with VLAN id 1, and another policy 
 counts the packets with VLAN id 1, the order of triggering the two policies reflects the high level intent; if 
 is triggered before 
, the two policies just stick to their scripts; otherwise, 
 is only to verify whether 
 correctly works. We assume the initial placement has already satisfied the high level intent, and therefore, the order of triggering the two policies cannot be violated. More generally, we say 
 depends on 
, if 
 will cut or produce packets to be processed by 
. To maintain the original intents, the order of two dependent policies cannot be changed. For example, if an endpoint policy is conflicting with a header modifying policy, then it can only be transferred between the ingress/egress and the header modifying policy.

Please notice that this constraint also forbids the transferring of header modifying policies, because the routing policies definitely depend on header modifying policies; if we transfer it to the next hop, the modification would break the routing path, because there is no routing policy that handles the unmodified headers in next hop; likewise, the pre hop is also infeasible, because there is no routing policy to forward the modified headers in the current switch.

In summary, we say a policy (or a slice of policy) can be transferred to a certain switch, if it satisfies the above two constraints, i.e., the routing policy restriction and the order of critical actions. Following this definition, we further define a one-step semantics-preserving function , which takes a policy  and an adjacent switch  as the input, and outputs a set of new policies. Specifically, , if neither  nor a slice of  can be transferred to ; , if  can be completed transfer to switch ; 
, if a slice of , denoted as 
, can be transferred to switch . The notation  is to replace  with value .

4. Finding optimal placement
4.1. Problem formulation
The optimal policy placement is a placement that the cost (i.e., the number of rules) of all switches is minimized. Previously, the optimal policy placement has been discussed in several papers (Kang et al., 2013, Arashloo et al., 2016), most of which models the problem as follows:  policies should be assigned to  switches, while each assignment (policy  to switch ) has its profit 
 and cost 
, and each switch has its capacity 
. The goal is to maximize the profits while assuring each switch does not run out of its capacity. Such model captures the well-known general assignment problem (GAP), thus is also NP-Hard. However, the original GAP assumes the cost 
 is fixed, and not dependent of the placement of other policies, while in our scenario, 
 depends on the policies previously assigned to switch , because the number of rules varies to the conflicts between the policies in the same switch.

In this paper, we define the above extended placement problem as policy optimal placement problem with dependent cost (POPDC). To address such problem, we divide POPDC into two sub-problems: (1) decide the combinations of policies (DCP), and (2) assign the combinations to the switches (ACS). These two sub-problems are independent, because DCP only considers the penalty of putting certain policies together, which determines the total the number of policies/rules of the network, while ACS focuses on finding an optimal placement for the combinations to satisfy the capacity constraint. In the following, we will first define the variables involved in POPDC, and address the two sub-problems respectively.

Variables and notations. The first variable in POPDC is the policy set that to be assigned, denoted as , which however cannot directly map to the original endpoint policy set 
. The reason is that it is possible that the policy cannot be assigned to a certain switch, or only a slice of the policy can be transferred to that switch, due to the semantics-preserving transfer restriction. To address this problem, we utilize the one-step semantics-preserving function  to divide the policies into fragments, each of which can be independently assigned to a switch. The set of these fragments forms the policy set . The divide process is illustrated in Algorithm 1.

The other variables in POPDC is quite straightforward: there are  switches in the network, each of which has a capacity 
. We use  to denote a set of policies, and  represents the cost of decoupling , which can be measured with the number of decoupled policies or the number of expanded rules.

DCP: decide the policy set to be assigned. To model DCP, we first expand all the candidate policy set. Assume we have  policies, there are 
 candidate combinations of policies, the set of which is denoted as 
, where 
. Our goal is to find a subset of , denoted as , to satisfy the following requirements: (1) the number of selected sets must not be larger than the number of switches, (2) the policy combinations in  are pairwise disjoint, (3) the union of  equals to , and (4) the total cost of  is minimal. The first goal constrains the number of sets to the number of switches, or the sets cannot be assigned to switches independently. The second and third goal is to seek a disjoint set cover of  and the last goal is to minimize the total costs, i.e., the number of policies/rules.

Based on the above analysis, we formulate DCP with the following integer linear program. (1)
(2)
(3)
(4)

Eq. (1) is to maximize the profit of the selected policy sets, where the profit is defined as the reciprocal of the policy set cost. Eq. (2) restricts that every policy must be selected exactly once to produce a set cover. Eq. (3) constrains the number of sets to the number of switches. Eq. (4) defines a 0–1 variable to represent every set is either selected or not. It is clear that DCP has the same representation with weighted disjoint set cover problem, which has been proved to be NP-Hard (Pananjady et al., 2015).

ACS: assign the policy sets to switches. Given an optimal policy sets  by DCP, the next step is to assign them to different switches. The goal of ACS is to ensure the assignment will not exceed the capacity of each switch. Assume we have  policy sets in ,  switches in the network, , we can formulate ACS with the following integer linear program. (5)
(6)
(7)
(8)
(9)

Notice we introduce a new cost parameter 
 to represent the cost of assigning set  to switch . Specifically, 
, if all policies in 
 can be transferred to switch ; 
, if at least one policy in 
 cannot be transferred to switch . With Eq. (5)–(9), ACS can be reduced to GAP, if we assume the profit of assigning a policy set equals to 1. Therefore, ACS is a NP-Complete problem.

In summary, due to the high complexity of both DCP and ACS, POPDC cannot be solved in polynomial time.

4.2. Heuristics of near-optimal placement searching
As discussed above, POPDC is computationally hard, an intuitive idea is to utilize the existing approximate algorithms to find near-optimal solutions. However, the first step of modeling DCP, i.e., expanding all the possible policy combinations as the candidates, would largely impact the total complexity of solving DCP, because it exponentially increases the problem scale. Therefore, we do not use existing approximate algorithms, but propose some simple heuristics to approach the optimal policy placement, under the acceptable time consumption. Specifically, the optimal placement is expected to satisfy the following requirements: (1) the number of rules in each switch should not go beyond the capacity of the switch, (2) the total the number of rules are minimized for the entire network, and (3) the standard deviation of the number of rules in each switch should be minimized, so it is not likely to overflow when a new policy comes.


Our basic idea is to greedily find a “conflict-maker”, i.e., the highest-cost policy, among all endpoint policies 
, and iteratively make a one-step semantics transfer to the target switch that leads to the best profit. The cost of policy  is measured by the total cost decrement of switch  if we remove  from . To find a conflict maker, a straightforward method is to traverse all policies, while simple heuristics and optimizations can be applied for this searching; we can use the number of conflicts produced by the policy as the cost instead of measuring the precise the number of rules, which may reduce the searching time, especially for TCAM-based switches; we can pre-compute the cost of all policies beforehand, because the transferring only impacts two switches, and it does not need to re-compute the cost for policies in the rest switches, which could accelerate the conflict-maker searching in the next round. With the conflict maker, we have to choose where to transfer it for larger profit. Specifically, we use  to denote the number of switches that exceeds the capacity in current placement, and  to denote the total the number of exceeded rules in the network. We further define  as the standard deviation of the cost for each switch, and use  to denote the total cost of the placement, which can be measured with the number of policies or the number of rules. Based on these notations, we define the profit of a placement as , where , if , and otherwise .

The searching process is to seek a better profit through the semantics-preserving transfer to the conflict-maker. If transferring any policy under current placement would not lead to a larger , the process ends, and the current placement is an optimal solution if . The complete process is illustrated in Algorithm 2. Notice if , the solution is not acceptable due to the exceeded capacity, which needs to be reported to operators for further process.

Incremental placement update. The optimized placement needs to be incrementally adjusted when adding or deleting policies. If a policy  is deleted from switch , we just remove all the sub-policies that produced by  (including  itself). Since the cost of  must be reduced due to the removal, we only need to recompute the profit of the adjacent switches of , to see whether some additional transferring can make larger profit. If a policy  is added to switch , we compute the conflicts it produces with the existing ones, and since the cost of  must be increased, we only need to try limited policy transferring from  to obtain a new optimal placement. The policy modification can be seen as a combination of deleting a policy and adding a policy. In practice, it is common that a group of policies are updated for an entire routing path, and we can re-perform CORA after that batch update.

5. Cooperation of local optimizations
In a nutshell, when CORA decides whether a rule is “worthy” of being transferred to an adjacent switch, it will compute a profit for the potential transferring, and performs the transferring only if the profit is the big enough. In computing the profit, CORA could employ the local conflict resolver to reveal the benefit of the transferring, i.e., the factual overhead if transferring this rule to another switch. The computation of profit is extensible in CORA’s architecture. Thus CORA can cooperate with all existing local conflict resolvers, by adopting their computation.

In this section, we check two typical local optimizations based on the policy aggregation, and demonstrate the compatibilities of CORA with these optimizations.

Local TCAM compression algorithms fall into two categories in the current study: tree-based algorithms (e.g., TCAM Razor Liu et al., 2010 and Ternary Razor Meiners et al., 2012) and list-based algorithms (e.g., Bit Weaving Meiners et al., 2012, Redundancy Removal Liu and Gouda, 2010, Equivalent packet classifiers Dong et al., 2006). The former algorithms can aggregate rules with prefix format, as they convert rules into a decision tree and traverse it for optimal aggregations. Apart from the prefix rules, there are also ternary rules, which can be aggregated by list-based algorithms: they compare rules in a rule set and aggregate those with specific bit relations.

5.1. Tree-based compression
We use the FIB aggregation algorithm (Khare et al., 2010, Zhao et al., 2010) as the typical tree-based compression method. Specifically, FIB aggregation first transforms entries’ prefix into binary string, and compose them to a binary tree. Next, it can aggregate entries that have the same next hop by inserting, deleting and combining binary tree’s nodes. As an example for the sibling nodes, the algorithm traverses the cardinality tree: if one node has the same next hop with its sibling node, then these two nodes can be deleted, and a new parent node with the same next hop and a longest common subsequence prefix of those two nodes can be inserted.

The traditional method is designed for one-dimension rules, while in our scenario, there would be many more in the flow table, e.g., 41 fields are supported in OpenFlow 1.5 (ONF, 2015). We assume a four-dimension case (SrcIP, DstIP, SrcPort and DstPort), and extend such algorithm to cooperate with the SDN-specific scenarios. Specifically, we first focus on the entries that are same in three dimensions, and aggregate the fourth by building a cardinality tree. Next, we repeat this process for other dimensions, until none of them can be aggregated. Consider the following two four-dimension policies, (192.168.2.2/32, 192.168.2.4/32, [1,5], [2,4]) and (192.168.2.3/32, 192.168.2.4/32, [1,5], [2,4]). Note that they have the DstIP, SrcPort and DstPort, we can form the SrcIP field into a binary tree: since the two SrcIP fields only differ in the last bit, they are sibling node in the binary tree. As a result, we can aggregate these two entries into one: (192.168.2.2/31, 192.168.2.4/32, [1,5], [2,4]). This entry can be further aggregated with others, following the above process.

5.2. List-based compression
The tree-based algorithms only work for prefix rules, i.e., each field in a rule is specified as a prefix bit string (e.g., 01**) where the “stars” only appear at the end of the string. In contrast, TCAM can work in the way that combine entry’s every field and process it as a whole. Each field of a TCAM rule is a ternary bit string (e.g., 0**1), where the stars can appear at any position. List-based algorithms, e.g., Bit Weaving (Meiners et al., 2012), is designed for aggregating ternary rules. To be specific, we mix four dimensions of entries into a long bit string, and adjacent TCAM entries that have the same decision with a hamming distance of one (e.g., differ in only one bit) can be merged into a single entry by replacing the bit with *. To improve the aggregation efficiency, some optimizations like bit swapping and bit merging can be applied.

6. Performance evaluation
6.1. Evaluation settings
In this section, we evaluate the semantics equivalence and optimization performance of the migrating operation in CORA. We implement CORA with 2000 lines of python code, which takes the topology, the capacity of switches, and the current policy placement as the input, and produces a new placement as the output.

We test CORA in three topologies, Stanford Backbone (Kazemian et al., 2012) and FatTree (), which have 26, 20, and 80 switches respectively. For each topology, we slice the global network address (0.0.0.0–255.255.255.255) into  sections, where  is the number of edge switches in the topology. We assign those network sections to the edge switches as the host IP they connect to. We then simulate abundant high-level intents for the topology. Specifically, we use ClassBench (Taylor and Turner, 2007) to generate packet classification rules (SrcIP/Mask, DstIP/Mask, SrcPortRange, DstPortRange, Action), which can be regarded as the endpoint intent. The Action in the rules is just an integer number indicating different endpoint actions, and we choose a specific number to denote the header modifying action, which modifies the SrcIP and SrcPort randomly, to simulate an NAT function. The default rules with long mask length are removed.

By mapping the SrcIP and DstIP ranges to the edge switch, we obtain the corresponding routing intent; the simple shortest path is generated by SrcIP and DstIP, and we split the intent if the IP ranges cross network sections. Then we can generate routing policies for each switch according to the routing intent, and randomly place an endpoint policy along the routing path by the endpoint intent. Notice if the endpoint policy is a header modifying policy, we need to adjust the routing policies in the post switches to maintain the forwarding path. In our evaluations, the capacity of each switch is set to be 500.

Based on the above settings, we generate three configurations for the evaluations, as shown in Table 2, where the number of expanded policies is measured by the priority method, and the number of rules represents the entries used in TCAM-based switches. Note that CORA follows the heuristics to optimize the rule set, so given the pre-generated configurations, we could obtain a fixed optimized form from CORA. As a result, we do not need to re-perform CORA to report the average performance.


Table 2. The policy configurations used in the evaluations.

Topology	#expanded policies	#rules	Standard deviation	#overflow switches
Stanford	4193	4902	586.07	1
Fattree(4)	4094	5418	231.35	3
Fattree(8)	3507	4888	107.60	3
Stanford	11 375	12 876	2041.76	2
Fattree(4)	10 278	18 017	2811.52	3
Fattree(8)	8054	12 906	834.19	2
Stanford	45 175	47 719	5383.16	5
Fattree(4)	41 040	58 175	7913.27	4
Fattree(8)	32 592	34 479	2904.13	5
6.2. Semantics equivalence
We use header space analysis (HSA) to verify the semantics equivalence of CORA (Kazemian et al., 2012). Specifically, we test all pairwise connectivity on the generated topologies and policy sets, and record the internal forwarding path as well as the ID of actions when traversing the network. The results show that both the forwarding path and the triggered actions are the same before and after performing CORA. That is, the semantics-preserving transferring provided by CORA retains the high-level intents.

6.3. Placement optimization
We apply the heuristic algorithm proposed in Section 4 to reconcile an optimal placement that leads to lower cost of the global network. Two key metrics are measured to evaluate the performance of CORA, the global policy cost, and the standard deviation of policy placement. We use the priority method to optimize the policies in the single switch, and assume the data plane uses TCAM-based switches to demonstrate the significance, so that the policy cost is measured by the number of expanded rules.

Fig. 5 shows the policy cost decrement after performing CORA for three policy sets on different topologies. On average, 79.78% policy cost can be eliminated by properly transferring policies, and the decrement can go up to 96.31% for severe conflicting policy placement. Fig. 7 depicts similar improvements of standard deviation obtained by CORA, 96.22% and 99.73% decrements are achieved in average and at most, respectively.

In fact, the performance of CORA is determined by two factors. The first is the maldistribution of policies in the first place; if the standard deviation is high for the original placement, large profit can be expected by finding an even distribution solution. For example, 
 on Fattree () has a higher deviation than it on Fattree (), thus leads to a more significant decrement on the policy cost, as shown in Figs. 7(b), (c) and 5(b), (c). Second, the optimization efficiency of CORA is dependent on how many target switches can be transferred to for a single policy; more targets means larger searching space and better chance of achieving a more optimal solution. For example, due to the worse connectivity, policy sets on Stanford topology obtains lower decrements than them on Fattree () topology (80.15% vs. 85.42% in average), though it has more switches (26 vs. 20), as shown in Fig. 5(a), (b).

We further compare CORA with two existing approaches, i.e., packing rules with rectangle (Kang et al., 2013) (referred as Pack), and the local conflict resolver (referred as Local). Fig. 6 shows that CORA can further reduce 74.3% and 47.9% rules compared with Pack and Local, respectively. The reason to the boost of CORA mainly comes from the ability of arbitrary and flexible rule transferring. In contrast, Local only optimizes the local rule set, while Pack can only transfer the rules that can be covered by a rectangle.

Finally we want to measure the gap between CORA and the optimal solution. However, due to the extremely high complexity, we cannot obtain the optimal placement in feasible time for any of the three configurations. Instead, we use a tiny configuration that consists of only 4 switches and 100 endpoint policies, and the results show that CORA can achieve 100% optimality. We will quantify the optimality of CORA in real configurations in our future work.

6.4. Flow table aggregation
To check the compatibility with the existing local conflict resolvers, we apply the tree-based aggregation algorithm to enhance CORA.

By profiling the basic case in Fig. 5, we find that the number of policies explodes because the naive priority-based optimization cuts the big policies into a set of much smaller ones, which may aggravate the number of rules in total. The aggregation-based compression method can combine many of those policies into a new bigger policy, making the policy migration more efficient. As shown in Fig. 8, Fig. 9, the total the number of rules and the standard deviation are further decreased if applying this local optimization. This experiment proves that CORA performs better with the help of the local conflict resolver.

6.5. Overhead
We measure the time cost of performing CORA on different policy configurations to ensure the optimization can be done within acceptable overhead. Fig. 10 shows that most optimizations can be done within 40 s.

The impact factors of the time cost of CORA are similar with them of optimization efficiency, i.e., the distribution of policy and the connectivity of the topology. Maldistribution and better connectivity need more time to achieve a good placement. Taking the two Fattree topologies as an example, Fattree () has a large deviation with a relative worse connectivity, while in contrast, the policies are distributed more evenly in Fattree () that has better connectivity. Therefore, the time cost of these two topologies are similar, as shown in Fig. 10(b), (c). Such feature also improves the scalability of CORA when handling large topologies.

We next simulate the scenario that many policies are updated by the high-level intents. Specifically, we randomly add policies and delete policies on an optimized placement, and measure how long it takes to reach a new optimal placement for CORA. Fig. 11 shows the time cost when modifying 10%–25% of the policies on Fattree () topology. Due to the incremental update algorithm used in CORA, limited policies and switches are involved for recomputation, so the time cost is small ( s) and stable.

The above experiments also depict the minor additional time consumed by the aggregation optimizations, which also benefits from the incremental update strategy.

7. Discussion
In this paper, we only transfer endpoint policies, while the routing policies are considered as fixed to ensure the semantics equivalence of routing intent. Though there are many limits for us to transfer routing policies, the effect will be better if we can take routing policies into account, there will be more space for us to migrate and the process will be more flexible.

The global optimization can be further optimized if policies are separable. In the global optimization, when we transform policy to other switch, the policy is assumed as an entire one. This assumption simplifies the optimization process but it suffers a bit of performance. In some cases, the transformation of entire policy hardly decrease the storage cost. However, if splitting corresponding policies into several sub-policies, and translate part of them to other switch, flow table explosion can be extremely decreased. The process of global optimization need to be further modified to work out the optimal solution.

Besides, in SDN scenario, not just transfer, much more work can be done to further minimize the storage cost of conflicting policies since we master the policy distribution, flow table situation and the forwarding paths of the entire network in the control plane.

8. Related work
Many work have been done for efficient policy placement in the network. However, those works also have limitations in one or some of the following aspects. Table 3 briefly classifies those works, and we detail them in the following.

Although compressing the flow tables for IP lookup and firewall in traditional network has been widely studied, only a limited number of fields (mostly five tuples) are involved (Kogan et al., 2015, Liu and Gouda, 2010, Meiners et al., 2012, Bremler-Barr and Hendler, 2012, Katta et al., 2014, Liu et al., 2010). In SDN scenario, switches in the data plane may check an unbounded number of match fields to realize fine-grained flow control, which drastically increases the complexity of applying the traditional compressing methods. This phenomenon can be inferred from other approaches studying a broader range of SDN-based architecture (Bhatia et al., 2019a, Bhatia et al., 2019b, Bhatia et al., 2020, Trivedi et al., 2018). We believe with the global view offered by SDN, more benefit can be gained by properly transferring the policies. Besides, as we have mentioned in Section 1, all the existing optimizations for a single switch can be expediently employed in CORA.


Table 3. Previous literatures related to policy conflicts resolving.

Approaches	Basic idea	Weakness
Local compressor (Kogan et al., 2015, Liu and Gouda, 2010, Meiners et al., 2012, Bremler-Barr and Hendler, 2012, Katta et al., 2014, Liu et al., 2010)	Compress flow tables in a single switch	Less effective when handling high-dimension conflicts in SDN
Network slicing (Al-Shabibi et al., 2014, Koponen et al., 2014, Drutskoy et al., 2013)	Isolate flow space for each policy	Disable the operation on the same traffic
High-level DSL (Monsanto et al., 2013, Arashloo et al., 2016, Prakash et al., 2015, Amin et al., 2019, He et al., 2017, Tian et al., 2019)	Resolve conflicts at the language level	Cannot assume all policies are described with the same DSL.
Another attempt to avoid the policy conflicts is to slice the network into pieces, providing isolated flow spaces for different network functions or tenants (Al-Shabibi et al., 2014, Koponen et al., 2014, Drutskoy et al., 2013). However, such technique does not address the root cause of policy conflict: it is common to observe that more than one applications may be engaged in the same flow. Those applications will issue policies for sharing resources, e.g., overlapped flow space.

Many SDN languages are proposed to facilitate composing network with individual program pieces (Foster et al., 2013, Monsanto et al., 2013, Voellmy et al., 2013, Arashloo et al., 2016, Prakash et al., 2015, Amin et al., 2019, He et al., 2017, Tian et al., 2019). The corresponding controllers of these languages will carefully place and prioritize the generated policies, so that the conflicts are resolved in the first place. We believe in near future, the rule conflicts cannot be completely avoided or detected at policy level, considering the SDN programs would not be unified by a single “perfect” language. As such, if multiple controllers coexist in a single network, none of them can handle the conflicts raise by the same-priority policies. Previous work that efficiently place the endpoint policies in different priorities are not suitable for this scenario due to the similar reason (Kang et al., 2013, Kanizo et al., 2013).

Ref. Kang et al. (2013) is the closest work to ours, which also addresses the problem of endpoint policy placement. The basic idea is to recursively find a cost-effective “rectangle” to cover the overlapped rules, process the covered flows in current switch, and leave the rest flows to the next switch (next cover), such that the number of rules can be reduced. However, since Ref. Kang et al. (2013) only selects “rectangles” in flow space, the space of “packing rules” is much smaller than CORA that can move arbitrary (fractions of) rules.

The traditional placement problem is reduced to GAP, and many existing approximate algorithms can be leveraged to obtain an optimal solution (Shmoys and Tardos, 1993). However, if we involve the policy conflicts into the problem, the cost of assigning a policy to a switch is not independent, which cannot be transformed to the original version of GAP. Several papers have investigated GAP with dependent cost, while they either assume there is only pairwise dependency between assignments (Mougouei et al., 2017, Burg et al., 1999), or just employ a global dependent variable (Tariri, 2013). In contrast, POPDC introduces more complex dependency, where the cost of each assignment is dependent on all previous assignments.

9. Conclusion
In this work, we propose CORA, a conflict razor for policies in SDN. In contrast to the policy conflict resolution in a local switch by most of the previous works, CORA solve the same problem in a distributed way. Specifically, it first detects the significant conflict-maker and then migrates it from the local switch to other switches while retaining the semantic equivalence. Since the centralized controller can grasp the global view of the entire network, such global state coordination is feasible. In this work, we identify CORA’s NP hardness and propose a simple heuristic to approach the optimal solution within an acceptable time bound. Our experiments demonstrate that, CORA can effectively reduce the flow table storage occupation by at least 49% within less than 40 s. CORA can well collaborate with the existing local conflict resolver.

