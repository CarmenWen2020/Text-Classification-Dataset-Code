In this paper we study if feedback strategies during formative assessment with polling technology have an impact on learning gains. We conduct a randomized experiment in physics class in upper secondary education with the web-based polling system Socrative comparing three conditions. In the cooperative condition students receive a combination of peer discussion and teacher feedback, while in the individual condition they receive only teacher feedback. In the control condition students do not receive any feedback, from either teacher or peers. To measure what individuals learn from teacher feedback, whether or not combined with peer discussions, students in all three conditions individually answer paired isomorphic multiple-choice questions. Per question, we study the probability to answer the second isomorphic question correct and compare this between conditions The analyses show that teacher feedback, regardless of the peer discussions, positively affects learning gains in question pairs of the treatment conditions in comparison with the control condition. However, the cooperative condition shows that the largest learning gains occur when peer discussion is followed by teacher feedback. The findings provide insights into which feedback strategies affect learning gains in question pairs and are important for teachers when implementing feedback strategies during formative assessments.

Previous
Next 
Keywords
Feedback strategies

Learning gain

Polling systems

Physics

Secondary education

1. Introduction
Feedback is the core of formative assessment and one of the most important aspects of students' learning (Black & Wiliam, 1998; Hattie & Timperley, 2007). It is seen as an ongoing process in which students in dialogue with their teacher and peers are encouraged to monitor, evaluate and regulate their own learning, to develop their abilities as self-regulated, metacognitively aware learners (Boud & Molloy, 2013; Evans, 2013). Feedback provided by teachers or peers that promotes to modify the teaching and learning activity is regarded as the bedrock of formative assessment (Black & Wiliam, 1998). Students receive feedback from teachers or peers on their understanding to make informed decisions in their next step of learning, while teachers receive feedback from students in order to reflect on their teaching and peers receive feedback from other students to detect and correct errors in their reasoning. Formative assessments raise students’ understanding, enhance learning gains and improve metacognitive awareness and performance (Hattie & Timperley, 2007). It is sometimes not easy for teachers to organize formative assessments, because they do not know which of the students understands the course content (Dudaitė & Prakapas, 2017). Real-time data collected by student response systems or polling technologies can assist teachers in tailoring feedback (Chien, Lee, Li, & Chang, 2015). Formative assessments can be easily organized by polling technologies1 that provide students and teachers with instant feedback (Ludvigsen, Krumsvik, & Furnes, 2015; Shapiro et al., 2017). In these systems, students answer multiple-choice questions posed by their teacher by pressing a button on their clicker device, smartphone or similar web-based response system, after which they receive feedback from the system, their peers or teacher.

The effectiveness of a formative assessment on learning gains will depend, among other things, on the chosen feedback strategy. The effectiveness - measured by learning gains - of specific feedback strategies has been studied extensively. Several studies have shown that the number of multiple-choice questions answered correctly increases as students re-vote the same question after peer discussions (e.g. Egelandsdal & Krumsvik, 2017; Porter, Bailey Lee, Simon, & Zingaro, 2011, pp. 45–52; Smith et al., 2009; Zingaro & Porter, 2014). They all reported that students learn conceptually during peer discussions and that they do not merely copy answers from their peers. Smith et al. (2009) and Egelandsdal and Krumsvik (2017) showed that a combination of peer discussion followed by teacher feedback leads to substantially greater learning gains when compared to peer discussions or teacher feedback alone. Smith et al. (2009) and Porter et al. (2011, pp. 45–52) also both found that students show the largest learning gains when they first engage in a peer discussion and then listen to an explanation by the teacher.

However, most studies do not control for selection issues, as they are not based upon randomized experiments. One of the contributions of the study at hand is that we set up a randomized experiment, allowing us to attribute effects. Furthermore, our study is not only limited to a condition in which students only receive teacher feedback combined with peer conditions (as in the studies of e.g. Egelandsdal and Krumsvik (2017), Porter et al. (2011, pp. 45–52), Smith et al. (2009) and Zingaro and Porter (2014)), but also includes a condition in which students only receive teacher feedback. By comparing these two treatment conditions with a control condition, we can draw strong conclusions about the contribution of a specific feedback strategy to learning gains. Finally, while most studies are based on small samples in (mostly) university settings with a limited number of multiple-choice questions, we use results from a large randomized experiment in upper secondary education where formative assessments with polling systems are rapidly increasing and research into their effectiveness is deemed necessary (Molin, Haelermans, Cabus, & Groot, 2020).

The main contribution of this study is that we address whether technology/polling system supported assessment activities impact learning gains. We provide insight into feedback strategies and learning gains and compare learning gains in question pairs of treatment conditions with the control condition. The research questions that guide the study are:

1.
Do technology/polling system supported assessment activities enhance learning gains?

2.
Are learning gains modified by peer discussions?

This study proceeds with an overview of the literature in Section 2. The setup of the experiment and the identification strategy is explained in Section 3. Section 4 and Section 5 presents the results and Section 6 concludes the study and discusses the findings.

2. Literature
2.1. Non-explanatory feedback
The aim of providing feedback is to make the learning process visible to students (Hattie & Timperley, 2007). Feedback given does not only provide information about past performances, but also helps students (in the short term) to generate new knowledge and (in the long term) to improve performance (Egelandsdal & Krumsvik, 2019a; Jonsson, 2013). However, feedback does not automatically result in positive effects on student performance. Meta-analysis findings of Bangert-Drowns, Kulik, Kulik, and Morgan (1991) revealed that feedback will not affect performance when students are simply told whether their answer is correct or incorrect. Such limited feedback is not explanatory and insufficiently helps students to reflect on course content, to reveal misunderstandings, and to provide information about what is important to learn and what is needed to study further (Bangert-Drowns et al., 1991; Kluger & DeNisi, 1996; Rakoczy, Klieme, Bürgermeister, & Harks, 2008). The studies of Chen, Whittinghill, and Kadlowec (2010), Oswald, Blake, and Santiago (2014) and Yourstone, Kraye, and Albaum (2008) all showed that simply giving a polling system to students while not providing explanatory feedback, is not effective in improving learning performance, compared to students who receive explanatory feedback.

2.2. Teacher feedback
A more typical and commonly used way of using polling devices is to show the correct answer and provide teacher feedback (Bachman & Bachman, 2011; Mayer et al., 2009; Molin, Cabus, Haelermans, & Groot, 2019). Before showing the correct answer, voting results are collected and presented visually, for example as a histogram, in front of the class, offering immediate feedback to both students and the teacher. Showing visuals such as histograms helps students to reflect whether they understand the concept and offers them prompts to monitor their level of understanding in relation to their peers (Blasco-Arcas, Buil, Hernández-Ortega, & Sese, 2013; Sun, 2014). This histogram provides students with feedback in an anonymous way, showing that they are not alone in their thinking, especially when they answered the question incorrectly (Kay & LeSage, 2009; Perez et al., 2010). At the same time, the teacher receives feedback on the students’ understanding, which enables him/her to give a specific classroom explanation of the problem when describing the thought processes regarding the correct answer.2 This feedback informs all students about potential misunderstandings rather than only those students to whom teacher questions are directed (Faber, Luyten, & Visscher, 2017). Compared to just showing the given answers and mentioning which one is correct, teacher feedback is meaningful for all students and leads to a better understanding, as teachers have more subject-specific knowledge and use content-specific terms that provide cues on how to solve problems (Chin, 2006; Erdogan & Campbell, 2008; Voerman, Meijer, Korthagen, & Simons, 2012). Solving problems using a step-by-step demonstration provides clarity to students by identifying gaps in knowledge, helping them to correct misunderstandings and developing an understanding of expectations and standards (Juwah et al., 2004; Nicol & Macfarlane-Dick, 2006). However, even in situations where students answer a question correctly, teacher feedback may contain information on how the question can be addressed more effectively. The studies of Bachman and Bachman (2011), Campbell and Mayer (2009), Mayer et al. (2009) and Molin et al. (2019) reported that showing a histogram and hearing teachers providing an explanation for the correct answer positively affects performance on course exams.

2.3. Peer discussions combined with teacher feedback
In a more extensive way of polling device use, feedback is provided in a peer discussion. When peer and teacher feedback are combined, students first answer a multiple-choice question individually, discuss this question with their peers for a few minutes and then re-vote the question with a new, potentially revised answer based on the conversations with their peers, after which the teacher displays the histogram and reveals and explains the correct answer. This most popular method of polling device use is often called a cooperative approach, based on the work of Mazur (1997, pp. 9–18). By peer discussions, students get the opportunity to explain and justify their answer. They can try to convince their peers about the correctness of their answer and listen to the views and thought processes of their peers (Crossgrove & Curran, 2008; Egelandsdal & Krumsvik, 2019b, pp. 1–15; Lantz, 2010; Levesque, 2011). This combined approach of peer discussion followed by teacher explanation has been shown to improve student learning (Barth-Cohen et al., 2016; Levesque, 2011; Porter et al., 2013; Smith, Wood, Krauter, & Knight, 2011; Vickrey, Rosploch, Rahmanian, Pilarz, & Stains, 2015). During peer discussions, students share and (re)construct knowledge while building on their own understanding or on the idea of a peer. When students clarify their thinking and connect with peers who may have other ideas, they will be exposed to different ways of thinking, and build knowledge that may not have been available before, resulting in a reinforcement of understanding (Barth-Cohen et al., 2016; Chi, De Leeuw, Chiu, & LaVancher, 1994; Coleman, 1998; Knight, Wise, & Southard, 2013; Tullis & Goldstone, 2020). Previous studies indicated that the processes of self-explanation in peer discussions facilitates students to detect and correct errors in their reasoning (Aleven & Koedinger, 2002; Atkinson, Renkl, & Merrill, 2003; Chi et al., 1994; Mathan & Koedinger, 2005). Moreover, recipient students benefit from peer discussions as peers may be more effective explainers than teachers. Peers have a similar background and may be better at finding useful examples or clarifying misunderstandings than a teacher, as peers can describe concepts in a (more) similar language with familiar terms (Caldwell, 2007; Mc Donough & Foote, 2015; Perez et al., 2010; Tullis & Goldstone, 2020). Interactions with peers who may have different ideas or ways of explaining may help students to identify problems that they will not find alone (Knight et al., 2013; Zhonggen, 2017, pp. 158–171). In a peer discussion pedagogy, where the same question is re-answered, students have to decide (based on possible new ideas) whether to choose the same or a different answer. Several studies have shown that the frequency of correct voting increases after peer discussions (Crouch & Mazur, 2001; Egelandsdal & Krumsvik, 2017; Lasry, Charles, & Whittaker, 2016; Mazur, 1997, pp. 9–18; Porter et al., 2011; Smith et al., 2009, 2011, pp. 45–52; Vickrey et al., 2015; Zingaro & Porter, 2014). Learning gains are found because students in a peer condition are more inclined to switch from an incorrect vote to a correct vote than from a correct vote to an incorrect vote (Mazur, 1997, pp. 9–18; Miller, Schell, Ho, Lukoff, & Mazur, 2015; Tullis & Goldstone, 2020). An explanation for these learning gains can be that discussing answers with peers prompts students to clarify their thinking. Through self-explanations, students identify flaws in reasoning and through listening to the views and thought processes of peers, students build knowledge and obtain a deeper understanding of the course content (Chi et al., 1994; Coleman, 1998; Knight et al., 2013). Additionally, students can build on each other's line of reasoning, even when both students initially gave incorrect answers. They may still increase their understanding due to the opportunity of contrasting and comparing their thoughts and ideas (Smith et al., 2009; Versteeg, van Blankenstein, Putter, & Steendijk, 2019).

2.4. Learning from peer discussions
Another interpretation of ‘an increase in learning gains’ could be that students simply choose the same answer as their more skilled or more dominating peer during peer discussions; a situation where no learning actually takes place (Nielsen, Hansen-Nygård, & Stav, 2012; Wolfe, 2012). To completely preclude an eventual copying effect of answers, Smith et al. (2009) added a second question, which requires the same conceptual understanding of solutions, but which, like the first question, also needs to be answered individually. They found that students increase their correct answering by 21 percent points when voting this second question in relation to the first question. In a similar study, Egelandsdal and Krumsvik (2017) found an improvement of 12 percent points in correctly answering the second question in relation to the first question. The studies of Smith et al. (2009) and Egelandsdal and Krumsvik (2017) both showed that students who do not understand concepts at the first individual question, can learn from their peers and transfer acquired knowledge to new similar questions. Nevertheless, it is possible that students discuss incorrect ideas that are not captured in the answer options of the multiple-choice questions, or answer questions that are inconsistent with the ideas they discuss (James & Willoughby, 2011). For these reasons, it seems important for a teacher to combine peer discussions with teacher feedback, as this combination provides more understanding than the explanation of a peer or teacher alone (Hunsu, Adesope, & Bayly, 2016; Nielsen et al., 2012; Smith et al., 2011; Zingaro & Porter, 2014).

3. Materials and methods
3.1. The design of the experiment
3.1.1. Experimental design
To examine the impact of feedback on learning gains in question pairs in formative assessments with polling devices, we use isomorphic questions, in line with Smith et al. (2011), Porter et al. (2011, pp. 45–52) and Zingaro and Porter (2014). Accordingly, we set up an intervention with two treatment conditions and one untreated condition. We evaluate whether teacher feedback (an individual approach) or peer discussion followed by teacher feedback (a cooperative approach) leads to an improvement in students learning relative to the no feedback condition. This intervention includes three types of questions: ,  (PD is an abbreviation for “Peer Discussion”) and , as discussed further below.

The intervention took place over a period of 10 weeks. Each week students answered on average four pairs of questions  and . Each concept question  was followed by a paired isomorphic question , which is similar in difficulty or complexity and assesses the same understanding as the concept question , but varies in context or numerical values (Smith et al., 2011). Each concept and isomorphic question included four different answer options, of which one is correct. Incorrect answer options include a number of distractors or misconceptions and could not be eliminated without consideration. In addition, a fifth option “I don't know” was included (Caldwell, 2007; Kay & LeSage, 2009). This latter option should minimize the incidence of blindly guessing when students really do not understand the concept. To get instant feedback based on student answers, we used the student response system Socrative. This is a free of charge web-based student response system that is available via the website www.socrative.com and can be run on smartphones and laptops. It allows teachers in the Socrative Teacher module to easily prepare and manage formative assessments by controlling the flow of questions (Balta & Tzafilkou, 2019; Coca & Sliško, 2017). Students accessed Socrative using their smartphones and identified themselves by a six-digit student number in the Socrative Student module. As this number was randomly selected by the students themselves, students remained anonymous for the teacher.

3.1.2. Procedures
Each multiple-choice question in Socrative was displayed on the students’ small smartphone screens and on a large screen in front of the classroom. Depending on the difficulty of the question, students were given two to four min to consider the question individually, without any interaction between them. All student responses, including null responses, were collected by the Socrative software. Although all treated and untreated students used Socrative, the way in which feedback was provided to the students differs between treatment conditions.

Students in the individual condition received feedback on concept questions  of voting results with a histogram. This histogram showed the correct answer and incorrect answers, and a distribution (with percentages) of all anonymous responses. Simultaneously, the teacher explained the correct answer, with feedback aimed at correcting misunderstandings and flaws in logic, focusing on a step-by-step walkthrough of solving the problem. See the upper part of Fig. 1.

Fig. 1
Download : Download high-res image (344KB)
Download : Download full-size image
Fig. 1. Description of the experimental design.

After answering the concept question  individually, students in the cooperative condition did not receive immediate feedback. Instead of showing the histogram and revealing the correct answer, students were encouraged to discuss this question with their peers next to them for several minutes. Students explained and justified their thinking and tried to convince their peer why the answer they chose was correct. In the meantime, the teacher walked around in the classroom, listened to students’ reasoning and prompted students to discuss the reasons behind their answers. After discussions, students individually re-voted on the same identical concept question. After submitting a new (and potentially changed) vote , the teacher displayed a histogram of class responses and explained the correct answer, similar to in the individual condition (see middle part of Fig. 1).

Students in the control condition received no feedback on concept question , but only heard the correct answer from the teacher. They saw no histogram, received no problem-solving explanations from their teacher, and did not discuss their solutions with their peers (see bottom part of Fig. 1). To avoid confounding of time on task, we agreed with all teachers that the formative assessments would take place once a week at the end of a lesson. Each assessment lasted about 20 min, regardless of the condition.

Next, as a measure of what students individually learned from this process (independent of whether or not they did peer discussions or received teacher feedback), students in all three conditions were asked to individually answer the paired isomorphic question . Here, they did not have the opportunity to discuss this question with their peers. After recording the votes, the teacher in the individual and cooperative condition showed the histogram of students’ responses and explained the solution of , while the teacher in the control condition only revealed the correct answer without feedback (see the dashed parts of Fig. 1).3

Because students in the control condition did not receive any kind of feedback, we were aware that this could result in potentially penalizing effects. Therefore, we gave all students of all treatment conditions (control condition, individual condition and cooperative condition) the opportunity to voluntary view the questions on a website after each class and read the limited written feedback on all answer options. Although we do not have information about whether students made use of this website, we would expect that the natural behavior toward looking at this website is the same in all three conditions, given the complete random assignment of students into classes, and classes into conditions.

3.1.3. Physics questions
A physics teacher and the researchers developed a database, specifically for this purpose, containing 450 paired multiple-choice questions from which teachers chose questions that related to the current content of their physics course (mechanics, waves, thermodynamics, electromagnetism or modern physics). The  and  questions pairs originate from multiple-choice questions of standardized national physics exams constructed by the Dutch CITO (Dutch abbreviation for Central Institute for Test Development) and were selected for higher-order thinking skills (classified at the application and analysis level of Bloom's taxonomy (Bloom, Engelhart, Furst, Hill, & Krathwohl, 1956)) that involve deeper conceptual understanding by requiring several thinking steps. In line with the studies of Egelandsdal and Krumsvik (2017) and Smith et al. (2011) the order of questions in each pair were randomized to minimize any bias for asking easier  questions or to eliminate own preferences. Based on this randomization, each question pair was fixed and categorized as  and . All conditions answered  first, followed by .

Prior to the intervention, all question pairs used were judged for assessing the same understanding by two independent physics teachers who had experience with writing isomorphic questions. For each question pair, the requirement was that the questions were actually isomorphic and that the same physics principle should be solved. Pairs that were judged by both physics teachers as not testing identical concepts were completely removed from the database by the researchers. The database originally contained 474 question pairs. Both physics teachers independently rated 24 question pairs as not identical. These 24 non-identical question pairs were then completely removed from the database. Regarding the remaining 450 question pairs, we estimated a Cronbach's alpha of 0.81, indicating a high overall internal item consistency. Of these 450 question pairs, both physics teachers rated in total 17 question pairs as different (in which one teacher considered the questions as identical, but the other teacher did not). The 17 question pairs were discussed by both teachers and revised such that all question pairs were finally rated as isomorphic. Finally, the database contains 450 question pairs that are considered isomorphic by both physics teachers.

Each week, when preparing a formative assessment, teachers were completely free to select four paired sets ( and ) of multiple-choice questions that matched the content of their lessons. Each pair of questions was selected as a fixed pair in this process. See Fig. 2 for two examples of paired  and  questions. The correct answers in Fig. 2 are in bold typeface.

Fig. 2
Download : Download high-res image (2MB)
Download : Download full-size image
Fig. 2. Two examples of two paired questions used in this study.

3.2. Data
Our database contained 12,253 responses on question pairs. In order to control for differences between the students that answer the questions, we matched the students’ demographic information (gender, age, education level and physics test score of previous year) to each answered question pair by means of the unique six-digit student number. A closer look at the data showed that not all the answered question pairs met our requirements, as the data showed that in 585 occasions of responses a student in the cooperative condition did not mention the student number of the peer.4 Reasons for this could be that they forgot to fill in the number of the peer or that they were not sitting next to another student who acted as a peer, which are actually two very different situations with respect to getting feedback from a peer or not. Because it is not clear whether or not these questions the  vote was discussed with a peer, we excluded the 585 responses from our database.5 Therefore, we can delete student responses from the database (without violating the assumptions underlying the RCT). Furthermore, 67 responses of 7 questions pairs that were answered 100% correctly on average in a condition were also excluded from the database, as there were no learning gains possible.6 This concerned 14 responses on 1 question pair from students in the control condition, 35 responses on 3 question pair from students in the individual condition and 18 responses on 3 question pair from students in the cooperative condition.

The descriptive statistics of the remaining number of answered question pairs are summarized in Table 2. The final database contains in total 11,601 responses on question pairs and includes 1942 responses on 89 question pairs in the control condition, 4289 responses on 155 question pairs in the individual condition and 5370 responses on 195 question pairs in the cooperative condition. A total of 439 question pairs have been answered in one or more of our treatment conditions. Of these 439 question pairs, 99 question pairs (with a total 2709 responses) are answered in one or more classes in all three treatment conditions (not visible from Table 2).


Table 1. T-tests between students in the cooperative condition.

Students who answer one or more question pairs without peer discussions (N = 141)c	Students who answer all question pairs with peer discussions (N = 106)		
M	SD	M	SD	t	p
Gendera	0.51	0.50	0.48	0.49	0.46	0.65
Age	15.87	1.04	15.72	1.06	1.10	0.27
Education levelb	0.74	0.44	0.67	0.47	1.16	0.25
Physics test score previous year	6.62	1.05	6.61	1.02	0.10	0.92
*p < 0.10, **p < 0.05, ***p < 0.01.

a
Female = 1.

b
General secondary education = 0, pre-university education = 1.

c
141 students in the cooperative condition answered one or more question pairs without peer discussions. These students answered a total of 1–7 question pairs without a peer.


Table 2. Descriptive statistics of answered question pairs and responses in treated and untreated conditions.

Ns (number of students in the condition)	Nq (asked number of question pairs)	n (responses on question pairs of all students)	Min. (answered question pairs of a student)	Max. (answered question pairs of a student)
Control condition	111	89	1942	8	40
Individual condition	169	155	4289	8	40
Cooperative condition	247	195	5370	6	40
3.3. Descriptive statistics
Based on a survey, about 48% of the 527 students are female. Seventy percent of them are enrolled in the pre-university education track (the highest track in Dutch secondary education), the remainder of the students are enrolled in the general secondary education (the middle track in Dutch secondary education). At the start of the experiment, the students are on average 15.8 years old (SD = 0.94). The average score on physics tests in the previous year is 6.61 (SD = 0.96), where grades range between 1 and 10 (10 is outstanding and 5.5 is barely sufficient for passing).

The quality of the randomization was examined using analysis of variance (ANOVA) tests. Table 3 presents the comparison of students’ pre-treatment variables of the control condition and the two treated conditions (individual condition and cooperative condition). The ANOVA tests show that students in the treated conditions are, on average, similar to students of the control condition. In Sections 4.1 Results learning gains, 4.2 Robustness analyses of learning gains, we will control for all these variables in regressions to further reduce potential omitted variable bias.


Table 3. Comparison between untreated and treated conditions using ANOVA.

Control condition (N = 111)	Individual condition (N = 169)	Cooperative condition (N = 247)		
M	SD	M	SD	M	SD	F	P
Gendera	0.52	0.50	0.43	0.50	0.50	0.50	1.55	0.21
Age	15.61	0.67	15.78	0.93	15.80	1.05	1.61	0.20
Education levelb	0.68	0.47	0.72	0.45	0.71	0.46	0.28	0.75
Physics test score previous year	6.67	1.03	6.56	0.76	6.62	1.04	0.43	0.65
*p < 0.10, **p < 0.05, ***p < 0.01.

a
Female = 1.

b
General secondary education = 0, pre-university education = 1.

3.4. Participants and randomization
The study was conducted at the start of the school year 2018–2019 in six secondary schools in the southern part of the Netherlands. All schools are located in an area, outside the highly urbanised, central region of the Netherlands and are representative of the average secondary school in the Netherlands.7 In total, 629 students in 30 classes, belonging to the 10th to 12th grade (upper secondary education) of general higher secondary education (five years) and pre-university education (six years), are part of the experiment. The intervention lasted 10 weeks (from September to December). One week before the start of the experiment, students voluntarily filled in a survey in the presence of the teacher. This survey gathered demographic information (their self-chosen six-digit student number they use to log in Socrative, gender, age, education level and physics test score of previous year).

We excluded the question pairs from 92 students from the analysis because they attended very few classes or hardly participated in the weekly assessments (N = 57), left school or class during the duration of the experiment (N = 6) or did not fill out the survey of the experiment (N = 29). On top of that, the question pairs of another 10 students are excluded because they logged each assessment with pseudonyms (and not with their self-selected six-digit student number) into Socrative, so that the data from different assessments were not traceable to the students. The final sample includes 439 question pairs, nested in 527 students who are nested within 30 classes and taught by 12 teachers. Of these 30 classes, nine classes (N = 169) are in the individual condition, fifteen classes (N = 247) are in the cooperative condition and six classes (N = 111) are in the control condition.

Well before the start of the intervention, seven schools were selected, based on previous contacts of the authors, and contacted to determine their potential interest to participate in this study. The principals were reassured that the study would not interfere with other issues at school. Six schools consented to participate and agreed with the treatment conditions of the different groups. The principals of each school nominated the teachers and classes to participate in this study. To ensure that the intervention and the use of the technology would be used correctly, all teachers participated in a 1-h training session, which included an explanation of the motivation for the intervention, the requirements pertaining to the feedback under which the treatments had to occur and a training in how to use Socrative in the classroom. The teachers also received the database with paired multiple-choice questions for preparing the formative assessments.

We also explained to the principals of the schools the importance of randomization of the participants, and suggested randomizing students over classes before the timetables of the courses were made, to prevent timetabling issues interfering with the possibility of randomization. Therefore, the students at each school were first randomly assigned by scheduling software to one of the physics classes. The software randomly divided students, based on their courses and education level. Next, the researchers randomly assigned classes within schools to the individual condition, the cooperative condition or the control condition. As students are randomly assigned to classes, and classes are randomly assigned to teachers, in fact, students are also randomly assigned to questions pairs. By randomizing in this way, we tried to minimize possible teacher effects, as selection into the treated and untreated conditions did not depend on the teacher. By including six schools in the experiment, and by randomizing within schools, we also minimize possible school effects and increase the validity of our study (Molin et al., 2020).

3.5. Outcome variable
Students’ learning gains were assessed using the individual student responses in the weekly formative assessments. Each assessment contained on average four pairs of questions. For each pair, we used the aforementioned notation , , and , with the first vote (and second vote in the cooperative condition) taken on the first concept question and the second (third, in case of the cooperative condition) vote taken on the second isomorphic question. A score of one (1) was given for each question answered correctly and a score of zero (0) was given for each question answered incorrectly. To determine the average learning gain of an answered question pair in a condition, we averaged the results of each question pair  over all students in each condition, followed by computing the normalized learning gain, again over all students: (Hake, 1998; Marx & Cummings, 2007). Here, the average of  is divided by the maximum possible average gain , which is a reliable comparison of average gains between students with different  scores (Marx & Cummings, 2007; Porter et al., 2011, pp. 45–52; Taylor et al., 2018; Vickrey et al., 2015) and used as a standard metric to demonstrate improvements (Balta & Tzafilkou, 2019). By including the maximum possible average gain , we control for the likelihood that  has been answered incorrectly and simultaneously for the difficulty of the question pair. An extensive description of this empirical strategy can be found in the Appendix.

For our analyses, we aggregate the 11,601 student responses (for each of the two treatment and control conditions separately) into a new database with 439 question pairs, with the average ,  and scores determined for each question pair. In this new database, we calculate the average normalized learning gain  for each of the 439 question pairs and complement the average scores of the question pairs with the mean scores of the pre-treatment characteristics (gender, age, education level and physics test score previous year) of the students who answered that specific question pair.

In the present study, we use multivariate regressions to estimate the effects on learning gains of teacher feedback, or a combination of peer discussions and teacher feedback. Next to the treatment status, we control for the mean students’ pre-treatment characteristics gender, age, education level and physics test score previous year. The pre-treatment characteristics of students will mainly be used in the regression analysis to check for robustness of the results. To allow direct comparison of the findings, the scores of all variables included in the analysis are converted to standardized (z-) scores, except the dichotomous student variables gender (female = 1) and education level (pre-university education = 1).

4. Results
4.1. Results learning gains
Table 4 summarizes the results of the effect of feedback strategies with polling systems on learning gains (the average normalized learning gains  between  and , given the difficulty of the question pairs). See the Appendix of how learning gains  are defined. We compare the learning gains in the question pairs of the treatment conditions with the control condition. Model 1 is a basic model that only includes the treatment status. Compared to the control condition, a significant positive treatment effect is observed in the individual condition ( 
 = 0.25, p < 0.01) and in the cooperative condition ( 
 = 0.34, p < 0.01). This implies that teacher feedback, whether or not combined with peer discussions, positively affects learning outcomes compared with the control condition which do not receive any kind of feedback. In Model 2, we increase the precision of these estimates by including the covariates gender and age to the analysis (we remind that gender is the average female population and that age refers to the average age of the students who answered the question pairs). The effects of the treatments slightly decrease, but the significance is retained with 
 = 0.21 (p < 0.01) and 
 = 0.29 (p < 0.01), respectively. Furthermore, gender (
= - 0.31, p < 0.01) and age (
= 0.051, p < 0.05) are significant predictors of learning gains. This means that girls score 31 percent lower on learning gains compared to boys, while learning gains increase by 5.1% as age increases by 1 standard deviation.


Table 4. Multivariate regression analyses predicting learning gains.

Model 1
Learning gainsa	Model 2
Learning gainsa	Model 3
Learning gainsa	Model 4A
Learning gainsa	Model 4B
Learning gainsa
(β1) Individual condition	0.25***
(0.038)	0.21***
(0.042)	0.17***
(0.043)	0.17***
(0.043)	0.19***
(0.038)
(β2) Cooperative condition	0.34***
(0.036)	0.29***
(0.040)	0.29***
(0.039)	0.29***
(0.038)	0.28***
(0.034)

(γ1) Gender (female = 1)	No	- 0.31***
(0.11)	- 0.47***
(0.12)	- 0.43***
(0.12)	- 0.34***
(0.086)
(γ2) Age	No	0.051**
(0.020)	- 0.0018
(0.026)	- 0.0086
(0.026)	- 0.0062
(0.020)
(γ3) Education level	No	No	0.12***
(0.038)	0.13***
(0.038)	0.11***
(0.030)
(γ4) Physics test score prev. year	No	No	- 0.13**
(0.055)	- 0.14**
(0.055)	- 0.069
(0.052)

Fixed effectsb	No	No	No	Yes
time	Yes
teachers

Constant	0.31***
(0.031)	0.49***
(0.072)	0.48***
(0.071)	0.46***
(0.086)	0.43***
(0.052)

Observations	439	439	439	439	766
R2	0.18	0.21	0.26	0.26	0.26
F	43.13
(2, 429)	27.56
(4, 427)	24.42
(6, 425)	11.16
(15, 416)	21.99
(6, 759)
Significance level denoted at *p < 0.10, **p < 0.05, ***p < 0.01.

a
The control condition is the reference category of the treatment conditions.

b
Fixed effects are measured by including week dummies. There are 10 week dummies in total.

Adding the covariates ‘education level’ and ‘physics test score previous year’ (which are again average values of pre-treatment characteristics of students who answered the paired questions) to the analysis show that the effect of the treatment in Model 3 decreases for the individual condition (
 = 0.17, p < 0.01) and remains unchanged for the cooperative condition (
 = 0.29, p < 0.01). Furthermore, gender (
= - 0.47, p < 0.01), education level (
= 0.12, p < 0.01) and physics test score previous year (
= - 0.13, p < 0.05) are significant predictors of learning gains. This means that students with a pre-university education (six years) experience 12% more learning gains than students with a general higher secondary education (five years), which is a significant difference. Furthermore, learning gains decrease by 13% as students increase their physics score in previous year by 1 standard deviation. By adding the variable education level in Model 3, we notice that the variable age loses its significance. The reason for this is that education level significantly correlates with age (ρ = 0.32, p < 0.01), since students who are in pre-university education (six years) are also older than students who are in general higher secondary education (five years).

To see whether we control for all unobserved student characteristics that are constant over time, as hypothesized in the appendix, we control in Model 4 A for time dummies. The analysis in Model 4 A shows that the estimates of 
 remain constant as week dummies are included. As a robustness analysis, we controlled for teacher fixed effects in Model 4 B. This did not change our conclusions, with estimates of 
 = 0.19 (p < 0.01) and 
 = 0.28 (p < 0.01). From this we can conclude that results are not driven by differences in teacher characteristics across the classrooms and schools.

4.2. Robustness analyses of learning gains
To check further robustness of our results, we perform two additional analyses which are shown in Table 5. The first robustness check that we perform is an analysis for which we do not use all answered question pairs of our data, but only those pairs that are answered in all three conditions. It concerns in total 33 of the same question pairs in both the control condition, the individual condition and the cooperative condition. This limited number of question pairs is only 22.5% of all 439 questions pairs in total, but the results of the first analysis produce similar results for the treated conditions to those in Table 4. We have argued in the appendix that by the way we have measured learning gains , student ability and the level of difficulty of question pairs no longer play a role in explaining the results of learning gains. By randomly assigning students to classes, and, hereby, also to question pairs, we are allowed to include question pairs that are not asked in all treatment conditions (shown in Table 4).


Table 5. Robustness analyses with outcome learning gains.

Robustness analyses	Learning gainsa
Only paired questions asked in all three conditions	Learning gainsa
Only paired questions answered with a  correctness of <80%
(β1) Individual condition	0.13**
(0.078)	0.15***
(0.036)
(β2) Cooperative condition	0.33***
(0.091)	0.25***
(0.031)

(γ1) Gender (female = 1)	- 0.50*
(0.30)	- 0.40***
(0.11)
(γ2) Age	- 0.041
(0.060)	- 0.029
(0.023)
(γ3) Education level	0.085
(0.089)	0.12***
(0.035)
(γ4) Physics test score previous year	- 0.22
(0.15)	- 0.12**
(0.047)

Constant	0.51
(0.16)	0.38***
(0.062)

Observations	99	332
R2	0.26	0.28
F	5.96
(6, 89)	23.79
(6, 318)
Significance level denoted at *p < 0.10, **p < 0.05, ***p < 0.01.

a
The control condition is the reference category of the treatment conditions.

The second robustness check is an analysis that is done to check that the effects we find are not due to the fact that we have questions that are too easy in our data. As noted by Crouch and Mazur (2001), Smith et al. (2011) and Zingaro and Porter (2014), isomorphic questions that are too easy are insufficiently challenging and leave little opportunity for gains in learning. Although there is no standard cutoff for “too easy” (Zingaro & Porter, 2014), Smith et al. (2011) and Zingaro and Porter (2014) dropped questions where the individual  vote is more than 80% correct. If we also choose to do this, we exclude 107 question pairs out of 439 (10 question pairs from students in the control condition, 36 question pairs from students in the individual condition and 61 question pairs from students in the cooperative condition). The results of the second robustness check with “sufficiently challenging” question pairs show effects similar to those presented in Table 4.

All in all, we show that the results of learning gains  are robust to several model specifications and covariates. Therefore, we can conclude that teacher feedback or a combination of peer discussions and teacher feedback have a positive significant effect on learning gains.

5. Further evidence on potential learnings
In this section, we aim to provide insights into the contribution of peer discussions to the comprehension of concepts. Fig. 3 plots flowcharts of students’ response patterns of paired questions in the untreated and treated conditions. The flowcharts of the control condition and individual condition trace response patterns of concept questions  and paired isomorphic questions . The flowchart of the cooperative condition is expanded with response patterns of questions  (the re-votes of  after peer discussion). To make fair comparisons between results, we only included the 33 question pairs that are asked in all treatment conditions. By doing so, we only compare responses on question pairs of the same learning material and the same level of understanding.

Fig. 3
Download : Download high-res image (359KB)
Download : Download full-size image
Fig. 3. Flowchart of untreated and treated conditions8.

The top two branches of all flowcharts correspond to the percentages of  questions that are answered correctly (left branch) and incorrectly (right branch). The bottom branches correspond to the percentages of split subgroups of follow-up  questions that are answered correctly and incorrectly. The flowchart of the cooperative condition includes a middle layer with branches of percentages of  questions that are answered correctly (left branches) and incorrectly (right branches). The percentages in Fig. 3 are relative and compare the averages across all responses. For example, in the cooperative condition, 89.9% of the 41.7% correctly answered  questions are also answered correctly after peer discussions. Of this subgroup, another 72.6% of the  questions are also answered correctly.

In the light of the analysis of the contribution of the peer discussion to the comprehension of concepts, a group worthy of further study is the group of students in the cooperative condition who answer  incorrectly and  correctly. Porter et al. (2011, pp. 45–52) defined this group as the ‘potential learner group’ (PLG), as they can have the potential to learn from peer discussions. That is, they learn from their peers and master the concepts of  if they are able to transfer acquired knowledge to the new isomorphic question . The flowchart of the cooperative condition in our study shows that 66.5% of the potential learner group answer  correctly (Fig. 3). That is, over two-thirds of these students learn from peer discussions, combined with teacher feedback on . The extent to which this potential learner group learns can be determined by comparing them to a subgroup of students who already master the concepts of  from the start. By doing this, we select those students in the control condition who answered  correctly (Porter et al., 2011, pp. 45–52; Zingaro & Porter, 2014). These students belong to our reference group (REF), as they are expected to answer  correctly, without receiving feedback on the previous question . However, the flowchart in the control condition shows that not all students in this reference group answer  correctly; only 62.9% of the students who answer  correctly do so (Fig. 3). It is therefore recommended by Porter et al. (2011, pp. 45–52) and Zingaro and Porter (2014) to normalize the expectations of the potential learner group to answer  correctly based on this number. That is, as not all questions  are answered correctly in the reference group by the students that answered  correctly, we should reduce our expectations for the potential learner group in the cooperative condition.9 Doing so, we find that the potential learner group in the cooperative condition is 106% (= 66.5/62.9) as likely to answer  correct as the reference group in the control condition. This implies that proportionally more potential learners in the cooperative condition answer  correct than students in the control condition who understood the concept from the beginning.

It is also possible that students in the potential learner group may answer  correctly due to teacher feedback on , and not as a result of peer discussions (because they simply choose the same correct answer from their more skilled peer, without understanding the concept). In order to find out whether the potential learner group in the cooperative condition comprehends concepts from peer discussions, we examine a subgroup of students who answer  incorrectly, but in which the peers answer  correctly (the left part of Fig. 4), and another subgroup of students in which  is answered incorrectly by both the student and the peer (the right part of Fig. 4). We compare both subgroups to a subgroup of students of the individual condition who answer  incorrectly and  correctly. This subgroup in the individual condition does not understand the concept of  (as do students in the potential learner group in Fig. 4), but learn from teacher feedback (as well as the students in the potential learner group).10 The flowchart of the students in the potential learner group who answered  incorrectly while the peer answered  correctly (left part in Fig. 4) shows that 81.6% of the initially incorrect answers  are changed in correct answers . Of this subgroup, another 67.6% of the  questions are also answered correctly. Another student response pattern is observed when  is answered incorrectly by both the student and the peer (right part of Fig. 4); 21.4% of the answers are changed into a correct . However, of this subgroup, another 64.9% of the  questions are answered correctly. When we compare the flowcharts of two potential learner groups in Fig. 4 with the flowchart of the individual condition (Fig. 3), we observe that the 50.5% incorrect answers  that are changed into correct answers  in the individual condition is less than the 67.6% and 64.9% of the potential learner groups (Fig. 4). Based on these numbers, we hypothesize that the potential learners in the cooperative condition could learn from the peer discussions, and not solely from the teacher feedback.

Fig. 4
Download : Download high-res image (280KB)
Download : Download full-size image
Fig. 4. Flowcharts of two subgroups of students in the cooperative condition.

6. Conclusion and discussion
The purpose of the present study was to analyze whether technology/polling system supported assessment activities are effective in increasing learning gains in physics education. Therefore, we conducted a randomized experimental trial with sufficient statistical power in upper secondary education.

Our first research question was if technology/polling system supported assessment activities enhance students' learning gains. Therefore, we evaluated whether receiving teacher feedback on concept questions (an individual approach), or peer discussions followed by teacher feedback (a cooperative approach) led to an improvement of comprehension when answering a new isomorphic question relative to students who do not discuss their votes with peers, or do not receive teacher feedback (the control condition). The findings show that there are significant positive effects of teacher feedback, whether or not combined with peer discussions, on learning gains compared to students who receive no feedback. The results imply a Cohen's d effect size of 0.83 for teacher feedback and 1.13 for peer discussion combined with teacher feedback.11 Both effect sizes are large (Cohen, 1988) and according to Hattie and Timperley (2007) equivalent to effect sizes of providing cues to students (d = 1.10) and receiving feedback from teacher, students and peers on how to do solve problems more effectively (d = 0.95). Most improvement of learning gains occurs when peer discussion is immediately followed by teacher feedback. These results answer Porter et al.’s (2011, pp. 45–52) open question whether learning gains occur between  and  if, in addition to peer discussions, students also receive teacher feedback. Our results are also similar to what is found in previous studies in science classrooms (Barth-Cohen et al., 2016; Smith et al., 2009, 2011; Zingaro & Porter, 2014) and support the overall conclusions that a combination of peer discussion followed with teacher feedback provides more conceptual understanding than an explanation or feedback from a teacher alone (Hunsu et al., 2016; Nielsen et al., 2012; Smith et al., 2011; Zingaro & Porter, 2014). Additional analyses of our results show that student characteristics, time dummies and the level of difficulty of question pairs do not play a role in explaining the results of learning gains, on top of the differences due to feedback conditions.

The second research question explored whether learning gains are modified by peer discussions. The findings show that students in the cooperative condition who could learn from peer discussions (i.e. the students in the so-called potential learner group who answer  incorrectly and  correctly) are also likely to learn from peer discussions. These potential learners answer proportionally more  correctly than students who understood the concept of the questions at the beginning. These findings are in line with the findings of Smith et al. (2009) and Egelandsdal and Krumsvik (2017). We hypothesize that peer discussions help students to select the correct answer by prompting them to verbalize their solutions. Students learn from their peers and transfer acquired knowledge to new follow-up isomorphic questions. These findings are also in line with informal observations and conversations with students during our study. Students reveal that peer discussions break the monotony of passive listening to the teacher and encourage them to explain their own reasoning and listen to what others have to say. Peer discussions give them a sense of active participation and stimulate them to give a joint  answer. Students appreciate it when peer discussions are immediately followed by teacher feedback. They emphasize the importance of this feedback in order to fully understand solutions or explanations when students are not sure of the correctness of their votes.

6.1. Generalizability
The findings in this study are generalizable to other educational settings. We firmly believe that the results can be applied to other secondary schools in the Netherlands, as our schools are representative for the average secondary school in the Netherlands. We showed that all statistics for these schools are within half of a standard deviation of the average of all variables (Molin et al., 2020). We also argue that the results are not content specific, as similar results have been demonstrated across several disciplines within secondary education and higher education, such as psychology (Egelandsdal & Krumsvik, 2017), biology (Knight et al., 2013; Smith et al., 2009, 2011), computer science (Porter et al., 2011, pp. 45–52; Zingaro & Porter, 2014), engineering statistics (Kjolsing & Van Den Einde, 2016) and physics (Barth-Cohen et al., 2016; Pollock, Chasteen, Dubson, & Perkins, 2010). Furthermore, we used only question pairs that are isomorphic. To this purpose, pairs were randomized prior to the intervention and assessed for equality by two independent physics teachers, which implies that there is no reason to assume that there are learning gains from the fact that  questions would be easier than  questions. We also used the web-based program Socrative, which is very similar to the polling software Kahoot! and TurningPoint. For this reason, we assume that these polling programs will lead to the same results. Evidence using other software should confirm the generalization of our results.

6.2. Limitations
There are at least three limitations that need to be considered when interpreting our findings. First, we demonstrated that peer discussions combined with teacher feedback have a positive effect on learning gains, but we did not attempt to compare only peer discussion as a separate condition between  and . If we would have done this, we could have compared a peer discussion condition with a control condition and draw strong conclusions about the contribution of peer discussions to learning gains. Second, students in the cooperative condition were asked to discuss their answers on  with the student next to them in class. Given that students were free to choose a place in class, they probably tend to sit next to other students who share the same interests or who have a similar level of understanding, resulting in situations that students probably agree more than expected. Peer discussions could lead to even more verbal explanations and new knowledge if we randomly paired students (Tullis & Goldstone, 2020). In that case we would expect the results to be even larger than what we find in the study at hand. Future research should examine the extent of learning gains when students are randomly paired. Third, we used polling technologies that allow students to answer only multiple-choice questions with a limited number of possible answers. As students have to choose from a range of answers, polling technologies restrict teachers from posing questions that require students to evaluate issues or express their creative ideas. Multiple-choice questions cannot assess the highest levels (the evaluating and creating levels) in the Anderson and Bloom's taxonomy (Anderson & Bloom, 2001) as all answers are presented, preventing students from coming up with new models or hypotheses on their own (Crowe, Dirks, & Wenderoth, 2008). Thus, with respect to higher-order thinking, teachers using polling technologies in formative assessments are limited to asking questions at the application and analysis level (as occurred in the study at hand).

6.3. General conclusion
As a general conclusion, it can be stated that teacher feedback, whether or not combined with peer discussions, improves learning outcomes. However, the largest learning gains occur when peer discussions are immediately followed by teacher feedback. Analyses of pathways of responses in the flowcharts indicate that students in the cooperative condition are likely to learn from peer discussions. Although the results suggest that peer discussion combined with teacher feedback increase learning outcomes in the short-term, we do not know to what extent these gains prompt students to retrieve information for the long-term. We showed that students who both justify their own explanations and examine the explanations of their peers apply their acquired knowledge to new isomorphic questions. Following evidence from studies investigating the effects of feedback strategies with polling technologies on metacognitive awareness (Brady, Seli, & Rosenthal, 2013; Jones, Antonenko, & Greenwood, 2012; Molin et al., 2020), we predict that peer discussions combined with teacher feedback can not only enhance short-term learning, but can also support retention of information in the long-term. Further studies are needed to evaluate the long-term effectiveness of peer discussions and teacher feedback when using polling technology.