Large university classes often face challenges in enhancing active learning, repetition and feedback in the classroom which are essential for promoting student learning. In this study, we evaluated the implementation of digital tools (lecture recordings, question tool, classroom response system and virtual reality) regarding their perceived impact on active learning, repetition, and feedback in a large university class. The study applied a mixed methods design and collected data from a survey (95 students) and focus groups (11 students). The results show that students enjoyed using the tools because they enriched the lecture. However, students perceived differences regarding the impacts on active learning, repetition, and feedback. The perceived impacts of the classroom response system and the lecture recordings were rated high whereas the perceived impacts of the question tool and the VR modules were rated lower. Recommendations on how to use these digital tools in large classroom settings are provided.

Previous
Next 
Keywords
Higher education

Digital tools

Active learning

Repetition

Feedback

1. Introduction
Effective learning and teaching can be a challenge in traditional, large lecture classes (Yang, Ghislandi, & Dellantonio, 2018). Students listen passively and absorb the new information presented by the lecturer (Felder & Brent, 2016). They usually do not actively participate in the lecture and exchange ideas with the lecturer due to time and organizational constraints. Therefore, students lack the opportunity to actively think about the new information and to create a deep understanding of the taught material (Yang et al., 2018). The contents of the lectures are not brought into a larger context and remain a short-term rather than a long-term memory (Brown, Roediger, & McDaniel, 2014). Another obstacle in large lecture classes is that lecturers do not receive feedback on what students have understood and what they should further focus on. This prevents student-centered learning and focusing on the most important issues during the lecture. In a traditional, large classroom setting without digital technologies, the mass of students makes it hard for lecturers to integrate strategies and techniques promoting active learning, repetition, and feedback, which are essential for students' learning and performance (Schneider & Preckel, 2017). An active learning environment increases students' attention and engagement (Kay, MacDonald, & DiGiuseppe, 2019). Repetition allows students to study the lectures’ contents and memorize new information (Dahlin & Watkins, 2000), and regular feedback enables students to monitor their learning progress and discover knowledge gaps (Agarwal & Bain, 2019; Hattie & Timperley, 2007). These techniques also help students to transfer the new information from short-term to long-term memory (Brown et al., 2014). Digital tools can support active learning, repetition, and feedback in the classroom. The integration of digital technologies in education has increased significantly over the last decade (Hashim, 2018). However, while the importance of active learning, feedback, and repetition for traditional classroom settings has been demonstrated in many studies (Schneider & Preckel, 2017), research that examines the implementation of digital tools regarding the perceived impact on active learning, feedback, and repetition in large lecture classes is scarce. This lack of research is even more pronounced regarding newer digital technologies such as virtual reality and question tools. To fill this research gap, we developed an instrument to evaluate the implementation of digital tools regarding the perceived impact on active learning, repetition, and feedback. Four digital tools were used in a large psychology class: lecture recordings, a question tool, a classroom response system (CRS), and virtual reality (VR). Using a mixed methods design, we derived recommendations for using digital technologies to enhance active learning, repetition, and feedback in large university classes. In the following sections, we first discuss the four digital technologies and then we summarize the literature on active learning, feedback, and repetition related to our study.

1.1. Digital technologies used in our study
As the name implies, lecture recordings are recordings of the lecturer's screen and audio commentary. The recordings are uploaded to an online platform where the students can access them. Universities increasingly record lectures and make them available online for students (Brockfeld, Müller, & de Laffolie, 2018). They are much appreciated by students because they are useful for revision, exam preparation, catching-up on missed lectures, and taking notes at their own pace (Copley, 2007). Lecture recordings also increase exam performance (Nordmann, Calder, Bishop, Irwin, & Comber, 2019) and provide a high degree of freedom to study material when and where students want.

The question tool is a communication tool, which students use during class. Students can ask questions anonymously by entering them in a chat window using their smartphones or laptops. The questions are displayed on the lecturer's screen and the lecturer can answer them in class. This tool especially benefits students who are too shy and anxious to interrupt the lecture and it also increases participation (Montgomery, Evans, Harrison, & Damian, 2019) and improves interaction between students and lecturers (Palinko et al., 2018).

The CRS is an interactive electronic quiz tool (Castillo-Manzano, Castro-Nuño, López-Valpuesta, Sanz-Díaz, & Yñiguez, 2016). The lecturer presents multiple-choice or single-choice questions in the presentation and each student answers them on their smartphone or laptop. The CRS system collects all answers in real-time and the lecturer can display and elaborate on the results to the whole class. It provides the lecturer with insight to the students’ level of understanding of the topic (Wood & Shirazi, 2020) and because it is anonymous, shy students who normally would not participate in lectures are more likely to actively engage using the CRS (Heaslip, Donovan, & Cullen, 2014). Several meta-analysis measuring the effects of CRS have been conducted (Castillo-Manzano et al., 2016; Wood & Shirazi, 2020). Advantages of the CRS are that it increases enjoyment, attention, participation, and classroom dynamics (López-Quintero, Varo-Martínez, Ana, Laguna Luna & Pontes-Pedrajas, 2016), as well as academic achievements (Ma, Steger, Doolittle, & Stewart, 2018).

VR is a technology that uses computer hardware and software to create an artificial environment, which is similar to the real world. These virtual worlds are three-dimensional simulated environments (Chalil Madathil & Greenstein, 2017). Users are able to look around the artificial world, move around in it, and interact with virtual features or items. Only few studies have investigated VR in education and they reported positive impacts on variables like motivation (Villena Taranilla, Cózar-Gutiérrez, González-Calero, & López Cirugeda, 2019) and student performance (Ray & Deb, 2016). In general, VR in education is more commonly used in science, technology, mathematics, and medicine but less so in psychology (Radianti, Majchrzak, Fromm, & Wohlgenannt, 2020).

1.2. Active learning, repetition, and feedback
Active learning is a broadly investigated construct in education. It refers to a variety of activities, instructions, and interpretations (Freeman et al., 2014; Prince, 2004) which aim to promote student activity and engagement in the learning process (Prince, 2004) and to encourage students to think about what they are doing (Bonwell & Eison, 1991). Emphasis is put on developing students skills and student exploration rather than passively transmitting and receiving information (Bonwell & Eison, 1991). Research has demonstrated that active learning methods facilitate learning and increase student performance (Freeman et al., 2014). During a traditional, large lecture, active learning is hard to implement because lecturers have difficulties involving all students and providing them opportunities to participate and apply their newly acquired knowledge (Montgomery et al., 2019). Consequently, knowledge gaps and ambiguities remain undiscovered. Based on the literature, which is presented in the following, it can be assumed that the question tool, the CRS, and VR modules can support active learning in the classroom. Regarding the question tool, Palinko et al. (2018) found that students asked a lot more questions when using a question tool compared to raising hands and that activity and understanding of course content increased. Looi et al. (2010) discovered that question tools have the potential to create a flexible learning environment and to transform the lecture from teacher-centered to an active student-centered learning environment. Regarding the CRS, empirical research shows that it is suitable for promoting active learning in the classroom (Heaslip et al., 2014). It also aids the comprehension of the course contents (Fuad, Deb, Etim, & Gloster, 2018) and offers all students the opportunity to participate in class (Heaslip et al., 2014). It can also be assumed that VR environments promote active learning because they encourage “learning by doing” (Radianti et al., 2020) and exploring the virtual world (Fabris, Rathner, Fong, & Sevigny, 2019). Empirical research has shown that virtual reality environments increase motivation and engagement (Villena Taranilla et al., 2019), which are essential aspects of active learning.

Empirical research has also shown that repetition is beneficial for learning (Dahlin & Watkins, 2000). It is necessary to achieve proficiency and is positively linked with final exam performance (Andergassen, Mödritscher & Neumann et al., 2014). Dahlin and Watkins (2000) defined two purposes of repetition. Firstly, repetition creates a deep impression of the new information so that the information is engraved more deeply in the memory and secondly, a new understanding for the learned information is created. Ideally, the repetition is set up as a retrieval practice which – in contrast to simply re-reading the material – involves a certain amount of mental struggle, as this has been shown to increase long-term retainment (Agarwal & Bain, 2019). Based on the empirical research, we assume that lecture recordings and CRS promote repetition. Recent studies have shown that if lecture recordings are available, the majority of students use them to review lectures' contents and to deepen their knowledge (Morris, Swinnerton, & Coop, 2019). Using the CRS is also positively correlated with repetition in the classroom (Collier & Kawash, 2017). Repeating contents by answering multiple-choice questions strengthens the student's knowledge, allows information to be transmitted to long-term memory and improves learning success (Collier & Kawash, 2017).

Feedback in the classroom is an essential factor to promote learning and academic achievement (Hattie & Timperley, 2007; Schneider & Preckel, 2017). Feedback is a process during which students receive information on their current performance and understanding. It provides them the opportunity to monitor their learning progress, to recognize their strengths and weaknesses, and to see what they need to focus on (Hattie & Timperley, 2007) which is particularly important because students often overestimate how much they know and remember (Agarwal & Bain, 2019). It is important to close the gap between students’ current and desired level of understanding (Hattie & Timperley, 2007). In traditional, large lecture classes, it is often difficult to provide feedback to all students due to a lack of interactivity between students and lecturer (Yang et al., 2018). The CRS and question tool can provide a solution to this problem because they deliver individual feedback to all students. When using the CRS in class, students receive timely feedback to their responses. This helps them monitor their learning progress and identify knowledge gaps (Ismaile & Alhosban, 2018; López-Quintero, Varo-Martínez, Ana, Laguna-Luna, & Pontes-Pedrajas, 2016). When students use the question tool, they receive relevant feedback on what they have not understood. Even if they do not ask questions with the question tool themselves, they can see whether they are able to answer the questions other students ask and compare their understanding with that of other students (Montgomery et al., 2019).

1.3. Present study
Our study was conducted with first-year psychology students (bachelor's degree) who were enrolled in the course “General Psychology I”. The course consisted of 3 × 45 min lectures for 12 weeks during which students used the four digital tools. In the first lecture, the four tools were introduced to the students. From then on, every lecture was recorded and uploaded so that students could watch the lecture recordings at their leisure. In this study the lecture recordings included the lecture slides as well as the audio of the lecturer. The lecture recordings were edited (removing parts that were not part of the lecture, e.g., breaks, and boosting the volume of the audio track if the microphone setting had been too low) and uploaded to the course platform within a few days of the lecture. The question tool and the CRS were integrated into each lecture. Both are web applications, which students were able to access with their smartphones or laptops. The question tool had been custom made and allowed students to enter their questions anonymously. It was made available for the duration of the lecture and the submitted questions were only visible to the lecturer. The lecturer reviewed the questions during the break. Most questions (approximately 90%) were answered directly by the lecturer at the beginning of the following lecture. The whole process usually took about 10 min per 45 min lecture, resulting in an average duration of about 30 min per week. We decided to use the CRS by combining it with course revision tasks because repetition is crucial for long term retention (Pechenkina, Laurence, Oates, Eldridge, & Hunter, 2017). The CRS was used each week at the beginning of the first lecture. Students answered three single-choice questions. Overall results were shared with the class on screen. If the students requested it or if any question was only answered correctly by less than 20% of the students, the lecturer discussed the question and the answers in depth. The whole process usually took about 15 min. This approach was motivated by several goals: a) to encourage students to revisit the course content between lectures, b) to activate students during lectures, c) to give students a preview of typical exam questions, and d) to provide feedback to students and the lecturer on the students' learning progress. In the fifth lecture, the first of three VR modules which dealt with color perception, was shown to the students. The students were provided Google Cardboards by the university (free of charge) and they used their smart phones to access the web based VR. Each module required some form of simple interaction which was done by focusing the center circle on the desired option. The first module allowed students to experience the additivity in color perception by mixing light of different wave lengths (additive color mixing). Students could select light sources of different wavelengths and experience how their color perception changes. After the lecture, two further VR modules about distance perception were made available to the students which they could engage in voluntarily at home. The modules two and three simulated a car or a train, respectively, drawing nearer and then stopping in different light (day/night) and environment conditions (rain/snow/clear and headlights on/off). The students' task was to estimate the distance to the car/train. In the seventh lecture, another VR module about the actor-observer effect in an interview was made accessible. Using a 360-degree video showing a job interview, the students were asked to evaluate the candidate on three dimensions (ability to deal with criticism and conflict, service orientation, and expertise). The module took 4 min to complete, and the aggregated results were shown to the students and discussed in class to gain a deeper understanding on how the emotional expressions of the interviewers influence the perceived competences of the interviewed persons and experimental designs.

1.4. Research questions
This study investigates to which extent students perceive an effect of using the above-mentioned four digital tools on active learning, repetition, and feedback in the classroom. Based on the students’ perceptions it also develops ideas for further improving and developing the tools and their implementation. We posed the following research questions:

1.
How do students perceive the lecture recordings, the question tool, the CRS, and VR modules and their implementation in class?

2a.
How do students perceive the impact of using lecture recordings, the question tool, the CRS, and VR modules on active learning, repetition, and feedback?

2b.
How does the perceived impact vary between the tools?

3.
How can digital tools and their implementation be optimized and developed for future use?

1.5. Research design
In this study, a convergent mixed methods design was applied to answer the research questions (see Fig. 1). In this design, both quantitative and qualitative data are collected and analyzed separately, and results are integrated during the discussion using a side-by-side approach (Creswell & Creswell, 2018). A survey was conducted to collect the quantitative data after twelve weeks. One week later, at the end of the semester, qualitative data were collected using focus groups to get deeper insights into the students’ thoughts of the tools and their implementation in class.

Fig. 1
Download : Download high-res image (97KB)
Download : Download full-size image
Fig. 1. Convergent Design based on Creswell and Creswell (2018).

1.6. Research hypotheses
Based on the literature review, we had the following hypotheses:

Hypothesis 1a

Students perceive differences between the digital tools and their implementation regarding the impact on perceived active learning.

Hypothesis 1b

The question tool, the CRS, and VR have a higher impact on perceived active learning than lecture recordings.

Hypothesis 2a

Students perceive differences between the digital tools and their implementation regarding the impact on repetition.

Hypothesis 2b

The perceived impact on repetition of the lecture recordings and CRS are higher than the perceived impact on repetition of the question tool and VR.

Hypothesis 3a

Students perceive differences between the digital tools and their implementation regarding the impact on feedback.

Hypothesis 3b

For feedback, the CRS and the question tool have a higher perceived impact than VR and lecture recordings.

2. Quantitative study
2.1. Method
2.1.1. Participants
The study participants were students from a psychology class. All of them used the four digital tools during the semester. 95 out of 99 students completed the questionnaire successfully (Nmale = 19/Nfemale = 76). Questionnaires, which were not completed successfully, were excluded from the analysis. The study complied with the American Psychological Association Code of Ethics. All students gave their consent to participate in the study and to their data being used and processed.

2.1.2. Design
The study used a within-subjects design with the digital tools as an independent variable and active learning, repetition, and feedback as the dependent variable. The students completed a four-part questionnaire. Each part contained the same items to assess active learning, repetition, and feedback but referred to the various digital tools.

2.1.3. Questionnaire for active learning, repetition, and feedback (QARF 1.0)
The purpose of the quantitative study was to collect data about how students perceive the tools’ impact on active learning, repetition, and feedback. The questionnaire used in this study was developed according to the standards and guidelines of the APA for creating surveys (AERA, American Educational Research Association American Psychological Association & National Council on Measurement in Education, 2014). In the first step, a thorough literature research of the three constructs was conducted. Based on the findings, items were developed for each construct. In the next phase, the initial item list was discussed with two experts. During the discussion, the items were reviewed and adjusted. There were multiple discussion and improvement cycles until the questionnaire was finished. As a final step, the comprehensibility of the items was tested on three students who were familiar with the four digital tools. The feedback of these students was integrated into the final version of the questionnaire for active learning, repetition, and feedback (QARF 1.0). The final questionnaire (see Table 1) consisted of six items for each construct using a 7-point Likert Scale with a response format from 1 (very unlikely) to 7 (very likely). We also included one item for expected learning success for validation of the questionnaire.


Table 1. Items of the questionnaire for active learning, repetition, and feedback (QARF 1.0.).

Construct	Item
Active Learning	Using the [tool name] in this lecture encouraged me to ask questions during the lecture. (Q1)
By using the [tool name] in this lecture I was more active in the lecture. (Q2)
Using the [tool name] in this lecture made me explore the contents of the lecture more. (Q3)
By using the [tool name] in this lecture I was more attentive during the lecture. (Q4)
Using the [tool name] in this lecture encouraged me to think critically about what I am learning. (Q5)
Using the [tool name] made it easier for me to follow the lecture. (Q6)
Repetition	Using the [tool name] in this lecture helped me to better memorize the content of the lecture. (Q7)
Using the [tool name] in this lecture helped me to repeat the content of the lecture. (Q8)
Using the [tool name] in this lecture made me deal more often with the topics of the lecture. (Q9)
By repeating the content with the [tool name] I understood the topics of the lecture better. (Q10)
Repeating the content with the [tool name] helped me to develop a deeper understanding of the content. (Q11)
By repeating the content with the [tool name] I learned something new. (Q12)
Feedback	The use of the [tool name] in this lecture helped me to clarify misunderstandings. (Q13)
By using the [tool name] in this lecture I knew how well I mastered the content of the lecture. (Q14)
By using the [tool name] in this lecture I was aware of my current level of learning. (Q15)
Using the [tool name] in this lecture showed me what I still have to work on. (Q16)
The use of the [tool name] in this lecture showed me what content I need to further focus on. (Q17)
The use of the [tool name] in this lecture helped me to learn the content of the lecture. (Q18)
Expected learning success	The use of the [tool name] increased my learning success
2.1.4. Procedure
The participants completed the online survey at the end of their 12th and final lecture. It was a live survey, and all students present in the lecture were asked to participate. Completing the questionnaire was voluntary and took approximately 10 min.

2.1.5. Analysis
The collected data were analyzed with Jamovi Version 1.2.2.0 (the jamovi Thejamovi project, 2020). To validate the constructs active learning, repetition, and feedback and model their influence on expected learning success, partial least squares modeling (PLS) was used. Expected learning success is a single-item construct which is only used to link and validate the three primary constructs. PLS has become a key modeling approach for multivariate statistics in educational research (Lin et al., 2020) and is particularly suited to exploratory structural equation modeling and can model a latent factor with only one manifest variable (Hair, Hult, Ringle, & Sarstedt, 2016). We used Smart PLS version 3.3.2. Based on Hair et al. (2016), with a sample of 95 participants, we are able to detect effect sizes larger than 0.12 with a statistical power of 80% and α 0.05. Considering the high impact active learning, repetition, and feedback have on learning success (Freeman et al., 2014; Hattie & Timperley, 2007), we expect significantly higher effect sizes for which our sample size is sufficiently large. To analyze how students perceived the impact of the four digital tools on active learning, repetition, and feedback, one-way repeated measures analyses of variance (ANOVA) were used.

2.2. Results
2.2.1. Partial least squares analyses (PLSA)
To show the validity of the three factors active learning, repetition, and feedback, we employed PLSA. We added expected learning success – onto which each factor loaded directly – to represent all factors in a single model.

Measurement models. The outer standardized loadings for feedback, repetition, active learning, and expected learning success for each digital tool are shown in Table 2. For the CRS, the manifest item repetition 12 (“By repeating the content with the “tool” I learned something new”) had an outer loading of 0.641 and the manifest item active learning 13 (“The use of the [tool name] in this lecture helped me to clarify misunderstandings”) had an outer loading of 0.673. We decided to keep these items - even though items with outer loadings smaller than .7 are usually remove because their explanatory power is considered weak (Hair et al., 2016) - to maintain the comparability between all the tools, particularly regarding the ANOVAs (section 2.2.2.).


Table 2. Outer standardized loadings for active learning (AL), repetition (REP), and feedback (FB) for each tool (CRS, lecture recordings, question tool, and VR).

Lecture recordings	Question tool	CRS	VR
FB	REP	AL	FB	REP	AL	FB	REP	AL	FB	REP	AL
FB 1	.795			.795			.800			.875		
FB 2	.886			.886			.901			.957		
FB 3	.895			.895			.923			.937		
FB 4	.928			.928			.912			.945		
FB 5	.929			.929			.902			.881		
FB 6	.870			.870			.767			.814		
REP 7		.869			.869			.793			.854	
REP 8		.875			.875			.797			.907	
REP 9		.891			.891			.714			.866	
REP 10		.902			.902			.789			.924	
REP 11		.917			.917			.768			.949	
REP 12		.808			.808			.641			.836	
AL 13			.825			.825			.673			.730
AL 14			.938			.938			.785			.807
AL 15			.925			.925			.798			.896
AL 16			.933			.933			.899			.904
AL 17			.841			.841			.824			.835
AL 18			.904			.904			.850			.915
To determine the internal consistency of the measurement scales, we used Cronbach's alpha (Cronbach, 1988) (Table 3). All Cronbach alpha values were above 0.80 which is considered good (Nunnally, 1978). Composite reliability was very good with values above 0.90 (Brunner & Suss, 2005) and the average extracted variance was acceptable with values above 0.60 (Table 3).


Table 3. Cronbach's Alpha, Composite reliability, and Average Variance Extracted for active learning (AL), repetition (REP), and feedback (FB) for each tool (lecture recordings, question tool, CRS, and VR).

Tool	Construct	Cronbach's Alpha	Composite reliability	Average Variance Extracted
Lecture recordings	FB	.927	.942	.733
REP	.906	.928	.683
AL	.908	.927	.680
Question tool	FB	.944	.956	.783
REP	.940	.953	.770
AL	.950	.960	.802
CRS	FB	.934	.949	.756
REP	.847	.886	.566
AL	.892	.918	.653
VR	FB	.954	.963	.815
REP	.947	.958	.793
AL	.922	.940	.723
We calculated the discriminant validity with the heterotrait-multitrait (HTMT) approach (Hair et al., 2016) in Table 4. Values below 0.90 show discriminant validity between two reflective constructs.


Table 4. Heterotrait-multitrait between each reflective factor relationship per tool (Lecture recordings, Question tool, CRS, and VR).

Tool	FB<−> AL	ELS<−>AL	ELS<−>FB	REP<−>AL	REP<−>FB	REP<−>ELS
Lecture recordings	.509	.527	.688	.550	.848	.702
Question tool	.856	.876	.808	.866	.866	.796
CRS	.368	.482	.594	.743	.741	.663
VR	.805	.772	.728	.886	.834	.856
Structural Models. By bootstrapping, PLS can do significance testing with T-statistics (Hair et al., 2016). It does this by taking subsamples (we took 10′000) with replacement to calculate the standard errors which allows for the estimation of T-values for significance testing. Table 5 shows the path coefficients for each factor (active Learning (AL), feedback (FB), repetition (REP)) loading onto PLS per tool.


Table 5. Path coefficients for each factor loading onto PLS and R2 for expected learning success per tool (Lecture recordings, Question tool, CRS, and VR).

Tool	AL- > PLS	FB- > PLS	REP- > PLS	R2
Lecture recording	.223**	.334*	.293*	.548
Question tool	.569***	.222*	.126	.760
CRS	.167	.319**	.301*	.452
VR	.127	.111	.640***	.712
Note. Bootstrap with 10,000 samples. One-tailed significance testing. * Significant at α = 0.05. ** Significant at α = 0.01. *** Significant at α < 0.001.

The path coefficients for each factor loading onto expected learning success and the corresponding R2 value are shown for each tool in Table 5.

2.2.2. Analyses of variance (ANOVA)
We conducted one-way repeated measures ANOVAs to find out whether the four digital tools differ significantly from each other regarding their impact on active learning, repetition, and feedback. Fig. 2 displays the ratings of the digital tools’ impact on active learning, repetition, and feedback. Mean scores, standard deviations and one-way analysis of variance results are shown in Table 6. The ANOVAs revealed a significant main effect for all three variables.

Fig. 2
Download : Download high-res image (253KB)
Download : Download full-size image
Fig. 2. Ratings of tools' impact on active learning, repetition, and feedback. Note. Error bars are ± one standard error of the mean.


Table 6. Mean scores, Standard Deviations and One-Way Analysis of Variance results of the different tools regarding active learning, repetition, and feedback.

Measure	Lecture recordings	Question tool	CRS	VR	F	ηp2
M	SD	M	SD	M	SD	M	SD
Active learning	3.29	1.39	3.49	1.77	3.98	1.42	3.06	1.51	11.1***	0.11
Repetition	5.08	1.28	3.55	1.6	5.49	1.05	3.47	1.59	70.6***	0.43
Feedback	4.92	1.32	3.53	1.52	6.11	0.87	2.43	1.25	201***	0.68
Note. ***p < .001.

For active learning, CRS had the highest perceived impact followed by the question tool. Lecture recordings and VR were perceived to be less relevant. Post hoc pairwise comparisons showed that lecture recordings and question tool did not differ significantly from each other and neither did the lecture recordings and VR while all other means differed significantly from each other. Regarding repetition, CRS again had the highest impact. Followed by the lecture recordings, the question tool and VR. Post hoc pairwise comparisons indicated that the question tool and VR did not differ significantly from each other. All other tools were rated significantly differently. For feedback, we found that CRS had the highest rating and highest perceived impact on feedback followed by the lecture recordings. The impacts of question tool and VR were rated rather low. Post hoc pairwise comparisons indicated that all four digital tools differed significantly from each other.

3. Discussion
PLS analysis showed that active learning, repetition, and feedback meet the requirements of the measurement model, thereby supporting the validity of the constructs and the survey. The results of the ANOVA indicate that there are significant differences between the tools regarding their impact on active learning, repetition, and feedback. This confirms our Hypotheses 1a, 2a, and 3a. Students perceived that the CRS had the highest impact on all three factors. It is well suited to enhance active learning, repetition, and feedback in the classroom. In contrast to that, VR had the lowest perceived impact, and it can be assumed that it is less suited to enhance active learning, repetition, and feedback in the classroom.

The perceived impact of all four digital tools on active learning was relatively similar and rather low compared to repetition and feedback. Also, the expected results that CRS, the question tool, and VR modules would have a higher impact than lecture recordings were not met. The results showed that lecture recordings had a higher impact than VR modules. Therefore, the Hypothesis 1b, that the lecture recordings have the lowest impact on active learning, is false. A reason for the rather low impact on active learning could be that the digital tools were not implemented in a way that allowed for enough active learning. More activities might need to be implemented to meet that threshold. Comparing the four tools, the impact of CRS on active learning was rated the highest. VR modules had the lowest perceived impact on active learning, which is surprising because VR is used actively and promotes learning by doing (Radianti et al., 2020). Structural modelling for each digital tool showed that active learning had significant impacts on expected learning success for the question tool and lecture recordings, but not for the CRS and VR.

Regarding repetition, there were noticeable differences between the tools. The impact of the CRS was again rated the highest, followed by the lecture recordings, which were rated slightly lower. The CRS and lecture recordings had a significantly higher impact on repetition than the VR modules and the question tool which confirmed Hypothesis 2b. It can be concluded that both the CRS and lecture recordings are well suited to enhance memorizing and understanding course contents. In comparison, the perceived effects of the question tool and VR modules were rated lower. These results are not surprising because the tools are more suited to clarifying or exploring new aspects of the course content. Structural modelling showed that repetition had significant influence on expected learning success with VR having the highest loadings, followed by lecture recording and CRS and finally question tool.

Regarding feedback, there are also clear differences between the tools. However, the Hypothesis 3b was only partially met. We had expected that the CRS and the question tool would have a higher perceived effect on feedback than VR modules and lecture recordings. The results showed that CRS had indeed the highest perceived impact on feedback. But the perceived impact of the question tool was lower than the perceived impact of the lecture recordings. The results show that students see the CRS as a useful tool to monitor their own learning, to identify their knowledge gaps and to see what they need to focus on. The perceived impact of lecture recordings was also rated high. A reason for that could be that students have the option to clarify misunderstandings and to monitor what they already know or do not know by listening to lectures again. The question tool had a significantly lower perceived impact on feedback. This is surprising. We had assumed that the question tool would enhance the students' monitoring of their learning progress and the comparison of their performance with that of the other students. In addition, Montgomery et al. (2019) also found that a question tool promotes feedback. These results are supported by the structural model where feedback had a significant impact on the students’ expected learning success for lecture recording and the CRS, but not for the other two tools.

4. Qualitative study
4.1. Method
4.1.1. Participants
The participants were students of the same psychology class. They had taken part in the survey described in study 1. Therefore, they were well-informed about the investigated constructs of the study and the items used in the survey. Eleven students participated voluntarily in the focus group discussions. The first focus group consisted of four participants (4 females) and the second of seven participants (2 males and 5 females). Again, all participants gave their consent to participate in the study. This study also complied with the American Psychological Association Code of Ethics.

4.1.2. Materials
The purpose of the qualitative study was to get in-depth information on how students perceived the tools, the tools’ impact on active learning, repetition, and feedback and on how the tools and their implementation might be improved. The focus group guideline consisted of broad themes and open questions so that the moderator did not have to interact a lot with the participants. A discussion in a focus group usually begins with a warm-up and a broad theme and then goes into depth (Masadeh, 2012). Following that approach, the first part of the focus group consisted of an introduction and warm-up with some information regarding the study and the agenda of the focus group (3 min). The second part consisted of questions about how students perceived the implementation of the tools and what they liked or disliked about them (15 min). In the third phase, students were presented with a definition of active learning, repetition and feedback and were encouraged to discuss in-depth to what extent tools had an impact on these constructs (15 min). During the fourth part, students were asked to consider how tools and their implementation might be improved (15 min). In the last phase, the focus group ended with a closing statement (2 min).

4.1.3. Procedure
We organized two focus group discussions. The discussions took place at the end of the semester after the 12th lecture of the course “General Psychology”. The focus group sessions lasted around 50 min and were audio recorded.

4.1.4. Analysis
The audio recordings of the focus groups were transcribed and analyzed with MaxQDA. The analysis followed Knodel’s (1993) practical approach for designing and analyzing focus group studies. Knodel (1993) states that an analysis of focus group data consists of a mechanical and an interpretive part. The mechanical part includes organizing the data into segments and the interpretive part includes organizing the textual data into segments by coding the data. He calls the process of coding “Code Mapping”. The purpose of Code Mapping is to create a final Code System that includes all codes identified by the analysis (Fig. 3).

Fig. 3
Download : Download high-res image (581KB)
Download : Download full-size image
Fig. 3. Codesystem.

4.1.4.1. Objectivity and reliability
To increase objectivity in this study, the procedure of data analysis is clearly documented so that other researcher can replicate understand and verify findings. Reliability of the data can be assessed by comparing statements within and across focus group sessions (Knodel, 1993). In this study, the statements and codes of both focus groups were placed side by side and analyzed. Both groups answered similarly to the questions and identified the same key aspects. These results indicate a good level of reliability.

5. Results and discussion
General impression of the tools. The most discussed tools during the focus groups were the question tool and the VR modules. Participants described many positive but also some negative aspects of these two tools. They liked the question tool because they had the opportunity to ask questions throughout the entire lecture and it facilitated participation for shy students. They also thought that some of the questions were helpful for understanding the lectures’ contents and gaining more in-depth knowledge. On the other hand, they criticized that some questions asked by the students were not directly related to the lecture and therefore not interesting to all students. They also expressed negative views on the amount of time that was spent on answering some of the questions (“It takes so much time. Sometimes we only manage to go through 30 slides because we discuss things from the question tool for so long”).

Regarding the VR modules, students enjoyed getting to know a new technology (“I also encountered things that I had not known before. For example, VR is something I have never tried before.“). Also, VR introduced some enjoyable moments and variety to the lecture. However, they disliked parts of the implementation. Explaining how VR worked and getting it to run on all the students’ smartphones took a lot of time. Moreover, they said that while it was fun to use VR, it did not have an effect on their learning (“It takes a lot of time until everyone is ready, has the right browser and started the module on their smartphone. For me, the time spent and what you learn at the end of it do not stand in a good proportion.“).

Participants also discussed the CRS but in contrast to the other tools, they only mentioned positive aspects (“I liked the CRS most. I was always looking forward using it because it was fun”). They said it was a great way to repeat the lectures’ contents and a good preparation for the exam.

Lecture recordings were discussed the least. In general, students were content with this tool. It offered them the option to take notes at their own pace because if they missed something they could just watch the lecture again at home.

Tools' impact on active learning, repetition, and feedback. Referring to active learning, students mentioned the question tool both in a positive and negative way. It made them think more critically about the lectures’ contents to find out what they had not understood and how they would phrase it as a question. Doing that made them more focused and engaged during the lecture. On the other hand, some of the questions asked by students were irrelevant to the others and sometimes the time spent on answering the questions was quite long. The participants reported that they occasionally did not pay attention and did not follow the lecture anymore (“I noticed that I did not pay attention because the questions were not relevant for me.“). Regarding the other tools, students were not especially aware of their impact on active learning so there were only few mentions.

Regarding repetition, students mainly mentioned the CRS and lecture recordings in a positive way. There were hardly any mentions of the question tool and VR. The students said that the CRS helped to study and memorize lectures’ contents. They also described the lecture recordings as a great way to study lectures. If students had not understood something during the lesson, they could always go back to it with the lecture recordings.

Finally, regarding feedback participants especially mentioned the positive impact of the CRS. Students liked that it provided direct feedback, and that they immediately saw whether they understood the topics or not. The questions gave them hints about what they still needed to focus on (“You get direct feedback on whether you have understood something from the lecture or not. You also know which areas you might have to focus on or whether you still have questions about important topics.“). Participants also mentioned that the question tool provided feedback. The questions asked in the tool helped them monitor their understanding and compare themselves to other students (“For some questions, I noticed that I could answer them without any problems and for others I noticed that I should look at the topic again”).

Ideas for improvement. During the final part of the focus groups, the participants were asked how the digital tools might be improved to increase their impact on learning. In general, participants were very content and expressed the wish that the tools should also implemented in other courses as they see many benefits. However, some ideas for improvement were mentioned, especially for the question tool and the VR modules. A big concern was the number of questions asked with the question tool. Participants expressed an interest in reducing the number of questions. Questions should only be answered when they are relevant to all students and fit the course material. Participants suggested that questions in the question tool are filtered and summarized by the lecturer so that not all questions are answered. Another idea was the introduction of a voting system. With the voting system students could vote on questions of other students, for example with a thumbs-up button and lecturers could answer only most up-voted questions. Regarding the VR modules, participants proposed the development of other modules with a higher learning effect (“Maybe there are more complex topics where VR really helps to understand them”). For the VR modules they did at home, they would also appreciate direct feedback on how they had performed.

6. General discussion
In the following sections, results from both study methods are integrated using a side-by-side approach as suggested by Creswell and Creswell (2018) to answer the research questions and identify similarities and differences between the studies.

6.1. Perception of tools and implementation in class
The first research question focused on how students perceived the tools and their implementation during class. Data collected in the focus groups revealed that students enjoyed using the tools. They brought variety to the lecture and some of the tools supported their learning of contents. However, they perceived differences in the effectiveness of the tools. Results of the quantitative study indicate that students followed a pattern when rating the impact of a tool on active learning, repetition, and feedback. If the perceived impact of a tool on active learning was rated highly, the impact on repetition and feedback of this tool was rated similarly highly. A reason for that could be that students see tools as either effective or ineffective without differentiating if the tools have more or less impact on the different constructs. Regarding the tools, the lecture recordings received high rates for its perceived impact on repetition and feedback in the quantitative study. Students also emphasized in the qualitative study that it is a great tool to revisit the contents of the lecture and clarify open questions. Previous studies have also reported that students are enthusiastic about using lecture recordings (Copley, 2007) and that they gained more in-depth understanding, and clarification and that they were beneficial for exam preparation (Morris et al., 2019). The question tool had a low perceived impact compared to the lecture recordings and the CRS. Ratings were similar for active learning, repetition, and feedback. The focus group participants reported that they liked using the tool in general and the possibilities it offered but there were some aspects of implementation that need improvement. The impact of the CRS on students' perception of active learning, feedback, and repetition was rated the highest. Data from the focus groups support this impression. Participants enjoyed using the tool because it was fun and appropriate in terms of time and effort. It also helped them to learn the contents of the lecture and to prepare for the exam. These results are in line with current literature. Most studies investigating the effects of CRS in a classroom, identified positive aspects like an increase in attention, engagement (Wood & Shirazi, 2020), motivation (Licorish, Owen, Daniel, & George, 2018), interactivity (Heaslip et al., 2014), and academic performance (Ma et al., 2018). It was also shown that it provides instant feedback (Ismaile & Alhosban, 2018; López-Quintero et al., 2016) and is a suitable tool for student repetition (Collier & Kawash, 2017). In contrast to that, students rated the impact of the VR modules on all three constructs low. Focus group participants confirmed that VR modules were less useful to them. They enjoyed exploring the new technology, but it did not help them learn lectures’ contents. They also criticized that the VR environments were rather artificial and did not feel like the real-world. Because of that, we assume that immersion, which is an important aspect for effective implementation of VR in the classroom (Cheng & Tsai, 2019), was low.

6.2. Perceived tools’ impact on active learning, repetition, and feedback
The second research question aimed to investigate the tools' perceived impacts on active learning, repetition, and feedback and to what extent there are differences between the tools. Regarding active learning, all four tools had a similar and rather low perceived impact. Focus group participants mentioned that they did not feel more activated and focused during the lecture. A reason for this result could be that integrating digital tools in a traditional lecture class is not sufficient to create an active learning environment. Researchers have shown that collaboration between students and group work are an important factor to enhance active learning (Fidalgo-Blanco, Sein-Echaluce, & García-Peñalvo, 2019). The digital tools presented in this study, however, are more likely to be used individually. To promote active learning, the implementation of tools needs to be changed or further activities need to be integrated into the lecture. Among all four tools, the perceived impact of the CRS on active learning was rated highest. This supports the findings of other researchers who found that using the CRS increases participation and engagement in class (Heaslip et al., 2014). The impact of VR was rated the lowest which is surprising because VR is an active tool, which promotes learning by doing (Radianti et al., 2020). It also contradicts Fabris et al. (2019) who concluded from his meta-analysis that VR has the potential to act as an active learning tool as it encourages active participation and self-directed learning. A reason for the low perceived impact could be the implementation and design of the VR modules. Radianti et al. (2020) concluded from their meta-analysis that realistic surroundings and basic interaction design elements are essential for VR modules. Regarding both aspects, further improvement of the VR modules used in this study are necessary. For repetition, the quantitative data revealed clearer differences between the tools. Students rated the impact of the CRS and lecture recordings highest. During the focus groups, participants said that these two tools offer students the opportunity to improve their understanding and help them memorize the contents. Other studies have similar findings for the positive effects of CRS on repetition (Collier & Kawash, 2017) and of the lecture recordings on repetition (Morris et al., 2019). The perceived impact of the question tool and VR modules was rated low and during the focus groups, participants barely mentioned them while discussing repetition. This result is not surprising because both tools are more about exploring new aspects of the course material than repeating what they already know. Regarding feedback, again students rated the perceived impact of the CRS and lecture recordings the highest. Taking the quantitative and qualitative results together, we can conclude that the CRS helped students to monitor their own learning. The fact that the CRS provides instant feedback for students is supported by several other studies (Ismaile & Alhosban, 2018; López-Quintero et al., 2016; Wood & Shirazi, 2020). The results for the lecture recordings are surprising. During the focus groups, participants barely discussed the lecture recordings’ perceived impact on feedback. Only one participant said that the lecture recordings do not provide direct feedback, which is contradictory to the quantitative results. One reason could be that by listening to the lecture again, students receive some feedback by realizing which parts they have already understood and where they have gaps in their understanding. The question tool had a low impact on feedback according to the quantitative data. In comparison, focus group participants mentioned that the question tool had a positive influence on feedback. It allowed them to monitor their own learning process; see what questions they were able to answer themselves or what they still needed to focus on. They also had the opportunity to benchmark their knowledge with other students. The positive impact of a question tool on feedback was also investigated by Montgomery et al. (2019). A reason for the contradictory results could be that students perceived some negative aspects regarding the implementation of the question tool (too many irrelevant questions; too much time spent on answering questions) which negatively influenced the ratings.

6.3. Further development of tools and their implementation
The third research question asked how tools and their implementation can be improved. Results from the focus groups revealed that there is a lot of potential for the question tool and the VR modules. Students liked the question tool and the opportunities it delivered to them but also mentioned the need to change the integration of the tool in the lecture. The number of questions needs to be limited. Participants suggested guidelines for students that clarify which kind of questions are suitable or the integration of a voting system. Regarding VR modules, one of the main criticisms was that modules were not relevant for learning. Students saw no benefit compared to the traditional lecture. This begs the question for which kind of content VR is useful. Further investigation is necessary to determine whether the effectiveness of VR modules is limited to a topic or subject.

6.4. Limitations and further research
It is important to recognize limitations and lessons learned of the present study, and to derive ideas for future research. As it is the case for all studies using a highly realistic educational setting, our study has several limitations that could be addressed in future research. One limitation of our study is that usage differed across tools. During each week, about 30 min were devoted for the question tool and about 15 min for the CRS. E-lectures were provided to students for learning at their own pace at home. The VR modules were used in class by all attending students and they could also use them at home at their own pace. Although these differences in usage and implementation cannot explain the pattern of results that we found, further research is needed to examine whether different results would be obtained when using another instructional design and implementation of the digital technologies. It is reasonable to assume that in our study, VR, the question tool, and the CRS required active learning that go beyond mere “watching and listening to an instructor and taking notes” (Felder & Brent, 2016, p. 111) because active processing of information was required in our implementation. However, using learning analytics to collect data on how exactly the technologies are used by the students could provide interesting insights on how, for example, retrieval practice, spacing, and interleaving are related to perceived impact on active learning, feedback, and repetition. Another limitation is that we only assessed students’ perceptions. It would be valuable to conduct further research to examine how perceived impact relates to objective impact measured for example by exam grades. Also, investigating if some digital tools are more appealing for some types of students would bring new insights for improving the implementation of digital tools in the classroom of digital technologies in large classes.

A further area of investigation is the construct of perceived learning success. In our study we used a single item construct, which helped us validate the three main constructs active learning, feedback, and repetition with their respective scales and items. While single-item constructs can achieve high levels of validity (Jovanović & Lazić, 2020), developing a multiple-item construct for perceived learning success is recommended for future studies to examine the impacts of active learning, feedback, and repetition on perceived learning success.

6.5. Practical Implications
With the above limitations in mind, findings of our study offer several implications for educators, lecturers, and educational institutions. Traditional, large lecture classes often fail in providing effective learning environments for students (Yang et al., 2018). The implementation of digital tools in these classes has the potential to enhance active learning, repetition, and feedback and to promote effective learning. Digital tools are cost-effective, scalable, and easy to apply. They provide an easy way to add variety to the class and make it more exciting for students. The four digital tools evaluated in this study also offer the possibility to address students at a more individual level and to involve all students in the lecture. When implementing tools, lecturers need to keep in mind that the implementation is an important factor. Tools should not take too much time from the lecture. They should rather be used as a short and fun interruption to activate students and to keep the attention levels high. Moreover, contents presented with the tools should be closely monitored and they should complement the core of the lecture and not extend far beyond that. This is important for students to receive feedback on their current performance effectively. Another aspect is that lecturers need to moderate the tools. For example, the question tools can be moderated by using a voting system and responding only to the most voted questions. The CRS and VR modules should incorporate short, instant feedback on how students perform. Moreover, results of the study indicate that if lecturers want to enhance repetition or feedback in their class, the CRS is the most suitable tool among the four evaluated tools followed by the lecture recordings. The question tool and VR modules have a lower impact on perceived repetition and feedback if they are implemented as we did in this study. Lecturers need to consider the findings of this study regarding further ideas for improvement and development when implementing the question tool or VR modules.

7. Conclusion
Digital technologies offer new possibilities to increase active learning, feedback, and repetition in large lecture classes. We developed a method to evaluate the implementation of digital tools regarding perceived impact on active learning, feedback, and repetition. These are important factors for learning effectiveness. We found that students enjoyed using the digital tools, because they brought variety to the lecture and supported learning. The CRS was perceived as the most valuable for enhancing active learning, repetition, and feedback closely followed by the lecture recordings. By investigating the impacts of virtual reality, we also expanded the current knowledge about virtual reality in education. Based on our quantitative and qualitative results, we provided recommendations for improving the implementation of digital technologies in large lecture classes.

