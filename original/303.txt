The popularity of context-aware services is improving the quality of life, while raising serious privacy issues. In order for users to receive quality service, they are at risk of leaking private information by adversaries that are possibly eavesdropping on the data and/or by the untrusted service platform selling off its data to adversaries. Game theory has been utilized as a powerful tool to achieve privacy preservation by strategically balancing the trade-off between profit (service) and cost (data leakage) for the user. However, most of the existing schemes cannot fully exploit the power of game theory, as they fail to depict the mutual relationship between any two (of the three) parties involved: user, platform, and adversary. Existing schemes are also not always able to provide specific guidance for a user to reduce the impact of the joint threats from the platform and adversary. In this paper, we design a privacy-preserving game to quantify the three parties’ concerns and capture interactions between any two of them. We also identify the best strategy for each party at a fine-grained level, i.e. specific settings, not simply binary choices. We validate the performance of our proposed game model through both a theoretical analysis and experiments.

Keywords
Privacy protection
Game theory
Context-aware services


1. Introduction
Thanks to the rapid development and popularity of context-aware services, such as recommendation, navigation, and social association, individuals' lives have become more comfortable and convenient than ever before (Zheng and Cai, 2020; Cheng et al., 2021). We can use Yelp to find a popular restaurant, use Facebook to keep up with our friends, and use Google Maps to find the way to a destination. When enjoying such personalized services, we need to provide these service/application platforms with our personal data, e.g., location, weight, age, and income. Unfortunately, service platforms cannot always be trusted by users raising serious privacy issues, which lies in two aspects. On one hand, more personal data is needed to acquire higher quality of service, resulting in that even more private/sensitive information could be inferred from our submitted data. On the other hand, users' personal data may be shared or resold by service platforms to adversaries, as is common practice (Cai et al., 2018; Armstrong, 2016; Cai and He, 2019). Besides having a risk of leaking personal data via the platform, the data may be captured via malicious attacks, such as eavesdropping by an adversary. According to the statistics from (Wang et al., 2016), 55% of iOS applications and 59.7% of Android applications surreptitiously leak user's personal data.

Based on this, users are suffering joint threats of privacy leakage from untrusted platforms as well as adversaries, which we depict in Fig. 1. There is no doubt that, in the era of information, the collection and the use of personal data are major privacy concerns for individuals (Kokolakis, 2017), and such concerns will only grow over time (TRUSTe/NCSA, 2016). Thus, the ongoing progress of context-aware services, the increasingly serious privacy leakage, and the growing privacy concern together make data privacy preservation imperative for users.

Fig. 1

Fig. 1. Structure of thee-party game.

In the past years, privacy-preserving mechanisms have received a lot of attention from researchers. Besides cryptography, game theory has been widely applied as a strategic methodology to search for optimal strategies balancing the trade-off between the benefit of sharing data and cost of privacy disclosure (Sfar et al., 2017; Wu et al., 2017; Hussain et al., 2018). Note that most of the existing research only focuses on the interaction between two opposite parties (Shokri et al., 2017), i.e., using a defender-attacker game model. In (Li et al., 2018; Vakilinia et al., 2017), various three-party game models are proposed. But, the game models of (Wang et al., 2017a, 2017b) are not “real” three-party models, because they fail to depict the interaction between any two of the three parties, i.e. they considered either data resale by the platform or attacks by the adversary. Additionally, the schemes in (Wang et al., 2017a, 2017b; KarimiAdl et al., 2012) only provide a binary solution. Specifically, the schemes only determine whether the user should submit their data to receive services (and risk loss of privacy) or not submit any data (and risk poor, to no quality of service).

Further exploring the mutual relationships among user, platform, and adversary would be more helpful for the user to defend against both the untrusted platform and the adversary. Moreover, it would be beneficial to produce a more fine-grained solution, so that a user could have the option to provide obscured data and still receive adequate service. For this purpose, this paper aims to design a three-party game model among the three antithetic parties for users to simultaneously protect their privacy from untrusted service platforms and adversaries. Such a realistic and complicated game model challenges us in the following aspects: (i) Complicated game structure. As shown in Fig. 1, the interaction occurs between any two of the three parties, increasing the difficulty in addressing the three parties’ individual concerns – how does the user assess the potential risk of privacy loss and determine the granularity when submitting personal data; how does the platform determine data resale with consideration of the risk of reputation loss; and how does the adversary make a choice between purchase and eavesdropping? (ii) Joint threats. In such a complicated game, the user has to defend the joint threats from both the platform and the adversary, which may be hard to accomplish. (iii) Multiple data attributes. For many services, it is common that users need to submit multiple data attributes that could be correlated together. Any obscurity applied to one attribute would need to be correlated accordingly. (iv) Theoretical analysis & solution. Designing, analyzing, and solving the proposed three-party game are destined to be difficult due to the complexity of the game structure and correlated data attributes.

Our research endeavor to overcome the above challenges is briefly introduced as follows. Firstly, in our game model, we link the three parties by elaborately quantifying their concerns and mutual interactions, such that they are inseparable. Secondly, based on our game model, we perform a theoretical analysis to rigorously prove the optimal strategies of the three parties, including the optimal data release granularity for the user, the optimal data resale strategy for the platform, and the optimal probability to purchase data (or launch an attack) for the adversary. Finally, we conduct simulations with abstracted privacy protection settings from surveys to validate the effectiveness of our proposed game model.

To the best of our knowledge, we are the first to provide a fine-grained analysis on the behaviors and interactions for the user, platform, and adversary with considering resistance to the joint threats. Our major contributions are summarized as below:

•
We design a three-party game to capture the complicated interactions among the user, platform, and adversary, with a goal to defend against the joint threats to the user's privacy from both untrusted platform and adversary.

•
We present an in-depth theoretical analysis to identify the best strategy of each of the three parties: user, platform and adversary.

•
We perform comprehensive simulations with abstracted privacy protection settings from surveys to evaluate the performance of our game model, regarding the optimal strategy, cost, and utility for each of the three parties.

The rest of the paper is organized as follows. Section 2 summarizes the related work. Our game model is introduced in Section 3. The optimal strategy of each party and the performance of our game are analyzed in Section 4 and Section 5, respectively. Finally, Section 6 briefly concludes this paper and discusses our future work.

2. Related work
Game theory is a popular and efficient methodology to capture interaction between defender and adversary. In this section, we mainly summarize the most related literature in the area of game-theoretical privacy preservation, in which according to the type of game model, the existing work can be classified into two major categories, i.e., two-party and three-party game.

Most of the existing work investigates the interaction between two parties: user/data owner and adversary. In (Chorppath and Alpcan, 2013; Shokri et al., 2012, 2017; Rottondi et al., 2017; Sfar et al., 2017), games are based on a two-player model, i.e., one-against-one. When there are multiple users trying to maintain a certain privacy preserving level, the user-adversary game can be modeled as an n-player game (Wu et al., 2017; Liu et al., 2013; Freudiger et al., 2013; Ma et al., 2017; Ying and Nayak, 2017; Xu et al., 2017), but with the drawback that all users must have the same settings. Another drawback is that the two-party game cannot depict the interactions among three antithetic parties.

Recently, three-party game models have been proposed to study complicated privacy issues among user/data owner, service provider/data requester, and adversary. In (Li et al., 2018), Li et al. designed a hierarchical game, incorporating a user-service provider game and a user-attacker game, to maximize the service provider's utility while assisting the user in defending the attacker. Adl et al. (KarimiAdl et al., 2012) proposed a three-party sequential game to analyze the interactions among a data provider, a data collector, and a data user (i.e., the adversary), which can guide the data provider and the data collector to find the optimal strategies deciding whether to cooperate with the data user. In (Wang et al., 2017a, 2017b), Wang et al. studied the interactions among a user, an application, and an adversary to answer two questions: whether the user should submit data and whether the application should resell the user's data? To resolve the trade-off between sharing advantages and privacy exposure of cybersecurity information exchange system, Vakilinia et al. (Vakilinia et al., 2017) designed a three-party game for privacy-preserving cybersecurity information exchange framework consisting of an attacker, an organization, and a cybersecurity information exchange system. However, the three-party games in (Li et al., 2018; KarimiAdl et al., 2012; Wang et al., 2017a, 2017b; Vakilinia et al., 2017) fail to build the mutual interaction between any two of the three parties, and the strategy of each party in (KarimiAdl et al., 2012; Wang et al., 2017a, 2017b) is coarse-grained, or binary, by indicating “whether to cooperate with opponents or not”.

Contrasting from the existing work, we establish a three-party game to capture the mutual interaction between any two of the three antithetic parties (including user/data owner, service provider/data requester, and adversary) and aim to identify their strategies on “whether and how to defend (or cooperate with) others”, which can offer fine-grained guidance to the three parties.

3. Three-party game model
In this section, the interaction among user, platform, and adversary is modeled as a three-party game, in which their strategies, benefits, and costs are mathematically formulated.

3.1. User model
We consider the following scenario: a user submits personal dataset, denoted by D = {d1, d2, …, dn}, to a platform to acquire data-based service, where the dataset could contain one or more attributes and di (1 ≤ i ≤ n) is the data of attribute i. Due to privacy concerns, the user may report data attributes with different data release granularity. Formally, the data release granularity of attribute i is defined as gi ∈ [0, 1], and the corresponding data granularity set is G = {g1, g2, …, gi, …, gn}. Specifically, with a larger gi, the data of attribute i is less obscured, revealing more personal/sensitive information; for examples, gi = 0 if di does not contain any personal data, and gi = 1 if di is fully accurate. For example, when the user provides age information to the platform. It can use the age range instead of the exact range. If the user (assume age 24) sets the granularity to 0, the user will provide its age range 1–100. The provided user information actually provides no personal age information. If the user sets the granularity to 0.9, the user will provide an age range of 10, like 20–29 years old. If the user sets the granularity to 1, the user will provide the accurate age 24 to the platform. In this paper, we use data granularity as a measurement of data quality/obscurity.

As the data release granularity increases, the quality of user's requested service is increased with diminishing marginal benefit (Xu et al., 2015). Suppose that the quality of attribute i-based service can achieve a maximum value qi when gi = 1. Then, the relationship between the quality of attribute i-based service and data release granularity gi can be formulated to be 
. In addition, any two data attributes may correlate with each other, and such correlation can be exploited to infer more sensitive information (Zhu et al., 2015; Zhang et al., 2016). Let eij represent the correlation between attribute i and attribute j. Due to correlations among data attributes, the data of attribute i not only contributes to the quality of attribute i-based service, but also contributes to the quality of attribute j-based service. Thus, given the user's dataset D, data release granularity set G, and data correlation {eij}, the overall service quality can be estimated as follows.(1)

While enjoying the service provided by the platform, privacy leakage incurred by data submission brings privacy loss to the user. One possible method for this privacy loss could be due to a malicious attack by an adversary that eavesdrops on the data submitted by the user. In real-world scenarios, the working efficiency of information retrieval is restricted by many factors, such as equipment performance and retrieval technique. Different adversaries may have different work efficiency and different type of work efficiency. For example, when an adversary cannot always succeed when they launch an attack, the work efficiency could be the success rate. In another scenario, an adversary cannot acquire all the data in an attack. The work efficiency can be the expected percentage in an attack. The working efficiency of eavesdropping at the adversary side is denoted by φ ∈ [0, 1], so the granularity of eavesdropped data is φgi. Assume the adversary purchases data from the platform with probability b and the probability of eavesdropping is 1 − b. Then, the expected cost due to eavesdropping of dataset D is defined as
where ci is the unit privacy cost when gi = 1.

Another possible method for privacy loss could be that the user's submitted data is resold by the platform to a third-party (e.g., adversary) for more profit. To avoid too much sensitive information been sold, the platform will add noise or use privacy project techniques such as θ-differential privacy or k-anonymity before they sell the data. In this scenario, the selling strategy of the platform is the privacy protection level θ of differential privacy or 
 
 of k-anonymity. We define the set of platform's resale strategy as S = {s1, s2, …, sn}, where si ∈ [0, 1] and sigi is the resold data granularity of attribute i. The platform does not resell di if si = 0 but resell all collected di if si = 1. The expected privacy cost due to data resale at the platform side can be computed by

By combining the received service quality and the experienced privacy cost, the user's utility can be calculated in Eq. (2).(2) 
 where λ is the convention rate between service quality and privacy cost, i.e., one unit of privacy cost is equivalent to λ units of service quality loss. Moreover, λ is also used to measure the user's privacy preference; that is, the user would care more about privacy cost than service quality if λ is large, and the service quality outweighs the privacy cost if λ is small.

In our proposed three-party game, the user aims to maximize its utility by balancing the trade-off between service quality and privacy cost by strategically setting the granularity set G. Accordingly, the optimization problem at the user side is 
 
 

3.2. Platform model
The platform provides users with requested services based on their submitted data. For instance, Google provides navigation service to users based on their input location.

While providing service to the user, the platform has its private valuation, defined to be Vp, for the collected data from the user. With user's data, the platform can obtain profit from data-based production, such as data statistic analysis and new product development. From the viewpoint that data is a type of potential productivity, the value of data can be computed according to the standard form of Cobb-Douglas production function (Meeusen and van Den Broeck, 1977) as
where θp is the total value productivity of the platform, and ζp ∈ (0, 1) is the platform's value output elasticities of G.

To get extra benefits, the platform may resell the collected data to a third party (i.e., the adversary). Assume that pi is the unit data price of attributes i with gi = 1, so the expected payment received from the adversary is(3)
in which b is the adversary's purchase probability and si represents the platform's resale strategy.

However, reselling the user's data may cause the risk of reputation loss at the user side and/or in public. According to the instantaneous risk function (Fershtman and Kamien, 1987; Hu et al., 2015), we can define the risk of reputation loss due to data resale of attribute i as
where l1 and l2 are constant parameters of the risk estimation function. Since there may exist a correlation between two data attributes, the adversary can infer more personal/sensitive information from one data attribute to another, leading to an increase in the reputation loss at the platform side. Accordingly, the risk of reputation loss can be estimated as

In addition, there exists a data processing cost cp at the platform side. Since the data processing cost may be determined by the processing technology, which is out of the scope of this paper, we assume cp is a system parameter for simplicity. Therefore, the platform's utility, denoted by Up, can be defined to be(4) 
 

One can see that the platform faces a struggle between benefit and reputation cost from data resale. More specifically, reselling more accurate data can enhance the profit while damaging reputation, but reselling less accurate data can reduce reputation loss while losing attractiveness of data resale. Thus, to improve utility via balancing the trade-off between benefit and cost, the platform needs to choose a proper resale strategy S. Formally, the optimization problem of the platform is formulated as 
 
 

3.3. Adversary model
To retrieve the user's private information, the adversary could purchase data from the platform with probability b or eavesdrop on the communication between the user and the platform with probability 1 − b. With respect to each data attribute i, the granularity of purchased data is sigi, and that of the eavesdropped data is φgi.

The adversary also has private valuation for the obtained data. With the analysis similar to that in Section 3.2, we can utilize Cobb-Douglas production function (Meeusen and van Den Broeck, 1977) to compute adversary's private valuation as
where θa is the data productivity of the adversary and ζa ∈ (0, 1) is the adversary's value output elasticities of data.

We suppose that the adversary can obtain all the data in D through eavesdropping at a cost (e.g., equipment and time) that can be quantified by a quadratic cost function(Osborne, 2009; Mohsenian-Rad et al., 2010), i.e.,
where σ1 > 0, σ2 ≥ 0, and σ3 ≥ 0 are constant parameters of the quadratic cost function. Note that when the adversary does not eavesdrop, there still is a cost because it needs to purchase equipment and resources for eavesdropping. If the adversary chooses to purchase data from the platform, the expected payment paid to the platform is formulated in Eq. (3).

To sum up, the utility of the adversary, denoted by Ua, can be computed to be(5) 
 In the three-party game, the adversary faces the trade-off between data purchase and data eavesdropping, i.e., the probability to purchase/eavesdrop data. Therefore, to improve utility, the adversary has to choose a proper purchase probability b to maximize its utility, which can be formulated as the following optimization problem. 
 
 

4. Nash Equilibrium analysis
In this section, we conduct in-depth theoretical analysis of the three parties’ strategies and the relationships among their strategies.

4.1. Nash Equilibrium
In game theory, a Nash equilibrium is a strategy profile E∗ with the property that no party can unilaterally do better by choosing an action different from E∗, given that other parties adhere to E∗ (Osborne, 2009). Accordingly, the Nash equilibrium of our proposed three-party game can be defined in Definition 1.

Definition 1

A strategy profile E∗ = (G∗, S∗, b∗) is called Nash Equilibrium for the proposed three-party game if the following properties simultaneously hold: 
 

4.2. Strategy analysis of user
To solve the optimization problem of the user, we analyze the concavity of its utility function. The first-order partial derivative and the second-order partial derivatives of Eq. (2) are obtained, respectively. 
 
  
 
 
 

To find the maximum value, we need to solve the following system of equations.(6) 
 
 
 
 
 

All the solutions of the system of equations are the extreme points of user's utility. To find the global maximum value, we create the corresponding Hessian matrix: 
 
 
 
 
 
 
 
 
 
 
 

The user has a maximum utility only if the matrix is a negative definite matrix. When either of the following two conditions holds, a matrix is negative definite (Horn and Johnson, 2012): (1) all its eigenvalues are less than 0; and (2) the even order principal minors are larger than 0 and odd order principal minors are less than 0. In other words, when the Hessian matrix of the user's utility function can meet anyone of the above two conditions, the user's optimal strategy can be found by solving Eq. (6).

We take the scenario where eij = 0 for i, j ∈ [1, n] as an illustrative example. In this scenario, the first-order partial derivative and the second-order partial derivatives of the utility function are as follows.
 
 
 

Then we derive the corresponding Hessian matrix, i.e., 
 
 Because the even order principal minors are larger than 0 and the odd order principal minors are less than 0, the matrix Hu is a negative definite matrix. Therefore, the utility function has a maximum value and the maximum points can be calculated by solving Eq. (6), i.e.,(7) 
 
 
 
 
 

Since gi ∈ [0, 1], the best data release granularity for attribute i is 
.

From the results, to preserve data privacy, the user should decrease the data granularity gi of attribute i if the platform increases data resale strategy si.

4.3. Strategy analysis of platform
We can compute the Hessian matrix to analyze the concavity of the platform's utility function as follows. 
 
 
 
 
 
 
 
 
 
 
 where
 
and 
 
 

The platform has a maximum utility only if the Hessian matrix is a negative definite matrix that can satisfy either of the following two conditions (Horn and Johnson, 2012):

(1) all eigenvalues of Hp are less than 0; and (2) the even order principal minors of Hp are larger than 0 and odd order principal minors of Hp are less than 0.

If the maximum value exists, we can find the best strategy of the platform by solving the system of equations as shown below.(8) 
 
 
 
 
 

In Eq. (8), we have 
 
 

We use the scenario when eij = 0 (i, j ∈ [0, 1]) as an example for demonstration. In this scenario, the first-order partial derivative and the second-order partial derivatives of the utility function are obtained in the following.
 
 
 

Then we derive the Hessian matrix as: 
 
 

Because the even order principal minors are larger than 0 and the odd order principal minors are less than 0, the matrix Hp is a negative definite matrix. Thus, the platform's utility function has a maximum value and the maximum points can be calculated by solving Eq. (8), i.e.,(9) 
 
 
 
 
 

As si ∈ [0, 1], the best resale strategy for attribute i is 
.

According to the above analysis, to avoid too much reputation loss, the platform should decrease the value of si if the user increases gi. Nevertheless, the granularity of resold data, sigi, may be increased, bringing a profit increase to the platform. On the other hand, if the adversary prefers to purchase data rather than eavesdropping (i.e., enhance purchase probability b to a sufficiently large value), the platform can increase the value of si to earn more profit, in which the reputation loss maybe compensated by the payment from the adversary.

4.4. Strategy analysis of adversary
To maximize the utility, the adversary has to find the best strategy b∗. The first-order partial derivative and the second-order partial derivative of U with respect to b are respectively calculated by
 
and
 

Since 
 
, the utility function of the adversary is a concave function, which means the maximum value is achievable when 
 
. Thus, by setting 
 
, the solution is
 
Because b ∈ [0, 1], the best purchase probability is b∗ ​= ​max{ min{b, 1}, 0}.

According to the above result, one can find that the purchase probability b is reduced when the working efficiency of eavesdropping φ and/or data price pi increases. This is because the adversary prefers to eavesdrop rather than buying data for cost reduction if the granularity of eavesdropped data is higher than that of the purchased data. However, the relationship between the adversary's strategy and the platform's strategy and the relationship between the adversary's strategy and the user's strategy are not straightforward, because the purchase probability is also affected by the working efficiency φ, the price pi for attribute i, the data productivity of the adversary θa, and the data value output elasticities of adversary ζa. These complicated relationships will be investigated in our simulations.

4.5. Deployment in realistic scenario
The theoretical NE analysis provides general guidance for all realistic context-aware services. To deploy the proposed framework in a realistic scenario, we suggest following these steps. Step 1 - Data collection. When any role wants to use this framework, it should collect data of all the other roles as background knowledge to set up the parameters. Step 2 - Parameter setting. The collected data will be used to calculate the actual parameters by using linear regression. Step 3 - Check the NE conditions. We analyze the conditions that make sure NE exists. The parameters will be used to check the conditions to make sure NE exists in the specific scenario. Step 4 - Calculate the optimal strategy. If the NE exists in the specific scenario, the optimal strategy can be calculated by following Eqs. (4.2) and (4.3).

5. Simulation
In this section, we study the interactions among the user, the platform, and the adversary via intensive simulations. In this paper, we assume that the user has multiple attributes in its dataset. However, in some cases, the user's dataset has only one attribute. To provide a detailed simulation result, we study the interactions among three parties in two scenarios: i) the user has one attribute in its dataset; ii) the user has more than one attribute in its dataset.

5.1. Interactions among three parties with one attribute
We first discuss the interaction among the three parties when the user's dataset has only one attribute, D = {d1}. The default settings of the parameters are as follows. These value are selected because these parameters can provide better analysis that visualizing the theoretical NE analysis.

The granularity of d1 is g1 = 0.6. The unit privacy cost due to leakage of d1 is c1 = 3. Based on d1, the user can get maximum service quality q1 = 50. The convention rate λ of the user is 0.1. The platform resells d1 by using reselling strategy s1 = 0.6 with the price p1 = 20. The other parameters in the platform's utility function are: θp = 15, ζp = 0.6, l1 = 5, l2 = 10, cp = 1. The adversary has a purchase probability b = 0.6 and working efficiency φ = 0.2. The other parameters in the adversary's utility function are: σ1 = 1.5, σ2 = 1, σ3 = 1, θa = 15, ζa = 0.6.

The simulations that follow depict different strategies by varying certain parameters from the perspective of each of the three parties.

5.1.1. Simulation result of User's utility and optimal strategy
The results of the user's utility are presented in Figs. 2 and 3, from which we observe that the utility increases at first and then decreases as the granularity increases. The reason lies in two aspects: (i) when the granularity g1 increases from 0 to a certain value (e.g., 3.46 in line s1 = 0.8 of Fig. 2 and 3.55 in line b1 = 0.8 of Fig. 3), the increase rate of privacy cost is smaller than that of received service quality, therefore the utility increases; and (ii) when g1 continues increasing from such a certain value, the increase rate of privacy cost is larger than that of received service quality, leading to a decrease in the utility. In fact, such a certain value corresponds to the optimal granularity.

Besides, as shown in Fig. 2, the user's utility Uu decreases as the platform increases the value of its reselling strategy s1. This is because when the platform increases the value of reselling strategy s1, the granularity of the reselling data increases. That increases user's privacy cost, resulting in decreasing of user's utility.

Furthermore, the user's utility Uu also decreases as adversary increases its purchase probability as shown in Fig. 3. Because the purchased data of the adversary has a higher granularity than eavesdropped data, when the adversary increases the probability of purchase and decreases the probability of eavesdropping, the user has more privacy cost, leading to a decrease in the user's utility.

Fig. 4 states the optimal strategy of the user. We can see that the user decreases data granularity if the platform increases the value of reselling strategy s1. When the platform increases the value of reselling strategy s1, the granularity of the reselling data increases, thus increasing user's privacy cost. To reduce privacy cost, the user should decrease data granularity as shown in Fig. 4.

Fig. 4 also reveals how the user adjusts its strategy when the adversary uses different strategies. In Fig. 4, we can see that the three lines (b1 = 0.4, b1 = 0.6, and b1 = 0.8) intersect at the point where s1 = φ = 0.2. When s1 = φ, the adversary's purchased data has the same data granularity with eavesdropped data, the user has the same privacy cost no matter what the adversary prefers, purchase or eavesdropping. Thus, the user does not need to change its data granularity as the adversary change its strategy when s1 = φ.

Fig. 4 shows that the user adjusts its strategy according to the adversary's strategy as well as the platform's strategy: (i) the user decreases the granularity of the data as the adversary decreases the probability of data purchase and increases the probability of eavesdropping when the platform resells data with strategy s1 < φ. (ii) the user decreases the granularity of the data as the adversary increases the probability of data purchase and decreases the probability of eavesdropping when the platform resells data with strategy s1 > φ. The reason lies in two aspects: (i) when s1 < φ, the adversary's eavesdropped data has a higher granularity than purchased data. Thus, the eavesdropping causes more privacy cost than data reselling to the user. To decrease privacy cost, the user should decrease the data granularity if the adversary increases the probability of eavesdropping and decreases the probability of data purchase.

(ii) On the contrary, when s1 > φ, the adversary's purchased data has a higher granularity than eavesdropped data. Thus, the data reselling causes more privacy cost than eavesdropping to the user. To decrease privacy cost, the user should decrease the data granularity if the adversary decreases the probability of eavesdropping and increases the probability of data purchase.

5.1.2. Simulation result of Platform's utility and optimal strategy
From Fig. 5, we can tell that the platform's utility increases at first and then decreases over the increase of reselling strategy S = {s1}. When s1 increases from 0 to a certain value (e.g., 13.34 in line g1 = 0.8), the increase rate of the cost is smaller than that of the profit, resulting in an improvement of utility; however, when s1 continues increasing from such a certain value, the increase rate of the cost is larger than that of the profit, further reducing the utility. In other words, there is an optimal value of s1 for the platform to balance the profit of data resale and cost of reputation loss.

In Fig. 5, when the data granularity g1 increases, the granularity of reselling data increases and brings more profit to the platform, leading to the increase of utility of the platform.

Fig. 6 reveals how the platform adjusts its optimal strategy when the user and adversary choose different strategies. When the user's granularity increases, the optimal strategy of the platform decreases. Both the profit and the reputation loss increase if the user increases granularity. However, the higher profit cannot make up the increased reputation loss. Thus, the platform should decrease the value of s1 to reduce reputation loss.

Moreover, Fig. 6 reveals that the optimal strategy of the platform increases if the adversary increases its purchase probability b and decreases its eavesdropping probability 1 − b. When the adversary increases the purchase probability b, the expected payment to the platform increases. Thus, to earn more profit, the platform increases the value of s1 as the adversary increases the purchase probability b, as shown in Fig. 6.

5.1.3. Simulation result of Adversary's optimal strategy
The study of the adversary's optimal strategy is shown in Fig. 7. From this figure, we can see that the optimal strategy of the adversary decreases as the user increases the granularity g1 or the platform increases the value of its reselling strategy s1. When the user increases the granularity g1 or the platform increases the value of its reselling strategy s1, the granularity of reselling data increases correspondingly, resulting in the increasing of data's price. To decrease the payment, the adversary decreases the probability of data purchase (which also increases the probability of eavesdropping) when the user increases the granularity g1, or the platform increases the value of its reselling strategy s1 as shown in Fig. 7.

Fig. 7
Fig. 7. Optimal strategy of adversary under various G and b1.

5.2. Interactions among three parties with multiple attributes
According the aforementioned analysis, the theoretical optimal strategies of the user and the platform may not always exist. For computation feasibility, we utilize a parallel machining learning algorithm termed Particle Swarm Optimization (PSO) (Eberhart and Kennedy, 1995), to find the quazi-optimal strategies for the user and the platform, which is performed in the following manner: (i) we run the simulation 100 times; (ii) in each time, each initial strategy and the update vector in each iteration are randomly generated. and (iii) we use the strategy which has the largest utility as the final output. The results derived from PSO are consistent with that in single attribute scenario, thus validating the simulation result. For more details about the implementation of PSO, please refer to (Github).

We use abstracted privacy protection settings from surveys as the inputs of the user and platform. More specifically, based on the privacy survey published by IBM (IBM, 1999) and Data Protection Survey published by SANA (Filkins, 2017), we extract four user's strategies and four platform's strategies. As shown in Table 1, Gr, Gh, Gg, and Gf, are the user's strategies used for Retail applications, Healthcare applications, Government applications, and Financial applications, respectively. Correspondingly, in Table 1, Sr, Sh, Sg, and Sf are the strategies of Retail platforms, Healthcare platforms, Government platforms, and Financial platforms, respectively.


Table 1. Extracted strategies.

Application	Strategy setting of {Income, Age, Race}
Retail	Gr = {0.2, 0.3, 0.4}, Sr = {0.5, 0.7, 0.8}
Healthcare	Gh = {0.3, 0.4, 0.5}, Sh = {0.4, 0.6, 0.7}
Government	Gg = {0.4, 0.5, 0.7}, Sg = {0.3, 0.5, 0.6}
Financial	Gf = {0.6, 0.7, 0.8}, Sf = {0.2, 0.4, 0.5}
Each extracted strategy contains three data attributes, including income, age, and race. We set the correlation coefficient between income and age as 0.1, the correlation coefficient between income and race as 0.01, and the correlation coefficient between age and race as 0. For the three data attributes, the values of maximum service quality are q1 = 60, q2 = 50, and q3 = 40, the unit privacy costs are c1 = 15, c2 = 10, and c3 = 5, and the unit data prices are p1 = 20, p2 = 15, and p3 = 10. The convention rate λ in Eq. (2) is 0.1. The other parameters in the platform's utility function are: θp = 15, ζp = 0.6, l1 = 5, l2 = 10, and cp = 1. The adversary has a purchase probability b = 0.6 and working efficiency φ = 0.2. The other parameters in the adversary's utility function are: σ1 = 1.5, σ2 = 1, σ3 = 1, θa = 15, and ζa = 0.6.

The simulations that follow depict different strategies by varying certain parameters from the perspective of each of the three parties.

5.2.1. Results of User's utility and optimal strategy
We analyze user's utility by using real platform's strategy Sr, Sh, Sg, and Sf and increasing the granularity of each attribute from G0 to G8 as shown in Table 2. The results of the user's utility are presented in Fig. 8, from which we observe that the utility increases at first and then decreases as the granularity increases. The reason lies in two aspects: (i) when the granularity of each attribute in user's granularity set G increases from G0 to a certain granularity set (e.g., G6 in line S = Sg), the increase rate of privacy cost is smaller than that of received service quality, therefore the user's utility increases; and (ii) when the granularity of each attribute in user's granularity set G continues increasing from such a certain strategy set, the increase rate of privacy cost is larger than that of received service quality, leading to a decrease in the utility. In fact, such a certain granularity set corresponds to the optimal granularity set among granularity sets from G0 to G8.


Table 2. Strategy simulation setting.

Notation	Strategy Setting
G0, S0	{0.0, 0.1, 0.2}
G1, S1	{0.1, 0.2, 0.3}
G2, S2	{0.2, 0.3, 0.4}
G3, S3	{0.3, 0.4, 0.5}
G4, S4	{0.4, 0.5, 0.6}
G5, S5	{0.5, 0.6, 0.7}
G6, S6	{0.6, 0.7, 0.8}
G7, S7	{0.7, 0.8, 0.9}
G7, S7	{0.8, 0.9, 1.0}
Also, as shown in Fig. 8, the user's utility Uu decreases as the platform increases the value of resale strategy of each attribute from Sr to Sf. This is because when the platform increases the value of resale strategy, the granularity of the resale data increases, enhancing user's privacy cost and reducing user's utility. In particular, the user can gain a larger utility when using Financial Application than other applications because Financial Platform resells user's data with the lowest granularity.

The user's best strategies defending against the platform's different strategies and adversary's different strategies are respectively shown in Fig. 9, Fig. 10, where each line indicates the user's optimal data release granularity for one attribute. The results of Fig. 9 are obtained when the platform uses the strategies s1 = 0.2, s2 = 0.4, and s3 = 0.6. The results of Fig. 10 are obtained when the adversary adopts the data resale strategies b = 0.2, 0.4, 0.6.

Fig. 9 shows that the user decreases data release granularity to protect data privacy as the platform increases the data resale granularity from S0 to S8. On the other hand, when the value of resale strategy is small (e.g., S0, S1, S2), the user's optimal release granularity of data attribute 1 (corresponding to g1) is larger than those of attributes 2 and 3 (corresponding to g2 and g3, respectively). Since data attribute 1 has the largest service quality value q1, the user can get more profit from submitting data attribute 1, which can compensate the cost of privacy loss. On the contrary, when the value of resale strategy becomes large (e.g., S3, S4, S5, S6, S7, S8), releasing attribute 1 causes more privacy cost as data attribute 1 has the largest unit privacy cost c1. As a result, to reduce privacy cost, the user releases less information regarding data attribute 1, i.e., the optimal release granularity of attribute 1 is less than those of other two attributes.

From Fig. 10, we can see that the optimal release granularity g1 does not change as the adversary changes its strategy, and the optimal release granularities g2 and g3 decreases when the adversary increases the data purchase probability and decreases the eavesdropping probability.

When the platform resells attribute 1 (i.e., the line corresponds to g1), the platform uses resale strategy s1 = 0.2 that is the same as the working efficiency of eavesdropping φ. This means the granularity of purchased data and that of eavesdropped data are equal at the adversary side. Thus, the change of the adversary's strategy will not affect the user's utility. This is the reason why the best release granularity of data attribute 1 does not change when the adversary changes its strategy b under the scenario s1 = φ as we discussed for Fig. 4.

On the other hand, the adversary can get higher data granularities of attribute 2 and 3 (corresponding to g2 and g3, respectively) via purchasing rather than eavesdropping because s2 > φ and s3 > φ. Thus, the data resale from the platform causes more privacy cost than eavesdropping to the user. As a result, the best data release granularities g2 and g3 decreases as the adversary increases the data purchase probability and decreases the eavesdropping probability.

The changes of user's optimal release strategies in Figs. 9 and 10 confirms that data release strategy is also affected by data diversity (e.g., different data attributes have different privacy costs and resale prices).

5.2.2. Results of Platform's utility and optimal strategy
Fig. 11 reports the results of the platform's utility, in which the platform's strategies are set to be S0 to S8 according to Table 2 with user's strategy being Gr, Gh, Gg, and Gf as listed in Table 1. From Fig. 11, we can tell that the platform's utility increases at first and then decreases over the increase of the value of user's data release granularity, as we discussed for Fig. 5. When the data resale granularity of each attribute increases from the value in S0 to the value in a certain resale strategy set (e.g., S2 in line G = Gg), the increase rate of the cost is smaller than that of the profit, resulting in an improvement of utility to the platform; however, when the data resale granularity of each attribute continues increasing from the value in such a certain strategy set, the increase rate of the cost is larger than that of the profit, further reducing the platform's utility. In other words, there is an optimal resale strategy set for the platform to balance the profit of data resale and cost of reputation loss.

Fig. 11
Fig. 11. Utility of platform under various S and G.

Besides, the platform's utility increases as the user increases the release granularity of each attribute from Gr to Gf. This is because the platform can get more accurate data and more resale profit if the user increases the data release granularity. Particularly, the Financial platform can the highest utility because the user submits data with higher granularity to the Financial platform than other three platforms.

The optimal strategy for reselling each data attribute at the platform side are drawn in Fig. 12, from which one can observe that the optimal resale granularity of each attribute decreases when the corresponding data release granularity rises. Notice that as the data release granularity of each attribute increases, the growth rate of reputation cost from data resale becomes larger than the growth rate of the profit from data resale. Therefore, to reduce reputation cost, the platform decreases the resale granularity of each attribute.

Fig. 12
Fig. 12. Optimal strategy of platform under various G.

Fig. 13 presents the changes of platform's strategy when the adversary enhances the purchase probability. We can see that the optimal resale strategy for each attribute increases as the purchase probability is increased (i.e., the eavesdropping probability is reduced). When the adversary increases the probability of purchase (decreases the probability of eavesdropping), the increase rate of data resale profit is larger than the cost of reputation loss. Thus, to get more profit, the platform increases the data resale granularity.

Fig. 13
Fig. 13. Optimal strategy of platform under various b.

In Figs. 12 and 13, the value of optimal resale strategy of attribute 1 is higher than that of other attributes, for which the reasons lie in two aspects: (i) more information regarding data attribute 1 is released from the user (see Fig. 10), indicating that less obscured data is available for resale; and (ii) the unit price of attribute 1 is larger than those of other two attributes, implying that more payment can be received by reselling data of attribute 1. These results reflect the fact that a platform may adopt different resale strategies for different data attributes due to data diversity.

5.2.3. Simulation result of Adversary's optimal strategy
Fig. 14 shows the adversary's optimal strategy when the user and the platform use the strategies from Table 1. As shown in Fig. 14, the adversary decreases the data purchase probability (i.e., increases the eavesdropping probability) when the user increases the data release granularity of each attribute in the dataset from Gr to Gf, or the platform increases the data resale strategy of each attribute in the dataset from Sf to Sr. The increase of data release/resale granularity will raise the data price, so the adversary need to decreases the data purchase probability to save cost, which is the same as shown in Fig. 7.

Fig. 14
Fig. 14. Optimal strategy of adversary under various G and b.

5.2.4. Comparison with two-party game
In this subsection, a comparison between our proposed three-party game and the existing two-party game is performed. According to current research (Chorppath and Alpcan, 2013; Shokri et al., 2012, 2017; Rottondi et al., 2017; Sfar et al., 2017), there are two types of platforms in two-party game: (i) trusted platform that never resells user's data, and (ii) untrusted platform that resells all collected data.

Figs. 15 and 16 show the user's utilities and costs, respectively. On one hand, the user has the highest utility and the lowest cost when the platform is trusted because the potential privacy risk of data resale is ignored and the privacy cost is underestimated. On the other hand, the user has the lowest utility and highest cost when the platform is untrusted because the untrusted platform resells all its collected data and brings more privacy loss to the user. However, the trusted platform is an ideal model, and the untrusted platform, which sells off all data is rare in real life. A more realistic model is a platform that chooses the optimal strategy by balancing the tradeoff between profit and cost. Our three-party game model, captures the actions of this more realistic model.

Fig. 15
Fig. 15. Utility comparison of user under various platform.

Fig. 16
Fig. 16. Cost comparison of user under various platform.

From the comparison of platform's utility in Fig. 17, it can be seen that a platform can get the highest utility by adaptively reselling user's data; specifically, by adaptively reselling user's data, a platform can get more profit than the trusted platform and suffers less reputation cost than the untrusted platform. This is consistent with the fact that a platform usually owns the ability to adjust its strategy for enhancing profit, further confirming the effectiveness of our proposed game model.

Fig. 17
Fig. 17. Utility comparison of various platforms.

6. Conclusion and future work
This paper studies privacy preservation for context-aware services. To provide realistic optimal strategies for both the user and the platform, we propose a three-party game model that captures the interactions between any two of the parties: user, platform and adversary. Interactions include privacy leakage and data resale at the platform side, as well as malicious attacks at the adversary side. Our solution determines an optimal fine-grained strategy for the user and platform, so that the user can choose an optimal data granularity to balance service quality and privacy leakage and that the platform can choose the optimal reselling strategy to balance profit and reputation loss. Our model also accounts for the correlations between multiple data attributes provided by a user.

To find out the best strategy for each party, we conduct a rigorous theoretical analysis. We also perform simulations using abstracted privacy protection settings from surveys to validate the effectiveness of our proposed game model.

We plan to extend this work to an m-user scenario, where the interconnected behaviors of the multiple users, the platform, and the adversary become more complex than the proposed model. This extension will also need to involve another layer of interactions between the users themselves, further complicating the model.