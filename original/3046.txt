The purpose of this study was to examine one model for training new online instructors and measure the influence it had on their teaching effectiveness and knowledge integration. The Technological Pedagogical Content Knowledge (TPACK) model served as the conceptual framework. Using a quantitatively-driven mixed-methods design, three data sources were used: (1) instructors' pre and post-training course syllabi, (2) pre and post-training student evaluations of teaching scores, and (3) results of a follow-up online survey. The findings of this study revealed that instructors demonstrated: (a) statistically significant changes in their incorporation of elements into the redesign of course syllabi and (b) improvements in their teaching abilities as self-reported in the follow-up survey. However, there were no significant changes in their student evaluations of teaching pre- and post-training. Overall, instructors demonstrated modest improvements in their teaching effectiveness.

Previous
Next 
Keywords
Online teaching

TPACK

Teaching effectiveness

Higher education

Faculty development

1. Introduction
Recent decades show steady growth in online education, with institutions providing more online courses and programs (Allen and Seaman, 2013, Allen et al., 2016). Institutions of higher education continue to expand their online offerings to meet student demand and, in so doing, make college accessible to more students (Kampov-Polevoi, 2010; Picciano, 2006). As a result, instructors and course designers are now encouraged to convert more face-to-face courses to online formats in traditional brick and mortar institutions. Two important bodies of research show that (1) effective training can influence the quality of one's teaching (Cole et al., 2004; Knight, Carrese, & Wright, 2007; Steinert et al., 2006) and (2) instructors who teach online need thorough and continued support (Abel, 2005; Luck & McQuiggan, 2006; Reidinger & Rosenberg, 2006; Shelton, 2011; Smith, 2005).

Scholars in the field have identified many techniques, methods, and approaches to aid in the training and support of online instructors (Bailey & Card, 2009; Lackey, 2011; Lewis & Abdul-Hamid, 2006; Segrave, Holt, & Farmer, 2005; Young, 2006). Little is known, however, about the actual influence various approaches have on instructors (Wolf, 2006). Therefore, while it may frequently be assumed and expected that training programs have far-reaching impacts, rigorous evaluation studies are necessary to confirm whether this is true. Thus, the purpose of this study was to examine the relationship between one model of training for online faculty at a large, public research institution and the ways in which the program influenced the participants' online teaching effectiveness and knowledge integration. Thus, the study's guiding research question asked, “How did participating in an intensive course redesign intervention influence instructors' teaching effectiveness and knowledge integration?”

The design and results of this study contribute to the field in two ways. First, the findings provide insights about one model of training and its influence on teacher effectiveness. Four key features categorize the training program investigated in the present study; it was: (1) intensive (i.e., highly concentrated and thorough); (2) designed and led by experienced faculty developers; (3) highly interactive and hands-on, and (4) delivered in a hybrid format. By identifying if (and if so, how) this program model influenced teaching effectiveness online, the findings can guide the development of similar programs at other institutions. Second, this study begins to address the gap in the current literature about the relationship between the approaches used for preparing instructors to teach online and the influence they have on teaching effectiveness. The following section provides an overview of the theoretical framework and relevant literature that guided this study. Next, the methodology and findings are presented and the paper closes with a discussion of the findings and important considerations for practitioners.

1.1. Theoretical framework
The theoretical framework used to guide this study was the Technological Pedagogical Content Knowledge (TPACK) model, developed by Mishra and Koehler (2006). TPACK built upon the work of Shulman, 1986, Shulman, 1987, who first described the concept of pedagogical content knowledge (PCK) and posited that simply possessing content knowledge and basic pedagogical strategies is insufficient to capture the knowledge of effective teachers. Further, Shulman maintained that content knowledge and pedagogical knowledge should not be treated as mutually exclusive domains (Shulman, 1987). Mishra and Koehler argued that, while Schulman's framework still held true, it did not account for the tremendous growth in teaching and learning technologies, stating:

Technologies have come to the forefront of educational discourse because of the availability of a range of new, primarily digital, technologies and requirements for learning how to apply them to teaching (Mishra & Koehler, 2006, p. 1023).

Thus, Mishra and Koehler's extension, illustrated in Fig. 1, incorporated technology to better understand and describe the skills and knowledge necessary for effective pedagogical practice in technology-enhanced environments. TPACK addressed the integration of subject matter (content), technology, and an understanding of teaching and learning (pedagogy).

Fig. 1
Download : Download high-res image (80KB)
Download : Download full-size image
Fig. 1. Conceptualization of TCPK relationships.

Adapted from Mishra and Koehler (2006).
Mishra and Koehler (2006) also noted that many had viewed technology as a distinct component; however, they argued that this thinking was incorrect and that new and complicated relationships formed by the overlap and integration of these three elements. In their discussion of TPACK, Mishra and Koehler (2006) argued that simply knowing how to use technology differs from knowing how to teach with it. TPACK was useful to the present study during multiple stages of the research process and is directly applicable to faculty development for online instruction. The individual knowledge areas of the model (namely, pedagogy and technology) are the skills that developers emphasize in many training programs, such as the one studied here. Although the TPACK framework did not guide the development of the program, the model does serve as a lens for examining a past program that emphasized principles consistent with the framework.

2. Literature review
Gorsky and Blau (2009) defined the concept of teaching effectiveness as “how an instructor can best direct, facilitate, and support students toward certain academic ends, such as achievement and satisfaction” (para. 1). Researchers have investigated the notion of teacher effectiveness for decades in both face-to-face and online environments. Early research on this topic held that certain processes (conditions or teaching acts) influenced student outcomes and learning. This belief came to be known as the process-product model (Dunkin & Biddle, 1974) and, as Seidel and Shavelson (2007) noted, nearly all subsequent reviews and meta-analyses on teacher effectiveness were based on this model (see Anderson, 2004; Lipsey & Wilson, 1993; Scheerens & Bosker, 1997; Walberg & Paik, 2000; Wayne & Youngs, 2003).

In one of the seminal works on teaching effectiveness, also following the process-product line of thinking, Chickering and Gamson (1987) suggested that there are seven specific behaviors “intended as guidelines for faculty members, students, and administrators… to improve teaching and learning” in higher education (p. 3). These behaviors include: (1) encouraging student-faculty contact; (2) encouraging cooperation between students; (3) encouraging active learning; (4) providing prompt feedback; (5) emphasizing time on task; (6) communicating high expectations; and (7) respecting diverse talents and ways of learning. The work of Chickering and Gamson (1987) served as both a foundation and a framework for many other studies that contributed to the existing knowledge about teaching effectiveness. More recently, the focus on effective teaching shifted to online education (Arbaugh & Hornik, 2006; Bangert, 2006; Chickering & Ehrmann, 1996; Tirrelll & Quick, 2012).

Some researchers have suggested that high-quality educational development can influence the quality of one's teaching (Cole et al., 2004; Daly, 2012; Knight et al., 2007; Steinert et al., 2006). Researchers who have studied educational development for online instructors have utilized a variety of methodological approaches, but typically focus on one of two broad categories of impact: (1) beliefs, confidence, and attitudes or (2) teaching behaviors, abilities, and effectiveness.

Steinert et al. (2006) conducted a systematic review of faculty development initiatives designed to improve instructors' teaching effectiveness. By reviewing 53 studies, the researchers sought to identify the effects of these educational development interventions on the knowledge, attitudes, and skills of instructors in medical education. The interventions studied included workshops, seminars, courses, and longitudinal programs, and the researchers identified five common features. These features include: (1) high participant satisfaction with the interventions; (2) positive changes in attitudes toward teaching and faculty development; (3) changes in knowledge of key educational principles; (4) changes in behaviors; and (5) training that incorporated experiential learning, promoted collegial relationships, offered feedback from facilitators, and used a variety of educational methods in the training.

Knight et al. (2007) carried out a qualitative assessment of the long-term impact of one particular program by sending an open-ended questionnaire to 200 past participants who completed the training. Based on these results, the researchers identified four broad areas of impact that the program had on participants: (1) intrapersonal development (e.g., commitment to reflection, time management); (2) interpersonal development (e.g., better listening skills, and conflict management skills); (3) teacher development (e.g., overall teaching ability, confidence, satisfaction, and continued use of topics from training); and (4) career development (e.g., the influence on their careers or benefits of faculty development).

Roman, Kelsey, and Lin (2010) explored the topic of online instructor skill development and sought to identify the training content and methods of delivery that were most useful and beneficial to participants. Their study focused on two sections of a six-week intensive course designed to prepare 40 faculty to teach online at a large land grant university. The program was an immersive online course for instructors and focused on distant learner behaviors, communication practices, developing effective online teaching strategies, and classroom management techniques. Data sources included a survey of participants about their experiences and analysis of the participants' weekly written reflections about their teaching. The findings of this study revealed that approximately three-fourths of instructors believed that both their technological and pedagogical skills for teaching online improved; they also felt more comfortable teaching online because of the training. As the authors concluded, “The importance of technical and pedagogical support for online instructors cannot be overstated,” (Roman et al., 2010, sec. 12 para. 1), and suggested that future researchers investigate methods and forms of training programs used for online instructors.

Findings from the literature consistently demonstrate the importance of effectively using both technology and pedagogy in the online environment. As Koehler and Mishra (2005) argued, “Merely introducing technology to the educational process is not enough to ensure technology integration since technology alone does not lead to change” (p. 132). Some researchers have expanded on the distinction between the two by noting the differences between a “technology-driven” approach and one that is “pedagogy-driven” (Brinthaupt, Fisher, Gardner, Raffo, & Woodard, 2011; Clark-Ibanez & Scott, 2008; Colpaert, 2006; Fish & Wickersham, 2009; Snyder, 2009). A technology-driven approach occurs when an instructor pays less attention to how a specific technology might bolster the learning and teaching goals, and more to the integration of technological tools for their own sake. Conversely, a pedagogy-driven approach occurs when the instructor identifies what is necessary from a teaching and learning perspective, the best methods to achieve it, and then selects the available technology to make that possible (Colpaert, 2006).

Some studies have shown an increased emphasis on the importance of integrating both pedagogical and technological knowledge and the relationship between to two, rather than treating them as separate skillsets (Bailey & Card, 2009; Ertmer & Ottenbreit-Leftwich, 2010; Georgina & Hosford, 2009). Given this notion, knowledge in the field continues to expand via studies about the integration of technology and pedagogy via the TPACK model (Ashe & Bibi, 2011; Benson & Ward, 2013; Doering, Veletsianos, Scharber, & Miller, 2009; Koehler & Mishra, 2005; Koehler, Mishra, Hershey, & Peruski, 2004; Koehler, Mishra, & Yahya, 2007; Rientes, Brouwer, & Lygo-Baker, 2013; Stover & Veres, 2013).

In a study by Benson and Ward (2013), for example, the authors used the TPACK model as a framework for evaluating teacher expertise in a higher education context. The authors were interested in (a) understanding how the model manifests itself in practice and (b) identifying the factors that explain variation in the balanced depiction of the model (see Fig. 1). The authors studied three experienced, post-secondary online teachers and their courses by conducting in-depth interviews and by acting as observers in their courses. The researchers monitored four key aspects of each course: the syllabus, announcements, instructional modules, and the discussion board. By using thematic content analysis of their data, and by applying the TPACK components as a priori coding categories, the authors reached two key findings. First, the instructors had differing levels of knowledge in the three areas, although content knowledge was consistently among their most substantial areas of knowledge. Second, even when instructors possessed knowledge in the three areas, “the development of knowledge overlap depends on many factors and is a different process for each instructor” (p. 168).

Stover and Veres (2013) also conducted research using the TPACK framework in higher education. They were interested in using the framework as a way to understand participant learning in the various knowledge areas, since “the majority of college and universities have bifurcated professional development programs that address these types of knowledge separately” (p. 94). In their study, they asked eleven graduate students enrolled in an online Instructional Design course at a public university in the Midwest to self-report their learning of technology, pedagogy, content, and technology-pedagogy-content (TPACK) via a survey before and after the course ended. Using a paired t-test of the scores, the researchers found significant improvements across TPACK learning overall, but found that the participants rated themselves lower on technology learning than they did on the pedagogical and content knowledge areas.

The research reviewed here demonstrates a need for additional research that collects data from a variety of sources extending beyond self-report data (surveys, interviews, written reflections, etc.). While collecting self-report data can provide valuable information, it may also present research challenges. For example, social desirability bias among respondents and difficulty verifying self-report information are just two important considerations when drawing conclusions from such information (Sallis & Saelens, 2000). The present study attempts to address this issue by merging self-report data with additional sources in order to compile a more complete picture of the impacts of training. The present study also aims to build upon the notion that effective online teaching requires the integration of new kinds of knowledge (i.e., technology and pedagogy).

3. Methods
3.1. Research design
This study follows a concurrent embedded mixed methods research design, as described by Teddlie and Tashakkori (2009) and Creswell (2009). In this design, the researcher leverages both qualitative and quantitative methodologies to address the research questions. Creswell and Plano Clark (2011) expanded further on the notion of embedded research as one of the six major design types that involves the collection and analysis of both qualitative and quantitative data within a traditional design, which enhances the overall strategy. This study is a primarily quantitative design that is supported by qualitative data and methods.

3.1.1. Research setting and participants
This study took place at a large, high-research land grant university located in the southeastern United States. This institution, like many others, is moving toward increasing its online courses and programs. The training was offered during four consecutive summers (2011–2014) via a collaborative partnership between the faculty development and information technology centers on campus. The training lasted approximately three weeks, was delivered in a hybrid format, and was available to selected university instructors who were interested in redesigning an existing face-to-face course into a hybrid, flipped, or online format. The purpose of the training was to help participants through the redesign process, while also demonstrating best teaching practices and innovative uses of technology. In all, 92 full-time instructor participants from all across campus completed the program; in it, they learned course design elements and prepared their new online course sites, syllabi, lesson plans, assignments, and assessments. Individuals who finished the program and completed all program requirements received a $2500 stipend from the university.

To participate in this study, all participants met five key criteria. These individuals: (1) completed all requirements of the training program; (2) delivered their course in its redesigned format; (3) continued their employment at university for the duration of the study; (4) had regular employee status (i.e., graduate students were excluded); and (5) provided informed consent for analysis of their data and work products. These parameters reduced the population of instructors for this study from 92 to 28 (2011, n = 5; 2012, n = 11; 2013, n = 8; and 2014, n = 4).

3.1.2. Data sources
Three sources of data were used to answer the research question. First, pre- and post-training student evaluation of teaching scores (SETs) were collected for the course the instructors redesigned in the program. Specifically, pre-training data was for the face-to-face version of the course, and the post-training data was for the redesigned (online, hybrid, or flipped) course. These data were compared to determine what, if any, changes were noted by their students over time.

The university where this study took place collects a common set of four items on every course, regardless of discipline, course level, or student enrollment. All SET items were rated on a 5-point Likert scale (1 = Very Poor to 5 = Excellent) and included individual ratings for: (1) the course as a whole, (2) instructor contribution to the course, (3) instructor effectiveness in teaching the material, and (4) course organization. Additionally, for the purpose of this study, an average score of the four items was also calculated. SET data were retrieved from an online repository for every semester (pre- and post- training) that the course had been taught by the instructor participant. In preparation for analysis, the data were checked for normality and outliers via boxplots. Only one outlier was detected in a single case across all four items; further inspection revealed the case to be extreme, and it was excluded from analysis. A second series of Shapiro Wilk's tests confirmed normality for the remaining sample (n = 27) across each of the four areas. These data were aggregated and analyzed for all instructors across each of the four items, as well as for the average score. A paired t-test measured the difference between the pre and post-training scores for all participants by item as well as for the average score.

As the second data source, versions of each participant's pre- and post-training syllabi for the redesigned course were compared; the purpose of the comparison was to document the type and extent of changes participants made because of the training program. To analyze these data, a 20-point scorecard was used to review each version of the syllabus. Scorecard criteria were based on key components of the training, which was based on the scholarly literature. It is important to note that scoring did not evaluate the quality of any particular section of the syllabus, but did establish whether each component was included in it. Each component that was included received one point on the scorecard for both the pre- and post-training versions of the syllabi, which made it possible to compute: (a) a pre-training syllabus score for each instructor [Pre Instructor Score], (b) a post-training syllabus score for each instructor [Post Instructor Score], (c) the sum of each pre-training syllabus component across all participants [Pre Component Total], and (d) the sum of each post-training syllabus component across all participants [Post Component Total].

Prior to analysis, the data were checked for normality and outliers using boxplots. In an examination of the Pre Instructor Scores and Post Instructor Scores, no outliers were detected and a Shapiro-Wilks test confirmed normality of both the pre-training (p = .747) and post-training syllabi data (p = .591). A paired t-test was used to measure the difference between the pre and post-training instructor scores.

As the third data source, participants received a follow-up survey one year after the program concluded its final year. The survey contained both Likert-scale and open-ended items and collected information about the instructors' current teaching practices, self-ratings of TPACK-related tasks, knowledge retained from the training, and the extent to which individuals continue to implement and use practices and tools learned in the program. All 92 training program participants received the survey and 43 completed response sets were received for a response rate of 47%. The survey data was analyzed both quantitatively and qualitatively. Descriptive statistics were used for quantitative analysis and reporting, while iterative coding was used to analyze textual data and identify emergent themes.

3.2. Findings
3.2.1. SETs
Again, SET items were rated on a 5-point Likert scale (1 = Very Poor to 5 = Excellent). Comparisons of pre- and post-training SET data revealed that the instructors' scores did not improve across any of the four measures. There were no significant differences in the four individual item scores when using a paired t-test to compare pre- and post-training SET data for the course as a whole, course content, instructor contribution, and instructor effectiveness. Additionally, a paired t-test of group differences comparing the participants' collective pre- and post-training averages of the four items revealed no statistically significant difference (see Table 1). As indicated in the SET scores, students perceived little to no difference in any of these areas when they evaluated their instructors on the face-to-face and online formats of the same course.


Table 1. Aggregated mean scores for instructor SET data (n = 27).

Rating item statement	M (pre)	S.D. (pre)	M (post)	S.D. (post)	M Difference (pre-post)	S.D. Difference (pre-post)	t-Test results
1.
The course as a whole

3.74	0.46	3.64	0.63	−0.11	0.50	t (26) = 1.098, p = .282
2.
Course content

3.77	0.43	3.69	0.53	−0.07	0.44	t (26) = 0.859, p = .398
3.
The instructor's contribution to the course

4.03	0.42	3.88	0.70	−0.15	0.53	t (26) = 1.487, p = .149
4.
The instructor's effectiveness in teaching the material

3.84	0.50	3.69	0.72	−0.15	0.52	t (26) = 1.313, p = .201
Average of items 1–4	3.84	0.44	3.72	0.64	−0.12	0.48	t (26) = 0.714, p = .482
Note: each entry for pre- and post- intervention data as well as averaged scores were each computed as individual variables. Confidence interval for averaged items is (−0.32, 0.10), which includes the value of 0.

3.2.2. Syllabi
Nineteen of the 20 criteria included in the scorecard showed a positive increase across all instructors from [Pre Component Total] to [Post Component Total], indicating that instructors added the components to their post-training syllabi. Table 2 provides information for each of the 20 scorecard criteria points by showing the percent of instructors who included this item both the pre and post-training versions.


Table 2. Summary of changes from pre- to post-training course syllabi.

Syllabus scorecard criteria	Syllabi pre-training (%)	Syllabi post-training (%)
Detailed course information	87.5	95.8
Course learning outcomes	75.0	91.7
Program/dept. outcomes	0.0	20.8
Instructor contact information	95.8	91.7
Course description/statement	87.5	100.0
Course value statement	12.5	83.3
Learning environment statement	12.5	87.5
Info about required texts	87.5	95.8
Technology resource info	8.3	79.2
Grading/assessment/evaluations	91.7	100.0
Complete assignment info	54.2	79.2
How to be successful	54.2	91.7
Methods of feedback	12.5	75.0
University policies	62.5	87.5
Links to key student resources	41.7	75.0
Entire course calendar outline	79.2	95.8
Course community statement	12.5	33.3
Use of instructional technology	20.8	45.8
Required technology skills	8.3	16.7
Accessibility information	65.4	69.6
Additionally, individual instructor scores were calculated for each instructor's pre- and post-training syllabi by adding the total points out of 20 for each version of the syllabi, resulting in [Pre Instructor Score] and [Post Instructor Score]. Comparisons of these scores revealed that instructors scored considerably higher on the post-training syllabus scorecard (M = 15.19, SD = 2.16) than they did on their pre-training version (M = 9.38, SD = 2.54). A paired t-test of group differences for the totals of [Pre Instructor Score] and [Post Instructor Score] revealed a significant difference between the two groups (t = −8.610, p < .001).

3.2.3. Survey
Section one of the survey contained statements about online teaching skills that were designed and validated by Schmidt et al. (2009) and later applied by Shin and Lee (2009). As summarized in Table 3, instructors reported the extent to which they perceived their TPACK abilities using a 5-point scale (1 = Strongly Disagree to 5 = Strongly Agree). The respondents rated themselves highest in their abilities to integrate the three components of the TPACK model, on their use of the tools they select to support student learning and research and on their use of strategies from the training they received that combine pedagogy and technology. Participants rated themselves lowest on their abilities to help their colleagues coordinate the use of technology and on how easily they could adapt to unexpected changes that arise in the tools they utilize. Although respondents were relatively confident about choosing the technologies they wanted to use to enhance their students' learning and to enhance their teaching, they felt slightly less confident in evaluating new tools independently.


Table 3. Mean scores for instructors' self-reported TPACK skills (n = 43).

Item statement	M	S.D.
I can teach lessons that appropriately combine course content, technology, and teaching approaches.	4.26	0.73
I can use the tools that I select to support student learning and research	4.19	0.79
I can use strategies that combine what I learned in my training about technology and teaching.	4.19	0.76
I can choose technologies that enhance my students' learning.	4.00	0.82
I can select technologies that enhance what and how I teach.	3.98	0.86
I can evaluate and select new technologies based on their usefulness to specific goals of the content.	3.93	0.86
I can easily adapt to changes in existing technology tools that I utilize.	3.65	1.2
I can help others to coordinate the use of technologies and teaching approaches.	3.53	1.0
Note: Items adapted from Schmidt et al. (2009).

The second set of questions contained a series of fifteen, 2-part items, as shown in Table 4. The first part of each question contained stems about specific skills that were heavily emphasized during the training. After rating their abilities using a four-point scale (1 = Poor, 2 = Fair, 3 = Good, 4 = Excellent), respondents were then asked about the extent to which each skill improved as a direct result of the training program. For the second part of the item, respondents used a three-point scale (1 = Not at All, 2 = Somewhat, 3 = Very Much). The column with the heading “Credit” indicates whether participants credited the training program as having a high (H), moderate (M) or low (L) level of impact on that particular skill.


Table 4. Mean scores and program credit given for self-ratings of online teaching skills.

Item statement	N	M	S.D.	Credit
My ability to plan a course	36	3.58	0.50	L
My organization in teaching	35	3.46	0.61	H
My ability to facilitate the course	36	3.36	0.64	H
My interpersonal communication with students	35	3.34	0.59	M
The clarity of my written communication with students	35	3.31	0.68	M
My multicultural competence	35	3.23	0.73	M
The quality of feedback I provide to my students	35	3.17	0.66	H
My use of student collaboration (group work)	36	3.08	0.81	M
My use of higher order questioning	36	3.06	0.53	L
My knowledge of student learning differences	35	3.03	0.51	M
My use of classroom assessment techniques (CATs)	35	3.00	0.87	H
My familiarity with web-based teaching tools	36	2.72	0.74	L
My knowledge of online teaching methods	36	2.67	0.93	L
My familiarity with adult learning theory	36	2.64	0.96	M
My ability to actively engage students online	35	2.34	1.06	M
Impact level (“Credit”) was based on the percent of respondents who credited the program with their self-reported ability using the following criteria. First, high level (H) was assigned if >60% responded the skill was “very much” a result of the training. Second, moderate level (M) was assigned if >30% but <60% of the instructors responded that the skill was “very much” a result of the training. Finally, low level (L) was assigned if <30% responded that the skill was “very much” a result of the training or >30% responded that the skill was “not at all” due to the training.

Although participants rated themselves highest in course planning, they believed the training program was of little influence in this area. Respondents had similarly high self-ratings in the areas of organization in teaching and facilitation abilities. However, the instructors attributed a high level of credit to the program for their levels of skill in these areas. Instructors rated themselves lowest on their ability to engage students and on their familiarity with adult learning theory; respondents attributed a moderate level of credit to the program in both of these areas. Instructors also rated themselves relatively low regarding their knowledge of online teaching methods and their familiarity with web-based teaching tools; in these areas, they also attributed a low level of credit to the program for their abilities.

One survey question asked participants what they believe to be an online instructor's most important role to fulfill. Of the four choices, respondents answered: pedagogical (57.5%), social (20%), managerial (20%), and technical (2.5%). A chi-square goodness-of-fit-test was used to determine whether the probability distribution for the respondents' opinions about which of the four key roles was the most important followed the distribution pi = 0.25, where i = choices 1, 2, 3, and 4. If the distribution held, there would be no disparities in the preferences for each answer. Results of the chi square test were statistically significant, indicating significant disparities among the probabilities. That is, the frequencies of responses in favor of the pedagogical role were higher than expected under the assumption that each of the four options would be equally distributed; likewise, frequencies for the technical role appeared far less than expected, χ2(3, n = 40) = 25.8 p < .001).

In final section of the survey, respondents described the new skills, concepts, ideas, methods, or tools they learned in the training and discussed what they had implemented following the training. Table 5 summarizes the instructors' responses in terms of pedagogy and technology; the responses are separated by column, according to what they said they learned (left) and what they said they changed or implemented (right).


Table 5. Participants' self-reported learning and instructional changes.

What participants learned from the training	What participants changed following the training
Pedagogy	
•
“That the onus is on [the faculty] to create the learning environment, but it is essential for students to come prepared”

•
“I give more thought to higher-level thinking and learning than I did previously”

•
“Information on active learning techniques; ways to engage students in collaborative learning; and information about rubrics.”

•
“I continue to use the group methods we practiced in the sessions”

•
“I learned the importance of making connections with your students the first day of class and group work”

•
“I have started to incorporate the students doing some work outside of the class for the current lesson”

•
“I learned skills for promoting successful teamwork and the use of peer evaluations”

•
“Syllabus design”

•
“I learned how to plan a lesson and conduct assessments”

•
“Classroom engagement strategies”

•
“Minute papers”

•
“I learned about scaffolding and backward course design”

Technology	
•
“I was introduced to the iPad, and now I have purchased one, but I have not yet used it in class”

•
“I feel comfortable with aspects of Blackboard that I did not understand previously and use it much more effectively”

•
“I learned the use of collaborative learning tools such as wikis”

•
“I learned video screen capture and was introduced to Camtasia, which I now use every year”

•
“We went through so much tech stuff so quickly that it was hard to fully learn and remember most of it. I've retained only the parts that I immediately implemented”

•
“I like using Zoom now for online synchronous classes but am still trying to figure out a few things since I teach using manual communication”

•
“Blackboard tests and assignments”

•
“I learned about using clickers in my classroom. I am still working on using them in all of the classes I teach”

•
“Clickers

Participants shared a variety of pedagogical and technological skills learned during the program, and most were still able to name specific skills and tools that they learned about during the training. However, recollection did not necessarily translate into action. For example, one participant shared, “I've retained only the parts that I immediately implemented,” and another stated, “I like using Zoom now for online synchronous classes, but am still trying to figure out a few things.” Another respondent reported that she “give[s] more thought to higher-level thinking and learning than [she] did previously,” but again, this did not necessarily translate into measurable action or improvement.

4. Discussion
Integrating the three sources of data revealed that instructors demonstrated change in two of the three areas: (1) the redesign of their syllabi and (2) their abilities reported in the follow-up survey. The instructors believed that they had improved their pedagogical practice and that their self-confidence about their teaching ability increased as a result of the training, but this was not found with respect to technology. Even when instructors taught in the new format, they felt less technologically skilled. This finding was further supported by the statistically significant result that they believed the pedagogical role was far more important than the technological role. Based on the findings of this study, there are four important points to consider.

4.1. Multiple data sources aid understanding
First, the integration of three data sources collected from multiple perspectives over a four-year window provided a far more complete understanding of the program's influence than could have been provided by one or two of those sources alone. The use of a single source would have likely led to incorrect or exaggerated assumptions about the possible impact of training. For example, an examination of only the syllabus data would have demonstrated considerable change, but SET data alone would have suggested that the training was at best without impact and at worst detrimental to the instructors' teaching effectiveness. Multiple data sources supported a more thorough understanding of the changes that take place over time and they enrich the conclusions that may be drawn strictly from self-report data. Steinert et al. (2006) called for “more rigorous designs and a greater use of qualitative and mixed methods…to capture the complexity of [faculty development] interventions” (p. 497). For faculty developers, it is common to collect and rely on participant satisfaction data. However, it is important to utilize multiple sources of information to better understand the complexities of such an experience and the possible impacts that training has on participants.

4.2. Participants were already effective teachers
The absence of statistically significant changes to the participants' student evaluation scores was an interesting finding, although even a cursory examination of their pre-training SETs revealed their average scores were already relatively high. The relatively high initial scores left less room for improvement following the program, which may explain why there was ultimately no significant change in their SET scores. A study by [REDACTED] found that SET scores consistently improved for instructors who participated in a different faculty development intervention when the starting scores were lower overall. Perhaps, if the instructors' pre-training SETs had been lower, there would have been a demonstrable improvement.

4.3. Substantial change appears to be incremental
From a faculty development perspective, it was a pleasant surprise to see how much the instructors changed in their post-training syllabi. By incorporating components such as a course value statement, information about student resources, clearly-defined roles and expectations, or feedback methods, instructors planned for and established clear communication channels with their students. This observation is consistent with Chickering and Gamson's (1987) classic work on teaching effectiveness, in which they recommended communicating high expectations and providing prompt feedback. One possible explanation for the significant change in this area is that such adjustments to the syllabus are among the easiest changes to make. Instructors could demonstrate almost immediate change, as opposed to learning new programs or increasing teaching evaluation scores, which can require an extended period of time. Additions to the course syllabi represented a small, incremental change that the instructors achieved relatively quickly and easily. The inclusion of many new components demonstrated that the training program did influence the instructors' thinking about their approaches to teaching. However, it was not clear if such changes had any impact on their teaching effectiveness in the classroom.

4.4. Full integration of pedagogy and technology is difficult
In this study, it appears that the instructors did not perceive pedagogy and technology to be integrated, but rather as separate, distinct components throughout the process. While crediting the program with the development of their skills, instructors rated themselves higher on pedagogical skill development than on technological skill development, supporting the notion that they did not perceive the two as intimately related to one another. This finding was consistent with that of Benson and Ward (2013), who found that the circles of the TPACK framework are not actually of equal size for most instructors. For participants in the present study, the pedagogical knowledge circle would be shown as larger than the technological knowledge circle.

The TPACK framework idealizes a continuous focus on the intersection of all three components: content, pedagogy and technology (Benson & Ward, 2013; Koehler & Mishra, 2005; Koehler et al., 2004; Koehler, Mishra, Koehler et al., 2007; Mishra & Koehler, 2006; Stover & Veres, 2013). Given that the findings of this study revealed an absence of full pedagogical and technological knowledge integration, this model may not explain how instructors actually experience the process of learning to teach online. Indeed, as Stover and Veres (2013) argued,

“The majority of colleges and universities have bifurcated professional development.

programs that address these types of knowledge separately which results in unrelated separate professional development programs that do not emphasize the importance of the relationship between technology, pedagogy, and content” (p. 94).

It is quite common for faculty to learn much of their content knowledge prior to ever stepping foot inside a classroom. For instructors who begin their careers teaching exclusively face-to-face, they learn and develop pedagogical skills next, as Shulman (1987) suggests. For the instructors who later begin teaching online, full integration of technological knowledge with the other components they have already mastered (content and pedagogy) may prove to be more of a challenge. In the present study, learning development appeared to be more linear than integrated.

Another reason for the absence of full integration could simply be that technology changes faster, and far more frequently, than pedagogy. Ertmer and Ottenbreit-Leftwich (2010) found that many online instructors feel like “perpetual novices” when it comes to technology (p. 261). This may be because, while one can develop and retain effective pedagogical skills and learn how to integrate them with new content, technology changes so rapidly that it can be difficult to keep up with the progress.

The training program described in this study aimed to integrate pedagogy and technology for online instructors, but did not fully achieve this goal in practice. The notion of full integration is complicated, both from a teaching and a learning perspective. To be sure, some institutions, such as the University of Maryland, have merged their pedagogy and technology teams into a single unit. While this may not be a panacea for all discrepancies between pedagogy and technology with respect to how instructors learn, it does lend support the notion of earlier and more consistent integration being more effective. Ultimately, when considering the findings of this study and where they fit with current knowledge and program delivery in the field, more information will be needed on how this integration is best achieved in training.