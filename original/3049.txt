Abstract
Developing data visualisation tools to support academics in the classroom is a challenging process due to the key requirements of usefulness and scalability, and the constraints of a university ecosystem. Here we describe the evolution of an enterprise-level, teacher-facing dashboard, designed to display data about students' enrolments and use of the Learning Management System in a meaningful way, and summarise the challenges and lessons we encountered along the way. This large university has a maturing learning analytics unit, a new, data-friendly LMS system, and data-savvy and data-hungry executive leadership. Yet the experienced pathway and evolutionary steps evidence the points that need be resolved to successfully deliver and transition to learning analytics solutions that have previously been conceptually proposed or tested at small scales in other studies. The key findings through the process highlight the level of uplift (in tech, capacity and capability) that universities need to meet contemporary demands and future possibilities.

Keywords
Data pipelines
Data-driven insights
University strategy
Educational platforms
Learning analytics

1. Introduction
A major paradigm shift in digital data access recently occurred when computer and internet systems were able to capture and analyse large records of information (Kantardzic, 2011; Manyika et al., 2011; Taylor & Munguia, 2018). The cascading effect of data access is data visualisation and consumption: tools for understanding large amounts of data collected from software. This effect has resulted in analytics tools surfacing in areas outside of their traditional integration in science and economics, into areas such as business and education. In higher education, a large part of the emphasis for adopting digital data analytics tools was placed on providing student-generated data from learning management systems (as the main data source) back to academics (Ferguson et al., 2015; Park & Jo, 2015; Schwendimann et al., 2016). This process had a lofty goal of identifying student engagement levels and ultimately the ability to predict future learning states, supporting academics to support their students (Long & Siemens, 2011; Munguia, in press). However, it has been a treacherous road, where road blocks can be found in three main areas: systems infrastructure, pedagogical integration, and applied usage.

Universities have an appetite for insights into their students' performance but tend to be slow at incorporating this information into strategic direction (Colvin, Dawson, Wade, & Gašević, 2017). Traditionally, universities have relied on their analytics teams from the business side of the enterprise to analyse and manage datasets, because these datasets were directly related to government and institutional reports, and to understand fiscal numbers (Campbell, DeBlouis, & Oblinger, 2007). Early efforts at universities had identified the need for an academic lens to incorporate student course activity into serviceable platforms that could provide feedback for teachers to improve engagement and ultimately minimize student attrition (e.g., Asif, Merceron, Ali, & Haider, 2017; Macfadyen & Dawson, 2010). One reason for this was that, at the surface, business needs and academic needs can seem similar, given that they both require metric design and analysis delivery through a platform (or dashboard) (Campbell et al., 2007). Yet, the metrics and approach are fundamentally different for educational (e.g., student) data and business (e.g., financial) data, and the decisions made from these are different.

Designing a suitable strategy that facilitates the use of data to inform teaching is not simply a case of building a new business reporting tool (a process that is difficult enough). Nonetheless, many organizations commission their business intelligence or information technology services to design the strategy and platform without academic consultation (Fig. 1). When the approach comes from the executive, often the message gets diluted or does not align with the expectations of academics, and such strategy can become overtaken from a business perspective (Baer & Norris, 2017). Alternatively, when strategic innovation is done by academics, these often do not capture the attention of and adoption by the larger organisation, regardless of how good or useful they are (Fig. 1). Further, a recent survey (Tsai & Gašević, 2017) found that few universities had designed and implemented a policy on learning analytics, let alone acted on data-driven insights. Therefore, universities interested in implementing learning analytics are navigating uncharted waters (Colvin et al., 2017).

Fig. 1
Download : Download high-res image (216KB)
Download : Download full-size image
Fig. 1. Top-down vs. bottom-up approaches in designing and delivering learning analytics solutions at university level.

Here, we showcase the evolutionary steps that a large Australian university has undergone in the last few years attempting to deliver a learning analytics platform to initiate and integrate data-driven insights into its pedagogical landscape. Specifically, this platform is a tool accessible by teachers containing data and visualizations about their students. Theoretical work on learning analytics suggests processes by which academics can adopt and reflect on these tools, and evidence of learning analytics in practice, occurs in massive open online courses (MOOCs) and classrooms (e.g., Clow, 2013; Jovanović, Gašević, Dawson, Pardo, & Mirriahi, 2017) – however, there are no publications outlining the experience that a whole university has undergone in attempting to deliver a learning analytics solution. We present our experiences with the main stakeholders and the evolutionary steps, by first describing the context within which this project was developed. We then outline the key groups that provided input for tool development and the datasets used, and the different evolutionary iterations of the tool, including data processes and the platform at each stage. Finally, we discuss key lessons that may be beneficial when trying to establish a learning analytics strategy that incorporates user-facing data at a university-wide level.

2. Setting, initial motivation and strategy
The work described here took place in a large Australian university located in a major city, it serves both vocational and higher education sectors and has multiple campuses within Australia and overseas. The Learning Analytics team was created in 2016 as a central unit and tasked with innovating and providing analytical services to student-facing staff.

In 2016 the university began the transition to Canvas, a new Learning Management System (LMS), piloting 50 courses2 in July 2017, with all courses transitioned by March 2018. The new LMS provided a clear opportunity to utilize its data across the university, as it provided a suite of different tools for accessing information on each course that could potentially be beneficial for teaching as well as shaping the Learning and Teaching (L&T) strategy. The first tool proposed was a dashboard for academics, that would be accessible within the LMS to facilitate its adoption and use by academics. The platform would show a set of visualised metrics that could be filtered by student and with cohort comparison, along with a series of suggestions of how and when the data could prove useful (Fig. 2).

Fig. 2
Download : Download high-res image (289KB)
Download : Download full-size image
Fig. 2. Set of guidelines to help academics interpret LMS data at different points in a semester. Some of these components are currently aspirational, however it shows the path from reporting functions to predicting and reflecting on events.

We adopted a fail-fast strategy to quickly gain insights into developing a platform. The original plan was to test metrics with academics by December 2017 and have a testing environment for n = 5 courses by March 2018 (the first semester of the year). A platform aimed at all courses was expected to be ready by the second semester (July 2018). The pace of these milestones was fast yet acceptable for two reasons. First, a project like this one had many unknowns which would take too long to scope without road-testing ideas (Fig. 3). Second, the strategy was to request feedback and rely on it as a minimum viable product that would improve over time, not to deliver a perfect product the first time around. This second point was important because it would dictate the way we communicated with staff. Our communications would be designed to ensure users understood it was a live pilot, aimed at constantly making improvements and learn from mistakes (e.g., Highsmith, 2010), therefore minimizing their frustration and instead increase their willingness to cooperate and be open to the tool.

Fig. 3
Download : Download high-res image (732KB)
Download : Download full-size image
Fig. 3. Two data pipelines – the real and an ideal pipeline to contrast how large Canvas data is handled. The top flow chart represents a seamless pipeline that could be as real time as Canvas data are generated and harvested by universities. The bottom flow chart represents the current state and the different transformations the data undergoes through the pipeline, resulting in relatively static visualizations.

With the project aimed at scaffolding a platform as we gained insights, we expected a relatively fast development phase, with a focus on piloting an early version of the tool as quickly as possible to initiate the evolution based on user feedback. However, while we also expected the platform through which data insights were served to develop throughout the project, we underestimated how difficult this would be, and the unexpected places we would end up. The following sections lay out the three main iterations of the tool and attempt to describe the environments in which each came to be. The three iterations (Table 1) are named based on colloquial terminology from within the team as “Unicorn” our starting point focusing on schematics and metrics, “Valkyrie” a working dashboard provided to a small group of academics to initiate user feedback, and “Valhalla” a larger pilot providing data to a larger group of people within a purpose-built LTI. At each stage, the iterations focused on what was already possible with available data, what could be possible and what would likely be most useful.


Table 1. Aims, proposed methods and actual methods in each of the three project stages. The proposed project methodology was significantly altered by ITS, resulting in a large deviation between the final version and the original aims. Yet, at the end of the last stage, there was recognition that the original methods were possible and realistic.

Stage	Original Aim	Method
1. Unicorn	Pilot phase, define metrics, construct pipelines	Proposed: Canvas Data and SIS pipeline in a Postgres DB. Tables loaded to an app server accessed through a dashboard.
2. Valkyrie	Users piloting real data setup and providing feedback and testing robustness of pipeline.	Reality: lack of app server forced the production of static HTML pages published into piloting participants through Canvas API.
3. Valhalla	Larger scale solution, demonstrating a stable and interactive platform	Reality: Canvas Data transformed into Oracle warehouse, and then loaded into a SQL Server (except for requests dataset, which bypassed the warehouse). HTML pages display data for teachers and stored in a newly created app server and custom-built LTI.
3. Input groups
Ultimately, the number of stakeholders for a learning analytics tools are extensive, with potential implications for students, teachers, and administrative staff. However, in getting this project off the ground, there were two key groups of people that required engaging: The Learning Analytics Community of Practice, and Information Technology Services.

4. Community of Practice (CoP)
The Learning Analytics Community of Practice (CoP) comprised a small group of academics and L&T support staff. As practicing teachers, they were aware of the gaps in data between what they needed and what they could obtain from existing systems. During our decision-making process around which data to extract and visualize, we engaged with and received assistance from academic members of the CoP. For example, they noted that practical access to student-specific data was often hindered by the preceding attempt to provide an overly-flexible tool that would satisfy disparate (and not often aligned) requests. In the existing system, a teacher could look up enrolment data for a single student at a time, but the process involved several click-through steps, which was prohibitively time-consuming if trying to build up a full picture of enrolments for a course of a hundred students.

A member of the CoP devised a process to visualize program enrolment distribution for his class, resulting in a restructure of the course once he understood the diversity of programs represented in his class. Through the CoP discussions, it was clear that such a visualisation would be useful to many teaching academics, and the project adopted this graph; in doing so, the Student Information System (SIS) was included as a second data source alongside data directly from the LMS. The resulting visual of program and course enrolments has garnered the most positive feedback to date. This continuous collaboration between the tool designers and practitioners helped design and map out the learning analytics tool from an L&T perspective.

5. Information Technology Services unit (ITS)
As part of the strategy, there was also consultation from a technical perspective, involving staff from the Information Technology Services unit (ITS). There were three main points of discussion: data storage, data pipelines, and a staging platform by which the dashboard could be delivered to academics in the LMS. The initial plan was to design a product that, once it was ready for release, would be passed on to ITS for maintenance, with updates designed and generated by the Learning Analytics team based on their research and consultation with academics. However, engaging with ITS proved more difficult than originally anticipated, as described in the stages of the project below. There were two main reasons for this. Firstly, there was a lack of trust in the ability of in-house staff to produce quality insights, as this approach did not align with the ITS strategy of sourcing products already found in the IT market, a strategy based on trying to reduce maintenance and R&D associated with platforms and services, and hence keeping costs down. Secondly, the L&T strategy of the Learning Analytics group was considered out of scope for ITS when bringing in the new LMS and its data.

6. Input data
6.1. Canvas
Canvas provides data in two main ways – a set of tables downloadable for an institution through the Canvas Data API, and a set of REST API end points that return data in json format, with content more strictly curated to match a user's individual access token. The data available in each case are similar, but not identical: essentially, the Canvas Data API makes a broad set of data available to a small set of users, while the REST API end points make narrower sets of data available to a wide range of users. For example, the page_views API endpoint can be used to examine an individual user's global page view history across Canvas. The user identifier is needed as an input variable, and the results will refer only to one user at a time; but a successful response depends on also passing in an access token that is either owned by that particular user, or has higher-level access than a ‘Teacher’ Canvas user type can obtain. The wiki_page_fact downloadable table, part of the Canvas Data API, lists all pages associated with the institution, and the number of times they have been viewed by all users.

Canvas Data consists of 105 fact and dimension tables in a star schema developed and occasionally added to by Instructure (parent company), alongside the so-called requests table, which is generated from Canvas log files and, after some interpretation, can be used to extract user clickstream data.3 Non-aggregated, individual actions linked to courses can only be obtained through interpreting clickstream data, for which the requests table is a key resource. Due to the absence of aggregation the requests table is the largest by far (Taylor & Munguia, 2018). Since the gradual introduction of Canvas in this university in 2016, this table is now 2 TB in size, with almost 2.5 billion rows, and is currently growing at a rate of 15 million rows per business day during the semester. To obtain Canvas Data from the Data API (as opposed to user-level API access) a high-level institution token is required, as data relating to all courses and users across the institution are made available for download. As a central group providing data and analysis to support the University's business of Learning and Teaching, we were able to obtain such a token. However, we have observed that academics engaged in learning analytics research are not often afforded access to this type of data because they do not form part of a central unit.

6.2. Student Information System (SIS)
Student data such as enrolment, demographics, academic performance and graduation pathways are stored in a central Student Information System named SAMS (Student Administration Management System). Over the last 16 years, SAMS has evolved and continued to grow with a vast number of tables and fields, some in regular use and others from legacy processes and attempts to create efficiencies. The data are stored in an Oracle database, with its origins as a local implementation of PeopleSoft. The specifics of this implementation are well known to specialist staff within the university, but few users interact directly with the SIS data (rather than with an intermediary application), and even fewer have knowledge of which of the thousands of tables and fields could potentially be useful for learning analytics metrics. There are several current systems that serve restricted or manipulated forms of the SIS data to users with assorted front-end applications, such as a SAP Business Objects Universe. However, data tend to flow just one way from the SIS, with write-access strictly limited, both for people and for applications. Enrolment data from the SIS is uploaded to Canvas, but no data flows directly back to the SIS from Canvas.

One prominent example of the one-way flow of data from the SIS is that final course grades in the SIS are not linked directly to the LMS (either Canvas, or its predecessor, Blackboard). Instead, there is a customised bridging application in which final course grades are entered manually by academics, based on the LMS or any other temporary storage place they use to monitor grades for courses in progress. This manual process takes place at the end of every semester and has been a source of frustration for academics and administrative staff. However, this reflects the different roles of the SIS and the LMS: the SIS does not record grades for courses in progress, only final grades for completed courses. The understanding is that the LMS is the ‘source of truth’ for courses in progress, but not for official administrative records. In the CoP we found that academic staff would like to have access to data that bridge the two systems, such as viewing the completed (SIS) courses for students in a current Canvas course.

We considered the SIS to be the single ‘source of truth’ for student enrolment data and for grades of completed courses, both for visualizing SIS enrolments in the dashboard and for identifying corresponding enrolments in the Canvas data. We were granted access to a reporting copy of the SIS production database, updated every 24 h; It was relatively straightforward to query SIS once a database connection had been granted and a suitable query was built (based on a combination of advice from users with specialist knowledge of the SIS, and direct exploration of the database contents). However, for manipulating data into new formats, or joining to other data sources, in practice the SIS data needed to be extracted in bulk. This meant moving data to a different database where write-access was permissible (such as for building custom views or temporary tables), and where running large or repetitive queries would not cause inconvenience to other users across the university. Hence, we implemented a process wherein relevant SIS data was regularly extracted to our tool's working database. This extraction process causes slight deviations from the original data, and therefore from the single source of truth (e.g., exact number of SIS student enrolments per course at a given time).

Matching enrolments in the SIS to enrolments in Canvas was a non-trivial process, reflecting the need, when integrating large and complex systems such as a new LMS, to consider how data may be extracted and integrated into queries with data from existing systems. It is possible to join Canvas data to the SIS data, but these joins must be undertaken with care. For a start, the core terminology differs between the two systems: a ‘course’ in Canvas has a different meaning to a ‘course’ in the SIS. In the SIS, students enrol in a ‘class’, each of which contains a ‘course’ code: course codes recur across different semesters, but class codes are unique combinations of course, semester and campus. On Canvas, a ‘course’ is essentially a ‘course shell’: a collection of online learning resources. Canvas courses may also contain multiple ‘sections’ corresponding to different groups of students within the same course shell. The facilitation of joins between the two systems depends upon the enforcement of business rules. Neither system breaks down if the business rules are broken; it simply becomes harder to usefully join data from the two systems.

In the case of higher education (HE) courses (the focus of the tool), business rules were enforced that did allow for relatively straightforward identification of matching SIS class enrolments to Canvas course enrolments, even in the complicated cases where multiple separate classes in the SIS were taught together in Canvas as so-called ‘sections’. However, this process in the vocational education (VE) sector of the university is considerably more complicated, with over 25 different sets of alternative business rules having been identified as possibly ways to identify matching enrolments between SIS and Canvas. As a result, the tool has not yet been implemented for the vocational sector.

7. The three stages of the project
7.1. Stage 1: ‘Unicorn (a nice idea that does not exist)
The initial development of metrics was based on an earlier long period of exploration (~8 months) of the Canvas data and the SIS. In our experience, an exploration period such as this is not common, as universities are required to quickly produce outcomes, and the business analytics units often tasked with these solutions rely on quick insights often associated with non-academic standards. However, it enabled us to become the institutional experts with a deep understanding of how Canvas data work and how they can be integrated with the SIS.

We reviewed the literature and the approach that Instructure used in its ‘Canvas Analytics’ tool (a set of Canvas-based metrics which, at this stage of our project was partially developed). We then developed a list of possible metrics classified according to how useful we considered them to be (including those originally proposed in Fig. 2), and their feasibility given the data structures of Canvas and the SIS. We also interviewed researchers at other similar Universities regarding their experience both with Canvas data and with teacher-facing dashboards.

We then arranged meetings with the CoP members to present an assortment of these visualised metrics, in which we received essential feedback regarding which metrics they considered to be useful, which were (in the words of one attendee) ‘interesting but not very useful’, and what they would like to include that we hadn't considered. For example, the University has a policy of not employing pre-requisite courses, so one academic requested a view of the SIS courses that his students have previously taken, so that he could identify any students that may lack certain skills and thus require additional assistance in his course. We also requested feedback on a proposed layout of our dashboard, however as will be outlined in the description of Stage 2, this ultimately was not utilised due to the severe restrictions imposed on publishing our metrics.

7.1.1. Data pipelines during stage 1
A combination of factors led to the decision to focus on the Canvas Data tables (rather than use of the Canvas API end points) for the first iteration of tool development. These included a desire to develop our own metrics from essentially ‘raw’ data (where ‘raw’ is in quotes to acknowledge that choices have been made by the Canvas developers regarding which data to collect in the first place), the capabilities of our own developers and an expectation about how the potential visualisation tool would function.

During the initial development phase, we downloaded Canvas Data directly to a local server and pushed the tables into a local secure PostgreSQL database that was separate to other university systems. This process has been maintained and we have found this local housing of our own copy of the dataset to be the most effective way to explore it and develop this project and others. Here, new metrics could be developed and tested quickly, with real data but minimum risk. It functioned, and continues to function, as a “sandbox”.

7.1.2. Platform during stage 1
Identifying tool availability and usability turned out to be the most challenging piece of our development process, and quickly became our main focus once the early development of metrics was complete, as we commenced discussions with ITS around how they would be able to support the project. Our original vision was for a bespoke, dynamic, student-level dashboard that we would build in-house, and to this end, we built a proof-of-concept dashboard in R Shiny. We also wanted the dashboard to integrate with Canvas course shells to avoid additional login fatigue of users. However, there were significant concerns around the implementation of security, a combination of reasonable and widely-accepted questions about ensuring student data remain private, and a tricky university culture that discourages sharing of teaching and performance data. It was unclear whether the platform would be able to scale to thousands of users, but the prospect of testing and prototyping was difficult in this environment.

7.2. Stage 2: ‘Valkyrie (carrying learning analytics insights to academic champions)
In addition to serious questions regarding what platform we could use that would be secure, robust and scalable, it was not clear how Canvas data would be made accessible to our application (beyond our local “sandbox” server, which was not appropriate for such a tool), nor where ETL (Extract, Transform, Load) processes necessary to the project would occur. ITS were understandably concerned with minimizing risk and centralizing data delivery services, outweighing the desire to test and explore options. During this period of increasing frustration with the lack of momentum and trust within the project, we developed an alternative method to serve visualised data to academics, that was small in scale and therefore circumnavigated the restrictions imposed by university systems. While this was not intended as a long-term solution, our overarching strategy relied heavily on obtaining feedback quickly to assist the project's evolution, and we wanted to continue developing the metrics included in the tool in parallel with determining the process by which they would be served to end-users.

7.2.1. Data pipelines during stage 2
Canvas data continued to be downloaded and stored on the local server during this stage, along with a copied subset of SIS data. Python and R scripts for ETL and visualisation were developed and controlled locally.

7.2.2. Platform for stage 2
Because of the circumstances described above, we developed a strategy for delivering data to academics without a dashboard. With this methodology, the metrics were visualised not within a dashboard but in a series of mostly-static html pages, which were then delivered directly into a page within the Canvas course shell (unpublished to be invisible to students) via a Canvas front-facing API that enables writing directly to pages.4 This second proof-of-concept was limited to just four interested academics, allowing us to continue to rely on our local Canvas database and server, and to gain some additional feedback both on the presentation of the metrics and on the mode of delivery (directly into the course shell). This approach also allowed us to demonstrate a desire for this sort of tool from teaching academics, which was a critical element for getting high-level support for the project, and increased support from ITS.

7.3. Stage 3: ‘Valhalla’ (a learning analytics space for academics to feast on data)
The short-term ‘Valkyrie’ initiative was a success as it proved a system secure enough and stable enough to upgrade its piloting by ITS and therefore obtaining greater technical support. In the interests of building on what had already been developed, and piloting the project to a broader audience as soon as possible, the decision was made to continue serving the metrics as html pages. However, the method of delivery was updated to a tool built directly into Canvas as an LTI (Learning Tools Interoperability, IMS Global Learning Consortium) that would serve the html pages, and that handled things like the user type for which it was visible and an error message when the data were unavailable.

7.3.1. Data pipelines during stage 3
As the project progressed and gained structure, the move from storing and processing data in our Learning Analytics sandbox became increasingly complex and fractious, as we needed to transition to using Canvas data and storage on a dedicated server in order to gain ITS support. When considering the requirements for a database that was to form the basis of an application potentially used by thousands of courses in the University, the use of an open-source system such as PostgreSQL with no preceding applications at the university, or contractual agreements in place, was understandably concerning to the ITS team. Ultimately Microsoft SQL Server was the only option made available to us in the longer term, since this was already supported by the University. ITS had therefore commenced developing a process for moving and updating the Canvas data to this server.

Prior to this, the Canvas Data were not available in the ITS servers to be used as a sandbox. Instead, the only set of Canvas Data outside our “sandbox” was pulled into the University's data warehouse, which was separated by a firewall from the SQL Server made available to us. What resulted was a rather convoluted process wherein the flat files were downloaded from Canvas, pushed to an Oracle database (the university's data warehouse), and then copied via a SQL Server Integration Service (SSIS) job through to the SQL Server database available to us (Fig. 4).

Fig. 4
Download : Download high-res image (645KB)
Download : Download full-size image
Fig. 4. Current state of the Valhalla learning analytics dashboard. (a) The first page shows the program diversity represented in a demonstrative course, and the distribution of co-enrolments in a given semester, providing the academic context on the background of students. (b) The second page shows activity on the LMS as a function of week in the semester. It provides a high-level understanding of whether a cohort is accessing and navigating the course. (c) The third page shows content access broken down by module, providing insights into when different sections and information are being accessed. All three panels have a small level of interactivity, allowing for more information to be displayed when hovering.

This progression was a compromise as it was doable but not efficient. While we were happy to receive any ITS support, the details of delivering Canvas data were not ideal, nor viewed as such at the time. Firstly, this process was not robust, and the resulting application has been impacted by delays and breakdowns in data provision (11 times in the last eight months). Secondly, during this process, all fields were converted to text: this quickly but not efficiently handles the variations in data types between the database systems. In addition to the process itself adding a further 24 h of lag time to the data, the introduction of non-native text fields greatly increased the size of the required storage and added a confusing set of representations for null fields. Finally, the field names and table names were truncated, and the correct text encoding was lost. None of these problems were present during Stages 1 and 2 while using Canvas data on a PostgreSQL database built locally.

The move to SQL Server and ITS support also then required additional re-scripting to ensure the application code would cast fields appropriately along with handling two SQL dialects (as we were still developing mainly with our local PostgreSQL database). We adapted to this new environment and built code to suit this, but this detracted from time on other activities. Part of the design and publication process was hindered by not having the data consistently available. We note that at the time of writing, a new methodology has been initiated for the transportation of Canvas requests data that bypasses the Oracle data warehouse and the SSIS job, and instead loads the data directly into SQL Server, which will alleviate some of the issues described above.

7.3.2. Platform during stage 3
A bespoke LTI was built by ITS, that sits within a Canvas course shell and can be turned on and off for specific courses. It is by default not visible to students, and only users with a teacher-type enrolment in the course are able to authorise its use. Once a day, a script processes the Canvas and SIS data of all included courses and produces a set of three html pages for each course, which sit on a server accessible to the tool. When an authorised user accesses the tool, it fetches and displays the html pages based on the course ID, which the user can then tab between. An essential element is a link to a feedback form, through which users can log issues and suggestions directly from the tool.

The current metrics are displayed across three pages and have received useful feedback (Fig. 4). The first page describes the programs in which the students are enrolled as well as the other courses they are enrolled in this semester; this information is extracted from the SIS and allows academics to tailor their teaching to the experience-level of their students. The second page shows the number of students active in the course shell (as a percentage of the class size), and the average number of page views of those students, every day of the semester and segmented into weeks. This gives the teacher an idea of the overall engagement patterns of their students. The third page shows the total number of page views through time grouped by module (where modules are commonly defined by week of the semester or major topic), providing a view of where students are directing their attention each week. In each of the pages there is a yellow icon that describes the graphs in more detail, and hover-over details such as the program name or count of students are included.

While functional and useful, this current iteration of our learning analytics tool is quite limited relative to the originally-envisioned dynamic dashboard, and the irony was not lost on us when an ITS member new to the project asked why we didn't just build a bespoke interactive dashboard.

Currently, the tool has piloted in 207 courses late in 2018, and again with 228 courses in the first half of 2019. Going forward, we expect to continue developing the chosen metrics and data visualizations based on constructive feedback that we have received so far. In addition, now that a desire for these metrics is being fostered among teaching academics, discussions are recommencing regarding a scalable, filterable, dynamic method of serving the data as a long-term solution. A key part of the future process will be the involvement of a steering group, to have more consistent input from both end users and the executive team. In particular, we expect to focus on the benefits of providing general student demographic and enrolment data to teachers within their course shells, as the latest version of Canvas Analytics includes metrics that overlap considerably with our own tool (for example, page views over time), while the integration with institutional data can only be handled from within the institution. This choice is also driven by the fact that the collecting and visualisation of program and course enrolments has garnered the most positive feedback from users to date.

8. Lessons for the future
8.1. Working with academics
Central to any new learning and teaching innovation strategy is engaging with academics, ensuring feedback is provided on the tools needed and the ways by which an innovative process can be implemented (Buerk, & Buerk & Mudigonda, 2014; Baer & Norris, 2017). The key aspect to address is creating motivation with academics to ensure a tool like this is perceived as collaborative rather than imposed. However, all efforts have to be made to reduce the number of systems, platforms and login events that academics have to do, as learning how to use something new is time consuming. Through this process, it is important to adopt and recognize ideas from academics, as they are ultimately the teaching users and teaching experts in a university that understand the context well (Table 2).


Table 2. List of components that need to be in place for each stage and their quality. The state of these components is what this project experienced during its evolution.

Quality	Data	Analysis	Publication
Well	-A maturing database setup with dedicated infrastructure processes (backup and recovery) with documented student data sources and flows.	-Decision making on initial metrics is crucial in developing insights to support and justify academics teaching decisions.	-Insight automation, processes were established and easily maintained once published metrics were available.
Poor	- The initial setup lacked dedicated servers. Vendors such as Microsoft recommends as best practice dedicated servers for all data and insight needs.
-Vendor announcement (Instructure), on changes to the data structure, was lacking and sometimes non-existent affecting the quality of insight produced.

- A sandbox environment did not exist and hampered innovation, ad hoc analysis, data transformation and insight generation.	-The decision to use static rather than a real-time reporting dashboard resulted in a higher latency of insights published. A static dashboard was restrictive to academics as it had limited drill down capabilities providing general rather than specific data views.
Good enough	-Database replication was provisioned to reduce data delays and avoid data loss.	-Data aggregation and analysis were scoped as stackable as we learned from platform uptake.	- HTML files were provided as a delivery mechanism to serve metrics and provide a high-level overview with rudimentary user interaction capabilities.
8.2. Working with ITS
The relationship between the Learning Analytics group and ITS proved difficult for two main reasons. First, there was a lack of trust in the in-house ability to produce quality insights, as ITS strategy was to purchase ready-made products and therefore reduce the maintenance of platforms. The leaders of ITS were risk-averse, leading to a lack of buy-in (or ability) to support or allocate resources to applications or platforms that were not part of the standard ITS ecosystem, such as an application programmed in the Python language, or a database management system not built by Microsoft. As we found out, avoiding risk rather than managing risk can create much larger problems down the track, and does not necessarily reduce risk; rather, risk is simply shifted elsewhere (Fig. 3, Table 2). Second, providing support for the Learning Analytics group and its L&T strategy was often determined to be out of scope for ITS plans and strategies, not fitting neatly into the existing models for engaging with users (academics and professional staff). This was particularly true as requirements for the project were under constant negotiation during its evolution as we explored what was possible.

Communication between developing and supporting groups during this process is particularly important, and this is especially true when setting out expectations for how the project will change over time. It would have been useful early on to acknowledge and discuss the tension between the standard operating model for ITS in supporting other groups around the university – wherein an established list of requirements is usually provided at the beginning – and the necessarily changing nature of requirements in a project undergoing active development.

8.3. Key hurdles
Based on this project experience, we believe there are four main hurdles that need to be managed to successfully reach a stable state where L&T innovation is met with enterprise-level delivery. These hurdles need to be resolved in this sequence to avoid having to backtrack.

First, push for the creation, recognition and adoption of a university-wide data strategy that takes learning and teaching into account. Often the strategies that are in place are business-oriented, causing a call for learning analytics strategies to be created or incorporated into existing strategies (Ferguson, 2012; Greller & Drachsler, 2012; Munguia, 2020; Sclater, Peasgood, & Mullan, 2016). Therefore, going forward focus should be given to a broader, more general data strategy (e.g., Tsai et al., 2018). For example, if this broader strategy had been accepted from the beginning, the Oracle-to-SSIS-to-SQL server pipeline could have been avoided. Second, given that any the solution needs to be scalable, there needs be clear understanding by the IT services unit that the starting step is a standalone server and platform that can connect to the LMS with live data (as opposed to static data representing a snapshot). Third, ensure that early adopters and academic enthusiasts provide feedback on a minimum viable product concerning analyses and metrics. This step is critical as it will set the foundation for how data may need to be analysed, but it is not the final state. Fourth, the institutional strategy requires starting from reporting functions and moving into predictive states, as there are lessons to be learned when starting in this direction, rather than the opposite. In part, insights obtained through feedback in reporting states (such as the analytics tool described here), allow for correct aggregation of data and definition of predictive indices and functions. When predictive models are done without this insight, there is risk that students may not be getting the correct support. This step, as it rolls out, requires a support system for academic and professional staff using these data, as decision-making from data is often feared. Clear guidelines and explanations of when and how to interpret data (e.g., Fig. 2) need to be communicated (e.g., Sclater et al., 2016). As an example, the Rapid Outcome Mapping Approach (ROMA) of the Sheila project has recently identified ways to develop strategies and maintain communication as systems and practices are implemented (Tsai et al., 2018).

9. Conclusions
How can universities drive the transformation of the L&T analytics culture into a blend of research, innovation, and enterprise-level service delivery (e.g., Tsai et al., 2018)? The journey at this university is either a common recurrence across universities interested in adopting enterprise-level learning analytics solutions, or a complete outlier. However, based on interactions with members of other institutions, it seems it is a common occurrence (pers. obs.; Tsai & Gašević, 2017). The initial rollout of this project did not reach its goal, however it was very useful because it highlighted areas that needed improvement, and provided clear direction for improving systems, communication and academic engagement (Fig. 3). To transform a university into a learning analytics space, we recognized four main obstacles to overcome, (a) creation, recognition and adoption of a data strategy (b) defining foundational elements through engaged academics such as a community of practice; (c) support from ITS to deliver platforms that can support data pipelines and (d) a 3–5 year strategy that outlines how initial feedback from reporting will advise on how to create university-specific predictive functions, and strategic placement of support mechanisms for academics. In the past, products that aim to be generic enough to help more than one institution have failed (Baer & Norris, 2017; Clow et al., 2017; Sclater et al., 2016), therefore, we recommend universities to rely on their own L&T experts to help reach learning analytics solutions.