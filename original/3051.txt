A vast number of studies reported exciting innovations and practices in the field of Learning Analytics (LA). Whilst they provided substantial insights, most of these studies have been implemented in single-course or small-scale settings. There are only a few studies that are large-scale and institutional-wide adaptations of LA and have explored the stakeholders' perspectives (i.e., teachers, students, researchers, management) and involvement with LA. This study reports on one such large-scale and long-term implementation of Predictive Learning Analytics (PLA) spanning a period of 4 years at a distance learning university. OU Analyse (OUA) is the PLA system used in this study, providing predictive insights to teachers about students and their chance of passing a course. Over the last 4 years, OUA has been accessed by 1159 unique teachers and reached 23,180 students in 231 undergraduate online courses. The aim of this study is twofold: (a) to reflect on the macro-level of adoption by detailing usage, challenges, and factors facilitating adoption at an organisational level, and (b) to detail the micro-level of adoption, that is the teachers' perspectives about OUA. Amongst the factors shown to be critical to the scalable PLA implementation were: Faculty's engagement with OUA, teachers as “champions”, evidence generation and dissemination, digital literacy, and conceptions about teaching online.

Previous
Next 
Keywords
Predictive Learning Analytics (PLA)

Higher education

Distance learning

Scalable implementation

OU Analyse

1. Introduction
Across the globe Higher Education Institutions (HEIs) are exploring opportunities technology affords to provide a consistent and personalised service to students and other stakeholders (e.g., Herodotou et al., 2017; Rienties et al., 2016; Gasevic, Dawson, Rogers, & Gasevic, 2016; Gelan et al., 2018; Tait, 2018). In the last 9 years, Learning Analytics (LA) has been strongly ‘pushed’ forward by policy makers, managers, teachers, and researchers as a means to address student retention (e.g., Larrabee Sønderlund, Hughes, & Smith, 2019; Zacharis, 2015), to improve learning design (Colvin et al., 2015; Ferguson et al., 2016; Macfadyen & Dawson, 2010), and to provide real-time actionable feedback to teachers and students (Cheng, Liang, & Tsai, 2015; Jovanović, Gašević, Dawson, Pardo, & Mirriahi, 2017; Scheffel et al., 2017; Tempelaar, Niculescu, Rienties, Giesbers, & Gijselaers, 2012).

A range of mostly western HEIs have started to explore the use of LA dashboards that can display learner and learning behaviour to teachers and instructional designers, and provide just-in-time support to students (Herodotou et al., 2017; Bodily et al., 2018; Jivet, Scheffel, Specht, & Drachsler, 2018; Scheffel et al., 2017). Furthermore, several HEIs have developed Predictive Learning Analytics (PLA) approaches, or have adopted existing integrated predictive solutions embedded into Virtual Learning Environments (VLEs), to help identify students who may be ‘at risk’ of failing (Calvert, 2014; Wagner & Longanecker, 2016).

Although substantial progress has been made in terms of early adoption and uptake of LA in the form of experiments and single-course designs, several researchers have argued that most LA adaptations are mainly on a small, micro level (e.g., Dawson et al., 2018; Ferguson et al., 2016; Gasevic et al., 2016; Higher Education Commission, 2016; Viberg, Hatakka, Bälter, & Mavroudi, 2018). While some of the LA and technology conferences might give the impression that ‘everyone’ is using LA, in reality most institutions across the globe, and teachers in particular, have limited or no experience with LA (Ferguson et al., 2016; Ferguson & Clow, 2017; Viberg et al., 2018). There is only a handful of institutions that have adopted LA as a main organisational approach. One such example is the Open University, UK (OU) (e.g., Ferguson & Clow, 2017; Higher Education Commission, 2016; Hoel, Griffiths, & Chen, 2017; Raths, 2016). The OU is the first university to implement an institutional ethics policy in LA (Slade & Boroowa, 2014), has a university-wide implementation of PLA for its 170,000+ students (Herodotou et al., 2017; Calvert, 2014; Wolff, Zdrahal, Nikolov, & Pantucek, 2013), and has worked extensively with teachers to use (near) real-time data of students to inform their teaching and learning practice (Herodotou et al., 2019a, Herodotou et al., 2019b, Herodotou et al., 2019c; Rienties and Toetenel, 2016, Rienties et al., 2017).

Nonetheless, a recent study about the state of LA at the OU (Rienties et al., 2019) indicated that there is substantial room for improvement in how the organisation and its stakeholders use LA, in particular: a) Improved communication supported by LA, b) Personalisation to recognise unique distance learners' needs, c) Integrated design from inquiry to lifelong learning, and d) Development of a strong evidence base about what works and what does not. In this paper, we focus specifically on the second area: LA could be used to support ‘at risk’ students who may struggle with course content and assessments, by tailoring teaching and support staff resources. OU Analyse (OUA) is one approach used at the OU to tackle this issue. During the last 4 years, OUA has been implemented on a large scale in 231 undergraduate courses, engaging 1159 unique teachers, and reaching 23,180 students. A range of studies have shown that OUA is effective both in terms of identifying students at risk at an early stage (Wolff, Zdrahal, Herrmannova, Kuzilek, & Hlosta, 2014), and helping teachers to effectively support their students (Herodotou et al., 2017, Herodotou et al., 2019a, Herodotou et al., 2019b). Yet, in line with other research (e.g., Arbaugh, 2014; van Leeuwen, 2018), large differences in actual OUA usage by teachers were reported (Herodotou et al., 2017, Herodotou et al., 2019a, Herodotou et al., 2019b), with some teachers actively using OUA, while others using it only sporadically. Furthermore, there was substantial divergence in terms of uptake within particular Faculties and qualifications (Herodotou et al., 2019a, Herodotou et al., 2019b). In order to better understand the complex dynamics of OUA uptake on a large scale and inform strategies of scalable PLA adoption, this study will use two complementary perspectives. On a macro level, an interdisciplinary project team responsible for OUA implementation and evaluation will reflect on the main challenges and factors facilitating implementation and adoption by 1159 teachers in 231 courses over a period of 4 years. On a micro level, the experiences of eight teachers and how they make use of OUA in their daily practice will be explored. This study aims to address the following two Research Objectives (ROs):

RO1: To reflect, at a macro-level, on the use of PLA by detailing the degree of OUA usage, challenges, and aspects facilitating adoption, over a period of 4 years.

RO2: To reflect, at a micro-level, on the use of PLA by detailing the perspectives of teachers who make use of OUA.

2. Literature review
2.1. Predictive Learning Analytics (PLA) in higher education
As defined by the Higher Education Commission (2016, p. 53), PLA “can identify which students may not complete their degree on time or even hand in individual assignments, which is already being seen in the UK through the OU Analyse tool. Apart from the OU the Commission does not believe that any UK institution has made significant headway in this area”. Indeed, most LA studies to date have been focused on improving learning outcomes, while less than 6% of 252 reported studies used LA at a large scale (Viberg et al., 2018). Similar findings were reported by Ferguson and Clow (2017), who after reviewing 123 LA studies identified that most studies were small scale and lacked a strong evidence-based research approach.

During the last 4 years, the OU has been developing, conceptualising and implementing large-scale PLA applications (e.g., Calvert, 2014; Kuzilek, Hlosta, Herrmannova, Zdrahal, & Wolff, 2015; Wolff et al., 2013) that had a large impact on LA applications at other institutions. In particular, one generic PLA system built upon regression analysis of 30+ student indicators (Calvert, 2014) provides ‘risk-profiling’ to teachers and supports services at four time points during a course presentation. In addition to that, OUA - a more fine-grained, machine learning PLA system provides weekly predictions about assignment submission and gives recommender options to teachers in 231 undergraduate courses (Herodotou et al., 2017; Kuzilek et al., 2015; Wolff et al., 2013). Insights from these two PLA systems are provided directly to teachers and student support teams. Given the importance of teachers in using LA (Herodotou et al., 2017; Rienties et al., 2016; van Leeuwen, 2018), the next section explores some of the key factors that might influence whether (or not) teachers are likely to use LA in their practice.

2.2. Innovation in Higher Education Institutions (HEIs)
2.2.1. The role of the organisation
HEIs are often characterised by resistance to change and adaptation (Rienties, 2014; Chandler, 2013). Resistance to change is often linked to organisational culture and clear expectations sustained by both academic and professional staff with long-standing positions (Chandler, 2013). Change may happen at the organisational level (e.g., adoption of technology), yet not endorsed at the individual level (e.g., limited change in actual teaching-practice using technology). Reasons explaining resistance in HEIs are either organisational such as resource allocation, unprepared leaders with a lack of vision, poor communication between involved stakeholders, or individual such as Faculty members who tend to be reluctant to change (habit, fear of the unknown etc.) (Coetsee, 1993; Piderit, 2000). At the individual level, resistance may be passive or active, the former referring to non-participation and avoidance, and the latter to arguing and blaming (Piderit, 2000). The introduction of PLA may go deep into some of the core roles of teachers and academics, which may cause passive or active resistance to this change (Herodotou et al., 2017).

Organisational studies (Chandler, 2013; Piderit, 2000) have consistently found that uptake of new innovations needs to be supported from both a senior management level as well as from the ‘shop floor’. For example, working with a large group of course teams at the OU on a micro level and for a sustained period of time allowed teachers to become comfortable with using LA as a daily practice (Rienties and Toetenel, 2016). At the same time, even after working intensively on a micro level, several groups of teachers remained relatively sceptical towards integrating LA in their practice. Follow-up interviews highlighted that most academics were not necessarily negative towards the change of method, instead they were primarily worried about how data could be utilised by senior management.

In terms of PLA uptake, Dawson et al. (2018) interviewed 32 senior leaders (i.e., Vice-Chancellors, DVCs) and found that institutions either followed a top-down instrumental approach to adoption, or an emergent innovators bottom-up approach through a strong consultation process. Yet, most institutions had limited adoption of LA and used them on a small scale. For example, Herodotou et al., 2019a, Herodotou et al., 2019b interviewed 20 education stakeholders involved in PLA implementation and identified positive perceptions about using PLA especially in distance learning institutions across all participants. Nonetheless, participants noted challenges related to management priorities, teachers, and evidence of effectiveness. PLA adoption in HEIs could be facilitated by providing institution-specific evidence of effectiveness, proposing specific student support interventions, promoting effective communication across stakeholder, using PLA data to inform decisions, including teachers in the process of adoption, allocating managerial time for adoption, and using PLA to complement the teaching practice (Herodotou et al., 2019a, Herodotou et al., 2019b; van Leeuwen, 2018).

While in previous studies we detailed the perspectives of educational managers involved in using OUA in their courses and produced recommendations for the organisational adoption of LA (Herodotou et al., 2019a, Herodotou et al., 2019b), in this study we complemented this account by reflecting on the actual process of implementation and evaluation of OUA. In particular, we detailed our own experiences as an interdisciplinary research group managing a university-wide initiative, and the challenges we faced alongside the conditions that facilitated scalable adoption (RO1).

2.2.2. The role of teachers
Mixed findings are reported in studies assessing the use of PLA data and visualisations by teachers (Herodotou et al., 2017; van Leeuwen, Janssen, Erkens, & Brekelmans, 2014). In a study with 28 teachers, van Leeuwen et al. (2014) identified that teachers who received LA visualisations of collaboration activities were better able to identify student participation problems. Also, teachers were found to intervene more often in ‘problematic’ groups, as opposed to a control group that did not receive LA visualisations. Yet, in a follow-up study with 40 teachers, van Leeuwen, Janssen, Erkens, and Brekelmans (2015) showed that teachers with access to LA did not get better at detecting problematic groups, but they could provide more support to students experiencing problems. LA insights influence the teachers' behaviour, by opening up interaction and communication between the teachers and students, leading to pedagogical interventions (Rienties et al., 2019; van Leeuwen, 2018). Although teachers perceived PLA visualisations to be useful, some teachers found it difficult to connect the PLA information to concrete interventions (van Leeuwen, 2018). Positive outcomes were also reported by McKenney and Mor (2015); they reported that engagement with analytics software enhanced teachers' professional development.

One of the reasons explaining why teachers might be resistant to adopting PLA may be related to their acceptance of technology. The Technology-Acceptance Model (TAM) by Davis, Bagozzi, and Warshaw (1989) is based on the well-established theory of Planned Behaviour (Ajzen, 1991), and states that the intention to use a technology is influenced by two factors: (a) Perceived Usefulness (PU: For example, whether a teacher thinks that the use of PLA can enhance their teaching or help students' performance) and (b) Perceived Ease of Use (PEU: For example, how easy or difficult is it to use PLA). A range of studies have found that users' technology-acceptance, as conceptualised in TAM, has considerable impact on the adoption of information systems.

Furthermore, the influence of PU and PEU has consistently been shown in educational research (e.g., Ali, Asadi, Gašević, Jovanović, & Hatala, 2013; Pynoo et al., 2011; Teo, 2010). TAM has proved to be highly informative in explaining teachers' uptake of educational technology (Šumak, Heričko, & Pušnik, 2011). For example, Teo (2010) found that PU and PEU were key determinants of the attitudes towards computer use by 239 pre-service teachers. Also, in an experimental lab study with 95 OU teachers, Rienties et al. (2018) identified that the vast majority of teaching staff perceived LA visualisations as being useful (PU), yet only one third of them as being easy to use (PEU). This was partly explained by the number of tools examined concurrently and their early level of development. Similarly, Author, A. et al. (2017) analysed interview data from six online teachers and identified contradictions between actual PLA usage (technology acceptance) and PU, suggesting that greater PU may not necessarily result in greater usage and acceptance.

Follow-up studies at the OU by Herodotou et al., 2017 found mixed effects on student performance when teachers were given access to PLA. These effects were likely to be explained by teachers making limited use of PLA in their practice. Follow-up interviews revealed that teachers had positive views about using PLA in teaching, as they recognised their usefulness for complementing the teaching practice and being ‘on top of things’. Also, in a multi-methods study with 59 teachers, and more than 1300 students, Herodotou et al., (2019a) identified that teachers' engagement with predictive data was the second most significant factor explaining student performance, after previous best score. In a follow-up study with 189 teachers and 14 K students in 15 undergraduate courses, Author, A. et al. (2019b) identified that teachers who made ‘average’ use of OUA benefited their students the most; those students were found to present significantly better performance than peers in the previous year's presentation during which the same respective teachers made no use of PLA. Yet, Dazo, Stepanek, Chauhan, and Dorn (2017) identified that frequency of visits decreased between semesters after analysing usage data of 14 teachers. A follow-up focus group with six teachers pointed out that teachers faced challenges in interpreting the data, and this led some of them to shift to other methods of monitoring students' progress, such as reading their posts.

Overall, there is an emerging body of evidence showing that PLA can be effective in some cases, yet not in others, raising the need for more, robust, and longitudinal research beyond a single context or discipline. As argued by Ferguson and Clow (2017) and Viberg et al. (2018), in order to provide convincing evidence to key stakeholders, including teachers, it is essential that the LA community shows consistent and reproducible results about the conditions under which PLA may (or not) work. Towards this direction, in our previous work we interviewed a small sample of teachers and identified how technology acceptance related to usage patterns (Author A. Accepted 1). Given the small sample size of the previous study, in the present study (RO2) we sought to reproduce, expand, and elaborate on our initial understanding by identifying further factors - other than technology acceptance - that may explain usage patterns. Also, we adopted a focus-group face-to-face methodology (rather than interviews as in previous studies, see 4.2.2.1) and enabled participants to access and use OUA in real time as a means to facilitate reflections and in-depth group discussions.

3. Methodology
3.1. Settings: OU Analyse (OUA)
OUA is a predictive system used to identify learners at risk of failing their studies (Fig. 1). OUA predicts on a weekly basis whether (or not) a given student will submit their next teacher-marked assignment. The OUA dashboard visualises predictive information about who is at risk of not submitting their next assignment for individual students, as well as VLE engagement, and assignment submission rates at the cohort level. It uses a traffic light system to pinpoint: (a) in red students at risk of not submitting their assignment, (b) in amber those with a moderate probability of not submitting, and (c) in green those who are likely to submit and be successful.

Fig. 1
Download : Download high-res image (316KB)
Download : Download full-size image
Fig. 1. OUA dashboard with VLE and predictions for individual students.

Predictions of students at-risk of not submitting their next assignment were constructed by machine learning algorithms that make use of two types of data: (a) static data: demographics such as age, gender, geographic region, previous education, and (b) behavioural data: students' interactions within the VLE hosting a course. These sources of data were shown to be significant indicators of predicting students' assignment submission (Kuzilek et al., 2015; Wolff et al., 2013). To assess the quality of predictions by OUA, in each week confusion matrix values (True Positive, True Negative, False Positive, False Negative) for all courses were averaged in order for balance the relative size of respective courses. From this matrix, Precision, Recall, F-measure, and Accuracy for the courses under study were computed. Precision refers the proportion of students that OUA correctly identified as “not submit” out of all students identified as “not submit”. Recall refers to the proportion of those students correctly identified as “not submit” of all students that have not submitted an assignment. F-Score is the harmonic mean of Precision and Recall, and Accuracy is the proportion of all correct predictions to the total number of students. Students “at risk” (‘will not submit next assignment’) is the class under examination. Predictions were produced for only those students who did not submit their assignments in the respective week.

OUA employs three machine learning methods: (1) Naïve Bayes classifier (NB), (2) Classification and regression tree (CART), (3) k-Nearest Neighbours (k-NN). Those were used to develop four predictive models: (1) NB, (2) CART, (3) k-NN with demographic data, and (4) k-NN with VLE data. Combining results from these four models was shown to improve overall predictive performance. Two versions of k-NN were used due to the different nature of the values measured, i.e. numeric VLE data and categorical demographic data. These four models considered different properties of student data and complemented each other. Each model classified each student into two classes: (a) will/will-not submit next assessment and (b) will fail/pass the course. The final outcome of the prediction was produced by combining the outcomes and using voting techniques from all four models (Kuzilek et al., 2015; Wolff et al., 2013). When three or all four models gave a prediction of ‘will-not submit’, the result of the prediction was ‘will-not submit next assignment’. When zero, one, or two models only vote ‘will- not submit’, the outcome was ‘will submit’.

3.2. Analysis of stakeholders' perspectives
3.2.1. Project management's perspective
The first part of this study reflected on the degree of OUA usage, challenges and factors facilitating adoption during a period of 4 years (RO1), as perceived by an interdisciplinary project management team (the six authors of this paper: one post-doc technical designer; one post-doc qualitative researcher; one senior project manager; one senior lecturer in educational technology, two Professors in Machine Learning and LA). The first step of this analysis was to visualise OUA adoption in terms of numbers of teachers and other staff accessing OUA per academic year. The second step was the production of usage statistics across course presentations showing the level of teachers' usage across the four university Faculties. The third step was to discuss these graphs by reflecting on challenges and aspects facilitating adoption.

3.2.2. Teachers' perspective
The second part of this study detailed the perspectives of eight teachers who were given access to OUA, as captured through 2, 4-h, evaluation workshops. The workshops aimed to identify factors that potentially could explain any differences in the degree of OUA adoption by teachers, and could inform further steps in terms of how to facilitate adoption and enable a scalable implementation. In the next sections, we detail the process of data collection and analysis.

3.2.2.1. Sample
Eight teachers joined the two face-to-face workshops (N = 8). These were four male and four female, from the Science (n = 7) and Business and Law (n = 1) Faculties, with an average teaching experience at the OU of 14 years, and a mean age of 54 years. Teachers were self-selected and identified after responding to an email from the project management team requesting for volunteers to take part in OUA evaluation workshops. Financial incentives in the form of travel reimbursement and subsistence were offered to participants. The response rate was relatively low, but consistent with previous OUA evaluation studies (Herodotou et al., 2017). This low response rate can be explained by various factors. First, the teaching model at the OU is quite different from other distance or face-to-face universities. Second, most teachers working at the OU are geographically spread across the four nations in the UK, with substantial travel time to come to the main campus. Third, teachers are part-time employees of the OU, often having other part/full time occupations, and/or caring responsibilities that may inhibit them from taking part in any optional research-related activity. Fourth, teachers' contracts do not foresee involvement in any research or evaluation activities. Their work is remote and virtual - they communicate online with groups of 15–20 students, correct assignments, run tutorials, and assign grades. They rarely visit the university campus and may have found it difficult to attend the face-to-face workshops.

The authors acknowledge that the low response rate may have certain implications in terms of sample representativeness. Participating teachers may, for example, be technology prone individuals, with special interest in how technologies like OUA can support learning, or sceptical teachers with specific opinions as to how online students may learn best. Such broader perceptions about teaching and learning may have an influence on the use or acceptance of OUA. Therefore, the first activity of the workshops (see 4.2.2.2) aimed to understand teachers' beliefs about online teaching and learning, and potentially relate these to perceptions about OUA. It is also noted that given the difficulties in engaging a representative sample of teachers at the university under study, multiple studies took place (see Herodotou et al., 2019a; Author, A., in preparation; Rienties et al., 2019) in order to capture the breath of opinions of different teachers and other stakeholders, and build a robust picture of the factors possibly explaining the degree of PLA adoption. Overall, the authors recognise the self-selecting biases related to this approach of identifying participants (Rienties and Toetenel, 2016; Torgerson & Torgerson, 2008) and consider this in the data analysis and interpretation.

3.2.2.2. Process of data collection
Data were collected from two, 4-h, face-to-face workshops with teachers who had access to OUA. The workshops resembled the format of focus group interviews (Vaughn, Schumm, & Sinagub, 1996), and they built on our previous work of interviewing teachers about their experiences of using OUA (Herodotou et al., 2019a, Herodotou et al., 2019b). In line with Van Leeuwen and colleagues (van Leeuwen, 2018; van Leeuwen et al., 2015), they aimed to gain in-depth insights by allowing participants to exchange ideas and discuss possible (dis)agreements in relation to their perceptions about OUA (e.g., whether they perceive OUA as useful, under which conditions, and why). All participants consented for the sessions to be audio-recorded and anonymised data to be used in future reporting and dissemination activities.

Through Activity 1 we sought to understand teachers' general beliefs about teaching and learning in online settings, including previous teaching practices, and how these may explain technology acceptance and use of OUA. Activities 2 and 3 were framed to understand and explain teachers' usage and perceptions about OUA specifically from a TAM perspective. The workshop collected data about: (a) participants' perceptions and practices about online teaching and (b) their understanding, perceptions, and future intentions in relation to OUA (see TAM). They were structured around a set of group and individual activities participants were asked to complete:

•
Activity 1: “Before OUA” (group discussion, audio-recorded): How would you characterise your relationship with your students? When and how often do you get in touch with them? Do you monitor their activities online and if so how? What is your approach to students who may struggle with their studies?

•
Activity 2: “OUA usage patterns” (individual activity, paper-based): Participants were asked to complete a worksheet about OUA usage, which included a set of screenshots and questions about OUA access, features they use, and their understanding of it (see TAM)

•
Activity 3: “Perceptions about OUA” (group discussion, audio-recorded): Do you find OUA or specific features of it particularly useful and if so, in what respect? How do you use or would you use OUA with your students? What conclusions can you draw about students' performance? Did OUA change your existing teaching approach in any ways? (see TAM)

3.2.2.3. Process of data analysis
Workshop data from Activity 1, 2 and 3 were entered into NVivo. We used thematic analysis (Boyatzis, 1998; Kvale, 1996) to identify emerging themes related to the aims of the workshop such as existing student support approaches, usefulness and challenges (see Table 1). Author 1 and Author 5 coded the data independently and compared emergent themes to ensure inter-rater reliability. The inter-rater percentage agreement, which equalled to 85%, was calculated by dividing the number of times both researchers agreed by the total number of times coding was possible (Boyatzis, 1998). As a result of this comparative exercise, few codes were renamed to enhance comprehension and other were merged together to avoid overlaps. After agreement was reached, changes were fed into the coding of the rest of the transcripts.


Table 1. Themes emerging from thematic analysis.

Main themes	Subthemes
Existing student support approaches	
•
Perceptions about teaching online

•
Contacting students

•
Monitoring performance and contact

Accessing OUA	
•
What points during the course presentation OUA is accessed by teachers

OUA features	
•
VLE data

•
Predictive data

•
VLE and predictive data

OUA usefulness	
•
Specific features of OUA,

•
Design of online courses,

•
Type of students

OUA challenges	
•
Selection of OUA features

•
Accuracy of predictions

•
Access to OUA

Data literacy	
•
Understanding OUA

•
Training sessions

4. Results
4.1. RO1: To reflect, at a macro-level, on the use of PLA by detailing the degree of OUA usage, challenges, and aspects facilitating adoption, over a period of 4 years
In terms of RO1, the first step of this reflective account was to visualise the degree of OUA adoption over the last four academic years (2015/16–2018/19), in terms of (a) numbers of unique teachers, and (b) usage patterns of unique teachers over the course of a presentation. The number of teachers accessing OUA at least once increased considerably over the 4 years (see Fig. 2), with 52 teachers in 2015/16 and 1159 in 2018/19. Yet, the overall percentage of those accessing OUA out of those who were granted access per year varied between 33% and 89%.

Fig. 2
Download : Download high-res image (146KB)
Download : Download full-size image
Fig. 2. OUA adoption by teachers during the last 4 academic years.

Fig. 3, Fig. 4, Fig. 5, Fig. 6 illustrate the average weekly usage of OUA by online teachers across the four university Faculties. In 2015/2016 (Fig. 3), teachers in Education courses were more actively engaged with OUA, in particular the first half of the presentation. The overall average engagement dropped over time across all Faculties. In 2016/2017 (Fig. 4), teachers in Social Science were more actively engaged than their peers in any other Faculty. In 2017/2018 (Fig. 5), there was a considerable drop in participation, especially in Social Science, with Science being the most active Faculty. In 2018/19 (Fig. 6), participation across all Faculties was observed, with Business/Law being the most active, especially during the first 10 weeks of the course presentations. In the next paragraphs, we explain these trends by reflecting on the challenges and factors facilitating adoption over time, as experienced by the project management team.

Fig. 3
Download : Download high-res image (143KB)
Download : Download full-size image
Fig. 3. Percentage of teachers accessing OUA relative to those teachers with access to OUA per academic year (2015/16).

Fig. 4
Download : Download high-res image (145KB)
Download : Download full-size image
Fig. 4. Percentage of teachers accessing OUA relative to those teachers with access to OUA per academic year (2016/17).

Fig. 5
Download : Download high-res image (117KB)
Download : Download full-size image
Fig. 5. Percentage of teachers accessing OUA relative to those teachers with access to OUA per academic year (2017/18).

Fig. 6
Download : Download high-res image (116KB)
Download : Download full-size image
Fig. 6. Percentage of teachers accessing OUA relative to those teachers with access to OUA per academic year (2018/19).

The OUA project built on existing analytics work at the OU and aimed to generate evidence about the impact of OUA on improving student retention and performance, a strategic objective of the university. It was set-up as an interdisciplinary project with colleagues from Faculties, teachers, academics, information technology and evaluation experts. Several channels of communication were set up that facilitated interactions, including a support mailbox and forum, training sessions, and regular meetings with involved stakeholders.

In 2015/16, OUA was piloted with a small group of volunteer teachers and course chairs from ten courses across three Faculties (Science, Social Science, Education). Teachers were self-selected, resulting in a 89% OUA usage out of those teachers who were given access to the system (see Fig. 2). Project evaluation focused on issues of OUA access and their relationship to student outcomes (Herodotou et al., 2017). In 2016/17, a new round of piloting was set up with 24 courses. The evaluation plan foresaw the use of experimental methodologies (Randomised Control Trials, A/B tests), due to their robustness in generating more conclusive evidence of impact on student outcomes. Yet, this methodological approach was not taken forward by the Faculties; during that period, the OU went through a tremendous institutional change related to the teaching and learning policy, which caused substantial concerns about teachers' workloads, and resulted in Faculties being unwilling to engage with these experimental pilots. Therefore, the methodological approach was revised and participation on a voluntary basis was adopted. This resulted in a significant drop in participation (33%). Additional factors were also related to that drop, as flagged in the communication with Faculties and teachers, including the simultaneous running of other retention initiatives, the introduction of a new tuition technology (replacement of Eluminate with Adobe Connect), teachers requesting extra payment for participation in the pilots, and the renegotiation of teachers' employment contracts. These issues were mainly coming from the Education and Business/Law Faculties potentially explaining the low OUA usage (see Fig. 4). They also affected piloting in 2017/18; only two Faculties (Education and Science) participated officially in pilots with teacher-volunteers from 22 courses, with 63 less teachers accessing OUA compared to the year before (see Fig. 5).

In 2018/19, a major change took place; all Faculties agreed to embed a link to OUA in the teachers' support homepage, thus enabling easy access to all teaching staff across the university. Prior to that, staff were expected to access OUA through a separate URL. This decision resulted in a considerably large number of teachers accessing OUA – 1159 unique teachers, as compared to hundreds previously, and had certain implications. It meant that not only teachers, but also course chairs and managers recognised the value of OUA, and agreed on supporting its use across their Faculties (see Herodotou et al., 2019a, Herodotou et al., 2019b). Fig. 7 showcased that an average of 35% of teachers and 57% of course chairs accessed OUA. In particular Business/Law systematically encouraged usage of OUA - by developing a coherent teaching and intervention strategy and actively promoted the availability of OUA as a tool to monitor students' progress as well as a means to trigger possible interventions - resulting in more than half of the staff (56.5%) accessing OUA, the highest percentage across Faculties. This suggests that Faculty buy-in and support can considerably facilitate the degree of adoption towards a scalable implementation.

Fig. 7
Download : Download high-res image (356KB)
Download : Download full-size image
Fig. 7. Staff usage of OUA in 2018/19 across four faculties.

Two other factors built in the design of the project may well have facilitated adoption: First of all, four teachers acting as so-called “OUA champions” were recruited to provide training and support to teachers before the start of their courses every year the project ran. They shared authentic, practice-based examples about how to use and act upon OUA insights, and acted as an interface between the technical team and the user base throughout the project implementation. These teachers raised awareness and generated interest in OUA across the university. Yet, this specific teacher-centred communication channel also revealed a lack of digital skills amongst some of the teaching staff, and highlighted the need for additional support in relation to interpreting OUA visualisations and effectively supporting students at risk.

Secondly, the pilots were systematically evaluated across the 4 years of implementation, resulting in a growing evidence-based account about the effectiveness of OUA. Insights were produced yearly and by the end of course presentations, and were regularly disseminated by different members of the project team in Faculty and university wide events, raising awareness and sparking further discussions and interest about OUA. Education managers proposed this as a factor that has the potential to facilitate adoption (Herodotou et al., 2019a, Herodotou et al., 2019b).

4.2. RO2: To reflect, at a micro-level, on the use of PLA by detailing the perspectives of teachers who make use of OUA
The second part of this study focussed on RO2 and produced a fine-grained narrative about the lived experiences of eight OUA teachers. Fig. 8 shows the average OUA usage patterns of the eight workshop participants, up to the day before the workshops took place. Four of the teachers had a relatively active engagement with OUA (P2, P3, P4, P5) whereas the rest of them had a relatively low participation. Yet, P1, P6 and P8 accessed OUA in weeks during which assignments were submitted. Due to the nature of the workshop (audio recording of focus-group discussions), we could not map OUA usage with teachers' perspectives analysed below. Such an analysis can be found in our previous work, with a different group of teachers (Author A., accepted 1). In the remainder of this study, we unpacked teachers' existing study support approaches, how they accessed OUA, how they made use of OUA features, whether they thought OUA was useful, how they made sense of OU, and what the main challenges of using OUA might be (see Table 1, emerging themes from data analysis).

Fig. 8
Download : Download high-res image (159KB)
Download : Download full-size image
Fig. 8. OUA usage by the eight participating teachers.

4.2.1. Existing student support approaches
The discussions of eight teachers around how they approached and supported their students revealed a great depth of variation and a degree of personalisation in the proposed student support mechanisms. This variation indicated that there was no standard or unique way of how and when teachers approached students in order to ensure that they progressed with their studies. This was a decision made by each individual teacher based on their own perceptions about how to best support students' learning. Participating teachers agreed on the need to contact students, yet the frequency and way of contacting students varied considerably, as shown in Fig. 8.

In terms of how often these eight teachers got in touch with students, some teachers explained that they were very proactive and they tended to email, text, or phone their students, as well as regularly posting discussion threads in forums. Others got in touch with certain students, such as those with accessibility needs, assuming that these groups of students might need additional support. Others mentioned that certain courses might have specific requirements they had to follow, such as setting up an appointment with students during the first 2 weeks of a course. As explained:

−
Female 3: “I tend to ask what they want from it, if I see someone that has got a D marker, can you let me know how you want me to help with your study [...] Like I say I'm not be like [name of teacher removed]. I don't phone all my students, I don't chase them up because as I say they are adult learners.”

−
Male 4: “Well I don't chase them every week, I send out lots of emails and stuff on the tutor group forum...I only give them another phone call if they are falling behind or not logging on.”

−
Female 3: “Are all students positive about that? The reason I don't do that is because students will say I work at the Open University and the reason I study with that University is because it is distance learning and I don't expect you to be checking up on me every week.” (Workshop 1)

The above excerpt revealed that the frequency of contacting students was associated with the teachers' conceptions of who the students were and their assumptions of why they studied online. In particular, ‘sending out a lot of emails’ might be perceived by some as ‘chasing’ or ‘spoon feeding’ students, and therefore viewed as inappropriate to adult learners, and one that may inhibit students from becoming independent learners. Also, studying at a distance learning institution entailed certain connotations, such as that students do not need to be checked regularly. The excerpts showcased two opposing student support approaches: For example, Male 4 was acting over and above the course requirements, and contacted students regularly and through varied means, while Female 3 was less proactive and perceived students as not needing frequent communication.

In addition to the personalised ways of contacting students, teachers explained that students themselves and/or the university Faculty might explicitly define or influence how students are contacted. Some students were interested in being known by their names, and had personal contact with the teacher, whereas others viewed teachers as those marking assignments. Students' perceptions of the role of the teacher could be influenced by Faculty and course regulations:

Some students the ones that really want you …my name is Winston and I want everything I do in the university to say my name, whereas other students will say, I'm studying multiple things and you are just the person marking the stuff ...As far as they are concerned ... you just happen to be the person that is going to mark their work ...the starters they tend to know you as the face…Also, how your faculty works it, we have this thing where, I have my students but they can go to any tutorial and so I'm not their face really.

(Female 3, Workshop 1)
A variety of approaches was used as a means to communicate and support students, including emails, phone calls, and forum threads. Teachers preferred emailing than phoning students because most students might not answer their phones, or were unavailable: “putting the kids to bed or just making dinner” (Female 5, Workshop 1). Yet, emailing students did not ensure that students would reply: “But at least, I like emails because you have got a written record” (Female 3, Workshop 1). In addition to that, some teachers devised and tested additional approaches to supporting students such as the “cuppa sessions” (cup of tea sessions): “One of the things I started doing this year ... I said I will always be in the tutorial group Tuesday 8–9; they never turn up, never” (Male 3, Workshop 2). The fact that students might not be responsive or ignore the communication of teachers was a major challenge reported. Yet, how this challenge was addressed depended on how teachers perceived and explained the behaviour of those “ghost” students. In the excerpt below, teachers viewed “non-responsiveness” as not intentional:

−
Female 2: “I say this is the course you signed up for, these are the requirements, it's a very short letter ...and I would like you to please respond to this email because I need to know this communication is working [...] if they don't respond my assumption is they didn't get it. I always treat them, as they haven't got it, the email address is wrong.”

−
Male 1:“So if they don't respond you bombard them with more emails until they have had enough and they respond?”

−
Female 2: “Yes. Basically, they get the second email if they don't respond ...I assume the email address is wrong, it has nothing to do with the email and so then I will send you a letter, [...]I never accuse them of not responding intentionally, I always assume there are external circumstances prevented them from replying.” (Workshop 2)

In terms of recording and keeping track of communication with students, teachers either “put the students name in and search the whole email history that is behind the conversation” (Female 2, Workshop 2) or developed their own working sheets where they noted down the student contact history. As shown in the discussion below, not all teachers were as systematic in their monitoring approaches. This discussion stressed the need for designing and testing tools that can help online teachers monitor communication with students, including student progress and interactions.

Anything I have done with a student I have [it] on my A4 sheet of paper...non-submission of an assignment I send an email and if it's a double R, I sent two reminders if you haven't submitted...If I put an E...it means I gave you an extension...X means got the TMA everything is fine...I can immediately see I need to email X,Y and Z. (Female 2, Workshop 2)

4.2.2. Accessing OUA
Participants tended to use OUA at the beginning of the course, close to the submission of an assignment, when they had concerns about specific students or weekly. As explained: “It depends how active my students are in the Tutor Group Forum/email. If they are active then not often, if not then I can use it as a monitoring tool and so more often.” (P2, Workshop 1). In a follow-up question about: “At what specific points during the duration of a module presentation [do you use OUA]?”, the majority said 2–3 weeks before the submission of an assignment, others said early in the life cycle of a course, one participant said monthly and another one every Wednesday when OUA updated the respective predictions. These responses showcased that the actual usage of the system was mainly linked to the assignment submission deadlines, when students were silent, or when teachers had specific concerns about particular students.

4.2.3. OUA features
Teachers commented on the usefulness of VLE data in identifying whether students were engaged with course materials, in particular the number of clicks, and the tasks that had been checked by them throughout the week. This was viewed as an aspect of student participation that could not be monitored in another way, was provided systematically and on time in order for teachers to react and support students who faced challenges:

OUA gives me another depth, tells me all the online activity which is important ... that is something I cannot monitor from home, I have my papers I only have my feedback every eight, or four to eight weeks and that is sometimes too late, if someone is on a level two Physics course because they don't understand the maths that is used by the time the TMA [assignment] comes there is a high chance I have lost that student, so seeing lack of activity in the beginning that the student is not getting past chapter one/chapter two would be really useful information. (Female 2, Workshop 2)

They also commented on the line graph in OUA comparing the individual level of engagement to the whole cohort of students, and its usefulness in identifying whether a student is performing better or worse than the average student. As explained: ‘I believe it is seeing the visual of student's engagement and also to get a sense of the general cohort engagement so you can compare.’ (Male 1, Workshop 2).

In the individual Activity 2 in which participants were asked to complete a worksheet with screenshots from OUA, the majority of participants (n = 6) circled multiple features of OUA, including both the VLE and predictive data as being useful, including features such as predictions at student level, risk of failure, risk of non-submission, next assignment prediction, and the VLE graph. Other features like the time machine, the nearest students' comparison, trends, and filtering were less often selected. One teacher circled only the VLE data, and one other only the predictive data. It was not clear which source of information was perceived as the most significant, as some participants assigned a star (indicating the most significant feature) to VLE data, others to predictive data, and others to both sources of information. As explained: “Predictions - tells me at a glance what a student is achieving/not achieving and student course data tells me individual student engagement data” (Participant 8) and “Seeing how much they are interacting with the website is most useful as it gives an idea of whether they are keeping up with the work via the VLE.” (Participant 2).

4.2.4. OUA usefulness
A unique contribution of this study is that we specifically probed teachers to reflect on their use of OUA by providing them with paper-based activities showing the functionality of OUA. The OUA usefulness was discussed in relation to: 1) specific features of the OUA dashboard and how these could help make informed decisions, 2) the design of online courses, in particular whether they were entirely hosted online, or whether they also had offline components such as printed material, and 3) type of students e.g., new to the university. Teachers perceived OUA as a tool that could help in identifying students who struggle with their studies and they need extra support, or they need a reminder that they should engage with the online material. Students flagged as “green” were viewed as not needing support: “It's quite good to find those ones that fall between the cracks, the ones that are struggling and asking for help [...]then there's ones on green and you think I can pretty much ignore them, they are doing the work.” (Female 1, Workshop 1).

Some teachers viewed OUA as the only means to gain information about their students' engagement with the online material. As explained below, without OUA the respective teacher could not know whether their students faced difficulties, and whether she needed to intervene. Other teachers viewed OUA as a tool that could save them time by not having to access and check dispersed sources of student information (i.e., previously the OU had various data bases and tools in different systems and locations). In particular, teachers made reference to student data in the OU SAS portal such as the last time a student logged into VLE, that could inform whether a student is engaging with respective learning activities:

I'm concerned about students and see evidence that things are not right, I can contact the student a week or two weeks before the TMA and say is everything ok? I didn't have the tool before because...I didn't know when the student was engaging at all or not. (Female 2, Workshop 2)

How to help students in a better way... this is a very clever little tool... if you say to an AL (teacher) how you can save yourself an hour a week going through the data then it's starting to look very promising. (Male 1, Workshop 2)

Participating teachers viewed OUA as particularly useful for students who are new to the university, and who might more likely face challenges in terms of adjusting to online study (Calvert, 2014; Coughlan, Ullmann, & Lister, 2017). As the OU has an Open Entry policy, whereby any student with no previous qualifications can join the university, teachers are more likely to need support to identify and intervene with newcomers who might face difficulties. Teachers viewed OUA as a tool that could help “tailor themselves” to individual students, especially Level 1 undergraduate students, who were less likely to get in touch with teachers and request for help, as opposed to more experienced students (e.g., Level 3): “They don't know what they are doing...and it's up to you to make sure you can help them out so any information on Level 1. Level 3 you know, you know if there is a problem they are just as aware of it as you [are].” (Male 3, Workshop 2).

In addition, participating teachers perceived OUA as particularly useful when a course was entirely hosted on the VLE, and had no offline components, such as links to printed materials (e.g., books). OUA information about courses with offline components might present misleading information about students, by showing them as not interacting with VLE, when in reality they might have read the materials offline. In these cases, teachers had no information as to whether students engaged with printed material, and OUA was less likely to give them reliable information either: “my course is mainly with books, so they work in the VLE then get told go and read that chapter. You can at least see if they are engaging in the VLE...So at least then you have some idea of what they might have looked at.” (Female 1, Workshop 1). These findings were also reported by blended studies in LA, which highlighted that predicting behaviour from off-line tasks is complex and cumbersome (Jovanović et al., 2017; Larrabee Sønderlund et al., 2019).

4.2.5. Understanding OUA
Data literacy was a theme that emerged when teachers were asked to note down their understanding of OUA features. While overall participants were aware of what the VLE and predictive graphs meant, some noted that they did not understand some features of OUA, such as the filter functionality (Participant, 8). Furthermore, some participants had incomplete or incorrect understanding of some of the functionalities of OUA. For example, Participant 2 explained: “VLE data shows a comparison of my cohort's interaction with the VLE compared to another cohort...Time machine can see an individual's interaction with the VLE compared with the whole unit cohort.” This interpretation was not entirely correct as VLE data show how the entire cohort of students (not only the teachers' group) compares to last year's cohort, whereas the time machine functionality enables teachers to preview the cohort and student activity in previous weeks of the course (i.e., move the time back).

Aligning with the above observation, teachers raised the need for hands-on, collaborative workshops about OUA, and suggestions of how to act upon data. They contrasted the format of the OUA training sessions to the workshop design of the present study; the former were online conferencing sessions delivered by teachers-experts in using OUA, and they mainly showcased OUA features and functionality, alongside ways to support students. What was missing from these sessions, according to participants, were opportunities to engage and use OUA, raise questions and discuss with colleagues:

I think what would be useful is...workshops where you have people sitting in front of a computer and they can work through the data themselves, like hands on learning of the possibilities...it's a group where people can work through the data together and see all the aspects, doing an OU live session is quite nice but you have forgotten half of it afterwards. (Female 2,Workshop 2)

4.2.6. OUA challenges
Participating teachers commented on specific aspects of OUA that could improve their engagement with the system, related to: (a) Selection of OUA features: Teachers valued certain features of OUA more than others; for example, as explained below, cohort-level data were viewed as not useful at all, and therefore teachers should be able to select those OUA features they would like to have access to: “Could you actually say to individual Tutors [teachers] which data do you want? ... Some of the other stuff I look at every four or five weeks if I could pick and choose what data I got when just by default” (Male 3, Workshop 2). (b) Accuracy of predictions: Teachers expressed concerns in relation to the accuracy of student predictions, as explained: “looking at the prediction of OU Analyse, it said a student would fail, it just happened to be a student I met at a face to face tutorial I thought if the student makes the effort in this tutorial is to be very well...I don't know what happened to this student now but I like that I could say no, I think your prediction on this case was wrong” (Female 2, Workshop 2). Other issues affecting the accuracy of OUA were assignment extensions, which were not always recorded by the system, and the regularity of updating OUA data, which took place once a week, rather than daily: “The whole week layout, it means sometimes by the time you know someone is not logging in almost two weeks have gone by”(Female 3, Workshop 1).

5. Discussion
This study described a large-scale implementation of a PLA tool called OUA at a distance learning university by reflecting on the macro-level of use, that is the challenges and factors facilitating implementation over a period of 4 years, as experienced by the research or project management team behind the project (RO1). Furthermore, this study analysed on a micro-level the diverse perspectives of eight teachers who used OUA in their courses (RO2). In relation to RO1, a set of factors were shown to facilitate project implementation and adoption by teachers. The numerical analysis of teachers who had used OUA revealed a significant and steady increase in unique users during the last 4 years, with 52 users in the first year and 1159 in 2018. Yet, the degree of OUA usage by those users as captured by log-file data was shown to vary across academic years and Faculties; certain Faculties were more engaged over the years than others, and in particular during the first half of a respective course.

This variation in frequency and intensity of OUA use could in part be explained by teacher-related institutional changes (i.e., tuition policy, systems' use, employment contracts, payment for joining pilots, other retention initiatives) that resulted in the Faculties' resistance to roll out OUA across all of their courses in 2015–2017. It is proposed that future PLA implementations should consider wider organisational or teacher-related changes that may take place at an institution and which could influence the uptake of PLA. The formation of an interdisciplinary project management team consisting of, for example, Faculty representatives, teachers, academics, education managers, information technology and evaluation experts, could tackle such issues and facilitate implementation. In the current study, we as project team gave a ‘voice’ to a range of stakeholders involved in PLA, and facilitated communication and meaningful interaction – OUA related problems could be communicated, negotiated and potentially solved – through a range of media, including a support mailbox and forum, training sessions, and regular meetings with respective stakeholders.

The substantial increase in adoption in 2018 could be explained by Faculties gradually recognising and appreciating the value of OUA (see Herodotou et al., 2019a, Herodotou et al., 2019b), and actively promoting its use across all of their courses. A prominent example of scalable implementation took place in the Faculty of Business/Law. This Faculty developed and promoted a coherent teaching and intervention strategy that included the use of OUA, and resulted in 56.5% engagement of staff, the highest across the university. Faculties can play a critical role in the process of adoption, as their explicit and active engagement can facilitate adoption (Dawson et al., 2018; Viberg et al., 2018). In particular, the development of a Faculty-wide policy that details and promotes the use of PLA could be an effective way of raising awareness amongst teachers and increasing the number of users.

The ongoing technical development of the OUA dashboard also influenced adoption across the OU. While originally accessed through an external link, the dashboard has now been embedded into the teachers' support homepage. This is a straightforward way of giving access to the tool to teachers, and is closely linked to the everyday practices of teachers. Also, the ‘teachers-champions’ approach – i.e., OUA training sessions were delivered by teachers- colleagues who shared the ‘same language of communication’ as teachers – raised awareness about OUA, especially within the teaching community and potentially assisted adoption. Finally, the systematic production and presentation of evidence of impact from the project team backed up the efforts made for adoption and showcased the value of OUA in supporting student learning.

In relation to RO2, the focus group discussions with experienced middle-aged teachers and their responses to a set of paper-based activities provided deep insights about the micro-level of use; they revealed a diversity of approaches in relation to contacting and monitoring students and their progress. Some teachers were considerably proactive and systematic in their use of OUA, while others were acting on a need to know basis. These practices were shown to relate to certain conceptions of teaching, in particular perceptions about online students as being either independent learners or requiring constant monitoring, support, and communication. A major challenge reported by teachers was that online students often did not respond to their communication. Aligning with the above conceptions, some teachers were very persistent in contacting students, till communication was established, while others were less active and ceased efforts after a few unsuccessful attempts. It is noted that the great majority of participants were coming from the same Faculty (Science) yet in contrast to existing studies (Norton, Richardson, Hartley, Newstead, & Mayes, 2005), they were found to have varied conceptions about what teaching looks like in online settings. Some teachers were shown to systematically pursue frequent communication that could contribute to the development of online presence and the formation of relationships with students (Dockter, 2016). Other teachers viewed students as not requiring this type of support and communication, contradicting existing studies and the self-reported need of students for online presence that can facilitate success (Rienties et al., 2018; Lin, Wang, & Lin, 2012; Muñoz Carril, González Sanmamed, & Hernández Sellés, 2013). It could be argued that the lack of an established, university-wide policy as to how teachers should communicate and monitor students in a PLA environment resulted in a substantial variation of approaches, which might not always be to the best benefit of students. At the same time, this more laissez-faire approach may help students develop more self-regulating skills and manage their own time.

Access to OUA was mainly linked to assignment submission deadlines and students who were “silent” or raising concerns to teachers. What is yet to be explored is whether this frequency of accessing OUA is adequate for intervening on time. For example, Herodotou et al. (2019b) showed that a certain degree of usage enabled teachers to achieve better learning outcomes compared to previous years when they did not use OUA. In terms of whether certain OUA features were perceived as more useful, a discrepancy was observed between the focus-group discussions and the individual paper-based activity. In the former, participants commented on VLE data as being very useful for the ongoing monitoring of students' participation, yet in the latter they considered both VLE and predictive data as being equally significant. Drawing from TAM, they expressed high levels of Perceived Usefulness (PU), especially under specific conditions: (a) identify students who struggled with their studies and need extra support, (b) OUA saves teacher time by not having to check on distributed sources of student information, (c) OUA is particularly useful for monitoring newcomers to the university, and for whom previous information is relatively limited, and (d) courses entirely hosted on VLE with no links to printed material. What was particularly revealing was that students who were flagged as “green” were viewed as not requiring support. This perception contradicts existing literature that notes the need of students to not only pass a course, but also challenge and grow that can be facilitated through a strong online teachers' presence (e.g., Lin et al., 2012)

In terms of Perceived Ease of Use (PEU), teachers did not express any significant concerns in terms of difficulty in accessing OUA, apart from their need to choose OUA features they deemed more significant and improve the accuracy of predictions. What was particularly surprising was the fact that some teachers' understanding of OUA was either limited or incomplete. This may be an indication of low PEU that could implicitly affect the degree of technology acceptance or systematic OUA use. Along these lines, specific teaching perceptions, i.e., students do not need constant monitoring, might be an expression of low PU of OUA, explaining limited usage of the system by some teachers. Yet, the fact that teachers raised the need for interactive training may be an indication of change in PU, after discussions with colleagues in the two workshops, an approach that could facilitate further adoption in the future.

6. Conclusions
In line with recommendations by two systematic literature reviews on LA (Ferguson & Clow, 2017; Viberg et al., 2018), an interdisciplinary project team of six authors reflected on the large-scale implementation of PLA over a period of 4 years. It detailed the complex and diverse perspectives of various stakeholders involved in the macro- and micro-levels of adoption. They showcased that an emergent bottom-up approach of PLA through a strong consultation process (Dawson et al., 2018) and support by both the senior management and the ‘shop floor’ - teachers (Chandler, 2013; Piderit, 2000) can facilitate scalable implementation of LA. Such approach could be enacted by distinct interdisciplinary teams that are allocated time (Herodotou et al., 2019a, Herodotou et al., 2019b) to work across and within the different levels of an organisation to curate perspectives, negotiate ideas, and tackle challenges. The engagement of teachers as ‘champions’ and Faculty representatives in the process of piloting and evaluation, alongside the systematic generation of evidence of impact, were shown to positively influence the process of evaluation and degree of adoption. These combined factors allowed for technical development time in response to users' needs and raised awareness of PLA across the university.

Yet, the degree of OUA usage across courses remained relatively limited (Herodotou et al., 2017; van Leeuwen et al., 2014), raising the need for further organisational support and additional studies to illuminate further the micro-level of use in particular why some teachers choose to make limited or no use of PLA. In this study, we build on our existing line of work (Author A. et al., accepted 1, 2) and propose two new factors as potentially explaining this trend, related to teaching beliefs about online learning and teaching and digital literacy (Dazo et al., 2017). One way of tackling the latter is the provision of a suite of interactive training workshops over time that allow for continued discussions and exchange of ideas amongst teachers, and enable continuous professional development and quality enhancement. The former is rather more challenging, yet a course- or Faculty-wide policy detailing the teachers' obligations in contacting and monitoring students could be beneficial.

