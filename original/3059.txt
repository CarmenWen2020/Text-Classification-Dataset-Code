The SHEILA framework provides a policy and strategy framework informing the strategic implementation and use of learning analytics. However, as evidenced in several ‘ground-up’ implementations of tools, the institutional preparedness and the governance around use often comes secondary to the policy. In this paper we depart from familiar approaches and evaluate one such example of a tool's development (SRES – Student Relationship Engagement System). SRES' adoption and scaling across two institutions are evaluated using an auto-ethnographic approach scaffolded through the dimensions of the SHEILA framework, focusing on the individual perspectives of the institutional champions who have been central to this journey. This practical approach and the emerging insights may enable other institutions to identify areas of potential improvement and inform senior academic managers about the strategic requirements to scale the approach, accounting for aspects not considered in the initial ‘organic growth’ of the implementation.

Previous
Keywords
Learning analytics

Adoption framework

Scalability and sustainability of LA

Institutional adoption

Analytic autoethnography

1. Introduction
Educational technologies have promised to redefine how education is designed, supported and achieved. Technologies such as learning management systems (LMS), video lecture capture, embedded social media platforms, and interactive online textbooks have become commonplace in many Higher Education Institutions (HEI). However, the benefits of technology have been under close scrutiny (Henderson, Selwyn, & Aston, 2017; Kirkwood & Price, 2014). Most educational technologies have the inherent ability to track activity and generate data which can help individual users (students) to get better insight into their learning. Perhaps the most useful enhancement introduced with educational technology is that of learning analytics (LA), a field that promises to turn student data into digestible, actionable, and even predictable patterns to guide teaching and learning (Ferguson et al., 2016; Siemens & Baker, 2012). LA has been defined as "the measurement, collection, analysis and reporting of data about learning and their context, for purposes of understanding and optimizing learning and the environment in which it occurs" (Siemens & Gasevic, 2012, p. 1). While hype surrounding LA continues to grow, many LA designers and researchers have begun to question how LA initiatives can turn from ideal aspirations into everyday educational practices, as pilots and interventions often seem to stall or result in limited uptake (Dawson et al., 2018; Ferguson & Clow, 2017).

Looming largest in the discussion of how to ensure widespread adoption of LA is how institutions can support initiatives through clear strategic planning and governance. Mirroring technology adoption, issues related to adoption and use of LA are well known in the higher education context. Despite a broad consensus that learning analytics (LA) has the potential to positively affect education (Ferguson et al., 2016; Siemens & Baker, 2012), its adoption in HEI has been sluggish (Ferguson & Clow, 2017), and "the pace of adoption of analytics within education organizations can be categorized as at best sporadic, and at worst resistant" (Dawson et al., 2018, p. 237). Several obstacles have been described, including issues around data (quality, access, ownership, data literacy, analytical capacity), organisational landscape (governance, culture, preparedness, funding) and technical aspects (implementation, adoption, change-management, availability of tools) (Bichsel, 2012; Ferguson et al., 2016; Tsai & Gasevic, 2017). A large body of evidence suggests that in order to enable sustainable implementation and use of analytics, HEIs must align adoption with institutional vision and goals (Siemens, 2013) and would benefit from clear strategic planning and governance to cope with potential resistance to change (Macfadyen, Dawson, Pardo, & Gaševic, 2014). Research suggests that a clear definition of a policy framework to ensure alignment of LA purposes and uses of data (Slade & Prinsloo, 2013) should be paired with broad stakeholder engagement which puts ethical use of data at its centre (Drachsler et al., 2016; West et al., 2016). Recent research has also stressed the importance of including teachers in the design, implementation and even analysis of LA tools (Colvin et al., 2016; Dollinger, Liu, Arthars, & Lodge, 2019). As West et al. (2015) noted, understanding teachers' needs, especially around how they prefer to balance personal contact with students and the automation of teaching approaches, may be critical to support LA tools. And yet, to date, research is only just beginning to emerge that explores how teachers can become the champions and drivers of LA practice and design.

To explore the question of how LA adoption can be supported and scaled, in this article we will undertake a cross-institutional and research-practitioner collaboration to understand how the LA practice with the SRES tool unfolded at two Australian HEIs. Our approach therefore will add to the ongoing literature that discusses the challenges and barriers to teacher adoption of LA, such as concerns about academic workload, unsuitable tools, and confusion or discomfort around datasets that may negatively affect teachers' trust or engagement with LA (Colvin et al., 2016; Macfadyen & Dawson, 2012). To guide our comparative analysis, we will use the SHEILA (Supporting Higher Education to Integrated Learning Analytics) framework, originally created by a cross-European research team in 2018 (Tsai et al., 2018) to systematically evaluate the strategic pathway to implementation. In particular, we will draw on one output of the framework known as ROMA (Rapid Outcome Mapping Approach) which can be used as a guide to help plan and implement LA at an institutional level, although we will use it retrospectively as a tool to evaluate implementation.

1.1. LA adoption, complexity, and policy frameworks
Macfayden and colleagues (2014) observed that in order to overcome the challenges to adoption in LA, scholars and practitioners need to adopt a whole-of-institution organisational change management perspective that supports strategic planning and comprehensive policies. When viewed as complex systems it becomes apparent that "simplistic approaches to policy development are doomed to fail" (Macfadyen et al., 2014, p. 18). Addressing the challenges of engendering change at an institutional level seems to require strategic planning and comprehensive policy that is linked to the HEI's vision. Focusing on LA and assessment of learning, Macfadyen and colleagues discussed two key challenges: influencing stakeholder understanding, and development of technological infrastructure at an institutional level. Within the backdrop of complexity leadership theory, Dawson and colleagues (2018) explored the nuances of adoption and described two broad categories of institutions which either: "i) followed an instrumental approach to adoption - typically top-down leadership, large scale project with high technology focus yet demonstrating limited staff uptake; or ii) were characterized as emergent innovators, bottom-up strong consultation process, but with subsequent challenges in communicating and scaling up innovations." (Dawson et al., 2018, p. 236). This is echoed by others indicating that even if a LA initiative is pushed and supported top-down, support is needed from both senior management and the work floor (Herodotou, Rienties, Verdin, & Boroowa, 2019; Rienties et al., 2016).

Considering the type of top-down leadership approaches, the SHEILA framework provides an important milestone for the LA community. SHEILA presents a multi-dimensional scaffold to iteratively plan the development of a strategy and policy framework, enabling HEIs to implement and adopt LA. Its development focuses primarily on three major challenges: 1) demand on resources; 2) issues around ethics and privacy; and 3) stakeholder engagement and adoption. At the heart of the framework is ROMA introduced by Young and Mendizabel (2009) and applied to LA by Macfayden and colleagues (2014). This revolves around eight steps which are stated to enable institutions to define and iteratively re-define policy objectives to drive LA initiatives (Fig. 1). Steps range from mapping the political context to developing engagement strategies; and unlike traditional linear tools, ROMA can be implemented "iteratively to help allow for unexpected changes in complex settings" (Tsai et al., 2018, p. 322).

Fig. 1
Download : Download high-res image (215KB)
Download : Download full-size image
Fig. 1. The Rapid Outcome Mapping Approach (ROMA) at the centre of the SHEILA framework (adapted from Macfadyen et al., 2014; Young & Mendizabel, 2009).

Although the range of interviews gathered in the process of building the framework were grounded in complex issues and interrelations between people (students, staff, and support services), tools (for learning, like LMS, databases, and institutional software tools), and analytics (referring both to the data and the capability to extract useful and actionable intelligence), paving the way for a policy framework driving institutional strategies, we will argue that the framework has not yet been deployed to understand individual perspectives. This approach may be valuable in evaluating Dawson and colleagues' second category of LA adoption, which focuses on a bottom-up approach.

More specifically, we draw attention to the perspectives of tinkerers and champions in the development of such frameworks. The term tinkerers appeared in the LA field in the work of Blikstein and colleagues (Blikstein, 2011; Blikstein et al., 2014) and was derived from the work on epistemological pluralism by Turkle and Papert (1992). Specifically, Blikstein (2011) referred to two modes of programming in which tinkerers and planners display different approaches to solve programming problems. By tinkerers and champions, we refer to teachers and other staff at HEIs who are involved in the ongoing development, implementation, and/or use of LA tools (hence taking an active role in the technical development process). They become catalysts for change, provide help and support to colleagues, and actively promote the adoption of such tools. These teaching and support staff tend to have a strong working knowledge of not only the LA tool but also the political and pedagogical contexts in which tools are implemented and therefore are essential to enable sustainable adoption. As the primary developers and users of LA, tinkerers and champions also contribute valuable knowledge about the challenges and opportunities associated with widespread implementation of LA that can help support the co-creation of solutions.

Given the consensus for the effectiveness and impact that LA tools can have on students' learning, it is important to reflect on the case of a specific, mature, LA tool and analyse the patterns of implementation and adoption more systematically. However, this paper moves starkly away from the typical experimental studies of LA tools and introduces a novel approach to examine and evaluate the development, implementation, and adoption of a LA tool in two different institutions. Adopting an analytical autoethnographic approach, the authors narrate, analyse, and reflect on their experience of LA adoption through the lens of the people who develop and use LA. Further, the authors critically appraise their own narratives through the use of the eight steps of the SHEILA framework to explore the adoption and implementation of the SRES, which has already gained traction across several institutions and brought together collaborations across HEIs (Liu, Bartimote-Aufflick, Pardo, & Bridgeman, 2017; McDonald et al., 2016; Vigentini et al., 2017). Here we will focus on two similar institutions to enable a comparison, but with unique contexts to differentiate the pathways of implementation, to begin to unravel what it is like for researchers, practitioners, and developers to adopt LA tools on an individual level, with particular focus on scalability and sustainability.

1.2. Context of the study and SRES tool
The SRES (https://sres.io) is a bespoke development initiated in 2012 by a team of teachers from the University of Sydney to meet their immediate pedagogical needs of scalable engagement and student support. The majority of LA approaches over the past decade have focused on the warehousing and visualisation and/or predictive modelling of institutionally-available datasets for teachers and students (Duval, 2011; Dyckhoff, Zielke, Bültmann, Chatti, & Schroeder, 2012; Herodotou et al., 2019; Sønderlund, Hughes, & Smith, 2019). However, SRES was initiated to explicitly leverage teacher knowledge and understanding of their own learning and teaching context using small but meaningful data in order to allow them to support students directly (Liu et al., 2017). Recently, Pardo and colleagues (2018) have reiterated this need for LA to refocus on connecting teachers' contextual knowledge with data to enable generalisable and scalable student support. Compared to dashboards and predictive models, there is a relatively limited set of examples of this type of LA tool. MEAP (Moodle Engagement Analytics Plugin (Liu, Atif, Froissard, & Richards, 2015) and EASI (Jones & Clark, 2014) are early examples of LMS plugins that gave teachers the ability to leverage behavioural data from the learning process, such as logins, resource accesses, and grades to provide direct-to-student support from within the tool. The University of Michigan's E2Coach is also inherently student-focussed and provided targeted messaging to provide study advice, although it had a stronger reliance on large professional teams to support staff using the tool and predictive models; further, support messages were primarily sourced from students as opposed to teachers (Wright, McKay, Hershock, Miller, & Tritz, 2014). More recently, Blackboard Inc.'s 'Personalized Learning Designer' is an example of a commercial LA tool that focuses on scalable student directed actions, giving teachers the ability to design automated messaging and other actions that can be triggered by rules based on activity, discussion, grades, and other events (Blackboard, 2018). Pardo and colleagues (Pardo et al., 2018) have developed OnTask (https://www.ontasklearning.org/), a LA tool that provides teachers a web-based tool which allows teachers to create 'personalised learning support actions' from data imported in the tool which does not depend on an LMS.

In focussing on teacher knowledge and small but directly relevant data, SRES is most similar to EASI, MEAP, and OnTask (which has multiple versions). Compared with the first two, it is agnostic of the LMS used. Compared to all three, SRES covers the entire range of human-centred ‘data lifecycle’ needs (Arthars et al., 2019); that is, it addresses the real and contextualised data needs of students and teachers: capture, collate, analyse, act on, evaluate, reflect. SRES helps to capture relevant, live data from many learning environments (e.g. tutors using its web app to enter grades and feedback while interacting with students; students adding peer reviews and self-assessments) and curate this with other data (e.g. LMS engagement, quiz performance, discussion participation, grading feedback, etc) directly in the tool. Teachers have full control over the analyses performed on the data: beyond simple calculations and manipulations of data, SRES enables customisable dashboards and advanced machine learning. Teachers can also efficiently act on the data and analyses, such as by creating personalised 'nudges'. SRES enables the creation of content that students can access through the LMS, web pages, email, or SMS: this content can be static (e.g. written feedback, additional resources, encouragement) and/or interactive (e.g. requesting student reflections). Further, SRES has built-in mechanisms to collect engagement information on this personalised content, allowing students to provide ‘feedback on feedback’: students are invited to vote to indicate whether it was helpful as well as to provide open-ended comments. The responses from students feeds back to teachers, who close the loop by evaluating the impact of their actions and reflecting on these for future designs. In particular, the focus on applying data to supporting student learning helps to address one of the main gaps identified by Ferguson and Clow (Ferguson & Clow, 2017).

The data 'platform' characteristics reflect the fact that all these data-enabled processes are entirely controlled by the teacher within a ‘one-stop’ system: SRES itself. As mentioned, other LA tools provide mail merge capabilities for personalised messaging, or produce dashboards pre-prepared by data scientists, but SRES is unique in that all aspects of the data lifecycle are completely determined by teachers and their understanding of their students' contexts and needs. This approach was also reported by McDonald and colleagues (McDonald et al., 2016), who emphasized the importance of meeting "grassroots classroom needs" (p. 404) when building out a collaborative LA venture. As described in the two cases, SRES has a well-established user-base and several active partners. Its relevance has been widely recognised (e.g. 2017 Pearson ACODE Award for Innovation in Technology Enhanced Learning) making it a very good case for further analysis. The work stemming from the use of the tool also informed further development work on personalisation of feedback and was captured in a large multi-institutional international collaboration in the OnTask Project and further work on the personalisation of feedback at scale using LA (Pardo et al., 2018; Vigentini et al., 2019).

2. Methods
The qualitative approach we have adopted to analyse the implementations across the two institutions is analytic autoethnography. This approach allows researchers to evaluate cases and to cross-validate observations through a process of "engaged dialogue" as opposed to one of "detached discovery" (Anderson, 2006, p. 382). The approach here in applying analytic autoethnography to the two cases, is partly informed by 1) the first-hand experience of implementation; 2) the interpretations emergent from the different contexts; 3) the systematic use of the SHEILA framework as a grounding reference to self-reflection; and 4) dialogue with informants in the form of qualitative interviews and survey data conducted by the authors throughout the period (the University of Sydney Human Research ethics approval number2017/018).

Autoethnography uses the author's own experiences "to garner insights into the larger culture or subculture" that they are a part of (Patton, 2005, p. 102). Autoethnography grew from a need to "address aspects of social life that were neglected by social scientists" (Ellis & Adams, 2014). The primary source of data are the author's own experiences, meaning they are part of the story that is being told (Atkinson, Coffey, & Delamont, 2003). More recently, two forms of autoethnography have emerged; analytic and evocative. We have applied the analytic form as our objective is to develop theoretical explanations as opposed to narrative text that neither abstracts nor explains (Ellis & Bochner, 2000; Hughes & Pennington, 2016). There are five key features that differentiate the analytic approach from the evocative: "(1) complete member researcher (CMR) status, (2) analytic reflexivity, (3) narrative visibility, (4) dialogue with informants beyond the self, and (5) commitment to theoretical analysis" (Anderson, 2006, p. 378). In our case, CMR status has been opportunistic as opposed to convert, as researchers were members prior to deciding to conduct the research (Anderson, 2006).

In the context of academics working to implement and support the diffusion of innovation in LA over multiple years, this approach to data collection and analysis offers a rare window into individual experiences that would normally be left uncovered by other qualitative research approaches. For example, while it would be possible to conduct and analyse interviews, these would serve only to scratch the surface of their lived experiences. Although autoethnography is not commonly seen in research areas such as LA, it is becoming increasingly utilised in research situated in the context of higher education where the auto ethnographer may be a researcher, teacher or other higher education employee (Doloriert and Sambrook, 2009, Doloriert and Sambrook, 2011; Sambrook, Stewart, & Roberts, 2008). This approach has also been used in research on the adoption of technologies in higher education (Kidd, 2010). For these reasons the authors believe that the approach will provide useful insights in the retrospective examination of the implementation process for SRES across two institutions, enabling a thorough reflection on the components feeding into policy generation using the SHEILA framework. Our research questions include:

I.
What contextual and institutional factors may help support the widespread adoption and implementation of new LA tools?

II.
Can the SHEILA framework be used to benchmark and capture unique processes and steps that affect LA adoption and implementation?

III.
Using the SHEILA framework retrospectively, what critical factors can be identified that were necessary to support successful and sustainable LA adoption?

3. Findings: a dialogic analysis of the SHEILA dimensions
To provide insights into the environmental factors impacting the individual practitioners' experiences and perspectives, we will briefly summarise the creation of the tool as well as the contexts of each institution, stepping through the dimensions of the SHEILA framework in a 'dialogic style'. While the narrative style of autoethnography implies the use of the 'I' when recounting the process and events, we opted to maintain a style using the third person, which allows the capture of both the context and the individuals' reactions in context. Each section is tagged with a specific headline providing a signpost to which ROMA process step it refers to (e.g. Step 2. map political context). We use the SHEILA framework, together with analytic autoethnographic reflection, and include dialogue with informants in the form of qualitative interviews, to retrospectively evaluate the institutional environment at two time points: (T1; circa 2013–2015) and current state (T2; circa 2017–2019). The fully articulated SHEILA framework for the two universities at T1 and T2 is included in the Appendix to this paper. Each dimension provides an opportunity to describe the context, identify and critically appraise key issues, and extract salient elements affecting the sustainable adoption of the platform in context.

3.1. Mapping the political context
3.1.1. University of Sydney: driving LA for teachers and students
The University of Sydney is a large research-intensive university in Australia where SRES was first developed and implemented in 2012. SRES stemmed from two initiatives that both served distinct needs for the teachers who developed them. One initiative was a web application for attendance tracking, where teaching teams could use their mobile devices to scan students' ID cards to register attendance. The second initiative was another web application that, at the time, afforded databasing and mail-merge capabilities, providing teachers with a central repository of data uploaded from CSV files connected to a simple mail-merge editor and email engine. Both of these initiatives addressed needs in the large (thousands of students) first-year science cohorts which sparked their development. When these initiatives merged to become SRES, the LA field was still in its infancy and predominantly working with big data at the institutional scale using predictive algorithms and targeting retention problems.

The political context at T1 was predominantly characterised by five main themes – 1) student experience, 2) staff workload, 3) sector trends, 4) technological solutions, and 5) data. The internal drivers were student experience and staff workload, which were often in opposition: increasing cohort sizes diminished teachers' ability to effectively manage and support their students. This was a sector trend related to a policy initiative by the federal government to remove limits on student enrolments. At the same time, another sector trend was to enthusiastically jump at vendor solutions for LA, particularly for the purposes of improving retention in the face of student experience challenges created by massification. Technological solutions from vendors were either non-existent or consisted of basic monitoring dashboards, which were not fit for purpose from many teachers' perspectives. These complexities were coupled with issues around data availability (was key data available electronically?) and governance (who owns data, and who is allowed access?). Together, this meant that teachers did not have the tools to address issues of the student experience at scale.

3.1.2. UNSW Sydney: student experience and personalisation at the centre
The second university is also a large, public, research-intensive, metropolitan university in Australia. A complex political and socio-technical context, characterised by a federated organisational structure, makes the landscape at UNSW rather typical case - but unique in its own way - in HE and worth investigating in more detail. Within the backdrop of a strong institutional strategy (https://www.2025.unsw.edu.au), implementations and developments in the educational technology space are either top-down (i.e. mandated from central bodies for everyone and provisioned over long implementation cycles, with appropriate support structures, but not always catering for everyone's needs) or bottom-up (i.e. initiated and championed by early adopters in the schools, until enough momentum and consensus is reached, to become too visible to ignore or problems to tackle for institutional adoption).

The deployment SRES fit well with the principles of personalisation of the 2025 strategy and therefore, in principle, would include key priorities appealing to all staff. However, the pilot fell into the 'bottom-up' approach in which an individual (in the same vein as the case for the University of Sydney, both a tinkerer and champion) created enough momentum around the platform with the support of teachers in the faculties. Teaching staff involved in the project understood the opportunities of the platform as they all coordinated introductory courses across disciplines sharing two fundamental problems: 1) very large and diverse cohorts (between 800 and 1600 students) requiring substantial logistical time and effort investment; 2) an aspiration to make feedback to students more personal and relevant. The deployment of the platform was only possible via a close collaboration between tinkerers, who were able to leverage their specific knowledge and understanding of the respective contexts to rapidly deploy the platform.

3.1.3. Critical appraisal
Several themes recurred across the two accounts: in both cases, there was a bottom-up approach to development facilitated by tinkerers attempting to solve common problems for teachers of large courses. The initiatives also shared the same spirit and focus: 1) helping the logistics of large courses; 2) personalisation of teacher-student interactions; 3) focus on data users (teachers), and similar care for students (student-centric learning and teaching philosophy). A key distinction at this step was the breadth of purpose – feedback personalisation was the focus at UNSW while the applications at the University of Sydney were much broader and included the whole gamut of the student data lifecycle (capture, curate, analyse, act on, evaluate, reflect; (Arthars et al., 2019). Although there were other differences in the context, governance, and implementation, in both cases the organic growth of the initiatives pointed to a general lack of institutional preparedness or strategic planning and the lack of a specific policy framework driving activities. This is without a doubt a critical reflection which emerged in the evaluation through the SHEILA framework and will be discussed further at a later stage. Interestingly, towards the end of T2 at the University of Sydney, SRES was included in institutional strategy documents as a key enabling technology for enhancing student experience. This was 2 years after development and support of SRES moved from a faculty to being taken up by staff from the central learning and teaching centre, paralleling the approach of analytics 'innovation centres' that scale up faculty projects (Buckingham Shum & McKay, 2018).

3.2. Identify key stakeholders
The primary users of educational data range from management to teachers to students, depending on its form and function. As we consider analytics from a learning perspective, the primary end-users are teachers and their students, with other stakeholder groups (such as IT and learning designers) supporting these primary users. Further, a typical additional stakeholder group are data controllers or stewards (those with responsibility for data access and privacy) even though these had a different level of involvement in the two institutions.

3.2.1. UNSW: improving feedback to students by gaining support and navigating politics
The identification of key stakeholders may seem easy, often achieved by identifying senior managers and gaining top-down support from them. As a reality, once the process is set in motion, the stakeholders involved in obtaining the right data to enhance the feedback process is much broader. These include other central or local support services for both students and staff as well as the staff guiding teachers in the process. In the case at UNSW, since the start of the process, these included data/IT services to enable access to relevant data in a timely manner, data governance roles aiding the definition of governing protocols to access and use the data, and, once data has been obtained, importantly included people with data literacy to effectively use the data and provide support for the appropriate deployment of feedback. This sort of top-down, inclusive approach seems to be in stark contrast with the actual reality: teaching staff were able to see the immediate and potential benefits of the tool.

Although there is ample scope in this dimension of the ROMA process to develop effective and usable policies to guide possible stakeholder engagement, in practice our adoption followed a different pathway: create a small proof of concept with a few teachers interested in feedback and personalisation, creating a strong partnership and the ability to provide full, direct support to test different ways of implementing feedback. A general sense of frustration in dealing with the institutional level and a strong sense of satisfaction in ensuring that the partners had a successful experience with the tool, characterised the deployment of SRES, even if this meant creating workarounds and manual processes (typical of the tinkerer approach). While there was no shortage of potential partners with an interest in the data, the biggest weakness of the approach taken is the lack of a systematic definition of roles and responsibilities in the collection of data and feedback process. This is where policy can provide visible changes and has yet to be developed.

A crucial aspect for the launch of the proof of concept was the internal capacity to run the platform in the central portfolio of the PVC Education. This was especially important to strengthen the partnership with teaching staff: if something went wrong, a direct line was available for teachers and course convenors to deliver practical solutions and achieve what they wanted. Maintaining the right balance between portraying the potential while managing expectations was an essential part of the success of the proof of concept. Interestingly, such 'hands-on' approaches do not scale well when project management approaches are implemented, as stakeholders quickly get detached from the responsive feedback loop characterising partnerships.

Another key stakeholder which must be considered is students: the most important outcome of the use of SRES at UNSW is achieving a better student experience in large cohorts by providing more personalised feedback. This was one of its key accomplishments, which, in one of the courses in which feedback was provided systematically, resulted in a bump in students' end-of-semester evaluations of 10%.

3.2.2. University of Sydney: focus on teachers and their students
Given that at T1 the primary stakeholders were teachers (and through actions arising from LA, their students), the main challenges identified through the SHEILA dimension were around workload, access, and purpose. On top of general workload implications of any new approach, there were additional challenges around fatigue of change and new software systems. A related challenge was ease of access to relevant data and affordances for action, which was very contextual depending on each unit and teacher. Underpinning these were the purposes of LA, from the perspective of these busy practitioners with real students and real units. The role of data stewards in provisioning access was unclear, as were the channels through which teachers could effectively access data for use. Moreover, outstanding questions surrounded the availability of meaningful data.

In the current state, perhaps unsurprisingly, the purpose has become the main driver for primary users. In keeping with one of the historical roots of SRES, the main purposes of teachers' use lie in attendance and participation tracking and personalised messaging. As a primarily face-to-face institution, most classes are in-person and many faculties have attendance guidelines stemming from accreditation requirements through to tacit custom. This had implications for the challenge of access, since such data were traditionally unavailable electronically, very difficult to curate, and often prohibitively time-consuming to act upon (such as informing students of missed classes or identifying at-risk students using these measures). Interestingly, teachers have reported the realignment of their workload due to their use of SRES; instead of labouring over administrative tasks, the efficiencies afforded by SRES have allowed them to focus more on personalisation and student experience. This is captured nicely in the following teacher statement: "I think it reduced my workload for you know something I probably would have done anyway. [...] at the same time I think having the ability to target people based on that [at risk status] probably does encourage you to do a bit more of that [support] work." The teacher-driven nature of SRES, in this instance, seemed to short-circuit the traditional role of the data steward; instead of using institutionally-available data such as demographics, teachers were now the ones capturing and curating relevant data directly from students and their teaching teams, without needing to access institutional datastores nor data scientists. However, the growth of SRES use at the University of Sydney has been accompanied by a shift in stakeholder profile: whilst the predominant stakeholders at T1 were teachers, at T2 this is more evenly distributed between teachers and educational support staff. Notably, these staff are not data scientists, but learning designers, educational technologists, and those with similar pedagogical support roles.

3.2.3. Critical appraisal
The identification of key stakeholders in both cases took the 'easier pathway'. Focusing on the end-users (teachers and their students, and support staff) led to strong partnerships with the developers (teachers themselves) without becoming overly restricted by institutional politics. This had advantages since it was easier to provide value to partner end-users, who could become champions and create a critical mass behind the needs which SRES could resolve. Yet, the tension between the successful partnerships with teachers and problems of sustainability and scalability of the project was palpable, especially in UNSW, where the proof of concept implementation encountered immediate resistance from IT and organisational units governing data access. One key aspect to keep in mind was that students were held at the centre of attention as recipients of LA: this has been used to steer the projects in both universities but was essential at UNSW where the improvement of the student experience was an essential goal of the university strategy.

3.3. Identify desired behaviour changes
3.3.1. UNSW: juggling politics while demonstrating success
The behavioural changes identified at T1 covered different stakeholders at different levels in the organisation. Some action statements were related to the alignment of intents (in practical terms more than at the strategic level) and leveraged on an understanding that adequate support would be provided to inform the provision of feedback. If the focus of the teaching staff is on the logistics of a large course, the intent may be limited to the smooth running of a course rather than focusing on the improvement of outcomes. Interestingly, logistics and outcomes are not exclusive or incompatible, but the focus of the teaching staff may prevent conversations focusing on the specific outcomes of the platform.

3.3.2. University of Sydney: shifting from data to action
SRES implementation led to two key outcomes: a positive influence on student experience, and a more organised data-informed approach to teaching. A key challenge for both of these actions was whether teachers were capable of connecting the right data to the right action to enhance student learning and experience. This involved not only an understanding of data but also, crucially, of pedagogy and student support. Related challenges included the support structures available to both assist teachers with this, and also as referral endpoints. In this context, SHEILA also identified policy challenges around the effects of LA on students, as well as issues around transparency and privacy.

A key benefit of the teacher-driven approach has been that data collected and actions thus generated have been highly contextualised to the needs of individual units, teachers, and students. However, the localised nature of this has led to two main drawbacks. The first was the increased need for locally provisioned support for students (triggered as a result of personalised interventions, for instance) and for teachers (in technical support for using SRES). For example, teachers have identified that as a result of using SRES to collect and analyse data, they have become more aware of, and felt more compelled to help, students who may need support. As one teacher reflected, "You can't just leave them [students] and say you're at risk". The second involves unintended consequences of the tone and/or content of teachers' personalised messages to students, which may (as it did in one instance) be interpreted by students from an overly negative and potentially damaging perspective. This has led to reflection by teaching staff on the need to consider the wording of their communications with students and how they may be interpreted. As one teacher said when reflecting on how she constructed emails to students: "I don't go out intending to be particularly scary. Apparently, it just comes across."

3.3.3. Critical appraisal
There are two aspects which emerged from the examination of this step: 1) the importance of the longitudinal evaluation of the implementation, and 2) a question about the drivers for change. In both cases, the articulation of challenges and actions within the SHEILA framework led to reflection about several questions. Who needs to change, and at what level of the organisation? What level and type of support is required to make adoption easier? What knowledge and expertise are required to make feedback effective?

In both universities, the broadening of the use of the SRES in T2 led to increased data use and capacity in teachers, with positive effects on the student experience. A sophistication of the understanding of data led to question the accuracy of the data (i.e. the methods for collecting and measuring students' data). Further, teachers' ability within the platform to obtain immediate feedback from students about whether personalised messages were helpful and whether messages were opened enabled teachers to move to deeper questions about the learning process. Feedback from students also provided insight into what they found useful but also opened up questions about the effectiveness of teachers in creating 'nudges' for students which could be more useful.

3.4. Develop engagement strategy
3.4.1. University of Sydney: leveraging success and refine the tool
The original scope of SRES was to provide immediate support for the developers' first-year classes and that of colleagues. Engagement and dissemination external to this group was not planned. This presents an interesting departure from the SHEILA steps, in that the organic growth of SRES missed this key consideration. However, a key tacit engagement strategy during T1 involved co-design with teachers to add needed functionality to SRES. Other organic engagement strategies during T1 involved small-scale user training. The main concerns about adoption were around measuring and demonstrating impact and progressing primary users along the adoption pipeline from being interested, to trialling, to becoming local champions (Colvin et al., 2016).

It was not until the start of T2 that a more systematic engagement strategy was formed, largely based on the lessons from T1. During T1, engagement rested on colleagues sharing the value-in-use of SRES to others and, in many cases, training others in using the platform. The T2 engagement strategies were developed around three main themes - co-design, successful cases and best practice (champions), and training. These were not dissimilar to the tacit T1 engagement strategies, just more deliberate and systematic.

The rapid adoption of SRES accompanied by a diversification of application contexts and increase in the pool of use cases allowed the capture and incorporation of many design ideas in a co-design process with primary users. Local champions continued to play a key role in organically disseminating SRES with colleagues; whilst originally the champions were mainly faculty, this has expanded to support staff, primarily learning designers. The importance of having champions who could speak to the benefits of SRES and provide support is highlighted in the following statement from a learning designer: "Oh they [teaching staff] definitely wouldn't have picked it up at all if we'd just said 'here's a nice tool, it'll do this stuff for you, here's a tutorial, a user guide'". Training needs have also increased and have transitioned towards a 'train the trainer' approach. As with previous ROMA steps, the teacher-driven approach has operated largely in a policy vacuum in terms of LA-specific policies, instead being subject to standard institutional learning and teaching policies and strategies.

3.4.2. UNSW: a balancing act to pave the way for policy
At UNSW, engagement was clearly seen as highly dependent on the target stakeholders. In particular it was clearly identified that there were well defined activities which were not the priority when adoption followed an organic growth. For example, when SRES was deployed and piloted, end-users were at the centre (i.e. teachers and students) and technical stakeholders, like IT departments or data units, were only involved when required. There are several reasons why this was an easier pathway: often central units may introduce requirements or problems which hinder the process creating process-dependent blockages. On the other side, in an ideal world, widening stakeholder involvement and transparency were considered more conducive to sustainable adoption. Therefore, striking the balance in managing stakeholders' involvement was essential. While there was an intention to adopt principles of co-design and partnership, this would have been subject to longer timelines which was not available when the pilot started. For this reason, a very controlled proof of concept was initiated, involving only primary end-users and helped to specify and evaluate the requirements with rapid iterative cycles from the minimum viable product. In order to achieve this, a strong partnership model with direct support lines for staff enabled the platform to launch in a few courses with medium to large number of students.

This approach enabled quick wins, demonstrating the value of the platform in supporting the logistic of large courses, but also identifying several areas requiring attention and support. For example, while data generated in the learning and teaching process became easily accessible (i.e. attendance, engagement, and participation in class or performance from external sources like quizzes), automatic data feeds were not available at the outset. This meant that something relatively simple like having an automatically updated list of enrolled students was a challenge. Also, several questions emerged about the approach to feedback (i.e. frequency and content) from academics and the infrastructure to support this was very limited. For example, the feedback to students was either rather generic and based on performance, directing them to the appropriate services without much more, or specific and based on their own performance compared to their peers. However, little consideration was given to the potential differential effects of such comparison. In all the examples provided, the existence of specific guidelines or procedure would have allowed a more coordinated intervention enabled by the personalisation of feedback.

3.4.3. Critical appraisal
While the development of an explicit engagement strategy is one of the key elements for the development of an appropriate policy framework, this did not seem to be a priority for either of the two cases. This raises interesting questions about who is responsible for setting the priorities: in both cases we had tinkerers who seized opportunities to leverage perceived needs and provide an appealing solution. In this sense, Scott argued that "[e]ducational changes do not unfold spontaneously – they have to be led. Depending on the nature and scope of the change, different people may take on the role of change leader." (Scott, 2000, p. 187). A key aspect is about the role of the tinkerer as a 'lone wolf' or as an agent for change. Obviously, the perception from other parts of the organisation is essential, but the way the perception is shaped is mediated by the context. Only a structured approach to understand the complexity is conducive to a sustainable solution. Some of the key questions may include: why and when is tinkering blocked? What are the processes to gain appropriate support? Who is deciding what is suitable to scale? Again, looking at the educational literature on change in higher education, Bamber and colleagues (Bamber, Trowler, & Saunders, 2009) pointed out that change looks different at different levels and there are gaps between what is imagined and planned and what gets to practical implementation, especially because there are different perceptions of what change entails in those involved in the process. The fact that at the University of Sydney, T2 saw a faster and wider uptake of SRES compared to T1, perhaps validates the importance of SHEILA's systematic approach and the deliberate articulation of a strategic plan.

3.5. Analyse internal capacity to effect change
3.5.1. University of Sydney: scaling capability of the system
Three overarching themes emerged from the reflection on the process: people, data, and the interconnecting technology. For people, the predominant factors for action were motivation for innovation and a lack of culture and capacity for working with data. The SHEILA framework allowed the articulation of related challenges around change fatigue, lack of access to relevant training, a dominating research focus which saw education as a distant secondary priority, and the difficulty in reaching institutional management. For what concerns data, the challenges lay in the siloed or unavailable/uncaptured nature of data that teachers, as primary users, were interested in. The related theme of interconnecting technologyhighlighted the lack of interoperability between data silos, and concerns around maintenance and sustainability of LA solutions.

At T2, SRES is now seeing wide institutional buy-in including from senior management. Part of this is due to the groundswell of uptake from teachers themselves (over 1700 teachers use it), making SRES too widespread to ignore. This bottom-up approach taken by the SRES initiative at the University of Sydney is a relatively rare example of successful upscaling; had high-level buy-in been required from the outset, the initiative may not have taken root. Despite being an unfamiliar platform, teachers have tended to be sufficiently motivated to learn, persist with, and eventually innovate with SRES. This appears to be related to SRES addressing specific felt needs, per the third ROMA step, 'identifying key internal drivers'. The teacher-driven nature of SRES has allowed teachers to each take a different approach to data: instead of needing to adapt their practices and develop data literacies to suit pre-existing data and analytics surfaced from institutional data warehouses, SRES affords teachers the ability to create and use their own databases with data meaningful to them.

Serendipitously, one of the major challenges of interconnecting technology was solved through the institution's move to the Canvas LMS close to the beginning of T2. The relatively open and, importantly, user-driven application programming interface (API) capabilities of Canvas allowed SRES to quickly tap into key data that teachers needed, such as enrolment, grades, progress, and assessments which at T1 was primarily a manual process. The ability for teachers themselves to provision access to API/data from Canvas made a large impact: for early adopters it reduced administrative burdens and opened up new forms of data, and for later adopters it reduced the barriers to adoption around manual imports and data currency.

Finally, the maintenance and sustainability of SRES have not been without its challenges. One foundational approach has involved participatory design with teachers, which enabled continuous improvement and development of the platform. Further, institutional support from IT was limited: despite this raising obvious sustainability and single-point-of-failure challenges, a significant benefit of this independence has been the ability to have rapid feature improvement turnarounds when users requested features.

3.5.2. UNSW: enabling teachers to do their jobs better
At UNSW the main theme emerging upon reflection was the focus on preparedness of individuals as part of the organisation rather than the organisation itself. Hence the necessity of ensuring that individuals have the necessary data literacy to actually do something with the data was as important as the ability to formulate effective feedback.

However, this implied a systematic approach to staff development which did not exist in these areas and would require a considerable alignment of resources. For example, having the establishment of policy supporting education-focused careers could have steered the institutional work, but incentives would need to have been provided to support professional development to ensure sustainable adoption and use of the platform in day-to-day activities. Given the culture of the organisation, it is possible that only by mandating use might adoption at scale be granted. This is an oxymoron to the basic philosophy of SRES which encourages teachers to leverage data to improve their practice and students' experience and outcomes.

This time-based evaluation raised important questions about ways to develop capacity as well as what would be essential to scale SRES adoption in a sustainable manner. As well as some typical issues related to technology adoption and change management, the clear implication of the analysis was the need to establish an appropriate working group to effectively respond to the issues emerging and provide guidance/support to the stakeholders involved, especially leading the way in the establishment and monitoring for the implementation.

3.5.3. Critical appraisal
The considerations about internal capacity to effect change are critically connected to the earlier discussion about strategy development. On one hand we have the drive to improve the system along with people's ability to use features in the system. These two go hand-in-hand: if the system is too complex, technology acceptance models tells us that the system will not gain traction (Petter, DeLone, & McLean, 2008), so as the perception of the users in their efficacy in using the system. The responsive implementation model has been tailored to shepherd users through the journey in the hope that new users' learning curves would be more attainable. However, the literature on change taught us that it is important to empower individuals (or communities) in effecting change (Fullan, 2005) and therefore it was important to encourage and celebrate good practice and connect stakeholders under the same strategic intents (Marshall, 2010). While the system improvement cycle was deployed as intended, in order to support the latter a broad support is required (including management structure and governance) as the success of champions can only go to a certain length (i.e. focusing on personal gains) before it starts to become less effective. As we will examine in the next section, success means different things to different people, and this must be considered in the policy framework.

3.6. Establish monitoring and learning frameworks
The SHEILA framework articulates the need to define both qualitative and quantitative success measures. This implies wide engagement with stakeholders to identify and measure success which often creates tensions dictated by priorities.

3.6.1. UNSW: measuring learning vs measuring success
At UNSW, two key dependencies drove reflection: the success for individuals taking part in the project and the organisational support and alignment of the implementation with the university goals. Interestingly, this SHEILA dimension provided an excellent opportunity to characterise the 'desired state': in particular, the articulation and discussion of evaluation metrics, indicators of success, and stages of completion seemed to be a perfect starting point, but also a relatively simple approach to enlist the parameters which a LA policy at UNSW should actually focus on. For individuals involved, the focus seemed to be ultimately around the improvement of student experience and outcome. The tool, or the analytics informing the customisation of feedback, is just a means to an end. At the institutional level other issues emerged about the scalability and sustainability of the platform, focussing on related requirements to ensure that success in a single course could be extended across an entire curriculum. Further, the effectiveness of the tool must be set against the broader ecosystem of tools which are made available: this meant that appropriate procurement processes needed to come into play to determine how well the tool could scale, interconnect with other offerings, and complement their functionality. For example, how does SRES differ (in both features and functionalities) with a customer relationship management tool which is deployed in parallel at institutional level? How does it integrate with the existing LMS and, if it does not, what are the pathways available? These are just some of the questions driven by the consideration of outcomes.

3.6.2. University of Sydney: what counts for learning and who drives developments?
Two major themes were identified through SHEILA: 1) measuring and sharing impact, 2) enabling and leveraging co-creation.

Challenges for the former are apparent sector- and field-wide and are related to the noisiness in educational data and the difficulties in isolating the effects of particular innovations, especially when teachers working in live courses can and often make simultaneous changes which have a cascading effect on the success of any implementation. The difficulty around defining success measures has remained an issue at T2. A part of the problem lay in the nature of SRES as a flexible platform - there were a multitude of ways that teachers could (and did) use the platform, which compounded the difficulty of defining and measuring success and impact as these were inherently context-dependent. While one teacher may use it to deliver rich and tailored feedback based on a range of data curated in SRES, another may use it to merely provide course announcements addressed to individual students' names. In just these two simple examples, the success measures would differ greatly: the former possibly examining changes in engagement and assessment performance, and the latter possibly examining views or click-throughs. Keeping in mind the complexity of success measures, teachers using SRES were increasingly reporting improvements in the areas behind the key intrinsic motivators identified in ROMA step 1, that is, student experience and learning. They were sharing these with colleagues, which contributed to the wider adoption of SRES (Liu et al., 2017; Vigentini et al., 2017).

The importance placed on user engagement and co-design implicitly created challenges, especially in the level of engagement and related management of expectations. While certain users were very active and heavily contributed to the definition of an implicit list of priorities, others struggled to keep up with some of the changes, not always understanding the reasons behind them. As adoption spread, co-creation has become an even more important part of the SRES initiative in providing primary users ownership and control over its design and implementation (Dollinger et al., 2019).

3.6.3. Critical appraisal
While the identification of key success metrics was relatively straightforward, it was the articulation of the relevance of these metrics to various stakeholders that made the reflection on this step particularly informative. In this sense, the key issue of whether measuring student learning or measuring the success of the implementation is a very important one to consider for sustainability and scalability. On one hand the ultimate goal of tracking students is to provide them with appropriate support leading to improved outcomes and experience. Yet, in the process of doing this, other issues emerged which spoke to the effectiveness of the mode of teaching and supporting students. The value of attendance as a proxy of engagement, set against the actual performance metrics is a reasonable starting point to determine what to say to students falling above or below certain thresholds. But who sets these thresholds and how? And what support mechanisms are offered to those who do not reach these thresholds?

Another consideration related to success was the ability to place the new platform within the existing ecosystem (of tools, cultures, and practices) which enabled alignment with strategic priorities. In both cases, maintaining the focus on end-users meant a very responsive approach to iterative developments. However, the consideration of where the platform fits in the organisation has become apparent at T2 in the University of Sydney, in which the inclusion of the tool as a cog in the strategic implementation of analytics made SRES a critical component of the ecosystem, therefore a full set of new criteria had to be satisfied in order to take that place. At UNSW, the same issue went to provide the foundation of the development of a completely separate tool, focussed on personalised messaging, bringing to the fore the requirements of UNSW's ecosystem in order to facilitate the implementation, and leveraging faculty champions to contribute directly in the development in order to push forward. The only missing piece was a coordinated policy framework, but this is where attention is currently focusing on.

4. Discussion
In this paper we have taken the six steps within the ROMA process (part of the SHEILA framework) to analyse, from an individual level, the adoption and implementation of a specific LA platform, SRES, across two universities. In the review by Dawson and colleagues, they identified three broad types of adoption models: 1) input models, in which a set of antecedents or affordances underpin the adoption; 2) output models, in which a state of maturity is reached through a progression driving developments over time; and 3) process models, in which a sequence or the achievement of certain steps/states are required in order to achieve adoption (Dawson et al., 2018). Further, they proposed that complexity leadership theory enabled them to capture the complex interactions between environmental pressures and internal forces (enabling leadership, adaptive functions, and administrative functions) leading them to classify the existing example of literature in top-down vs bottom-up approaches. From the review of the two cases presented here, both can be considered bottom-up, with enterprising tinkerers innovating and diffusing an LA platform to scale at their institutions. The explicit adoption of the interpretational lens of a process model (SHEILA) and the autoethnographic approach presented a unique opportunity to better understand the key components (or reasons) why the platform was successfully scaled in the University of Sydney, but did not, in its previous form, scale in UNSW.

Our findings highlighted significant points of interest across the dimensions of the framework and the research questions, which span over three integrated themes: 1) focus on the primary end-users' needs; 2) resourcing and context; and 3) understanding the broader value of LA tools. These themes will be discussed below, generalising the individual narratives against the broader LA literature, while considering three questions:

1)
if LA is as good as its potential is described, why is it so hard to find sustainable implementations?

2)
in a similar vein to the consideration of the change in response to technology (Marshall, 2010), should HEIs change in response to the perceived advantages of LA?

3)
What did we learn from the two cases to empower tinkerers to become change leaders in sustainable LA implementations?

4.1. Focus on targeting teachers' needs and advocating for co-creation
As evidenced through our analysis of the political context of LA, the initial hype and excitement surrounded LA practices led to the proliferation of tools without a deep understanding of how these tools matched the needs of teachers. In worst case scenarios the decision to use a specific tool may have been directed top-down from management. Senior academic managers often have little or no teaching in their workload and therefore can become out of touch from the grassroot needs of an ever-changing student cohort. In the best cases, the decision comes from teachers, but they may lack nuanced understandings about the tools they are picking or perhaps are overly optimistic about their own capability or time needed to implement the tool. This has led researchers to lament the "disconnect" between LA tools and what teachers need (Corrin, Kennedy, & Mulder, 2013, p. 204) and, recently, state that "[w]hile many tools have been created, aiming to benefit academics and students… few appear to have explored or report on what academics actually want" (West et al., 2018, p. 125).

This has led others to propose models such as co-creation to improve the applicability and therefore uptake of LA: "if LA was co-created with users [this] may align more closely to the needs and expectations of users, thus supporting greater adoption" (Dollinger & Lodge, 2018, p. 98). A key feature of this is shared goals; indeed, if decision-making for LA included a variety of stakeholders from software designers to senior management to (particularly) coordinators and casual tutors collaborating (i.e. sharing knowledge and goals) towards selecting useful tools and designing implementation strategies, the needs of primary users may be better addressed and, by extension, adoption enhanced. This fits with Rogers' theory of diffusion of innovation that indicates that users' perception of 'relative advantage' of an innovation over existing practices, which may include economic, social, satisfaction, and convenience improvements, influences its rate of adoption (Rogers, 2003).

This may help to partly explain the differential adoption success between the University of Sydney and UNSW. The wide functionality of SRES was available to teachers at both institutions but was limited, at least in the pilot, at UNSW, to providing templated feedback based on uploaded data. In comparison, teachers at the University of Sydney were free to utilise any features that suited their context, with many opting to use SRES for flexible in-situ data capture and subsequent message- and web-based personalisation. It may be that this allowed the platform to address a greater range of teachers' needs and expectations and provide the necessary relative advantage over diverse current practices. Further, effective co-creation hinges on 'value-in-use'; that is, end users creating added value through the application of a tool over and above its mere functionality (Payne, Storbacka, & Frow, 2008). It could be argued that through extended exposure to a wider set of features allowed primary users at the University of Sydney the chance to draw more benefits from SRES. Additionally, the proximity of the main developer with partner users at the University of Sydney allowed the latter more direct access to provide input as part of the participatory design process, potentially leading to increased buy-in and acceptance (Bergvall-Kåreborn & Ståhlbrost, 2008).

Of course, an individual's progression from knowledge to interest to adoption of an innovation is not a unary process. In addition to the need for innovations to address felt needs and provide a relative advantage, the community and connections that exist around an innovation serve to provide knowledge about, demonstration of, and persuasion for its application (Rogers, 2003). While teachers are often the main point of focus when exploring the adoption of LA, adoption is a multifaceted activity involving many diverse stakeholders as part of these communities. Learning designers, IT workers, and students are all needed to also collaborate to further assist the primary users (i.e. teachers) of LA platforms and tools. Support networks therefore need to extend to include these stakeholders not only to share technical knowledge, but also perspectives that can foster goodwill and motivation to participate in this space. In some 'successful' cases of LA, we have seen situations where academics are resourced with dedicated professional staff to assist with LA implementation and usage. However, this solution is temporary and does not create sustainable practice, but rather dependency. Thus, one long-term solution towards integrated teachers needs is to foster reciprocal dialogue between all stakeholders. Herodotou et al. (2019) emphasized the important role of managers in softening resistance to change; this is in line with previous work focusing on the positioning in the social network to promote diffusion of innovation (Jippes et al., 2013; Moolenaar, Daly, & Sleegers, 2010), but the action of managers can only be successful within a strategic implementation program which is likely to be driven top-down (Herodotou et al., 2019). This seems to be reflected in the cases presented, and in particular the impetus of the case in the University of Sydney, which successfully evolved to a full institutional program.

4.2. Reflecting on resourcing and the building blocks of successful LA initiatives
The reflection about what allowed for a successful and sustainable LA initiative was grounded in the ability to provision appropriate levels of support and adaptivity to sustain uptake. Balancing the customisation of the platform with users' capabilities, the appropriate data literacy support with the individuals' drive and motivation to help their students, as well as the ability to gain appropriate organisational support to let the platform thrive, are all important aspects in the success of LA initiatives. Interestingly, strategic leadership without grass-roots support does not seem to be a feature in the initial phase. The fact that LA is frequently evoked in higher education (the external pressure in Dawson and colleagues' complexity model) as an inevitable future does not warrant automatic adoption. From our analyses of stakeholder perspectives, even with the stakeholders who are the most engaged (those that use software platforms regularly in their teaching practices), there needs to be more reflection on the purpose and aims of LA. For example, teachers partnering in SRES adoption in both institutions reached a higher level of sophistication over time and started to question whether using the platform could help support student success (broader scope – macro-level effect). However, the majority simply focused to personalise support or visualizations that allowed students to regulate their own learning progress (meso-level at a course or program level, shifting to the micro-level when focusing on the individual student). A few teachers began questioning their own data literacy, and this speaks volumes about the set of capabilities which a modern teacher in higher education must possess in order to foster effective student learning. There are no wrong or right answers in this space, but often in practice we see all the various aims of LA thrown into a single framework, policy, or intervention. Perhaps the adoption and implementation of LA suffers because we are trying to do too much.

The SHEILA framework also recognizes the need to develop an engagement strategy. This propagates not only the need to support reflection on the aims/purposes of LA, but also to think about the resourcing of LA. While management in HEIs are often interested in wide adoption, it is likely that first LA needs to be supported through organic growth of tools and platforms. This means allowing those involved in LA time: time to think, time to train, to discuss, and eventually begin to integrate platforms into their practices. Time is a precious commodity in the typical academic workload; therefore, if management is serious about ensuring the adoption and implementation of LA tools, they need to first allot time for staff to learn and develop approaches.

4.3. Contextualising researchers' projected value of LA for stakeholders
Comparing the uptake and sustained usage of SRES from the University of Sydney and UNSW, it was clear that the bottom-up approach at the former seems to have generated greater institutional buy-in required from the outset to maintain wide and impactful adoption. To us, this signifies not that every institution needs to develop their own tool, but rather there is not enough attention placed on how to translate the value of LA to stakeholders. The eight recommendations proposed by Herodotou et al. (2019) are particularly relevant in evaluating the lack of progress in UNSW. The 'allocation of time for stakeholders' and the 'allocation of manager time to make the initiative a priority' are obvious weaknesses in the process.

The translation of value requires analysis of not only the institutional culture and ensuring teachers feel supported, but also an audit of current technical constraints. As we could see from our own experiences, if the University of Sydney did not have a change of the LMS, there may not have been an opportunity to speed up the uptake of SRES, and possibly, it would have faltered as in UNSW. While often not the focus of research papers, technical glitches and software bugs do have the ability to thwart uptake and progress. Again, this stresses the need to take a co-creation lens to the adoption of LA, where technical experts, research, teachers, and other stakeholders can share in equal decision-making. However, there is more behind this than a lucky opportunity realised at the University of Sydney: as testified in the SHEILA articulation, at T2 several more elements aligned to enable SRES to be in the right place and the right time with a critical mass of primary users and advocates which made it very hard to ignore. At UNSW, such an alignment has not been reached yet, but the strenuous work of the tinkerer and partners of SRES implementation project created the environment for a stepwise change. This was leveraged with another tool with a subset of functions to SRES, but which was owned and developed by UNSW and was therefore tightly integrated with UNSW's existing ecosystem. This brought the ability to drive the principles behind SRES forward and re-brand the project behind the strategic implementation of the personalisation of feedback. An interesting question is whether an integrated strategy with a single tool, instead of diverting resources towards another one, would have brought more impetus to the adoption of SRES.

Looking back at the implementation process, it was apparent that the principles behind co-creation, partnership models, and close support (characterised by responsive development) were all essential to build the initial successes. And yet, co-creation did not come without its challenges. As we noted in our own experience, SRES is a flexible platform, in contrast to many other LA tools. It is, ironically, the same flexibility towards user preferences which can also make collating training mechanisms or evaluations difficult. As mentioned previously, this may allude to a 'too much and all at once' scenario where there is not enough underlying training and discussion to support stakeholders before the introduction of flexibility. This is a fine line between providing solutions that meet the diverse needs of individual teachers and (over)simplifying tools that make dialogue and evaluation easy. It also makes it difficult to provide the kind of systematic evaluations of effectiveness desirable for senior academic managers. The impressive achievements in a single course may alert academic managers of the potential, but this has to be diluted in the existing ecosystem as well as idiosyncrasies of the organisation.

5. Conclusion: supporting tinkerers to drive sustainable adoption
In this paper we led the reader through the systematic exploration of two case studies, looking at institutional adoption of a LA platform through the lens of analytic autoethnography and the scaffold of a process model of LA adoption (SHEILA framework). One of the two was successful and it is reaching a maturity phase in which the platform has become central to the implementation of whole-university strategy. In the second case, the adoption for this platform stalled, but the work continues leveraging the successes and lesson learned, and it is now morphing into an initiative more broadly aligned with the university's specific processes.

The deeper study of the process and longitudinal reflection provided a unique opportunity to better understand the elements leading to success of the University of Sydney, and the partial success at UNSW. Although there is evidence in the literature suggesting that hybrid approaches are important, it seems difficult to find published evidence at the institutional level. Most cases available are at a course level, and do not focus enough on the broader contextual elements unpacking the complexity hinted at by Dawson and colleagues (2018).

Despite the initial belief that the missing component was a specific policy (or strategic planning) which should drive initiatives beyond the initial push from tinkerers, we explicitly noted that the success of the bottom-up approach is what generally created a critical mass necessary to support sustainable changes, which in turn helped to shape policy in the University of Sydney. Yet, the major hurdle to shift from a proof of concept to an institutional tool seemed to be strongly tied to the ability to manage up, and gain support from senior academic leaders to sustain the resourcing and scaling of the potential demonstrated by the partners and champions. As demonstrated in the case of UNSW, even with the support of several senior academic figures, the issue of institutional preparedness and the pragmatic technology challenges specific to the context remain to be resolved. The cases provide an insightful discussion about the differences in the change management and the knowledge and attitudes of the stakeholders involved, particularly when engaging middle and senior managers in the process can have for long-term, strategic deployment and diffusion of LA tools.

In respect to the research questions posed, the paper furthers work in the field in two major areas. Firstly, it demonstrates the use of a novel methodology to systematically research the adoption of LA tools in HEI, analysing both the stories of the actors involved a well as reflecting on the socio-technical context and the policy frameworks required to lead to a successful implementation. Secondly, it provides an opportunity to extend the use of the SHEILA framework. We discussed at length the contextual and institutional factors leading to the adoption of LA tools. Implicitly, this demonstrates the effectiveness of the SHEILA framework not only in driving the creation policy, but also as a useful scaffold for evaluating and reflecting on the processes, issues and the socio-technical context which enabled the authors to revisit the institutional stories and discuss their process of adoption. Finally, the framework facilitated the identification of the critical factors differentiating the two implementations and surface the drivers for the success and failure in achieving widespread adoption in the two cases.

Appendix 1. The University of Sydney compiled SHEILA Framework

ROMA step	SHEILA component	University of Sydney at T1	University of Sydney at T2
Map the political context	Action	
•
2016–2020 University Strategy highlights the richer use of LA to provide feedback to staff and students on engagement, understanding, and skill, and drive adjustments to processes for increased student learning.

•
Sector trend to invest in LA.

•
Students feeling disconnected, especially in larger classes. A need/desire to provide a higher level of support to/interaction with students.

•
Strategy ongoing, additional supporting Student Experience Strategy to focus on relational and other aspects of student experience. SRES is a key enabler in the new strategy to enable personalisation of learning and improved connections.

•
Sector trend is still apparent but seems appetite has declined, perhaps related to lack of demonstrable impact.

•
Class sizes still increasing, need is still there.

Challenge	
•
Existing platforms either did not afford any LA capability or was not actionable.

•
Teachers have heavy workloads, are time-starved, and many are intolerant of failure.

•
Existing solutions in the market mainly focus on addressing retention problems.

•
A number of departments within the university vye for control, access, and use of data.

•
Prevailing approaches to analytics are post-hoc and/or based solely on demographic factors.

•
Data exists in various silos, or completely offline.

•
Needs and pressures of teachers in different contexts can be very diverse.

•
SRES platform exists now to afford insight and action. Some other LA tools exist but are information-only.

•
Workloads unchanged or heavier. Failure tolerance varies among innovators and middle.

•
Vendor solutions feature less.

•
A number of departments still vying for control/access/use of data.

•
Shift away from purely demographic analyses although some departments are still focussed on this.

•
Data still in silos but increasingly being warehoused in some form (either EDW or SRES).

•
Teacher needs are still at the centre; some don't use LA/SRES because of smaller classes, although some small-class teachers still find value. Varies.

Policy	
•
What is the governance around data access and usage?

•
Which problems are to be addressed by using LA?

•
What is the institutional approach to procuring or developing LA tools/platforms?

•
What are other policies (institution, faculty, school levels) that may interact with the purposes, operation, and outputs of LA?

•
New LA guidelines from 2016. Learning Analytics Advisory Board implemented.

•
No policy around problems to be addressed; left in the hands of teachers.

•
Institutional approach seems more accepting of bespoke internal development and not just purchasing vendor solutions.

•
A number of policies (and enshrined practices) influence the adoption of SRES, such as attendance.

Identify key stakeholders	Action	
•
Identify primary users of learning analytics (e.g., students, teaching staff, and senior managers).

•
Identify external partners (e.g., vendors)

•
Identify current owners and stewards of data silos.

•
Primary users as teachers and managers, impacting students.

•
External partners no longer a force. Internal partners/departments feature more.

•
Owners and stewards are clearer.

Challenge	
•
Define ownership and responsibilities among diverse professional groups within the university.

•
Time and workload issues for teachers who are the eventual implementers of LA.

•
How can data users practicably access relevant data in a timely fashion?

•
Struggle for power/control by key stakeholders.

•
Mixed understandings of the needs and purposes around LA by different stakeholders.

•
Differentiating the ethical responsibilities around data for research vs data for QA/QE.

•
Owners and those responsible are clearer. Teachers at the centre of local data.

•
Workload still an issue but SRES has shifted the use of time.

•
Timely access to data is afforded through SRES.

•
Power struggle still exists but more relevant data now exists in SRES.

•
Mixed understandings still exist. Teachers are the key stakeholders and are increasingly seen as this.

•
Clarifications surrounding use of data for teaching vs research.

Policy	
•
Who can access data?

•
Who owns data?

•
Who is the data controller?

•
Will learning analytics exclude certain groups of students? Will there be mechanisms to address inequality? Will it be of benefit to students?

•
What are the workload implications and provisions of using and adopting LA?

•
What kinds of technical, pedagogical, data supports will be available?

•
Teachers ‘can’ access but actually can't for siloed data because infrastructure does not exist still.

•
Ownership seems to lie with data stewards.

•
Controllers are data stewards/heads of portfolios.

•
‘Supporting student learning experience and outcomes’ is a key guideline in use of LA. Silence on exclusion and inequality.

•
Workload implications unclear; SRES has caused a repurposing of workload.

•
Technical and pedagogical supports seem to be most important. Less focus on data and analytics support - less need?

Identify desired behaviour changes	Action	Mind inadvertent consequences and make sure the benefits of learning analytics to students outweigh risks.
Increased connectedness between teachers and students, and reduced isolation especially in large cohorts.
Less ad-hoc management and access to data.
Actions to help students that are more data-informed and personalised.
Better data-driven approach to understanding student performance, engagement, and skills.	
•
Some inadvertent consequences observed. Mostly beneficial.

•
Connectedness seems to have increased.

•
Data management is better in pockets of the university through SRES. Data access improved,

•
Increased personalisation.

•
Data-driven approach to understanding is relatively unchanged - a product of the nature of SRES not specifically addressing this?

Challenge	
•
Immature skills of interpreting data lead to wrong decisions.

•
Workload implications of providing downstream support as a result of LA interventions.

•
Availability of people to support implementers in using LA.

•
Reliance on teachers to interpret relatively simple but relevant data.

•
Unclear the impact on pressures on downstream support.

•
Increasing community of academics, designers, and support staff fluent in SRES.

Policy	
•
How will transparency be achieved throughout a project cycle (data collection, analysis, and usage)?

•
How will privacy of students be protected especially in terms of more sensitive information?

•
Are there policies, restrictions, guidance, etc. around the forms of teacher actions that may arise from LA?

•
Who will benefit from learning analytics?

•
Data is completely transparent to teachers in SRES.

•
Privacy limited by authorised use usually to teachers and teaching team.

•
Lack of policies around teacher actions arising from LA. Still almost exclusively teacher-driven and determined.

•
Beneficiaries still students and teachers, mainly.

Develop an engagement strategy	Action	
•
Demonstrate value or impact of LA pilots.

•
Align learning analytics with the wider institutional strategies or introduce learning analytics into the university's strategy.

•
Raise awareness and understanding of learning analytics among teaching staff and students.

•
Provide training for users (e.g., how to operate the tools, how to interpret data, how to transfer data into action).

•
Nurture local faculty champions to promote LA to colleagues via word-of-mouth.

•
Invite teaching staff to contribute their professional knowledge to the design and implementation of learning analytics.

•
Engage with existing LA cases and literature.

•
Consider providing a safe environment (e.g., a sandbox) for testing or research purposes.

•
Consider establishing an ethics committee.

•
Targeting the right stakeholders - who might accept and trial LA, who might push for its use, etc.

•
Value/impact still hard to measure although many anecdotes are available.

•
Inclusion of SRES and personalisation in new Student Experience Strategy.

•
Local champions play a key role in raising awareness.

•
Local champions and central team play a key role in training.

•
Champions actively disseminate through word of mouth with colleagues.

•
Teaching and other staff are regularly invtied to contribute and share ideas.

•
Existing LA cases are dissimilar.

•
Implementers are encouraged to test either through themselves or local designers.

•
Learning Analytics Advisory Board exists, created from 2016, but does not directly speak into educational matters.

•
Teachers/coordinators seem to be the stakeholders with the best ability to connect real problems and needs with potential solutions. Support staff are critical in providing extra comfort and support to smooth adoption.

Challenge	
•
Learning analytics removing student agency from them by drawing attention away from their’ own responsibility for learning.

•
Interventions or actions may have unintended effects on students depending on how they are designed, delivered.

•
Sufficient support for early adopters to become champions without significantly adding to their workload.

•
Impacts or successes of LA may be difficult to measure or prove.

•
Learning analytics may induce fear and discomfort about surveillance.

•
Potential for LA tools to be used by cooridnators to manage/surveil staff in addition to supporting students.

•
Removal of student agency still a concern.

•
Interventions/actions are teacher-dependent so may have unintended effects.

•
Workload is an issue for champions especially if they are training others. This has tended to fall to local or central designers.

•
Impacts/successes still difficult to measure, especially because multiple innovations tend to be implemented simultaneously.

•
Surveillance of students has not been seen as such a big issue - possibly because of the purpose/outputs of SRES.

•
Surveillance of staff has not been an application of SRES specifically.

Policy	
•
What are the objectives for learning analytics? How do they align with the institution's vision for education?

•
How much flexibility and scope will teachers have to design and deliver their own interventions and actions?

•
How will the results of analytics be interpreted within the context? What kinds of expertise needs to be involved in this process? Does it include teaching staff and students?

•
Will learning analytics be used as a deficit model targeted at supporting students at risk of failure?

•
Student experience is a renewed focus. LA aligning with this.

•
Teachers largely have free reign.

•
LA results and outputs largely determined by teachers working in the context of their units.

•
Deficit model is sometimes apparent but it depends on the use by individual teachers. This is discouraged during training.

Analyse internal capacity to effect change	Action	
•
Evaluate intrinsic motivators for innovation and use of LA.

•
Evaluate institutional culture (e.g., trust in data and openness to changes and innovation).

•
Evaluate resources, tools, and platforms available for primary users to uptake learning analytics.

•
Evaluate human capacity (e.g., data literacy, relevant expertise, staff workload, opportunities for skill transfer).

•
Evaluate and modify technological infrastructure to increase true interoperability of data between silos.

•
Evaluate risks.

•
Train teaching (and/vs) support staff in selected LA platforms.

•
Main motivators of workload/time/efficiency, and improving student learning, feedback, personalisation.

•
Culture relatively unchanged but possibly more attuned to and/or accepting of data.

•
Main LA tool is SRES; other peripheral tools exist.

•
Somewhat increased human capacity?

•
Interoperability has been serendipitously enabled by move to Canvas with its open API.

•
Risks still unknown; continuous discovery.

•
Teachers and support staff both trained; training seems to start with teachers but increasingly seeing support staff training needs increase (train the trainers) as adoption becomes more widespread. Initial contact still predominantly with teachers.

Challenge	
•
Institution-wide buy-in is hard to reach.

•
Data is held in silos or is inaccessible to electronic systems.

•
Establishing and maintaining an LA platform is costly (in terms of time, budget, etc).

•
Reluctance to change is present among some teaching staff (e.g., try new or unfamiliar technologies, or change teaching styles).

•
Instructors are more interested in establishing a research profile than enhancing teaching and learning.

•
Training could be difficult to deliver when staff lack time.

•
Staff changes/turnover might affect the continuity of LA platform use in areas of the institution.

•
Different levels of comfort and understanding of technology (and data, and teaching) might affect the uptake of LA.

•
Buy-in operates locally from the bottom-up.

•
Data increasingly available due to inputs through SRES or through Canvas use.

•
Maintenance is predominantly down to one person with increasing support from ICT and other central designers. Potential point of failure.

•
Reluctance can be overcome through local champions.

•
Research/teaching tension largely unchanged, although the university is trying to balance this more.

•
Training needs to be more just-in-time but this presents workload challenges for trainers.

•
Staff turnover is a problem - both when teachers move on (motivation for use decreases) and support staff move on (technical know-how decreases). There are cases of both retention and discontinuation with SRES when turnover occurs.

•
Initial learning curve is fairly high but users often start with specific parts of the platform and then expand use.

Policy	
•
Will the design of selected learning analytics tools address teaching and learning needs?

•
How will the implementation address the problem of time poor among teaching staff?

•
How to best balance and support the workload stressors of teaching staff between research/teaching/admin and innovation?

•
What training will be deployed to scale up data literacy and incorporate learning analytics into daily practice? Will the training be compulsory for any stakeholder?

•
Are there related policies in the university that the policy sits alongside/above/below?

•
Are there any national/international policies that this policy has to adhere to?

•
What are the principles and policies around moral/ethical obligation to act on supporting students if LA identifies and/or affords the need to?

•
Tool (SRES) is co-designed and co-evolves alongside teacher and staff needs.

•
Workload is redistributed although time does remain an issue. Learning curve can be steep.

•
Research/teaching balance is still troublesome. Many innovators see the initial time committment worthwhile.

•
Training is highly recommended, or at least working alongside an increasing community of locally experienced staff.

•
‘Learning analytics principles’ exist since 2016.

•
Policies exist like privacy and security legislation.

•
Policies largely silent on obligations, no change, apart from the usual learning and teaching policies around obligations to support student learning.

Establish monitoring and learning frameworks	Action	
•
Involve primary users in co-creation of LA.

•
Establish qualitative and quantitative indicators of success.

•
Increase reach and organic championing of LA.

•
Seek feedback from primary users through various channels.

•
Primary users have been involved closely throughout.

•
Indicators of success have not been explicitly established - it varies depending on use.

•
Organic growth and champions are apparent.

•
Various channels are available for primary users to feed back (e.g. through training, service, and other encounters)

Challenge	
•
It could be hard to isolate learning analytics from parallel projects that support the same goals (e.g., enhance learning and teaching) when measuring success.

•
Wrongly assume causal relationship between learning outcomes and interventions or engagement patterns.

•
Inherently noisy and difficult to measure impact of educational innovations.

•
Lack of precise measurement instruments.

•
Low participation of primary stakeholders in some top-down decisions around LA.

•
Overly depend on data that is conveniently available to justify a learning phenomenon.

•
Manage expectations (e.g., deliverables and impact).

•
Still troublesome to isolate impact of LA/SRES vs other interventions.

•
Causal relationship still troublesome and also depends on how LA/SRES is applied in varying contexts.

•
Noise is still troublesome.

•
Lack of measurement instruments is still troublesome but new survey feature within SRES helps somewhat. This is fed back to teachers also.

•
Low participation and consultation still problematic.

•
Less reliance on convenience data because SRES enables direct capture.

•
Managing expectations is still important but so is providing continuity of support and engagement with adopters.

Policy	
•
What defines success or failure? How will success be measured? What are success indicators?

•
What avenues are there to share innovations - both successes and failures?

•
What scope will teaching staff have to be protected from potential degradation in student evaluations while innovating?

•
Success indicators are elusive but seem to be politicised around blunt KPIs.

•
Increase avenues to share innovations e.g. through institutional blogs, institutional social media, masterclass sessions, etc.

•
Protection from student evaluations an ongoing issue.


Appendix 2. UNSW Sydney compiled SHEILA Framework

ROMA step	SHEILA component	UNSW at T1	UNSW at T2
Map the political context	Action	
•
Alignment with 2025 Strategy and the four pillars for education: 1) inspired learning through inspired teaching, 2) digitization; 3) feedback through dialogue; 4) building communities

•
Support from PVCE to fulfil the strategy; academics championing innovation; support in managing large courses

•
Personalisation of feedback is a central tenet of the strategy and therefore work supporting this is encouraged

•
Potential for large impact due to the large number of students with varied background, needs and motivations

•
SRES as catalyst to encourage change in processes and pipelines

•
Tightening of the benefit realisation for the 2025 Strategy with severe budget cuts

•
Unchanged support from PVCE to fulfil the strategy, but re-prioritazation required;

•
Academics championing innovation: attention to ‘education-focussed career’ academics and the Scientia Education Academy as potential outlets for partnerships;

•
Unchanged focus in enahncing and supporting large foundation courses

Challenge	
•
No specific policy on LA

•
Federated/distributed context

•
Requiring approval and/or buy-in from several players

•
Possible lack of buy-in (or plain resistance) from educators (unless there are perceived benefits)

•
There is no one-size-fits-all model, even within one institution (different disciplines and learning modes)

•
Wrongly assume that learning analytics can solve all problems without having identified key questions to answer (data driven approach)

•
Institutions feel pressured to adopt learning analytics even though the needs for it are unclear

•
Learning analytics does not generate new insights into the understanding of learning or teaching

•
No prior knowledge of the SRES tool

•
A CRM solution does something similar, but with a top-down approach and no access for teachers

•
How tools like SRES help to improve personalisation of feedback?

Unchanged
•
No specific policy on LA

•
Federated/distributed context

•
Requiring buy-in from several players

New
•
Reinforced need to share the benefit-realisation for educators (both at personal and organisational level)

•
Limited knowledge of SRES, dependency on central support for using the tool

•
questions raised about the level of insight provided by data

•
helping to progress research into the effectiveness of L&T

Policy	
•
What are the reasons for adopting learning analytics (e.g., to improve teaching and learning)?

•
Which problems are to be addressed by using LA?

•
How do institutional objectives align with personal benefits for teaching staff and students?

•
How tools like SRES help to improve personalisation of feedback?

•
What support channels must be enabled to ensure sustainability?

Identify key stakeholders	Action	
•
Even with central PVCE support, several stakeholders are needed for data governance, data access, technical support, as well as academic ADEs, HoS

•
Identify primary users of learning analytics (e.g., students, teaching staff, and senior managers).

•
Identify professional teams (e.g., IT, legal team, strategy team, Student Support, Student Registry, library).

•
Identify academic teams (e.g. Learning & Teaching committee, Digital Learning Committee, research project teams)

•
Identify internal advocates of learning analytics among members of faculties (bottom-up approach).

•
Identify required expertise (e.g., learning analytics expertise, IT expertise, statistical expertise, educational expertise, psychological expertise)

•
Even with central PVCE support, several stakeholders are needed for data governance, data access, technical support, as well as academic ADEs, HoS


•
Determine clear responsibilities and support channels

Challenge	
•
Data governance and sharing (particularly with external parties) requires a careful check of security issues and breaches of privacy

•
Potential power-play from some stakeholders

•
Data access from a technical perspecitve

•
Misalignment of strategic intents

•
Access to funding and support

•
Lack of a shared vision between stakeholders and/or misalignment of intents

•
Lack of understanding of potential benefits

•
Define ownership and responsibilities among diverse professional groups within the university

•
Risk marginalising hard-to-reach students by drawing a distinction between students who opt out and those who opt into a learning analytics service

•
The choice of opt-out or not opt-in could affect those who choose to opt in regarding the quality of data and services provided

The failure in engaging certain individuals/units led to the creation of workarounds rather than sustainable solutions
•
Power-play from some stakeholders blocks or slows down the process

•
Access to funding and support this is rapidily changing due to sever budget cuts

•
Lack of a shared vision remains

•
Misalignment of strategic intents rremains

•
Lack of understanding of potential benefits; partly achieved, but need stronger champions in the senior leadership

Policy	
•
Who is the data controller?

•
Who can access data?

•
Who owns data?

•
How will anonymity policy be applied to the processing and presentation of data?

•
Can collected data be edited or deleted upon request?

•
Will data be shared with researchers?

•
Will data be shared with external parties? Is it justifiable?

•
Whose data will be collected?

•
Who is the policy for? Whose working activities will the policy shape?

•
How will responsibilities be defined for each stakeholder?

•
Will learning analytics exclude certain groups of students? Will there be mechanisms to address inequality?

•
Will the policy cover those who choose to opt out (or not to opt into) a learning analytics service?

•
How will the current policy be covering?

•
How will consent be obtained and when?

•
What are the circumstances where obtaining further consent is necessary?

•
Is there an option to opt-out of (or opt into) any data collection and analysis? When will the option be available?

•
Will students have a free choice of whether or not to accept interventions based on analytics?

•
Some progress made in identifying interested parties

•
Initiation of conversations about data ccess and data sharing agreements

Identify desired behaviour changes	Action	
•
Identify expected changes to the current context and key stakeholders (e.g., teaching staff and students)

•
Identify areas where different stakeholders will be supported by learning analytics (macro level, institution, meso level department/programme, and micro level teaching staff and students)

•
Consider responsibilities and implications for all stakeholders

•
Mind inadvertent consequences and make sure the benefits of learning analytics to students outweigh risks

•
Monitoring students' behavioural changes

•
Alignment of intents and key strategic actions

•
Provision of appropriate resourcing (funding, but also ‘political’ support)

•
Establishment of appropriate governance

•
Agreement on principles and practices

None of the following have been implemented
•
Alignment of intents

•
Alignment of key strategic actions

•
Provision of appropriate resourcing

•
Establishment of appropriate governance

•
Agreement on principles and practices

Challenge	
•
People mistrust the result of an analysis if the process is not transparent or if the analytical model is too complicated to understand

•
Unethical profiling of students may occur when selecting those that are more likely to succeed

•
Learning analytics can reveal what was/is happening and predict what is likely to happen, but it may not explain the observed phenomenon or provide a direct solution

•
Students may be prone to choose subjects where they are likely to perform well

•
Those who need support may not necessarily make use of information from learning analytics

•
Exploring issues if data accuracy and data automation of pipelines

•
Need to do more work with students (co-design) to evaluate the effectiveness of communications and data dashboards

•
Explore in more detail the ‘so what?’ question

Policy	
•
How will transparency be achieved throughout a project cycle (data collection, analysis, and usage)?

•
What positive changes will learning analytics bring to the current situation (e.g., learning and teaching landscapes)?

•
Why are these changes important to us?

•
Who will benefit from learning analytics?

•
How will the purpose and functions of learning analytics be communicated to primary users?

•
What are the mechanisms to deal with inadvertent consequences? (and list what the inadvertent consequences may be)

•
In the pilot projects, an improvement in both student experience and outcomes was recorded, making the implementation not only viable, but promising for scaling

Develop an engagement strategy	Action	
•
Move from tinkerers (and proof of concept) to ‘business as usual’ practices

•
Establish an appropriate governance board with clearly defined accountabilities (including teaching staff and students) and define a clear leadership structure

•
Articulate principles driving the initiative (or program of work) and align them to strategy

•
Demonstrate potential benefits to the stakeholders and the university

•
Showcase cases of good practice as supporting evidence

•
Do not shy away from failures as lesson learned

•
Consult relevant policies and codes of practice (e.g., Jisc's Code of Practice for Learning Analytics, and data protection policies)

•
Seek funding (or identify sources of sustainable support)

•
Align learning analytics with the wider institutional strategies or introduce learning analytics into the university's strategy

•
Engage with research projects locally or through collaboration with other institutions

No progress has been made in the following areas:
•
Moving from tinkerers to business as usual practices [this worsened with the tightening of budgets]

•
Establish an appropriate governance board [several POC proof of concepts have started, but with unclear outcomes]

•
Articulate principles driving the initiative and align them to strategy

•
Some progress has been done with the following

•
Demonstrate potential benefits to the stakeholders and the university

•
Showcase cases of good practice as supporting evidence

•
Do not shy away from failures as lesson learned

•
Articulate and highlight ethical issues with LA implementations

Challenge	Challenges related to students
•
There are conflicts between good intentions to support students and unintentional intrusion into privacy

•
Surveillance leads to conscious or unconscious behavioural alteration that is against the goals of learning analytics

•
It is arguable to base predictive models on pre-determined factors, such as demographic characteristics

•
Learning analytics removing student agency from them by drawing attention away from their own responsibility for learning

•
Overloading primary users with too many messages about analytics results

•
Disengaged students remain hard to reach

•
Feedback is provided without proper support, which leaves students in anxiety or complacency, thereby demotivating them

•
Focus on identifying students at risk and overlook the pedagogical design of curriculum or learning support

•
Peer comparison, especially for underperforming students, may demotivate them alltogether

Challenges related to staff
•
What incentives/rewards are available?

•
What are the benefits of adoption?

•
Is it going to save time/effort in admin?

•
Where is the training/support coming from?

•
Most of the challenges presented remain for both students and staff; to a great extent these are all affecting the sustainability of the implementation

Policy	
•
What are the objectives for learning analytics? How do they align with the institution's vision for education?

•
Will learning analytics be used as a management tool to monitor students or staff?

•
Will learning analytics be used as a deficit model targeted at supporting students at risk of failure?

•
What kinds of data will be collected to achieve the identified objectives?

•
When will data be collected?

•
What is the scope of data collection?

•
What are the methods of data collection?

•
What kinds of data will be presented? How? To whom?

•
How will the results of analytics be interpreted within the context? What kinds of expertise needs to be involved in this process? Does it include teaching staff and students?

•
How will the results of analytics be communicated in a way that motivates learning?

•
How will resources be distributed efficiently and fairly as a result of the analysis of data?

•
Will there be interventions based on analytics? What are the circumstance?

•
Will learning support and resources be made available to all?

•
All of these remain open and unresolved questions: why? Who needs to be involved? Do we require a formal governance structure?

Analyse internal capacity to effect change	Action	
•
Requires both individuals and management

•
Need technical expertise to support tools/initiatives

•
Focus on evidece-based practice and data literacy

•
Promote experimentation and champion the pioneers

•
Include domain expert to support the early adopters and develop sustainable support strategies

•
Agree who is doing what, when and how

•
Change management required /cultural shift


•
Evaluate institutional culture (e.g., trust in data and openness to changes and innovation)

•
Evaluate technological infrastructure

•
Evaluate existing legal framework and its applicability for learning analytics

•
Evaluate financial capacity

•
Evaluate human capacity (e.g., data literacy, relevant expertise, staff workload, opportunities for skill transfer)

•
Establish indicators of data quality and system efficacy

•
Evaluate risks

•
Requires both individuals and management

•
Need technical expertise to support tools/initiatives

•
Focus on evidece-based practice and data literacy

•
Promote experimentation and champion the pioneers

•
Include domain expert to support the early adopters and develop sustainable support strategies

•
Agree who is doing what, when and how

•
Change management required /cultural shift

Challenge	
•
Data is noisy (or plainly not good enough)

•
Some useful data remains inaccessible

•
Data is held in silos

•
Setting up a learning analytics environment is costly

•
Central steering groups and individual project groups do not coordinate

•
The difficulty of comprehending algorithms leads to disengagement with or distrust of learning analytics among primary stakeholders

•
Results of analytics are interpreted and communicated by people without proper understanding of data (e.g., fail to contextualise data or interpret it with sufficient statistical knowledge)


•
Engaging students with institutional policies in an informed way

•
The maturity of data literacy varies among stakeholders and faculties

•
Instructors are more interested in establishing a research profile than enhancing teaching and learning

•
Senior managers are more interested in financial benefits to the institution than the benefits in enhancing learning and teaching

•
There is no common understanding of learning analytics among stakeholders at different levels (e.g., managers, teaching staff, IT officers, and students)

•
Reluctance to change is present among some teaching staff (e.g., try new or unfamiliar technologies, or change teaching styles)

•
Training could be difficult to deliver when staff lack time

•
Limited awareness or discussion regarding privacy and ethical issues cripple the adoption of learning analytics when issues arise

•
The lack of critical self-reflection skills reduces the chance to benefit from learning analytics

•
Must identify the priorities for different stakeholderrs

•
Characterise challenges based on the level of the organisation and target iteratively

•
Must widen the support from individual educators to senior managers across org units

Policy	
•
Are there related policies in the university that the policy sits alongside/above/below?

•
Are there any national/international policies that this policy has to adhere to?

•
What training will be deployed to scale up data literacy and incorporate learning analytics into daily practice? Will the training be compulsory for any stakeholder?

•
What communication channels or feedback mechanisms will be in place?

•
Will the design of selected learning analytics tools address teaching and learning needs?

•
How will the implementation address the problem of time poor among teaching staff?

•
How will data be stored and disposed?

•
How often will the efficiency and security of existing data infrastructure be evaluated?

•
How will data integrity be achieved?

•
Is there an application procedure for using learning analytics for research or teaching purposes? Are the procedures different?

•
Questions remain the same; need to continue to work in building stakeholders' support and consensus in order to proceed

Establish monitoring and learning frameworks	Action	
•
Negotiate evaluation questions and measures of success with individual academics

•
Set up measurable milestones

•
Establish qualitative and quantitative indicators of success

•
Develop methods to triangulate analytics results

•
Seek feedback from primary users through various channels

•
General improvement of the student experience

•
Improvement in the learning outcomes

•
Improvement of the logistics/organisation of courses

•
Improved effectiveness of communication

•
Leverage on domain-specific data-driven decisions

•
Improved sense of accomplishment in teachers

•
Achieved improvements in automation of workflows

•
Made available data for students [with different aims]

•
Enabled to encourage better student self-regulation of learning

•
Enhanced the way the organisation maintains data pipelines and collects, stores and uses data about learning and the learning processes

•
Established sustainable support processes to advise academics on feedback practices

•
Established clear and scalable processes to enable feedback at scale by making the appropriate data available to staff

Achieved in various measure
•
Negotiate evaluation questions and measures of success with individual academics?

•
General improvement of the student experience

•
Improvement in the learning outcomes

•
Improvement of the logistics/organisation of courses

•
Improved effectiveness of communication

•
Leverage on domain-specific data-driven decisions

•
Improved sense of accomplishment in teachers

•
Achieved improvements in automation of workflows

•
Made available data for students [with different aims]

•
Enabled to encourage better student self-regulation of learning

Not achieved with work in progress
•
Enhanced the way the organisation maintains data pipelines and collects, stores and uses data about learning and the learning processes

•
Established sustainable support processes to advise academics on feedback practices

•
Established clear and scalable processes to enable feedback at scale by making the appropriate data available to staff

Challenge	There are two key dependencies:
•
the success of the individual pilots using SRES to drive activity

•
the organisational support and alignment with the intended goals of the pilot project

•
Low participation of primary stakeholders in top-down consultations (e.g., survey and meetings)

•
Manage expectations (e.g., deliverables and impact)

•
Fail to recognise and address limitations of data and analytics models (e.g., uncapturable factors of learning, ineffective metrics, existing bias, inaccuracy of predictions)

•
Overly depend on data that is conveniently available to justify a learning phenomenon

•
Fail to contextualise data

•
Wrongly assume causal relationship between learning outcomes and interventions or engagement patterns

•
Interventions introduced to one course may have negative impact on student engagement in another course

•
Emphasise measuring output (learning or teaching performance) and overlook developing input (e.g., strategies, skill development)

•
Overlook the differences between individuals in their learning or teaching approaches

•
Definitions of learning vary, which impacts the way data is collected, analysed, and interpreted

•
The implementation of SRES provided some clear wins for academics and students, however the benefit realisation for the organisation remain still unclear

•
the difficulty in creating appropriate automation processes remains a challenge

•
the lack of scalable support for academics is a risk

•
the lack of a sustainable resourcing model provides a possible fatal risk for the implementation

•
to date, the limited impact of the implementation is a major weakness in the institutional adoption, however the lack of resources would make it impossible to scale

Policy	
•
How often will the policy be reviewed and updated?

•
Who will be responsible for the policy?

•
What defines success or failure? How will success be measured? What are success indicators?

•
Who defines success measures? What expertise needs to be involved?

•
When will evaluation take place?

•
Who will carry out the evaluation of impact?

•
What are the limitations of learning analytics (what is learning analytics not meant to do)?

•
Will any access to data lead to stereotypes and biased results (e.g., marking exams or assignments biasedly)?

•
Are there any measures to ensure that students are equipped with sufficient knowledge to make opt-in/out decisions?