This paper examines the discrete learning strategies employed within a massive open online course and their relationship to the student learning experience. The theoretical framework centered on the Community of Inquiry model of online education, which outlines the three critical dimensions (presences) of student learning experience: teaching, social, and cognitive presence. The Community of Inquiry survey instrument, administered as the part of the post-course survey, was used to measure student perceived levels of the three presences. Cluster analysis revealed three different groups of students with unique study strategies: limited users, selective users, and broad users. The strategies adopted significantly differed in student use of available tools and resources, final course grade, as well as the perceived levels of cognitive presence. The results also indicate there were significant differences regarding student commitment to learning, motivations and goals for enrolling in a MOOC, as well as goal orientation, approaches to learning, and the use of different study strategies. Implications for research and practice of online learning are further discussed.

Previous
Next 
Keywords
Community of inquiry model

Massive open online courses

Study strategies

Online learning

Higher education

1. Introduction
Arguably, one of the most interesting recent developments in the domain of online and distance education has been the emergence of Massive Open Online Courses (MOOCs). Although the design of individual MOOCs can substantially differ, for the most part, they are (mostly) freely available, fully online, not-for-credit courses that can be enrolled by anyone interested in the course topic. Unlike previous generations of distance and online learning, MOOCs generate new opportunities to access vast amounts of student-generated data that can be used to assess the impact of instructional approaches and provide insights into the complexities of human learning (Reich, 2015).

There are several characteristics of MOOCs that influence the design of the teaching model and therefore, the student learning experience. These characteristics relate to cohort size, diversity and informal learning context. For instance, the number of students enrolling in MOOCs tends to be much larger than in a traditional for-credit online or blended courses. MOOCs often have enrolments up to tens or even hundreds of thousands of students (Coughlan, 2015). The unprecedented size of the student cohort in MOOCs challenges commonly accepted online pedagogical approaches. The sheer volume of students militates against the scaling of more socially oriented teaching practices to this (relatively) new setting. Understandably, MOOC pedagogies have therefore tended to focus on more scalable teaching models such as content transmission. In these instances, the technology is used to scale and enable behaviorist models of learning and teaching (Ng & Widom, 2014; Rodriguez, 2012). Secondly, the student population in MOOCs is much more diverse, with substantial differences in, for example, prior knowledge, age, education level, and English language proficiency (Ho et al., 2014). Likewise, as participating in MOOCs is primarily an informal and non-accredited learning activity, student motivations for enrolling are more diverse than within formal educational settings (Kizilcec & Schneider, 2015).

The characteristics of MOOCs create a learning context that directly challenges the design and implementation of pedagogical practices that aim to promote social interactions (Akyol & Garrison, 2008; Garrison, 2011; Poquet, 2017). This is in contrast with the contemporary educational psychology that shows significant learning skills being developed through social interactions such as critical thinking, creativity, collaboration, and communication.1 Moreover, modern educational research provides many approaches to developing, facilitating, and directing effective online and blended learning experiences by taking advantage of the technological systems that support information seeking and knowledge building. In this regard, the research around the widely-used Community of Inquiry (CoI) model (Garrison, Anderson, & Archer, 1999) provides much empirical evidence on the significance of the synergy between teaching, socialization, and cognition for improving the student learning experience.

It is also important to recognize that successful teaching and learning requires more than the provision of different educational tools and resources (Lust, Juarez Collazo, Elen, & Clarebout, 2012). Contemporary educational psychology emphasizes the role of human agency in shaping student learning experience and provides substantial evidence on the importance of metacognitive skills to effectively leverage the available technological affordances (Winne, 2006). To improve the overall MOOC learning experience, it is essential to understand the ways in which students make use of the available technological affordances and how these tools affect their learning outcomes. In this regard, several theoretical models, such as self-regulated learning (SRL) (Bjork, Dunlosky, & Kornell, 2013; Winne & Hadwin, 1998), goal orientation (Senko, Hulleman, & Harackiewicz, 2011), and approaches to learning (Trigwell & Prosser, 1991), provide a foundation for understanding how students' agency influences the learning strategies they adopt.

This paper reports on a study investigating how students' adopted learning strategies influence their overall learning experience in a MOOC. The study is framed within the CoI model (Garrison et al., 1999), with its holistic view of the online and blended learning experience. Using the previously validated methodology for study strategy identification by Kovanović, Gašević, Joksimović, Hatala, and Adesope (2015), in this paper, we:

1.
Identify student learning strategies based on the trace data of students' use of available tools and resources. The study strategies are indicative of students' agency and decision-making processes.

2.
Examine how the different strategies relate to student perceptions of cognitive, social, and teaching presence. These three key dimensions of the CoI model, capture the student online learning experience and the development of critical and higher order thinking skills.

3.
Examine how the different strategies relate to differences in learning outcomes through the analysis of their differences with regards to the final course grade. While secondary to our main analysis of the differences in the three CoI presences, the analysis of final course grade differences provides additional insights into the differences between identified strategies, in terms of student retention and course completion.

4.
Interpret the identified strategies through the analysis of clustering differences, responses to pre-course and post-course surveys, and students' final course grades. These data provide the supplementary information necessary to better understand how human agency processes shape the identified study strategies.

5.
Evaluate the differences between MOOCs and traditional, for-credit online/blended courses regarding students' study strategies, by comparing the results of the present study with the existing research within traditional online/blended setting.

2. Background work
2.1. The Community of Inquiry model
The Community of Inquiry (CoI) model (Garrison et al., 1999) is a popular pedagogical model that outlines the critical dimensions that influence the student online and blended learning experience. The CoI model is rooted in the social constructivist notion of learning and focuses on knowledge (co)creation through peer-to-peer social interactions (Garrison et al., 1999). The model outlines three key dimensions of learning, also known as presences:

1.
Cognitive presence focuses on students' development of critical and higher-order thinking, and is operationalized through the four phases of inquiry learning cycle:

a.
Triggering event (problem conceptualization),

b.
Exploration (idea generation),

c.
Integration (knowledge synthesis), and

d.
Resolution (knowledge application and vicarious testing) (Garrison, Anderson, & Archer, 2001).

2.
Social presence refers to the development of social interactions among the learning group within a productive social climate. Social presence includes i) open communication, ii) affective expression, and iii) group cohesion (Rourke, Anderson, Garrison, & Archer, 1999).

3.
Teaching presence outlines an instructors' role before and during the course, including i) course organization & design, ii) direct instruction, and iii) facilitation (Anderson, Rourke, Garrison, & Archer, 2001).

Although the CoI model was designed for assessing the quality of inquiry-based online learning, its broad applicability resulted in the broader adoption applied as a general model of students' online and blended learning experience (Anderson & Dron, 2010; Swan & Ice, 2010). To assess the levels of the three presences, researchers have either adopted the content analysis of students' discussion messages using pre-defined coding schemes for each presence (Garrison et al., 2001) or used a self-reported instrument consisting of 34 Likert-scale items designed to measure students' perceived levels of the three CoI presences. The CoI survey instrument has been extensively validated in traditional online settings (see Garrison, Anderson, & Archer, 2010), and more recently in the MOOC context (Kovanović et al., 2018).

2.2. Self-regulated learning and educational technology
A principle underpinning contemporary educational research is that students are active agents who monitor, regulate, and control their learning process (Anderson & Dron, 2010; Winne, 2006). In this regard, self-regulated learning (Bjork et al., 2013; Winne & Hadwin, 1998), a major theory of learning in the contemporary educational psychology, provides the theoretical foundation for understanding the human agency in the learning domain. An important aspect of self-regulation in online and blended learning settings is that it includes decisions regarding, if, how, and when a student chooses to use a particular digital learning tool and technology (Azevedo, 2005; Winne, 2006). Prior research indicates that many students fail to effectively regulate their use of the available tools and resources in a way that maximizes their learning. Most of the available learning tools are significantly underused by a majority of students (Ellis, Marcus, & Taylor, 2005; Kovanović et al., 2015; Lust, Elen, & Clarebout, 2013a; Lust, Vandewaetere, Ceulemans, Elen, & Clarebout, 2011). This is particularly significant for complex online learning environments (Shen, Cho, Tsai, & Marra, 2013), as many students simply lack awareness, knowledge, or motivation to use the available tools to aid their learning (Lust et al., 2013a; Winne, 2006).

In the context of the CoI model, numerous studies (e.g. Akyol & Garrison, 2011; Garrison & Akyol, 2013; Shea et al., 2012; Shea et al., 2013; Shea et al., 2014; Shea and Bidjerano, 2010, Shea and Bidjerano, 2012) have noted that metacognition and self-regulation are key for understanding the student online learning experience, given its self-directed and social nature (Shea & Bidjerano, 2010). As suggested by Shea et al. (2012, 2013, 2014), the self-directed and social nature of learning within communities of inquiry implies that any development of cognitive presence is contingent on both self and co-regulation of student learning. These are in turn supported through effective teaching presence activities (i.e., instructional design, direct facilitation, and instruction), as well as through peer guidance (Shea et al., 2012, 2013, 2014).

2.3. Psychological factors of study strategy adoption
A key reason for the observed differences in students' adoption of study strategies pertains to the students' ability to regulate their learning effectively (Clarebout, Elen, Collazo, Lust, & Jiang, 2013; Lust et al., 2013a). For example, in the context of study strategies extracted from trace data, Lust et al. (2013a) showed that while the majority of students regulated their use of the available tools, only 3% had done so effectively and in alignment with the course objectives. A vast majority of students (59%) in Lust et al. (2013a) study used a very limited set of tools, demonstrating a lack of ability to effectively regulate their learning activities in alignment with the learning tasks defined in the course design (Perkins, 1985).

The literature also suggests that the choice of study strategies is directly related to students' goal orientations (Lust, Elen, & Clarebout, 2013b). Lust et al. (2013b) found that undergraduate students of educational science who focused on gaining competence (i.e., had high mastery goal orientation) had more active use of the available study resources than those who had low levels of mastery goal orientation. By contrast, students who focused on demonstrating competence (i.e., had high-performance goal orientation) exhibited a limited resource use (Lust et al., 2013b).

Students' study strategies are commonly associated with the construct of approaches to learning (Trigwell & Prosser, 1991). Deep approaches to learning (i.e., focus on content understanding) are found to be associated with mastery goal orientation (Phan, 2008) and higher student success (Trigwell & Prosser, 1991), while surface approaches to learning (i.e., focus on content reproduction) are found to be associated with performance goal orientation (Phan, 2008) and low learning outcomes (Trigwell & Prosser, 1991). For example, Wise, Speer, Marbouti, and Hsiao (2013) identified a connection between the minimal cognitive engagement in discussions and surface approaches to learning and high cognitive engagement in discussions and deep approaches to learning. However, students' strategic behavior can be associated with both deep and surface approaches to learning (Entwistle, 2009). For example, Gašević, Jovanović, Pardo, & Dawson, 2017 found that students reporting deep approaches to learning adopted more diverse study strategies, with a right balance of strategies that promote learning and more strategies that are more performance-oriented (e.g., summative assessment). By contrast, students adopting a surface approach to learning focused predominantly on performance-oriented activities (Gašević et al. (2017).

2.4. Common student study strategies
A significant number of studies used the data collected by digital learning environments to examine ways in which students utilize available technology and its effects on learning outcomes. A systematic review by Lust et al. (2012) pointed out at the plenty of research evidence which suggests that students differ in their study strategies and that those differences have a substantial effect on students' course performance. In this subsection, we review the existing literature for the commonly reported study strategies in online and blended learning, which is essential for understanding the results of the present study.

2.4.1. Common study strategies in online learning
Broadly speaking, the previous research (e.g., Agnihotri et al., 2015; Bovo et al., 2013; Gašević et al., 2017; Jovanović, Gašević, Dawson, Pardo, & Mirriahi, 2017; Lust et al., 2011; Lust et al., 2013a; Rodrigues et al., 2016; Valle & Duffy, 2009; Wise et al., 2013; Yen & Lee, 2011) suggests there are three distinct groups of students based on their adopted study strategies (Table 1). The first group of students is characterized by their low level of engagement with the learning system. These students also exhibit low effort and cognitive engagement in the course, performance goal orientation, surface approach to learning, and poor regulation of their tool use (Wise et al., 2013; Valle & Duffy, 2009; Kovanovi´ c et al., 2015; Lust et al., 2011, Lust et al., 2013a, Lust et al., 2013b). By contrast, the second group is characterized by a high level of student engagement and active participation in the course. The students are primarily driven by a mastery approach goal orientation, demonstrate high cognitive engagement, a deep approach to learning and, not surprisingly, achieve a solid course performance. The final group reported in the literature is characterized by their selective use of the available learning tools and resources. Students with this strategy tend to focus on completing particular learning tasks, have high levels of regulatory behavior (Kovanović et al., 2015;Valle & Duffy, 2009; Wise et al., 2013) and also exhibit positive study strategies (Valle & Duffy, 2009; Wise et al., 2013). Unlike intensive users (Lust et al., 2013b), their selective tool use and strategic behavior seem to be more indicative of their efforts to manage their time constraints (Wise et al., 2013) rather their mastery or performance goal orientation.


Table 1. Common groups of students based on their adopted study strategies.

Name	Characteristics	References
Disengaged students	
•
Low effort and engagement

•
Performance goal orientation

•
Surface approach to learning

•
Poor regulation of tool use

“Disengaged” (Bergner, Kerr, & Pritchard, 2015; Kizilcec, Piech, & Schneider, 2013; Rodrigues et al., 2016), “Nousers” (Kovanović et al., 2015; Lust et al., 2011; Lust et al., 2013a; Lust et al., 2013b), “Minimalist in effort” (Valle & Duffy, 2009), “Superficial listeners, intermittent talkers” (Wise et al., 2013), “Passive participants” (Bovo, Sanchez, Héguy, & Duthen, 2013; Hill, 2013; Milligan, Littlejohn, & Margaryan, 2013; Sharma, Jermann, & Dillenbourg, 2015), “Inactive” (Li, Kidziński, Jermann, & Dillenbourg, 2015), and “Low achievers” (Agnihotri, Aghababyan, Mojarad, Riedesel, & Essa, 2015).

Engaged students	
•
High effort and engagement

•
Active course participation

•
Mastery approach goal orientation

•
Deep approach to learning

“Engaged” (Rodrigues et al., 2016), “Intensive users” (Kovanović et al., 2015, Lust et al., 2011, Lust et al., 2013a), “Intensive active users” (Lust et al., 2013b), “Broad listeners, reflective talkers” (Wise et al., 2013), “Active participants” (Bovo et al., 2013; Hill, 2013; Milligan et al., 2013; Sharma et al., 2015), “High achievers” (Agnihotri et al., 2015), “Self-driven” (Valle & Duffy, 2009), “Completing” (Bergner et al., 2015; Kizilcec et al., 2013), and “Keen completers” (Ferguson et al., 2015).

Selective students	
•
Selective tool and resource use

•
Focus on task completion

•
High self-regulation of learning

“Selective users” (Lust et al., 2013a, Lust et al., 2013b), “Task-focused” (Kovanović et al., 2015), “Get it done” (Valle & Duffy, 2009), “Concentrated listeners, integrated talkers” (Wise et al., 2013), and “Efficiency-oriented” (Yen & Lee, 2011).

MOOC-specific study strategies
Auditing students	
•
Focus on exploring course topic

•
No intent in completing the course

•
Active use of video materials

•
Limited use of online discussions

•
Limited use of course assessment

“Lurkers” (Milligan et al., 2013), “Observers” (Hill, 2013), “Auditing” (Kizilcec et al., 2013), and “Viewers” (Sharma et al., 2015).

Sporadic students	
•
Focus on specific course topic(s)

•
Active one or two weeks

“Samplers” (Bergner et al., 2015; Ferguson et al., 2015; Kizilcec et al., 2013), “Dropins” (Hill, 2013), “Sporadic” (Rodrigues et al., 2016), and “Wiki-users” (Sharma et al., 2015).
2.4.2. Common study strategies within MOOCs
Given the specifics of the MOOC context and the high diversity in student motivation (Kizilcec & Schneider, 2015) and demographics (Hennis, Topolovec, Poquet, & de Vries, 2016), studies focusing on the MOOC context have reported several additional groups of students based on study strategies. For instance, due to open nature of MOOCs, a significant number of students termed “auditing students” (Table 1), are only interested in exploring a topic, without any intention of completing the course. The participation of these students primarily focuses on reviewing the video materials (and optionally in-video quizzes) with very infrequent (if at all) completion of graded assignments or discussion participation. The second common group of students in MOOCs, called “sporadic users” (Table 1), are the ones who engage with the course video materials for one or two weeks of the course. For example, some students intentionally enroll in several similar courses before committing to one of them, while others enroll in a course to find information about a particular topic without the intention to take an active part in the course (Kizilcec et al., 2013).

2.4.3. Study strategies within communities of inquiry
Within the context of communities of inquiry, there have been relatively few studies looking at students' study strategies and the use of available technology. Using self-reported instruments, Rubin, Fernandes, and Avgerinou (2013) examined the association between students' perceptions of the three CoI presences and perceived value of technology affordances. Of direct importance for the present study is the finding that the perceived level of cognitive presence was found to be predicted by the perceived ease of online communication and the perceived amount of online information, while the perceived ease of finding information was marginally significant (Rubin et al., 2013). These findings were expected, as effective participation in a community of inquiry and reaching higher levels of cognitive presence requires participation in online discussions, and engagement in various learning activities (e.g., online information-seeking, self-assessment, and completion of course assignments).

Similarly, in their study of students' study strategies within the CoI-based course, Kovanović et al. (2015) revealed six different groups of students based on their adopted study strategies. These groups include: i) task-focused users characterized by the strategic course engagement, ii) content-focused users focused on static course content, iii) no users represented by the overall low course involvement, iv) highly intensive users defined by their intensive and broad course activity, v) content-focused intensive users, characterized by their intense content-related activity, and vi) socially-focused intensive users who exhibit intensive discussion-related activity.

Kovanović et al. (2015) study revealed substantial differences between students that adopted different study strategies in their cognitive presence. Moreover, several strategies were associated with high levels of cognitive presence, emphasizing multiple ways in which students can thrive within communities of inquiry (Kovanović et al., 2015).

3. Research questions
The existing literature provides substantial evidence for the differences in study strategies and the effect they have on students' learning outcomes. However, there is a limited understanding of student adopted learning strategies within the CoI research, and even less so within the MOOC context. While reports by Rubin et al. (2013) and Kovanović et al. (2015) examined students' study strategies within communities of inquiry, these studies focused on traditional, for-credit, online courses. For this reason, in the present study, we have concentrated on understanding communities of inquiry within the MOOC context. In particular, we are interested in the extent to which the study strategies previously noted in traditional online courses are also reflected in MOOCs. Furthermore, the present study explores the association between a student's study strategies and the development of the three CoI presences and final course outcomes in order to assess the effects of the adopted study strategies on student learning. As such, the three research questions investigated are:

Research Question 1

What are the main study strategies within MOOC communities of inquiry?

Research Question 2

What is the association between the identified study strategies and the perceived levels of the three CoI presences within the MOOC context?

Research Question 3

What is the association between the identified study strategies and MOOC students' final course grade?

To address these questions, we build on the existing literature on the association between students' study strategies (i.e., Kovanović et al., 2015; Rubin et al., 2013), self-regulation (Akyol & Garrison, 2011; Garrison & Akyol, 2013) and student success within the context of communities of inquiry. Although we expect to find similar study strategies as in the existing literature (see Section 2), we are interested in examining how the open nature, as well as the massive scale of MOOCs, influences student technology selections within communities of inquiry. To better understand the identified strategies, we also examine students' responses to pre- and post-course surveys as a source of additional insights into the nature of the differences among the identified study strategies. Similarly, we look at how the effects of internal student learning regulation, agency, and approaches to learning – as manifested through different studentsâ€™ study strategies – affects the perceived levels of the three CoI presences as well as students' final course grades.
4. Methods and materials
4.1. Course context and study data
In this study, we used the data from the Fall 2014 offering of “Introduction to Functional Programming” MOOC offered on the edX platform by the Delft University of Technology. The course was eight weeks long and focused on introducing basics of functional programming using the Haskell programming language. The course was enrolled by 38,029 students, out of whom 23,648 logged into the course at least once, and 1968 obtained the course certificate (60% threshold, for the details, see Hennis et al., 2016). The course was delivered using pre-recorded video lectures, online discussions, and wiki pages. During the course, students were expected to complete eleven homework and seven labs implemented as single-attempt multiple-choice questions. The homework and labs were valued at 40% and 60% of the final course grade, respectively. In total, there were 286 graded questions, as each question was submitted individually.

Before the course start, students were asked to complete a pre-course survey consisting of 60 questions covering basic demographics, enrollment motivations, course expectations, prior knowledge, experience with the edX platform, English-language proficiency, and the level of support for completing the course. After the course, students were asked to complete the post-course survey with 97 questions about the overall course quality, their experience, and their use of different course components. The post-course survey also included the 34-item CoI survey questionnaire instrument which is developed by Arbaugh et al. (2008) and which is used to measure the perceived levels of the three CoI presences.

The collected data consisted of edX trace data logs, online discussions in JSON format, and student survey responses. Trace data was available for 23,648 students, while pre- and post-course survey data were available for 4909 and 1040 students, respectively.

4.2. Measuring instrument
To identify different study strategies, we extracted from the trace data various measures of student course engagement (Table 2), grouped into seven groups: i) course access, ii) assignments, iii) video lectures, iv) course navigation, v) discussion access, vi) discussion contribution, and vii) discussion reputation. The selected measures are based on the work of Kovanović et al. (2015), Lust et al. (2011, 2013a,b), and Valle and Duffy (2009).


Table 2. Measures used in the present study.

#	Variable	Description
Trace data measures
Group 1: Course access
1	OpenHome	No. of times course main page was opened.
2	OpenWiki	No. of times course wiki was opened.
3	OpenProgress	No. of times progress page was opened.
4	OpenSyllabus	No. of times syllabus page was opened.
5	OpenCourseware	No. of times modules were opened.
6	OpenCoursewareItem	No. of times module pages were opened.

Group 2: Assignments
7	AssignmentStart	No. of times assignments were opened.
8	AssignmentSubmit	No. of times assignments were submitted.

Group 3: Video lectures
9	VideoLoad	No. of times video lectures were loaded.
10	VideoPlay	No. of times video lectures were played.
11	VideoPause	No. of times video lectures were paused.
12	VideoChangeSpeed	No. of times video lecture speed was changed.
13	VideoShowSubs	No. of times video subtitles were shown.

Group 4: Course navigation
14	ModuleNext	No. of times next module link was used.
15	ModulePrev	No. of times previous module link was used.
16	ModuleJump	No. of times “goto” link was used.

Group 5: Discussion access
17	ThreadAccess	No. of times threads were opened regularly.
18	ThreadAccessInline	No. of times topics were opened from modules.
19	DiscussionSearch	No. of times forum search was performed.

Group 6: Discussion contribution
20	DiscussionsStarted	No. of regular topics started.
21	QuestionsStarted	No. of QA topics started.
22	CommentsWritten	No. of responses/comments written.
23	ThreadsCharsAvg	Avg. no. of characters per thread.
24	CommentsCharsAvg	Avg. no. of characters per response/comment.
25	UpvotesGiven	No. of upvotes given.
26	CommentsEnd.Given	No. of comment endorsements given.

Group 7: Discussion reputation
27	CommentsReceived	No. of replies/comments received.
28	UpvotesReceived	No. of upvotes received.
29	CommentsEnd.Rec.	No. of comment endorsements received.

Outcome measures
Course grades
1	FinalGrade	Final course grade.

CoI: Perceived levels of cognitive presence
2	Trig.EventLevel	Perceived level of triggering event phrase.
3	ExplorationLevel	Perceived level of exploration phrase.
4	IntegrationLevel	Perceived level of integration phrase.
5	ResolutionLevel	Perceived level of resolution phrase.

CoI: Perceived levels of teaching presence
6	Org.DesignLevel	Perceived level of organization & design.
7	FacilitationLevel	Perceived level of facilitation.
8	DirectInst.Level	Perceived level of direct instruction.

CoI: Perceived levels of social presence
9	AffectiveExp.Level	Perceived level of affective expression.
10	OpenComm.Level	Perceived level of open communication.
11	GroupCohesionLevel	Perceived level of group cohesion.

Survey measures
Pre-course survey measures	See Table A.11.
Post-course survey measures	See Tabel A.12.
We also extracted ten cumulative measures related to the perceived levels of the sub-components of three CoI presences (Table 2) from the 34 items of the CoI survey instrument included in the post-course survey. As each sub-component was measured by 3–4 survey items, the cumulative measures were calculated by averaging the responses of the associated survey items. The reliability analysis of CoI instrument (details reported in the Table A.10 in the Appendix), was assessed using Cronbach's α measure (Cronbach, 1951), which showed all survey items having good or excellent reliability scores, suggesting a reliable measurement instrument (Kline, 1999). The overall reliability indices for sub-scale of CoI presences were a bit lower but still at either acceptable level or better (except for the Group Cohesion dimension of Social Presence which had a questionable level of reliability). Looking at the cumulative reliability indices per each CoI presence (Table A.10), they were all very high, at 0.89 or above. Besides the analysis of Cronbach's α, we also examined item-rest correlations for each survey items, which were all sufficiently high and above the commonly used threshold of 0.30 from the literature (Everitt, 2002; Field, Miles, & Field, 2012).

Finally, we also examined students' final course grades and a subset of answers to relevant pre- and post-course survey questions. From 60 pre-course survey items, we extracted 39 relevant measures described in detail the Appendix (Table A.11). The pre-survey items that were excluded related to student's ethnicity, nationality, parents' nationalities, country of the last degree, current job industry, prospects of becoming a student at the Delft University of Technology, and how they found out about edX and this course. Similarly, from 97 post-course survey items – in addition to measures of three CoI presences – we also extracted 53 relevant measures described in the Appendix (Table A.12). The excluded post-course survey items were either open-ended feedback questions, questions about aspirations of becoming a student at the Delft University of Technology, questions about recommending the course and institution to others, or about taking more courses by the same institution.

4.3. Cluster analysis procedure
Cluster analysis procedure closely followed the approach of Kovanović et al. (2015), given a similar study design and goals. We used the agglomerative hierarchical clustering with the Euclidean distance and Ward's agglomeration criteria (Hastie, Tibshirani, & Friedman, 2013), a clustering method widely used for similar types of problems (e.g., Valle & Duffy, 2009; Wise et al., 2013). The cluster analysis procedure was implemented in R programming language version 3.3.2 (R Core Team, 2018) using hclust function from stats package.

Before cluster analysis, we standardized all 29 measures to make them equally important, as commonly done in cluster analysis (Hastie et al., 2013). To select the optimal number of clusters, we evaluated the height of the merging steps in the clustering dendrogram (Fig. 1) as an indicator of the clusters' relative similarity. After the optimal number of clusters had been selected, we summarized the identified clusters by computing their centroids (i.e., cluster “center points”), which were mean scores of all clustering measures for all cluster members.

Fig. 1
Download : Download high-res image (85KB)
Download : Download full-size image
Fig. 1. Dendrogram of student clustering.

4.4. Statistical analysis procedure
To assess the differences in different measures between the identified clusters, we used the multivariate analyses of variance (MANOVA) (Tabachnick & Fidell, 2007), as done by Kovanović et al. (2015). In particular, we used one-way MANOVAs with cluster assignment as a single independent measure, and with different trace data and outcome measures as the dependent measures. To protect from assumption violations, we used Pillai's trace statistic which is considered to be more robust against assumption violations than more commonly used Wilks' Λ statistic (Field et al., 2012). As a final protection measure, we compared the results of MANOVA with the results of robust rank-based variation of MANOVA (Nath & Pavur, 1985).

Significant MANOVAs were followed by a series of univariate analysis of variance (ANOVA) tests for each of the dependent measures. The use of univariate tests after a significant multivariate effect is considered a sound protection measure against the inflation of Type I error rate (Bock, 1985). However, as this is true only for dependent variables for which a significant multivariate effect was observed (Bray & Maxwell, 1985), we also used the Bonferroni correction procedure. Before ANOVAs, the homogeneity of variance was examined using Levene's test, and in cases in which it was significant, we used Kruskal-Wallis tests. Significant ANOVAs were followed by a Tukey pairwise posthoc analysis (or pairwise Kruskal-Wallis comparisons).

Finally, as univariate analyses assess the differences in each variable separately, to examine the multivariate differences between the clusters, we used discriminant factor analysis (DFA) (McLachlan, 2004], which is another commonly used technique to follow up MANOVA (Field et al., 2012). Within DFA, differences in one or more continuous dependent variables are examined between the levels of one independent categorical variable. DFA produces a set of linear combinations of continuous variables and is therefore similar in many ways to Principal Component Analysis (PCA) (Jolliffe, 2011). However, while PCA focuses on creating linear combinations of variables (called linear discriminant functions) that maximize explained variance (Jolliffe, 2011), DFA produces linear combinations which maximize the discrimination between the levels of the categorical independent variable. While DFA can also be used without MANOVA, it is commonly used as a MANOVA follow-up method for examining multivariate differences between different groups of independent measure, which are in our case different clusters of students. As such, a combined use of ANOVAs and DFA provides a comprehensive assessment of the multivariate differences between the identified clusters (Field et al., 2012).

We also examined cluster differences in students' final grades and their answers to pre- and post-course surveys (Table A.11 and Table A.12). The homogeneity of variance was examined using Levene's test, and depending on its result, we either used ANOVA or Kruskal-Wallis test. For the survey items with categorical responses, we used the chi-square test of independence to examine the differences in the category distributions between the identified clusters. To improve the accuracy of the chi-square estimates, as suggested by Field et al. (2012), if a category had the expected value less than one for one of the clusters (i.e., a particular cell of the contingency table), we either grouped the category with another category or completely removed it from the analysis. The same pre-processing of the categories was done when more than 20% of the category's cells had the expected values of less than five.

Due to a large number of survey items being examined, we opted not to use the popular correction procedures, as they – besides lowering the Type-I error rate – also inflate the Type-II error rate (Perneger, 1998), due to the correlated nature of different items (Conneely & Boehnke, 2007). While resampling-based corrections provide much higher statistical power by accounting for the correlations among multiple measures (see Ge, Dudoit, & Speed, 2003; Westfall & Young, 1993), they are primarily used when a single test (e.g., t-test) is repetitively performed on the same subjects data (e.g., a genome study). In the present study, those methods could not be applied as three different tests were used (i.e., chi-square, ANOVA, and Kruskal-Wallis) depending on the specifics of individual survey item. Finally, since survey analysis is primarily used for to provide additional information for cluster interpretation, we are more worried about the potentially inflated Type-II error rate rather than the inflated Type-I error rate.

In terms of implementation, all statistical analyses were conducted using R programming language (R Core Team, 2018), version 3.3.2 and its statistical packages. In addition to base R package distribution, we also used heplots package (Friendly, 2010) to calculate effect sizes, agricolae package (de Mendiburu, 2017) to conduct Kruskal-Wallis analyses, MASS package (Venables & Ripley, 2002) to conduct DFA analyses, rrcov package (Todorov & Filzmoser, 2009) to run robust rank-based MANOVA, and gmodels package (Warnes et al., 2018) to run chi-square analyses and posthoc contingency table analyses.

5. Results
5.1. Clustering results
Fig. 1 shows the resulting clustering dendrogram of the agglomerative hierarchical clustering with Euclidean distance and Ward's criteria on the 29 features of student course engagement. Starting with the two-cluster solution, we evaluated different clustering solutions by examining the differences in their cluster centroids and the average Silhouette index. While two-cluster solution achieved the highest average Silhouette index, we decided to use three-cluster solution in one which the larger of the two clusters was split into two smaller sub-clusters for two reasons. First, by looking at the centroids in the three-cluster solution (Fig. 2 and Table A.13), we can see substantial differences between the two sub-clusters among the features associated with individual learning measures which are aligned with the literature on students' MOOC study strategies (Hill, 2013; Kizilcec et al., 2013; Milligan et al., 2013; Sharma et al., 2015). Secondly, the use of two-cluster solution would result in one cluster accounting for 80% of the student population and a basic distinction between very highly engaged students and all other students which had very little practical value. As a result, in the rest of the paper, we used the three-cluster solution.

Fig. 2
Download : Download high-res image (349KB)
Download : Download full-size image
Fig. 2. Cluster analysis results. Features relating to the use of online discussions are shown in light gray.

To ease the interpretation of the results, we labeled each cluster (Table 3) based on their key characteristics (Fig. 2 and Table A.13). Cluster 1 was the largest, with 15,868 students, while clusters 2 and 3 were much smaller, with 3532 and 4248 students, respectively. Cluster 1 students (“limited users”), accounting for 67% of the learners in our sample, had the below average course engagement, focusing almost exclusively on video lectures, with insufficient engagement with course assignments and online discussions. By contrast, cluster 2 students (“selective users”), accounting for 15% of the dataset, exhibited much more engaged behavior with a more diverse set of tools and resources. Although they showed a higher volume of discussion-reading activity, they seldom actively engaged in the discussions. Finally, cluster 3 students (“broad users”), representing 18% of the course population, showed a strong course engagement spanning across all of the courses' components and activities. The broad users also showed active participation in online discussions, including the creation of new discussions, posting new comments, and upvoting and endorsing other's posts and comments.


Table 3. Key differences between the identified clusters.

Cluster #	Students (%)	Label	Characteristics
Cluster 1	15,868 (67%)	Limited users	Low course engagement,
No discussion activity.
Cluster 2	3532 (15%)	Selective users	Average course engagement,
Almost no discussion activity.
Cluster 3	4248 (18%)	Broad users	High course engagement,
Use of all course resources
Total	23,648 (100%)		
5.2. Cluster differences in trace data measures
A one-way MANOVA with cluster assignment as an independent measure, and 29 trace data metrics (Table 2) as the dependent measures showed a significant effect, Pillai's Trace =0.86, F(58,4,724) = 614.4, p < 0.0001. The multivariate effect size was η2 = 0.43 which is considered a large effect size (Cohen, 1988), and indicates that 43% of the variability in the canonically-derived dependent variable can be explained by the students' cluster assignment. A robust rank-based MANOVA (Nath & Pavur, 1985) confirmed these findings and yielded significant results, Wilks' Λrank = 0.20, χ2(44) = 3, 7590, p < 0.0001. It should be noted that before running the robust rank-based MANOVA, we removed seven discussion-related variables (i.e., DiscussionsStarted, QuestionsStarted,

CommentsEndorsementsGiven, CommentsEndorsementsReceived,

CommentsReceived, ThreadsCharsAvg, and UpvotesReceived) due to the low variability of scores for limited and selective users (Table A.14).

After the significant MANOVA, we conducted a series of posthoc ANOVA analyses for each of the dependent variables. As Levene's F test indicated a significant departure from homogeneity of variance for all trace data variables (mostly because of low scores for the limited users), we used one-way Kruskal-Wallis test with Bonferroni correction. The tests indicated significant differences for all trace data measures, p < 0.001. A follow-up pairwise analysis indicated all three cluster pairs (i.e., 1–2, 1–3, and 2–3) were significantly different regarding course access, assignments, video lectures, course navigation, and discussion access groups (Table 4), while for the variables about discussion contribution and discussion reputation groups, only the differences between the limited and broad users, and the selective and broad users were significant.


Table 4. Results of the ANOVA analysis for the between-cluster differences in technology use. Significance level α = 0.0125 (0.05/4).

Levene's	Kruskal-Wallis
#	Variable	F(2, 23640)	p	χ2(2)	p	Sig. pairs
Course access
1	OpenHome	200.1	<0.001	11,830	<0.001	1–2, 1–3, 2–3
2	OpenWiki	2976	<0.001	6351	<0.001	1–2, 1–3, 2–3
3	OpenProgress	1426	<0.001	10,620	<0.001	1–2, 1–3, 2–3
4	OpenSyllabus	82.81	<0.001	5986	<0.001	1–2, 1–3, 2–3
5	OpenCourseware	1843	<0.001	12,300	<0.001	1–2, 1–3, 2–3
6	OpenCoursewareItem	220	<0.001	14,310	<0.001	1–2, 1–3, 2–3

Assignments
7	AssignmentStart	4078	<0.001	14,640	<0.001	1–2, 1–3, 2–3
8	AssignmentSubmit	17,770	<0.001	14,380	<0.001	1–2, 1–3, 2–3

Video lectures
9	VideoLoad	6989	<0.001	13,030	<0.001	1–2, 1–3, 2–3
10	VideoPlay	231.1	<0.001	10,010	<0.001	1–2, 1–3, 2–3
11	VideoPause	943.6	<0.001	10,260	<0.001	1–2, 1–3, 2–3
12	VideoChangeSpeed	1831	<0.001	4352	<0.001	1–2, 1–3, 2–3
13	VideoShowSubs	9206	<0.001	9050	<0.001	1–2, 1–3, 2–3

Course navigation
14	ModuleNext	8459	<0.001	10,860	<0.001	1–2, 1–3, 2–3
15	ModulePrev	2521	<0.001	7796	<0.001	1–2, 1–3, 2–3
16	ModuleJump	6671	<0.001	12,590	<0.001	1–2, 1–3, 2–3

Discussion access
17	ThreadAccess	448.1	<0.001	11,120	<0.001	1–2, 1–3, 2–3
18	ThreadAccessInline	3652	<0.001	12,020	<0.001	1–2, 1–3, 2–3
19	DiscussionSearch	947.9	<0.001	5976	<0.001	1–2, 1–3, 2–3

Discussion contribution
20	DiscussionsStarted	323.8	<0.001	1733	<0.001	1–3, 2–3
21	QuestionsStarted	332.5	<0.001	1726	<0.001	1–3, 2–3
22	CommentsWritten	66.16	<0.001	4012	<0.001	1–3, 2–3
23	ThreadsCharsAvg	791.5	<0.001	2872	<0.001	1–3, 2–3
24	CommentsCharsAvg	1230	<0.001	4015	<0.001	1–3, 2–3
25	UpvotesGiven	265.6	<0.001	3050	<0.001	1–3, 2–3
26	CommentsEndors.Given	5.251	0.005	348.2	<0.001	1–3, 2–3

Discussion reputation
27	CommentsReceived	354.7	<0.001	2828	<0.001	1–3, 2–3
28	UpvotesReceived	49.14	<0.001	1477	<0.001	1–3, 2–3
29	CommentsEndors.Rec.	17.48	<0.001	527.7	<0.001	1–3, 2–3
We also used DFA as a follow up to the significant MANOVA analysis. DFA produced two linear discriminant functions (i.e., LD1 and LD2), which were linear combinations of dependent measures explaining the most variability in the independent variable (i.e., cluster assignment). The standardized loadings of the two discriminant functions are visualized in Fig. 3a and are presented in detail in Appendix (Table A.15). The LD1 function explained 97% of the variability in the cluster assignment, while LD2 explained the remaining 3%. The LD1 score was positively associated with the number of course logins, discussion posting activity, the use of course assignments, wiki, and navigation functionalities (“next module” and “jump to module”), and the number of video loading events. By contrast, LD1 was negatively related to the number of times course syllabus and progress pages were accessed. Similarly, the LD2 score was positively associated with the number of course logins, use of course wiki, navigation functionalities (“next module” and “jump to module” actions), and the number of received upvotes. LD2 was also highly negatively associated with the number of assignment submissions, received endorsements, and use of course progress page, syllabus, and discussions.

Fig. 3
Download : Download high-res image (809KB)
Download : Download full-size image
Fig. 3. Results of discriminant function analysis for the multivariate between-cluster differences in technology use.

The distribution of DFA scores (Fig. 3b, c, and d) indicates that the clusters were reasonably well separated, primarily alongside the LD1 function, (as expected given it explained 97% of the variance). The limited users had highly concentrated scores, whereas the scores of the selective and broad users were more dispersed. The selective users had slightly higher scores on LD1 and slightly lower LD2 scores than broad users. Finally, broad users had the most disperse scores on both discriminant functions, with generally much higher LD1 scores.

5.3. Cluster differences in CoI measures
5.3.1. Cognitive presence differences
The results of MANOVA, with the cluster assignment as a single independent measure and the four cumulative measures of the perceived levels of cognitive presence as the dependent measures, was significant, Pillai's Trace = 0.02, F(8, 1626) = 2.05, p = .038. This result was also confirmed by the statistically significant robust rank-based MANOVA (Wilks' Λrank = 0.98, χ2(8) = 15.85, p = .044). The multivariate effect size of η2 = 0.01 was obtained which is considered a small effect size (Cohen, 1988).

The significant MANOVA was followed by one-way ANOVAs with the Bonferroni correction for each of the four dependent measures of cognitive presence (Table 5). The homogeneity of variance assumption was confirmed using Levene's test for all four measures of cognitive presence (Table 5a). Significant ANOVA was observed for the differences in the perceived levels of resolution, F(2,815) = 5.86, p = .003 with a small effect size (Cohen, 1988) of η2 = 0.014. A pairwise comparison revealed significant differences between the limited and selective users, with the latter group having 0.21 point on average (on a 5-point Likert scale) higher perceived resolution levels than the former group. Similarly, we observed a statistically significant difference between the limited and broad users, with the latter group having 0.20 point on average higher perceived levels of resolution than the former group. The difference between selective and broad users was not significant.


Table 5. Cognitive presence analysis results.

a. Results of the ANOVA analysis for the differences between clusters regarding the perceived levels of cognitive presence. Significance level α = 0.0125 (0.05/4).

Levene's	ANOVAs
#	Variable	F(2,815)	p	F(2,815)	p	η2
1	TriggeringEventLevel	1.46	0.23	1.99	0.14	0.004
2	ExplorationLevel	0.66	0.52	0.42	0.65	0.001
3	IntegrationLevel	0.42	0.65	0.75	0.47	0.001
4	ResolutionLevel	0.87	0.42	5.86	0.003	0.014

b. Tukey posthoc pairwise comparisons of cluster centers.
Variable	Cluster Pair	Difference	P adjusted
ResolutionLevel	1–2	−0.21	0.023
1–3	−0.20	0.017
2–3	0.01	0.993
The DFA results (Fig. 4 and Table A.16) showed the that two discriminant functions (LD1 and LD2) accounted for 94% and 6% of the variability in the students' cluster assignments, respectively. The standardized loadings of the discriminant functions (Fig. 4a) revealed that the LD1 scores were positively associated with perceived values of integration, and negatively associated with perceived values of other three phases of cognitive presence, most notably the resolution phase. By contrast, the LD2 scores were negatively associated with the perceived levels of the first two phases of cognitive presence (triggering events and exploration) and positively associated with the perceived levels of the latter two phases (integration and resolution). The distribution of the discriminant functions scores (Fig. 4b) revealed a high overlap between the clusters, which is expected given the smaller effect size observed. The DFA results showed that limited users had higher LD1 scores than selective and broad users, whereas selective users had the most disperse scores, with the highest percentage of students with low LD1 scores. The LD2 scores indicate a minimal distinction between the clusters, with the selective users being slightly less likely to have very low LD2 scores.

Fig. 4
Download : Download high-res image (600KB)
Download : Download full-size image
Fig. 4. Results of the discriminant function analysis for the multivariate differences between clusters in the levels of cognitive presence.

5.3.2. Teaching and social presence differences
We also examined the cluster differences regarding the perceived levels of teaching and social presences. We conducted two MANOVA analyses with the cluster assignment as the single independent variable and the corresponding cumulative measures of teaching and social presences as corresponding dependent variables. Regarding the teaching presence, MANOVA analysis yielded non-significant results, Pillai's Trace = 0.006, F(6, 1652) = 0.83, p = .54 which were also confirmed by the non-significant robust rank-based MANOVA results, Wilks' Λrank = 0.99, χ2(6) = 6.83, p = .33. With respect to social presence, MANOVA also indicated no statistical differences between the three clusters, Pillai's Trace = 0.008, F(6, 1620) = 1.08, p = .37 which was also confirmed by non-significant robust rank-based MANOVA, Wilks' Λrank = 0.99, χ2(6) = 4.07, p = .66.

5.4. Final course grade differences
Finally, we conducted a one-way ANOVA with the final grade as the dependent variable and the cluster assignment as the single independent variable. As between-cluster variance of final grade was non-homogeneous (Levene's F(2, 23640) = 23.29, p < 0.001), we performed a Kruskal-Wallis test which revealed significant differences between the clusters regarding students' final grade, χ2(2) = 70.79, p < 0.001. Posthoc pairwise Kruskal-Wallis comparisons indicated significant differences between the limited and broad users, and also between the limited and selective users, with the limited users having higher mean ranks than other two groups (i.e., lower grade).

We examined the between-cluster differences in grade distributions. The distribution of grades in each cluster (Fig. 5) indicates the relationship between cluster assignment and grades was not linear, and there was also a clear spike in the 60%–70% grade range, likely caused by the threshold (60%) for receiving a course certificate. A chi-square independence test showed a significant relationship (χ2(18) = 65.63, p < 0.000001) between the cluster assignment and student grade decile.2

Fig. 5
Download : Download high-res image (133KB)
Download : Download full-size image
Fig. 5. Percentage of students in each cluster having the final course grade in a particular grade range (grades below 20% are omitted).

As a posthoc analysis, we looked at the cells in the contingency table (Table 6) with adjusted standardized residuals higher than 1.96, which correspond to the z-score value associated with α = .05 (Agresti, 2002). The results showed that limited users had a significantly higher proportion of 0%–10% grades, and significantly lower proportion of grades in 30%–40%, 60%–70%, 80%–90%, and 90%–100% ranges than other two clusters. By contrast, selective users had a statistically lower number of grades in the range 0%–10% range and statistically a higher number of grades in the 60%–70% range (which is the first range above the certificate threshold of 60%) than other clusters. Finally, broad users had also a statistically lower proportion of grades in the 0%–10% range and a higher proportion of grades in the top two ranges (80%–90% and 90%–100%) than other clusters. Interestingly, there was also significantly more broad users with the grades in 30%–40% range than those in the other two clusters (Table A.17).


Table 6. Differences in the final course grade between the identified clusters. Significant cells i.e., those with adjusted standardized residuals (Agresti, 2002) of 1.96 and above, are marked boldface.

Grade range	Value	Cluster 1	Cluster 2	Cluster 3	Grade total
(0%,10%]	Observed:	13,400	2892	3415	19,707
Expected:	13,223.56	2943.38	3540.06	
Adj. std. res.:	6.55	−2.52	−5.68	
(10%,20%]	Observed:	450	104	136	690
Expected:	463.00	103.06	123.95	
Adj. std. res.:	−1.07	0.10	1.21	
(20%,30%]	Observed:	227	52	74	353
Expected:	236.87	52.72	63.41	
Adj. std. res.:	−1.13	−0.11	1.48	
(30%,40%]	Observed:	189	59	85	333
Expected:	223.45	49.74	59.82	
Adj. std. res.:	−4.05	1.43	3.62	
(40%,50%]	Observed:	247	70	68	385
Expected:	258.34	57.50	69.16	
Adj. std. res.:	−1.24	1.80	−0.16	
(50%,60%]	Observed:	177	40	55	272
Expected:	182.51	40.63	48.86	
Adj. std. res.:	−0.72	−0.11	0.98	
(60%,70%]	Observed:	335	101	109	545
Expected:	365.70	81.40	97.90	
Adj. std. res.:	−2.83	2.38	1.25	
(70%,80%]	Observed:	204	51	62	317
Expected:	212.71	47.35	56.94	
Adj. std. res.:	−1.05	0.58	0.74	
(80%,90%]	Observed:	258	67	97	422
Expected:	283.17	63.03	75.81	
Adj. std. res.:	−2.63	0.55	2.71	
(90%,100%]	Observed:	381	96	147	624
Expected:	418.71	93.20	112.09	
Adj. std. res.:	−3.26	0.32	3.69	
Cluster total:	15,868	3532	4248	23,648
5.5. Analysis of cluster differences in course survey responses
We also examined the between-cluster differences in students' answers to the pre- (Table 7) and post-course surveys (Table 9). For numerical and Likert scale items, depending on Levene's test, we used one-way ANOVA or Kruskal-Wallis test, while for items with categorical responses, we used the chi-square independence test (Table A.18).


Table 7. Results of the analysis of pre-course survey (cf. Table A.11) differences between the identified clusters. Italics indicate marginal significant cluster differences (p < 0.1).

#	Variable	Test	p	Sig. pairs
Basic demographics
1	Gender	χ2(2) = 2.70	0.26	
2	Age	F(2, 3770) = 2.70	0.07	

Enrollment factors
3	ImpFactorPrevEdxExp	H(2) = 19.97	<.001	3–1
4	ImpFactorPrevTudelftExp	F(2, 4005) = 2.07	0.12	
5	ImpFactorTudelftRep	H(2) = 7.13	0.03	3–1
6	ImpFactorProfRep	F(2, 4017) = 3.87	0.02	3–1
7	ImpFactorEarningCert	F(2, 4022) = 0.05	0.95	
8	ImpFactorCourseUniq	F(2, 4024) = 0.42	0.66	
9	ImpFactorRecommendation	F(2, 4006) = 0.96	0.38	

Motivation
10	DeterminedToLearn	F(2, 4302) = 6.35	<.01	3–1, 2–1
11	DeterminedToComplete	F(2, 4302) = 1.50	0.22	
12	Hours	F(2, 4300) = 2.70	0.07	
13	CourseInterest	χ2(6) = 15.26	0.02	see Table 8.

Background knowledge
14	LevelOfEd	χ2(12) = 11.25	0.51	
15	EdBackFunctProgRel	χ2(6) = 2.28	0.89	
16	PrevExpInTheField	χ2(6) = 11.47	0.07	
17	Occupation	χ2(10) = 13.96	0.17	
18	YearsOfWorkExp	F(2, 2700) = 1.17	0.31	

Course expectations
19	ExpectFunChallenge	F(2, 4297) = 1.66	0.19	
20	ExpectRelevance	F(2, 4297) = 3.82	0.02	3–1
21	ExpectForumFeedbackTeam	F(2, 4290) = 0.62	0.54	
22	ExpectForumPeerInteract	F(2, 4292) = 0.08	0.91	

Study strategies
23	HowLikelyStudyGroup	F(2, 4294) = 4.20	0.01	3–1
24	HowLikelyMakeFrieds	F(2, 4293) = 0.75	0.47	
25	HowLikelyLookExtraMat	F(2, 4296) = 0.50	0.60	
26	HowLikelyPostDisc	F(2, 4293) = 2.19	0.11	
27	IntentShareExpertise	F(2, 3969) = 0.94	0.39	
28	PrefAloneVsWithOthers	χ2(2) = 3.06	0.22	
29	PrefTeacherVsStudentResp	F(2, 4017) = 2.17	0.11	

Study support
30	SupportBy	χ2(2) = 0.18	0.90	
31	SupportHow	χ2(2) = 3.09	0.80	
32	StudyDuringWorkHrs	χ2(2) = 1.27	0.53	

Experience with online environments
33	FreqSocialMediaUse	H(2) = 1.75	0.41	
34	FreqForumUse	F(2, 3979) = 0.69	0.50	
35	OnlineClassesTaken	H(2) = 3.67	0.15	
36	OnlineClassesComp	F(2, 3442) = 2.05	0.12	

Language fluency
37	EnglishFluencyLevel	F(2, 3836) = 0.86	0.42	
38	EnglishHowComfortable	F(2, 3836) = 2.27	0.10	
39	EnglishHowOften	F(2, 3836) = 2.03	0.13	
Results revealed significant differences between the clusters regarding the importance of their previous edX experience (ImpFactorPrevEdxExp), the reputation of the university offering the course (ImpFactorTudelftRep), and the reputation of the instructors teaching the course (ImpFactorProfRep). These factors had significantly higher importance for the broad users than for the limited users. The analysis also revealed significant differences relating to the primary course interests (CourseInterest, see Table 8), with limited users being significantly more driven by the general curiosity about the topic, whereas selective and broad users were significantly less motivated by the general curiosity about the topic. Broad users were also significantly driven by their current occupation, unlike limited users, who were significantly less driven by the same factor. Interestingly, the importance of obtaining a degree in the course domain was not significantly higher or lower for any of the clusters, indicating that it was occurring to the extent that was expected.


Table 8. Contingency table for the between-cluster differences in enrollment motivations. Significant cells i.e., those with adjusted standardized residuals (Agresti, 2002) of 1.96 and above, are marked boldface.

#	Course interest	Value	Cluster 1	Cluster 2	Cluster 3	Interest total
1	Ambition to get a degree	Observed:	176	52	56	284
Expected:	183.66	45.42	54.92	
Adj. std. res.:	−0.99	1.10	0.17	
2	Current occupation	Observed:	696	197	250	1143
Expected:	739.17	182.81	221.01	
Adj. std. res.:	−3.13	1.34	2.55	
3	Curiosity about the topic	Observed:	1776	401	489	2666
Expected:	1724.09	426.41	515.51	
Adj. std. res.:	3.49	−2.23	−2.16	
4	Other	Observed:	61	20	15	96
Expected:	62.08	15.35	18.56	
Adj. std. res.:	−0.23	1.31	−0.93	
Cluster total:	2709	670	810	4189
The pre-course survey analysis also revealed a statistically significant difference between students regarding their determination to learn the course content (DeterminedToLearn), with the broad users being significantly more determined to learn than limited users (the difference between selective and the limited users was marginal, p = .055). Results also showed significant differences with regards to students' expectations of course's practical relevance (ExpectRelevance). However, the posthoc analysis did not reveal any significant differences, with only a marginal difference between the limited and broad users, p = .061. Finally, the results revealed significant differences for studying in groups (HowLikelyStudyGroup), with broad users participating in study groups significantly more than limited users.

Finally, we analyzed the students' post-course survey responses. The results revealed significant differences regarding the use of additional learning materials (LookedAtExtraMaterials), with broad and selective users reporting higher use of additional resources than limited users. The results also revealed significant differences relating to the use of online discussions (SharedExpertiseWithOthers), with broad users reporting significantly higher levels of knowledge sharing than limited users; this self-reported result is consistent with the results of the objective measures of discussion participation. Interestingly, when asked whether they indented to share their knowledge more than they did (IntendedToShareMore), the limited users reported having strong intentions to do so, though they failed to realize during the course.

6. Discussion
6.1. RQ 1: study strategies in MOOC communities of inquiry
The results of the cluster analysis showed substantial and consistent differences in adopted study strategies between the three student groups. The observed differences are well aligned with the existing research of student strategies in both traditional (Kovanović et al., 2015; Lust et al., 2011; Lust et al., 2013a; Lust et al., 2013b; Valle & Duffy, 2009) and MOOC (Ferguson et al., 2015; Hill, 2013; Kizilcec et al., 2013; Li et al., 2015; Milligan et al., 2013; Rodrigues et al., 2016; Sharma et al., 2015) contexts.

Although the effect size of η2 = 0.43 can be interpreted as large (Cohen, 1988), it is slightly smaller than η2 = 0.54 reported within the traditional online setting (Kovanović et al., 2015). These differences are not surprising, as students in MOOC and traditional online courses have different internal and external conditions upon which they regulate their learning (Butler & Winne, 1995; Winne & Hadwin, 1998), different motivations (Kizilcec & Schneider, 2015), and different prior knowledge and demographic characteristics (Hennis et al., 2016).

The cluster centroids (Fig. 2 and Table A.13) demonstrated a consistent pattern of the insufficient tool use by the limited users, focused and strategic use by the selective users, and diverse and very intensive use by the broad users. The most significant differences concern the use of graded assignments, which is consistent with the existing accounts regarding the difference in student commitment to course assessment (Ferguson et al., 2015; Hill, 2013; Kizilcec et al., 2013; Sharma et al., 2015). On average, limited, selective, and broad users submitted one, 21, and 180 graded submissions, respectively. These differences indicate a firm commitment of the broad users towards course completion, which also resulted in a significantly higher number of broad users completing the course than those from the other two clusters (Table 6).

The results also revealed the significant differences in students' use of online discussions, which is well aligned with the existing literature on students' online discussion use (Kovanović et al., 2015; Wise et al., 2013). While limited users almost entirely ignored the discussions, selective users utilized discussions in a passive manner, by viewing other students' postings (Table A.13), and broad users actively participated in the discussions. As indicated by Wise, Hausknecht, and Zhao (2014), while active discussion participation is most beneficial for students' learning, even passive discussion participation can have substantial benefits for students learning.

Looking at students' survey responses (Table 7, Table 9), we can see that students' course participation strongly relates to variations in student motivations (Kizilcec & Schneider, 2015), conceptions of learning through discussions (Bliuc, Ellis, Goodyear, & Piggott, 2010), and self-regulation of own learning (Winne, 2006). However, while Kovanović et al. (2015) reported larger differences in the amount of participation in online discussions, the current study revealed larger differences relating to the use of static course resources. This is likely caused by the differences in course designs. Namely, discussion participation in the course from which the data were used in the Kovanović et al. (2015) study was planned and integrated into the design of learning activities. Previous research showed that such integration of discussions into the course design is directly related to the quantity and quality of students' discussion participation (Gašević, Adesope, Joksimović, & Kovanović, 2015).


Table 9. Results of the analysis of post-course survey (cf. Table A.12) differences between the identified clusters.

#	Variable	Statistical test	p	Sig. pairs
Course engagement level
1	ParticipationLevel	χ2(6) = 5.47	0.48	
2	HoursDedicated	F(2,856) = 0.20	0.82	
3	WorkedHard	F(2,925) = 0.18	0.83	
4	FamilyWorkObligationsAside	F(2,924) = 0.25	0.78	
5	PlannedAndOrganizedLearning	F(2,923) = 0.08	0.93	
6	LookedAtExtraMaterials	F(2,095) = 4.19	0.01	3–1, 3–2

Social interactions
7	ForumHowOften	F(2,908) = 0.67	0.51	
8	PostedInDiscussions	F(2,904) = 0.75	0.47	
9	SharedExpertiseWithOthers	F(2,906) = 3.72	0.02	3–1
10	IntendedToShareMore	H(2) = 14	<.001	1–3
11	ConnWithCourseTeam	F(2,904) = 1.03	0.36	
12	ConnWithOtherStudents	F(2,906) = 0.77	0.46	
13	ConnWithNewFriends	F(2,093) = 0.53	0.59	
14	WouldLikedConnectMore	F(2,905) = 2.95	0.053	
15	PartInStudyGroup	F(2,903) = 0.30	0.74	

Challenges to course participation
16	PersonalMedicalIssue	F(2,954) = 1.18	0.31	
17	ProfessionalObligation	F(2,956) = 0.03	0.97	
18	NotGettingFeedback	F(2,955) = 0.08	0.92	
19	FeelingLonely	F(2,954) = 0.11	0.90	
20	TechOrAccessibilityIssue	F(2,955) = 0.78	0.46	
21	KeepingUpWithCoursePace	F(2,954) = 1.63	0.20	
22	FamilyObligation	F(2,956) = 0.13	0.88	
23	AnotherCourseObligation	F(2,956) = 0.10	0.90	

Technical challenges to course participation
24	SlowInternet	F(2,944) = 0.03	0.97	
25	InternetNetworkProblems	F(2,944) = 0.52	0.60	
26	ElectricityNetworkProblems	F(2,944) = 2.12	0.12	
27	NotHavingAccessPers.Comp.	F(2,944) = 1.13	0.32	
28	HardwareProblems	F(2,943) = 1.27	0.28	
29	NotHavingMobileAccess	F(2,945) = 1.53	0.22	
30	AccessibilityUsability	F(2,944) = 2.50	0.08	

Course appropriate
31	CourseExpectationsRealistic	F(2,927) = 1.86	0.16	
32	CourseMeetYourExpectations	F(2,813) = 1.78	0.17	
33	LevelOfEnglishAppropriate	F(2,929) = 0.83	0.43	
34	CourseRelevantToOccupation	F(2,928) = 1.52	0.22	
35	PriorKnowledgeMadeItEasier	F(2,929) = 0.76	0.47	

Course support
36	CourseInspiredToStudy	F(2,895) = 1.45	0.23	
37	ForumWasHelpfulForMe	F(2,907) = 0.19	0.82	
38	OthersHelpedMeInTheCourse	F(2,906) = 1.35	0.26	
39	ReceivedSupportStudents	F(2,917) = 1.29	0.27	
40	ReceivedSupportCourseTeam	F(2,916) = 1.17	0.31	
41	SupportFromCourseTeamForums	F(2,862) = 0.25	0.78	
42	OthersCouldHelpMore	F(2,904) = 2.44	0.09	
43	ForumCouldBeMoreHelpful	F(2,905) = 2.81	0.06	

Course design and quality evaluation
44	DifficultyLevelOfTheCourse	F(2,883) = 1.11	0.33	
45	AmountOfWorkRequired	F(2,896) = 2.13	0.12	
46	PaceOfTheCourse	F(2,896) = 0.01	0.99	
47	DurationOfTheCourse	F(2,897) = 0.26	0.77	
48	LecturesExercisesBalance	F(2,894) = 1.55	0.21	
49	CourseOverallQuality	F(2,897) = 0.35	0.70	
50	AssignmentAndExamQuality	F(2,896) = 0.66	0.52	
51	VideoLecturesQuality	F(2,895) = 2.08	0.12	
52	FeedbackQuality	F(2,880) = 1.97	0.14	
53	EdxEaseOfUseAndQuality	F(2,897) = 1.46	0.23	
The DFA results showed that the LD1 function explained almost the entire variability (97%) in students' cluster membership (Fig. 3a and Table A.15). As expected, all limited users obtained similar DFA scores which suggest little variability in their behavior, whereas high variability of the DFA scores for the broad users was aligned with their highly active study approach. As LD1 is positively associated with all the essential course components (i.e., videos, assessment, and discussions) and positive learning strategies (e.g., active discussion participation, revisiting behavior, self-directed course navigation, and frequency of course access), it could be best described as the students' overall course engagement. While this is well aligned with Kovanović et al. (2015), where LD1 was also reflective of the students' overall engagement, the LD1 function in the present study accounted for a much higher percentage of variability in student cluster assignment (97% vs. 69%). Although this warrants further examination, it seems that the diversity in MOOC student motivations (Kizilcec et al., 2013; Kizilcec & Schneider, 2015) produces greater variability in students' overall course engagement.

The LD2 function of the DFA results can be best described as selectivity of course experience, which aligns with several studies in the MOOC context that also found clusters of selective and strategic learners (Ferguson et al., 2015; Hill, 2013; Kizilcec et al., 2013; Rodrigues et al., 2016; Sharma et al., 2015). It seems that students with high LDA2 scores mostly viewed the course as a set of resources and materials, accessing the video lectures selectively (given the positive coefficient for “jump module” action), without submitting the graded assignments nor accessing the course progress pages.

6.2. RQ 2: study strategies and development of CoI presences
The study results revealed significant between-cluster regarding the perceived levels of cognitive presence, which is aligned with the findings of Kovanović et al. (2015) in traditional online courses. However, contrary to the Kovanović et al. (2015 study, the effect size in the present study is small, and there are several possible explanations for it. First, given the open nature of MOOCs, the course had a less structured organization without strong expectations regarding discussion participation; discussion scaffolding is shown to affect students' cognitive presence development (Gašević et al., 2015). Secondly, there is an inherent self-selection bias of students who completed the survey. As more engaged students from all three clusters were probably more likely to fill out the survey, the between-cluster differences were likely rendered smaller than they really are. Finally, given the cluster differences in prior knowledge, motivation, and course expectations (Kizilcec et al., 2013; Kizilcec & Schneider, 2015), it is likely that students applied different standards of performance when answering the survey questions (Butler & Winne, 1995; Winne, 2006; Winne & Hadwin, 1998). For instance, the survey question #1 “Problems posed increased my interest in course issues” assessing the triggering event phase, was likely interpreted differently by students with different enrollment motivations or prior knowledge.

The univariate analysis showed significant between-cluster differences regarding the perceived levels of the resolution phase. In particular, the limited users had lower perceived levels of resolution than selective and broad users. This aligns with the existing reports of the challenges to reach higher levels of cognitive presence, and, in particular, the resolution phase (Garrison et al., 2001, Garrison et al., 2010). To reach higher levels of cognitive presence, the use of online discussions should also be better integrated into the course design (Penny & Murphy, 2009; Rovai, 2007), which includes planning and effective scaffolding of the discussion participation (Gašević et al., 2015). Finally, the present results align with the results of a recent exploratory factor analysis (Kovanović et al., 2018) of the CoI instrument in MOOCs. The study by Kovanović et al. (2018) revealed different dynamics of the resolution phase in MOOCs in comparison to those of conventional online course. The different dynamics was likely due to the shorter duration, limited role of instructor, and more diverse student body in MOOCs in comparison to the conventional online courses.

The DFA results (Fig. 4 and Table A.16) indicated that the LD1 function explained almost all of the variability in students' cluster membership (94%). As LD1 was positively associated only with the integration phase, high LD1 scores indicate a high level of integration relative to the other three phases. This contrasts the findings of Kovanović et al. (2015) study where all LD1 coefficients had the same direction. The limited users, found in the current study, exhibited slightly higher LD1 scores than the other two groups, suggesting a focus on the integration phase whereas the other two groups more emphasized the resolution phase. A likely interpretation for the DFA results is that the differences between clusters regarding prior knowledge, motivation, and course expectations resulted in the learning behavior that led to different levels of cognitive presence. For instance, with primarily professional interests in the course by selective and broad users, their higher emphasis on the resolution phase is not surprising. Similarly, primarily a general interest in the course by the limited users is possibly well aligned with the greater emphasis on the integration phase. Of course, this interpretation warrants future research and validation.

Interestingly, our analysis of cluster differences failed to find any significant differences regarding teaching and social presences. While further research is necessary, there are several potential explanations. First, given the self-selection bias in students' survey responses, it is likely that more active and engaged students in all three groups completed the questionnaire, thus rendering the between-cluster differences smaller than they actually are. Secondly, due to a large number of students, MOOCs typically capitalize on pre-recorded lectures and automated assessments, with limited facilitation and direct instruction of students by the instructors. Hence, as students were exposed to similar course experiences, there was a reduced possibility of the differences in their perceptions of teaching presence. Finally, the existing literature recognizes the critical importance of cohort size and course duration for the development of social presence. Significant challenges are reported in the literature in connection with the development of social presence in large cohorts and short courses (Akyol & Garrison, 2008; Garrison, 2011; Poquet, 2017). As the course under study was eight-weeks long and with more than 20,000 students, it is likely that the majority of students were not able to develop their social presence on to desired levels.

6.3. RQ 3: study strategies and final course grade differences
The study results also revealed the significant between-cluster differences in final course grade. Our findings indicate the significant differences between limited users and the other two groups of students. These findings are not surprising, given that limited users showed the less active use of course assessment and lower ambition to complete the course, arising likely from being primarily driven by the general curiosity about the topic (Table 7). These findings are also well-aligned with the existing studies in the MOOC literature which indicated significant differences in course final outcomes between different groups of students (Bergner et al., 2015; Ferguson et al., 2015; Li et al., 2015; Rodrigues et al., 2016; Sharma et al., 2015). Similar findings are also reported in the context of traditional online courses, with less active and engaged students achieving significantly lower course grade (Bovo et al., 2013; Lust et al., 2011; Lust et al., 2013a; Yen & Lee, 2011).

While our results revealed between-cluster differences in final course grade, the analysis of grade distribution is more interesting. As shown by the final course grade distribution (Fig. 5), there is a clear spike just above the course certificate threshold of 60% suggesting a clear focus of many students on obtaining a course certificate in all three clusters. However, the relative increase in the percentages of students below and above the threshold is the strongest for selective users, which is aligned with their goal-oriented course participation. It is also interesting to note that the percentage of students in the deciles above the certificate threshold is increasing across all three clusters, suggesting that many of the students who obtain the course certificate, they often strive to achieve a high grade. Looking at the percentages of students in these three deciles above the threshold, we see that broad users show a consistent linear increase across all three deciles, whereas selective users show a steep jump between the last two steps. While this warrants further examination, it is likely caused by the same goal-oriented nature of selective users which pushes them above the certificate threshold.

Our results also reveal an interesting spike of broad users in the 30%–40% grade range (Fig. 5), which is also supported by the contingency table analysis (Table 6). Given that homework accounted for 40% of the final grade, it is likely that this spike is caused by the group of broad users who did not complete lab assignments, which were more demanding, and instead only focused on multiple-choice homework, which required less time investment. The further analysis is needed to examine whether this is an indicator of the two sub-types of broad users, where one subgroup focuses solely on homework assignments whereas the other subgroup focuses on completion of all course assessment pieces.

6.4. Cluster interpretations
6.4.1. Cluster one: limited users
With 15,868 students, limited users were the largest student group, accounting for two-thirds of the student population. Overall, limited users exhibited a below-average course engagement and insufficient use of the available technological affordances (Fig. 2 and Table A.13). As such, limited users were most similar to the “disengaged” and “auditing” students reported in the previous studies (see Section 2).

As the way in which students learn online indicates their commitment to learning (Valle & Duffy, 2009), it is likely that the limited users lacked sufficient levels of intrinsic motivation to engage in learning. They exhibited lower determination to learn than the other two student groups, and primarily focused on video lectures, without completing the graded assignments, without strong intentions to seek and use additional learning materials or join study groups (Table 8). Not surprisingly, a vast majority of the limited users obtained final grades in the range between 0%–10% (Table 6). Given their very limited engagement in learning activities, it is also likely that the limited users had surface approaches to learning (Trigwell & Prosser, 1991).

The limited users also reported lower perceived levels of resolution phase than other students (Table 5b). This was likely caused by their limited motivation to participate in the course. They also lack work-related interests in the course which would provide students with incentives to further develop their cognitive presence. Another contributing factor was their focus on content-related activities, without active participation in online discussions. Interestingly, while they did not participate in the forums, they reported intentions to participate in online discussions. Hence, it is likely that these students had insufficient self-regulating skills required for successful participation in online discussions (Hew, Cheung, & Ng, 2010). This is consistent with the existing research showing the challenges with learning regulation by a significant portion of the student population (Dunlosky & Lipko, 2007; Kovanović et al., 2015; Lust et al., 2013a). As such, these students might benefit from instructional support regarding the interactions with other students (Cho & Kim, 2013).

6.4.2. Cluster two: selective users
Selective users, which represented 15% of the course students, were primarily characterized by their focus on obtaining a course certificate (Table 6). They exhibited active and selective use of the available tools and resources, focusing primarily on video lectures and graded assignments (Fig. 2 and Table A.13). Although significantly more active than limited users, they seldom participated actively in discussions, and treated them mostly as another “static resource.” Given their goal-oriented use of the available affordances, they show similarities with “selective” students from the published literature (see Section 2). They were also less likely to participate due to general interest in the course subject and also less likely to seek extra learning materials (Table 2).

Selective users also showed higher perceived levels of the resolution phase than limited users and similar levels as broad users. The higher levels of cognitive presence are the result of their greater commitment to learning and more active participation, likely arising from professional interests in the course topic other than just the personal curiosity about the topic.

While the purposeful and selective use of different tools and resources is an indicator of the ability to regulate learning, it should be noted that it can be combined with both surface and deep approaches to learning (Entwistle, 2009). In the case of selective users, their highly strategic and focused use of tools indicates that these students likely adopted performance goal orientation (Phan, 2008), coupled with surface approaches to learning (Trigwell & Prosser, 1991) with the primary intention of obtaining the course certificate. Moreover, having mostly passive use of discussions, selective users likely did not perceive participation in discussions as a valuable learning activity (Dennen, 2008) and had a fragmented notion of learning in online discussions (Bliuc et al., 2010). As with the limited users, it is likely that instructional interventions which target their interaction with other students can be beneficial (Cho & Kim, 2013). As shown by Wise et al. (2013), for students with performance goal orientations, designing and embedding active participation in the activity requirements can be particularly beneficial and encourages the development of cognitive presence and deeper knowledge comprehension.

6.4.3. Cluster three: broad users
Broad users, representing 18% of the course participants, were primarily characterized by their highly active course participation and broad use of the available tools and resources (Fig. 2). They engaged significantly more than limited and selective users with all available tools and resources, including online discussions. They are most similar to “engaged” students reported in the literature (see Section 2). Not surprisingly, they were also more likely to achieve highest grades (i.e., in 80%–100% range) and less likely to obtain grades in 0%–10% range than the students in other two clusters.

When deciding whether to enroll in a course, broad users put a significant weight on their previous edX experiences and the reputation of course instructors and their institution (Table 7). Also, their current professional occupation, rather than a general curiosity about the topic, represents a key motivational factor for the broad users (Table 8). They were significantly more determined to learn and join study groups than limited users and more likely to join study groups and employ additional learning materials than both limited and selective users (Table 9). This indicates broad users' awareness of successful study strategies (Lust et al., 2013a) and the high level of learning regulatory behavior, which are both associated with better learning outcomes (Winne & Hadwin, 1998). As such, it is likely that broad users adopted mastery goal orientations, and deep approaches to learning (Bliuc et al., 2010), which are both associated with higher course success (Trigwell & Prosser, 1991).

Similar to selective users, broad users exhibited higher perceived levels of resolution than limited users. This is not surprising, given their greater commitment to learning, more focused interest in the course, use of additional course materials, and active participation in online discussions than that of the limited users. With the active use of online discussions, it is likely that broad users had cohesive conceptions of learning in online discussions (Bliuc et al., 2010) and perceive them as a valuable learning activity. As student-led discussions were shown to better foster cognitive presence development than instructor-led discussions (De Wever, Keer, Schellens, & Valcke, 2010; Schellens, Keer, Wever, & Valcke, 2007), the broad users are good candidates for student moderators who will guide discussions into productive directions and assist other students from the other two clusters (Kovanović et al., 2016; Schrire, 2006).

6.5. Limitations
There are several limitations of the present study. First, to identify different study strategies we adopted a particular cluster analysis approach, and it is likely that the use of a different clustering method would result in different clustering results. Deciding how many clusters (i.e., study strategies) to use is also inherently subjective and can also have a significant impact on the clustering results. While our clustering approach was informed by the existing literature [Valle & Duffy, 2009; Wise et al., 2013; Kovanović et al., 2015), the exploratory nature of cluster analysis leaves space for different results and their interpretation. Second, as the study is correlational in nature, it is not possible to establish a causal link between the adoption of different study strategies and the development of cognitive presence. Third, the use of survey instruments in the study raises issues of self-selection bias, despite a high number of participants in our study. Fourth, in our study, we conducted a significant number of statistical tests which increase the chances of Type I errors, although we used correction procedures where possible. While the results of our analyses provide a coherent picture of student learning strategies which is well aligned with the existing literature, there is still a possibility that some of the significant findings are the result of chance alone and not representative of the actual cluster differences. Finally, as with the majority of studies of human behavior, there are many potential factors affecting student learning in online contexts which are not captured by the trace nor survey data and thus, remained unaccounted for in the present study.

7. Conclusions
There are several significant implications for MOOC research and practice of the present study. First, our results show that a simple provision of the different technological affordances and resources is not sufficient to secure their successful use by the students. As noted by Clarebout et al. (2013) and Lust et al., 2012, Lust et al., 2013a, a majority of students do not use the available tools appropriately, signaling the lack of metacognitive capacity, skills, and motivation to use the provided technology effectively. In MOOCs, this seems to be even more pronounced than in the case of traditional, for-credit, online courses, given the greater diversity in the student body, and the lack of strong extrinsic motivations, in particular, course credits.

We can recommend that the three different study strategies identified require different instructional support and assistance. It seems that the limited users would more benefit more from interventions focusing on their limited participation in online discussion rather than interventions focusing on their use of course assignments, as completing assignments is not in the scope of their course participation. Although selective users could also benefit from the interventions focused on the discussion participation, they would likely more benefit from the interventions promoting mastery learning, given their performance-oriented learning focus. Finally, the broad users, with their strong course commitment and active engagement would likely benefit from taking more active roles in the course, for example, by being promoted to student moderators and providing more personalized support to the less engaged course participants.

The present study provides an example of how the analytics methods developed for traditional online setting can be successfully utilized within MOOCs. In this study, we adopted the procedure from the study by Kovanović et al. (2015), initially developed for the analysis of a traditional, for-credit online course. With the rising importance of study replication and validation of research findings in new contexts, the present study represents a particular example of successful replication of an existing method in MOOC setting.

From the research perspective, the present study shows significant challenges related to the use of survey data with the diverse body of students. The self-selection bias and different standards and baselines for question interpretation seem particularly pronounced in MOOC research. These factors likely contributed to the smaller effect size found in this study than in related studies reported in the literature (Kovanović et al., 2015). While these issues with survey instruments are all well known, the diversity of MOOC participants makes these issues even more emphasized than they are in other educational settings.

Appendix A. Additional materials
In this section, we provide more details about the adopted survey instruments and more detailed results of the performed analyses. First, the results of the reliability analysis for the CoI survey instrument are shown on Tabel A.10. Next, the relevant parts of pre-course and post-course (Tabel A.12) surveys are provided in Tabel A.11 and Tabel A.12, respectively. We also listed the detailed description of cluster centroids (Tabel A.13), and mean values of all outcome measures for the three identified clusters (Tabel A.14). Similarly, the list of standardized discriminant function loadings for the technology use and cognitive presence measures are given in Tabel A.15 and Tabel A.16, respectively. Finally, the mean scores for both pre-course and post-course survey questions for the students in the three clusters are shown in Tabel A.17 and Tabel A.18, respectively.


Table A.10. Results of CoI survey reliability analysis.

Cronbach's α	Item-rest r
Scale	Sub-scale	Item	Scale avg.	Sub-scale avg.	Item
Teaching Presence	0.94			0.71		
 Design & Organization		0.87			0.76	
 Item 1			0.93			0.70
 Item 2			0.93			0.74
 Item 3			0.93			0.73
 Item 4			0.94			0.55
 Facilitation		0.77			0.87	
 Item 5			0.93			0.76
 Item 6			0.93			0.78
 Item 7			0.93			0.75
 Item 8			0.93			0.82
 Item 9			0.94			0.54
 Direct Instruction		0.90			0.74	
 Item 10			0.93			0.73
 Item 11			0.93			0.81
 Item 12			0.93			0.67
 Item 13			0.94			0.57
Social Presence	0.89			0.65		
 Affective Expression		0.83			0.57	
 Item 14			0.88			0.58
 Item 15			0.89			0.53
 Item 16			0.89			0.47
 Interactivity		0.72			0.69	
 Item 17			0.87			0.70
 Item 18			0.86			0.78
 Item 19			0.86			0.80
 Group Cohesion		0.66			0.77	
 Item 20			0.87			0.71
 Item 21			0.88			0.66
 Item 22			0.88			0.64
Cognitive Presence	0.91			0.64		
 Trig. Event		0.79			0.71	
 Item 23			0.90			0.68
 Item 24			0.90			0.70
 Item 25			0.90			0.73
 Exploration		0.76			0.80	
 Item 26			0.91			0.50
 Item 27			0.90			0.57
 Item 28			0.91			0.45
 Integration		0.83			0.62	
 Item 29			0.90			0.66
 Item 30			0.90			0.72
 Item 31			0.90			0.73
 Resolution		0.84			0.62	
 Item 32			0.90			0.70
 Item 33			0.90			0.62
 Item 34			0.90			0.64

Table A.11. Pre-course survey questions used in the present study.

#	Variable	Question text	Type
Basic demographics
1	Gender	What is your gender?	M|F
2	Age	How old are you?	Numeric
Enrollment factors	Group: How important were the following factors for your choice for this course?	
3	ImpFactorPrevEdxExp	My experience with other courses on edX	5-item LS
4	ImpFactorPrevTudelftExp	My experience with other DelftX courses on edX	5-item LS
5	ImpFactorTudelftRep	The status and reputation of Delft University of Technology	5-item LS
6	ImpFactorProfRep	The status and reputation of the professor(s) involved	5-item LS
7	ImpFactorEarningCert	The possibility of earning a Statement of Accomplishment / Verified Certificate	5-item LS
8	ImpFactorCourseUniq	The uniqueness of this course	5-item LS
9	ImpFactorRecommendation	A recommendation from someone else	5-item LS

Motivation
10	DeterminedToLearn	To what extent are you determined to learn (more) about functional programming?	5-item LS
11	DeterminedToComplete	To what extent are you determined to complete this course?	5-item LS
12	Hours	How many hours per week do you expect to dedicate to this course?	5-item LS
13	CourseInterest	What describes your interest for registering for this course?	6-item MC

Background knowledge
14	LevelOfEd	What is the highest level of education that you have?	9-item MC
15	EdBackFunctProgRel	Is your educational background related to (functional) programming?	4-item MC
16	PrevExpInTheField	Do you have professional experience in this field?	4-item MC
17	Occupation	Which of the following best describes your occupation?	8-item MC
18	YearsOfWorkExp	How many years of working experience do you have?	Numeric
Course expectations	Group: Please rate how important each of the following aspects is for you in this course:	
19	ExpectFunChallenge	The fun and challenge the course offers	5-item LS
20	ExpectRelevance	The practical relevance of the course for me	5-item LS
21	ExpectForumFeedbackTeam	The feedback of the course team on the forum	5-item LS
22	ExpectForumPeerInteract	The interaction with peers on the forum	5-item LS

Study strategies
23	HowLikelyStudyGroup	How likely do you think it is that you will join a study group?	5-item LS
24	HowLikelyMakeFrieds	How likely do you think it is that you will make friends with others in this course?	5-item LS
25	HowLikelyLookExtraMat	How likely do you think it is that you will look at extra materials for this course?	5-item LS
26	HowLikelyPostDisc	How likely do you think it is that you will post or comment on the discussion board?	5-item LS
27	IntentShareExpertise	How much do you intent to share your expertise with other students in this class?	5-item LS
28	PrefAloneVsWithOthers	Do you prefer to do this course alone or with others?	Alone|With others
29	PrefTeacherVsStudentResp	To what extent do you prefer a course that is organized by a teacher or institution versus a course where you as a student are responsible for organizing your own learning activities?	Numeric (0−100)

Study support
30	SupportBy	Who supports you in doing this course?	6-item MC
31	SupportHow	How are you supported?	6-item MC
32	StudyDuringWorkHrs	Are you allowed to do this course during work hours?	Yes|No

Experience with online environments
33	FreqSocialMediaUse	How often do you use social media (i.e. Facebook, Twitter, Google+, Weibo, etc.)?	5-item LS
34	FreqForumUse	How often do you contribute questions or answers to online forums?	5-item LS
35	OnlineClassesTaken	How many online classes have you ever taken before?	5-item LS
36	OnlineClassesComp	How many online classes have you ever completed?	5-item LS

Language fluency
37	EnglishFluencyLevel	What is your present level of English fluency?	5-item LS
38	EnglishHowComfortable	How comfortable are you communicating in English?	5-item LS
39	EnglishHowOften	How often do you communicate in English?	5-item LS

Table A.12. Post-course survey questions used in the present study.

#	Variable	Question text	Type
Course engagement level
1	ParticipationLevel	Since the start of this class, how would you describe your participation level?	6-item MC
2	HoursDedicated	How many hours did you dedicate to this course per week?	Numeric
3	WorkedHard	Do you agree: “I worked really hard at things I didn't understand.”	5-item LS
4	FamilyWorkObligationsAside	Do you agree: “I had to put aside family/work obligations because of the course.”	5-item LS
5	PlannedAndOrganizedLearning	Do you agree: “I planned and organized my learning well.”	5-item LS
6	LookedAtExtraMaterials	How often have you looked at extra materials for this course?	5-item LS

Social interactions
7	ForumHowOften	How often did you take a look and read discussions on the forum?	5-item LS
8	PostedInDiscussions	Howe often have you posted a comment or question on the course discussion board?	5-item LS
9	SharedExpertiseWithOthers	Do you agree: “I shared my expertise with other students.”	5-item LS
10	IntendedToShareMore	Do you agree: “I intended to share my expertise with other students more.”	5-item LS
11	ConnWithCourseTeam	How often have you contacted the instructors?	5-item LS
12	ConnWithOtherStudents	Do you agree: “I connected with other students.”	5-item LS
13	ConnWithNewFriends	How often have you connected with new friends from this course?	5-item LS
14	WouldLikedConnectMore	Do you agree: “I would have liked to connect with other students more.”	5-item LS
15	PartInStudyGroup	How often have you participated a study group?	5-item LS
Challenges to course participation	Group: Did any of the following negatively affect your participation in the course?	
16	PersonalMedicalIssue	A personal or medical issue	5-item LS
17	ProfessionalObligation	Important professional obligations	5-item LS
18	NotGettingFeedback	Not getting (the right) feedback when I needed it	5-item LS
19	FeelingLonely	The feeling of loneliness or missing interaction with peers	5-item LS
20	TechOrAccessibilityIssue	Technical or accessibility issues	5-item LS
21	KeepingUpWithCoursePace	Problems with having to keep up with the pace of the course	5-item LS
22	FamilyObligation	Important family obligations	5-item LS
23	AnotherCourseObligation	Obligations from another (online) course or study	5-item LS
Technical challenges to participation	Group: how much did each of the technical issues affect your participation?	
24	SlowInternet	Slow Internet	5-item LS
25	InternetNetworkProblems	Internet network problems	5-item LS
26	ElectricityNetworkProblems	Electricity network problems	5-item LS
27	NotHavingAccessPers.Comp.	Not having access to a personal computer	5-item LS
28	HardwareProblems	Hardware problems	5-item LS
29	NotHavingMobileAccess	Not being able to access it on my mobile device	5-item LS
30	AccessibilityUsability	Usability or accessibility of the website (i.e. because of a disability)	5-item LS
Course appropriate	Group: How much do you agree with the following statements?	
31	CourseExpectationsRealistic	“My expectations about the course were realistic”	5-item LS
32	CourseMeetYourExpectations	Did the course meet your expectations?	3-item LS
33	LevelOfEnglishAppropriate	“My level of English was appropriate for this course.”	5-item LS
34	CourseRelevantToOccupation	“The course was relevant for my profession or current occupation.”	5-item LS
35	PriorKnowledgeMadeItEasier	“My prior knowledge made it easier to complete/understand assignments/lectures.”	5-item LS

Course support
36	CourseInspiredToStudy	Do you agree: “This MOOC inspired me to continue studying in this field.”	5-item LS
37	ForumWasHelpfulForMe	Do you agree: “The course forum was helpful for me.”	5-item LS
38	OthersHelpedMeInTheCourse	Do you agree: “Others helped me in this course.”	5-item LS
39	ReceivedSupportStudents	Do you agree: “I received support from other students.”	5-item LS
40	ReceivedSupportCourseTeam	Do you agree: “I received support from the Teaching Assistants or teacher(s).”	5-item LS
41	SupportFromCourseTeamForums	How would you rate the support from the course team on the forum?	5-item LS
41	OthersCouldHelpMore	Do you agree: “I think others could have helped me more in this course.”	5-item LS
43	ForumCouldBeMoreHelpful	Do you agree: “I think the course forum could have been more helpful for me.”	5-item LS

Course design and quality evaluation
44	DifficultyLevelOfTheCourse	The difficulty level of the course was:	5-item LS
45	AmountOfWorkRequired	The amount of work required for the course was:	5-item LS
46	PaceOfTheCourse	The pace of the course was:	5-item LS
47	DurationOfTheCourse	The duration of the course (number of weeks) was:	5-item LS
48	LecturesExercisesBalance	How would you rate the balance between lectures and exercises?	5-item LS
49	CourseOverallQuality	How would you rate the overall quality of the course?	5-item LS
50	AssignmentAndExamQuality	How would you rate the quality of the assignments and exams?	5-item LS
51	VideoLecturesQuality	How would you rate the quality of the video lectures?	5-item LS
52	FeedbackQuality	How would you rate the feedback on completed quizzes and assignments?	5-item LS
53	EdxEaseOfUseAndQuality	How would you rate The ease of use and overall quality of the edX platform?	5-item LS

Table A.13. The centroids of the three identified clusters.

#	Variable	Cluster 1
(N = 15,868)	Cluster 2
(N = 3,532)	Cluster 3
(N = 4,248)
Course access
1	OpenHome	2.55 (2.37)	7.91 (5.78)	27.94 (112.26)
2	OpenWiki	0.28 (0.55)	1.20 (1.58)	2.79 (3.63)
3	OpenProgress	0.55 (1.34)	3.47 (6.01)	26.76 (53.78)
4	OpenSyllabus	0.49 (1.12)	1.41 (1.76)	4.67 (33.84)
5	OpenCourseware	1.92 (2.12)	7.40 (5.82)	31.54 (44.43)
6	OpenCoursewareItem	5.38 (12.92)	27.16 (18.46)	113.35 (351.48)

Assignments
7	AssignmentStart	0.60 (2.10)	7.83 (11.18)	38.66 (41.44)
8	AssignmentSubmit	1.13 (4.68)	20.72 (30.04)	179.20 (109.74)

Video lectures
9	VideoLoad	3.74 (4.83)	21.66 (14.18)	60.99 (37.27)
10	VideoPlay	8.26 (27.82)	44.85 (67.97)	215.82 (1010.97)
11	VideoPause	3.74 (9.09)	29.12 (114.00)	90.70 (175.11)
12	VideoChangeSpeed	0.29 (0.97)	2.02 (4.62)	6.11 (12.64)
13	VideoShowSubs	2.76 (4.01)	15.88 (13.06)	41.64 (36.23)

Course navigation
14	ModuleNext	1.74 (2.45)	10.33 (7.63)	30.95 (24.74)
15	ModulePrev	0.09 (0.34)	1.05 (1.57)	3.54 (6.35)
16	ModuleJump	2.55 (4.84)	17.10 (16.25)	62.34 (51.50)

Discussion access
17	ThreadAccess	0.49 (2.94)	3.80 (9.53)	70.97 (307.41)
18	ThreadAccessInline	0.05 (0.26)	0.76 (1.33)	6.61 (9.97)
19	DiscussionSearch	0.03 (0.33)	0.12 (0.54)	3.71 (11.72)

Discussion contribution
20	DiscussionsStarted	0.00 (0.00)	0.00 (0.02)	0.16 (0.90)
21	QuestionsStarted	0.00 (0.00)	0.00 (0.00)	0.14 (0.75)
22	CommentsWritten	0.00 (0.03)	0.00 (0.07)	1.62 (19.60)
23	ThreadsCharsAvg	0.00 (0.00)	0.03 (2.02)	61.57 (215.54)
24	CommentsCharsAvg	0.02 (0.81)	0.11 (2.08)	55.17 (154.86)
25	UpvotesGiven	0.00 (0.06)	0.03 (0.28)	1.10 (6.61)
26	CommentsEnd.Given	0.00 (0.00)	0.00 (0.00)	0.18 (7.81)

Discussion reputation
27	CommentsReceived	0.00 (0.00)	0.00 (0.02)	1.62 (8.50)
28	UpvotesReceived	0.00 (0.00)	0.00 (0.00)	0.86 (12.10)
29	CommentsEnd.Rec.	0.00 (0.00)	0.00 (0.00)	0.15 (3.44)

Table A.14. Scores on the outcome variables of the identified clusters.

#	Variable	Cluster 1	Cluster 2	Cluster 3
Course grades
1	FinalGrade	0.09 (0.23)	0.10 (0.24)	0.11 (0.26)

CoI: Perceived levels of cognitive presence
2	TriggeringEventLevel	3.8 (0.81)	3.9 (0.72)	4.0 (0.73)
3	ExplorationLevel	3.6 (0.67)	3.6 (0.66)	3.7 (0.66)
4	IntegrationLevel	3.7 (0.74)	3.7 (0.67)	3.7 (0.70)
5	ResolutionLevel	3.5 (0.84)	3.7 (0.85)	3.7 (0.79)

CoI: Perceived levels of teaching presence
6	Org.DesignLevel	3.9 (0.77)	3.9 (0.78)	4.0 (0.73)
7	FacilitationLevel	3.7 (0.77)	3.7 (0.75)	3.8 (0.73)
8	DirectInst.Level	3.4 (0.83)	3.4 (0.84)	3.5 (0.78)

CoI: Perceived levels of social presence
9	AffectiveExp.Level	2.9 (0.69)	3.0 (0.73)	3.0 (0.67)
10	OpenComm.Level	3.2 (0.78)	3.3 (0.78)	3.2 (0.81)
11	GroupCohesionLevel	3.0 (0.60)	3.2 (0.59)	3.1 (0.60)

Table A.15. Standardized DFA of clustering variables.

#	Variable	LD1	LD2
Course access
1	OpenHome	0.21	0.31
2	OpenWiki	0.12	0.40
3	OpenProgress	−0.16	−0.26
4	OpenSyllabus	−0.32	−0.33
5	OpenCourseware	−0.04	−0.22
6	OpenCoursewareItem	−0.02	−0.14

Assignments
7	AssignmentStart	0.30	−0.01
8	AssignmentSubmit	0.45	−0.83

Video lectures
9	VideoLoad	0.24	0.18
10	VideoPlay	0.01	−0.12
11	VideoPause	0.02	0.05
12	VideoChangeSpeed	0.09	0.05
13	VideoShowSubs	0.08	0.13

Course navigation
14	ModuleNext	0.43	0.73
15	ModulePrev	−0.01	−0.27
16	ModuleJump	0.27	0.55

Discussion access
17	ThreadAccess	−0.08	−0.10
18	ThreadAccessInline	−0.17	−0.29
19	DiscussionSearch	−0.08	−0.11

Discussion contribution
20	DiscussionsStarted	−0.00	0.01
21	QuestionsStarted	0.05	−0.06
22	CommentsWritten	0.19	0.07
23	ThreadsCharsAvg	0.10	−0.09
24	CommentsCharsAvg	0.15	−0.14
25	UpvotesGiven	−0.04	−0.04
26	CommentsEnd.Given	0.03	−0.12

Discussion reputation
27	CommentsReceived	−0.06	−0.01
28	UpvotesReceived	−0.05	0.45
29	CommentsEnd.Rec.	−0.09	−0.38
Variance Explained	0.97	0.03

Table A.16. Standardized DFA loadings of cognitive presence variables.

#	Variable	LD1	LD2
1	TriggeringEventLevel	−0.32	−0.64
2	ExplorationLevel	−0.14	−0.79
3	IntegrationLevel	0.81	0.10
4	ResolutionLevel	−1.12	0.54
Variance Explained	0.94	0.06

Table A.17. The average scores of answers to the course surveys of students in the three identified clusters.

Cluster 1
(Students = 15,868)	Cluster 2
(Students = 3,532)	Cluster 3
(Students = 4,248)
Mean (SD)	Responses	Mean (SD)	Responses	Mean (SD)	Responses
#	Variable		(Cluster %)		(Cluster %)		(Cluster %)
Basic demographics
1	Gender	1.06 (0.26)	2505 (16%)	1.07 (0.30)	619 (18%)	1.06 (0.28)	741 (17%)
2	Age	33.46 (10.56)	2449 (15%)	32.49 (9.96)	600 (17%)	33.76 (10.79)	724 (17%)

Enrollment factors
3	ImpFactorPrevEdxExp	2.21 (1.32)	2606 (16%)	2.12 (1.34)	648 (18%)	1.98 (1.27)	773 (18%)
4	ImpFactorPrevTudelftExp	1.40 (0.83)	2592 (16%)	1.33 (0.77)	646 (18%)	1.35 (0.82)	770 (18%)
5	ImpFactorTudelftRep	1.85 (1.11)	2597 (16%)	1.78 (1.10)	647 (18%)	1.74 (1.08)	770 (18%)
6	ImpFactorProfRep	2.75 (1.46)	2598 (16%)	2.84 (1.50)	648 (18%)	2.91 (1.47)	774 (18%)
7	ImpFactorEarningCert	1.96 (1.18)	2605 (16%)	1.97 (1.19)	648 (18%)	1.95 (1.18)	772 (18%)
8	ImpFactorCourseUniq	3.09 (1.21)	2604 (16%)	3.14 (1.23)	649 (18%)	3.09 (1.23)	774 (18%)
9	ImpFactorRecommendation	1.88 (1.26)	2592 (16%)	1.90 (1.24)	645 (18%)	1.95 (1.27)	772 (18%)

Motivation
10	DeterminedToLearn	3.92 (0.82)	2778 (18%)	4.00 (0.81)	685 (19%)	4.02 (0.79)	842 (20%)
11	DeterminedToComplete	3.73 (0.83)	2777 (18%)	3.77 (0.85)	685 (19%)	3.77 (0.83)	843 (20%)
12	Hours	2.29 (0.78)	2778 (18%)	2.35 (0.80)	683 (19%)	2.36 (0.81)	842 (20%)
13	CourseInterest	4.61 (1.96)	2745 (17%)	4.39 (2.03)	678 (19%)	4.39 (2.02)	823 (19%)

Background knowledge
14	LevelOfEd	3.02 (1.38)	2495 (17%)	3.08 (1.42)	615 (17%)	2.93 (1.24)	745 (18%)
15	EdBackFunctProgRel	1.84 (1.00)	2498 (16%)	1.86 (0.99)	615 (17%)	1.86 (1.00)	742 (17%)
16	PrevExpInTheField	1.94 (1.06)	2496 (16%)	2.00 (1.05)	615 (17%)	2.07 (1.07)	743 (17%)
17	Occupation	3.03 (1.33)	2496 (16%)	2.94 (1.23)	616 (17%)	2.99 (1.18)	743 (17%)
18	YearsOfWorkExp	11.85 (12.99)	1726 (11%)	10.97 (8.30)	439 (12%)	12.01 (9.47)	538 (13%)

Course expectations
19	ExpectFunChallenge	3.90 (0.83)	2772 (17%)	3.95 (0.81)	685 (19%)	3.88 (0.85)	843 (20%)
20	ExpectRelevance	3.72 (0.95)	2772 (17%)	3.80 (0.92)	685 (19%)	3.81 (0.92)	843 (20%)
21	ExpectForumFeedbackTeam	2.58 (1.01)	2768 (17%)	2.62 (1.04)	685 (19%)	2.62 (1.05)	840 (20%)
22	ExpectForumPeerInteract	2.29 (1.00)	2769 (17%)	2.31 (1.04)	685 (19%)	2.30 (1.01)	841 (20%)

Study strategies
23	HowLikelyStudyGroup	1.80 (0.94)	2770 (17%)	1.87 (0.98)	685 (19%)	1.90 (0.98)	842 (20%)
24	HowLikelyMakeFrieds	1.86 (0.89)	2770 (17%)	1.86 (0.91)	685 (19%)	1.90 (0.91)	841 (20%)
25	HowLikelyLookExtraMat	3.61 (0.91)	2771 (17%)	3.65 (0.90)	685 (19%)	3.63 (0.91)	843 (20%)
26	HowLikelyPostDisc	2.73 (1.00)	2770 (17%)	2.79 (1.01)	685 (19%)	2.80 (0.99)	841 (20%)
27	IntentShareExpertise	3.04 (0.86)	2569 (16%)	3.02 (0.91)	638 (18%)	3.08 (0.87)	765 (18%)
28	PrefAloneVsWithOthers	1.23 (0.42)	2574 (16%)	1.26 (0.44)	637 (18%)	1.25 (0.43)	763 (18%)
29	PrefTeacherVsStudentResp	42.40 (24.26)	2599 (16%)	40.68 (24.80)	647 (18%)	43.33 (23.42)	774 (18%)

Study support
30	SupportBy	1.19 (0.75)	2479 (16%)	1.21 (0.85)	616 (17%)	1.18 (0.74)	739 (17%)
31	SupportHow	3.44 (1.68)	186 (1%)	3.65 (1.57)	43 (1%)	3.48 (1.72)	52 (1%)
32	StudyDuringWorkHrs	1.72 (0.45)	1748 (11%)	1.71 (0.45)	444 (13%)	1.70 (0.46)	544 (13%)

Experience with online environments
33	FreqSocialMediaUse	3.03 (1.19)	2575 (16%)	3.03 (1.16)	639 (18%)	3.09 (1.25)	768 (18%)
34	FreqForumUse	2.51 (0.82)	2575 (16%)	2.55 (0.83)	639 (18%)	2.50 (0.78)	768 (18%)
35	OnlineClassesTaken	3.36 (1.20)	2574 (16%)	3.29 (1.28)	639 (18%)	3.26 (1.29)	768 (18%)
36	OnlineClassesComp	3.48 (1.48)	2262 (14%)	3.41 (1.49)	538 (15%)	3.36 (1.47)	645 (15%)

Language fluency
37	EnglishFluencyLevel	4.14 (0.98)	2483 (16%)	4.18 (0.97)	616 (17%)	4.19 (0.94)	740 (17%)
38	EnglishHowComfortable	4.34 (0.90)	2483 (16%)	4.41 (0.83)	616 (17%)	4.38 (0.86)	740 (17%)
39	EnglishHowOften	3.87 (1.03)	2483 (16%)	3.96 (0.99)	616 (17%)	3.91 (1.00)	740 (17%)

Table A.18. The average scores of answers to the course surveys of students in the three identified clusters.

Cluster 1
(Students = 15,868)	Cluster 2
(Students = 3,532)	Cluster 3
(Students = 4,248)
Mean (SD)	Responses	Mean (SD)	Responses	Mean (SD)	Responses
#	Variable		(Cluster %)		(Cluster %)		(Cluster %)
Course engagement level
1	ParticipationLevel	5.16 (0.91)	615 (4%)	5.30 (0.85)	164 (5%)	5.27 (0.79)	208 (5%)
2	HoursDedicated	6.27 (4.10)	532 (3%)	6.45 (6.89)	139 (4%)	6.50 (4.05)	188 (4%)
3	WorkedHard	1.15 (0.50)	556 (4%)	1.18 (0.47)	154 (4%)	1.17 (0.53)	196 (5%)
4	FamilyWorkObligationsAside	1.09 (0.36)	556 (4%)	1.06 (0.37)	154 (4%)	1.11 (0.46)	196 (5%)
5	PlannedAndOrganizedLearning	1.23 (0.56)	557 (4%)	1.17 (0.51)	154 (4%)	1.25 (0.53)	196 (5%)
6	LookedAtExtraMaterials	3.25 (1.05)	557 (4%)	3.17 (1.00)	155 (4%)	3.46 (0.90)	196 (5%)

Social interactions
7	ForumHowOften	1.54 (0.81)	557 (4%)	1.46 (0.73)	154 (4%)	1.56 (0.81)	196 (5%)
8	PostedInDiscussions	3.27 (0.90)	561 (4%)	3.28 (0.86)	152 (4%)	3.36 (0.87)	198 (5%)
9	SharedExpertiseWithOthers	3.44 (0.97)	570 (4%)	3.48 (0.99)	157 (4%)	3.42 (1.01)	201 (5%)
10	IntendedToShareMore	2.29 (1.03)	570 (4%)	2.31 (0.96)	156 (4%)	2.35 (1.01)	201 (5%)
11	ConnWithCourseTeam	3.04 (0.94)	570 (4%)	3.01 (0.96)	155 (4%)	3.05 (0.90)	201 (5%)
12	ConnWithOtherStudents	1.74 (0.99)	559 (4%)	1.71 (1.03)	154 (4%)	1.83 (0.95)	196 (5%)
13	ConnWithNewFriends	1.80 (1.04)	559 (4%)	1.79 (1.09)	154 (4%)	2.03 (1.06)	196 (5%)
14	WouldLikedConnectMore	2.59 (1.30)	559 (4%)	2.50 (1.37)	154 (4%)	2.72 (1.29)	196 (5%)
15	PartInStudyGroup	3.45 (1.16)	560 (4%)	3.38 (1.29)	155 (4%)	3.42 (1.17)	195 (5%)

Challenges to course participation
16	PersonalMedicalIssue	1.29 (0.72)	594 (4%)	1.31 (0.78)	160 (5%)	1.21 (0.53)	203 (5%)
17	ProfessionalObligation	2.33 (1.18)	595 (4%)	2.31 (1.14)	160 (5%)	2.33 (1.11)	204 (5%)
18	NotGettingFeedback	1.44 (0.81)	594 (4%)	1.42 (0.87)	161 (5%)	1.42 (0.82)	203 (5%)
19	FeelingLonely	1.29 (0.65)	594 (4%)	1.29 (0.60)	160 (5%)	1.32 (0.67)	203 (5%)
20	TechOrAccessibilityIssue	1.15 (0.50)	594 (4%)	1.20 (0.59)	161 (5%)	1.15 (0.55)	203 (5%)
21	KeepingUpWithCoursePace	1.98 (1.07)	594 (4%)	1.86 (1.06)	160 (5%)	1.84 (1.03)	203 (5%)
22	FamilyObligation	2.01 (1.10)	595 (4%)	1.96 (1.10)	160 (5%)	1.99 (1.01)	204 (5%)
23	AnotherCourseObligation	1.64 (0.97)	596 (4%)	1.64 (1.00)	160 (5%)	1.60 (1.02)	203 (5%)

Technical challenges to course participation
24	SlowInternet	1.18 (0.56)	588 (4%)	1.18 (0.61)	157 (4%)	1.17 (0.48)	202 (5%)
25	InternetNetworkProblems	1.19 (0.56)	588 (4%)	1.24 (0.70)	157 (4%)	1.20 (0.63)	202 (5%)
26	ElectricityNetworkProblems	1.05 (0.33)	588 (4%)	1.06 (0.34)	157 (4%)	1.00 (0.07)	202 (5%)
27	NotHavingAccessPers.Comp.	1.07 (0.37)	588 (4%)	1.10 (0.49)	157 (4%)	1.03 (0.32)	202 (5%)
28	HardwareProblems	1.05 (0.32)	587 (4%)	1.01 (0.08)	157 (4%)	1.05 (0.36)	202 (5%)
29	NotHavingMobileAccess	1.24 (0.64)	589 (4%)	1.33 (0.78)	157 (4%)	1.21 (0.60)	202 (5%)
30	AccessibilityUsability	1.17 (0.55)	588 (4%)	1.11 (0.45)	157 (4%)	1.24 (0.63)	202 (5%)

Course appropriate
31	CourseExpectationsRealistic	3.48 (0.88)	572 (4%)	3.54 (0.83)	157 (4%)	3.61 (0.84)	201 (5%)
32	CourseMeetYourExpectations	4.57 (0.66)	573 (4%)	4.53 (0.72)	158 (4%)	4.50 (0.69)	201 (5%)
33	LevelOfEnglishAppropriate	3.30 (1.11)	572 (4%)	3.44 (1.03)	158 (4%)	3.25 (1.10)	201 (5%)
34	CourseRelevantToOccupation	3.55 (1.01)	573 (4%)	3.65 (0.92)	158 (4%)	3.61 (0.95)	201 (5%)
35	PriorKnowledgeMadeItEasier	1.95 (0.72)	502 (3%)	1.83 (0.70)	138 (4%)	1.96 (0.70)	176 (4%)

Course support
36	CourseInspiredToStudy	3.69 (0.74)	545 (3%)	3.69 (0.72)	150 (4%)	3.60 (0.78)	191 (4%)
37	ForumWasHelpfulForMe	3.24 (0.61)	550 (3%)	3.17 (0.54)	155 (4%)	3.14 (0.55)	194 (5%)
38	OthersHelpedMeInTheCourse	3.28 (0.61)	551 (3%)	3.29 (0.57)	154 (4%)	3.28 (0.60)	194 (5%)
39	ReceivedSupportStudents	2.87 (0.53)	553 (3%)	2.87 (0.42)	154 (4%)	2.84 (0.47)	193 (5%)
40	ReceivedSupportCourseTeam	4.01 (0.84)	553 (3%)	4.07 (0.83)	154 (4%)	4.04 (0.81)	193 (5%)
41	SupportFromCourseTeamForums	3.74 (1.05)	552 (3%)	3.69 (1.10)	154 (4%)	3.81 (0.99)	193 (5%)
42	OthersCouldHelpMore	3.54 (1.07)	550 (3%)	3.64 (1.02)	154 (4%)	3.68 (0.92)	193 (5%)
43	ForumCouldBeMoreHelpful	4.00 (0.92)	552 (3%)	4.15 (0.81)	154 (4%)	4.09 (0.83)	192 (5%)

Course design and quality evaluation
44	DifficultyLevelOfTheCourse	3.10 (1.10)	543 (3%)	3.14 (1.03)	153 (4%)	3.28 (1.04)	187 (4%)
45	AmountOfWorkRequired	3.69 (0.95)	535 (3%)	3.74 (0.89)	149 (4%)	3.68 (0.90)	181 (4%)
46	PaceOfTheCourse	3.82 (0.97)	553 (3%)	3.72 (1.00)	154 (4%)	3.70 (0.98)	193 (5%)
47	DurationOfTheCourse	3.95 (1.07)	552 (3%)	4.05 (1.07)	154 (4%)	4.09 (1.00)	192 (5%)
48	LecturesExercisesBalance	2.50 (1.02)	557 (4%)	2.53 (1.06)	154 (4%)	2.69 (0.97)	196 (5%)
49	CourseOverallQuality	2.68 (1.04)	558 (4%)	2.66 (1.07)	154 (4%)	2.87 (1.06)	196 (5%)
50	AssignmentAndExamQuality	1.94 (0.97)	566 (4%)	1.90 (1.02)	154 (4%)	2.06 (1.08)	200 (5%)
51	VideoLecturesQuality	1.86 (1.06)	566 (4%)	1.75 (0.99)	154 (4%)	1.92 (1.05)	199 (5%)
52	FeedbackQuality	2.76 (1.06)	558 (4%)	2.90 (1.07)	154 (4%)	2.96 (1.03)	196 (5%)
53	EdxEaseOfUseAndQuality	2.62 (1.02)	558 (4%)	2.73 (1.04)	154 (4%)	2.93 (0.92)	196 (5%)
