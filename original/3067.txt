This paper reports on the initial stage of a design-based research program that aims to devise student-facing social learning analytics in a postsecondary setting. In order to foster student discussion in online classes, we devised an analytics toolkit that turns discussion forum data into information for students to reflect upon. The first pilot study conducted in an online course showed both early promises and challenges with this effort. In particular, students reported usefulness of the toolkit for monitoring posting behaviors and setting participation goals. Content analysis of post content linked tool usage with students' increased attempts to solicit peer responses to their posts and increased reflection on individual participation. However, the tool was not found useful or easy to use by some students, and it did not show benefits in other aspects of online discussion, such as crafting stronger argumentation or reflecting on group processes. These findings highlight the need of social learning analytics to strive for tool simplicity, scaffold students' development in data literacy, and bring analytics and pedagogical designs into close coordination.

Previous
Next 
Keywords
Learning analytics

Higher education

Online discussion

Design-based research

Social network analysis

1. Introduction
Inspired by the social perspective of learning, dialogic processes are often emphasized in various contexts of learning. In postsecondary education, for example, online discourse is widely integrated in hybrid classes (Cesareni, Cacciamani, & Fujita, 2016), massive open online courses (Huang, Dasgupta, Ghosh, Manning, & Sanders, 2014), and in-service teacher education (Chen, Chen, & Tsai, 2009). It becomes imperative for educators, researchers, practitioners, and technology designers to seek ways to foster learner engagement in these discourse environments in education.

Extant research has documented a myriad of pedagogical and technological approaches to improving educational discourse. Pedagogically, facilitating student role-taking could foster quality participation in online discussion (Cesareni et al., 2016, Wise and Chiu, 2014); collaboration scripts can be designed to engage learners in discourse practices beyond their current reach (Fischer, Kollar, Stegmann, & Wecker, 2013). On the technical end, while threaded discussion poses as the norm, many research and design teams are invested in developing discourse environments for purposes that are traditionally underserved by threaded discussion (Marbouti and Wise, 2016, Scardamalia, 2004). More recently, because of the rise of learning analytics (Siemens, 2013), it becomes increasingly appealing and feasible to harness discourse data for actionable insights that can aid information navigation, sense-making, and decision-making by learners and instructors (Chen and Zhang, 2016, Dawson, 2010, Wise et al., 2014).

This paper reports on the initial stage of a design-based research project that aimed to devise a student-facing learning analytics toolkit for online discussion in undergraduate classrooms. The developed toolkit combines social network analysis with lexical analysis to produce insights into students' forum participation. By providing students access to the toolkit and coupled pedagogical supports, it was hoped that students would become more aware of their participation patterns and adjust participation accordingly. In the following sections, we ground this study in pertinent literature, describe the research design, report research findings from the first pilot study, and discuss research and practical implications.

2. Background
2.1. Social and cultural perspectives of learning
Learning in all settings is arguably a social process that cannot be fully accounted by cognition and behaviors of the individual. As sociocultural perspectives of learning recognize, intimate connections exist between individual psychological functioning and sociocultural aspects of mind (Lave and Wenger, 1991, Vygotsky, 1978, Wertsch, 1993), informing educational approaches that emphasize learner discourse (Michaels et al., 2007, Scardamalia and Bereiter, 2006), distributed intelligence (Pea, 1993), and group cognition (Stahl, 2006). Indeed, human knowledge is not discrete objects readily obtainable by learners out of context; rather, knowledge is situated in unique sociocultural scenarios, negotiated within communities, constructed through purposeful social activities, and created through emergent interactions (Bandura, 2001, Bereiter, 2002, Brown et al., 1989, Dunbar, 1995, John-Steiner and Mahn, 1996, Lave and Wenger, 1991). Such perspectives of learning manifest in both face-to-face and virtual learning spaces, where learning is commonly mediated by cultural tools and artifacts (Engeström et al., 1999, Wertsch, 1993). In online learning specifically, discussion forums are widely used to mediate social interactions in classes of all kinds—large or small, online or hybrid, scripted or self-organized. In the open, networked space, one's learning is enabled by forming connections with people and specialized information sources, with the resulting connectivity empowering further learning (Siemens, 2005). Our evolving understanding of learning in such contexts requires us to move beyond individualistic metrics to consider various interpersonal and group-level phenomena.

In higher education, quality social interaction matters for student success. Students' social interaction takes place in different forms and involves multiple stakeholders. For instance, on college campuses, social interaction in student interest groups are positively related to student persistence (Tinto & Goodsell-Love, 1993). In classrooms, students with more frequent interactions possess stronger sense of community and have better performance (Dawson, 2006). However, emerging student networks in a classroom may also form a gatekeeping mechanism that separates high- and low-achieving students, creating obstacles to richer peer interaction (Chen and Huang, (under review), Vaquero and Cebrian, 2013). To support student success in college, researchers and practitioners are heavily invested in fostering social interaction, by applying social media tools (Lee & Bonk, 2016), facilitating high-level knowledge processes in classroom discussion (Anderson et al., 2008, Wise and Chiu, 2014), and developing analytics tools to provide formative feedback about students' social participation (Dawson, 2010, Wise et al., 2014).

2.2. Social learning analytics
Informed by social and cultural perspectives of learning, social learning analytics (SLA; Buckingham Shum & Ferguson, 2012) as a subdomain of learning analytics devises analytic affordances for sociocultural aspects of learning. SLA shares learning analytics' focus on turning educational data into actionable insights in order to close the feedback loop in learning (Gašević et al., 2015, Siemens, 2013). Reflecting its appreciation of sociocultural learning perspectives, SLA draws on a “substantial body of work demonstrating that new skills and ideas are not solely individual achievements, but are developed … through interaction and collaboration” (Buckingham Shum & Ferguson, 2012, p. 5). In contrast with learning analytics that emphasize individual learning, SLA attempts to account for the sociocultural contexts wherein learning takes places.

Analytical techniques applied in SLA broadly include social network analysis, text mining, discourse analytics, visual analytics, and so forth (Buckingham Shum & Ferguson, 2012). These techniques are not necessarily new. For example, social network analysis (SNA; Scott, 2013, Wasserman and Faust, 1994) as a methodology or an analytical approach has been broadly applied in educational settings. For example, SNA is used to study group cohesion and interaction patterns in computer-supported collaborative learning (de Laat et al., 2007, Nurmela et al., 1999), social networks formed through blogging in a graduate course (Lee & Bonk, 2016), peripheral participation in an online community of teachers (Smith Risser & Bottoms, 2014), and social relations of international students in large classrooms (Rienties, Héliot, & Jindal-Snape, 2013). In higher education, the network centrality measures have shown to be predictive of students' reported sense of community; a student's existing social network influences the type of support and information she receives (Dawson, 2008). In summary, SNA, as a pillar of SLA, offers a unique angle and a pivotal approach for the analysis of social learning phenomena.

Even though SNA as a research methodology has matured, what remains relatively new is its application in providing instant or semi-instant feedback when learning is still in progress. Examples of such applications include SNAPP (i.e., Social Networks Adapting Pedagogical Practice), which provides real-time visualizations of social interactions in discussion forums (Dawson, Bakharia, & Heathcote, 2010); a social awareness tool facilitating peer assistance based on friendship relations and student knowledge (Lin & Lai, 2013); and a social network monitoring system allowing users to register and visualize their interactions (Cadima, Ferreira, Monguet, Ojeda, & Fernandez, 2010). Such efforts of turning analyses of social networks into real-world applications are burgeoning given their recognized value for catalyzing changes in authentic settings. The same story could be told about other analytical techniques used in SLA such as text mining, discourse analysis, and natural language processing (Lang, Siemens, Wise, & Gašević, 2017).

Despite these recent advances, this line of SLA research could be further developed in three directions. First, while learning analytics tools designed for instructors are readily available (Lockyer et al., 2013, Kosba et al., 2007, Mazza and Botturi, 2007, Scheuer and Zinn, 2007), what remains lacking is an attempt to give students access to general learning analytics and SLA tools. This status quo might be due to a pedagogical proposition (Knight, Buckingham Shum, & Littleton, 2014), or “bias” (Scardamalia & Bereiter, 2008), which reserves higher-level decisions for the instructor instead of the students. If treating students as autonomous agents is an ethical principle for learning analytics (Slade & Prinsloo, 2013), designing tools that directly engage students in choice-making becomes imperative (Chen & Zhang, 2016). However, such a shift would incur significant changes with power dynamics in classrooms, and its effectiveness would rest on a range of contextual factors (Ali et al., 2012, Buckingham Shum and Ferguson, 2012). Thus, it becomes critical to explore challenges and issues in the development of student-facing SLA.

Even less visible in the literature are detailed pedagogical supports for student-facing SLA. Given the importance of data literacy for any type of analytics, providing proper supports for the end user is crucial for meaningful and ethical uses (Slade & Prinsloo, 2013). In predictive learning analytics, analytical results, such as the likelihood of passing a course, could be communicated to the student via a personalized email send on behalf of the instructor (e.g., Huberth, Chen, Tritz, & McKay, 2015). In SLA, because the analytical techniques and results tend to be more multifaceted and contextualized in social learning environments, pedagogical supports for student use are even more important and difficult to develop. It is thus unsurprising that existent pedagogical supports for SLA are meant for instructors. For instance, guidelines for interpreting network visualizations of online discussion are developed for instructors to recognize discourse patterns or isolated students (Dawson et al., 2010). In a few rare cases where students are invited to review and act upon network visualizations, student supports in the form of survey questions or prompt sheets were provided (Yang et al., 2016, Yoon, 2011). But in general, more work is needed to design and empirically test pedagogical supports for student use of SLA.

Finally, few efforts have been made to piece together different strands of SLA into a holistic view of social learning (Buckingham Shum & Ferguson, 2012). Most tools are designed to provide insights into one particular aspect of learning, for example, social connections based on behavioral logs (Bakharia & Dawson, 2011). Few applications address multiple aspects of learning such as the behavioral, social, and semantic aspects (Kitto et al., 2016). Despite an appeal for more holistic views of learning, meaningfully integrating information from multiple analytical aspects remains challenging.

2.3. The present study
This study attempted to bridge gaps in these areas of SLA by developing and testing a student-facing SLA tool that combines social network analysis and lexical analysis. Following the design-based research approach (Collins, Joseph, & Bielaczyc, 2004), the present study, as the initial step of a larger research program, aimed to devise a SLA toolkit to foster student engagement in online discussion. Through an iterative process of design, this toolkit was developed to turn discussion data from the Canvas learning management system into student-facing representations. The developed toolkit, named CanvasNet, was comprised of two components that addressed two distinctive targets of analysis:

1.
Social network analytics for social interactions (McInnerney & Roberts, 2004): A social network analysis component providing visualizations and statistics of social interactions in Canvas discussion forums.

2.
Lexical analytics for conceptual engagement (Nussbaum & Sinatra, 2003): A lexical analysis component showing snapshots of trending terms in student posts and contrast a student's personal lexicon with the group one for probable conceptual expansion.

The SLA tool was designed to provide generic supports for student self-reflection, with an expectation that purposeful use of the tool by a student would boost her social and conceptual engagement in online discussion. For example, a student who intends to interact with all classmates may choose to interact more with weakly connected peers after seeing the social network visualization, and another student who focuses on course-related content may choose to focus on terms she has not dealt with. Therefore, for an early-stage analytics tool, technical affordances were intentionally left generic and open to various interpretive frames the student may introduce (Wise & Vytasek, 2017).

Reflecting the nature of design-based research (Edelson, 2002), through iterative design and testing, the present study aimed to: (a) develop design frameworks to guide the design of student-facing SLA and corresponding pedagogical interventions in online college classrooms; and (b) develop new domain theories of learning analytics implementation (such as a theory of facilitating data literacy for meaningful use of SLA by students). Specifically, we attempted to answer the following research questions:

1.
Did students use the SLA toolkit? To what extent did students find the toolkit useful? To what extent did students find the toolkit usable?

2.
To what extent did the toolkit foster social engagement in online discussions?

3.
To what extent did the toolkit facilitate conceptual engagement reflected in the content of student posts?

3. Methods
3.1. Context and participants
The research context was an undergraduate online course offered at a large public university in the United States of America. Throughout a 14-week semester, this course explored the intersection between technology and ethics in modern societies. The design of the course was informed by sociocultural perspectives of learning. Online, asynchronous discussion was a central component throughout this course. Students participated in weekly discussions, in which they posted reflections on readings and interacted with peers. They used a video-based communication tool named Flipgrid during the first and last weeks of class and the Canvas threaded discussion forum for the other weeks. When posting on Canvas, they were required to post minimally one initial post and respond to a peer's post.

Participants were undergraduate students from two course sections, with 20 and 19 students respectively, taught by a same instructor, who was also the principal investigator of this study. Students came from a range of disciplines including psychology, medicine, biochemistry, economics, management, and journalism. Nearly three-quarter of them were international students or English-language learners.

3.2. Tool design
Two phases of design emerged in this study, with a consistent goal of refining the presentation, communication, and integration of information provided by the SLA tool. In Phase 1, a semi-automatic analytics approach was used. Specifically, a script was written to extract and analyze Canvas discussion data, and then to generate information for the instructor to feed back to the class during Week 2–4. Corresponding to two analytic targets described above, semi-automatic feedback produced by the tool in this phase included (a) a static social network visualization showing student interactions, and (b) a term cloud visualizing top terms from student posts in the previous week.

Building on the Phase 1 implementation, a fully-automatic and interactive version of the SLA tool was implemented in Phase 2 and then applied during Week 9–12. The Phase 2 design incorporated changes made to reduce the instructor's workload and to support student-facing functionalities. Reflecting two components of the SLA tool described above, this version included (a) an interactive social network visualization combined with on-demand SNA metrics at both group and individual levels (see Fig. 1a); (b) an interactive term cloud that visualized top terms in discussion and also presented terms not covered by the current user (Fig. 1b). Various forms of interactivity were supported: in the network visualization, students could mouse-over a node (representing a student) to inspect its basic statistics and its connected nodes; in the term cloud, students could click on one term to inspect its count of occurrences. For both visualizations, students could adjust the date range and hide/show the instructor. Overall, compared to Phase 1 the refined version offered richer information and opportunities for student to “drill down” (Roth et al., 1996) for varied reflective purposes.

Fig. 1
Download : Download high-res image (383KB)
Download : Download full-size image
Fig. 1. The developed SLA tool, Version 2. Left—Social network analytics. A visualization of student interactions is provided. The teacher is hidden by default. Students could identify themselves and explore whom they were talking with by interacting with the visualization. Basic social network metrics (e.g., degree and density) were provided under the “Network Metrics” tab. Right—Lexical analytics. The term cloud visualized the most frequent words used in forum posts. Additional information to the top is provided to highlight frequent terms the current student had and had not produced by that point in the semester.

3.3. Research procedures and pedagogical supports
Corresponding to the two design phases described above, a two-phase, time-lag research design was implemented, treating two sections of the class, Sections A and B, both as experimental groups. Specifically, the focal intervention—which comprised the SLA tool and its pedagogical supports—was conducted first in Section A during Phase 1 and then extended to Section B during Phase 2. This design enabled the examination of potential intervention effects by comparing Section A to B in Phase 1 and comparing Phase 2 to 1 for Section B. Both sections' students were informed of the study at the beginning of the semester, and it was made clear that their voluntary participation would not affect the course grade whatsoever. Therefore, the degree to which Section A students were influenced by the Hawthorne or trial effect was slight if it existed at all.

In Phase 1 (Week 2–4), the SLA tool was introduced to Section A. In particular, the instructor embedded visualizations generated by the semi-automatic version of this tool in weekly course announcements and provided guidance for student sense-making. Below is an excerpt of the Week 2 announcement where the instructor modeled interpretation of the tool's feedback and discussed ways to consider the information.

First, below is a "sociogram"—a visualization of your interactions in Week 1 discussion forum. Exciting to see strong connections already forming in our class! In case this is the first time are seeing a sociogram, it is a popular technique to understand social dynamics within a group of people. In this sociogram, a node represents one of you, with its size denoting the number of posts one makes. An edge between two nodes represents both the direction and the count of replies between two persons. For example, the edge between nodes 6 and 14 means 14 has replied to 6, but not vice versa; in contrast, the edge between 5 and 11 is strong and reciprocal. I was VERY impressed by the connections you've made in Week 1…

Second, below is a "word cloud" showing you top words in our discussion. In this visualization, the bigger a word is, the more frequent it was mentioned in our discussion. I am sure all of us have talked about "ethics", but each of us may have focused on a certain area (e.g., normative ethics, morality, personal decision making). This visualization gives you an overview of key topics in our discussion. Please feel free to check out terms you did not consider when contributing your posts.

In Phase 2 (Week 9–12), the interactive version of the SLA tool was introduced to both sections in Week 9 through a reflection activity. In this activity, the instructor announced the tool in a discussion thread (Fig. 2), and provided further explanations of its purpose and intended usage along with tips and an accompanying reflection exercise involving the tool. The prompts were open ended as we were curious about interpretative lens students might bring to this activity (Wise & Vytasek, 2017). Students were prompted to voluntarily post reflections in the thread, with no extra incentives attached to this activity. After the tool was introduced, in the following weeks until Week 12, students were reminded to engage in similar reflection activities using the tool. Pedagogical supports, in the form of pedagogical groundings and contextualized interpretations, were provided by the instructor as well. Ethical considerations were given in both technical and pedagogical designs, with students' names hidden and interpretations focused on possible future actions instead of lower performance.

Fig. 2
Download : Download high-res image (387KB)
Download : Download full-size image
Fig. 2. Discussion thread introducing the SLA tool (i.e., CanvasNet) to both sections and prompting students to reflect on their discussion participation.

In Week 14, the final week of the class, students in both sections were asked to post reflective videos regarding their course experiences, with one video covering the usability and usefulness of the SLA toolkit.

3.4. Data and analyses
To address the research questions, we applied a mixed-methods approach to a rich dataset collected from the study. Data and analyses with corresponding research questions tagged besides each data source are included below.

3.4.1. System logs of the SLA tool and student reflection on tool use (Question Set #1)
To address the first set of research questions related to student use of the SLA tool, we analyzed student access to the tool and its specific features recorded in system logs. To understand student perceptions with the tool, we coded student reflection videos posted in the final week of the course, using an established scheme of software usefulness and usability (Davis, 1989). We also analyzed student reflection posts in class forums that were related to their tool use experiences.

3.4.2. Cavans system logs of forum interactions (Question Set #2)
To understand the extent to which the SLA tool had fostered social engagement, log data from Canvas discussion forums were collected. Social network analysis was conducted on student interactions, with a focus on comparing Phases 1 and 2 in two sections.

3.4.3. Forum posts by students (Question Set #3)
To examine the extent to which the SLA tool facilitated conceptual and social engagement in online discussion, students' forum posts (N = 1126) were collected from two sections and then coded for the quality of their online "speaking behaviors" in five aspects: Argumentation, Responsiveness, Elicitation, Reflection on Individual Process, and Reflection on Group Process (see Table 1; Wise, Zhao et al., 2014). In this study, we expect these aspects to be deeply intertwined because deeper reflection on class discussion could potentially lead to stronger argumentations, more responses, and more attempts to elicit responses from peers. Thus, in this analysis we aimed to uncover early promise in facilitating general discussion quality and did not attempt to attribute each coding dimension to related features of the tool. Three coders first coded 15% of all posts, reaching an initial inter-coder agreement > 0.60 (Krippendorff's α). They discussed and resolved disagreements and then coded the remaining data separately.


Table 1. Coding scheme for “speaking variables”.

Dimensions	Levels
Content
Argumentation	0	1	2	3
No argument (e.g., no source)	Unsupported argument (e.g., position only)	Simple argument (e.g., position + reasoning)	Complex argument (e.g., position + further opposition)

Discursiveness
Responsiveness	0	1	2	3
None	Acknowledging	Responding to an idea	Responding to multiple ideas
Elicitation	0	1	2	3
None	Questions not clearly to anyone	Questions directly to one person	Questions directly to the group

Reflectivity
Reflection on individual process	0	1	2	
No reflection	Shallow reflection (e.g., reflect on one's own post without further explanation)	Deep reflection (e.g., explain how the learning process shape one's idea)	
Reflection on group process	0	1	2	
No reflection	Shallow reflection (e.g., reflect on the group process without further explanation)	Deep reflection (e.g., explain how the learning process shaped by group's idea)	
Adapted from Wise, Zhao et al., 2014.

4. Results
4.1. Descriptive statistics of student discussion behaviors
To situate analyses addressing the major research questions, we first present an overview of online discussion activities in two sections of the course. Table 2 reports the weekly average number of posts per student and weekly average count of words per student in each section. In both sections, the number of posts remained largely unchanged across two phases, but the number of words increased. Notably, Section B demonstrated more active posting activities but also higher standard deviations in both phases; closer inspection identified four “superposters” (Huang et al., 2014) in Section B (P2, F2, A2, I2) who contributed to this result.


Table 2. Means and standard deviations of average post count and average word count per student per week in each course section.

Section	Phase 1	Phase 2
Avg. post #	Avg. word #	Avg. post #	Avg. word #
A	1.91 (0.85)	296.02 (137.08)	1.87 (0.56)	305.30 (137.75)
B	2.39 (1.02)	410.28 (295.02)	2.52 (1.09)	463.91 (305.41)
4.2. Use, usefulness, and usability of the SLA toolkit
Students did make use of the SLA tool. In the final reflection videos, 16 (out of 20) students in Section A and 11 (out of 19) students in Section B reported using the tool. Further content analysis of the 'students' final reflection videos revealed details about student use or nonuse. Notably, more students from Section 2 reported nonuse of the tool regardless of the same pedagogical supports provided to them in Phase 2.

Student click logs in Phase 2 also provided direct evidence of their interactions with the SLA tool. Overall, 364 user actions were recorded, indicating considerable but not intensive use for 27 users. As summarized in Table 3, recorded user actions included launching the tool, viewing various components of the tool (including the network visualization, network-level metrics, individual-level metrics, and the term cloud), and clicking on a term in the term cloud. Because the network visualization was the landing page of the tool (see Fig. 1a), it was unsurprising that students made the most use of it (for 149 times). Students also accessed the social network metrics at both the network and individual levels, for 57 and 26 times respectively; 20 unique students elected to check their individual information, leading to 192 of 364 actions, while other actions remained at the whole network level. In addition, students viewed the term cloud 26 times but rarely clicked on individual terms. Overall, the descriptive analysis of system logs indicated considerable usage and exploration of the SLA tool by student users.


Table 3. Summary of student access to tool features.

User action	Tool feature	Count
Launch	The SLA tool	82
View	Network (sociogram)	149
View	Network-level metrics	57
View	Individual-level metrics	26
View	Term cloud	26
Click on	A term	3
View	About page	21
Based on'students' final reflection videos, among these students who used the SLA toolkit, eight students in Section A and one student in Section B found it useful for particular reasons, whereas five students in Section A and one student in Section B declared the tool was not useful for them. The most popular usage reported by students was to monitor posting behaviors. For example, as two students reported:

“CanvasNet was actually interesting… So in the network summary, I can check how many [times I interacted] with a person.”

“I also found it's interesting to see all those words used throughout the semester, and what topic I was talking about [the] most.”

In addition, students also used the SLA tool to set participation goals. As one student mentioned:

“I used the CanvasNet [a] few times and I figured out that I was the person who got the most comments. So I think I had a lot of attention from others… So I tried to write comments to others… like commenting and interacting [with] each other.”

Nevertheless, some students tried the SLA tool but did not find it useful. They found the tool presented “a lot of unnecessary information [that] was not very helpful,” or claimed that “it was not really growing my learning.”

In terms of the tool's usability, student reflections pointed out the need for further interface refinement and user guidance. For example, two students reported:

“I did not really understand… The big cloud is like an electronic cloud [with] arrows pointing everywhere… I am not really sure about how those numbers are calculated…”

“And for CanvasNet I tried once or twice, but I did not really get it. I thought it was not quite easy to look at.”

In contrast, a Computer Science major student in Section B (F2) demonstrated sophisticated data literacy that supported his use of the tool. As he reflected:

In CanvasNet, I set up a "Date Range" … and found my ID on the network map. This is really interesting and I have 32 replies for two months. My position on the network map is in the middle. … In October period, my position is somewhat changed. It located in bottom of network, not the middle. However, my position is still connected with various people and also connected with orange color teacher. I have 14 replies in October period, so I think my interactions are loosen little bit in comparison with September. However, my interactions with other people are going well according to the whole network.

In summary, over the course of this study, students made substantive use of the developed SLA tool. They perceived varied levels of usefulness and usability, with data literacy of the student functioning as a possible moderating factor.

4.3. Social interactions
It was intended in the research design to first describe social interactions in both classes and then compare (a) between Sections A and B in Phase 1, and (b) between Phase 1 and 2 in Section B. Network measures and visualizations of both sections in two phases are presented in Table 4 and Fig. 3.


Table 4. SNA measures of student interaction networks.

Section A	Section B
Phase 1	Phase 2	Phase 1	Phase 2
Nodes	19	19	19	19
Edges	44	66	66	112
Density	0.13	0.19	0.19	0.33
Average degree (SD)	4.10 (2.33)	5.68 (3.11)	5.47 (2.82)	8.21 (3.78)
Average path length	11.40	8.01	5.28	3.87
Reciprocity	0.18	0.12	0.30	0.34
Transitivity	0.26	0.42	0.30	0.49
Centralization	0.15	0.23	0.14	0.27
Fig. 3
Download : Download high-res image (528KB)
Download : Download full-size image
Fig. 3. Social interaction networks. Note: In each graph, the size of a node denotes a student's degree (the sum of outdegree and indegree), and the width of a tie represents the interaction frequency between two nodes. Gray nodes represent tool users, and white nodes are nonusers. Networks are rendered as undirected for simplicity.

To begin with, in Phase 1, Section B—with no access to the SLA tool—demonstrated higher connectedness (implying a more interactive network) and lower network centralization (implying a more equally-distributed network) than Section A. This finding was reflected by higher numbers of edges, average degree, density, reciprocity, and transitivity in Section B than these measures in Section A (see Table 4). However, higher standard deviation of average degree—which measures the number of contacts a student made—was also observed in Section B. This finding was consistent with prior descriptive statistics of student posts showing “superposter” behaviors in Section B. Detailed inspection identified five highly socially active students in Section B (including students P2, F2, A2), who had equal or higher degree than even the most active students in Section A. In sum, in Phase 1 Section A did not demonstrated higher interactivity than Section B.

Comparing Phase 1 and 2 in Section B, we found increases in most network measures including edges, density, average degree, reciprocity, and transitivity, as well as increased centralization (see Table 4). Phase 2 had higher average degree than Phase 1 but also higher standard deviation. Therefore, Section B demonstrated increased interactivity in Phase 2 but such an increase was not evenly distributed among all students.

We also distinguished users and nonusers of the SLA tool in Fig. 3. However, no clear patterns were discernible from these interaction networks. In Section A, nonusers (white nodes) remained at the periphery in both phases, while nonusers in Section B occupied similar positions, central or peripheral, across the two phases.

4.4. Conceptual engagement
To analyze conceptual engagement in students' online discussion, we coded all forum posts in two sections using the “speaking variables” scheme, which comprises five dimensions: Argumentation, Responsiveness, Elicitation, Reflection on Individual Processes, and Reflection on Group Processes (Wise, Zhao et al., 2014). Descriptive statistics of coding results are presented in Table 5. Multiple factorial analyses of variance (ANOVAs) were performed to assess whether the five coding dimensions could be predicted from course section (Section A vs. B), tool use (use vs. nonuse), phase (Phase 1 vs. Phase 2), and the interactions among them. Based on the research design, it was hypothesized that in Phase 1 Section A would perform better than Section B, and Section B would perform better in Phase 2 than Phase 1; it was also hypothesized that tool use would benefit conceptual engagement in all dimensions. Results of ANOVAs did not confirm all of these initial hypotheses. However, the following findings emerged from the analysis. First, for Argumentation, nonusers (M = 2.20) actually performed better than users of the SLA tool (M = 2.07), F(1, 578) = 4.53, p < 0.05. This finding could be connected to the fact that fewer Section B students accessed the tool while they generated more posts and more words. Second, for Elicitation, both section and usage were identified as main factors, F(1, 578) = 10.39, p = 0.001 and F(1, 578) = 24.77, p < 0.001, and the interaction was also significant, F(1, 578) = 4.16, p < 0.05; as shown in Table 5, while Section B performed better than Section A on Elicitation, students who used the tool also generally performed better. Third, for Reflection on Individual Process, Section A started much lower (M = 0.05) than Section B (M = 0.19), but made more improvement across two phases; the interaction effect was marginally significant, F(1, 578) = 3.52, p = 0.06, implying a potential benefit for Section A who had access to the SLA tool for a longer time period.


Table 5. Means of conceptual engagement coding results.

Section A (n = 20)	Section B (n = 19)
Users (16)	Nonusers (4)	Users (11)	Nonusers (8)
Phase 1	Phase 2	Phase 1	Phase 2	Phase 1	Phase 2	Phase 1	Phase 2
Argumentation	2.20	2.15	2.38	2.02	2.15	1.90	2.13	2.22
Responsiveness	0.76	0.77	0.58	0.73	0.68	0.80	0.92	0.80
Elicitation	0.22	0.07	0.04	0.00	0.28	0.20	0.10	0.07
Reflection/individual	0.06	0.15	0.00	0.10	0.25	0.12	0.16	0.20
Reflection/group	0.02	0.04	0.00	0.00	0.01	0.01	0.00	0.02
Given both promise and complication of these findings, we took a deeper look at the content of posts from tool users. It became apparent that the quality of a student's posts was multi-faceted at the individual level and socially interdependent at the group level. To a certain degree, varied dimensions of conceptual engagement of one student could manifest differently in different discussion contexts. At the group level, the social and conceptual aspects are intertwined, with the development in one influencing the other. To illustrate this observation, below we present one example discussion thread from each section.

The first example involves E1 and L1 from Section A, who were not connected in Phase 1 but became connected during Phase 2 (see Fig. 3). Note also E1 was central in both phases, whereas L1 moved from the peripheral to the center across two phases. When responding to L1's post in the thread, E1 identified L1's main idea (Responsiveness-Level 2) and reflected on the implications of E1's idea on her earlier posts (Individual reflection-Level 1):

I like how you mentioned the potential struggles of choosing reliable sources. In my response this week, I talked about how great it would be if we could get the internet to various parts of the world. I should have also discussed the potential problems that could arise if impressionable children all of a sudden have access to the oftentimes graphic information on the internet.

Student E1, Section A, Phase 2
The second example, from Section B, involves P2 and B2 who were connected in both phases (see Fig. 3). When responding to B2, P2 attempted to expand multiple ideas (Responsiveness-Level 3) by directly soliciting B2's responses to several questions (Elicitation-Level 2; Individual reflection-Level 0):

Hey [B2], I was thinking about Kozma's example of using ThinkerTools for understanding Newtonian mechanics (page 4). Is that similar to your experience with quantum mechanics? Do you think there are ThinkerTool equivalents for other courses? That might be a fun topic to use for an elevator pitch!

Student P2, Section B, Phase 2
While both posts' Argumentation level was 1, they demonstrated different aspects of conceptual engagement valued in this online course. For B2 and P2 from Section B, their substantial interactions across two phases, as observed in Fig. 3, reflected a stronger rapport between them, which might have paved the way for higher level Elicitation in P2's response to B2. In contrast, E1 and L1 did not develop such a solid social connection through forum interactions, and this might have triggered E1 to reflect on herself when responding to L1's post, instead of being more socially discursive (e.g., greeting L1, eliciting more ideas from L1). Plausibly, given a similar conceptual idea put forward by one student, responses from other students could reflect different qualities (e.g., elicitation, reflection) depending on their prior social encounters. Therefore, while prior statistical results did not conclusively demonstrate the utility of the tool for all dimensions of conceptual engagement, this could be related to the intricate interconnections between the social and conceptual aspects of online discussion that cannot be neatly decoupled in the study context.

5. Discussion
5.1. Addressing research questions
In response to the first set of questions regarding the use, usefulness and usability of the developed SLA tool, we found most students accessed the tool, and students in Section A who were introduced to the tool earlier reported broader usage. This could be related to the timing when the tool was introduced. It is plausible students in Section B may have formed a forum participation routine during the first half of the semester and were less interested in this tool. Based on the click-log analysis, students who did use the tool accessed multiple features offered by it, covering both individual and class-level information.

Students who found the SLA tool useful used it to monitor posting behaviors and to help setting participation goals, whereas some other students either did not find the provided information useful or did not see a clear reason for using the tool. In terms of usability, results indicated the need for additional support to develop the data literacy skills necessary for interpreting the analytics.

The potential influence of the toolkit on students' social engagement was rather complex. In Phase 1, students in Section B—who did not have access to the tool—nonetheless had a higher connected and more balanced network than Section A, who had access to feedback information about their discussion. In Phase 2, even though Section B continued to demonstrate stronger connectedness than Section A, social interactions increased in both sections. Given the existence of several Section B superposters (Huang et al., 2014), whose posting behaviors have clearly influenced these statistical contrasts, these results showed promise of the introduced SLA tool for facilitating students' social engagement.

In terms of the conceptual engagement, no significant differences were uncovered from most of the “speaking behavior” dimensions (Wise, Hausknecht, & Zhao, 2014). Nonetheless, Section B students demonstrated higher-level Elicitation in their posts, as did students who used the tool in both sections (in comparison with nonusers). Section A students, who had longer exposure to the tool, achieved more improvement in Reflection on Individual Process, even though they were still lagging behind Section B students.

Connecting the social and conceptual aspects together, analysis of student posts revealed their intricate interconnections as the semester progressed. While the use of the SLA tool may have facilitated students' metacognition and self-regulation (Winne & Hadwin, 2010), as demonstrated in the content of their reflection videos and forum posts, the interactions among different dimensions of conceptual engagement and between the social and conceptual aspects of online discourse were not observed in statistical comparisons. This finding points to the need to consider multiple aspects of discourse engagement together—in both post-hoc analysis and analytics design. This argument for integrative views of online learning is in line with existing conceptual models (Garrison, Cleveland-Innes, and Fung, 2010), as well as calls for more integrative analysis of learners' online participation (Martinez, Dimitriadis, Rubia, Gómez, & de la Fuente, 2003). Overall, the present study revealed potential but inconclusive efficacy of the SLA toolkit in facilitating students' social and conceptual engagement in online discussion, and illuminated both design and empirical gaps that remain to be addressed in future design iterations.

5.2. Student use of social learning analytics
Design-based research yields both domain theories of studied phenomena and design principles intended to guide future design actions (Edelson, 2002). The present study contributes to domain understanding of student-facing social learning analytics (SLA) by demonstrating a number of factors acting as designing constraints.

First, student agency has been recognized in the learning analytics literature as both a design goal (Chen & Zhang, 2016) and an ethical principle (Slade & Prinsloo, 2013). Pioneering work on Open Learner Models—which typically involve showing the learner an underlying model of their knowledge—has demonstrated convincing benefits of opening such information to learners and nurturing learner control (Bull & Kay, 2010). Together with prospects are challenges with Open Learner Models, which include creating a usable interface and supporting learner interaction with such analytic systems. The same prospects and challenges are reflected in this study. A key motivation of creating the SLA tool was to make available to students discourse information that is otherwise less detectable, and to hereby facilitate student reflection and decision-making. Results demonstrated that some students could take advantage of such responsibility while some others were less appreciative. Further pedagogical supports for students to assume such agency are needed.

Second, learning dispositions—“learners' orientation toward learning” that propels them to behave in a certain way (Buckingham Shum & Deakin Crick, 2012, p. 93)—play an important part in social learning. Learning dispositions in the context of online learning could be motivational, affective, social, and/or cultural. Students in this study brought with them varied dispositions that impacted their online participation. For example, one student with less motivation to sustain meaningful online discourse responded to another student by merely posting “I am on the same page with you.” Such learning dispositions could also influence student reaction to learning analytics. For example, a student who sees the introduced SLA tool as an inconvenient “add-on” to their coursework, instead of an opportunity to engage and reflect, is less likely to benefit from such a tool. Design of any learning analytics application needs to consider learning dispositions representative in the local context.

Finally, students' data literacy also figured as a factor in this study. Recognizing a gap in student supports for the use of SLA, we intentionally provided scaffolds at multiple occasions for student sense-making of analytical results. However, research findings indicate even stronger supports are needed for students to make sense of provided information, as are more effective user interface design that embodies such supports. Taken together, designers of student-facing SLA tools need to recognize that learners bring with them a complex combination of attitudes, dispositions, knowledge, and skills, and then respond to these learner characteristics in analytics design.

5.3. Teacher inquiry with support from learning analytics
This study, in which the lead investigator also acted as the instructor, provides an example of iteratively refining an aspect of teaching to promote a specific genre of learning (Laurillard, 2012). Even though this study does not focus on the teacher side, it shows the possibility of using learning analytics to support teacher inquiry (Mor et al., 2015, Persico and Pozzi, 2015). For example, the results indicated that the developed SLA tool did not show uptake in Section B, potentially because students had established expectations for class participation. From a reflective teacher's perspective, the timing of introducing analytics becomes important, so that the teacher would mindfully decide when to introduce such a tool in future efforts. Similarly, the lack of change with some students in discussion engagement could motivate the teacher to create new pedagogical designs to better align the tool with course activities, so that the motivation of using the tool become more apparent. With such a learning analytics tool, especially if the teacher has a say about the tool design, the teacher becomes better equipped to be an innovative “change agent” in education (van der Heijden, Geldens, Beijaard, & Popeijus, 2015).

With regard to cost-effectiveness and scalability of supporting such teacher inquiry with learning analytics, a few lessons were learned from this study. First, teamwork is needed because expecting an instructor to serve the combined roles of teacher and design researcher is unrealistic. In this exploratory study, the teacher-researcher was facing challenges in designing an ongoing class, interacting with students, monitoring progress, designing analytics, interpreting analytics, and exploring ways to integrate analytic results in course design. These tasks are daunting for any single teacher and will thus require dynamic collaboration with instructional designers and learning technologists. Second, to realize the promise of integrating learning analytics and learning design to support teacher inquiry, we need to ask important questions at the institutional level: What institutional resources are needed to support such teacher inquiry? To what extent should a teacher be involved in or make decisions regarding tool design? To what extent are institutional goals of applying learning analytics aligned with goals of instructors? Institutional supports—technical, financial, social, and cultural (Colvin et al., 2015)—are required to encourage and sustain teacher inquiry with learning analytics.

6. Conclusions
Learning analytics are commonly reserved for the teacher (Dyckhoff, Zielke, Bültmann, Chatti, & Schroeder, 2012). This paper reports on an early-phase design study that aimed to devise social learning analytics to be directly used by students. Results showed promise of the developed toolkit for meeting this goal and also pointed out areas that need to be addressed in future research and design. In particular, students reported usefulness of the toolkit for monitoring posting behaviors and setting participation goals, and tool usage was linked to more attempts of students to elicit responses from each other and more increases in reflection on individual participation. For future design work in this area, important considerations should be given to the intricate connections between the social and conceptual aspects of online discourse, as well as learners' varied dispositions and background knowledge (e.g., data literacy). Future research efforts should also anticipate such intricacy to achieve a more integrative view of social learning in the online discussion context.

