Online and blended instructors are increasingly providing student feedback via asynchronous video, and students have reported in previous research that they are better able to perceive their instructors' social presence in video as compared to text. However, research is lacking that examines actual feedback comments for indicators of social presence. We addressed this gap by coding for indicators of social presence in 422 text and asynchronous video feedback comments provided to preservice teachers in three blended and online courses. Minimal differences were found in the frequency of social presence indicators between text and video feedback. However, we warn against interpreting this finding too simplistically. While text and video feedback had similar numbers of indicators, the indicators in video feedback may have had a larger impact on social presence due to the richness of the medium. Further research is needed in order to understand how text and video feedback promote social presence in online courses.

Previous
Next 
Keywords
Video instruction

Asynchronous video feedback

Online learning

Blended learning

Social presence

Distance learning

1. Importance of feedback in online and blended education
Online course enrollments have grown dramatically in higher education (Allen et al., 2016, Parsad and Lewis, 2008) despite attrition rates believed to be higher than those in face-to-face environments (Carpenter et al., 2004, Patterson and McFadden, 2009). Although the causes of the higher attrition rates are complex (Picciano, 2006), Gaytan (2015) explained that a major contributor is the quality of interactions and feedback that students receive from their instructors. Boling, Hough, Krinsky, Saleem, and Stevens (2012) added that instructor feedback can influence affective course outcomes and build strong student-instructor relationships. Establishing student-instructor relationships can be especially challenging in courses where face-to-face exposure is limited and the majority of communication and feedback is done through asynchronous text. This lack of face-to-face communication can make it difficult for students to perceive their instructors' social presence (National Union of Students, 2008).

Some have argued that blending face-to-face and online communication can help to retain students who would otherwise fail to persist in fully-online courses (Picciano, 2006). However, by adding face-to-face class sessions, instructors remove some of the flexibility that online students require as well as other advantages of asynchronous learning (e.g. time to reflect between exchanges, more personalized communication, and a high level of student participation) (Graham, 2006). By improving the quality of online feedback that they provide their students, blended course instructors may be able to minimize their face-to-face contact with students and maintain a high degree of flexibility. As a result some blended instructors are providing feedback via asynchronous, one-to-one, video as a way to more easily establish their social presence while simultaneously providing detailed feedback to students (Borup et al., 2012, Borup et al., 2014, Griffiths and Graham, 2009, Thompson and Lee, 2012). Research examining video feedback's impact on instructor social presence has largely relied on self-reported data; additional research is needed that analyzes actual feedback content. In our study, we attempted to meet this need by reviewing 422 pieces of text and asynchronous video feedback given by six instructors in a preservice instructional technology course. In this article, we first review the literature on social presence and audio/video feedback, and present the findings from our study.

2. Literature review
2.1. Social presence
Short, Williams, and Christie (1976) originally defined social presence as “the degree of salience of the other person” (p. 65) in mediated communication. Short et al. (1976) emphasized that social presence was an objective attribute of the communication tool based on the amount of communication cues it was able to convey. Gunawardena (1995) later shifted the focus from the communication tool to participants' communication behavior by claiming that participants could “cultivate” their social presence (p. 162). In their Community of Inquiry (CoI) framework, Garrison, Anderson, and Archer (2000) continued this shift of focus by defining social presence as “the ability of participants in the Community of Inquiry to project their personal characteristics into the community, thereby presenting themselves to the other participants as ‘real people’” (p. 89).

Garrison et al. (2000) would later explain that one of the driving forces behind creating the CoI framework was to connect social presence to teaching and learning elements in a community of inquiry. More specifically, the researchers viewed social presence as primarily a support to cognitive presence because participants in a community of inquiry were more likely to engage content-focused dialogue following the establishment of social presence. The CoI also explained that students were more likely to establish their social presence when instructors established teaching presence by designing, facilitating, and directing cognitive and social processes (Anderson, Rourke, Garrison, & Archer, 2001).

The CoI framework “resulted in a flurry of new research” (Swan & Ice, 2010, p. 1). However, there are two primary limitations that should be acknowledged when using the framework to examine the impact of video feedback on instructor social presence. First, although Garrison and his colleagues explained that many of the online instructor responsibilities overlapped with the construct of social presence (Anderson et al., 2001), social presence was largely viewed as a student attribute. Lowenthal and Lowenthal (2010) recognized this limitation and made the distinction between CoI's teaching presence and instructor social presence. Other research indicates instructor social presence has a larger impact on course outcomes than student social presence (Swan & Shih, 2005).

Second, research using the CoI framework has focused largely on how students express social presence in text discussion board activities despite technological advances that have made it possible for instructors to more easily communicate via video (Archer, 2010). Rourke and the original authors of the CoI framework (Rourke, Anderson, Garrison, & Archer, 1999) analyzed students' discussion board comments and identified three categories of messages that helped students establish their social presence:

•
Affective—the expression of emotions, use of humor, and self-disclosure.

•
Interactive—continuing a thread, quoting from others' messages, referring explicitly to others' messages, asking questions, complementing, expressing appreciation, and expressing agreement.

•
Cohesive—using vocatives, phatics, or salutations and addressing or referring to the group using inclusive pronouns.

However, Rourke et al. (1999) could not easily identify all social presence indicators. While aggregate interrater reliability coefficients amongst authors were high with a range of 0.91 to 0.95, reliability coefficients varied widely depending on which indicators were being measured. For example, the authors reported reliability coefficients of 1.0 (total agreement) on objective indicators such as continuing a thread and addressing participants by name and much lower coefficients when the indicators required more subjective interpretation. For instance, humor had a reliability coefficient of 0.25. If trained coders had difficulty recognizing the more subjective indicators of social presence, it is likely that the actual discussion board participants also varied in their perceptions and interpretations of others' messages.

Rourke et al.'s analysis of actual text discussion board comments confirmed that social presence could be established in text-only environments, but the low reliability coefficients on some social presence indicators also supported Garrison et al.'s (2000) previous acknowledgement that “the lack of visual cues [in text] may present particular challenges to establishing social presence” (p. 95). Indeed, Walter, Ortbach, and Niehaves (2015) studied perceptions of social presence and feedback quality when given by either humans or computers via text, audio, or video and concluded that media richness played a significant role in perceived social presence. As a result, more research is needed that examines instructors' attempts to use asynchronous video feedback to provide quality feedback as well as more effectively establish their social presence.

Although the CoI framework's definition of social presence can provide insights into how video feedback impacts instructor social presence, the framework focused largely on text-based environments and viewed social presence as a student attribute. Archer (2010), one of the original authors of the CoI Framework, explained that attempts to “broaden the scope of the CoI framework entails a new look at the overall rational for the framework” (p. 69). Unfortunately little research has attempted to broaden the scope of the CoI framework and examine video feedback's impact on instructor social presence, or even that of other multimedia communication types such as audio feedback, even though the tools for audio feedback have been robust for many years.

Analyzing video feedback comments can add to the literature on social presence because feedback has a social dimension even when its primary focus is on course content (Evans, 2013), and feedback given with high social presence is perceived to be more useful (Walter et al., 2015). Boling et al. (2012) added that instructor feedback plays an important role in building instructor–student relationships. Before we consider the literature on video feedback as it may relate to social presence, we will first discuss what is known about audio feedback as a foundational precursor to the research that has been conducted on the more media-rich video feedback.

2.2. Asynchronous audio feedback
Most of the research we have reviewed below has relied on students' self-reported data on surveys and interviews, possibly because social presence can be somewhat subjective (Garrison et al., 2000). The research examining student and instructor perceptions of online feedback modes has proved helpful in identifying several advantages and disadvantages of various feedback modes and has explained what students and instructors value. Understanding what students' value in their course interactions can affect the way in which these communications are utilized.

Although student perceptions of audio feedback have been mixed, the majority of students in the following studies we reviewed found several advantages of audio feedback in establishing instructor social presence. For example, analysis of student surveys and interviews found that students generally enjoyed hearing their instructor's voice through audio feedback (Cuthrell et al., 2009, Wallace and Moore, 2012), and others indicated that audio feedback felt more supportive (Gallien and Oomen-Early, 2008, Gould and Day, 2013, Ice et al., 2010), interactive (Thompson and Lee, 2012, Wood et al., 2011), and personal (Cuthrell et al., 2009, Gallien and Oomen-Early, 2008, Gould and Day, 2013, Ice et al., 2010, Rodway-Dyer et al., 2011, Wallace and Moore, 2012, Wood et al., 2011). While most students agreed that audio feedback enhanced their relationships with their instructors (Gallien and Oomen-Early, 2008, Wallace and Moore, 2012), some felt that audio feedback came across more critical and harsh than text feedback (Gould and Day, 2013, Rodway-Dyer et al., 2011). Despite potential drawbacks to audio feedback, most students felt that audio feedback was delivered positively, and that receiving audio feedback was motivating (Wood et al., 2011).

Studies on teacher perceptions of audio feedback were more limited than those examining student perceptions, but instructors perceived similar advantages to audio feedback. Specifically, instructors found that providing audio feedback afforded more opportunity to make feedback personal and detailed (Gould and Day, 2013, Lunt and Curran, 2010). For example, the instructors in Lunt and Curran's (2010) study appreciated that audio feedback allowed them to soften their voice to make feedback delivery more personable and personalized. Although they acknowledged advantages of audio feedback, some instructors in Gould and Day (2013) found recording audio feedback to be nerve-wracking, and worried that students would take their negative feedback and “put it on Facebook” (p. 9). Overall, however, most student and teacher perceptions of audio feedback have been positive. In addition to studying social presence, some researchers have looked at the quality of audio feedback (Reynolds & Russell, 2008) as well as the time it required instructors to give feedback in different modes. For instance, the tutor in Lunt and Curran's (2010) study found text feedback to be much more time consuming, taking an average of 5 min to give audio feedback, and 30 min to give text feedback. In contrast, the instructors in Kirschner, van den Brink, and Meester's (1991) study took an average of 53 min on each piece of audio feedback, and 49 min on text feedback. However, they found that audio feedback was 1.7 times the length of text feedback, and determined that audio feedback was more efficient, even if it took instructors slightly more time to complete.

2.3. Asynchronous video feedback
In general, the research reviewed above implies that audio feedback can be advantageous to social presence. In addition to audio feedback, instructors have provided asynchronous, one-to-one, video feedback, under the assumption that the visual nature of video can extend the advantages of audio feedback. As expected, students and instructors have reported several common advantages to the use and utility of video feedback in establishing social presence. For example, instructors and students alike have asserted that asynchronous video feedback felt more interactive and intimate (Borup et al., 2012, Borup et al., 2014, Barrow, 2012, Ford, 2015, Griffiths and Graham, 2009, Thompson and Lee, 2012). Students claimed that watching video feedback and hearing instructors call them by name while speaking to them specifically felt “one-on-one” (Parton, Crain-Dorough, & Hancock, 2010) and personal (Borup et al., 2014, Harper et al., 2012, Henderson and Phillips, 2015, McCarthy, 2015, Moore and Filling, 2012, Parton et al., 2010, Silva, 2012), like the instructor knew them (Parton et al., 2010), cared about them (Borup et al., 2014, Henderson and Phillips, 2015), and valued them (Harper et al., 2012). Additionally, students in several studies have reported that it was easier to understand their instructors through video than through text (Borup et al., 2015, Harper et al., 2012, Henderson and Phillips, 2015, McCarthy, 2015, Parton et al., 2010, Silva, 2012, Thompson and Lee, 2012). Similarly, when instructors gave video feedback, they reported that it was easier to give encouragement and communicate authentically because they could more naturally “speak to the student” than they could in text (Harper et al., 2012, p. 4). These perceptions were confirmed by students who claimed that video feedback made teachers feel more real, motivating them to complete assignments (Borup et al., 2012, Harper et al., 2012, Henderson and Phillips, 2015).

Students have also highlighted potential drawbacks to receiving video feedback. Over 20% of 126 undergraduate and postgraduate student participants in Henderson and Phillips' (2015) research reported that they felt anxious to watch their video feedback and over 10% of students found it difficult to contextualize the video feedback comments within their written projects. McCarthy (2015) reported that students preferred video feedback to text and audio feedback but found video feedback more time consuming to download. Furthermore, Borup et al., (2014) found that students were more likely to respond to text feedback via the Learning Management System because using text was more convenient. Additionally, students in this sample were drawn primarily from blended courses that met face-to-face throughout the semester, and as a result, some students didn't feel as much need for social presence in their feedback. For example, one student mentioned that asynchronous video feedback would be useful for “forming a student-teacher relationship” (p. 245) in a fully online class, but isn't as necessary in situations where instructors and students can interact in person. This implies that the advantages of video feedback might differ depending on course structure.

2.4. Limitations to research on audio and video feedback
Garrison et al.'s (2000) definition of social presence placed an emphasis not only on participants' communicative behavior but also on how that behavior is perceived by others. However, research examining audio and video feedback has largely examined student and instructor perceptions while all but ignoring the actual communicative behavior on which those perceptions were based. While the research examining student perceptions of online feedback has proved helpful in identifying what students' value, perceptions and preferences do not always indicate interaction quality. For example, notwithstanding Reynolds and Russell's (2008) finding that peer-reviewed audio feedback was of higher quality than text feedback, students preferred to give and receive text feedback because they perceived text feedback to be more time efficient. Similarly, the majority of students in Borup, West, & Thomas, (2015) study preferred text feedback, even though an analysis of the feedback showed that students rated video feedback to contain more praise, support, and relationship building comments. When asked about this inconsistency in interviews, several students explained that the perceived efficiency of the text feedback surpassed the advantages video feedback had to offer.

As a result, researchers seeking to understand how mode of communication can impact social presence should not rely solely on student perceptions. However, few studies have compared actual faculty-student interactions for differences in social presence indicators based on communication mode. Through qualitative coding of transcriptions of video and text feedback, we attempted to study this gap in this paper. The investigation of this research question will fill the gap in the present literature by examining the communicative behavior in instructor feedback that has mostly been ignored in previous research. When combined with previous literature, the results of this analysis will aid in providing a fuller understanding of Garrison et al.'s (2000) definition of social presence. However, our research does not examine how the level of feedback impacted affective or cognitive outcomes—an important topic of inquiry in future research. More specifically our research addressed the following question: How does the frequency of social presence indicators vary based on the mode of communication used by the instructors (i.e. text and video)?

3. Method
This research used a qualitative approach to code for indicators of social presence in feedback comments.

3.1. Context and participants
Research was conducted in eight sections of three different one-credit technology integration courses required for education majors at a large university in the Midwestern United States (see Table 1). These courses were selected because they required students to familiarize themselves in the first week with video communication tools, and because the courses were structured in a way to minimize differences, since they used similar assignments, online teaching materials, and tutorials. The courses also emphasized instructor feedback as a way of assisting students in revising their projects. All sections used Canvas as a Learning Management System (LMS) because it allowed users to communicate using both video and text.


Table 1. Layout of course sections and feedback mode assignments.

Course	Instructor	Course structure	Feedback mode: Assignment #1	Feedback mode: Assignment #2	Feedback mode: Assignment #3
Tech4SecEd	Paul	Online	Video	Video	Text
Robert	Online	Video	Video	Text
Paul	Blended	Text	Text	Video
James	Blended	Text	Text	Video
Tech4ECE	David	Blended	Text	Text	Video
David	Blended	Video	Video	Text
Tech4ElEd	John	Blended	Video	Video	Text
William	Blended	Video	Video	Text
The courses in which feedback were analyzed enrolled 167 students into eight sections taught by six instructors. Technology for Secondary Education (Tech4SecEd) enrolled 54 secondary education majors in four sections. Technology for Early Childhood Education (Tech4ECE) enrolled 54 students in two sections that focused on integrating technology into grades K-2 and Technology for Elementary Education (Tech4ElEd) enrolled 59 students in two sections that specialized in technology integration for grades 3–6. Tech4SecEd was required for all secondary education majors and both Tech4ECE and Tech4ElEd were required for all elementary education majors.

All sections of Tech4ECE and Tech4ElEd and two sections of Tech4SecEd were primarily blended in format and met face-to-face 4–5 times during the 14 week semester, with the majority of content being taught online. Two sections of Tech4SecEd were primarily online and met face-to-face once at the beginning of the semester, with the remaining coursework presented fully online (see Table 1). In addition, a series of “open labs” were taught by the various instructors throughout the semester so students (whether online or blended) could come in for assistance on assignments if desired.

3.2. Data collection
All students received instructor feedback online on three major assignments that were similar across course number and section. The first assignment required students to create a personal website or blog. The second assignment required students to create an instructional video. The third assignment required students to create a presentation or complete a project. Elementary education students created presentations regarding their experiences using technology during their four-week practicum experience, while secondary education students completed projects using technologies for their specific content areas.

Prior to the beginning of the course, we gave feedback assignments to the eight instructors of the 10 sections of the courses (two instructors taught two sections). Instructors of the first five sections were asked to use text feedback for the first two assignments, and asynchronous video feedback for the third assignment. Instructors from the remaining five sections were asked to give video feedback for the first two assignments, and text feedback for the third assignment. During data extraction, two course sections were dropped from the first group (text feedback first) because one instructor failed to provide regular feedback on all the major assignments and one instructor's feedback comments were deleted from Canvas prior to data extraction. This resulted in three course sections being part of the text feedback first group and five sections were part of the second group that gave video feedback first (see Table 1). Instructors were also encouraged to include elements of social presence in their feedback, in order to ensure quality and consistency across sections of the courses. Prior to the start of the semester, instructors met with members of the research team who provided them with feedback guidelines that were based on previous research: (1) address the student by name, (2) identify things in common that they had with the student (an effective response to student self-disclosure that promotes greater sharing and thus social presence), (3) welcome the student to the class, (4) offer help, and (5) look into the webcam when giving feedback. Once the course began, a member of the research team continued to meet with instructors to answer questions that they had. We found that these guidelines helped to provide consistency in instructors' understanding of how social presence is established using text and video while still allowing for instructor differences in feedback style and delivery. This was especially important because instructors varied in their experience and understanding of effective online feedback practices that would have impacted their feedback practices and made comparisons more difficult. As a result, the feedback instructions undoubtedly impacted the frequency of certain social presence indicators but it also allowed us to conduct our research across sections with greater consistency. Additionally, these guidelines helped ensure that the students taking the teacher education courses in our department received quality feedback from their instructors. This training should also be considered when interpreting our findings and different results would be likely when examining instructors with more varied understanding of effective practices for establishing their social presence online.

All feedback was provided in Canvas as a response to each assignment submission. These feedback comments were given to the students so they would a) understand why they received the grade they did and b) know how they could apply the projects to their future classrooms. Text feedback comments were provided in either paragraph format or bullet points. Video feedback comments were webcam recordings of the instructor speaking to the student (see Table 2 for example screenshots). Occasionally, one instructor recorded a screencast while also showing a webcam recording of their face giving feedback.


Table 2. Example feedback comments: text and asynchronous video.

Example feedback comments
Text	“[student name], Great video. Good use of images and video along with narration. I like that you focused on one concept and really played to that. The kids at the beginning singing Ring around the Rosie were almost haunting given the subject. For the badge, I would ask you to make the text more readable. I think making it bold would go a long way. Other than that you got the badge. One note on the text. The red color works well if font is bold. Increasing the width of the text will make it more readable, especially in the limited time the viewer has to read it. Good use of video and images.”
Video	“Hi [student name], you did a great job with your Online Portfolio Setup, it looks real good, you got everything, right, so…and on time…even before the due…er, turned in even before the due dates, so that's great. It's good to have you in the class. I looked through your introductory paragraph and saw that you're interested in getting a master's degree. That's cool. I′m getting a master's degree too, so I highly recommend it. It's a good choice. Anyways, you seem to be really on top of things, but let me know if there's anything I can do to help you out this semester and we'll see you.”
After the semester was completed, all instructor feedback comments from the three major assignments were extracted from Canvas. Video feedback comments were transcribed in order to make coding more convenient. For example feedback comments, see Table 2.

3.3. Data analysis
We used the codes established by Rourke et al. (1999) as a guide for our inquiry. While Swan (2003) and Hughes, Ventura, and Dando (2007) also provided similar sets of indicators for social presence, we chose to use Rourke et al. (1999) because it is the most cited manuscript. These codes identified indicators of three major categories of social presence in online environments: affective responses, interactive responses, and cohesive responses. Since Rourke et al.'s (1999) indicators were designed specifically for text-based computer conferencing; we made some adjustments to each indicator to make them more specific to instructor feedback used in both text and asynchronous video format. We eliminated four indicators that were more applicable to group discussion than they were to individual feedback; namely, we did not code for inclusive pronouns, expressing agreement, or for quoting and referring to others' messages. We also brainstormed additional indicators of social presence specific to instructor feedback and included them in our coding scheme. The following are the final indicators of social presence we used to code feedback comments in text and video format (see Table 3).


Table 3. Indicators of social presence in text and video feedback.

Category	Indicator	Definition	Example in text feedback	Example in video feedback
Affective	Expression of emotions	Expression of emotion that differs from the instructor's general tone or demeanor. E.g. exclamation marks, capitalization, emoticons, smiling, laughing, animation, disappointment, and irritation.	“LOVE your glog:-)”	

Use of humor	Initiating or responding to humor. E.g. jokes, irony, sarcasm, teasing, laughing, “LOL,” “haha.”	“That's pretty sweet that your team won State 2 × in a row—if I ever need to give someone a swift kick in the pants, I'll call on you.”	“… That's funny. That's why a lot of professors don't like to write on the board these days.”
Self-disclosure	Presents details of life outside of class.	“I remember playing Bananagrams, I used to play all the time with a teenager that I worked with.”	“I hear you play piano and violin, I didn't know that, that's actually something we're exploring with our oldest right now, she is taking lessons right now for both.”
Visual self-disclosure	Visual and auditory stimuli present details of the instructor's life outside of class. Includes background visuals & background noise.	N/A	A spouse did a “thumbs up” in the background while the instructor gave feedback.

Interactive	Continuing a thread	Referring to previous conversations the instructor has had with that student. E.g. answering questions, connecting concepts.	“I appreciate your comment about the Promethean Board—it can definitely be more than just a glorified projector.”	“And like you said in your comments, if we had more technology…if we all had more technology, we'd basically all want iPads because they're the best, but it looks like you did a pretty good job with what you had.”
Asking questions	The instructor asks the student questions while giving feedback.	“Do you think students respond well having their work assessed in front of the whole class?”	“Hopefully things are going well for your new little baby. I believe this is your second one, right?”
Complimenting and expressing appreciation	Complementing the student's work, skill, or quality; Thanking the student for being in the class.	“Mary, Great video. It was very well done with a great mix of instruction (which was engaging) and humor. Good work.”	“Wonderful job on the interviews. I thought that was a really nice touch.”
Acknowledging students' self-disclosure	Acknowledging details of the student's life outside of class.	“Everything on the website is where it should be! That's a hilarious story about you and cursive writing in 3rd grade.”	“I see you've changed your last name as well… Congratulations, obviously you've gotten married.”
Cohesive	Vocatives	Addressing students by name.	“Cool video Riley!”	“Good job Casey”
Phatics, salutations	Communication that serves a purely social function; greetings, closures.	“I hope you've had a good experience in this class!”	“Hope you have a great semester and that things go well.”
General support	Support that is not specific to a particular aspect of an assignment. E.g. encouragement, helpfulness.	“If there's anything I can do to help you this semester, let me know!”	“If you have any other questions or suggestions, just shoot me an email and I′ll be more than happy to see what I can do.”
Specific support	Support that is specific to a particular aspect of an assignment. E.g. links to tutorials and articles, explanations on how to complete tasks.	“The link for video tutorials shouldn't go only to the tutorials for using Google Sites — it should go to the main website (since it has helpful tutorials on a BUNCH of different technologies), which is [website URL has been removed for peer-review purposes].”	“There's a YouTube video that can help you get your slideshow embedded: [video URL has been removed for peer-review purposes]”
Each instructor feedback message (see Table 2) was coded for each indicator of social presence. As a result a specific social presence indicator could be coded multiple times in the same feedback message. Additionally, one statement by an instructor could be coded for multiple indicators. For example, the statement “[student name], Great video!” in text feedback would be coded as Complimenting and Expressing Appreciation, Vocatives, and Expression of Emotions. If the instructor were to say “good job” later in the same feedback message, then Complimenting and Expressing Appreciation would be coded again. Apart from a couple of the indicators (e.g. “Vocatives” required the use of the student's name, which was usually one word), code phrases were used to indicate each social presence indicator. The purpose of the coding was to count the number of times each social presence indicator occurred in each feedback message.

We coded all feedback comments using Dedoose, an online tool used for coding qualitative data in the social sciences. Asynchronous video feedback was coded by both reading the transcriptions and watching the videos in order to code for visual self-disclosure and expression of emotions. Once coding was completed, we used Dedoose to count the number of times each indicator was coded for text feedback and video feedback. We then divided the total number of codes by the total number of feedback comments in each group to calculate averages. We also ran additional analyses in order to compare coding frequencies by assignment, instructor, and feedback mode.

3.4. Trustworthiness
After the lead researcher coded the first 20% of feedback comments, a second researcher reviewed the codes using the Dedoose interrater agreement test. This feature allowed the second researcher to see each excerpt the first researcher had coded and select the code they would apply to that excerpt. Once the second researcher had completed coding the excerpts, Dedoose provided a report with interrater agreement statistics for each code. After reviewing the report, the two researchers met and discussed discrepancies until interrater agreement reached over 80% on each code. Once this level of agreement was reached, the lead researcher completed the coding for the remaining feedback comments.

Before coding the video feedback for visual indicators such as expression of emotion and visual self-disclosure, the lead researcher met with the second researcher to review sample videos and discuss coding procedure. As the lead researcher completed the coding, the two researchers met once a week to discuss and refine coding criteria.

4. Results
One hundred ninety-six text feedback comments and 226 video feedback comments were coded for indicators of social presence and calculating average word counts (see Table 4).


Table 4. Social presence indicators for text and video feedback.

Indicator	Total frequencies in all feedback comments	Average frequencies per feedback comment
Text (196 comments total)	Video (226 comments total)	Text	Video
Affective	Expression of emotions	344	381	1.76	1.69
Self-disclosure	37	68	0.19	0.30
Use of humor	20	62	0.10	0.27
Visual self-disclosure	0	46	0	0.20
Cohesive	Addressing students by name	201	241	1.03	1.07
General support	59	156	0.30	0.69
Phatics	150	477	0.77	2.11
Specific support	44	76	0.22	0.34
Interactive	Acknowledging students' self-disclosure	25	57	0.13	0.25
Asking questions	101	36	0.52	0.16
Complimenting and expressing appreciation	543	974	2.77	4.31
Continuing a thread	37	30	0.19	0.13
Note. Average frequencies were calculated by dividing the total frequencies by the number of feedback comments.

4.1. Indicators of social presence
4.2. Word counts and assignment comparisons
While word counts are useful for description purposes, we warn against interpreting these counts too simplistically, as word counts do not necessarily measure quality or concision. Asynchronous video feedback comments were longer on average than text feedback comments, with video feedback averaging 190.46 words, and text feedback averaging 103.1 words. When comparing average word counts for the three major assignments, video feedback stayed consistent in length for Assignments 1 and 2, and dropped for Assignment 3. On the other hand, text feedback was longest for Assignment 1, and dropped for Assignments 2 and 3 (see Fig. 4).

Fig. 4
Download : Download high-res image (113KB)
Download : Download full-size image
Fig. 4. Average word counts of text and video feedback for Assignments 1, 2, and 3.

Feedback comments for Assignment 1 were lengthy in part because instructors were establishing their social presence at the beginning of the semester by following the instructions we gave. For example, identifying commonalities with students and welcoming the students to the class were especially relevant at the beginning of the semester. Specifically, the codes “General Support,” “Self-Disclosure,” and “Acknowledgement of Student Self-Disclosure” were coded more frequently in feedback for Assignment 1 than the other two assignments (see Fig. 5, Fig. 6, Fig. 7). This shows that instructors made more of an effort to get to know their students and offer their support during the first piece of feedback they gave during the semester, regardless of whether they used text or video to give that feedback.

Fig. 5
Download : Download high-res image (95KB)
Download : Download full-size image
Fig. 5. Average frequency of “general support” in text and video feedback for Assignments 1, 2, and 3.

Fig. 6
Download : Download high-res image (100KB)
Download : Download full-size image
Fig. 6. Average frequency of “self-disclosure” in text and video feedback for Assignments 1, 2, and 3.

Fig. 7
Download : Download high-res image (113KB)
Download : Download full-size image
Fig. 7. Average frequency of “acknowledgement of student self-disclosure” in text and video feedback for Assignments 1, 2, and 3.

5. Discussion
In this section we will first discuss the social presence indicators that were more frequently identified in video feedback followed by those more common in text feedback. We conclude this section by highlighting some of the contributions of this research, limitations of this research and recommendations for future research.

5.1. Indicators more frequent in asynchronous video feedback
Most indicators of social presence were coded slightly more frequently in video feedback. However, video feedback contained notably higher frequencies in only two codes: “Phatics, Salutations” and “Complimenting and Expressing Appreciation.” These codes were shown by instructors repeatedly telling their students they did a good job throughout their videos, as well as using social niceties such as “have a nice day” and “I'm glad you are in my class.” The tendency for instructors to include more phatics and compliments in video feedback could be why students and instructors in previous research have perceived that video feedback contained more praise (Borup et al., 2015), was more personal (Borup et al., 2014, Harper et al., 2012, Henderson and Phillips, 2015, McCarthy, 2015, Moore and Filling, 2012, Parton et al., 2010, Silva, 2012), and conversational in ways that more closely approached face-to-face interactions as compared to text (Borup et al., 2014, Silva, 2012, Thompson and Lee, 2012). Additionally, the higher frequency of these two codes could help explain why video feedback had a higher word count than text feedback for all three major assignments.

5.2. Indicators more frequent in text feedback
While most indicators of social presence were slightly more prevalent in video feedback, the codes “Expression of Emotions,” “Asking Questions,” and “Continuing a Thread” were coded slightly more frequently in text feedback.

5.2.1. Expression of emotions
In our analysis, we found a slightly higher frequency of expression of emotion in text feedback than we did in video feedback. This finding seems to conflict with Borup et al., (2014) findings, in which instructors reported that it was easier to express emotion through video, and students perceived video feedback to contain more emotion. However, we believe this discrepancy can partially be explained by the methods we used to code expression of emotion. In order to code for emotion in video feedback, we watched each video and coded for facial expression and voice tone that differed from the instructor's general demeanor. While some expressions were easy to code for, others were more ambiguous. Meanwhile, it was easier to code expression in text because we simply looked for exclamation points, all caps, and emoticons.

Additionally, emotional expressiveness varied between instructors (see Fig. 8). While some instructors like James and John showed a clear preference for expressing emotion through video feedback, other instructors like David expressed emotion more frequently in text feedback.

Fig. 8
Download : Download high-res image (117KB)
Download : Download full-size image
Fig. 8. Average frequency of “expression of emotion” in text and video feedback for each instructor.

To understand the differences in emotional expression further, we compared the feedback styles of James, John, and David. While James and John tended to use exclamation points sparingly in text feedback, David used them generously (for examples, see Table 5). Additionally, James and John tended to smile when addressing students and complimenting them in video feedback, while David visibly expressed emotion less frequently. This variation between instructors shows that while video is a rich medium that has the potential to convey emotion, emotional expressiveness itself is an “ability” that can vary between individuals (Garrison et al., 2000, p. 95). If an individual has difficulty conveying emotion through facial expression and voice tone, they may or may not have the same difficulty in text-based communication. For some instructors, it may be easier for them to be expressive in text feedback. For example, an instructor in Borup et al., (2014) research suggested that it was easier for him to fake emotion in text than in video feedback, or that he could “use an exclamation mark in text, when [he] wouldn't come across using an exclamation mark in person” (p. 240). Additionally, the use of emoticons may not convey emotion in the same way for everyone. While emoticons are often used to strengthen a message, express emotion, or express humor (Derks, Bos, & Von Grumbkow, 2008), some users tend to use emoticons habitually in internet related conversation (Salló, 2011). For these reasons, the question of whether video or text feedback can convey more emotion seems more dependent on personality differences and habits, and advice for which medium to use would vary accordingly.


Table 5. Text feedback: examples from James and David.

Example text feedback
James	“Great site. In order to earn the badge, I recommend a few changes: 1 – Remove comments on your pages. 2 – Flashcards. Great, but I recommend centering the module or expanding the width to remove some of the white space on the page. 3 – Your Portfolio page should not be blank. Add some text explaining why it is there. Good work [student name].”
David	“Great work, [student name]! Everything's where it needs to be, and it's even turned in early! Awesome! Hopefully in this class we can get you to become obsessed with technology and then you can spend the other half of your life savings on tech toys! (just kidding) Let me know if there's anything I can do to help you this semester.”
Even though emotional expression did not occur more frequently in video feedback than it did in text feedback, the instances of emotion in video feedback may have been more intense than text feedback because of the richness of the medium. This could be why students in previous research perceived emotions in video feedback to be more authentic (Borup et al., 2012, Borup et al., 2014, Henderson and Phillips, 2015). While an instructor could include a smiley face in their text feedback, students valued seeing their instructor genuinely smile, and trusted that smile more than the smiley face in text because “the face is credible” (Borup et al., 2014, p. 242). Therefore, the emotion conveyed in text and video feedback may not have had an equal effect on social presence, even though emotional expression occurred with about the same frequency in both modes.

5.2.2. Interactive indicators
Besides “Expression of Emotions,” the other two indicators that occurred more frequently in text feedback were “Asking Questions” and “Continuing a Thread.” Interestingly, both of these codes fall under the category of interactive responses. While students in previous studies (Borup et al., 2014, Thompson and Lee, 2012) perceived video feedback to be more conversational and interactive, and better at establishing rapport (West & Turner, 2015) (which could possibly be explained by the high frequency of complimenting and expressing appreciation in video feedback), students also said that they were more likely to respond to text feedback. Considering that most excerpts coded as “Continuing a Thread” involved instructors responding to student questions or concerns, it could be that instructors like David and Paul also felt more comfortable responding to questions in text form (see Fig. 11). This difference is significant, as students being able to ask questions (and assumedly receive answers) correlates with higher performance in the class (He, 2013).

In our study, instructors giving video feedback may have found it easier to ask questions in separate text comments outside of their video feedback, accounting for some of the difference. Since our instructors were assigned to only use one mode of feedback per assignment, we did not transcribe any text comments that instructors may have added to their video feedback. However, future research could investigate the frequency in which instructors add additional text comments to video feedback, and whether text comments added to video feedback share differences or commonalities with text feedback that is not accompanied by video. Overall, further research is needed to understand differences in interactive indicators between video and text feedback.

5.3. A follow-up analysis on instructor differences
Since emotional expressiveness in text and video feedback varied by individual instructors, we conducted a follow-up analysis to investigate whether instructor preference is a factor across all social presence indicators (see Fig. 9, Fig. 10, Fig. 11, Fig. 12, Fig. 13, Fig. 14, Fig. 15, Fig. 16, Fig. 17, Fig. 18). For affective indicators of social presence, there were individual differences in the way instructors showed affect through text and video feedback. For example, while most instructors showed more humor in video feedback, Robert showed slightly more humor in text feedback (see Fig. 9). Additionally, instructors differed where they showed self-disclosure, with Paul disclosing equally in text and video feedback, David, William, and Robert disclosing more in video feedback, and James disclosing more in text feedback (see Fig. 10).

Fig. 9
Download : Download high-res image (88KB)
Download : Download full-size image
Fig. 9. Average frequency of “use of humor” in text and video feedback for each instructor.

Fig. 10
Download : Download high-res image (96KB)
Download : Download full-size image
Fig. 10. Average frequency of “self-disclosure” in text and video feedback for each instructor.

Fig. 11
Download : Download high-res image (99KB)
Download : Download full-size image
Fig. 11. Average frequency of “continuing a thread” in text and video feedback for each instructor.

Fig. 12
Download : Download high-res image (87KB)
Download : Download full-size image
Fig. 12. Average frequency of “asking questions” in text and video feedback for each instructor.

Fig. 13
Download : Download high-res image (132KB)
Download : Download full-size image
Fig. 13. Average frequency of “complimenting and expressing appreciation” in text and video feedback for each instructor.

Fig. 14
Download : Download high-res image (103KB)
Download : Download full-size image
Fig. 14. Average frequency of “acknowledging student self-disclosure” in text and video feedback for each instructor.

Fig. 15
Download : Download high-res image (107KB)
Download : Download full-size image
Fig. 15. Average frequency of “addressing students by name” in text and video feedback for each instructor.

Fig. 16
Download : Download high-res image (95KB)
Download : Download full-size image
Fig. 16. Average frequency of “phatics” in text and video feedback for each instructor.

Fig. 17
Download : Download high-res image (86KB)
Download : Download full-size image
Fig. 17. Average frequency of “general support” in text and video feedback for each instructor.

Fig. 18
Download : Download high-res image (83KB)
Download : Download full-size image
Fig. 18. Average frequency of “specific support” in text and video feedback for each instructor.

Individual differences also emerged in interactive indicators of social presence. For example, Paul asked questions much more frequently in text feedback (see Fig. 12). While the majority of instructors complimented and expressed appreciation to their students more frequently in video feedback, William complimented his students more frequently in text feedback (see Fig. 13). William also acknowledged student self-disclosure very frequently in his video feedback, while the remaining instructors rarely acknowledged student self-disclosure in their feedback (see Fig. 14).

Frequencies of cohesive indicators of social presence also varied by instructor. For example, half of the instructors addressed students by name more frequently in text feedback while the other half addressed students by name more frequently in video feedback (see Fig. 15). While the majority of instructors used phatics more in video feedback, William used phatics more often in text feedback (see Fig. 16). When it came to support, Robert and William were the most generally supportive in video feedback (see Fig. 17) while Paul and William were the most specifically supportive in video feedback (see Fig. 18). However, other instructors showed very little difference in their frequency of support through text or video feedback.

This follow-up analysis indicates that the individual characteristics of the instructors influenced whether they displayed social presence indicators more often through text or video feedback. This further supports the notion that social presence is an “ability” that can vary between individuals (Garrison et al., 2000, p. 95) and across different modes of communication. Additionally, this analysis shows that even though instructors were encouraged to use elements of social presence in their feedback, the way in which the instructors did so varied due to communication mode and individual factors. Further research is needed to further investigate the individual characteristics that lead to the ability to effectively display social presence indicators in text and video feedback.

5.4. Research contributions
This research contributes to the literature on social presence by providing an analysis of communicative behavior in instructor feedback provided through text and video. While past research has found that students perceive video feedback to have high levels of social presence overall (Borup et al., 2012, Borup et al., 2014, Barrow, 2012, Ford, 2015, Griffiths and Graham, 2009, Thompson and Lee, 2012), few prior studies have included a content analysis of the feedback provided. The current analysis can be combined with previous research that utilized self-report methodology in order to achieve a more complete view of social presence as defined by Garrison et al. (2000).

In addition, the analysis provided in this article shows evidence that social presence can be identified in both text and video feedback. The authors achieved at least an 80% inter-rater reliability statistic for the coding of each social presence indicator, in contrast to Rourke et al. (1999), in which raters found difficulty reaching agreement on some of the more subjective social presence indicators. This suggests that the students receiving the feedback may have also been better able to recognize the subjective social presence indicators as well. While students likely still vary in their perceptions and interpretations of social presence indicators, this analysis shows that social presence indicators can be identified with agreement between different individuals. Furthermore, an important contribution of Rourke et al.'s (1999) research was a clear description of social presence indicators researchers could identify when coding discussion board comments. This enabled future researchers to have a more coordinated effort when coding comments. Similarly, this research may provide a better shared understanding of social presence indicators in video feedback comments that can aid future researchers to better identify and code indicators of social presence in video comments.

In summation, the current analysis provides valuable information for application and future analysis. With a small amount of instruction as to how to include social presence in feedback, and with instructor differences in the display of social presence between text and video feedback, average frequencies of social presence indicators were fairly close in text and video feedback. However, similar student samples have still perceived video feedback to have more social presence (Authors, 2014; Ford, 2015, West and Turner, 2015). This disparity between the frequency of social presence indicators and student perception suggests that factors other than the frequency of social presence might influence student perceptions. Additionally, this disparity also suggests that video feedback might be preferable to text feedback in establishing social presence even when instructors attentively include social presence indicators in their feedback. Future research can investigate factors that influence social presence other than frequency or visibility of social presence indicators.

5.5. Research limitations and future directions
The following are possible limitations of this research along with opportunities for future directions.

5.5.1. Analysis of social presence
While the purpose of this analysis was to objectively code feedback comments, there were several elements of social presence that we were not able to code in this study. Previous research has shown that students and instructors perceive asynchronous video feedback to be more effective at establishing social presence and rapport (Borup et al., 2014, West and Turner, 2015). While asynchronous video does allow recipients to see and hear affective, interactive, and cohesive indicators, the increase in social presence might in part have to do with factors other than what is said or seen in video. For example, students have generally perceived video feedback to be more personalized (Harper et al., 2012, Moore and Filling, 2012, Parton et al., 2010, Silva, 2012), and some students stated it was because they knew their instructor was communicating with them directly, and that the feedback was specifically for them (Borup et al., 2014). Similarly, Kim and Thayne (2015) found that in video-based instruction that made use of relationship-building strategies, students felt more positively about the instruction and had higher learning gains. In our current study, we coded for relationship-building strategies such as facial expression, voice tone, and visual self-disclosure, but we did not code for the feeling students might get when they see their instructor's face and hear their instructor give individualized feedback. Further research is needed in order to understand how much social presence depends on visual and auditory cues, and how much social presence comes from students simply seeing their instructor speaking directly to them in a personalized and specific way.

5.5.2. Gender of instructors
Seven of the eight instructors hired to teach Technology in Education courses during this semester were male, and the feedback comments given by the one female instructor were accidentally deleted from Canvas prior to data extraction. This resulted in an all-male sample of instructors in our analysis. While we are unaware of research as to how this gender bias may have affected feedback approaches in text and video format specifically, some studies imply that gender might affect preference for written or spoken communication. More specifically, Ching and Hsu (2015) found that female students were more likely to prefer audio and video discussion board comments over text and were more likely to report that the richness of audio and video enabled them to strengthen their connection with their peers. Caspi, Chajut, and Saporta (2008) also found that female students over-proportionally posted messages in web-based conferences, while male students over-proportionally spoke in face-to-face classrooms. In regards to online asynchronous environments, men and women have been found to differ in their use of emoticons in online text communication. For example, Wolf (2000) found that women were more likely to use emoticons to demonstrate humor, while men were more likely to use emoticons to show sarcasm. More recently, Brunet and Schmidt (2010) found that women's emoticon usage depended on the situation; women used more emoticons when they could be seen through a webcam than when their webcams were turned off. Additionally, gender may influence emotional expression in professional environments. For example, an analysis of human resource professionals found that women were more likely to suppress negative emotion and simulate positive emotion, while men were more likely to suppress positive emotion and simulate negative emotion (Simpson & Stroh, 2004). Overall, we acknowledge sampling feedback from only male instructors is a study limitation; feedback samples from female instructors may provide different results.

5.5.3. A primarily blended sample
Two of the eight sections of the courses that we analyzed were almost fully online (students met face-to-face once during the 14 week semester), and six of the sections were blended (students met face-to-face 4–5 times during the 14 week semester). Since students in these sections had the opportunity to meet with their instructors face-to-face, it might be that using video feedback to establish social presence was not as necessary as it would have been in sections taught fully online. This is supported by the finding by Borup et al., (2014) that some students in blended courses felt little need for relationship building in their feedback. However, since all sections of the Technology in Education courses met face-to-face less frequently than traditional face-to-face courses, we argue that asynchronous video feedback might be useful in increasing social presence in blended courses as well as fully online courses. However, we acknowledge that using a combination of blended and online courses for this analysis is a limitation of the design of this research. Further research is needed to determine usefulness of asynchronous video feedback in establishing social presence in blended courses. Additionally, future researches with samples of fully online courses are needed in order to evaluate the frequency and impact of social presence in fully online courses.

5.5.4. Different types of video feedback
In our study, we presented instructors with some principles for establishing social presence with their students, and allowed them to create videos (and text) feedback to best apply these principles. A few of the videos were screencasts that also showed a webcam of the instructor's face, but most comments were simple webcam videos. West and Turner (2015) studied screencast feedback specifically with first year education students, and found that nearly three times as many preferred the screencast feedback to text feedback, believing it to be clearer, of higher quality, and helpful to building rapport. Future research should investigate more specifically whether the various ways video can be produced and presented (screencast, webcam video, video with animation, etc.) has an influence on perceived social presence by comparing these video styles directly.

Additionally, some recent efforts have focused on creating video technology that could help instructors establish social presence with groups of students. For example, three-dimensional teleconferencing systems are being developed that can allow a speaker to make eye contact, gaze, and attention with multiple people at once (Jones et al., 2009). This could have implications for establishing social presence more efficiently with groups of students in one-to-many video communication, but more research is needed on its effectiveness.

5.5.5. Instruction of social presence
Prior to the beginning of the semester, instructors were given guidelines as to how to include social presence in their feedback. One reason that we provided instructors with this small amount of training was to ensure quality in student feedback. Since the data for this analysis came from field research, or feedback given for real assignments given to students taking teacher education courses in our department, we wanted to ensure that our instructors received training in subject matter that would improve student growth and retention. Even with these guidelines, instructors still showed a variety of individual differences in the ways that they displayed social presence in their feedback (see Fig. 9, Fig. 10, Fig. 11, Fig. 12, Fig. 13, Fig. 14, Fig. 15, Fig. 16, Fig. 17, Fig. 18). While giving these instructions undoubtedly impacted the frequency of certain social presence indicators, it allowed us to increase the quality of instruction provided to students. These guidelines also helped to provide consistency in instructors' efforts to establish the social presence in student feedback. However, we acknowledge that these guidelines could be considered a limitation of this research because we did not collect baseline data of instructor feedback prior to receiving these guidelines.

5.5.6. Mixing text and video feedback
Another aspect of our methodology that can be acknowledged is the assignment of both text and video feedback within each course. We did this so all of the instructors teaching the teacher education courses in our department could gain experience providing both text and video feedback. Additionally, this allowed all of the students to experience both forms of feedback. However, since patterns of social presence can change over the course of the semester, with instructors establishing their social presence toward the beginning of the semester, using both text and video in each course could have influenced the frequency of social presence indicators. This effect should have been counterbalanced with equal numbers of instructors assigned to text and video feedback for each assignment. However, we acknowledge this as a limitation because some of the instructor feedback was lost in data extraction due to technical difficulties (see Section 3.2). Future analyses of social presence can avoid this limitation by assigning only one feedback mode per course across all assignments.

5.5.7. Pairing self-report analyses with content analysis
This research sought to provide a more thorough view of Garrison et al.'s (2000) definition of social presence through the analysis of the communicative behavior of instructor feedback. We believe that this analysis can be added to the knowledge generated through previous literature that used self-report methods in order to more fully understand the construct of social presence. Future research should attempt to combine self-report methodology and content analysis of communication behavior in order to provide a more integrated investigation of social presence in instructor feedback.

5.6. Suggestions for practitioners
While more research is needed in order to fully understand how different feedback modes promote social presence, this research has identified several ways in which instructors might improve their social presence using online feedback.

5.6.1. Establishing social presence using asynchronous video
The instructors in our study put more effort into establishing their social presence during the feedback for the first assignment, regardless if that feedback was given using text or video. Specifically, instructors self-disclosed, acknowledged student self-disclosure, and provided general support more frequently in Assignment 1 than in subsequent assignments (see Fig. 5, Fig. 6, Fig. 7). If instructors wish to establish their social presence at the beginning of a course, it might be useful to strategically use video feedback for the first assignment. This way, instructors will be using the richest medium when they tend to put the most effort into establishing social presence. Additionally, effectively establishing social presence at the onset of a course might make it easier for instructors to maintain their social presence in following interactions, regardless of communication mode.

5.6.2. Emotional expressiveness
As illustrated in Fig. 8, instructors in our study varied in how often they expressed emotion in their text and video feedback. While some instructors easily expressed emotion using video, others more easily expressed emotion in text. If emotional expressiveness itself is an “ability” (Garrison et al., 2000, p. 95), and social presence is a “literacy” similar to technological literacy or rhetorical literacy (Dikkers, Whiteside, & Lewis, 2013), then we believe that elements of social presence can be taught and learned. For example, if instructors find that it is difficult for them to show expressiveness in either their text or video feedback, they might benefit from professional development in how to effectively use exclamation points, capitalization, and emoticons in text feedback and how to show more emotion using facial expression and voice tone in video feedback. However, Garrison et al. (2000) also explained that social presence is dependent on how communicative behavior is perceived by their receiver. As a result, it is not guaranteed that instructors who develop the ability to communicate effectively will still establish a high degree of social presence with each of their students and it is important that instructors work to understand their students' perceptions of their feedback comments.

5.6.3. Visual self-disclosure
In our study, visual self-disclosure was coded an average of 0.2 times per piece of video feedback. We believe that this infrequency of visual self-disclosure was a missed opportunity on the instructors' part. Although some forms of visual self-disclosure are largely unplanned (i.e. a family member walking past in the background), other forms can be thoughtfully planned. For instance, instead of sitting in front of a white wall, instructors can sit in front of a bookshelf that contains personal items such framed photographs. One example of an instructor who does this is Christopher Haskell at Boise State University, who records many of his videos in a room with his personal items such as his diploma, bicycle, and bookshelf arranged artistically in the background (for reference, see https://www.youtube.com/watch?t=252&v=1bCZkhdrn0k).

By deliberately using visual self-disclosure, instructors can help students get to know them as people as well as instructors. For example, students in Borup et al., (2014) study stated that seeing elements of visual self-disclosure such as toys on the ground helped them recognize that their instructor was “just a person too” (p. 243). Additionally, visual self-disclosure helped these students feel like they were more included in their instructors' lives, which enhanced feelings of closeness. Similarly, Walter et al. (2015) found in their comparison of human and non-human feedback via text, audio, and video that social presence generally was dependent on media richness, and was an important factor in participants perceiving the feedback to be useful.

6. Conclusion
To date, most research concerning social presence in online education has relied on the perceptions of students and instructors. While this research has been helpful in identifying advantages and disadvantages of text and asynchronous video feedback, it has not fully explained how different communication modes influence social presence because perceptions do not always indicate interaction quality. We attempted to address this gap by coding for affective, cohesive, and interactive indicators of social presence in text and video feedback comments.

Overall, the differences we found in observed social presence indicators between text and asynchronous video feedback were minimal. However, we warn against interpreting this finding too simplistically by concluding that instructors' social presence was the same regardless of the mode of communication. Our research only measured the frequency, and not quality, of the indicators. For instance, it is likely that a video of an instructor smiling has a larger impact on social presence than an emoticon in text feedback, especially considering that students have perceived video feedback to be more emotional (Borup et al., 2012, Borup et al., 2014, Henderson and Phillips, 2015). Additionally, our study did not take into account the feeling students might get by simply seeing their instructor communicate specifically to them. Therefore, further research is needed to fully understand how video and text feedback promote social presence in online courses.

