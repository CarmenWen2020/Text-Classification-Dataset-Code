Unequal stakeholder engagement is a common pitfall of adoption approaches of learning analytics in higher education leading to lower buy-in and flawed tools that fail to meet the needs of their target groups. With each design decision, we make assumptions on how learners will make sense of the visualisations, but we know very little about how students make sense of dashboard and which aspects influence their sense-making. We investigated how learner goals and self-regulated learning (SRL) skills influence dashboard sense-making following a mixed-methods research methodology: a qualitative pre-study followed-up with an extensive quantitative study with 247 university students. We uncovered three latent variables for sense-making: transparency of design, reference frames and support for action. SRL skills are predictors for how relevant students find these constructs. Learner goals have a significant effect only on the perceived relevance of reference frames. Knowing which factors influence students' sense-making will lead to more inclusive and flexible designs that will cater to the needs of both novice and expert learners.

Previous
Next 
Keywords
Self-regulated learning

Learning analytics dashboard

Sense-making

Student-facing learning analytics

Learner goals

Higher education

1. Introduction
The vast amount of learning data collected from online learning platforms gives us opportunities to understand and optimise learning like never before. While interest in learning analytics (LA) among higher education institutions (HEIs) continues to grow, unequal engagement of stakeholders at different levels is a crucial challenge that hinders large-scale adoption (Tsai et al., 2018). Only 6% of LA research in HEI published between 2012 and 2018 reported deployment of the tool coupled with students' or teachers' involvement (Viberg, Hatakka, Bälter, & Mavroudi, 2018). This top-down approach can lead to unequal buy-in among stakeholders and distrust in LA if their concerns are not acknowledged and addressed (Tsai & Gašević, 2017). The few studies that investigated students' perspectives on LA focused on understanding whether students recognise the benefits of LA and why and how data about their learning will be collected (Whitelock-Wainwright, Gašević, & Tejeiro, 2017). In order to build effective learning analytics interventions with long-lasting impact, higher education institutions need to focus on more than just students' concerns on the use of data. They need to understand how students interact with LA tools and which factors influence such interaction patterns.

Learning analytics dashboards (LADs) are tools that support students and teachers in making informed decisions about the learning and teaching process. More than half of existing dashboards are used in a higher education setting (Schwendimann et al., 2016). Such applications have the potential to be used as powerful metacognitive tools for learners (Charleer, Klerkx, Duval, De Laet, & Verbert, 2016; Durall & Gros, 2014) as they make learners aware of their learning performance and behaviour and can support reflection. However, synthesising meaningful learning data in a format intuitively understandable by students is not a trivial task. The mismatch between the interface design of such interventions and the lack of data literacy of its users is a significant concern with regards to the application and adoption of learning analytics (Kitto, Cross, Waters, & Lupton, 2015). At one end, higher education institutions and LA dashboard designers need to identify relevant learning data and then encode it into indicators, graphs, numbers, colours or text. At the other end, students need to decode this information, put it into context, evaluate and interpret it, and make decisions on how to proceed. This process on the learner's end is known as sense-making (Verbert, Duval, Klerkx, Govaerts, & Santos, 2013) and although there is plenty of research on how to build dashboards, we know very little about how learners read and interpret such graphic displays.

Through co-design strategies, higher education institutions can gather input directly from stakeholders to improve the usefulness and usability of LA interventions (Dollinger & Lodge, 2018). Generating higher engagement and feelings of ownership are indisputable benefits to be gained by using this approach (Treasure-Jones, Dent-Spargo, & Dharmaratne, 2018). At the same time, the outcomes of co-design sessions are highly dependent on the relatively low number of students participating in such workshops, their data literacy level, experience, motivations, and even their trust in the whole co-creation process (Dollinger & Lodge, 2018). Would the resulting dashboard designs be as effective when deployed on a grander scale with a heterogeneous population, university-wide for example? In most cases, the answer is no: more and more evidence shows that LADs should move away from a “one-size-fits-all” design philosophy (Gašević, Dawson, Rogers, & Gasevic, 2016; Jivet, Scheffel, Drachsler, & Specht, 2017; Teasley, 2017). Numerous factors, both external, e.g., instructional conditions, timeliness of feedback, as well as internal, e.g., motivation, goals and self-efficacy, affect academic success and the impact of feedback interventions (Gašević et al., 2016; Winne, 1996). While most studies looked at the impact of different LAD designs by measuring changes in behaviour, achievement and sometimes skill (Bodily & Verbert, 2017; Jivet, Scheffel, Specht, & Drachsler, 2018), less research focused on how to cater to the needs of different students. Thus, the question of how to choose the right ‘size’ for every learner arises.

With each design decision, we make assumptions about how learners will make sense of the information visualisation, and we expect that they will all reach a shared understanding of the analytics presented to them (Clow, 2012). Despite this expectation, we know very little about the process of dashboard sense-making. Dollinger and Lodge (2018) urged LA experts to “look beyond themselves and their own perspectives and expertise to innovate LA platforms and interventions”.

The present study is part of a broader initiative that aims to develop a learning analytics tool for learning design at Zuyd Hogeschool, a higher education institution in the Netherlands. The purpose of the system is to support teachers and students with insights into the learning process, in particular with the learning design of courses, the timing of feedback from and to students and their metacognitive competencies. Following a design science process (Hevner, March, Park, & Ram, 2004), in previous iterations, we identified challenges and requirements for such a system by strategically engaging teachers and students in the development of the system via focus groups and surveys. Results showed that both students and teachers requested highly personalised LA interfaces (Schmitz et al., 2018). Furthermore, students' self-reported metacognitive competencies were moderately correlated with their online activity and academic performance (Schmitz et al., 2018).

This paper delves deeper into the learners' perspective, trying to understand which dashboard features support students in turning the information displayed on dashboards into action and whether self-regulated learning skills and learner goals influence which features students find relevant on a dashboard. Self-regulated learning (SRL) theory, one important area of research within educational psychology, is the most common theoretical foundation used in the design of LADs (Jivet et al., 2017). In order to self-regulate, learners need a goal to strive towards (Pintrich, 1999). Once we know what dashboard features learners find relevant and how these two aspects relate to that perceived relevance, we could align the design of dashboards with the needs and level of experience of the learner, leading to more inclusive designs which scaffold the development of data literacy skills.

1.1. Sense-making with dashboards
From a psychological perspective, sense-making is a concept related to creativity, curiosity, comprehension, mental modelling and situation awareness. Sense-making has been defined as “a motivated, continuous effort to understand connections (which can be among people, places, and events) in order to anticipate their trajectories and act effectively” (Klein, Moon, & Hoffman, 2006a). Sense-making involves, on the one hand, the formation of a mental model, backwards-looking and explanatory, and on the other hand, a simulation of this model, which is forward-looking and anticipatory (Klein, Moon, & Hoffman, 2006b). In the context of LADs, Verbert et al. (2013) proposed the LA process model that describes awareness and reflection as the first two steps towards creating impact through behaviour changes. Reflection should be followed by sense-making, a phase in which students interpret the information displayed on the dashboard through which they gain new insights and decide on what they should do next. Considering both approaches, for this paper, we will expand this definition of sense-making to the process of understanding and interpreting visualised learning data and deciding on the next learning actions.

While there is much research about how to design LA visualisations (Bodily & Verbert, 2017; Charleer et al., 2016; Echeverria et al., 2018; Jivet et al., 2018; Martinez-Maldonado et al., 2016; Pérez-Álvarez, Maldonado-Mahauad, & Pérez-Sanagustín, 2018) and what students expect from LA (Roberts, Howell, & Seaman, 2017; Schumacher & Ifenthaler, 2018a; Whitelock-Wainwright et al., 2017), we know less about how students make sense of the learning information encoded on dashboards. Through interviews and survey, Corrin and de Barba (2014) explored students' interpretation of feedback delivered on LADs and showed that university students had strong abilities to reflect and plan. Both of these abilities are linked to SRL. Aguilar and Baek (2019) investigated the relationship between information seeking preference and help-seeking practices among college students, suggesting that students that could read graphs better might not seek help when needed.

Learners need a “representative reference frame” for interpreting their data (Wise, 2014). A few works investigated whether such anchor points for comparison influence students' sense-making. Aguilar (2016) explored motivational responses of at-risk college students to simple line graph visualisations that were designed with achievement-goal theory affordances, i.e., self-focused (mastery) or comparative (performance) information. The visualisations were perceived in a manner consistent with achievement goal theory, suggesting that design decisions influence students' response and sense-making. Using think-aloud protocols, Lim, Dawson, Joksimovic, and Gašević (2019) examined students' sense-making of LADs that used different reference frames: self-referenced, course-referenced and peer-referenced. In students' responses, they identified six themes around the reasons for students' preference of certain graphs: ease of understanding, whether it provides a breakdown of information, whether it shows trends, whether it facilitates comparison, appearance and accuracy. Their results indicated that baseline self-regulation did not influence affective responses and the motivational impact of the LADs used in the study. Following a similar research direction, Schumacher and Ifenthaler (2018b) investigated how university students' achievement goal orientation and academic self-concept, i.e. their preferred standard of comparison: others, past self or learning objectives, influenced what they expected from LA.

1.2. Self-regulated learning and learner goals
Gašević, Tsai, Dawson, & Pardo, 2019 highlighted the need to inform learning analytics by educational research and relevant practice as a key aspect to be considered when adopting LA. Self-regulated learning is the theoretical foundation mostly used for designing LADs (Jivet et al., 2017). SRL is described as a process that occurs in three phases: forethought, performance and reflection (Zimmerman, Boekarts, Pintrich, & Zeidner, 2000), and as a set of strategies, e.g., goal setting, strategic planning, time management, help-seeking (Pintrich, 2000). Learners with SRL skills are metacognitively, motivationally and behaviourally active actors in their learning, leading to higher academic achievements compared to novice learners (Broadbent & Poon, 2015; Cleary & Chen, 2009; Zimmerman & Martinez-Pons, 1990). Recent studies looked at online student behaviour and revealed that highly self-regulated learners behave differently from more novice learners. Kizilcec, Pérez-Sanagustín, and Maldonado (2017) showed that learners with stronger SRL skills were more likely to revisit previously studied course materials. Furthermore, goal setting and strategic planning predicted goal achievement, while help-seeking was associated with lower goal attainment. Another study matched sequences of student interaction in MOOCs with theory-based SRL strategies and identified three clusters of learners based on their behaviour patterns (Maldonado-Mahauad, Pérez-Sanagustín, Kizilcec, Morales, & Munoz-Gama, 2018).

Learner goals are an essential aspect of SRL (Pintrich, 1999; Zimmerman, 1990). Winne and Hadwin (2012) describe SRL as (i) identifying a gap between the current state and the desired state, and (ii) undertaking action to close this gap. Learner goals are an expression of what students want to achieve. In a higher education setting, Stark et al. (1989) distinguishes between academic goals and intellectual goals based on their origin and the motivations that lie behind them. Academic goals are more “functional and situational” and are associated with receiving a degree. Such goals are likely to be passed on by parents or society. On the other hand, intellectual goals are more likely to stem from intrinsic motivation, a genuine desire to learn and are less subject to change with the context. In another study, Pintrich (1999) distinguishes between three types of goal orientations depending on whether the learner focuses on (i) mastering the task, (ii) obtaining grades or pleasing others (parents or teachers), and (iii) comparing one's performance and ability to others. While investigating the role of motivation in promoting SRL among American college students, Pintrich (1999) found that mastery goals were strongly positively related to the use of cognitive and self-regulatory strategies. Extrinsic goals were negatively related to SRL and slightly positively related to performance, suggesting that focusing on extrinsic goals like obtaining grades seemed to help college students with regards to performance. A similar distinction was found in less formal learning environments. For example, in MOOCs, Littlejohn, Hood, Milligan, and Mustain (2016) discovered that MOOC learners' goals centred around the development of knowledge and expertise on the one hand, and gaining a certificate of completion on the other hand. These two goals affected the way learners used the course material and what markers they used as evidence of their learning. Thus, knowing student goals helps to identify factors that directly influence student behaviour or the support students need.

Within the SRL paradigm, learners are agents as the power of action lies with them (Winne, 2013; Zimmerman, 1990). Such learners plan, set goals, self-monitor and self-evaluate their knowledge acquisition process. They also assess the usefulness of external feedback, including LADs, and decide how to respond accordingly (Price, Handley, Millar, & O'donovan, 2010). Thus, LA interventions do not change learners' behaviour, but instead, they create opportunities for learners to develop the information they currently hold and support them in making decisions (Roll & Winne, 2015). The assumption that drives the research presented in this paper is that if learner goals and SRL skills influence students' learning behaviour, they might also influence how learners perceive and use dashboards. Therefore, in this study, we will look at how students' dashboard needs are influenced by their level of SRL skills and two goals: (a) mastering the topic: to learn as much as possible about the topic of the course and be highly effective - in line with a mastery goal orientation, and (b) passing the course: to complete the course and earn a certificate while being highly efficient - in line with an extrinsic orientation.

1.3. Research questions
In this work, we aim to address the following gaps in learning analytics dashboards literature. Firstly, current research is limited to exploring whether students can interpret dashboards and we know little about how students make sense of dashboards. Secondly, there is little research on what aspects influence the sense-making of LADs and what students need in order to decide on their next learning actions. In this work, we investigate the importance of two related aspects: SRL skills and learner goals. The following research questions frame our study:

RQ1 What dashboard design elements do learners use when interpreting information on a dashboard?

RQ2 Are learner goals related to the perceived relevance of dashboard design elements?

RQ3 Are self-regulated learning skills related to the perceived relevance of dashboard design elements?

RQ4 Is the relationship between self-regulated learning skills and the perceived relevance of dashboard design elements dependent on the goals learners have?

To gain an initial understanding of RQ1, we conducted a qualitative pre-study described in Section 2. We address the other three questions through an extensive quantitative study. With these insights, we can build more meaningful LADs which do not merely rely on the designers' assumptions on how learners will make sense of the visualisations. Furthermore, knowing which aspects influence how students perceive dashboards would lead to more flexible designs that will cater to the needs of both novice and experienced self-regulated learners.

2. Qualitative pre-study
Prior to the extensive quantitative study, we ran a qualitative pre-study in several courses on LA. This pre-study aimed to gain a deeper insight into what features learners notice when interpreting the information displayed on a dashboard (RQ1). We also looked for any indication of whether students find different dashboard elements relevant depending on their goals within a course. We describe the procedure that we followed and the outcomes that we further investigated in the quantitative study.

2.1. Context and methods
The study was integrated as an assignment in LA courses delivered by the authors of this paper to different stakeholder groups. The 23 students enrolled in the courses were professionals working as instructional designers, learning managers or coordinators of online learning centres as well as students enrolled in a Master on Educational Science. The goal of the assignment was to encourage the professionals following our course to empathise with the users of a LA tool. Looking at a system from the perspective of a user could bring useful insights that would support dashboard creators to improve their designs. As part of the assignment, students analysed and evaluated a LA dashboard mock-up from two learner perspectives: (A) their learner goal was mastering the topic: to learn as much as possible about the topic of the course and be highly effective and (B) their learner goal was passing the course: to complete the course and earn a certificate while being highly efficient (see Section 1.2 for why we chose these two goals).

The dashboard mock-up used in the study is presented in Fig. 1. The mock-up was designed as a learning resource to make course participants aware of pitfalls when designing student dashboards. For this purpose, the indicators and design elements included on the dashboard are among the most common design elements used on current dashboard designs (Bodily & Verbert, 2017). Its design was not connected to the learning design of our course, and the data presented on it was thus fictional. The description of the assignment given to the students/participants of our courses explained that the purpose of the dashboard was to support its users in self-regulating their learning. This entailed setting goals and planning their learning (e.g., possibility of selecting their motivation and objectives for the course at the top of the dashboard), monitoring their performance (e.g., both in terms of activities completed in the left panel but also learning behaviour indicators in the right panel) and triggering self-reflection and self-evaluation of their learning process (e.g., showing predictions as well as highlighting problematic areas with red). As the mock-up was a learning resource, we intentionally included several design flaws to bring the students' attention to them through their own experience. For example, the colours used were inconsistent (the progress bars use three different shades of blue that are not explained), design elements are not explained (although the grey portions of the progress bars on the left panel use the same shade of grey as the indicator predictions in the right panel, the relation is not clarified), the indicators are not displayed on scales. We left the design vague and open to interpretation on purpose in order to emphasise the multiple possible interpretations they have to be aware of when designing LA tools.

Fig. 1
Download : Download high-res image (807KB)
Download : Download full-size image
Fig. 1. The dashboard mock-up used in the qualitative study was used as a learning resource in a Trusted Learning Analytics course and it contains on purpose several design flaws.

All 23 students had to analyse the same mock-up presented in Fig. 1 from both learner goal perspectives, i.e., (A) mastering the topic of the course and (B) passing the course. For each perspective, we asked students to evaluate their performance. Furthermore, students had to explain how they would change their behaviour based on the feedback they received, what features of the dashboard motivate or demotivate them from achieving their goal and what else about their learning they would like to see on the dashboard. The text of the 23 submitted assignments was manually inspected and analysed in order to identify dashboard features that were noticed by students. We selected features mentioned by students in their performance evaluation as well as features that students found motivating, demotivating or wished to see on the dashboard. These selected features do not refer only to critique brought to the dashboard design but also features that were appreciated by students, offering us a more detailed picture of what students might look for on a dashboard. For example, from the following excerpt, “It is also interesting to see the ‘predictors’ which could act as ‘alerts’ to guide me.”, two items were extracted:” predictions of learning behaviour by the end of the course” and” seeing areas in need of improvement highlighted on the dashboard”. We identified 26 dashboard elements that we described next.

2.2. Insights
2.2.1. Dashboard elements that students ‘look at’
Students used a great number of details when interpreting the information displayed on dashboards. Furthermore, they had a long list of wishes with regards to what they would like to see on a dashboard. These ranged from types of indicators (e.g., competencies acquired, the knowledge gained, grades or even self-reflection on their progress) and ways of visualising them (e.g., broken down by weeks of the course or by topics covered by the course) to features contextualising the information displayed (e.g., norm- or criterion-based reference frames) and features that support taking action (e.g., planning support or recommendations).

The list of 26 dashboard elements identified in the text of the assignments was the basis for a survey that was used in the empirical study to assess how relevant such elements are for students' sense-making. These dashboard design elements are listed in Table 1.

2.2.2. Learner goal effect
We noticed several patterns between the answers in the two scenarios. Firstly, with regards to the types of information used for interpreting information on the dashboard in scenario A, students showed more interest in their competency levels and the knowledge that they gained. In contrast, students aiming to pass the course requested information on the completion and certification criteria. Grades were mentioned in both scenarios, although the reasons for wanting this information differed. When interested in the topic of the course, students considered grades as a proxy for how much they have learned (Student_6: “results of all grade assignments would be useful to display and record as I find it useful to have as a learner, in order to measure how ‘much’ I have ‘well’ learned”). In the second scenario, however, grades were connected with the requirements for completing the course (Student_6: “[grading] would be needed for me to understand how close I am to reach my goal against the needed scores.”). Secondly, predictions on behavioural indicators were a cause of concern for students in scenario A (Student_11: “It really demotivates me to see the predictions for the next few weeks, although I appreciate seeing before it happens”). These indicators were usually disregarded by students in scenario B, except for one student who wanted to see a prediction of their final grade.

Students requested additional features that would help them decide on a course of action in both scenarios. However, in scenario A, requesting recommendations on what topic to cover and areas where they could improve their knowledge (Student_2: “I would prefer […] recommended readings, videos or learning activities based on students' data that have been more successful than me in the areas where I failed”) was more common than in scenario B, where students requested additional information on the ‘metadata’ of the course activities (e.g., estimated time of completion) in order to be able to select the ‘path of least resistance’ towards passing the course (Student_14: “It would be interesting to know: average time of activities, in order to know if I'm in the right way and if I could end it on time”). Finally, in some cases, students explicitly requested different designs for their dashboard since their goals were different in the two scenarios (Student_10: “I would expect a completely different view [in scenario B] because the objective is very different”).

Our qualitative pre-study revealed that students use multiple dashboard elements when making sense of the displayed information, and the used elements might differ depending on what their goal is. We sought out to confirm these findings and to investigate the research questions outlined in Section 1.3 through an extensive quantitative study. More specifically, we wanted to find out whether learner goals and self-regulated learning skills affect the perceived relevance of dashboard elements.

3. Methods
3.1. Participants and study design
After the qualitative pre-study, we conducted an extensive quantitative study in September 2018 with 247 first- and second-year students enrolled at the faculty of computer science of Zuyd Hogeschool. Participation in the study was voluntary, and the participants did not receive any incentives. Moreover, in order to minimise the danger of not answering truthfully to the survey, students were made aware that the collected data was anonymous and their participation in the study would in no way affect their activity and grades at the institution. We collected full answers from 169 first-year Bachelor and Associate Degree students and 78 second-year Bachelor students. According to self-reported demographic data, the majority of students are male (87.9%). 26.7% of the students belong to the 16–18 age group, 33.2% are aged 19–20, and 25.1% are aged 21–23. The rest 15% of the students are 24 years old and above.

The data collection sessions lasted for 45 minutes and took place during 16 workshop sessions with 12–20 participants each. We followed a between-group experimental design, randomly assigning the participants between two experimental conditions. Participants under each condition were primed to complete the study tasks with a specific learner goal in mind: (A) mastering the topic or (B) passing the course. The number of participants in each condition was NA = 124 and NB = 123. Both conditions followed the same study procedure. In the introduction, the students were briefed about the purpose of the study, their role in the study, the data collected, how the data will be processed and their rights as data subjects. Students willing to participate were required to sign a consent form in order to participate in the study. Next, in order to establish a common language with the study participants, we presented a dashboard prototype described in the next section. Finally, the study participants filled out an online survey (see Section 3.2).

3.2. Measures and materials
3.2.1. Dashboard mock-up
For this quantitative study, we used a simplified dashboard mock-up compared to the pre-study (presented in Fig. 2). Although filling out the online survey did not require participants to inspect a dashboard mock-up, we wanted to establish a common language with learners and make sure they are familiar with the general concept of a learning dashboard. In order to reduce confusion about the design among participants, we removed all design flaws that we had intentionally added to the dashboard used in the qualitative pre-study. We used a single shade of blue to fill the progress bars, we removed the grey areas that represented predictions for the end of the week, and we added the actual value of the indicators in each progress bar. The learning behaviour indicators in need of improvement are marked with a red exclamation mark to attract attention. Furthermore, we reduced the number of activity indicators in the left panel to time invested, the percentage of reading material accessed and the percentage of graded assignments submitted. On the right panel, we reduced the number of learning behaviour indicators to three: presence on the online platform, interaction on the course discussion forum and timeliness of assignment submission. The description of each indicator was visible in a pop-up. The presented data was again fictional. The only difference between the dashboard mock-ups introduced in the two experimental conditions was that we adjusted the learner goal displayed at the top of the dashboard to prime participants for their assigned experimental condition.

Fig. 2
Download : Download high-res image (224KB)
Download : Download full-size image
Fig. 2. Dashboard mock-up used in the quantitative study for experimental condition A. In the experimental condition B, the goal at the top of the dashboard was set to ‘Pass the course’ and all other indicators remained the same.

3.2.2. Survey
Study participants were asked to fill out a survey that gathered (a) demographic data, (b) an assessment of their SRL skills, and (c) an assessment of the dashboard elements used for sense-making. The first set of questions collected demographic data about the participants: gender, age, highest educational level finished, and the school year they started their education at Zuyd Hogeschool.

To assess the students' level of SRL, we used four scales from the OSL-Q questionnaire (Barnard, Lan, To, Paton, & Lai, 2009): goal setting, time management, help-seeking and self-evaluation, 16 items in total. Among a wide array of available questionnaires that measure SRL (Roth, Ogrin, & Schmitz, 2016), we settled on the OSL-Q because the survey measures SRL in blended and online learning environments, the setting of our study, and it has been extensively validated in the literature showing an adequate overall internal consistency of scores with α = 0.90 (Barnard et al., 2009). Out of the six subscales that the OSL-Q measures, we selected only four as these were related to the dashboard elements that were included in the final part of our survey. For the four individual scales used in this study, values for Cronbach alpha ranged from .69 to .86, revealing sufficient score reliability on the subscale level (Barnard et al., 2009). All items in the SRL survey were rated on a 5-point Likert scale where 1 is ‘not at all true for me’, and 5 is ‘very true for me’. Higher scores on this scale indicate better self-regulation by students.

The third part of the survey gathered study participants' subjective evaluation of the relevance of 26 different dashboard elements for evaluating their performance and deciding on their next steps. The items included in this part were based on the results of the qualitative pre-study (see Table 1). These items were rated on a 5-point Likert scale where 1 is ‘extremely irrelevant’, and 5 is ‘extremely relevant’.

3.3. Data collection
The data collection yielded 255 full answers to the survey. After a careful screening of the answers, we removed eight entries as we had reasons to believe they were not truthful because the self-reported demographic data did not make sense (1 entry), the replies to the open question of the survey were unrelated to the topic (4) or the participant gave the same answer to all questions (3) in combination with a very short total time spent on answering the questions (below 4 minutesmin). Thus, the data used in the analyses for each of the valid 247 survey entries are:

•
learner goal, i.e., the condition to which the participant was assigned: mastering the topic (condition A) or passing the course (condition B)

•
SRL scores consisting of the 4 SRL subscale scores calculated as averages of the items within each of the four scales: goal setting, time management, help-seeking and self-evaluation, and the combined SRL score averaged over all the items

•
perceived relevance of 26 dashboard design elements from the dashboard sense-making survey

3.4. Data analyses
We answer the research questions by discussing the descriptive statistics for the collected data and the results of several statistical tests. In order to identify underlying constructs that support dashboard sense-making (RQ1) and to reduce the data set to a more manageable number of variables in the analyses, we conducted an exploratory factor analysis (EFA) on the 26 items of the sense-making survey. We favoured this analysis over a more theory-driven factorisation, e.g., a confirmatory factor analysis, because there are no theoretical models that explain how learners make sense of learning dashboards or other models that classify LA dashboard features that support sense-making. We explained the resulting three sense-making factors in Section 4.1 due to their significance to the study and the subsequent analyses.

Further, we used the Mann-Whitney test to identify significant differences between the perceived relevance of the three sense-making factors between our two experimental conditions, i.e., the learner goals assigned to study participants (RQ2). To determine the effect of SRL on the perceived relevance of dashboard elements (RQ3), we ran a multiple linear regression for each of the underlying sense-making factors identified through the EFA to verify the effect of the assigned learner goal and SRL on the perceived relevance of each factor. Since SRL is a complex construct and the combined SRL score in our study included four subscales, we wished to determine whether the perceived relevance of the three sense-making factors varies depending on the learner goal while controlling for the effects of the four SRL subscales (goal setting, time management, help-seeking and self-evaluation). Since the four subscales are highly correlated (see Table 8), a multiple regression analysis with the four subscales as predictors would not reveal the total effects of each subscale. Thus, we performed a MANCOVA analysis with the four SRL subscales as covariates, the learner goal as the independent variable and the three sense-making factors as dependent variables.

We investigated the effect of the learner goals on the predictive power of SRL skills (RQ4) by adding the interaction between the experimental condition and the overall SRL score as a predictor to the regression model from RQ3. We conducted all analyses with jamovi (https://www.jamovi.org/).


Table 1. Dashboard elements used as survey items in the sense-making survey used in the present study and descriptive statistics for the answers in the two experimental conditions: A. mastering the course and B. passing the course. Each item was rated on a 5-point Likert scale where 1 is ‘extremely irrelevant’ and 5 is ‘extremely relevant’.

Scenario A (N = 124)	Scenario B (N = 123)	Mann-Whitney	Effect size
Mean	Median	St.d.	Mean	Median	St.d.	p	Cohen's d
SM1	Seeing my overall grade	4.38	4	0.619	4.52	5	0.632	.043⁎	−0.23
SM2	Seeing indicators about the course activities that I completed	4.00	4	0.883	3.85	4	0.897	.146	0.17
SM3	Seeing indicators about how I learn	3.70	4	0.996	3.49	4	0.872	.054†	0.23
SM4	Seeing requirements for passing the course	4.46	5	0.655	4.59	5	0.600	.110	−0.20
SM5	Having my goal at the top of the dashboard as a reminder of my motivation and objectives	3.72	4	1.064	3.44	3	1.153	.058†	0.25
SM6	Seeing my performance in comparison to what is maximum activities possible in the course	3.81	4	0.871	3.56	4	0.993	.048⁎	0.26
SM7	Seeing my performance in comparison to the other students	3.17	3	1.215	2.89	3	1.186	.078†	0.23
SM8	Seeing my performance in comparison to my past performance	4.05	4	0.785	3.81	4	0.917	.062†	0.28
SM9	Seeing my performance in comparison to my goals	4.00	4	0.865	3.97	4	0.829	.688	0.04
SM10	Seeing my areas in need of improvement highlighted on the dashboard	4.14	4	0.758	4.05	4	0.756	.320	0.12
SM11	Seeing the predictions of my learning behaviour by the end of the course	3.40	4	1.011	3.36	3	0.959	.624	0.04
SM12	Having a standard to compare my information to	3.85	4	0.766	3.67	4	0.816	.063†	0.23
SM13	Having explanations of how dashboard elements and information relate to each other	3.45	4	0.859	3.31	3	0.888	.173	0.16
SM14	Having explanations of how information is calculated	3.53	4	1.024	3.59	4	0.966	.811	−0.05
SM15	Having explanations of how the information is relevant to my goal	3.64	4	0.931	3.67	4	0.825	.942	−0.04
SM16	Having explanations of how the information is relevant to my learning	3.57	4	0.956	3.63	4	0.792	.670	−0.07
SM17	Having explanations on the scales on which this information is displayed	3.52	4	0.860	3.30	3	0.789	.019⁎	0.26
SM18	Having an overview over my information from the beginning of the course up to the current week.	3.77	4	0.856	3.82	4	0.758	.665	−0.07
SM19	Having my information broken down by topics covered by the course.	3.74	4	0.845	3.87	4	0.768	.239	−0.16
SM20	Having a consistent use of colours.	3.58	4	1.155	3.64	4	1.095	.674	−0.05
SM21	Being able to set goals and edit them	3.82	4	0.902	3.86	4	0.852	.661	−0.04
SM22	Being able to access the content of the course where I have difficulties directly from the dashboard	3.96	4	0.887	4.07	4	0.748	.437	−0.14
SM23	Receiving information that helps me plan my learning (e.g. estimated time need for each lesson)	3.94	4	0.895	3.91	4	0.878	.837	0.04
SM24	Receiving recommendations on how I could change my learning behaviour to learn more efficiently	3.75	4	1.072	3.81	4	0.995	.764	−0.06
SM25	Receiving recommendations on what topics I need to cover next or which topics I should redo	3.96	4	0.810	3.98	4	0.810	.694	−0.03
SM26	Being able to contact the teacher through the dashboard	3.62	4	1.025	3.88	4	0.920	.051†	−0.26
⁎ p < 0.05.; † p < 0.1.

4. Results
The findings of the study are reported in the following sections. In the first part, the overall survey results will be examined, and the factors resulting from the exploratory factor analysis will be presented (RQ1). We will then proceed with investigating the effect of learner goals on sense-making (RQ2) and examining the effect of SRL on sense-making (RQ3). Finally, we will investigate the differences between the two experimental conditions concerning the effects of the overall SRL score on each of the sense-making factors (RQ4).

4.1. Dashboard elements relevant for sense-making (RQ1)
We briefly present the results of the full survey across all 247 entries in Table 1. As a first step, we looked at the highest-rated items in each of the two conditions, (A) mastering the topic of a course and (B) passing the course. Among the ten highest rated items, seven items are common between the two conditions. In both conditions, seeing the requirements for passing the course as well as seeing the overall grade were the most relevant items. Further, participants with a mastery goal rated the following items very relevant: seeing the areas in need of improvement highlighted on the dashboard, seeing their performance in comparison to their past performance and seeing information about the completed course activities. In condition B, the highly relevant items were: being able to access content directly from the dashboard, seeing the areas in need of improvement highlighted on the dashboard and receiving recommendations on what topics to cover next. The lowest rated item in both conditions is “seeing one's performance in comparison to the other students”.

In order to identify underlying themes that support dashboard sense-making among the 26 dashboard elements evaluated in the sense-making survey, we conducted an exploratory factor analysis (EFA). Before running the EFA, we checked the factorability of the 26 items. The overall Kaiser-Meyer-Olkin (KMO) measure of sampling adequacy was .834, although four items presented individual KMO values below .70 (SM1, SM7, SM20 and SM26). Once we removed these four items, the new KMO value was .866 (‘great’ according to Hutcheson and Sofroniou (1999)). Bartlett's test of sphericity is highly significant, χ2(231) = 1739, p < .001, indicating that correlations between items were sufficiently large for factor analysis. We conducted the EFA on the 22 remaining items with oblique rotation (Oblimin). The analysis identified five factors based on parallel analysis. However, several items were discarded because they presented cross-loadings (1 item), or they did not meet the loading threshold of 0.4 on any factor (4 items). Table 2 shows the items loading on each resulting factor. We set the factor loading threshold to 0.4, an adequate threshold considering our sample size (Pituch & Stevens, 2015).


Table 2. Exploratory Factor Analysis using Oblimin rotation and number of factors determined by parallel analysis: factor loadings (>0.4), factor reliability (Cronbach's α) and percentage of variance explained by each factor.

Factors
Sense-making survey items	1	2	3	4	5
Factor 1: Transparency of design
SM13	Having explanations of how dashboard elements and information relate to each other	.466				
SM14	Having explanations of how information is calculated	.536				
SM15	Having explanations of how the information is relevant to my goal	.737				
SM16	Having explanations of how the information is relevant to my learning	.678				
SM17	Having explanations on the scales on which this information is displayed	.657				

Factor 2: Data & reference frames
SM2	Seeing indicators about the course activities that I completed		.417			
SM3	Seeing indicators about how I learn		.631			
SM6	Seeing my performance in comparison to what is maximum activities possible in the course		.407			
SM8	Seeing my performance in comparison to my past performance		.496			
SM11	Seeing the predictions of my learning behaviour by the end of the course		.432			

Factor 3: Support for action
SM23	Receiving information that helps me plan my learning (e.g. estimated time need for each lesson)			.524		
SM24	Receiving recommendations on how I could change my learning behaviour to learn more efficiently			.780		
SM25	Receiving recommendations on what topics I need to cover next or which topics I should redo			.535		

Discarded items
SM4	Seeing requirements for passing the course					
SM5	Having my goal at the top of the dashboard as a reminder of my motivation and objectives					
SM10	Seeing my areas in need of improvement highlighted on the dashboard					
SM12	Having a standard to compare my information to					
SM18	Having an overview over my information from the beginning of the course up to the current week.				.671	
SM19	Having my information broken down by topics covered by the course.				.566	
SM21	Being able to set goals and edit them					.535
SM22	Being able to access the content of the course where I have difficulties directly from the dashboard					.474
SM9	Seeing my performance in comparison to my goals		.422			.458
Chronbach's α	.777	.672	.756	.596	.581
% of variance explained	11.06	10.12	8.17	7.69	6.22
The EFA identified five factors for evaluating one's performance and deciding on the next steps, i.e., sense-making. Next, we closely inspected the resulting factors in order to determine whether the items fit together semantically and whether they implement the same design requirement for a LAD.

Factor 1 Transparency of the design - groups five items that include the word “explanations”, suggesting that they bring clarifications, open up the obscure inner-workings of the LA system to the user and make it easier for users to grasp the algorithms behind the LAD. These features include explanations on how different dashboard elements relate to each other, how indicators are calculated, why they are relevant for learning and goal achievement, and what the scales are on which the indicators are displayed. Thus, items grouped under this factor address the transparency of the design.

Factor 2 Reference frames - includes five items that describe types of learning indicators displayed on the dashboard (e.g., indicators about course activities completed and indicators about how students learn), and anchor points for comparison that students can use to interpret whether they are performing well or not. Literature labels such anchor points “reference frames” (Jivet et al., 2017; Wise, Zhao, & Hausknecht, 2014).

Factor 3 Support for action - clusters three items that encourage and help students to follow-up on the feedback they received through the dashboard and take concrete actions: recommendation on what course topics to tackle next, receiving additional information on topic difficulty and the estimated time of completion, as well as recommendations on how to change their behaviour.

While the first three factors include several items and provide a meaningful aspect related to dashboards, factors 4 and 5 group few items that do not fit together semantically and present a low reliability score. Thus, as we do not have enough information to make inferences about these two factors, we did not include them in the following analyses. Therefore, for this study, we reduced the original 26 items in the sense-making survey to three sense-making factors resulting from the EFA that describe the relevance of design elements for sense-making: transparency of design (factor 1), data and frames of reference (factor 2) and support for action (factor 3). Together, the three factors explained 29.35% of the variance within our dataset.

4.2. Learner goals and sense-making (RQ2)
In order to check whether there are differences between how learners rated the relevance of dashboard elements in the two experimental conditions, i.e., aiming to (A) master the topic or (B) pass the course, we ran Mann-Whitney tests for the three sense-making factors. As Table 3 shows, we found significant differences between the perceived relevance of the reference frames factor. The items under this factor received consistently higher ratings from participants with a mastery goal compared to the participants in condition B, i.e., passing the course. More specifically, when aiming to master the topic of the course, students rated seeing their performance in comparison with the maximum possible in the course (MA = 3.81, SDA = 0.87, MedA = 4; MB = 3.56, SDB = 0.99, MedB = 3; p = .048; d = 0.263), seeing learning behaviour indicators (MA = 3.70, SDA = 1.00, MedA = 4; MB = 3.49, SDB = 0.87, MedB = 44; p = .054; d = 0.228) and seeing their performance in comparison to their past performance (MA = 4.05, SDA = 0.79, MedA = 4; MB = 3.81, SDB = 0.92, MedA = 4; p = .062; d = 0.276) consistently higher. The effect size for the Mann-Whitney tests measured by Cohen's d are small to medium (Sawilowsky, 2009).


Table 3. Descriptive statistics on the perceived relevance for the three sense-making factors, the results of the Mann-Whitney tests and the effect sizes of the Mann-Whitney tests.

A (N = 124)	B (N = 123)	Mann-Whitney	Effect size
Mean	Median	St.d.	Mean	Median	St.d.	p	Cohen's d
Transparency of design	3.54	3.60	0.690	3.50	3.60	0.606	.528	0.063
Reference frames	3.79	3.80	0.615	3.61	3.60	0.590	.033⁎	0.294
Support for action	3.88	4.00	0.756	3.90	4.00	0.745	.729	−0.024
⁎
p < 0.05.

4.3. Self-regulated learning and sense-making (RQ3)
Before running the multiple regression analyses, we verified that there are no significant differences between the study participants in condition A and condition B with regards to the reported SRL scores. Table 4 presents the descriptive statistics of the four SRL sub-scale scores, the combined SRL score and the results of the Mann-Whitney comparing the SRL scores between the two experimental conditions.


Table 4. Descriptive statistics of the SRL scores for the students under the two experimental conditions. There are no significant differences between the two groups with regards to the level of SRL skills.

Scenario A (N = 124)	Scenario B (N = 123)	Mann-Whitney	Effect size
Mean	Median	St.d.	Mean	Median	St.d.	p	Cohen's d
Overall SRL score	3.34	3.31	0.497	3.25	3.31	0.480	.238	0.176
SRL: Goal setting	3.35	3.40	0.609	3.26	3.40	0.544	.418	0.147
SRL: Time management	3.22	3.30	0.779	3.10	3.30	0.826	.333	0.150
SRL: Help-seeking	3.51	3.50	0.631	3.42	3.50	0.621	.168	0.135
SRL: Self-evaluation	3.29	3.50	0.744	3.23	3.30	0.648	.379	0.077
We used multiple regression analyses, one for each of the three sense-making factors, to investigate if the combined SRL score and the learner goal significantly predicted participants' perceived relevance of the three sense-making factors. As the learner goal is a binary predictor, we set condition B as the reference value for the analyses. All three regression analyses passed the autocorrelation and collinearity assumption checks, and all three equations are significant (see Table 5). The results presented in Table 5 show that the combined SRL is a highly significant predictor (p < .001) for all three sense-making factors. There is a positive relationship between the combined SRL score and each sense-making factor, suggesting that the higher the combined SRL score of a learner is, the more relevant they consider sense-making factors. On the other hand, the learner goal is not a significant predictor in either of the equations. The equations explain only 4.7% of the variance in the dataset for transparency of design, 12.8% for reference frames and 6.5% for support for action, suggesting that there are other aspects apart from SRL skills and learner goal that influence the relevance of these factors in sense-making.


Table 5. Coefficients of the multiple linear regressions with transparency of design, reference frames and support for action as the dependent variables. Condition B was used as the reference level for the binary predictor learner goal.

Transparency of design	Reference frames	Support for action
B	SE B	β	p	B	SE B	β	p	B	SE B	β	p
Intercept	2.58	0.28			2.36	0.25			2.60	0.32		
Learner goal (B)	0.02	0.08	0.01	0.838	0.14	0.07	0.12	.052	−0.05	0.09	−0.03	.579
Overall SRL score	0.29	0.08	0.22	<.001⁎⁎⁎	0.41	0.07	0.33	<.001⁎⁎⁎	0.39	0.10	0.26	<.001⁎⁎⁎
R2	0.047	0.128	0.065
F(df1, df2)	6.01 (2, 244)	17.9 (2, 244)	8.52 (2, 244)
p	.003⁎⁎	<.001⁎⁎⁎	<.001⁎⁎⁎
⁎⁎⁎
p < 0.001.

⁎⁎
p < 0.01.

Since SRL is a complex construct and the combined SRL score in our study included four subscales, we determined whether the perceived relevance of the three sense-making factors varies depending on the learner goal while controlling for the effects of the four SRL subscales at the same time through a MANCOVA analysis. Computing Wilks' Lambda (Λ), the results of the multivariate tests (see Table 6) show that two SRL sub-scales, goal setting and help-seeking, had a significant effect on the perceived relevance of the three sense-making factors. We followed up with separate univariate tests on the outcome variables in order to see which relationships between the four SRL subscales and the three sense-making factors are contributing to these outcomes.


Table 6. Results of the MANCOVA using Wilks Lambda (Λ) with the learner goal as the independent variable, the four SRL subscales as covariates and the three sense-making factors as dependent variables.

Wilks' Lambda (Λ)	F (df1, df2)	p
Learner goal	.992	0.66 (3, 239)	.060
SRL: Goal setting	.899	8.99 (3, 239)	<.001⁎⁎⁎
SRL: Time management	.949	4.25 (3, 239)	.419
SRL: Help seeking	.964	2.94 (3, 239)	.016⁎
SRL: Self-evaluation	.970	2.48 (3, 239)	.095
⁎⁎⁎
p < 0.001.

⁎
p < 0.05.

The results presented in Table 7 show that there are particular relations between the SRL sub-scales and the three sense-making factors. For instance, help-seeking and self-evaluation skills influence the relevance of the transparency of design factor. There is a significant relationship between the relevance of reference frames and the level of goal-setting skills and help-seeking. Finally, the perceived relevance of features under the support for action factor are significantly affected by goal setting skills. Each SRL sub-scale correlates positively with each of the three sense-making factors (see Table 8).


Table 7. Results of the univariate tests following up the MANCOVA analysis.

Transparency	Reference frames	Support for action
SS	df	MS	F	p	SS	df	MS	F	p	SS	df	MS	F	p
Learner goal	0.104	1	0.104	0.26	.612	1.941	1	1.941	5.93	.016⁎	0.020	1	0.0201	0.04	.846
SRL: Goal setting	1.400	1	1.400	3.46	.064	5.789	1	5.789	17.69	<.001⁎⁎⁎	5.813	1	5.8125	10.91	.001⁎⁎
SRL: Time mgmt	0.314	1	0.314	0.78	.379	0.905	1	0.905	2.77	.098	0.422	1	0.422	0.79	.374
SRL: Help seeking	2.222	1	2.222	5.50	.020⁎	3.013	1	3.013	9.21	.003⁎⁎	1.438	1	1.4379	2.70	.102
SRL: Self-evaluation	2.079	1	2.079	5.14	.024⁎	0.374	1	0.374	1.14	.286	1.850	1	1.8496	3.47	.064
Residuals	97.437	241	0.404			78.848	241	0.327			128.428	241	0.5329		
⁎⁎⁎
p < 0.001.

⁎⁎
p < 0.01.

⁎
p < 0.05.


Table 8. Pearson correlation coefficients for the relations between the three sense-making factors, the overall SRL score and the four SRL subscales.

Overall SRL	SRL: GS	SRL: TM	SRL: HS	SRL: SE	Transparency	Reference frames	Support for action
Overall SRL	–	.783⁎⁎⁎	.688⁎⁎⁎	.696⁎⁎⁎	.775⁎⁎⁎	.216⁎⁎⁎	.338⁎⁎⁎	.253⁎⁎
SRL: Goal setting		–	.473⁎⁎⁎	.364⁎⁎⁎	.421⁎⁎⁎	.118	.262⁎⁎⁎	.204⁎⁎
SRL: Time management			–	.238⁎⁎⁎	.367⁎⁎⁎	.105	.217⁎⁎⁎	.144⁎
SRL: Help-seeking				–	.471⁎⁎⁎	.184⁎⁎	.277⁎⁎⁎	.172⁎⁎
SRL: Self-evaluation					–	.225⁎⁎⁎	.243⁎⁎⁎	.225⁎⁎⁎
Transparency of design						–	.451⁎⁎⁎	.384⁎⁎⁎
Reference frames							–	.432⁎⁎⁎
Support for action								–
⁎⁎⁎
p < 0.001.

⁎⁎
p < 0.01.

⁎
p < 0.05.

4.4. Interaction effects (RQ4)
With RQ4, we investigated whether learner goals influence the effect of SRL skills on how learners perceive the relevance of sense-making factors. In other words, is the relationship between the combined SRL score and the perceived relevance of the sense-making factors different in condition A compared to condition B?

Fig. 3 illustrates the relationships between the combined SRL score and the perceived relevance of the three sense-making factors. In all three cases, we noted positive relationships both in condition A and in condition B. Furthermore, the regression lines in condition A are steeper than in condition B, suggesting a stronger relationship in condition A than in condition B. Put differently, for the same increase in SRL score, there is a higher increase in the score of the sense-making factor when the learner goal is mastering the topic than when the learner goal is to pass the course.

Fig. 3
Download : Download high-res image (536KB)
Download : Download full-size image
Fig. 3. Scatterplots of the relationship between the SRL score and the relevance of the three sense-making factors with linear regression lines for the two conditions: (A) master the course content (blue) and (B) pass the course (yellow).

To provide statistical support for these observations, we extended the regressions conducted under RQ3 by adding the interaction term between the two initial predictors, i.e., learner goal and the combined SRL score, as an input variable to the model. The regression coefficients are presented in Table 9. While the combined SRL score remained a significant predictor for all three sense-making factors, we noted a significant interaction effect only for transparency of design. This result suggests that when students are pursuing a mastery goal, they are inclined to value the transparency of the dashboard design more than when they are striving for a passing grade. We did not observe the same effect with regards to the other two sense-making factors.


Table 9. Coefficients of the multiple linear regressions with transparency of design, reference frames and support for action as the dependent variables. Condition B was used as the reference level for the binary predictor learner goal.

Transparency of design	Reference frames	Support for action
B	SE B	β	p	B	SE B	β	p	B	SE B	β	p
Intercept	2.60	0.28			2.37	0.25			2.61	0.32		
Learner goal (B)	−1.08	0.55	−0.83	.052	−0.54	0.50	−0.45	.274	−0.82	0.63	−0.55	.199
Overall SRL score	0.28	0.08	0.21	<.001⁎⁎⁎	0.40	0.07	0.32	<.001⁎⁎⁎	0.39	0.10	0.25	<.001⁎⁎⁎
Learner goal (B) × SRL	0.33	0.17	0.26	.046⁎	0.21	0.15	0.17	.163	0.23	0.19	0.16	.155
R2	0.062	0.135	0.071
F (df1, df2)	5.39 (3, 243)	12.6 (3, 243)	6.19 (3, 243)
p	.001⁎⁎	<.001⁎⁎⁎	<.001⁎⁎⁎
⁎⁎⁎
p < 0.001.

⁎⁎
p < 0.01.

⁎
p < 0.05.

5. Discussion
In our empirical study, we explored higher education students' perspectives on what they find relevant on a LAD and whether this perceived relevance is affected by their SRL skills and learner goals. Through a qualitative pre-study, we identified 26 dashboard elements that students reason about while ‘reading’ a learning dashboard. We followed up with an extensive quantitative study where 247 students rated how relevant these 26 dashboard elements are for evaluating their performance and choosing their next steps while being primed to assume one of two learning goals: mastering the topic of the course or passing the course. We collected self-reported data about the study participants with regards to their SRL skills on four sub-scales: goal setting, time management, help-seeking and self-evaluation, and conducted analyses to determine relations between the perceived relevance of the dashboard features, SRL skills and learner goals. We uncovered three latent variables for sense-making: transparency of design, reference frames and support for action. When embedded in dashboard designs, these constructs are related to how receptive students are to such interventions. We discuss the relationships between learner goals, SRL skills and these three constructs, and we conclude the section with outlining implications for dashboard design in higher education settings.

5.1. Dashboard features that support sense-making
The dashboard elements clustered under the construct transparency of design relate to embedding explanatory information in the design. Such explanations can be of great help in assisting learners in understanding what sort of information they are receiving, how the shown indicators were calculated and why they are relevant for their learning and their goals. The role of transparency has been extensively researched in human-computer interaction (Hamilton, Karahalios, Sandvig, & Eslami, 2014). Transparency can foster trust in systems that otherwise might seem like a ‘black box’ (Drachsler, 2018; O'Donovan & Smyth, 2005), but providing too much information can diminish this trust (Kizilcec, 2016). Trust is an important issue that needs to be addressed in the LA field as the extent to which the data is trustworthy to support decision making is a common concern among stakeholders (Bodily et al., 2018; Drachsler & Greller, 2016). Indeed, students would engage with the system if they trusted the data and understood how the ‘scores’ are calculated (de Quincey, Briggs, Kyriacou, & Waller, 2019). In this aspect, the LA community can build on literature in open learner model (OLM) research where fostering student trust in the system through “inspectable or negotiated” learner models has been more prominent (Bull & Kay, 2016).

The second underlying construct for sense-making, reference frames, brings together dashboard elements that create anchor points for comparison that students need in order to interpret their data (Wise, 2014) and features that facilitate this comparison. Through the qualitative pre-study, we uncovered four standards which students could use for comparison: what is maximum possible in the course, their goals, their past performance, and other students' performance. These four standards fit under the three reference frames identified by Jivet et al. (2017): achievement, progress and social. We were puzzled to discover that the social-referenced frame of comparison was the lowest rated item among 26 others by students in our study, although it is the most common feature implemented in existing LADs (Jivet et al., 2017). One possible explanation for this inconsistency can be attributed to differences in cultural contexts where LADs were implemented and tested. Culture is defined as the ensemble of core values society has and the mindsets, attitudes, practices, behaviours and role-models that reflect these values (Hofstede, 1991). According to Hofstede's model for cultural differences across organisations and nations, the masculinity vs femininity dimension describes the dominant gender role patterns, i.e., the learned standards of interpersonal interaction considered socially appropriate (Hofstede, 2001). In masculine societies, assertiveness and competitiveness are dominant, while at the other end of the spectrum, feminine societies value cooperation and collaboration highly, caring for the weaker society members and quality of life. In educational contexts, in masculine societies, students compete with each other in class and failure in school can seriously damage one's self-esteem. In contrast, in feminine societies, students practice mutual solidarity and failure in school is seen as a minor incident and more as an opportunity for personal development (Hofstede, 1986). An overwhelming majority of published LA research is set in the US, UK, Australia and more recently Japan, a pattern reflected in the high number of publications with authors affiliated with universities from these countries (Ochoa & Merceron, 2018). Considering that these countries score high on the masculinity dimension (Hofstede, 2001), the popularity of social comparison and studies that support its effectiveness on LA dashboards (Davis et al., 2017; Guerra, Hosseini, Somyurek, & Brusilovsky, 2016; Haynes, Teasley, Hayley, Oster, & Whitmer, 2018) are not unfathomable. Our study was set in the Netherlands, one of the countries with the highest femininity score, which could explain the disinterest of the students in comparing their performance to their peers. However, these are just early observations for this assumption. The learning analytics community is lacking an empirical base to describe the effect that culture might have on the design of dashboards and the preferred reference frame for students that grow up in a particular cultural context.

To overcome this lack of knowledge, we are currently preparing several workshops and studies to explore this hypothesis further and gather empirical evidence. While we are aware that individual preferences might not reflect cultural dispositions, such cultural models like Hofstede's offer frameworks to evaluate and understand the use and uptake of LA tools on larger scales. In any case, whether culture plays a role or not, the mere fact that what students report as relevant to see and use on a dashboard differs from what most LADs use in terms of reference frames demands further attention. It is imperative to urgently address this lack of knowledge as recent works have shown that the common practice in LAD can demotivate learners, damage their self-esteem and generate distress, disappointment and anxiety (Aguilar, 2016; Corrin & de Barba, 2015; Howell, Roberts, Seaman, & Gibson, 2018; Lim et al., 2019) in both high achieving students and low performing ones.

The third sense-making factor, support action, groups features which are highly valued by students, e.g., recommendations on what topics to tackle and behaviour changes or receiving information that helps learners plan their learning. Our finding is in line with theoretical frameworks that listed the provision of opportunities to take action and close the gap between current and desired performance as one of the seven principles of good feedback practices (Nicol & Macfarlane-Dick, 2006). The support for action features we identified from student input fall under the recommendation component of LA that support SRL, explaining what should change and how to change it (Winne, 2017). When invited to design their LA, students requested actionable feedback as they required detailed explanations on how to improve individual scores (de Quincey et al., 2019). For the learning dashboard to have an impact, sense-making needs to be followed by action. Thus, incorporating features that support taking action into a learning dashboard can increase the probability that learners follow-up on their feedback. Nonetheless, only 17% of the surveyed student-facing LA contained both a visualisation component and a recommendation component (Bodily & Verbert, 2017b). This low number might be attributed to the fact that most of the existing dashboard designs aim to foster awareness and reflection only, although being aware does not imply that remedial actions are “being taken and that learning outcomes are improved” (Jivet et al., 2017). Our empirical findings come to support Gibson and Martinez-Maldonado (2017)’s standpoint that there is an overemphasis on user interface design when creating LA systems. Deeper levels of the LA systems need to be considered as well: the algorithms, the data structures and the information flow.

Although these three factors describe concrete features that support sense-making, they explain only 30% of the variance in our dataset. Our exploratory factor analysis discarded several items that could not be factored, reducing thus the possibility of uncovering other underlying themes that support sense-making. Furthermore, the 26 items used in our survey were suggested by potential users of these dashboards and not LA or HCI experts. Thus, the resulting three themes could be implemented on dashboard interfaces through other design elements that were not included in this study. The purpose of the study was not to develop and validate a model that fully describes students' sense-making process, but rather to identify themes that could be relevant in the development of LADs based on students' input. Nonetheless, our initial results could pave the way towards building a theoretical framework that further supports HEI and LA designers in creating effective LA interventions that cater to students' different needs.

5.2. Learner goals, SRL and sense-making
Regarding RQ2, we found evidence that there is an association between learner goals and how relevant students find reference frames. For learners whose goal was to master the topic of the course seeing their performance in comparison to what is maximum possible in the course, seeing learning behaviour indicators and seeing their performance in comparison to their past performance was more relevant than for students with an extrinsic goal. For learners interested in passing the course, having the overall grade displayed on the dashboard replaced the need for having the goal displayed as well.

We did not find any relationship between learner goals and how relevant students found the other two sense-making factors. It is possible that the setting of our study influenced these results, making it difficult for some participants to identify with the goal of mastering the topic of the course (see Section 7). Nonetheless, if we consider the insights gained from the qualitative pre-study and we analyse the ratings of different items (see Table 1), there is a tendency for learners pursuing topic mastery to be more interested in their learning behaviour in order to maximise their learning gains. On the other hand, students wanting to pass the course are looking for input on finding the easiest path to achieve their goal.

According to our findings, there is a strong positive relationship between SRL skills and how relevant students find all three sense-making factors (RQ3). The higher the overall SRL score, the higher rated the three factors were. This insight suggests that students with higher SRL skills are more inclined to use dashboards that present proper frames of reference, are transparent and support follow-up actions, giving us a glimpse into what highly-experienced students deem useful feedback. At the same time, this relationship shows that novice self-regulated learners are not appreciative of such features, which could be caused, in part, because they lack the metacognitive abilities to recognise the potential of these features. As a consequence, LADs that do not cater to the needs of less experienced learners could create an even bigger gap between highly skilled learners that seek out and use external sources of feedback like dashboards, and students that do not.

The interaction effect between the overall SRL score and the learner goals (RQ4) was significant only in the regression model built for transparency of design. This suggests that when students are pursuing a mastery goal, they are inclined to value the transparency of the dashboard design more than when they are pursuing a passing grade. Such features give students clues about how the dashboard information is related to their goal, their learning, how it is calculated and what the scales are on which information is displayed. The fact that we found such information to be more relevant for students under the mastery goal we used in our study is in line with a study from Pintrich (1999) where mastery goals were found to be strongly positively related to the use of cognitive strategies and self-regulatory strategies, while extrinsic goals showed negative relations to SRL.

5.3. Sense-making and SRL sub-scales
Our data indicated a positive association between help-seeking and self-evaluation skills and the perceived relevance of the transparency of design. In an empirical study investigating the relationship between information-seeking preferences and help-seeking practices, Aguilar and Baek (2019) found evidence that students capable of reading graphs may avoid seeking help like attending office hours, visiting study centres or emailing a professor. In our study, students that scored high on help-seeking skills, i.e., learners who recognise their need of help and know where and when to look for it (Zimmerman & Martinez-Pons, 1990), considered transparency of the design more relevant than students with lower help-seeking skills. This might suggest that offering students transparent feedback that they can use to compare against the outcomes of their self-evaluation would reduce the need of asking for help from instructors.

We found a significant positive relationship between the perceived relevance of reference frames on the one hand and goal-setting and help-seeking skills on the other. Winne (2017) defined goals as standards that a learning outcome should fulfil. He further categorises goals into self-referenced goals, criterion-referenced goals and norm-referenced goals, similar to the dashboard elements that fall under our reference frames cluster and the reference frames used on existing LADs (Jivet et al., 2017). In this context, reference frames could be used by learners with higher goal-setting and help-seeking skills to judge what are appropriate and achievable goals to pursue.

Finally, we found that self-reported goal-setting skills are positively associated with how relevant students find support for action. As SRL is a cyclical process with a goal-setting phase following a self-evaluation phase from a previous cycle (Zimmerman et al., 2000), this relation shows that students could use the support for action offered by the dashboard in order to adjust their goals and plans. Since students with low goal-setting skills do not find such features relevant, they might overlook them, creating an even bigger gap between how expert and novice SRL learners manage their learning and benefit from a LAD.

5.4. Implications for dashboard design in HEI
Based on the insights gathered in this study, we propose a few recommendations for the design of LADs used in higher education institutions. Our proposals extend the list of recommendations for practice and future research (Bodily & Verbert, 2017) and recommendations for the design and evaluation of LADs for learners (Jivet et al., 2018).

1.
Strategically involving students in the design LA tools generates input on how to create inclusive tools that provide balanced opportunities to the students, thus increasing buy-in from this stakeholder group.

2.
HEIs should recognise and cater to the different needs of students with different SRL levels in order not to put learners with lower SRL skills at a disadvantage or hinder the development of students with higher levels of SRL.

3.
The way students with high SRL skills use dashboards and how they make sense of it could be modelled and used to scaffold the development of expertise in using external feedback in novice SRL students through adaptive dashboard features.

4.
Transparency of design should be implemented in a seamless manner catering to the different levels of help-seeking of learners.

5.
Reference frames should be adjusted to fit learner goals.

6.
Dashboards should move beyond being passive displays of information but should integrate support for action that enables students to take immediate action.

6. Limitations
The study has several limitations. Firstly, the blended higher education setting of our study limits the generalisability of our findings to other learning settings like distance higher education or MOOCs. The study participants were first- and second-year students in a higher education programme that required them to obtain a fixed amount of credits in their first year to be able to continue their education. This external constraint could have shaped the mindset of the students making it more difficult for them to assume an assigned mastery learner goal. Another study conducted at the same institution uncovered similar findings (Schmitz et al., 2018). Secondly, study participants rated the relevance of dashboard elements while working on a prototype with fictional data. Imagining what they might do in the assigned conditions became another hurdle. Thirdly, as with any study that measures SRL through questionnaires, our results are highly dependent on the truthfulness with which students answer and the level of awareness students already have about themselves. Finally, we operationalised the relevance of different dashboard elements for sense-making by asking students how relevant they find these items when “evaluating [their] performance in [the assigned] condition and deciding on [their] next steps while using the dashboard prototype”. Whether all study participants could identify with a situation where their feedback is presented on a dashboard and whether they can answer questions about their learning are open questions. In future studies, we intend to determine whether students also use the same items while learning by providing them with a real-time dashboard that incorporates relevant features identified in this study.

7. Conclusion
Through the work presented in this article, we aimed to get a better understanding of how university students make sense of the information presented on learning analytics dashboards and whether self-regulated learning skills and learner goals influence the perceived relevance of different dashboard elements. Our results showed that while SRL skills influence how relevant students find the transparency of the design, the reference frames and the support for action embedded in the dashboard, learner goals can shape what reference frames students find relevant for their particular situation. Looking only at these two aspects, our models explained only 5–13% of the variance of the sense-making survey dataset. Numerous other aspects could influence what students find relevant on a LAD: motivations, beliefs, knowledge of tasks, tactics, learning strategies, cultural background, level of education, available learning material. Further exploring how these aspects relate to sense-making constructs will paint a more nuanced picture of how students disentangle and make meaning out of the bits of information concealed in dashboards.

During the study, it became clear that a tool that supports students in understanding their learning behaviour while starting studying at a higher education institution might prove invaluable. Students would not only get immediate feedback on their learning performance, but they would be able to identify their strengths or weaknesses, gain insight into their learning strategies and build and refine their personal learner model, developing SRL skills in the process. Designing LA tools that provide equal opportunities should not happen by chance, but they should be driven by design requirements. LA tools have the potential to bridge the skill gap between novice and expert learners. Not catering to the needs of students with lower SRL can bring already highly skilled students to an advantage. At the same time, trivial designs could hinder the development of SRL skills in students with already higher levels of self-regulation. In this context, inclusive and flexible designs do not refer to adapting features to students preferences, but rather to their needs and skill levels. However, a more delicate question to answer is whether we should design dashboards that support simply passing the course or whether we should better aim to encourage students to master the topic of the course.