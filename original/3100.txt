We investigated activity-level student engagement in blended learning classes at the university level. We used intensive longitudinal methodology to collect activity level engagement data throughout a semester for 68 students enrolled in six blended courses across two universities. We used structural equation modeling to gain a holistic understanding of learning environments, including the influence of personal characteristics, course design, and student perceptions of the learning experience on in-the-moment cognitive and emotional engagement. To investigate longitudinal relationships between emotional and cognitive engagement, we employed cross-lagged modeling techniques. Findings showed that course design and student perception variables had a greater influence on engagement than individual student characteristics and that student multitasking had a strong negative influence on engagement. Students' perceptions of the importance of the activity had a strong positive influence on both cognitive and emotional engagement. An important outcome of engagement was the students' perceptions that they were learning and improving.

Previous
Next 
Keywords
Emotional engagement

Cognitive engagement

Blended learning

Structural equation modeling

Intensive longitudinal methods

Higher education

1. Introduction
Student engagement has been labeled the “holy grail of learning” (Sinatra, Heddy, & Lombardi, 2015, p.1). At the higher education level it has been linked to important outcomes such as grades, persistence, and college completion (Kuh et al., 2008, Robinson and Hullinger, 2008). Student engagement is presumed to be malleable through direct interventions and changing contexts (Fredricks et al., 2004, Lawson and Lawson, 2013, Skinner and Pitzer, 2012). This ability to directly influence student engagement makes the understanding of engagement within specific contexts important to improving learning experiences and outcomes.

One important specific context for studying learner engagement is in blended learning classrooms. Blended learning is the “thoughtful integration of classroom face-to-face learning experiences with online learning experiences” (Garrison & Kanuka, 2004, p. 96). Improving student engagement has been an important goal in blended learning course design across the globe (Garrison and Kanuka, 2004, Graham and Robison, 2007, Spring et al., 2016). However, despite the recent increase in the practice of blended learning in higher education and its accompanying research, very little empirical research has focused exclusively on student engagement in blended learning experiences (Halverson, Graham, Spring, Drysdale, & Henrie, 2014).

Blended learning courses provide a distinctive context for studying learner engagement. Students need to navigate between different modes of instruction and increase their self-motivation to successfully engage in the online portions of the course (Meyer, 2014, Norberg et al., 2011). Most current measures of engagement are retrospective and employed at the course level. These types of measures do not have the potential to capture the ongoing complexity of blended courses, nor to provide clues about improving instruction in the moment. For researchers to fully understand students' engagement in blended learning, they need to collect student engagement data in both modalities. Intensive longitudinal methods can generate important insights about student engagement in blended learning by capturing the immediate experiences of student engagement throughout an entire blended course and linking those experiences to specific educational activities and contexts (Eccles and Wang, 2012, Lawson and Lawson, 2013, Shernoff et al., 2003). Collecting intensive longitudinal data enabled us to investigate blended learning experiences holistically over the course of an entire semester.

2. Literature review and research questions
2.1. Engagement
Three important issues are salient to an exploration of student engagement. First, engagement may be conceptualized as two or three distinct sub-constructs. Second, the interactive nature of engagement must be considered, with specific individual and contextual variables interacting to facilitate engagement. Third, engagement needs be conceptualized and studied at a consistent level.

2.1.1. Definition and conceptualization of engagement
Student engagement is associated with psychological investment and effort (Newmann, Wehlage, & Lamborn, 1992). Examples of definitions include “active, energetic, and approach-oriented involvement with academic tasks” (Pekrun & Linnenbrink-Garcia, 2012, p.260) and “concentrated attentions, interest, and enjoyment” with instruction (Shernoff et al., 2003, p.159). Both of these definitions include elements of emotion, cognition and behavior on the part of the student. Theoretic and empirical efforts to bring precision to the concept of engagement have led to a proposed multifaceted learner engagement model composed of two or three sub-constructs. Two common sub-constructs are emotional engagement, defined as students' affective reactions in the classroom, and cognitive engagement, specified as students' effort directed toward learning. Some researchers also include a sub-construct of behavioral engagement. (For a comprehensive review of the use of these sub-constructs see Fredricks et al., 2004). However, the term behavioral is not used with consistency. Sometimes it refers to procedural behaviors such as attendance and homework. Other times it is defined as “effort, attention and persistence” (Skinner, Furrer, Marchand, & Kindermann, 2008, p.766), which overlaps with common definitions of cognitive engagement. Sinatra et al. (2015) emphasized that operationalizing these categories makes the boundaries between the sub-constructs less clear. In presenting this research we use the two-sub-construct model consisting of emotional and cognitive engagement, as our study concludes that their interaction, particularly at the activity level, which is the focus of this research, leads to the greatest learning gains. We define cognitive engagement as the mental energy students apply to learning, with emotional engagement as the positive emotional responses students have with learning. It has been proposed that emotional engagement proceeds or positively influences cognitive engagement (Fredricks et al., 2004). This relationship is still unclear. Over a decade after Fredricks et al.'s proposal, Janosz (2012) claimed we need to better understand to what extent emotional and cognitive engagement have separate and cumulative outcomes on student learning.

2.1.2. Engagement as a product of individual and contextual influences
The connection of engagement with positive outcomes warrants its investigation. Additionally important is that unlike academic success factors believed to be outside an educator's control, such as parental education and income levels, engagement is presumed to be malleable by direct intervention and by changes in context and environment (Fredricks et al., 2004, Lawson and Lawson, 2013, Skinner et al., 2009, Skinner and Pitzer, 2012). For example, Park, Holloway, Arendtsz, Bempechat, and Li (2012) found that student emotional engagement was influenced by the variables associated with self-determination theory (social relatedness, autonomy and competence). They concluded that “students emotional engagement is more accurately conceptualized and measured as a process that is sensitive to context rather than a global score” (p. 396).

Contextual variables that influence student engagement have been labeled as engagement facilitators (Skinner & Pitzer, 2012) or influences (Shernoff, 2010), which are of two types: environmental and perceptual. Environmental influences on student engagement include the particulars of the learning activity, such as the type of activity (Shernoff, 2010, Shernoff, 2013), class size (Gleason, 2012), and modality (face-to-face or online). Shernoff (2010) used the term perceptual phenomena to describe the different ways various classroom activities are experienced and internalized, including perceptions of autonomy, relatedness, challenge, and relevance.

It is thus assumed that engagement is an interaction of the individual qualities the learner brings to the learning situation and the contextual qualities facilitated by the course design elements created by the instructor or instructional designer, as illustrated by Fig. 1. Inside the box representing engagement are those experiences that combine with the learner and instructor contributions to produce emotion and effort necessary for engagement. A more nuanced understanding of engagement can result if research techniques measure these “inside the box” factors as they occur, also controlling for the outside the box factors of learner characteristics and course design elements. This level of analysis requires a clear differentiation between those emotions and behaviors that indicate engagement and those that facilitate engagement (Ainley, 2012, Lawson and Lawson, 2013, Skinner and Pitzer, 2012, Skinner et al., 2008).

Fig. 1
Download : Download high-res image (36KB)
Download : Download full-size image
Fig. 1. Model of learner engagement.

2.1.3. Levels of analysis in engagement research
Engagement has been conceptualized and studied at three distinct levels of analysis: the institutional level, the course level, and the activity level. The level chosen directly impacts operationalization of engagement, measurement, relevant facilitators, and associated outcomes (Skinner & Pitzer, 2012).

Activity level (“in-the-moment”) engagement has received less attention and research than institutional and course level engagement (Eccles and Wang, 2012, Sinatra et al., 2015). However, research at this level, especially over time, would help educators understand the extent to which “engagement is a function of stable and enduring qualities or a function of contextual factors” (Fredricks et al., 2004, p.67). Several methodological challenges are associated with activity level engagement research. For example, how can engagement be measured “in-the-moment” without changing the moment? Which indicators of engagement are adequate at this level? Suggested methods for this level include observation, experience sampling, trace or log data, and biometrics (Eccles and Wang, 2012, Greene, 2015, Henrie et al., 2015, Sinatra et al., 2015). A later section of this literature review further explores the use of experience sampling research in student engagement.

2.2. Engagement in blended learning
The definition of blended learning is fluid (Oliver & Trigwell, 2005). A foundational definition is the “thoughtful integration of classroom face-to-face learning experiences with online learning experiences” (Garrison & Kanuka, 2004, p.96). Other definitions focus on the percentage of time allocated for traditional face-to-face settings and online learning activities: for example, 50% instruction online and 50% instruction face to face (Bernard et al., 2009) or somewhere between 30 and 79% online (Allen, Seaman, & Garrett, 2007). Graham (2013) reviewed the varied definitions of blended learning and concluded that the most common use of the term entails a combination of traditional face-to-face and online instruction. He further noted continued debates about how much online work is required for a course to be considered “blended learning” and whether the definition requires a reduction in seat time. Meanwhile, as the exact definition of the term remains contested, the use of blended learning pedagogy is increasing dramatically on higher education campuses across the globe (Bernard et al., 2014, Lim and Wang, 2015, Norberg et al., 2011).

One reason for this increase in blended course design is the perception that blended learning has the potential to increase student engagement (Dringus and Seagull, 2013, Garrison and Kanuka, 2004, Graham and Robison, 2007). For example, blended learning designs tap into students' affinity and preference for technology (Dziuban, Moskal, Brophy-Ellison, & Shea, 2007), incorporate more active learning experiences than regular lecture-oriented classes (Dziuban et al., 2007), and balance community building with efficiency (Garrison & Kanuka, 2004).

However, blended classes can also make engagement more difficult for students, as they must navigate between instructional modalities (Baneljee, 2011). The online portions of the course, in particular, can be challenging in terms of student engagement: for students to engage, for instructors to gauge, and for researchers to measure. Dziuban et al. (2007) have suggested as examples of these challenges, the need for students to be more proactive and the isolation students may experience due to reduced classroom interaction with instructors and peers (see also Norberg et al., 2011). Meyer (2014) mentioned several personal characteristics that make engagement in online settings difficult, including low self-efficacy, low resilience, and low self-regulation, as well as preference for face-to-face learning settings. These additional student characteristics need to be included when investigating student engagement in blended learning. According to Sun and Rueda (2012), “There are unresolved issues related to students' engagement in the [online] learning process” (p.191). Of course, in the face-to-face settings, engagement is also hard to identify (Dixson, 2012, Handelsman et al., 2005).

If we are to better understand engagement in blended learning, we need more research into how students navigate between the two modalities of blended learning along with how these two modalities interact to better support students and improve blended learning design (Bliuc et al., 2007, Means et al., 2013). A clear methodology that provides commensurate data for investigating engagement across in-person and online learning experiences must be established. This type of research requires studying engagement at the activity level. Intensive longitudinal Methods, which were designed for this type of person-centered context-specific research, have rich potential for providing a better understanding of engagement as students navigate different modalities throughout an entire blended learning course.

2.3. Intensive longitudinal methodology
Intensive longitudinal methods involve collecting multiple measurements of a phenomena from individuals, over time in order to model a change process (Bolger & Laurenceau, 2013). Experience sampling methodology (ESM) is a form of intensive longitudinal methods in which the data is collected in the natural context of an individual's daily life. The data collected include what the subject is doing and feeling, along with as much contextual information as possible. These data enable an understanding of how certain psychological experiences change over time as well as the contextual factors that influence that change (Bolger and Laurenceau, 2013, Fleeson, 2007, Hektner et al., 2007). Two major advantages have been found for using longitudinal experience sampling for researching engagement. First, engagement data are tied to specific activities occurring in specific contexts, rather than to an abstract concept of engagement in general. Hektner et al. (2007) explained that “the researcher is able to link variations in attention, interest, or challenge to specific instructional practices or conditions” (p. 229). Second, student self-reports of engagement are more reliable because they are collected in the moment and not diminished by recall (Eccles and Wang, 2012, Park et al., 2012, Sinatra et al., 2015). A potential concern of intensive longitudinal methods is that repeated data collection may cause reactivity, the potential for the repeated collection procedure to affect or change the participant's experience. Certain situations have been shown to make reactivity more likely, such as participants trying to change a behavior, but overall, the novelty of repeatedly completing a short survey form dissipates after two or three days (Bolger et al., 2003, Conner et al., 2009).

Previous educational applications of experience sampling method research have generated important understandings about student engagement (Conner and Mehl, 2012, Hektner et al., 2007). For example, Shernoff et al. (2003) found that high school student engagement was enhanced when the activity challenge and students' perceived skill levels were high and in balance, as well as when students had some control over the learning activity and found it relevant. Shernoff and Schmidt (2008) found that the link between engagement and achievement was weaker for Black students than for White students, and that engagement indicator of “being on task” was more strongly related to outcomes for Black students than for White students. Using ESM with high school students, Park et al. (2012) found that student engagement varied by the contextual variables autonomy, social support, and relatedness. Peterson and Miller (2004) used an ESM model to investigate collaborative learning in a college education course, but only collected data twice during the semester. They found higher levels of engagement during collaborative learning activities than in large group instruction.

A challenge to using ESM specifically to study student engagement is deciding on the indicators to include in the survey instrument. Most existing engagement scales include items that do not apply to “in-the-moment” experience, but rather relate to the student's overall experience: for example, “Do you complete assigned homework for the course?” A large ESM engagement research project, The Sloan Study of Youth and Social Development (SSYSD), used a definition of engagement that did not include any sub-constructs. Instead, the Shernoff et al. (2003), Shernoff and Schmidt (2008)) research team used a composite variable of interest, concentration, and enjoyment which they referred to as “engagement.” Park et al.’s (2012) high school ESM study used the same three indicators—interest, concentration and enjoyment—to make a composite variable but labeled it “emotional engagement.” Fredricks et al., 2004 stated that ESM could gauge emotional engagement but said nothing about its application to cognitive or behavioral engagement. Sinatra et al. (2015) argued that ESM can be used to assess how students interact with others and with the particular learning environment to “produce a particular type, level, or form of engagement” without indicating the particular aspect of engagement they observed (p. 9). Recently Curtis et al. (under review) validated an engagement scale measuring both emotional and cognitive engagement at the activity level in a college setting. This scale is further described in the Methods section of this paper.

2.4. Research questions
The purpose of this research project has been to expand our understanding of student engagement in blended learning by using intensive longitudinal, experience sampling methods to capture the day-by-day experiences of students as they navigate a semester long blended learning course. These methods allow us to explore how much of student engagement is a stable characteristic of the student and how much engagement varies by course design elements that are controlled by an instructor. We are also interested in whether emotional engagement facilitates cognitive engagement as well as how the modality of the learning activity influences engagement. These are particularly salient issues in a blended learning setting where some students may have a harder time sustaining engagement as they navigate back and forth between face-to-face and online learning activities. Furthermore, as institutions around the world are rapidly adopting blended learning models, this provides an opportunity to explore pedagogical approaches to blended learning that can transform learning for higher education students (Oakley, 2016, Zurita et al., 2015).

We specifically investigated two research questions:

1.
What impact do instructional design decisions, student characteristics, and student perceptions have on student emotional and cognitive engagement in blended learning classroom? What is the impact of the modality of the instruction (online or face-to-face) on these influences?

2.
How are emotional and cognitive engagement related longitudinally throughout a semester blended learning class? Does higher emotional engagement at the beginning of the course proceed and lead to greater cognitive engagement later in the course?

3. Method
3.1. Context
Data were collected from students attending six different blended learning courses held at two medium-sized universities (approximately 30,000 and 25,000 students each) located in the western United States. One university has an approximate 51% acceptance rate while the second university has open enrollment and serves a diverse student body. A multi-course, multi-institution design was constructed in order to overcome the idiosyncratic aspects of individual courses, to gain access to wide variety of students, and to improve generalizability to the study of student engagement in blended courses (Baneljee, 2011). Each of the blended learning courses in this study offered three hours of credit, met at least once every week during the 16-week semester and used online learning activities to reduce class time by at least one day a week. To increase the diversity of the student sample, five of the six courses were first or second year general education courses, designed for students with a variety of majors and goals. Two courses were first-year writing composition courses, three were second-year humanities courses, and one was an upper division nursing science course. Each instructor had received formal training in blended learning course design and had previously taught the course using a blended design. Class time in these courses was typically used for lecture, class discussion and small group work. Online work included watching videos, reading articles, taking tests and quizzes, and writing essays and critical responses. Five of the courses had between 14 and 25 students, while one course had 80 students. Teachers were recruited to participate in this research through email solicitation. We chose these courses because we believe they represent “typical” blended learning courses that are held on many campuses. The point of this study was not to test or evaluate any particular blended course design or intervention, but to glean understandings of student engagement in blended courses that could be globally applied.

3.2. Participants
Students in each class were recruited through a class visit by a member of the research team; they were offered gift cards as incentives for their participation. They were informed that their participation was voluntary and would not impact their grade in any way. Instructors were not informed as to which students were participating in the research. Students who agreed to participate signed a consent form and provided us with their contact information so that we could send survey links and reminders throughout the semester. IRB protocol was followed and approved at both universities. A total of 68 students chose to participate, one of whom did not complete the initial learner characteristics survey. Participation rates in the various classes ranged from 17% to 54%. Students reported being affiliated with 29 different majors, including “undecided.” Demographic information for the participating students, including their previous experience with online and blended learning, is provided in Table 1.


Table 1. Participant information.

Characteristic	Percentage of the sample
Year in school	
 Freshman (n = 11)	16%
 Sophomore (n = 22)	33%
 Junior (n = 12)	18%
 Senior (n = 22)	33%
 First generation college student	21%
Age	
 18–20 yrs. (n = 13)	19%
 21–25 yrs. (n = 47)	70%
 26–30 yrs. (n = 3)	4%
 31–35 yrs. (n = 0)	0%
 36 + years (n = 4)	6%
Gender	
 Female (n = 29)	43%
 Male (n = 38)	57%
Previous online and blended experience	
 Had taken a previous online course (n = 36)	55%
 Had taken a previous blended course (n = 18)	27%
3.3. Data collection
We created an event-contingent ESM design, a methodology that requires participants to complete a report every time a pre-defined event occurs (Bolger & Laurenceau, 2013). This design was a compromise from a pure ESM design because we wanted to collect data over the entire semester, and instructors did not want their class time to be repeatedly interrupted by our surveys. (In contrast, the Sloan Study of Youth and Social Development, a national high school student engagement ESM study, collected data through randomly timed surveys for seven days (Shernoff & Schmidt, 2008)). Instead, we designated two pre-defined data collection triggers for the blended learning classes: leaving a face-to-face class and completing an online activity. Starting the third week of the semester, each student was instructed to complete the ESM survey instrument twice a week: immediately after a face-to-face class and again after completing online course work. The surveys were accessed online through Qualtrics. Most students took these surveys on their mobile devices. To remind students to complete them, text and email reminders with survey links were sent to students at the exact completion time of each face-to-face class and the day online assignments were due and the evening prior to the due date. While it would have been preferable to survey students while truly in-the-moment during their class or online time, most surveys were completed within an hour of attending class and on the day of completing online work and so we consider this data to accurately reflect their day-to-day engagement experiences. Students completed 1400 surveys over the course of the semester Compliance to the twice weekly protocol varied; this is discussed in the Missing data section of this paper.

3.4. Longitudinal ESM instrument
We modeled our ESM instrument after the ESM instrument used in the longitudinal Sloan Study of Youth and Social Development, as presented in Hektner et al. (2007). Using this instrument as a template, we also used Skinner and Pitzer's (2012) model of items suggested as indicators and facilitators of engagement. The resulting 33-question instrument was designed to require less than three minutes to complete. Not every one of the 33 items are included in this specific analysis and modeling of the data, as some items were created to address separate research questions. A copy of this ESM instrument is provided in the Appendix of this article.

3.5. Engagement latent factors
Using these repeated surveys to investigate student engagement with Structural Equation Modeling was a two-step process. First, latent factors to measure engagement were created and tested on multiple data sets. Second, facilitator variables were added to the model to explore relationships. In a previous analysis Henrie et al. (under review), established a two-factor model of this engagement survey instrument which maintained good model fit across data sets. The first factor was labeled cognitive engagement because it demonstrated effort and cognitive energy. The three items that made up this factor were “How well were you concentrating?” “Rate yourself passive to active.” “Rate yourself focused to distracted.” The second factor was labeled emotional engagement and the four items that made up this factor were “Did you enjoy this activity?” “Was this activity interesting?” “Did you wish you had been doing something else?” “Rate yourself excited to bored.” Descriptive information for these same variables from our data set is shown in Table 2.


Table 2. Description of engagement variables.

Factor	Indicator	Min/max	Mean	Standard deviation	Skewness	Kurtosis
Emotional engagement	Enjoy (did you enjoy this activity?)	1–5	3.062	1.13	− 0.252	− 0.698
Interesting (was this activity interesting?)	1–5	3.252	1.15	− 0.384	− 0.695
Something else (did you wish you had been doing something else?)	1–5	3.397	1.27	− 0.271	− 1.041
Excited-bored (excited to bored bimodal response)	1–7	3.958	1.40	0.243	− 0.472
Cognitive engagement	Concentration (how well were you concentrating?)	1–5	3.395	1.13	− 0.357	− 0.689
Passive-active (passive to active bimodal response)	1–7	3.682	1.60	0.188	− 0.875
Focused-distracted (focused to distracted bi-modal response)	1–7	3.207	1.69	0.645	− 0.492
3.6. Facilitating variables
We included variables from three categories of engagement facilitators: student personal characteristics, course design characteristics, and in-the-moment interpersonal perceptions— factors that vary with each instructional activity. Descriptive data for these variables are found in Table 3.


Table 3. Descriptive information for facilitating variables.

Level of analysis	Variable	Min/max	Mean	Standard deviation	Skewness	Kurtosis
Individual level	Gender	0 = male; 1 = female	0.57	0.498	− 0.304	− 1.966
Year in school	1/4	2.65	1.117	− 0.050	− 1.395
GPA	1/5	3.37	0.976	− 1.005	0.557
Interest factor score	3.22/2.08	0.074	1.371	− 0.409	− 5.24
Self-efficacy factor score	− 2.03/0.944	− 0.02	0.756	− 0.759	0.137
Tech self-efficacy factor score	− 2.017/0.655	0.040	0.636	− 1.190	1.197
Preferred mode	0 = online; 2 = f2f	0.655	0.489	− 0.495	− 1.809
1st generation college	0 = parent w/college
1 = no parent college	0.32	0.471	0.772	− 1.448
Instructional design	Mode	1 = online; 2 = f2f	1.49	0.500	0.046	− 2.001
Choice	1/5	3.29	1.068	− 0.464	− 0.401
Peer interaction	0 = no; 1 = yes	0.189	0.392	1.586	0.515
Content interaction	0 = no; 1 = yes	0.1705	0.376	1.755	1.080
Active learning	0 = no; 1 = yes	0.249	0.433	1.162	− 0.652
Student perception	Lonely-sociable	1/7	4.15	1.262	0.128	0.234
Challenge	1/5	2.23	1.090	0.608	− 0.441
Relate	1/5	3.67	1.076	− 0.649	− 0.140
Important	1/5	2.87	1.212	0.057	− 0.935
Learning	1/5	3.29	1.068	− 0.464	− 0.401
Total other	0/12	1.66	1.385	2.583	9.606
Personal characteristics were gathered from an initial one-time survey given to all participating students during the second week of the semester. These variables represent the type of personal qualities that students bring to the instructional experiences that remain constant throughout the semester. Based on a review of previous engagement and blended learning research, we included five self-reported variables: gender, GPA (4-point scale), year in school, first-generation college status, and preferred learning environment (online, face-to-face). In addition, three latent factors were created from this initial survey to measure (a) self-efficacy (4 items from the Motivated Strategies for Learning Questionnaire (MLSQ) (Pintrich, Smith, Garcia, & McKeachie, 1993)), (b) initial subject matter interest (3 items created for this survey), and (c) computer self-efficacy (3 items created for this survey). Computer self-efficacy has been theorized to impact students' ability to navigate online learning activities (Bates & Khasawneh, 2007). The items used to create these three factors as well as the factor loadings are included in Appendix 2. The fit statistics for this model of latent learner qualities was RMSEA = 0.018, CFI: 0.959, TLI: 0.942, SRMR: 0.088.

Course design variables were created from the ESM survey. Rather than merely comparing online with face-to-face activities, in the traditional manner of blended learning research, we created five variables to represent the course design elements the instructor brings to the learning experience: (a) the mode of the activity (face-to-face or online), (b) the amount of choice students had in the activity (1-5 scale), and (c) the type of activity, represented by a series of dummy variables that indicate lecture, peer interaction activities, content interaction activities, or active learning activities. The coding scheme for these variables can be found in Table 4. The most frequent learning activity was lecture, and the dummy scheme was created to compare the other activities to lectures. Therefore, in the study results for the variables peer interaction, content interaction, and active learning will all be comparing those activities to lectures. Instructor presence is also an important aspect of course design but was not independently measured in this study but we did control for class membership in the SEM model. Interestingly, two of the instructors each taught two of the exact same courses in this study and for both of sets of courses there were significant differences in each of their section's students' mean emotional and cognitive engagement scores. This perhaps indicates that individual instructor was not a distinguishing facilitator of engagement for this particular group of courses.


Table 4. Coding scheme for learning activity variables.

Category	Indicators	Total
Lecture	Lecture	N = 473
Peer interaction	Student presentations (n = 43)	N = 258
Class discussion (n = 42)
Discussion board (n = 118)
Small group work (n = 36)
Create group project (n = 19)
Content interaction	Video (n = 159)	N = 232
Reading textbook (n = 29)
Reading articles (n = 44)
Active learning	Quiz or test (n = 154)	N = 339
Studying for quiz or test (n = 5)
Working on project alone (n = 33)
Researching online (n = 14)
Writing paper or response (n = 133)
The final group of facilitator variables is in-the-moment student perceptions and affective states. These intrapsychic and interpersonal variables measure how the student is experiencing the learning activity as it is taking place. Though these variables measure student perceptions, these perceptions are created in response to a certain context that includes the student, the instructor, the instructional experience designed by the instructor, and the other students (Shernoff, 2010). This category included five variables:

1.
Relate. Can you relate it to what you already know? (5 point Likert-type scale)

2.
Importance. Is the activity important to you? (5 point Likert-type scale)

3.
Learning. Are you learning anything or getting better at something? (5 point Likert-type scale)

4.
Challenge. How challenging is the activity? (5 point Likert-type scale)

5.
Loneliness-sociability. How lonely or sociable was the activity for you? (bi-modal 7 point scale: lonely = 1 and sociable = 7)

As they took the ESM survey, students were asked to check all of the other things they were doing as they participated in the learning activity, such as listening to music, eating, engaging in social media, texting, daydreaming, and watching media. We created the variable multitasking, which summed all of their additional activities into a single number.

3.7. SEM assumptions
All variables used in this analysis were checked for the SEM assumptions of normal distribution, linear relationship, normality of residuals, and equality of variance through descriptive statistics, histograms, and plots of regression residuals. Independent observations were not assumed, since data were longitudinal and thus specified as complex and clustered according to ID number in the analysis. Since only six courses were involved, clustering at the class level was not optimal, and we could not achieve convergence when we modeled class membership as a random effect. Class membership was partially accounted for by including a series of dummy variables representing membership in individual classes in the Full Information Maximum Likelihood (FIML) auxiliary command in Mplus discussed in the next section.

3.8. Missing data
We were using data from two sources: bi-weekly ESM surveys and an individual learner characteristics survey collected from participants at the beginning of the research project. We were missing one learner characteristics survey. The ESM surveys had missing data on two levels: missing surveys and missing data on the submitted surveys.

3.8.1. Missing ESM surveys
The research design specified that each student would complete two surveys per week over the 11½ weeks of the study: one after each face-to-face class experience and one after each online class experience. Therefore, a student with 100% participation would have completed 23 surveys. The actual response rates for students ranged from 4% (1 completed survey) to 152% (35 completed surveys). A perfect completion rate would have generated 1,587 surveys, and we collected 1400, amounting to a response rate of 88%, with 12% of the surveys missing.

Computing the amount of missing data, however, has been more complex. Sixteen students completed more than 23 surveys. Either they reported on more than one face-to-face class during a week (one of the classes met twice weekly) or on more than one online experience. Thus both extra data and missing data had to be explained. Not wanting to eliminate any of the surveys, we ran a regression on the completion rate for each student, with all of the covariates in order to understand and explain both the missing and extra data. Gender, year in school, and class membership (dummy coded to test for each class individually) all had a statistically significant relationship to the individual response rate. Together these variables accounted for nearly half of the variance in missing data; therefore, assuming the data are missing at random in the presence of these variables seems appropriate. In order to best handle these missing data we used Full Information Maximum Likelihood (FIML) in Mplus, which handles MAR well (Little & Rubin, 2014), adding gender, year in school, and class membership (coded individually) in the auxiliary command.

3.8.2. Missing survey data
The individual surveys submitted also had data missing. For individual items, these ranged from .04% (mode) to .07% (bored to excited). We dummy-coded each variable for whether it was missing or not and ran regressions to explain the missing data. Only gender was significant, as males were more likely to have missing items. Gender was included in the FIML approach to missing data in Mplus.

For both missing surveys and missing data within surveys, regression analysis has shown the data to be missing at random (MAR), since the likelihood of students with missing surveys and questions with missing answers can be explained by other covariates in the data set. By including these covariates in the auxiliary command in Mplus, we significantly reduced the potential of bias on the estimates.

4. Structural equation modeling
The structural equation modeling of these data was accomplished in three steps. First, we established model fit for the latent variables cognitive and emotional engagement. Second, we added individual level variables and in-the-moment variables indicating course design and student perceptions of the experience to predict cognitive and emotional engagement. Third, we created cross-lagged models to explore the potential causal relationship between cognitive and emotional engagement over time.

4.1. Latent engagement variables
Using the MLR estimator in Mplus (v.7.3) and TYPE IS COMPLEX (cluster is person I.D.), two latent variables, emotional engagement and cognitive engagement, were modeled as described in Table 2. The underlying components of the cognitive engagement latent factor measured active, focused, concentration. Since the “focused to distracted” variable was measured along a continuum of focused (low end) to distracted (high end), the lower value indicates more focus and less distraction, hence the negative loading. The underlying components of the emotional engagement factor measured enjoyment, positive energy, interest, and lack of boredom. The variable “something else” asked students to rate how much they would rather be doing something else besides the learning activity. A lower score indicates a lack of wanting to be doing something else, and so there is a negative loading on the emotional engagement factor. Similarly, since the “excited to bored” variable was measured along a continuum of exited (low end) to bored (high end), the lower value indicates more positive energy and less boredom, hence the negative loading.

The initial model did not display the expected fit statistics based on the Curtis et al. (under review) research, so we consulted the modification indices and adjusted the model to allow the residual variance of interesting and enjoyment to correlate. The fit statistics for this adjusted model were RMSEA = 0.069, CFI = 0.960, TLI = 0.930, SRMR = 0.039, AIC = 28284.50, BIC = 28693.53, sample size adjusted BIC = 28445.88. Guidelines indicate that a RMSEA value between 0.05 and 0.08 indicates good fit (Browne & Cudeck, 1993), a CFI value above 0.90 indicates good fit (Wang & Wang, 2012), a TLI value above 0.90 indicates good fit (Wang & Wang, 2012), and a SRMR value of less than 0.08 indicates good fit (Hu & Bentler, 1999); thus we decided this model had good fit. In order to confirm that a model with two latent engagement factors (cognitive and emotional) was the best fit for the data, we also modeled a single engagement factor with the same variables. Fit indices for this single factor model using the previously stated fit guidelines were poor (RMSEA = 0.134, CFI = 0.837; TLI = 0.737; SRMR = 0.064). In addition, the single factor model had lower AIC, BIC and aBIC values. Factor loadings for the two-factor model, as well as the individual ICC values for each item (since this is a multilevel model) are shown in Table 5.


Table 5. Engagement CFA loadings and correlations.

Unstandardized loading	Standard error	Intra class correlation	Correlations
Cognitive engagement				With emotional engagement .693
Passive to active	1.00⁎⁎	0.00	0.28	
Focused to distracted	− 1.78⁎⁎	0.18	0.25	
Concentration	1.18⁎⁎	0.12	0.18	

Emotional engagement
Enjoyment	1.00⁎⁎	0.00	0.29	With interest .465
Something else	− 1.09⁎⁎	0.06	0.41	
Interesting	0.94⁎⁎	0.03	0.30	
Excited to bored	− 1.17⁎⁎	0.10	0.24	
⁎⁎
p < 001.

4.2. Predictors of latent engagement variables
With a good model of engagement established, the next step of analysis was to examine the relationship of the two latent engagement variables: cognitive engagement and emotional engagement. Of the three categories of predictor variables, learner characteristic variables were assumed to remain constant throughout the semester, and course design variables and perceptual variables were assumed to vary from time point to time point because they are experienced “in the moment.” To determine the most appropriate statistical technique, we tested for random slopes across time for both cognitive and emotional engagement using a two-level model. Cognitive engagement had a small negative (− 0.002) non-significant (p = 0.757) slope. Emotional engagement also had a small negative (− 0.012) non-significant (p = 0.052) slope. In addition, we graphed and visually analyzed the slopes. We were able to assume no growth in the final model because the effects were small. In addition, given the large number of items in relationship to our small sample size (68 students) we ran the model with a Bayesian estimator which better handles small sample sizes (Heck and Thomas, 2015, Muthen, 2010). We saw the same pattern of results and so are reporting on the MLR estimator because the results are more commonly understood. Based on these results we continued with the SEM, TYPE IS COMPLEX, model approach in Mplus. The full model provides insight into the impact of all the different types of variables on both emotional and cognitive engagement in the presence of each other. The full model is described in Fig. 2; the results of the model are shown in Table 6.

Fig. 2
Download : Download high-res image (368KB)
Download : Download full-size image
Fig. 2. SEM model of emotional and cognitive engagement.


Table 6. Results of full SEM model, facilitators regressed on cognitive and emotional engagement.

Variable	Cognitive	Emotional
B	S. E. B	β (standardized)	B	S. E. B	β (standardized)
Individual level
Gender	0.10	0.08	0.06	− 0.21⁎⁎⁎	0.06	− 0.11
Prior GPA	− 0.06⁎	0.04	− 0.09	0.01	0.03	0.01
Subject Interest	− 0.03	0.03	− 0.06	0.08⁎⁎⁎	0.03	0.16
Self -Efficacy	0.14⁎	0.07	0.13	− 0.01	0.06	− 0.01
IT self-efficacy	0.02	0.06	0.02	− 0.16⁎⁎⁎	0.05	− 0.11
Preferred mode (f2f = 1)	0.07	0.08	0.04	0.07	0.09	0.04

Class design
Mode (online or f2f)	0.19⁎⁎	0.07	0.12	− 0.06	0.05	− 0.03
Choice in activity	0.01	0.02	0.01	0.10⁎⁎⁎	0.03	0.12
Peer interaction	0.19⁎⁎	0.06	0.09	0.10	0.05	0.03
Content interaction	− 0.05	0.08	− 0.02	0.14⁎	0.07	0.05
Active learning	0.20⁎⁎	0.08	0.10	− 0.13⁎	0.06	− 0.06

Perceptual
Lonely-sociable	0.07⁎⁎	0.03	0.11	0.13⁎⁎⁎	0.02	0.18
Challenge	0.07⁎	0.03	0.10	− 0.13⁎⁎⁎	0.03	− 0.15
Relate	0.12⁎⁎⁎	0.03	0.16	0.09⁎⁎	0.03	0.01
Important	0.16⁎⁎⁎	0.04	0.23	0.28⁎⁎⁎	0.03	0.36
Learning	0.22⁎⁎⁎	0.04	0.29	0.36⁎⁎⁎	0.03	0.41
Multitasking	− 0.15⁎⁎⁎	0.03	− 0.25	− 0.07⁎⁎⁎	0.02	− 0.1
Note N = 1271.

⁎
p < 0.05.

⁎⁎
p < 0.01.

⁎⁎⁎
p < 0.001.

4.3. SEM results
The results indicated that emotional and cognitive engagement have been uniquely influenced by a combination of individual, course design, and perceptual variables. The value of this research is to show how each variable influences engagement, controlling for all of the other variables in the model. The results of the SEM are organized according to the three groups of predictor variables, starting with the variables of learner characteristics.

4.3.1. Learner characteristics variables
In the group of individual level variables, a student's self-efficacy had the greatest impact on cognitive engagement (β of 0.13, p < 0.05), and a student's initial subject interest, as measured during the first week of class, had the greatest impact on emotional engagement (β of 0.16, p < 001). Neither variable had a significant influence on the other type of engagement. Gender had a significant negative influence (β-0.11, p < 0.001) on emotional engagement, with females being less engaged emotionally, but no significant impact on cognitive engagement. GPA had a significant negative influence (β = − 0.09, p < 0.05) on cognitive engagement, as students with higher GPAs had lower cognitive engagement, but no significant impact on emotional engagement. Computer self-efficacy had a significant negative impact on emotional engagement (β = − 0.11, p < 0.001): the higher a person's technical self-efficacy, the lower his or her emotional engagement. The rest of the variables—year in school, preferred mode of instruction, and first generation college status—did not have a significant impact on cognitive or emotional engagement.

4.3.2. Course design variables
Among the instructional design variables, the mode of the activity (face to face or online) had a significant positive beta on cognitive engagement (β = 0.12, p < 0.01): Online activities were more cognitively engaging than face-to-face interaction. This may be explained by most courses being designed to have quizzes and tests taken online rather than during face-to-face class time. Mode did not have a significant impact on emotional engagement. In contrast, choice (the degree to which students felt they had a choice affecting the activity) had a significant positive impact on emotional engagement (β = 0.12, p < 0.001) but not on cognitive engagement. Peer interaction activities (as compared to lectures) had a significant, positive influence on cognitive engagement (β = 0.09, p < 0.01) but not on emotional engagement. Compared to lectures, content interaction activities had a significant positive influence (β = 0.05, p < 0.05) on emotional engagement but not on cognitive engagement. In contrast, when comparing active learning activities with lectures, we found a significant positive influence on cognitive engagement (β = 0.10, p < 0.01) but a negative influence on emotional engagement (β = − 0.06, p < 0.05).

4.3.3. Student perception variables
The next five variables measured students' in-the-moment perceptions of learning activities. These will be discussed in order of greatest to smallest effect size. Feeling like they were learning something or getting better at something had the greatest impact on both emotional engagement (β = 0.41, p < 0.001) and cognitive engagement (β = 0.29, p < 0.001) of any variable in the model. Perceiving the learning activity as important to them had the second greatest impact on emotional engagement (β = 0.36, p < 0.001) and the third highest impact on cognitive engagement (β = 0.23, p < 0.001). The more sociable students felt during an activity, the higher their emotional (β = 0.18, p < 0.001) and cognitive engagement (β = 0.11, p < 0.01). The final two variables in this category had interesting differences in their effects on emotional and cognitive engagement. Being able to relate the learning activity to something students already knew would positively impact both cognitive (β = 0.16, p < 0.001) and emotional engagement (β = 0.01, p < 0.01), but the size of the effect was much greater for cognitive engagement. The level of challenge the student perceived in the activity had a positive effect on cognitive engagement (β = 0.10, p < 0.05), but a larger negative effect on emotional engagement (β = − 0.15, p < 0.001). A possible interpretation is that students do not always enjoy being challenged and cognitively engaged.

Finally, multitasking, the measure of how many other things students were doing during a learning activity, is not just a perceptual variable, but an indication of how the student is choosing to interact with the learning activity. While this variable had a negative effect on both cognitive (β = − 0.25, p < 0.001) and emotional engagement (β = − 0.1, p < .001), it had the second largest impact on cognitive engagement of any variable in the model.

This complete model explained quite a bit of the variance of emotional engagement (R2 = 0.817) and a moderate amount of the variance of cognitive engagement (R2 = 0.527).

4.4. Cross lag model
In addition to the regression analysis, we investigated the longitudinal interaction between emotional and cognitive engagement over the duration of a semester class. Specifically, we were interested in discovering whether high emotional engagement led to an increase in cognitive engagement later in the semester. In order to model an SEM cross lag model we computed each student's mean emotional and cognitive engagement for the first, second, and final third of the semester. Due to the small sample size of 66 individuals for this analysis, we used the Bayes estimator in Mplus to achieve convergence (Kenny, 1975, Kenny, 2005). The cross-lag model, diagramed in Fig. 3, computed the betas for the influence of emotional engagement at Time 1(A1) on emotional engagement at Time 2 (A2) and for emotional engagement at Time 2 (A2) on emotional engagement at Time 3 (A3). The same pattern has been computed for cognitive engagement at Time 1 on cognitive engagement at Time 2 and for cognitive engagement at Time 2 on cognitive engagement at Time 3. These self-influencing lag effects are indicated by the horizontal arrows; all are significant, as expected. Controlling for these, the model also computed the betas for the influence of emotional engagement at Time 1 on cognitive engagement at Time 2 (A1B2) and for emotional engagement at Time 2 on cognitive engagement at Time 3, as well as the corresponding influences of cognitive engagement on emotional engagement. These betas are indicated with the cross-diagonal arrows. The results, as shown in Table 6, did not indicate any significant relationship between emotional and cognitive engagement between Time Points 1, 2, or 3. We ran both unconstrained (betas being free to differ between time points) and constrained (betas being constrained to be the same between time points). Though there was no difference in the pattern of results, we have reported the constrained betas because the constrained model had a lower BIC, indicating better model fit. The BIC for the constrained model was 453.013, and the BIC for the unconstrained model was 468.843 (Table 7).

Fig. 3
Download : Download high-res image (42KB)
Download : Download full-size image
Fig. 3. Cross lag model.


Table 7. Results of cross lag models.

Cross lag model	Time 1 to time 2 unstandardized betas	Time 2 to time 3 unstandardized betas
Emotional & cognitive engagement	Emotional on emotional (A1 A2)	0.71⁎	Emotional on emotional (A2 A3)	0.71⁎
Cognitive on cognitive (B1 B2)	0.66⁎	Cognitive on cognitive (B2 B3)	0.66⁎
Emotional on cognitive (A1 B2)	0.06	Emotional on cognitive (A2 B3)	0.06
Cognitive on emotional (B1A2)	0.08	Cognitive on emotional (B2A3)	0.08
Learning & emotional engagement	Learning on learning (A1 A2)	0.57⁎	Learning on learning (A2 A3)	0.57⁎
Emotional on emotional (B1 B2)	076⁎	Emotional on emotional (B2 B3)	076⁎
Learning on emotional (A1 B2)	0.05	Learning on emotional (A2 B3)	0.05
Emotional on Learning (B1A2)	0.19⁎	Emotional on learning(B2A3)	0.19⁎
Learning & cognitive engagement	Learning on learning (A1 A2)	0.51⁎	Learning on learning (A2 A3)	0.51⁎
Cognitive on cognitive (B1 B2)	0.68⁎	Cognitive on cognitive (B2 B3)	0.68⁎
Learning on cognitive (A1 B2)	0.06	Learning on cognitive (A2 B3)	0.06
Cognitive on learning (B1A2)	0.26⁎	Cognitive on learning (B2A3)	0.26⁎
Important & emotional engagement	Important on important (A1 A2)	0.85⁎	Important on important (A2 A3)	0.85⁎
Emotional on emotional (B1 B2)	0.68⁎	Emotional on emotional (B2 B3)	0.68⁎
Important on emotional (A1 B2)	0.15	Important on emotional (A2 B3)	0.15
Emotional on important (B1A2)	0.02	Emotional on important (B2A3)	0.02
Important & cognitive engagement	Important on important (A1 A2)	0.79⁎	Important on important (A1 A2)	0.79⁎
Cognitive on cognitive (B1 B2)	0.68⁎	Cognitive on cognitive (B1 B2)	0.68⁎
Important on cognitive (A1 B2)	0.08	Important on cognitive (A1 B2)	0.08
Cognitive on important (B1A2)	0.10	Cognitive on important (B1A2)	0.10
Multitasking & emotional engagement	Multitasking on multitasking (A1 A2)	0.68⁎	Multitasking on multitasking (A2 A3)	0.68⁎
Emotional on emotional (B1 B2)	0.75⁎	Emotional on emotional (B2 B3)	0.75⁎
Multitasking on emotional (A1 B2)	− 0.04	Multitasking on emotional (A2 B3)	− 0.04
Emotional on multitasking (B1A2)	0.034	Emotional on multitasking (B2A3)	0.034
Multitasking & cognitive engagement	Multitasking on multitasking (A1 A2)	0.66⁎	Multitasking on multitasking (A1 A2)	0.66⁎
Cognitive on cognitive (B1 B2)	0.79⁎	Cognitive on cognitive (B1 B2)	0.79⁎
Multitasking on cognitive (A1 B2)	0.04	Multitasking on cognitive (A1 B2)	0.04
Cognitive on multitasking (B1A2)	− 0.03	Cognitive on multitasking (B1A2)	− 0.03
Note. N = 66.

Model was constrained to identical betas between the two time periods.

⁎
Significant Bayes p value.

We next explored the longitudinal relationship between emotional and cognitive engagement and the facilitators that had significant betas in the previous model. We chose to investigate learning (whether the student was learning anything or getting better at something), importance (whether the activity was important to the student), and multitasking since these had the largest betas in the SEM regression model. As with the previous model, we tested constrained and unconstrained iterations. The BIC was lower for the constrained models, indicating better model fit, and so those results are reported. The variables importance and multitasking did not show any significant relationship with emotional or cognitive engagement at Time Points 1, 2, or 3. Learning, however, showed a significant relationship that was the opposite of our hypothesis. Rather than facilitating cognitive and emotional engagement, learning appears to be a significant outcome of both cognitive and emotional engagement. Cognitive and emotional engagement at Time Point 1 had a significant impact on “learning” – students' perception that they were learning something or getting better at something - at Time Point 2. Similarly, cognitive and emotional engagement at Time Point 2 also had a significant impact on students' sense of learning at Time Point 3. The results are shown in Table 6.

These results are exploratory because our sample size was small (n = 66) at this level of analysis. However, the results justify studying engagement longitudinally at the activity level to gain a more complete understanding of the distinction between facilitators and outcomes of engagement throughout a course.

5. Discussion
This study provided a valuable opportunity to gain a holistic understanding of student engagement in higher education blended learning classrooms. We will first discuss the implications of this research for student engagement generally and then engagement in blended learning classroom specifically.

Increasing student engagement is a primary objective for those who advocate for blended learning adoption in higher education settings in every region of the world. Student engagement is theorized to be multi-faceted, including a cognitive and an emotional component, and to be malleable by instructor intervention. Our analysis supports this multi-faceted view of engagement. While emotional and cognitive engagement were correlated and they both led to the outcome of students' perceptions of learning and getting better at something, they were each uniquely influenced by different aspects of individual student and classroom characteristics. Not surprisingly, a student's self-efficacy (measured in general terms at the beginning of class) was positively related to their cognitive engagement and a student's interest in the subject matter (as measured at the beginning of the class) was positively related to their emotional engagement. Though we did not intend to explore Pekrun's control-value theory of achievement emotions (Pekrun, 2006) with this research, these findings do somewhat confirm his hypothesis that students who have a positive sense that they can perform an academic task and perceive that the task has value or interest to them, will experience enjoyment (a key component of our emotional engagement factor) and will be willing to exert more cognitive effort toward the activity (a key component of our cognitive engagement factor). (For a discussion of the control-value theory of emotions in online settings see Daniels & Stupnisky, 2012). Further, we found that as students rated the specific learning activities as “important to them” and reported that they could relate the material to previous learning (measures of value), both their cognitive and emotional engagement increased, reinforcing the control-value theory's prediction of positive value and control appraisals leading to positive effects on cognitive and emotional engagement. It is important for instructors of students in higher education to clearly communicate the value of each learning experience in order to facilitate engagement, regardless of the modality in which the learning activity occurs.

A unique contribution of this research was exploring the impact that different learning activities had on engagement in higher education classrooms. Surprisingly, peer-interaction activities increased cognitive engagement, but not emotional engagement. However, students' ratings of feeling social (as opposed to lonely) at the time of the activity had a strong positive impact on both cognitive and emotional engagement. This indicates that feelings of sociality may operate independently of instructor directed peer-interaction. Lower levels of emotional engagement were associated with both active-learning activities and when students found the activities challenging. These findings support those that caution that emotional engagement, when it is defined by enjoyment and positive feelings, is not always possible during effortful learning experiences (Baker, D'Mello, Rodrigo, & Graesser, 2010). Instead, students will, and probably should, experience frustration and other negative emotions when working to gain knowledge and mastery. In fact, both active learning and challenging activities were associated with higher cognitive engagement, despite lower emotional engagement. These types of situations are why a nuanced view of engagement, one that includes both cognitive and emotional components is helpful to fully understanding student engagement.

In considering longitudinal effects of emotional and cognitive engagement, we did not find the hypothesized relationship that emotional engagement proceeds and facilitates cognitive engagement (Pekrun and Linnenbrink-Garcia, 2012, Skinner and Pitzer, 2012). In this sample these two distinct types of engagement did not seem to influence each other over the course of the semester. This might indicate that young adult and adult learners are less reliant on their emotional experiences to facilitate their cognitive engagement. It might also be due to our small sample size relative to the complexity of the model. Due to the complexity of this model, a larger sample size is needed to support or contradict this finding. However, the longitudinal models did reveal that distinctions can be made between facilitators and outcomes of engagement. Specifically, student perceptions that they are learning something or improving in some way appear to be an outcome of engagement rather than a facilitator. This finding strengthens Shernoff's (2013) argument for many positive outcomes of engagement beyond grades. Future research with a larger data set will be able to further clarify and expand our understanding of the longitudinal nature of engagement over the course of a semester.

In terms of engagement in blended learning classes, our findings contribute to an understanding of how students navigate between the modalities of online and face-to-face learning activities and dispel a few concerns about the use of technology to replace seat-time. First, according to our findings, the location of the learning activity is less important to engagement than the actual pedagogical elements an instructor designs and the student perceptions of the learning experience. It is often assumed that the face-to-face component of blended learning classes contributes to meeting the emotional and social needs of students (Velasquez, Graham, & Osguthorpe, 2013) However, we found that the face-to-face setting of these classes was related to an increase in cognitive engagement and had no significant impact on emotional engagement, when controlling for the instructional activity. In addition to whatever social and emotional support in- person interactions provide, these in-person interactions appear to be important to students' cognitive engagement as well.

Blended and online learning course design is associated with flexibility and more opportunities for student choice in learning activities (Gu, 2016, Horn and Staker, 2014, Reeve and Tseng, 2011). We found that having choice in a learning activity was associated with higher levels of emotional engagement. Designing learning activities that provide student autonomy and choice and allow them to pursue personal interests is one of those factors within the instructors control that contribute to higher levels of emotional engagement for higher education students.

Other factors that have been mentioned as possible impediments to student engagement in blended courses were not found to be significant in our analysis. Lower scores of technology-related self-efficacy did not have a negative impact on student engagement. In fact, students with higher levels of technology self-efficacy had lower levels of emotional engagement. This might be due to the fact that none of the courses in our study were technology-related subjects. In addition, a student's first generation status, their previous experience with online or blended courses, and their preference for face-to-face or online learning were all not found to have an impact on their engagement and so were dropped from an already complex model. The only impediment we did find to engagement was the high amount of multi-tasking by students as they participated in on-line learning activities.

In conclusion, our research helps clarify aspects of student engagement in blended learning, higher education classes. The good news is that the pedagogical decisions an instructor makes appear to have a stronger impact on student engagement than the location of the learning activity or the individual characteristics of the learner. Learning activities that provide learner choices, develop sociality, are perceived as important to the student and are seen as relevant or related to existing student knowledge are all associated with higher levels of both cognitive and emotional engagement. Learning activities that are challenging and require active participation are related to higher levels of cognitive engagement, but might lead to a temporary decline in emotional engagement.

This research has solidified our belief that blended learning instruction has the potential to improve student engagement in higher education. The flexibility of instructional activities facilitates student choice, autonomy and value appraisals while providing regular interaction with peers and the instructor. Future research in this area should explore specific pedagogical techniques that can be implemented in both online and face-to-face settings that enhance students' perceptions of control, value, and interest and that decrease tendency of students to multi-task when interacting with technology.

Unfortunately, our small sample size did not allow us to definitively understand the relationship of emotional and cognitive engagement over time for young adult and adult learners. We recommend exploring a new research design that allows for both the longitudinal data collection along with a larger sample size in order to gain the statistical power to validate a complex model of change in latent variables over time.

Appendix 1
ESM survey instrument

[Date and time collected in survey software]

Are you reporting on a face-to-face or online learning activity?

Where are you?

What is the main learning activity you were doing (or just did in class)?

What else were you doing?

Did you enjoy what you were doing? (5-point scale: 1 = not at all to 5 = very much)

How well were you concentrating? (5-point scale: 1 = not at all to 5 = very much)

Did you feel good about yourself? (5-point scale: 1 = not at all to 5 = very much)

Were you learning anything or getting better at something? (5-point scale: 1 = not at all to 5 = very much)

Did you have some choice in picking this activity? (5-point scale: 1 = not at all to 5 = very much)

Did you experience frustration? (5-point scale: 1 = not at all to 5 = very much)

Did you set a goal for yourself prior to the class or activity? (5-point scale: 1 = not at all to 5 = very much)

Did you feel socially connected to anybody during this learning activity? (5-point scale: 1 = not at all to 5 = very much)

Your mood as you participated in the learning activity (all on 7-point bi-modal scale):

Happy–sad

Passive–active

Worried–relaxed

Lonely–sociable

Excited–bored

Focused–distracted

Curious–apathetic

Who were you with? (Check all that apply.)

How you felt about the main activity:

How challenging was it? (Ainley, 2012, Allen et al., 2007, Baker et al., 2010, Baneljee, 2011, Bates and Khasawneh, 2007)

Was it important to you? (Ainley, 2012, Allen et al., 2007, Baker et al., 2010, Baneljee, 2011, Bates and Khasawneh, 2007)

How skilled are you at it? (Ainley, 2012, Allen et al., 2007, Baker et al., 2010, Baneljee, 2011, Bates and Khasawneh, 2007)

Did you wish you had been doing something else? (Ainley, 2012, Allen et al., 2007, Baker et al., 2010, Baneljee, 2011, Bates and Khasawneh, 2007)

Was this activity interesting? (Ainley, 2012, Allen et al., 2007, Baker et al., 2010, Baneljee, 2011, Bates and Khasawneh, 2007)

How important was it to your future goals? (Ainley, 2012, Allen et al., 2007, Baker et al., 2010, Baneljee, 2011, Bates and Khasawneh, 2007)

Were you able to relate it to what you already know? (Ainley, 2012, Allen et al., 2007, Baker et al., 2010, Baneljee, 2011, Bates and Khasawneh, 2007)

Actions since you last filled out the survey:

How hard have you worked to keep up with this class? (Ainley, 2012, Allen et al., 2007, Baker et al., 2010, Baneljee, 2011, Bates and Khasawneh, 2007)

How much time have you spent on this class?

Have you interacted (in person, email, phone or online) with the teacher? (y/n)

Have you interacted (in person, email, phone or online) with classmates? (y/n)

Appendix 2
Learner characteristics latent variable CFA results

Variable	Unstandardized loading	S.E.
Self-efficacy
I believe I will receive an excellent grade in this class.	1.00	0.00
I am confident I can understand the most complex material in this course.	1.10	0.15
I am confident I can do an excellent job on the assignments and tests in this course.	1.05	0.08
Considering the difficulty of this course, the teacher, and my skills, I think I can do well in this class.	0.91	0.07

Subject interest
I like the subject matter of this course.	1.00	0.00
I am very interested in the content area of this course.	0.92	0.14
Understanding the subject matter of this course is very important to me.	0.519	0.13

Computer/tech self-efficacy
I am capable of solving or getting help to solve my computer-related problems.	1.00	0.00
I am very comfortable doing class work that is online.	1.30	0.32
I am capable of using the internet to find information I need.	0.59	0.23
Fit statistics: RMSEA = 0.018, CFI: 0.959, TLI: 0.942, SRMR: 0.088.

