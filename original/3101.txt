This study examined how the epistemic prompts designed to activate learners' epistemic cognition in sourcing affect their multimodal multiple-document reading (MMDR), considering learners' justification for knowing behaviors, prior knowledge, and gender. Participants were 48 university students, 28 females (58.3%), and 20 males (42.7%). Students were randomly assigned to an experimental group with epistemic-prompts that automatically generate a reflection report in addition to a note-taking function or a control group with a note-taking function only. Results showed that students relied primarily on YouTube videos for their MMDR with higher credibility ratings for the YouTube video than written texts. Besides, epistemic prompting, along with the automatically generated reflection report, enhanced students' scores on constructed-response items for the experimental group and helped them create a coherent mental representation of information from diverse multimodal sources beyond the effect of personal justification and multiple justifications. Implications of the study results are made to enhance students' epistemic cognition during their MMDR.

Previous
Next 
Keywords
Multimodal multiple-document reading

Epistemic metacognitive skills

Epistemic prompts

Epistemic reflection report

Justification for knowing

1. Introduction
In the 21st-century knowledge society, people increasingly rely on the Internet for both general and professional information. They need to constantly read digital texts of different modalities enhanced by features such as static or animated graphs, sounds, videos, hyperlinks, and other interactive elements for scientific information to address their daily needs (Lee and Wu, 2012, Lee and Wu, 2013; Stadtler and Bromme, 2013). The ability to read multiple documents and integrate information across different sources has become a part of digital literacy and one of the primary competences for 21st-century citizenship (Barzilai & Ka'adan, 2017; Barzilai, Zohar, & Mor-Hagani, 2018; Organisation for Economic Co-operation and Development [OECD], 2016; Wu, 2014; Wu & Peng, 2017). A general definition of digital literacy refers to the skillful operation of various digital devices, the employment of technologies for effective communication, and the application of digitally based resources considering the ethical, moral, and legal issues (Eshet, 2004; Ng, 2012). Beyond the utilization of technology and multimodal media, digital literacy involves the cognitive process of “searching, vetting and integrating information into the meaning-making process during online learning” at a more granular level (p. 56, Greene et al., 2014). The cognitive process of digital literacy is in line with the multiple-document reading paradigm. Notably, multiple-document reading entails knowledge construction from multiple sources via actively conducting source evaluation, producing meaningful explanations and arguments, and critically comparing and contrasting information within each document and among documents (Brante & Strømsø, 2017; Cromley, 2018). Viewed from Kitchner's three-level model of cognition, metacognition, and epistemic cognition, multiple-document reading involves learners' cognition to read and understand the information within each document (the 1st level) and attend to the progress of the first-level tasks (the 2nd level). Meanwhile, the third level monitors both cognition and metacognition processes regarding the epistemic aspects of their limits, criteria, and certainty of knowing within a single document and across multiple documents. Thus, epistemic cognition represents learners' thinking about the epistemic characteristics of specific information, knowledge claims, and their source as well as engaging in reasoning about the elements mentioned above (Barzilai & Ka'adan, 2017).

Most studies on multiple-document reading have focused on written documents only (e.g., Barzilai and Ka'adan, 2017; Strømsø, Bråten, Britt, & Ferguson, 2013). Other digital texts, such as YouTube videos, have become popular resources for learning among the younger generation due to the higher degree of flexibility for self-paced learning. A survey of students' attitudes toward using YouTube as a complementary learning tool showed that students believed they could learn a lot by watching videos about school subjects compared to reading books (Moghavvemi, Sulaiman, Jaafar, & Kasem, 2018). They also reported watching YouTube videos for academic learning, solving problems, and learning new things (Moghavvemi et al., 2018). Learning science using YouTube videos in formal or informal settings has gained increasing popularity (Jaffar, 2012; Welbourne & Grant, 2016). Students who believed it is a social norm to watch YouTube videos for science learning had more definite intentions to seek science videos on YouTube (Rosenthal, 2018). The finding implies that they would direct their behavior toward the socially acknowledged norm and increase their use of YouTube for science learning. Thus, watching videos for learning purposes has become an integral part of students' daily literacy practices. However, little is known about students' multiple-document reading involving digital texts of different modalities. To avoid confusion, we referred to the multiple-document reading of unimodal texts (i.e. written texts in this study) as unimodal multiple document reading (UMDR), and the multiple-document reading of multimodal texts as multimodal multiple document reading (MMDR) in this research.

Research on UMDR showed that individual differences in justification for knowing (i.e., the standards and criteria people use to justify their knowledge claims) are predictive of their UMDR performance (e.g. Bråten, Ferguson, Strømsø, & Anmarkrud, 2013; Ferguson & Bråten, 2013). Mainly, the effective integration of multiple documents is built upon a coherent mental representation of information from different sources. As a result, many studies have investigated students' UMDR based on their arguments in essays or open-ended questions for evidence of effective integration (Barzilai and Ka'adan, 2017; Mason, Junyent, & Tornatora, 2014). However, forming a coherent representation of the multiple information sources remains difficult for students due to factors such as the information being spread out over different texts and degrees of agreement and disagreement between texts (Braasch & Bråten, 2017). Reviewing 21 published studies, Barzilai et al. (2018) found medium-to-large effect sizes of using instructional practices (e.g., modeling, group discussion, and direct instruction) to enhance students' integration of UMDR. Some work in the context of information seeking, and UMDR has used paper prompts (e.g., Barzilai and Ka'adan, 2017; Kammerer, Meier, & Stahl, 2016) or digital prompts (e.g., Stadtler and Bromme, 2007, Stadtler and Bromme, 2008) for source evaluation. However, there are still gaps in the research needing attention. Mainly, only 23.0% of all reviewed studies in Barzilai et al. (2018) used digital technologies to support document integration by providing instruction or intervention. Therefore, integrating digital technologies to enhance UMDR is still an under-researched area. Besides, with the increase of multimodal resources, integrating and comparing information sources of different modalities on the Internet has become a major challenge for learners. In the literature, a significant focus was placed on the process of multiple-document comprehension literature (e.g., Bråten, Britt, Strømsø, & Rouet, 2011; Rouet & Britt, 2011), with nearly few about the process of multimodal comprehension (primarily about testing the effect of media presentation principles, e.g., Mayer, 2005). The evidence above highlighted desiderata for a theory of MMDR and necessitated more research in this regard (Cromley, 2018).

Therefore, the purpose of this study was twofold. First, we explored whether technology-integrated epistemic prompts that activate learners' epistemic cognition in sourcing to reflect on the nature of knowledge and the process of knowing can facilitate learners' MMDR. Second, we examined learners' justification for knowing (including personal justification and multiple justification) based on their arguments in the constructed response items to understand better how different types of justification for knowing along with the epistemic prompts predicted learners' MMDR performance. Owing to the deficiency of MMDR literature, we built on the theoretical framework of UMDR and reviewed research about the justification for knowing and sourcing to describe their relations with UMDR and to inform the development of an MMDR theoretical framework.

2. Literature review
2.1. The theoretical framework related to UMDR
The paradigm of UMDR has several features in common with traditional single-document reading; however, it requires more advanced document-synthesizing abilities and reading strategies, as outlined in the following.

According to Kintsch (1988), there are several layers of representation. The surface code refers to the wording and syntax of the words and sentences. Textbase appears in a propositional form and focuses on inferences across paragraphs. Situation models are built through mental representation between the interaction of readers' prior knowledge and the written texts. Finally, text genre refers to document types, such as journal articles or online news. In addition to the above layers, another layer, the intertext model, is proposed in the so-called documents model (Britt, Perfetti, Sandak, & Rouet, 1999; Perfetti, Rouet, & Britt, 1999). An intertext model detects conflicts and similarities among documents with appropriate references to the author, publisher/types of media, and publication (Perfetti et al., 1999).

Rouet and Britt (2011) maintained that reading is a contextualized activity, whereby people approach texts relevant to the task at hand. Thus, they proposed the MD-TRACE model (Multiple-Document Task-Based Relevance Assessment and Content Extraction). Within this model, task relevance refers to the extent to which the text contains information needed to solve a given problem or complete a task. In contrast, task importance concerns locating a text segment to understand the text (McCrudden & Schraw, 2007).

Readers may switch from task importance to task relevance depending on the goal of their UMDR (Rouet & Britt, 2011). Notably, we tend to construct our mental model by integrating prior knowledge with the current document content; nevertheless, when documents contain conflicting information, a multiple-situations model may be created (Strømsø et al., 2013). Thus, the situations model is the readers' synthesis and interpretation of the multiple documents, whereas the intertext model provides additional tags or sources to the arguments (Strømsø et al., 2013).

Based on the MD-TRACE model, we go through a recursive cycle of multiple-document reading as follows: (a) create or update task model; (b) evaluate information needed; (c) interact with documents to assess the relevance of the information, process text contents, and create/update documents model; and (d) create/update task product and evaluate whether the product meets goals (Rouet & Britt, 2011).

Individual differences in epistemic beliefs (i.e., our beliefs of what knowledge is; nature of knowledge) and how we get to know (the process of knowing) are associated with our construction of the situations model and the intertext model (Bråten et al., 2011). According to Hofer (2000), four epistemic beliefs are subsuming the nature of knowledge and the process of knowing: certainty, simplicity of knowledge, source, and justification of knowing beliefs. Among these, source and justification beliefs have been found to influence multiple-document reading performance (e.g., Anmarkrud, Bråten, & Strømsø, 2014; Brandmo & Bråten, 2018; Bråten et al., 2013; Mason et al., 2014). According to Bråten et al. (2011), the source is highly related to the construction of the intertext model because readers need to evaluate the credibility, main purposes, and genre of the text to create a source-content link in their argument. On the other hand, the justification for knowing is related more to the situations model; specifically, those believing that knowledge construction is achieved via critical thinking, scientific inquiry, and justification from multiple sources are more likely to integrate information from diverse perspectives and situations.

Building on the theoretical framework of UMDR, the current study provided technology-integrated scaffolds to activate learners' epistemic cognition in sourcing during their MMDR and examined their justification for knowing in text integration to investigate university students' MMDR performance.

2.2. Justification for knowing and UMDR
Justification for knowing are the standards and criteria people apply to justify their knowledge claims. Using the think-aloud approach, Ferguson, Br’aaten, and Strømsø (2012) found “justification for knowing, ranging from justification of knowledge claims through observation and authority, or on the basis of what feels right, to the use of rules of inquiry and the evaluation and integration of different sources (p. 104).” Recently, Bråten, Brandmo, and Kammerer (2019) validated the trichotomous framework of justification for knowing, namely personal justification, justification by authority, and justification by multiple sources, in an internet-specific justification inventory. Empirically, learners' justification for knowing is associated with their performance on UMDR. In a study examining the judgment of trustworthiness in two texts (a science text and a newspaper article) about climate change, those who relied on personal justification based on their own opinions and document content as criteria rated both documents less trustworthy (Strømsø, Bråten, & Britt, 2011).

In contrast, those who used multiple criteria, including their own opinions, author, and content, rated the science text as being more trustworthy. Further, investigating the predictivity of three types of justification on UMDR, researchers found personal justification (using personal opinion as justification criterion) was negatively related to essay performance. In contrast, justification by multiple sources was positively related to essay performance (Bråten et al., 2013). On the other hand, justification by the authority was not related to UMDR. Similar findings were found in Brandmo and Bråten (2018).

Justification for knowing can be assessed in various ways, via observation using a think-a-loud protocol or via psychological measurement using questionnaires (e.g., Bråten et al., 2019; Ferguson et al., 2012). In this study, we sought evidence of students' justification for knowing by quantifying the number of justification for knowing in learners' constructed response items and tested the predictivity of different types of justification for knowing indicators on students' MMDR.

2.3. Research related to sourcing and UMDR
Building a mental representation of information from multiple sources involves several components and skills, including selecting and locating relevant information, evaluating and sourcing, and synthesizing and integrating (Leu et al., 2015; Rouet & Britt, 2011). Among the components and skills, sourcing is the process of attending to the source of a document, judging its trustworthiness, and referring to the document source for predicting or interpreting an article content (List, Alexander, & Stephens, 2017; Helge, Strømsø et al., 2013; Wineburg, 1991). Research on learners' epistemic cognition may shed some light on their UMDR practices about sourcing. Students who believed that information should be evaluated based on rules of inquiry and cross-checked from multiple pages spent more time browsing the information returned by the search query and accessed more pages further down the search engine list (Salmerón & Kammerer, 2012). University students with more advanced epistemic beliefs also had more recurrent searching behavior and more in-depth exploration of multiple sources during their online search (Hsu, Tsai, Hou, & Tsai, 2014). Moreover, in evaluating the least trustworthy websites, university students with more informed Internet-specific epistemic beliefs characterized by multiple justification across different sources rated the websites lower than those with less informed epistemic beliefs (Knight et al., 2017).

Nevertheless, in an analysis of 18 university students' spontaneous sourcing in reading six controversial texts, 67.7% of the sourcing verbalization was paying attention to the source information, 27.3% pointing to the evaluation of trustworthiness, and only 0.8% and 4.7% indicating the use of source information for interpreting or predicting contents using a think-aloud strategy (Strømsø et al., 2013). The interviews conducted by Paul, Macedo-Rouet, Rouet, and Stadtler (2017) on 44 ninth-graders in Germany and France provided some insight into why students seldom conduct sourcing. Notably, students attributed school as a sourcing-unfriendly environment and lack of external prompts as the primary contextual reason why they would refrain from source evaluation; that is, students were lacking in intrinsic motivation for sourcing and focused more on content information instead. The authors noted that students were often provided with materials for study at school instead of being encouraged to evaluate the validity of information (Paul et al., 2017).

Though sourcing, especially the evaluation of source credibility and using it for prediction and interpretation, is not commonly observed among students, the sourcing behavior may enhance readers' construction of a coherent mental representation of multiple documents. Specifically, the result of Strømsø et al. (2013) showed that university students who spontaneously commented on the source information were more likely to include source citations in their essays regarding cell phones and radiation. In another study, Mason et al. (2014) taught 134 ninth graders how to evaluate the source of online documents by evaluating the purpose and knowledge of the author, determining whether the information is based on scientific evidence, assessing one's understanding of the content, and whether explanation on a specific website coincide with scientific knowledge from other sources based on Wiley et al. (2009). Their findings showed that the experimental group had better knowledge transfer and argumentation; moreover, members of the experimental group visited irrelevant webpages less frequently than their counterparts in the control group.

2.4. The application of prompts to activate epistemic cognition and enhance sourcing
Sourcing is positively associated with UMDR outcomes, such as source-content links and argumentation (e.g., Mason et al., 2014; Strømsø et al., 2013). However, students rarely attend to source information (e.g., author, types of publication) even though they are capable of conducting a source evaluation on the credibility and validity of the content (Kobayashi, 2014; Barzilai, Tzadok, & Eshet-Alkalai, 2015; Paul et al., 2017). Some researchers provided students with digital prompts to monitor their comprehension and to evaluate the credibility of the information content; their results showed that participants receiving evaluation prompts or both evaluation and monitoring prompts enhanced their source knowledge and commented more on the source information in their arguments (Stadtler and Bromme, 2007, Stadtler and Bromme, 2008). Other researchers offered written prompts, in the forms of organizers, worksheets, or checklists for source evaluation. For example, Barzilai and Ka'adan (2017) provided two consecutive organizers to facilitate students' epistemic cognition in source evaluation and information integration from multiple sources; these researchers found significant improvement in integration over time and better performance by the experimental than the control group.

Further, Kammerer et al. (2016) employed a source cueing worksheet in studying students' UMDR and demonstrated better argumentation and more elaborated learning in students, as a result. Kim and Hannafin (2016) used an online annotation system and a source evaluation checklist to facilitate students' mental representation of their argumentation for the experimental group. The control group received only a checklist. Study results showed that the online annotation system helped enhance the quality of students' argumentation.

Overall, results of the above studies suggest that prompting students to reflect on the source of information and providing them with a space for information integration is predictive of performance on UMDR in terms of source knowledge, source-content links, text integration, and argumentation quality (Barzilai & Ka'adan, 2017;Kammerer et al., 2016; Kim & Hannafin, 2016; M. Stadtler and Bromme, 2007, Stadtler and Bromme, 2008).

In the current study, we provided a web-based note-taking tool as a baseline for students in both the experimental and control groups to construct their intertext representation. Also, we offered epistemic prompts for students in the experimental group to facilitate their epistemic cognition in sourcing (e.g., to reflect on the relevance, credibility, type of website, and source of information) and to summarize the content of the information they read whenever they finished reading a webpage. Students in the experimental group could also view a report of their epistemic reflection after they finished their task.

The technology-integrated scaffolds (i.e., the epistemic prompts and note-taking function) were intended to activate learners' epistemic cognition during their MMDR from various multimodal sources with the following functions.

a)
Relieving working memory: MMDR places high demands on learners' working memory due to the complex processes in monitoring, evaluating, and synthesizing various multimodal sources. The note-taking tool may serve as a temporary store for information and knowledge integration to relieve learners' cognitive load due to the limited capacity in their working memory (Jansen, Lakens, & IJsselsteijn, 2017; Lin, Lee, Wang, & Lin, 2016; Piolat, Olive, & Kellogg, 2005).

b)
Scaffolding epistemic cognition: We assumed that integrating information from various multimodal sources is facilitated by making sources explicit, which entails awareness and regulation of learners' epistemic cognition about the specific information, knowledge claims, and source (Barzilai & Ka'adan, 2017). Thus, learners are prompted to provide relevance and reliability ratings of the web page, the type and source of information, and a summary or reflection of the content whenever they finish reading a web page.

c)
Facilitating text integration: Successful MMDR requires connecting knowledge claims with and among multimodal sources using rhetorical predicates (e.g., “contradict” or “corroborate”) to form an intertext model (Strømsø et al., 2013). The epistemic prompting design will record learners' ratings and reflections for each source and automatically generate a report upon completing the learning task. Therefore, learners will have access to the report to review their reading traces and reflect upon their ratings and inputs/summaries of the sources to compare the similarities and differences.

2.5. Purpose of the study
This study was designed to advance our understanding of the theories behind and application of MMDR encompassing both YouTube videos and written texts on the Internet among university students. Specifically, we tested the effect of technology-integrated scaffolds, including digital epistemic prompting and note-taking, on students' MMDR, considering their justification for knowing based on different sources of information. To that end, we attempted to answer the following research questions.

RQ1

What are learners' justification for knowing behaviors, quantified by the number of personal justifications, justification by assigned texts, justification by search, and justification by multiple sources as found in their responses in the constructed response item?

RQ2

How does epistemic prompting designed to activate epistemic cognition in sourcing affect university students' MMDR?

RQ3

How does the epistemic prompting designed to activate epistemic cognition in sourcing affect university students' MMDR, controlling for individual differences in gender and justification for knowing, including personal justification and multiple justification?

3. Method
3.1. Participants
Forty-eight Taiwanese university students enrolled in an educational psychology course participated in the study in return for partial course credit. Twenty-eight students were female (58.3%), and 20 were male (42.7%). Students were randomly assigned to an experimental group with epistemic-prompts with reports derived from their responses to the prompts and a note-taking function or a control group with the note-taking function only. No association between gender and treatment group was found, Pearson chi-square = 1.371, df = 1, p = .242. The recruitment and data collection process were shown in Fig. 1 and was approved by the research ethics committee in the author's institute.

Fig. 1
Download : Download high-res image (397KB)
Download : Download full-size image
Fig. 1. The experimental procedure.

3.2. Procedure
Fig. 1 illustrates the experimental procedure. First, the instructor provided a task orientation by explaining the purpose of the experiment (20 mins). In particular, participants were told that the relationship between neuroscience and learning is an essential issue in educational psychology; however, there are no consistent and definite answers about brain plasticity. Therefore, they would need to research this topic by reading and watching the three assigned texts and performing additional online searches for a school project. Students received 20 min of training on how to use the note-taking (control: Note-only) and note-taking plus epistemic prompting functions (experimental: Prompt+Note). After a 10-min break, students started the 40-min MMDR, including the three assigned texts and their additional free online search about the topic. Students could take a 10-min break after the MMDR task. They then took the multiple-choice post-test (10 min) and the constructed-response post-test (40 min).

3.3. Materials
Three online texts were assigned for students to read: A YouTube video and two online articles. The YouTube video was a TEDTalk entitled “After watching this, your brain will not be the same” (14 mins and 25 s in length) by a neuroscience professor (Boyd, 2015). The transcript of this video is about 3621 Chinese characters in length. The tone of TEDTalks is characterized by the basic and descriptive vocabulary that is easily understood, targeting a general readership of non-specialists. The first written article, also written by a neuroscientist, was entitled “The critical ingredient for the brain” from Scientific American, targeting general but well-educated readers using technical languages. The article is 2541 characters in length. Both media are renowned for disseminating scientific knowledge to the public by famous scientists. The second written article was entitled “Overturning your understanding: New research findings said adult brains make no new neurons.” The article was from TechNews, an online newspaper carrying popular science content for general readers with interests in science and technology subjects. The TechNews article holds that adult brains no longer make new neuron cells, which is a conflicting view from the first two texts. The article is 1274 characters in length. Though the three texts have different tones in writing or discourse, the contents of the texts are based on scientific evidence.

According to Sun, Morita, and Stark (1985), native college students can read Chinese at a speed of 580 characters/min. Therefore, the two written articles may take 4.4 mins and 2.2 mins, respectively, for students to finish. In addition to the two articles, students also needed to watch the assigned YouTube video. Therefore, it may take about 21 min for the students to complete all the assigned texts. Thus, we allocated 80 min for students to complete the assigned texts and take notes or perform additional online searches about the topic.

3.4. Assessment of MMDR
The assessment of MMDR was designed in line with the assessment framework of Programme for International Student Assessment (PISA) 2018 (OECD), including three aspects of text processing involving both single-document and multiple-document processing, namely, locating information, understanding, as well as evaluating and reflecting. Locating information assesses students' ability to search the relevant text and access and retrieve information within a text. Understanding assesses students' ability to represent literal meaning and generate inferences. Finally, evaluating and reflecting is a higher-level process requiring students to assess the quality and credibility of the text, reflect on content and form, and detect and handle conflicts.

We included multiple-choice (MC) and constructed-response (CR) items to assess students' MMDR. We used 10 MC items to test students' prior knowledge about the basic information of neuroscience, covering contents on the brain areas and the corresponding functions. Additionally, another set of 10 MC items was created to test students' MMDR to locate information and understand the assigned texts. Sample questions include “Why do London taxi cab drivers who have to memorize a map of London to get their taxi cab license have larger brain regions devoted to spatial or mapping memories?” (ans: Structural change in the brain). Furthermore, “Which of the following is a correct description about a critical period in the brain?” (ans: It may lead to a detrimental result if critical period starts too early, too late, or at an inappropriate time). One point was given for each correct answer on the MC items.

The CR items were designed to test students' ability to understand a single text (CR1) or reflect and evaluate a single text (CR2). CR3 and CR4 assessed students' ability to evaluate and interpret multiple texts and detect and handle conflicts in them by using evidence from the assigned texts or additional online search. A detailed description and rating of the CR items are presented in Table 1.


Table 1. Rubrics for scoring constructed-response items.

Item	Description	Rating Criteria
CR1	In “The critical ingredient for the brain,” gamma-aminobutyric acid (GABA), a critical ingredient that inhibits the neuron activities, is introduced. Why does the author mention GABA and how does it relate to the critical period in brain development?	CR1 requires students to present the literal meaning of GABA and generate GABA inferences and the critical period for its presence. One point was given to participants referring to neuron genesis during infancy, synapse pruning, GABA functions, and the consequences of increasing or reducing GABA at the beginning of the critical period. Max = 5, min = 0.
CR2	What information does Dr. Lara Boyd try to convey? What are her perspectives about learning and brain plasticity?	CR2 requires students to reflect on the content and form of the assigned text. One point was given for every accurate recall of information from the TEDTalk by Dr. Lara Boyd, including characteristics of brain plasticity (definition, individual differences, positive and negative consequences), forms of brain plasticity, constraints of brain plasticity, and how to enhance learning given brain plasticity. Max = 8, min = 0.
CR3	Are there similarities or differences between the three assigned texts? Please compare and contrast the perspectives in the three assigned texts.	CR3 requires students to evaluate and interpret information in the three assigned articles while detecting and handling conflicts. One point was given for each correct pairwise comparison of viewpoints among the three articles, including viewpoints about neuron genesis in adults, learning and brain plasticity, the restart of the critical period, etc. Max = 6, min = 0.
CR4	Do you agree or disagree with the idea of brain plasticity? Please articulate your position and provide evidence to support your arguments based on information of the three assigned texts or your additional online search.	CR4: Grading was based on the position taken and the number of correct arguments made based on the assigned articles, information from an online search, or personal experiences. Max = 6, min = 1.
3.5. Coding for justification for knowing
Justification for knowing was coded as follows: personal justification, justification by assigned texts, justification by search, and multiple justification. The personal justification was coded 1 when students justified their knowing based solely on their own opinions; otherwise, it was coded 0. Justification by assigned texts was calculated based on the number of different source comments that students mentioned from the assigned texts in their CR responses (max = 3 and min = 0). Justification by search was coded 1 for students who justified their claims using additional online searches and 0 for those who did not. Multiple justification quantifies the degree to which students used information from multiple sources to justify their claim, including the information from the three assigned texts (0–3), personal opinions (0–1), and additional online searches (0–1) (max = 5 and min = 0).

3.6. The epistemic-prompting system
The epistemic prompting system was built upon a chrome extension that included a note-taking and an epistemic prompting function. As shown in Fig. 2, students can take notes while searching online about neuroscience and learning. Meanwhile, we included the epistemic-prompting function (as shown in Fig. 3) to assist students in monitoring and evaluating the source information. Specifically, in the experimental group, students were prompted to evaluate the relevance, credibility, type of website (blog, forum, official websites, newspaper/magazine, academic/science websites), and source of information (rumors, opinions, authority/expert, scientific evidence, multiple sources) whenever they finished reading a webpage. Relevance and credibility were rated on a 5-point Likert scale, ranging from 1 not at all relevant/credible to 5 extremely relevant/credible. Students also had to briefly summarize the webpage information and provide their opinions about the webpage. At the end of the MMDR task, students in the experimental group were also instructed to review their online epistemic reflection report generated automatically by the system. The individualized report is presented in a temporal order based on the time the student visited the webpage, with columns showing the URL of each webpage, the student's ratings of relevance and credibility of the webpage, types of publication, and sources of the content as well as the student's summary of each webpage visited in a matrix form, as shown in Fig. 4.

Fig. 2
Download : Download high-res image (307KB)
Download : Download full-size image
Fig. 2. Screenshot of the note-taking function.

Fig. 3
Download : Download high-res image (230KB)
Download : Download full-size image
Fig. 3. Screenshot of the epistemic prompt.

Fig. 4
Download : Download high-res image (355KB)
Download : Download full-size image
Fig. 4. Screenshot of a student's automatically generated epistemic reflection report.

3.7. Data analysis
We computed descriptive statistics for students' performance on the MC items and CR items, their use of personal justification, justification by assigned texts, justification by search, and multiple justification. Independent-samples t-tests were conducted to test the effect of epistemic prompting on students' performance on both the MC and CR items. Additionally, we performed a repeated-measure ANOVA to test the mean differences between students' ratings of the relevance and reliability of the three assigned texts for the experimental group. Finally, hierarchical linear regressions were performed to test the predictive validity of individual differences in gender and types of justification for knowing and the effect of epistemic prompting on students' MMDR performance. Power analyses were conducted to ensure sufficient power to detect significant effects with the current sample size. The power analyses for the independent samples t-test, hierarchical regression analysis, and repeated measure ANOVA were conducted based on the procedure proposed by Cohen (1988) using the pwr package and WebPower packages using R (The R Core Team, 2020).

4. Results
4.1. Descriptive statistics and correlations
Table 2 shows the frequency, percent, and cumulated percent of each justification for knowing by their distinct levels. Among the 48 participants, six (12.5%) justified their claims based solely on their personal opinions. As for justification by assigned texts, six students (12.5%) used none of the assigned texts, which coincided with the results of the use of personal justification. Twenty-eight (58.3%) justified their knowing based on one assigned text; specifically, they used the information from the YouTube video to justify their claims. Respectively, seven students used 2 or 3 assigned texts to justify their arguments.


Table 2. Frequency of types of justification for knowing.

Freq.	%	Cum. %
Personal justification only
No	42	87.5	87.5
Yes	6	12.5	100.0
Total	48	100	
Justification by assigned texts (# of texts)
0	6	12.5	12.5
1	28	58.3	70.8
2	7	14.6	85.4
3	7	14.6	100.0
Total	48	100	
Justification by search
No	40	83.3	83.3
Yes	8	16.7	100.0
Total	48	100	
Multiple justification (# of different sources)
1	15	31.3	31.3
2	20	41.7	72.9
3	9	18.8	91.7
4	3	6.3	97.9
5	1	2.1	100.0
Total	48	100	
Regarding justification by search, 8 (16.7%) out of the 48 students used information from their additional searches to justify their claim. Finally, for multiple justification, 15 students (31.3%) used a single source to justify their knowing, 20 (41.7%) used two sources, and 13 students (27.1%) used three or more sources to justify their knowing. Table 3 presents the descriptive statistics of the observed variables. On average, students justified their claims using 1.31 assigned texts. The skewness and kurtosis values of the observed interval variables were within ±3, suggesting no non-normality problem.


Table 3. Descriptive statistics of the observed variables.

PJ	JAT	JS	MJ	CR1	CR2	CR3	CR4	Total CR	Total MC
Mean/%	0.13	1.31	0.17	2.06	1.77	4.21	3.10	3.31	12.40	7.10
SD	0.33	0.88	0.38	0.98	1.65	1.66	1.56	1.34	3.89	1.45
Min	0	0	0	1	0	0	0	1	5	3
Max	1	3	1	5	5	8	6	6	23	10
Skewness	–	0.70	–	0.87	0.56	0.09	−0.15	−0.22	0.30	−0.85
Kurtosis	–	−0.11	–	0.60	−0.71	0.52	−0.13	−0.79	−0.05	0.94
Note. PJ: Personal justification; JAT: Justification by assigned texts; JS: Justification by search; MJ: Multiple justification.

PJ and JS are dichotomous variables; thus, they are reported in percent of participants showing the justification behavior and are not calculated for skewness and kurtosis.

For the experimental group, we tested the mean differences between their ratings of relevance and credibility of the assigned digital texts using the available data. Mauchly's test of sphericity was not violated for credibility, Mauchly's W = 0.685, χ2 = 4.916, df = 2, p = .086, but was violated for relevance, Mauchly's W = 0.334, χ2=14.258, df = 2, p = .001. Therefore, the Greenhouse-Geisser correction was used to calculate the F value for relevance. Students rated the three assigned texts equally relevant (M = 3.37 for the Scientific American text, M = 3.06 for the newspaper article, and M = 3.56 for the YouTube video), F(1.2, 16.8) = 3.193, p = .086, but they thought the newspaper article (M = 2.53) was less credible than the Scientific American text (M = 3.11) and the YouTube video (M = 3.19), F(2, 28) = 8.235, p = .002, and partial η2=37%.

4.2. Results of independent sample t-tests
Table 4 exhibits the independent samples t-test results on the prior knowledge and MMDR items for the experimental and control groups. As illustrated, there was no mean score difference in prior knowledge and total MC between the two groups (MN = 5.52, MPN = 5.21, t = 0.70, p = .486 for prior knowledge; MN = 7.08, MPN = 7.13, t = −0.10, p = .922 for total MC).


Table 4. Independent samples t-tests on prior knowledge and multimodal multiple document reading.

Note-only N = 24)	Prompt+Note (N = 24)	
Measures	M	SD	M	SD	t	p	Cohen's d
Prior knowledge	5.52	1.43	5.21	1.64	0.703	0.486	0.20
Total MC	7.08	1.44	7.13	1.48	−0.10	0.922	0.03
CR1	1.46	1.53	2.08	1.74	−1.32	0.193	0.38
CR2	3.46	1.41	4.96	1.57	−3.47	0.001	1.01
CR3	2.25	1.19	3.96	1.43	−4.50	<0.001	1.30
CR4	2.92	1.38	3.71	1.20	−2.12	0.039	0.61
Total CR	10.08	2.95	14.71	3.32	−5.11	<0.001	1.47
Regarding the CR items, CR1 and CR2 require learners to understand or evaluate a single text, while CR3 and CR4 involve evaluating and interpreting multiple texts as well as detecting and handling conflicts among multiple texts. The Prompt+Note group scored significantly higher than the Note-only group on CR2 (MN = 1.46, MPN = 2.08, t = −3.47, p = .001, Cohen's d = 1.01), CR3 (MN = 2.25, MPN = 3.96, t = −4.50, p < .001, Cohen's d = 1.30), CR4 (MN = 2.92, MPN = 3.71, t = −2.12, p = .039, Cohen's d = 0.61), and the total CR (MN = 10.08, MPN = 14.71, t = −5.11, p < .001, Cohen's d = 1.47) with medium-to-large effect sizes. No statistically difference was found on CR1 or Total MC between the Prompt+Note group and the Note-only group.

4.3. Results of the hierarchical regression analysis
We regressed the total CR scores on participants' gender, treatment group, and use of personal justification and multiple justification. Table 5 presents the result of the hierarchical regression analysis. As illustrated, gender was not a significant predictor of students' total CR scores in Model 1, F(1, 46) = 1.27, p = .27, but it was still included as a covariate in the follow-up analyses. In Model 2, students' two justification variables explained a considerable amount of variance in total CR scores, F(3, 44) = 6.06, p = .002, R2 = 29%, and adjusted R2 = 24%. Personal justification was negatively associated with students' total CR (BPJ = −3.97, t = −2.45, p = .018) while multiple justification was positively associated with students' total CR (BMJ = 1.14, t = 2.09, p = .042). In Model 3, treatment status was included in the analysis, yielding an increase of 26.5% and 26% in R2 and adjusted R2, respectively, F(4, 43) = 13.28, p < .001. Significant predictors of students' total CR scores were multiple justification (BMJ = 1.26, t = 2.87, p = .006) and treatment status (Btreatment = 2.05, t = 5.00, p < .001). On average, for every additional source used to justify for knowing, students' total CR increased by 1.26 points controlling for other variables. Besides, those in the experimental group scored 2.05 points higher in the total CR on average than their counterparts in the control group, holding everything constant.


Table 5. Hierarchical regression of gender, treatment, and types of justification on the total CR scores.

Model	Variable	B	SE	Beta	t	p	Tolerance	VIF	R2	Adj R2
1	Intercept	12.93	0.73		17.66	<0.001			0.03	0.01
Male	−1.28	1.13	−0.16	−1.13	0.27	1.00	1.00		
2	Intercept	10.93	1.31		8.37	<0.001			0.29	0.24
Male	−0.94	1.02	−0.12	−0.92	0.36	0.94	1.06		
Personal justification	−3.97	1.62	−0.34	−2.45	0.02	0.83	1.21		
Multiple justification	1.14	0.55	0.29	2.09	0.04	0.86	1.17		
3	Intercept	8.24	1.18		6.98	<0.001			0.55	0.51
Male	−0.46	0.83	−0.06	−0.55	0.59	0.93	1.08		
Personal justification	−2.42	1.34	−0.21	−1.81	0.08	0.79	1.27		
Multiple justification	1.26	0.44	0.32	2.87	0.01	0.85	1.17		
Treatment	2.05	0.41	0.53	5.00	<0.001	0.92	1.09		
5. Discussion
5.1. Justification for knowing in MMDR
This study advances our understanding of justification behavior and epistemic prompting intervention effect on university students' MMDR. Specifically, we found that university students mostly relied on the YouTube video to justify their knowing. Although we did not specify which digital text was the primary text, the length of the YouTube video transcript was about the total length of the other two written texts. They rated the YouTube video equally relevant as the other two sources but as more credible than the newspaper article.

Working with MMDR places a high demand on working memory due to the different multimodal information presented (Sweller, Ayres, & Kalyuga, 2011). As a result, in this study, nearly 60% of the participants used only one source from the assigned digital texts, namely the TEDTalk YouTube video. This finding may be explained by reference to the study by Moghavvemi et al. (2018), where students reported that they could learn a lot from watching videos about school subjects in comparison to reading books. In Youtube videos, especially informational videos such as TEDTalk, the speaker delivers his or her presentation in a very persuasive tone with graphic or visual aids. Therefore, the video itself is a multimodal presentation of information. Students likely prefer watching videos for learning because they can gain a retrieval advantage when they encode the information via multiple modalities (Shams & Seitz, 2008). However, the videos on YouTube are of mixed quality. It is often suggested to view videos via the YouTube recommendation system (Welbourne & Grant, 2016). In a sample of 718 participants from 18 to 82 years of age, researchers showed that less than 20% (n = 142) of the participants performed a voluntary online search for further information during a credibility assessment task on Youtube videos (Michalovich & Hershkovitz, 2020). Based on findings in the literature and in the current study, we view it as a critical practice to activate students' epistemic cognition and teach them how to evaluate the quality of videos and validate the information from multiple sources.

5.2. The effect of epistemic prompting on MMDR
Epistemic prompting exhibited a medium-to-large effect size on students' performance on CR items, but no differential effect was found on their MC performance. The MC items evaluated students' ability to locate and understand the texts. These are basic skills that require less critical thinking to compare information across different sources; thus, additional instructional support or intervention may not affect students' performance on these aspects (Lee, 2015, Lee, 2018).

In contrast, the additional epistemic prompting did influence students' performance on the CR items, except on CR1, which focuses on representing the literal meaning and understanding the mechanism of a specific concept. The remaining CR items assess students' advanced MMDR ability in reflecting on the form and content of the texts and integrating information across different sources or detecting and handling conflicting viewpoints.

Researchers have shown that prompting students to evaluate source credibility and author expertise enhanced their source knowledge and generated more comments on the source information (Stadtler and Bromme, 2007, Stadtler and Bromme, 2008). Moreover, sourcing is a critical practice in knowledge construction from various online sources (Rouet & Britt, 2011). It is associated with more complex arguments and better integration from the evidence of think-aloud during reading (Barzilai et al., 2015).

Our finding is in line with the arguments of these studies: Students in the experimental group performed better on MMDR integration and detecting and handling conflicts. We employed the digital epistemic prompts to activate students' epistemic cognition in sourcing and attend to the relevance, credibility, type of publication, and content source. In a similar vein, Barzilai and Ka'adan (2017) provided students with two organizers as strategic scaffolds to assist their sourcing and UMDR integration. In the current study, we streamlined students' MMDR by prompting them to reflect on the source information and summarize the webpage content during their MMDR process. Specifically, the epistemic evaluation report automatically derived from students' responses to the epistemic prompts. It served as strategic scaffolds for students to construct a coherent mental representation of the multiple documents consisting of diverse viewpoints. The individualized epistemic reflection report enabled students to review their web-browsing traces sequentially and compare the credibility and summarized content in a single and coherent matrix, leading to enhanced performance for the experimental group.

Though we found a positive effect of epistemic prompting on learners' MMDR, the current study included a limited range of sources (a single Youtube source), source credibility (high), and task prompts (integration). Further work considering more sources, varied credibility, and task prompts (e.g., to provide a summary, an argument, or a recommendation) may provide a more comprehensive view regarding the use of epistemic prompts on MMDR.

5.3. The predictive validity of justification for knowing and epistemic prompting on MMDR beyond gender
Gender did not exhibit a differential effect on students' total CR scores, but students' justification for knowing and the effect of epistemic prompting did. Particularly, multiple justification was positively associated with students' total CR, and personal justification was negatively associated with students' total CR. Our results are in line with findings and arguments of previous research (Bråten et al., 2013; Brandmo & Bråten, 2018; Scheufele & Krause, 2019). Nevertheless, we found that personal justification no longer negatively predicted students' MMDR after epistemic prompting was included in the hierarchical regression. It seemed that epistemic prompting mitigated the negative influence of personal justification on MMDR. Specifically, by prompting students to reflect on the relevance, credibility, and source information and summarizing the visited pages, students may be engaged in an active knowledge-construction process to think about the source information and the information the author intended to convey. Notably, the individualized epistemic reflection report offered a sweeping view of the source information and the content summary of the multiple multimodal documents, which may facilitate students' MMDR integration to avoid justifying their argument based on personal opinions. This argument was partly supported by our additional analysis on the differences in the percentages of personal justification in the control group (20.83%) and the experimental group (4.17%), even though the result was only marginally significant due to a power issue, ∆ %  = 16.66 % , χ2 = 2.982, df = 1 p = .08.

Colwell, Hunt-Barron, and Reinking (2013) showed that students placed a higher value on completing an assignment than on critically evaluating the online information. However, in the open online environment, there is reliable and unreliable information, and source evaluation should be an integral part of digital literacy. Our study results suggest that epistemic prompting can be provided for students as an external scaffold when they perform MMDR. Further, the automatically generated epistemic reflection report can help students create a coherent mental representation of the multiple multimodal documents involved.

6. Limitations and conclusions
The results of this study are promising and may be used to inform instructional practices. Nevertheless, they need to be interpreted in light of certain limitations. First, some study effects were only marginally significant (e.g., the difference in the percentages of personal justification in the experimental and control groups). Future studies could include a larger sample size to validate the effect. Second, the current study focused on testing the effect of technology-integrated scaffolds on MMDR without imposing instructional interventions on source evaluation. Future studies could incorporate source evaluation training as a more effective intervention strategy for MMDR. Third, we intended to include both video and written texts for MMDR by balancing the transcript length of the video with that of the written texts. However, the fixed duration of the YouTube video required students to spend more time on the video because reading written texts is faster than watching or listening to materials of similar length. Therefore, the effects of different length combinations of digital texts and whether learners change the speed of video viewing (e.g., using the ×1.5, ×2.0 speed function) warrant future research.

Overall, the study findings highlight the need to provide epistemic prompting as strategic scaffolds for students' MMDR and contribute to the development of the MMDR theoretical framework. Our results verified the trend that students preferred using YouTube videos for learning (e.g., Moghavvemi et al., 2018). The study results further revealed that a majority of students relied on the YouTube video as the only external source for MMDR, possibly due to the more persuasive presentation format of the multimodal texts and the retrieval advantage via encoding information from multimodal sources (Shams & Seitz, 2008). Moreover, students rated the YouTube video as more reliable than the newspaper article, pointing to the importance of teaching students how to evaluate the source and quality of YouTube videos for MMDR. Most importantly, we demonstrated that epistemic prompting, along with an automatically generated epistemic reflection report, could help students create a coherent mental representation of information from diverse multimodal sources beyond the effect of personal justification and multiple justification. Therefore, we suggest using the epistemic prompting system with automatically generated reports as strategic scaffolds to facilitate university students' epistemic cognition during their MMDR.

