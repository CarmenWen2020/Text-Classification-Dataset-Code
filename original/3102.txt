This study addresses overload and chaos in MOOC discussion forums by developing a model to categorize threads based on whether or not they are substantially related to course content. A linguistic model was built based on manually coded starting posts in threads from a statistics MOOC, and tested on the second offering of the course, another statistics MOOC, a psychology MOOC, a physiology MOOC, and a test set of reply posts. Results showed that content-related starting posts had distinct linguistic features that appeared unrelated to the domain. The model demonstrated good reliability for all starting posts in statistics and psychology as well as for reply posts (accuracy ranged from 0.80 to 0.85). Reliability for starting posts in physiology was lower but still provided reasonably good predictive ability (accuracy was 0.73). The classification model was useful across all time segments of the courses; the number of views and votes threads received were not helpful.

Previous
Next 
Keywords
Massive open online courses

Social interaction

Discussion forums

Natural language processing

Computer-mediated communication

Content analysis

1. Introduction
Massive open online courses (MOOCs) are online learning environments freely available to anyone with web access (DeBoer, Ho, Stump, & Breslow, 2014). Generally charging no cost for participation and setting no prerequisite requirements, the courses often attract thousands or even hundreds of thousands of registrations. In the past several years, MOOCs have seen dramatic growth in terms of the number of participating institutions, courses offered, and learners involved. By 2015, over 500 higher education institutions offered more than 4,000 MOOCs, and the number of learners reached 35 million, doubling the enrollments from the previous year (Shah, 2015).

These large-scale online environments present exciting and accessible learning opportunities for a broad range of people across the world. In addition, MOOCs provide the opportunity for collecting fine-grained data about learning processes (Reich, 2015) and personalizing learning experiences in unprecedented ways (Paquette, G., et al., 2015, Williams, J. J., et al., 2014). However, MOOC learners are highly diversified in terms of their backgrounds, motivations, learning goals, and behavioral patterns, which poses challenges for traditional pedagogies (Ferguson, R. and Clow, D., 2015, Kizilcec, R. F., et al., 2013). In addition, the extremely low instructor-to-learner ratio creates a need for particular attention to learning support (Hew & Cheung, 2014).

Interaction with other students and the instructor is one powerful way to provide learning support to students. Interaction is also a key element of quality of online learning generally (Trentin, 2000) and MOOCs in particular (Khalil & Ebner, 2013). While many different forms of interaction online are possible theoretically, in practice online discussion forums are currently the de facto primary venue for interaction in MOOCs. Discussion forums are valued by instructors as an important instrument for understanding and intervening in learning activities (Jiang, Z., et al., 2015, Stephens-Martinez, K., et al., 2014), while learners use them for giving and getting help for challenges they encounter in their learning (Breslow, L., et al., 2013, Stump, G. S., et al., 2013).

In order for these activities to happen effectively, instructors and learners need to be able to find the messages relevant to their purposes. However, due to the large number of participants in MOOCs, discussion forums are often plagued by information overload and chaos (Brinton, C.G., et al., 2014, McGuire, R., 2013). On top of this, a large proportion of MOOC posts are not directly related to the course (Brinton et al., 2014). As a result, forums become overwhelming and confusing for users to navigate (Hollands & Tirthali, 2014). This is an exacerbation of the same problems seen in traditional discussion forums for over a decade (Dringus, L. P. and Ellis, T., 2005, Herring, S., 1999, Peters, V. L. and Hewitt, J., 2010), made more challenging by the scale of MOOCs. Such problems can lead to low levels of responsiveness (Huang, Dasgupta, Ghosh, Manning, & Sanders, 2014), which means the desired understanding, intervening, help-giving, and help-getting activities are not occurring as intended (Gütl, Rizzardini, Chang, & Morales, 2014).

Currently, there are very limited means for addressing overload and disorder in MOOC discussion forums. One commonly used strategy is to set sub-forums for different purposes. However, misplaced posts are common in MOOCs (Rossi & Gnawali, 2014), indicating that this strategy has not been very successful. An alternative approach has been to ask learners to tag their posts, making it easier for others to identify different types of postings. But just like with sub-forums, there is no guarantee that learners will tag their posts in accurate and consistent ways. In addition, many MOOC forums allow learners to sort posts by the number of views and votes made by other users. However, such forms of peer recommendations are subject to bias due to positioning effects (Lerman & Hogg, 2014) and the disproportionate influence of early support (the “rich get richer” phenomenon). More importantly, since these features simply indicate the general “popularity” of messages, their value in distinguishing the different types of information posts can contain is questionable (Cui & Wise, 2015). This indicates the need for novel tools that can more effectively assist instructors and learners in navigating the complicated landscape of MOOC discussion forums.

In this study, we address the overload problem in MOOC discussion forums by developing a model to automatically classify threads as substantially related to the course material or not. In the following sections, we first review prior efforts to address the problem of overload in online discussion forums in general and in MOOC discussion forums in particular. We then justify why helping instructors and learners easily distinguish content-related and non-content-related posts is a useful and novel contribution to this problem space. Following this, we describe our methods for operationalizing the concept of “content-relatedness” to manually code thread starting posts from three MOOC discussion forums in statistics, one in psychology, and one in physiology, as well as reply posts from one of the statistics MOOCs. We subsequently report the development and testing of a classification model based on extracted linguistic features to automatically identify content-related threads and content-related replies. Through this work, we aim to build a foundation for tools that can help MOOC instructors and learners locate forum threads addressing the learning of course materials more easily.

2. Literature review
2.1. Research and efforts to address overload in online discussions
Overload, chaos and a resulting lack of responsivity have been documented problems in online discussion forums for over 15 years (Herring, S., 1999, Thomas, M. J. W., 2002). Research has shown that even prior to MOOCs, students often felt overwhelmed by the number of messages present in traditional online discussions (Dringus, L. P. and Ellis, T., 2005, Peters, V. L. and Hewitt, J., 2010) and reported that they found few useful indicators to help them navigate (Wise, Marbouti, Hsiao, & Hausknecht, 2012). As a result, students have often employed non-learning-oriented strategies such as reading posts because they appear at the top or bottom of the forum (Wise et al., 2012) or replying to posts simply because they had been made most recently (Chan, J. C. C., et al., 2009, Hewitt, J., 2003).

While a great deal of research has examined how to structure and script traditional online discussions so that students create desired kinds of posts, there has been relatively less attention to supporting the process of attending to the posts of others (Wise, Speer, Marbouti, & Hsiao, 2013). Those attempts that have been made fall into three categories. The first class of approaches has attempted to integrate student recommendation features (e.g., views and votes) as a tool to support discussion forum navigation. For instance, Makos, Lee, and Zingaro (2014) have found that student recommendations could be used to identify posts of higher cognitive complexity in a traditional online discussion forum. However, this approach does not seem to have translated well to the MOOC context as the relationship between recommendation and post quality is not as strong (Cui & Wise, 2015). In addition, the common inclusion of recommendation technologies in MOOC forums has not prevented problems of overload and chaos. The second class of approaches to supporting discussion forum navigation has used visualization techniques to make discussion structures salient. For instance, Marbouti and Wise (2016) built Starburst, a graphical online discussion interface that visualizes relations between posts and replies as a dynamic hyperbolic tree to support learner engagement with connected chains of ideas. Similarly, Rafaeli and Kent (2015) designed Ligilo, a flexible platform for making and displaying networked connections between posts in a web-like fashion. While these approaches are promising in the context of traditional online discussions, they have thus far been designed to support navigation within collections of related posts, rather than across diverse and potentially unrelated topics. There are also a series of visualization challenges that arise for discussions at scale that need to be addressed if such approaches are to be applied in the context of MOOCs (Marbouti & Wise, 2016). The final category of approaches to supporting traditional discussion forum navigation has attempted to classify posts automatically based on various features. In one example, Lin, Hsieh, and Chuang (2009) used text-mining techniques to classify discussion posts according to post genres, such as announcement, question, and conflict. In another, Kim and Kang (2014) used speech act analysis and natural language processing methods to detect speech patterns and identify unanswered questions/unresolved issues in online discussion. Classification of discussion posts based on linguistic and structural features appears to be a productive and robust approach to addressing the problem of overload in online discussions that can effectively scale to large-scale learning environments such as MOOCs. Further discussion of classification efforts in MOOCs follows below.

2.2. Research and efforts to classify posts in MOOC discussions
Research efforts to address overload in MOOC discussion forums using posts classification technologies have proceeded in two general ways. The first is machine-oriented solutions that use automatic tools to diagnose learner's posts and provide prescribed help or resource as appropriate. These efforts do not seek to support interaction in discussion forums but rather find alternatives to it. For example, Agrawal, Venkatraman, Leonard, and Paepcke (2015) targeted the lack of responsiveness in MOOC forums with an automatic tool that detects confusion in posts and recommends relevant video clips to the questioning learner. Such approaches have value in providing instant help to learners in need of simple answers to straightforward questions; however, learners' needs that are more complex or idiosyncratic may not be sufficiently addressed by preexisting material, and pointing learners to videos they have already viewed could be frustrating, creating a negative affective state for learning (D'Mello & Graesser, 2012). In such cases, responsive human interaction may be a more useful resource for support and learning (Moore, 1989). In addition, by reducing the need and opportunity for human interaction, automated solutions risk exacerbating the lack of community in MOOCs, which has been a prominent issue associated with learning difficulties and dropouts (Khalil & Ebner, 2014).

The second way that post classification has been used to address disorder in MOOC discussion forums is to support human interaction. Such efforts have generally been either instructor-oriented or learner-oriented. Instructor-oriented approaches aim to assist instructors in making efficient and effective interventions in discussion forums. For example, Chandrasekaran, Kan, Tan, and Ragupathi (2015) and Chaturvedi, Goldwasser, and Daumé (2014) both aimed to optimize instructors' intervention decisions with post recommending systems. They built models based on instructors' intervention histories and tested how well the models could identify where instructors had chosen to intervene. While such models can allow instructors to replicate their current intervention patterns more efficiently in the future, this approach overlooks the important question of if the existing patterns are desirable in the first place. Given that typically instructors may not have reviewed all existing posts before deciding where to intervene and that subjectiveness and arbitrariness are common in these decisions (Chandrasekaran et al., 2015), models based on prior intervention history alone may be insufficient (or even harmful) in meeting the goal of identifying posts in which intervention should occur.

Taking a different approach to support instructor intervention, Rossi and Gnawali (2014) aimed to use existing discussion forum structures to inform instructors' reading decisions. They assumed that threads in the same sub-forums generally involve the same types of interactions and thus have common features that can help identify misplaced ones. Using the sub-forum titles (e.g., Lectures, Assignments, and Meetups) as labels, they extracted five types of language-independent thread features: thread structure (e.g., length and breadth), underlying social network (e.g., number and density of users within a thread), popularity (e.g., number of views and votes), temporal dynamics (e.g., message rate), and content (e.g., quantity of text and hyperlinks). Using this feature set, the researchers built a model to identify misplaced threads in each sub-forum. Their results showed that these non-linguistic features could be useful for identifying “small talk” threads (the majority of which were found in Meetups) but had limited utility in differentiating between other post categories. Finally, Jiang et al. (2015) addressed instructors' intervention decisions from a social network perspective. Instead of classifying and filtering posts based on their content, they aimed at identifying a small group of prominent learners to amplify the effect of (an inherently limited number of) instructor interventions. They modeled MOOC forums as a social network and developed an algorithm to identify the most influential learners. The underlying assumption was that by making responses to these learners' posts, the effects of the instructor's intervention would be disseminated to a large number of other learners (though this logic was not tested in that study). If effective, this approach has the potential to broadcast instructors' influence most widely with the minimum intervention cost; however, it overlooks the specific learning needs of less prominent or networked learners. Additionally, the most influential learners may not be the ones who need the instructor's help the most.

In contrast to instructor-oriented approaches which have looked for general, high-impact ways to intervene, learner-oriented approaches have focused more on addressing specific learners' needs and promoting personalized interactions. For example, Yang, Piergallini, Howley, and Rosé (2014) aimed at assisting learners' reading choices and developed a model that recommends discussion threads that match learners' interests as reflected by their previous activities. Similarly, Yang, Adamson, and Rosé (2014) built a question recommendation system that analyzes learners' intellectual and behavioral characteristics and recommends people to answer questions considering relevance to their interests, level of intellectual challenge, and anticipated workload. These approaches provide learners with personalized interactions and make effective use of learners' participation efforts in discussion forums; they are thus one productive solution path to pursue. However, sorting and recommending posts based on historical learning behaviors create a self-reinforcing narrowing of the field of vision that may not sufficiently reflect learners' evolving interests and needs and does not support them in exploring for diversified learning opportunities. Thus modeling based only on prior activity and interests may not be desirable as a sole solution strategy. Another approach that can help address learners' needs is topic-based post classification, which could help learners to browse a more organized set of posts for ones that may interest or benefit them. Topic-based classification has been an objective of several prior research efforts; however, the scope of the topics has not been clearly or consistently defined. Brinton et al. (2014) categorized posts as “course-related” (including course-specific discussions and course logistics) versus “small talk” (for social purposes). They built a model based on topical features of these two categories to classify and rank posts according to their relevance to the course. While effective for its given purpose, their categorization did not distinguish between discussions related to the learning of course material and those about logistical and technical issues, which are substantially different. Such a distinction was made in Stump et al.'s (2013) framework for post topics in MOOC forums, but this initial study did not go beyond defining categories to create models to identify such posts. In summary, principled identification of learning-content-related MOOC posts remains a promising, but as yet not fully realized, line of research.

3. The current study
In the current research, we extend the work of these prior studies to address information overload in MOOC discussion forums by providing a clear and justifiable way to categorize posts as content-related or not and build a classification model based on this categorization. Our approach is designed to address the needs of both instructors and learners in finding relevant posts by focusing on the identification of those which are substantively related to the content of the course material. While there are certainly other reasons that students and instructors may come to a discussion forum (for example to report or find out about technical/logistic problems or make social connections), we argue that communication about course content-related issues is an important function of MOOC discussion forums and where the greatest direct learning-related activity takes place. In addition, it is a particular kind of interaction, different in character from technical and social exchanges.

From the instructor's perspective, identification of content-related posts can be helpful in directing their attention and domain expertise to where it can be of most value. Other posts, such as those which have social purposes, may not be directed at or need instructor intervention (Rossi & Gnawali, 2014). Moreover, those posts that do require non-peer responses can involve diversified issues, many of which would be best addressed by other members of the MOOC instructional team such as TA and technical staff (Chandrasekaran et al., 2015). Additionally, even if an instructor chooses to engage with both content- and non-content-related posts, it may be most efficient to do in batches, focusing their cognitive efforts on similar tasks at a time.

From the learner's perspective, identifying posts based on whether they address questions about the course material is also desirable. While MOOC learners are diversified in learning goals, engagement patterns, and workload commitment, for the majority of students who do more than just enroll, the purpose in taking a MOOC is learning-related, with logistic and social concerns serving secondary supporting roles (Kizilcec et al., 2013). Thus, allowing learners to find the posts that address course-content-related issues can support their learning. Importantly, by making the category all course-content-related posts, rather than those on specific topics (as might be achieved with a search functionality), our approach allows learners to benefit from valuable peer questions that they might not have thought to ask themselves. From the question-asking side, this approach also helps threads that substantially involve the course material get more attention and potential responses. In this way, a content-related identification tool can help foster learning interactions in the discussion forums.

3.1. Study framing
In this work, we define content-related starting posts as those that seek/provide help, information, or resources directly related to the course subject. This includes posts that ask or answer subject-related questions, state subject-related ideas, and those that share/comment on external resources related to course subject (see Table 1). Posts that do not do any of these things are labeled as non-content-related; in the future, they could be divided into subcategories such as socializing, technical problems, logistical questions, etc. This binary classification goes beyond the work of Brinton et al. (2014) in classifying MOOC discussion forum posts based on topical features, and builds on the work of Stump et al. (2013) in distinguishing between discussions related to learning of course materials and those related to logistical and technical topics to provide a refined criterion for distinguishing forum activities substantially related to learning of course material from other activities. To our knowledge, no previous work has focused in this way on identifying content-related discussion threads to facilitate human interaction in MOOCs.


Table 1. Illustrative examples of content-related and non-content-related posts.

Content-related starting post examples	Non-content-related starting post examples
Could anyone provide an explanation of why the mean is not a measure of the spread of data?	Thank you for a great course! When does the final exam occur and when will I have the certificate?
What is the difference between unadjusted and adjusted risk ratio?	Does the video require some specific browser to open or tool to download?
The content of discussions in forums can be analyzed at various levels (e.g., thread, post, sentence, idea). For the purposes of this study, the discussion thread as represented by the starting post is the most useful unit of analysis for building a model for three reasons. First, MOOC discussions are presented to learners as threaded conversations, thus the thread is the initial level at which learners navigate and make decision about what to read. Specifically, learners often use the thread starting post to determine whether or not they are interested to dig deeper into the replies. Second, although a discussion thread may change direction as new participants join (Stump et al., 2013), the starting post reflects the primary intention of the thread initiator and generally largely scopes the content of the subsequent replies. To verify this proposition for the current data set, an analysis was conducted to compare the content-relatedness of replies to their corresponding starting post. A total of 102 starting posts (53 content-related, 49 non-content-related) were randomly selected from StatMed’14 dataset (see Section 4.1) and 202 reply posts in these threads were double coded using the same coding scheme, procedures, and reliability checks as for the main study (see Section 4.3). Of the 104 replies to the content-related starting posts, 51 (49%) were also content-related. In contrast, of the 98 replies to the non-content-related starting posts, only 15 (15%) were content-related. The distinct difference in the proportion of content- and non-content-related replies to the two different types of starting posts suggests that the starting post is a reasonably good indicator of thread composition. Third, when building a classification model, it is important to get as broad of a set of relevant features as possible to help the model generalize beyond the training set. Given that coding resources are limited, a decision needed to be made as to whether the training set should consist of starting posts from a large number of threads or starting posts and replies from a smaller group of threads. Given that starting posts had a higher proportion of content-related messages than the threads overall (including replies) and that drawing posts from a larger number of threads would potentially include discussion of more different topics, it made sense to focus on starting posts for feature extraction and model training. To address the possible limitations of this approach, how well the model works on reply posts was also flagged for investigation.

This study attempts to classify thread starting posts as content-related or not based primarily on linguistic features. This is done both because structural features (such as the sub-forum theme and number of views, votes, and replies) have not been found to be consistently good indicators of thread topic (Cui and Wise, 2015, Rossi, L.A. and Gnawali, O., 2014) and because an examination of the language used to communicate ideas in a post is a direct way to probe what the message is about. Specifically, we take a supervised modeling approach in which features commonly used in content-related and non-content-related posts are extracted based on initial manual post coding. We intentionally mine the posts for students' actual language use rather than using pre-defined keywords extracted from course materials because our goal is to build a resilient model that works across courses without having to define key words every time the model is used on a new course. Nonetheless, generalizability is still a potential limitation of any language-based approach. Thus, this study was specifically designed to test the extent to which a linguistic classification model can generalize beyond its initial context of generation. To do so, we used a tiered strategy in which a model was generated and then tested on successively more distant course contexts. Given the data available for the project, the courses used were a statistics course, a second offering of the same statistics course, a different statistics course, a psychology course, and a physiology course.

Finally, we also note that the proportion of content- and non-content-related posts in discussion forums may vary over time due to changes in learners' interests and needs as a MOOC proceeds (Brinton et al., 2014). For example, when a course just begins, learners may generate many posts to initiate study groups and ask for software-related technical assistance; when the course is about to conclude, learners may flock in to express gratitude and ask about credential issuance. Variation in proportions of topics may affect the performance of a linguistic model. For this reason, an additional important concern for the applicability of a model generated on completed course data to a live MOOC situation that was investigated in this study was how well the model performed on data from different time segments in each of the courses.

3.2. Research questions
The overarching goal of this study was to investigate the possibilities for using natural language processing and machine learning techniques to distinguish content-related and non-content-related starting posts in MOOC discussion forums. This was done through answering six specific research questions (see Table 2).


Table 2. Summary of research questions.

RQ 1	Do starting posts of content-related threads in a statistics MOOC discussion forum have linguistic features that distinguish them from starting posts of non-content-related threads?
RQ 2	Can linguistic features be used to create a model that reliably identifies starting posts of content-related threads in a statistics MOOC discussion forum?
RQ 3	Are the number of views and votes useful in predicting content-related starting posts?
RQ 4	Does the model generalize to another offering of (a) the same statistics MOOC; (b) a different statistics MOOC; and (c) MOOCs on other topics?
RQ 5	How well does the model identify starting posts of content-related threads from particular time segments of a course?
RQ 6	How well does the model identify content-related replies?
The first piece of the logic chain is establishing whether content-related and non-content-related starting posts use language differently and thus have distinct linguistic features that can be used for classification (RQ1). If distinctive linguistic features are found, it may then be possible to use these to build a classifier that detects whether or not a starting post is content-related (RQ2). Apart from the linguistic features of starting posts, peer recommendations such as views and votes are potential indicators (or distractors) in the identification of content-related threads (RQ3). If a reliable classifier is built, the next question is to which degree it is useful on data generated in similar situations. The most similar situation in the case of MOOCs would be a new group of students taking the same course (RQ4a), followed by a new group of students taking a different course on the same general topic (RQ4b), and finally courses on increasingly distal topics (RQ4c). If the model is useful for classification across different courses generally, it is important to assess how well it performs on data from different time segments in each course (RQ5). Finally, it is important to assess how well the model built based on thread starting posts performs on reply posts (RQ6).

4. Methods
4.1. Data sources
This study was conducted on data from five completed MOOCs offered between 2012 and 2014 (see Table 3). The initial examination of linguistic features and modeling efforts were conducted on data from an offering of Statistics in Medicine (StatMed’13). The generalizability of this model was tested on data from a later offering of the same course (StatMed’14), a different statistics course Statistical Learning (StatLearn), a psychology course Introduction to Psychology as a Science (PSY), and a physiology course Your Body in the World (YBW). Introduction to Psychology as a Science was offered on Coursera while the other four were all offered on Stanford's open-source platform Lagunita (originally called Stanford OpenEdX). The usefulness of the number of views and votes was tested on all five courses; time segments could be evaluated only for the external test sets StatMed’14, StatLearn, PSY, and YBW. The model's performance on reply posts was tested on reply posts from StatMed’14.


Table 3. Data sources and usage.

Course name	Platform	Domain	Usage in research
Statistics in Medicine (StatMed’13)	Lagunita	Statistics	Modeling and cross-validation
Statistics in Medicine (StatMed’14)	Lagunita	Statistics	Cross-offering generalizability
Statistical Learning (StatLearn)	Lagunita	Statistics	Cross-course generalizability
Intro to Psychology as a Science (PSY)	Coursera	Psychology	Cross-domain generalizability (near)
Your Body in the World (YBW)	Lagunita	Physiology	Cross-domain generalizability (far)
4.1.1. Course contexts
Statistics in Medicine (StatMed) is a 9-week introductory course on probability and statistics with a special focus on medical contexts. There were no prerequisites for taking the course. Course materials included lecture videos and optional readings. Assessment consisted of quizzes, homework assignments, and a final exam. The course provided a discussion forum for interaction in nine topic areas (see Fig. 1.a).

Fig. 1
Download : Download high-res image (301KB)
Download : Download full-size image
Fig. 1. Forum structures for (a) StatMed, (b) StatLearn, (c) PSY, and (d) YBW.

*A duplicate sub-forum with the same name specific to a video lecture on the topic was also available.

Statistical Learning (StatLearn) is a 9-week introductory course on supervised learning with a focus on regression and classification methods. The prerequisites included introductory courses in statistics, linear algebra, and computing. Course materials included lecture videos and readings. Assessment was based on the completion of quizzes. The course provided a discussion forum for interaction in six topic areas (see Fig. 1.b).

Introduction to Psychology as a Science (PSY) is a 12-week introductory psychology course without any prerequisites. Course materials included lecture videos and readings from an Open Learning Initiative (OLI) online textbook. Assessment consisted of weekly timed multiple-choice quizzes and a final exam. The course provided a discussion forum for interaction in six topic areas (see Fig. 1c).

Your Body in the World (YBW) is a 9-week introductory environmental physiology course without any prerequisites. Course materials included lecture videos and embedded questions and quizzes. Assessment consisted of weekly timed multiple-choice quizzes and a final exam. The course provided a discussion forum for interaction in fifteen topic areas (see Fig. 1d).

4.1.2. The datasets
Data from the Lagunita courses were obtained as part of the MOOCPosts Dataset (datastage.stanford.edu/StanfordMoocPosts). The dataset consists of a selection of randomly chosen forum posts from a variety of MOOC courses. The subset of the data pertaining to StatMed’13, StatMed’14, StatLearn, and YBW was used in this study. Forum information provided in the dataset included the following: thread id, post id, post position in thread (starting post or reply post), post text, post creation date and time, number of times post was viewed, and number of votes post received. Thread titles were not included in the dataset. Data from the PSY course had been obtained and coded as part of a previous study and included all posts from two sub-forums General Discussion and Q&A from the course. Forum information provided in the dataset included the following: thread id, post id, post position in thread, post title and text, post creation data and time, number of times the thread was viewed, and net number of votes (positive minus negative) the thread received. The total number of threads and posts obtained for each course are shown in Table 4.


Table 4. Number of threads and posts in the provided datasets.

Course name	No. of threads	No. of posts (starting posts and replies)
StatMed’13	844	3320
StatMed’14	310	1218
StatLearn	626	3030
PSY	438	2307
YBW	825	2467
4.2. Data preparation
The entire set of 844 starting posts for StatMed’13 was targeted for coding in order to have sufficient data to train the model. Smaller sets of starting posts from StatMed’14 (all 310 starting posts), StatLearn (300 randomly selected starting posts), and YBW (300 randomly selected starting posts) were selected to serve as external test sets. The entire PSY corpus (438 starting posts) had been previously coded for content-relatedness as part of a prior study and was also used as an external test set. During manual coding of the starting posts, reply posts from each data set were used for contextual information when necessary. In addition, a selection of reply posts from StatMed’14 (from the 908 available) were targeted for coding. First, 102 threads (containing 202 replies) were randomly selected for verifying the alignment between starting post and replies (see Section 3.1). Second, an additional 200 reply posts from throughout the available corpus of replies were randomly selected to increase the reply test set.

In cleaning the data, seven duplicated posts and two posts containing foreign language were removed. An additional seven posts were removed during the coding process because it was not possible to make a coding decision without the (missing) thread titles. After all post removals, 99% of data remained in the analysis, with the number of starting posts in StatMed’13, StatMed’14, StatLearn, PSY, and YBW datasets included being 837, 304, 298, 438, and 299, respectively. The total number of posts in the reply post dataset was 402.

4.3. Coding
Each post was coded by two researchers as relating substantively to the course material or not according to the definition set out in Section 3.1. A coding guide with detailed category descriptions and examples was developed through four rounds of iterative coding and coding guide revisions. To aid in the coding, three common types of posts in each category were identified (see Fig. 2). These types are not exhaustive of the kinds of content and non-content posts in the data set but were helpful in quickly identifying and coding frequently occurring post types. They also provided useful language for the coders' discussion during reconciliation.

Fig. 2
Download : Download high-res image (861KB)
Download : Download full-size image
Fig. 2. Common types of content- and non-content-related posts.

A rule of leniency towards the content-related category was adopted for borderline cases so as to maximally capture content-related linguistic features. Coder training was conducted on data not included in the study in cycles of 25 posts until reliability as indexed by Krippendorff's alpha was stable at an acceptable level (α > 0.70). Study data were then coded in subsets of 100 to 170 posts for all courses. All differences were discussed and reconciled before the coders proceeded to the next subset. Coding reliability was good for starting posts from all courses: StatMed’13 (α = 0.77), StatMed’14 (α = 0.82), StatLearn (α = 0.84), YBW (α = 0.75) and for the reply posts from StatMed’14: 202 same-thread posts (α = 0.79); 200 randomly selected posts (α = 0.80). The prior coding of the PSY data according to the same rubric also had good reliability (α = 0.82).

4.4. Feature extraction and modeling
Lightside Researcher's Workbench v2.3.1 was used to perform feature extraction on coded starting posts in StatMed’13, using the basic bag-of-words feature set and a rare threshold of 5. Unigrams and bigrams alone were most useful for characterizing and modeling posts. Stopwords were not removed from the feature lists because they were found to be helpful for modeling. This follows a general trend in the information retrieval literature of including stopwords in feature sets (Manning, Raghavan, & Schütze, 2008). A total of 2410 features were extracted. After Arabic numbers, symbols, and features substantially incorporated by other features (e.g., “ ’m ” is incorporated by “I_’m”) were removed, 2236 features remained.

All 2236 features were then used to train a binary L2 regularized logistic regression classification model (thread starting post as content-related or not). The model was first evaluated by ten-fold cross-validation, and then on independent test sets from StatMed’14, StatLearn, PSY, and YBW. Additional modeling was conducted to test the usefulness of the number of views and votes starting posts received as additional features for classification. The model's ability to classify starting posts from different time segments in the course was tested by dividing each of the test datasets for StatMed’14, StatLearn, PSY, and YBW into three equal-sized subsets according to the post's time of creation. Finally, the model was tested on the reply post dataset from StatMed’14.

5. Results
Content-related and non-content-related starting posts were relatively equal in proportion across StatMed’13, StatMed’14, and StatLearn (47%, 54%, and 51% content-related starting posts, respectively). The proportion of content-related posts was somewhat lower in YBW (40%) and lower still in PSY (28%). In the 402 reply posts from StatMed’14, the proportion of content-related posts was 34%.

5.1. Research question 1: Linguistic features of thread starting posts
Stark differences were found between the top 30 features (ranked by kappa) of content- and non-content-related starting posts in StatMed’13. At a basic level, the lists were composed of completely distinct terms. To probe this distinction further, features were organized into categories based on examination of their uses in the text (see Table 5). The vast majority (85%) of the features across both categories did not appear to be related to the course's subject domain. Many features of content-related starting posts were associated with the process of learning (e.g., “understand” as in “I don't understand why the y axis title is the proportion not published.”), involved question words (e.g., “which” as in “Which one is equal to ‘positive’ in a disease case?”), or were terms describing relationships between ideas (e.g., “between” as in “The correlation coefficient ranges again between negative 1 and positive 1?”). For content-related starting posts, several linguistic features related to the course domain were also found. None of the features of non-content-related starting posts appeared to be related to the course domain. Rather they were terms related to course tasks (e.g., “homework” as in “I cannot figure out how to submit my homework.”), resources (e.g., “videos” as in “I cannot download any of [the] videos for unit 4.”), and those that referred to the course itself (e.g., “this_course” as in “This course is quite advance[d] among other MOOCs on statistics.”). In addition, several terms referring to effort/action (e.g., “made”), quality/quantity (e.g., “all”), appreciation (e.g., “great”), and pronouns (e.g., “my”) were found for non-content-related posts. Existence /condition (see Table 5) was the only feature category that did not show stark difference between content- and non-content-related features.


Table 5. Top 30 features of content-related and non-content-related starting posts organized by category.

Category	Content-related	Non-content-related
Course subject	value, mean*, calculate, probability, p, difference*, standard, data, test	
Learning process	understand, example, mean*, difference*, question*	
Question words	how*, what*, which*, why*, does*, is*, are*	was*
Connectors	of, of_the, in, in_the, about, between, that, then, if, which,* why*, how*, what*	on
Existence/condition	is*, are*, does*, not	was*, had*, have*
Course tasks and platform resources	question*	final, homework, submit, the_final, answers, quizzes, exam, work*, download, videos, course, the_course, this_course
Pronouns	we	my, I_have, but_I, I_had, your, all*
Quality/quantity		again, all*, time*
Effort/action		work*, had*,have*, made
Appreciation		great, thank_you, thank
⁎
An asterisk indicates a word that appears in more than one category due to different uses.

Stark differences between content- and non-content-related posts were also observed for top features within the same categories. The only content-related feature in the course task category refers to a very specific form of course task (i.e., “question” as in “What does y mean in question set 3?”) while the six non-content-related features refer to more general tasks (e.g., “homework” as in “Is using excel considered cheating on the homework?”). Similarly, the category of pronouns was dominated by non-content-related features containing a variety of first-person singular references (e. g., “I” as in “I have time off only during weekends.”) while the only content-related pronoun feature was the first-person plural pronoun “we” (as in “Can we apply this [statistical] test if group sizes are different?”). Collectively, these findings indicate content-related starting posts in StatMed’13 have distinguishing linguistic features from non-content-related ones, and both sets of features do not appear to be strongly related to the course domain. Importantly, these patterns are stable across the dataset, even when the relationship of the feature to the classification is not intuitively obvious (as in the case of the first-person pronouns). Patterns in linguistic features for content-related and non-content-related posts are explored further in Section 5.7.

5.2. Research question 2: Identifying content-related threads
The model created showed reasonably good reliability in identifying content-related starting posts in StatMed’13 (accuracy = 0.80, kappa = 0.61). Recall was 79% and precision was also 79%. These results show that, at a basic level, linguistic modeling can reliably identify starting posts of content-related threads in a statistics MOOC discussion forum.

5.3. Research question 3: Predictive value of the number of views and votes
Adding the number of views and votes each starting post received as additional features to the base model did not produce substantial improvement (see Table 6). In addition, models created using only these features produced very poor classification results (all kappa < 0.13). These results indicate the number of views and votes were not useful in this context for identifying content-related starting posts.


Table 6. Reliability statistics of the base model and with additional features for StatMed’13 (cross-validation).

Base model	+ No. of views	+ No. of votes
Accuracy	0.80	0.80	0.80
Kappa	0.61	0.60	0.61
Recall	0.79	0.79	0.78
Precision	0.79	0.79	0.80
5.4. Research question 4: Testing the Model's cross-course generalizability
To examine the generalizability of the model, it was first tested on the starting posts from a second offering of the same course, StatMed’14. Results achieved comparable reliability to the initial dataset (accuracy = 0.81, kappa = 0.62). Recall was 85% and precision was 81%. To find out how well the model would perform on a different course on a similar subject, the model was tested on the coded starting posts from StatLearn. The results were consistent with those of the previous trials (accuracy = 0.80, kappa = 0.60). Recall was 90% and precision was 76%. These results show that the linguistic model has reasonably good generalizability within the domain of statistics.

To examine how well the model worked for posts from courses outside of statistics, it was tested on the coded starting posts from PSY and YBW. The results from PSY showed a decrease in reliability from that found for the statistics courses, though the overall accuracy was still relatively high (accuracy = 0.80, kappa = 0.52). Recall was 72% and precision was 62%. The results from YBW showed a further decrease in reliability (accuracy = 0.73, kappa = 0.42). Recall was 60% and precision was 68%.

The overall trends in accuracy, kappa, recall, and precision across the increasingly distant course contexts are shown in Fig. 3. Although the model achieved reasonable cross-domain generalizability, there was a drop in performance for the more distal contexts. Possible reasons, including differences in the percentage of content-related posts in the datasets, will be presented in the discussion section. Similar to StatMed’13, adding the number of thread views and votes to the other course models did not improve their reliability (see Appendix A).

Fig, 3
Download : Download high-res image (225KB)
Download : Download full-size image
Fig, 3. (a). Cross-domain trend for accuracy and kappa.

(b). Cross-domain trend for recall and precision.

5.5. Research question 5: Testing the model on different time segments in a course
The proportion of content-related starting posts in different time segments varied in StatMed’14, StatLearn, PSY, and YBW (see Fig. 4). In StatLearn and PSY, the distribution pattern was similar to that anticipated for the flow of class discussion (see Section 3.1): a higher concentration of content-related starting posts in the middle of the course with lower concentrations at the start and end. In StatMed’14, however, the proportion of content-related starting posts increased continually over the time segments. In YBW, the proportion of content-related starting posts was the same in the first two time segments, but lower in the final segment of the course.

Fig. 4
Download : Download high-res image (298KB)
Download : Download full-size image
Fig. 4. Results of StatMed’13 model on data from different time segments in (a) StatMed’14 (b) StatLearn (c) PSY and (d) YBW.

To test how well the model classified posts from different time segments in the courses, it was applied separately to each of three temporal subsets in StatMed’14, StatLearn, PSY, and YBW (see Fig. 4 and Appendix B). The model achieved relatively consistent classification results across time segments in all courses except StatLearn. The best consistency was observed within PSY and YBW. In PSY, accuracy of the model on the three different time segments ranged between 0.78 and 0.81 (very similar to the overall accuracy of 0.80), while in YBW, accuracy ranged from 0.69 to 0.75 (with an overall accuracy of 0.73); results for kappa were similar. Somewhat more fluctuation in reliability between time segments was seen in StatMed’14, where accuracy ranged between 0.76 and 0.84 (overall accuracy = 0.81) and StatLearn, where accuracy ranged from 0.75 to 0.88 (overall accuracy = 0.80).

Looking at specific time segments across courses, overall reliability (as indexed by kappa) was lowest for the middle time segments in both StatMed’14 and StatLearn. For StatMed’14, this seemed to be related to a reduced level of recall (see Appendix B). For StatLearn, both recall and precision remained high for the middle time segment, thus the depression in kappa could be due to the greater proportion of actual content-related posts in the subset. This exemplifies the critique that has been levied against the kappa statistic as being overly sensitive to the base rate proportions, resulting in a low value even when the actual ratings themselves have high levels of accuracy (Feinstein & Cicchetti, 1990). The first time segment of StatLearn also showed a substantially lower level of precision than both other time segments (0.63 compared with 0.78 and 0.84). This may be related to the low level of content-related posts in this segment. Conversely, the middle time segment for PSY, which had the highest percentage of content-related posts for this course (38% vs 23–24%), showed higher precision than the other two time segments. The middle time segment of YBW also had higher precision than the other two time segments for this course, though the percentage of content-related posts in the segment was almost the same as in the first segment.

5.6. Research question 6: Testing the model on reply posts
To examine how well the model (built based on starting posts) identifies content-related replies, it was tested on the reply post dataset from StatMed’14. Results showed reasonably good reliability (accuracy = 0.85, kappa = 0.68). Recall was 88% and precision was 74%. These results show that the classification model can also be reliably used to identify content-related replies.

5.7. Exploratory examination of cross-course similarities in linguistic patterns
An exploratory follow-up examination of the linguistic features of content-related and non-content-related starting posts in all five courses was conducted to investigate possible reasons why the model did and did not transfer well across courses as well as to identify future avenues for model improvement. Lightside Researcher's Workbench v2.3.1 was used to extract the top 30 unigram and bigram features (ranked by kappa) from coded content-related and non-content-related starting posts in StatMed’14, StatLearn, PSY, and YBW in the same way that initial feature extraction had been performed on StatMed’13. Features were then sorted into the same categories used for StatMed’13 (see Table 5); all features were found to fit into the existing category set with the minor adjustment that “appreciation” was expanded to “greetings/appreciation”.

5.7.1. Top linguistic features of content-related and non-content-related starting posts
Initial examination showed that within each of the four test courses, the top 30 content-related and non-content-related features were almost entirely distinct from each other [see Appendix C for the complete list of extracted features; note the one exception that in StatLearn “is” was found as a top content-related feature while “BOL_is” (‘is’ at the beginning of a sentence) was found to be a top non-content-related feature]. The distinct nature of content-related and non-content-related features was mostly maintained across courses as well with only a small number of linguistic elements (“on”, “have”, and “my”) appearing as features in both classes of posts.

5.7.2. Top feature distribution by category
The feature distribution of five courses by category was visualized in a multiple bar graph (see Fig. 5). Inspection confirmed that the majority of top features did not appear to be related to any of the particular course domains. Features that appeared to be related to the course topics represented only 5% to 17% of the top 60 features (30 each for content-related and non-content-related) across all five courses. The visualization also highlights several clear patterns of feature categories across courses. First, the majority of categories are differentially distributed across content-related and non-content-related posts. For instance, in all courses but YBW, terms related to course subject and those referring to the learning process were found as top features only for content-related posts, while terms related to appreciation or greetings appeared as top features only for non-content-related posts. Similar though less extreme patterns were seen for connectors and question words (primarily found as features for content-related posts across all courses) and course task and people-related words (primarily found as features for non-content-related posts). The remaining three categories of existence/condition, quality/quantity, and effort/action did not demonstrate such clear contrasts. Importantly, these patterns of top feature distribution by category across posts classes were consistent for all courses, with the exception of YBW which showed some deviation, specifically in the categories of course subject, learning process and people (see Fig. 5).

Fig. 5
Download : Download high-res image (753KB)
Download : Download full-size image
Fig. 5. Top 30 content-related/non-content-related linguistic features for each course organized by feature category.

5.7.3. Common top features across courses
Although feature distribution by category for different courses demonstrated similar patterns, many of the specific features of different courses that fell into the same category were unique (see Fig. 6). Of the total 300 top features found across the five courses, only 5 were found in all courses; 9 were found in four courses; 12 in three courses, and 46 in two courses. The remaining 228 linguistic elements were each found as a top feature in only a single course. However, there was good separation among the top features—only three terms out of the three hundred were found as top features for content-related posts in one course and non-content-related posts in another. Finally, it is important to note that several features (terms) were used in multiple ways in the posts and thus fit into more than one category. This was the case for 16% (n = 48) of the top 300 features with most of the multiple-use terms fitting into question words as one of the categories (see Appendix C).

Fig. 6
Download : Download high-res image (717KB)
Download : Download full-size image
Fig. 6. Specific features found in more than one course.

6. Discussion
This research investigated the automatic identification of content-related threads as an approach to address problems of information overload and chaos in MOOC discussion forums. Building on prior findings that MOOC discussion forum posts can be categorized as pertaining to the learning of course material or not (Stump et al., 2013) and that classification tools can sort posts for users based on relevance (Brinton et al., 2014), this study extended the line of research by building a classification model to support students and teachers in finding content-related threads. Results showed that content-related starting posts in a statistics MOOC had distinguishing linguistic features from non-content-related ones that could be used reliably for classification. Highly weighted extracted features were mainly terms that did not appear to relate to the subject-matter domain. The model demonstrated good generalizability to a second offering of the same course, a different course on statistics, and a course on psychology; generalizability to a physiology course was lower, but still reasonably good. The model was useful for identifying content-related starting posts across all time segments of the courses, but numbers of views and votes were not helpful for classification. Finally, initial evidence suggests that the model is also useful for identifying content-related replies.

6.1. Notable characteristics of the model
The success of our approach upholds the value of simple modeling. The literature shows that many models built for classification purposes in MOOC discussion forums have been based on complicated feature sets (e.g. Brinton, C.G., et al., 2014, Rossi, L.A. and Gnawali, O., 2014), but our model was built using only unigram and bigram linguistic features and achieved consistently good classification results across four of the five courses studied (accuracy > 0.80, kappa > 0.52, recall > 0.72, precision > 0.62). This indicates that complex feature combinations are not necessarily needed for useful classification results. Second, our findings affirm the indicative power of linguistic features for topic-based post classification in MOOC discussion forums. Notably, in contrast to the customary practice of excluding stopwords for classification modeling of MOOC posts (Agrawal, A., et al., 2015, Chandrasekaran, M. K., et al., 2015), our test on feature combinations found that including stopwords improved the model. In fact, a large proportion of the top features of content- and non-content-related starting posts were commonly used stopwords. Therefore, it is worthwhile to further explore and interpret the usefulness of stopwords in topic-based classification of MOOC discussions. Finally, our finding that the number of views and votes were not helpful for our classification purposes suggests the need to reexamine the role of these features in MOOC discussion forums. Our results echo previous findings (Cui & Wise, 2015) that these features are not good indicators of the content-relatedness of MOOC discussion posts. Similarly, Rossi and Gnawali (2014) found the number of views and votes to be useful in detecting social “small talk” threads, but not for distinguishing between other kinds of posts as we sought to do here. Notably, this is different from findings from more formal education contexts. For example, in a traditional discussion forum in a university class, the contents of messages which were “liked” were of higher cognitive complexity than those messages which were not (Makos et al., 2014). The power of learning context difference is very apparent here: while Makos et al. (2014) studied a small graduate-level course which emphasized online discussions as a site for social knowledge construction, MOOC discussion forums are designed to support more diversified functions including learning support, course management, technical support, feedbacks, and social interaction. Given such varied purposes of use, the indicative power of the number of views and all-purpose votes are questionable. Instead, it may be fruitful to have a (limited) variety of vote types available to users—for example, “critical content issue” and “pressing logistical/technical concern”. Such tailored votes could be useful indicators in future prediction models.

6.2. Generalizability
At the most basic level, the model demonstrated good generalizability to a second offering of the same course. While a rudimentary form of transfer, this is still of value since given the high cost of development, many MOOC instructors plan to offer the same course multiple times (Hollands & Tirthali, 2014). More importantly, the model showed virtually identical performance on a completely independent course from the same domain (although both courses contained math and statistics topics, they were substantially different in many aspects, such as prerequisites, specific content covered, assessment, and instructor). This suggests good promise for within-domain use of the model, an important context for research and application (Reich, 2015). The model's performance dropped somewhat when tested on data from a psychology course (near transfer test) and substantially when tested on data from a physiology course (far transfer test), suggesting the need for further work in developing a model that can perform across content domains.

A primary concern in considering the cross-domain applicability of a model is the domain-dependence of its features. The findings of this study showed that top features of content-related starting posts across all courses were terms related to the process of learning and understanding, question words, terms that connected ideas, and words related to the course domain. In contrast, the top features of non-content-related starting posts were terms related to the course tasks and platform, effort/action, appreciation, and first-person singular pronouns. Notably, the vast majority of top features across both categories were language related to the course-taking process and interpersonal concerns, with less than 20% being words that constituted specific domain-based terminology. While somewhat counterintuitive (c.f. Rossi & Gnawali, 2014), the notion that linguistic features can be robust to domain influence has a logical explanation. Although content-related posts may contain more domain-related language generally, given the diversity of course topics about which questions might be asked, there are few particular domain-specific words that will be used frequently enough to be identified as top features. Thus, it is the language that surrounds the asking of such questions (interrogatives, learning process words, connectors) that are used again and again regardless of the specific focus of a given question.

The predominance of non-domain-specific linguistic features in this work is powerful with respect to the potential for building a classifier with broad application across courses and domains of learning. However, it is important to note that while feature categories for content-related and non-content-related posts were similar across courses, the specific language used was often distinct. The diversity of features within the same category in different courses indicates that the features extracted from a single or a small number of courses may not substantially represent the full extent of language used across different courses. For example, even disregarding specific vocabulary, students in different domains may ask questions differently (compare: “Is it possible to write the algebra function with one definition?” versus “I am wondering about if anyone has thought about how literature can inspire people to violence?”). Approaches to address this issue include using a larger training set for model generation and building a composite model based on features extracted separately from a diverse set of courses.

Another factor to consider related to generalizability is the differences in how (for what purpose) discussion forums are used in different courses and other differences in course design. Gašević, Dawson, Rogers, and Gašević (2016) demonstrate that differences in instructional conditions and the ways technology is used in particular courses have a serious impact on what factors are most useful for making predictions and the overall predictive power available. This was noted earlier in comparing the current findings for MOOC Q&A forums with the different results obtained by Makos et al. (2014) in studying small group discussions used for collective knowledge construction. Following this logic, it is possible that the model's decreased performance in Your Body in the World is due less to a difference of domain (physiology vs psychology and statistics) than one of pedagogy. This course took a more personal approach to the material, focusing on six central topics tied to learners' daily lives, which led to discussions containing many reflections on personal experiences. In contrast, the design of all four other courses focused on mastering an objective body of knowledge. Differences in course pedagogy may also account for variations in the proportion of content-related posts during different time segments of the course. Future work exploring generalizability of MOOC discussion models needs to consider both content and pedagogy dimensions. We note the critical role of obtaining access to data sources from discussions in particular kinds of courses to pursue such work and echo earlier calls from the MOOC research community about the need for policies and infrastructure that support data sharing to allow for robust modeling and testing practices (Reich, 2015).

6.3. Practical and pedagogical implications
The model developed here can be applied to support MOOC learning and pedagogy in multiple ways. Importantly, the use of the model for real-time application (i.e., datasets in progress) is supported by the fact that it tested well across multiple different time segments of the data.

The most straightforward approach for applying the model is to use it to drive a content-based thread-sorting feature in MOOC discussion forum interfaces. This would help instructors and learners to navigate the morass of discussion posts to more efficiently find threads related to the learning of course material. To give a sense of how a filtering feature would change instructors' and learners' experiences using the forums, consider that the overall proportion of content-related posts in StatMed’13 was 47%. If an instructor or learner read all discussion threads indiscriminately, more than half of their effort would be consumed by ones not related to the course content. With the help of a model-based filter, users could narrow their task to less than half the total threads in the forum of which almost 80% would be content-related. In courses where the actual proportion of content-related posts is lower (for example, less than a third of all threads in Intro to Psychology as a Science were content related) use of the model would bring even larger benefits.

Finding content-related threads is important pedagogically because currently MOOC instructors are often overwhelmed by the quantity and diversity of discussion forum activities (Brinton, C.G., et al., 2014, Hollands, F. M. and Tirthali, D., 2014) and have limited means to make use of them to benefit learning. Filtering for content-related threads allows instructors to concentrate on activities directly related to the learning of the course subject and to make informed intervention decisions. In addition, instructors could use the collection of filtered content-related posts as a form of feedback on the course to quickly assess the areas in which learners are struggling and make timely pedagogical adjustments. Moreover, for learners, a filter for content-related threads can help them find learning interaction opportunities more easily, creating an encouraging environment for collaborative and peer-supported learning. Finally, presenting content-related threads as a category for learners to browse creates the opportunity for fortuitous encounters with learning topics or questions that they themselves may not have thought to ask.

In addition, as the model can reliably identify content-related reply posts, it is technically possible to run it at the post level and pull out a highly purified collection of content-related posts (regardless of the thread they are in). This corpus could serve important research interests, such as discourse analysis and topic modeling. However, how well a decontextualized collection of content-related posts would benefit learning is questionable, since the core pedagogical proposition of online discussion relates to the exchange of a series of related comments (Marbouti & Wise, 2016). Therefore, how MOOC learners make use of discussion content and how such a filter should best be used to support learning is a pedagogical question that merits further investigation.

Another use of the model could be to drive a live tagging tool that analyzes the posts being composed and suggests a label for them (content-related or not) that learners could accept or change. This could contribute to post findability as described above, while also providing opportunities for additional model training and improvement. Equally important, the act of having students tag their own posts has potential metacognitive benefits in prompting learners to reflect on what type of interactions they are attempting to initiate (Schellens, Van Keer, De Wever, & Valcke, 2009). This follows a long tradition in the literature on computer-supported collaboration of scaffolding student interactions to support learning (Järvelä, Häkkinen, Arvaja, & Leinonen, 2004). Different from prior systems which required students to label their posts on their own [for example, see the ACT system (Duffy, Dueber, & Hawley, 1998) and CSILE/Knowledge Forum (Scardamalia, M., 2004, Scardamalia, M. and Bereiter, C., 2014)], a model-driven tool could provide students with guidance about what label to use. Another possibility would be to have the model operate in the background, only becoming active when a student assigns a label that varies from one expected by the model. In general, the use of language-driven models to aid collaborative scaffolding presents multiple new exciting avenues for research.

Taken a step further, the model could be used to identify learners who have particular posting patterns, for example, asking some number of content-related questions or asking a large volume of exclusively non-content-related questions. Such patterns may be indicative of particular goal-orientations (e.g., performance, mastery, work-avoidance) which are more or less beneficial for learning in online discussions (Wise et al., 2012). This creates the potential for targeted interventions that could help encourage learners to adjust their engagement patterns. However, whether it is appropriate to adopt such a proactive learning-oriented intervention will be largely dependent on an instructor's perspectives on learning and instruction and the overarching goals of the course.

6.4. Implications for research
The field of learning analytics is currently working to move beyond aggregated approaches to effectively segment distinct kinds of course contexts (Gašević et al., 2016) and students (Huang, J., et al., 2014, Kizilcec, R. F., et al., 2013). We argue that attention to data partitioning needs to also extend to different classes of activities by students within these contexts. Specifically, while prior research on MOOC discussions has looked for patterns in posts made and relations between these and learning outcomes, this work has not yet made the important differentiation between the different purposes discussions are being used for and the consequent different patterns and relations that may be associated with these purposes (Brooks, C., et al., 2014, Cui and Wise, 2015). By classifying discussions based on content-relatedness as a preliminary step before other analyses are conducted, research may reveal different patterns in discussion use (and different effects of these) than when all discussion is considered together indiscriminately. For example, the positive relations found between discussion forum engagement and learning performance (Huang et al., 2014) may be more (or less) prominent if only the engagement related to the course content is used in analysis. Similarly, the finding of a lack of effect of instructors' intervention on learning outcomes (Tomkin & Charlevoix, 2014) may change when only content-related interventions are considered. In considering the complicated relations between the affective dimension and dropout (Wen, Yang, & Rosé, 2014), the same kinds of emotions revealed in content-related and non-content-related posts may be indicative of different attitudes towards the course. Finally, analysis of the relationship between student retention and social network position in discussions (Yang, Sinha, Adamson, & Rosé, 2013) may reveal distinct configurations and associations for content-based ties compared with those formed around social or logistical concerns. This final distinction is particularly important in the context of long-standing research on college retention showing that persistence is associated with membership in academically related but not purely social communities (Kuh, 2002). These findings have been echoed more recently in the specific context of MOOC research showing that the most socially connected students can be differentiated from those who performed best in the course based on their language use (Dowell et al., 2015). Thus, the recognition that different kinds of discussion activity may serve different purposes (and predict different kinds of outcomes) for MOOC learners can allow us to search more precisely for patterns and relationships that provide insight into learning.

6.5. Limitations
This study has limitations with respect to the conceptualization, coding and modeling of “content-relatedness”, and the use of thread starting posts as the primary target for modeling. The choice to focus on content-related posts was made based on common concerns of educators from a learning perspective. Posts on other topics, such as course logistics, may be important to instructors in other contexts and can be targeted accordingly. In defining the scope of “content-relatedness”, there was a challenge in being inclusive enough to maximally meet the practical needs of a variety of forum users and at the same time limit the amount of extraneous posts included. For example, while this study included posts which shared external resources related to the course material as content-related, many instructors and learners may target a narrower conception of content-relatedness due to the limited time and energy they have available. There were also several practical challenges in labeling posts as content- versus non-content-related. The most notable of these occurred for posts in which students appeared to report errors in the course materials—such posts often used similar language to refer to low-level (e.g. typos), high-level (e.g. incorrect concepts), and technical (e.g. course policy) errors. These posts could be correct or incorrect in their assessment of what was potentially in error and varied in tone from assertive to confused to suspicious (regardless of the type and veracity of the error). The difficulties and ambiguities for human raters in coding such posts as content-related or not raise questions about their quality as a benchmark for machine classification. In future work, a separate “error reporting” category would worth exploring. A different kind of challenge arose from the fact that learners often combined content-related and non-content-related concerns in a single post. Since coding criterion was based on the presence of content-related comments, such posts were not hard for manual coding. However, the posts themselves contained mixed linguistic features which could cause confusion for training and testing the model. Post hoc inspection revealed that many of the misclassified posts were ones which contained both content- and non-content-related topics. This is a more difficult problem to address since it is impractical (though perhaps attractive) to oblige students to address only one issue per post. One potential solution could be to manually segment a data corpus of messages at the sentence level for initial model training and subsequently use this model to detect and flag the presence of both content- and non-content-related topics in a post, or to calculate the percentage of content-relatedness for each post. Finally, the model was built based on thread starting posts and tested on this type of post for cross-course and cross-domain generalizability. However, threads that start with a non-content-related post may contain content-related replies (and vice versa). Thus application of the model to starting posts only could miss emergent content-related reply streams or incorrectly identify threads which diverge from an initial content-related focus. As the model demonstrated good reliability when applied to reply posts as well as starting ones, future research can use the model to classify both starting posts and replies to provide more nuanced diagnostics about the content-relatedness of each thread. Such application could also detect individual content-related replies or portions of discussions within threads for particular research purposes.

7. Conclusion
Due to the quantity and disorganization of posts in MOOC discussion forums, learners and instructors often face an overwhelming workload to find the relevant ones to read and reply to (Brinton, C.G., et al., 2014, Hollands, F. M. and Tirthali, D., 2014). Specifically, a central goal of MOOCs is learning, yet a relatively small percentage of posts are substantially related to understanding the subject matter of the course. This study targeted the automatic identification of such content-related posts as a potential tool to help students and instructors navigate MOOC discussion forums more effectively. The resulting model reliably used linguistic features to identify content-related starting posts across multiple courses from statistics and psychology over all time periods within these courses. Generalizability to a physiology course with a somewhat different pedagogical approach was not as good. The model was useful to reliably identify content-related reply posts in the second offering of the medical statistics course. This work makes both theoretical and practical contributions to the pedagogy of large-scale online discussions. Theoretically, the study enhances understanding of the linguistic qualities of discussion posts associated with course content and highlights exciting potential for cross-domains modeling. It also raises doubts about the use of generic views and vote counts as useful indicators of post quality in MOOC discussions. Practically, it is a step towards a tool for using this information to help learners and instructors more effectively contribute and locate content-related posts and thus derive the intended learning benefits these online discussions have the potential to provide. Finally, by presenting a principled and straightforward way to categorize discussion posts according to content-relatedness, the study offers the MOOC research community a tool for attending to the diversified nature of interactions taking place in MOOC online discussion forums and for making a distinction between different types of discussion interactions as a starting point for more targeted research moving forward.

Appendix A. Reliability statistics of the base model and with additional features for all five courses.

Base model	+ No. of views	+ No. of votes
StatMed’13	Accuracy	0.80	0.80	0.80
Kappa	0.61	0.60	0.61
Recall	0.79	0.79	0.78
Precision	0.79	0.79	0.80
StatMed’14	Accuracy	0.81	0.81	0.81
Kappa	0.62	0.61	0.61
Recall	0.85	0.84	0.86
Precision	0.81	0.81	0.80
StatLearn	Accuracy	0.80	0.80	0.82
Kappa	0.60	0.59	0.63
Recall	0.90	0.90	0.90
Precision	0.76	0.75	0.78
PSY	Accuracy	0.80	0.80	0.79
Kappa	0.52	0.53	0.51
Recall	0.72	0.72	0.72
Precision	0.62	0.63	0.61
YBW	Accuracy	0.73	0.72	0.74
Kappa	0.42	0.41	0.44
Recall	0.60	0.60	0.62
Precision	0.68	0.67	0.69
Appendix B. Results of StatMed’13 model on data from different time segments in StatMed’14, StatLearn, PSY, and YBW.

Course	Dataset	% C	A	K	R	P
StatMed’14	1st third (n = 102)	47%	0.83	0.67	0.92	0.77
2nd third (n = 101)	53%	0.76	0.53	0.72	0.81
3rd third (n = 101)	62%	0.84	0.66	0.90	0.85
Whole set (n = 304)	54%	0.81	0.62	0.85	0.81
StatLearn	1st third (n = 100)	38%	0.75	0.50	0.84	0.63
2nd third (n = 99)	65%	0.77	0.46	0.89	0.78
3rd third (n = 99)	52%	0.88	0.76	0.94	0.84
Whole set (n = 298)	51%	0.80	0.60	0.90	0.76
PSY	1st third (n = 146)	23%	0.79	0.46	0.70	0.52
2nd third (n = 146)	38%	0.78	0.54	0.73	0.70
3rd third (n = 146)	24%	0.81	0.52	0.74	0.58
Whole set (n = 438)	28%	0.80	0.52	0.72	0.62
YBW	1st third (n = 100)	42%	0.69	0.36	0.62	0.63
2nd third (n = 100)	43%	0.75	0.48	0.60	0.76
3rd third (n = 99)	34%	0.74	0.40	0.56	0.63
Whole set (n = 299)	40%	0.73	0.42	0.60	0.68
% C = percent of content-related starting posts in data subset, A = accuracy, K = kappa, R = recall, P = precision.

Appendix C. Top 30 features (ranked by Kappa) of content-related and non-content-related starting posts in five courses.

Content-related	Non-content-related
Courses	StatMed’13	StatMed’14	StatLearn	PSY	YBW	StatMed’13	StatMed’14	StatLearn	PSY	YBW
Course subject	value, mean*, calculate, probability, p, difference*, standard, data, test	calculate, data, value	data, linear, model, regression, values, variables	age, mental, research	blood, heat, body, temperature, core, cold					changes, warm*, physiology *, life
Learning process	understand, example, mean*, difference*, question*	question*	understand	question*, answer						learning, see*
Question words	how*, what*, which*, why*, does*, is*, are*	can*, do*, how*, is*, what*, why*	can*, is*, what*, which*	why*, what*, is*, does*	are*, When, would*, why*, is*	was*	will*	BOL_is	will*	
Connectors	of, of_the, in, in_the, about, between, that, then, if, which* why*, how*, what*	about, and, and_the, but, by, from, if, in, of, of_the, or, so, than, that, that_the, the, with, how*, what*, why*	and, as , between, if, in, in_the, of, of_the, on, or, to, with, that, this, which*, what*	but, by, in_the, as, that, or, then, in, of_the, of, and_the, between, than, by_the, to_the, why*, when, what*	of, the, on, in_the, on_the, to_the, or, and, to, that, if, why*	on	like*	after, who		
Existence/condition	is*, are*, does*, not	is*, can*, not, do*	can*, is*	is*, were, does*	have*, would*, is*, are*, get*, ‘s*(is)	was*, had*, have*	is_there, there, will*, like_to, like*		will*	am, no
Quality/quantity			a, best*, only	more, an, correct	more, one*, an	again, all*, time*(period), time*(occasion),	good*, just	lot, much, a_lot		over, good*, well, really
Course tasks (platform/course/course structure/course tasks/resources)	question*	question*		question*		final, homework, submit, the_final, answers, quizzes, exam, work*, download, videos, course, the_course, this_course	accomplishment, course, download, final, homework, lecture, lectures, module, the_lecture, the_videos, video, videos, deducer, answers, my_answer, part, week, problem	course, course_EOL, lectures, quiz, the_course, the_videos, this_course, video, videos, discussion, download, downloaded, R, end_of, access	course, this_course, final, quiz, the_course, exam, videos, grade, certificate, final_exam, coursera, courses, grades, material, quizzes	physiology*, module, course, this_course
Effort/action		do*	use		get*, have*	work*, had*,have*, made			find,	see*
People	we	me, we	my	their	they, people, one*, ‘s* (possessive), ‘s* (us)	my, I_have, but_I, I_had, your, all*	I_just, my, you_for	BOL_I, you, people, can_you	I, my, BOL_I, I_am, everyone, I_just, I_can, I_have,	my, and_I, I, you, my_name, name, name_is, everyone, I_’m
Appreciation/greetings			best*			great, thank_you, thank	good*, thank_you, regards	thank, thank_you, please, great, great_course	thanks, BOL_hi, BOL_hello, hello, hi	fun, great, thank, thanks, enjoyed, fantastic, warm*, good*
