An increasing number of higher education institutions have deployed learning management systems (LMSs) to support learning and teaching processes. Accordingly, data-driven research has been conducted to understand the impact of student participation within these systems on student outcomes. However, most research has focused on small samples or has used variables that are expensive to measure, which limits its generalizability. This article presents a prediction model based on low-cost variables and a sophisticated algorithm, to predict early which students attending large classes (with more than 50 enrollments) who are at risk of failing a course. Therefore, it will enable instructors and educational managers to carry out early interventions to prevent course failure. The results overperform other approaches in terms of accuracy, cost, and generalization. Moreover, LMS usage information improved the model by up to 12.28% in terms of root-mean-square error, enabling better early identification of at-risk students.

Previous
Keywords
Learning management systems

Prediction of academic performance

Educational data mining

Postsecondary education

1. Introduction
The higher education institutions of today make extensive use of digital technology (Selwyn, 2014). At present, the use of database systems to manage student demographic information and academic records is widespread. Moreover, many institutions have deployed learning management systems (LMSs) to enhance teaching practices (Graham, 2006). Currently, it is common for courses in traditional classrooms to be supported by a digital version (Cole & Foster, 2007), which is employed for the delivery of electronic educational resources, support of interactive environments, and management of assessments (Cavus & Zabadi, 2014).

With the rapid growth of digital technology, a large amount of student data has become available (Freitas et al., 2015). This has generated both opportunities and challenges. Opportunities include using data to answer questions about the learning habits of students, the practice of teachers, and the strategic deployment of learning technology (Nunn, Avella, Kanai, & Kebritchi, 2016). The positive outcomes for higher education can already be seen from the practical use of tools such as early warning systems, attention meta-data, recommender systems, and tutoring and learner models (Siemens, 2012). On the other hand, one of the biggest challenges that educational institutions face is how to understand and learn from data. Digital systems pursue specific goals, with a greater focus on administration or on learning. At the same time, they usually support multiple hierarchy levels (i.e. school, course and student) and specific contexts (i.e. a particular student in a particular course and at a particular time), that take place within specific time frames (i.e. data recorded through a semester or year) (Romero & Ventura, 2013). Hence, they are independent and cannot be combined effortlessly. This generates problems regarding the best way to capture, organize and productively use these data.

A research area that has emerged to tackle these aspects of student data is educational data mining (EDM) (R. Baker & Yacef, 2009). This field is concerned with developing, researching, and applying computerized methods to detect patterns in large collections of data, which would otherwise be difficult or impossible to analyze because of the large volumes (Romero, Ventura, Pechenizkiy, & Baker, 2010). One of the main lines of investigation has been the prediction of student performance (West, Heath, & Huijser, 2015). This has been an active and relevant topic because traditional grade-based evaluations are currently the most widespread approach to measuring and monitoring the learning process McKenzie and Schweitzer, 2001, Ransdell, 2001, Arnold and Pistilli, 2012. This line of research has been addressed in EDM from various perspectives: early identification of at-risk students, which allows for timely counseling as well as coaching to increase student success rate and retention Vandamme et al., 2007, Macfadyen and Dawson, 2010, Falkner and Falkner, 2012, Marbouti et al., 2016; student dropout detection and reduction, which is especially popular for online courses, which tend to have high dropout rates Cocea and Weibelzahl, 2006, Lykourentzou et al., 2009; and student clustering, in which they are grouped based on common patterns to predict their performance (Romero, Ventura, Espejo, & Hervás, 2008) or identify similar behavior, such as poor learning resulting from misuse of the tutoring systems (R. S. Baker, Corbett, & Koedinger, 2004).

However, this line of investigation has presented some problems and limitations. Most of the published studies work with small, particular populations from a course, degree, or year Minaei-Bidgoli et al., 2003, Macfadyen and Dawson, 2010, Baradwaj and Pal, 2011, Arnold and Pistilli, 2012, Huang and Fang, 2013, Cerezo et al., 2016. In addition, they are usually applied in situations where a specific learning design is to be tested, which does not necessarily reflect the actual activity of instructors and students who do not have the support of learning design specialists (Grabe & Sigler, 2002). Both characteristics undermine the replication of such studies (Romero & Ventura, 2013). Another limitation is that many of these models are based on internal assessment instruments for each course, such as partial grades, quizzes, and attendance (Marbouti et al., 2016;), or rely on active participation from instructors to take questionnaires McKenzie and Schweitzer, 2001, Vandamme et al., 2007, Kember and Leung, 2009, build collaborative environments (García, Romero, Ventura, & De Castro, 2011), and apply EDM models by themselves, which may be a complex and costly endeavor.

In this paper, we present a student performance prediction model to help with the early identification of at-risk students attending large classes (with more than 50 enrollments) before the end of the semester (towards mid-semester). This will enable instructors to apply educational interventions to prevent students from failing. Our prediction model does not require any extra active effort from either instructors or students (it is, therefore, low cost), as we include readily accessible institution-wide data from digital systems covering 119,366 course enrollments, 21,314 students, and 811 unique courses, collected over a time span of three semesters. We integrated data from two sources. The first is the student administrative information system (DARA) that stores the students' academic and demographic information. The second is the LMS (SAKAI) that records usage information of student actions within the system. Undergraduate courses with available data were taught face-to-face, complemented by an LMS website. This means that courses are delivered face-to-face, with weekly classes over an academic semester. At the same time, there is a LMS website available for teachers to manage content, like supplementary material and presentation slides, publish announcements, post comments in online discussions or manage assessments.

The main research question addressed in this study is: which combination of algorithms and low-cost variables (not requiring active effort from either instructors or students) is the most relevant for predicting student performance at an institutional level? To address this question, we analyzed a set of low-cost variables grouped into three scenarios and used them in three predictive models. The first is a popular approach used to predict student performance: linear regression (Neter, Kutner, Nachtsheim, & Wasserman, 1996). The second is an alternative to linear regressions when they are influential observations: robust linear regression (Fox, 1997). The third consists of a sophisticated machine learning algorithm: random forest (Breiman, 2001), which is well known for its performance and capacity for dealing with noisy data and overfitting (Fernández-Delgado, Cernadas, Barro, & Amorim, 2014). We evaluated these models for the identified variables and also experimented with different student sets to evaluate the generalizability of our approach.

2. Methodology
2.1. Dataset collection
The data were collected from a comprehensive research-oriented university and included information from 21,314 undergraduate students over three semesters from the second term of 2013 (2013-2) to the second term of 2014 (2014-2). Data were pre-processed and filtered, which resulted in an analysis of 119,366 the students' course enrollment records.

The sources of the data were two independent systems: the official university LMS, SAKAI, which produces detailed activity logs for each user, accounting for a total of 27,339,752 raw records; and DARA, which stores demographic information of students at the moment of enrollment as well as their past and current academic status, including grades and courses enrolled in, which accounts for a total of 386,573 raw records. The number of records per semester is detailed in Table 1. As can be seen, there are more DARA records from the first semester than from the second semester; the reason behind this is that the regular admission period is during the first semester. Likewise, SAKAI showed similar behavior; however, when comparing SAKAI records from similar semesters (2013-2 and 2014-2), an increase of 41.11% in the number of records can be observed. The main reason for this trend was a university policy aimed at increasing SAKAI usage, which included training workshops for teachers, so they could learn how to use SAKAI tools, product of the fact that the SAKAI system was introduced at the end of 2012 to replace an old, in-house designed, less versatile LMS.


Table 1. Summary of data sources and number of records.

Records by semester	
Source	Data	2013-2	2014-1	2014-2	Total
Official LMS (SAKAI)	Log of actions performed by students in the system	6,921,388	10,651,283	9,767,081	27,339,752
Academic records (DARA)	Background and demographic students' information as well as past and current academic situation	120,281	145,623	120,669	386,573
2.2. Dataset processing
The goal of this step is to generate a dataset to which algorithms can be readily applied, and subsequently analyze the outputs (Fayyad, Piatetsky-Shapiro, & Smyth, 1996). Therefore, the data from the various sources were combined into a standard database. The main challenges involved fixing data inconsistencies; for instance, some students' grades were not introduced into the database until the following term. The differences in the data formats and codification standards in each database were also addressed. As variables such as CGPA and SAKAI usage are strongly teacher and course dependent, they were transformed using z-score transformation. The final set of predictors included student demographic and academic data, which do not change throughout the academic semester, as well as the students' LMS usage data, which change as students advance through the semester. The predictors used, including their description, source, average, and standard deviation, are listed in Table 2. The details of each step of the dataset processing are reviewed in the following subsections.


Table 2. Source, description and statistics of predictors on each set.

Large Courses	Large Top 20%	Large Top 10%
ID	Source	Description	AVG	STD	AVG	STD	AVG	STD
F_GRAD	DARA	Dependant variable. Student’ final grade given a course	0.68	0.16	0.64	0.15	0.64	0.15
AGE	DARA	Student age based on birth date	21.17	2.34	21.06	2.17	21.06	2.03
GEND	DARA	Gender Male (0) or Female (1)	−	−	−	−	−	−
F_SCHO	DARA	Private (0) or State (1) funding of student high school	−	−	−	−	−	−
HS_CGPA	DARA	High school CGPA score	691	65.02	697	65.94	712	62.55
EX_LEN	DARA	Language score in national university selection exam	681	64.96	676	65.94	688	64.44
EX_MAT	DARA	Maths score in national university selection exam	693	72.28	703	71.86	720	72.94
SCHOOL	DARA	University school of the student	−	−	−	−	−	−
C_DURA	DARA	Official career duration in semesters	9.76	1.48	9.99	1.43	10.20	1.59
CRED_S	DARA	Sum of course credits for that semester	52.83	23.16	51.16	17.95	50.75	16.46
E_SEM	DARA	Number of semesters enrolled in the university	4.85	3.12	4.75	2.99	4.91	2.86
CGPA	DARA	University cumulative grade point average	0.69	0.12	0.68	0.12	0.69	0.12
WEB_READ	LMS	External web resource accessed and downloaded	8.64	5.79	18.37	8.58	20.47	9.09
CONT_READ	LMS	Resource item accessed and downloaded	6.12	6.47	12.83	9.54	12.14	11.05
SYLL_READ	LMS	Syllabus item accessed and downloaded	5.28	5.24	9.76	8.67	12.15	9.73
MSG_READ	LMS	Message read	1.71	2.63	2.92	3.95	3.30	4.42
MSG_NEW	LMS	New message composed and sent	0.15	0.52	0.24	0.74	0.28	0.85
MSG_REPLY	LMS	Message reply sent	0.04	0.26	0.07	0.37	0.09	0.44
MSG_FORW	LMS	Message forwarded	0.01	0.11	0.01	0.14	0.02	0.14
QUIZ_SUB	LMS	Quiz submitted by student	0.17	1.15	0.76	2.53	1.27	3.15
AS_SUB	LMS	Assignment submission read by student	0.25	0.85	0.50	1.25	0.56	1.43
AS_REV	LMS	Assignment revised	0.00	0.03	0.00	0.05	0.00	0.07
SEC_READ	LMS	Section made with lesson builder tool read	0.06	0.84	0.22	1.69	0.32	2.07
FOR_READ	LMS	Forum topic accessed	0.10	0.67	0.22	1.06	0.26	1.11
FOR_RESP	LMS	Forum topic response created	0.10	0.66	0.22	1.05	0.26	1.11
CHAT_NEW	LMS	New chat message	0.00	0.03	0.00	0.06	0.00	0.09
POLL_VOTE	LMS	Poll vote entered	0.00	0.05	0.00	0.00	0.00	0.00
POLL_READ	LMS	View poll results	0.00	0.07	0.00	0.00	0.00	0.00
NEWREAD	LMS	News feed accessed	0.00	0.05	0.00	0.00	0.00	0.00
WIKI_READ	LMS	Wiki section read	0.01	0.15	0.00	0.00	0.00	0.00
WIKI_REV	LMS	Wiki page revised	0.00	0.03	0.00	0.00	0.00	0.00
WIKI_NEW	LMS	New wiki page created	0.00	0.03	0.00	0.00	0.00	0.00
PREF_UPD	LMS	Profile preferences updated	0.00	0.02	0.00	0.04	0.00	0.03
PREF_ADD	LMS	Profile preferences added	0.00	0.03	0.00	0.02	0.00	0.02
PROF_PRIV	LMS	Profile privacy settings changed	0.00	0.02	0.00	0.02	0.00	0.02
PROF_PREF	LMS	Personal Preferences entry created	0.00	0.01	0.00	0.01	0.00	0.01
Large Courses set includes students records, by course, of courses with at least 50 students and at least one LMS record by student on average. Large Top 20% and Large Top 10% are subsets of Large Course sorted by average of students' count LMS records.

2.2.1. Data integration
The process of generating a unified standard dataset required expert technical computational knowledge to guarantee data integrity while changes were made by any member of the research group, as well as task overload (the execution of an algorithm may block some portions of the dataset). First, the data were extracted from both interfaces and encrypted, to guarantee the students' privacy. Second, some values were re-codified into a standard format, compatible with the standard database system, for example, interpreting the date ‘12-12-12’ can be confusing, and the same is true for decimal numbers with different decimal markers, unique identifiers, and null values. Third, the data were checked to detect any inconsistencies in the records. For instance, some records showed changes in grades that had been updated at the beginning of the next semester. In addition, student demographic information was only stored in the latest DARA record and deleted from previous versions.

2.2.2. Data selection
The main purpose of this section is to identify valuable LMS data sources for prediction algorithms. As LMS usage is highly variable across courses, as Park et al., 2016, Gašević et al., 2016 show, this process was performed over aggregated statistics at a course level. The selection process is summarized in Fig. 3 and finishes with three subsets: Large Courses, Large Top 20% and Large Top 10%.

Fig. 3
Download : Download high-res image (255KB)
Download : Download full-size image
Fig. 3. Selection process to identify LMS data of relevant courses.

There is an expected trade-off between the effort required to maintain an LMS, which could pay off when the course enrollment is large, and the use of other simpler options. For instance, an instructor of a specialized course with five students may choose to coordinate with them via email; however, this would be very difficult to manage in a class with 100 enrolled students; in this case, LMSs become essential. To analyze this effect, a course was defined as ‘using an LMS’ when the average raw sum of all LMS activities performed by students was greater than a specific constant (C = 1), which means that each student used the LMS on average at least once. In addition, a course was considered large when it contained more than 50 students (Shaw, 2013), and small when it had fewer than 30 students.

Fig. 1 shows a graph representing the number of courses defined as large (left Y axis) for each academic unit (X axis) and the percentage of large courses identified as ‘ using LMS’ as a black line (right Y axis). The full codification of academic units is available in Appendix A. Only a few academic units have large courses without ‘ using LMS’; Engineering (A01) being the lowest, which is explained by the use of an ad-hoc LMS that replaces the functions of SAKAI; the same is true for Mathematics (A07). It is interesting to note that the LMS usage weighted mean across academic units (by number of courses) in large courses is 95.33%, after removing Engineering and Mathematics. This suggests that LMSs are a valuable tool for large courses, where standardized rules are necessary. In contrast, the situation for small courses is analyzed in Fig. 2. The percentage of small courses using LMS tends to be lower and less stable, and even after removing Engineering and Mathematics, the LMS usage weighted mean was 70.32%, a difference of 25.01% in comparison to large courses.

Fig. 1
Download : Download high-res image (291KB)
Download : Download full-size image
Fig. 1. Number of large courses grouped by academic unit and percentage use of LMS.

Fig. 2
Download : Download high-res image (321KB)
Download : Download full-size image
Fig. 2. Number of small courses grouped by academic unit and percentage use of LMS.

The rest of the analysis only considers large courses which used the LMS, classified into three sets: Large Courses, Large Top 20% and Large Top 10%. The first set comprised large courses in which LMS was used at least once and consists of 119,366 student records; it represents an institution-wide context. The second set comprises data from the top 20% large courses, sorted by the students' average LMS use; therefore, it is a subset of the Large Courses set. It has 19,232 records and represents a transition between the first and final set in terms of homogeneity. The final set includes data from the top 10% of courses, also sorted by average LMS use, therefore it is a subset of Large Top 20%, it has 9925 records and represents a more homogeneous data sample from courses that rely on LMS to manage their teaching and learning process.

2.2.3. Reduction and transformation
The values of the LMS usage variables in the dataset were defined by taking into account the total number of activities performed each semester by each student in each course, and grouping them according to their SAKAI activity type. The studied activities were selected by expert criteria from the literature Laurillard, 2013a, Laurillard, 2013b, Salmon, 2004, González, 2012 in accordance with their potential relevance within the learning process. To prevent feeding the algorithm with off-task behavior (R. S. Baker, 2007), all actions of the same type, performed by the same student, during a 60-s interval were counted as one single action. Despite Kovanović et al. (2015) showing that the length of such an interval can have a significant impact on the results, in this study the combination of different scenarios and algorithms did not show a statistical difference in performance terms for 40, 60, 80 and 90 s.

Table 2 shows the statistics for LMS variables in the dataset. When moving from Large Courses to Large Top 10%, the average number of SAKAI activities increases and the standard deviation decreases. However, even in Large Top 10%, these variables show a large standard deviation; therefore, any prediction model intending to use this information must be capable of dealing with noisy data. In addition, it can be seen in the Large Courses set that activities WEB_READ, CONT_READ, SYLL_READ, and MSG_READ represented more than 95% of the LMS usage data. Such activities denote a passive student attitude, as these are mostly managed by teachers. On the other hand, the use of tools associated with an active attitude such as MSG_NEW, MSG_REPLY, FOR_RESP, FOR_READ, POOL_VOTE, POOL_READ, and WIKI_READ accounted for less than 5% of the data. This shows that instructors tend to use the LMS in a more traditional fashion, such as a content management system (CMS); that is, only for content delivery. This behavior, as Kember, McNaught, Chong, Lam, and Cheng (2010) suggested, could be because a more interactive use of an LMS demands more effort and time from teachers.

The last step of data processing was to transform the CGPA and LMS usage variables. To compare CGPA and LMS usage across different courses, a z-score transformation was applied, because the value of these variables is strongly teacher- and course-dependent. For instance, a high grade in a course where high grades are frequent is less valuable than a high grade in a course where low grades are frequent. At the same time, LMS usage depends on the way instructors employ their course websites. In an a posteriori analysis, other transformations such as min–max, logarithm, and divide by average showed reduced performance when compared with z-score transformation. In addition, final grades were transformed from a local scale (1.0 to 7.0) to the interval 0–1.

A total of 36 predictor variables were identified: 25 from LMS and 11 from DARA. Each predictor's description, source, average and standard deviation are listed in Table 2.

2.3. Prediction models
To predict the students' final grade in each course, three models were compared: linear regression (LR) (Neter et al., 1996), robust linear regression (RLR) (Fox, 1997), and random forest (RF) (Breiman, 2001). RLR is an alternative to traditional LR in cases when there are outliers or influential observers, which are weighted with lower values based on how well they behave. Likewise, RF is an ensemble of unpruned decision trees, where each tree is constructed using a different sample of the data, generated through bootstrapping (with replacement). In addition, to increase the independence of the trees, only a few predictors were used as candidates at each split. In this way, the final prediction is generated by averaging the predictions of individual trees. Fernández-Delgado et al. (2014) claimed that it is the best algorithm family. The goal behind this comparison was to find out the best model given the available predictors. Other possible algorithms included in this selection are neural networks and support vector machines, owing to their good performance in the literature (Shahiri, Husain, et al., 2015). However, they were discarded, because they provide less information on how their predictions are made, their data preparation is longer, and their parameter tune-up process is more complex. The main properties required by these algorithms were:

Outlier Resistant.
The statistical analysis revealed that the data vary greatly. This is because of intrinsic reasons, due to the nature of the variables, as well as extrinsic factors. As it is common for different database systems to be managed by different teams for different goals, actual records may reflect practices that were not disclosed by these teams, which may affect the consistency of the records. For instance, students who suspended their studies and re-enrolled later did not have their enrollment date updated, therefore it was not possible to identify student cases who had suspended some semesters. In addition, in some courses, students caught cheating in a test are penalized with the minimum grade in that test, or even as a final course grade. However, no records of these incidents were available. Such situations might have a negative impact on the prediction models, yet these records are hard to identify and cannot be discarded with certainty.

Generalizability.
Prediction models shall not overfit the training data, so that the predictions will become valid for new data. This property is key to predicting student grades for various courses.

Scalability.
The algorithm should scale up to large datasets, either through a high efficiency or parallel processing, and still learn from all the data and provide predictions for every student in each course within a limited time frame.

2.4. Experiments
As indicated in Section 2.2.2, the data were divided into three sets, Large Top 10%, Large Top 20%, and Large Courses, and tested in the following scenarios.

1.
Fixed data (FD): Includes the students' information recorded at the beginning of the semester as demographic information, as well as the student's academic performance. That is, all the data contained in DARA.

2.
Fixed data plus full-term semester LMS data (F-LMS): Consists of FD plus each student's LMS usage metric until the end of the semester, aggregated by LMS activity.

3.
Fixed data plus midterm semester LMS data (M-LMS): Consists of FD plus each student's LMS usage metric until the third month of the academic term, aggregated by activity. Usually, midterm tests results are available by this time.

The goal of comparing the FD and F-LMS scenarios was to measure the effects of including LMS usage data in the prediction models, whereas the goal of the M-LMS scenario is to exploit this information to predict final grades before students take their final exams, giving instructors a time frame in which they can intervene and prevent student failure.

2.5. Error metrics
A 10-fold cross-validation method was used to obtain error metrics with lower variance (Friedman, Hastie, & Tibshirani, 2009). This consists of splitting each set into 10 folds at random, and using 9 folds to train the model, predicting the remaining 10th fold (which is considered ‘new’ data), and calculating the prediction error metrics. This process was repeated 10 times to make predictions for each fold (by changing the training and prediction folds).

Although a student's final grade is a continuous variable and, so a continuous error metric is better suited for it, from an educational perspective, students at risk of failing are especially interesting when educational interventions are applied. Therefore, generalized versions of classification error metrics were included, to provide a deeper insight into at-risk students. There is a possibility that LMS data could provide more information about at-risk students than about those who are not under such risk.

Selected error metrics suitable for continuous prediction models are listed in Table 3. The most common is the adjusted coefficient of determination 
 
, which indicates the proportion of the variance in the dependent variable that is predicted by the model, adjusted by the number of predictors p and the sample size n. The mean absolute error (MAE) and root-mean-square error (RMSE) are standard error metrics used in statistical models, and there are good arguments for encouraging the use of each metric Willmott and Matsuura, 2005, Chai and Draxler, 2014. MAE is the average absolute difference between estimated and real values, whereas RMSE is the sample standard deviation of the difference between estimated and real values. The average prediction accuracy (APA) indicates how well the model predicts the final grade of all students on average, and the percentage of accurate predictions (PAP) is calculated as the number of accurate predictions divided by the total number of predictions. Following Huang and Fang (2013), a predicted value was identified as ‘correct’ when it was within 90–110% of the actual value.


Table 3. Continuous error metrics for evaluating and comparing the performance of the proposed prediction models.

Name	Formula	Range
 
 
 
 
0–1
Mean Absolute Error (MAE)	
 
1–0
Root Mean Square Error (RMSE)	
 
1–0
Average prediction Accuracy (APA)	
 
 
0–100
Percentage Accurate		
Predictions (PAP)	
 
0–100
 	
Selected metrics suitable for classification problems are shown in Table 4. As the predicted value is continuous, a generalized version proposed by Torgo and Ribeiro (2009) was applied, which is detailed in Appendix B. This is approximately equivalent to predicting two states, approved (A) and reprobated (R), based on whether the students have the minimal grade, using 0.5 as a threshold, to pass the course or not, and calculating the error metrics over such predicted states. The Precision metric indicates how likely the predicted state is to be equal to the actual state. Recall indicates how many results were predicted correctly, given the whole population sharing that state. Finally, the F-score combines Precision and Recall because there is a trend towards a reduction of Precision when Recall increases (the reverse is also true). Thus, the focus will be on this metric: approved (A-Fscore) and reprobated (R-Fscore).


Table 4. Classification error metrics for evaluating and comparing the performance of the proposed prediction models on approved and reprobated students.

Name	Formula	Range
Precision	
 
0–1
Recall	
 
0–1
F-score	
 
0–1
2.6. Variable relevance
Analysis of variable relevance was only performed for the top performer algorithm, which, as presented in Section 3, corresponds to Random Forest. The metric analyzed is the percentage increase in mean squared error, which is calculated as follows. First, a prediction is made on the out-of-bag portion (data that was not used to build the tress) and the mean squared error (MSE) is calculated and recorded as MSE0. Second, the selected variable is replaced by a random sampling without a replacement of itself and a new prediction is made once again and the MSE is calculated and recorded this time as MSE1. Third, the difference between MSE0 and MSE1 is averaged over all generated trees, and normalized by the standard deviation of the differences (Liaw & Wiener, 2002).

3. Results
3.1. Differences across sets and scenarios
As the results listed in Table 5, Table 6, and 7 show, the evaluation metrics worsen as we move from set Large Top 10% (N = 9925) to Large Top 20% (N = 19,232) and Large Courses (N = 119,366). At a 0.05 Wilcoxon Signed Ranked Test significance, 
 
 decreased on average by 0.0142, 0.0084, and 0.0230 for LR, RLR, and RF, respectively, whereas RMSE increased by 0.0037, 0.0031, and 0.0047 on average for LR, RLR, and RF, respectively. Likewise, when moving from Large Top 20% to Large Courses, 
 
 decreased on average by 0.0254, 0.0250, and 0.0482 for LR, RLR and RF, respectively, whereas RMSE increased by 0.0104, 0.0103, and 0.0127, respectively. This occurred due to the addition of records of other courses and students, making the sample more heterogeneous. The only exception was A-Fscore, which showed an increase proportional to the number of approved student course enrollments, which took place because there was a class imbalance: approved students were the larger group, which accounted for 8777 records in Large Top 10% (88.43% of the set); 16,961 records in Large Top 20% (88.19% of the set); and 108,687 records in the Large Courses set (91.05%). Thus, A-Fscore tended to improve as the approved class became more common and provided more information. It is worth noting that the optimized metric is RMSE and applying re-balancing techniques does not improve this metric.


Table 5. Linear regression performance metrics on the proposed scenarios.

Subset	Scenario	R2	MAE	RMSE	APA	PAP	A-Fscore	R-Fscore
Large Top 10%	FD	0.5275	0.0766	0.1021	83.46	53.49	0.9367	0.4505
N=9925	F-LMS	0.5269	0.0765	0.1021	83.57	53.46	0.9367	0.4523
M-LMS	0.5196	0.0766	0.1029	83.50	53.65	0.9369	0.4503
Large Top 20%	FD	0.5101	0.0805	0.1061	82.33	50.78	0.9343	0.4210
N=19,232	F-LMS	0.5108	0.0804	0.1060	82.43	50.82	0.9360	0.4367
M-LMS	0.5105	0.0803	0.1060	82.40	50.96	0.9351	0.4303
Large Courses	FD	0.4846	0.0885	0.1165	81.58	49.36	0.9555	0.4215
N=119,366	F-LMS	0.4852	0.0885	0.1164	81.60	49.32	0.9557	0.4244
M-LMS	0.4854	0.0885	0.1164	81.60	49.36	0.9557	0.4248

Table 6. Robust linear regression performance metrics on the proposed scenarios.

Subset	Scenario	R2	MAE	RMSE	APA	PEP	A-Fscore	R-Fscore
Large Top 10%	FD	0.5248	0.0762	0.1024	83.22	54.03	0.9394	0.4344
N=9925	F-LMS	0.5143	0.0762	0.1035	83.31	54.29	0.9393	0.4374
M-LMS	0.5106	0.0763	0.1039	83.25	54.09	0.9392	0.4341
Large Top 20%	FD	0.5078	0.0801	0.1063	82.05	51.38	0.9372	0.4089
N=19,232	F-LMS	0.5083	0.0801	0.1063	82.14	51.31	0.9379	0.4171
M-LMS	0.5084	0.0800	0.1063	82.12	51.39	0.9373	0.4145
Large Courses	FD	0.4828	0.0882	0.1167	81.31	49.90	0.9566	0.4042
N=119,366	F-LMS	0.4832	0.0883	0.1166	81.31	49.84	0.9568	0.4058
M-LMS	0.4836	0.0883	0.1166	81.32	49.88	0.9567	0.4051

Table 7. Random forest performance metrics on the proposed scenarios.

Subset	Scenario	R2	MAE	RMSE	APA	PEP	A-Fscore	R-Fscore
Large Top 10%	FD	0.6080	0.0681	0.0930	84.86	59.62	0.9419	0.4742
N=9925	F-LMS	0.6726	0.0617	0.0850	86.20	63.85	0.9448	0.5176
M-LMS	0.6736	0.0615	0.0848	86.15	64.03	0.9453	0.5349
Large Top 20%	FD	0.5713	0.0738	0.0993	83.47	55.36	0.9390	0.4633
N=19,232	F-LMS	0.6565	0.0653	0.0888	85.17	61.54	0.9444	0.5308
M-LMS	0.6575	0.0651	0.0887	85.19	61.78	0.9448	0.5432
Large Courses	FD	0.5057	0.0856	0.1141	82.32	51.30	0.9549	0.4392
N=119,366	F-LMS	0.6146	0.0745	0.1007	84.37	57.81	0.9606	0.5142
M-LMS	0.6203	0.0737	0.1000	84.46	58.27	0.9603	0.5252
Table 5 lists the LR performance across scenarios FD (fixed data only), F-LMS (LMS usage measured until the end of the term), and M-LMS (LMS usage measured until midterm). When comparing the FD and F-LMS scenarios, at a 0.05 Wilcoxon Signed Ranked Test significance, 
 
 only differed in the Large Courses set, for which it improved by 0.0006, whereas among the other metrics, the highest improvement was in R-Fscore, with values of 0.0157 (Large Top 20%) and 0.0029 (Large Courses). RMSE and MAE showed no significant changes; in addition, there were no improvements between the F-LMS and M-LMS scenarios. This shows that including LMS usage in the LR model produces a slight to no improvement. Table 6 shows RLR performance data where a similar pattern is observable between the FD and F-LMS scenarios, considering a 0.05 Wilcoxon Signed Ranked Test significance; the only difference for 
 
 occurred in the Large Courses set, which improved by 0.0004. The other metrics showed a maximum improvement for R-Fscore of 0.0082 (Large Top 20%) and 0.0016 (Large Courses). RMSE and MAE showed no statistical significant changes. Furthermore, there was no significant difference between F-LMS and M-LMS scenarios. Again, this shows that adding LMS usage to a RLR model causes slight to no improvement.

In contrast to LR and RLR, Table 7 shows that RF performance improved when comparing FD and F-LMS scenarios. At a 0.05 Wilcoxon Signed Ranked Test significance, 
 
 increased across the three sets by 0.0646 (Large Top 10%), 0.0852 (Large Top 20%), and 0.1089 (Large Courses). Likewise, RMSE decreased by 0.0080 (Large Top 10%), 0.0104 (Large Top 20%), and 0.0133 (Large Courses). In addition, as reprobated students are interesting, R-Fscore shows improvements of 0.0435 (Large Top 10%), 0.0674 (Large Top 20%), and 0.0750 (Large Courses). Moreover, when comparing the performance between the F-LMS and M-LMS scenarios, at a 0.05 Wilcoxon Signed Ranked Test significance, we observed no differences between sets Large Top 10% and Large Top 20%, whereas set Large Courses showed a slight net improvement in favor of M-LMS for MAE and PAP (0.0007 and 0.4650, respectively). This means that RF performance improved using LMS data; furthermore, RF performance in the M-LMS scenario was as good as in the F-LMS or, in fact, slightly better. One possible reason behind the model's improvement after including LMS data could lie with student behavior related with their performance. In general, non-failing students with high grades are exempted from taking final exams; therefore they show decreasing or no activity in the LMS at the end of the semester. However, underperforming students do have to take final exams and their system usage should increase or at least stay constant until the end of the semester, adding noise to the aggregated LMS usage metrics.

3.2. Differences across algorithms
When comparing RLR against LR, at a 0.05 Wilcoxon Signed Ranked Test significance, the former provided slightly better results in terms of A-Fscore (values varied between 0.0009 and 0.0029) and PAP (values varied between 0.43 and 0.83), over the three sets. However, 
 
 produced slightly worse results at a 0.05 Wilcoxon Signed Ranked Test significance, − 0.0055 on average, as well as for the remaining metrics. The reason for this could be explained by the exclusion that RLR makes of valuable examples of students who failed, and were too far off the population median. Owing to the overall worse error metrics of RLR and the popularity of LR, RF will be compared against LR.

RF stands out with better metrics in almost every scenario when compared to LR (and RLR). As the results listed in Table 5, Table 6, Table 7 show, RF performs better even without LMS usage data. When comparing LR and RF in the FD scenario, at a 0.05 Wilcoxon Signed Ranked Test significance, 
 
 increased by 0.081 (Large Top 10%), 0.0612 (Large Top 20%), and 0.0211 (Large Courses), whereas RMSE decreased by 0.0091 (Large Top 10%), 0.0068 (Large Top 20%), and 0.0024 (Large Courses). A large difference appeared when comparing LR against RF in the M-LMS scenario, at a 0.05 Wilcoxon Signed Ranked Test significance, 
 
 increased by 0.1540 (Large Top 10%), 0.1469 (Large Top 20%), and 0.1420 (Large Courses), whereas RMSE decreased by 0.0181 (Large Top 10%), 0.0173 (Large Top 20%), and 0.0024 (Large Courses). In addition, it is worth noting that R-Fscore showed increments of 0.0846 (Large Top 10%), 0.1129 (Large Top 20%), and 0.1005 (Large Courses). Hence, RF produced better predictions than LR and RLR at an early stage of the semester, which means it has a better chance of identifying students at risk and applying timely educational interventions.

Fig. 4 shows the actual grades on the X axis and the average predicted values by RF and LR in the M-LMS scenario over the set Large Courses on the Y axis. The diagonal dashed line represents a perfect prediction, whereas the error bars for each point represent the confidence interval of the predictions. It is worth noting that both algorithms follow the actual grade trend, with low variance in the intervals where there are more samples (98.24% of actual grades are in the interval 0.25–1). Furthermore, the RF line is closer to a perfect fit than LR, which means that, on average, RF outperforms LR, especially when it comes to lower grades.

Fig. 4
Download : Download high-res image (257KB)
Download : Download full-size image
Fig. 4. Random forest and linear regression weighted predictions over the set Large Courses using LMS usage data measured until midterm.

3.3. Overall variable relevance
As the RF model over the set Large Top 10% in M-LMS scenario showed the best results, variable relevance was calculated over it. In addition, to provide a simpler understanding of the impact of variables, their relevance was classified as Very Low for less than 5%, Low for 5–35%, Medium for 35–65% and High for over 65%.

Fig. 5 shows the percentage increase in terms of MSE (X axis) for each predictor (Y axis) listed in Table 2. The most relevant variables were the students' cumulative grade point average (CGPA) with high relevance, and the SCHOOL in which they were enrolled with medium relevance. Other variables such as degree duration (C_DUR), age (AGE), number of semesters enrolled in the university (E_SEM), and university selection exam scores (EX_MAT and EX_LENG) showed a low relevance. Gender (GEND), and type of high school funding (F_SCHOOL, private or public) showed a very low relevance. Regarding the LMS, 11 out of the 25 variables showed a low impact on grade prediction, whereas the remaining variables showed a very low impact.

Fig. 5
Download : Download high-res image (391KB)
Download : Download full-size image
Fig. 5. Variable relevance rank in the set Large Top 10% using LMS usage data measured until midterm.

4. Discussion
Most studies use only one data source, including student academic demographic data McKenzie and Schweitzer, 2001, Saarela and Kärkkäinen, 2015, self-reported questionnaires Vandamme et al., 2007, Mishra et al., 2014, or LMS data Romero et al., 2013, Zacharis, 2015 focusing, in particular, on online courses Minaei-Bidgoli et al., 2003, Lykourentzou et al., 2009, Macfadyen and Dawson, 2010 and massive open online courses (Xing, Guo, Petakovic, & Goggins, 2015). Even though this makes data collection easier, it limits its predictive and generalization capacities. Moreover, typical approaches require an active effort from instructors, such as taking questionnaires or tuning up the models, which undermines their adoption (Aldunate & Nussbaum, 2013). There is some research that uses multiple data sources collected from automated systems (low-cost variables) (Arnold & Pistilli, 2012). These use past and present accumulated student grades, LMS usage and student demographic data. However, they provide no information about the algorithms applied. Other studies have a narrow scope and use only a few courses as their focus Huang and Fang, 2013, Kotsiantis et al., 2013. As Gašević et al. (2016) demonstrated, course-specific activities can have a relevant impact on final grades in the scope of a course, yet those prediction models cannot be extrapolated to an institution-wide context.

However, there are some studies that use similar data in an institution-wide context and that can be compared with the present study. In terms of model performance, the results of the present research for LR, used as a baseline, are better than those presented by Elbadrawy, Studham, and Karypis (2015). In addition, this study's RF approach shows a better performance, in terms of RMSE, than their collaborative regression model in the proposed scenarios. Nonetheless, comparing these results with other studies has its limitations due to the lack of similar conditions in terms of dataset size and the calculated error metrics. This study obtained similar results (in terms of PAP) when the RF results are compared, using the Large Top 10% set (3275 students), with the best algorithm and variable combination of Huang and Fang (2013) (323 students). However, these results tend to underperform compared with Huang and Fang’s when comparing the RF results using the Large Courses set (21,314 students).

Having said that, given the RF model and data sources (past performance, demographics, and LMS usage), the overall precision of this method (89.5% in set Large Top 10%, 89.5% in set Large Top 20% and 91.9% in set Large Courses) falls within the high range when compared with recent literature (Shahiri et al., 2015), with the additional characteristic that it can be applied in an institution-wide context. Although we recommend the use of F-score metric to compare models, for the purposes of comparison (with other studies) the results, in terms of precision and recall, were added in Appendix C. To be able to scale up these models to an institutional level, a z-score transformation was applied on the CGPA and LMS usage variables to increase their comparability using each course as context. Despite this transformation, the performance of the RF model, considering all the error metrics exposed, tended to decrease as the data heterogeneity increased (e.g. by adding more courses from various schools, with different sizes and degrees of LMS usage), which is in accordance with the generalization problem reported by Vandamme et al. (2007).

In terms of variable relevance, we found that the most significant variable was CGPA, in line with the literature (Shahiri et al., 2015), and the SCHOOL that the student was a part of, which is a proxy for grade differences across courses. A similar alignment with the literature was found for almost all DARA predictors, except for GEND (gender) and F_SCHO (high school funding source). Both showed an MSE increase of 4.03% and 1.32%, which indicates a low relevance. However, this is similar to the relevance rank reported by Mendez, Buskirk, Lohr, and Haag (2008). At any rate, like other demographic variables that correlate with student performance (Wright, 1921), the prediction relevance of these variables should not be considered as evidence of causality.

Also, the relevance of some variables could be obscured due to the presence of partial dependencies, for example, using a non-parametric test of independence at a 0.05 significance (Hoeffding, 1948) over CGPA with FOR_READ and FOR_WRITE, it showed that complete independence between these variables was unlikely, so in the absence of CGPA, the predictors FOR_READ and FOR_WRITE might increase their relevance. Yet, the three variables included together in the model improved the prediction, supporting their use.

In terms of its actual use by instructors, the proposed RF model can be more easily adapted to the particularities of the data than other algorithms, such as Bayesian networks (Xenos, 2004), given that it has fewer parameters to adjust (Breiman, 2001), with the number of trees the most relevant parameter in the model (Oshiro, Perez, & Baranauskas, 2012). Furthermore, RF is not sensitive to linear transformations in predictors; hence, predictors do not require special scale transformations such as min–max, which simplifies the use of the algorithm. However, a limitation of using RF, is that calculating the impact effect of variables with low relevance is fuzzy, since the relationships between variables are complex and its impact on the final grade is computed using hundreds of trees.

Nonetheless, it is relevant to note that the RF model can be further refined with a RMSE of 0.100 and 
 
 of 0.6203 in the Large Courses set and M-LMS scenario. This improvement can even be surpassed by overcoming the limitations of generating variables that represent the whole learning process, or polishing the algorithms with more complex techniques. The first limitation was addressed in this research by transforming the LMS usage data and past performance variables in such a way that it enabled them to be compared between different courses, and the students' enrolled school was added also to take the learner context into account. Other approaches to address this issue have been the following: Polyzou and Karypis (2016) described a model applied over course-specific features (only for College of Science and Engineering courses), which was generated using historical data that showed better results than linear regression and collaborative multi-regression models. Also, Romero et al. (2013) labeled forum interactions according to their relevance to the course topics, and Valsamidis, Kontogiannis, Kazanidis, Theodosiou, and Karakos (2012) proposed quality metrics for course resources in LMSs. However, these approaches are costly in their current form, and therefore further research is needed to generate standardized metrics that can automatically measure the learning process quality. Regarding the second limitation of using more complex techniques, as Polikar (2006) described, models can be enhanced by assembling multiple algorithms, which is akin to asking the opinion of several independent experts and weighting their responses. By using this technique, Kotsiantis, Patriarcheas, and Xenos (2010) showed an improvement in accuracy of around 1%. Thus, the challenge of finding better ensembles in educational contexts is still open.

Last, some intrinsic limitations are worth noting. The present study was conducted in a particular institution, which has a specific, context dependent student profile. Analyzed courses were mainly structured around a face-to-face format, using the LMS more as a document repository rather than a core learning tool. However, given the size of the university and the variety of their degrees, it may represent the reality of several educational institutions.

5. Conclusions
This research presents several contributions and implications for educational practice. Firstly, it proposes a low-cost model for the prediction of student performance that does not require active effort for data collection. This may be useful for instructors and educational managers as it relies on data that already exists in universities. Thus, efforts may be concentrated on supporting at-risk students rather than data gathering. Second, it presented an algorithm, Random Forest, that performed better than traditional LR and RLR, to an even greater extent after introducing LMS usage data. Furthermore, LMS data helped to improve predictions for at-risk students, more than it did for students with high and average grades. Therefore, this suggests that researchers who want to apply more accurate prediction models should use the above-mentioned algorithm applied to socio-demographic and academic data in conjunction with data from the LMS. Third, the present study provides some evidence that including aggregated LMS usage data generated similar or better predictions at midterm than at the end of the semester, when all semester's information is available. In this manner, the model may be useful for early detection of at-risk students, providing the basis for adopting early actions to prevent student failure. Fourth, these advantages show that it is possible for institutions to create specialized analytics offices dedicated to monitoring and predicting student performance. These units may provide information as early as mid-semester to instructors, program coordinators, and learning support professionals to work with students who are struggling. Finally, our results show that when LMS data is incorporated, prediction improves, which may be an incentive for further up-take of LMS systems, particularly in institutions where instructors face larger classes and cannot make personal contact with all their students.