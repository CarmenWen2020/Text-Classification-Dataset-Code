Existing studies have shed light on policies and strategies for learning analytics (LA) adoption, yet there is limited understanding of associations among factors that influence adoption processes or the change in priorities when institutional experience with LA increases. This paper addresses this gap by presenting a study based on interviews with institutional leaders from 27 European higher education institutions. Results showed that experienced institutions demonstrated more interest in exploring learning behaviour and pedagogical reformation than simply measuring a phenomenon. Experienced institutions also paid more attention to methodological approaches to LA than data constraints, and demonstrated a broader involvement of teachers and students. This paper also identifies inter-related connections between prevailing challenges that impede the scaling of LA. Based on the results, we suggest regular evaluations of LA adoption to ensure the alignment of strategy and desired changes. We also identify three areas that require particular attention when forming short-term goals for LA at different phases of adoption

Previous
Next 
Keywords
Epistemic network analysis

Higher education

Learning analytics

Adoption strategy

Adoption factors, adoption experience

1. Introduction
1.1. Learning analytics in Europe
Learning analytics (LA) is commonly defined as “the measurement, collection, analysis and reporting of data about learners and their contexts, for purposes of understanding and optimising learning and the environments in which it occurs” (Long, Siemens, Conole, & Gašević, 2011, para. 5). It has been repeatedly reported as an essential educational technology to support adaptive learning (Adams Becker et al., 2017; EDUCAUSE, 2018; Johnson et al., 2016). Research has also identified a number of benefits of LA, including its offer of targeted course offerings, personalising learning, improved learning outcomes, teaching performance, and curriculum design, and enhanced post-educational employment opportunities (Avella, Kebritchi, Nunn, & Kanai, 2016). In the context of higher education institutions (HEIs), there is an increasing demand to measure, demonstrate, and improve performance. As a result, LA has emerged as a new solution to addressing issues around retention, progression, and the enhancement of student success (Ferguson, 2012; Siemens, Dawson, & Lynch, 2013).

In recent years, the development of LA has been notable in Europe, where large-scale research projects have been commissioned by the European Commission to build a community to facilitate knowledge exchange, e.g., LACE project (http://www.laceproject.eu/) and engage various stakeholders in the process of policy formation, e.g., SHEILA project (https://sheilaproject.eu/). Specifically, LA has been highlighted as an area of strategic interest in the UK higher education (Scotland, 2018; Shacklock, 2016) as a response to continuous pressure from the change in funding models, a marketisation culture, international competition, and Brexit upheavels. An early report publised by Jisc (Sclater, 2014) showed emerging interest among UK HEIs in using LA to enhance student learning experience in areas such as feedback, learner agency, and learning performance and progression. In the following year, a survey carried out by the Heads of eLearning Forum (HelF) found that among the 53 UK HEIs that responded, a third was preparing to implement LA, a fifth had partially implemented LA, and one had fully implemented LA. Nevertheless, about a half of the institutions did not use LA at all. Subsequently, the UK Higher Education Commission carried out an investigation (Shacklock, 2016, p.26) in response to the increasing importance of data and analytics in higher education. One of the recommendations made by the report (ibid.) states, “All HEIs should consider introducing an appropriate learning analytics system to improve student support/ performance at their institution”. Importantly, this report highlights the crucial role of senior leadeship in driving the adoption of LA forward with a strong strategic vision. In response, we started a series of interviews in the same year with senior managers from 21 UK HEIs and 6 HEIs in mainland Europe to understand existing activities and plans concerning learning analytics, so as to formulate strategic directions for institutional adoption.

1.2. Social factors of LA adoption
While interest in LA among HEIs continues to grow, limited strategic planning for LA deployment has been identified as a critical factor of narrow scaling of adoption in higher education (Siemens et al., 2013; Viberg, Hatakka, Bälter, & Mavroudi, 2018). As Colvin et al. (2016) point out, a strategic vision that responds to the needs of an organisation is critical for a long-term impact and the development of institutional capability for LA. It has also been argued that the complex nature of higher education structures and its ecosystem requires a complexity leadership model that can respond to external changes in an agile fashion and turn tensions between the need to innovate and the need to produce into opportunities for LA (Dawson et al., 2018). Driven by the need to unpack and address tensions that might occur when LA is introduced to an institution, we set out to explore potential connections between factors that can influence LA adoption. While existing studies highlight key influences of social factors on LA adoption, such as leadership, strategy, culture, ethics and privacy, and analytic capabilities (Colvin et al., 2016; Greller & Drachsler, 2012; Siemens, Dawson, & Eshleman, 2018; Tsai et al., 2018), there has been limited research that systematically investigates the relationships between these factors, which arguably need to be treated as a whole rather than individual component in a complex system (Uhl-Bien, Marion, & McKelvey, 2007). In recent years, there have been consistent calls for evaluations of LA in terms of its impacts on learning and institutional practice as a whole (Ferguson & Clow, 2017; Kitto, Shum, & Gibson, 2018; Viberg et al., 2018). Crucial to evaluation are clear short-term goals that allow institutions to review, clarify, and adjust their vision so as to sustain the impetus of innovation in the long haul (Kotter, 2006). In light of this, we also investigate whether connections between LA adoption factors vary between institutions that are comparatively new to LA and those that have had formal engagement with LA, i.e., initiatives supported by the university, for more than a year. We explore two questions in this study:

(1).
How do factors that influence LA adoption processes in higher education associate with each other?

(2).
Do the connections of adoption factors differ between institutions that have adopted LA for less than a year and those that have adopted LA for more than a year?

1.3. Paper structure
This paper presents the findings of an exploratory study based on 29 interviews with 27 European HEIs. We analysed the data using mixed methods. That is, we followed a qualitative approach to collect and code the data thematically. Followed by that, we used a quantitative method, Epistemic Network Analysis (ENA), to identify and visualise the connections between the thematic codes, so as to guide further inspection of the interview data. ENA uses statistical computation to visualise the connections between concepts that could otherwise be difficult to observe or systematically present from qualitative data. This study is not intended to generalise observations of or make comparison between HEIs in mainland Europe and UK. Instead, the intension is to obtain in-depth information about the adoption of LA in a small group of institutions that share a certain degree of cultural similarity in a geographical region, for the purpose of informing institutional strategy for LA. In the rest of the paper, we first discuss key literature that has contributed to our understanding of prominent challenges of LA and key factors that influence the success of LA. Thereafter, we detail the methods that we have undertaken to collect, process, and analyse the interview data. Finally, we present prominent connections between selected themes, including goals, approaches, ethics, challenges, stakeholder involvement, and success. This paper concludes with three areas that require particular attention when forming short-term goals for LA at different phases of adoption.

1.4. Literature review
In this section, we first discuss the prevailing challenges associated with the adoption of LA in higher education, so as to provide a perspective on the ‘challenge’ codes that we used to analyse collected interview data. Then, we review the existing adoption framework for LA to identify critical factors that influence LA deployment and provide a theoretical background of the other codes presented in this paper.

1.5. Social complexities in LA adoption
Research has found that the most significant challenges that confront HEIs in terms of LA adoption are not technical, but social (Ferguson, 2012; Howell, Roberts, Seaman, & Gibson, 2018; Roberts, Howell, Seaman, & Gibson, 2016; Siemens, 2013; Siemens et al., 2013; Tsai & Gašević, 2017; Tsai et al., 2018). For example, Tsai et al. (2018) identified three areas of prominent challenges: the demand for resources, ethics and privacy, and stakeholder involvement. Resources primarily refer to data, funding, and people. In terms of data, significant issues concern the quality and scope of data that can reflect learning experiences accurately. In addition, the ease of obtaining and integrating data from various sources has also been reported constantly as a major challenge (Arroway, Morgan, O'Keefe, & Yanosky, 2016; Siemens, 2013). With regards to funding, EDUCAUSE studies (Arroway et al., 2016; Yanosky, 2009) reveal that LA often needs to compete with other institutional priorities, resulting in a challenge of obtaining sufficient financial support to supply an enabling infrastructure. Human resources primarily concern the capability and capacity to implement LA and to act on the results produced by LA. For example, a shortage of skilled people that have the ability to process, analyse, interpret data, and link it with pedagogy has been identified as a prominent challenge that worsens the gap between needs and solutions (Norris & Baer, 2013; Tsai & Gašević, 2017; Rienties, Herodotou, Olney, Schencks, & Boroowa, 2018), thereby impeding the scaling of LA research to enterprise solutions (Siemens et al., 2013). In addition, as LA serves the purpose of informing learning, teaching, and managerial decisions with data-based evidence, it is crucial to develop ‘data literacy’ among key users; that is, the skill to interpret the analysis of data critically (Wolff, Moore, Zdrahal, Hlosta, & Kuzilek, 2016).

LA involves the use of personal data to provide targeted support. As a result, ethics and privacy issues have also emerged as a significant challenge that is unresolved (Siemens et al., 2013) and continues to affect buy-in from key stakeholders (Drachsler & Greller, 2016). Some prominent concerns include the risk of intruding privacy, the difficulty of assuming informed consent due to unequal power relationships (Roberts et al., 2016; Slade & Prinsloo, 2013), the dilemma between keeping data anonymous and exploiting the most value (Drachsler & Greller, 2012), the potential of demotivating or stereotyping learners, the deprivation of learner autonomy (Roberts et al., 2016), data integrity, and potential data misuses (Howell, Roberts, Seaman, & Gibson, 2018). These unresolved issues have inspired the development of DELICATE (Drachsler & Greller, 2016), a checklist for institutions, researchers, and educators to self-check ethical and privacy requirements to carry out LA.

Finally, stakeholder involvement has been considered as crucial to the success of LA deployment. As educational systems are stable and resistant to change (Macfadyen, Dawson, Pardo, & Gašević, 2014), for academics to adopt LA, they need to perceive it to be pedagogically useful (Gašević, Dawson, & Siemens, 2015) as well as necessary in terms of addressing existing learning or teaching challenges (Ali, Asadi, Gašević, Jovanovic, & Hatala, 2013; Howell, Roberts, Seaman, & Gibson, 2018). However, discrepancies in existing experience and knowledge of data among different stakeholders often result in the challenge of finding common ground among stakeholders and meeting everyone's expectations (Tsai et al., 2018). Moreover, existing heavy workloads for academics often lead to anxiety about their limited capacity to incorporate LA into teaching (Tsai, Poquet, Dawson, Pardo, & Gašević, 2019; Howell, Roberts, Seaman, & Gibson, 2018). In addition, insufficient support from institutional leadership to drive strategic planning and policy development for LA has also been identified as a critical challenge that slows down the deployment of LA at institutional level (Colvin, Dawson, Wade, & Gašević, 2017; Norris & Baer, 2013; Siemens et al., 2013).

In light of the prevailing challenges, several frameworks have been proposed to ensure effective and ethical adoption of LA.

1.6. Systemic adoption of learning analytics
Colvin, Dawson, Wade, & Gašević, 2017 reviewed current models of LA deployment and identified three types of models: input, output, and process models. Input models focus on antecedent affordances that enable LA. For example, Greller and Drachsler (2012) proposed an LA framework to gauge understanding and expectations towards learning analytics among key stakeholders. This framework consists of six critical dimensions: stakeholders, objectives, data, method, constraints, and competences. Among these dimensions, constraints in particular focuses on observations of ethical and privacy limitations. Another established input model is the Learning Analytics Readiness Instrument (LARI) (Oster, Lonn, Pistilli, & Brown, 2016), which also includes six key elements: culture, data management expertise, data analysis expertise, communication and policy application, and training. Unlike the framework proposed by Greller and Drachsler (2012), LARI was designed to identify the processes institutions use when discerning their readiness to implement learning analytics. What is interesting about this study is that the data factor that was the largest component in the alpha analysis of this instrument (Arnold, Lonn, & Pistilli, 2014) has fallen out of the beta version of LARI. Instead, the culture factor emerged as the most influential factor when evaluating institutional readiness (Oster, Lonn, Pistilli, & Brown, 2016). This finding highlights the crucial role of institutional culture prior to or during the early adoption of LA.

Process models lay out key steps in the process of adopting LA (Colvin, Dawson, Wade, & Gašević, 2017). For example, the SHEILA framework (Tsai et al., 2018) builds on the RAPID Outcomes Mapping Approach (ROMA) (Young & Mendizabel, 2009), which defines six operational dimensions after the initial goal-setting step. These dimensions cover political contexts, stakeholders, desired changes, engagement strategy, capacity assessment, and evaluation. The SHEILA framework expands upon ROMA with action plans, challenge mitigation, and policy prompts. The framework aims to facilitate readiness assessment, strategy formation, and policy development. Unlike the two input models (Greller & Drachsler, 2012; Oster, Lonn, Pistilli, & Brown, 2016) that did not emphasise connections between the suggested elements, this process model highlights an iterative cycle to treat factors that influence LA adoption as mutually influential variables. For example, when identifying a (new) desired change, all the other dimensions need to be (re-)examined to ensure their connections. This applies to any strategic decision made related to a particular action, challenge, or policy process.

An output model of LA presents adoption maturity in multiple levels (Colvin, Dawson, Wade, & Gašević, 2017). For example, an LA sophistication model proposed by Siemens et al. (2013) includes five stages of maturity: awareness, experimentation, LA adoption (at student, faculty, and organisation levels), organisation transformation, and sector transformation. The model presents a vision of ideal progression of LA deployment, even though the reality is still far from the transformation stages (Viberg et al., 2018), and issues that hamper the scalability are often more entwined than linear (Tsai, Poquet, Dawson, Pardo, & Gašević, 2019). Based on the input, process, and output models introduced in this section, we can see four intertwined factors that influence the adoption of LA (Fig. 1). The context factors involve the political context of the institution, the drivers for LA, the resource capacity (funding, technology, capability), and the institutional culture. The strategy factors involve defining objectives, methods to adopt LA (including communication strategy), evaluation, and policy. The people factors include all issues related to stakeholders, ethics, and privacy. The challenge factor involve challenges or constraints related to any of the other three factors.

Fig. 1
Download : Download high-res image (68KB)
Download : Download full-size image
Fig. 1. Key factors of LA adoption.

As Fig. 1 shows, issues that confront HEIs in the adoption of LA tend to be entangled in a complex social system. The purpose of our study is to unfold the intertwined relationship between these social factors and learn from the changing patterns that might emerge from institutions with different experience of LA, so as to provide insights that may help institutions evaluate LA adoption progress, adjust short-term goals, and examine a long-term vision. To this end, an exploratory study was carried out to investigate key factors that influence LA deployment European HEIs. Specifically, we adopted a quantitative ethnographic approach to visualise patterns of associations between emerging themes (represented by codes) in the data. We elaborate on this method and the data collection process in the next section.

2. Methodology
2.1. A quantitative ethnographic approach
A quantitative ethnographic approach highlights the role of culture in turning a large set of data into meaningful information (Shaffer, 2017). The cultural significance in the interpretation of data is grounded in the tradition of ethnographic studies that are interested in how and why people attribute certain meanings to what they say or do (Taylor, 2001). Fundamentally, a quantitative ethnographic approach marries the strengths of both qualitative and quantitative research – the former allows researchers to gain a deep understanding of the underlying intentions, attitudes, values, and motivations of the behaviours of a small population, whereas the latter allows the observation of these aspects of a usually larger population to be quantified as trends (Robson & McCartan, 2016). Rooted in a quantitative ethnographic approach, Epistematic Network Analysis (ENA) (Shaffer et al., 2009) was developed initially to understand how a web of concepts and meanings connected to each other that may describe a particular group of subjects under study. While initially developed for the analysis of coded communication transcripts, ENA has been increasingly used to analyse other types of data where co-occurrence between a set of codes is used to assess the relationships among them. Within the field of education, ENA has been successfully used to examine the association between course content and critical thinking (Ferreira, Kovanovic, Gašević, & Rolim, 2018), collaborative learning (Gašević, Joksimovic, Eagan, & Shaffer, 2019), transferability of knowledge (Shaffer, 2004), mentoring (Nash & Shaffer, 2011), learning strategies (Matcha, Gašević, Uzir, Jovanovic, & Pardo, 2019), and technological pedagogical content knowledge of teachers (Zhang, Liu, & Cai, in press). In order to understand the dynamic relationships between factors that impact LA adoption, we employ this method to help us untangle the complexity of the social dimensions that need to be considered in institutional strategy for LA.

ENA is a graph-based method for examining relationships among a set of concepts in a codified dataset, such as those produced by quantitative content analysis. Unlike other graph-based methods that focus on analysing large datasets with simple relations, ENA is suitable for the analysis of domains with a small number of concepts (represented as network nodes) that have rich relationships (represented as network edges) among them (Shaffer, 2004). It can also be used to examine the differences in concept relationships for study subjects by their attributes. In our case, we compare relationships among elements of LA adoption for institutions with different amount of LA experience.

In more technical terms, ENA works by examining the co-occurrence of codes (representing concepts) within a set of stanzas, which are text excerpts (e.g., paragraphs and sentences) where co-occurrence represents a meaningful relationship for each of the units of analysis (i.e., study subjects). In our case, units of analysis correspond to different institutions, stanzas to conversation utterances during an interview (each utterance is articulated by one speaker at a time) and codes to the individual codes defined in a coding scheme. The first step in the ENA analysis was creating representations of the co-occurrence networks of analysis units in a high dimensional space. Then, a two-dimensional projection of the space (represented by X- and Y-axes) was created using singular-value decomposition (SVD), which is a dimensionality reduction method akin to factor analysis (FA) or principal component analysis (PCA). The purpose of creating a projection space is to enable the representation of high-dimensional data visually. Each data point in the projection space is a unit of analysis (i.e., institution), while axes represent the first two singular values that cover most variability in the original analytic space. For each analysis unit, ENA produces a network graph that captures the relationships among codes and their association with the two singular values. Thus, we can interpret positions of data points (institutions) on a projection graph by visualising their centroids and distance to the positions of codes on an ENA network plot. However, it is important to note that – unlike PCA and FA – the focus is not on deriving and validating latent factors represented by extracted components/factors. Within ENA, the focus of analysis is on interpreting positions of analysis units relative to codes, and relationships between codes for each analysis unit (or group of units). As a result, variance explained by two singular values is not critical. The purpose of the projection space is to visualise positions of data points as best as possible, realising the limitations of such visualisation.

ENA can also use sliding window stanzas where N adjacent stanzas are treated as a single unit. This is typically done when individual stanzas “build upon” each other (e.g., interview responses). In our case, we used two conversation utterances as a moving stanza to examine connections between ideas (presented through codes) mentioned within one utterance (one speaker's response) and across two adjacent utterances, as interviews involve at least two people taking turns to respond to each other, and most of our interviews were carried out on a one-to-one basis. Finally, to further improve visualisation of analysis units on the projection space, ENA also supports rotation of extracted SVD loadings using group means. In this case, the first singular value axis is fixed to pass through two group means in the analytic space, visually maximising the difference among the groups on projection space. However, since first singular value axis is fixed to pass through group means, it would typically capture less variance than the second singular value component, which was constructed to capture the most variance possible. In this study, we used a means rotation procedure when extracting SVD loadings of our projection space, which is clearly visible by positions of group means on X-axis.

2.2. Data collection and processing
To gain an overview of LA-related activities in HEIs, we carried out 29 interviews with institutional leaders from 27 universities between August 2016 and February 2017. An opportunistic sampling (Tracy, 2013) method was adopted to take advantage of the researchers' existing network and influence, which was primarily based in the UK. We sent out invitations to institutional leaders of all Scottish universities and subsequently extended the invitations outside Scotland based on existing initiatives that have been reported formerly in literature or informally in professional network meetings. The second wave of the invitation reached 17 universities from the rest of the UK and to some of the major universities in 5 European countries. The final study sample is as follows: Scotland (9), England (10), Wales (1), Northern Ireland (1), Ireland (2), Croatia (1), Norway (1), Romania (1), and the Czech Republic (1). Among the 27 universities (Table 1), all are comprehensive universities except for one that focuses on Science and Engineering subjects only. The majority (19) are medium-size universities (student population ranges between 10,000 and 30,000), seven are considered large universities (more than 30,000 students), and one considered small (less than 10,000 students). All but eight institutions were established after 1901. In terms of international league ranking, three of the institutions are among the top 100 universities in the world.


Table 1. Profiles of the study sample.

Institution	Location	Size	Subject coverage	World ranking	Experience (years)
U1	UK	Large	Comprehensive	Top 100	1–3
U2	UK	Medium	Comprehensive	Under 100	<1
U3	UK	Large	Comprehensive	Under 100	>3
U4	UK	Medium	Comprehensive	Under 100	1–3
U5	Ireland	Medium	Comprehensive	Under 100	1–3
U6	UK	Medium	Comprehensive	Under 100	<1
U7	UK	Small	Comprehensive	Under 100	1–3
U8	UK	Medium	Comprehensive	Top 100	1–3
U9	UK	Medium	Comprehensive	Under 100	1–3
U10	UK	Medium	Comprehensive	Under 100	<1
U11	UK	Medium	Comprehensive	Under 100	1–3
U12	UK	Large	Comprehensive	Top 100	1–3
U13	UK	Medium	Comprehensive	Under 100	<1
U14	UK	Medium	Comprehensive	Under 100	1–3
U15	UK	Medium	Comprehensive	Under 100	1–3
U16	Ireland	Medium	Comprehensive	Under 100	<1
U17	UK	Large	Comprehensive	Under 100	<1
U18	UK	Medium	Comprehensive	Under 100	1–3
U19	UK	Medium	Comprehensive	Under 100	<1
U20	UK	Medium	Comprehensive	Under 100	<1
U21	UK	Medium	Comprehensive	Under 100	1–3
U22	UK	Large	Comprehensive	Under 100	1–3
U23	UK	Medium	Comprehensive	Under 100	1–3
U24	Croatia	Large	Comprehensive	Under 100	1–3
U25	Norway	Medium	Comprehensive	Under 100	1–3
U26	Romania	Medium	Sci-Eng focus	Under 100	>3
U27	Czech Republic	Large	Comprehensive	Under 100	<1
The participants in the interviews ranged from (Vice) Principals/Deans of Learning and Teaching to Heads of IT, Directors of E-learning Centres, and positions established especially for learning analytics research and development. The decision to focus on this group of stakeholders was because they had an overarching view of their institutions. Each of these interviews lasted for about 60 min. The number of participants in each interview ranged from 1 to 3, and some participants from the same institution attended the interviews separately. This resulted in a total number of 41 participants and 29 interviews. We treated interviews from the same institution as one unit of analysis in ENA. This was a research design choice as our research interest was in finding out the adoption status in each institution rather than differences of views among different interviewees in the same institution. In cases where there were more than one interviewee, it was arranged due to structured responsibilities in institutions and hence different pieces of knowledge about LA adoption being held by different people.

The interviews were semi-structured (Robson & McCartan, 2016) to allow the interviewer to reorder, omit, or add questions based on the interviewer's observation of what seemed most appropriate to gain relevant information about LA adoption in the interviewees' institutional contexts. Ten interview questions were developed based on the four key factors of LA adoption (Fig. 1) to investigate (1) institutional plans for LA, (2) motivations for LA, (3) adopted strategy, (4) strategy development processes, (5) readiness preparations, (6) success and evaluation, (7) success enablers, (8) challenges, (9) ethical and privacy considerations, and (10) the interviewee's views of essential elements in an LA policy (the interview protorcol is accessible at http://bit.ly/leverage_interview). All the interviews started with an oral introduction to learning analytics, to align the understanding between the interviewer and the interviewees. All interviews were carried out online and video-recorded with consent received from the participants in advance. Subsequently, the interviews were transcribed verbatim.

A coding scheme was first developed based on a systematic literature review (Tsai & Gašević, 2017) and informed by the coding scheme of a large-scale study (Colvin et al., 2016) that investigated the state of LA adoption among Australian HEIs. The coding scheme was subsequently updated as new themes emerged during the initial inspection of the data that involved the researcher reading through the transcripts and noting down key themes. The final coding scheme contained two types of variables – implementation variables and readiness variables. The implementation variables include 14 groups of thematic codes that capture different aspects of institutional LA implementation, whereas the readiness variables include seven groups of thematic codes that represent different critical factors affecting the readiness of institution for LA adoption. Table 1 shows the 21 thematic groups of codes and the four key factors of adoption (Fig. 1) that they are associated with. As we have argued previously, these social aspects tend to be interwined. Thus, although each of the 21 themes reflect a particular social dimension, they tend to be applied beyond one dimension. For example, ‘stakeholder involvement’ might be used when the interviewee describes the institutional culture for LA, a strategic step to involve institutional members, procedures to ensure ethical use of data, or a challenge related to key stakeholders (Table 2).


Table 2. Coding scheme – thematic groups.

Adoption factors	Implementation variables	Readiness variables
Context	(1) goals	(1) technology
(2) experience	(2) funding
(3) educational data warehouse	(3) analytical culture
(4) policies	
Strategy	(5) approach	(4) leadership
(6) primary users	(5) analytical capabilities
(7) scope	(6) policy conceptualisation
(8) analytics elements	
(9) interventions	
(10) strategy development	
(11) evaluation	
(12) success	
People	(13) ethics	(7) stakeholder involvement
Challenge	(14) challenges	
Each of these themes contains 2 to 11 codes, resulting in a total of 99 codes. The definitions of these codes and associated themes can be found in the full coding scheme accessible at http://bit.ly/leverage_coding. The interviews were coded using NVivo by two researchers who manually applied the codes to words and sentences that reflect the concepts embedded in these codes. To ensure the consistency of the coding, the two researchers practiced coding 2 interviews, which involved three meetings in total to discuss the use of each code and resolve disagreement until the inter-reliability test of the final exercise indicated ‘excellent agreement’ between the coders (Cohen's Kappa = 0.86). Subsequently, the two researchers divided the interview transcripts and coded them individually. At the end of the process, one of the researchers (the principal researcher) again checked through the transcripts that were coded by the assistant researcher, to ensure the integrity of the data.

2.3. Analysis procedure
Given the 99 codes in the coding scheme, we first treated the 21 thematic groups as individual codes by considering all the sub-codes within a thematic unit as the same code. After conducting ENA on the level of thematic units, we identified themes that exhibited strong associations between them (see Section ‘Thematic analysis’) and further examined the relationship between these themes by pairs (see Section ‘Pairwise analysis of thematic groups’). For each pair of themes, we constructed ENA graphs consisting of all the sub-codes from given themes. We focused on inspecting interactions between the sub-codes under different themes rather than within the same theme at this stage. For example, when constructing ENA graphs of the sub-codes of ‘goals’ and ‘approach’, we were interested in identifying connections between the two themes, so as to understand how the adopted approaches might differ when different goals were set up for LA. Given our interest in learning whether or not associations between adoption factors vary among institution with different adoption experience, we labeled institutions by their experience at the content analysis phase: (1) less than one year (2) one to three years (3) more than three years However, as the result showed that only two institutions had adopted LA for more than three years, we decided to merge this group with the ‘one to three years’ group. In this paper, we compared institutions by grouping them as ‘novice institutions’ – institutions with less than one year of experience with LA (n = 9), and ‘experienced institutions’ – institutions with one or more years of experience with LA (n = 18). Considering that institutions with more than three years of experience might be more mature in some aspects, we individually inspected these to see if they demonstrated different patterns compared to the rest of the ‘experienced institutions’. However, these two institutions do not exhibit particularly unique traits when compared to the other institutions in the experienced group, as shown in the ENA networks presented in the following sections.

To compare the network graphs between experienced and novice institutions, we used two approaches commonly applied in ENA. First, ENA subtraction graphs show the differences between two ENA networks. Edge weights in a subtraction graph represent the differences in edge weights between the two compared networks and the colour indicates the stronger network. In our case, a blue edge would indicate a stronger relationship in the graph of novice institutions, whereas an orange edge would indicate a stronger relationship in the network of more experienced institutions. Second, the differences between the two institution groups on values along both X-axis (SVD1) and Y-axis (SVD2) were then compared by using a series of t-tests where the threshold for statistical difference was set initially at 0.05 and Bonferroni correction was then applied to avoid type I errors. As explained earlier, a means rotation procedure was carried out by positioning group means on X-axis to improve the visualisation of group differences. It is therefore expected that the comparison of group differences should focus on X-axis. During the pairwise analysis stage, we identified that the most dynamic interactions between themes were institutional goals versus approaches to LA, and LA challenges versus considerations of ethical and privacy issues (see Section Pairwise analysis of thematic groups'). However, we also found interesting connections among the sub-codes of some themes, including ‘stakeholder-involvement’, ‘challenges’, and ‘success' during this process. Therefore, we constructed ENA graphs for each of the three thematic units to see how different sub-codes under the same theme might connect to each other, and how the patterns might vary among institutions with different experience adopting LA (see Section ‘The analysis of individual themes'). For the sake of readability, we use abbreviations for all the nodes in the graphs. As ENA highlights connections between codes based on their co-occurrence, nodes that demonstrated no connections were excluded from the graphs.

The analysis was fully implemented in R programming language, using the rENA package (Marquart et al., 2018) for ENA analysis. Before running ENA, we first converted the data coded in NVivo into the format which is required by the rENA package. To do this, we first exported the data from NVivo into the CSV format, and then converted it into the format suitable for rENA, using custom Python programming scripts. In the next sections, we first present the results of an overall thematic analysis followed by the results of a pairwise analysis of selected thematic groups. In each of the sections, we report on the connections between the investigated themes as well as interview quotes when appropriate to illustrate the connections. Where interview quotes are included, we label them with the letter, U (University), followed by a number between 1 and 27 to distinguish the 27 universities that we have interviewed.

3. Results
3.1. Thematic analysis
The epistemic network in Fig. 2 shows the connection between the 21 thematic units under implementation variable and readiness variable:

(1).
Goal (goals)

(2).
Exp (experience)

(3).
EDW (education data warehouse)

(4).
Policy (policies)

(5).
Apprch (approach)

(6).
PrimUsr (primary users)

(7).
Scope (scope)

(8).
Anl.Elm (analytics elements)

(9).
Int (interventions)

(10).
StratDev (strategy development)

(11).
Eval (evaluation)

(12).
Succ (success)

(13).
Ethics (ethics)

(14).
ChIngs (challenges)

(15).
Tech (technology)

(16).
Fund (funding)

(17).
Anl.Cul (analytical culture)

(18).
Lead (leadership)

(19).
Anl.Cap (analytical capability)

(20).
PolicyConc (policy conceptualisation)

(21).
StholInv (stakeholder involvement)

Fig. 2
Download : Download high-res image (399KB)
Download : Download full-size image
Fig. 2. The overall mean ENA network for all implementation (green) and readiness (purple) variable groupss. The strongest connections are between the goal (Goal) and approach (Apprch) themes and between the challenge (Chlngs) and ethics (Ethics) themes. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)

The X-axis corresponds to the first singular value (SVD1) and explains 5.4% of the variance in the networks of the study subjects (institutions), while the Y-axis (SVD2) corresponds to the second singular value that explains additional 11.4% of the variance in networks of the subjects. The green nodes belong to the implementation variable group while the purple nodes belong to the readiness variable group. The thickness of the lines between nodes indicates the strength of their connections. In ENA terms, the strengths of connection between two codes represent the frequency of their co-occurrence across stanzas, which are in our case two consecutive conversation utterances. Fig. 2 shows that the strongest connections are between the following themes:

1.
Goal (goal) versus Apprch (approach)

2.
Chlngs (challenges) versus Ethics (ethics)

3.2. Pairwise analysis of thematic groups
Following the identification of two pairs of thematic units that demonstrated the strongest connections, we carried out pairwise analysis of these connections by examining the associations among the sub-codes in every two thematic units listed above (Goal vs. Apprch and Chlngs vs. Ethics). We plotted the centroids of the epistemic networks of the experienced (1 year or more) and novice (less than one year) institutions separately to understand whether connections between LA adoption factors might change when the experience of LA increases. The results are presented in the next two sub-sections.

3.2.1. Goals and approach associations
The goal theme contains three levels of goals for LA: Learning (learning goals), Teaching (teaching goals), and Institutional (institutional goals). They reflect the motivations, expectations and conceptualisation of LA. The approach theme contains five approaches (methodologies) to adopting LA: ProblemLed (problem-led approach), DataLed (data-led approach), Measuring (measuring approach), Exploratory (exploratory approach), and Experimental (experimental approach). The epistemic networks shown in Fig. 3 visualise the connections between the eight sub-codes – blue dots are goals and red dots are approaches. The X-axis explains 9.1% of the variance in the subjects' networks, while the Y-axis explains an additional 15.0% of the variance in the subjects' networks. Institutions with lower values on X-axis show a higher tendency towards data-led and exploratory approaches, whereas higher values on X-axis represent a focus on problem-led and measuring approaches. Institutions with lower values on Y-axis demonstrate an inclination towards learning-level goals, while high values represent focus on problem-led.

Fig. 3
Download : Download high-res image (615KB)
Download : Download full-size image
Fig. 3. Goal and approach associations. In graphs (a) to (c), blue dots are goals and red dots are approaches. In graph (d), blue dots are novice institutions and orange dots are experienced institutions. The squares on the X-axis represent group means, and each square is surrounded by a rectangle representing 95% confidence intervals. Both novice and experienced institutions had a strong connection between institutional goals (Institutional) and problem-led approaches (ProblemLed). Experienced institutions also showed a strong connection between teaching-level goals (Teaching) and exploratory approaches (Exploratory). (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)

We were primarily interested in the interactions between sub-codes from two different themes. The results showed that both novice and experienced institutions had strong connections between institutional goals and problem-led approaches. Such connections suggest that problem-solving approaches were usually adopted to improve institutional performance, e.g., student retention and satisfaction. The network of novice institutions (Fig. 3a) also displays a strong connection between institution-level goals and measuring approaches, which suggests that LA was often adopted as a measuring tool for institutional performance, e.g., student retention rate. By contrast, more experienced institutions (Fig. 3b) showed strong connections between teaching-level goals and exploratory approaches. This connection demonstrates an interest in using LA beyond just measuring but to explore further factors that might contribute to an observed phenomena for the purpose of enhancing teaching. For example, the interviewees from U5 and U11 (experienced institutions) indicated:

We're looking at the way that MOOCs have used Twitter to engage learners in a conversation. So that is a degree of analytics that we're drawing on to see to what extent whether there's any correlation between student success, completion rates and MOOCs and the way they've brought pedagogically Twitter into the learning experience. – U5

There's kind of that pedagogical side in terms of well can we actually see any patterns and trends and can we unpack that and can we hit support people to develop more engaging and effective learning and teaching experiences. – U11

The difference between novice and experienced institutions is made apparent by subtracting the edge weights (representing the co-occurrence of codes) in the two networks. As Fig. 3c shows, the blue connections are stronger among the novice institutions, whereas the orange connections are stronger among the experienced ones. A two-sample t-test assuming unequal variance showed a significant difference between the two mean networks along the X-axis (t(14.21) = −3.80, p = 0.00, Cohen's d = 1.63), but no significant difference along the Y-axis (t(18.30) = 0.00, p = 1.00, Cohen's d = 0.00) due to the means rotation (aligning group means along the X-axis). Fig. 3d shows the projection of the individual network of each institution by their centroid.

3.2.2. Challenge and ethics associations
The challenge theme considers past, existing and potential challenges at the institutions or general downside of LA that participants have perceived, whereas the ethics theme considers various aspects of ethics (including privacy) that have been considered in their planning and implementation of LA. The epistemic networks shown in Fig. 4 visualise the connections between the sub-codes of ‘ethics’ (red colour nodes) and those of ‘challenge’ (light blue colour nodes). There seven ethics codes include Anonymity (considerations of anonymity), Transparency (considerations of transparency), Access (considerations of data access), Consent (considerations of consent-seeking), Ownership (considerations of data ownership), LimDisc (limited discussion of ethical issues), and LimAware (limited awareness of ethical issues). The eight challenge codes include NoChallenges (no challenges identified), EthicsPrivacy (ethics and privacy-related challenge), Capabilities (capability-related challenge), DataLimit (data limitation-related challenge), Resources (resource-related challenge), BuyIn (buy-in-related challenge), Methodologies (methodology-related challenge), and Relevance (relevance challenge). The X-axis explains 6.4% of the variance in the networks of the institutions, while the Y-axis explains an additional 16.2% of the variance in the networks. Institutions with lower values on the X-axis show a higher tendency towards challenges around buy-in, resources, and methodologies, whereas higher values on the X-axis show a higher tendency towards challenges around relevance and data limitations. Institutions with lower values on the Y-axis demonstrate an inclination towards ethical considerations of data ownership and consent-seeking, while high values incline towards limited awareness or discussion of ethical issues.

Fig. 4
Download : Download high-res image (793KB)
Download : Download full-size image
Fig. 4. Challenge and ethics associations. In graphs (a) to (c), blue dots are challenges and red dots are ethics related Issus. In graph (d), blue dots are novice institutions and orange dots are experienced institutions. The squares on the X-axis represent group means, and each square is surrounded by a rectangle representing 95% confidence intervals. Both novice and experienced institutions demonstrated a strong connection between consent-seekign processes (Consent) and challenges of ethics and privacy (EthicsPrivacy). Novice institutions also showed a strong connection between data access (Access) and challenges of ethics and privacy (EthicsPrivacy). (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)

Focusing on the associations between challenges and ethical considerations, we found that both novice and experienced institutions demonstrated a strong connection between Consent (considerations of consent) and EthicsPrivacy (challenges related to ethics and privacy) (Fig. 4a and b). Moreover, novice institutions emphasised the connection between Access (data access) and EthicsPrivacy (challenges related to ethics and privacy) (Fig. 4a). This suggests that while issues around consent-seeking processes remained challenging among all the institutions, new adopters of LA particularly faced pressure to clarify who had access to data. For example, one interview explained this challenge:

It took us about three or four months to come to a final agreed data access agreement. If I'm honest there will be further iterations and amendments probably as we go through the project to incorporate new findings. And it's also a hugely sensitive area that we have to be sure that we're both meeting our legislative requirements under the data protection act but also we're meeting the high expectations of our staff and students in that area. So, we're being very careful. We're taking baby steps and making sure that all parties are informed as we go. – U10

The subtraction of the two groups of institutions (Fig. 4c) shows that experienced institutions demonstrate more connections between various challenges, which might indicate a higher number of encounters with, and hence awareness of, challenges as the deployment of LA progresses. A two-sample t-test assuming unequal variance showed a significant difference between the two mean networks along the X-axis (t(22.79–10.28, p = 0.00, Cohen's d = 3.66), but no significant difference along the Y-axis (t(17.22) = 0.00, p = 1.00, Cohen's d = 0.00) due to the means rotation. The centroids of the individual network of each institution projected in Fig. 4d showed two distinct groups by LA adoption experience.

3.3. The analysis of individual themes
3.3.1. Challenges
We further analysed connections among the challenge codes (Fig. 5). The X-axis explained 9.0% of the variance in the networks of the institutions, while the Y-axis explained an additional 18.7% of the variance in the networks. Institutions with lower values on the X-axis show a higher tendency towards challenges related to resources, whereas higher values on the X-axis show a higher tendency towards challenges around capabilities, relevance and no challenges. Institutions with lower values on the Y-axis demonstrate an inclination towards capability-related challenges, while high values incline towards data-related limitations and methodology-related challenges.

Fig. 5
Download : Download high-res image (731KB)
Download : Download full-size image
Fig. 5. Challenge sub-codes associations. In graph (d), blue dots are novice institutions and orange dots are experienced institutions. The squares on the X-axis represent group means, and each square is surrounded by a rectangle representing 95% confidence intervals. Both novice and experienced institutions demonstrated strong connections between challenges of ethics and privacy (EthicsPrivacy) and challenges of gaining buy-in from stakeholders (BuyIn). Experienced institutions also tended to associate resource-related challenges (Resources) with buy-in challenges (BuyIn) and capability challenges (Capabilities). (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)

The results showed that both novice and experienced institutions demonstrated strong connections between EthicsPrivacy (ethics and privacy) and BuyIn (buy-in) (Fig. 5a and b). Moreover, the novice institutions also emphasised the association between EthicsPrivacy (ethics and privacy) and DataLimit (challenges related to data limitations). By contrast, the experienced insitutions tended to associate EthicsPrivacy (ethics and privacy) with Methodologies (methods adopted to approach LA) and Resources (including human, financial, and technological resources). This shows that that ethical and privacy challenges had strong impacts on buy-in from key stakeholders for both groups of institutions. The novice institutions tended to associate certain ethics and privacy challenges with the limitations of data in terms of accounting for learning (e.g., what can data tell about learners), whereas experienced institutions tended to associate ethics and privacy challenges with methods adopted to implement LA (e.g., data-driven, weak pedagogical grounding, and de-contextualising data) and with resources (e.g., technological affordances concerning anonymity and consent-seeking). This indicates that experienced institutions have a higher tendency to move beyond conceptual discussion on ‘what’ data can(not) do to focus on ‘how’ LA should be implemented. For example, the interviewee from U3, one of the two institutions that have adopted LA for more than three years, pointed out a challenge that illustrates the connections between ethics and privacy issues and both methodology and resource challenges:

There's a lot going on with students that we don't collect in terms of data points. And in fact, you know, in learning itself is a process that… we could observe and infer from behaviours but quite difficult to actually collect data evidence on actual learning. – U3

The results also showed that experienced institutions tended to associate Resources (resource related challenges, e.g., technological infrastructure, data, funding, and human resources) with BuyIn (buy-in challenges) and Capabilities (capability related challenges). This is made apparent in the subtracted network (Fig. 5c). This shows that experienced institutions tended to encounter more ‘people’ challenges, such as the acceptance of LA among different stakeholders and the shortage of skills to implement LA. For example, the challenge of getting buy-in from teaching staff has often been attributed to a prevailing perception of LA as additional work:

I think we have to be very realistic. Academic staff are time poor. They've got lots of pressures on them to do all sorts of things. Barely making the data available, even if it's done in a way that's very clean looking, very simple, a clean dashboard and it shows them how to react and what they might do, I still don't think we can rely upon them to act on that. – U5

Similarly, another interviewee suggested that such a perception could discourage staff from exploring a new technology, leading to low confidence in one's own ability to make a good use of LA:

Our staff are afraid to play…. There's the two factors that actually in their mind say, ‘we can't, it's gonna take up too much time’. If you gave them a TV flicker and sat them in front of the TV and said, ‘play’, they'd happily play. But they associate it [LA] with work and so they say, ‘we can't play’. So it's a learning and it's a pedagogical culture. – U22

On the other hand, buy-in from staff and the availability of data are also closely related. For example, one interviewee pointed out that staff's negative feelings about potential surveillance introduced by LA have made the access to data difficult:

How do we use that data without the staff thinking that this is a witch hunt, or that we are actually, you know, spying on them? – U4

A two-sample t-test assuming unequal variance showed a significant difference between the two mean networks along the X-axis (t(21.23) = −8.23, p = 0.00, Cohen's d = 3.03), but no significant difference along the Y-axis (t(15.73) = 0.00, p = 1.00, Cohen's d = 0.00) due to the means rotation. The difference in the challenge patterns by institutional experience is also observable from the projection graph of the centroids of individual institution networks in Fig. 5d.

3.3.2. Stakeholder involvement
The ‘stakeholder involvement’ theme identifies the extent to which the planning or implementation of LA have involved consultations with stakeholders at various levels. The epistemic networks shown in Fig. 6 display connections among five sub-codes: HighLevel (high-level stakeholder, i.e., senior managers), SupportLevel (support-level stakeholder, e.g., IT units and Student Services), PrimaryTeachers (primary stakeholders – teachers), PrimaryStudents (primary stakeholders – students), and External (external stakeholders, i.e., third parties). The X-axis explains 13.4% of the variance in the networks of the institutions, while the Y-axis explains additional 30.6% of the variance in the networks. Institutions with lower values on the X-axis show an inclination towards the involvement of students, while high values represent a focus on involving high-level and external stakeholders. Institutions with lower values on the Y-axis show a higher tendency towards the involvement of support stakeholders, whereas higher values on the Y-axis represent a focus on involving teachers.

Fig. 6
Download : Download high-res image (587KB)
Download : Download full-size image
Fig. 6. Stakeholder-involvement sub-codes associations. In graph (d), blue dots are novice institutions and orange dots are experienced institutions. The squares on the X-axis represent group means, and each square is surrounded by a rectangle representing 95% confidence intervals. Both novice and experienced institutions demonstrated a strong connection between the involvement of managers (HighLevel) and that of professional services (SupportLevel). Experienced institutions also showed a strong connection between the involvement of teachers (PrimaryTeachers) and that of students (PrimaryStudents). (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)

Both novice and experienced institutions demonstrated a strong connection between HighLevel (high-level stakeholder) and SupportLevel (support-level stakeholder) (Fig. 6a and b). That is to say, when senior managers were involved, support-level stakeholders (e.g., IT officers, Student Services, legal teams, and LA experts) were often involved too. This suggests that institutions or projects that had support from key leadership tended to have better access to technological resources and other relevant skill support. The results also showed that the experienced institutions had a strong connection between PrimaryTeachers (teachers) and PrimaryStudents (students). This indicates a more active involvement of both primary stakeholders (i.e., teachers and students) among the experienced institutions than among the novice institutions. One of the common engagement strategies shared by the interviewees is to include student representatives and faculty members in the steering group for LA. For example, one of interviewees from U2 explained the importance of involving primary stakeholders:

The easy thing for us to do would be to establish a very high level approach to learning analytics and then tell people what to do. What we're trying to do is balance a small amount of that with a large amount of growth from within the University to help drive a relevant strategy. So a bit more bottom up and top down. – U2

A two-sample t-test assuming unequal variance showed a significant difference between the two mean networks along the X-axis (t(17.05) = −2.73, p = 0.01, Cohen's d = 1.09), but no significant difference along the Y-axis (t(17.86) = 0.00, p = 1.00, Cohen's d = 0.00) due to the means rotation. The difference is made apparent in the subtracted network in Fig. 6c and the projection of the centroids of individual institution networks in Fig. 6d.

3.3.3. Success
The ‘success’ theme identifies the success of LA that interviewees self-claimed. Fig. 7 shows the connections among eight sub-codes: Culture (culture-oriented), Experience (experience-oriented), Institution (institution-oriented), Teaching (teaching-oriented), Learning (learning-oriented), Infrastructure (infrastructure-oriented), Capability (capability-oriented), and NoConfirmedSuccess (no confirmed success). The X-axis of the network explains 14.8% of the variance in the networks of the institutions, while the Y-axis explains additional 17.4% of the variance in the networks. Institutions with lower values on the X-axis demonstrate an inclination towards experience- and culture-oriented success, while high values represent a focus on capability- and institution-oriented success. Institutions with lower values on the Y-axis show a higher tendency towards no confirmed success and capability-oriented success, whereas higher values on the Y-axis represent a focus on culture-oriented success.

Fig. 7
Download : Download high-res image (523KB)
Download : Download full-size image
Fig. 7. Success sub-codes associations. In graph (d), blue dots are novice institutions and orange dots are experienced institutions. The squares on the X-axis represent group means, and each square is surrounded by a rectangle representing 95% confidence intervals. The novice institutions showed a strong connection between the success in improving the data culture (Culture) and gaining experience of LA (Experience), whereas the experienced institutions did not demonstrate particularly strong connections among the sub-codes. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)

The ENA graphs showed a strong connection between Culture (culture-oriented success) and Experience (experience-oriented success) among the novice institutions (Fig. 7a). For example, one of the interviewees from U20 indicated:

The success of it [LA], I think, is that it's showing the possibilities and I think opened up other people's imaginations to what could be possible if we had even more or different data. – U20

The experienced institutions did not demonstrate particularly strong connections among the sub-codes, although the success of positive impacts on learning, teaching, and institutional performance was visible (Fig. 7b). Note that the size of the nodes is determined using a normalised scale of the relative edge weight. Fig. 7c shows that novice institutions dominated the subtracted network. As ENA only accounts for codes that co-occur within a specified stanza (a conversational utterance with a moving window of two in our case), any aspect of success claimed by the institution is visible in the ENA graph only if it is mentioned along with another aspect of success. Therefore, we can only conclude that no obvious connections among different type of success were observed among experienced institutions. As the majority of the interviewees were reserved about commenting on the success of their LA implementation due to early phases, we have relatively less data being coded in this category, and hence several institutions share the same centroids (Fig. 7d). Nevertheless, the t-tests revealed significant differences across the X-axis of the two groups of institutions (t(9.31) = −4.19, p = 0.00, Cohen's d = 2.22), but no significant difference along the Y-axis (t(9.09) = 0.00, p = 1.00, Cohen's d = 0.00) due to the means rotation. In particular, the results highlighted a connection between the improvement of a data culture (e.g., trust in data and the inclination to make data-informed decisions) and gained experience (e.g., meeting short-term goals and tackling challenges) among institutions that were new to LA.

4. Discussion
In this paper, we examine the following questions using an exploratory research approach:

(1).
How do factors that influence LA adoption processes in higher education associate with each other?

(2).
Do the connections between adoption factors differ between institutions that have adopted LA for less than a year and those that have adopted LA for more than a year?

We use ENA analysis to visualise connections among prominent factors that influence the adoption of LA, and we further inspect the connections manually by extracting the interview data coded under corresponding themes. The results revealed interesting connections between ‘goal’ and ‘approach’ themes and between ‘challenge’ and ‘ethics’ themes, in addition to connections among the sub-codes under three independent themes: ‘challenges’, ‘stakeholder involvement’, and ‘success’. The results show different associations among these factors between institutions that have adopted LA for less than a year and those that have adopted for more than one year. In this section, we first answer the research questions by summarising common associations of LA adoption factors among the studied institutions, and examining institutional priorities among institutions with different LA adoption experience. Then, we discuss implications of the associations for institutional strategic planning.

4.1. Common associations of LA adoption factors
We have observed a dominant trend in using LA to improve institutional performance or management (institutional-level goals) among the studied institutions. The strong connection between institutional-level goals and the problem-led approach (using LA to resolve a pre-identified problem) indicates a primary interest in using LA to address problems related to institutional performance or management. This reflects the political pressure that HEIs are currently under – providing evidence to demonstrate and enhance excellence and quality (Shacklock, 2016).

The investigation into the connections between challenges and ethics revealed that the most common challenges of ethics and privacy that confronted the institutions were related to consent-seeking. This finding reflects the political context in Europe where the enforcement of the European General Data Protection Regulation 2016/679 (GDPR) required higher education institutions to update existing practices to ensure that consent is first sought before any collection of personal data takes place unless the premise is to serve a legitimate purpose or public interest (The European Union, 2016). It is likely that the institutions were still finding a way to manage data under the new law and to deal with possible changes to existing practices and data infrastructure.

The analysis of the challenge theme showed that issues of ethics and privacy had the strongest connection with challenges of buy-in, which concurs with Drachsler & Greller, 2016 who argue that unresolved issues of ethics and privacy have significant impacts on the uptake of LA among key stakeholders. Moreover, the analysis of the stakeholder theme shows that support from senior managers often leads to better access to various supports required for LA (e.g., IT), which reaffirms the role of key leadership in scaling LA at higher education (Colvin et al., 2016; Tsai & Gašević, 2017; Dawson et al., 2018).

Finally, the success reported by the interviewees shows a strong connection between the increase of experience and the improvement of institutional culture for LA, particularly among institutions that are relatively new to LA.

4.2. LA adoption experience and the changing priorities
We observed different patterns of connections between adoption factors among institutions that have varying levels of experience. Although both novice and experienced institutions demonstrate a strong inclination towards using LA to address problems related to institutional performance and management, the experienced institutions showed a higher interest in enhancing teaching-related decisions (teaching-level goal) by exploring certain learning or teaching patterns shown in data further (exploratory approach). This implies that as institutions' experience with LA increases, their conceptualisation of LA changes; that is, moving from the perception of LA as a solution model to an innovation model. In line with this observation, a systematic literature review by Viberg et al. (2018) has identified a trend of movement from measuring and predicting drop-outs to exploring student learning experience as the field of LA matures. Similarly, an Australian study identified two trends of deployment patterns (Colvin et al., 2016): one group demonstrated a tendency to adopt a solution-focused model, mostly to address student retention, whereas the other group was more inclined to see LA as a potential disruption and innovation to improve the quality of student learning. In our study, we identified similar trends when comparing institutions by their experience with LA.

As discussed earlier, existing research has identified culture to be a key factor in driving the adoption of LA (Greller & Drachsler, 2012; Macfadyen & Dawson, 2012; Oster, Lonn, Pistilli, & Brown, 2016). A mature data culture is formed when members of institutions show genuine interest in using data to inform decision-making, demonstrate the ability to interpret data critically, and decide on an appropriate action to take in response to the data. In other words, data needs to be treated as integral to the institutional practices that involve all the institutional members. Pivotal to an enabling culture for LA are stakeholder buy-in and capabilities. The results showed that the experienced institutions were particularly aware of (or experienced with) the impacts of stakeholder buy-in and capabilities on the institutional capacity of human resources to implement LA. This might be due to their broader involvement of primary stakeholders (both teachers and students) compared to novice institutions. Although experienced institutions did not highlight connections between the success of gaining experience and increasing institutional culture, we can see that adoption experience has contributed to better awareness of people-related challenges, which is undoubtedly a sign of maturity in a culture for LA.

The results show that ethical and privacy challenges are highly associated with ‘what’ data could or could not do and ‘how’ learning analytics was implemented. In particular, the novice institutions were more concerned about the implications of data constraints for ethics and privacy while the experienced institutions emphasised the impacts of methods adopted to approach LA. As discussed earlier, ethics and privacy have a significant impact on buy-in, whereas buy-in and capabilities are related to the resource capacity of the institution. The inter-related connections among the challenges are visualised in Fig. 8. Although our data show that institutional leaders did not explicitly draw connections between ‘capabilities and ‘methodologies' as much as the connections between other ‘challenge’ codes, the literature has shown that knowledge and skills of LA are likely to affect decisions of methods to approach LA (Norris & Baer, 2013; Siemens et al., 2013; Tsai & Gašević, 2017). Therefore, we added a dotted line between ‘capabilities' and ‘methodologies' to acknowledge this, despite the fact that our interviewees did not seem to demonstrate prominent awareness or experience of the connection between these two factors.

Fig. 8
Download : Download high-res image (48KB)
Download : Download full-size image
Fig. 8. Connections among the key challenges that confronted the 27 institutions.

4.3. Moving forward
In this study, we have observed inter-related challenges and different priorities among institutions with different levels of adoption experience. The results highlight a movement from a measuring culture to an exploratory one, a data-centred concern to a methodology-centred concern, and an involvement of high-level stakeholders to a more equal engagement with primary-level stakeholders. This shows a change in the conceptualisation of LA and approaches to incorporate LA into institutional operations. The trajectory of maturity observed in this study can be used to inform a roadmap of LA adoption, especially for institutions that are new to LA. Drawing on the six iterative dimensions of the process model, SHEILA framework (Tsai et al., 2018) (1. mapping political context, 2. identifying key stakeholders, 3. identifying desired behaviour changes, 4. developing engagement strategy, 5. analysing internal capacity to effect change, and 6. establishing monitoring and learning frameworks), institutions will need to evaluate LA adoption progress regularly (Dimension 6) and the end of the first year may be a crucial time for the evaluation and adjustment of strategy. For example, the political context (Dimension 1), especially drivers for LA, may change as an institution's experience and understanding of LA increase. In our study, we observed a positive connection between experience and a data culture. Based on the drivers, institutions will need to identify relevant stakeholders (Dimension 2). In our study, we have observed an increase in the involvement of teaching staff and students after the initial year of adoption, which corresponds to the third phase of the sophistication model proposed by Siemens et al. (2013) and implies that one year may be a minimum period of time to move beyond the first two phases (awareness and experimentation). Based on the changing drivers and involvement of stakeholders, institutions will also need to re-evaluate desired changes (Dimension 3) and strategy (Dimension 4) to ensure that LA serves the needs. For example, our study has identified a rising interest in going beyond measuring and reporting data about learners to exploring underlying factors of an observed phenomenon. This indicates a shift of strategy and potentially a change in institutional needs (e.g., teaching innovations). As institution's experience with LA increases, so will their capacity evolve (Dimension 5) (e.g., institutional awareness, culture, infrastructure, availability of data, and buy-in from stakeholders), which in turn will allow institutions to shape LA strategy accordingly. For example, our study has observed increasing attention to the overall methodology adopted to deploy LA.

Drawing on the observations of the change in priorities among the institutions under study and key factors of LA adoption (Fig. 1), we have also identified three areas that require particular attention at different phases of adoption:

First, contextual factors are the most prominent in the early phase as they can help institutions clarify their needs for LA, capacity to implement it, and strategic directions for adoption. The short-term goals at this phase will thus tend to focus on clarifying data sources, usability and usefulness, so as to determine a suitable approach to collecting and using data to achieve desired changes. However, as suggested above, institutions should review the progress of adoption and make necessary adjustments on a regular basis, as the institution's experience with LA increases.

Second, as institutions move from wrangling data to considering the overall approach to adopting LA, short-term goals should focus more on addressing people related issues, such as needs, ethics, privacy, communication, capacity and capability. It is crucial to ensure that teaching staff and students are included in the discussion of these topics and related decisions, as effective adoption of LA relies on the knowledge of pedagogical theories and learning experience (Gašević et al., 2015).

Third, ethical and privacy issues should inform short-term goals throughout adoption cycles, as also identified by LA experts in a group concept mapping study (Scheffel, Tsai, Drachsler, & Gašević, 2019). Key to addressing these issues are a clear awareness of the limitations of data and technology, in addition to the ability to adopt appropriate methods to address challenges that impede effective and ethical adoption of LA. Thus, training for key stakeholders should go beyond operation of tools to raise critical awareness and improve data literacy.

4.4. Limitations
We acknowledge that the grouping of the 27 institutions by experience in this study was crude. The decision in using one year as a threshold of maturity was made given that only two institutions had more than three years of experience with LA. Although the current study has not shown distinct difference between the two most experienced institutions (more than 3 years) and the rest of institutions in the same group, future studies could address this limitation by including a larger sample. Our study is based on a sample collected from a particular group of stakeholders in Europe (mainly UK) and thus the observed phenomena are not meant to be representative of general trends. Nevertheless, given that LA was a fairly new concept to many HEIs at the time when the data was collected, and institution-level adoption remains low among HEIs today, this study provides the best snapshot of institutional adoption available to the field. Future research may replicate the study in different contexts and with different stakeholders to obtain a more nuanced comparison of connections between different adoption factors.

