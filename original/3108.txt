The purpose of this study was to propose and validate a measurable model of learner participation in the context of online asynchronous discussion. We supplemented the findings of a main study (Study 1) with an empirical case study (Study 2). In Study 1, we modeled three levels of learner participation: peripheral participant, inbound participant, and full participant. Subsequent validation tests confirmed that the proposed learner participation profiles were sufficiently distinct from one another in terms of message quality and final score. Study 2 demonstrated how the profiling method might be duplicated in various online collaborative learning contexts. In addition, the case study further demonstrated the validity and reliability of the profiles using mixed data. The results show that the classifications explained learner engagement level in three dimensions: cognition, behavior, and emotion.

Previous
Next 
Keywords
Learner participation

Online discussion

Computer-supported collaborative learning

Social network

Learner profiles

1. Introduction
Computer-supported collaborative learning (CSCL) studies support the idea that knowledge building is a collaborative process achieved through seamless conversation and shared ideas among students in a learning community (Stahl, Koschmann, & Suthers, 2006; Xing, Kim, & Goggins, 2015). Given the pervasive use of online learning in education, stimulating and sustaining productive student interaction is crucial to helping students actively participate, interact with one another, and achieve learning goals in online learning environments (Dillenbourg, Järvelä, & Fischer, 2009; Xie, Di Tosto, Lu, & Cho, 2018).

To foster effective learner collaboration, teachers need to track participation level of individual students and offer support to students with different needs (Desmarais & Baker, 2012; Jeong, Hmelo-Silver, & Yu, 2014). In the context of asynchronous online collaborative learning, the current study focused on two major challenges: characterizing different levels of learner participation during online group interaction and suggesting viable classification methods.

From a developmental perspective, we regarded learner participation in CSCL as a progression of significant engagement in online discussion activities. We proposed a conceptual framework to characterize distinctive levels of learner participation along this progression. Numerous studies have proposed and analyzed different student roles in CSCL environments, including student leader (Harrer, Zeini, & Ziebarth, 2009; Xie et al., 2018), moderator (De Wever, Van Keer, Schellens, & Valcke, 2010; Xie, Yu, & Bradshaw, 2014), active member (Jahnke, 2010), and marginal participant (Wortham, 1999). For example, Xie et al. (2018) investigated a peer-moderated online learning environment in which students were appointed leadership roles. Although previous studies have deepened our understanding of various learner roles, most have focused on one particular level of learner participation at a time.

Marcos-García, Martínez-Monés, and Dimitriadis (2015) proposed a comprehensive model of learner roles in CSCL. Their research centered on emergent roles characterized by different learner participation in online discussions. Their extensive literature review resulted in seven student roles, from most to least participation: leader, coordinator, animator, active, peripheral, quiet, and missing. Despite its unique characterization of various roles, we found three limitations. First, no clear theoretical framework accounted for the spectrum of learner participation. Second, some boundaries between the pre-defined classifications seemed unclear. For instance, “quiet” learners could also be classified as “peripheral” because they might learn passively (Chi & Wylie, 2014; Lave, 1991), perhaps logging in to read the postings of other students instead of posting their own (Xie et al., 2014). Also, “quiet” and “missing” could overlap because both indicate an inactive or invisible status. Third, “leader” could involve a wide range of leadership styles, some of which might overlap with categories of lower participation (e.g., coordinator). The development of learning communities (e.g., online discussion) relies on internal leaders who play multiple roles: thoughtful commenter, active networker, coordinator, facilitator, broker, etc. (Denison, Hooijberg, & Quinn, 1995; Quinn, 1984; Wenger, 2000). To tackle these issues, we used the theory of community of practice (CoP; Davis, 2005; Lave, 1991; Wenger, 2000) to design an alternative model of learner participation in online discussion, a model comprised of distinct levels of learner participation.

We posited that learner participation might be assessed by analyzing how individual students conduct themselves during group interaction, which is considered the most essential path to meaning making and knowledge building in distance and online learning (Bernard et al., 2009; Moore, 1989; Thomas, 2013; Woo & Reeves, 2007). The evolving diversity of group interaction can be attributed to various levels of learner participation (Harper & Quaye, 2009; Jordan, 2014; O'Brien & Toms, 2008). CSCL studies have used social network analysis (SNA) as a way to evaluate structural changes in a learning community and shifts in individual student role during online collaboration (Cela, Sicilia, & Sánchez, 2015; Luhrs & McAnally-Salas, 2016; Marcos-García et al., 2015; Xie et al., 2018). For example, previous studies have used centrality measures—multiple metrics calculated by SNA that are assigned to individuals in a network to rank participants by popularity, influence, and mediation and to indicate the relative importance of those participants in the community (Marcos-García et al., 2015; Rabbany, Elatia, Takaffoli, & Zaïane, 2014; Wasserman & Faust, 1994).

We leveraged multiple SNA metrics that were likely to indicate different learner characteristics in terms of online discussion participation. On the basis of these pre-determined characteristics, we developed a computational model that characterizes individual participation level. These profiles for online collaboration could help instructors provide adaptive feedback to individual students. For example, group awareness tools, including the visualization of group dynamics and individual contribution, promoted more equally distributed participation (Janssen, Erkens, & Kirschner, 2011; Jin, 2017).

In this study, we proposed a measurable model of learner participation in an online collaborative setting and investigate how well the model characterized individuals in online discussions. To achieve these goals, we took a two-step approach: a main study followed by a case study. The context of the main study (Study 1) was a blended undergraduate writing course that featured wiki-provided asynchronous online discussion forums. The forums were designed to build a supportive community of interest in which students shared a common learning goal (e.g., developing academic writing skills) and contributed to critical conversations to help each other solve problems (i.e., writing academic essays). Within this context, we (a) proposed a conceptual model to differentiate qualitatively distinct levels of learner participation during online group interaction, (b) explored data models that use SNA to determine student profiles and track transitions from one profile to another, and (c) tested the degree to which the identified participation levels differentiated cognitive effort and final scores.

In the subsequent case study (Study 2), we applied the learner participation classification method to a graduate-level online course. In particular, we leveraged mixed data to describe the extent to which the classification results differentiated behavioral and emotional engagement as well as cognitive engagement.

2. Study 1: learner participation profiles
2.1. A conceptual model of learner participation
We posited that individual student participation in an online discussion creates dynamic learner transactions from which important learner roles emerge (Rabbany, Takaffoli, & Zaïane, 2011). The theories of CSCL explain how an individual becomes a significant member of a learning community in a specific discipline (Jones, 2016; Lave, 1991). Through continual interaction on an online discussion forum, a group of people can (a) deepen their knowledge and expertise related to a shared interest or (b) solve a set of problems (Davis, 2005; Floding & Swier, 2011).

In the context of the current study, the students pursued the shared goal of developing academic writing skills and helped each other sharpen arguments in their writing for the duration of a semester. Learning occurs through a social negotiation of meaning that results from learner interaction in shared activities (Bandura, 1977; Cobb & Yackel, 1996). From this perspective, becoming a significant member of a discussion community relates to becoming a knowledgeable individual (Lave, 1991).

2.1.1. Three levels of learner participation
We assumed that shifts in membership explained by legitimate peripheral participation (LPP) would apply to shifts in learner participation level in online discussion. LPP explains that newcomers become central participants by interacting with current experts in a community (Brown & Duguid, 1991; Dennen, 2014; Lave & Wenger, 1991). Wenger (2010) argued that the temporal nature of identity necessitates longitudinal tracking to determine learning trajectories based on continual peer interaction. Building on the trajectory model suggested by LLP (Davis, 2005; Lave, 1991; Wenger, 2010), we characterized three participation trajectories, from most engaged to least engaged: Full participants are students who contribute to building a community of interest. They take more control of communication between other students and trigger frequent interactions among other students (i.e., mediation) by sending or receiving a high number of messages (i.e., influence and popularity). Inbound participants are students who become more active in group conversations, raising interesting questions and meaningful feedback. Also, their growing engagement in critical conversation likely draws more attention from others, leading other students to send messages to them. Peripheral participants are typically new to an online collaborative learning community. Their participation is partial because they prefer to observe conversations and their engagement in group discussion is selective. Some remain inactive in group discussion.

2.1.2. SNA for profiling learner participation
For more than a decade, scholars have used SNA to represent learner interaction and to investigate individual participation levels during collaborative work (Cho, Gay, Davidson, & Ingraffea, 2007; Dado, Hecking, Bodemer, & Hoppe, 2017; Dawson, 2008; Harper & Quaye, 2009; Jeong et al., 2014; Jordan, 2014; O'Brien & Toms, 2008; Prinsen, Volman, & Terwel, 2007; Wasserman & Faust, 1994; Xing et al., 2015). Drawing on network data (i.e., student-student transaction data) extracted from online discussion forums, we analyzed how each learner was situated in a network in order to classify participation levels in an online community (Haythornthwaite, 2002; Luhrs & McAnally-Salas, 2016; Sparrowe, Liden, Wayne, & Kraimer, 2001).

Our premise was that SNA attributes are likely to indicate individual participation levels (see Table 1). SNA computes network metrics that can describe the roles that individuals play in a discussion group. Specifically, SNA metrics indicate the following three attributes: popularity, influence, and mediation. In this study, popularity refers to how many feedback messages a student in a group receives from other students. In-degree centrality (i.e., the number of messages received by a student) indicates the popularity of an individual (Baldwin, Bedell, & Johnson, 1997). Influence refers to how many feedback messages a student in a group sends to other students (i.e., prominence in group collaboration; Rabbany et al., 2014). Out-degree centrality (i.e., the number of messages sent by a student) indicates the influence of an individual (Luhrs & McAnally-Salas, 2016; Xing et al., 2015). Mediation is the extent to which a student takes control of the connections between other students in a learning network (Marcos-García et al., 2015). Previous studies have used either of the two centrality measures to examine mediation: betweenness centrality (i.e., the extent to which a student controls the communication between two other students in a community; Burt, 2009; Freeman, 1979) and closeness centrality (i.e., how fast and easily a student reaches out to all other students in a network; Burkhardt & Brass, 1990; Cho et al., 2007). The challenge is to integrate these multiple attributes of learner participation and find corresponding metrics (Marcos-García et al., 2015).


Table 1. A model of learner participation in online discussion.

Participation level	Definition	SNA attribute	Message quality
Level 3 (L3): Full participant	Frequently interacts; provides meaningful feedback; contributes to building a community	At least two overt attributes	High
Level 2 (L2): Inbound participant	More actively participates in conversations; raises interesting questions; provides meaningful feedback	At least one outstanding attribute	Moderate
Level 1 (L1): Peripheral participant	Participates in group interaction but does not noticeably engage in peer transactions; or remains inactive, does not participate, or only absorbs information from peers	Likely to improve but low in all three attributes	Low
2.1.3. Relation to message quality
We also posited that learner participation level would correspond to message quality. Specifically, we proposed that participation level might indicate cognitive effort to master a complex task (Fredricks, Blumenfeld, & Paris, 2004). In the context of this study, the complex task was to offer meaningful feedback in forum messages. Woo, Chu, and Li (2013) found that messages posted by highly engaged students addressed content-related changes in ideas, content, and organization of a writing sample more than messages posted by their counterparts. Content-related changes are associated with sophisticated thinking skills such as evaluation, clarification, suggestion, and alteration (Faigley & Witte, 1981; Woo et al., 2013).

We also assumed that different levels of learner participation would influence final score. For example, previous findings suggest that higher posting frequency in CSCL led to higher learning performance (Agudo-Peregrina, Iglesias-Pradas, Conde-Gonzalez, & Hernandez-García, 2014; Finnegan, Morris, & Lee, 2009; Macfadyen & Dawson, 2010). Students who lack proper understanding of a writing task (e.g., prior experience) might be passive in conversation and prefer to observe the interaction of other peers (Chi & Wylie, 2014). In the other direction, learner participation can help build topical knowledge and higher-order thinking skills (e.g., critical thinking; Dillenbourg, Järvelä, & Fischer, 2009). Student writing ability, in particular, can improve with regular and appropriate peer feedback (Graham, Macarthur, Schwartz, & Page-Voth, 1992; Schunk & Swartz, 1993).

3. Methods
We used mixed data gathered from a wiki-based online discussion forum to design and validate a model for detecting learner participation levels. The following research questions guided the current study:

RQ1

What is a computational process that can detect different learner participation levels (i.e., participation profiles)?

RQ2

To what extent do learner participation levels differ from one another?

RQ3

To what extent do learner participation levels relate to final scores?

3.1. Participants
Our sample of 56 participants consisted of students enrolled in the three sections of a wiki-enhanced writing course (n = 19, 20, and 17). To analyze learner participation during group interaction, we used all of the participation information available in the wiki log. Student scores were available for 52 students. The number of male and female students was nearly equal (n = 29 and 27, respectively), and senior students (n = 34) outnumbered junior students (n = 22). All student log and score data were anonymized and made available to the researchers.

3.2. Wiki-enhanced writing course
A blended wiki-enhanced undergraduate writing course was designed to engage students through group interaction, improve their ownership of their own writing, and help them compose well-reasoned academic essays. The course used wiki-based discussion forums to facilitate peer interaction. The course focused on preventing a downturn in student engagement, especially in the middle part of the course. To maintain student engagement and continual development of writing skills, the instructors divided the semester into four periods and distributed major assignments evenly. The students determined their own writing topics based on their expertise and built an online writing portfolio to demonstrate that expertise. The major assignments included Recommended Resources (RR), Writing Project 1 (WP1), and Writing Project 2 (WP2). Each student created a list of RR, an annotated bibliography of online resources associated with his or her writing topics. WP1 consisted of two distinct posts (WP1P1 and WP1P2). From a writer's perspective, students were expected to generate informed, insightful, principled, and original positions on recent and resonant developments in their areas of inquiry. In contrast, WP2 casted the students in the role of responder, and every student created a posting in response to a recent entry on a high-quality blog that addressed current and consequential phenomena in his or her area of inquiry. Course activities are described in more detail in Appendix A.

3.3. Data collection
We used stored traces of student activity on the discussion forums to prepare two SNA data sets: the node list (i.e., a list of students in each discussion) and the edge list (i.e., a list of student pairs in a discussion). We used an edge to define a transaction between a student sent a comment and the student who received the comment. The daily transactions were aggregated into the four equal time periods of the semester to track learner participation level. We did not include the first period because no discussion activity occurred. The total number of messages across the three sections was 570: 175 in period 2, 162 in period 3, and 233 in period 4.

Also, we used final scores as a performance variable. Scores on the final versions of each writing entry (i.e., WP1P1, WP1P2, and WP2), along with scores on major assignments and participation in online discussions, were aggregated into final scores for the course (i.e., maximum of 1000 points). However, for analysis, we omitted the points for participation in online discussion (i.e., maximum of 100 points) due to possible correlations between those scores and participation levels. In addition, WP2 was graded earlier than the other writing assignments, which were scored at the end of the semester. We regarded the WP2 score (i.e., maximum of 100 points) as an independent covariate that might partly reflect prior writing ability and influence the variance of the final scores. Therefore, the recalculated final scores for data analysis had a maximum of 800 points.

3.4. Quantitative data analysis
We used NodeXL software to process the network data and calculate SNA metric values. The four SNA metrics used in this study were in-degree, out-degree, betweenness, and closeness centrality. We first analyzed the correlations of the SNA metrics to build suitable combinations for the three attributes (i.e., popularity, influence, and mediation). Then we suggested a computational process to classify learner participation levels. Next, we reviewed the descriptive statistics of the metrics for each learner participation level. Third, we used cluster analysis to crosscheck the proposed computational model of learner participation level (Bergman, Magnusson, Khouri, & B. M., 2003; Kaufman & Rousseeuw, 1990; Sarstedt & Mooi, 2014). Fourth, we performed Analysis of Variance (ANOVA) to compare the proposed model to the cluster analysis outcomes. In the last step, we used Analysis of Covariance (ANCOVA) to test whether the participation levels differentiated the students' final scores, using the WP2 score as a covariate.

3.5. Qualitative data analysis
A message sent by a student in the discussion forum served as the unit of analysis. Focusing on the participants' messages in period 4, in order to examine message quality, we used closed coding to develop a set of pre-determined categories (Merriam, 1998), referring to the classification of content-related changes in Woo et al. (2013). The content-related classifications consisted of alteration, suggestion, clarification, and evaluation (see Appendix B). Referring to the codebook, two independent coders conducted multi-round sample coding and then reviewed and discussed their conflicting views to refine the coding approach. The final Kappa score (i.e., inter-rater reliability) for all the categories reached at least 0.79, indicating that agreement for each code was acceptable (very good Kappa value >0.81; Altman, 1991; Cohen, 1960).

4. Results
4.1. Measurable model of learner participation
We first examined the pre-selected SNA metrics (i.e., in-degree, out-degree, betweenness, and closeness) to determine an optimal set that might suit multiple attributes of student participation in online discussions. The averages for in-degree, out-degree, and betweenness centrality grew over time (i.e., averages of in-degree and out-degree were 2.56, 2.57, and 3.53 in periods 2, 3, and 4, respectively; averages of betweenness changed from 0.41–0.86 to 26.95), while closeness centrality values decreased (averages of closeness were 0.39, 0.26, and 0.05 in periods 2, 3, and 4, respectively), indicating that students tended to increase interaction and form more connected discussion communities. Overall correlations among in-degree, out-degree, and betweenness centrality from one period to the next (N = 56) remained relatively constant, ranging from r = 0.35 to r = 0.98, while closeness only related to in-degree and out-degree in period 3 (see Table 2).


Table 2. Correlations among learner metrics.

Period	Learner metrics	1	2	3	4
P2	1: In-degree	–			
2: Out-degree	0.98⁎	–		
3: Betweenness	0.41⁎	0.35⁎	–	
4: Closeness	0.16	0.15	0.00	–
P3	1: In-degree	–			
2: Out-degree	0.93⁎	–		
3: Betweenness	0.62⁎	0.65⁎	–	
4: Closeness	0.46⁎	0.48⁎	0.04	–
P4	1: In-degree	–			
2: Out-degree	0.63⁎	–		
3: Betweenness	0.40⁎	0.37⁎	–	
4: Closeness	−0.14	−0.19	−0.18	–
Note. The numbers in the first row (i.e., 1, 2, 3, and 4) indicate In-degree, Out-degree, Betweenness, and Closeness. N = 56.

⁎
p < .05.

We then reviewed associations between the centrality measures and the final scores (see Table 3). Similar to the positive relationships reported in previous studies (Cho et al., 2007; Xing et al., 2015), in-degree and out-degree were moderately correlated with final scores. However, betweenness and closeness were not.


Table 3. Correlations among SNA metrics and writing performance for all sessions.

Final score
Period 2	In-degree	0.06
Out-degree	0.04
Betweenness	0.11
Closeness	−0.11
Period 3	In-degree	0.36⁎
Out-degree	0.37⁎
Betweenness	0.06
Closeness	0.26
Period 4	In-degree	0.42⁎
Out-degree	0.44⁎
Betweenness	0.20
Closeness	−0.06
Note. N = 52.

⁎
p < .05.

These findings suggest that the centrality metrics point to attributes that are contingent upon the dynamics of learner networks. For example, frequent inbound and outbound messages might not always correspond to a mediation role. Some students might only interact with a few other students. Even if they sent and received a greater number of messages, their levels of connection to others in the network might be moderate to low. Also, compared to in-degree and out-degree centralities, betweenness and closeness distribution tend to be skewed (i.e., power-law distribution; Cohen, Delling, Pajor, & Werneck, 2014; Leydesdorff, 2007; Newman, 2010). Therefore, the correlations are indefinite, and each centrality metric might only identify the strongest students (Tsourakakis, Kang, Miller, & Faloutsos, 2009).

 We concluded that a single metric was insufficient to explain student participation in online discussion. Therefore, the potential of learner participation to predict final scores depended on a combination of centrality metrics, each one describing a different attribute of learner participation. To form a parsimonious set of metrics, along with in-degree and out-degree, we included betweenness for two reasons: (a) in this data set, betweenness was not only “constantly” related to in-degree (i.e., popularity) and out-degree (i.e., influence) centrality measures but was also sufficiently distinct from the other measures and (b) the definition of betweenness—the degree to which a student controls communication between other students—fit the concept of mediation.

We proposed that three SNA metrics would indicate three different but interrelated attributes of learner participation during group interaction (i.e., popularity, influence, and mediation). Each metric yields a ranked list of students (i.e., metric values in descending order). These metric values and their ranges are contingent on contextual factors such as group size and teacher scaffolding for collaboration. To determine which student has particular characteristics, therefore, a threshold must be set (Zouaq, Gasevic, & Hatala, 2011). One way is to set a cut-off value using the mean, a particular percentage (e.g., over 75%), or a pre-determined number of students from each ranked list (Kim et al., 2016; Kouznetsov & Zouaq, 2014; Zouaq et al., 2011). For example, the mean threshold might designate any centrality value greater than or equal to the average value of the metric. In the current study, we regarded the mean threshold as too generous and lacking empirical precision, and the first ranks threshold with k number seemed arbitrary (i.e., Top-K). Consequently, we settled on a filtering approach using the 75th percentile, which we regarded as sufficient to identify students who were distinct from others (Crocker & Algina, 1986; Godin & Shephard, 1985; Stuetzer, Koehler, Carley, & Thiem, 2013).

We computed a cut-off for each metric per period (i.e., the 75th percentile value) and classified individual participation levels (see Table 4). The computational model proposed that those who had at least two attributes greater than the threshold values were full participants. Inbound participants included two types of learners: those for whom one of the metrics exceeded the threshold value and those for whom all the metrics were lower than the threshold but the out-degree values were >1. We associated all other combinations with the peripheral participant level. Different combinations of metric values were matched with sub-profiles. For example, topical leaders shared opinions about a few issues in the discussion; thus, their overall mediation levels were not marked (i.e., In-degree >75, Out-degree >75, and Betweenness <75). Another example is the occasional participant who scantly posted comments (e.g., no >1). However, occasional presence can sometimes draw more attention (i.e., In-degree >75, Out-degree = 0 or 1, and Betweenness <75). Despite further distinctions between sub-profiles, we focused on the upper-level classifications: peripheral, inbound, and full participation.


Table 4. Selection threshold.

Level	Sub-profiles	Threshold values for each metric
In-degree	Out-degree	Betweenness
Full participant (Insider)	Leader	>75a	>75	>75
Topical leader	>75	>75	<75
Transactional facilitator	<75	>75	>75
Attractive facilitator	>75	<75	>75
Inbound participant	Attractive participant	>75	<75	<75
Issue seeker	<75	<75	>75
Active commenter	<75	>75	<75
Transitioning participant	<75	<75	<75
Peripheral participant	Occasional participant	>75	Zero or 1b	<75
Marginal participant	<75	Zero or 1	<75
a
75 indicates the 75th percentile value.

b
Out-degree value of zero or one indicates a student whose number of comments (i.e., messages sent by a student) was no more than one.

4.2. Differences in learner participation
Table 5 summarizes the learner participation profiles and associated averages for in-degree, out-degree, and betweenness centrality. In addition, we transformed these metrics values into standardized scores and computed standardized mean scores to show distinct differences among the learner participation profiles (Flunger et al., 2017). As depicted in Fig. 1, across all periods, full participants had the highest values for all three SNA metrics, followed by inbound participants and peripheral participants.


Table 5. Average values of each metric by learner participation level.

Learner participation level	In-degree	Out-degree	Betweenness
Period 2 (P2)
L3: Full participant (n = 32)	3.38 (0.28)	3.34 (0.48)	0.69 (1.38)
L2: Inbound participant (n = 9)	2.00 (0.00)	2.00 (0.49)	0.00 (0.00)
L1: Peripheral participant (n = 15)	0.92 (0.55)	1.00 (0.00)	0.00 (0.00)

Period 3 (P3)
L3: Full participant (n = 28)	3.43 (0.63)	3.32 (0.67)	1.46 (2.34)
L2: Inbound participant (n = 12)	1.75 (0.75)	2.00 (0.00)	0.08 (0.29)
L1: Peripheral participant (n = 16)	0.82 (0.40)	0.82 (0.40)	0.00 (0.00)

Period 4 (P4)
L3: Full participant (n = 23)	4.57 (1.34)	4.30 (0.82)	45.32 (33.81)
L2: Inbound participant (n = 28)	2.57 (1.40)	2.89 (0.74)	15.70 (28.31)
L1: Peripheral participant (n = 5)	1.40 (1.34)	0.80 (0.45)	0.00 (0.00)
Note. The value in parenthesis is a standard deviation. N = 56.

Fig. 1
Download : Download high-res image (153KB)
Download : Download full-size image
Fig. 1. Learner participation levels over three periods.

4.3. Validation
Next, we used cluster analysis to crosscheck the three levels of learner participation. First, we performed a hierarchical cluster analysis to identify the number of clusters in the data. As reported in Table 2, in-degree and out-degree values in periods 2 and 3 were highly correlated with each other above 0.9. Because collinearity might overrepresent some aspects of these variables (Kaufman & Rousseeuw, 1990), we used the data in period 4. We plotted the coefficients (i.e., distances) against the number of clusters. The biggest jump was identified at 53, resulting in three clusters, which we computed by extracting the step of elbow (53) from 56 cases.

Then, based on the number of clusters (K = 3), the K-means analysis yielded cluster center values, which are the clustering variables' average values in all the cases in a cluster (see Table 6). The averages of betweenness were displayed from highest to lowest in the order of clusters 3, 2, and 1. However, for in-degree and out-degree, clusters 2 and 3 were the same and only a short distance from cluster 1.


Table 6. Final cluster centers and ANOVA results.

Variable	Cluster
1 (n = 35)	2 (n = 14)	3 (n = 7)
In-degree	3.00	4.00	4.00
Out-degree	3.00	4.00	4.00
Betweenness	6.49	38.53	102.21
Note. N = 56.

 Next, we used ANOVAs to compare the proposed profiling method (model 1) and the K-means clustering (model 2) with regard to distance between learner participation levels. As described in Table 7, all of the mean values of the metrics for both models differed significantly across the clusters (p < .05). We further investigated pair-wise contrasts between the levels using a Bonferroni correction. More pairs of levels in model 1 exhibited significant mean differences for in-degree and out-degree than in model 2, while model 2 better distinguished the levels in terms of betweenness. From a data standpoint, the ANOVA findings show that model 1 formed three levels that were more distinguishable than those formed by model 2.


Table 7. ANOVA results for three metrics by model 1 and model 2 in period 4.

Model	Level	Error	F	P	Post-hoc comparison
Mean square	df	Mean square	df	Pair-wise comparisons⁎	P
M1	In-degree	34.86	2	1.88	53	18.53	0.000	L3-L2	0.000
L3-L1	0.000
Out-degree	29.54	2	0.57	53	51.59	0.000	All pairs	0.000
Betweenness	7461.48	2	0.573	53	8.45	0.001	L3-L2	0.003
L3-L1	0.010
M2	In-degree	15.29	2	2.62	53	5.83	0.005	L3-L1	0.010
Out-degree	6.45	2	1.44	53	4.47	0.016	L3-L1	0.030
Betweenness	28,080.79	2	104.62	53	268.41	0.000	All pairs	0.000
Note. Model 1 (M1) denotes the proposed learner participation profiling method. Model 2 (M2) refers to the K-means cluster analysis.

⁎
Bonferroni correction was applied for pair-wise Type I error. N = 56. p < .05.

4.3.1. Message quality
To test the assumption that learner participation levels would be associated with message quality, we performed content analysis using messages in period 4, coding for instances related to content changes (i.e., evaluation, clarification, suggestion, and alteration) that might represent cognitive effort and thus indicate message quality (see examples in Appendix B).

We coded a total of 604 instances (326, 265, and 12 cases for full, inbound, and peripheral participant, respectively). For each code, we also calculated an average number of instances per individual in each participation level (see Table 8). Full participant (L3) had the highest numbers across all four content-related changes, followed by inbound participant and peripheral participant. This analysis demonstrates that a higher number of quality messages were created by upper-level participants than lower-level participants.


Table 8. Number of instances across the levels in period 4.

Level	Alteration	Suggestion	Clarification	Evaluation	Total
L1: Peripheral participant (n = 5)	2(0.4)	6(1.2)	1(0.2)	4(0.8)	13(2.6)
L2: Inbound participant (n = 28)	67(2.4)	92(3.3)	21(0.8)	85(3.0)	265(9.5)
L3: Full participant (n = 23)	67(2.9)	126(5.5)	22(1.0)	111(4.8)	326(14.2)
Total	136(2.43)	224(4.00)	44(0.79)	200(3.57)	604(10.79)
4.3.2. Participation levels and final score
We investigated the extent to which learner participation level predicted final scores. The descriptive statistics demonstrate that final scores tended to increase from peripheral participant (L1) to full participant (L3) in periods 3 and 4 in model 1 (see Table 9). However, in period 2, final scores achieved by peripheral students were greater than inbound students. Initial participation in online discussion might have been motivated by the novelty effect of new learning activities and thus not sufficiently indicative of existing knowledge and skills. Model 2 (i.e., K-means clusters) showed that level 2 had a greater value than level 3. Based on this data, model 1 distinguished final scores better than model 2. Accordingly, we continued with the proposed profiling method.


Table 9. Means and standard deviations of final scores across participation levels.

Period 2	Period 3	Period 4
M1	M2
N	M	SD	N	M	SD	N	M	SD	N	M	SD
Level 1	14	631.50	57.41	15	569.93	170.62	4	444.75	289.97	33	607.91	124.71
Level 2	7	583.71	95.16	10	618.70	49.69	27	624.85	64.94	12	650.83	29.74
Level 3	31	628.39	120.78	27	654.48	43.22	21	655.10	42.89	7	648.00	56.15
Note. Model 1 (M1) denotes the proposed learner participation level. Model 2 (M2) refers to the K-means cluster analysis. N = 52.

Last, we performed one-way ANCOVA for each period to test whether participation level significantly differentiated final scores (controlling WP2 scores). One-way ANCOVAs showed no statistical significance in periods 2 and 3. The effects of participation level in the first two periods were mediated by WP2 scores and became insignificant. The ANCOVA for period 4 produced a significant result. Levene's test revealed that the data met the homogeneity assumption (F = 0.659, p = .522, p < .05). Period 4 data revealed that final scores differed across learner participation levels [F(2,48) = 4.68, p = .014, p < .05] (see Table 10). The ANCOVA results confirmed that the highest scores were achieved by full participants, followed by inbound and peripheral participants.


Table 10. Analysis of covariance summary.

Effect	ANCOVA test
Sum of squares	df	Mean square	F	Sig.	η2
M1: Period 2 (P2)
WP2	394,002.61	1	394,002.61	134.32	0.000	0.737
P2 level	3151.76	2	1578.88	0.54	0.588	0.022
Error	140,801.67	48	2933.37			

M1: Period 3 (P3)
WP2	337,334.03	1	337,334.03	114.837	0.000	0.705
P3 level	2953.69	2	1476.85	0.50	0.608	0.021
Error	140,999.75	48	2937.50			

M1: Period 4 (P4)
WP2	278,249.41	1	278,249.41	110.882	0.000	0.698
P4 level	23,500.88	2	11,750.43	4.68	0.014	0.163
Error	120,452.56	48	2509.43			

M2: Period 4
WP2	387,475.23	1	387,475.23	133.94	0.000	0.736
Level	5096.28	2	2548.14	0.881	0.421	0.035
Error	138,857.16	48	2892.86			
Note. N = 52. P < .05.

Bold indicates that participation level in Period 4 significantly differentiated final scores.

5. Study 2: An empirical case of the proposed profiling method
Study 1 showed that the theoretical framework and computational model is a viable way to describe learner differences in online discussion participation. We supplemented this initial validation study with an empirical case study to demonstrate how the proposed classification method might address a specific need in research and practice.

5.1. Problem context
Nine graduate students enrolled in a graduate-level online course about foundations of instructional design and technology. Course assignments included a series of asynchronous online discussions over a 12-week period. Each student uploaded his/her weekly reading journal on an assigned set of readings. This non-trivial posting included a substantive remark or question accompanied by an appropriate summary of the readings. The critical questions raised by the students in their weekly journals prompted discussion. Students had to read and provide feedback on at least three of their peers' reading reflections each week. In addition, students had to moderate the discussion for at least two assigned weeks. Student moderators commented on postings, answered questions, and facilitated discussion among their peers. Despite the small size of the group (N = 9), the students posted an average of 42 messages each week (a total of 506 transactions over the 12 weeks). Due to the complexity of online discussion threads, the instructor had difficulty determining who was moderating and which students persistently participated in the weekly discussions.

5.2. Assumptions for solution
We posited that the L3 (i.e., Full Participant) classification would indicate which students played significant roles in a given week. Based on this premise, we used the profiling method to classify individual participation each week. Students who were at the L3 level for more than three weeks (i.e., students who moderated discussion) and whose median participation levels were equal to or greater than L2 (i.e., Inbound Participant) every week were considered quality participants.

Study 1 demonstrated that participation levels were associated with message quality. In addition to this cognitive aspect, we assumed that different levels of learner participation in online learning might be attributed to multiple learner engagement dimensions, including behavioral and emotional (Appleton, Christenson, Kim, & Reschly, 2006; Harper & Quaye, 2009). Behavioral engagement is commonly defined as the observable behaviors necessary to academic success (Heddy & Sinatra, 2013), such as participation in group communication. Emotional engagement is inseparable from group learning because feelings such as interest, boredom, happiness, and frustration are linked to individual characteristics and social relationships in a learning community (Artino & Jones, 2012).

Specifically, we expected that the quality participants would be more engaged than their counterparts in all three dimensions: behavioral, emotional, and cognitive. That is, the computed classification would be validated by learner engagement data. Working under these assumptions, we computed learner participation levels and identified quality participants. One central question guided this supplementary case study: To what extent is the identification of quality participants attributable to their engagement levels?

6. Methods
Conducting this supplementary case study with a small group of individuals, we used mixed data to determine the effectiveness of the learner participation profiling method in a specific context. The goal was to describe an application of the proposed computation model. Methodological triangulation and multiple data sources can help measure the utility value of a proposed model and increase the reliability and validity of interpretations (Stake, 1978; Yin, 1984). The data we used in this study included student log data stored in an online learning platform and text messages posted by the students. The total number of messages during the 12-week period was 506.

6.1. SNA metrics
We aggregated the daily transactions in the discussion forums on a weekly basis. Then we created the node list and the edge list for each week. Using NodeXL, we ran these processed network data to produce SNA metrics for each week and used these metrics to classify individual participation levels.

6.2. Cognitive engagement
To measure cognitive engagement, we used the Practical Inquiry Model, which defines a hierarchical order of four different cognitive activities from least to most engaging: triggering event, exploration, integration, and resolution (Garrison, Anderson, & Archer, 2001). Using a coding scheme (see Appendix C), two coders analyzed individual messages to determine cognitive engagement levels. The weighted Kappa for the ordinal outcomes was 0.90 (very good; >0.81). Each message was given an ordinal value corresponding to its cognitive level (i.e., 1 = Trigger Event, 2 = Exploration, 3 = Integration, and 4 = Resolution).

6.3. Behavioral engagement
The online learning platform tracked individual access to the learning units for each week: weekly guide, reading materials, supplementary resources, discussion forum, etc. Using log data, we computed the completion ratio by dividing the number of units accessed by the total number of units in a week. For example, a ratio value of 1 indicated that a student visited all units in a given week. We considered this ratio an indicator of learner behavioral engagement.

6.4. Emotional engagement
We used a machine learning tool (i.e., IBM tone analyzer) to analyze posted messages and yield probabilities of five discrete emotions: anger, disgust, fear, sadness, and joy (Wang & Pal, 2015). Previous findings reported good accuracy levels (e.g., 80.7%, Cagnol, 2017; 68%, IBM Cloud, 2018). We considered a probability value higher than 0.5 means that an emotion was likely to be perceived. Using individual probabilities, we determined whether a message showed significant positive (i.e., joy), negative (i.e., anger, disgust, fear, and sadness), or a mixture of both positive and negative emotions; based on this determination, we created three binary variables: (a) emotion-positive, (b) emotion-negative, or (c) emotion-complex. We counted the total occurrences (i.e., a maximum of 12) of each emotional state for each student across the 12-week period.

6.5. Final score
Final scores aggregated all grading items that were awarded points. We omitted the participation points (i.e., a maximum of 10 points), yielding an adjusted final score no higher than 90 points for analysis.

7. Results
7.1. Identification of quality participants
We first analyzed individual participation levels over the 12-week period using the profiling method. As described in Table 11, the profiling method yielded numbers with small variations across the weeks. The number of students at the L3 level ranged from 1 to 3, and two L3 students emerged in over half of the weeks. We identified five quality participants who met the selection criteria: (a) being at the L3 level >3 weeks and (b) having a median level of participation equal to or greater than L2.


Table 11. Proportions of the three levels across 12 weeks.

W2	W3	W4	W5	W6	W7	W9	W10	W11	W12	W13	W15
L3	22% (2)	11% (1)	33% (3)	0% (0)	22% (2)	22% (2)	22% (2)	22% (2)	22% (2)	22% (2)	11% (1)	11% (1)
L2	67% (6)	78% (7)	67% (6)	78% (7)	67% (6)	56% (5)	56% (5)	67% (6)	67% (6)	78% (7)	78% (7)	78% (7)
L1	11% (1)	11% (1)	0% (0)	22% (2)	11% (1)	22% (2)	22% (2)	11% (1)	11% (1)	0% (0)	11% (1)	11% (1)
Note. N = 9. A number in a parenthesis indicates the number of students classified in the level.

7.2. Validation of the identification
We validated the identification of quality participants by triangulating the results with learner engagement data. Table 12 shows that throughout the course, quality participants (i.e., group 1) demonstrated higher cognitive, behavioral, and emotional engagement and also achieved higher scores than their counterparts (i.e., group 2). For example, students in group 1 showed far more complex emotions (M = 5.60) than group 2 (M = 2.00). Final scores in group 1 were also higher than group 2. In addition, we looked at the details of cognitive engagement. Group 1 posted messages indicating higher levels of cognitive presence more often than group 2 (see Table 13). For example, messages at the Integration level in group 1 were twice the number posted by group 2 (M = 16 and 8.75, respectively).


Table 12. Averages of learner engagement dimensions by group.

CE	BE	EEP	EEN	EEC	Score
Group 1 (n = 5)	2.19	0.75	11.00	5.60	5.60	89.60
Group 2 (n = 4)	2.04	0.68	9.75	2.00	2.00	85.75
Note. Cognitive Engagement (CE), Behavioral Engagement (BE), Emotional Engagement-Positive Emotions (EEP), Emotional Engagement-Negative Emotions (EEP), and Emotional Engagement-Complex (EEC). EEC indicates a message showing both significant positive and negative emotions.


Table 13. Total number and average of each cognitive presence category by group.

Triggering	Exploration	Integration	Resolution	Other
Group 1 (n = 5)	21.00 (4.20)	152.00 (30.40)	80.00 (16.00)	0.00 (−)	21.00 (4.20)
Group 2 (n = 4)	25.00 (6.25)	85.00 (21.25)	35.00 (8.75)	0.00 (−)	9.00 (2.25)
Next, weekly data showed that group 1 demonstrated distinctive trends in the learner engagement dimensions compared to group 2 (see Table 14 and Fig. 2). Group 1 tended to remain at similar cognitive engagement levels, while group 2 was less consistent but usually lower than group 1 before exerting more effort closer to the end of the semester. Behavioral engagement varied in both groups, but the access ratio of group 1was mostly higher than group 2. The most interesting trends emerged in the emotion dimension. Group 1 disclosed positive and/or negative emotions more often than group 2. We used an emotion value to indicate the proportion of students in a group who showed significant emotions (see Table 14). For example, a value of 0.60 for group 1 in week 2 means that 60% of the students in that group expressed that type of emotion at a significant level. As depicted in Fig. 2, the trends in emotion-complex demonstrate that group 1 more constantly showed a mixture of positive and negative emotions than group 2. This finding is consistent with previous findings: (a) leading students exerted emotional influence (Ashkanasy, Humphrey, & Huy, 2017; Wang, 2018), (b) negative emotions played a critical role in successful learning (Artino & Jones, 2012; Marchand & Gutierrez, 2012), and (c) students who value the tasks but also feel frustrated demonstrated higher engagement (i.e., both positive and negative emotions; Artino & Jones, 2012; Yip, 2009).


Table 14. Changes in learner engagement across weeks by group.

W2	W3	W4	W5	W6	W7	W9	W10	W11	W12	W13	W15
CE	G1	2.33	2.23	2.40	2.07	2.37	2.33	2.21	2.05	1.93	2.02	2.18	2.18
G2	1.97	1.97	1.61	2.14	2.06	2.58	1.78	1.94	2.00	2.08	2.44	2.28
BE	G1	0.75	0.73	0.87	0.53	0.73	0.80	0.80	0.53	0.90	0.90	0.67	0.80
G2	0.69	0.67	0.58	0.42	0.67	0.50	0.90	0.75	0.90	0.80	0.58	0.80
EE_P	G1	1.00	1.00	1.00	0.80	1.00	1.00	0.80	0.80	0.80	1.00	1.00	0.80
G2	0.75	1.00	1.00	0.75	0.75	0.75	0.75	0.75	1.00	1.00	0.75	0.50
EE_N	G1	0.60	0.60	0.40	0.40	0.60	0.60	0.40	0.40	0.60	0.00	0.40	0.60
G2	0.00	0.25	0.50	0.25	0.25	0.00	0.00	0.00	0.00	0.00	0.25	0.50
EE_C	G1	0.60	0.60	0.40	0.40	0.60	0.60	0.40	0.40	0.60	0.00	0.40	0.60
G2	0.00	0.25	0.50	0.25	0.25	0.00	0.00	0.00	0.00	0.00	0.25	0.50
Note. Cognitive Engagement (CE), Behavioral Engagement (BE), Emotional Engagement-Positive Emotions (EEP), Emotional Engagement-Negative Emotions (EEP), and Emotional Engagement-Complex (EEC). EEC indicates a message showing both significant positive and negative emotions.

Fig. 2
Download : Download high-res image (194KB)
Download : Download full-size image
Fig. 2. Trends in learner engagement.

Based on our cross-comparison results, we construed that students classified as quality participants engaged in the discussion activities better than their counterparts. To validate this conclusion further, we conducted in-depth reviews of messages posted by randomly selected students: Meggie and Sara from group 1, and Jesse and Susan from group 2.

Meggie and Sara frequently posted messages that represented higher-order cognitive presence (e.g., Integration). They strived to integrate ideas from other students, bridge information from a chain of conversations, and generate conclusions. The following message from Meggie provides an example:

Both [student X] and [student Y] pose very important considerations. In answer to student X's question, generally the most logical way to monitor both positive and negative consequences would [be] to set up as many objective, relevant evaluation measures as practicality would allow. But as [student Z] said, for the most ardent proponents, even these measures may not be interpreted as crucial if they challenge expectations and hopes. [I guess the solution] would involve fostering the attitudes of skepticism that we discussed last week.

Also, they expanded on previous messages by adding substantial agreement. In doing so, they converged ideas, as one of Sara's messages exemplifies:

I appreciate the cartoons.... It [poignantly illustrates]… [I agree with] your argument that, “each person is an individual with unique mental models that cause him/her to act, respond, and ultimately learn differently.” This idea has important implications [because] it means that each instructional experience is unique for each learner.… With that said, it seems like an impossible task to expect learners to show the same understanding on standardized testing.

Next, referring to the computed emotional results, we qualitatively investigated emotions that might be reflected in posted messages. In the course, students were encouraged to provide constructive criticism along with positive comments. We found that group 1 tended to provide balanced feedback in accordance with the instructions. The machine learning tool interpreted praise as positive emotion and criticism as negative emotion. The following posting from Meggie shows a balance of positive and negative emotions:

[I agree] that the search for one absolute theory on learning is fruitless, but the search in itself is human nature. As human beings, we are constantly [bombarded with] so many stimuli that we [must simplify] them in our effort to make meaning. All of our mental representations are inherently simplifications. Theories are [just another way] humans try to simplify patterns we see in the world. In themselves, [I do not think] they are [a bad thing] – as long as they [are not followed] dogmatically. If they are used in combination, as many here have suggested, I think [it is possible] to find a combination of simplifications that helps instructors make learner-focused decisions.

Students in group 1 also expressed negative emotions frequently. Effortful participation in online conversations might be more likely to occur for students who are serious-minded about the tasks. The critical reflections of these students on their work and ability included difficulties and feelings of frustration they experienced. While such critical self-evaluation could be interpreted as negative emotion, it could also signify positive changes in participation. Previous findings highlight the positive role of negative feelings (Artino & Jones, 2012; Cassady & Johnson, 2002). The following posting from Sara exemplifies the nature of negative emotions experienced by quality participants:

[I am sad] to say I am one of the sad statistics of new teachers leaving the profession within five years. My main reason for leaving, like you, was the bureaucratic policy decisions that [frustrated] my attempts to best meet my students' needs. I [loved] my students, but I [did not love] the proverbial hoops and red tape.

…The way standards are written, or at least the way administration interprets them, largely pressure teachers to implement all the best practices, with all available technology, all the time, for all students. [That simply ISN'T best practice]. [It actually limits] teachers' abilities to personalize instruction for their particular students.

In contrast, Jesse and Susan from group 2 showed different cognitive and emotional states. With regard to cognitive presence, they often produced messages associated with the triggering event (the lowest cognitive presence). Rather than suggesting diverse information, developing and integrating ideas, they tended to post questions or shift to a new direction:

Can we suggest that people develop differently based on different varying psychosocial crises, then turn around and suggest that these different people would not learn best in different ways?—Jesse (asking questions).

That's actually a great idea, and I think it's perfectly doable. [I wonder if] the sudden upswing in gamification and rewards is tied to the fact that the majority of working people and students have grown up playing video games. It's a reward system we recognize.—Susan (taking the discussion in a new direction).

Student emotions in group 2 were mostly positive. They often praised and restated what peers posted but added few meaningful criticisms, trends that might have been read as positive emotion. Counterintuitively, previous findings suggest that positive emotions had no significant effect on student engagement and achievement (Pekrun, 2006; Sansone, Smith, Thoman, & MacNamara, 2012). For example, according to Pekrun (2006), “positive achievement emotions do not always exert positive effects” (p. 327). Positive emotions, insufficient participation, and relatively lower achievement were all observed characteristics in group 2, as exemplified in the following posting from Jesse:

[Very astute.] I believe there is a lot of subjectivity here as you pointed out with your fitness example. I know personally [that] I do next to zero evaluation of my training worthiness once it has been deployed. At that point, I am on to the next project. And [I agree] that you would almost need a full-time person to accomplish that monumental task.

8. Discussion
We proposed a measurable model of learner engagement during group interaction in an asynchronous discussion setting and explored the utility value of the model through the main study (Study 1), in which we proposed a computational model of learner participation level, and a supplementary case study (Study 2), which further validated the method in a specific context.

9. Findings
In Study 1, we defined three distinct levels of learner participation during group interaction (from most to least engaged): full, inbound, and peripheral participant. Based on the idea that a network of learner interaction in online discourse will feature different levels of participation (Palonen & Hakkarainen, 2013), we used SNA to quantify individual differences in participation. Because no consensus on which SNA metrics are the most accurate and reliable exists, we explored metrics that were most frequently used in previous studies: in-degree, out-degree, betweenness, and closeness centrality. We first included in-degree and out-degree centralities, which corresponded to popularity and influence, respectively. For mediation, we used betweenness centrality because it was constantly associated with the other two centralities. Moreover, we considered betweenness to be closer in meaning to the concept of mediation. Following the suggestion that studies should consider the multifaceted nature of learner relationships in a collaborative community (Cho et al., 2007; Marcos-García et al., 2015), we combined three metrics (i.e., in-degree, out-degree, and betweenness centrality) to characterize the participation of each student during group interaction.

Filtering using a 75th percentile threshold effectively identified students who belonged to each respective profile. The model of learner participation revealed noticeable differences between the profiles in in-degree, out-degree, and betweenness centrality. Subsequent hierarchical cluster analysis confirmed that three clusters existed in the data. We then performed K-means cluster analysis. ANOVAs including subsequent post-hoc comparisons revealed that the computational learner profiling method we proposed distinguished the three levels better than the K-means method.

Subsequent qualitative analysis showed that upper-level students exerted more cognitive effort than lower-level students. This result supports our assumption that the profiles are associated with message quality. In addition, the ANCOVA tests indicate that participation level had a tangible connection to final scores (e.g., the highest averages belonged to full participants). This finding is consistent with previous studies. For example, Cho et al. (2007) found that students who had strategically advantageous positions outperformed their peers. The findings of the current study suggest that encouraging students (a) to exchange comments and (b) to facilitate conversations among peers can help them perform better.

We conducted Study 2 as an empirical case of the proposed profiling method. This supplementary study further demonstrated the validity and utility value of the method. We deployed the profiling method to address a specific instructional need of an online course. Within that context, we re-validated the compuational approach by compairing the resulant classifications to levels of cognitive, behavioral, emotional engagement.

The results suggest that the quality participants identified by the profiling method showed higher engagement in cogntion, behavior, and emotion. Also, their final scores were higher than their counterparts. For example, the quality participants demonstrated frequent negative emotions along with positive emotions. This finding indicates that those students often created posts that combined positive comments with constructive criticism, contributing to their successful learning (Artino & Jones, 2012; Marchand & Gutierrez, 2012; Pekrun, 2006).

Subsequent trends analysis clearly depicted differences between the quality participants and their counterparts. The trends imply that the learner participation profiles might also be an effective means of tracking learner changes over time. Tracking participation patterns over time is crucial to determining whether students are developing (Carr, Cox, Eden, & Hanslo, 2004; Prinsen et al., 2007). In addition, the qualitative reviews of student messages fleshed out how the quality participants presented their higher-order cognition and their positive and negative emotions.

10. Implications
The findings of this study advance theory, methodology, and pedagogical application associated with asynchronous CSCL environments. The results of this study support two ideas: (a) learning and performance stem from social and communicative interaction in a learning community (Bandura, 1991; Brown & Duguid, 1991; Lave & Wenger, 1991), and (b) learning is a process of becoming an insider of a learning community (Bernard et al., 2009; Lave & Wenger, 1991; Wenger, 2000; Woo & Reeves, 2007). One possible interpretation is that preparing critiques requires students to look at the work of others in advance. Such investigation entails a vicarious learning experience that helps students discern their own strengths and weaknesses (Grabe & Kaplan, 1996; Lin & Chien, 2009; Pajares, 2003). Receiving meaningful peer feedback enables students to reflect on their outcomes, make corrections, and favorably respond to commenters (Straub, 1997). Such positive experiences could encourge students to join discussions with other peers and actively share what they have learned. All of these steps help students develop target knowledge and improve higher-order thinking skills, potentially resulting in higher scores (Bruning & Horn, 2000).

Concerning methodology, CSCL research has a strong interest in computational models that externally represent and analyze engagement in collaborative learning at both group and individual levels (Dado et al., 2017; Desmarais & Baker, 2012; Jeong et al., 2014). We proposed a conceptual model of learner participation during group interaction, along with data-analytic methods (i.e., SNA) to determine individual profiles. More specifically, we used a fine-grain set of objective data to represent individual engagement behaviors, in part to address a concern expressed in Henrie, Halverson, and Graham (2015) that a majority of studies have measured student participation using non-objective methods, such as self-report surveys and interviews. The validated model of learner participation suggests that we can empirically detect learner trajectories across the three levels. This progression model can be used to monitor the development of participation in collaborative learning over a period of time.

In terms of pedagogical application, the measurable model of learner engagement could guide the development of an analytics-ready system that notifies instructors and students about engagement level. Given data-rich engagement information, students could self-regulate their knowledge and skills as they progress through levels of engagement in asynchronous CSCL environments. For example, using SNA and the learner participation profiling method, we can design a visual participation feedback (VPF) system that could visualize participation in the form of a learner network, a learner participation table, and individualized feedback messages (Janssen et al., 2011; Jin, 2017). This VPF could be distributed to students via learning management systems.

As our results show, students who are typically more passive might benefit from this kind of online setting (Asterhan & Eisenmann, 2011; Ng, Cheung, & Hew, 2010). Enhanced knowledge about student behavior is critical for instructors who want to stimulate and sustain productive student interaction. The extent to which instructors regulate and structure online discussions can impact student participation levels (Bowen, 2012; Hsieh & Tsai, 2012; Thomas, 2013). For example, an instructor might build small heterogeneous groups of students that demonstrated diverse levels of engagement in previous collaborative tasks (Wang, Lin, & Sun, 2007). Also, by detecting at-risk students (e.g., students who remain marginal participants over an extended period of time), instructors can provide customized instruction in order to encourage deeper engagement.

11. Limitations and suggestions
The findings of this study open pathways to future research. To start, this study is the first to test a model of learner participation. Caution is advised in generalizing the findings, and further study is required to test other conditions. More specifically, the current study focused on a particular CSCL context in which students performed a variety of problem tasks relevant to their academic domains with the support of peer critique. Different CSCL problem situations are likely to generate different learner dynamics and structures. Presumably, learner participation profiles are applicable across various CSCL contexts, yet appropriate decision criteria might vary. For example, the current study used a 75th percentile threshold. Even though this threshold effectively classified learner profiles, other types of thresholds (e.g., top-ranked k or more conservative percentiles) should be explored in various contexts in order to establish an appropriate one. Hence, the implications of this study are tentative.

Second, many unanswered questions remain about the mechanisms and nuances of learner interaction in an emergent learner network. For example, different learner participation levels can be attributed to individual differences. Considering that CSCL requires a great deal of reading and writing on screen, English-language learners (ELLs), who often come from non-English-speaking homes, spend more time preparing for a discussion and write fewer posts than native speakers of English (Prinsen et al., 2007). English language proficiency could be a critical determinant of the behavioral patterns of ELLs during group interaction, possibly resulting in lower performance. In contrast, for ELLs who are slow to interact but have outstanding learning ability, their relative positions in a learner network might not indicate their actual performance levels.

Third, scholars should consider controlling for more potential covariates. In this study, we investigated whether the profile, as a categorical variable, differentiated final scores by controlling the variation attributed to the covariate: the WP2 score. WP2 scores significantly interfered with the effect of the profiles on final scores; however, because WP2 scores were measured in period 3, they might not have sufficiently reflected baseline ability (i.e., prior knowledge and skills in writing). For more rigorous testing, researchers should consider potential covariates when designing a study (e.g., SAT language scores).

Last, the current study investigated individual levels of participation in the context of whole class conversations. Different group sizes could engender different sets of profiles and transitions. For instance, students in small groups are likely to communicate with one another more frequently from the beginning of a course than students in a group with all of their classmates. Peripheral participants would likely be rare in small groups, and students would likely move faster toward full participation. In contrast, if an individual student dominated communication and information distribution in a small group, only two profiles would be likely to emerge: full participant and peripheral participant. To address these possibilities, scholars need to investigate alternative models in various online collaboration settings.

12. Closing thoughts
The current study is unique in that the proposed model and associated detection methods consider the multi-faceted nature of learner interaction and thus differentiate student participation within a theoretically justified framework. The learner participation levels can be identified by analyzing emergent learner networks and the metrics obtained from SNA. The suggested model and associated methods for detecting levels of learner participation can help teachers (a) understand individual student participation levels in CSCL and (b) provide instructional support and feedback tailored to individual students or groups. Thus, scholars should also investigate instructional and feedback strategies associated with each level of learner participation, along with general strategies for CSCL. In particular, insufficient participation levels could be detected earlier, and at-risk students could be shown how to benefit more from collaborative learning.

Acknowledgements
The study reported in this paper is based on the work in “How People Develop Learning Leadership: Learner Characteristics, Leadership Style, and the Dynamics of Asynchronous Online Learning” supported by Spencer Foundation (#201900017).

Appendix A. Schedule of wiki-enhanced writing course

Period	Week	Key activity	Assignment	Place	Data collection
Wiki	F2F
P1	WK1	Orientation to the Wiki			〇	Pre-survey
WK2	Create professional profiles on the Wiki		〇		
Explore recommended resources (RR)	RR	〇		
WK3	Emphasis proposal workshop			〇	
Emphasis conference			〇	
P2	WK4	Post RR on the Wiki	RR	〇		
WK5	Peer feedback on RR postings	RR	〇		
Instructor's feedback on RR postings		〇		
WK6	Prepare WP1P1 (ver. 1)	WP1P1	〇		
WK7	Post WP1P1 (ver. 1) on the Wiki	WP1P1	〇		
WP1P1 workshop	WP1P1		〇	
Peer feedback on the Wiki		〇		
P3	WK8	Post WP1P1 (ver. 2) on the Wiki	WP1P1	〇		
WP1P1 (ver. 2) presentation	WP1P1		〇	
WK9	WP1P2 proposal workshop			〇	
WK10	Spring Break (No class)
WK11	Post WP2 on the Wiki	WP2	〇		
Present WP2 in the class	WP2		〇	
Peer feedback on the Wiki	WP2	〇		
WK12	Post WP1P2 (ver. 1) on the Wiki	WP1P2	〇		
WP1P2 workshop	WP1P2		〇	
Peer feedback on the Wiki	WP1P2	〇		
Instructor's feedback on WP2	WP2	〇		
Grade WP2	WP2			WP2
P4	WK13	Post WP1P2 (ver. 2) on the Wiki	WP1P2	〇		
Post WP1P1 (ver. 3) on the Wiki	WP1P1	〇		
WK14	Post WP1P1 (ver. 4) on the Wiki	WP1P1			
Grade WP1P1 (ver. 4)	WP1P1			WP1P1
Post WP1P2 (ver. 3) on the Wiki	WP1P2	〇		
WK15	Post WP1P2 (ver. 4) on the Wiki	WP1P2	〇		
WP1P2 Conference	WP1P2		〇	
Grade WP1P2 (ver. 4)	WP1P2			WP1P2
WK16	Prepare writing portfolio				Post-Survey
Final grade				
Note. WP1P1: the first posting for writing project 1, WP1P2: the second posting for writing project 1, WP2: writing project 2.

Appendix B. Coding scheme for message quality

Code/keyword	Definition	Excerpts
Evaluation	Statements between online learners that judge the accuracy, comprehensiveness, relevance, or other aspects of an end project or its components; learner awareness of what they do not know or understand; comments on either good or bad features of the writing.	“I thought this was a very interesting post and I was intrigued by your combination of marketing and psychology.”
“In terms of content, I think that my paragraph on artistic freedom is needlessly long; it could be shortened and applied specifically to freedom in film.”
Clarification	Seeking, offering, or providing clarification between online learners; probing for explanations and justifications; drawing attention of other online learners to difficulties that might interfere with task completion or other outcomes.	“Why the first line, give birth to a baby daughter and the mum said baby sister or brother?”
“Why might one say that the technology is not cost-effective, and are there any examples of a technology proving overall a poor asset?”
“As far as your thesis goes, I am not exactly sure what you are looking to contribute for ‘value added.’”
Suggestion	Seeking, offering, or providing a direction for change; comments relating to a plan for improvement.	“In the thesis, if possible, I would try, in part, to describe the solution you propose in your conclusion.”
“I think that in your thesis you should address the point you come to in your conclusion. Essentially, this will make your argument very clear.”
“I would like to see more of the ‘although’ argument. Why might one say that the technology is not cost-effective, and are there any examples of a technology proving overall a poor asset?”
“For example, you spend a long time discussing the founding of positive psychology, in particular the meeting of the various scholars, but do precisely define what positive psychology is.”
Alteration	Seeking, offering, or providing specific changes in organization of the content or changes in the form such as wording, grammar, and punctuation	“The main issues I noticed were some instances of informal language use, use of contractions, and some grammatical and punctuation errors.”
“The use of hyperlinks will allow the reader to gain a better understanding of the topic being discussed.”
Appendix C. Coding scheme for cognitive presence

Code/keyword	Indicators/ socio-cognitive processes	Excerpts
Triggering event (evocative)	Recognizing the problem: Presenting background information that culminates in a question.
Sense of puzzlement: Asking questions
Messages that take discussion in a new direction	“I found it interesting in the reading that much of technology failures had some commonality. First, … [Do teachers really wield that much influence over what technology the classroom may or may not utilize?]”
“That's actually a great idea, and I think it's perfectly doable. [I wonder if] the sudden upswing in gamification and rewards is tied to the fact that the majority of working people and students have grown up playing video games.”
Exploration (inquisitive)	Divergence (among group members): Unsubstantiated contradiction of previous ideas
Divergence (within a message): Many different ideas/themes presented in one message
Information exchange: Personal narratives/descriptions/facts (not used as evidence to support a conclusion)
Suggestions for consideration: message explicitly characterized as exploration (e.g., Does that seem about right? Am I way off the mark?)
Brainstorming: Adds to established points but does not systematically defend/justify/develop addition
Leaps to conclusions: offers unsupported opinions	“[First,] there are many, many teachers, so even if a few teachers take.... [Second,] the education sector typically has a short attention.... So, perhaps it isn't that teachers yield so much power, but rather if a new instructional practice/technology doesn't yield quick results, tax payers and local governments will start looking to new ideas. It's more reactive than proactive.”
“I honestly [think] the worst type of e-learning [I've experienced] was administered through professional development. Our administrators tried to create learning paths about reading; however, I just didn't find it beneficial. Usually, e-learning provides demos and models that I can visually see, but this one simply didn't. Now perhaps it would've been better if it weren't conducted after school but that was definitely the worst e-learning I've ever experienced.”
Integration (tentative)	Convergence (among group members): Reference to previous message followed by substantiated agreement, e.g., “I agree because”; Building on, adding to others' ideas
Convergence (within a message): Justified, developed, defensible, yet tentative hypothesis
Connecting ideas, synthesis: Integrating information from various sources: textbook, articles, personal experience
Creating solutions: Explicit characterization of message as a solution by participant	“I, [like the others who have commented here], [agree with your point.] Inversely, schools are wary of implementing some technology because they worry they won't see the effects for a long time. [Tied into this are Spector's comments] on integration”
“I appreciate the cartoons.... [I agree with your argument] that, ‘each person is an individual with unique mental models that cause him/her to act, respond, and ultimately learn differently.’ This idea has [important implications because] it means that each instructional experience is unique for each learner.”
Resolution (committed)	“Vicarious” application to real world
“Testing” solutions
Defending “solutions”	No case identified
Other	No relation to cognitive presence	
