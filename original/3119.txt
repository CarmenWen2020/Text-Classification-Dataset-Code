For service implementations to be widely adopted, it is necessary for the expectations of the key stakeholders to be considered. Failure to do so may lead to services reflecting ideological gaps, which will inadvertently create dissatisfaction among its users. Learning analytics research has begun to recognise the importance of understanding the student perspective towards the services that could be potentially offered; however, student engagement remains low. Furthermore, there has been no attempt to explore whether students can be segmented into different groups based on their expectations towards learning analytics services. In doing so, it allows for a greater understanding of what is and is not expected from learning analytics services within a sample of students. The current exploratory work addresses this limitation by using the three-step approach to latent class analysis to understand whether student expectations of learning analytics services can clearly be segmented, using self-report data obtained from a sample of students at an Open University in the Netherlands. The findings show that student expectations regarding ethical and privacy elements of a learning analytics service are consistent across all groups; however, those expectations of service features are quite variable. These results are discussed in relation to previous work on student stakeholder perspectives, policy development, and the European General Data Protection Regulation (GDPR).

Previous
Next 
Keywords
Learning analytics

Student expectations

Higher education

Individual differences

Human factors

1. Introduction
Higher education institutions are collecting an unprecedented amount of data, from logs captured by the institutional virtual learning environment to library access frequency (Sclater, Peasgood, & Mullan, 2016). Behind these actions there is a belief that a better understanding of the student learning progress through the analyses undertaken, resulting in interventions designed to improve teaching and learning (Dawson, Joksimovic, Poquet, & Siemens, 2019; Gašević, Kovanović, & Joksimović, 2017; Siemens, 2013). This use of learning analytics is primarily motivated by a drive to address the limited learning support and low retention rates that are key performance indicators of higher education (Sclater et al., 2016; Siemens & Long, 2011; Tsai & Gašević, 2017b).

The advantages that learning analytics services can bring to higher education have been recognised by numerous institutions, but adoption rates remain low (Tsai et al., 2020; Tsai & Gašević, 2017a; Viberg, Hatakka, Bälter, & Mavroudi, 2018). Nevertheless, institutions recognise that successful implementation of learning analytics services requires student engagement (Buckingham Shum, Ferguson, & Martinez-Maldonado, 2019; Ferguson et al., 2014; Jivet et al., 2020; Tsai and Gašević, 2016, Tsai and Gašević, 2017b; Tsai, Moreno-Marcos, Tammets, Kollom, & Gašević, 2018). As without gauging and understanding what students expect from learning analytics, future services will inadvertently create an ideological gap (Ng & Forbes, 2009; Whitelock-Wainwright, Gašević, & Tejeiro, 2017). This is where the service offered is a reflection of management needs, but not what students expect (Ng & Forbes, 2009). Dissatisfaction becomes a likely outcome that occurs in these instances where expectations of the primary stakeholder are not met (Ng & Forbes, 2009). To offset this possibility of students being dissatisfied with learning analytics, researchers have begun to explore student expectations of such services (Ifenthaler & Schumacher, 2016; Roberts, Howell, & Seaman, 2017; Roberts, Howell, Seaman, & Gibson, 2016; Schumacher & Ifenthaler, 2018; Slade & Prinsloo, 2014; Tsai, Perrotta, & Gašević, 2020; Tsai, Whitelock-Wainwright, & Gašević, 2020). The studies cited above suggest that students expect a learning analytics service that facilitates self-regulated learning (Lim et al., 2020; Roberts et al., 2016; Schumacher & Ifenthaler, 2018) and promotes learner agency (Roberts et al., 2016; Tsai, Perrotta, & Gašević, 2020). The existing studies also showed that students are open to the idea of their data being used for these purposes (Fisher, Valenzuela, & Whale, 2014) provided their privacy is protected (Ifenthaler & Schumacher, 2016; Slade & Prinsloo, 2014; Tsai, Whitelock-Wainwright, & Gašević, 2020) and informed consent obtained in advance (Slade & Prinsloo, 2014; Sun, Mhaidli, Watel, Brooks, & Schaub, 2019) as now mandated by the General Data Protection Regulation1 (GDPR) in the European Union (Sclater, 2017).

Student expectations towards learning analytics services should not be assumed to be homogenous across the population; rather there is likely to be a degree of heterogeneity (Authors, 2020). This heterogeneity assumption is support by several accounts. First, the literature demonstrates that students who have just commenced their studies in higher education require more direction and support from higher education institutions to become independent learners than students who have already spent some time in higher education institutions (Thomas, Hockings, Ottaway, & Jones, 2015). Second, mature students often depended on family and friends as the main sources of support in higher education with limited expectation for institutional support (Heagney & Benson, 2017). Third, learning analytic services can be considered as a form of feedback (Matcha, Uzir, Gašević, & Pardo, 2020). However, providing regular feedback may not be necessary for all students given the development of their skills to monitor and control (i.e., self-regulate) their learning (van de Pol et al., 2010; Winne, 2017). Finally, demographic and academic information of students can play a significant role in types of supports students may need from higher education institutions (Gašević, Dawson, Rogers, & Gasevic, 2016). Common examples of such information used in the learning analytics literature are subject matter (Gašević et al., 2016; Morris & Finnegan, 2009), student origin (e.g., domestic vs international) (Dawson, Jovanović, Gašević, & Pardo, 2017; Gašević et al., 2016; Tempelaar, Rienties, & Nguyen, 2017), and genders of the students. Consideration of subject matter is predicated on the assumption of different pedagogies followed across different subject domains (Gašević et al., 2016; Morris & Finnegan, 2009). Consideration of student origin is justified by student differences in academic background (Tempelaar et al., 2017) or in needs for support services (Glew et al., 2019) of domestic students in comparison to international students. Gender is commonly used in education literature to study differences in academic characteristics and outcomes (Sheard, 2009).

Obliged to act based on the results of learning analytics and who is responsible for student success by acting upon learning analytics are essential issues that need to be carefully considered in connection with student expectations. Some institutions such as Nottingham Trent University implemented a mandatory learning analytics service as part of their strategy that rests on the assumption that the university is obliged to act based on learning analytics for the sake of student support (Nottingham Trent University, 2016; Sclater et al., 2016). In many institutions however teaching staff say they are often seen as responsible for student success and thus will be tasks to communicate the results of learning analytics with students (Howell, Roberts, Seaman, & Gibson, 2018). Related research in learning analytics however suggests that institutions and students should share the responsibility for student learning (Prinsloo & Slade, 2017), which is well aligned with the objectives for students to become independent learners (Thomas et al., 2015) who can control their own learning (van de Pol et al., 2010). As not all learners can be ready to independently learning immediately upon commencement of their studies, it is essential for higher education institutions to understand needs and expectations of different student subpopulations for effective implementation of a learning analytics service. Unfortunately, the existing literature has offered limited empirical accounts about student subpopulations and their different expectations of learning analytics services.

The goal of this paper is to therefore address this current gap by exploring the heterogeneity found in student expectations of learning analytics services using latent class analysis.

1.1. Definition of stakeholder expectations
Adoption of information systems has been extensively studied (Davis, 1989; Venkatesh & Bala, 2008; Venkatesh, Morris, Davis, & Davis, 2003), with particular emphasis on beliefs in the post-adoption phase (i.e., once the information system has been implemented). Even though this work has been fundamental in understanding the complexity of introducing new information systems, the importance of pre-adoption beliefs cannot be ignored (Karahanna, Straub, & Chervany, 1999). As early work by Davis and Venkatesh (2004) shows, expectations of an information system (i.e., pre-adoption beliefs) are valid predictors of actual system usage. More recently, Venkatesh and colleagues have demonstrated the importance of measuring user expectations of information systems, particularly in relation to technology use (Brown et al., 2012, Brown et al., 2014; Venkatesh & Goyal, 2010). The practical implication from this work has been the importance for management to ensure that user expectations of information systems are at a realistic level.

When information systems do fail, it can be attributed to an organisation being unable to provide a service that aligns with stakeholder expectations (Lyytinen & Hirschheim, 1988). Possible ways in which management can avoid services falling short of stakeholder expectations have previously been discussed (Brown et al., 2012, Brown et al., 2014; Davis & Venkatesh, 2004; Venkatesh & Goyal, 2010), with particular emphasis placed on strategies to be undertaken in the pre-implementation stages of development (Boonstra, Boddy, & Bell, 2008; Ginzberg, 1981; Jackson & Fearon, 2014). In the case of Davis and Venkatesh (2004), they highlight the importance of gauging stakeholder expectations early in the design process as a way of understanding attitudes towards the system in development. Likewise, Jackson and Fearon (2014) emphasise the importance of management taking a proactive stance in understanding stakeholder expectations, but also adopting approaches that avoid creating inflated expectations. In other words, if stakeholders can formulate realistic expectations towards the information system, it can mitigate against large discrepancies between beliefs and experience that are attributable to dissatisfaction (Brown et al., 2012, Brown et al., 2014; Venkatesh & Goyal, 2010).

In this study, we follow the definition of expectations that has previously been used in the learning analytics literature (Authors, 2019). Specifically, an expectation is defined as a belief about “the perceived likelihood that a product possesses a certain characteristic or attribute, or will lead to a particular event or outcome” (Olson & Dover, 1976, p. 169). Accordingly, an expectation in learning are defined as “as a belief about the likelihood that future implementation and running of learning analytics services will possess certain features” (Whitelock‐Wainwright et al., 2019, p. xx). For example, a learner's perceived likelihood that LA service will provide feedback on how the learner is progressing towards their set goals. Expectations are significance for human cognition (Roese & Sherman, 2007). Expectations influence how an individual manages their behaviours and motivation in a particular context (Bandura, 1977, Bandura, 1982; Elliot & Church, 1997). Expectations are also used as reference points for assessing our actual experiences (Christiaens, Verhaeghe, & Bracke, 2008; Festinger, 1957) and for estimating how hard some expectations can change (Ngafeeson & Midha, 2014; Nov & Ye, 2008). According to Thompson and Suñol (1995), expectations can be divided into four types: ideal, predicted, normative, and unformed. An ideal expectation represent wanted outcomes, or what an individual wants would like to have in a service (Leung, Silvius, Pimlott, Dalziel, & Drummond, 2009). A predicted (or realistic) expectation what an individual realistically expects the service is the most likely to be. Indeed, the existing literature supports the distinction between ideal and realistic expectations as two different subtypes (Askari, Liss, Erchull, Staebell, & Axelson, 2010; David, Montgomery, Stan, DiLorenzo, & Erblich, 2004; Dowling & Rickwood, 2016). The other two expectation subtypes are about what service an individual believes is justified to have (normative expectation) and the situations where the individual is unable to form expectations from a service (unformed expectations). In learning analytics, predicted and ideal expectations have been studied as this distinction allows the researchers and practitioners to gauge what students realistically expect from learning analytics services (predicted expectations), whilst also being mindful of what students' desire (ideal expectations). This distinction between predicted and realistic expectations is used as a foundation of the construction of the Student Expectations from Learning Analytics Questionnaire (SELAQ) (Whitelock‐Wainwright et al., 2019), which has been used in the analysis of several higher education institutions in Europe and Latin America (Hilliger et al., 2020a; Kollom et al., 2021).

1.2. Prior studies on stakeholder expectations of learning analytics
The abovementioned literature highlights the importance of gauging stakeholder expectations and this resonates with the implementation of learning analytics services, specifically with regards to future adoption. A recent survey shows that many Higher Education Institutions in Europe can be considered as being within the early stages of learning analytics service implementations (Tsai, Rates, et al., 2020). This effectively equates to the pre-implementation stages of information system development, as these institutions have no learning analytics service in place, but have plans for such services in the future. It is at this point where stakeholders should be involved in design and implementation decisions to either align the service with their expectations or mitigate against inflated expectations (Buckingham Shum et al., 2019; Jackson & Fearon, 2014). In the context of developing learning analytics services, however, it has been reported that the level of engagement from stakeholders has been unequal (Tsai & Gašević, 2017b). A pertinent example of limited engagement with stakeholders, particularly students, has been the development of the learning analytics code of practice (Sclater, 2016). Included in this code of practice is the theme that learning analytics services should be used to benefit students, input from students came from a single representative of the National Union of Students. Even though Sclater's (2016) code of practice has an important role in regulating institutional learning analytics services, it may lead to the creation of learning analytics services that are not reflective of student expectations (Whitelock-Wainwright et al., 2017). When a service is not in alignment with stakeholder expectations, this is known as an ideological gap and is associated with user dissatisfaction (Ng & Forbes, 2009).

It would be incorrect to state that learning analytics research has neglected the importance of understanding student beliefs towards possible learning analytics services. There have been developments in understanding student expectations towards learning analytics service features (Arnold & Sclater, 2017;Jivet et al., 2020 ; Roberts et al., 2017 ; Schumacher & Ifenthaler, 2017) and student beliefs towards ethical procedures (Ifenthaler & Schumacher, 2016; Jones et al., 2020; Roberts et al., 2016; Slade & Prinsloo, 2014; Slade, Prinsloo, & Khalil, 2019; Tsai, Whitelock-Wainwright, & Gašević, 2020). Across each of these studies, the authors have shown that the beliefs held by students cannot be overlooked. Moreover, they provide a valuable perspective from those whose data will eventually be used in learning analytics services, which cannot be addressed from focusing on the views of management alone. Nevertheless, gauging student expectations of learning analytics services is not an easy feat, particularly on account of the population size, which is a concern in information system implementations (Lyytinen & Hirschheim, 1988). While qualitative work has provided rich description of student beliefs towards learning analytics services (Roberts et al., 2016, Roberts et al., 2017; Slade & Prinsloo, 2014), these tend to focus on relatively small samples. In information systems research, Szajna and Scamell (1993) have previously encouraged the development of psychometric instruments to gauge stakeholder expectations at different stages of implementations, which also offers a solution to exploring learning analytics service beliefs on a larger scale. Such instrument has recently been developed and validated to measure expectations of students from learning analytics services (Authors, 2019) as outlined in Section 2.2. While this instrument has been used across several international contexts (Whitelock‐Wainwright et al., 2020; Hilliger et al., 2020b), there has not been any prior research that attempted to understand if student expectations for learning analytics services can be meaningful segmented to enable higher education institutions to serve to the specific expectations of different student subpopulations.

1.3. Segmenting stakeholder expectations
Gauging student expectations of learning analytics services offers institutions the possibility of offering a service that meets student expectations, or the chance to manage inflated expectations. Although progress has been made to explore student expectations of potential learning analytics services (Jivet et al., 2020; Roberts et al., 2017; Schumacher & Ifenthaler, 2017), emphasis has been placed on viewing these beliefs on the population level. While the findings of this work have been important in emphasising the need to accommodate the student perspective in learning analytics service implementations, it cannot be assumed that all students hold similar expectations (Roberts et al., 2017; Teasley, 2017).

Expectations-based segmentation has been shown to be a useful approach in understanding what users want from a service (Diaz-Martin, Iglesias, Vazquez, & Ruiz, 2000). In doing so, it offers service providers with an opportunity to tailor a service to meet the expectations the user holds, which should increase satisfaction (Diaz-Martin et al., 2000; Webster, 1989). This approach has been applied in a Higher Education Institute where Blasco and Saura (2006) segmented students based on their expectations towards elements of the service offered by a university (e.g., faculty members' level of theoretical knowledge). According to Blasco and Saura (2006), the ability to segment students by their service expectations can facilitate changes to policies that regulate the service in place. Thus, if the service provider can identify and effectively align the service with these differences in expectations, greater levels of satisfaction with the service are likely to result.

Given the value that expectation-based segmentation could have in providing a learning analytics service that aligns well with student expectations, the current study sought to answer four research questions:

RQ1

Can students be meaningfully segmented on the basis of their ideal expectations of learning analytics services?

RQ2

Can students be meaningfully segmented on the basis of their predicted expectations of learning analytics services?

RQ3

If students can be meaningfully segmented on the basis of their ideal and predicted expectations, what covariates predict their assignment to a particular class?

RQ4

Are the class assignments given to students stable or variable across the ideal and predicted expectation scales?

The focus of these research questions is on the exploration of whether expectations towards learning analytics services are homogenous within a sample of students. It is motivated by the view that not all students will hold the same expectations towards learning analytics and in order to maximise uptake, we need to understand whether a one size fits all solution is a viable solution. To this end, we conducted a latent class analysis to segment respondents based on their expectations, and further explored the demographic characteristics of the groups identified.

2. Method
2.1. Sample
The survey was distributed to the entire student population (~16,000) at the Open University of the Netherlands and a total of 1247 responses (Females = 705, 57%) to the SELAQ were collected. Seven respondents provided incorrect age details (e.g., 0, 99, and 251) or omitted these details entirely. As the analysis required the data to contain no missing values, these seven respondents were omitted; the following sample descriptive statistics pertain to the 1240 respondents (Females = 700, 56%).

Of the remaining 1240 respondents who did provide accurate age details, their ages ranged from 18 to 82 years of age (Mage = 44.81, SD = 12.14). The average age of the sample is in line with the student population, who are typically older adults seeking to develop skills during their career. The three faculties that make up the university were almost equally represented in this sample: 33% (n = 411) were students of culture and jurisprudence, 33% (n = 413) were students of management, science, and technology, and 34% (n = 416) were students of psychology and education. Majority of the sample were composed of undergraduate students (n = 790, 64%) and masters students (n = 447, 36%); PhD students only accounted for 0.002% of the sample (n = 3). Due to the sample only being composed of small number of PhD students, they were grouped with the master students to form a postgraduate category (n = 450, 36%). Finally, majority of the respondents identified themselves as being from the Dutch students (n = 1119, 90%), whilst only a small number of respondents stated they were either European students (i.e., students who were from other European countries other than the Netherlands, n = 106, 9%) or Overseas students (n = 15, 1%). Given the small number of students who identified themselves as Overseas, any findings should be interpreted with caution.

2.2. Instrument
To measure student expectations of learning analytics, the SELAQ was used. We have developed and validated SELAQ (Whitelock‐Wainwright et al., 2019) to assist higher education institutions in their pursuit of implementing learning analytics services and to increase stakeholder engagement. The purpose of this instrument is not to replace qualitative explorations of student expectations, but as a method to accommodate a greater number of student beliefs into learning analytics service implementations. Thus, whilst the SELAQ can provide institutions with a general understanding of what a large number of students expect of learning analytics services, qualitative methods can be in conjunction to obtain detailed insights into student beliefs.

The SELAQ has been presented as providing researchers with a means of obtaining valid measures of student expectations towards learning analytics services (Authors, 2019). Items cover various service features that have previously been discussed in the literature, specifically on self-regulated learning (Lim et al., 2020; Roberts et al., 2016; Schumacher & Ifenthaler, 2018) and linkages with the learning sciences (Marzouk et al., 2016). However, there has yet to be an attempt at utilising the collected SELAQ data to provide a detailed exploration of how expectations of learning analytics service may vary within the student population. Given the importance of gauging and managing expectations early on in the implementation of information systems (Brown et al., 2012, Brown et al., 2014; Jackson & Fearon, 2014; Venkatesh & Goyal, 2010), there is a need for institutions to proactively engage in such behaviours before learning analytics services are implemented. On this basis, the current research seeks to present an exploratory study of how the SELAQ can be used to understand student expectations (ideal and predicted) of future learning analytics services.

The questionnaire hosted on the Qualtrics platform; an invitation to participate was distributed to all students at the university. The questionnaire itself contains 12 items (Table 1), five of which account for Ethical and Privacy Expectations (EP1 to EP5) and seven refer to Service Expectations (S1 to S7). Responses to each item are made on two scales using seven-point Likert scales (1 = Strongly Disagree, 7 = Strongly Agree). Development of these items required careful consideration of the limited applications of learning analytics in higher education; thus, items are phrased quite generally to maximise participant understanding (Authors, 2019). The two response scales correspond to ideal (Ideally, I would like this to happen) and predicted expectations (In reality, I expect this to happen). Ideal expectations measures what students desire from a learning analytics service, whilst predicted expectations measure the learning analytics service student expect in reality. As a result, it has been shown that response distributions on the ideal expectation scale elicit stronger responses than the predicted expectation scale (Authors, 2019, 2020; see supplementary material). Prior work developing and validating the SELAQ has shown the scales to be reliable and valid (Authors, 2019, 2020). Initial development of the scale was carried out across three samples of higher education students in the United Kingdom, wherein survey responses and item feedback were used to refine the number of items and the item wordings. Both scales (ideal and predicted) were found to have good composite reliability values (0.94 and 0.95, respectively). Since then, this scale has been translated and validated to be used in the Netherlands and Spain (Authors, 2020), wherein the originally proposed two-factor model was supported. The instrument has also been used in several Latin American higher education institutions (Authors, 2020).


Table 1. 12 Items of the SELAQ with factor key.

Key	Item
EP1	The university will ask for my consent before using any identifiable data about myself (e.g., ethnicity, age, and gender)
EP2	The university will ensure that all my educational data will be kept securely
EP3	The university will ask for my consent before my educational data is outsourced for analysis by third party companies
EP4	The university will ask for my consent to collect, use, and analyse any of my educational data (e.g., grades, attendance, and virtual learning environment accesses)
EP5	The university will request further consent if my educational data is being used for a purpose different to what was originally stated
S1	The university will regularly update me about my learning progress based on the analysis of my educational data
S2	The learning analytics service will be used to promote student decision making (e.g., encouraging you to adjust your set learning goals based upon the feedback provided to you and draw your own conclusions from the outputs received)
S3	The learning analytics service will show how my learning progress compares to my learning goals/the course objectives
S4	The learning analytics service will present me with a complete profile of my learning across every module (e.g., number of accesses to online material and attendance)
S5	The teaching staff will be competent in incorporating analytics into the feedback and support they provide to me
S6	The teaching staff will have an obligation to act (i.e., support me) if the analytics show that I am at-risk of failing, underperforming, or if I could improve my learning
S7	The feedback from the learning analytics service will be used to promote academic and professional skill development (e.g., essay writing and referencing) for my future employability
2.3. Analysis
The current study applied latent class analysis (LCA) in an exploratory approach to gauge and segment student expectations of learning analytics services, addressing RQ1 and RQ2. Covariates were also included in the latent class model in order to gain a greater understanding of what characteristics typically define the groups identified, which answered RQ3. For RQ4, a contingency table was created to explore whether student class assignment was stable or variable across the two expectation scales (ideal and predicted).

The raw data was analysed using the three-step approach to latent class analysis (Vermunt, 2010), which was carried out in Mplus 8.1 (Muthén & Muthén, 2017). The only assumption LCA has is that the data are categorical as it was the case in our study (i.e., seven-point Likert scales). LCA does not make any assumptions related to linearity, normal distribution or homogeneity (Hagenaars & McCutcheon, 2002). LCA is a form of finite mixture model allows for soft-clustering method that calculates probabilities of group assignment (Hagenaars & McCutcheon, 2002), unlike hard clustering algorithms that assign each instance to exactly one group. That is, instead of finding clusters with an arbitrary distance measure as commonly done in cluster analysis, an LCA model identifies a distribution of the data and gauges probabilities that certain cases are members of certain latent classes. Since an LCA produces a statistical model based on the data used in analysis, it can also assess goodness of fit (e.g., Akaike Information Criterion), unlike commonly used cluster analysis methods. While a cluster analysis can only assign individual cases to clusters, LCA can also include co-variates that predict membership of cases in latent classes.

For the analysis of the collected data, we analysed the ideal and predicted expectation scales separately. An assessment of the response distributions for each scale shows the data to contain ceiling effects (Supplementary Material), particularly with regards to the ideal expectation scale. This is anticipated as the ideal expectation scale corresponds to a desired level of service so responses on this scale are likely to be high. Therefore, the data collected from the SELAQ was treated as categorical. As for the model covariates, the age variable was treated as continuous, whereas, the remaining variables were dummy coded. These dummy coded variables were gender (0 = male, 1 = female), management, science, and technology (0 = culture and jurisprudence, 1 = management, science, and technology), psychology and education (0 = culture and jurisprudence, 1 = psychology and education), Postgraduate Student (0 = Undergraduate Student, 1 = Postgraduate Student), European Student (0 = Dutch Student, 1 = European Student), and Overseas Student (0 = Dutch Student, 1 = Overseas Student). These covariates allowed for the exploration of whether gender, age, faculty, level of study, or student origin were associated with latent class assignment.

As for the latent class model building, we followed the steps outlined by Masyn (2013), which can be decomposed into assessments of absolute fit (standardised residuals), relative fit (e.g., the Bayesian Information Criterion), and classification diagnostics (i.e., accuracy as measured by relative entropy). Throughout the latent class model building, it is necessary that the interpretability of the solution needs to be considered (Lanza & Rhoades, 2013). For Lanza and Rhoades (2013), they recommend that class interpretability should be guided by a clear separation between classes, classes being easily labelled, and patterns that are logical. To assist in decisions regarding the interpretability of a solution, we followed the step taken by Oberski (2016) and use profile plots. These plots provide the estimated class means as opposed to the estimated distributions (Oberski, 2016). This is because there are seven possible categories (1 = Strongly Disagree, 7 = Strongly Agree), which makes plots of estimated distributions difficult to read (Oberski, 2016).

Thus, to provide an overview of the steps taken in this analysis, we increased the number of classes to extract until either the solution could not be identified, or the number of classes would affect the interpretability of the solution (Hagenaars & McCutcheon, 2002). These models would then be compared on the basis of their relative fit. Throughout each stage, decisions regarding the selection of a candidate model were also determined by the class interpretability. Once a suitable candidate model had been identified, the latent class regression was then ran. For the purpose of this paper, the alpha level was set at 5% for determining whether an effect is considered to be statistically significant. A detailed presentation of the analysis steps are presented in the supplementary materials.

3. Results
The results presentation that follows is a summarised account of the model building steps, which led to a three-class and a four-class solution being retained for the ideal and predicted expectation scales, respectively. Additionally, the results of the logistic regressions for each scale are presented. For a detailed account of the results, readers are directed to the supplementary materials.

3.1. Ideal expectation scale
Analysis of the ideal expectation using the three-step approach to latent class analysis led to the extraction of a three class solution, answering RQ1 and RQ3. The following labels were used to describe these classes: the Inflated Ideal Expectation group (Class One; n = 334, 26.94%), the Low Ideal Service Expectation group (Class Two; n = 306, 24.68%), and the High Ideal Expectation group (Class Three; n = 600, 48.39%). For this scale, the Service Expectation items (S1–S7) could be used to differentiate between the three groups (Fig. 1). Among these items, Item S6 (obligation to act) received the lowest average rating across all the groups. The results of the latent class regression showed that only age was associated with assignment to class one or two (Table 2); thus, addressing RQ3.

Fig. 1
Download : Download high-res image (111KB)
Download : Download full-size image
Fig. 1. Profile plot - estimated means for ideal expectation items for the three class solution


Table 2. Logistic regressions using the three step method with the three class solution.

Class one	Class two
Covariate	Estimate	Standard error	P-Value	Estimate	Standard error	P-value
Gender	0.028	0.157	0.860	0.249	0.165	0.133
Age	0.018	0.006	0.004	0.014	0.006	0.032
Management, Science, and Technology	0.356	0.196	0.069	−0.113	0.211	0.592
Psychology and Education	0.251	0.190	0.187	−0.037	0.188	0.844
Postgraduate	0.073	0.154	0.637	−0.304	0.174	0.082
European Student	0.332	0.251	0.186	−0.033	0.285	0.907
Overseas Student	0.059	0.674	0.930	0.235	0.636	0.712
3.2. Predicted expectations
Analysis of the predicted expectation scale using the three-step approach to latent class analysis led to the extraction of a four class solution, which answers RQ2 and RQ3. The following labels were used to describe these classes: the High Predicted Expectation group (Class One; n = 500, 40.32%), the Indifferent Predicted Expectation group (Class Two; n = 377, 30.40%), the Inflated Predicted Expectation group (Class Three; n = 172, 13.87%), and the Low Predicted Service Expectation group (Class Four; n = 191, 15.40%). It was found that only one class (the Indifferent Predicted Expectation group) could be differentiated on the basis of Ethical and Privacy Expectation items (EP1–EP5). Whereas, all classes could be differentiated from one another when it came to Service Expectation items (S1–S7; Fig. 2). The latent class regression showed age to be associated with assignment to class one and two, whilst European students were less likely to be in class two (Table 3), which addresses RQ3.

Fig. 2
Download : Download high-res image (117KB)
Download : Download full-size image
Fig. 2. Profile plot - estimated means for predicted expectation items for the four class solution


Table 3. Logistic regressions using the three step method with the four class solution.

Class one	Class two	Class three
Covariate	Estimate	Standard error	P-value	Estimate	Standard error	P-value	Estimate	Standard error	P-value
Gender	−0.180	0.199	0.367	−0.359	0.211	0.089	−0.287	0.241	0.233
Age	−0.015	0.008	0.045	−0.024	0.008	0.003	0.010	0.009	0.272
Management, Science, and Technology	0.130	0.252	0.607	−0.058	0.267	0.828	0.250	0.297	0.401
Psychology and Education	0.281	0.232	0.226	−0.064	0.243	0.791	0.220	0.285	0.440
Postgraduate	0.236	0.207	0.256	0.075	0.222	0.737	0.083	0.244	0.733
European Student	−0.194	0.305	0.524	−0.927	0.382	0.015	0.476	0.337	0.158
Overseas Student	0.755	1.128	0.503	−0.189	1.307	0.885	2.066	1.154	0.073
-

3.3. Class transitions
Transitions between class assignments for the ideal and predicted expectation scales are presented in Table 4, which addresses RQ4. Those in the High Expectation and Inflated Expectation groups for the ideal expectation scale appeared to move to the Low Service Expectation group on the predicted expectation scale (n = 350 and n = 111, respectively). A large proportion of students in the Inflated Expectation group on the ideal expectation scale moved to the Indifferent Expectation group on the predicted expectation scale (n = 146). In some instances, students in the Low Service Expectation group for the ideal expectation scale were assigned to either the High Expectation or Inflated Expectation groups on the predicted expectation scale (n = 139 and n = 118, respectively). Finally, some students assigned to the High Expectation group on the ideal expectation scale were assigned to the Inflated Expectation group on the predicted expectation scale (n = 204).


Table 4. Transitions between Identified Classes based on the Ideal and Predicted Expectation Scales.

Ideal expectation scale
Low service expectation group	High expectation group	Inflated expectation group
Predicted expectation scale	Low service expectation group	39	350	111
Indifferent expectation group	10	16	146
High expectation group	139	30	22
Inflated expectation group	118	204	55
4. Discussion
This exploratory paper sought to gauge and segment students based on their expectations of learning analytics services using three-step approach to latent class analysis. The findings show that for the ideal expectation scale, there are three types of response patterns within the student population. Whereas, for the predicted expectation scale, four types of responses patterns were identified. Segmentation of student expectations is an important step as failure to gauge service user expectations is attributed to the eventual failure of information system implementations (Lyytinen & Hirschheim, 1988). Moreover, by devising ways to measure user expectations, institutions can readily identify unrealistic expectations (Jackson & Fearon, 2014). This can then lead to the creation of solutions that seek to manage these expectations early on so that eventual experience of the service does not fall short of what is expected, reducing the feelings of dissatisfaction that arise with large discrepancies (Brown et al., 2014; Venkatesh & Goyal, 2010). In the following sections, the research questions are fleshed out with a focus on describing the results and using prior findings to facilitate the interpretations made.

4.1. Ideal expectations
Based on the findings of the current study, it was found that students can be meaningfully segmented based on their ideal expectations of learning analytics services (RQ1). The three classes identified from the responses to the ideal expectation are labelled as the Inflated Ideal Expectation group, the High Ideal Expectation group, and the Low Ideal Service Expectation group. It is important to acknowledge that where these groups become differentiated is in relation to the Service Expectation items, as average responses on the Ethical and Privacy Expectation items are similar. From this, the Ethical and Privacy Expectation items can be viewed as not being useful in differentiating these groups from one another. However, it also shows that irrespective of the services that could be offered through the university implementing learning analytics, students have strong expectations regarding the ethical and privacy elements of such a service. In other words, whilst some students may not desire features that will enable them to track their progress towards a set goal, they do desire a university to seek consent and ensure that all data is secure. This is an important point for informing the development of learning analytics policies as it shows all students have a desire for their ethical and privacy concerns to be adequately addressed, aligning with previous findings (Ifenthaler & Schumacher, 2016; Jones et al., 2020; Slade et al., 2019; Slade & Prinsloo, 2014; Tsai, Whitelock-Wainwright, & Gašević, 2020). Previous works are cited from this point onwards to help with additional interpretations of the data.

As for Service Expectations, the Inflated Ideal Expectation group is characterised by average item responses that were close to seven (Strongly Agree). The High Ideal Expectation group, on the other hand, was found to have average responses between categories five (Somewhat Agree) and six (Agree). Whereas, the Low Ideal Service Expectation group has average responses below category four (Neither Agree nor Disagree), falling close to categories three (Somewhat Disagree) and two (Disagree). It is, therefore, clear that there is one group who have the strongest ideal expectations for all possible features of a learning analytics service (Inflated Ideal Expectation group). This may indicate that these students view such features as being useful in supporting their learning and that this is what they desire the university to implement. The same can also be said of the High Ideal Expectation group, but their level of desire for these features is slightly weaker.

It has been previously shown in the work of Schumacher and Ifenthaler (2018) that students desired learning analytics service features that allow for learning progress to be monitored and that provide a profile of a student's learning. Similarly, Roberts et al. (2016) found first year students to favourably view learning analytics services on account of their potential to provide some form of direction to their learning experience. This is exemplified in the series of learning analytics templates presented by Marzouk et al. (2016), which shows that learning analytics services can support autonomy (e.g., select own goals), whilst also providing the capabilities for a learner to understand the importance of externally set goals. For some students, being able to structure and monitor their learning progress may be viewed favourably, particularly given the emphasis on independent learning at university (Thomas et al., 2015). Additionally, Thomas and colleagues found students to frequently report that they struggled during their initial transition into university on account of the limited direction given by teaching staff (Thomas et al., 2015). Therefore, the prospect of learning analytics services for some students (the Inflated Ideal Expectation group and High Ideal Expectation group) may be desirable on account of its potential to assist them in their adjustment to the culture of higher education.

For the Low Ideal Service Expectation group, they do not express any desire to receive any of these learning analytics features. It is possible that these students, as found in the work of Roberts et al. (2016), feel that learning analytics should not remove the ability for a student to make independent decisions. Put differently, whilst a university could intervene early if a student is at-risk of failing, these students may believe that this removes their ability to become reliant upon themselves. Thus, from a policy perspective, learning analytics cannot be a blanket implementation with all students receiving the same service. This has previously been hypothesised by Teasley (2017) and Roberts et al. (2017) who proposed a need for personalised or customisable learning analytic services. However, the hypothesis by Teasley was mostly based on a narrative review of the literature, while the proposition by Roberts et al. was based on a qualitative study with relatively small sample. Our study is the first to offer evidence in support these previous hypotheses and demonstrate differences in student expectations from learning analytics services.

An approach to implementation of learning analytics services, considering these group differences, would then be to offer different forms of services that align with what students expect. This resembles a scaffolding approach, whereby the level of service offered varies in accordance with what students need. However, the possibility of students receiving regular feedback, knowing how they are progressing, or having a complete profile of their learning may not encourage the student to assume responsibility for their learning (van de Pol et al., 2010). Thus, while those in the Inflated Ideal Expectation group or High Ideal Expectation group may desire these listed learning analytics services, it is necessary for steps to be taken to avoid dependency. A solution to this would be for such support systems to gradually be faded with time (van de Pol et al., 2010). This would then address the challenges of first year students becoming independent learners (Thomas et al., 2015) and the concerns relating to learning analytics services undermining student responsibility for their own learning (Roberts et al., 2016). As for those in the Low Ideal Service Expectation group, an adaptive approach to learning analytics services could be taken where the support offered varies in accordance with a student's learning progress (van de Pol et al., 2010). This latter point is important, as students who may not desire for their data to be used to provide learning analytics services will become disadvantaged as they will not reap the benefits offered (Sclater, 2017). Thus, students not desiring learning analytics service features does create an additional challenge as higher education institutions must decide how to satisfy student expectations but remain cognisant that such decisions can create further problems. One possible approach to tackle this issue could be introducing a mandatory learning analytics service that provides engagement metrics in the form of a dashboard, as already implemented at Nottingham Trent University (Nottingham Trent University, 2016; Sclater et al., 2016). In this way, students who have initially expressed low interest in learning analytics may change their expectations due to the exposure to or perceptions of the way their peers benefit from using the services (Sclater et al., 2016). Therefore, for the Low Ideal Service Expectation group of students, the usefulness of learning analytics services may not become apparent until they experience the tools provided or the academic benefits are realised.

In addition to the three types of responses identified, the pattern of average responses shows item S6 (the obligation to act) to be lowest for each group (as shown in Fig. 1). In the case of the Inflated Ideal Expectation and High Ideal Expectation groups, the average responses to S6 (the obligation to act) fall between five (Somewhat Agree) and six (Agree). Whilst these are positive responses, they do fall below the trends for the remaining 11 items. As for the Low Ideal Service Expectation group, these students, on average, appeared to express disagreement with this particular learning analytics service feature. This is important as there has been extensive discussions regarding the obligation to act, with Prinsloo and Slade (2017) stating that both the student and institution have a shared responsibility when it comes to learning. Put differently, it is not the sole responsibility of the institution to ensure that a student is successful, but the student themselves bears a responsibility to engage in the learning process (Howell et al., 2018).

As for the results of the latent class regression, it was found that class assignment was associated with one covariate (RQ3). More specifically, it was found that the likelihood of being either in the Inflated Ideal Expectation or High Ideal Expectation groups, compared to the Low Ideal Service Expectation group, increases with age. Studies have shown that mature students commonly identify family and friends as their main sources of support in higher education, whilst few sought institutional support, putting this down to being off-campus or low confidence (Heagney & Benson, 2017). It is, therefore, understandable that older students would desire the types of services that could be offered through learning analytics, as the feedback would be personalised (e.g., knowing how they are progressing in relation to a set goal) and their progress would be monitored (e.g., early alert systems). Put differently, learning analytics can be used strategically to strengthen the existing support infrastructure for distance-learning students, thus cultivating a sense of belonging among students and providing information that can help students better manage their studies and achieve learning goals.

4.2. Predicted expectations
The results of the study also found that students could be meaningfully segmented based on their predicted expectations of learning analytics services (RQ2). The results found that a four-class solution was deemed to be suitable for the predicted expectations scale. These four groups are labelled as the High Predicted Expectation group, the Indifferent Predicted Expectation group, the Inflated Predicted Expectation group, and the Low Predicted Service Expectation group. Below, the identified are described in further detail, with previous findings being used as a lens to facilitate interpretations.

In contrast to the Ideal Expectation scale, these four identified groups can be differentiated based on the Ethical and Privacy Expectation items (EP1 to EP5). Whilst the responses of these five items show a similar trend for classes one, two, and three, the responses for class four are considerably lower. Thus, unlike the ideal expectation scale, the Ethical and Privacy Expectation items can be used to differentiate between certain classes. Starting with the Indifferent Predicted Expectation group, it appears that EP1 (consent to use identifiable data) and EP2 (ensure all data is kept secure) received the highest average responses. Whereas, expectations regarding consenting to third party usage of data (EP3), consenting to data being collected and analysed (EP4), and consenting to data being used for an alternate purpose (EP5) was met with indifference (Neither Agree nor Disagree (4)). For these students, it appears that they did not necessarily expect the university to seek consent for collecting and analysing data, giving data to third party companies, or using data for alternative purposes. This may be on account of students being accustomed to a culture where companies readily collect and analyse data day to day basis; therefore, these students may be less resistant to universities engaging in such practices (Sclater, 2016). Similarly, it has been found that some students are not concerned over the usage of data extracted from the virtual learning environment (Fisher et al., 2014) or university studies (Ifenthaler & Schumacher, 2016). It may, therefore, be that for those in the Indifferent Predicted Expectation group, there is an expectation that the use of certain data by the university and third party companies will not require them to provide consent.

Compared to the Indifferent Predicted Expectation group, the remaining three classes (Low Predicted Service Expectation group, High Predicted Expectation group, and Inflated Predicted Expectation group) have strong expectations across all Ethical and Privacy Expectation items. Again this shows that majority of students, in reality, expect for the university to clearly set out how collected data is used and who has access to this data, but for the university to also seek consent before undertaking any form of learning analytics (Slade & Prinsloo, 2014). In the work of Ifenthaler and Schumacher (2016), it was found that in some instances students were open to data being shared (e.g., pertaining to their university studies), but certain data usage drew greater concern (e.g., use of personal data). Thus, whilst it may be that there is a degree of acceptability in what data the university uses, as found by Ifenthaler and Schumacher (2016), a majority of students realistically expect consent to be first sought. Given that this scale (predicted expectations) refers to what is expected of a learning analytics service in reality and the proportion of students across these three classes being high (n = 863; Low Predicted Service Expectation group, High Predicted Expectation group, and Inflated Predicted Expectation group), it does strengthen the view that the university takes steps to address these expectations. In particular, under the governance of the GDPR1, universities in the European Union do have the legal responsibility to inform students about any personal data collected and how it will be processed (Sclater, 2017). Thus, in conjunction with the expectations of students presented here, it remains necessary for the institution to be transparent and clearly articulate any data handling procedures.

For Service Expectation items (S1 to S7), the Inflated Predicted Expectation group have average responses close to seven (Strongly Agree) for majority of the items, apart from S6 (the obligation to act). The largest identified class, the High Predicted Expectation group (n = 500), have average responses between five (Somewhat Agree) and six (Agree). Thus, there is some variability across the Service Expectation items with regards to the strength of the predicted expectations. For example, students from these two groups show a high average response to S3 (knowing how progress compares to a set goal), but a weaker average response to S6 (the obligation to act). As for the Indifferent Predicted Expectation group (Class Two), the average responses do not show much variability around response category four (Neither Agree nor Disagree). This is indicative of these students not having formulated strong expectations towards the possible learning analytics services features and whether they would or would not realistically expect them to be implemented. As for the Low Predicted Service Expectation group (Class Four), these students tended to display disagreement with the university being capable of offering these learning analytics service features. The item with lowest average response for this group was S4 (receiving a complete learning profile), which resonates with the findings of Howell et al. (2018). In their work, Howell and colleagues found teaching staff to express concern over the anxiety that could be created as a result of the information overload that is possible with learning analytics services (e.g., students wanting to constantly know how they are performing in relation to others). In the case of this group of students (the Low Predicted Service Expectation group), they may view the possibility of a university being capable of feeding such information back or coping with sheer volume of students seeking additional support to make this service unattainable. As with the ideal expectation scale, item S6 (the obligation to act) does have the lowest average response for all classes apart from class four where it is the item with the second lowest response. Given that this scale corresponds to the type of learning analytics expected in reality, it is important to recognise how responses to this item compare to the other item responses. Put differently, S6 appears to be a feature that students generally do not expect a university to implement, but other items receive positive responses. For the High Predicted Expectation (Class One) and Inflated Predicted Expectation (Class Three) groups, features that include, but are not limited to, receiving regular updates (S1) and knowing how progress compares to set goals (S3) are expected to be implemented in reality. However, having a system in place that could place the responsibility of student success predominately with teaching staff (Howell et al., 2018; Prinsloo & Slade, 2017) does not elicit expectations that are comparable in strength. Again, this may refer to the issues previously raised in student focus groups, which refer to learning analytics services preventing students from being independent (Roberts et al., 2016). In contrast, the features in items S1 and S3 do not impede independence and can support self-regulated learning as it allows students to monitor their progress (Lim et al., 2020; Roberts et al., 2017; Schumacher & Ifenthaler, 2017).

The latent class regression results found class assignment to be associated with two covariates (RQ3). More specifically, the likelihood of being in the High Predicted Expectation group (Class One) or the Indifferent Predicted Expectation group (Class Two) decreases with age, compared to Low Predicted Service Expectation group (Class Four). The likelihood of being or not being in the Inflated Predicted Expectation group (Class Three) with increased age was not statistically significant. From this, it seems that the predicted expectations of older students are less likely to be high or at a level of indifference. For the ideal expectation scale, it was found that older students are more likely to be assigned to a class labelled the Inflated Ideal Expectation group; however, this was not found for the predicted expectation scale. Put differently, older students are no more likely to be classified in the Inflated Predicted Expectation group (Class Three) than Low Predicted Service Expectation group (Class Four).

In addition to the effect of age, it was also found that European students were less likely to be in the Indifferent Predicted Expectation group (Class Two) compared to Dutch students. This is important as it may be indicative of cross-cultural differences with regards to expectations of learning analytics services. It is, therefore, necessary for decision makers in higher education to understand whether student expectations of learning analytics services are culturally consistent or not, particularly given the global interest in learning analytics (Pardo et al., 2018).

4.3. Expectation transitions
To further understand student expectations of learning analytics services, an additional step was taken to explore class transitions between the two SELAQ scales (ideal and predicted expectations). The results generally show that class assignment is not consistent across the ideal and predicted expectation scales (RQ4). Previous literature on student stakeholder perspectives of learning analytics services is again used to offer additional interpretations from the descriptions of the data.

It was found that the largest proportion of students were assigned to the High Expectation group on the ideal expectation scale and the Low Service Expectation group on the predicted expectation scale (n = 350). In this instance, students may have high desires regarding learning analytics services, but do not realistically expect the university the types of services offered. This shows that the students hold quite pessimistic expectations of the university not being able to realistically implement learning analytics services. However, there have been numerous examples of universities being successful in implementing those learning analytics service features contained within the SELAQ (Sclater et al., 2016). Therefore, the university, upon knowing what student expect, can begin to challenge these expectations (Jackson & Fearon, 2014). From the perspective of cognitive dissonance, however, these expectations may not be easily challenged (Festinger, 1957). This is due to both an individual's resistance to change and the strength of the dissonance created by the university engaging in behaviours that challenge expectations (Festinger, 1957; Ngafeeson & Midha, 2014; Nov & Ye, 2008). Put differently, only when maximum dissonance is created (e.g., provide the services that are not realistically expected) can expectations of this group be challenged (Festinger, 1957).

There are also a group of students who move from the Low Service Expectation group on the ideal expectation scale to either the High Expectation or Inflated Expectation group (n = 139 and n = 118, respectively) on the predicted expectation scale. For these students, they appear to not desire any of the features of a learning analytics service but they do expect the university will implement such services. As previously discussed, Roberts et al. (2016) found a subset of students to express disinterest in the possibilities that learning analytics services can offer. Nevertheless, it is likely that students realise that in a society where data is regularly collected and processed, a university engaging in such practices may not be unexpected (Sclater, 2016).

4.4. Implications for policy
The findings of this current work are important for the development of a learning analytics policy that accounts for the perspectives of the student stakeholder group. One of the main takeaway points from analysing the SELAQ data using latent class analysis has been the identification of heterogeneous expectations found within the student population. Some students have inflated expectations of learning analytics services, whilst others have low expectations regarding the types of features that are offered. From knowing this information, it then becomes necessary for institutions to design and implement a learning analytics service that aligns with these diverse expectations. In addition, it could also allow for management to intervene early and manage the expectations of students in order to mitigate the effects of inflated expectations (e.g., dissatisfaction resulting from the large discrepancies between expectations and experience; Brown et al., 2012, Brown et al., 2014; Jackson & Fearon, 2014; Venkatesh & Goyal, 2010). Institutions interested in implementing learning analytics services should, on the basis of these results, be encouraged to take a proactive approach by gauging student expectations early on in order to provide a service that students can be satisfied with.

The approval of the GDPR by the European Parliament has important connotations for the implementation of future learning analytics services. Part of this legal act is for businesses to ensure that all personal data is securely processed and service users must provide informed consent to data processing. As found in the current work, a majority of students across all identified groups held strong expectations regarding the Ethical and Privacy Expectation items, all of which cover the main topics of the GDPR. Even in the case of the Indifferent Predicted Expectation group (Class Two), these students expressed slight agreement with items EP1 (consent to use personal data) and EP2 (ensuring data is secure). Therefore, the student perspectives regarding the ethical and privacy elements of a learning analytics service are in alignment with those points contained within the GDPR. On the basis of this information, it is recommended that those institutions interested in implementing learning analytics services first create a clear privacy policy that details how these ethical and privacy considerations will be addressed, as research has shown this area to be the most important aspect of a policy that governs the use of learning analytics (Scheffel, Tsai, Gašević, & Drachsler, 2019). These points have also been articulated by Sclater (2017), who has stated that consent must be sought for the collection and processing of sensitive data. Additionally, in the development of this document, it must also have input from stakeholders such as students so that their expectations can be gauged early on in the implementation stages (Davis & Venkatesh, 2004; Khalifa & Liu, 2003).

Under the GDPR, it is also stated that there must be a legitimate interest for processing data. In the case of learning analytics services, a university may view the potential to improve student learning as a legitimate interest for collecting and analysing data. From the findings of the current study, there were two groups who had desired and expected to receive majority of the learning analytics service features (e.g., regular updates on learning progress and receiving a completed profile of their learning). However, there were also students that were indifferent about the possible learning analytics service features and students who did not expect or desire any such features. This raises concerns regarding whether an institution does have a legitimate interest to collect and analyse student data as not all students expect these learning analytics services. Again, turning to the points raised by Sclater (2017), legitimate interest can be used to avoid seeking additional consent under circumstances where data is lawfully collected (e.g., virtual learning environment logs) and used (e.g., creating and sending personalised emails), whereas Cormack (2016) suggests that under this premise, consent-seeking is necessary only prior to actioning interventions. It is still necessary, however, that even under these circumstances the students are aware of such steps being taken (Sclater, 2017). Taking both the current findings and data handling discussions presented by Sclater (2017) into consideration, it is clear that whilst general processing of certain educational data by a university is permissible, there is no consensus from students with regards to expecting or desiring learning analytics services. As stipulated in the GDPR, the interests of the individuals must be weighed up with your own, taking into consideration how they would want their data to be used. For learning analytics services, this can easily be achieved through the use of the SELAQ and as discussed above, not all students expect their data to be used to provide such services. Therefore, there cannot be blanket implementation of learning analytics services within universities, students must have the right to decide whether to partake in such services or not.

4.5. Limitations
Our decisions regarding the candidate model selection were informed by the relative fit, classification diagnostics, local fit, and interpretability as introduced in Section 2.3. For both the ideal and predicted expectation scales, the proportion of absolute standardised residual values exceeding 3 was greater than the 5% guideline proposed by Masyn (2013). However, this only remains a guideline and Masyn (2013) did stipulate that if the proportion is in “notable excess” of 5% then the model fit is concerning (p. 567). In terms of the current models, our analysis showed that the interpretability, relative fit, and classification accuracy of the selected models were good. Therefore, we concluded that seeking to meet the general guideline of 5% for absolute fit by increasing the number of classes extracted was inappropriate. It remains necessary for follow up work to be undertaken to see whether the three and four class solutions for the ideal and predicted expectation scales, respectively, are supported in additional samples.

The inclusion of class transitions is useful in showing how what students may desires from learning analytics services does not equate to what they expect in reality. Whilst providing useful insights, there is still a need to understand why students change their expectations. As discussed in Ajzen's (2011) work, beliefs are shaped by background factors such as life values and personality. It is reasonable to extend this assertion to expectations, particularly as they are defined as beliefs about the future (Olson & Dover, 1976). Future research is therefore required to understand what shapes both the ideal and predicted expectations held. It may also be necessary to underdtake additional qualitative work to provide a rich understanding of what factors lead students to fall within the identified classes reported here.

The study was conducted at a single higher education institution with specific educational, political, and cultural context. Therefore, this study should be replicated in other contexts before any generalisability claims can be substantiated.

5. Conclusion and future work
The current work has provided answers to the four proposed research questions, and identified a need for higher education institutions to develop approaches to the implementation of learning analytics that cater to the expectations of different student subpopulations. Specifically, the work has shown that student expectations of learning analytics can be segmented through the application of latent class analysis. Each identified group shows a distinct profile of what is expected from a learning analytics service. The majority of the respondents expected the university to act ethically in the use of student data central to any service, whilst the expectations towards learning analytics services are not consistent. There is a need for follow up work to understand the reasons behind students holding contrasting expectations towards the types of learning analytics features that could be offered.

