With the mobile data traffic grows rapidly and the video data has a high proportion, the backhaul link faces great pressure. The conventional centralized architecture has been far from enough to satisfy the user demands. For reducing user response latency and easing backhaul stress, the streaming media contents should be proactively stored in the edge of network and new contents caching model needs to be presented. In this work, a video content collaborative caching strategy in the cloud-edge cooperative environment is proposed. In this strategy, first, the k-means algorithm is used to cluster the edge servers. Then the latency and caching cost gain bring by caching the content on the edge servers in the cluster and the collaborative cache domain are analyzed to establish the content caching problem. Further, the marginal gain is calculated by analyzing the latency and caching cost gain. Finally, in order to solve the content caching problem, this paper proposed a marginal gain based content caching algorithm. Experimental results prove the effectiveness of the proposed algorithm.

Previous
Next 
Keywords
Cloud-edge cooperative environment

K-means

Video content

Collaborative caching

1. Introduction
With the rapid development of smart mobile device, the traffic has grown exponentially. In fact, by 2021, more than 78% of global mobile data traffic will become video (Cisco Mobile V N I, 2017). For instance, the duration of video content uploaded to YouTube per minute is approximately 72 h. Such a large amount of data causes a heavy burden for backhaul link and the traditional centralized network model cannot satisfy the time-sensitive demand (Abu-El-Haija et al., 2016, Akhavanbitaghsir and Khonsari, 2018, Azimi et al., 2018, Bilal et al., 2019, Bilal and Erbad, 2017, Chae et al., 2017, Chen et al., 2017a, Chrobak and Noga, 1999, Cisco Mobile V N I, 2017, De Pellegrini et al., 2017, Hu et al., 2015, Jararweh et al., 2016, Kang et al., 2019). Therefore, the industry and academic community have proposed the cloud-edge cooperative environment architecture (Hu et al., 2015; Kwak et al., 2018). In that kind of architecture, users can obtain needed contents from nearby edge nodes, which can relieve backhaul pressure and reduce response latency (Abu-El-Haija et al., 2016, Akhavanbitaghsir and Khonsari, 2018, Araldo et al., 2014, Azimi et al., 2018)(De Pellegrini et al., 2017, Kwak et al., 2018, Li et al., 2017a, Li et al., 2017b, Li et al., 2020, Li et al., 2021, Li and Song, 2021, Liu et al., 2017, Ren et al., 2018, Song et al., 2017, Su et al., 2017, WU and ZHI, 2015, Xia et al., 2016, Yang et al., 2019, Yu et al., 2016)(Li et al., 2020, Li and Song, 2021). There are different schemes for caching video content at the edge of network. The video content caching in the cloud-edge cooperative environment is more flexible and efficient compared with other modes, such as the content caching in edge nodes only, content caching in the non-cooperative cloud and edge nodes (Wang et al., 2017; Chen et al., 2017a, Chen et al., 2017b, Chen et al., 2018, Chrobak and Noga, 1999, Cisco Mobile V N I, 2017, De Pellegrini et al., 2017, Hu et al., 2015). If the contents are cached on edge nodes only, then the computing and storage load of the edge node will increase, thereby reducing the user's content access efficiency (Chae et al., 2017, Chen et al., 2017a; Abu-El-Haija et al., 2016). As to the caching scheme in the non-cooperative cloud and edge nodes environment, the edge node cannot access the cache state of other nodes in real-time. However, in the cloud-edge cooperative environment, the video segments can be stored in different edge nodes and the cache state will be shared among edge nodes (Abu-El-Haija et al., 2016, Araldo et al., 2014; Chen et al., 2017b, Ha and Satyanarayanan, 2015; Ren et al., 2018).

In Fig. 1, the cloud-edge cooperative environment consists of multiple edge servers and users and a remote cloud data center. In this environment, the users and the edge servers are distributed in different locations. The remote cloud data center is mainly responsible for caching the video contents in different edge servers, and the users request the video contents from the edge servers nearby according to their data access requirements (Li et al., 2017b). Due to the limited cache capacity, a single edge server cannot cache all video content (Akhavanbitaghsir and Khonsari, 2018, Araldo et al., 2014, Azimi et al., 2018, Chae et al., 2017, Chen et al., 2017a, Eb et al., 2020, Ha and Satyanarayanan, 2015, Koutsopoulos and Iosifidis, 2010, Kwak et al., 2018, Li and Jing, 2018). Therefore, when an edge server does not cache a video content requested by the user, this edge server will obtain this video content by collaboration with other edge servers, i.e., this edge server requests and obtains the video content from other edge servers, and then forwards the video content to the user (Sun et al., 2019, Taleb et al., 2017, Tran et al., 2017a, Tran et al., 2017b, Wang et al., 2020, WU and ZHI, 2015). Compared with the way that users directly request video content from the remote cloud data center, the way of collaboration between edge servers can greatly reduce the content access latency (Abu-El-Haija et al., 2016; Li et al., 2020a, Li et al., 2020b). However, If the content requested by the user is not cached in all edge servers, the user's request will be forwarded to the remote cloud server, which will provide user with the video content (Kwak et al., 2018, Li et al., 2017b, Li et al., 2018, Li et al., 2020a, Li et al., 2020c, Li and Jing, 2018, Li et al., 2020d).


Fig. 1. Video content caching process on cloud-edge cooperative environment.

In addition, in Fig. 1, users are mobile in the cloud-edge cooperative environment, which allows users to move to different geographic locations at different times, and connect to the nearest edge server at their location to obtain the computing and storage resources. Therefore, due to user mobility, each user can access multiple edge servers. As shown in Fig. 1, user A and user B are respectively located in the service range of edge server 1 and edge server 2 at time t1. Therefore, user A and user B are connected to server 1 and server 2 respectively at time t1. At time t2, user A and user B move to the service range of edge server 2 and edge server M, respectively. At this time, user A and user B are connected to server 2 and server M, respectively.

In order to overcome the defect of the backhaul link in traditional cellular systems, caching video contents on edge networks has been proven to be able to improve the user experience (Akhavanbitaghsir and Khonsari, 2018, Azimi et al., 2018, Bastug et al., 2014, Bilal et al., 2019, Chae et al., 2017, Chen et al., 2017a). Since the type of base stations is different, the mobile network is a heterogeneous environment. The content caching scheme should consider the computing and storage capabilities of the edge node and use the limited cache space of the edge node as much as possible. The existing caching strategies still have the following problems:

(1)
Most content caching strategies only consider the cooperation among traditional cells and ignore the cloud-edge cooperative architecture. The content caching scheme in cloud-edge cooperative architecture not only utilizes the nearby edge nodes to decrease latency for users, but also takes advantage of the powerful computing ability of the cloud server.

(2)
The majority of content caching strategies in the cloud-edge cooperative environment focus on optimizing the latency without considering the cost factor, which causes the outcome cannot be applied to the actual scenario.

To solve these problems, a video content collaborative caching strategy in the cloud-edge cooperative environment is proposed. In this strategy, the K-Means algorithm is used to cluster the edge servers. Then the gains of latency and caching cost bring by caching the contents on edge servers in the cluster and cooperative cache domain is calculated to formulate the content caching problem. Further, the marginal gain of content caching is calculated by analyzing the gains of latency and caching cost. Finally, the content caching problem is solved by a proposed marginal gain based content caching algorithm. In addition, when the edge server's cache capacity is insufficient and the new content is about to be cached, the passive cache replacement process will be initiated, i.e., the edge server will delete part of the content with lower marginal gain to cache the new contents.

The main contributions of this paper are as follows:

(1)
To reduce the latency and the cost in the cloud-edge collaborative environment, a content collaborative caching strategy is proposed. In this strategy, the edge servers are divided into multiple clusters, and edge servers between clusters and between cooperative cache domains provide users with content in a collaborative manner to reduce content access latency. On the other hand, the latency gain brought by the cached content to the clusters and the collaborative cache domain and the cache cost gain is considered in this strategy, which further reduces the content caching cost and the content access latency.

(2)
The maximization of delay gain and cache cost gain is considered as the content caching problem considered in this paper. To solve this problem, a content collaborative caching algorithm is proposed. This algorithm calculates the marginal gain by analyzing the latency gain and cache cost gain, and caches the content according to the marginal gain to achieve the trade-off between latency and cache cost.

(3)
The YouTube-8M video dataset is used for experimental verification and the experiments are executed in real-world environments. The experiment results show that our algorithms are better than the benchmark methods in terms of the content download latency, the content caching cost, the cache hit ratio, and the backhaul traffic load.

The rest of this paper is arranged as follows: The related works are reviewed in Section 2. The overview of the models of the cooperative caching strategy is provided in Section 3. In Section 4, the corresponding algorithms are presented. Furthermore, the performance evaluation is shown in Section 5. In Section 6, the conclusion and future work are described.

2. Related work
2.1. Cloud-edge cooperative environment
Traditional caching schemes in centralized cloud processing network architecture do not consider wireless network characteristics such as dynamic traffic loading and interference (Xia et al., 2016). Both industry and academia have proposed various network architectures in which clouds and edges collaboratively work (Azimi et al., 2018, Bilal et al., 2019, Chen et al., 2018, De Pellegrini et al., 2017, Jararweh et al., 2016, Li et al., 2017a, Li et al., 2020a, Li et al., 2020, Majeed et al., 2017, Ren et al., 2018, Song et al., 2017, Su et al., 2017, Taleb et al., 2017, Tran et al., 2017b, Xia et al., 2016, Yang et al., 2019, Yu et al., 2016). A cooperative caching strategy is proposed by Li Q et al. (Li et al., 2017a) to reduces the content provisioning cost of the software-defined hyper-cellular network. A computing task caching scheme is proposed by Chen M et al. (Chen et al., 2018), which can jointly optimize the computation, caching and communication on the edge cloud. Li C et al. (Li et al., 2020a) propose an energy-efficient fault-tolerant replica management scheme in the cloud-edge cooperative environment with the deadline and budget constraints. To overcome the limited resource of a single cloudlet and meet the needs of users, Jararweh Y et al. (Jararweh et al., 2016) propose a layered model consisting of the mobile edge computing server and multiple cloudlets infrastructure, which is divided into multiple domains. A cloudlet controller in each domain is responsible for maintaining communication between edge nodes within the domain. Compare with traditional centralized network architectures, scholars find that the edge caching has the advantages in reducing the latency, and the consumption of bandwidth and energy (Azimi et al., 2018) (Li, Zhang, 2020). De Pellegrini F et al. (De Pellegrini et al., 2017) provide an optimal caching policy that has considered the popularity, the availability of contents and the spatial distribution. However, existing partial caching strategies do not consider the cooperation between edge-to-cloud and edge-to-edge. In terms of the cloud-edge cooperative caching strategy, Yu et al. (2016) study the application of the scalable video coding technology in inter-cell cooperative videos caching and scheduling to improve the user's cache capacity and QoE. Bilal K et al. (Bilal et al., 2019) propose a collaborative caching and processing approach to minimize network usage and content delivery network cost.

All of the approaches mentioned above have improved the overall performance of the system to some extent. However, most of them do not make good use of the powerful computing capacity of the cloud and the storage capacity of edge nodes. Besides, most existing works rarely considers the video content caching for the cloud-edge cooperative environment. In view of the trend of video stream application development and the promotion process of the cloud-edge cooperative environment, it is necessary to adapt and optimize the traditional content caching strategy.

2.2. Content caching for video content
Edge computing can provide low latency services for video and game applications (Bilal and Erbad, 2017; Taleb et al., 2017; Sun et al., 2019; Li et al., 2017b; Liu et al., 2017; Tran et al., 2017a; Li et al., 2020b; Chae et al., 2017; Ren et al., 2018; Li et al., 2018; Akhavanbitaghsir and Khonsari, 2018; Wu and Zhi, 2015; Li et al., 2021; Majeed et al., 2017). (Bilal and Erbad, 2017; Taleb et al., 2017) point out that edge computing can solve many domain limitations, such as interactive multi-view video, virtual view synthesis, online game and video streaming. Sun G et al. (Sun et al., 2019) present a content placement strategy to reduce average user visiting time and a graph-partitioning method to maximize the transmission rate. Li Q et al. (Li et al., 2017b) consider the general two-tier hierarchical edge caching framework including centralized control nodes and distributed traffic nodes, the optimal storage allocation factor is analyzed through the proposed non-cooperative and cooperative caching strategy. Liu J et al. (Liu et al., 2017) propose a centralized caching strategy based on the sub-module optimization theory, and a transport-aware distributed cache placement strategy based on the belief propagation. Tran T X et al. (Tran et al., 2017a) targeted real-time video transcoding and proposed an online video scheduling algorithm by formulating cooperative caching and processing problems as integer linear programs. The above researches focus on different problems of video caching, and propose different caching frameworks and algorithms in the cloud-edge cooperative environment. Li C et al. (Li et al., 2021) proposed a cache strategy based on user mobility and content popularity. In this cache strategy, the content caching operation is performed according to the user mobility and the content popularity. Li C et al. (Li et al., 2020d) proposed a cache prefetching strategy, which selects the contents to be prefetched based on the Bayesian network, and then selects edge nodes with lower loads to place the contents. Eb A et al. (Eb et al., 2020) proposed a joint caching strategy. In this strategy, different edge servers collaborate to cache different chunks of one video content, to use more efficiently the backhaul and storage resources, reduce the content delivery delay and minimize the network cost. Wang F et al. (Wang et al., 2020) proposed a multi-agent deep reinforcement learning based edge caching framework, where each edge can adaptively learn its own best policy in conjunction with other edges for intelligent caching. Su Z et al. (Su et al., 2017) proposed a framework to cache layered videos, where the layered videos are stored on the edge node. The number of layers is determined by considering the caching service cost and the video quality demands of the users.

Service performance in mobile networks depends on the amount and location of the content cached at the edge nodes (Li et al., 2020b). Chae S H et al. (Chae et al., 2017) proposes a content placement strategy considered the gains of the content diversity and the cooperative gain. Ren D et al. (Ren et al., 2018) design a hierarchical collaborative content caching strategy in mobile edge networks for reducing access latency and improving energy efficiency. Li Y et al. (Li et al., 2018) design the gigabit passive optical network architectures based on SDN for efficiently implementing cooperative video caching, and then they study two scenarios of cooperative video caching. To solve the content placement problem, Akhavanbitaghsir S et al. (Akhavanbitaghsir and Khonsari, 2018) proposed the centralize and distributed collaboration schemes. Hai-Bo et al. (WU and ZHI, 2015) propose a probability-based heuristic caching strategy that relates probabilities to content popularity and placement gains. Li C et al. (Li et al., 2021) propose a dynamic optimized replica placement strategy. The goal of this strategy is to reduce the network utilization and maintain the load balance among the nodes. A video content proactive cache scheme is proposed by Majeed M F et al. (Majeed et al., 2017). This scheme considers to reduce the number of hops between the caching nodes and the user.

The above work has studied the content caching strategy. However, the above works did not consider how to maintain the balance of latency and cost in the cloud-edge cooperative environment. Besides, the above works did not consider how to cache the new content when the edge node's cache capacity is insufficient. The proposed content caching strategy combines a passive cache replacement process, which can effectively solve the above problems by deleting a part of the content with low marginal gain.

3. Collaborative caching strategy in cloud-edge cooperative environment
3.1. System overview
It can be seen from Fig. 2, the cloud-edge cooperative environment considered in this paper includes the edge servers, the users with the mobile device and a remote cloud server. The edge server has the local cache capacity for caching some contents to reduce the delay of content download and the load of backhaul link. When a user needs to request content, they can access it from the local edge server (the edge server closest to the user). If local edge server does not cache this content, the local edge server requests this content from other edge servers in the collaborative cache domain and forwards it to the user. If the content is not cached in the collaborative cache domain, this content is requested by the local edge server from the remote cloud server and then forwarded to user. Assuming that the remote cloud server has a strong storage capacity, it stores all content. Therefore, the remote cloud server can proactively cache content on the edge servers during off-peak hours of the network. The cache operation can also be triggered on the edge server according to the request, i.e., the reactive cache. In general, for video content caching in cloud-edge cooperative environment, a commonly adopted method to cache the video file is to split each video file into multiple segments for caching.

Fig. 2
Fig. 2. The overview of content caching model in cloud-edge cooperative environment.

Furthermore, it can be seen from Fig. 2, the content collaborative caching strategy proposed in this paper runs in the remote cloud server, and its running process is as follows: (1) The K-Means algorithm is used to cluster edge servers in the cloud-edge cooperative environment. After clustering the edge servers, the local edge server will preferentially request the content required by the user from the edge service in the cluster. If the content is not cached in cluster, this content will be requested by the local edge server from other edge servers in the collaborative cache domain or the cloud server. (2) The latency gain and the content caching cost gain caused by caching content in the edge server is calculated. (3) The content caching problem is established according to the latency gain and the content caching cost gain. (4) The content caching marginal gain is calculated by analyzing the latency gain and the content caching cost gain. (5) To solve the content caching problem, a content caching algorithm based on latency-cost trade-off is proposed.

The part of the notations used in this paper is shown in Table 1.


Table 1. Notation table.

Notions	Definition
The set of edge servers.
E	The number of edge servers.
The set of edge server cluster heads.
K	The number of edge server cluster heads.
The cluster with the cluster head .
The set of the edge server location.
The set of video files.
The set of segments contained in file .
A segment of file i.
The number of the segments of file i cached in the edge server .
The number of the segments of file i cached in the edge server.
The number of the segments contained in file i.
The maximum number of video file segments that en can cache.
D	The size of the video file segment.
The data transfer rate between user and edge server en.
B	The system bandwidth.
The proportion of bandwidth allocated to edge server en.
The data transfer rate between edge server en and edge server enc.
The latency of the user directly downloading the segment from the edge server en.
The latency of the user downloading the segment from the edge server enc in the cluster.
The latency for the edge server en to obtain the segment  from the remote cloud.
3.2. Mathematical model formulation
3.2.1. Edge server clustering based on K-Means
In a cloud-side collaboration environment, due to the randomness of the geographical distribution of edge servers (Kang et al., 2019), the network distance between different edge servers may be different. Although the data transmission speed between the edge servers is faster than that between the edge servers and the remote cloud data center. However, due to network congestion, data node failure and other reasons, the latency between edge servers may be higher than that between edge servers and cloud data center. For solving the above problem, in the proposed content caching strategy, the K-Means algorithm is used to cluster the edge servers. The clustering results (information about the cluster to which the edge servers belong) are sent to each edge server by cloud data center. When a user requests content through a local edge server, if the local edge server does not cache the content, then the local edge server first searches this content from other edge servers in the cluster where it is located. The reason is mainly because the distance between edge servers in the same cluster is relatively short, which results in low data transmission latency. If the content is not cached in the cluster, the local edge server calculates the latency of requesting the content from the collaborative cache domain and the remote cloud server, and selects the way with the shortest latency to obtain the content. This is mainly because the edge server located outside the cluster may be far away from the local edge server, resulting in a higher data transmission latency. Therefore, this method of selecting the content download mode by judging the transmission latency can further reduce the data transmission latency.

Let  and  denote the set of E edge server and the set of K edge server cluster heads, respectively. The cluster with the cluster head  is denoted as , which contains  edge servers. Let  denote the set of the edge server location, the location of edge server  be denoted as , where  and  denotes the abscissa and ordinate of edge server  respectively. In addition, the method mentioned in Section 3.1 in (Li and Jing, 2018) is used to determine the number of cluster heads K.

The steps of clustering edge servers using the K-Means method are as follows: (1) For E edge servers in the edge server set , K edge servers are selected from ES as the initial cluster heads. (2) For edge server , if  satisfies , i.e., the distance between en and cluster head  is the shortest compared to other cluster heads, then en is added to cluster  where  is located. (3) For cluster , the average value  of the coordinates of all edge servers in  is calculated. If  satisfies , then edge server en is selected as the new cluster head ch. (4) Steps 2 and 3 are repeated until the cluster head remains unchanged or changes little.

3.2.2. Models of latency and content cache cost gains
The cloud-edge cooperative environment studied in this article includes a remote cloud server, E edge servers, F video files, and multiple mobile users. Let  and  demotes the sets of edge server and video file respectively. Each video file is divided into several segments with D bits. Let  denote the set of segments of file , and  represents the total number of the segments of file I,  denotes a segment of file I,  represents the number of segments of file i cached in the edge server , which can be modeled as , where  denotes the relationship of  and en. If the segment  is cached in en then , otherwise . Let  denotes the maximum cache capacity of . Let the number of segments cached in en be the caching cost of en. Then, the cache cost of en satisfies the constraint . Therefore, the content cache cost gain of the edge servers can be modeled as follows(1)

If the user accesses the edge server en, and the segment  is cached in en, then the user can download the segment  directly from en. The latency of the user directly downloading the segment from the edge server en is(2)where D represents the size of the segment,  represents the data transfer rate between the user and en, which is modeled as follows:(3)where B represents the system bandwidth,  denotes the number of users accessing en.  denotes the proportion of bandwidth allocated to en, which satisfies  and .  denotes the ratio of signal to inference plus noise (Chen et al., 2017a), which can be regarded as a random variable. It can be seen from the models (2) and (3) that the change of the bandwidth allocation ratio  will affect the data transmission rate . The change of  will affect the latency . Therefore, a reasonable allocation of the bandwidth of the edge servers, i.e., determining the appropriate  can reduce the delay in content download.

If the segment  is not cached in en, then en requests the segment  from the cluster where en is located. If the edge server enc in the cluster caches the segment , the edge server en will download the segment from the edge server enc and forward it to the user. The data transfer rate between en and enc can be modeled as , where  is the bandwidth between the edge servers en and enc,  represents the ratio of signal to inference plus noise between en and enc. Therefore, the latency for user downloading the segment from enc in the cluster can be modeled as .

In the cloud-edge cooperative environment, edge servers are distributed in different geographical locations. Due to network congestion (Li et al., 2020c) and other reasons, the data transmission speed between edge servers in the collaborative cache domain is not always better than that between the edge server and the remote cloud server. If enc within the cluster does not cache the segment , the edge server en requests the segment  from the edge server within the collaborative caching domain outside the cluster. If the edge server ena in the collaborative cache domain caches the segment , then the edge server en will calculate the latencies of obtaining the segment  from the edge server ena and the remote cloud server, and select the way with the smallest latency to obtain the segment . Assuming that the latency for the edge server en to obtain the segment  from the remote cloud is , then the latency for the user to obtain the segment  from the remote cloud server through en can be expressed as .

The latency gain caused by caching the segment fs on the edge server en for users who access en and request fs can be modeled as(4)where  denotes the popularity of segment  in edge server en,  denotes the total number of times that the segments on en has been accessed,  represents the number of times that segment  on en has been accessed. When segment  is cached in the edge server en, users who download the segment  from other edge servers in the cluster where en is located will have a certain degree of latency gain. Assuming that user accessing edge server enc in the cluster needs to request segment , then the latency gain caused by caching the segment  in en for users who access enc and request the segment  can be modeled as(5)

Based on this, the average latency gain caused by caching the segment  in en for the cluster where en is located can be modeled as(6)where  represents number of edge servers in cluster where en is located. Similarly, the latency gain caused by caching the segment  in en for users who access ena in the collaborative cache domain and request the segment  can be modeled as(7)where  denotes the latency caused by the user downloading the segment  from the edge server ena in the collaborative cache domain. Based on this, the average latency gain caused by caching the segment  in en for the collaborative cache domain is located can be modeled as(8)where  represents number of edge servers in collaborative cache domain. In summary, the latency gain caused by caching the segments in the cloud-edge cooperative environment can be modeled as follows(9)

3.3. Problem formulation
In the cloud-edge cooperative environment, the caching cost and the content download latency affect each other. On the one hand, caching the content on the edge server reduces the content download latency for users. On the other hand, caching the content on the edge server increases the cache utilization rate, thereby increasing the content caching cost. Therefore, the optimization goal of the proposed content caching strategy tends to maintain a balance between content download latency and content caching cost. The problem of the optimal trade-off between latency and cost in content caching model can be presented as follows:(10)s.t.(10a)(10b)(10c)(10d)

(10a) is the edge server cache capacity constraint, which denotes the number of segments cached on the edge server that cannot exceed the maximum cache capacity of the edge server. (10b) is bandwidth allocation constraint, where  denotes the bandwidth allocation ratio of edge server en.  denotes the normalized value of the content download latency gain, which can be modeled as(11)where 
 denotes the maximum gain of content download latency, i.e., the latency gain brought by users downloading all contents from the edge servers. 
 denotes the minimum gain of content download latency, i.e., the latency gain brought by users downloading all contents from the remote cloud server. In this paper, 
 is set to 0, i.e., downloading contents from cloud center will not bring latency gain. 
 
 denotes the normalized value of the content caching cost gain, which is(12)
 
 
where 
 denotes minimum gain of the content caching cost, i.e., the content caching cost gain brought by that all the cache space of edge servers are used to cache contents. 
 denotes the maximum gain of the content caching cost. In this paper, 
 is set to 1, i.e., data caching cost brought by caching only one segment in the edge server.

However, the content caching is discrete and the bandwidth allocation is continuous. Therefore, P1 is a mixed integer programming problem. The method that ignores the direct relationship between bandwidth allocation and content caching is used to solve P1. The solution process of P1 is divided into two steps: First, the optimal bandwidth allocation ratio 
 is obtained by using the Lagrange multiplier method (Koutsopoulos and Iosifidis, 2010). Second, a content caching algorithm is proposed for solving P1 and obtain latency-cost trade-off threshold, and then the contents are cached in the edge servers according to this threshold.

4. Solution
The proposed content caching strategy is mainly composed of the edge servers clustering and the content caching. For the process of the edge servers clustering: The K-Means algorithm is used to divide all edge servers in cloud-edge cooperative environment into K clusters. Then, the latencies required for the user to download the segment from the local edge server, the cluster, the collaborative cache domain and remote cloud server are respectively calculated. For the process of content caching: First, the latency gain and content caching cost gain brought by caching segment are calculated. Then, the content caching marginal gain is calculated according to the obtained latency gain and content caching cost gain. Finally, the Latency-cost trade-off threshold is calculated by analyzing the content caching marginal gain, and the content caching in the edge server is performed according to the threshold. In addition, in order to reduce the load of data processing and data transmission, the processes of edge server clustering and content caching be performed during off-peak hours of content requests, such as every night (Mehrizi et al., 2019).

4.1. Content caching algorithm
According to (10), the content cache gain in the cloud-edge cooperative environment can be modeled as 
 
 
. Then, after segment 
 is cached in edge server en, the content download latency gain 
 
 and the content caching cost 
 
 change accordingly, i.e., 
 
 increases and 
 
 decreases. So, the content cache gain changes accordingly, which can be modeled as 
 
 
, where 
 
 and 
 
 respectively represent the latency gain and the content caching cost gain that change after the segment 
 is cached. Based on this, after caching the segment 
 of file i on the edge server, the marginal gain of content caching can be modeled as 
. If another segment of file i is continuously cached in the edge server, then the content caching gain changes from 
 to 
 
 
, and the marginal gain changes from 
 to ′.

To ensure that the content download latency can be reduced, caching the segment 
 needs to make the condition 
 satisfied. 
 means that caching the segment 
 can reduce the content download time under the premise of a certain content caching cost. Therefore, the proposed strategy finds maximum number of segments of file i cached in the edge server according to condition 
, which is called the latency-cost trade-off threshold. The remote cloud server caches the segments on the edge servers according to this threshold to maintain the balance between the latency and the content caching cost.


Algorithm 1. Latency-Cost Trade-Off Threshold-based Content Caching Algorithm

INPUT: The set of edge server: ; The set of file: ; The set of segment of file i:
OUTPUT: 
 latency-cost trade-off threshold list
1: en = 1
2: WHILE  DO
3: The transmission ratio 
 of the edge server en is calculated; en++
4: END WHILE
5:
, 
 //The number of segments of file i cached on the edge server en is initialized to 1, the latency-cost trade-off threshold list is initialized to NULL.
6: FOR EACH file  DO
7: FOR EACH segment 
 DO
8: The content caching gain 
 brought by caching the segment 
 is calculated
9: END FOR
10: The quick sort algorithm is used to sort the segments in file i in order of content caching gain 
 from largest to smallest.
11: END FOR
12: FOR EACH edge server en ∈ ES DO
13: FOR EACH file  DO
14: WHILE 
 DO
15: IF 
 THEN
16: 
 //If the condition 
 is met, the number of segments of file i cached in en is increased
17: ELSE
18: 
 is added to 
19: break
20: END IF
21: END WHILE
22: END FOR
23: END FOR 24: RETURN 
;
The process of latency-cost trade-off threshold-based content caching algorithm is described in Algorithm 1. In this algorithm, ,  and 
 are obtained after being initialized. The transmission ratio 
 of  is calculated (line 1–4). Number of segments of file i cached on the edge server en is initialized to 1, and the latency-cost trade-off threshold list is initialized to NULL (line 5). The content caching gain 
 brought by caching the segment 
 of file i is calculated (line 6–9). The quick sort algorithm is used to sort the segments in file i in order of content caching gain 
 from largest to smallest (line 10–11). For edge server , all segments of file  are evaluated in turn whether they meet the condition 
. If the condition is met, the threshold 
 of file i is increased by 1 (line 12–16), otherwise the calculation of the threshold 
 of file i is stopped and 
 is added to 
. The calculation of the threshold of the next file is started (line 17–21). Latency-cost trade-off threshold list 
 is returned (line 24).

The complexity analysis of Algorithm 1 is as follows: In line 2–4, the time complexity is , E represents the number of edge servers. In line 6–11, the time complexity is , F and FS represent the number of files and the number of segments contained in the file respectively.  is the time complexity required for quick sorting of FS segments. In line 12–23, the time complexity is O (E × F × FS). Therefore, the time complexity of Algorithm 1 is O (E × F × FS). The flowchart of the latency-cost trade-off threshold-based content caching algorithm is shown in Fig. 3.

Fig. 3
Fig. 3. The flowchart of Algorithm 1.

After the threshold 
 is obtained by Algorithm 1, the cloud server caches the segments of file i to edge server en in the order of the marginal gain from large to small, until the number of placed segments reaches the threshold 
. In the process of caching the segment, if the cache space of the edge server en is full, en will perform the process of marginal gain-based content replacement to delete part of the old segment to cache the new segments. The process of marginal gain-based content replacement is as follows: The marginal gain of segments cached in en are calculated. If the marginal gain of the segment to be cached is greater than the marginal gain of any segment cached in edge server en, then the segment with the smallest marginal gain in edge server en is deleted to cache the new segment.


Algorithm 2. K-Means based Edge Server Clustering Algorithm

Input: : The set of edge servers; K: The number of clusters; : The set of cluster head; : The set of coordinates of edge server;
Output: ;
1: K edge servers are randomly selected as initial cluster heads and added to the set CH.
2: While CH_OLD≠CH do
3: CH_OLD = CH//CH_OLD is initialized, which is used to record the result of the last clustering iteration.
4: H = ∞//H is initialized, which is used to record the cluster head with the shortest distance from the edge server.
5: HH = 0//HH is initialized, it is used to record the distance between the edge server and the cluster head.
6: For each edge server en ∈ ES do
7: For each cluster head ch ∈ CH do
8: If H> then
9: HH = , H = ch
10: End if
11: End for
12: en is added to cluster 
 where cluster head H is located.
13: End for
14: For each cluster head  do
15:  //x and y are initialized, they are used to record the sum of the coordinates of all edge servers in the cluster.
16: For 
 do
17: 
18: End for
19: The average coordinate 
 of all edge servers in cluster 
 is calculated according to x and y.
20: End for
21: For each cluster head  do
22: Z = ∞//Z is initialized, which is used to record the minimum distance between the edge server in the cluster and the average coordinate.
23: ZZ = 0//ZZ is initialized, which is used to record the new cluster head
24: For 
 do
25: If Z>
 do
26: Z = 
, ZZ = en
27: End if
28: End for
29: ch = ZZ//Cluster head ch is updated
30: End for
31: End while
32: return 
4.2. Edge server clustering algorithm
In Algorithm 2, the process of the K-Means based edge server clustering algorithm is described. In this algorithm, the set  of edge servers, the number K of clusters, the set  of cluster heads and the set  of edge server coordinates are obtained after being initialized. K edge servers are randomly selected as initial cluster heads and added to the set CH (line 1). CH_OLD is initialized, which is used to record the result of the last clustering iteration (line 3). If the distance between the edge server  and the cluster head  is the shortest compared to the distance between the edge server  and other cluster heads, then  is added to the cluster 
 where  is located (line 6–13). The position 
 corresponding to the average of the coordinates of all edge servers in cluster 
 is calculated (line 14–20). If the edge server 
 in cluster 
 is the closest to location 
, then en is selected as the new cluster head of cluster 
 (line 21–30). If the condition CH_OLD≠CH is not met, then line 32 of the Algorithm 2 is executed. The set of cluster head  is returned (line 32).

The complexity analysis of Algorithm 2 is as follows: In line 6–13, the algorithm complexity is O (E × K), E and K represent number of edge servers and number of cluster heads respectively. In line 14–20 and line 21–30, the algorithm complexity of is O(K × C), C represents the average number of edge servers included in all clusters. Assuming that the while loop of line 2–31 will end after N iterations, since C < E, the time complexity of K-Means based edge server clustering algorithm is O (E × K × N). The flowchart of K-Means based edge server clustering algorithm is shown in Fig. 4. The flowchart of the k-means based edge server clustering algorithm is shown in Fig. 3.

Fig. 4
Fig. 4. The flowchart of Algorithm 2.

5. Experiment
5.1. Experiment settings
The experimental environment of the proposed content caching algorithm is presented in Fig. 5. To be more specific, the hardware platform includes a remote cloud center, 10 edge servers and 16 mobile devices. The hardware platform includes a remote cloud data center (CS) and 10 edge servers (ES1~10). Edge servers are deployed in the three campuses of Wuhan University of Technology (Yujiatou campus, Mafangshan campus, and Nanhu campus), of which ES1~4 are deployed on Yujiatou campus, ES5~7 are deployed on Mafangshan campus, ES8 ~10 are deployed on the Nanhu campus. Yujiatou campus is about 9 km and 11 km away from Mafangshan campus and Nanhu campus, and the distance between Mafangshan campus and Nanhu campus is about 3 km. The mobile devices are the different types of mobile phones, which can be connected to the edge servers through the wireless access points. Each mobile device is carried by a worker, and the workers move between different edge servers. When a worker carrying a mobile device moves within the service range of an edge server, the mobile device will connect to this edge server. When a worker moves from the service area of an edge server to that of another edge server, the mobile device will disconnect from the former and establish a connection with the latter. The detailed information of hardware is presented in Table 2. In terms of software, Openstack++ cloud platform (Ha and Satyanarayanan, 2015) is deployed on the edge servers to provide computing, storage, and network related services. Meanwhile, the video files are distributed from remote cloud to edge servers.

Fig. 5
Fig. 5. The experimental environment of optimized content caching strategy.

The dataset used in this paper is obtained from a large video dataset YouTube-8M, which is published by Google (Abu-El-Haija et al., 2016). The YouTube-8M includes 8 million video URLs and the duration of all videos exceed 0.5 Million hours. Furthermore, each video has at least 1000 frames, the duration of these videos is between 120s and 500s. In the experiment, 700 videos are downloaded from YouTube-8M as the video source file.

5.2. Evaluation metrics
In this paper, Content Caching Cost (CCC), Content Download Latency (CDL), Cache Hit Ratio (CHR), and Backhaul Traffic Load (BTL) (Tran et al., 2017b) are used to evaluate the performance of the proposed algorithm. CHR can be modeled as(13)
 
where 
 represents the number of times that 
 is obtained from the edge server, 
 represents the total number of times 
 is requested. CDL represents the average latency of users downloading content from edge servers. CCC represents the normalized value of the total number of segments cached in edge servers. CCC can be modeled as(14)
 
where 
 denotes the maximum content caching cost, i.e., the cost of all the cache space of all edge servers being used for caching content. 
 denotes the minimum content caching cost. In this paper, 
 is set to 0, i.e., if no contents are cached in the edge servers, there will be no content caching cost. BTL represents the traffic from the remote cloud server through the backhaul link, which can be modeled as(15)
where 
 represents the number of times 
 has been requested on the remote cloud server, D denotes the length of the segment.

5.3. Benchmark algorithms
To evaluate the performance of proposed algorithm, the following algorithms are selected as the benchmark algorithms:

5.3.1. Mobility prediction based-content caching (MPCC) (Song et al., 2017)
This algorithm uses Markov chains to predict user mobility. Then based on the user's content request, this algorithm caches the content at the edge node that the user is most likely to reach in the future.

5.3.2. Segment based-caching optimization (SCO) (Wang et al., 2019)
In this algorithm, the video content is divided into multiple segments for caching. At the same time, the algorithm considers the popularity of video content segments and content transmission delay.

5.3.3. Local content cloud based-cooperative caching (LCCC) (Yang et al., 2019)
This algorithm considers the influence of the capacity of the cooperative link and backhaul link between edge nodes. And the goal of this algorithm is to minimize the content download delay and transmission cost.

5.3.4. Hit ratio maximization (HRM) (Chen et al., 2017b)
This algorithm analyzes the tradeoff between transmission diversity and content diversity. Also, the algorithm researches the cache space assignment optimization for maximizing the performance of cache service.

5.3.5. Cost minimization (CM) (Araldo et al., 2014)
This algorithm presents two optimized caching models. One is to choose minimize the cost as the primary target and the other is to choose maximize the hit ratio as the primary goal. Our algorithm is compared with the cost-minimize method.

5.4. Comparison and analysis
In Fig. 6, the CHR, CDL, CCC and BTL of the proposed content caching algorithm is evaluated under the changes of Relative Cache Capacity (RCC). Specifically, CHR and CCC increase as the RCC increases, whereas the CDL and BTL decrease. In Fig. 8, the HRM algorithm presents the highest CHR in Fig. 8(a), but does not achieve the lowest CDL in Fig. 8(b). This is because the HRM only considers the diversity of cached contents but not the bandwidth allocation. However, the proposed algorithm reduces the CDL by 21.32% compared with the HRM algorithm. For CM algorithm, the proposed algorithm increases the CHR by 11.74% and reduces the CDL by 35.36%. As shown in Fig. 8(c), the CM algorithm can save the most cost. However, most of the cost saved of CM algorithm comes from the content caching costs. Therefore, the BTL of CM algorithm is the highest. The proposed algorithm has a CCC of 15.12% higher than the CM algorithm, while the BTL is reduced by 21.53%.

Fig. 6
Fig. 6. Impact of cache size.

In Fig. 7(a)~(c), the CHR performance of the proposed content caching algorithm is evaluated under the changes of Relative Cache Capacity (RCC), Number of Edge Servers (NES), and Number of Users (NU). In Fig. 7(a) and (b), CHR increases as the increase of RCC and NES. This is mainly because the larger the RCC, the more contents the edge server can cache, which allows more contents can be downloaded in edge servers, thereby improving the CHR. On the other hand, in Fig. 7(b), the more NES, the more edge servers can be used to cache content, thereby improving the CHR. In Fig. 7(c), the CHR decreases with increase of NU. This is mainly because the increase of NU increases the number of users' content requests. Due to the limited cache capacity, the edge server cannot meet the increasing users' content requests, thereby reducing the CHR. In addition, in Fig. 7, compared to LCCC, SCO, and MPCC, the proposed content caching algorithm increases the CHR by 12.28%, 22.37%, and 35.23% on average. The reason is as follows: The proposed content caching algorithm take into account the content popularity when constructing latency gain model, which allows edge servers to cache more content with high popularity, thereby improving CHR.

Fig. 7
Fig. 7. Performance evaluation of Cache Hit Ratio.

Fig. 8
Fig. 8. Performance evaluation of Content Download Latency.

In Fig. 8(a)~(d), the CDL performance of proposed content caching algorithm is evaluated under the changes of Relative Cache Capacity (RCC), File Request Arrival Rate (FRAR), Number of Edge Servers (NES), and Number of Users (NU). In Fig. 8(a) and (b), CDL increases with the increase of RCC and decreases with increase of FRAR. This is mainly because the increase of RCC allows more contents are cached on the edge server, so that users can obtain more requested contents from edge servers, thereby decreasing the CDL. On the other hand, the higher the FRAR, the higher the data processing load of edge server, which increases CDL. In Fig. 8(c) and (d), the CDL decreases with the increase of RCC and increases with increase of FRAR. This is mainly because the increase of NEC allows more content to be cached on edge servers, thereby reducing CDL. On the other hand, the increase in the NU increases the number of content requests, which increases the edge server's load, thereby increases CDL. In Fig. 8, compared to LCCC, SCO, and MPCC, the proposed cache caching algorithm decreases the CDL by about 9.52%, 27.36%, and 38.17%. The reason is as follows: In our algorithm, if the local edge server does not cache the requested content, then local edge server will request the content from cluster. If the cluster does not cache the content, then local edge server looks for it in the collaboration cache domain. If the content exists in the collaboration cache domain, the local server determines the delay for downloading the content from the collaboration cache domain and cloud server, and selects the method with the shortest delay to obtain the content. This method of providing content to users through collaboration between edge nodes can effectively reduce the delay of content download. In LCCC, only adjacent edge nodes collaborate to form a local cloud. If the local cloud dose not cache the requested content, then the content is directly requested in the remote cloud. Although LCCC reduces latency to some extent by means of edge node collaboration, it ignores the impact of edge node collaboration on latency performance in the collaboration cache domain. Therefore, our algorithm is better in terms of latency.

In Fig. 9(a)~(c), the Content Caching Cost (CCC) of algorithms is evaluated under the changes of Relative Cache Capacity (RCC), and the File Request Arrival Rate (FRAR). In Fig. 9(a), the CCC increases with the increase of RCC. This is mainly because with the increase of RCC, the edge servers can cache more contents, which increases the cost of content caching, thereby increasing the CCC. In Fig. 9(b), the CCC increases as the increase of FRAR. This is mainly because the increase in the number of user content requests makes the content caching algorithm place more contents in the edge servers, thereby increasing CCC. In Fig. 9, compared to LCCC, SCO, and MPCC, our algorithm decreases the CCC by about 15.71%, 26.14%, and 35.87%. This is mainly because our algorithm optimizes content caching cost, so the performance of our algorithm is better than comparison algorithms in terms of content caching cost.

Fig. 9
Fig. 9. Performance evaluation of Content Caching Cost.

In Fig. 10(a)~(b), the Backhaul Traffic Load (BTL) performance is evaluated under the changes of Relative Cache Capacity (RCC) and File Request Arrival Rate (FRAR). In Fig. 10(a), BTL decreases with the increase of RCC. This is mainly because the increase in RCC allows edge servers to cache more contents, which increases the number of times users get contents from edge servers, thereby reducing BTL. In Fig. 10(b), BTL increases as increase of RCC. The reason is as follows: Due to the limited cache capacity, the edge server cannot cache all files to meet the increasing user file requests, which makes more and more users unable to get the contents from the edge server, thereby increasing BTL. Compared to LCCC, SCO, and MPCC, our algorithm reduces the BTL by 18.14%, 36.45%, and 41.63% on average.

Fig. 10
Fig. 10. Performance evaluation of Backhaul Traffic Load.

In Fig. 11, the impact of the marginal gain-based passive cache replacement method on the performance of the proposed content caching algorithm is evaluated. In this figure, Random cache replacement-based Content Caching Algorithm (RCCA) and the LRU based Content Caching Algorithm (LRUCPA) are used as the comparison algorithm of the Marginal Gain cache replacement-based Content Caching Algorithm (MGCPA). RANDOM and LRU are the proposed content caching algorithms that integrate the random cache replacement method and the LRU cache replacement method (Chrobak and Noga, 1999), respectively. While MG is the proposed content caching method that integrates the marginal gain based-passive cache replacement method. When the cache capacity is insufficient, the random cache replacement method deletes a random content from the cache space to cache new content, and the LRU cache replacement method deletes the least recently used content.

Fig. 11
Fig. 11. The impact of the marginal gain-based passive cache replacement method on the performance of the proposed content caching algorithm.

In Fig. 11, Content Download Latency (CDL) of the content caching algorithms is evaluated under the changes of Relative Cache Capacity (RCC) and File Request Arrival Rate (FRAR). Specifically, CDL decreases and increases with the increase of RCC and FRAR respectively. Compared to RCCA and LRUCPA, MGCPA reduces CDL by about 30.23% and 18.91%. The main reason is that MGCPA considers the marginal gain when deleting content, where the marginal gain is constructed by considering content download latency and content caching cost. MGCPA selects the content with the smallest marginal gain in the edge server to delete, which makes the deleted content have the least impact on the latency. For RCCA and LRUCPA, they did not consider the impact of deleted content on the delay when performing cache replacement. Therefore, the CDL of MGCPA is lower than that of RCCA and LRUCPA.

6. Conclusion and future work
The optimization of the content caching problem in the cloud-edge cooperative environment is studied in this paper. We designed a content collaborative caching strategy, which comprehensively considers the cache cost gain and the latency gain brought by caching content on the edge server, and formulates the content caching problem that maximizes the latency gain and the cache cost gain. To solve this problem, we propose a content cooperative caching algorithm, which calculates the marginal gain of content caching by analyzing the latency gain and cache cost gain, and caches the content according to the marginal gain to achieve the trade-off between latency and caching cost. We use a real-world YouTube-8M video dataset for extensive experimental validation. We demonstrate that our content cache strategy is superior to traditional cache management strategies in cache hit ratio, content download latency, content caching cost and backhaul traffic load. Furthermore, our model will be extended in the future. We hope to consider more factors when define the content popularity and analyze the popularity evolution.