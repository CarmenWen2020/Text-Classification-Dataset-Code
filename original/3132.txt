Online summer courses offer opportunities to catch-up or stay on-track with course credits for students who cannot otherwise attend face-to-face summer courses. While online courses may have certain advantages, participation patterns and student success in summer terms are not yet well understood. This quantitative study analyzed four years of institutional data cumulating in 72,441 course enrollments of 23,610 students in 433 courses during summer terms at a large public research university. Multi-level logistic regression models indicated that characteristics including gender, in-state residency, admission test scores, previous online course enrollment, and course size, among others, can influence student enrollment by course modality. Multi-way fixed effects linear regression models indicated that student grades were slightly lower in online courses compared to face-to-face courses. However, at-risk college student populations (low-income students, first-generation students, low-performing students) were not found to suffer additional course performance penalties of online course participation.

Previous
Next 
Keywords
Higher education

Online courses

Distance education

Summer session

At-risk students

1. Introduction
Online learning and distance education are becoming an integral part of higher education in the United States. In 2003, only 15.6% of all degree-seeking U.S. undergraduate students had enrolled in at least one distance education class during their higher education career. As of 2014, more than a quarter of all undergraduate students (27.7% or 4.8 million students) had enrolled in at least one distance education class (Snyder, de Brey, & Dillow, 2016). Similarly, higher education administrators increasingly value online education as a critical component of long-term strategic plans for their institutions (Allen & Seaman, 2013). Advocates suggest that online learning might not only provide more cost-effective instruction but also increase access to learning opportunities and college completion for students (Shea & Bidjerano, 2014).

However, most research on college-level online course settings suggests that students do not perform as well in online courses compared to traditional face-to-face courses with respect to course completion, course grades, and subsequent college enrollment (Atchley, Wingenbach, & Akers, 2013; Bettinger, Fox, Loeb, & Taylor, 2017; Figlio, Rush, & Yin, 2013; Jaggars & Xu, 2016; Xu and Jaggars, 2011, Xu and Jaggars, 2013, Xu and Jaggars, 2014). These performance gaps are often attributed to student abilities in self-directed learning, which is particularly important in fully online course work to stay on pace with the course learning materials and objectives (Broadbent, 2017; Cho, Kim, & Choi, 2017; Firmin et al., 2014; Kizilcec, Pérez-Sanagustín, & Maldonado, 2017; Parkes, Stein, & Reading, 2015; You, 2016). Nonetheless, online courses can fulfill an important role to complement an institution's traditional face-to-face course schedule and to help advance students' academic success in higher education. For instance, colleges that offer online courses during summer terms might provide students who work full-time off-campus or students who temporarily moved to a different city a flexible opportunity to maintain or even accelerate their time-to-degree (e.g., Fish & Kowalik, 2009; Taylor & Doane, 2003).

While numerous research studies have compared the outcomes of either online courses with face-to-face courses or summer with non-summer courses across post-secondary contexts, the research base on the intersection of online learning and summer course learning remains thin. This study attempts to address this gap by examining student enrollment during summer terms and the effect of course modality on student performance. Consequently, this study is framed by the following three research questions:

Research question 1: What types of students enroll in college courses during summer terms?

Research question 2: What student- and course-level characteristics predict student enrollment in online courses (compared to face-to-face courses) during summer terms?

Research question 3: What is the impact of course modality on student course performance during summer terms?

2. Background
2.1. Distance learning in higher education
Online college courses can have certain advantages over traditional face-to-face courses. Most importantly, online instruction allows students to take courses at any time from virtually any location. This provides students with more flexible learning opportunities, to learn at a time that is most beneficial for their schedule, unlike face-to-face courses, which are dictated by a specific time and place (Nguyen, 2015). Online courses also better serve student needs by providing greater access to courses that students may have failed earlier or that may not be available due to scheduling constraints or course over-enrollment (Bailey, Gosper, Ifenthaler, Ware, & Kretzschma, 2018).

Despite the potential advantages of online courses and some evidence that online course settings are indeed advantageous to students (Fischer, Zhou, Rodriguez, Warschauer, & King, 2019; Dolan, Hancock, & Wareing, 2015; Means, Toyama, Murphy, & Baki, 2013), most research examining college-level courses suggests that students do not perform as well in online courses compared to traditional face-to-face courses (Bettinger et al., 2017; Figlio et al., 2013; Jaggars & Xu, 2016; Xu and Jaggars, 2011, Xu and Jaggars, 2013, Xu and Jaggars, 2014). For instance, a recent study looking at > 230,000 students who enrolled in > 168,000 sections across four years at a non-selective four-year college found that students who took the online version of a course performed worse on average than students who took the face-to-face version of the course (Bettinger et al., 2017). This study also found that students who enrolled in online courses were also more likely to obtain lower grades in future courses and less likely to be enrolled in school a year later. Similarly, course completion has been found to be lower for students enrolled in online courses (Atchley et al., 2013; Levy, 2007; Murphy & Stewart, 2017; Rovai, 2003).

Part of the problem has to do with students' ability to self-direct their learning. While self-directed learning skills are desirable and important for student learning in any modality, self-directed learning is particularly important in fully online course work to stay on pace with the course learning materials and objectives (Broadbent, 2017; Cho et al., 2017; Firmin et al., 2014; Kizilcec et al., 2017; Parkes et al., 2015; You, 2016). For instance, in a recent systematic review of the literature, important self-directed learning strategies, such as management, metacognitive monitoring, and effort regulation, were associated with academic achievement among college students enrolled in online courses (Broadbent & Poon, 2015). Consequently, performance gaps between online and face-to-face coursework might be attributed to students' inability to self-direct their own learning experiences.

In particular, students who are traditionally at-risk in college settings (e.g., low-income students, underperforming students, students from underrepresented racial/ethnical backgrounds, first-generation students) might lack the necessary experience or self-directed learning skills required to succeed in online learning environments (Williams & Hellman, 2004). While online course participation can provide broader access to higher education learning opportunities (Castillo, 2013; Green, 2006), research suggests that at-risk students tend to suffer additional course performance penalties in online course environments (Figlio et al., 2013; Kaupp, 2012; Xu and Jaggars, 2013, Xu and Jaggars, 2014). For instance, Figlio et al. (2013) randomly assigned students to an online or face-to-face lecture format in a large introductory microeconomics course and found that Latino students experienced an additional course performance penalty in an online versus face-to-face format compared to their non-Latino peers.

2.2. Summer terms in the U.S. higher education system
Summer constitutes a unique term in the academic calendar for most 4-year colleges and universities in the United States. Since the University of Chicago's incorporation of the summer quarter in its regular academic year in 1891, the popularity of summer sessions has grown in the U.S. (Harbor & Nemelka, 2016). In 2012, more than two million students chose to enroll in summer sessions throughout North America (Lytle, Kops, & Seaman, 2014). Universities offer summer courses for many reasons that may be related to institutions' core academics mission, potential for academic innovation, as well as economic consideration.

From the perspective of the core academic mission, universities need to offer a sufficient number of courses to allow students to graduate with their desired degree within expected time frames. Summer courses provide students with opportunities to enroll in courses counting towards degree requirements to help students to graduate and to maintain or even reduce their time-to-degree (Donnelly & Kessler, 2000; Fish & Kowalik, 2009; Martin, 2003; Smith & Byrd, 2015; Taylor & Doane, 2003). Summer courses can enable students to catch up on coursework that could not be completed during the regular academic year, for instance, because students did not initially meet the academic course requirements.

Also, universities might provide students with additional course enrollment opportunities in high-demand courses that could not be offered in sufficient capacity during the regular academic year (e.g., courses requiring laboratory access), addressing overflow problems (Donnelly & Kessler, 2000; Fish & Kowalik, 2009; Martin, 2003; Smith & Byrd, 2015; Taylor & Doane, 2003). Nationally representative data of the U.S. National Educational Longitudinal Study (NELS:88/2000) indicated that students who earned at least 4 credits during summer terms were substantially more likely to complete their Bachelor's degree (Adelman, 2006).

Additionally, universities might offer summer courses to facilitate academic innovation. Summer courses can provide instructors with opportunities to utilize alternative course settings, unconventional course formats, and novel teaching strategies to expose students to learning experiences that might differ from courses offered during the regular academic year (Fish & Kowalik, 2009; Martin, 2003; Smith & Byrd, 2015). For instance, Fish and Kowalik (2009) reported that reasons for students to enroll in summer terms included smaller class sizes, a more flexible schedule, and better faculty support compared to the regular academic year. Also, summer terms provide universities with opportunities to attract visiting scholars to not only enhance research activities and collaborations in hosting departments but also enable students to enroll in courses that might not be traditionally offered at their institution (Heinz & Lewis, 2009).

Finally, summer terms are attractive for both universities and students from an economic perspective. On the one hand, universities can utilize their existing facilities and personnel resources to generate additional revenue during summer terms through student tuition fees (Dev, 2005; Lytle et al., 2014; Smith & Byrd, 2015). On the other hand, summer courses might also provide financial incentives to students with reduced tuition fees during the summer compared to the regular academic year, as well as a potential reduction of students' time-to-degree, which might be associated with lower overall fees and costs (Fish & Kowalik, 2009).

2.3. Enrollment patterns in summer courses
Another area of work has focused on understanding what types of students enroll in summer courses and whether these differences are associated with better or worse learning outcomes. When examining background characteristics, some work found that students' gender did not substantially influence summer enrollment (Smith & Read, 2013; Taylor & Doane, 2003). However, students' age was positively related to enrolling in summer courses, such that older students were more likely to enroll in summer terms. This work also found that African American students were most likely to enroll in summer terms compared to Hispanic and White students. Interestingly, students enrolled in summer courses had lower overall family income and higher student loans, and were more likely to have full-time work obligations (Smith & Read, 2013).

When examining students' academic standing, studies have found that standardized college admission test scores (i.e., SAT/ACT examinations) and college grade point averages (GPAs) are mostly not associated with a greater likelihood of enrolling in summer terms (Seamon, 2004; Smith & Read, 2013; Taylor & Doane, 2003). Most prior studies that examined academic performance indicated that student performance (i.e., course grades, failure rates) did not substantially differ between courses during summer and the regular academic year (Anastasi, 2007; Seamon, 2004; Simunich, 2016). However, some studies indicated that summer course participation might help improve student performance (Austin & Gustafson, 2006; Seamon, 2004).

In general, studies have examined enrollment patterns and effects of (a) summer learning and (b) online learning separately; the present study explores the intersection of these two areas of study by examining them simultaneously. Better understanding students' summer and online enrollment patterns and course success are critical for expanding and improving existing post-secondary summer programs.

3. Methodology
3.1. Data sources and sample
This study was conducted at a large public research university in California, as part of a National Science Foundation-funded project that systematically explores the effectiveness of online, hybrid, and flipped STEM courses in large research universities. Data for this study were provided from a number of sources on campus that collect and curate institutional data including Admissions, Office of Financial Aid and Scholarships, Summer Session, the Registrar's Office, the Office of Institutional Research, and the Office of Information Technology.

This study focused on course enrollments among degree-seeking undergraduate students in lecture courses that were graded on a letter-grade scale (in contrast to a pass/no-pass distinction) and awarded at least four credit points during summer terms from 2014 to 2017 (almost all lecture courses award four credit points; 12 credit points are considered full-time enrollment during spring, fall, and winter terms, whereas 6 credit points are considered full-time enrollment during summer terms). Courses in this study were described by department code and course title (e.g., “BIO SCI 98: Biochemistry”) accounting for cross-listed courses across multiple departments. Notably, a course can have multiple course sections in the same term (e.g., “BIO SCI 98: Biochemistry, Section A” and “BIO SCI 98: Biochemistry, Section B”). Course sections of the same course might be taught by different instructors or be offered in a different course modality. Overall, the sample included 72,441 course enrollments from 23,610 students in a total of 433 courses.

3.2. Measures
3.2.1. Dependent variables
The dependent variables of this study were course modality (research question 2) and student course performance (research question 3). Course modality was a dichotomous variable (0: face-to-face, 1: online) and was determined by examining the University Registrar's schedule of classes, which lists the course modality for each course. The third research question used course modality as the treatment variable considering the “online” course modality as the treatment. Student course performance was measured through the letter grade students received in the course. Course grades were treated as a continuous variable by converting letter grades to numerical grades, following the institution's grade conversion policy: A+, A, A-: 4.0, 4.0, 3,7; B+, B, B-: 3.3, 3.0, 2.7; C+, C, C-: 2.3, 2.0, 1.7; D+, D, D-:1.3, 1.0, 0.7; F: 0.0. Notably, both A+ and A were converted to the same numerical grade.

3.2.2. Independent variables
The independent variables used in this study included student-level variables (i.e., demographics, family background, and academic career) and course-level variables. Demographic variables included students' gender (0: Male, 1: Female), and students' racial/ethnic background (White; Asian or Asian American; Black or African American; Latino or Hispanic; American Indian, Alaska Native, or Pacific Islander). Family background variables included students' English language learner status (0: English or English and another language is students' first language, 1: Language other than English is students' first language), in-state residency status (0: Out-of-state student, 1: In-state student), low-income status (0: Not flagged as low-income based on family household income and household size using 185% of the U.S. poverty line, 1: Flagged as low-income), and first-generation college student status (0: at least one parent holds a Bachelor's degree or higher, 1: neither parent holds a Bachelor's degree).

Academic career variables included whether the student transferred to the current institution (0: Started college at same institution, 1: Transfer student), student admission scores (on a 60–300 scale) based on students' ACT/SAT examination results, student GPA during the term of course enrollment, the years of enrollment, the number of attempts re-taking the same course, and the number of online courses previously taken during students' college career. Also, a course-level independent variable that described the number of students enrolled in the course was included.

Notably, three student-level independent variables were used to generate the subgroups of at-risk college student populations. The dichotomized “first-generation college student status” and “low-income status” variables led to straightforward subgroup classifications. Student performance subgroup classifications were generated using students' admission test scores (e.g., SAT/ACT scores). Students with admission scores below half a standard deviation of the average admission score were assigned to the “low-high school performance” subgroup (admission score ≤ 210), students with admission scores within half a standard deviation of the average admission score (211 < admission score < 241) were assigned to the “medium-high school performance” subgroup, and students with admission scores higher than half a standard deviation of the average admission score were assigned to the “high-high school performance” subgroup (admission score > 240).

3.3. Analytical methods
The first research question was examined with a descriptive analysis of course enrollments and time-invariant student characteristics. The second research question applied two-level logistic regression analyses (e.g., Rabe-Hesketh & Skrondal, 2008). The models nested course enrollments within courses to examine student course enrollment patterns by course modality. Student-level characteristics were included as level-1 predictors and course-level characteristics were included as level-2 predictors. Table 1 lists the variables included in the logistic regression models. To account for the availability of online courses, two models with different samples were conducted including (a) all course enrollments and (b) course enrollments of courses that were offered in both face-to-face and online course modality during the same summer term to account for the availability of online courses to students. The third research question applied multi-way fixed effects linear regression models with standard errors clustered at the student level (Allison, 2009; Wooldridge, 2002). These models examined the impact of course modality on student grades. Fixed effects were sequentially introduced in the models. The first model only included student fixed and course fixed effects (two-way fixed effects model). The second and third model added year and instructor fixed effects respectively (three-way fixed effects model). The fourth and final model included student fixed effects, course fixed effects, year fixed effects, and instructor fixed effects (four-way fixed effects model).


Table 1. List of variables included in models.

RQ2	RQ3
Full model	Low-income	First-generation	Low-HS performance
Student grade	No	Yes⁎	Yes⁎	Yes⁎	Yes⁎
Course modality	Yes⁎	Yes	Yes	Yes	Yes
Female (vs. Male)	Yes	No	No	No	No
Race/Ethnicity (vs. White)	Yes	No	No	No	No
Asian
Black	Yes	No	No	No	No
Hispanic	Yes	No	No	No	No
Native	Yes	No	No	No	No
English language learner	Yes	No	No	No	No
In-state student residency	Yes	No	No	No	No
Low-income status	Yes	Yes×	Yes†	No	No
First-generation status	Yes	Yes×	No	Yes†	No
Current college GPA‡	Yes	Yes	Yes	Yes	Yes
Years enrolled in college‡	Yes	Yes	Yes	Yes	Yes
Transfer student status	Yes	No	No	No	No
College admission score‡,⁑	Yes	No	No	No	No
Low-HS performance indicator	No	Yes×	No	No	Yes†
Medium-HS performance indicator	No	Yes×	No	No	Yes†
High-HS performance indicator	No	Yes×	No	No	Yes†
Course repetitions‡	Yes	Yes	Yes	Yes	Yes
Number of online courses‡	Yes	Yes	Yes	Yes	Yes
Number of students in course‡	Yes	Yes	Yes	Yes	Yes
Student identifier	No	Yes◊	Yes◊	Yes◊	Yes◊
Course identifier	Yes○	Yes◊	Yes◊	Yes◊	Yes◊
Year identifier	No	Yes◊	Yes◊	Yes◊	Yes◊
Instructor identifier	No	Yes◊	Yes◊	Yes◊	Yes◊
Notes. HS = high school.

‡
Grand-mean centered.

⁑
z-score transformed.

⁎
Dependent variable.

×
Included in interaction term with course modality.

†
Grouping variable.

○
Nesting variable.

◊
Fixed effect variable.

Fixed effects modeling takes advantage of the panel level structure of data (i.e., longitudinal data using measurements over time). For instance, students often enroll in multiple courses across different summer terms. The main idea of fixed effects modeling is the introduction of a fixed effects variable that is constant across individuals. In the example of student fixed effects, characteristics such as age, gender, ethnicity, SAT/ACT scores do not change for students over time. While these characteristics are frequently measured in institutional data contexts, fixed effects also account for potentially endogenous student characteristics that are not measured. Thus, this method responds to the frequently encountered omitted variable bias in random effects models. Notably, the proportion of students with variation in the key variable (i.e., course modality) does not influence the consistency of the fixed effects estimator. To this degree, students who only enrolled in face-to-face courses (or online courses, respectively) do not threat the consistency of the fixed effects estimators, assuming that student selection is constant within the individual. However, limited variability in key variables across individuals increases the standard errors of the estimator, thus, decreasing its accuracy. This limitation can usually be addressed through sufficiently large sample sizes. Similar to student fixed effects, the same logic can be applied to course, instructor, and year fixed effects. Please refer to Allison (2009) or Wooldridge (2002) for further information on fixed effects modeling.

In addition to the above described models, heterogeneity effects for at-risk student populations (low-income students, first-generation students, low-high school performance students) were examined through the inclusion of interaction terms in the full four-way fixed effects model, as well as through subgroup analyses. The subgroup analyses applied four-way fixed effects models on the corresponding underlying sample. Table 1 describes the variables included in each model. Notably, time-invariant variables (e.g., student demographics) were not included in these models.

Across all models, continuous variables (i.e., current college GPA, years enrolled in college, number of course repetitions, number of online courses taken, number of students enrolled in course) were included as grand-mean centered variables. Similarly, students' admission scores were included as a z-score transformed and grand-mean centered variable. List-wise deletion missing data approaches were applied separately for each analytical model.

4. Results
4.1. Description of summer course enrollments
In summer terms from 2014 to 2017, 23,610 degree-seeking students enrolled in 433 different courses cumulating in a total of 72,441 course enrollments (Table 2). There were 8788 students (37.22%) who enrolled in both online and face-to-face courses representing a total of 37,743 course enrollments (52.10%). Out of the 433 courses included in this study, 400 courses had face-to-face sections and 79 courses had online sections. Notably, 46 courses had both online and face-to-face course sections representing a total of 20,417 course enrollments (28.18%). Notably, out of the 603 instructors teaching courses during the selected summer terms, 31 instructors taught both online and face-to-face course sections representing a total of 12,990 course enrollments (17.93%). Table 3 describes student demographic, family background, and time-invariant academic career information for degree-seeking undergraduate students enrolled in summer term (in the courses included in the study) and for all degree-seeking undergraduate students enrolled in the regular academic year during 2014–2017. Notably, the enrolled student body during summer terms was very similar to during the academic years with respect to students' gender (about 54% female), low-income status (about 33% classified as low-income), and college admission scores (about 226 ± 30 on a 60–300 scale).


Table 2. Descriptive information of summer course enrollments in 2014–2017.

Student level
 Total number of students	23,610
 Number of students taking one course	5171
 Number of students taking more than one course	18,439
 Number of students taking both OL and F2F courses	8788
Course level
 Total number of courses	433
 Number of course with one enrollment	4
 Number of courses with more than one enrollment	329
 Number of courses offering both OL and F2F sections	46
Instructor Level
 Total number of instructors	603
 Number of instructors with one enrollment	5
 Number of instructors with more than one enrollment	598
 Number of instructors teaching both OL and F2F sections	31
Enrollment Level
 Total number of course enrollments	72,441
 Number of enrollments of students taking both OL and F2F courses	37,743
 Percentage enrollments of students taking both OL and F2F courses	52.10%
 Number of enrollments in courses with both OL and F2F sections	20,417
 Percentage enrollments in courses with both OL and F2F sections	28.18%
 Number of enrollments with instructors teaching both OL and F2F sections	12,990
 Percentage enrollments with instructors teaching both OL and F2F sections	17.93%
Notes. OL = online, F2F = face-to-face.


Table 3. Descriptive information of time-invariant student characteristics for students enrolled in summer term and students enrolled in the regular academic year during 2014–2017.

Summer terms	Regular academic years
Percentage	N	Percentage	N
Female (vs. male)	54.62%	23,506	53.35%	57,060
Race/Ethnicity	13.94%	22,517	16.50%	55,743
White
Black or African American	2.85%	22,517	3.05%	55,743
Asian or Asian American	61.11%	22,517	54.96%	55,743
Hispanic or Latino	21.60%	22,517	24.97%	55,743
American Indian, Alaska native, or
Pacific Islander	0.50%	22,517	0.52%	55,743
English language learner	41.46%	23,119	37.02%	56,984
In-state student residency	78.08%	23,608	82.23%	57,292
Low-income status	32.27%	23,119	33.42%	56,985
First-generation status	49.60%	23,119	45.32%	55,217
Transfer student status	20.21%	23,119	24.03%	57,260
College admission score [Mean, SD]	225.94 ± 29.60	19,323	226.81 ± 29.77	46,030
Number of students	23,610		57,306	
Notes. Information displayed is prior to list-wise deletion.

Compared to students who took courses during the regular academic year, students who enrolled in the summer term tended to differ in racial/ethnic background, English language learner status, in-state residency, first-generation college student status, and transfer student status. While smaller percentages of White students (13.9% summer vs. 16.5% regular academic year) and Hispanic or Latino students (21.6% summer vs. 25.0% regular academic year) enrolled in summer term, higher percentages of Asian or Asian American students enrolled in summer term (61.1% summer vs. 55.0% regular academic year). In summer terms, slightly more students were English language learners (41.5% summer vs. 37.0%, regular academic year) and first-generation college students (49.6% summer vs. 45.3% regular academic year). In contrast, slightly less in-state students (78.1% summer vs. 82.2% regular academic year) and transfer students (20.2% summer vs. 24.0% regular academic year) enrolled in summer terms. Notably, none of the percentage differences in time-invariant student characteristics was larger than 5%, with the exception of the larger representation of Asian or Asian American students in summer terms (6.1% increase).

4.2. Examination of course enrollment patterns by course modality
The examination of course enrollment patterns by course modality identified several factors that were associated with a significant likelihood of online course enrollment (Table 4). Most factors remained significantly associated with online course enrollment examining comparing estimates from the full sample (i.e., including all course enrollments) and the restricted sample (i.e., only including course enrollments in courses that offered online and face-to-face courses in the same term). Notably, analysis on the full sample (and in part for the restricted sample) examines more generally how students enrolled in online classes are different from those in face-to-face courses at the institutional level, acknowledging that such difference could also be driven by factors including (a) student self-sorting by course delivery format, (b) availability of online courses across different fields of study, and (c) availability of online courses during different terms.


Table 4. Multi-level logistic regression analysis predicting student enrollment in online courses.

Model 1	Model 2
β	Odds ratio	p	β	Odds ratio	p
Intercept	−12.269	0.000	< 0.001	1.892	6.634	< 0.001
Level 1
Female (vs. Male)	0.216	1.241	< 0.001	0.224	1.251	< 0.001
Asian	−0.244	0.784	< 0.001	−0.340	0.712	< 0.001
Race/Ethnicity (vs. White) Black	0.200	1.222	0.123	0.039	1.040	0.819
Hispanic	−0.101	0.904	0.175	−0.333	0.717	0.001
Native	−0.291	0.747	0.344	−0.983	0.374	0.008
English language learner	0.039	1.040	0.377	0.034	1.034	0.567
In-state student residency	−0.352	0.703	< 0.001	−0.152	0.859	0.043
Low-income status	−0.038	0.963	0.404	−0.083	0.920	0.169
First-generation status	0.072	1.074	0.100	0.019	1.019	0.746
Current college GPA	−0.010	0.990	0.809	0.007	1.007	0.887
Years enrolled in college	0.064	1.066	0.006	0.022	1.023	0.473
Transfer student status	0.486	1.626	< 0.001	0.275	1.316	0.130
College admission score	0.306	1.358	< 0.001	0.143	1.153	< 0.001
Course repetitions	0.336	1.400	< 0.001	0.736	2.087	< 0.001
Number of online courses	0.263	1.301	< 0.001	0.201	1.223	< 0.001
 
Level 2
Number students in course	0.014	1.014	< 0.001	0.012	1.012	< 0.001
N (courses)	431			34		
N (enrollments)	60,798			9741		
Notes. Model 1 includes all course enrollments; Model 2 restricts the sample to enrollments for courses offered as both face-to-face and online courses in the same summer term.

4.2.1. Student-level factors
With respect to the examined students' demographic background, the odds of female students attempting online courses were 24.1% higher compared to male students, OR = 1.241, p < .001. The odds of attempting online courses for female students are stable if courses were offered as online and face-to-face courses in the same term, OR = 1.251, p < .001.

For Asian or Asian American students, the odds of enrolling in online courses were 21.6% lower compared to White students, OR = 0.784, p < .001. The odds of Asian or Asian American students attempting online courses were similarly low if courses were offered as online and face-to-face courses in the same term, OR = 0.712, p < .001. In addition, if courses were offered as online and face-to-face course in the same term, the odds of enrolling in online courses for Hispanic or Latino and American Indian, Alaska Native, or Pacific Islander were 28.3% and 62.6% lower compared to White students, OR = 0.717, p < .01 and OR = 0.374, p < .01, respectively.

The odds for in-state students to enroll in online courses were 29.7% lower compared to out-of-state students, OR = 0.703, p < .001. For courses that were offered as online and face-to-face courses in the same term, the odds for in-state students to enroll in online courses were 14.1% lower compared to out-of-state students, OR = 0.859, p < .05.

With respect to students' academic preparation and college history, each additional year a student is enrolled in college is related to a 6.6% increase in the odds of enrolling in online courses, OR = 1.066, p < .01. However, the odds were not significantly different if courses were offered as online and face-to-face courses in the same term, OR = 1.023, p = .473. Similarly, transfer students had overall 62.6% higher odds of enrolling in online courses compared to non-transfer students, OR = 1.626, p < .001, whereas the odds were not significantly different if courses were offered as online and face-to-face courses in the same term, OR = 1.316, p = .130.

Each standard deviation increase in student college admission scores related to a 35.8% increase in the odds of enrolling in online courses, OR = 1.358, p < .001. For courses that were offered as online and face-to-face courses in the same term, each standard deviation increase in student college admission scores related to a 15.3% increase in the odds of enrolling in online courses, OR = 1.153, p < .001.

Each course repetition was associated with a 40.0% increase in the odds of enrolling in online courses, OR = 1.400, p < .001. For courses that were offered as online and face-to-face courses in the same term, each course repetition was associated with a 108.7% increase in the odds of enrolling in online courses, OR = 2.087, p < .001.

Each previous online course a student took was associated with a 30.1% increase in the odds of online course enrollment, OR = 1.301, p < .001. This is similar to the 22.3% increase in the odds of online course enrollment for each previous online course a student took, if courses were offered as online and face-to-face courses in the same term, OR = 1.223, p < .001.

4.2.2. Course-level factors
On the course-level, course size was associated with online course enrollment. Each additional student increased the odds of a course being offered in the online course modality by 1.4%, OR = 1.014, p < .001. Similarly, if courses were offered as online and face-to-face courses in the same term, each additional student increased the odds of a course being offered in the online course modality by 1.2%, OR = 1.012, p < .001.

4.3. Impact of course modality on student success
This study then examined the impact of course modality on student performance, as measured by the letter grades students received at the end of a course. The results indicated a small but negative association of online course participation on student performance (Table 5). The multi-way fixed effects models controlled for a range of time-variant student and course-level characteristics such as students' current GPA, the years enrolled in college, the number of course repetitions, and the number of prior online courses taken, as well as the course-level total number of enrolled students and the course-level average years of students enrolled in college. Sequentially introduced fixed effects examined influences of unobserved characteristics. Notably, the inclusion of instructor fixed effects increased the estimates of the effect of course modality on student performance reducing the overall grade penalty. This indicated that unobserved time-invariant instructor characteristics were influential for student course grades. The full four-way fixed effects linear regression model indicated that the online course modality led to an overall 0.096 decrease in student grades, which is approaching significance, b = −0.096, p < .10. This represents a grade penalty of roughly one tenth of a letter grade on a 4.0 grading scale (e.g., B+ = 3.3, B = 3.0, B- = 2.7). Overall, the estimates of the impact of course modality on student grades were consistently significant and negative across model specifications, which indicated that students tend to perform worse in online courses compared to face-to-face courses.


Table 5. Multi-way fixed effects linear regression models predicting the effect of online enrollment on student grades.

Model 1	Model 2	Model 3	Model 4
Coefficient	−0.161***	−0.168***	−0.094~	−0.096~
(Standard error)	(0.033)	(0.033)	(0.052)	(0.052)
Student FE	Yes	Yes	Yes	Yes
Course FE	Yes	Yes	Yes	Yes
Year FE	No	Yes	No	Yes
Instructor FE	No	No	Yes	Yes
N	61,401	61,401	61,401	61,401
Notes. FE = fixed effects; standard errors are clustered at the student level; models included students' current GPA, the years enrolled in college, the number of course repetitions, the number of prior online courses taken, and the course-level total number of enrolled students as covariates; ~p < .10, *p < .05, **p < .01, ***p < .001.

Heterogeneity analysis on at-risk student populations indicated that low-income students, first-generation college students, and low-performing students did not perform worse in online courses (Table 6). While subgroup analyses of course enrollments for low-income students and non-low-income student subsamples indicated a small difference in the course modality coefficient (low-income: b = −0.177, SE = 0.076; non-low-income: b = −0.122, SE = 0.055), the interaction term in the full model indicated that this difference between income-based subgroups was not significant, b = 0.028, p > .10. Similarly, subgroup analyses of course enrollments for first-generation college student and non-first-generation college student subsamples indicated a small difference in the course modality coefficient (first-generation: b = −0.167, SE = 0.062; non-first-generation: b = −0.113, SE = 0.064), but the interaction term in the full model indicated that this difference was not significant, b = 0.007, p > .10.


Table 6. Subgroup analysis of the effect of online enrollment on student grades for at-risk student populations using four-way fixed effects linear regression models.

Coefficient	(S.E.)	N
Low-income students
Online * Low-income status†	0.028	(0.023)	61,401
Low-income subsample	−0.177*	(0.076)	21,465
Non-low-income subsample	−0.122*	(0.055)	39,936

First-generation college students
Online * First-generation-status†	0.007	(0.022)	61,401
First-generation subsample	−0.167**	(0.062)	31,596
Non-first-generation subsample	−0.113~	(0.064)	29,805

Low-HS performance students
Online*Medium-HS performance†
(vs. low-HS performance)	−0.055*	(0.028)	61,401
Online*High-HS performance†
(vs. low-HS performance)	−0.082**	(0.031)	61,401
Low-HS performance subsample	−0.051	(0.094)	14,105
Medium-HS performance subsample	−0.107	(0.070)	28,948
High-HS performance subsample	−0.225**	(0.079)	18,348
Notes. HS = high school; †: Full sample estimates; standard errors are clustered at the student level; models included students' current GPA, the years enrolled in college, the number of course repetitions, the number of prior online courses taken, and the course-level total number of enrolled students as covariates; ~p < .10, *p < .05, **p < .01, ***p < .001.

Using students' prior academic achievement in high school (e.g., SAT/ACT scores), students were grouped into three categories: low-high school (HS) performance, medium-HS performance, and high-HS performance. This study found that students in the low-HS performance subgroup suffered less of a grade penalty in online as compared to face-to-face courses than did students in the medium- and high-HS performance subgroups. The predicted performance penalty of online courses, compared to face-to-face courses, for enrollments of students in the low-HS performance group that was 0.055 (p < .05) grades less than for students in the medium-HS performance group and 0.082 (p < .01) grades less than for students in the high-HS performance group. This trend is also reflected in the predicted online course performance gaps in the subgroup analyses (low-HS performance subsample: b = −0.051, SE = 0.094; medium-HS performance subsample: b = −0.107, SE = 0.070; high-HS performance subsample: b = −0.225, SE = 0.079). Please note that a reduced online course grade penalty for students in the low-HS performance group does not imply that students' course grades were overall higher compared to students in the medium-HS or high-HS performance groups. On the contrary, students in the low-HS performance group received overall lower course grades across online and face-to-face courses (M = 2.70, SD = 1.11) compared to students in the medium-HS (M = 2.87, SD = 1.06) and high-HS performance (M = 3.10, SD = 1.00) groups (Fig. 1).

Fig. 1
Download : Download high-res image (84KB)
Download : Download full-size image
Fig. 1. Average course grade across all course enrollments by analytical subgroup with error bars representing the standard deviation.

4.4. Limitations
The main limitations of this study relate to the data sources. For instance, missing data on students' racial/ethnic background and gender might not be at random. Also, college admission data are missing for most transfer students. Another limitation relates to the treatment variable of this study (i.e., course modality). This variable might underestimate the use of online learning elements in face-to-face courses. Most courses make use of a customizable learning management system (LMS) environment. While the LMS mostly serves administrative purposes, some instructors might also utilize its capabilities to more directly influence student learning, for instance, by adopting online learning elements (e.g., requiring students to contribute to discussions in the LMS's online discussion boards) that create hybrid learning experiences for students. A finer-grained classification of courses in “face-to-face courses,” “hybrid courses,” and “online courses” would require a detailed examination of each course syllabus. Given the scope of this study with > 400 analyzed courses, obtaining and examining course syllabi was not feasible. Furthermore, this study only used institutional variables that were available at scale. Non-institutional variables including student motivation, self-efficacy, time-on-task, and social-emotional support, among others, were not included in the analysis despite their potential value in predicting course enrollment patterns and student performance. This limitation particularly pertains to the second research question as student enrollment in online courses might also be related to factors such as student self-selection by course delivery format, availability of online courses across different fields of study, and variation in online courses' availability during different terms. This study attempted to address some of these concerns by conducting analyses on both the full sample of all course enrollments and a restricted sample that only included courses that were offered as both online and face-to-face courses in the same term.

5. Discussion and conclusions
This study contributes to our understanding about who enrolls in online summer courses and how enrollment in an online course associates with academic success. Four conclusions of the study inform educational policy makers and higher education administrators.

First, the characteristics of students enrolled in undergraduate courses during summer terms are similar to those of students enrolled during the regular academic year. Notably, the distributions of gender, socioeconomic status, and prior performance did not differ between the summer term and regular academic year. This is similar to prior research that did not find substantial differences in summer enrollments for most student characteristics (e.g., Smith & Read, 2013; Taylor & Doane, 2003). Consequently, higher education administrators should feel encouraged to plan their summer schedules with their current student population from the regular academic year in mind.

Second, enrollment in online courses might be influenced by student, college experience, and course characteristics. In particular, female students and out-of-state students were more likely to enroll in online courses. This trend validates prior research on student demographic factors being influential for enrollment in online courses (Ortagus, 2017). For instance, out-of-state students who stay at home during summer break might benefit from the affordance of online learning. Additionally, academic preparation, course re-taking, and prior experiences with online courses, as well as the size of the course were found to relate to online course enrollments. These findings might encourage higher education administrators to better understand how students choose to opt for online course enrollments and how to better adapt their online course portfolios to students' needs.

Third, students perform slightly worse in online courses compared to face-to-face courses. Online course participation yields an approximate one tenth of a letter grade penalty. This finding is in line with prior work that observed grade penalties for online course enrollment (Bettinger et al., 2017; Xu and Jaggars, 2011, Xu and Jaggars, 2014). From a student perspective, this study (and most prior research) only examines “near effects” of online course participation such as end-of-course outcomes (e.g., courses grades). However, participation in online courses might yield greater benefits on “distal effects” such as accelerated degree advancement compared to not taking courses at all. Such analyses remain an area for future research. From an institutional perspective, online courses are often more flexible in their delivery and might require less associated financial resources (e.g., no physical space required, less enrollment size limitations). Thus, online courses might provide an attractive and cost-effective alternative instructional format for universities in selected courses in which the detected course performance penalties do not put students' overall learning goals at risk. Nonetheless, educational stakeholders should feel encouraged to allocate more resources to the improvement of students' self-directed learning skills and the overall quality of online instruction to narrow existing student performance gaps by course modality.

Fourth, student populations considered at-risk during their college careers are not found to suffer additional online course enrollment performance penalties. In particular, online course enrollment penalties for first-generation college students and low-income students were similar compared to their not at-risk counterparts. Surprisingly, students in a lower college admission scores subgroup suffered a comparatively smaller online course enrollment penalty. This study extends prior research that indicated additional challenges of online course participation for at-risk students (Figlio et al., 2013). Thus, this study provides a hopeful perspective that in the context of summer terms at this public research university, online course enrollments do not further disadvantage traditionally at-risk student populations. Replication studies at other universities with different student body compositions are encouraged to provide more robust guidance to educational policy makers and university administrators.

This study extends prior large-scale research studies that were mostly conducted at community colleges or for-profit universities (Bettinger et al., 2017; Kaupp, 2012; Xu and Jaggars, 2013, Xu and Jaggars, 2014) by investigating enrollment and course performance patterns at a selective research university in the intersection of summer learning and online learning. It reinforces the potential of online courses to provide a meaningful addition to students' academic experiences as overall magnitudes of student performance penalties were small. In particular, the absence of additional detrimental effects for at-risk student populations at selective research universities is encouraging.

5.1. Future directions
In all, this study motivates three directions of future research. A first set of studies could further our understanding with respect to student motivation to enroll in online courses by examining heterogeneity effects of online course uptake for courses that are offered in both online and face-to-face modalities and taught by the same instructor. A second set of studies could extend the current focus on “near success factors” by examining the influences of online course taking patterns on more distant student success factors including major persistence, time-to-degree, and graduation rates, among others. A third set of studies could examine online courses with the smallest grade penalties to identify best practices of effective and successful online course environments, in particular for at-risk student populations, to optimally support all students throughout their college careers and beyond.