We investigated the relationship between the proportion of time spent online in a blended course and student perceptions and performance. Students in 20 undergraduate courses offered in four different online blend proportions were surveyed on their perceptions, and their final course grades and cumulative grade point averages were obtained. A small but significant relationship was found between blend proportion and perceptions. Students in the Medium (36% to 40% online) and High (50% online) blends tended to have the most positive perceptions of blended learning compared to their peers in the Low (27% to 30% online) and Supplemental blends (100% face-to-face plus weekly online tutorial sessions). Those in the High and Medium blends performed significantly better than students in the other two blends, but no difference was found between the High and Medium blends. We concluded that instructors and institutions seeking to take full advantage of blended learning should consider replacing at least one-third of normal face-to-face time with online activities that facilitate student-to-student and instructor-to-student interaction.

Previous
Next 
Keywords
Blending learning

Hybrid learning

Online learning

Learning environment design

Teaching and learning with technology

1. Introduction
Blended learning, while offering many advantages to students, faculty, and institutions, can be particularly challenging to implement successfully in higher education (Dziuban, Hartman, Juge, Moskal, & Sorg, 2006; Owston, 2013). One of these challenges is encouraging faculty to rethink the way they have traditionally taught their courses and imagine how they could be taught in the blended mode (Garrison & Vaughan, 2008). A persistent question asked by faculty when they embark on the redesign process is how much time should be devoted to face-to-face classes and how much time to online activities (Alammary, Carbone, & Sheard, 2015). The published literature offers little guidance on this issue, therefore in this study we sought to investigate the issue from the perspective of student perceptions of and performance in blended learning courses where the proportion of time devoted to online activities varied. Our central research question was whether student perceptions and performance are related to the amount of time spent online in blended courses or, in other words, does the proportion of online time in and of itself really matter. We studied 20 undergraduate courses at a large urban university that were offered in four different mixes of online and face-to-face instruction across a variety of academic fields. In addition to contributing to the literature on blended learning, findings about these relationships will help inform practice by guiding design decisions about how much time might be devoted to online activities in blended courses. They will also help university academic administrators develop policies for implementing blended learning campus-wide (Owston, 2013).

Most blended learning research to date examining student perceptions and performance has concentrated on comparing blended learning to fully online and traditional face-to-face classroom instruction. The present research responds to calls to go beyond comparative studies to investigate factors that moderate or influence the impact of blended learning on students, such as amount of time spent online, instructional design, student preferences, technologies employed, and instructor presence (Means, Toyama, Murphy, & Baki, 2013; Zhao, Lei, Lai, & Tan, 2005). This type of research is now necessary because a consensus is emerging on the question about the relative efficacy of blended learning, as will be discussed later, so there is a diminishing need for comparative studies. Additionally, faculty and institutions typically decide a priori to use a blended approach for reasons such as providing more convenience and flexibility to students or better utilization of classroom space, as long as they are assured that students will achieve at least as well as they would in face-to-face classes. Therefore studies that consider the relative merits of various blended learning designs are of practical value.

A challenge when researching (and implementing) blended courses or programs is to clarify what the term itself means as there is no consensus in the literature on the definition of blended learning. Surprisingly many blended learning studies fail to operationally define the term. Those that do so may consider blended learning to be when in-class seat time is reduced and replaced by an equivalent amount of online time. The former Sloan Consortium, now the Online Learning Consortium, stated that a course can be considered blended when the amount of online time replaces from 30% to 79% of the total course time (Allen, Seaman, & Garrett, 2007). A broader definition for blended courses comes from the U.S. National Center for Education Statistics that defines blended courses merely as those having some reduced in-seat time (Parsad, Lewis, & Tice, 2008). Examples of courses designed using this replacement model are given by Asarta and Schmidt (2015) who list 20 studies conducted between 2003 and 2011 where seat time was reduced between 25% and 73%. Others are not as concerned about how much, if any, seat time is replaced by choosing to focus on different aspects of the blend. For example, Graham (2006) defines blended learning simply as the combination of face-to-face instruction and computer mediated instruction in an effort to reconcile differences in its definition found in the literature (p. 5). He goes on to classify blended learning into three models according to their primary educational purpose – to enable, enhance, or transform learning—without reference to the relative amount of time spent online. Garrison and Vaughan (2008) view blended learning as “the organic integration of thoughtfully selected and complementary face-to-face and online approaches and technologies” (p. 148), again without focusing on how much seat time is replaced or not. Indeed, in some cases the actual amount of time students spend on a course increases when an online component is added without taking anything away from the previous version of the course leading to the so-called “course and a half” syndrome (Garrison & Vaughan, 2008, p. 202). Most of the studies included in a recent meta-analysis by Means et al. (2013) comparing blended learning to face-to-face and fully online learning used online activities to extend—not replace—course time by at least 25% of the normal course time. The authors speculate that one of the reasons students in blended courses achieve higher than their counterparts in the other two instructional modes is that they spend more time engaging with the course resources, while others contend that interactions among students and students with the instructor explain the performance difference (Castaño-Muñoz, Duart, & Sancho-Vinuesa, 2014). In this study we chose to investigate student preferences and course performance in the replacement model because the university where we conducted the study adopted this model as campus-wide definition of blended learning.

2. Theoretical framework
Our study is framed by the literature on the context of blended learning design and implementation, student perceptions about studying in the blended format, and student performance in blended courses.

2.1. Blended learning design and implementation
In higher education blended learning has been implemented in a variety of contexts ranging from individual instructor-designed courses, to blended academic and professional programs, through to large institutional and system-wide initiatives. Bonk and Graham's (2006) Handbook of Blended Learning provides an overview of the diversity of implementations around the globe. Since its publication blended learning continues to expand rapidly and may soon become the norm for instructional design (Brown, 2016).

When designing and implementing blended learning the choice instructors make of the mix of online and face-to-face activities appears to be highly context dependent and contingent on the curricular level (e.g., the nature of the course content and instructional goals, online resources, availability of technology), the human resources level (e.g., student characteristics and learning preferences, instructor experience and teaching style), and the institutional level (e.g., institutional goals and priorities, quality assurance standards) (Diaz & Brown, 2010; Dziuban, Moskal, & Hartman, 2005; Mitchell & Honore, 2007). Some researchers maintain that there is no standard for deciding what content and what portion of a course should be online (Dziuban et al., 2005; Vaughan, 2007). Garrison and Vaughan (2008) emphasize that technology should not be simply added on to an existing face-to-face course, but effective use of the model requires a fundamental rethinking of the course design with the goal of optimizing student engagement. Given the lack of guidelines, Alammary et al. (2015) investigated the criteria instructors thought should be considered when determining the mix between the online and face-to-face components and what their relative importance should be. They found that out of 38 criteria in four different categories instructors rated the highest: (a) availability of technology to enable online delivery, (b) students' access to campus and technology, (c) teachers' willingness to try new teaching methods, and (d) the institutional support for teaching innovation and technology. The authors conclude that “the institution plays the most important role in determining the proportion of online components of blended courses” (p. 79). Likewise, Brown's (2016) systematic review of literature about faculty adoption of blended learning identified the same factors, but also found instructors' attitudes and beliefs about teaching and their workload as well as feedback from students as influences on their decision to employ blended learning.

The overwhelming body of research on blended learning indicates that the inclusion of on-site, face-to-face, sessions where active student participation and interaction with course content are encouraged tend to be more successful and supported by students as they help to establish immediate physical contact with other students in the class (Collopy & Arnold, 2009; Lim, Morris, & Kupritz, 2006). Similarly, the infusion of synchronous communications technologies, or at least a balanced mixture of synchronous and asynchronous technologies, into the online component of the blended course tends to increase the frequency and quality of student and faculty interaction as well as student engagement (Vaughan, 2007). The engagement of students in real-time interaction via video conferencing or instant messaging also helps create visually appealing dynamic experiences similar to those occurring in classroom-based course environments (Castle & McGuire, 2010; Kember, McNaught, Chong, Lam, & Cheng, 2010).

The recent emergence of guidelines and standards for the evaluation of online and blended courses can help instructors in alerting them to critical factors to consider when designing the online portion of blended courses. For example, the Quality Matters (http://qualitymatters.org) rubric sets seven standards for assessing online courses: course overview, learning objectives, assessment and measurement, instructional materials, course activities and learner interaction, course technology, learner support, and accessibility and usability. Each of these standards is in turn broken down into from 4 to 9 sub-criteria, some of which are deemed to be essential and others optional for a course to warrant the Quality Matters certification. Similarly, the Online Learning Consortium (http://onlinelearningconsortium.org) established five “pillars” – learning, faculty satisfaction, student satisfaction, scale, and access – to guide design for quality online education.

Unfortunately, while the literature on the design and implementation of blended environments addresses many of the benefits and limitations of blended learning as well as factors to consider when designing blended courses, it does not address the question of the relative merits of different blended models and their effects on student perceptions and performance.

2.2. Student perceptions
Blended learning is often perceived favourably by undergraduate students who are accustomed to a traditional mode of course delivery (Castle & McGuire, 2010; Cavanagh, 2012; Diaz & Brown, 2010; Farley, Jain, & Thomson, 2011; Lim et al., 2006). Dziuban et al. (2006) report that 85% of students were satisfied with their blended experience at the University of Central Florida, and that 67% would take another blended course. Owston, York, and Murtha (2013) found modestly less positive responses overall to the same two questions, but noted that attitudes are related to student achievement with the higher achievers being more positive toward blended learning that their lower achieving counterparts. A meta-analysis of 30 studies by Spanjers et al. (2015) that compared student satisfaction in blended versus traditional courses found a significant positive effect size favouring blended learning (g+ = 0.11, p < 0.05).

Despite the convenience offered by adding an online component to a course (such as increased learning flexibility, reduced student travel time, and cost savings for commuting students), traditional undergraduate students are modest in their desires about the amount of technology they want to see in a course. Annual undergraduate student surveys by the EDUCAUSE Center for Research Analysis consistently demonstrate this preference. The 2016 survey (N = 71,641), for example, showed that only 10% of students preferred entirely face-to-face courses and 7% fully online with the remaining 83% preferring a mix of online and face-to-face (Brooks, 2016). Despite the strong preference for blended courses some students are loathe to want any online component in their courses. Some of the reasons include: (a) students' close proximity to the university campus and hence no apparent need for any online activities; (b) their familiarity with the delivery of traditional instruction; (c) lack of technology skills and abilities in navigating the blended course; (d) apprehension about the reduction of face-to-face interaction; (e) feelings of information overload and increased workload; and (f) lack of instructor's guidance and attention (Ashton & Elliott, 2007; Diaz & Brown, 2010; Korr, Derwin, Greene, & Sokoloff, 2012; Lim et al., 2006; Poon, 2012; Rigby et al., 2012). There is also some evidence that upper year students tend to be more engaged in blended courses than first year students (Madriz & Nocente, 2016). This trend appears to continue beyond undergraduate study too. Graduate students often give preference to a high proportion of online activities, but appreciate the benefits of occasional face-to-face sessions, particularly at the beginning of a course, that provide them with opportunities to directly interact with faculty and engage with their peers on campus (Castle & McGuire, 2010; Fleck, 2012; Schuhmann & Skopek, 2009; Smyth, Houghton, Cooney, & Casey, 2012).

The only published study we were able to identify that empirically investigated how much blending students actually choose – as opposed to how they respond to surveys – was one by Asarta and Schmidt (2015). In this study the researchers compared attendance between a traditional university calculus course and an experimental blended version of it that had all lectures online. Both sections had the same amount of scheduled face-to-face class time, however students in the blended version had the choice of whether or not to attend classes where demonstrations and discussions were held but no lectures were given. All other aspects of the courses were the same including exams, the text, and assignments. Students in the experimental section had the opportunity to, in effect, create their own blended version of the course by choosing to attend any number of face-to-face classes ranging from all or none. Analysis of data revealed that on average students chose to reduce their attendance to between 49% and 63% of the time taking into account the typical class skip rate in traditional courses. The researchers concluded that, although student preferences should not be the only factor when designing a course, a reduction of about one class per week in a semi-weekly course schedule or 50% is what students find preferable for a blended course.

2.3. Performance in blended courses
2.3.1. Overall course performance
A consensus has emerged in the literature that students, on average, perform modestly better in blended courses when compared to those in online and face-to-face courses across a broad range of subject areas and institutional offerings. The University of Central Florida has been a pioneer in offering and tracking success and withdrawal rates in blended courses. Cavanagh (2012) reported that after 13 years of tracking student success (i.e., students receiving a “C” or higher) in the university's courses, students in blended courses based on the replacement model (n = 39,021) consistently had a higher success rate than those in fully online (n = 108,421) and face-to-face (n = 618,899) courses. Although the author presented only annual success rates, blended course success rates were several percentage points higher than the other two modes. A follow up study at the same institution by Moskal, Dziuban, and Hartman (2013) with an even larger sample (N = 913,688) found that 91% of students in blended courses met the same criterion of success, whereas the success rate for both fully online and face-to-face courses was approximately 88%.

Four separate meta-analyses also support the finding of the relative performance advantage of blended learning. Most widely cited is Means et al. (2013) in their meta-analysis of 50 effect sizes drawn from 45 studies who found that performance in online courses (blended and fully online together) was significantly higher than in face-to-face classes (g+ = 0.20, p < 0.001). When effect sizes were calculated for blended and fully online separately, blended course performance was higher than face-to-face (g+ = 0.35, p = 0.001), while the difference between fully online and face-to-face was not significant. Similarly when comparing performance in blended and face-to-face classes, Bernard and colleagues (Bernard, Borokhovski, Schmid, Tamim, & Abrami, 2014) found that performance in blended courses was significantly higher than in face-to-face courses (g+ = 0.33, p = 0.001), an effect size that was very close to Means et al.'s. Bernard et al.'s research examined 117 effect sizes. An effect size of 0.2 is considered to be small, 0.5 is medium-sized, and 0.8 is large according to Cohen (1988).

The two other meta-analyses looked at the question of what factors might influence student learning outcomes through the lens of systematic reviews of studies that compared blended learning to online and traditional face-to-face conditions. Vo, Zhu, and Diep's (2017) meta-analysis investigated student performance in STEM versus non-STEM courses using different end-of-course evaluation methods. They too found an overall effect size in favour of blended learning over traditional instruction (g+ = 0.385, p < 0.001) in their analysis of 51 studies; however, a more significant effect over traditional instruction was found for STEM courses (g+ = 0.496) versus non-STEM courses (g+ = 0.210). No difference was found between blended and traditional courses when different end-of-course evaluation methods were compared. In the meta-analysis cited earlier, Spanjers et al. (2015) compared blended and traditional learning when course performance was assessed by objective and subjective measures. Their analysis of 30 studies also favoured blended learning overall, and revealed an effect size for objective measures that was slightly higher than for subjective measures (g+ = 0.34, p < 0.05 versus g+ = 0.27, p < 0.05).

Two conclusions may be drawn from above meta-analyses. First, that performance in blended learning courses is typically higher than face-to-face courses with an average effect size in the middle to upper range of Cohen's classification of a small effect. Second, that blended learning has an effect size that is approximately the same as the average effect size (0.40) of all educational interventions Hattie (2015) reported in his synthesis of over 1200 meta-analytic studies of involving about a quarter billion students.

2.3.2. Portion of time online and performance in blended courses
There is evidence that suggests that the proportion of time devoted to online activities in a blended course is related to course performance. In the Means et al. (2013) study cited above the researchers looked at a number of moderator variables for blended versus face-to-face instruction in order to explain the performance differential. The variable of interest for the present study was time-on-task which had two values: equal or less amount of time spent online than face-to-face (i.e., ≤ 50% online), and more time online than face-to-face (i.e. > 50% online). Their findings approached significance (Q = 3.62, p = 0.06) favouring more time online. Bernard et al. (2014) also examined the portion of time spent online as a moderating variable. They considered two categories, up to 30% of course time and 30% to 50% of course time (they did not consider over 50% of time). They found a “definite trend” suggesting that more online course time results in higher achievement (Q = 0.47, p = 0.49), but it was not significant (p. 112). They recommended that further primary studies be conducted that examine the effects of amount of time spent online.

Zhao, Lei, Yan, Lai, and Tan's (2005) meta-analysis examined 51 studies comparing only online and face-to-face courses and found a non-significant effect size of 0.10 (p > 0.05) between both instructional models. More relevant for the present study is that the researchers also coded the studies in the sample on several additional variables, one of which was “media involvement” (p. 1848). The coding was from 1 (no technology used) to 10 (instruction was delivered completely with technology). In effect media involvement is a proxy for the proportion of time spent online in a blended course. Performance in studies classified as having a “medium” media involvement (i.e., coded from 6 to 8) was significantly higher when compared to face-to-face instruction (d = 0.50. p < 0.001); studies with a “high” media involvement (i.e., coded 9 to 10) had a smaller yet still significant effect size (d = 0.07, p < 0.001) (p. 1860). An implication of Zhao et al.'s study is that performance in blended courses with between 60% and 80% online is higher than in courses where more time is spent online.

3. Hypotheses
We examined two hypotheses about the proportion of time devoted to online activities in blended courses. The first related to student perceptions of blended learning. As discussed earlier there is persuasive evidence that students prefer blended learning environments over traditional face-to-face and fully online environments for a variety of reasons including convenience, flexibility, wanting some face-to-face social interaction, and a desire not to be over-burdened with technology (Brooks, 2016; Castle & McGuire, 2010; Diaz & Brown, 2010; Dziuban et al., 2006; Farley et al., 2011; Lim et al., 2006; Spanjers et al., 2015). No studies have directly addressed the question of the proportion of online time students most prefer in blended courses, although there is evidence to suggest a hypothesis. Asarta and Schmidt (2015) concluded that 50% online is what students will choose when given the free choice of participating face-to-face or online. The literature on course performance (discussed in more detail below) suggests that students tend to perform best when at least 50% of a course is online, so it follows that they would tend to prefer approximately this proportion. Therefore, we hypothesized that:

1.
Students tend to perceive blended learning more favourably as the proportion of online time increases up to 50%.

The second hypothesis is related to performance and the proportion of time online. Again, there is no literature directly addressing performance under different proportions of time online, however the three studies in the previous section suggest a hypothesis. To summarize, Means et al. (2013) suggest performance is highest when > 50% of a course is online; Bernard et al. (2014) found that performance is higher when 30% to 50% is online compared to < 30%; and Zhao et al. (2005) found that performance when 60% to 80% is online is superior to higher online levels. Given these findings one would expect that students on average would perform best in approximately a 50% blend, thus we hypothesized that:

2.
Students in a 50% blend of online and face-to-face will perform higher relative to the other three blends studied

4. Method
4.1. Setting
The research setting was a large urban university in Canada that has a very culturally- and racially-diverse student body, many of whom are the first generation in their family to attend a post-secondary institution. A vast majority of students commutes to campus and previous campus surveys suggest that approximately 45% of full time students work part time. Humanities and social sciences programs enroll most of the university's students.

Twenty fine arts, professional, and liberal arts undergraduate courses were studied. These courses had been redesigned by their instructors from a traditional lecture format to a blended instructional model. This meant that instead of offering the normal 3 h per week for 12 weeks of face-to-face lectures per semester, the faculty reduced the number of lectures offered and substituted online activities to make up for the reduced face-to-face time. Instructors could choose the amount of reduction of in-class time with which they were comfortable. Instructors were provided with a modest stipend to redesign their courses and they received priority support from the university's teaching support center and learning technologies support team to facilitate the redesign. An exception to this pattern was five fine arts courses where the regular large classroom lectures continued with their normal face-to-face schedule, but the usual 1 h per week small group face-to-face tutorial classes were replaced with online tutorial sessions. All five of these courses were introductory non-studio fine arts courses (art, dance, film, music, and theatre) for non-majors. Lectures for these courses were given in a large hall that accommodated the approximately 300 students enrolled in each of the courses. The online tutorial sections, led by teaching assistants (TAs), had between 28 and 38 students in each. TAs had little or no experience in online teaching, but were responsible for developing their own online environment within a short period of time in the Moodle course management system with the help of an instructional designer and the beginning of the course and throughout the semester.

The online activities varied from course to course, but generally consisted of online forums in which students and faculty interacted on structured problems to solve or discuss issues pertinent to their course. In some cases, videos of the previous lectures were made available. All courses used the Moodle course management system in which learning materials and resource links were posted and online discussions were carried out. Since instructors chose the amount of time they would devote to online activities there was variation across the 20 courses. For purposes of this study, we grouped the courses into four clusters according to the proportion of time that online activities replaced classroom time. Instructors expected that students would spend this portion of time participating in online work. The clusters were: Low blend (27% to 30% online – 7 courses); Medium blend (36% to 40% online – 3 courses); and High blend (50% online – 5 courses) and Supplemental blend (100% face-to-face lectures plus weekly online tutorial sessions that replaced compulsory face-to-face tutorials – 5 courses). The Supplemental blend was somewhat unusual in that one might normally expect online lectures and face-to-face tutorials; however the rationale for the Supplemental blend was that students might benefit from a more thoughtful exchange of ideas and have questions answered more effectively in online discussion groups. At the same time the supplemental model helped alleviate a shortage of tutorial rooms on the crowded campus. Important to note is that in this study the Supplemental blend describes a course where online activities replace a portion of face-to-face class time, whereas Twigg (2003) used the same term to describe a course where online activities are added above and beyond the normal class time.

Listed in Table 1 is the subject and year of study of each course as well as the blend proportion and number of students (N = 1020) who participated in the research. The research was approved by the university's human participants review board, participation in the study was voluntary, and all participants signed an informed consent form.


Table 1. Participants in study by subject and blend.

Course subject	Course year	Number of participants	Total
Low blend (27–30% online)	Medium blend (36–40% online)	High blend 50% online	Supplemental blend (100% F2F lectures plus online tutorials)
Administrative Studies (fundamentals emergency management)	2		34			34
Administrative Studies (personal taxation)	4	22				22
Anthropology (introduction to social anthropology)	1		22			22
Art (introductory for non-majors)	1				207	207
Dance (introductory for non-majors)	1				61	61
English (Irish poetry)	4			2		2
Film (introductory for non-majors)	1				174	174
Geography (global environmental change)	2			39		39
Health Studies (electronic health records)	3		21			21
Kinesiology (occupational biomechanics)	4	20				20
Kinesiology (chronic disease)	4			42		42
Philosophy (modes of reasoning)	1	16				16
Music (introductory for non-majors)	1				131	131
Nursing (children/youth rights)	4			19		19
Nursing (chronic health)	4			11		11
Nursing (self-development)	3	31				31
Political Science (political economy)	3	7				7
Psychology (aging)	3	41				41
Psychology (psychology of health)	3	49				49
Theatre (introductory for non-majors)	1				71	71
Total	186	77	113	644	1020	
4.2. Instruments
Toward the end of the semester, students were given a questionnaire to assess their perceptions about their blended learning experience. The questionnaire was described and used by Owston et al. (2013). From this instrument 13 Likert-type items were selected that were relevant to the present study which asked participants about various aspects of their experiences studying in the blended format relative to other (face-to-face) courses that they had taken. Participants responded on a 5-point scale (1 = Strongly Disagree, 2 = Disagree, 3 = Neutral, 4 = Agree, 5 = Strongly Agree). Paper versions of the questionnaire were administered in person by members of the research team. Participants marked their responses on machine readable bubble sheets. All present in class at the time of administration agreed to participate, although some participants did not respond to all items. As a result a total of 904 complete answer sets for the perception survey were obtained for analysis from the sample of 1024 respondents. The scale was found to have a high level of internal consistency, as determined by a Cronbach's alpha of 0.82.

Performance was assessed by final course grade which instructors assigned based on mid-term and final exams. Approximately 10% of the final grade was based on online participation. Attendance was not taken in classes, nor was any grade weighting given to face-to-face attendance. Grades and cumulative grade point averages (CGPA) were obtained from the registrar's office. CGPA was defined as the grade point average of the student from first entry into the university up until taking the course in this study. The university's grading system is based on a 9-point scale with 9 representing exceptional performance (A +) and 0 representing failing (F). Final grades and CGPAs were available for 699 students in the sample.

4.3. Data analysis
All data were analyzed using the IBM SPSS package version 24. Student perceptions were analyzed using a one-way MANOVA design with Questions (Q1 to Q13) as the dependent variables and Blend (Low, Medium, High, Supplemental) as the independent variable. This design was chosen over running separate ANOVAs for each dependent variable because all variables dealt with the same general construct (perceptions of blended learning) and all were closely related. MANOVA also has greater power to detect differences, and analyzing the variables together would result in a lower probability of Type I error than if the analyses were conducted separately.

Various tests were carried out to determine how well the dataset met the assumptions required for MANOVA. A Shapiro-Wilk test, which was run for each level of the independent variables, indicated that the data could not be assumed to be normally distributed (p < 0.05); nor was there homogeneity of the variance-covariance matrices as assessed by Box's test (p < 0.001). Neither of these was considered problematic given the robustness of the one-way MANOVA procedure. There was no multicollinearity as assessed by Pearson correlation between dependent variables (highest r = 0.69, p < 0.001). There was a moderately linear relationship between dependent variable scores for each blend, as assessed by scatterplots. Lastly, Mahalanobis distance calculations indicated that 14 cases were multivariate outliers as they exceeded the critical value of MD = 34.53 (p > 0.001), which was not considered an issue given the large sample size thus we proceeded with MANOVA for studying student perceptions.

Performance was analyzed using a one-way ANOVA design with GRADEDIFF as the dependent variable and BLEND as the independent variable. GRADEDIFF was derived from the difference between GRADE and CGPA scores. (ANCOVA with GRADE and CGPA as the independent and dependent variables respectively could not be used because the assumption of homogeneity of regression was violated.) GRADEDIFF scores for three of the blends – Medium, High, and Supplemental – were normally distributed as assessed by Sharpiro-Wilk's test, p > 0.05; however the test indicated that this was not the case for the Low Blend scores, p = 0.02. As ANOVA is robust to deviations from normality we proceeded with the analysis of performance.

4.4. Study limitations
There are several limitations to the design of this study which may affect the interpretation of the results. First, there was not a common template for the design of each of the courses in the study because, as noted above, individual faculty members made all instructional decisions about the amount of time to devote to online activities and their nature. Data were not collected on what actually transpired during the online sessions including the frequency and quality of interactions, the extent to which students were encouraged to interact with each other, and the role of the instructor in facilitating online discussions. We used a questionnaire developed for another study and one of the questions, Q12, asked students about the amount of time and effort required for their course. Because time and effort are two distinct constructs interpretation of the responses to this question may be problematic. Lastly, the measure of student performance was the final course grade. The study included a wide variety of subject areas and, even though a standard grade point average scale was used, instructors in different disciplines may have had different academic standards about what for example a grade of “A” represents.

5. Results
5.1. Student perceptions
Table 2 shows the means, standard deviations, and sample sizes for each of the 13 dependent variables for each of the four blends. Q4 (Convenience) had the highest overall mean (M = 3.89) across all questions, suggesting that students found the ability to work partial online the most desirable feature of blended learning. On the other hand, Q9 (Connected with other students) was the lowest rated question (M = 2.57), which may imply that the online component made the students feel more isolated than if they were in a traditional class. Also of interest is that students were approximately evenly split overall between rating the 13 Likert questions above and below the scale midpoint of 3. Q1 (Satisfaction), Q2 (Take another course), Q3 (Components enhanced each other), Q4 (Convenience), and Q13 (Improved understanding) were rated on average between 3 (Neutral) and 4 (Agree). On the other hand, Q5 (Engagement), Q6 (Asking questions), Q7 (Quantity of student interaction), Q8 (Quality of student interaction), Q9 (Connected with other students), Q10 (Quantity of instructor interaction), Q11 (Quality of instructor interaction), and Q12 (More time and effort) were rated on average between 2 (Disagree) and 3 (Neutral). These split perceptions give the impression that students liked the high level features of their blended course and their learning outcomes, but were less than satisfied with the internal pedagogical nature and learning environment.


Table 2. Mean, SD, and number of responses for each question and blend.

Abbreviated question	Blend	Mean	SD	n
Q1. I am satisfied with this course	Low	3.97	0.903	175
Medium	4.07	0.926	67
High	3.75	1.061	106
Supplemental	3.28	1.135	556
Total	3.52	1.116	904
Q2. I would take another course in the future	Low	3.86	1.197	175
Medium	4.09	1.203	67
High	3.67	1.3	106
Supplemental	3.3	1.249	556
Total	3.51	1.272	904
Q3. Online and F2F components enhanced each other	Low	3.62	1.086	175
Medium	3.82	1.029	67
High	3.38	1.191	106
Supplemental	3.01	1.213	556
Total	3.23	1.209	904
Q4. Course offered the convenience	Low	4.13	1.028	175
Medium	4.46	0.841	67
High	4.37	1.036	106
Supplemental	3.65	1.389	556
Total	3.89	1.289	904
Q5. I am more engaged in this course	Low	3.35	1.135	175
Medium	3.6	1.28	67
High	3.15	1.271	106
Supplemental	2.67	1.167	556
Total	2.92	1.229	904
Q6. I am likely to ask questions in this course	Low	3.15	1.045	175
Medium	3.43	1.104	67
High	3.2	1.027	106
Supplemental	2.56	1.134	556
Total	2.81	1.149	904
Q7. Amount of my interaction with other students increased	Low	2.83	1.18	175
Medium	3.19	1.171	67
High	3.25	1.301	106
Supplemental	2.36	1.159	556
Total	2.62	1.23	904
Q8. Quality of my interaction with other students was better	Low	2.82	1.163	175
Medium	3.19	1.062	67
High	3.32	1.231	106
Supplemental	2.47	1.122	556
Total	2.69	1.18	904
Q9. I feel connected with other students	Low	2.86	1.133	175
Medium	2.99	1.121	67
High	3.01	1.238	106
Supplemental	2.34	1.125	556
Total	2.57	1.175	904
Q10. Amount of my interaction with the instructor increased	Low	3.05	1.146	175
Medium	3.39	1.154	67
High	2.89	1.027	106
Supplemental	2.34	1.193	556
Total	2.62	1.218	904
Q11. Quality of my interaction with the instructor was better	Low	3.2	1.088	175
Medium	3.51	1.106	67
High	3	1.024	106
Supplemental	2.5	1.219	556
Total	2.77	1.217	904
Q12. This course required more time and effort	Low	2.9	1.123	175
Medium	2.99	1.225	67
High	2.91	1.142	106
Supplemental	3.01	1.162	556
Total	2.98	1.156	904
Q13. this course has improved my understanding of key concepts	Low	3.66	0.963	175
Medium	3.78	0.85	67
High	3.6	1.002	106
Supplemental	3.14	1.073	556
Total	3.34	1.059	904
In order to test for differences between blends Pillai's Trace was used with MANOVA, as it is generally considered the most robust of the multivariate analysis statistics for unequal sample sizes and non-normally distributed data. There was a statistically significant difference between the blends on the combined dependent variables, F(39, 2630) = 5.15, p < 0.001; Pillai's Trace V = 0.21; partial η2 = 0.07. Follow up univariate ANOVAs indicated significant differences (p < 0.001) for all of the dependent variables, except Q12 (this course required more time and effort) which was not significant (p = 0.64). These results are given in Table 3.


Table 3. Follow up ANOVA for dependent variables.

Dependent variable	Sum of Squares	df	Mean Square	F	Partial Eta Squared
Q1. I am satisfied with this course	93.58	3	31.19	27.21⁎	0.08
Q2. I would take another course in the future	71.58	3	23.86	15.47⁎	0.05
Q3. Online and F2F components enhanced each other	78.97	3	26.33	19.10⁎	0.06
Q4. Course offered the convenience	87.65	3	29.22	18.61⁎	0.06
Q5. I am more engaged in this course	105.37	3	35.12	25.14⁎	0.08
Q6. I am likely to ask questions in this course	97.45	3	32.48	26.71⁎	0.08
Q7. Amount of my interaction with other students increased	109.22	3	36.41	26.08⁎	0.08
Q8. Quality of my interaction with other students was better	90.22	3	30.07	23.19⁎	0.07
Q9. I feel connected with other students	75.86	3	25.29	19.45⁎	0.06
Q10. Amount of my interaction with the instructor increased	122.14	3	40.71	30.12⁎	0.09
Q11. Quality of my interaction with the instructor was better	115.47	3	38.49	28.35⁎	0.09
Q12. This course required more time and effort	2.28	3	0.76	0.57	0
Q13. This course has improved my understanding of key concepts	59.09	3	19.70	18.60⁎	0.06
⁎
Significant p < 0.001.

Post-hoc analyses were carried out with the Games-Howell test which does not rely on the assumption of equal variances of sample sizes. Results are given in Table 4. Evident in the table is that students in the Supplemental blend gave significantly lower ratings than those in the other three blends on all questions, with the exception of Q12 where the differences were not significant as noted above. Other significant differences found were that students in the High blend rated the amount (Q7) and quality (Q8) of interaction with other students higher than those in the Low blend. Students in the Medium blend rated the amount (Q10) and quality (Q11) of interaction with the instructor significantly higher than those in the High blend. The former students also perceived that the online and face-to-face components of their course enhanced each other (Q3) significantly more than the High blend students. Also noteworthy is that students in the Low blend did not rate any of the items significantly higher than the High or Medium blends. Thus our first hypothesis that students would tend to perceive blended learning more favourably overall as the proportion of online time increases to 50% is not supported.


Table 4. Post-hoc multiple comparisons of questions with Games-Howell test.

Dependent Variable	(I) Blend of online and face-to-face	(J) Blend of online and face-to-face	Mean Difference (I-J)	Std. Error	Sig.	95% Confidence Interval
Lower Bound	Upper Bound
Q1. I am satisfied with this course	Low	Medium	− 0.11	0.13	0.84	− 0.45	0.24
High	0.22	0.12	0.28	− 0.10	0.54
Supplemental	0.69	0.08	0⁎	0.47	0.9
Medium	High	0.33	0.15	0.14	− 0.07	0.73
Supplemental	0.80	0.12	0⁎	0.48	1.12
High	Supplemental	0.47	0.11	0⁎	0.17	0.76
Q2. I would take another course in the future	Low	Medium	− 0.23	0.17	0.54	− 0.68	0.22
High	0.19	0.16	0.62	− 0.22	0.59
Supplemental	0.56	0.11	0⁎	0.29	0.83
Medium	High	0.42	0.19	0.14	− 0.08	0.92
Supplemental	0.79	0.16	0⁎	0.38	1.2
High	Supplemental	0.37	0.14	0.04⁎	0.02	0.73
Q3. Online and F2F components enhanced each other	Low	Medium	− 0.20	0.15	0.55	− 0.59	0.19
High	0.25	0.14	0.31	− 0.12	0.61
Supplemental	0.61	0.10	0⁎	0.36	0.86
Medium	High	0.44	0.17	0.05⁎	0	0.89
Supplemental	0.81	0.14	0⁎	0.45	1.16
High	Supplemental	0.36	0.13	0.02⁎	0.04	0.69
Q4. Course offered the convenience	Low	Medium	− 0.33	0.13	0.05⁎	− 0.67	0
High	− 0.24	0.13	0.25	− 0.57	0.09
Supplemental	0.48	0.10	0⁎	0.23	0.73
Medium	High	0.09	0.14	0.91	− 0.28	0.47
Supplemental	0.81	0.12	0⁎	0.5	1.12
High	Supplemental	0.72	0.12	0⁎	0.41	1.02
Q5. I am more engaged in this course	Low	Medium	− 0.24	0.18	0.53	− 0.71	0.22
High	0.2	0.15	0.53	− 0.19	0.59
Supplemental	0.69	0.10	0⁎	0.43	0.94
Medium	High	0.45	0.20	0.12	− 0.07	0.96
Supplemental	0.93	0.16	0⁎	0.5	1.36
High	Supplemental	0.49	0.13	0⁎	0.14	0.83
Q6. I am likely to ask questions in this course	Low	Medium	− 0.28	0.16	0.27	− 0.69	0.12
High	− 0.05	0.13	0.98	− 0.38	0.28
Supplemental	0.59	0.09	0⁎	0.35	0.83
Medium	High	0.23	0.17	0.50	− 0.2	0.67
Supplemental	0.88	0.14	0⁎	0.5	1.25
High	Supplemental	0.64	0.11	0⁎	0.35	0.93
Q7. Amount of my interaction with other students increased	Low	Medium	− 0.36	0.17	0.15	− 0.8	0.08
High	− 0.41	0.16	0.04⁎	− 0.81	− 0.01
Supplemental	0.47	0.10	0⁎	0.21	0.74
Medium	High	− 0.05	0.19	0.99	− 0.55	0.44
Supplemental	0.83	0.15	0⁎	0.44	1.23
High	Supplemental	0.89	0.14	0⁎	0.53	1.24
Q8. Quality of my interaction with other students was better	Low	Medium	− 0.37	0.16	0.09	− 0.78	0.04
High	− 0.50	0.15	0.01⁎	− 0.88	− 0.11
Supplemental	0.36	0.10	0⁎	0.10	0.62
Medium	High	− 0.13	0.18	0.89	− 0.58	0.33
Supplemental	0.73	0.14	0⁎	0.37	1.09
High	Supplemental	0.85	0.13	0⁎	0.52	1.19
Q9. I feel connected with other students	Low	Medium	− 0.13	0.16	0.86	− 0.55	0.29
High	− 0.15	0.15	0.73	− 0.53	0.23
Supplemental	0.52	0.10	0⁎	0.26	0.77
Medium	High	− 0.02	0.18	1.00	− 0.5	0.45
Supplemental	0.65	0.15	0⁎	0.26	1.03
High	Supplemental	0.67	0.13	0⁎	0.33	1.01
Q10. Amount of my interaction with the instructor increased	Low	Medium	− 0.34	0.17	0.18	− 0.77	0.09
High	0.16	0.13	0.60	− 0.18	0.51
Supplemental	0.71	0.10	0⁎	0.45	0.97
Medium	High	0.50	0.17	0.02⁎	0.05	0.95
Supplemental	1.04	0.15	0⁎	0.65	1.44
High	Supplemental	0.54	0.11	0⁎	0.25	0.83
Q11. Quality of my interaction with the instructor was better	Low	Medium	− 0.31	0.16	0.22	− 0.72	0.1
High	0.2	0.13	0.41	− 0.13	0.53
Supplemental	0.70	0.10	0⁎	0.45	0.95
Medium	High	0.51	0.17	0.02⁎	0.07	0.94
Supplemental	1.01	0.15	0⁎	0.63	1.39
High	Supplemental	0.50	0.11	0⁎	0.21	0.79
Q12. This course required more time and effort	Low	Medium	− 0.08	0.17	0.96	− 0.53	0.37
High	0	0.14	1	− 0.36	0.36
Supplemental	− 0.11	0.10	0.67	− 0.37	0.14
Medium	High	0.08	0.19	0.97	− 0.41	0.56
Supplemental	− 0.03	0.16	1	− 0.44	0.38
High	Supplemental	− 0.11	0.12	0.81	− 0.42	0.21
Q13. this course has improved my understanding of key concepts	Low	Medium	− 0.12	0.13	0.78	− 0.45	0.21
High	0.05	0.12	0.97	− 0.26	0.37
Supplemental	0.51	0.09	0⁎	0.29	0.73
Medium	High	0.17	0.14	0.62	− 0.2	0.54
Supplemental	0.63	0.11	0⁎	0.34	0.93
High	Supplemental	0.46	0.11	0⁎	0.18	0.74
⁎
Significant p ≤ 0.05.

5.2. Course performance
Means, standard deviations, and sample sizes for GRADEDIFF for each blend are given in Table 5. What is immediately apparent from the table is that students in the Supplemental blend performed lower in their courses than their CGPA, resulting in a negative mean difference score (M = − 0.23). Students in the High blend had the highest mean difference (M = 0.68). A one-way between subjects ANOVA was conducted to compare the effect of blend on performance with GRADEDIFF as the dependent variable. The Welch ANOVA statistic was employed because the assumption of homogeneity of variances was violated, as assessed by Levine's test for equality of variances (p < 0.001). A significant effect of blend on performance for the four conditions was found, F(3, 254.5) = 17.3, p < 0.001.


Table 5. GRADEDIFF size, mean, and SD for each blend.

Blend	n	Mean	SD
Low	181	0.22	1.15
Medium	73	0.58	1.09
High	111	0.68	1.13
Supplemental	334	− 0.23	1.61
Total	699	0.12	1.42
Next post-hoc comparisons using the Games-Howell test were carried out. As indicated in Table 6 mean performance in the High blend was found to be significantly higher than those of the Low (p = 0.005) and Supplemental blends (p < 0.001), however no significant difference was found between the High and Medium blends (p = 0.945). Mean performance in the Supplemental blend was significantly lower than the other three blends. Fig. 1 illustrates the box plots and 95% confidence intervals for the four blends. We hypothesized that students in the High (50%) blend would perform higher than those in the other three blends. Therefore our hypothesis was not supported because High blend students out-performed only two of the three blends.


Table 6. Post-hoc multiple comparisons of pre-post difference scores with Games-Howell test.

(I) Proportion of online and face-to-face	(J) Proportion of online and face-to-face	Mean difference (I-J)	Std. error	Sig.	95% Confidence interval
Lower bound	Upper bound
Low	Medium	− 0.37	0.15	0.080	− 0.77	0.03
High	− 0.46	0.14	0.005⁎	− 0.82	− 0.11
Supplemental	0.44	0.12	0.002⁎	0.13	0.76
Medium	High	− 0.09	0.17	0.945	− 0.52	0.34
Supplemental	0.81	0.15	0⁎	0.41	1.21
High	Supplemental	0.90	0.14	0⁎	0.55	1.26
⁎
Mean difference significant at p < 0.05.

Fig. 1
Download : Download high-res image (70KB)
Download : Download full-size image
Fig. 1. GRADEDIFF means and confidence intervals for each blend.

6. Discussion
This study responds to calls for more nuanced research on aspects of blended learning that contribute to student success rather than overall comparative studies between blended and non-blended conditions. To this end we investigated two hypotheses using participants in 20 undergraduate courses that were offered in four different blend proportions: (1) that students tend to perceive blended learning more favourably as the proportion of online time increases up to 50%, and (2) that students in a 50% (High) blend would perform higher relative to the other three course blends.

6.1. Student perceptions and blend
With regard to student perceptions, our results taken as a whole are consistent with previous research (e.g., Owston et al., 2013) that shows that students tend to prefer blended learning over traditional classroom instruction, particularly with regard to their satisfaction (Q1: 68% Agreed and Strongly Agreed) and its convenience (Q4: 69% Agreed and Strongly Agreed). The feeling of being connected to other students in the course and to the instructor was rated less than satisfactory by students in our study. This less than positive experience may have diminished the learning environment as connectedness and sense of community are seen as highly desirable characteristics of undergraduate pedagogy in general (Chickering & Gamson, 1987) and of online learning in particular (Garrison, 2011).

Our results indicated a significant relationship between perceptions and proportion of time replaced by online activities (p < 0.001), although the effect size was very small (η2 = 0.07). Post-hoc analyses showed that students in the Supplemental blend rated all questions significantly lower than the other three blends except for the question about the course requiring more time and effort (Q12), where there were no significant differences. We do not have evidence on how successful the TAs were in leading the online tutorial sessions in the Supplemental blend, although on-going support was provided by the instructional designer and course instructor as the course progressed. One might speculate that the finding of overall low perceptions in the Supplemental blend may well be a function of large classes and relatively inexperienced TAs.

The finding that students in the High blend rated the amount and quality of interaction with other students higher than the Low blend is of particular interest. Although we did not gather data on the amount of time students engaged in online dialog, the explanation for these differences may have been that the instructors of the High blend courses felt compelled to create an online environment with substantial online discussion among students because they were meeting face-to-face for only half of the normal amount of time (i.e., only every other week). In contrast it is possible that instructors in the Low blend may not have made the effort to create an online community because they were seeing the students in class for only about an hour a week less. The finding that students in the Low blend did not rate any questions significantly higher than the Medium or High blends may have been for essentially the same reason: that is they may not have made a wholehearted effort to make a substantive online component because they were meeting students for only slightly less often than normal. Less clear is the reason why students in the Medium blend rated the amount and quality of interaction with the instructor significantly higher than those in the High blend as no data were available on the nature or frequency of interactions that took place in courses.

The only published study we were able to identify that offers a suggestion of the relationship between student perceptions and blend was conducted by Madriz and Nocente (2016). The researchers examined student satisfaction and engagement in seven different undergraduate courses (N = 569). Although their purpose was not to examine the relationship between perceptions and blend ratio, three of the seven courses studied had reduced seat time whereas the remaining four did not. Follow-up with one of the co-authors (N. Nocente, personal communication, December 13, 2016) revealed that the Chemistry, Geography, and Writing courses had blends that coincidentally matched the Low, Medium, and High blends respectively of this study. Although significant differences across the seven courses were reported on the engagement and satisfaction scales, post-hoc analyses showed that the mean ratings of the three courses with reduced seat time were approximately the same and none were rated higher than other courses without a seat time reduction. Thus given the findings of Madriz and Nocente and the equivocal findings of our study, we conclude that the relationship between student perceptions and blend is not strong. Even so the question deserves more study, but with data collected on potential mediating factors such as the amount and nature of online interaction among students and instructor and kinds of online activities in which students engaged.

6.2. Performance and blend
Our findings concerning student course performance are more straightforward. We hypothesized that students in the High blend would perform better than the other three blends as the literature (e.g., Bernard et al., 2014; Means et al., 2013) suggests that 50% may be near the optimum for performance. A lower proportion of online time would approach a fully face-to-face class and a higher proportion would approach a fully online course, and students in neither of these instructional models perform as well as blended instruction. The 50% proportion also coincides approximately with Asarta and Schmidt's (2015) finding on how often students actually choose to attend class when all lectures are available online and class attendance is optional.

We found a significant difference across the four blends in course performance. A follow up post-hoc comparison of the means showed that students in the High blend performed significantly higher than those in the other two blends, although there was no difference between High and Medium blends. Also noted was a trend of increasing student performance as the proportion of online time increases as shown in Fig. 1. We could not determine if the High blend is an inflection point beyond which performance decreases as none of the blends studied were > 50% online. Nonetheless, our findings suggest that those seeking to maximize the performance benefits of blended learning may wish to consider having at least a Medium (36% to 40%) and up to a High (50%) proportion of course activities online. This suggestion is based on our data for a broad range of social sciences and humanities courses; however, particular subject areas may not be as well suited to as much as 36% to 50% online time when there is a need for, say, oral language practice or development of hands-on or other in-person performance skills. By the same token other subject areas may be better suited to more online time such as those in the STEM fields requiring problem solving and practice. Thus the 36% to 50% recommendation should be taken only be a guideline subject to modification depending on specific course requirements.

Looking at student perceptions and performance together, our suggestion of favouring Medium and High blends still holds. Neither the Low nor the Supplemental blends exceeded the Medium or High blends on any of the items on the perception scale or on performance. Based on our findings the Supplemental blend is the least advantageous. Given the relatively short amount of time available for online activities, the apparent lack of structure given to them, and the absence of any integration of the activities into the main lectures, this finding was not unexpected for the Supplemental blend.

7. Conclusions and implications
We began by asking the nagging question of what proportion of time should be devoted to online activities when designing blended courses and whether it matters. There is no simple answer to this question because many other factors must also be taken into account. As discussed earlier these include student characteristics and access to technology, instructor attitudes and openness to new pedagogical approaches, institutional support, and nature of the subject matter (e.g., Alammary et al., 2015; Brown, 2016). Nonetheless we suggest that the findings of this study will help inform decisions about design aspects of blended courses. Across a wide variety of subject areas and course levels, student perceptions and performance appear to be higher when at least one-third to one-half of normal face-to-face time is replaced with online activities. Simply devoting more time to online activities in and of itself will not necessarily result in improved perceptions and performance. The activities must be designed so that they promote student-to-student and instructor-to-student interactions if the affordances of blended learning are to be realized. We concluded this because students in the High and Medium blends tended to rate questions about interactions more favourably that those in the Low and Supplemental blends. This conclusion is consistent within the social-constructivist perspective of learning whereby students participate actively in the learning process by discussing their ideas with one another, resolving disagreements, and collaborating on solving complex problems, and the role of the instructor is to design contexts and facilitate learning activities (Palincsar, 1998). Empirical support for our conclusion comes from Castaño-Muñoz et al. (2014) who studied a very large sample of students across three universities with the goal of explaining why students in blended courses performed better than their face-to-face counterparts. They found that students who engaged in interactive learning (communicating with lecturers, communicating with fellow students, online discussions about study topics, cooperative work) achieved significantly higher than those who learned individually (searching for information, bibliographic searches, looking up course materials, working with bookmarks, subscribing to mailing lists in study area).

The implications of the above conclusion are significant for the design of blended courses. Instructors and course developers need to plan the online portion of their blended courses so that opportunities for rich, meaningful interactions are central, particularly when devoting so much time of a course (one-third to one-half) to online activities. The interaction may stem from content students have independently studied or learned in class, formal debates on issues, role modeling, team problem solving, group projects, or other activities structured by the instructor, according to McGee and Reis (2012) in their extensive synthesis of the literature on best practices for blended course design. The authors also report that there is a consensus that prompt feedback is essential for engaging students online in blended courses, and that the instructor be continuously involved online with students throughout the course. In this regard Salmon (2011) provides helpful guidelines, illustrative case studies, and resources to assist instructors become effective moderators of online discussions.

Our conclusion is also relevant at the institutional level. When academic administrators are implementing blended learning campus-wide, they typically need to limit the varieties of blends to only a few options otherwise the possibility of better utilizing classroom space by placing courses in unused time slots is diminished (Dziuban, Hartman, Cavanagh, & Moskal, 2011). Our recommendation is for administrators to consider a one-third or a one-half seat time reduction. For example, a 3 h course would meet face-to-face only 2 h per week in a one-third reduction, or one and a half hours per week (or face-to-face every other week) for a one-half reduction. By doing so administrators could have some confidence that student perceptions and performance would not be diminished and possibly be superior to classes meeting the full amount of time face-to-face every week.

A further conclusion is that research into the proportion of time spent online is a fruitful mediating factor to consider when studying blended learning. We suggest this regardless of whether a comparative study with other learning models is undertaken, when looking at student preferences and performance within blended courses themselves, or when comparing blended learning outcomes across different fields of study. We also recommend that blends with > 50% online be studied (e.g., 75%) to assess whether 50% is an inflection point as far as performance is concerned and if the relationship between proportion online and performance diminishes beyond this point. A limitation of this research is that we did not have data on the nature or frequency of online activities that occurred in each course. We recommend that future studies collect data on the kinds of activities employed and extent of student engagement in the online portion of blended courses to help explain and understand any observed differences across the various blends being studied. Lastly it is imperative that, regardless of the characteristic of blended learning studied, researchers clarify whether the designers of the courses under study replaced face-to-face time with online activities or whether online activities supplemented the normal face-to-face time, and how much time was devoted to online activities under either model. Doing so will help in interpreting their findings because as this study suggests the proportion of online time appears to influence outcomes.

