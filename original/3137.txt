Online courses are a growing part of the community college experience, but many students, particularly minority students or students who are more at-risk, face challenges in passing those courses. This paper presents results from an experimental study of an effort to redesign a set of core introductory online courses to include a set of technology tools and instructional practices designed to improve students' experiences in the online environment. Results from the study showed that treatment students were less likely to withdraw, and minority students, specifically, were more likely to pass the class and to persist to the next year of college.

Previous
Next 
Keywords
Online learning

Community college: Technology

Community of Inquiry

1. Introduction
In 2016, approximately 6.1 million U.S. students were enrolled in two-year community colleges, an amount that is expected to grow 12% over the next ten years (National Center for Education Statistics, 2018a). These institutions are generally open access, enrolling all who seek to further their education and often serving those who are most disadvantaged.

Online courses are a growing component of the community college experience (Lokken, 2017) with approximately one-third of community college students enrolled in at least one online course (Ginder, Kelly-Reid, & Mann, 2018). These courses are valued by students for their flexibility, allowing people to take courses at their own time and sometimes at their own pace (Xu & Jaggars, 2011a). They are also valued by institutions as a way to reduce costs. Yet, research is fairly consistent that community college students frequently face challenges in online courses, with many students performing worse than in traditional face-to-face courses (Hart, Friedmann, & Hill, 2018; Jaggars, 2011; Palacios & Wood, 2016; Xu & Jaggars, 2011a; Xu & Jaggars, 2011b). For example, one study found successful completion of online courses was about 6–8 percentage points lower than in similar classes that were face to face (Hart et al., 2018). Results may be even worse for minority students, low-income students, and students who are underprepared (Jaggars, 2011).

Failing to complete courses successfully leads to decreased probability of persistence and degree attainment, a problem that is particularly pressing at community colleges. The average graduation rate for first-time, degree-seeking students in community colleges is 30% (National Center for Education Statistics, 2018b), while the drop-out rate is higher for minority students (Radford, Berkner, Wheeless, & Shepherd, 2010).

Wake Technical Community College (Wake Tech), the largest community college in North Carolina, USA, faces these challenges at a higher level. They have a lower-than-average graduation rate of 18% (Wake Technical Community College, 2019) and have sought to implement interventions to increase their success rate. They also have a growing number of students taking online courses. At the start of this project, approximately a quarter of their enrollments were in online courses with 18% of their student population enrolled exclusively online. Wake Tech's data showed that, overall, only 62% of students who began core online courses successfully completed those courses, a rate that was between 7 and 9 percentage points lower than the success rate in their face-to-face courses. Online pass rates were substantially worse for students of color who had success rates ranging from 46% to 50%. To address this issue, Wake Tech received funding from the U.S. Department of Education's First in the World grant competition for Project COMPASS, an effort to redesign the course delivery of a core set of online courses. This paper reports on the results of a study designed to examine the impact of Project COMPASS on core student outcomes.

Using a randomized controlled trial, we examined whether a redesign of introductory online courses at a large community college could increase the rates of completion of these courses and student success rates, particularly for minority students, when compared to the traditional online courses. We also explored whether the model increased students' persistence in college.

2. Theoretical framework
As noted above, the prevalence of online courses has been increasing, particularly in the community college setting (Lokken, 2017). Although research has generally shown positive results for online courses in comparison to face-to-face courses (Means, Toyama, Murphy, Bakia, & Jones, 2010), results have been different for community college students, many of whom come to school with more learning challenges than students in four-year institutions (Bailey, Jeong, & Cho, 2010). Studies have shown that community college students who took online courses were more likely to withdraw from the course and less likely to complete it successfully (Xu & Jaggars, 2011a: Xu & Jaggars, 2011b). Community college students who took online courses were also less likely to take a follow-up course in the same field and to receive a degree (Huntington-Klein, Cowan, & Goldhaber, 2017). As an article describing recent research noted, “taken together these large-scale studies clearly identify community college students to be at risk of significantly worse outcomes online than in face-to-face settings” (Shea & Bidjerano, 2019, p. 8).

Struggles in online courses may be caused by a combination of the characteristics of the students taking them and the nature of the online learning environment (Gregory & Lampley, 2016). For example, students taking online courses were more likely to be older, be employed, be married, and have dependents (Radford, 2011), which meant that students had more conflicts to balance with their courses (Hyllegard, Deng, & Hunter, 2008). The structure of the course may also pose challenges. Asynchronous online courses, which typically have defined start and ending dates as well as specific due dates for assignments and exams but do not have set times for class meeting or lectures, require students to keep track of their own work and to be self-motivated and able to effectively manage their time (Gregory & Lampley, 2016; Kerr, Rynearson, & Kerr, 2006).

Community colleges may not be able to address the out-of-school reasons for students not completing online courses; however, they can address issues related to the design of the course. As a result, Wake Tech decided to redesign a core set of online courses by utilizing the Community of Inquiry (COI) Framework (Garrison et al., 2000, Garrison et al., 2010), one of the most influential frameworks for online course design. The COI Framework was originally developed to address the issue that online learning represented a shift from more oral to text-based communication as courses became more asynchronous. This text-based learning involves a different sort of dynamic than face-to-face learning does (Garrison, Anderson, & Archer, 2001). The COI framework argues that there are three types of online presences that influence the type of learning that occurs in an online environment: 1) teaching presence, 2) social presence, and 3) cognitive presence (Arbaugh, 2007; Garrison et al., 2001). Higher reported levels of these presences were associated with higher performance in a course and higher perceived levels of learning (Arbaugh, 2008; Rockinson-Szapkiw, Wendt, Wighting, & Nisbet, 2016).

2.1. Teaching presence
Teaching presence is the extent to which the learning environment is designed and facilitated to support the other presences. It is also the extent to which there is direct instruction focused on the content of the course (Arbaugh et al., 2008). Additionally, Schreiner, Anderson, & Cantwell, 2011 found that teachers who engage in higher quality interactions with students are more likely to influence the students' willingness to learn and persist. As articulated by the staff for Project COMPASS, teaching presence is the kind of presence an instructor projects in an online community. For example, instructors with strong teaching presence are those who post frequently, actively remind students of deadlines, invite questions, respond quickly to students, and solicit and incorporate feedback.

2.2. Social presence
Social presence is defined as “the degree to which participants in computer-mediated communication feel affectively connected one to another” (Swan et al., 2008, p. 2) and is seen as critical in supporting students' online learning (Diaz, Swan, Ice, & Kupczynksi, 2010). In the project, social presence refers to the way in which students interact and are affectively connected with each other and with the instructors.

2.3. Cognitive presence
Cognitive presence is defined as the extent to which learners are able to “construct and confirm meaning through sustained reflection and discourse” (Arbaugh, 2007, p. 74). It emphasizes the engagement of students in critical thinking and is seen as challenging to implement in an online environment (Arbaugh, 2007). In the project, cognitive presence is expected to be enhanced by the type of questions instructors ask and the type of activities in which students are expected to engage.

The Wake Tech staff designed Project COMPASS such that it integrated technologies and instructional practices in an effort to increase these three presences. Further, because course completion rates among African-American males were lower than overall completion rates, the Wake Tech staff incorporated strategies for strengthening cultural relevance in materials and events as well as early alert interventions and strategies to reduce feelings of isolation and disengagement to promote retention (Twigg, 2005; Wood & Ireland, 2014). We describe the intervention below.

3. Intervention
Funded by a $3 million federal grant that began in 2015, the goal of Project COMPASS was to increase the number of students, particularly students of color, completing online courses and to improve the academic performance of those students, leading to an increase in the percentage of students who remain in postsecondary education. Project COMPASS involved the redesign of the delivery of a core set of online courses so that they incorporated a variety of technologies and strategies intended to increase the quality of the three types of online presence described above. Wake Tech staff – led by a Psychology instructor who had been teaching online for about five years – used both their own experience and the COI Framework to develop an intervention that would accomplish the targeted goals. While the instructional team did not include any people of color, the Project Coordinator, hired at the time of the award, was African American and served as a touchstone for maintaining cultural relevancy in serving the intended population. She encouraged the project to have multiple professional development sessions focused on developing culturally relevant lessons, to include leaders who are persons of color in events, and to ensure that diversity was reflected in course materials.

Project COMPASS hoped to increase the three COI presences by incorporating a set of what the developers called “High Tech” and “High Touch” strategies with the goal of creating a replicable and sustainable model. In this case, “High Tech” tools referred primarily to the increased use of commonly available technologies that were not regularly present in Wake Tech's online environment. “High Touch” strategies included strategies designed to increase the connection points between instructors and students and to provide a more culturally relevant experience. Table 1 below, lays out the High Tech and High Touch strategies, connecting them to the purpose of the tool, including, as appropriate, the COI presence the strategy was intended to impact.


Table 1. Project COMPASS strategies.

Strategy	Specific Expectations	Purpose(s)
High-Tech Tools
Use web conferencing technology to communicate with their students.	
–
Provide a week-one orientation within three days of course start date.

–
Teacher Presence

–
Hold weekly, voluntary live-streamed student gatherings to facilitate student-to-student interaction.

–
Social Presence

–
Live stream two office hours per week.

–
Teacher Presence

Use a texting technology (e.g., Remind, Regroup) to communicate with their classes.	
–
Engage students in live text chats for interventions (see below) or when initiated by the student.

–
Social Presence

Create custom video content.	
–
Create and deploy week-one orientation video.

–
Teacher Presence & Retention Strategy

–
Create and deploy weekly “announcement” videos.

–
Retention Strategy

Use discussion boards/forums to support collaborative inquiry and problem-solving and to facilitate student-to-student interaction.		
–
Social Presence & Cognitive Presence

High-Touch Strategies
Demonstrate a proactive communication style.	
–
Send due date reminders for all graded assignments using any of the following tools: texts, emails, Blackboard announcements.

–
Retention Strategy

–
Send one affirmational announcement/email broadcast to class every week.

–
Teacher Presence

–
Demonstrate high responsiveness by responding to all email/texts within six hours (during the hours of 8am-8pm), six days a week.

–
Teacher Presence

–
Offer multiple low-stakes opportunities for students to demonstrate mastery of content, providing both automated and personalized feedback.

–
Cognitive Presence

Engage in proactive intervention strategies designed to identify and mentor students before they get in trouble.	
–
Contact students who are taking the class for the second time during the first week of class. The goal of this contact is to provide information regarding support services available to the student.

–
Retention Strategy

–
Send weekly email/text follow-up to students who miss work during the previous week.

–
Retention Strategy

–
Attempt to contact students who do not log into Blackboard for seven days to offer help (using the student’s listed phone number).

–
Retention Strategy

Design classes that minimize barriers for minority students.	
–
Demonstrate inclusiveness by including images and topics that feature minorities and that emphasize multicultural issues where possible in the class.

–
Cultural Relevance

–
Include at least one major assignment with a multicultural component that demonstrates the importance of cultural awareness.

–
Cultural Relevance

–
Emphasize minority leaders in the field (e.g., psychologists, minority businesses).

–
Cultural Relevance

–
Use online meeting technology to host at least one live-streamed event with a minority speaker from campus during the semester.

–
Cultural Relevance

–
Provide online services for student support using online meeting technology. Student services can include library services, individualized learning center, and club events.

–
Retention Strategy

These strategies were first implemented in two high-enrollment online introductory courses, an introductory Psychology course (PSY) and an introductory Business course (BUS). These courses were selected because they were high enrollment gateway courses with lower-than-average successful completion rates.

Wake Tech put a number of supports in place to assist instructors in implementing the intervention. They created a video production studio and hired a production assistant so that instructors could make their own videos, and they made new (to Wake Tech) relevant technology applications—such as the Remind texting app and Adobe Connect for online conferencing—available to the instructors. Two Wake Tech instructors were the lead developers for the intervention and created a detailed written protocol for instructors to follow. The other treatment instructors received initial training in the protocol and then met regularly during the first semester of the study for “brown bag” lunches to discuss implementation. Finally, Wake Tech hired two individuals, an instructional designer and an instructional technologist, who were available to work with the instructors on integrating technology and the Project COMPASS protocol into their courses. The instructional designer and instructional technologist conducted needs sensing across instructors of both departments and provided skills-based professional development sessions as well as one-on-one technology assistance for treatment instructors. The instructional designer and instructional technologist worked most closely with the two lead instructors. They worked with the Psychology lead instructor to develop a semester-long gamification element that featured multi-ethnic characters to engage students in content designed to promote a growth mindset, familiarize students with campus resources, and identify success strategies, which was recognized in 2018 as a Blackboard Exemplary Course. In addition to helping the lead Business instructor incorporate the project strategies into the curriculum, the Instructional Designer and Instructional Technologist also helped to develop an open-source-based textbook to replace the existing one. This textbook was another strategy to remove barriers (in this case cost) to success for low-income students. Content, including organizers, readings, additional resources, visuals, and self-check activities were incorporated into the digital-based textbook. A print version was also made available through the bookstore for a nominal print fee.

The First in the World grant competition, which funded this project, had a requirement for a rigorous impact evaluation. In addition, the Wake Tech staff were interested in both testing the effectiveness of the model and building their institution's capacity to engage in rigorous research (Ryan & Wang, 2018). As a result, Wake Tech contracted with a team at the University of North Carolina at Greensboro to conduct an experimental study of the project's impact.

4. Materials and methods
This study was designed to look at the impact of the redesigned courses on key student outcomes. The specific research questions are:

1.
What was the impact of students' taking at least one redesigned online course on the percentage of students successfully completing the course when compared to traditional online courses?

2.
What was the impact of students' taking at least one redesigned online course on the percentage of students persisting in postsecondary education?

3.
To what extent did impacts differ by course?

4.
To what extent did impacts differ by minority status?

We addressed the research questions using an experimental design in which students who signed up for the same online course were randomly assigned to sections taught by Project COMPASS instructors (the treatment group) or to sections taught by instructors not participating in Project COMPASS (the control group). More details about the randomization process are available in Gicheva, Edmunds, Thrift, Hull, and Bray (2020).

4.1. Sample
The sample included students who enrolled in the targeted PSY or BUS courses in the Fall of 2017 or the Spring of 2018. The number of students for the fall and spring semesters combined was 1032 students who enrolled in PSY and 911 students who enrolled BUS. There were 35 students with concurrent enrollment in both classes; thus, the number of unique students included in the study was 1908.

Because we were interested in estimating Intent-to-Treat effects of the intervention, we kept all students who were originally randomly assigned in the analytic sample, even if they ended up dropping or being dropped from the course prior to the course starting. There were 69 students who enrolled in either BUS or PSY in the Fall 2017 semester and then enrolled in one of the study courses in the Spring 2018 semester. These students were randomized in the Fall of 2017 and their Fall 2017 data were included in the Fall 2017 cohort, but they were not randomized a second time and their data were excluded from the Spring 2018 sample. We excluded data from their subsequent class because of concerns that, if the intervention was successful at encouraging more students to persist in school, these students may have been more marginal students and would thus bias the impact estimates. Appendix A provides a CONSORT diagram documenting the creation of the analytic samples for the impact analyses.

The PSY sample included 434 students in 16 treatment sections and 598 in 20 control sections across the two semesters. The BUS sample included 478 students in 8 treatment sections and 433 students in 9 control sections. Table 2 below shows the characteristics of the students in the analytic samples for the two courses. The characteristics of minority and white or Asian students in the sample are shown separately in Appendix B, Table B-1.


Table 2. Baseline characteristics of core analytic sample, overall and by subject.

Characteristic	Overall	PSY	BUS
Treatment (N = 912)	Control (N = 1031)	Effect Size (SD)	Treatment (N = 434)	Control (N = 598)	Effect Size (SD)	Treatment (N = 478)	Control (N = 433)	Effect Size (SD)
% Female	59.6%	60.6%	−0.02 (0.49)	67.5%	65.6%	0.04 (0.473)	52.5%	53.8%	−0.03 (0.499)
% Hispanic	8.7%	11.3%	−0.09 (0.301)	8.5%	12.7%	−0.13 (0.312)	8.8%	9.2%	−0.01 (0.286)
% Black	28.7%	32.0%	−0.07 (0.46)	29.3%	30.4%	−0.02 (0.458)	28.2%	34.2%	−0.13 (0.463)
% White or Asian	55.6%	50.7%	0.10 (0.499)	55.5%	51.5%	0.08 (0.499)	55.6%	49.7%	0.12 (0.499)
Mean Age (in years)	25.1	25.8	−0.08 (9.046)	25.0	26.0	−0.10 (9.137)	25.1	25.5	−0.04 (8.943)
% Identified as disabled	1.6%	1.5%	0.01 (0.123)	1.6%	1.5%	0.01 (0.124)	1.7%	1.4%	0.02 (0.123)
% Pell Eligible	46.2%	50.3%	−0.08 (0.5)	46.5%	52.5%	−0.12 (0.5)	45.8%	47.3%	−0.03 (0.499)
GPA at start of semestera	2.49
(N = 507)	2.56
(N = 540)	−0.07 (0.928)	2.54
(N = 252)	2.66
(N = 323)	−0.13 (0.936)	2.44
(N = 255)	2.41
(N = 217)	0.04 (0.908)
a
Not all students had baseline GPA data. The sample numbers reflect the number of students with data.

4.2. Measures and data collection
This impact study used administrative data collected by Wake Tech as part of their regular course administration and data from the National Student Clearinghouse. The specific outcomes we analyzed are described below.

4.2.1. Withdrawal from the course
This measure captured students not completing the course at all and was defined as students never attending, withdrawing, or dropping the course after enrollment. It also included students who were randomly assigned but were then dropped for non-payment of tuition or fees.

4.2.2. Passing the course
This outcome equaled 1 for students who completed the targeted course with a grade of D or better and 0 for students who dropped, withdrew, or received a grade of F.

4.2.3. Successful completion of the targeted course
This was defined as completion of the course with a grade of A, B, or C, which allows for the course credit to transfer to a four-year institution. Students who completed the course with grades of D or F or who withdrew or dropped the course after enrolling were considered as not successfully completing the course.

Data for the three outcomes above came from Wake Tech's administrative records.

4.2.4. Persistence in postsecondary education: semester-to-semester
The first way in which we defined persistence was enrollment in and/or graduation from any postsecondary institution in the semester subsequent to participation in the study. For students enrolled in the fall of 2017, the subsequent semester was defined as enrollment with a start date from December 1st, 2017 through April 30th, 2018. For students enrolled in the spring of 2018, the subsequent semester was defined as any enrollment with a start date between May 1st and November 30th, 2018. This broad definition allowed for the varying structures of terms and semesters (trimesters, summer school, etc.) found at different institutions. Students who enrolled in the given period but withdrew shortly after were still considered as persisting. Graduating in the same time period was also counted as persisting, with any sort of credential being accepted.

4.2.5. Persistence in postsecondary education: year-to-year
The second definition of persistence that we used was enrollment in and/or graduation from any postsecondary institution within one year after the intervention. This definition was appropriate for community colleges, because many students enroll part time and may be able to enroll only in the fall or spring semester. This persistence measure was coded as 1 for students in the fall 2017 cohort who enrolled or graduated at any point between December 1st, 2017 and November 30th, 2018. For the spring 2018 cohort, year persistence equaled 1 if they enrolled or graduated at any point between May 1st, 2018 and April 30th, 2019.

Data for both persistence outcomes came from the National Student Clearinghouse and were linked to Wake Tech identifiers by Wake Tech staff.

We did not analyze course grades because receiving a grade of A-F was contingent on not dropping or withdrawing from the class, which was itself a function of the intervention. If the intervention was successful at increasing course completion rates, students who received a grade could no longer have been considered as randomly assigned to the treatment and control groups and the treatment group could have comprised of more marginal or low-performing students. Descriptive evidence of the effects of the course redesign on the grade distribution in each course is shown in Fig. 2 in the Results section and Figs. C-1 and C-2 in Appendix C.

Fig. 2
Download : Download high-res image (122KB)
Download : Download full-size image
Fig. 2. Distribution of course outcomes for all students in the sample.

Note: *p ≤ .05; **p ≤ .01.

We used the following student-level characteristics as covariates in our models.

•
Gender. Students self-identified as male or female. An indicator of gender was available for all students but one.

•
Race/ethnicity. Students self-identified as members of one racial category, including Asian, American Indian, Black/African-American, White, and Multiracial. Regardless of their racial selection, they could also choose to identify as Hispanic/Latino under ethnicity. Students who identified as African-American, Hispanic/Latino, Native American, or Multiracial were counted as minority students in our analyses. Nine percent (9%) of students did not disclose their race, ethnicity, or both.

•
Age. This was the student's age in years as of enrollment in the course. This variable was available for all students.

•
Disability status. Students were flagged if they self-reported a disability to the college. Students who did not have this flag were considered as not possessing a disability, and so there were no missing data.

•
Pell Eligibility. For students who submitted a Free Application for Federal Student Aid (FAFSA), this variable indicated if they qualified for Pell grants. This variable was considered a measure of student financial need. Students who were not identified as Pell eligible (whether or not they submitted the FAFSA) were considered as not eligible, and so there were no missing data.

•
Placement test scores. This variable included the student's score on any placement test from the following list: COMPASS1 Pre-Algebra, Accuplacer Arithmetic, SAT Math, ACT Math, Accuplacer Reading & Writing, and COMPASS Reading and Writing. We used high school GPA for 14% of the students, for whom none of these test scores were available. For students who were still missing an achievement measure, we used the student's cumulative GPA for all courses taken at Wake Tech prior to the semester in which they enrolled in one of the study courses (23%). We standardized each score to make the scales comparable. The achievement measure had missing values for 26% of students.

Because race, ethnicity and gender were missing only if students elected to not provide that information, missing values for these variables were recoded to a “not provided” category and students in that category were retained in the analysis. Missing baseline achievement scores were imputed using linear regression and the student characteristics from the main impact estimation model: treatment group indicator, age and age squared, gender, race, ethnicity, PELL eligibility, and disability status.

We also used information on the rates at which students completed the study courses successfully in prior semesters in order to control for instructors' baseline performance. Our study was an Individual Randomized Group Treatment Trial (IRGT) in which students were randomly assigned to teachers but the intervention occurred at the teacher level. As Weiss (2010) points out, the impact estimates from IRGT studies may be biased if there is variation in impacts by teacher and if teachers are not randomly assigned to the intervention. Including baseline instructor performance as a covariate in the analysis allowed us to hold constant at least some of the variation in underlying teacher effectiveness. For instructors who taught the targeted courses in at least one of the two semesters prior to the beginning of the study period, we constructed a measure of the percentage of students successfully completing the course prior to the start of the intervention. This measure was set to “0” for instructors who had not taught these classes previously, and we also included an indicator for new instructors.

4.3. Analyses
The analyses were conducted as Intent-to-Treat (ITT) analyses whereby students remained in their originally assigned groups (treatment or control) and were included in the analysis regardless of whether they ended up participating in the intervention or not. ITT is the standard for most impact studies as it keeps the original random assignment intact (Institute of Education Sciences, 2005) and ensures that results are not driven by attrition or students leaving the intervention. However, because ITT includes in the sample students who did not participate in the intervention, it has the potential to underestimate the true impact (Hollis & Campbell, 1999). To assess the bias introduced by non-participation, in addition to the impact analyses described below, we also repeated the analyses excluding a small share of students who were removed for non-payment prior to the course starting or who dropped the course before seeing their section assignment (no-shows). Four percent of our population were considered no-shows. No control students ended up enrolling in a treatment section, so there were no cross-overs in the sample (see Appendix A).

We estimated a two-level model with random assignment at the student level and students clustered within section. Outcomes were also measured at the student level. In the discussion that follows, we use i = 1, …, N to denote students and j = 1, …, M to denote course sections. We analyzed five separate outcomes: whether student i completed the course with a grade of C or higher, which we defined as successful completion; whether student i passed the course with a grade of D or better; whether student i dropped the course early in the semester or withdrew with a grade of “W”; whether student i was still enrolled or graduated in the semester following the intervention; and whether student i was still enrolled or graduated at any point during the year following the intervention.

Since our outcomes were binary, we estimated a logit model:(1)

The vector of student-level covariates Xijincluded 1) indicators for gender, race, and ethnicity; 2) age and age squared; 3) an indicator for disability; 4) an indicator for Pell eligibility; and 5) the achievement measure discussed in the previous section, including its imputed values; 6) an indicator for concurrent enrollment in PSY and BUS; and 7) an indicator for having taken the same course prior to the fall 2017 semester. The section-level controls in Zj included indicators for subject (PSY or BUS) and semester. The vector Zj also included the instructor covariates, namely successful completion rates for the time period prior to the start of the study and an indicator for whether a given instructor had previously taught online sections of these courses.

The treatment indicator Tij equals 1 if student i was randomly assigned to a redesigned section. The coefficient of interest α measures the effectiveness of the intervention on the log odds of success, and exp.(α) is the associated odds ratio. The marginal effect of the intervention for student i in section j, defined as the change in the probability of positive outcome caused by the intervention, is given by α[P(Yij = 1)][1 − P(Yij = 1)]. In the results section, we report the average of the marginal effects for students in the sample. The model treats ej as a random effect, making the assumption that ej is uncorrelated with Xij and Tij. If this assumption is satisfied, the estimate of α in (1) is unbiased and consistent.

We also estimated the model in Eq. (1) for several subgroups in order to assess whether impacts differed for minority students compared to white or Asian students and students in PSY compared to students in BUS. We report average marginal effects for the students in each subgroup.

Students had different probabilities of being randomly assigned to a treatment section depending on the time when they registered and the course they registered for. We used stabilized inverse probability weights in the impact analyses that were equal to the inverse of each student's individual probability of being assigned to their treatment condition multiplied by the overall probability, by course and semester, of being in the respective treatment or control group (Imbens & Rubin, 2015).

5. Results and discussion
5.1. Impact on course outcomes
We begin by showing descriptively in Fig. 2 how the distribution of outcomes differed between the treatment and control groups. We see evidence that the share of students who dropped or withdrew from the class was lower in the redesigned sections. There was a 2.6 percentage point increase in the share of students who completed the course but received an F, and a 1 percentage point increase in the share of students who completed with a D. There was little difference in the share of students receiving B's and C's, but slightly more students in the redesigned classes earned an A. Similar graphs for each course and by minority status are shown in Appendix C.

We explore the course completion impacts in more detail in Table 3, where we show results from the logistic regression model in (1).


Table 3. Impacts on course completion, intent-to-treat.

Population	Outcome	Treatment group	Control group	ITT
Estimated effects
Adjusted Mean	Standard
Deviation	Mean	Standard
Deviation	Adjusted Mean
Difference
(percentage point)	p-value
All Students	% Drop or Withdraw	27.36%	0.456	37.25%	0.484	−9.89%	[0.000]
% Completing the course with D of higher	62.81%	0.483	57.81%	0.494	5.00%	[0.027]
N(T) = 912 N(C) = 1031	% Completing the course with C or higher	56.26%	0.494	53.44%	0.499	2.82%	[0.343]
PSY	% Drop or Withdraw	28.58%	0.461	43.98%	0.497	−15.40%	[0.000]
% Completing the course with D of higher	61.44%	0.491	52.01%	0.500	9.43%	[0.003]
N(T) = 434 N(C) = 598	% Completing the course with C or higher	55.12%	0.500	46.49%	0.499	8.63%	[0.023]
BUS	% Drop or Withdraw	31.49%	0.452	27.94%	0.449	3.55%	[0.582]
% Completing the course with D of higher	63.60%	0.475	65.82%	0.475	−2.22%	[0.555]
N(T) = 478 N(C) = 433	% Completing the course with C or higher	56.94%	0.486	63.05%	0.483	−6.11%	[0.170]
Minority Students	% Drop or Withdraw	32.55%	0.481	45.38%	0.498	−12.83%	[0.000]
% Completing the course with D of higher	55.97%	0.499	47.48%	0.500	8.49%	[0.007]
N(T) = 381 N(C) = 476	% Completing the course with C or higher	47.95%	0.500	42.23%	0.494	5.72%	[0.216]
Non-Minority Students	% Drop or Withdraw	22.65%	0.428	30.78%	0.462	−8.13%	[0.002]

N(T) = 507 N(C) = 523	% Completing the course with D of higher	69.74%	0.457	66.54%	0.472	3.20%	[0.258]
% Completing the course with C or higher	63.61%	0.475	63.29%	0.482	0.32%	[0.922]
Note: The adjusted treatment mean is calculated by adding the impact estimate to the unadjusted control mean. When comparing impacts by course, the differences in impacts between the two courses were statistically significant. When comparing the impacts by minority status, the difference in impacts between the two groups (minority and non-minority) were not significant.

The results in Table 3 show that the treatment decreased the probability that a student withdrew or dropped the course by a statistically significant 10 percentage points. The probability of passing the course with grade D or better was 5 percentage points higher in the redesigned courses. There was a positive, but non-significant, impact on successful course completion for all students. Table 3 further shows that the beneficial impacts appear to be driven primarily by a larger difference between the treatment and control groups in the PSY course, while the impacts were statistically indistinguishable from zero or negative in the case of successful course completion for students in the BUS course. Minority students were a group that was particularly targeted by the intervention and, as the table shows, there was a statistically significant reduction in the percentage of students dropping the course and a statistically significant positive impact on the percentage of minority students completing the course with D or better. For white or Asian students, only the impact on drops and withdrawals was statistically significant. Although Table 3 shows evidence that the intervention was more effective for minority students, none of the differences between the estimated coefficients for minority and white or Asian students were statistically significant.

5.1.1. Exploring variations in impacts
To understand why the project might have had a successful impact in PSY but not in BUS, we collected additional contextual information. First, we noticed that, although the treatment instructors remained consistent across semesters, the control instructors changed over the semesters. Two of the BUS control instructors who taught in the spring of 2018 were very high performing teachers who had been online teachers of the year. The data showed that baseline success rates, which we included as covariates in the regressions, were slightly higher for the control compared to treatment instructors in PSY (58% versus 56%), but higher for treatment than control instructors in BUS (71% versus 58%). We did control for these baseline differences, but it is also possible that there were other instructor characteristics that were not captured by the covariates in the model.

To address the concern above, we explored further the influence of the two lead instructors, who had been using these types of strategies for a long time, by excluding their classes from the analyses. We chose to exclude the lead instructors because of their greater familiarity with the new methods and because they were most likely to use effective instructional strategies even in the absence of the formal intervention. When the lead instructors' classes were excluded from the analyses, the impact estimates decreased by 2 to 3 percentage points but remained statistically significant for minority students and for students in PSY. The impact on withdrawal rates for students of color changed from −12 to −10 percentage points, while the estimated impact on passing the class for this group remained at 7 percentage points. This suggests that the lead instructors themselves did have higher impacts, but that the intervention benefitted students of color even if administered by non‑lead instructors. The full table showing these results is included in Appendix B.

We also conducted observations focused on implementation of the instructional strategies for both treatment and control teachers (see Table 4). The goal of the observations was two-fold: 1) to follow good practice in impact studies and see if there were, in fact, differences in instructional practices between the treatment and control groups given that control teachers could have implemented many of these practices without the intervention (Kurki, Boyle, & Aladjem, 2006); and 2) explore whether instructional practices differed by course. Our methodology and findings for these analyses are presented in an accompanying paper (Edmunds, Gicheva, Thrift, & Hull, 2020), but we summarize key takeaways here. Analyses of the observations showed that treatment teachers in both BUS and PSY implemented the measured practices at a higher rate than the control teachers, with a somewhat higher treatment-control contrast in PSY. Additionally, the difference between treatment and control sections in PSY was higher with certain strategies (such as synchronous events and strategies associated with reducing barriers).


Table 4. Average observation scores and their impact.

Protocol Strategies	PSY	BUS
Treatment	Control	Impact on Completing the Course with C or Higher	Treatment	Control	Impact on Completing the Course with C or Higher
Synchronous Events	2.25	0.00	2.48%	0.67	0.00	5.87%
Announcements	2.00	1.29	8.72% *	2.22	1.13	0.77%
Personalized Videos (Internal, Orientation, Getting Started)	1.75	0.00	4.06%*	1.83	0.00	3.37%*
Reducing Barriers for Minorities (e.g., minority images, announcements of campus events related to cultural diversity/inclusiveness)	0.38	0.00	22.71%**	0.50	0.31	6.56%*
Threaded Discussions	2.25	2.63	0.18%	2.00	2.25	−1.46%
Total Implementation Score	1.73	0.78	11.42%**	1.44	0.74	5.56%
Note: Each impact estimate comes from a separate regression. *p ≤ .05; **p ≤ .001.

To explore whether the differences in practices might be explaining the differences in impact, we also conducted regression analyses that linked implementation of specific strategies to outcomes. Results from these analyses (also in Table 4), showed that higher implementation of reducing barriers was the strategy most strongly associated with reduced drops and withdrawals and increased successful course completion. As the table shows, implementing the program's expectations relative to reducing the barriers for minorities corresponded to a 23 percentage point increase in the probability of successful course completion in PSY and 7 percentage point increase in BUS; these point estimates are large, but it should be noted that the average number of such course components was lower than 1 in both courses. More detail on exploring the variations in impact across the two courses can be found in a separate paper (Edmunds et al., 2019).

5.1.2. Additional sensitivity analyses
Next, because our Intent-to-Treat design required the inclusion of students who were assigned to the course but never attended (NAs), we also conducted a sensitivity analysis that excluded the NAs. The findings (shown in Fig. 3 and Appendix B), were very similar to the main analysis with an overall negative impact on withdrawals of −9.5 percentage points (compared to −9.9 percentage points for the ITT analysis) and a positive impact of 4.6 percentage points on completing with D or better (compared to 5 percentage points for the ITT analysis).

Fig. 3
Download : Download high-res image (392KB)
Download : Download full-size image
Fig. 3. Sensitivity of impacts on course completion to alternative samples.

Note: *p ≤ .05; **p ≤ .01. NA is never attended.

The final sensitivity analyses involved conducting separate analyses for dropping the course (which students could do with no penalty prior to a set timepoint at the beginning of the course) and withdrawing from the course (which resulted in a “W” grade on a student's transcript and the student still having to pay full tuition); these results are also shown in Appendix B. The results show that for the groups with the highest overall impacts – minority students and students in PSY – the effect on course completion was mainly driven by fewer students withdrawing from the course. For non-minority students, however, the impact was higher for students who dropped the course than for later withdrawals. A small number of researchers have begun exploring who drops at what time with results suggesting that better prepared students might drop earlier in the semester and more poorly prepared students drop later in the semester (Bosshardt, 2004; McKinney, Novak, Hagedorn, & Luna-Torres, 2019). Our results suggest that different types of students do drop at different times and this area of research should be further explored.

5.2. Impacts on persistence
The theory of change for Project COMPASS argued that, if students were successful in their courses, they would also be more likely to remain in school. Given the results reported above, we might have expected to see increased persistence for minority students or for students in the PSY course. As Table 5 below shows, we did see positive impacts on year-to-year persistence for minority students with a statistically significant impact of 5.9 percentage points. The impact on all students' persistence year-to-year was also positive (3.2 percentage points, with a p-value of 0.08). No other impacts were statistically significant.


Table 5. Impacts on persistence, intent-to-treat.

Population	Outcome	Treatment Group	Control Group	ITT
Estimated Effects
Adjusted Mean	Standard
Deviation	Mean	Standard
Deviation	Adjusted Mean
Difference	p-value
All Students
N(T) = 912
N(C) = 1031	% Persisting to Next Year	80.33%	0.413	77.10%	0.420	3.23%	[0.082]
% Persisting to Next Semester	72.26%	0.450	70.60%	0.456	1.66%	[0.381]
PSY
N(T) = 434 N(C) = 598	% Persisting to Next Year	79.41%	0.404	76.40%	0.425	3.01%	[0.190]
% Persisting to Next Semester	71.73%	0.448	70.20%	0.458	1.53%	[0.576]
BUS
N(T) = 478 N(C) = 433	% Persisting to Next Year	78.10%	0.420	78.10%	0.414	0.004%	[0.999]
% Persisting to Next Semester	72.00%	0.453	71.10%	0.454	0.90%	[0.746]
Minority Students
N(T) = 381
N(C) = 476	% Persisting to Next Year	79.77%	0.427	73.90%	0.439	5.87%	[0.025]
% Persisting to Next Semester	69.54%	0.463	66.60%	0.472	2.94%	[0.306]
Non-Minority Students
N(T) = 507 N(C) =523	% Persisting to Next Year	81.67%	0.400	79.90%	0.401	1.77%	[0.480]
% Persisting to Next Semester	75.13%	0.440	74.00%	0.439	1.13%	[0.678]
Note: The adjusted treatment mean is calculated by adding the impact estimate to the unadjusted control mean. When comparing impacts by course, the differences in impacts between the two courses were statistically significant. When comparing the impacts by minority status, the difference in impacts between the two groups (minority and non-minority) were not significant.

6. Limitations
This study has many strengths, not the least is that it is a randomized controlled trial, which is rare in higher education settings. However, there are also limitations to the study, some of which we have already discussed. For example, although students were randomly assigned, teachers were not, which can cause challenges in interpreting the findings.

Another limitation is that the intervention is a collection of disparate strategies and the study was designed to test the impact of all of those strategies in combination. Although, we did conduct some exploratory analyses to look at which strategies were associated with better outcomes (Table 4), we were not able to definitively determine which strategies were most effective. Future research might attempt to test the efficacy of individual strategies or combinations of different strategies.

Third, the study did not include a formal cost analysis. Excluding the cost for the evaluation, approximately $2.7 million was spent on developing and implementing the intervention but we do not currently have an estimate for the actual implementation costs for the intervention. Defining these implementation costs would allow practitioners to determine whether the costs are outweighed by the benefits and the extent to which this intervention is more or less cost-effective than other interventions.

7. Conclusions
Project COMPASS was an effort to redesign the delivery of online courses to increase student completion rates and success in the courses. The model included a set of High Tech Tools and High Touch strategies designed to improve the teaching presence, social presence, and cognitive presence (Arbaugh, 2007) within online courses.

The analyses showed that the model was successful in reducing the percentage of students withdrawing from courses and increasing the percentage of minority students passing the courses. The estimated impacts for non-minority students were smaller. Minority students were a target for the intervention and some of the strategies, such as increasing the use of minority images and providing access to minority lectures and other events, were explicitly targeted to minority students. However, the study was not designed to determine conclusively which strategies were contributing the most to the changes in the intervention.

The study results do suggest that some of the students who did not withdraw may have ended up failing the course. At this point, there is very limited research on whether it is better for a student to drop a course or to continue and fail it. One study suggested that students who complete a course, even if they do not pass it, are more likely to persist than students who withdraw from a course (Bosshardt, 2004). We hope to use data from our study to explore this idea in more depth.

The study also found that there were differences in impact by subject with statistically significant negative impacts on withdrawals and statistically significant positive impacts on successful completion for PSY and no impact or negative impacts for BUS. Understanding these differences has been the focus of extensive analyses described in a separate paper (Edmunds et al., 2019). Conclusions from this paper suggest that the higher impacts may be due to some combination of teacher-level effects as well as some differences in implementation in specific strategies, particularly regarding efforts to reduce barriers for minority students.

Relative to persistence, the study found positive persistence results at the one-year point for minority students. This is consistent with the theory of change given that this was also the population that saw positive impacts in successful course completion. There were, however, no statistically significant findings for persistence when we looked only at the next semester. This does suggest that, as noted anecdotally by the Wake Tech staff, students may have patterns of enrollment that involve taking courses every other semester.

This study, which was conducted in introductory PSY and BUS courses, could be considered as an efficacy study testing whether the model works under ideal settings. Two of the course instructors were developers of the model and worked extensively with the other treatment instructors to implement the model. There were also extensive professional development requirements. The results suggest that the intervention can have an impact, at least in some courses. Separate analyses we have conducted suggested that higher implementation of these strategies was associated with positive outcomes. Figuring out how to implement these strategies effectively in different subjects and different types of institutions remains an area for future research.

