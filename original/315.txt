Online social networks provide an opportunity to spread messages and news fast and widely. One may appreciate the quick spread of legitimate news and messages but misinformation can also be spread quickly and may raise concerns, questioning reliability and trust in such networks. As a result, detecting misinformation and containing its spread has become a hot topic in social network analysis. When misinformation is detected, some actions may be necessary to reduce its propagation and impact on the network. Such actions aim to minimize the number of users influenced by misinformation. This paper reviews approaches for solving this problem of minimizing spread of misinformation in social networks and proposes a taxonomy of different methods.

Previous
Keywords
Social networks

Misinformation spread minimization

Influence minimization

Diffusion models

1. Introduction
The proliferation of internet technologies has led to an increasing number of online social networks and users. Several indicators suggest that the number of users keeps increasing and a large number of people have accepted online social networks as a major source of news. The potential of social networks has led to significant research in trying to propagate news widely by identifying so-called influential users (Zareie et al., 2019, Sheikhahmadi and Zareie, 2020). This problem, known as Influence Maximization (Domingos and Richardson, 2001, Kempe et al., 2003), has attracted lots of attention recently. However, spreading news fast gives rise to an adverse effect: misinformation can be spread fast too.

Users may spread misinformation inadvertently or with different financial and social motivations (Meel and Vishwakarma, 2019). Misinformation propagation has become a significant threat in social networks and reduces the reliability and confidence of the users towards news and messages. As quoted in Tong et al. (2018b), a report published by the World Economic Forum regards spreading misinformation as one of the top global economic risks. Spreading misinformation or unsubstantiated rumours may have widely negative impact and may lead to economic damages, significant disruption or even widespread panic; various examples have been mentioned in the literature (Wen et al., 2015, Fan et al., 2013, Wu et al., 2017, Fan et al., 2014, Tong et al., 2020, Fang et al., 2020). Misinformation may take different forms. In this paper, the term misinformation is used as a general term to refer to any false or inaccurate information which may be spread in online social networks intentionally or unintentionally.

Confronting misinformation in social networks has attracted lots of attention among researchers. To deal with it, there are several aspects that have to be addressed: (i) identification of misinformation among newly produced information, also known as misinformation detection, is important as early detection of misinformation decreases the chances of wide propagation with potentially adverse effects; (ii) detection of the sources of misinformation is important as it helps identify the culprits and potentially malicious users who initiate the propagation of misinformation; (iii) tracking subsequent (re)appearances of already detected misinformation, possibly in a slightly altered form but still misinformation; (iv) minimization of the spread of misinformation is another important aspect which aims to prevent the propagation of misinformation in the network. A plethora of methods have been proposed for each of these aspects in the literature.

Methods to detect misinformation and identify sources are reviewed in Bondielli and Marcelloni, 2019, Meel and Vishwakarma, 2019, Alzanin and Azmi, 2018 and Meel and Vishwakarma, 2019, Shelke and Attar, 2019, respectively. Methods to detect misinformation are based on mapping a stream of social media posts to a classification system that labels posts as misinformation or non-misinformation. Methods to identify sources are based on the network structure and propagation graph from which users or locations that initiate misinformation are identified. In Sharma et al. (2019), intervention methods for misinformation detection and mitigation are classified and reviewed. Approaches for the development of data mining tools for misinformation tracking and verification are reviewed in Zubiaga et al. (2018). In Ahsan et al. (2019), methods for detection and controlling rumour in social networks are reviewed from a multidisciplinary (Psychology, Sociology and Epidemiology) viewpoint; Ahsan et al. (2019) also reviews the features that favour wide propagation of misinformation. Yet, the literature lacks a comprehensive review and classification of the methods explicitly proposed to minimize the spread of misinformation, which act as an important deterrent when confronting misinformation. Although (Meel and Vishwakarma, 2019, Ahsan et al., 2019, Sharma et al., 2019) shortly pay attention to this topic, they do not focus on minimizing the spread of misinformation. This paper aims to fill this gap by reviewing and classifying all existing methods in the literature for the minimization of the spread of misinformation. In comparison with previous surveys, distinct differences of our work are:

•
We focus on approaches that minimize the spread of misinformation in social networks, after misinformation has been detected.

•
A new taxonomy and a comprehensive review of state-of-the-art methods is presented that offers extensive coverage of the subject.

•
Evaluation strategies that include real-world datasets and random models to generate synthetic datasets for evaluation purposes are also presented.

•
Current challenges and potential future directions are thoroughly discussed.

The rest of the paper is organized as follows: Section 2 contains definitions and background information. A formal definition of the problem of Minimizing the Spread of Misinformation (MSM) and key strategies to address it are presented in Section 3. A detailed discussion of the different methods to find solutions to the MSM problem is given in Sections 4 Blocking-based methods, 5 Clarification-based methods . Section 6 covers evaluation strategies and datasets for the assessment of different methods. Finally, Section 7 concludes the paper and discusses future research directions.

2. Preliminaries
2.1. Online social networks
An online social network is an abstraction that captures the interactions between people relying on some internet-based infrastructure. People join online social networks with different goals, such as socializing, keeping in touch with friends, as well as reading and/or sharing news. The ability of every user to spread news is an important benefit of online social networks but it has an adverse effect too. Alongside legitimate information, spreading misinformation may have some disruptive impact, including distrust and unreliability towards news (Shu et al., 2017).

In the literature, an online social network is modelled as a graph. The nodes and edges indicate users and relationships between them, respectively. In this paper, a social graph is denoted by , where  and  represent nodes and edges of the graph. If , it means there is a relationship between nodes  and  and these nodes are called neighbours.  denotes the set of neighbours of node ; the cardinality of this set indicates the degree of the node, i.e., . A weight  may be associated to each edge  indicating the influence (spreading) probability of node  on , that is, how likely (weight values closer to 1) or unlikely (weight values closer to 0) it is that node  can influence node . In some research, the network is considered as a directed graph. In a directed graph,  denotes  is an in-neighbour of  and  is an out-neighbour of , which assumes that influence is not bi-directional and if one node influences another the opposite is not necessarily true.

In principle, whether the edges are considered as directed or undirected depends on the nature of relationships in the network. For instance, friendship on Facebook is an undirected relationship while the follow relationship on Twitter is a directed relationship. In addition, in some research the network is considered as unweighted graph, which means that all edges have the same influence. If additional information is available, the influence between each pair of users can be determined; then, distinct weights are assigned to the edges and the network is modelled as a weighted graph.

2.2. Diffusion models
Different diffusion models have been proposed to simulate the process of spreading information and determine the influence of an initial set of spreader nodes. Modelling the behaviour of users in accepting and forwarding information in social networks is a challenging topic (Song and Dinh, 2014). Diffusion models aim to describe the propagation process based on some observations about the network. Thus different diffusion models are applied to model the spreading process. In principle there are three main classes for the commonly used diffusion models: threshold models (Borodin et al., 2010, Granovetter, 1978), cascading models (Carnes et al., 2007, Goldenberg et al., 2001), and epidemic models (Buscarino et al., 2008, Zhou et al., 2012).

The Linear Threshold (LT) model (Kempe et al., 2003) is the most popular threshold model. In this model, each node  has an activation threshold  and can be in either active or inactive state during propagation. In timestamp , the initial spreader nodes are set to active and all other nodes are set to inactive. In each timestamp , each inactive node  changes its state to active if , where  is the set of neighbours of node  which are active in . The propagation process continues until no node is activated in a timestamp. At the end, the number of active nodes indicates the influence of the initial spreader set.

The Independent Cascade (IC) model (Kempe et al., 2003) is a well-known cascading model. Same as with the LT diffusion model, each node can be in either active or inactive state. Initial spreader nodes are set as active in . In each timestamp , each node  activated in  has one chance to activate each of its neighbour  with probability . Regardless of whether  activates any of its neighbours or not, it moves to inactive state. This process continues until no node is activated in a timestamp . The number of nodes activated during the process indicates the influence of the initial spreader nodes. Sometimes, influence in the IC model can be determined using the live-edge technique (Kempe et al., 2003). In this technique some of the edges are set as live and some are set as blocked, randomly. If, after this process, there is a path between two nodes, this implies influence.

The Susceptible–Infected–Recovered (SIR) model (Pastor-Satorras et al., 2015) is a widely used epidemic model in the literature. In this model, each node can be in either susceptible (), infected (), or recovered () state. In timestamp , the initial spreader nodes are set to  and all other nodes are set to . In each timestamp , each infected node  moves to recovered state with probability  after its attempt to infect each of its susceptible neighbours with probability . The infection process continues until no infected nodes remain in the graph. At the end of the process, the number of recovered nodes represents the influence of the initial spreader set. The SIR model can be regarded as a generalization of the IC model, as the latter appears to be a special case of SIR in which .

In practice, the diffusion process may be repeated many times and the mean of the obtained results may be used to estimate the influence of initial spreader nodes.

2.3. Influence detection models
In this subsection different models to determine the influence of a set containing one or more nodes are described.

•
Simulation-based model: This model applies a diffusion model by repeating the simulation of spreading process a number of times and considering the mean of the obtained results as the influence of the set. The time complexity of a simulation-based model to determine an influential -size set is , where  is the number of times that diffusion process is repeated.

•
Path-based model: Maximum Influence Arborescence (MIA) (Chen et al., 2010) is the most popular path-based model that is based on the idea that the influence diffusion of a node is restricted to a local region. Two trees, known as Maximum Influence In-Arborscence (MIIA) and Maximum Influence On-Arborscence (MIOA), are generated to indicate influencers and influencees of a node, respectively. The size of these trees can be adjusted by a given parameter  to meet a trade-off between accuracy and time efficiency. The time complexity of the path-based model to determine an influential -size set is ; where ,  and  are the time complexity of constructing MIIA for each node, maximum size of MIIA and maximum size of MIOA, respectively.

•
Sampling-based model: Reverse Influence Sampling (RIS) (Borgs et al., 2014, Tang et al., 2014) is the most popular sampling model to approximate the influence of a set. The idea is to randomly generate  samples of the graph. In each sample a node is randomly selected and a set of nodes that can reach this node are determined as the reverse reachable set of the node. The more the number of samples that are covered by a set, the more influential the set is. The time complexity of the sampling-based model to determine an influential -size set is ; where  denotes the error of sampling.

•
Centrality-based model: This model applies centrality measures  (Lü et al., 2016) which use the graph structure to determine influence and vitality of each node or edge. Some popular centrality measures are betweenness, closeness, degree or weighted degree. This model is highly efficient with linear time complexity but suffers from low accuracy.

Depending on the influence detection model used, we can approximately determine the time complexity of each method in the rest of paper. In general, in terms of time complexity, these models can be ranked from high to low in the order: simulation-based, path-based, sampling-based and centrality-based.

3. The problem of minimizing the spread of misinformation
Different approaches have been utilized to detect misinformation  (Bondielli and Marcelloni, 2019). Independent of these approaches, once misinformation is detected, a containment strategy should be adopted to minimize the spread of misinformation. In brief, the problem of Minimizing the Spread of Misinformation (MSM) can be defined as follows. A set of Malicious Nodes (MN) intends to propagate misinformation in a social network. A solution to the MSM problem aims to minimize the number of nodes that accept (or are influenced by) this misinformation.

The solution can be broadly based on one of two strategies (Wen et al., 2014, Yan et al., 2019, Yang et al., 2020):

•
A blocking strategy (network disruption): a set of nodes or edges are blocked (or removed) to minimize the flow of misinformation in the network.

•
A clarification strategy (anti-rumour or counterbalance): true information is propagated in order to increase users’ awareness and reduce acceptance or spread of misinformation.

Formally, given a graph , a diffusion model , a set MN with size , solving MSM aims to find and apply a strategy  to minimize the influence of misinformation. Influence of misinformation is determined by the number of users who accept the misinformation during the spreading process following diffusion model . This aim is generally defined using Eq. (1). (1)The MSM problem can be also defined as a maximization problem: (2)where  and  represent the influence of the set  (essentially, this influence is the total number of users accepting the misinformation initiated by the users in the set ) when no containment strategy is applied and when a strategy  is applied to contain spreading, respectively. That is to say, MSM aims to find a strategy  to maximize the number of users who are protected from misinformation.

Selecting a set of nodes or edges to maximize  is an NP-complete problem (Kempe et al., 2003). In some occasions, the problem, as defined in Eqs. (1), (2), may be monotone and submodular, in which case greedy heuristics may find a solution within a factor of the optimal solution (Kempe et al., 2003). In function , monotonicity implies that, if an element is added to the set by strategy , it does not cause a decrease of the value of . If  is a monotone and submodular function, then for each element , .

As mentioned, the strategies to solve the MSM problem can be divided into two main categories: blocking-based and clarification-based. Blocking-based strategies degrade the topology of the graph and may be further subdivided into node blocking and edge blocking. Depending on the strategy, the problem, as defined in Eq. (2), can be further elaborated as follows.

Node blocking strategies aim to find a set of nodes, i.e., , whose removal minimizes the spread ability of MN in ;  and . The problem is then formally defined as in Eq. (3). (3)If a node is blocked this implies that all edges connected to the node are removed. This may lead to an excessive removal of edges, which may be undesirable. Blocking edges may be regarded as a more delicate strategy than blocking nodes.

Edge blocking strategies aim to find a set of edges, i.e., , whose removal minimizes the spread of misinformation in , where . The problem is formally defined as in Eq. (4). (4)

In practice, blocking strategies may impact users’ experience, who may complain or quit a network (Wang et al., 2017), while they may also be viewed as a violation of freedom expression (Hosni et al., 2019). This gives more ground to clarification-based strategies where a set of nodes, , is selected to carry out a truth campaign and propagate true (illustrative) information. In clarification-based strategies, the MSM problem is formally defined as in Eq. (5). (5)where  represents the spread ability of  when both sets  and  spread two opposite messages. Users receiving true information will not accept misinformation and will not forward it further in the network, thereby reducing spread of misinformation. In other words, rising the awareness of users prevents the adoption of misinformation in this strategy without degrading the graph as it is the case with blocking strategies. Yet, clarification-based strategies may be less efficient in reducing misinformation spread, as also noted in Hosni et al. (2019). In fact, an assessment of the advantages and disadvantages of both blocking and clarification strategies in Wen et al. (2014) has led to a compound method that is trying to combine the best of the two worlds.

When some users become victims of misinformation, they may resist to change their beliefs even if they later receive correct information. Because of this, blocking-based strategies may be superior to clarification-based strategies as they typically prevent the receipt of misinformation. On the other hand, blocking edges or even nodes for a long period of time may have a negative impact on user experience and may lead to the withdrawal of users from the network. In comparison to edge blocking, node blocking strategies may lead to higher disruption as all edges connected to the blocked nodes are removed.

Overall, the methods that have been developed to solve the problem of minimizing the spread of misinformation, in line with the key strategies discussed, can be broadly classified according to the hierarchy in Fig. 1. This classification is used in the following sections to review all relevant methods.

4. Blocking-based methods
This section covers methods relying on blocking nodes or edges. As discussed, in a blocking strategy a set of nodes or edges are removed to minimize the spread of misinformation in the network.

4.1. Node blocking methods
In these methods a set of critical nodes () is identified and these nodes are removed from the social graph; all edges associated with these nodes are accordingly removed. Node blocking methods are also known as node immunization methods in the literature (Yang et al., 2018, Wu et al., 2015, Wu et al., 2017). In principle, there are two different approaches for node blocking: (i) in a static approach,  is selected and nodes are blocked at the beginning of the propagation process; (ii) in an adaptive approach,  is selected and the nodes are blocked selectively during the process to take flow of misinformation into account, thus improving the performance of blocking.

The key properties of both static and adaptive node blocking methods are summarized in Table 1. For each method, the table lists information about the type of graph used to represent the network and the diffusion model. When there is no indication of the diffusion model used this is because the relevant work deviates from the three common choices. In order to approximately show the time complexity of each method, the model applied for influence detection in each method is also shown in the table.


Table 1. Properties of node blocking methods including class, graph type, diffusion model for propagation (Linear Threshold (LT), Independent Cascade (IC), Susceptible–Infected–Recovered (SIR)) and influence detection model (Influence Model).

Paper (year)	Class	Graph	Diffusion Model	Influence model
Directed	Weighted	LT	IC	SIR	
Holme et al. (2002)	Static	No	No				Centrality-based
Schneider et al. (2011)	Static	No	No			✓	Centrality-based
Dey and Roy (2017)	Static	No	No				Centrality-based
Wang et al. (2013)	Static	Yes	No		✓		Simulation-based
Tanınmış et al. (2020)	Static	Yes	Yes	✓			Sampling-based
Yao et al. (2015)	Static	Yes	Yes		✓		Centrality-based
Pham et al. (2018b)	Static	Yes	Yes	✓			Path-based
Wu et al. (2017)	Static	No	Yes			✓	Simulation-based
Pham et al. (2017)	Static	Yes	Yes	✓			Centrality-based
Pham et al. (2018a)	Static	Yes	Yes	✓			Simulation-based
Pham et al. (2019)	Static	Yes	Yes	✓	✓		Sampling-based
Zheng and Pan (2018)	Static	Yes	Yes		✓		Simulation-based
Yang et al. (2018)	Adaptive	Yes	Yes		✓		Path-based
Shi et al. (2019)	Adaptive	Yes	Yes	✓			Sampling-based
Song et al. (2015)	Adaptive	Yes	Yes				Simulation-based
Wang et al. (2017)	Adaptive	Yes	Yes		✓		Centrality-based
Wijayanto and Murata (2019)	Adaptive	No	Yes			✓	Centrality-based
4.1.1. Static approach
In Holme et al., 2002, Schneider et al., 2011, Dey and Roy, 2017, the impact of removing nodes with high centrality is assessed to determine which centrality measure is the most effective criterion to minimize the spread of misinformation. In Holme et al. (2002), the goal is to identify a set of nodes whose removal increases the average distance between each pair of nodes in the graph as this can delay the spread of information (and, consequently, misinformation too). The effect of removing nodes with high-degree and high-betweenness is assessed. The authors show that removal (or immunization) of the high betweenness nodes is a more efficient way to contain the spread of misinformation in the network. In Schneider et al. (2011), information spread is regarded as a function of the sum of the sizes of the largest connected clusters. A high-betweenness removal strategy is used iteratively to identify nodes that are immunized. In Dey and Roy (2017), a random walk algorithm is applied to measure the impact of blocking nodes with high centrality on the spread of information; degree distribution, betweenness and closeness centrality are considered for high centrality. The results show better performance when nodes with high closeness centrality are blocked. In all these methods, the source of misinformation (i.e., the  set) is ignored, so they can be considered as source-ignorant node blocking methods.

In other methods, selecting nodes for blocking is done by taking into account the  set. These methods can be regarded as source-aware node blocking methods. In some of these methods a budget constraint, like the number of blocking nodes or the maximum cost of blocking is considered, while some methods aim to minimize the overall cost of blocking assuming that blocking each node has a cost.

In Wang et al., 2013, Tanınmış et al., 2020, Yao et al., 2015, a set of  nodes, , is selected, using the constraints  and  (cf. Eq. (3)); removing these nodes (and their associated edges) the aim is to minimize the influence of . In Wang et al. (2013),  is initially empty and its members are selected iteratively. In each iteration, , the node with the maximum marginal gain is added to . The marginal gain obtained from blocking node  in iteration  is calculated using Eq. (6) as follows: (6)In the equation,  is obtained by removing nodes  and the edges connected to them;  is also obtained by removing  and the edges connected to them. The function  indicates the influence of  in graph . In Wang et al. (2013), influence is calculated using an IC diffusion model. Stochastic bi-level programming, in the form of leader–follower game, and one Tabu-based search meta-heuristic and one greedy heuristic are proposed to solve the problem in Tanınmış et al. (2020). In Yao et al. (2015), a topic-aware method is suggested. In this method, a topic vector  is taken into account to determine different topics in the social network. A weight vector 
 is also associated to each edge 
, where 
 indicates the strength of influence of user 
 on 
 on topic 
. Misinformation, which is propagated in the network, is represented by a vector 
, where 
 indicates the relevance of misinformation to topic 
. Given the vectors 
 and , the probability of spreading misinformation on each edge is calculated. Then, the top-k central nodes in the neighbourhood of nodes in  are selected for blocking. In order to define the top-k central nodes, a topic-aware betweenness and a topic-aware degree centrality measure are proposed.

In Pham et al. (2018b), blocking each node 
 has a cost 
. The goal is the identification of a set of nodes so that the total cost of blocking the nodes does not exceed a given budget . It is also assumed that misinformation is not propagated farther than  hops from the misinformation source. The authors first consider the problem with only one node as misinformation source. A sub-tree of depth  whose root is the source of misinformation is constructed. The influence of each node 
 on its child nodes is calculated based on a depth-search-first algorithm. A near optimal solution is then found using dynamic programming. To solve the problem in the general case, with more than one node as misinformation source, a greedy algorithm is proposed. The inefficiency of the greedy algorithm motivates the use of a speed-up approach (Zhang et al., 2016) to improve its performance. In the improved algorithm, misinformation sources are merged into a super source node  and the MIA method (Chen et al., 2010) is applied to determine the influence of each node. Nodes with a maximum ratio of influence per cost are selected iteratively until the budget is exhausted or no node can be selected with the remaining budget.

In Wu et al. (2017), it is supposed that there is just one node as the source of misinformation, i.e., , and misinformation is propagated up to  hops from the source. The goal is to block the nodes with the highest contagious probability, that is nodes that are most likely to get infected by misinformation. To do so, the contagious probability of each node is calculated based on the SIR diffusion model. Nodes whose probability is greater than a given threshold are considered as candidate nodes for blocking. By removing nodes with low spreading ability from the candidate set, the set  is finally identified. In Pham et al. (2017), the LT diffusion model is extended to propose a time-constraint deterministic LT model. A simulation-based greedy algorithm is then proposed to select a set of nodes whose removal minimizes the spread of misinformation. Due to the high time complexity of the simulation-based method, an efficient heuristic algorithm is also proposed.

Finally, in some research, the goal is to select the smallest set of nodes whose blocking causes a reduction to the spread of misinformation greater than a given threshold. The authors in Pham et al., 2018a, Pham et al., 2019 apply a sampling approach to find the smallest set of nodes whose removal ensures that no more than  users are influenced by misinformation. In Pham et al. (2018a), nodes with the maximum marginal gain are added to  with a greedy approach. In order to calculate the marginal gain of nodes the authors try different mechanisms such as the live-edge method (Kempe et al., 2003), a speed-up approach (Zhang et al., 2016) and a lazy-forward method (Leskovec et al., 2007b). The authors in Pham et al. (2019) emulate the spreading process using the LT and IC diffusion models. They show that the problem of the reduction of the spread of misinformation greater than a given threshold is not submodular in the IC model. They apply the speed-up approach (Zhang et al., 2016) to merge  nodes into a super source node and construct an instance 
 of graph ; the live-edge method is then used to obtain different sample graphs. For each sample, a Directed Acyclic Graph (DAG), rooted in super source node, is built using depth-first traversal to calculate the gain from blocking each node. Nodes with maximum gain are iteratively added to  with the gain of remaining nodes updated in each iteration.

The community structure of the network is taken into account in Zheng and Pan (2018). It is assumed that misinformation originates from a set of users in community 
. In addition to reducing the influence of  to less than a given threshold, the authors try to prevent influencing so-called bridge nodes (nodes which connect 
 to other communities). This prevents spreading of misinformation to the entire network. Based on minimum vertex cover set, a two-step greedy algorithm is proposed to select . In the first step, bridge and reachable nodes are identified using breadth-first traversal originating from nodes in ; then, the minimum number of nodes needed to protect the bridges are blocked. In the second step, while the influence of  is greater than the threshold, nodes with the maximum marginal gain are iteratively added to ; the set of reachable nodes is updated in each iteration.

4.1.2. Adaptive approach
Instead of selecting and blocking nodes at the beginning of the propagation process, critical nodes can be identified and blocked during the propagation process. Take the schematic graph in Fig. 2, for example. Suppose that node  is a malicious node, a source of misinformation, and we have the option of blocking two nodes. Using a static approach it is sensible to block nodes  and  (due to the greater number of out-neighbours compared to ) at the beginning of the propagation process (i.e., ) as this shields a large part of the graph (see Fig. 2(a)). However, suppose that at  propagation from node  flows as indicated by the red edges in Fig. 2(b). In this situation, node  appears to be unaffected and, hence, keeping node  blocked brings no benefit. Instead, blocking node  at  can be more important to stop further spread of misinformation from node . This example highlights that adaptive actions, depending on the flow of misinformation, may be more efficient in containing the propagation of misinformation. The goal of methods relying on an adaptive approach is to block nodes based on the flow of misinformation during the propagation process.

The connection between the static and the adaptive approach is considered in Wu et al. (2015). To do so, the SIR diffusion model is extended to model their relationship and then applied to assess the impact of the adaptive approach on the propagation process. The effect of the size of  and propagation probability on these approaches is also studied. The findings suggest that, in essence, the static and the adaptive approach may overall perform similarly, yet an adaptive approach may achieve this result with fewer nodes blocked.

In Yang et al., 2018, Shi et al., 2019, Song et al., 2015, at each timestamp  during the propagation process, some nodes are dynamically identified and blocked. The goal is to minimize the number of nodes influenced by misinformation at the end of the propagation process. In Yang et al. (2018), a heuristic is proposed to calculate the gain from blocking each node based on the propagation probability between a node and all other nodes in the network. At each timestamp , the node with the maximum gain, 
, is determined and blocked if the expectation of maximum gain at  is less than 
. This algorithm is repeated at each timestamp  until  nodes are blocked. The authors in Shi et al. (2019) propose two different policies for blocking nodes during the propagation process. In the k-R policy, nodes are blocked in  rounds and in each round an equal number of nodes, i.e., , with maximum marginal gain, are selected and blocked; a live-edge technique is applied to determine the marginal gain of nodes. In the -T policy, in each round, a decision on blocking some nodes is made based on the number of nodes that are reachable from infected nodes. The Reverse Influence Sampling method (Borgs et al., 2014, Tang et al., 2014) is also applied to propose a scalable implementation of these policies. In Song et al. (2015), it is supposed that apart from being influenced by neighbours in online social networks, users may be influenced by external sources during the propagation process. In this situation, the importance of using an adaptive approach to block nodes during the propagation process increases. A simulation-based method is proposed to estimate the number of nodes blocked in each timestamp , say 
. A heuristic is then proposed to compute the immunization ability (equivalent to gain from blocking) of each node. In each timestamp , 
 nodes with the highest immunization ability are determined and blocked; the immunization ability of remained nodes is then updated.

A dynamic node blocking method based on user experience is considered in Wang et al. (2017). Rumour popularity (indicating the interest of users to the topic of rumour) and the degree of tolerance to the period of time that users can be blocked are taken into account. Global popularity and individual tendency are integrated using the Ising model (Chelkak and Smirnov, 2012) to model rumour popularity over the time of propagation. User experience is employed to determine the threshold of tolerance to blocking time for users. The goal is to minimize the influence of a rumour by blocking  critical nodes under the constraint of the users’ tolerance threshold. A node blocking approach to minimize the spread of misinformation in temporal networks is studied in Wijayanto and Murata (2019). It is supposed that nodes and edges are dynamic during the propagation process and nodes are blocked dynamically over this process. The minimum vertex cover is applied to find critical nodes at each timestamp. Due to its time complexity, graph embedding techniques are used to construct a feature-based representation of each node and an approximate solution is determined with the help of refinement learning.

Each of the approaches for node blocking has advantages and disadvantages. Static approaches for node blocking are simple and cheap but may suffer from inaccuracy as they do not deal directly with the propagation pattern. On the other hand, adaptive approaches can improve the effects of blocking by taking into account the pattern of propagation in the network but at the expense of higher computational cost due to the need of monitoring and tracking the propagation pattern.

4.2. Edge blocking methods
In node blocking methods the objective is to remove nodes. Edges are blocked when the nodes connecting these edges are blocked. However, as each node may be connected to other nodes through a number of edges, this may remove a large number of edges to such an extent that it may drastically change the network structure. Edge blocking methods aim to address this by identifying a set of critical edges to block, thereby minimizing the spread of misinformation. There are two approaches for edge blocking: (i) a source-ignorant approach ignores the source of misinformation and aims to identify a set of edges whose removal minimizes the flow of information in the network; (ii) a source-aware approach considers the source(s) of misinformation to identify a set of edges for blocking. In both approaches, the goal is always to minimize the spread of misinformation in the network.

The key properties of the edge blocking methods are summarized in Table 2.


Table 2. Properties of edge blocking methods including source (ignorant or aware), graph type, diffusion model for propagation (Linear Threshold (LT), Independent Cascade (IC), Susceptible–Infected–Recovered (SIR)) and influence detection model (Influence Model).

Paper (year)	Source	Graph	Diffusion model	Influence model
Directed	Weighted	LT	IC	SIR	
Kimura et al. (2008b)	Ignorant	Yes	Yes	✓			Simulation-based
Kimura et al. (2008a)	Ignorant	Yes	No		✓		Simulation-based
Kimura et al. (2009)	Ignorant	Yes	No		✓		Simulation-based
Khalil et al. (2013)	Ignorant	Yes	Yes	✓			Simulation-based
Tong et al. (2012)	Ignorant	Yes	No				Centrality-based
Schneider et al. (2011)	Ignorant	No	No			✓	Centrality-based
Dey and Roy (2017)	Ignorant	No	No				Centrality-based
Yao et al. (2014)	Aware	Yes	Yes		✓		Simulation-based
Khalil et al. (2014)	Aware	Yes	Yes	✓			Simulation-based
Yan et al. (2019)	Aware	Yes	Yes		✓		Path-based
Kuhlman et al. (2013)	Aware	Yes	Yes	✓			Centrality-based
Wang et al. (2018)	Aware	Yes	Yes	✓			Sampling-based
Song and Dinh (2014)	Aware	Yes	No				Centrality-based
4.2.1. Source-ignorant approach
The problem of minimizing the spread of misinformation is expressed as a contamination degree minimization problem in Kimura et al., 2008b, Kimura et al., 2008a, Kimura et al., 2009. The contamination degree of the network is calculated based on the influence of all nodes in the network. In Kimura et al. (2008b), the problem is defined as the identification of a set of  edges, whose removal minimizes the average contamination of all nodes under the LT diffusion model. An iterative greedy algorithm is then proposed to solve the problem; in each iteration an edge whose removal minimizes the average contamination degree of nodes is selected for blocking. Due to the time complexity of the LT diffusion model, a method based on Bond Percolation (Kimura et al., 2007) is proposed to approximate the solution. The contamination degree minimization problem is defined under the IC diffusion model in Kimura et al. (2008a). A greedy and a bond percolation based method are then proposed to solve the problem. The contamination degree minimization problem is extended in Kimura et al. (2009) to define the worst contamination degree of nodes in the network. The worst contamination degree refers to the maximum influence of nodes in the graph, while average contamination degree refers to the expected influence of nodes. A greedy algorithm is then proposed to find a set of  edges whose removal minimizes the worst contamination degree of nodes in the graph. Due to the time inefficiency of the greedy algorithm, a bond percolation based method is also proposed to approximately solve the problem.

The authors in Khalil et al. (2013) aim to block a set of  edges to minimize the spread susceptibility of the network. The spread susceptibility of the network is defined as the summation of influence of all nodes. They prove that the problem is submodular and monotone under the LT diffusion model. A greedy algorithm using the live-edge method is then proposed, which guarantees a solution within 
 
 of the optimal solution. In Tong et al. (2012), the eigenvalue of the network matrix is considered as a measure for spread susceptibility in the network; the goal is to identify a set of edges whose removal minimizes the eigenvalue of the matrix. Based on eigenvalues a score for each edge is computed. Then, the  edges with the highest score are considered as a solution of the problem. The sum of the sizes of the largest connected clusters of the graph is defined as the spread susceptibility of network in Schneider et al. (2011); betweenness centrality of the edges is considered as a measure to select edges whose removal minimizes the spread susceptibility. The problem is defined under a random walk model in Dey and Roy (2017) and betweenness centrality of the edges is again used to select edges.

4.2.2. Source-aware approach
In this approach, it is assumed that a known set of malicious nodes, , is the source of the misinformation in the network. The goal is to identify a set of edges whose blocking (removal) minimizes the spread of misinformation that is initiated by nodes .

In Yao et al., 2014, Khalil et al., 2014, the aim is to find a set of edges , with size , to minimize the spread of misinformation. An iterative greedy algorithm, under the IC diffusion model, is proposed in Yao et al. (2014); in each iteration, the edge with the maximum marginal gain is added to . Due to the high computational time of calculating the influence of a set using diffusion models, the live-edge method is applied to propose an efficient iterative greedy method in Khalil et al. (2014). In addition, a descendant-counting tree structure is proposed to update the marginal gain of edges in each iteration of the greedy algorithm efficiently. In Yan et al. (2019), the goal is to block  edges of a candidate set to minimize the sum of the activation probability of nodes in the network. The activation probability of a node denotes the probability that the node is influenced by the nodes in , in other words, how vulnerable the node is to the misinformation spread by the nodes in . Then, a greedy algorithm is proposed that iteratively selects an edge with maximum marginal gain and updates the activation probability of nodes. In Kuhlman et al. (2013), it is assumed that blocking each edge has a cost. Problems under a budget constraint are defined and several greedy algorithms are then proposed.

In Wang et al., 2018, Song and Dinh, 2014, the problem is considered as a target-based problem. In this problem, the goal is to minimize the spread of misinformation towards a given target set . In Wang et al. (2018), the problem is solved under two scenarios: (i) unconstrained, where an unlimited number of edges may be blocked to protect ; (ii) constrained, where at most  edges are blocked to protect  to the best extent possible. The unconstrained scenario is solved using the minimum cut problem (Papadimitriou and Steiglitz, 1982). A sampling-based solution is proposed to select  critical edges in a greedy (and iterative) manner to solve the constrained scenario. The target-based problem is defined under an extension of a cascading diffusion model in Song and Dinh (2014); a mathematical programming method is then proposed to identify a set of critical edges.

Compared to source-ignorant edge blocking strategies, source-aware edge blocking strategies may be more effective in terms of blocking misinformation. However, trying to determine the sources accurately and fast is a challenging issue and the effort to achieve this may come at the expense of focusing on actual edge blocking.

5. Clarification-based methods
In these methods, once again, the assumption is that misinformation originates from a set of certain malicious nodes, . However, the aim is to identify a set of nodes, , to initiate a truth campaign, that is, to spread a clarification message that will counter the misinformation originating from . The ultimate goal is to minimize the number of users accepting (or influenced by) the misinformation. It is noted that this problem, first modelled by He et al. in He et al. (2012), is different from the related problem of competitive influence maximization (Bharathi et al., 2007), where multiple campaigns are trying to maximize their influence at the same time minimizing the influence of all other competing campaigns. To illustrate this, consider the directed graph shown in Fig. 3. Assume that node  is the originator of a misinformation campaign. When the aim is to minimize the spread of misinformation through a clarification message, selecting node  is the best choice as it stops node  from spreading misinformation further. However, in competitive influence maximization where the aim is to maximize the spread of an initiator’s own message, selecting node  would look the best choice.

Clarification-based methods are broadly divided into two categories: (i) campaign-oriented methods, where, given a limit for the size of the truth campaign, the aim is to identify appropriate nodes to initiate the truth campaign so that the spread of misinformation is minimized; (ii) protection-oriented methods, where the aim is to identify a minimum number of nodes to initiate the truth campaign so that a given proportion of users in the network are protected from misinformation.



Fig. 3. Clarification-based misinformation minimization versus competitive influence maximization.

The key properties of the clarification-based methods are summarized in Table 3. As noted, in these methods misinformation and truth are spread simultaneously. Thus, besides the type of graph and diffusion model, it is useful to annotate each method with additional information. The column SP (standing for spread probability) shows whether some research assumes that the spread probability of misinformation and truth on each edge is the same or it can differ (same or diff in the table). The column bias shows what happens when a node is activated by both misinformation and truth campaigns at the same time: negative means that the former wins, positive means that the latter wins, whereas unbiased means that some other criterion is used to decide (such as message popularity, users’ interest in message, etc.).


Table 3. Properties of clarification-based methods including: class (Str standing for structural, Beh standing for behaviour-aware, Pro standing for protection-oriented), graph type, diffusion model (Linear Threshold (LT), Independent Cascade (IC), Susceptible–Infected–Recovered (SIR)), influence detection model (Influence Model), spread probability (SP) and Bias.

Paper (year)	Class	Graph	Diffusion Model	Influence model	SP	Bias
Directed	Weighted	LT	IC	SIR			
He et al. (2012)	Str	Yes	Yes	✓			Path-based	Diff	Negative
Zhang et al. (2015)	Str	Yes	Yes	✓			Simulation-based	Diff	Unbiased
Liu et al. (2016)	Str	Yes	Yes	✓			Centrality-based	Same	Unbiased
Yang et al. (2019)	Str	Yes	Yes	✓			Centrality/Simulation-based	Same	Unbiased
Yang et al. (2020)	Str	Yes	Yes	✓			Centrality/Simulation-based	Same	Unbiased
Budak et al. (2011)	Str	Yes	Yes		✓		Centrality/Simulation-based	Diff	Positive
Wu and Pan (2017)	Str	Yes	Yes		✓		Path-based	Diff	Positive
Arazkhani et al. (2019)	Str	No	No		✓		Centrality-based	Same	Negative
Lv et al. (2019)	Str	Yes	Yes		✓		Centrality/Path-based	Same	Positive
Tong et al. (2020)	Str	Yes	Yes		✓		Sampling-based	Same	Negative
Tong and Du (2019)	Str	Yes	Yes		✓		Sampling-based	Same	Negative
Li et al. (2013)	Str	Yes	Yes	✓	✓		Centrality-based	Same	Positive
Tong et al. (2018a)	Str	Yes	Yes		✓		Sampling-based	Same	Negative
Song et al. (2017)	Beh	Yes	Yes		✓		Sampling-based	Same	Positive
Fan et al. (2014)	Beh	Yes	Yes	✓	✓		Simulation-based	Same	Negative
Fang et al. (2020)	Beh	Yes	Yes		✓		Sampling-based	Same	Unbiased
Litou et al. (2017)	Beh	Yes	Yes	✓			Simulation-based	Same	Unbiased
Hosni et al. (2020)	Beh	Yes	Yes				Centrality-based	Same	Unbiased
Chen et al. (2019)	Beh	Yes	Yes		✓		Sampling-based	Same	Negative
Tong et al. (2018b)	Beh	Yes	No		✓		Simulation	Same	Unbiased
Zhu et al. (2018)	Beh	Yes	Yes		✓		Path-based	Diff	Unbiased
Zhu et al. (2019)	Beh	Yes	Yes		✓		Path-based	Diff	Negative
Wu et al. (2018)	Beh	Yes	Yes			✓	Centrality-based	Same	Unbiased
Nguyen et al. (2012)	Pro	Yes	Yes	✓	✓		Sampling-based	Same	Positive
Nguyen et al. (2013)	Pro	Yes	Yes	✓	✓		Sampling-based	Same	Positive
Fan et al. (2013)	Pro	Yes	No		✓		Simulation-based	Same	Positive
Hosni et al. (2018b)	Pro	Yes	No		✓		Simulation-based	Same	Negative
5.1. Campaign-oriented methods
In these methods there is a budget , which is typically equivalent to the number of nodes that can be used for a truth campaign. The goal is to identify a set  containing at most  nodes to initiate a truth campaign to minimize the influence of , that is, to minimize the number of nodes activated (i.e., users influenced) by misinformation. According to the information considered to select , campaign-oriented methods can be divided into two categories: (i) structural methods that select  simply on the basis of structural information of graph; (ii) behaviour-aware methods where, in addition to graph structure, individual behaviour of users such as preferences, interests, personal profit or location may also be taken into account to select .

5.1.1. Structural methods
In these methods, a set of nodes is selected to initiate a truth campaign. The selection is based on structural properties of the network graph, something that makes these methods widely applicable as structural information is supposed to be readily available.

Some structural methods consider this problem using an LT diffusion model. In fact, this is the approach considered by the first paper in the topic (He et al., 2012), where a competitive LT diffusion model is proposed to simulate the spreading process of the two opposite campaigns by  and . In this model, each node has two thresholds 
 and 
, corresponding to an acceptance threshold for misinformation and truth, respectively. Each edge has two weights 
 and 
, corresponding to the spread probability for misinformation and truth, respectively. Each node can be in either inactive, ＋  active or  active state during the process. At timestamp , nodes in  and  are set to  active and ＋  active, respectively; all other nodes are set to inactive. At each timestamp , positive (truth) and negative (misinformation) influence propagate independently following the LT diffusion model. Each newly activated node changes its state to  active or ＋  active based on the campaign that activates the node and no longer changes its state in subsequent timestamps. If at a timestamp  a node is activated by two campaigns, negative influence wins. In order to identify influential users to add to , the MIA method (Chen et al., 2010) is utilized to construct a local directed acyclic graph and determine the influence of each node in containing misinformation spread. Nodes in  are identified in  iterations; in each iteration, the node with the maximum marginal containment influence is added to .

In Zhang et al., 2015, Liu et al., 2016, it is supposed that when a campaign initiates the propagation of some information (regardless of whether the message is true or not), the propagation is limited within  hops and fades after this time. In Zhang et al. (2015), each node has two different thresholds for accepting misinformation and truth; each edge has two different weights indicating spread probability of misinformation and truth. The LT diffusion model is then extended to simulate the spreading process by misinformation and truth campaigns at the same time. In this model, if a node is activated by two campaigns at the same timestamp, the node decides what message to adopt based on its own preferences. An algorithm is then proposed to find a set of nodes to include in the truth campaign. For this purpose, a set of nodes which may potentially be influenced by the misinformation campaign and have high spread ability (hence, they are influential) are detected as gateway nodes. The nodes for the truth campaign are then selected using a simulation based strategy whose aim is to get the truth campaign to influence gateway nodes before they are influenced by misinformation. An extension of the LT diffusion model is also proposed in Liu et al. (2016) to calculate the activation probability of each node by the misinformation campaign. An iterative method is then proposed to select the nodes of the truth campaign; in each iteration the node that minimizes the activation probability of all other nodes by misinformation is added to truth campaign.

In Yang et al., 2019, Yang et al., 2020, it is assumed that the opinion of users, who are influenced by misinformation, may change after receiving information from the truth campaign. The LT diffusion model is extended to simulate the propagation process under this assumption. Two aspects of the problem are then considered: either every node in the network can be selected as a member of truth campaign, or only a subset of predefined nodes. A greedy simulation based method and a page rank centrality based method are then proposed to solve the problem.

In other structural methods, the problem is defined using the IC diffusion model. A Campaign-Oblivious IC (COICM) model is proposed in Budak et al. (2011). In this model, each node can be in one of three states: C-state (activated by misinformation), L-state (activated by truth) or inactive state. A misinformation campaign and a truth campaign start spreading at the same time, . At each timestamp , each node 
 activated in  has a chance to activate each of its inactive neighbours. If 
 is activated by 
, the state of 
 changes to 
’s state and cannot change in subsequent timestamps. This process continues until no more node is activated. If a node is concurrently activated by two campaigns, the truth campaign wins. Applying the COICM model, a greedy algorithm is proposed to identify a near-optimal truth campaign. Due to the time complexity of the greedy algorithm, three heuristic methods based on high degree nodes, early infectees and likeliest infectees are also proposed.

The COICM model is also adopted in Wu and Pan, 2017, Arazkhani et al., 2019, Lv et al., 2019 to simulate the propagation process. In Wu and Pan (2017), the problem is considered under two scenarios: (i) CMIA-H, where the spread probability of edges for truth is 1; (ii) CMIA-O, where the spread probability of edges for truth is a value between . Applying the MIA method (Chen et al., 2010), an iterative greedy method is proposed where the node with the maximum containment influence is added to the truth campaign in each iteration. In Arazkhani et al. (2019), utilizing degree, betweenness and closeness centrality measures, a centrality-based method is proposed to select nodes for the truth campaign. A community-based method using the COICM model is proposed in Lv et al. (2019). In this method, the COCIM model is first applied to determine the communities and the number of malicious nodes (nodes in misinformation campaign) in each community. Based on the number of malicious nodes in each community, a proportion of nodes for the truth campaign is selected from the community. In Tong et al., 2020, Tong and Du, 2019, two sampling-based methods are proposed using the IC diffusion model. A set of reverse tuples are determined using graph sampling in Tong et al. (2020), based on which an approximation algorithm is proposed to select . Applying the RIS method (Borgs et al., 2014, Tang et al., 2014), a hybrid sampling method is proposed in Tong and Du (2019) to inform a greedy method to identify the truth campaign.

The authors in Li et al. (2013) argue that some nodes may get contaminated by misinformation and may spread it (inadvertently becoming members of the misinformation campaign) because they are unaware of the truth. Such nodes would change their mind if they are faced with the truth. In these circumstances, the problem is then to select  and  nodes to spread the truth and contain misinformation. Applying the LT and IC diffusion models, a greedy simulation-based method is proposed to select nodes with maximum marginal containment influence iteratively. In Tong et al. (2018a), it is assumed that more than one truth campaign may attempt to contain the spread of misinformation. To deal with this multi-campaign spreading problem, an extended multi-cascade IC diffusion model is proposed; then, applying game theory, a method is described to select nodes.

5.1.2. Behaviour-aware methods
In addition to network structure, user characteristics and behaviour are taken into account by behaviour-aware methods. The motivation is that individual user behaviour may allow more elaborate differentiation of nodes than purely structural methods.

In Song et al., 2017, Fan et al., 2014, a time delay is introduced to capture the time that two users may need to exchange information between them; the goal is to minimize the spread of misinformation by a deadline . In Song et al. (2017), each edge is associated with a login probability to denote how quickly information may be received. Depth-first traversal is first applied to determine the threat level of each node in the graph by  and build a DAG. Breadth-first traversal is then utilized to construct weighted reverse reachable trees. Then, for each node, a score is calculated based on the threat level and the influence of the node. The node with maximum score is selected for the truth campaign. Then, the score of remaining nodes is updated and the process is repeated iteratively until all required nodes are selected. In Fan et al. (2014), in addition to login probability, personal interest in misinformation and truth is taken into account for each user. The problem is then considered under the LT and IC diffusion models. Reachable nodes in  timestamps are selected as candidate nodes. Utilizing a Monte Carlo method, the nodes of the truth campaign are iteratively selected; a candidate node with maximum containment influence is added to the truth campaign in each iteration.

In Fang et al. (2020), personal interest in the information related to the misinformation is also taken into account following a source-ignorant approach. The RIS method (Borgs et al., 2014, Tang et al., 2014) is utilized to generate a collection of random reverse sets. A greedy method is then described for maximizing weighted coverage to identify  that considerably covers the reverse random sets.


Table 4. Random models used to generate synthetic graphs.

Model	Description	Used by
Barabasi–Albert model (Barabási and Albert, 1999)	Generates scale free networks which follow a power law distribution degree	Holme et al., 2002, Schneider et al., 2011, Zheng and Pan, 2018, Song and Dinh, 2014, Yang et al., 2019, Hosni et al., 2018b, Khalil et al., 2013
Watts–Strogatz model (Watts and Strogatz, 1998)	Generates small world networks which have a high clustering coefficient and a small average shortest path between pairs of nodes	Holme et al., 2002, Tanınmış et al., 2020, Song and Dinh, 2014, Yang et al., 2019, Hosni et al., 2018b
Erdos–Reyni model (Erdos and Rényi, 1960)	Generates networks with a small clustering coefficient and a small average shortest path between pairs of nodes	Holme et al., 2002, Schneider et al., 2011, Zheng and Pan, 2018, Song and Dinh, 2014
Dynamic attributed networks with community structure generation model (Largeron et al., 2017)	Generates dynamic networks with a community structure by using micro-operations and macro-operations	Wijayanto and Murata (2019)
Kronecker model (Leskovec et al., 2010a)	Generates real life networks with static (power law of degree and eigenvalue distribution, diameter) and temporal (densification of power law, shrinking diameter) properties	Khalil et al. (2014)
In Litou et al. (2017), it is considered that when a user accepts an opinion, they may change it after receiving other opinions. A credibility score and a renouncement threshold are considered for each node 
 in Litou et al. (2017); the former represents the trustworthiness of 
 and the latter expresses the how easy (or difficult) it is for 
 to renounce an opinion they had. An extension of the LT diffusion model is proposed to simulate propagation with these features. The nodes of the truth campaign are determined using a simulating annealing algorithm. Users’ background knowledge, a hesitating mechanism and a forgetting–remembering factor are taken into account in Hosni et al., 2020, Hosni et al., 2018a to model how a user is influenced by misinformation. A ‘Human Individual and Social Behaviors’ diffusion model is then proposed to model acceptance and spread of misinformation by users. The truth campaign is identified using a greedy algorithm in  iterations, with a node with maximum marginal containment influence added to the truth campaign in each iteration.

In Chen et al. (2019), an activity profit is assigned to each edge. The goal is to identify a set of nodes for the truth campaign so that high profit edges become more protected and less likely to be used to spread misinformation. The authors prove that the problem is not submodular nor monotone, and an approximation algorithm is then proposed. In Tong et al. (2018b), it is supposed that a misinformation campaign and several truth campaigns happen at the same time. The goal is to identify a truth campaign to minimize the spread of misinformation. A multi-cascade diffusion model is proposed to simulate the propagation process. In this model, each user has a priority for each of the cascades and how a message from each cascade may be perceived. This priority is determined based on the reputation of the source, personal opinion and reliability of message. A greedy algorithm is then proposed to determine upper and lower approximations and obtain a solution.

The location of users is taken into account in Zhu et al., 2018, Zhu et al., 2019. In Zhu et al. (2018), the goal is to minimize the number of users who are located in a region  and are activated (influenced) by a misinformation campaign. A quadtree is constructed based on the location of nodes; traversing this tree determines the nodes in . Dynamic programming is then proposed to determine the influence of different nodes on the nodes in  using the MIA method (Chen et al., 2010). Most influential nodes are greedily identified to contain the spread of misinformation in . In order to increase the efficiency of the proposed method, pruning nodes with small influence is suggested. In Zhu et al. (2019), this problem is more constrained as nodes for the truth campaign are selected from the nodes of a specific region; the solution comes through the extension of methods in Zhu et al. (2018). User mobility is taken into account in Wu et al. (2018) and the SIR diffusion model is extended to simulate rumour propagation in vehicular social networks. In order to contain the spread of misinformation, a set of vehicular nodes are then chosen to spread the truth among other nodes.

In comparison to structural methods, behaviour-aware methods can more effectively minimize the spread of misinformation as they consider user behaviour and preferences. However, such information is not always available in real-world applications, which means that structural methods may be more widely applicable.

5.2. Protection-oriented methods
In protection-oriented methods, the problem is the identification of a set  of minimum size so that a given percentage of users or part of the network are protected from misinformation; these users are not affected by the misinformation campaign.

The problem is modelled as 
-node protector in Nguyen et al., 2012, Nguyen et al., 2013. In this model, it is assumed that the spread of misinformation is triggered by a set  (source of misinformation), and can be propagated at most  hops from a source. The goal is to identify a set  of minimum size to protect a fraction of nodes, , . The set  can be known or unknown;  can be unconstrained () or constrained by an integer value. Therefore, the problem has four variations. When  is unknown (source-ignorant), the problem changes to influence maximization and it is about the selection of a set  that influences a fraction of nodes . An iterative greedy algorithm is proposed to solve the problem with both unconstrained and constrained ; a node with maximum marginal influence is iteratively added to  for as long as the influence of  is less than . Both the LT and the IC diffusion models can be used by this algorithm to calculate the influence of . When  is known (source-aware), with both unconstrained and constrained , if the number of reachable nodes by  is greater than , an iterative greedy algorithm is applied to protect some of these nodes and achieve the required  protection. Influential nodes are selected and added to  until the set protects . Due to the time complexity of determining the influence of nodes in each iteration, a community-based algorithm is also proposed to protect a fraction of nodes  in each community.

The community structure properties of a network are considered in Fan et al. (2013). It is supposed that the spread of misinformation is triggered by some users in community 
, i.e., 
. The goal is to contain misinformation within the community and prevent its propagation to other communities. For this purpose, so-called bridge nodes, which are nodes located out of 
 and have at least one neighbour in 
, are first determined. A smallest set of influential nodes

is identified to protect a fraction  of the bridge nodes. This problem is solved using greedy algorithms under two different scenarios: (i) opportunistic One-Activate-One where each active node attempts to influence one of its neighbours in the spreading process; (ii) deterministic One-Activate-Many where each active node attempts to influence all of its neighbours. In Hosni et al. (2018b), misinformation spread minimization in multiplex networks is considered; a multiplex network is composed by several social networks which are connected through overlapping users. Overlapping users can spread true information in several social networks; the goal is to identify the smallest set of nodes that have influence on the overlapping nodes as a way to reduce the influence of misinformation. To solve the problem, a greedy algorithm is proposed that iteratively selects a node with maximum marginal influence using the IC diffusion model.


Table 5. Graphs based on real-world datasets.

Dataset	Description	Used by
Wikipedia vote network (Leskovec et al., 2010b)	Voting data from the Wikipedia community where directed edges indicate users who voted for other users	Yang et al., 2019, Lv et al., 2019, Tong et al., 2020, Tong and Du, 2019, Li et al., 2013, Chen et al., 2019, Zheng and Pan, 2018, Pham et al., 2017, Yang et al., 2018, Yan et al., 2019, Wang et al., 2018
High energy physics collaboration network (Leskovec et al., 2007a)	Scientific collaboration between authors of papers from arXiv	Holme et al., 2002, Tanınmış et al., 2020, Shi et al., 2019, He et al., 2012, Wu and Pan, 2017, Li et al., 2013, Tong et al., 2018a, Fang et al., 2020, Litou et al., 2017, Chen et al., 2019, Nguyen et al., 2012, Fan et al., 2013, Song and Dinh, 2014
High-energy physics theory citation network (Leskovec et al., 2005)	Citation network from arXiv where a directed edge indicates a paper that cited another paper	Pham et al., 2018a, Pham et al., 2019, Yang et al., 2018, Shi et al., 2019, Yan et al., 2019, Khalil et al., 2014, He et al., 2012, Wu and Pan, 2017, Tong and Du, 2019, Tong et al., 2018b
Gnutella peer-to-peer network (Leskovec et al., 2007a)	Snapshots of the Gnutella peer-to-peer network where directed edges represent connections between hosts	Pham et al., 2018b, Pham et al., 2019, Zheng and Pan, 2018, Pham et al., 2017, Yang et al., 2018, Zhang et al., 2015, Chen et al., 2019
Epinion social network (Richardson et al., 2003)	Consumer reviews from Epinions.com where directed edges indicate a trust relationship	Lv et al., 2019, Tong et al., 2020, Tong and Du, 2019, Khalil et al., 2014, Wang et al., 2018, Zheng and Pan, 2018
Slashdot social network (Leskovec et al., 2009b)	Slashdot user community where edges indicate directed friend/foe links between users	Yan et al., 2019, Li et al., 2013, Hosni et al., 2020
Youtube social network (Yang and Leskovec, 2015)	Friendships between Youtube users	Tong et al., 2020, Tong and Du, 2019, Hosni et al., 2020, Hosni et al., 2018b
Twitter (Leskovec and Mcauley, 2012, De Domenico et al., 2013, Magnani and Rossi, 2011)	Relationships between Twitter users	Dey and Roy, 2017, Song et al., 2015, Wang et al., 2017, Wang et al., 2018, Song et al., 2017, Hosni et al., 2020, Tong et al., 2018b, Hosni et al., 2018b
Meme tracker (Leskovec et al., 2009a)	Networks of hyperlinks between news sites where directed edges point to the source	Khalil et al., 2013, Khalil et al., 2014, Song et al., 2015
Oregon autonomous systems (Leskovec et al., 2005)	A communication network where nodes and edges may be added or deleted over time	Pham et al., 2018b, Pham et al., 2018a, Pham et al., 2019, Tong et al., 2012, Song et al., 2015
Facebook (Leskovec and Mcauley, 2012, Magnani and Rossi, 2011)	Friendships between Facebook users	Yao et al., 2015, Wu et al., 2017, Wang et al., 2017, Yao et al., 2014, Kuhlman et al., 2013, Zhang et al., 2015, Arazkhani et al., 2019, Tong et al., 2018a, Hosni et al., 2020, Zhu et al., 2018, Zhu et al., 2019, Nguyen et al., 2012, Hosni et al., 2018b
6. Evaluation strategies and datasets
All methods proposed to solve the problem need to be evaluated regarding their performance. In principle, evaluation aims to assess the impact of a method on minimizing the number of nodes of a graph that will be influenced by misinformation. Different graphs are used, which include both synthetic graphs and graphs based on datasets from real-world networks.

In order to evaluate node or edge blocking methods, some nodes are randomly determined as malicious nodes and their spreading ability (that is, how many nodes are influenced by misinformation) is determined. Then, a blocking method is used to block a set of nodes (or, respectively, a set of edges) and the spreading ability of the malicious nodes is re-assessed. The decrease in spreading misinformation indicates the impact of the method.

In clarification-based methods, once again, some nodes are randomly determined as malicious nodes and their spreading ability is determined. Then, a method is used to identify a set of nodes, which will initiate a truth campaign. Both malicious nodes and truth campaign nodes will spread their messages. The number of nodes influenced by misinformation is calculated.

In addition to assessing the impact on minimizing the spread of misinformation, every method is usually assessed with respect to its running time. Typically, a method is ran multiple times and the average running time is reported.

Regarding datasets, random models are used to generate synthetic graphs based on a desired number of nodes, degree distribution, clustering coefficient, etc. A short description of the commonly used models along with the papers that use each of them is given in Table 4.

Table 5 lists some widely used real-world datasets along with a short description and references to papers that use them. Repositories of real-world datasets include: http://snap.stanford.edu, http://konect.cc and http://networkrepository.com.

7. Conclusion and future directions
In this paper, proposed methods for misinformation spread minimization were classified into two categories. In blocking-based methods the idea is to change the network structure; some nodes and/or edges are removed from the network to minimize the spread of misinformation. Blocking nodes and/or edges comes with a cost and may lead to a discredit of the network if it takes a long time to restore connectivity of nodes and edges. In clarification-based methods, the goal is to increase the awareness of users by spreading truth information. This approach does not have the challenges and costs of blocking, but it may be less efficient than a blocking approach.

There are various issues that may drive further research on this topic.

First, the methods proposed have been developed predominantly in the last decade and still need extensive evaluation using different types of networks, diffusion models, and so on. However, as already observed, time complexity is often a key limitation, which may become even more of an issue as there is a need to deal with increasingly larger networks and sophisticated models. This suggests that increased attention on efficiency and scalability of the proposed methods will be necessary to be successful when minimizing the spread of misinformation. In practical terms, it will not be viable for a method to take longer to find a solution that minimizes the spread of misinformation than the time needed for misinformation to propagate, as there is a risk that by the time a solution has been found the situation may have changed completely.

Second, all methods assume that the required data is readily available and correct. However, data availability cannot be always taken for granted. For example, access to complete data about a network and its features or data related to users’ behaviour may not be always feasible. Leaving aside issues related to privacy of some of this data, it appears that some methods may have to make decisions under some sort of uncertainty.

Third, most of the proposed methods are designed and evaluated through diffusion models. These models are approximations primarily based on structural information that may fail to generate an accurate diffusion pattern. This means that the performance of different methods may significantly change due to small changes in the diffusion pattern. As reported in Pei et al. (2015) the actual propagation pattern of information is likely affected by factors such as human behaviour, common preferences or beliefs and social reinforcement. Diffusion models that take these factors into account can more realistically model the spread of misinformation and help with the evaluation of methods to spread misinformation.

Fourth, as highlighted by this review, the bulk of the methods for minimizing the spread of misinformation are source-aware. When misinformation is detected, one has to pause and detect the sources of misinformation first. Clearly, the success of these methods depends on good methods to detect the sources of misinformation. Proposing methods that can minimize the spread of misinformation even if there is some uncertainty in source detection may also help.

Finally, a key element of the problem is the network structure. Existing research essentially assumes that the network structure is fixed and does not change over time. This may help find solutions but one may easily realize that more dynamic elements in the network structure may make it more realistic to capture the actual interactions in real-world social networks. In fact, multiplex networks (composed of multiple social networks with overlapping users) have already been discussed in some related research in the previous sections. One may think that temporal, dynamic (where the network structure changes over time) as well as multiplex networks may be viewed as a more relevant abstraction of social interactions than fixed networks. Algorithms to minimize the spread of misinformation will need to be developed for such networks.

