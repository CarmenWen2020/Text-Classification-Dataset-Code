Cluster randomized trials are frequently used in educational research for methodological reasons. This study aims to improve the efficiency of cluster randomized trials on computer/information literacy and computational thinking. The study employs a two-level hierarchical linear model to estimate (i) intraclass correlation coefficients, (ii) the amount of explained variances given selected predictors, and (iii) minimum detectable effect sizes given the set of plausible scenarios. Two data cycles from the International Computer and Information Study were used. The covariates at the student level are gender, interest in ICT, parents’ highest education level, ICT self-efficacy, and experience with computers. The covariates at school/teacher level are teacher’s ICT use, ratio of school size to the number of computers for student use, availability of ICT resources at school, approximate teacher age, and ICT self-efficacy. Findings showed that the most precise effect could be measured when student and teacher/school covariates are both adopted. Lastly, it was revealed that increasing the number of schools is effective to get the most precise effect.

Introduction
With the advancement in technology during the past few decades, the availability of Information and Communication Technologies (ICT) to human use has flourished. This change influenced educational practices in a way that ICT use has become part of teaching and learning practices, and so, often schooling systems have made efforts to adjust to the current situation by having students and teachers develop relevant skills.

Given the current conditions started by COVID-19, many educational institutions worldwide have converted their mode of teaching from face-to-face to online learning. This necessitated students and teachers to go online for educational needs. This change forced teachers and students to develop skills to get better ICT-related literacies. Therefore, the importance of the investigations with regard to ICT-related literacies in education is even more critical today.

The incorporation of ICT into education took place for a long time, while the conceptual understanding of ICT-use has kept evolving as ICT itself has been changing rapidly along the way. Given the current conditions started by COVID-19, many institutions worldwide have converted their mode of teaching from face-to-face to online and technology integration has gained more importance. Some of the research areas related to ICT in terms of embedding technology into educational practices are media literacy (Livingstone & Van der Graaf, 2008), digital literacy Bawden, 2008), computer literacy (Hoffman & Blake, 2003), information literacy (American Library Association, 2000), twenty-first-century skills (Dede, 2010), digital competence (Ferrari, 2012), and computational thinking (Wing, 2006). The skillset within each conceptualization, with some overlaps, differs such that being able to use computers/devices effectively (computer literacy), being able to locate, access, and process information (information literacy; American Library Association, 2000), to have the skills such as information management, critical thinking, creativity, collaboration, problem-solving (twenty-first-century skills; van Laar et al., 2017), to use computers to create solutions to real-life issues by decomposition, abstraction, algorithm design, debugging, iteration and generalization (computational thinking; Shute et al., 2017). The motivating idea behind these research endeavors is that we live in a society with an economic and social structure that necessitates individuals to develop relevant skills to effectively and successfully participate. As a result, enhancing education in terms of technology-related skills becomes essential in today’s technology-driven world.

There have been increasing efforts to measure and improve students’ ICT-related skills over the years in this context. One of the most significant efforts is an international assessment started in 2013 by the International Association for Evaluation of Educational Achievement (IEA), which is called the International Computer and Information Literacy Study (ICILS).

ICILS studies have provided evidence and implications for the improvement of efforts in learners’ ICT-related competencies. In this sense, these studies also have a potential to improve the design of experimental studies in ICT-related fields. From a methodological perspective, Cluster Randomized Trials (CRTs), a type of experimental research design, are considered the golden standard for educational research (Torgerson & Torgerson, 2001). In this study, we aim to improve the design of CRTs that are to be conducted on CIL and/or CT constructs to determine the design parameters such as intraclass correlation coefficients, amount of variance explained, and minimum detectable effect sizes. We hypothesize that countries would differ in terms of between-school and within-school variability, and thus, the CRTs to be designed within a specific country would depend on its contextual factors such as student demographics, teacher characteristics, and school characteristics (Fig. 1).

Fig. 1
figure 1
The list of countries that participated in an ICILS study at least once

Full size image
Theoretical framework
Computer and information literacy
Computer and information literacy (CIL) is partly derived from the prior research that attempted to characterize ICT-related concepts in educational research such as digital literacy, ICT literacy, and information literacy, etc. According to Ng (2012), digital literacy was characterized as a construct referring to the use of online and offline digital tools for learning. Lau and Yuen (2014) described ICT literacy as a multidimensional construct that encompasses information literacy, internet literacy, and computer literacy. In the ICILS framework, CIL was proposed as a multidimensional construct, including both computer and information literacy, which was defined as “an individual’s ability to use computers to investigate, create, and communicate in order to participate effectively at home, at school, in the workplace, and in society” (Fraillon et al., 2019, p. 16). The major difference between ICT literacy and CIL is the emphasis on computers in the definition of CIL. Unlike ICT literacy, encompassing the broader context of all digital tools, the focus of CIL is personal computers, including traditional computers and other devices with external keyboard and mouse (Fraillon et al., 2019).

In this perspective, ICILS studies have been one of the most significant steps forward in assessing CIL skills globally. The ICILS studies had two cycles so far, with 21 and 14 participant countries in 2013 and 2018, respectively. The purpose of the ICILS study was to prepare a framework that would help to discover how young students (8th graders) may improve their CIL-related skills (Fraillon, Schulz, Friedman, Ainley, & Gebhardt, 2015). The ICILS studies attracted researchers’ interest on the topic, and European Educational Research Journal devoted a special issue to ICILS 2013 due to the fact that it is the first of its kind being an assessment of CIL acquisition (Fraillon et al., 2014). In the ICILS studies, CIL was initially defined as a multidimensional construct with two dimensions having the following specified sub-dimensions:

1.
“Collecting and managing information: Knowing about and understanding computer use, accessing and evaluating information, managing information.”

2.
“Producing and exchanging information: Transforming information, creating information, sharing information, using information safely and securely” (Fraillon & Ainley, 2010).

Although there are two dimensions of CIL, studies have shown high correlations between the two constructs (Fraillon et al., 2014; Bos et al., 2014, as cited in Gerick et al., 2017); hence, CIL was used as a unidimensional construct in ICILS studies (Gerick et al., 2017).

Correlates of CIL
A significant number of studies have been conducted to investigate the factors influential on learners’ CIL and ICT-related traits. The relevant literature presented in the following sections showed that student, teacher, and school characteristics are linked to the concepts like CIL and ICT literacy.

Self-efficacy. In terms of learner characteristics, it was first shown that ICT self-efficacy is a predictor of CIL (Hatlevik et al., 2018; Rohatgi et al., 2016). Rohatgi et al. (2016) pointed out that the self-efficacy of learners in basic ICT skills is positively related to CIL while self-efficacy in advanced ICT skills is negatively related. Self-efficacy also functions as a mediator between learners’ ICT use and CIL (Hatlevik et al., 2018; Rohatgi et al., 2016). Lastly, Gebhardt et al. (2019) demonstrated that learners’ interest in and enjoyment of ICT positively linked with their CIL.

Gender. Prior research investigating gender differences in CIL had contradictory findings, as some studies found that female learners had higher CIL scores than males (Aydin, 2021; Gebhardt et al., 2019; Hatlevik et al., 2018; Punter et al., 2017). In contrast, Rohatgi et al. (2016) found out no gender differences in relation to ICT use, self-efficacy, and CIL. A study by Alkan and Meinck (2016) pointed out that the moderating effect of gender in the link between ICT use and CIL was weak.

Experience with ICT. Learner experience of ICT use was also demonstrated as an influential factor in CIL achievement. Studies reported a positive relationship between ICT use and CIL scores (Alkan & Meinck, 2016; Rohatgi et al., 2016) as well as the computer experience and the CIL achievement (Aydin, 2021; Hatlevik et al., 2018). Alkan and Meinck (2016) further showed that students frequently using ICT for social communication demonstrated better performance in CIL.

School ICT resources. The second consideration in the evaluation of learners’ CIL is the availability of ICT resources at schools. Gerick (2018) discovered that school characteristics in some countries have significant influences on learners’ CIL achievement. Similarly, Drossel, Eickelman and Gerick et al. (2017) showed that the availability of ICT resources at schools significantly influences learners’ usage. Additionally, Drossel et al. (2020) argued that students of the resilient schools have CIL scores above the average despite their low socio-economic status. Hatlevik et al. (2018) indicated that learner use and experience of ICT in schools are the significant predictors of their CIL. They, however, noted that the quality of ICT usage in classrooms deserves more attention than the frequency of usage.

Teacher characteristics. Teacher characteristics are also linked to the use of ICT use frequency for teaching, which might eventually influence student CIL scores. Teachers’ ICT self-efficacy (Drossel et al., 2017b; Siddiq et al., 2016), attitude and beliefs (Drossel et al., 2017b; Eickelmann & Vennemann, 2017), attendance in professional development (Drossel & Eickelmann, 2017), and availability of pedagogical (Gerick et al., 2017) and technical support (Drossel et al., 2017a) are denoted as the influential factors on teachers’ ICT use for teaching. Additionally, Eickelmann and Vennemann (2017) revealed that relatively younger teachers, mostly classified as ICT enthusiasts, reported a higher frequency of ICT usage in classes. Previous research showed that teachers’ age (Drossel et al., 2017a; Drossel et al., 2017b; Eickelmann & Vennemann, 2017; Siddiq et al., 2016), and gender (Drossel et al., 2017a; Drossel et al., 2017b) are influential on their ICT usage for teaching while Gebhardt et al. (2019) found out no significant influence of gender.

Computational thinking
Computational Thinking (CT) has been commonly adopted as a set of problem-solving skills, and it has taken much attention since Wing’s (2006) conceptualization. CT corresponds to solving real-life problems based on the fundamental concepts of computer science (Wing, 2006, 2008). However, it is not about just programming (Voogt et al., 2015); it is more about conceptualization or abstraction to solve problems, which is a combination of mathematical, engineering, and scientific thinking skills (Wing, 2006). Therefore, it is viewed as a twenty-first-century skill that students could acquire with or without the use of computers (Shute et al., 2017). CT is currently considered as a fundamental skill for all age groups and required in all disciplines (Kalelioglu et al., 2016). It has been, for instance, included in the competency standards for students and teachers by the International Society for Technology in Education (ISTE, 2016).

CT was included in ICILS 2018 assessment, for the first time, as a problem-solving competency. Although the consensus among CT’s definitions is limited to some extent (Shute et al., 2017; Voogt et al., 2015), it is defined in the ICILS framework as the “ability to recognize aspects of real-world problems which are appropriate for computational formulation and to evaluate and develop algorithmic solutions to those problems so that the solutions could be operationalized with a computer.” (Fraillon et al., 2019, p. 27). This definition likewise underscores conceptualizing, abstraction, designing systems (algorithmic solutions), and implementation via computers. Within ICILS 2018 framework, CT covered two strands and five aspects (Fraillon et al., 2019). These strands are (1) “Conceptualizing problems” and (2) “Operationalizing solutions” (Fraillon et al., 2019, p. 28). The first strand covers three aspects: “knowing about and understanding digital systems”, “formulating and analyzing problems”, and “collecting and representing relevant data” while the second strand covers “planning and evaluating solutions” and “developing algorithms, programs, and interfaces” (Fraillon et al., 2019, p. 28).

Correlates of CT
Learner demographics. Learner demographics related to CT skills were addressed in several studies (Tikva & Tambouris, 2021). In a recent study, Guggemos (2021) showed that learner characteristics such as CT self-concept, reasoning skills, gender, computer literacy and computer usage intensity, and learning opportunities at school are the predictors of CT skills. In a similar study, Durak and Saritepeci (2018) found out that thinking styles, ICT experience, internet usage intensity, and gender predict students’ CT skills. In the CT model proposed by Tikva and Tambouris (2021) learner demographics and the cognitive and non-cognitive factors were associated with CT knowledge and skills.

Interventions used to Improve Computational Thinking. A lot of interventions were designed and implemented to improve learners’ CT skills. Those interventions differ from one another in terms of their purpose and design. For instance, Hsu et al. (2018) identified project-based learning, problem-based learning, cooperative learning, and game-based learning as the most commonly used instructional methods, Tikva and Tambouris (2021) listed modeling/simulation-related and scaffolding strategies, Tang et al. (2020) indicated game-based learning and collaboration as the main pedagogical approaches. Other review studies specified reinforcement, reflection, information processing, scaffolding (Lye & Koh, 2014), storytelling (Ilic et al., 2018; Lye & Koh, 2014; Zhang & Nouri, 2019), and lectures (Ilic et al., 2018; Shute et al., 2017) as the commonly used methods. As for the tools/environments, programming tools (Ching et al., 2018; Hsu et al., 2018; Ilic et al., 2018; Lye & Koh, 2014; Tang et al., 2020; Tikva & Tambouris, 2021), robotics (Ching et al., 2018; Hsu et al., 2018; Ioannou & Makridou, 2018; Noh & Lee, 2020; Tikva & Tambouris, 2021), games or game development tools (Ching et al., 2018; Hsu et al., 2018; Kuo & Hsu, 2020; Tang et al., 2020), animation/virtual reality-related tools (Ching et al., 2018) and instructional modules (Shute et al., 2017) are stated as the frequently used interventions to improve CT skills.

One of the most common research designs in CT research is experimental research with an increasing usage frequency (Ilic et al., 2018). Scholars conducted empirical studies to investigate the effect of these interventions at diverse educational levels (Jun et al., 2017; Noh & Lee, 2020; Pala & Mıhçı Türker, 2021; Sung et al., 2017). The majority of the reviewed experimental studies used quasi-experimental research design, including static groups without random assignment (Garneli & Chorianopoulos, 2018, 2019; Kuo & Hsu, 2020; Tsai et al., 2017; Yadav et al., 2014) while some of them used the one-group experimental design with pre and post-test (Jaipal-Jamani & Angeli, 2017; Noh & Lee, 2020). On the other hand, the studies in which experimental and control groups were formed with random assignment were rarely observed (Cetin, 2016; Jun et al., 2017; Pala & Mıhçı Türker, 2021; Sung et al., 2017). Similarly, Hickmott et al. (2018), in their review of CT studies in math, concluded that none of the studies conducted experiments with random assignment.

Significance and purpose of the study
Many studies have implemented interventions to observe what works better for computer literacy (e.g., Varank, 2006), information literacy (e.g., Chen & Chengalur-Smith, 2015; Gross & Latham, 2013; Walton & Hepworth, 2011), and computational thinking (Ching et al., 2018; Hsu et al., 2018; Lye & Koh, 2014; Shute et al., 2017; Tang et al., 2020; Tikva & Tambouris, 2021). However, our examination of the design of the experiments revealed that the use of cluster randomized trials as an experimental design in this area is scarce. One of the reasons that the CRTs were not being used so often could be due to the fact that there is no available prior research evidence to support the design of such complicated experimental studies as CRTs. In this respect, our study aims to contribute to the relevant literature by improving the knowledge base for efficient and rigorous design of intervention studies, in the form of CRTs, on both CIL and CT. In order to achieve our goal, we used the ICILS datasets provided by IEA from 2013 and 2018 data cycles. We used three sets of predictors: student characteristics, teacher/school characteristics, and student and teacher/school characteristics. The details about the nature of our predictor sets and models are explained in the paper’s methods sections. The research questions that we intend to answer are:

1.
What are the ICCs for CIL across the countries that participated in ICILS 2013 and ICILS 2018?

2.
What are the ICCs for CT across the countries that participated in ICILS 2018?

3.
How much variance could be explained on CIL given the set of student and teacher/school level characteristics across the countries that participated in ICILS 2013 and ICILS 2018?

4.
How much variance could be explained on CT given the set of student and teacher/school level predictors across the countries that participated in ICILS 2018?

5.
What would be the minimum detectable effect sizes given the sets of student and teacher/school level predictors, and selected constraints on CIL across the countries that participated in ICILS 2013 and ICILS 2018?

6.
What would be the minimum detectable effect sizes given the sets of student and teacher/school level predictors, and selected constraints on CT across the countries that participated in ICILS 2018?

Methodology
Sample and variables
The data selected in this study were from the 2013 and 2018 cycles of the International Computer and Information Literacy Study (ICILS). The 2013 sample was approximately 60,000 students from 21 participating countries, whereas the 2018 sample was around 46,000 students from 14 participating countries. Eligibility criteria for participation in the study required that the students be at least 13.5 years old and at the 8th year of schooling. The study’s sampling design was described as ‘complex’ due to the use of multiple sampling strategies, including stratification of schools and randomization within schools for teachers and students (Drossel et al., 2017a).

The study’s 2013 cycle only provided CIL assessment results, whereas the 2018 cycle of the study assessed both CIL and CT achievements. In this study, two sets of covariates for the two cycles describing student and teacher/school characteristics were used. Student characteristics included gender, self-efficacy for information and communications technology (ICT) use, student’s interest in ICT, computer experience, and socio-economic status. The covariates used at the teacher/school level included the ratio of the school size to the number of computers in the school, ICT resources available in the school, the approximate age of the teacher, the ICT use of the teacher, and the self-efficacy of the teacher with regard to ICT use. The dependent variables (CT and CIL) were continuous latent variables estimated using a 1-parameter item response theory model (Rasch, 1993).

Analysis
A priori power analysis was conducted to estimate the adequate sample size in a study in order to generate accurate results by avoiding type II errors and utilizing resources economically. Computation of power analyses for cluster randomized trials necessitates five quantities; the intraclass correlation coefficients (ρ), the amount of variance explained (R2), the balance of the design (P), the sample sizes for the clusters (J), and the number of units within the clusters (N). Therefore, this study provided country-specific evidence produced by power analysis aiming to improve the design of the CRTs concentrating on the CIL or CT. The analysis section of this paper outlined the following: the overview of the models used, the sets of covariates selected, the calculation of ICCs and its standard errors, the amount of variances explained by the covariates, the minimum detectible effect sizes, and how analytical details (plausible values, weights, software etc.) were handled.

Multilevel approach
The rationale behind employing multilevel models is the clustering of units in some forms of groups such as classrooms, clinics, and families (Raudenbush & Bryk, 2002). In this respect, employing CRTs in educational research would generate data sets that have multilevel structure. Since the sampling design of ICILS, as stated in the technical report (Fraillon et al., 2020), involved a systematic clustering at the school level, a two-level multilevel model was considered as students nested in schools. Therefore, the specification of the multilevel model given the sampling design could be characterized as a null model.

Null model:

Yij=β0j+εij,εij∼N(0,σ2ε)
(1)
β0j=γ00+u0j,u0j∼N(0,σ2β)
(2)
where Yij is the value of the dependent variable for the ith student who attends to the jth school. β0j stands for the intercept of the jth school, and γ00 is the average intercepts across the schools within a specific country. Lastly, εij and u0j stand for student and school level residuals, respectively.

The null model was considered to be a two-level model without any covariates. Initially, a three-level approach was considered to be a better fit due to the nature of the ICILS datasets in which students were nested in teachers, and teachers were nested in schools. However, the teacher data failed to incorporate into the analysis as in a three-level model because the teacher data were not to linked to the student data due to the sampling procedures. Leaving the teacher data unused would not be wise as teachers are the central elements of any educational system. As a result, a central tendency measure (e.g., mean or median) of teacher covariates within a school was used at the second level as if it was a school level covariate. Please see Table 1 for more details about the models and the variables we have used.

Table 1 Variable descriptions by year and model
Full size table
Models used
Three models would be introduced for explaining additional variances as Model 1, Model 2, and Model 3 (full model). Model 1 included student-level covariates, Model 2 included teacher/school level covariates, and Model 3 included both student and teacher/school level covariates; please see Table 1 for more details about the models and the variables that were used. In these models, intercepts were freely estimated while the slopes were fixed across schools. Equations 3 and 4 illustrated the three models used:

Models with covariates at different/both level(s):

Yij=β0j+∑Kk=1βkjXkij+εij,
(3)
β0j=γ00+∑Kk=1γ0kX¯¯¯¯kj+u0j,
(4)
In Eqs. 3 and 4, i represents students, j represents schools, k represents covariates, Yij represents outcome variables (CIL or CT) for the ith student who attends the jth school, β0j represents the intercept for the jth school, γ00 represents the average intercepts across the schools within a country, βkj is the slope for the kth predictor for school j.

Intraclass correlations
The null model was used to see if there were substantial similarities among the students who attend the same school in terms of their CIL and CT scores. To do that, intraclass correlation coefficients (ICC) were estimated for each country. ICC represents the proportion of the between-school variance to the total variance, which was calculated as ρ=σ2β/(σ2β+σ2ε) where ρ is the ICC, σ2β is the between-school variance and σ2ε is the within-school variance. The denominator, (σ2β+σ2ε), in this equation was the total variance. Further, the uncertainty associated with the ICCs were calculated using Donner and Koval’s (1982) method as displayed in Kelcey and Phelps (2013) as shown below:

SE(ρ)=2(1−ρ)2[1+(N−1)ρ]2/N(N−1)J −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−√
(5)
where ρ is the ICC, J is the number of schools, and N is the number of students within a school.

Additional variances explained
In multilevel models, the practical utility of a model is often evaluated in a similar way to the linear regression but at each level separately. The following equations showed how the proportion of variance explained by the inclusion of the predictors was calculated:

R2ε=(σ2ε−σ2ε∼)/σ2ε
(6)
R2β=(σ2β−σ2β∼)/σ2β
(7)
The terms σ2ε∼ and σ2β∼ represented the residual terms for the conditional models at level 1 and level 2, respectively.

The amount of additional variance explained was calculated as the ratio of the reduction of unexplained variance to the total unexplained variance in a given level. Hence, R2ε and R2β were the proportion of variance explained at level 1 and level 2 by inclusion of the covariates.

Minimum detectable effect sizes
Minimum detectable effect size (MDES; Bloom, 1995) is a design coefficient representing the smallest effect that an intervention can detect. Therefore, smaller values of MDES indicate smaller effects that can be detected by an intervention, which would suggest a better accuracy of the results. In this study, MDES was calculated for a balanced design (P = 0.50) with a power value of 0.80 for a two-tailed test. Given these assumptions, the MDES were calculated using Hedges and Hedberg’s (2007) method.

MDES=M(J−2−K)P(1−P)−−−−−−−−√ρ(1−R2β)J+(1−ρ)(1−R2ε)NJ−−−−−−−−−−−−−−−−−−−−−−−−√
(8)
In this equation, ρ is the ICC, R2 is the proportion of variance explained by the covariates, N is the number of students who attend to a school, J is the number of schools (clusters) in a country, P (0.50) stands for representing a balanced design, and M is a design multiplier (M≈ta/2 + tβ for J − 2 − K degrees of freedom where a and β are type I and type II error rates, respectively). MDES for the specific instances of N (20, 40, 60) and J (10, 20, 30, 40) were calculated in order to estimate the efficiency of CRTs.

Plausible values, software, sampling weights, and syntax
The two variables, CIL and CT, had five plausible value estimates, which were randomly drawn with regard to the students’ ability from their posterior distribution. The models mentioned above (Null model, Model 1, Model 2 and Model 3) were first estimated for each plausible value, then the variance components (σ2β,σ2ε) were calculated within each model. Then variance components were averaged across five plausible values. The MDES were calculated given the ICCs, R2s, and previously mentioned constraints (N, J, P, power). R (R Core Team, 2020) was used for modeling the multilevel structure using ‘WeMix package (Bailey et al., 2020), which allowed to use sampling weights at each level separately. Lastly, MDESs were calculated using ‘PowerUpR’ package (Bulus et al., 2019).

Results
Descriptive statistics such as sample sizes, number of schools, average teacher age, average CIL scores, percentage of males, and the average of the selected independent variables within each country for the 2013 cycle were displayed in Table 2. It shows the average teacher ages were similar across countries due to the lack of variation as the approximate age of teachers was coded during data collection. CZE and DNK had the highest CIL averages; on the contrary, THA and TUR had the lowest CIL averages. TUR and THA had the lowest CIL scores with the highest standard deviations (greatest variability on CIL). CZE was the highest performing country with an average CIL score of 560. Student gender proportions across the countries were all very close to 50%.

Table 2 Descriptive statistics for 2013 ICILS
Full size table
Table 3 displays the intraclass correlations (ICC), standard errors of ICCs, and the R2 values. A small ICC indicated a smaller variability across schools within a country.

Table 3 ICC estimates and proportion of additional variances explained (ICILS 2013)
Full size table
Examination of the table shows that DEU and NLD had the highest ICCs whereas DNK, NOR, and CNL had the lowest ICCs. The amount of variance explained by Model 2 at level 1 was nearly negligible across the countries. Model 1 and Model 3 were similar to each other in explaining the variances at level 1. However, all three models had unique (country-dependent) contributions in explaining the variances at level 2. In addition, all three models explained more variance at level 2 than they did at level 1 in the majority of the countries.

Table 4 shows the descriptive statistics of the 2018 cycle of the ICILS study, including computer and information literacy (CIL) and computational thinking (CT) skills. Five countries (CHL, ITA, KAZ, RMO, and URY) did not collect data on CT, so that their CT scores were not available. KAZ and URY had the lowest CIL scores among the 14 countries. LUX and PRT had the lowest level of CT among the 9 countries that had data on CT.

Table 4 Descriptive statistics for CIL and CT in ICILS 2018
Full size table
Table 5 displays ICCs, and R2s for CIL and CT across the countries that participated in 2018. ICC for CIL was the lowest for KOR (0.14) and highest for DEU (0.58). Model 1 and Model 3 explained similar amounts of variances at level 1 for almost all of the countries. All three models explained large variances at level 2 for CIL at varying degrees. With regard to CIL that Model 1 explained more variance at level 2 than Model 2 did for the majority of the countries, even though Model 1 had only level 1 predictors. Examination of ICCs for CT indicated that CT scores varied across schools as ICCs ranged from 0.11 to 0.49. This means that the variations across schools were country-specific. Model 1 and Model 3 explained more variance at level 1 than Model 2 did. Meantime, all three models explained significant amount of variations at level 2. Additionally, the ICCs for CT scores are not free from the ICCs of the CIL scores.

Table 5 ICCs, R2s for CIL and CT in ICILS 2018
Full size table
Given the results displayed in Tables 2, 3, 4, 5, MDES values were calculated for Model 1-3, and results were displayed in Figs. 2, 3, 4. First of all, countries differed in terms of MDES even if the same model and research design specifications were used. For instance, MDES was 1.7 for NLD and 0.8 for SVN when Model 1 was used, assuming 20 students from each of 10 schools participated in a study. Second, increasing number of schools (J) from 10 to 40 decreased the MDES by about 50% for Model 1 and around 65% for Model 2 and Model 3. However, the increasing number of students (N) within a school while keeping the number of schools (J) fixed can hardly decrease the MDES. The differences were small enough to be ignored. These findings are evenly held for the 2018 dataset as well; please see Fig. 3 for more details. Further, there were four countries that participated in both cycles: CHL, DEU, DNK, and KOR. CHL and KOR had higher MDES in 2013 compared to 2018 estimates; in contrast, DEU and DNK had MDES values that were stable from 2013 to 2018. As a result, it is hard to say whether the MDES estimates are stable overtime or not. Lastly, the MDES estimates for the countries with high ICC (e.g., DEU, HKG) were higher even if large sample sizes are selected compared to the countries with smaller ICCs (e.g., DNK, NOR).

Fig. 2
figure 2
2013 MDES estimates for CIL scores. Note: We used heat maps to make it easier for visual understanding; interested readers are advised to check the actual tables in Appendix 1

Full size image
Fig. 3
figure 3
2018 MDES estimates for CIL scores

Full size image
Fig. 4
figure 4
2018 MDES estimates for CT scores

Full size image
MDES values with regard to CT scores for Model 1, Model 2, and Model 3 were displayed in Fig. 4. It was clear to see that countries differed from one another even if the same constraints were used to calculate MDES. Model 2 had slightly higher MDES values than Model 1 across the countries listed. Comparing Model 3 to the other two models, Model 3 was associated with smaller MDES but only for the conditions when the number of schools (J) were 20 or higher.

Conclusions and recommendations
This study is proposed to improve the design of CRTs in CIL and CT areas by estimating (i) intraclass correlation coefficients, (ii) the amount of variance explained given the covariates, and (iii) the MDES. The objective was accomplished by using the data provided by IEA. The findings showed that countries vary from one another in terms of the value of the ICCs, with differences of 0.44 in 2013 and 0.46 in 2018 for CIL scores. Meanwhile, the ICCs for the CT scores had a difference of 0.38 in 2018. Thus, ICCs were country-specific. Each country has its own degrees of variability among schools in terms of student CIL and CT scores due to its own background and culture. As a result, to ensure the design of cluster-randomized studies in CIL and CT field was neither underpowered (inaccurate) nor overpowered, which means sample resources are wasted, baseline data is necessary and essential. Our findings provided such baseline standards for further researchers to explore in these areas.

The finding of ICCs in the United States is that the ICCs of CIL and CT are 0.23 and 0.20, respectively, which are similar to the means of the ICCs in academic outcomes that include reading, math, and science achievements. It indicates that the studies in CIL and CT fields in the U.S. may adopt a similar standard as their academic outcome studies. However, prior research suggested that the estimated value of ICC is context-specific, and the variability may occur when the participants are in different grades, school districts, or states. By far, there is not sufficient research in the field of CIL and CT in different situations yet. Therefore, we recommend conducting more empirical work that under different situations across the states to obtain a more accurate range of ICCs in CIL and CT.

Another finding is regarding R2. That is, the covariates at level 1 and level 2 explained different amounts the within/between-school variance on CIL and CT students’ outcomes in many countries. However, the between-school variance can mostly be explained by level 1 covariates at level 2, rather than the level 2 covariates only. The student covariates at level 1 primarily explained the variances within the school. For example, the amounts of variance explained by covariates at level 1 (student level) in model 1 in the U.S. were less than 20%; in contrast, as we aggregated the same covariates to level 2, the amounts of explained variance increased to 50%. In addition, the largest amounts of explained between-school variance were in model 3 (with both level 1 and level 2 covariates). As mentioned above, the percent of variance explained by covariates in model 3 in the U.S. were up to 64% in CIL and 61% in CT, which were 10% higher than the amounts that only include aggregated student-level covariates at teacher/school level. However, the influence of covariates on reducing between-school variance depends strongly on the country (Kelcey et al., 2016; Spybrook et al., 2016). For example, the between-school variances explained by the teacher/school level covariates were up to 0.77 in LUX in CIL and CT, while it was 0.37 in DNW in CIL and 0.27 in CT.

This leads to the second key finding related to R2. The explanatory power of covariates varies by country. This is evidenced in the higher percentage of the achievement differences explained by level 2 covariates in LUX (77% in CIL and CT) compared to DNW (37% in CIL and 27% in CT). This indicates that the current covariates used in the analyses could not sufficiently reduce the school variance in DNW. Therefore, we encourage further research to explore other typical covariates locally to better explain the variability across the schools in different countries.

The third conclusion derived from our results is regarding MDES. As we can see from the formulas, the ICCs and MDES are related; that is, the smaller the ICC value, the smaller the MDES. The R2 and MDES are also related; the larger the R2, the smaller the MDES. The MDES calculations in this work were based on the empirical estimates of the ICCs and R2 ‘s as well as a fixed total number of clusters, cluster size, a two-tailed hypothesis test, a significance level set to 0.05, and a power set to 0.80. Findings suggested that just like the ICC and the R2s, the MDES is context-specific as well. For example, as we can see in Figs. 3 and 4, the ICCs of CIL and CT in the U.S. and DNW were 0.36 and 0.2, respectively. Consequently, the MDES were 1.4 and 0.7 in these two different countries. It suggested that larger ICC results in a larger MDES, which causes less accurate conclusions. In addition, the MDES was reduced by adding on the covariates. The more percent of variance explained, the smaller MDES were observed. Therefore, given the different empirical estimates of the ICCs and R2, the MDES is context-specific.

Lastly, the MDES for CIL and CT decreased as the number of schools increased rather than the number of students inflated. As seen in Figs. 3 and 4, for instance, in the U.S., increasing the number of students within a school from 20 to 60 to participate in a CRT is less likely to reduce the MDES to a significant degree. However, increasing the number of schools from 10 to 40 while keeping the sample size in each school fixed makes a huge decrease in MDES (less than %50 for the majority of instances). This conclusion holds even for the countries with very low ICCs, such as SVN, NOR, and DNK. As a result, we recommend that when planning sample size for CRTs, increasing the number of schools would help achieve smaller MDES values. In designing studies that are effective, researchers always struggle when planning the sampling stage. Small samples may lead to type II errors, while too large sample could waste economic resources. With this study, we encourage the use of CRTs when planning interventions for CIL or CT (Appendix Tables 6, 7, 8, 9).

Limitations
First of all, we incorporated teacher covariates into the analysis at the school level by using a central tendency quantity (e.g., mean, median) as if they were school characteristics. The reason for averaging teacher data was that there was no linkage between teacher and student data due to random sampling of teachers from a particular school. The only way to keep the teacher data in the analysis was to average them at the school level since the dataset had school ID for teachers; thus, it would have been a better strategy if we were able to incorporate teacher data at level 2 and school data at level 3.

Second, the data collection procedures were not exactly identical for the two cycles, which forced us to make some sort of subjective decisions when we choose covariates. The reason is that the survey was modified from 2013 to 2018 for some variables. For instance, one of the covariates at the teacher level is ‘Use of ICT for teaching at school’ (T_USETCH) in 2013, while the other one is “Use of ICT for classroom activities” (T_CLASACT) in 2018. There is another example regarding student experience with computers. For the 2013 cycle, we used the “Interest and enjoyment in using ICT” (S_INTRST); in contrast, “Students’ computer experience in years” (S_EXCOMP) was used in 2018. We showed the differences in data collection procedures in Table 1.

Lastly, the number of countries that participated in both cycles is only four (CHL, DNK, DEU, and KOR); therefore, it is hard to make general conclusions when ICC, R2, and MDES of CIL scores from 2013 to 2018 were compared.