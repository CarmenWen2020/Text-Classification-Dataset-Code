In automated essay scoring (AES) systems, similarity techniques are used to compute the score for student answers. Several methods to compute similarity have emerged over the years. However, only a few of them have been widely used in the AES domain. This work shows the findings of a ten-year review on similarity techniques applied in AES systems and discusses the efficiency and limitations of current methods. In the final review, thirty-four (34) articles published between 2010 and 2020 were included. The metrics used to evaluate the performance of the AES systems are also elaborated. The review was conducted using the Kitchenham method, whereby three research questions were formulated and a search strategy was developed. Research papers were chosen based on pre-defined inclusion and quality assessment criteria. This review has identified two types of similarity techniques used in AES systems. In addition, several methods were used to compute the score for student answers in the AES systems. The similarity computation in AES systems is dependent on several factors, hence many studies have combined multiple methods in a single system yielding good results. In addition, the review found that the quadratic weighted kappa (QWK) was most frequently used to evaluate AES systems.

Access provided by University of Auckland Library

Introduction
With the rapid advancement of technology and the development of natural language processing techniques, a variety of automated applications have emerged in many different fields. For instance, AES systems are among the applications which have emerged over the past few decades. According to (Lahitani et al., 2016; Munir et al., 2016; Zupanc & Bosnic, 2020) AES systems facilitate the correction of essay-type questions by relieving the burden of manual correction of instructors and reducing subjectivity during the correction. At the core of most AES systems, content scoring plays an important role in computing the marks for a given answer. There exist two major approaches to content scoring namely similarity-based and instance-based. In similarity-based methods, student answers are compared with model answers and then scores are allocated while instance-based methods learn a model of the features pertaining to an answer at different scores. In instance-based approaches, no reference answers are needed (Steimel & Riordan, 2020).

It is difficult to determine the similarities between objects since words and sentences may take on different meanings according to the context in which they are used. The similarity measure is determined using a mathematical function that assigns real numbers between 0 and 1 to objects that are compared. A zero value indicates that the objects are different whereas value one shows that they are almost equivalent. In the context of AES, the objects can be texts, sentences, paragraphs, or documents depending on the type of essay (Gomaa & Fahmy, 2014; Vlachos, 2017). Essay-type questions can vary from short answers to essays. Short answers have a limited amount of words ranging from one word to about 250 words and are usually close-ended compared to essays that are open-ended and longer than 250 words. In AES systems for short answers, the focus is more on content in contrast to essays where both style and content are important (Burrows et al., 2014; McNamara et al., 2015).

Several systematic literature reviews (SLR) were carried out focusing on the different types of existing AES systems (Blood, 2011; Hussein et al., 2019; Ke & Ng, 2019). However, in this work, the focus of the SLR is on the similarity measures used to compute the score of students’ answers in AES systems. Additionally, the findings of reviews carried out by (Gomaa & Fahmy, 2013; Wang & Dong, 2020) were based on text similarity techniques in general and not in the AES domain. Hence, this paper presents the outcomes of an SLR of research papers published during the last decade in the AES domain focusing on the similarity techniques and methods used to score students’ answers. The SLR was done based on the Kitchenham method (Kitchenham & Charters, 2007) and the findings were outlined using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines (Moher et al., 2009). The result presented in this paper can help researchers to gain an insight into the different similarity techniques and scoring methods used in the AES domain together with their corresponding challenges. Moreover, this article serves as a useful foundation for researchers as they can utilize it in their research to choose the most suitable approach in computing the similarity when designing new AES applications.

The remainder of this paper is organized as follows. The research background is presented in Section 2. There are three phases to the Kitchenham methodology described in Section 3, including planning, conducting, and reporting. In Section 4 the findings of the review are discussed and in Section 5 the conclusion of the paper is outlined.

Research background
In this section, the different approaches to compute similarity including lexical, syntactic, and semantic are discussed. In the earliest approaches, two text snippets were deemed similar if they contained the same words or characters. To compute the similarity between two text snippets, methods like term frequency-inverse document frequency (TF-IDF) and bag of words (BoW) were used. However, the aforementioned methods neither considered words having different meanings nor catered for word order. Hence, to address the limitations of the existing similarity computation methods several other similarity approaches have been proposed (Chandrasekaran & Mago, 2020; Kowsari et al., 2019). Similarity can be computed at the lexical, syntactic, and semantic levels.

Lexical
Lexical similarity can be described as the degree to which two given strings are similar in their character sequence. String-based algorithms namely the character-based and term-based similarity are used to measure lexical similarity (Prasetya et al., 2018). Some examples of character-based similarity algorithms include N-gram, Jaro Winkler, Longest Common Substring, Needleman-Wunsch, and Smith-Waterman among others (Kohila & Arunesh, 2016; Pradhan et al., 2015) and those of the term-based are Cosine similarity, Dice’s coefficient, Matching coefficient, Jaccard similarity, Overlap coefficient, and Block distance among others (Gomaa & Fahmy, 2013; Kohila & Arunesh, 2016).

Syntactic
The syntax is the organization of words in sentences and the study of the construction of sentences, together with the association of their components (Britannica, 2021). Syntactic features represent characteristics of the syntactic patterns present in the data e.g. the number of function words, punctuation and part-of-speech (POS) tag, n-grams (Lee & Bosch, 2017), bag-of-words among others. POS tags are the most commonly used type of syntactic information. POS tagging is the method of labeling each word in a text to reflect its functions in a grammatical context, based on its definition along with its surrounding context (Vuk & Dragan, 2015). In addition to POS tagging, parse trees can also be used to encode the complexity of the syntactic structure of a sentence (Filighera et al., 2020; Ke & Ng, 2019).

Semantic
Semantic similarity can be determined at different stages, ranging from a word, sentence to paragraphs and documents (Mugasib & Baraka, 2015). To compute semantic similarity either knowledge base or corpus, or both (hybrid) techniques are used.

Knowledge base
Knowledge base similarity approaches are established on structured resources such as encyclopedias, thesaurus, or dictionaries. Knowledge bases are very helpful as they comprise a highly structured and relevant source of information of words and their meanings. As the world's largest lexical database of English, WordNet is one of the most widely used knowledge-based resources for computing the shortest path of similarity between texts (Gonzalez-Agirre, 2017; Mugasib & Baraka, 2015; Rababah & Al-Taani, 2017).

Corpus-based
Corpus-based or statistical similarity is a measure of semantic similarity that estimates the similarity between two words based on information derived from large corpora. A corpus is a language resource consisting of a large collection of written or spoken texts (Gomaa & Fahmy, 2013). Corpus-based measures are important for measuring words, or concepts when there is no formal expression of knowledge available to get the comparison (Harispe et al., 2015). Based on the distributional hypothesis, words with similar meanings will typically be found in similar contexts (Vuk & Dragan, 2015). Latent Semantic Analysis (LSA) and extracting Distributional Similar words using Co-occurrences (DISCO) are among the popular techniques of corpus-based similarity. Embeddings also measure semantic similarity from a corpus of text. Embeddings are discussed in more detail in subsection 2.4.

Hybrid
By using a hybrid approach, which combines knowledge-based and corpus-based approaches, researchers are able to leverage both semantic methods for comparing the units of language and entities defined by ontologies (Harispe et al., 2015). Examples of such an approach include the use of WordNet and a corpus (Sakhapara et al., 2019; Wedisa et al., 2020).

Embeddings
An embedding is a vector that is represented in a low-dimensional space as a high-dimensional vector. As a general rule, an embedding attempts to capture some of the semantics of an input by placing inputs that are semantically similar close together within the embedding space. It is easier to perform machine learning using embeddings on large inputs such as sparse vectors representing words. Embeddings can be learned and reused across different models (Embeddings, 2020).

Word embeddings
The word embedding process encodes both semantic and syntactic information derived from unlabeled large corpora as a real-valued vector. The semantic information refers to the meaning of words whereas the syntactic one corresponds to their structural roles (Li & Yan, 2017; Wang et al., 2019). Some examples of word embedding algorithms are Word2vec, Global Vectors for Word Representation (GloVe), fastText, and Embeddings for Language Models (ELMo).

Sentence embeddings
The purpose of sentence embedding is to encode a variable-length input sentence into a fixed-length vector. To be more specific, sentence embedding techniques represent entire sentences and the associated semantic information as vectors (Zhu et al., 2018). There exist a variety of sentence embedding techniques for obtaining vector representations of sentences that can be used in NLP applications. The current state of the art includes SkipThought, BERT, InferSent, and Universal Sentence Encoder (USE) among others.

Many techniques were applied in AES systems as extensive research was carried out for the past decades. There is a need to classify the work done to get a better understanding of which similarity measures have been used in the AES systems so far and this leads to explaining the methodology.

Methodology
This review was conducted in accordance with the guidelines on undertaking systematic literature reviews provided by (Kitchenham & Charters, 2007), the three phases of which are planning, conducting, and reporting. The results have been conveyed according to PRISMA. guidelines (Moher et al., 2009). In the following subsections, each of these stages, as applied to our work, is discussed.

Planning the Review
The field of AES systems has utilized similarity techniques extensively, and this paper presents a review of the work in this field. The emphasis was on the similarity techniques and the scoring methods that have been used in the various existing AES systems, and the results obtained.

Research questions
This review examines research papers for specific insights and is steered by the following research questions (RQs):

RQ1. What techniques are used to compute similarity in AES systems?

Several techniques have been employed to compute similarity in a wide range of NLP applications. However, in the AES systems, particular techniques may be more appropriate.

RQ2. What methods are used by the similarity techniques to generate the scores in the AES systems?

Several methods can be used to generate a score by determining the similarity between two objects. However, in the AES domain, only a few of these methods can be applied.

RQ3. What metrics have been used by the AES systems to evaluate their effectiveness?

It is possible to assess the performance of automated systems using a variety of metrics. In the AES domain, only some of them are applicable.

Search strategy
The search strategy was designed to identify relevant works only within the field of AES. Therefore, the strategy consisted first of carrying out a manual search on Google Scholar using the keywords “similarity techniques AND (AES OR Automated Essay Scoring) MINUS Speech AND date filter”. As the most comprehensive academic search engine (Gusenbauer, 2019), Google Scholar was chosen since it indexes papers from a wide range of scholarly publishers and professional societies including Science Direct, IEEE, ACM, Taylor & Francis, Wiley, among others. It should be noted that the keyword search strategy may be too broad, as it indexes the keywords in the documents without being specific, and may produce results that may not be relevant. Accordingly, the search was followed by a manual screening of reference lists from relevant primary studies in keeping with the guidelines mentioned by (Kitchenham & Charters, 2007).

Inclusion and exclusion criteria
We have established a set of inclusion and exclusion criteria as shown in Table 1 that directs our research and targets the characteristics of the papers to be considered for this review.

Table 1 Inclusion and exclusion criteria
Full size table
The papers selected for inclusion were those that satisfied all inclusion criteria and none of the exclusion criteria. Only peer-reviewed research papers in English were included in the review. Research papers from journals, conferences, books, and theses were included. Only articles published from 2010 to 2020 were considered, as the field of Information Technology is evolving rapidly and it seems appropriate to limit the search to articles from the last 10 years. Also, research papers addressing the automated assessment of essays including short answers and written summaries were included. Research papers in the domain of AES but based on speech were excluded. Moreover, research papers that use similarity techniques in other NLP domains were not considered.

Performing the review
In subsection 3.2.1, the search strategy applied in subsection 3.1.2 is explained. The quality assessment and coding procedure is explained in subsection 3.2.2 and in subsection 3.2.3 the coding process is elaborated. Figure 1 shows the PRISMA flow diagram for the study selection process.

Fig. 1
figure 1
PRISMA flow diagram for the study selection process

Full size image
Search
As per the strategy described in Section 3.1.2, a keyword search was conducted on 16 April 2021. Initially, the exact key term “similarity techniques in automated essay scoring systems” was searched whereby a total of 24,400 results were obtained. Refining the search criteria taking into account both the inclusion and exclusion criteria with the date filter yielded 1,390 results.

Following the initial screening of 550 results using Google Scholar, we observed that the records following those 550 became less relevant to our study. These results were downloaded as Microsoft Excel document files from web pages of indexes. There were five (5) duplicate studies identified and eliminated, resulting in 545 studies to be examined. After the rigorous screening of titles and abstracts of these studies based on the inclusion and exclusion criteria, 188 studies have been chosen for further analysis as shown in Fig. 1. This was necessary in order to identify the primary studies that would provide useful insights into the research questions.

Quality assessment and coding procedure
Based on the inclusion and exclusion criteria, a preliminary screening was conducted. Following an examination of the full-text of the 188 selected primary studies, the 188 studies were further evaluated for "quality". It was essential that this process was followed to provide stricter inclusion and exclusion criteria, to evaluate the significance of individual studies when results are generated, and to steer recommendations for additional research (Kitchenham & Charters, 2007). The quality assessment was carried out by asking the following questions on the literature with respect to the quality criteria (QC).

QC1. Were the research papers focused on similarity-based techniques?

QC2. Were the research papers focused on instance-based techniques?

QC3. Have the similarity techniques been applied to English text only?

QC4. Have the methods used to generate the scores in the AES systems been properly evaluated?

The research papers were rejected when they did not answer any of the questions above. A total of 34 articles were selected for the final review after the quality assessment phase. However, we could not get access to one paper﻿ (Hendre et al., 2020a) and it was excluded in the final review bringing down the total to 33 studies. Hence, 26 studies were selected under QC1 and 7 studies under QC2. All the 33 studies met QC3 and QC4.

Data extraction
A rigorous evaluation process has been applied to the selected studies. Based on the research questions, the following data were extracted from each paper:

the full reference of the research paper

the similarity techniques used

the different methods which have been applied to compute scores in the AES systems

the metrics used to evaluate the effectiveness of the AES system in the studies

The extracted data were tabulated and shown in Table 2. The result column in Table 2 highlights the metrics used to evaluate the studies. Two types of metrics namely performance and error metrics were used. However, some studies used only the performance metrics while others used both the performance and error metrics. Further details about both metrics are discussed in sub-Section 4.3.

Table 2 Similarity techniques and the methods used to generate scores in AES systems
Full size table
Discussion
In the following sections, the responses to each of the research questions listed in Section 3.1.1 are examined in the context of the selected studies and data synthesis.

RQ1. What techniques are used to compute similarity in AES systems?
This review shows that two types of similarity techniques are used in AES systems. These techniques are elaborated below and the number of studies related to each technique is given in Table 3.

Table 3 The similarity techniques used in AES systems
Full size table
Similarity-based techniques
In this approach, the students’ answers are compared with a model (reference) answer from the instructor. The students’ answers are considered correct if they exceed a certain threshold value depending on the methods applied (Steimel & Riordan, 2020). In general, the similarity-based techniques do not need human intervention but there is no standard way in which model answers are recorded in the datasets. To increase accuracy, similarity-based techniques often need access to knowledge bases like WordNet (Roy et al., 2016). Moreover, in similarity-based approaches, if methods depend on lexical similarity to generate a score, then these methods will only perform well if answers are lexically similar, else they will not perform effectively (Horbach & Zesch, 2019). Studies that have harnessed the application of similarity-based techniques include (Alves dos Santos & Favero, 2015; Fauzi et al., 2017; Gautam & Rus, 2020; Hastings et al., 2012; Hendre et al., 2020b; Huang et al., 2018; Ikram & Castle, 2020; Islam & Haque, 2010; LaVoie et al., 2019; Mesgar & Strube, 2018; Mittal & Devi, 2016; Ndukwe et al., 2019; Oduntan & Adeyanju, 2017; Oduntan et al., 2015, 2016; Palma & Atkinson, 2018; Pramukantoro & Fauzi, 2016; Rahman, 2020; Saha & CH, 2019; Saha & Gupta, 2020; Sakhapara et al., 2019; Sendra et al., 2016; Suzen et al., 2020; Tashu & Horváth, 2020; Wang et al., 2018; Wedisa et al., 2020).

Instance-based techniques
In this approach, instances are learned by a supervised classification algorithm and stored in memory. When new instances have to be compared, a set of similar instances is retrieved from memory to make predictions about the target value of new instances. In AES systems, lexical properties of correct answers are learned by models and stored in memory. To predict the score of new answers, the model classifies the new answers based on learned answers. When compared to similarity-based approaches, the scoring process does not require external knowledge (Driessens & Džeroski, 2005; Horbach & Zesch, 2019; Shaker & Hüllermeie, 2012). The instanced-based techniques need a large dataset to train and test data to give high accuracy. In the case of supervised machine learning algorithms, data also needs to be labeled. The main problem with the labeled data is that they are task-dependent and cannot be applied in every AES system. Hence, constant instructor involvement is required to create the labeled data for a particular AES system and this can be time-consuming (Ghosh & Fatima, 2010; Kulkarni et al., 2014). However, nowadays with transfer learning the amount of data needed to train a model can be reduced. Transfer learning refers to applying knowledge from one domain to another domain in order to improve the learning performance in the target domain (Zhuang et al., 2021). Among the principal benefits of transfer learning are reducing both training time and the size of training data. Transfer learning allows a model to be built with relatively little training data because the model has already been pre-trained. This is especially valuable for AES systems where labeled datasets are required (Roy et al., 2016). Studies that have used instance-based approaches are (Beseiso & Alzahrani, 2020; Chen & Zhou, 2019; Cozma et al., 2018; Li et al., 2018; Steimel & Riordan, 2020; Tashu & Horváth, 2020; Yang et al., 2020).

From Table 3, it can be found that 26 studies were established on similarity-based compared to 7 studies on instance-based similarity techniques. Both similarity techniques were applied to mostly essay questions compared to short answers. Since the latter has limited text which provides fewer lexical features, it is more challenging to compute the score of short answers (Kulkarni et al., 2014).

For studies classified under the instance-based similarity technique, the automated student assessment prize (ASAP) dataset was mostly used for essays, and for the short answers, the ASAP-short answer scoring dataset (ASAP-SAS) was used. The ASAP dataset is an open dataset that has been utilized as a benchmark in several studies to compare results obtained from different AES systems. The ASAP dataset originated from the Kaggle competition held in 2012 which was sponsored by the Hewlett Foundation. This dataset consists of approximately 12,000 essays grouped into eight categories. The responses were written by students from grades 7 through 10. There is a wide range of essay lengths in the dataset, ranging from 120 to 500 words, and each essay has been graded by the instructors (Beseiso & Alzahrani, 2020; Yang et al., 2020). Table 4 shows the datasets that were used in the studies.

Table 4 Dataset used in studies for instance-based similarity techniques
Full size table
RQ2. What methods are used by the similarity techniques to generate the scores in the AES systems?
Various methods were used to compute similarity and generate a score in AES systems for both similarity-based and instance-based techniques. Figure 2 shows the number of studies using different methods to generate a score in the AES systems.

Fig. 2
figure 2
The number of studies using different methods to generate a score in the AES systems (Microsoft excel chart has been used to draw these two charts)

Full size image
In Table 5 the benefits and challenges of the most common methods to generate a score in AES systems are illustrated.

Table 5 Benefits and challenges of most common methods
Full size table
From Table 5 the methods that have been widely applied to calculate similarity and generate a score in AES systems in this review are described below:

N-gram
An n-gram is a continuous sequence of finite items from a given sequence. These items can include phonemes, syllables, words, among others and this is dependent upon the application (Sendra et al., 2016). For instance, a bigram is a two-word sequence while a trigram is a three-word sequence. N-gram models are useful for estimating the probability of a given word appearing in the last position of an n-gram given the preceding words (Jurafsky & Martin, 2018). In AES systems, to compute similarity, the analysis of words extracted from answers is not sufficient. In such cases, n-grams can be useful. By comparing n-grams, the similarity results can increase (Stefanoviˇc et al., 2019). In Oduntan et al.(2015), Alves dos Santos and Favero (2015), and Islam and Haque (2010), the n-grams were varied (unigrams, bigrams, and trigrams) and the results obtained were more concise.

LSA
LSA is based on the concept of vector space model where documents are first represented in high dimensional vector space. Then single value decomposition technique is applied to the document matrix to reduce the number of features thereby condensing all the important features into a smaller vector space (Manning et al., 2008; Martin & Berry, 2007). LSA and variants of the LSA methods are used to score students’ answers by comparing the latter to model answers from instructors. Both answers are first converted into vectors. Then the cosine angle between each student’s answer and reference answer is calculated to generate the similarity score. The latter is then multiplied with marks allocated by the instructor and the resulting value gives the score of the student’s answer (Alves dos Santos & Favero, 2015; Seifried, 2016). In their studies, (Alves dos Santos & Favero, 2015; Hastings et al., 2012; Islam & Haque, 2010; LaVoie et al., 2019; Mittal & Devi, 2016; Oduntan et al., 2018; Pramukantoro & Fauzi, 2016; Saha & Gupta, 2020; Sakhapara et al., 2019; Sendra et al., 2016) had used LSA and its variants thereby achieving good results.

Pre-trained embeddings
The pre-trained embeddings include the word embeddings (Word2Vec, GloVe, fasttext, ELMo) and sentence embeddings (SkipThought, InferSent, BERT, USE) (Nam et al., 2020). They encode words and sentences in dense vectors of real numbers respectively. To compute the similarity in the pre-trained embeddings, the cosine angle between the vectors is usually measured (Brück & Pouly, 2019). The application of pre-trained embeddings in several AES systems has shown good results (Beseiso & Alzahrani, 2020; Cozma et al., 2018; Hendre et al., 2020b; Li et al., 2018; Mesgar & Strube, 2018; Rahman, 2020; Saha & Gupta, 2020; Steimel & Riordan, 2020; Wedisa et al., 2020; Yang et al., 2020).

Neural network models
The term neural networks refers to a class of machine learning algorithms modeled loosely after the human brain that are designed to recognize and learn patterns (Imai et al., 2020). In AES systems, neural networks are used to learn essay representations, and then a scoring function is applied in order to calculate the essay scores based on the learned features (Yang et al., 2020). Recurrent Neural Network (RNN), Convolutional Neural Network, Long Short-Term Memory (LSTM) and Bidirectional-LSTM are models based on neural networks that have been used in AES systems (Huang et al., 2018; Mesgar & Strube, 2018; Saha & CH, 2019).

Combination of different methods
It can also be noted that in several studies, a combination of different methods has been used to score students' answers in both similarity-based and instance-based techniques. For instance, in the studies of (Beseiso & Alzahrani, 2020; Huang et al., 2018; Mittal & Devi, 2016; Palma & Atkinson, 2018; Saha & CH, 2019; Sakhapara et al., 2019; Wedisa et al., 2020), the accuracy of the systems was high with the hybrid techniques.

Table 6 highlights the comparison of several AES systems categorized by the widely used methods.

Table 6 Comparison of several AES systems categorized by the methods used
Full size table
RQ3. What metrics have been used by the AES systems to evaluate their effectiveness?
Several studies as shown in Table 2 have either used similarity-based or instanced-based approaches to generate a score for students’ answers. In this section, we will elaborate on the metrics that have been used by the AES systems in this review to evaluate the effectiveness of the studies?

Performance Metrics
To evaluate the performance of an AES system the scores generated by the latter are generally compared to that of human annotators (Chen & Zhou, 2019). This is known as the interrater agreement (human to machine) score. Several metrics can be used to assess the degree of interrater agreement, including Pearson's correlation, Spearman's rank correlation, Kendall's tau, Kappa, and the QWK (Lilja & Andersson, 2018).

As per Table 7, it can be noted that different performance metrics like correlation, accuracy, QWK, and F1-score were used to evaluate the performance of studies in this review. Some studies have used more than one metric to evaluate their system (Pramukantoro & Fauzi, 2016; Suzen et al., 2020).

Table 7 Performance metrics used in different studies
Full size table
i)
Correlation Coefficient

The correlation between two variables indicates the degree to which the variables are associated. Among the most commonly used correlation measures is the Pearson Product Moment Correlation (Pearson's correlation). Pearson’s correlation is a parametric measure of association that describes the extent to which the variables co-vary relative to the degree to which they vary independently. The greater the association, the more precisely one can use the value of one variable to forecast the other (Liu et al., 2014; Yannakoudakis & Cummin, 2015).

Several AES systems have used Pearson correlation as a common criterion to assess the consistency between human and machine scores (Liu et al., 2014). However, according to (Yannakoudakis & Cummin, 2015), an outlier can affect the value of the Pearson correlation to the extent that a high correlation is observed even though the data may not be linearly dependent. Hence, correlation measures do not reward scoring systems for their ability to accurately distinguish the thresholds that separate score or grade boundaries. The Pearson correlation is illustrated by Eq. (1) (Saha & CH, 2019).

r=n(∑xy)−(∑x)(∑y)[n∑x2−(∑x)2][n∑y2−(∑y)2]−−−−−−−−−−−−−−−−−−−−−−−−−−−−−√
(1)
where

r:
Pearson Coefficient

n:
number of answers

x:
the actual score given by human

y:
the score predicted by the system

ii)
Accuracy

Accuracy is the percentage of exact matches between the resolved (human) and predicted (machine) scores (Kumar & Boulanger, 2020), and this metric is usually used for classification problems (Beseiso & Alzahrani, 2020). For instance, in (Beseiso & Alzahrani, 2020) the performance of the AES model was measured in terms of accuracy, which was calculated using a confusion matrix that indicates the proportion of essays that were assigned a correct score label as expected. However, the accuracy metric should be used in conjunction with other types of evaluation metrics to reflect the reliability of the AES system. The formula to calculate the accuracy is given in Eq. (2).

Accuracy=NumberofcorrectpredictionsTotalofallpredictions×100
(2)
where:

predictions represent the score label.

iii)
QWK

Using the QWK, it is possible to assess the agreement between predicted scores and the actual scores of two human judges (Burrows et al., 2014). A quadratic weight matrix is used in QWK to consider quadratic weights and is computed between a machine predicted score for each essay and the resolved score for human raters on each set of essays (Liang et al., 2018). The value of the QWK metric usually ranges from 0 to 1 where 0 is the random agreement between the raters and 1 represents the absolute agreement between the raters. The QWK value may go below zero if the level of agreement between human raters is low (Smolentzov, 2012) and to value -1 when there is a total disagreement between the two raters (Jakobsson, 2019). The QWK is defined as follows (Liang et al., 2018):

Wi,j=(i−j)2(N−1)2
(3)
where:

i and j:
are the human rating and machine rating respectively

N :
is the number of possible ratings.

The matrix O is created over the answers ratings, such that Oi,j corresponds to the number of answers that received a rating i by a human and a rating j by machine. The histogram matrix of expected ratings, E, is computed under the assumption that there is no correlation between rating scores. This is calculated as the outer product between each rater’s histogram vector of ratings, normalized such that E and O have the same sum.

From these three matrices W, E and O, the QWK is determined by Eq. (4):

k=1−∑Wi,jOi,j∑Wi,jEi.j
(4)
The QWK evaluation metric was used as the official evaluation measure in the ASAP AES competition (Kaggle, 2012) and since then many AES systems have chosen this metric for evaluation (Kumar et al., 2019; Mathias & Bhattacharyya, 2018) to make a better comparison with research in the AES domain. A more detailed kappa coefficient scale was proposed by (Landis & Koch, 1977) as shown in Table 8.

Table 8 The detailed Kappa coefficient scale
Full size table
iv)
F1-Score

An assessment of the overall accuracy of results from machine learning is commonly done by analyzing precision, recall, and F1-score. Precision is the fraction of correctly selected items among all the selected items, while recall is a function of its correctly classified items and its misclassified items. The F1-score is derived from the weighted average of precision and recall (He, 2020; Sokolova et al., 2006). The F1-score reaches its optimal value at one and the worst score at zero (scikit-learn, 2020). To measure the performance of AES systems based on machine learning techniques, the aforementioned metrics should be used in combination with other metrics as they are biased measures (Link, 2015). The precision, recall, and F1-score are illustrated in Eqs. 5, 6, and 7 respectively (He, 2020).

precision=NumberoftruepositiveidentificationTotalnumberofpositiveidentification
(5)
recall=NumberoftruepositiveidentificationTotalnumberoftrueinstances
(6)
F1−score=2.precision.recallprecision+recall
(7)
where

the identification represents the score label

Error metrics
Error metrics like the root mean square error (RMSE), the mean square error (MSE), and the mean absolute error (MAE) are commonly used for regression problems. The RMSE is the square root of the average of the squared differences between the real value (human score) and the predicted value (machine score) (Pribadi et al., 2018). A high RMSE value shows that there is a considerable variation between the scores generated by the predicted value and the human-given marks (Saha & Gupta, 2020). Likewise, the MSE is the average of the squared differences between the real value and the predicted value. If the MSE value is very small, it reveals that the prediction model is very accurate in describing the experimental data (Chen & Zhou, 2019). The RMSE is defined as per Eq. (8) (Saha & Gupta, 2020):

RMSE=1n∑j=1n(Scoreactual(j)−Scorepredicted(j))2−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−⎷
(8)
where n is the number of answers

The MAE is the average of the absolute values of the differences between the predicted value and the real value. Much like RMSE lower the MAE value, less is the variation between machine score and human score (Citawan et al., 2018; Oduntan et al., 2018). The MAE is defined as per Eq. (9) (Citawan et al., 2018):

MAE=∑ni=1|xi−yi|n
(9)
where:

MAE:
Mean Absolute Value

xi:
the actual score given by humans

yi:
the predicted score by system

n:
numbers of data

A variety of metrics, including but certainly not limited to RMSEs, MSEs, and MAEs are often required to evaluate model performance (in this case, an AES system). Every single metric only offers one projection of the model errors and emphasizes, therefore, only a certain aspect of the error characteristics (Chai & Draxler, 2014).

From this review, it can be noted that all the studies did not use the same metrics to evaluate the performance of the AES systems. Consequently, it is difficult to compare and contrast the results obtained as each metric was computed differently. However, several studies have used the QWK as evaluation metrics. For instance, from this review, most of the AES systems based on the neural network models and pre-trained embeddings yielded a QWK value above 0.70 on the kappa coefficient scale. According to (Williamson et al., 2012) for high-stakes tests, the acceptable value for the QWK for automated scores should be at least 0.70. Hence the performance of the AES systems based on the aforementioned methods is laudable.

Conclusion
Several AES systems have been developed over the last few decades. The existing AES systems based on different types of questions have used several methods for content scoring. In this SLR, thirty-three (33) papers have been analyzed to provide insights on the different similarity techniques and methods used to generate a score for students’ answers. With the three research questions that have been set in the planning phase, this review has uncovered that there are two main types of similarity techniques used by AES systems. The first one is similarity-based where similarity is computed with a reference answer. The second one is instance-based which uses models to learn features of essays and then predicts a corresponding score. Both similarity techniques have used several methods to generate a score for students’ answers. The effectiveness and limitations of the similarity techniques and the most common methods used to compute scores have been discussed in detail. The performance metrics used in the AES systems also have been elaborated. Future work in the AES domain has to tackle the existing challenges to get more accurate results. This will help researchers to build more robust AES systems which will be more helpful to instructors.

Limitations
This study has some limitations as it was not feasible to critically evaluate the reviewed studies based on the methods employed. Different evaluation metrics were used in the various studies. For instance, some studies used the accuracy or F1-score while others used correlation or QWK as the evaluation metric. Many studies were also excluded from the final review since they were either not adequately evaluated or did not contain any results for the system evaluation. Studies based on AES systems that focused on similarity computation on non-English language and AES systems based on speech were not considered in this review.

Implications for future work
By answering the research questions set in this study, the latter has uncovered that most AES systems use similarity-based techniques, i.e. most AES systems are dependent on the model answer to compute similarity score. However, nowadays with the pre-trained embeddings, the instance-based similarity techniques are becoming common as less data is needed for training. This can encourage researchers to make use of the instance-based techniques to calculate the score of students’ answers in AES systems.

Most studies have used a combination of methods to compute similarity scores in this review. The application of different methods (hybrid) in a single system can yield better results compared to only one method. Consequently, the performance of AES systems can be improved by adopting hybrid techniques.

Measuring the performance of an AES system is challenging as several metrics are available. Ideally, a standard metric should be used by all AES systems to evaluate their efficiency allowing various researchers employing different methods to easily compare the AES systems. From this study, it was found that several AES systems used the QWK metric for performance evaluation as the latter was used as the official evaluation measure in the ASAP AES competition.

In light of the aforementioned findings, this review serves as a useful resource for researchers seeking to develop new approaches to measuring AES similarity.