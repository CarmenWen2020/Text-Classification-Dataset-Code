The principle of traditional education has transformed fundamentally within the last few years. The growth of new computer and communication technologies has provided learners with access to a variety of quality online education. Many educational institutions have shifted partially or completely to distance learning and online lectures as their major instruction methods. With these new models, various difficulties related to classroom management and students’ monitoring during online lectures face many instructors worldwide. In addition, educational administrators, such as school principals and university deans, find it very difficult to assure that students are participating and actively engaging during online lectures. In this paper, we present a framework that can be utilized to monitor online lectures and deduce important figures, such as the degree of students’ involvement, the percentage of students who are participating, and the amount of engagement of each student during each lecture. The proposed system analyzes the recorded lectures and performs voice diarization on the audio signals to produce the required outputs. The system was tested during five online courses in multiple majors. The results illustrate the importance of monitoring the online lectures of distance learning courses to ensure satisfactory students’ participation.

Introduction
In conventional education, the physical classroom is the main context where students meet their instructors, understand the learning material effectively, and engage in various learning processes. Students are required to be present on time and participate actively in order to grasp, obtain and apply the competencies and information shared by their instructors and classmates. These engagements produce the necessary environment for successful learning (Abdullah et al., 2012).

The standard methods of classroom engagement changed with the proliferation of Internet-based distance learning in the last decade. In addition, the recent COVID-19 pandemic and the lockdown rules that were applied in most countries increased the dependency on online learning. During the last year, the closing of school and university campuses forced many educational institutions around the world to rapidly transform their existing teaching and research materials to a format that is appropriate for online delivery. This transformation has driven many instructors and academics to learn new online teaching tools and methodologies, with little or no training and minimum previous experience (Dwivedi et al. 2020). In addition, while teaching online lectures, many instructors find themselves facing new situations and having to deal with them, such as: maintaining class discipline, providing online engagement opportunities, checking and following up with quiet students, and finding successful methods to guide students to self-learn without face-to-face assistance (Kebritchi et al., 2017).

In order to provide a successful learning atmosphere for students during online sessions, instructors are required to adopt new pedagogical techniques that are suitable for distance communications. For example, Dwivedi et al. (2020) indicate in their study that instructors should give more attention to students’ chats during online lectures that are conducted via a virtual classroom tool. Also, the authors suggested that instructors should provide time for students before or after the session to discuss lecture-related aspects. Another important point that was mentioned by the authors is that instructors should ask students to keep their cameras ON while engaging in collaborative activities. This allows instructors to check the participation of students and their seriousness in fulfilling their duties.

Since many instructors who are used to traditional teaching lack the experience and don’t know the requirements for giving successful online lectures, students in these lectures are not given the opportunities to make full use of distance learning practices. In a quick review of the recorded online lectures of several courses that were conducted at three universities in Lebanon, it was noticed that many of these lectures were instructor-centered, and students were just listeners in most of the lecture. Only a few lectures featured the instructor giving students opportunities to engage in learning activities or discuss lecture-related matters.

Monitoring students’ participation during online lectures is a very important factor for ensuring successful course delivery. Tobin (2004) states the importance of monitoring the students’ activities during online lectures. For this reason, the instructors of online courses should continuously examine and evaluate the participation and progress of each student using techniques that are aligned with quality online learning (Yang & Cornelious, 2004). However, this requires doing the monitoring process manually by watching the recorded video of each lecture and building up a profile for each student that includes his/her progress in individual and collaborative online activities, which is a very difficult and time-consuming task.

In this paper, we present a novel system for monitoring students’ participation during online sessions that are conducted via a virtual classroom tool such as Zoom, Webex, Adobe Connect, etc. Our system utilizes the concept of voice diarization (also called speaker diarization), which is the process of identifying and labeling different speakers from a speech segment. In the proposed system, the recorded audio files of online lectures are analyzed in order to deduce several important outputs, such as the percentage of participation of each student and the overall students’ involvement in the lecture. The proposed system saves the results into the database of the university management system, where they can be accessed by the instructors and administrators. By combining and analyzing the results from the lectures of a certain course, the system is able to produce accurate figures about the students’ participation actions and build a participation profile for each student. These outputs are vital for university administrators and instructors to be able to detect students who require help or training and those who are not putting enough effort into their studies. Our system helps these administrators in their quest to improve future online courses and make sure they are better managed and more student-focused.

The remainder of this paper is organized as follows: the next section provides a quick review of several research works that tackle the topics of classroom monitoring and students’ participation. Section 3 describes the details of the proposed system and its various operations. Section 4 presents the method that was used to test the system. In Sect. 5, the results of the performed experiments are presented and their importance to the university instructors is discussed. Section 6 concludes the paper with some final remarks.

Literature review
Students’ engagement is conceptualized as the processes that demonstrate constructive participation in learning activities (Ben-Eliyahu et al., 2018). Several researchers (Ben-Eliyahu et al., 2018; Pekrun & Linnenbrink-Garcia, 2012; Fredricks et al., 2004) outlined three main dimensions of engagement, which are affective (or emotional), behavioral, and cognitive. In general, cognitive engagement can be observed in activities such as solving problems, using thinking skills detailed in Bloom's Taxonomy, participating in question and answer sessions, and implementing learning strategies that help the student to remember, organize, and understand the learning material (Pekrun & Linnenbrink-Garcia, 2012). Affective engagement can be seen in any action or expression that indicates the student’s willingness and enthusiasm to interact with the learning material. It refers to students’ affective reactions while learning such as interest, boredom, happiness, sadness, and anxiety (Fredricks et al., 2004). Finally, behavioral engagement is the learner’s physical actions that demonstrate their involvement in the content. It can be determined by observing the amount of participation in the course through discussion posts, assignments completed, page views, and academic tasks.

In general, students’ participation in classroom lectures belongs to all three dimensions of engagement. When students discuss with the instructor or their classmates, they are thinking about issues related to the learning material or trying to understand ambiguous or complex matters, which is a form of cognitive engagement. In addition, the student who asks questions or states his/her opinion about learning issues or debates with the classroom participants shows that he is interested in the learning material. In general, the student’s type and method of participation during the classroom discussions reflect his emotional state and contribute to his/her affective engagement. Finally, participating in discussions, asking questions, and explaining and debating viewpoints and opinions is a form of physical activity that falls into the category of behavioral engagement. This section first highlights the importance of classroom participation and its effect on students’ learning. Next, several research works that described methods for monitoring students’ participation in online courses are outlined and discussed.

A major element of successful learning during classroom sessions is students’ participation. Students are required to engage actively during various classroom events by playing the role of information seekers. Abdullah et al. (2012) describe the various reasons that encourage students to engage during classes. According to the study, the first duty is on the instructors who are required to design engagement strategies and exercise suitable techniques that build a responsive classroom. In addition, instructors should differentiate between active, passive, and weak participants and focus on pushing the last two types to increase their active participation by inviting them to speak up, affirming and valuing their contributions, giving marks/grades for active participation, and applying a variety of instruction methods.

Several studies discussed the relationship between students’ learning and their participation in classroom discussions (Dallimore et al., 2010; Handelsman et al., 2005; Rocca, 2010; Starmer et al., 2015). Rocca (2010) provided a detailed description of the various benefits of students’ classroom participation. The discussion was based on an extensive literature review. The author defines the “contribution to in-class discussions” as one of the main aspects of students’ participation. Handelsman et al. (2005) stated that students have been found to earn higher grades as their classroom participation increases. The study made by the authors revealed that the student engagement factor explained 28% of the variance in their examination grades. Dallimore et al. (2010) state that “class discussions enhance learning by increasing engagement, helping students retain and remember information, providing confirmation of what they have learned, providing clarification and deepening their understanding”. Another study by Starmer et al. (2015) emphasizes the great effect of classroom participation on the students’ examination scores. The authors state that “Full participation in the course was related to higher examination scores” and “achievement of higher levels of learning”.

Students’ participation during online courses has a similar relationship to performance as it does in conventional courses, as the study by Douglas & Alemanne (2007) proves. The authors state some important strategies to provide participation opportunities to students in online courses, such as having regular online discussion threads. In addition, the authors describe important tools for monitoring students’ activities during online courses, most of which are built into learning management systems (LMS). These tools allow instructors to track how often a student visited a course website and what he/she did there. They identify when a student did not listen to an online lecture or clicked a recommended link, the number of times a student posted on discussion panels, etc. The instructor can collect and group statistics from these tools to build a participation profile for each student. Wut and Xu (2021) stated that instructors who teach online classes should use encouragement, incentives, breakout rooms, and various engagement techniques to effectively facilitate and manage student-to-instructor and student-to-student interactions. The authors recommend that instructors should develop assessment rubrics to evaluate student interaction performance during the online class, and provide additional marks to further motivate student interactions.

Jinbiao and Bin (2017) introduce the online teaching model, which combines traditional classrooms with online sessions. In this model, the student is the protagonist and the instructor is in the supporting role. The main knowledge is obtained online, and classroom teaching is used as a supplement whenever needed. The authors identify several techniques that allow students to participate during online lectures such as open discussions, chatting groups, social media pages, etc. In addition, teachers can set up personal course websites, personal blogs, and other forms of media to provide each student with multiple possibilities to participate and show their opinions, works, achievements, etc.

A framework that enables academic institutions to systematically collect and analyze data from distance learning platforms is presented by Peled and Rashty (1999). The framework is built using three main components: collecting and analyzing data from Web log files of online courses; creating smart tools that monitor the academic performance of students and associate the results with demographic information and students’ backgrounds; and creating distance learning focused student surveys that assess various aspects related to online courses, instructors, and utilized tools and technologies. Another model that tutors can use to understand interactions and gain information about the social and cognitive processes that take place in a computer supported collaborative learning (CSCL) environment was proposed by Persico et al. (2010). This model utilizes specialized software that runs along with online sessions and records information related to students’ interactions and involvement (reading and writing messages, uploading and downloading documents, logging in and out of the environment), in addition to the tutor’s instructions and comments.

Bouhnik and Marcus (2006) identified four different types of interactions for students who are participating in an online classroom, which are interaction with content, interaction with the instructor, interaction with classmates, and interaction with the system. The authors state that tutors should use the special tools that online technology offers to monitor the students’ participation in the course and interact with them in a manner that prevents them from “getting lost”. Also, the online system should make the data related to self and group interactions accessible for both the students and instructor. This practice assists the teacher in monitoring the students and their respective progress and allows each student to evaluate his or her achievements in comparison to his/her classmates.

Another study by Akhtar et al. (2017) uses an online learning and teaching platform called SurreyConnect in order to analyze a set of elements related to students’ online activities that include log-in and log-out data, time spent on a particular application, and time spent on assignments. The system performed further analysis that included: number and ID of unique students; total number of students; average time spent in the class; average seating row; and total working time. From the collected data the system identifies the key indicators such as student self and group participation. Similar to this system, the work by Alachiotis et al. (2019) depends on analyzing Moodle’s log files using data mining tools in order to trace the daily and weekly activities of the learners concerning the distribution of access to resources, forum participation, and quizzes and assignments submission. The authors found that the distribution of students’ access to the contents of the online course and their participation in the quizzes, assignments, and fora are useful factors for tracing the interest and the dedication of students. In addition, the relationship between active participation and quiz submission was studied and a multiple linear regression method and sensitivity analysis was used to predict the learners’ performance in the final examination of the online course.

Belousova et al. (2019) argue that the effective functioning of the distance learning system must include the organization of student targeted self-monitoring of educational outcomes. Students rethink and summarize the material studied, use the knowledge to solve practical problems, and acquire the skills and experience to monitor their progress independently. The results of the study prove that students explore the subject more deeply and responsibly if they know in advance that they will be monitored regularly. Using such strategies, instructors push students to develop the need for self-control. Similar works that emphasize the importance of students’ participation for successful self-directed online learning (SDL) in a Massive Open Online Course (MOOC) were presented by (Zhu & Bonk, 2019) and (Chapman et al., 2016). The results of these studies show that MOOC instructors considered self-monitoring skills critical for SDL. To foster students’ self-monitoring, MOOC instructors reported that they facilitated students’ self-monitoring by helping students and providing them with both internal and external feedback.

A system for stimulating students’ participation in distance learning was proposed by Gomez et al. (2016). The system places the student in an intelligent learning environment and is capable of delivering appropriate context-related learning contents, based on location, time, date, interaction of the student, profile of the student, and so on. The system includes reasoning capability that uses a context-based ontology to deliver the most suitable learning resources to each student. In addition, the system can deliver timely and adapted learning content that helps students to actively engage with their classmates and the instructor whenever possible. Alowayr and Badii (2014) present a Monitoring and Analysis Tool for the E-learning Program (MATEP) which is used to provide enhanced levels of learning and trusted assimilation in an e-learning delivery context by creating an environment of real-time interaction between learners and their instructors. The authors present an analysis of the impact of an integrated learning path that an e-learning system may employ to track the online activities and evaluate the performance of learners.

Very few works in the literature have utilized speaker diarization for educational purposes. Dubey et al. (2016) present an unsupervised speaker diarization model that utilizes a G3 algorithm for speaker change detection. The purpose of the system is to measure various communication metrics in Peer-led team learning (PLTL) sessions. However, the proposed system performs poorly as compared to modern diarization techniques, since the authors reported a Diarization Error Rate (DER) equal to 25%, which is much less than current systems that produce DERs between 5 and 10%. James et al. (2019) propose a system that uses speech processing algorithms in order to detect speakers and social behavior from audio recordings in classrooms. The proposed system extracts non-verbal speech cues and low-level audio features from speech segments in order to infer the general climate in the classroom.

The research works described in this section offer various solutions for monitoring students’ participation during physical and online sessions. However, none of these works were able to calculate the exact participation of each student in the class in every online session and use it to monitor the student’s overall participation in the course lectures. In this paper, we utilize voice diarization to detect the various participants during an online lecture and the amount of participation of each one. By combining results from multiple sessions, the instructor can build a participation profile for each student in the class, identify the profiles of weak students, and undertake corresponding measures. To the best of our knowledge, our system is the first that utilizes voice diarization for enabling instructors to detect weakly participating students and help them to improve their learning.

Utilizing voice diarization for monitoring online lectures
The main idea behind our proposed approach for monitoring students’ participation during classroom sessions is simple: we record the complete audio signals during the session via an advanced audio recording device, such as a microphone array (which is capable of recording high-quality audio within a large surrounding area), then we analyze the recorded audio signal by means of voice diarization (also called speaker diarization) technique in order to know who spoke when, and the duration of his/her speaking (details of voice diarization will be explained later in this section). This project was started before the beginning of the COVID-19 pandemic. Our initial framework was designed for physical classroom sessions. In the original design, we use a microphone array (Respeaker Mic Array 2.0), whose details can be found on https://wiki.seeedstudio.com/ReSpeaker_Mic_Array_v2.0/, to record the audio signals during a lecture. The microphone array is placed in the center of the classroom and connected to a Raspberry Pi 4 module. The latter receives the audio signals from the microphone array and transmits them via WiFi to the classroom gateway, which sends them to the university Moodle Server (the proposed system was integrated into the Moodle LMS, as we will explain later). The Moodle LMS performs voice diarization on the audio signals (given prior voice templates). An overview of the original framework that illustrates the described hardware is shown in Fig. 1.

Fig. 1
figure 1
Overview of the proposed Class Monitoring System for physical classroom lectures

Full size image
With the start of the lockdown due to the COVID-19 pandemic, physical classes were completely suspended. Hence, we were not able to test the original framework in physical lectures at the university campus. For this reason, the implementation was slightly modified: instead of capturing the audio signals from physical lectures, the system obtains and processes the videos of the online lectures that were recorded using the Zoom tool that was applied by our university as the standard teaching mechanism during the lockdown, as did many educational institutions worldwide. Hence, the framework becomes as shown in Fig. 2: first, the instructor and students of the monitored course are requested to send to the tool recordings that contain their voices speaking a sample text. These recordings are used by the machine learning algorithm that is applied in voice diarization to train the voice of each participant in the lecture. Hence, the instructor and each student record their voices while speaking the sample text, send the resulting audio files to the tool’s database, and the latters are saved as training data.

Fig. 2
figure 2
Details and operations of the proposed Class Monitoring system for online lectures

Full size image
Next, we obtained an agreement from the university administration to add the Moodle plugin to the university Moodle Server, and to copy the videos of the online lectures of the monitored courses to the Moodle plugin. Once an instructor gives an online session, the Moodle plugin accesses a copy of the recorded video of the session, extracts the audio signal from the video file, and inputs the audio signal to the voice diarization module. The latter applies machine learning algorithms to the audio signal and the training data to produce the voice diarization outputs, which will be explained later. These outputs are saved to the Moodle account of the course instructor. The latter will make use of the results to adjust future lectures, as we will explain later. An overview of these operations is shown in Fig. 2. We call our tool DIAMOND, which stands for voice DIArization for MONitoring Distant lectures. DIAMOND can be used for measuring the per-lecture and overall participation of each student during an online course. This work is part of the LearnSmart research project, which we presented in (Mershad & Wakim, 2018) and (Mershad et al., 2020). In the remaining of this section, we first explain the main concepts related to voice diarization. Next, we present the details of the Moodle plugin that we used for building and testing our system.

Voice diarization
Voice diarization is the process of analyzing an audio signal to identify the people who spoke during the time of the signal, determine when each one of them spoke, and the duration of his/her speech. This process includes dividing the audio signal into segments based on the speaker’s identity. In its general form, voice diarization is a union of voice segmentation and speaker clustering. The first is used to detect the points in the audio file at which a new speaker starts talking, while the second aims at exploiting the distinct features of the voice of each speaker in order to group the segments of the speaker together (Anguera et al., 2012).

In order to perform successful and accurate speaker diarization, the audio signal must pass through a series of modules, as identified by (Bredin et al., 2020) and illustrated in Fig. 3. The first module is “Feature Extraction” which is used by machine learning algorithms for identifying the parameters that will be used for training the utilized neural network. Examples of famous feature extracting techniques are Mel-frequency cepstrum (MFCC) and spectrograms. Since the analysis of large audio files is neither feasible nor effective, DIAMOND depends on analyzing segments of the audio signal. First, at training time, it extracts the features of the voice of each speaker from the fixed-text audio files. At test time, the audio file of the lecture is first divided into small overlapping segments; and then analyzed by comparing the features from each segment to those in the training data.

Fig. 3
figure 3
Set of modules used in the speaker diarization system

Full size image
The second module “Voice Activity Detection” is used to remove the periods of silence and those that include background noise only from the audio signal and exclude them from the analysis. This is done by comparing the total score of each segment with a tunable threshold θVAD and only segments whose total score is greater than this threshold are considered as speech. The next module that the signal passes through depends on the features of the current segment. If the current segment contains features from two or more training files, which indicates that multiple speakers are talking at the same time, then the signal goes to the “Overlapped Speech Detection” module. Here, the features of the segment are compared to those in all training data and each training file whose features score more than a tunable threshold θOSD is considered a speaker in this segment. If the current segment contains similar features to the previous segment then the algorithm moves to processing the next segment. Finally, if the current segment contains different features from the previous segment, then the system moves to the module “Speaker Change Detection”, in which the training data whose features achieve the highest score when compared with the features of the new segment will determine the new speaker, provided that the score must be greater than a tunable threshold θSCD.

The next module “Speaker Embedding” enhances the identification process by adding a lingual context to the features used previously. Speaker Embedding uses specific features that are taken from the hidden layer neuron activations of Deep Neural Networks (DNN), and are proved to be helpful for speaker verification especially to distinguish speakers who do not exist in the training set (Rouvier et al., 2015). The most critical module is “Clustering”, in which the segments of each speaker are grouped together. The traditional algorithm for Clustering is the Probabilistic Linear Discriminant Analysis (PLDA) which is used for calculating the similarity between two speech segments. In DIAMOND, we utilize the Sequential Bidirectional Long Short-Term Memory (Bi-LSTM) algorithm that was proposed by Lin et al. (2019) for measuring the similarity matrix between all segments of the audio file. In addition, Spectral clustering is employed after the similarity matrix to achieve better accuracy. With these enhancements that mix multiple best approaches from the literature we were able to reach a Diarization Error Rate (DER) of less than 5% in our system.

In order to select the best voice diarization method for DIAMOND, we tested three related systems: Pyannote Audio (Bredin et al., 2020), UIS-RNN (Zhang et al., 2019), and Sincnet (Ravanelli & Bengio, 2018). We conducted extensive tests on the audio files that were extracted from 109 online lectures using each one of these tools. The total length of the audio files that we used in our tests is equal to 134 h (average length of 73.77 min per lecture). The average number of students per lecture was equal to 14.88. The results of our tests showed that Pyannote Audio combined with Bi-LSTM and SC produced an average DER equal to 4.89%. On the other hand, the average DERs produced by UIS-RNN and Sincnet were equal to 7.53% and 9.71%. Hence, we adopted Pyannote Audio with Bi-LSTM and SC in DIAMOND.

The final module that the audio signal passes through is “Resegmentation”, which is the task of optimizing speech change limits and attributes that were produced by the “Clustering” module. Note that the “Resegmentation” module is not pre-trained. Rather, a new resegmentation model is trained from scratch using the output of the diarization pipeline.

DIAMOND moodle plugin
The DIAMOND tool was implemented as a plugin within the Moodle learning management system. Within the plugin, various functions are employed: reading a video file, extracting the audio signal from the video file and saving it as a WAV file, performing voice diarization on the audio signal, determining the time intervals during which the instructor and each student spoke, using a plotting tool to draw a graphical representation of the diarization output, saving the diarization output into an Excel file, and combining diarization outputs from multiple files to create a student participation profile. In order for the instructor to use the system, he/she should add the training data into a specific folder labeled “training” inside the DIAMOND plugin. The instructor records his voice speaking sample text that is predefined by the machine learning tool in DIAMOND, and asks the students in the class to do the same. After that, the instructor checks that the audio files are correct and clear, converts the ones that require so into WAV format, and uploads them to the “training” folder.

After the DIAMOND plugin is installed into Moodle, a link labeled “Analyze with DIAMOND” appears near to each video that is uploaded by the instructor to Moodle, as illustrated in Fig. 4. When the instructor clicks on the link, the DIAMOND main page is loaded by Moodle. This page contains the video that can be played using Moodle’s media player. In addition, the page contains a button labeled “Analyze Video” near the Moodle media player (Fig. 5). When the instructor clicks on this button, the voice diarization operations start: first, the video file is loaded into memory, then the audio signal is extracted from the video file and saved as a WAV file. Next, the steps that were described in Sect. 3.1 are executed, and the audio signal is segmented into parts, where each part contains distinct voice features. Each part (or segment) is compared by the machine learning tool with the features in the training data, and the voice (or voices, in case of multiple persons speaking together) of the person who is speaking in the segment is identified. After all segments are analyzed, the diarization outputs are generated. These outputs comprise the start and end time of each segment, the names of the people who are speaking during the segment (names are known by the machine learning tool from the names of the training files, since each file must be labeled with the name of the person whose voice is in the file), the total number of seconds spoken by each person, and the percentage of the total duration spoken by each person (the percentage is calculated by dividing the person’s total speaking duration by the total file duration).

Fig. 4
figure 4
Link that starts DIAMOND appears near to each video in a Moodle course

Full size image
Fig. 5
figure 5
DIAMOND main page that shows the selected video and the button that is used to analyze it

Full size image
After the diarization outputs are calculated, they are plotted into a graph and displayed below the video, as illustrated in Fig. 6 (the instructor and students’ names were blurred for privacy purposes). The figure shows the analyzed audio signal and the resulting segments below it, where each segment is colored with a distinct color according to the person who is speaking during the segment, and displayed in the row that is labeled by the person’s name. In case a segment contains multiple voices, it is displayed in all rows of the persons who are speaking during the segment. In Fig. 6, we notice that the instructor (first row) is the one who spoke the most during the lecture (3015 s which correspond to 63.7% of the total duration), some students spoke in one or more segments (ranging between 31 and 243 s per student), while some students did not speak at all during the lecture (no segments in their rows). To the right of the segments, there are two columns that show the total duration spoken by each person (in seconds) and the corresponding percentage. The diarization outputs are saved into the Moodle database. In addition, a link labeled “View Analysis” is added to the video (near to the “Analyze with DIAMOND” link that is shown in Fig. 4). The instructor can use this link at any time in the future to view the diarization outputs and create students’ participation profiles, as we will shortly explain.

Fig. 6
figure 6
Diarization outputs and results as shown on the DIAMOND main page along with two buttons to save the results and create students’ participation profiles

Full size image
At the bottom of the figure, there are two buttons: “Save to Excel” and “Create Participation Profile”. The first is used by the instructor to save the diarization output to a separate Excel file, where each segment in the graph is represented in the Excel file by a column that is labeled as: {Start Time} − {End Time}; and each person who is speaking during a segment is labeled by “1” in his/her Excel cell under the column of this segment. The other button on the DIAMOND page, which is “Create Participation Profile”, is used by the instructor to create a participation profile for a certain student. When the instructor clicks on this button, a pop-up window is shown asking the instructor to enter a student’s name. After the instructor enters the name, the DIAMOND plugin searches within the diarization outputs of all videos in the course for the results of this student, and combines these results together into an Excel file that contains the title and date of each lecture, the student participation duration and percentage in each lecture, in addition to the values of several Excel functions that are applied to the student’s data, such as the average participation time and percentage for all lectures, the standard deviation per lecture and for all lectures, and the number and percentage of lectures with zero participation. The student’s participation profile is saved as an Excel file in a folder labeled profiles inside the DIAMOND plugin.

Method
The DIAMOND tool was tested as follows: we selected five courses that were taught at our university since the start of the COVID-19 pandemic, which are: "COM490 Media Planning and Management", "IDS255 Interior Design I", "CCN211 Networking I", "MCE202 Mechanics I", and "MKT420 Sales Management". These five courses were chosen among other courses in order to cover all faculties in the university (a course was selected from each faculty). Another reason for selecting these courses is the number of students in each of them. The five courses included (7, 11, 15, 22, and 32) students respectively, which was intended so that we monitor courses with various numbers of students, ranging from the minimum possible per Section (7) up to a full Section (32). In addition, the instructors of all five courses followed a common approach in giving the online lectures: they used the Zoom tool (https://zoom.us/) to explain and discuss the lecture material with the class, and utilized the options within the Zoom tool to create collaborative tasks and engage the students. All online lectures given in each of these courses were recorded and uploaded by the instructor to the Moodle page of the course. Before starting to monitor these courses, we obtained the required authorization from the university administration and installed the DIAMOND tool within the university Moodle framework at the university mainframe. The university Moodle server was updated by giving the instructors of the five courses the necessary authorization to use the DIAMOND tools from their Moodle accounts.

For each of these courses, we explained the purpose and the operations of the DIAMOND tool to the course instructor, and the tasks that he/she is required to do in order to monitor the participation of his/her course students. First, each one of the instructors is required to log in to his/her Moodle account, download the DIAMOND tool from the university Moodle server, and install it on his/her device. After that, the DIAMOND tool becomes integrated within the instructor's Moodle account within the instructor’s device, and all the features of DIAMOND become available and ready to be used by the instructor. Before the instructor uses the DIAMOND tool, he/she should ask each student in the course to record his/her voice speaking the sample text required for training the machine learning tool in DIAMOND, and upload the resulting audio files to the “training” folder in Moodle. We also check the training audio files to make sure that they are accurate and suitable for the voice diarization tool. When the instructor uploads a video to the Moodle course, the DIAMOND plugin adds the link "Analyze with DIAMOND" near to the video (Fig. 4). When the instructor clicks on this link, the voice diarization system within DIAMOND is executed and the diarization outputs are displayed on the DIAMOND main page, as shown in Fig. 6.

We asked the instructors of the five courses to generate the participation profiles (that can be generated by clicking the “Create Participation Profile” on DIAMOND’s main page) for the course students at four checkpoints during the course: 1) after a quarter of the course is finished (i.e., after seven lectures), 2) after half of the course is finished and directly before the course midterm exam (i.e., after 15 lectures), 3) after three quarters of the course are finished (i.e., after 22 lectures), and 4) after the course is completed (30 lectures). After each of the first three checkpoints, each instructor should identify from the generated students' participation profiles the students who are not participating or are weakly participating during the lectures. We defined two general rules that indicate weak participation, which are: 1) if the students’ average participation time per lecture is less than one minute, and 2) if the percentage of lectures with zero participation is greater than 30% (for example, suppose that 15 lectures were analyzed in the participation profile, then if the student has 5 or more lectures with zero participation this is considered weak participation). However, we did not force the instructors to consider these rules strictly. Rather, we let the instructors modify the thresholds (1 min for the first rule and 30% for the second rule) as they deem appropriate. This was considered due to the fact that certain courses offer participation opportunities more than other courses. Hence, the students are required to participate more in such courses and accordingly the thresholds of the two rules should be increased. In the end, we let the instructors define the values of the thresholds for the two rules according to what they think is appropriate for their courses, and we asked them to save the thresholds of the two rules each time they identify the students with weak participation.

After each of the first three checkpoints, the instructor of each course identifies the students whose DIAMOND profiles show weak participation. Among these students, the instructors were asked to select the students who achieved failing grades in the course assessments that were conducted before the checkpoint. Since, in some cases, a student who is weakly participating in the classroom discussions can be compensating by engaging in off-class activities. For this reason, weakly participating students were identified as those who had both 1) weak participation in the DIAMOND participation profile, and 2) an average failing grade in the course assessments. Next, the instructor was required to design and implement a strategy for engaging the weakly participating students more and more in the upcoming lectures, by asking them questions, inviting them to give their opinions on certain matters, assigning them tasks to do and report during the lectures, etc. Each of the five instructors was requested to offer to the weakly participating students more participation opportunities and push them to engage more during the next lectures, without affecting the participation of other students in the class. In addition, each instructor was asked to schedule office-hours meetings for the weakly participating students and discuss with them the reasons for not participating during the lectures. If the instructor finds that a student is not participating due to a personality trait, such as suffering from public speaking anxiety, the instructor arranges with the student affairs office to provide the student with suitable assistance and professional treatment if necessary.

After each of the four checkpoints, we calculate the percentage of weakly participating students among the total number of students in the class. We aim to see whether the strategies that were implemented by the instructors will result in decreasing the percentage of students with weak participation. In addition, we want to observe if the average participation percentage per lecture will increase gradually after each checkpoint. In general, our goal is to observe whether monitoring the participation of students during online lectures and taking corresponding measures to help students who are not participating enough and failing the course assessments will result in better participation during future lectures and will eventually help them to pass.

Results and discussion
After each of the four checkpoints that were described in Sect. 4, we collect from the instructors of the five courses the students’ participation profiles that are generated by DIAMOND. The participation profile of a student contains the student’s participation seconds and percentage for each lecture before the checkpoint. In order to determine if the student is weakly participating or not, we calculate the average number of participation seconds for all lectures in the participation profile, and compare the result with the threshold of the first rule that was stated in Sect. 4. We call this threshold Th1, and we give it a default value of 60 s. If the instructor decides to change Th1, we set its value to that chosen by the instructor. Also, we calculate for each student the percentage of lectures in which he/she did not participate (i.e., zero seconds participation), and we compare the result with the threshold of the second rule, which we call Th2. Similar to Th1, Th2 was set to a default value of 30%. However, we change the value of Th2 to that set by the instructor if this happens. The student is considered weakly participating if his/her average number of participation seconds is less than Th1, or if his/her percentage of zero participation lectures is greater than Th2, or both. Also, the student must have obtained an overall failing grade on the course assessments before the checkpoint. Students who are passing the course are not considered as weakly participating.

We study in this section four performance parameters, which are: 1) NS1, which is the number of students in the class whose average participation seconds is less than Th1, 2) NS2, which is the number of students whose percentage of zero-participation lectures is greater than Th2, 3) PWP, which is the percentage of students in the class with weak participation, and 4) AP, which is the average participation minutes for all students in all the lectures of the course that were analyzed at the checkpoint. Here, NS1 gives us the number of students who are weakly participating on average per lecture (Rule 1), NS2 gives us the number of students who are not participating in a lot of lectures (Rule 2), PWP shows us the percentage of students in the class who are weakly participating (due to Rule 1, Rule 2, or both), and AP allows us to compare the average number of participation minutes per lecture at different checkpoints to see if the overall participation is increasing or not. The four parameters were calculated for each course at each checkpoint. Note that we exclude from the calculations of the four parameters the data of the students who were absent in the lecture. In addition to these parameters, we present a section in which we compare the grades of the students after each checkpoint in order to observe the effect of our method on the students’ performance.

Results of NS 1
The total number of failing students whose average participation seconds per lecture is less than Th1 (i.e., NS1) is shown in Fig. 7 for the five courses. For each course, the corresponding graph shows the value of Th1 that was set by the instructor (or the default value if the instructor did not change it). The graph of each course illustrates the value of NS1 after each checkpoint. We can see from the figure that NS1 decreases after each checkpoint for all five courses, which proves that the number of failing students who are weakly engaging during the lectures decreases on average as the number of lectures increases. In other words, more students who were weakly engaging in the previous lectures are participating more often in the next lectures and their participation seconds become above Th1, which moves them from weakly participating students to actively participating ones. In addition, these students (i.e., who did not appear in the later checkpoints) were obtaining an average failing grade but their grades improved and they started to pass the course (since only failing students at each checkpoint are examined).

Fig. 7
figure 7
Number of students after each checkpoint whose average participation seconds is less than Th1

Full size image
The five courses that were studied in the testing of DIAMOND showed that the instructors’ strategies in pushing the weakly participating students to engage during the next lectures succeeded to a high degree. If we examine the graphs in Fig. 7, we can see that NS1 in the COM490 course decreased from 3 students after the first checkpoint to only 1 student after the course finished (the course contained seven students). Similarly, the NS1 of IDS255 decreased from 4 to 0 students (total of 11 students), the NS1 of CCN211 decreased from 8 to 2 students (total of 15 students), the NS1 of MCE202 decreased from 11 to 2 students (total of 22 students), and the NS1 of MKT420 decreased from 17 to 7 students (total of 32 students). The fact that some students in the five courses remained weakly participating after the courses finished and failed the course illustrates that they did not respond to the instructors’ engagement strategies for various reasons. Such students require dedicated assistance, and a plan should be implemented by the university administration to discover the problem or condition of each student and maybe assign a special assistant or advisor in order to solve their problems or conditions. If not monitored and discovered, the weak participation of such students is most likely to continue in future courses, which will hinder their graduation chances.

We notice in the graphs of Fig. 7 that each of the five instructors chose his/her own value of Th1. While three instructors (those of CCN211, MCE202, and MKT420) chose to keep the default value of Th1 (60 s), the instructor of COM490 changed it to 120 s, and that of IDS255 changed it to 90 s. There are two main factors that play a role in defining the best value of Th1, which are the number of students in the class and the average amount of participation opportunities in the course lectures. As the number of students increases, the participation time within the lecture needs to be divided among a larger number of students. Hence, the participation time per student is expected to decrease on average. This results in a smaller value of Th1. On the contrary, if the number of students in the class is small, more participation time will be dedicated to each student, and hence; Th1 should be increased. As for the second factor, if the course material and the pedagogical procedures applied by the course instructor allow students to participate often enough, then Th1 should be increased. In contrast, if the course material and/or the instructor's teaching methods do not allow students to participate a lot, then Th1 should be decreased. In the five courses, we notice that the COM490 course contains only 7 students. In addition, the teaching methods of this course require students’ frequent interactions and participation. Hence, the course instructor chose to increase Th1 to 120 s. A similar approach was done by the instructor of the IDS255 course. The remaining courses contained a larger number of students, and their materials do not offer participation opportunities as much as those of the first two courses. Hence, the instructors of these three courses chose to keep Th1 at its default value. In general, the decision of each of the five instructors was made after he/she observed the participation of his/her students in the first seven lectures (i.e., after Checkpoint 1), which gave him/her an overview of the overall participation in the class, and helped him/her in defining the best value of Th1.

Results of NS 2
The graphs in Fig. 8 illustrate the number of failing students at each checkpoint whose percentage of lectures with zero participation is greater than Th2. Each graph shows the values of NS2 for a certain course after each of the four checkpoints. Similar to NS1, we notice that NS2 decreases as the number of lectures increases in all five courses. For COM490, NS2 decreases from 2 students after the first checkpoint to zero students after the course is finished. Similarly for the other four courses: for IDS255, NS2 decreased from 5 to zero students, for CCN211 it decreased from 5 to 1 student, for MCE202 it decreased from 8 to 2 students, and for MKT420 it decreased from 12 to 1 student. This decrease proves that the instructors’ strategies to engage the students more resulted in a much less number of students not participating in each lecture (i.e., a fewer number of students with zero-participation); and hence, the percentage of zero-participation lectures decreased on average as the number of lectures increased.

Fig. 8
figure 8
Number of students after each checkpoint whose percentage of zero-participation lectures is greater than Th2

Full size image
Similar to the results in Fig. 7, we notice from the graphs in Fig. 8 that some students in the five courses continued to have a high percentage of zero-participation lectures after the courses finished and failed the course (2 students in MCE202 and one student in CCN211 and MKT420). As we will observe in the results of the next section, all these students were also included in NS1 (i.e., they satisfied the conditions of both Rule 1 and Rule 2). Among the students who were mentioned in the previous section who need special attention in order to push them to participate and engage more in future courses, these students who matched both Rule 1 and Rule 2 are the most severe cases who require rapid assistance and more care. This is due to the fact that their participation is very weak, since both their average participation seconds is still low and their percentage of zero-participation lectures is still high after the end of the courses, regardless of the big efforts that were made by the instructors to encourage them to partake and interact during the online lectures.

With respect to Th2, we notice that two instructors (those of the CCN211 and MCE202 courses) kept Th2 at its default value. On the other hand, the instructors of COM490 and IDS255 decreased it to 20% and the instructor of the MKT420 increased it to 40%. As we previously stated, the COM490 and IDS255 courses contain a lot of participation opportunities and their number of students is small, which should make each student participate in most lectures. Hence, the instructors of these two courses believed that a percentage of zero-participation lectures greater than 20% should indicate weak participation. On the other hand, the instructor of the MKT420 course noticed that with the large number of students in the class, not all students have a chance for participating in each lecture. Hence, she increased Th2 to 40%.

Results of P WP
The results of Sect. 5.1 give us the number of failing students with weak participation due to Rule 1, while those of Sect. 5.2 show the number of failing students who are weakly participating according to Rule 2. However, as we stated before, there are some students who will satisfy both Rule 1 and Rule 2. In this section, in order to know the total percentage of weakly participating students in each course, we calculate PWP after each checkpoint by adding the number of weakly participating students due to Rule 1, Rule 2, or both (i.e., a student is added only once) and then dividing the result by the total number of students in the class. The results of PWP for the five courses are shown in Fig. 9. We notice from the graphs in Fig. 9 that PWP decreases exponentially for all five courses. For COM490, PWP decreases from 57% after the first checkpoint to 14% after the course ends; for IDS255, it decreases from 55 to 0%; for CCN211, it decreases from 53 to 13%; for MCE202, it decreases from 50 to 9%; and for MKT420, it decreases from 59 to 22%.

Fig. 9
figure 9
Percentage of weakly participating students after each checkpoint

Full size image
The results of PWP illustrate the excellent success of the strategies that were applied by the instructors to increase the participation of weakly participating students and make them pass the course at the end. This success would not be achieved if the instructors did not identify the failing students who are weakly participating at the early stages of the course and gave them more engagement and participation opportunities. This shows the importance of using the DIAMOND tool in order to identify these students. In addition, when the instructor calculates the participation profiles of the students at several stages during the course and saves them to the Moodle database, the university administration is able to track the participation of each student and identify the students who are participating well all the time, those who are improving their participation, and those who are facing problems or have certain conditions that are preventing them from participating well. By examining the participation profile and the grades of each student, the administration can devise a plan or a solution for each of the weakly participating students.

Results of AP
The average participation per lecture (AP) of the students of each course during all lectures that are given at each checkpoint is shown in Fig. 10. The AP is computed by calculating the average participating seconds for all students in each lecture, taking the average for all lectures before the checkpoint, and then converting to minutes. Contrary to PWP, the AP increases gradually for all courses. For COM490, it increases from 5.53 after the first checkpoint to 7.27 min after all lectures are finished. Similarly for the other four courses: for IDS255, the AP increases from 2.54 to 4.43 min; for CCN211, it increases from 1.9 to 3.1 min; for MCE202, it increases from 2.1 to 4 min; and for MKT420 it increases from 2.28 to 4.15 min.

Fig. 10
figure 10
Average participation minutes per student after each checkpoint

Full size image
We confirm from the AP results the observations that we stated before: the COM490 and IDS255 courses offer more participation opportunities for students and the number of students in each of these courses is low. Hence, the average participation per student in these courses is high. On the other hand, the CCN211 course does not offer a lot of participation opportunities for the students. This can be confirmed by observing that although the number of students in this course is not high, the average student participation minutes is low compared to the other courses, which shows that the CCN211 course instructor should increase the overall amount of participation and collaboration activities during the course lectures. On the other hand, although several students in the MKT420 course are weakly participating (as shown in the graphs of MKT420 in Figs. 7, 8, and 9), there are many other students in the course who are excellently participating during the lectures, since the average participation minutes per student reached 4.15 min at the end of the course. This shows that the MKT420 instructor is giving sufficient participation opportunities, but some students are not making use of them, as we previously illustrated.

Students’ grades
In this section, we examine how the average class grade in each of the five courses varied after each checkpoint. First, we note from the previous sections that we can deduce the total number of failing students who weakly participated in the lectures from PWP (by multiplying PWP with the number of students in the class). From Fig. 9, we can see that our method helped 3, 6, 6, 9, and 12 students to pass the five courses respectively. Each of these students was failing the course assessments after the first checkpoint, but was able with the help of the instructor participation strategies to increase his/her participation and eventually pass the course.

Figure 11 shows the average class grade after each checkpoint. We can see that the class average increased by 9%, 10%, 12%, 24%, and 15% (between the first and the last checkpoints) for the five courses respectively, which illustrates the effect of the method that we described in Sect. 4 on the average grades of the students. Combined with the number of failing students that we described in the previous paragraph, we can deduce that our method was very effective in enhancing the students’ performance and helping them to pass their courses.

Fig. 11
figure 11
Average class grade after each checkpoint

Full size image
Instructors’ survey
In order to obtain the feedback and impressions of the instructors who utilized the DIAMOND tool, we conducted a short survey among the 14 instructors who have used the tool so far. The survey contained the following five questions that were answered via the traditional 5-point Likert Scale:

The DIAMOND tool helped me to monitor the participation of each student in each lecture

The DIAMOND tool helped me to know if I am giving the students enough participation opportunities

The DIAMOND tool enabled me to detect the lectures in which I should focus on engaging students more

Using the DIAMOND tool enabled me to define the students who require dedicated help

Using the DIAMOND tool had a good effect on the student's overall engagement and performance

In addition, the survey contained two open-ended questions: 1. “Please indicate the most important advantage of the DIAMOND tool”, and 2. “What suggestions do you have for improving this tool?”.

The results of the closed-ended questions are shown in Table 1. We can see that all instructors agreed on the first point. Among the 14 instructors, 11 agreed that DIAMOND helped them to detect if they should give students more participation opportunities and 9 agreed that they used DIAMOND to know the lectures that need more participation activities. 13 among the 14 instructors agreed that they used DIAMOND to detect students who require help, and 12 instructors agreed that their utilization of DIAMOND helped them to improve the performance of their students.

Table 1 Results of the first part of the Instructors’ Survey
Full size table
With respect to the open-ended questions, we combined the instructors’ similar answers together. The answers to the first question were as follows: four instructors stated that the most important advantage of DIAMOND is that it offers the instructor the ability to obtain an overview of the participation of each student in each lecture. Four instructors stated that DIAMOND’s main advantage is that it helps the students to improve their performance and obtain better grades after they start to participate more in the lectures, three instructors mentioned that they consider the most important advantage is that DIAMOND helps the instructors to identify the students who are facing difficulties following up during the lectures, two instructors stated that DIAMOND urges the instructor to increase the participation activities, and one instructor answered that he considers the most important advantage is that both instructors and students become more aware of the importance of students’ participation in each lecture.

As for the second open-ended question, some instructors suggested adding a text-to-speech feature to the voice diarization to be able to identify fruitful participation. Other instructors mentioned that we need to identify a standard acceptable participation level for each course to help the instructors calculate the ideal values of Th1 and Th2, and some instructors asked to combine DIAMOND with other tools such as monitoring Moodle’s log-in files in order to track the students’ participation from several perspectives. Finally, one instructor suggested that we should provide the participation statistics after each lecture to the students so that each student can compare his/her participation with other students. Note that all these suggestions will be considered for future work in order to improve the DIAMOND tool.

Students’ survey
A very important factor in identifying the benefits and limitations of the DIAMOND tool is the students’ viewpoint and judgment. In this section, we conducted a survey among all students who were identified as weakly participating after the first checkpoint. These students were selected from the fourteen courses in which DIAMOND has been tested so far. The total number of students who participated in the students’ survey is equal to 93. Before giving the survey to the students, we explained to them how the DIAMOND tool works, how they were selected by the instructors because of their weak classroom participation, and the strategies that the instructors made to help them improve their participation. The survey contained six Likert Scale close-ended questions and one open-ended question. The close-ended questions are:

The instructor of the (X) course told me that I need to increase my participation during the course lectures

The instructor met me during office hours to discuss my participation in the course lectures

The instructor helped me to participate more in the discussions during the lectures and engage in the class activities

With the help of the instructor, I became keener to express my thoughts and opinions during the lectures

My increased participation during the lectures played a role in improving my grades in the course assessments

The experience that I gained due to the implementation of the DIAMOND tool made me more aware of the importance of classroom participation

On the other hand, the open-ended question is: “What suggestions do you have for improving the DIAMOND tool?”.

The results of the close-ended questions are shown in Table 2. From the table, we can see that the percentage of students who answered with “strongly agree” or “agree” ranged between 59% (question number 4) and 86% (question number 1). On the other hand, the percentage of students who answered with “disagree” or “strongly disagree” ranged between 7.5% (first question) and 32% (question number 4). The results in Table 2 show that most students did consider the DIAMOND tool helpful in improving their classroom participation, their awareness of the importance of participation, and their overall grades. In addition, 66% indicated that the instructor strategies helped them to participate more (third question), and 59% agreed that they became keener to express their thought and opinions during the lectures (fourth question). Although not all the instructors were able to discuss the participation issue with all their students during office hours; as the results of question number 2 indicate, most students were aware that the instructor wants them to increase their classroom participation; as can be deduced from the results of the first question.

Table 2 Results of the first part of the Students’ Survey
Full size table
With respect to the open-ended question, we received a large number of suggestions from the students. We selected the important ones and summarized them. Many students mentioned that the DIAMOND tool should be integrated into the student’s Moodle account so that the student can observe and track his/her participation. Many students stated that DIAMOND should take into consideration several factors that affect the student’s participation in the lectures, such as the large number of students in the section. Other students stated that some students have special cases (such as personality problems) that should be considered when calculating their participation. Some students requested that the tool be utilized in more courses, while a few students stated that the detection accuracy of the tool should be increased, as they believe they participated more than the tool indicated. A few students answered that the instructor should provide more participation activities during the lectures. Note that we are working on integrating DIAMOND into the students’ Moodle accounts. In addition, the instructors were asked to contact the student affairs department to deal with students’ special cases and needs that affect their participation. Finally, the issue of the participation being affected by the large number of students in the section will be studied in the future in order to determine the most suitable values of Th1 and Th2 based on the number of students in the section, and the best possible methods to increase the students’ participation in such sections.

Overall, we deduce from the numerical and survey results the importance of using the DIAMOND tool to detect students who are weakly participating during online lectures and offer them the appropriate help during the next lectures. From one side, DIAMOND is very helpful for instructors to know important statistics and deduce from them several facts related to the students’ participation, identify the weakly participating students, and help them in order to increase the percentage of successful students in the class. From another side, DIAMOND enables the educational institution administrators to obtain insight into the details of each lecture, such as the overall participation of students during the lecture, the overall participation of each student, the number of students who are not participating, the number of students who are weakly participating, the percentage of the lecture time during which students participated, etc. These figures are very important for the educational institution administrators: they allow them to detect courses in which the participation of students is weak and take the necessary measures. They also allow them to detect students who are not or weakly participating in several courses and help them to overcome their problems. Finally, the described statistics help the administrators to identify the instructors who are offering few participation opportunities during online lectures and help them to transform their lectures into a more interactive and engaging environment.

Conclusion
One of the main challenges that face instructors while giving online courses is related to following up with students during the online lectures and identifying the students who are not participating adequately. If such students are not pushed to engage more during the online lectures, they could face problems with their studies. In this paper, we presented the DIAMOND tool that can be used by instructors to monitor the participation of each student during an online lecture and build a participation profile for the student that indicates his/her average participation time and percentage in all lectures. By calculating the students’ average participation per lecture, the instructor is able to identify the students with overall weak participation and take the necessary measures to help them and solve the problems that are hindering them from participating. In addition, DIAMOND enables educational administrators to check the average participation of students and the total number of students who are not participating in the lectures of each course in order to identify courses in which the number of weakly participating students is high and/or the percentage of students’ participation is low and take the necessary measures to solve any problems or weaknesses that may exist in these courses.

The DIAMOND tool was tested within five online courses. For each course, the instructor generated the participation profiles of the students at four checkpoints during the course. After each checkpoint, the instructor identified from the participation profiles the students who are weakly participating in the lectures that were given before the checkpoint, and implemented a strategy that focuses on pushing these students to participate and engage more during the next lectures. The results showed that identifying weakly participating students and implementing a strategy to encourage them to participate more in the next lectures leads to decreasing the number of weakly participating students and improving the overall grades of the students. In addition, utilizing the DIAMOND tool was vital in reducing the number of failing students by an average of 43%.

For future work, we will work on combining the DIAMOND tool with another one that analyzes the online learning platform that is used by the students in order to obtain an overall overview of the student engagement in the course. The intended tool will be used to calculate the off-class engagement by monitoring the student activities and interaction with the online course material (for example, the total time spent on online studying, number of actions taken by the student while opening the online platform, etc.). On the other hand, DIAMOND will be used to measure the student’s in-class participation. This allows the instructor to obtain a more accurate figure of the student’s engagement. Another important future work will be integrating a speech-to-text module within the diarization process in order to exclude speech segments that are not considered as educational discussion from the diarization analysis. As it is well known, there are cases in which classroom discussions deviate from the course topics to unrelated matters. Our next goal is to be able to identify these speaking periods that should not be considered as participation and remove them from the diarization process. In order to do that, we are building a new layer on top of our speech recognition model. This new layer includes two main modules: a speech-to-text module that translates each speech segment into words. The second module is a machine learning algorithm that compares the words of the translated segment with the various texts in the course materials to produce a similarity score and hence deduce the percentage of relationship between the analyzed speech segment and the course material. This enhancement to the DIAMOND tool is very important for making the diarization analysis include only course-related discussions that can be considered as participation and removing the speaking segments that are general discussions on course-unrelated material.