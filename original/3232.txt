With the rapid development of web-based applications, clicking on hyperlinks has become a general means for accessing various network services. Understanding the visiting behavior of web users not only helps improve the personalized service quality and user experience, but also plays an important role in network management and early threat detection. Click-stream identification is a fundamental issue for user behavior analysis. However, most existing approaches are designed for non-encrypted HTTP requests and only focus on server-side scenarios, which makes them inapplicable to the increasingly popular HTTPS and network-side management. In this work, we propose an encryption-independent scheme from a network-side perspective that adopts the web traffic collected at the network boundary to identify the HTTP(S) requests generated by the click actions of web users. The proposed scheme employs hidden Markov models (HMMs) to describe the time-varying behavior of click and non-click web traffic. A deep neural network (DNN) is integrated into the HMMs to capture the context of web traffic, which eliminates the limitations caused by the independence hypothesis of the traditional HMMs. Finally, a DNN-based rear classifier is proposed to determine the type of HTTP(S) requests according to the fitting degree between the HTTP(S) requests and the HMM-based behavior models. We derive the algorithms for model learning and click identification. Experiments are conducted to validate the proposed approach. Performance-related issues and comparisons are discussed. Results show that both the average precision and recall rate of the proposed approach exceed 92%, which is better than most existing benchmark methods in terms of performance and stability.

Previous
Next 
Keywords
Web traffic

Click

Traffic behavior

Hidden Markov model

Deep neural network

1. Introduction
User behavior analysis (UBA) has been widely used to identify customers (Zaim et al., 2019), realize personalized service (Jiang et al., 2019), improve service quality (Wang et al., 2017), monitor and filter the access behavior (Peng et al., 2016, Cao et al., 2020), detect anomalies (Cao et al., 2017) and track the illegal collection of personal information in mobile applications (Liu et al., 2020). In these applications accurately extracting the click-streams is a fundamental issue to be addressed.

A click-stream comprises only those HTTP(S) requests that correspond to the web objects clicked by users. As a click-stream can directly indicate the concerns, intentions, and actions of web users, it has received much attention in UBA. Click-streams are currently extracted using three types of methods. The first type is designed for the server-side or service provider. One of the well-known methods is based on the type of accessed files (Cooley et al., 1999, Huiying and Wei, 2004, Chitraa et al., 2013) and the scripts embedded in the web pages (Silverstein et al., 1999, Spink et al., 2005, Rafter and Smyth, 2001). For example, htm/html files are usually considered to have a greater probability of being clicked by users than other elements of web pages (Anand and Aggarwal, 2012), while the scripts and controllers embedded in webpages can be used to record the mouse actions of users (Benevenuto et al., 2009). In addition, some methods based on machine learning are also used for click identification (Xie and Yu, 2008, Xu et al., 2013). However, the main issue of these type of methods is that they can only record the access paths on a specific web site rather than the complete click-trajectories of users. Considering that a user’s web surfing usually involves multiple web sites and servers, client-side methods have been developed to obtain the complete click-actions of users. For example, plugins or resident codes are implanted into the terminals of web users to obtain their click-actions and targets (Atterer et al., 2006, Huang and White, 2010). However, this type of method is not widely accepted because it infringes on the privacy and security of users.

To address the limitations of these methods, network-side methods have received a lot of attention (Schneider et al., 2009, Xie et al., 2013, Ben Houidi et al., 2014, Vassio et al., 2016, Rizothanasis et al., 2016). In these works, information located in the fields of HTTP header is measured from the web traffic and is adopted for click-request identification. Commonly used fields include “URL”, “Referer”, “Content-Type” and “User-Agent”. Compared with the methods designed for the server-side and the client-side, the network-side methods based on the information of the HTTP header show better performance because they can reconstruct a user’s complete click-trajectory when (s)he is surfing from one web site to another, particularly for those scenarios wherein management strategies are required to be adjusted based on the click-behavior of users, e.g., enterprise networks. However, the popularity of HTTPS has brought new challenges to these methods. As HTTPS encrypts the entire HTTP message, HTTP header-based methods cannot correctly access the required information. Therefore, the traditional network-side identification methods gradually become unworkable. Although the encryption of HTTPS does not involve most of the information used for UBA, such as the source/destination IP/port, the target host, the size of request/response, and the session duration, UBA on the network-side becomes extremely difficult because of the lack of effective methods in identifying click-requests from the HTTPS traffic.

To tackle this challenge, in this work we propose a new scheme to identify click-requests on the network-side, such as the access networks and the backbone networks of the Internet Service Providers (ISPs). As usually only encrypted HTTPS traffic can be observed on the network-side instead of plaintext HTTP, we achieve the click-request identification through traffic behavior without involving any information of the payload. Considering that there are only two possible sources for each HTTP(S) request, i.e., an HTTP(S) request can only be generated by a user’s click-action or emitted automatically by a browser to request the embedded element of the web page, the essence of this scheme is to solve a binary classification problem for the HTTP(S) traffic. In contrast to existing works, the proposed scheme utilizes the inherent behavior characteristics of each type of the HTTP(S) request for identification rather than relying on the plain text information of the HTTP header. Consequently, the proposed scheme is encryption-independent and does not involve the privacy of users. To this end, we adopt two hidden Markov models (HMMs) to describe the time-varying behavior of the web traffic driven by the click activities of users and the automatic operations of browsers, respectively. Furthermore, a deep neural network (DNN) is integrated into the HMMs to capture the context of the web traffic and eliminate the limitations caused by the independence hypothesis of the traditional HMMs. Finally, a DNN-based rear classifier is proposed to determine the type of the HTTP(S) requests according to the fitting degree between the HTTP(S) request and the behavior models. The main contributions of this work are threefold.

•
A new behavior-based classification scheme is introduced for click-request identification that is suitable for UBA on the network-side and is encryption-independent.

•
A new HMM-DNN integrated model is proposed to capture the time-varying behavior patterns of click and non-click web traffic, which makes full use of the context-related information of the HTTP(S) request to be identified.

•
New algorithms that do not rely on the Gaussian mixture models are derived for model learning and click-request detection. Experiments are conducted to validate the proposed approach.

The rest of the paper is organized as follows. In Section 2, we review the related works. In Section 3, we introduce the rationale of the proposed scheme. Experiments and performance evaluations are shown in Section 4. We discuss the performance related issues of the proposed approach in Section 5 and finally summarize our work in Section 6.

2. Related work
We summarize three types of methods for click-stream identification, including the server-side, the client-side, and the network-side.

The server-side methods have been well studied in the past two decades. The simplest way is using the suffix name of the accessed files to identify the click actions. For example, HTML files are considered to have a high probability of being accessed by click actions of users, whereas the JPGs and the GIFs are usually automatically requested by browsers (Cooley et al., 1999); the CGIs and the “robots.txt” are often accessed by the scripts (Huiying and Wei, 2004) and the crawlers (Anand and Aggarwal, 2012, Chitraa et al., 2013), respectively. According to these experiences, click-requests can be identified by filtering the files that are unrelated to the click-actions of users. Although this type of methods is simple and intuitive, it lacks versatility. For instance, on a web site that provides image download services, JPG files are likely to be clicked by users, which makes the file suffix unusable. In addition to the file name, another alternative is to use embedded scripts and controllers to capture the behavior of users. For example, scripts are embedded in search engines (Silverstein et al., 1999, Spink et al., 2005) to log users’ search behavior. The basis of this method is that the HTTP(S) requests driven by the click actions of users usually include submission information, such as query terms and user-specified modifiers, but those issued automatically by the browsers usually do not carry additional parameters with the URLs. According to this assumption, researchers use scripts to record requests with parameters and label them as users’ click-requests. Rafter and Smyth (2001) proposed a system that builds a user profile from analyzing the server log of a job search site. In this system, each line of the log file records an action of user-click, which is obtained by ignoring requests unrelated to the job service. Another similar example comes from social networking applications. A social network aggregator (Benevenuto et al., 2009) provides a common interface for users to access their accounts in various online social networks and directly obtain the click-stream data of users through the embedded scripts. In addition, machine learning is also used for click identification on the server-side. For example, hidden semi-Markov model is adopted to model the arrival process of the HTTP requests observed on the server-side (Xie and Yu, 2008, Xu et al., 2013). Based on the hyperlink relationship between the web pages provided by the web site, the model can be well trained and used for click identification. The main limitation of these server-side methods is that they can only extract the local click-paths for a specific web site instead of the full click-trajectories on the Internet. Moreover, some factors such as the proxy server and browser cache may affect the integrity of the server data, leading to incomplete click-paths.

In order to obtain a complete click-path of each web user, client-side methods have been developed. For example, a web proxy is deployed in an enterprise network and inserts scripts into each web page forwarded to the users to retrieve the mouse and keyboard events of users (Atterer et al., 2006). Another common method is to install plugins in the browser to record the interaction information between users and servers, e.g., “Timestamp”, “URL” and “Referer” (Huang and White, 2010). Although the click-streams can be obtained directly by implanting codes in the terminals of users, the client-side methods are often criticized because they involve user privacy and system security.

To address the limitations of the above two methods, network-side methods have attracted widespread attention. In these methods, the fields of HTTP header are widely used for click-request identification, e.g., “Content-Type”, “Referer” and “URL”. In Schneider’s work (Schneider et al., 2009), regular expressions were used to represent the URL patterns for click and non-click HTTP requests and were further employed to filter and identify the click-requests.  Xie et al. (2013) assumed that each click-request should have at least  embedded objects. Based on this assumption, they first used the field of “Referer” to determine the hyperlink relationship between the accessed web objects and then identified the click-stream by using the rules with a predefined . In Ben’s work (Ben Houidi et al., 2014), the documents indicated by the “Referer” field of a request were regarded as a click-request. Then four filtering mechanisms were designed, including F-Referer, F-Children, F-Type, and F-Ad. F-Referer was used to filter all possible click-requests based on the “Referer” field. F-Children was responsible for eliminating those requests with a small number of embedded objects. F-Type and F-Ad were used to further exclude non-click requests by the predefined filename suffixes and advertising blacklist, respectively. In Vassio’s work (Vassio et al., 2016), each request was described by  features selected from the HTTP header, including the “Content Type”, the length of the “URL”, and the number of children web pages derived from the main document. These features were divided into  decision trees that determine the click requests by voting. According to our survey, most existing works rely on the information from the HTTP header, which causes them to be used only for non-encrypted traffic instead of HTTPS scenarios. In addition, even under non-encrypted conditions, some HTTP header fields may be unavailable because they are hidden for security reasons (Ruiz-Martnez, 2012), e.g., the “Referer” field. In order to deal with the HTTPS traffic, Rizothanasis proposed a method based on time thresholds (Rizothanasis et al., 2016), where a request was determined as a click-request if the time interval between it and the two adjacent requests before and after is greater than the expected thresholds. The previous interval represents the duration between two consecutive click operations, whereas the next interval means that the related inline requests will appear at least one RTT after clicking. The limitation of this method is that it relies on the predefined thresholds, which affects its versatility and makes it unsuitable for the dynamically changing network scenarios.

In summary, server-side methods cannot obtain a complete click-trajectory of a user, while the client-side methods infringe on the privacy and security of users. Because most existing network-side methods rely on the plaintext information in the HTTP header, they do not apply to the increasingly popular HTTPS.

3. The proposed scheme
3.1. Rationale
Fig. 1 shows the basic framework of the proposed scheme. We start with a simple scenario in which a user clicks a hyperlink of a webpage. As shown in the upper left part of Fig. 1, the interaction process can be abstracted into four steps. First, the user’s click-action triggers an HTTP(S) request to obtain the main page, i.e., the arrow labeled by ① in Fig. 1. Second, the server returns the main web document that contains the hyperlinks of all embedded objects to the user, i.e., the arrow labeled by ② in Fig. 1. Third, the user’s browser automatically issues two HTTP(S) requests for all embedded objects according to the hyperlinks given in the returned document, i.e., the arrow labeled by ③ in Fig. 1. Finally, the browser obtains all inline objects from the server and renders them on the screen, i.e., the arrow labeled by ④ in Fig. 1. Based on the single-page access, we can further describe the process of continuously accessing multiple pages, and illustrate it in the upper right part of Fig. 1. In this process, there are three elements related to this work, including the clicked objects, requested objects, and the HTTP(S) request sequence from user to server. The clicked objects refer to the targets directly requested by the click actions of users, e.g., the HTML files. The requested objects represent a series of required web resources that are related to a clicked object, including the clicked object and its embedded objects. The HTTP(S) request sequence from user to server refers to the HTTP(S) traffic corresponding to the requested objects.


Download : Download high-res image (285KB)
Download : Download full-size image
Fig. 1. Framework of the proposed scheme.

During the visit of page0 shown in Fig. 1, there are two types of HTTP(S) requests, including a click request (CR) and two spontaneous requests (SRs). The CR corresponds to the target web object that is requested via the user’s active click-action, such as the main web document denoted by a rectangle shown in Fig. 1. The SRs are launched automatically by the user’s browser to download the embedded objects of the clicked webpage, such as the requests for objects denoted by the ellipse and the trapezoid shown in Fig. 1. In contrast to existing works, we do not determine the category of each HTTP(S) request by analyzing its features in isolation, but from the perspective of time-varying behavior patterns of the request-flow (RF). Because there are only two types of requests (CRs/SRs), any fragment of an RF can only be one of two situations: starting with a CR or an SR. Therefore, the attribute inference of a single HTTP(S) request can be converted into a category analysis of an RF-fragment, i.e., using sequence analysis instead of individual detection. Theoretically, the sequence behavior can provide more available information for the CR identification than a single request because the former makes full use of the context-related information of the HTTP(S) request for its click attribute identification.

Based on this idea, at the lower right of Fig. 1 we adopt two models to describe the potential behavior mechanisms for the fragments of the CR-sequences and SR-sequences, respectively. Thus, the classification of an RF-fragment can be further solved by evaluating how well it fits the behavior models, which is a typical time-series classification problem. Traditionally, the likelihood of an RF-fragment fitting to a given model has been widely used as an indicator for classification, i.e., the label of the model with the maximum likelihood is considered the optimal category of the RF-fragment to be detected. However, this hard-decision-based method often leads to large deviations. In particular, when the likelihoods of the candidate models have similar statistical probabilities, it is not easy to make an effective distinction. To address this limitation, in this work we do not directly determine the category of a fragment by the values of the fitting degree between it and the behavior models, but achieve the final classification by adopting a DNN-based rear classifier to analyze the joint statistical distribution of the fitting degrees calculated by the behavior models.

To derive an analytical mathematical model, we use two finite state machines (FSMs) to describe the time-varying behavior mechanisms of CR-sequences and SR-sequences, respectively. The states of these FSMs come from a shared discrete state set in which each element represents a specific behavior pattern for launching an HTTP(S) request and controls the external features of the upstream requests. As shown in Fig. 2, for a given RF-fragment, there are two related time-varying processes: an observable feature sequence and a hidden state chain. The observable feature sequence denotes the external attributes of the RF-fragment starting with the HTTP(S) request to be identified, whereas the hidden state chain describes the underlying time-varying behavior evolution mechanism of an observed RF-fragment and controls its external behavior features. The transition between two adjacent states represents a switch in behavior patterns of the RF-fragment. In most practical applications, the hidden state chain is usually unmeasurable and can only be estimated by the observable feature sequence.


Download : Download high-res image (188KB)
Download : Download full-size image
Fig. 2. Modeling the traffic behavior by FSM.

Similar to most machine-learning-based solutions, the proposed scheme consists of two phases. The first phase is model learning, which estimates the parameters of both the FSM-based behavior models and the DNN-based rear classifier. The second phase is real-time detection based on the trained behavior models and classifier. For a given HTTP(S) request , identification of its click attribute is equivalent to the category analysis of an RF-fragment that starts with . The category analysis of an RF-fragment can be further solved by two steps: (i) evaluating the fitting degree of the RF-fragment to each behavior model and (ii) utilizing the rear classifier to evaluate the values of the fitting degree and determine the click attribute of the RF-fragment.

3.2. Formulation
Let 
 denote the th HTTP(S) request of an RF-fragment with length . Let 
 and 
 denote the random variables for the observable feature vector and the corresponding hidden state of 
, respectively. Accordingly, we use the lowercase variables 
 and 
 to denote the instances of 
 and 
, respectively. Thus, for an RF-fragment 
, 
 and 
 form a complete sequence-pair to characterize the time-varying behavior of 
, including the external observable traffic features and internal unmeasurable driving mechanisms. We further define an initial state function 
 to denote the probability that behavior pattern  is selected to generate 
: (1)
where  is the set of all possible hidden states.

Theoretically, any two elements in the hidden state sequence 
 are related and interacting, i.e., the choice of any state 
 requires a comprehensive consideration of the impact of the other states 
. To highlight the modeling approach of the proposed scheme, we only take the first-order neighboring relationship as an example to introduce the modeling scheme, i.e., the hidden state of an HTTP(S) request is only affected by the state of its adjacent previous request.1 To this end, the interaction model of behavior patterns can be described by Eq. (2): (2)

Let 
 denote the conditional probability for the adjacent behavior patterns: (3)

We further use the output probability function 
 to describe the statistical relationship between the underlying behavior pattern  and the observed traffic features 
 of 
: (4)
where 
 denotes the D-dimensional observation vector of 
.

These formulas form a typical first-order HMM (Rabiner, 1989). Although classic HMMs have shown excellent performance in time-series modeling, they suffer from three innate limitations. First, the independence assumption widely adopted by HMMs causes the model to be unable to describe the context of observations. Second, in most HMMs the emission probability function 
 is usually limited to common statistical distributions such as the Gaussian mixture model (GMM), which makes it difficult to describe the complex and nonlinear relationships between the hidden states and the observations. Moreover, it is difficult for the classic HMMs to process the high-dimensional observation vectors.

Thus, we develop a new context-dependent HMM based on DNN. Different from the definition of 
, we define a context-dependent posterior probability function 
 by Eq. (5): (5)
where  and  represent the lower and upper limits of the RF-fragment’s truncation window, respectively. We further adopt a DNN to calculate the 
, which is shown in Fig. 3. The input layer of the DNN consists of  neuron nodes. Because the observation vector is -dimensional, each neuron of the input layer shown in Fig. 3 is actually composed of  sub-nodes. The output layer consists of  neuron nodes, each of which outputs a posterior probability of the corresponding hidden state when 
 is given.


Download : Download high-res image (187KB)
Download : Download full-size image
Fig. 3. DNN for the emission probability function of HMM.

Consequently, the parameter of the proposed approach consists of three parts: (i) the initial state function and the state transition function of HMMs 
, where  denotes the model instance for CR- or SR-sequences, (ii) the parameters  of the DNN for calculating the context-dependent posterior probability 
, and (iii) the parameters  of the DNN-based rear classifier.

3.3. Parameters and states estimation
Different from the existing GMM-based DNN-HMM designed for speech recognition (Yan et al., 2013) and content type recognition (Tan et al., 2019), in this work, we introduce a new GMM-free training algorithm that makes the proposed integrated HMM-DNN scheme independent from the GMM. We estimate the parameters of the HMM-DNN models simultaneously using a new iteration approach. Fig. 4 shows our parameter estimation method. There are three model instances in this scheme, including two independent HMMs for RF-fragments starting with CRs and SRs, respectively, and a shared DNN for the posterior probability function 
. Considering that both HMMs for CRs and SRs use the same parameter estimation algorithm, except for the training data in the following algorithm derivation, we do not make a distinction between the two HMM instances unless a special note is made.


Download : Download high-res image (260KB)
Download : Download full-size image
Fig. 4. Estimating the parameters of the behavior models.

As shown in Fig. 4, the iterative training of the integrated HMM-DNN model consists of four steps:

•
Step 1: using the training data to initialize the parameters of the models and estimate the hidden states,

•
Step 2: updating the parameters of the HMMs and DNN simultaneously based on the results of the previous iteration,

•
Step 3: inferring the new hidden states for the observations by the updated parameters of the models based on the Viterbi algorithm  (Forney, 1973).

•
Step 4: looping until the stop condition of the iteration is met.

To achieve the above method, we let 
 and 
 denote the forward and backward variables of the  sequence sample, respectively, where  is the size of the training CR-fragments or SR-fragments. The definitions and recursive formulas of 
 and 
 are given by Eqs. (6), (7), respectively. (6)
 
  (7)
 
 where 
 denotes the length of the th sample and 
 
. 
 
 and 
 are the priori probabilities of the observed feature 
 and the hidden state , respectively.

Let 
 denote the posterior joint probability of two adjacent hidden states 
 and 
 given the feature sequence 
: (8)
 
 
 is the likelihood of the th sequence sample relative to the model , which acts as the normalization factor and is calculated using Eq. (9): (9)
 

Let 
 denote the posterior probability of 
 given the feature sequence 
: (10)
 

Based on the forward–backward algorithm (Rabiner, 1989), the initial state probability function 
 and the state transition probability function 
 of the HMM-DNN can be estimated by Eqs. (11), (12), respectively. (11)
 
(12)
 

Given the parameters of 
 and the posterior probability 
, we can infer the hidden state sequence for each RF-fragment via the Viterbi algorithm. For a feature sequence 
, let 
 denote the maximum probability of the underlying state path that accounts for the first  requests and ends in state . The definition and the recursive formulas of 
 are given by Eq. (13): (13)
 
 
 

Let 
 record the state of 
 of the path with the maximum probability: (14)
 
 

Then, we can obtain 
 by backtracking the state path: (15)
 
 

To evaluate the fitting degree between the model and the data, we define an average logarithmic probability  by (16)
 
 
 

In the above iterative operations, we require two functions: the coefficient 
 and the posterior state probability 
. 
 is calculated using the priori probabilities of 
 
 and 
. For a given training data set, 
 
 is a constant that can be obtained by the statistical distribution of the observations, and 
 can be estimated by the frequency of each hidden state based on the state sequences inferred by the Viterbi algorithm after each iteration.

Different from the traditional HMMs that use regular statistical distributions (e.g., GMM) or a discrete state output matrix to describe the conditional probability 
, here we adopt the context-related posterior probability function 
 and calculate it using a DNN. The DNN consists of a full-connection BP neural network with  layers, including one input layer,  hidden layers, and one output layer (Jain et al., 1996). The numbers of neurons in the input layer and output layer are equal to  and , respectively. Let 
 (
) denote the output of the th neuron in the th layer, 
 (
) denote the connection weight between the th neuron of the  layer and the th neuron of the th layer, and 
 (
) denote the offset of the th neuron in layer . Then, the output of the th neuron in layer  is calculated by Eq. (17): (17)
where 
 is hyperbolic tangent.

Given a training data 
 and its hidden state 
 obtained by the Viterbi algorithm, the purpose of DNN learning is to realize the following equation by adjusting its weight parameters: (18)
 
 

To evaluate the error between the theoretical output and the DNN’s actual output, we define the cross-entropy  by Eq. (19): (19)
 
where 
 is the total number of observation vectors for DNN training. 
 and 
 are the actual output and the theoretical value of the th neuron node in the DNN’s output layer, respectively.

The weights and offsets are updated using the BP algorithm (Rumelhart et al., 1986) at every iteration as follows: (20)
 
 
 
 
 where  and  denote the learning rate and the cross-entropy, respectively. The updated items are calculated by (21)
 
 
 and the recursive formula of 
 is given by Eq. (22). (22)
 where 
 denotes the input of the th neuron in the th layer. The training for the DNN stops when the cross-entropy reaches the expected threshold.


Download : Download high-res image (137KB)
Download : Download full-size image
We show the complete pseudocode for the parameter training of the integrated HMM-DNN model in Algorithm 1, where the right superscript  is used to label the behavior models of CR-sequences and SR-sequences. The inputs of the algorithm are the observation sequences, the maximum number of iterations , and a minimum positive number .  and  are used to evaluate the convergence of the model and are assigned before implementing the algorithm. The output is the parameters of HMMs and DNN. In the initialization process (2nd line), 
 and 
 can be initialized by prior knowledge or clustering approaches, e.g., K-means. After 
 is initialized by pre-training, it can be used to calculate the 
. Before the stop condition is reached, there are three parts to be trained in each iteration, including two HMMs (lines -) and one DNN (lines 8 and 9). All of them can be executed in parallel, as they are independent of each other. 
 shown in the 10th line is estimated by the Viterbi algorithm, i.e., Eqs. (13)–(15).

Once the parameter estimation of the behavior models is completed, the rear classifier is trained. Let 
 denote the HTTP(S) request to be detected. As shown in Fig. 5, the classifier uses the likelihoods outputted by the two HMM-DNN models to classify the RF-fragment 
 and further determines the click attribute of 
. The proposed rear classifier consists of a fully connected DNN that includes two inputs and two outputs. The inputs are the likelihoods of labeled training sequences calculated through the two behavior models, whereas the outputs are the labels of the RF-fragments to be identified. Similar to the above DNN for the context-related posterior probability function 
, the weight coefficients of the rear classifier can be estimated by general DNN algorithms, such as the BP algorithm.


Download : Download high-res image (119KB)
Download : Download full-size image
Fig. 5. Identifying the CR by the rear classifier.

3.4. Click identification and deployment
Given an RF-fragment 
, the mission of the proposed scheme is to determine whether 
 is emitted by a user’s click-action or the automatic operation of the user’s browser. The pseudocode of click identification is shown in Algorithm 2. The algorithm proceeds in three steps:

•
Step 1: Calculate the posterior probability 
 by the trained DNN (lines -) and the feature sequence 
 of 
,

•
Step 2: Calculate the likelihoods (
) of 
 fitting to the CR and SR behavior models, respectively,

•
Step 3: Using the rear classifier, label the click attribute of 
 based on the likelihoods (
).



Download : Download high-res image (61KB)
Download : Download full-size image
As the proposed scheme is designed for the network-side, it can be easily deployed in actual networks. The deployment involves the following two steps: (i) collecting traffic data from the mirror port of the aggregation switch located at the network and (ii) inputting the collected traffic data into the proposed scheme. The complexity of the entire deployment is mainly contributed by the computation of the models (two HMMs and two DNNs).

4. Experiments
This section evaluates the performance of the proposed scheme on real traffic data provided by a group of volunteers. We develop a simple browser plugin to record the click information (e.g., URL and timestamp) for labeling and install it in each volunteer’s browser that is specially configured for this experiment. When the volunteers are willing to share their click data, they can access the websites through the customized browser. The traffic collected on the mirror port of the aggregation switch is then labeled using the log files of the browser plugins. The collected data were approximately  GB in size, and included 17,863 RF-fragments, 1,046 of which are CR-fragments. We further divide these data into two sets for training and testing.2

4.1. Evaluation metrics
The evaluation metrics used in this work include accuracy, precision (), recall (),  (Singh et al., 2018), and the kappa coefficient () (Peng et al., 2016). The accuracy is used to evaluate the classification performance of the shared DNN, and it is the percentage of the observation vectors that are correctly classified. Other metrics are used to evaluate the overall performance of the models, they are defined by Eqs. (23)–(26), respectively. (23)
 
(24)
 
(25)
 
(26)
 
 where TP, FP, and FN denote the numbers of true positives, false positives, and false negatives, respectively.  is a comprehensive metric that can be seen as the harmonic mean of  and .  is used to measure the agreement between the classification result and the true label.  signifies excellent agreement, while  means poor agreement (Fleiss et al., 2003). 
 and 
 in Eq. (26) are calculated by Eqs. (27), (28): (27)
 
(28)
 
 where TN denotes the number of true negatives. In this work, the CR-fragments are treated as the positive samples.

The data analysis process includes the following steps:

•
Splitting the RFs into equal-length fragments, and extract the feature sequences,

•
Training the HMM-DNN models and the rear classifier,

•
Calculating the likelihoods of the test sequences fitting to the HMM-DNN models,

•
Inferring the click attribute of each test sequence by the rear classifier,

•
Evaluating the performance based on the given metrics.

4.2. Features
Similar to most of the applications based on machine learning, the choice of observation characteristics will affect the final decision. Here, we only adopt three time-related features to verify the performance of the proposed approach. These features are defined by Eqs. (29)–(31). (29)
(30)
(31)
 
 where 
 () denotes the time of the th observed HTTP(S) request 
.  is the length of the backward truncation window used to calculate 
 and 
 for 
. Thus, 
 is the time interval between 
 and 
, and 
 and 
 are the means and the standard deviation of 
, respectively. Fig. 6 shows the cumulative distributions of these temporal characteristics for the CR-fragments and the SR-fragments. From a statistical point of view, these features show significant differences, which indicates their usability in click attribute identification. Moreover, these results are consistent with the basic principles of web access shown in Fig. 1. For example, the interval between a CR and its previous request in the same flow mainly depends on the frequency of the click actions, while the interval between two adjacent SRs is usually very small because they are issued concurrently by the browser. Therefore, these time-related features (
, and 
) are suitable for forming the observation vector 
 for click attribute identification.

To form an observation sequence 
 based on these features, there are four parameters that need to be considered, including the size of the backward truncation window  shown in Eqs. (30), (31), the length of context (), and the size of RF-fragment (). Fig. 7 shows the relationships between these variables and the given request (
) to be identified.

Because  cannot be obtained by calculation, here we employ an alternative method to determine the optimal value of  by investigating the joint entropies (
) (Adi et al., 2017) of 
 and the correlation (
) between requests. Given  samples of RF-fragments and , 
 is calculated by (32)
 
 
where  and 
.  and 
 denote the index and the length of the th sample, respectively. 
 is calculated by (33)
 
where 
 is the total number of requests. 
 represents the number of requests with the same “Host” as 
 in the subsequent  requests. Fig. 8 shows the curves of 
 and 
 vs.  for CRs and SRs. 
 and 
 are normalized by Z-scores (Kreyszig, 2011), respectively. The result shows that both the entropies of the CRs and SRs gradually increase and tend to stabilize as the value of  increases, but both the correlations gradually decrease. To balance the two types of indicators, in the following experiment, we let .


Download : Download high-res image (183KB)
Download : Download full-size image
Fig. 7. Schematic diagram of the RF.


Download : Download high-res image (261KB)
Download : Download full-size image
Fig. 8. The influence of N.


Download : Download high-res image (505KB)
Download : Download full-size image
Fig. 9. The performance of models with different parameters.

Fig. 9 shows the impact of ,  and  on the identification performance. Each value in the figures is the average of  experiments. The results indicate the following relationships:

•
There is a weak positive correlation between  and the detection performance, including  and ;

•
 is positively correlated with  but is negatively correlated with  in some cases, e.g., ;

•
 shows that all combinations of parameters achieved good results.

Based on these results, in the following experiments, we let , , and  take the values of , , and , respectively.

4.3. Configuration of the DNN-based models
We consider three pending factors for the DNN-based models, including the activation function, loss function and structure.

The sigmoid function 
 and the hyperbolic tangent 
 are the most frequently used activation functions.  usually converges faster than  because its outputs are close to zero (LeCun et al., 2012). In Table 1, we compare the accuracy and time cost of the  and the  for the DNN. The results indicate that in the case of similar accuracy, the running time of DNN with  is less than that with . Therefore, we adopt  to be the activation function of the DNN in this work.

The loss function is used to estimate the fitting degree of the model. In addition to the cross-entropy (CE) 
, the mean square error (MSE) 
 is also often used. Similar to Eq. (19), 
 is defined by (34)
 
 Fig. 10 shows the normalized values of MSE and CE versus the number of epochs in the DNN training phase. The results indicate that the value of CE decreases faster than that of MSE, which means that CE can reduce the number of iterations. Therefore, CE is used as the loss function in this experiment.


Table 1. The performance of DNN using different activation functions.

Activation functions	Accuracy (%)	Time (s)
95.54	582.61
96.37	501.05

Download : Download high-res image (167KB)
Download : Download full-size image
Fig. 10. The curves of cost.

The optimal structure can be determined by two steps (Yu and Deng, 2015): (i) determining the optimal number of hidden layer nodes () in a single-layer neural network and (ii) adding more hidden layers () with the same number of nodes. Fig. 11(a) shows the influence of  on the accuracy of DNN with . The results indicate that the accuracy of  is better than others. Considering the trade-off between accuracy and computational complexity,  is set to 24 in the following experiments. Fig. 11(b) shows the influence of  on the accuracy of DNN. Results indicate that the accuracy (96.74%) of  is better than others. Moreover, the results show that the accuracy of the DNN is quite stable as the values of  and  increase. Thus, the shared DNN is set to four hidden layers, with  nodes per layer.


Download : Download high-res image (339KB)
Download : Download full-size image
Fig. 11. Accuracy of the different DNN structures.

4.4. The behavior characteristics of the models
•
The SR-sequences usually start from state 3 and continue to maintain self-transfer;

•
The CR-sequences usually start from state 1 and then transfer to state 2 and 3 in turn and finally stay in state 3.

Furthermore, Fig. 13, Fig. 13 show the typical state transition sequences of the SR- and CR-sequences, respectively. We sort them according to their occurrence probabilities in the training data. The typical time-varying state sequences  and  account for 92.11% and 61.52% of all SR- and CR-sequences, whereas the remaining 19.08% of CR-sequences begin in state . These results indicate that the proposed model can effectively capture the time-varying behavior characteristics of both the CR- and SR-sequences.

4.5. Comparison of the decision means
Fig. 14 shows the distribution of all RF-fragments in the two-dimensional space composed of the logarithmic likelihoods of both 
 and 
. “＋ ” and “○” represent the true labels of RF-fragments as the SR and CR, respectively. 
 and 
 denote the logarithmic likelihoods calculated using 
 and 
, respectively. The essence of click attribute identification is to separate CRs from the observed HTTP(S) traffic. As this step is independent of the HMM-DNN model, we compare different decision means for the optimal solution. There are three options, including (i) the single model threshold (
), (ii) the dual model threshold (
), and (iii) the DNN rear classifier with the dual model. The decision formulas of (i) and (ii) are given by (35), (36): (35) 
 (36) 
  where 
 and 
 denote the likelihood outputted by the 
 and the 
, respectively. 
 and 
 are the thresholds calculated by the historical data. Fig. 15 shows the precision–recall curves (PRCs) for both threshold-based methods. This result indicates that the recognition performance of the dual model is better than that of the single model.


Download : Download high-res image (320KB)
Download : Download full-size image
Fig. 14. The distribution of logarithmic likelihoods.


Download : Download high-res image (169KB)
Download : Download full-size image
Fig. 15. The PRCs of the threshold-based methods.

To evaluate the performance of the threshold-based methods and the DNN rear classifier, we adopt Zou’s algorithm (Zou et al., 2016) to determine the best threshold for 
 and compare it with the DNN rear classifier using the same test data. The average results are shown in Table 2. Although the  of 
 is slightly higher than that of the DNN rear classifier, the other indicators are not as good as it, including the , the , and the . Thus, thresholds calculated based on limited historical data are not suitable for complex application scenarios.


Table 2. The performance of 
 and the DNN rear classifier.

Methods				
1.000	0.759	0.862	0.773
BP	0.938	0.975	0.956	0.918
4.6. Performance comparison
To validate the proposed HMM-DNN, we compare its recognition performance with other peer methods. The selected methods are divided into two categories: (i) the first is based on the features of single-request, including support vector machine with sigmoid (S-SVM) and radial basis function (R-SVM) as the kernel function (Kong et al., 2017), Gaussian Naive Bayes (GNB) (Fadlil et al., 2017), k-means (Kumari et al., 2016), GMM (Alizadeh et al., 2015), J48 decision trees (DT) (Sahu and Mehtre, 2015), multilayer perceptron (MLP) (Ramchoun et al., 2016), and “” (Rizothanasis et al., 2016); (ii) the second is based on the sequence analysis, including recurrent neural networks (RNN) (Yin et al., 2017), long short-term memory networks (LSTM) (Vu et al., 2018), and HMM-GMM. For each method, the same data and features are used for model training and determining the optimal model configuration.


Download : Download high-res image (718KB)
Download : Download full-size image
Fig. 16. Comparison with other methods.

In Fig. 16, we show the results of the performance comparison from four aspects, including the precision in Fig. 16(a), the recall in Fig. 16(b), the  in Fig. 16(c), and the  in Fig. 16(d). These results are based on the data that come from  independent random experiments. An intuitive conclusion is that sequence-based methods are generally better than those based on single-request. Moreover, compared to other solutions, the proposed HMM-DNN shows two advantages: (i) the average of each indicator is quite stable and better than most other methods; (ii) both the box and the tail of the composite indicator,  and , are more compact than other methods. We notice that the performance of both RNN and LSTM in this work is not so good, although they perform well in other fields. The reason mainly comes from two aspects. First, in RNN the number of derivative terms increases with the number of hidden layers, which causes the gradient to decay quickly and terminates the update of the parameters early. A similar issue has been identified in Le’s work (Le et al., 2015). Therefore, it is difficult for RNN to effectively capture the underlying laws of the data. Second, CR may not be suitable for LSTM, because in each sequence the number of non-click requests is much larger than that of the CRs, which results in the signal characteristics of the CRs being easily overwhelmed by non-click requests and not being detected by the LSTM. This limitation has been mentioned in Tax’s work when they apply the LSTM to log analysis (Tax et al., 2017). Moreover, both the RNN and LSTM are based on a neural network, which makes them inevitably have the inherent limitations of neural networks, such as the over-fitting issue, inexplicable model, and parameter instability. All of these issues affect their final performance.

Furthermore, the detailed statistical results of the three best methods (“”, HMM-GMM, and HMM-DNN) are listed in Table 3. Although “” provides the best performance in the methods of single-request, the predefined thresholds affect its versatility. The statistical results of HMM-GMM and HMM-DNN show that the performance of DNN is better than that of GMM. For example, HMM-DNN increases the mean of  by 5.2 percentage points, whereas each of its indicators has a smaller standard deviation. The reason is that GMMs are statistically inefficient for modeling the data that lie on or near a nonlinear manifold in the data space (Hinton et al., 2012); however, the relationship between the hidden states and the observations is usually nonlinear.


Table 3. Statistical results of P+N, HMM-GMM, and HMM-DNN (, , and  denote the median, mean, and standard deviation, respectively).

Methods		
8	0.893	0.891	0.016	0.798	0.796	0.029
11	0.882	0.881	0.032	0.777	0.779	0.058
12	0.943	0.933	0.025	0.892	0.881	0.042
5. Discussion
5.1. Number of hidden states
The number of hidden states of HMM has always been the main factor affecting its application, because currently there is no suitable mathematical method to solve it. In this work, we adopt a simple method to determine the optimal number of hidden states from the perspective of clustering. For the proposed model, each hidden state represents a specific behavior pattern for launching the HTTP(S) requests, and can also be treated as a class of observations. Therefore, the number of hidden states is equivalent to the number of clusters of the observations. Based on this idea, we use Eq. (37) to evaluate the clustering performance and employ it to determine the number of hidden states. (37)
where  denotes the number of different clusters. 
 is the size of the th cluster. 
 denotes the distance between the th observation vector in the th cluster and the centroid of the th cluster. 
 is the average distance of the th cluster.  represents the mean square error of the total distance of the clusters. Then, we use -means to test the performance of different number of states and plot the results in Fig. 17. It indicates that  tends to be stable when the number of clusters is greater than . Considering the performance and computational complexity, we choose  to be the number of hidden states.

5.2. Convergence and complexity analysis
The convergence of the model directly affects its execution efficiency. For the HMM framework, the average logarithmic probability  calculated by Eq. (16) is a good indicator for evaluating the convergence. Fig. 18 shows the curves of normalized  vs. the number of iterations. The results indicate that both the click and non-click models (
 and 
) tend to converge after seven iterations.


Download : Download high-res image (189KB)
Download : Download full-size image
Fig. 18. The variation in the likelihood of the model.

The computational complexity is related to the number of parameters that need to be estimated. Let  be the number of an HMM’s hidden states and  and  be the length of the context.  is the dimension of the observation vector.  is the size of RF-fragments. Let 
 and 
 be the number of nodes per layer and the hidden layers for a neural network  {shared DNN, rear classifier}, respectively. The training phase must train two HMMs, each with 
 parameters. The shared DNN has 
 parameters; meanwhile, the classifier that evaluates the likelihoods must train 
 parameters. In the identification phase, the computational complexity of calculating the likelihood of an RF-fragment through the HMM-DNN is 
, which includes the computation of the forward algorithm of the HMMs and the DNN for generating the posterior probabilities. Moreover, the computational complexity of identifying the CR using the rear classifier is 
. The total computational complexity of identifying whether an RF-fragment starts with a CR then becomes 
. In addition, the storage space of the HMM-DNN parameters must accommodate four parts:  for the initial state probability, 
 for the transition probability, 
 for the weight and offset of the shared DNN, and 
 for the weight and offset of the rear classifier.

6. Conclusion and future work
In this work, a new scheme was proposed to identify CRs based on the traffic behavior. We adopted two HMMs to describe the time-varying behavior of click and non-click web traffic. DNN was integrated into the HMMs to capture the context of the observed traffic features and eliminate the limitations caused by the independence hypothesis of the traditional HMMs. In contrast to the existing works, we designed new GMM-independent algorithms for this integrated model. Finally, we introduced a DNN-based rear classifier to determine the click attribute of an HTTP(S) request according to the fitting degree between the requests and the behavior models. We evaluated the performance of the proposed scheme with some existing benchmark methods. The experiment results showed that the proposed behavior-based scheme can effectively identify the click property of HTTP(S) requests. Since the proposed scheme does not rely on the payload information of the packets/requests, it is encryption independent and suitable for both the network-side and server-side.

Some interesting issues arising from this work will be further explored in our future research. For example, in this work we only adopt the features related to the time of HTTP(S) requests. However, in the experiments we have found that the information of the TCP connections is also very useful. Combining the information of the network layer and the TCP layer might improve the effectiveness of our method. Moreover, the type of website affects the access patterns of users, which leads to a significant difference in the traffic generated by clicks and makes it difficult to use a unified method for all types of recognition. A fine-grained identification scheme is a challenge that warrants subsequent investigation.