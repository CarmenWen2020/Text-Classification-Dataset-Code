Many social studies and practical cases suggest that people's consumption behaviors and social behaviors are not isolated but interrelated. However, most existing research either predicts users' consumption preference or recommends friends to users without dealing with them simultaneously. In this paper, we propose a novel framework called MutualRec that jointly accomplishes the two tasks based on graph neural networks, attention mechanisms, and mutualistic model. MutualRec first uses a spatial attention layer and a spectral attention layer to learn latent embeddings from observed data, and then merges them via a mutualistic attention layer. The first two layers can relieve data sparsity without violating users' preference sequence, while the last captures the relationship between user’ consumption and social behaviors. We demonstrate the effectiveness of MutualRec in both social recommendation and link prediction via extensive experiments.

Previous
Next 
Keywords
Social networks

Joint recommendations

Mutualistic model

Graph neural network

Attention

1. Introduction
Social recommendation and link prediction are two important tasks in social networks. Traditionally, the two tasks are independent as they consider users' consumption preferences and social link behaviors respectively (He et al., 2015; Ding et al., 2017; Liao et al., 2018; Hamilton et al., 2017; Abu-El-Haija et al., 2018; Wang et al., 2019). However, social scientists have suggested that the two tasks are not isolated but tightly dependent (Aral et al., 2009; Libai et al., 2010; Aiello et al., 2012; Jiang et al., 2014)—users’ preferences can be easily influenced by social links while users with similar interests are likely to build connections (Aral et al., 2009). An example (Fig. 1) could be the following: David may join a volleyball team as his Friend Bob did, and Alice may make friend with David knowing he is also fond of violin.

Fig. 1
Download : Download high-res image (446KB)
Download : Download full-size image
Fig. 1. The illustration of practical scenario.

Although some attempts have been made to leverage users' social relationship for item recommendation (Guo et al., 2015; Hu et al., 2019; Fan et al., 2019; Wu et al., 2019), they mostly use one task's information as the auxiliary information to lift the performance of the other task, instead of accomplishing the two tasks simultaneously. The limited work that solves social recommendation and link prediction as joint tasks appears only very recently, and they rely on either matrix factorization (MF) (Wu et al., 2017; Shu et al., 2018) or neural frameworks (Wu et al., 2018). However, the MF-based approaches are susceptible to data sparsity as the latent vector factorized from sparse matrix can not fully reflect users' preference. Besides, all of them neglect the differed importance of items and friends to a user. Instead of assuming equal contributions of items or other users, some people prefer light food to spicy food, and some people would like to differentiate others into strangers, acquaintance, common friends, and bosom friends. In addition, these joint models use a straightforward concatenation or weighted sum operation to combine users' consumption preferences and social links, and thus are limited in representing their mutual influence. A novel joint model that simultaneously solve the recommendation and link prediction tasks is required.

We further extend our previous work (Xiao et al., 2020) and propose a novel approach called MutualRec, which incorporates graph neural network, attention mechanism, and mutualistic model to address the above challenges. The graph neural network can efficiently mitigate data sparsity problem via considering the graph structure instead of a sparse matrix, which turns the input into meaningful node-node connection information in place of unknown zero values [? 12], the attention mechanism makes the training process be aware of users' preference, and the mutualistic model (Kim and Lin, 2006; Boucher, 1988) accounts for the mutual influence relationship between tasks. The mutualistic model originates from exploring the implicit mutual interactive relationship between two species via dynamically simulating the growth rate in real nature and digging up the latent elements in biology. It has plenty of theoretic support (Kim and Lin, 2006; Boucher, 1988; Bascompte, 2019; Costa-Pereira et al., 2018) for modeling the mutual reinforcement relationship for users' consumption preference and social links. MutualRec first extracts user's consumption preference embedding and social preference embedding from a spatial attention layer and a spectral attention layer. It then merges those embeddings into a mutualistic attention layer for predicting ratings and link value simultaneously.

Our contributions are threefold:

•
We propose a novel framework for jointly modeling users' consumption preference and social links, which has not been fully explored in our preliminary work. The proposed model outperforms state-of-the-art schemes in the joint tasks.

•
We design a mutualistic attention mechanism inspired by the mutualistic model (Kim and Lin, 2006), which can better reflect the implicit relationship between users' consumption preference and social links. To the best of our knowledge, MutualRec is the first to incorporate the biological model into recommender systems.

•
We have conducted extensive experiments on four real-world datasets. The results show MutualRec achieves better accuracy in both social recommendation and link prediction.

The rest of this paper is organized as follows: Section 2 reviews the related work. Section 3 introduces the mutualistic model and the research problem. Section 4 presents our approach, MutualRec. Section 5 reports an analysis of our experimental results. Finally, Section 6 concludes the paper.

2. Related work
Social recommendation. Exploiting social relationship has been a popular topic in recommender systems (RSs), considering that users are likely to be influenced by the people close to them, attested by social correlation theories (Anagnostopoulos et al., 2008). Considerable work has incoporated social elements into matrix factorization techniques to improve the recommendation performance (Guo et al., 2015). For example, Guo et al. (2015) propose TrustSVD, which integrates SVD++ with social relationship to solve data sparsity and cold start problems; Graph neural networks (GNN) represent another thread of research in social recommendation, given its ability in learning graph structure data (Ying et al., 2018; Kipf and Welling). Hu et al. (2019) propose HERS, which considers direct and indirect social relationship between users in a heterogeneous information networks to solve cold start and data sparsity problems; Fan et al. (2019) propose GraphRec, a graph attention-based scheme to jointly captures interactions in user-item and user-user graphs; Wu et al. (2019) propose DANSER, dual graph attention networks, to learn deep representations for social effects such as social homophily and influence, and item-to-item homophily and influence.

Social link prediction. Link prediction aims to predict the potential new links for a target user based on the partially observed links in a social network (Xiao et al., 2020a, 2020b). Early work (He et al., 2015) uses non-negative matrix factorization to predict the unknown value of friendship. For example, Ding et al. (2017) propose BayDNN, which combines a convolutional network with Bayesian matrix factorization to make friends recommendation. Graph representative learning, which considers the structure of node, has become popular (Perozzi et al., 2014; Liao et al., 2018; Hamilton et al., 2017; Abu-El-Haija et al., 2018) to overcome data sparsity. The related techniques include learning node embeddings (e.g., DeepWalk (Perozzi et al., 2014) and ASNE (Liao et al., 2018)) and learning latent representations of sub-structure for graphs (e.g., GraphSage, Attenwalk, and HAN). GraphSage (Hamilton et al., 2017) is a general inductive framework that leverages nodes feature information to generate node embeddings for unseen links; Attenwalk (Abu-El-Haija et al., 2018) is an attention model on the power series of the transition matrix, which guides the random walk to optimize an upstream objective; HAN (Wang et al., 2019) applied node-level and semantic-level attentions to a heterogeneous information network.

Obviously, all of these approaches mentioned above only tackle either of these two tasks.

Joint user preference and social links model. Sociologists have validated the correlation between users' consumption preference and social links (Aral et al., 2009; Libai et al., 2010; Aiello et al., 2012; Jiang et al., 2014). According to the social influence theory, people in social networks are more likely to bond and interact with other users who have similar preferences and attributes (Friedkin, 2006; Crandall et al., 2008; La Fond and Neville, 2010), and the preference of a user is more easy to be influenced by her trusted people such as friends (La Fond and Neville, 2010). Inspired by the above, a few efforts have attempted to jointly model consumption preference and social links of a user in social networks. For example, Wu et al. (2017) capture the temporal dynamics of users' consumption behaviors and social behaviors; Shu et al. (2018) use transfer learning to extract user features from existing social media to predict users' consumption preference and suggest new links on the newly launched social websites; Wu et al. (2018) propose a general neural framework to jointly model the evolution of users' consumption preference and social links. Such work usually uses simple concatenation (Wu et al., 2018) or weighted sum operations to combine users' consumption preferences and social links, and therefore cannot well model their full relationship. Although Xiao et al. (Xiao et al., 2020) first attempt to model the latent mutual relationship between users' consumption behaviors and social behaviors via deploying mutualistic idea, they also ignore the different influence brought by items or friends on target user's preference.

However, these above joint models have three drawbacks: 1) the works based on matrix factorization (e.g., (Wu et al., 2017; Shu et al., 2018)) are susceptible to data sparsity; 2) Many existing studies only provide coarse representations (e.g., binary representations) for measuring the preference of users on items and relationships; 3) most current approaches do not consider the correlation about user behavioral preferences and social inter-activities. Our MutualRec advances these current approaches in several aspects: 1) We deploy graph represent learning methods to mitigate the data sparsity; 2) We adopt attention mechanism to capture users' various preference on items and friends; 3) We leverage mutualistic model to better simulate the mutual reinforcement relationship between users’ consumption preference and social links. To the best of our knowledge, our model is the first one to incorporate the mutualistic model in this field of research.

3. Preliminaries
In this section, we introduce the mutualistic model and formulate our problem. Table 1 shows the main notations used in this paper.


Table 1. Notations.

Symbols	Description
𝓡	The user-item rating matrix (i.e., user-item graph)
𝓤, 𝓘, μi, ij	User set, Item set, the ith user, the jth item
rij	The rating of ij by μi
𝓢, siv	The user-user social graph, the link between μi and μv
d	The length of embedding vector
P, pi	User-specific matrix, the user-specific embedding of μi, pi ∈ Pm×d
Q, qj	Item-specific matrix, the item-specific embedding of ij, qj ∈ Qn×d
F, fi	User latent social matrix, the latent social embedding of μi, fi ∈ Fm×d
𝓝
The set of items rated by μi
𝓝
The set of users rated ij
𝓝
The set of users directly linked with μi
𝓘
The item influence embedding of μi
𝓢
The social item-specific embedding of μi
𝓘
The consumption preference embedding of μi
𝓢
The social preference embedding of μi
The user preference embedding of μi
The user social embedding of μi
The mutual embedding of μi
The mutual preference embedding of μi
The mutual social embedding of μi
⊞, ⊗, ⊙	Concatenation operation, dot production, element-wise ×
MLP(⋅), φ	Multi-Layer Perceptron, activation function
W/Wj, b/bj	The weights and biases of neural network
3.1. The mutualistic model
The Mutualistic model describes the ecological interaction between two or more species where each species benefits (Bronstein, 2015). Examples of mutualism includes flowering plants being pollinated by animals, vascular plants being dispersed by animals, and corals with zooxanthellae. Many researchers try to dynamically simulate the variation between species behind the mutualistic phenomenon via partial differential equations (Kim and Lin, 2006; Boucher, 1988; Bascompte, 2019; Costa-Pereira et al., 2018) such as Lotka-Volterra equations (Boucher, 1988). Lotka-Volterra assumed that the growth of a species depends on its self-interaction and the benefits obtained from the interaction with the other species, which is formulated as follows:(1) 
 where N1, N2 represent the quantity of two species; 
 (i = 1,2) denotes the growth velocity of each species; r1, r2, α1, α2 are positive parameters. The Lotka-Volterra equations aim to find an equilibrium from rather than solving equation (1). Such an insight lowers the complexity of the research on mutualism because not all the partial differential equations have a unique solution (Bellman, 1948). Later work has considered more complicated factors including the restriction of external resource (Kim and Lin, 2006), influence from multi-species (Bascompte, 2019), and feedback control between species (Lin, 2018; Vaidyathan, 2015).

3.2. Why mutualistic model is motivated
The mutual reinforcement relationship between users' consumption preference and social links has been validated to be existing (Shu et al., 2018; Wu et al., 2017, 2018; Aiello et al., 2012; Jiang et al., 2014): a user prefers to contact or share her shopping experience with those who have similar interests or hobbies, which strengthens their connections; user's social relationship can enhances the recommendation performance (Guo et al., 2015; Fan et al., 2019; Hu et al., 2019; Peng et al., 2018). The mutualisic model has been extensively studied (Kim and Lin, 2006; Boucher, 1988; Bascompte, 2019; Costa-Pereira et al., 2018) and has the advantage of modeling the mutual interaction between species via exploring the latent mutual interactive elements, which makes it a good fit to model the above mutual reinforcement relationship in social networks. We design the following strategy which adapts Lotka-Volterra to our tasks:(2) 
 where 
, 
 denote the μi's preference embedding and social embedding; 
 and 
 represent the mutual embedding for social recommendation and link prediction tasks, respectively; α1 and α2 are attention weights for the training, quantifying the preference sequence on items and intimacy degree on friends; we have used element-wise × (i.e., ⊙) to simulate the mutual interaction, according to the mutualistic model.

3.3. Problem Formulation
We consider both implicit feedback of users and social relationship among users. We denote a user-item rating matrix that contains m users and n items by 𝓡
, where rij = 1 if user μi rated item ij otherwise 0. We use 𝓝
 and 𝓝
 to respectively denote the set of items rated by μi and the set of users who have rated ij. We use 𝓢
 to represent social graph, where siv = 1 if μi has a relation to μv and 0 otherwise. In our work, a social graph is a bi-directional graph, i.e., 𝓢 is a symmetric matrix.

Problem Formulation. Given an observed record in 𝓡 and social graph 𝓢, the tasks of our model are to 1) grasp the feature representation of each user in consumption preference and social preference. 2) quantify the social influence in user preference and homophily influence in social links. 3) simultaneously predict unobserved ratings and links in 𝓡 and 𝓢, i.e., the probability of μi clicking a new candidate item 
 and establishing a new link with a new user 
.

4. Scheme design
The overall architecture of MutualRec is shown in Fig. 2. Our model mainly contains four components: spatial attention layer, spectral attention layer, mutualistic attention layer, and predicted layer. The spatial attention layer aims to learn the consumption preference embedding from rating matrix 𝓡. Since a user's consumption preference depends on two parts in social RSs, i.e., items preferred by herself and her friends (Guo et al., 2015), we decide to learn users' consumption preference embedding from these two aspects, i.e., item influence embedding and social item embedding. Then it is intuitive to obtain user's consumption preference embedding. The spectral attention layer mainly focuses on learning user's social preference embedding from social graph 𝓢. According to the social influence theory, people in social networks are more willing to generate interactions with others who have similar hobbies and attributes (Friedkin, 2006; Crandall et al., 2008). Therefore, we can extract a user's social preference embedding via her friends. After determining user's consumption preference embedding and social preference embedding, we will obtain preference embedding and social embedding, which are regarded as the input of mutualistic attention layer. The mutualistic attention layer is inspired by mutualistic model (Fort and Mungan, 1808). The target of this layer is to adaptively model the mutual relationship between user's consumption preference and social links, which generates mutual preference embedding and mutual social embedding. These two embeddings are applied in the predicted layer to predict unobserved ratings and links. The parameters of MutualRec are trained via prediction. We use pi and qj to respectively represent user-specific embedding and item-specific embedding, where d is the dimensionality of embedding vector.

Fig. 2
Download : Download high-res image (1MB)
Download : Download full-size image
Fig. 2. The Architecture of MutualRec (best viewed in color). It contains four components: spatial attention layer, spectral attention layer, mutualistic attention layer and predicted layer, where αW means the attention weights generated by attention network, pointed by fuchsia dotted line. αW colored by yellow means attention weights. (For interpretation of the references to color in this figure legend, the reader is referred to the Web version of this article.)

4.1. Spatial attention layer
Spatial attention layer is to learn consumption preference embedding 
 for μi shown in the up-left of Fig. 2. To synthetically grasp 
, two challengings should be solved first: 1) The historical consumption records of some users are quite sparse. These sparse record may lead to a terrible representation for users; 2) each user has a unique preference sequence for her preferred items. For example, Alice preferred comedy to animation, and thus comedy should be ranked in front of animation when making a recommendation. Therefore how to impair the negative effect of sparse record and guarantee preference sequence during the whole representation learning process is quite important. To address these two challenges, our spatial attention layer considers graph structure and preference sequence by incorporating spatial graph neural network (Hamilton et al., 2017) and attention mechanism (Veličković et al.). Graph structure and attention mechanism have been validated to efficiently solve data sparsity (Ying et al., 2018; Hu et al., 2019) and guarantee users’ preference order in recommendation (Fan et al., 2019). Therefore, our spatial attention layer can effectively address these two challengings when learning 
. Next, we will introduce how to generate 
 in detail.

In social networks, a user's preference is generally influenced by items purchased by her and her friends (Guo et al., 2015). Therefore a user-item rating graph can be split into two non-Euclidean subgraphs: target user's rating graph and target user's friends' rating graph shown in the up-left of Fig. 2. The target user's ratings are used to generate item influence embedding 
 while her friends' ratings are to generate social item embedding 
.

•
Item influence embedding 
: Item influence embedding 
 is related to μi's historical records of items, which reflects the effect of items on user's preference. Generally, each user μi has her historical records on items that are regarded as neighbor nodes directly connected with μi according to graph theory. We use 𝓝
 to denote those items. For 
𝓝
, there may exist other neighbor users who also purchased item ij. We use 𝓝
 to represent these users. A simple example can help to understand such connectivity shown in the up-left of Fig. 2. In Fig. 2, user μi purchased two items denoted by yellow hexagon, i.e., i1 and i4 composed of 𝓝
, where i1 is also purchased by user μ3 apart from μi and forms 𝓝
. So is 𝓝
. Such a graph structure considers neighborhood effect and has been validated efficiently in Hamilton et al. (2017). However, different from Hamilton et al. (2017), ours considers the user preference sequence by merging the attention mechanism.

Hence, to obtain 
, we first aggregate the feature of users in 𝓝
, the aggregated feature vector of item ij is marked by 
:(3)
𝓝
where ak denotes the attention weight of preference degree between 
𝓝
 and ij, which is computed as follows.(4) 
𝓝
 
 After obtaining all the 
 (
𝓝
), we aggregate them to generate item influence embedding 
:(5)
𝓝
where aμk is the attention weight of preference degree between μi and 
𝓝
, which is computed as:(6) 
𝓝
 
 

•
Social item embedding 
: Social item embedding 
 reflects the preference influence of μi's friends on μi. We use 𝓝
 to denote the set of μi's friends. Each user μl in 𝓝
 has her own historical records on items, denoted by 𝓝
. For example in the up-left of Fig. 2, user μi has two friends colored by blue in user-item ratings graph, i.e., μ1 and μ2, where their purchased items are marked by 𝓝
 and 𝓝
, respectively. In order to efficiently learn 
, we adopt the similar structure applied in 
 shown in Fig. 2. We first aggregate the features of items in 𝓝
 (
𝓝
), the aggregated feature vector of user μl is denoted by 
:(7)
𝓝
where ηlj is the attention weights reflecting the preference degree of μl on ij (
𝓝
) computed as follows.(8) 
𝓝
 
 After getting all 
 (
𝓝
), we aggregate them generate social item embedding 
:(9)
𝓝
where ηil is the attention weight which reflecting the intimacy of μi on 
𝓝
 from the perspective of consumption.(10) 
𝓝
 
 

After determining item influence embedding 
 and social item embedding 
, we obtain μi's consumption preference embedding 
 as follows:(11)

To lower the computation burden, we adopt a randomly sampling strategy introduced in Hamilton et al. (2017) to form 𝓝
 and 𝓝
.

4.2. Spectral attention layer
This layer aims to learn social preference embedding 
 for μi (shown at the bottom left of Fig. 2). In social recommendation scenario, the user is more willing to contact or even make friends with others who share similar consumption preference or interests (Liao et al., 2018). Therefore, we can be aware of a user's social preference (i.e., what kind of people she would like to acquaint) via mining her social graph. In order to lean 
, we adopt a framework based on spectral graph convolution network (Kipf and Welling; Defferrard et al., 2016). The motivation is: 1) spectral graph convolution network considers the implicit connection between nodes when extracting features (Kipf and Welling; Defferrard et al., 2016). Such consideration can efficiently relieve the negative effect of data sparsity (Defferrard et al., 2016). 2) Compared with the spatial graph network, the key of spectral graph convolution network is to construct a spectral kernel which demands that the input should be a symmetric matrix (Kipf and Welling). In our work, social graph 𝓢 is a symmetric matrix which satisfies the input condition of spectral graph convolution network. Also, Another reality indicates that the intimacy between people is distinguished (Katz and Nguyen, 2016), e.g., people classify other people into several categories according to intimacy, which includes strangers, acquaintance, common friends, bosom friends, etc. To hold such intimacy, we apply the attention mechanism while generating 
. The details will be elaborated as follows.

Given a social graph 𝓢, we first construct graph convolution kernel denoted by 
𝓢
 according to Chung and Graham (1997); Kipf and Welling:

1)
Construct a normalized Laplacian matrix 𝓛: 𝓛
 
𝓢
 
, where Im is an m × m identity matrix, D is the m × m diagonal degree matrix defined as Dnn = ∑jSm,j.

2)
Construct 
𝓢
 as: 
𝓢
, where U and λ are the eigen-vector matrix and eigen-value matrix of 𝓛.

For ∀μi, we can get her links set 𝓝
. Then we get her friends’ social features Soci via:(12)
𝓢
𝓝
Then we get the social preference embedding 
 of μi as follows:(13)
𝓝
where κik is the attention weight reflecting the intimacy degree between μi and μk (
𝓝
), which is computed as follows,(14) 
𝓝
 
 

4.3. Mutualistic attention layer
The mutualistic attention layer aims to model the mutual implicit reinforcement relationship between user's consumption preference and social links (Aral et al., 2009). This layer is inspired by mutualistic model, a biologic model that focuses on studying the ecological interaction between two species where each species benefits (Bronstein, 2015). The motivation for considering mutualistic model is briefly stated as follows:

1)
According to social influence theory, the user always prefers to contact or share her shopping experience with others who have similar interests or hobbies. Therefore, people who share similar interests are easier to make friends (Aiello et al., 2012; Jiang et al., 2014). In other words, a user's interest or consumption preference can contribute to making new friends in social networks.

2)
users' social links have been prevalently applied in item recommendation and validated to be efficient in recommendation performance (Guo et al., 2015; Fan et al., 2019; Hu et al., 2019).

3)
Mutualistic model intuitively specializes in modeling the mutual reinforcement relationship between species via exploring the latent mutual elements which positively influence the growth of species. The idea behind the mutualistic model provides us a possibility when modeling the mutual reinforcement relationship between users' consumption preference and social links. More details can be seen in Section 3 Preliminaries, 3.1 The mutualistic model.2.

Next, we introduce how to construct our mutualistic attention layer. First, we prepare the input of this layer, i.e., user preference embedding 
, user social embedding 
, after obtaining μ's consumption preference embedding 
 and social preference embedding 
 from spatial attention layer and spectral attention layer. Then we respectively aggregate them with μi's specific embedding pi and latent social embedding fi (both colored by red in Fig. 2) to get the user preference embedding 
, user social embedding 
 as:(15) 
 In the following, we construct the mutual influence between user's consumption preference and social links according to Section 3.2, where the new generating embedding is named mutual embedding 
.(16)
Here we stress a crucial point that no matter for a social recommendation or links prediction, the preference sequence on items and friends of users should be considered and these two preferences are different. For social recommendation, the mutual embedding should be computed as:(17) 
 For link prediction, the mutual embedding should be computed as:(18) 
 where ζ(⋅) is a softmax function, 
 and 
 are vector composed of attention weights, which synthetically reflect the preference degree on items and intimacy on friends. Then we intuitively produce μi's mutual preference embedding 
 and mutual social embedding 
(19) 
 

4.4. Predicted layer
After obtaining μ's mutual preference embedding 
 and mutual social embedding 
, we will predict μs's consumption preference towards a new item and social link towards a new user. Firstly, we respectively transform 
 and 
 into the input of predicted layer shown in Fig. 2, where the new embedding vectors are colored by red and light blue, denoted by 
 and 
. They are computed as follows:(20) 
 Then for a given item specific embedding qj or user latent social embedding fv, the predicted rating 
 and link 
 are derived as follows(21) 
 

4.5. Joint training
To estimate model parameters of MutualRec, an appropriate loss function should be specified to guarantee the recommendation performance. Since our work considers the implicit feedback for social recommendation and links prediction, we opt for a BPR loss, which has been widely used to optimize recommendation approaches (Rendle et al., 2009). It is a pair-wise loss function that considers the relative order between observed and unobserved user-item/user-user interactions. More specifically, BPR assumes that the observed interactions including user-item/user-user, which are more reflective of a user's preference no matter on items or friends, should be assigned higher prediction values than those unobserved ones. The objection loss function for optimizing MutualRec is defined as:(22)
𝓞
𝓞
where 𝓞𝓡
𝓡
 and 𝓞
𝓢
𝓢
 denote training set for social recommendation and links prediction, ‘+’ indicates the observed data in graph, while ‘−’ denotes the unobserved data sampled from graph; σ(⋅) is the sigmoid function; Θ = {P, Q, F} which is used to regularization to avoid over-fitting problem. We conduct L2 regularization parameterized by λ on Θ.

At last, we adopt Adam (Kingma et al.) to minimize the loss function and update the model parameters. The Adam adapts the learning rate for each parameter by performing smaller updates for frequent and larger updates for infrequent parameters. Compared with other optimizer such as RMSprop, AdaGrad, etc., Adam method yields faster convergence and mitigates the burden of fine-tuning the learning rate.

5. Experiments
5.1. Datasets and statistics
The following four real-world datasets are applied in our experiments.

•
Flixster.1 This dataset contains information of user-movie ratings and user-user friendship from Flixster which is an American social movie site.

•
Epinions.2 This is the Epinions dataset which is composed of user-user trust relationship and user-item ratings from Epinions.

•
Gowalla.3 This dataset is crawled from Gowalla which is a location-based social networking website where users share their locations by checking-in.

•
Lastfm.4 This dataset contains social networking, tagging, and music artist listening information from a set of two thousand users from Lastfm online music system.

We filtered out users who have less than 4 consumption records and social links, items which are rated less than 4 users. After that, each user's ratings are binarized with the method introduced in Section 3.3. Table 2 shows the statistics of the four datasets after filtering.


Table 2. Statistics of datasets.

Datasets	Flixster	Epinions	Gowalla	Lastfm
Users	13522	6015	3334	1185
Services/items	7237	16706	5839	2334
Ratings	850038	222184	42967	46501
Links	263432	129272	48652	22466
R-Density (%)	0.86%	0.44%	0.22%	3.36%
L-Density (%)	0.29%	0.71%	0.88%	3.20%
5.2. Evaluation methodologies
We apply 4-fold cross-validation to evaluate the performance of various approaches. In each run, we randomly sample 75% ratings and social links as training and the rest 25% as testing for each user. Then a Top-N recommendation list in a descendent sequence is generated, where N is varied as 3,5,10,15,20,25 to compare the difference of results.

The recommendation quality is measured by three metrics: Precision@N, Recall@N, and NDCG@N, which have been widely applied to evaluate the quality of recommendation lists regarding different aspects (Wu et al., 2018). Precision@N, Recall@N reflects the recommendation accuracy, as they only consider the hit numbers and ignore the rank positions. NDCG@N (Xiao et al., 2020) is a rank-aware measure with which higher-ranked positive items are prioritized. The computation of these four metrics are defined as follows:(23) 
 
 
 
 
 where  
  (Xiao et al., 2020), RecN is a top-N recommended items list derived by approaches, testlist is a set containing items in a user's test data, ϱ(i) is a binarized parameter, i.e., ϱ(i) = 1 if the item at position i in RecN is also in testlist otherwise 0. Note that these metrics are consistent with each other, i.e., if a model performs better than another model on one metric, it is more likely that it will also produce better results on another metric.

5.3. Compared approaches
The overall compared experiments are divided into two parts: social recommendation and links prediction. For social recommendation, we compare with the following approaches:

•
CrossFire (Shu et al., 2018) is a cross social media framework based on matrix factorization technique (Lee and Seung, 2001), which simultaneously joints friend and item recommendation tasks.

•
ELJP (Wu et al., 2017) is a spatial matrix factorization-based (Lee and Seung, 2001) methodology which simultaneously model the evolution of users' preferences and social links in Social Network Services (SNS) platforms.

•
NJM (Wu et al., 2018) jointly models the evolution of user feedback and social links. This approach considers the dynamic user preferences, dynamic item properties and time-dependent social links in social networks during preference and links prediction.

•
MGNN (Xiao et al., 2020) is the first work to model the mutual relationship between users' consumption preference and social links via mutualistic model.

•
TrustSVD (Guo et al., 2015) incorporates the social relationship influences from social neighbours into targeted users' predicted ratings.

•
PinSage (Ying et al., 2018) is a state-of-the-art model for designing efficient convolutional operation for web-scale recommendations. As the original PinSage aims to generate items embedding, we apply it to reconstruct the user-item bipartite graph for recommendation.

•
HERS (Hu et al., 2019) is proposed to model the heterogeneous relations in social recommendation tasks by considering users' and items' influential contexts.

•
GraphRec (Fan et al., 2019) is a graph attention-based network approach to jointly captures interactions and opinions in user-item bipartite graph, which considers heterogeneous strengths including social links and ratings.

•
DANSER (Wu et al., 2019) is a deep dual graph attention-based network framework which models four interactions existing in practice, i.e., social homophily, social influence, item-to-item homophily and item-to-item influence.

For links prediction, the compared approaches are listed as follows besides CrossFire (Shu et al., 2018), NJM (Wu et al., 2018), ELJP (Wu et al., 2017) and MGNN (Xiao et al., 2020):

•
BayDNN (Ding et al., 2017) applies a one-dimensional convolutional neural network (CNN) to extract latent feature representation and utilizes Bayesian ranking method to obtain users' preferences on unknown social links.

•
GraphSage (Hamilton et al., 2017) exploits the neighborhood structure through sampled paths on the graph and user-specific aggregators to get the embedding of the target node. We generalize users' embedding via GraphSage to reconstruct user-user links matrix.

•
ASNE (Liao et al., 2018) is a social network embedding framework which learns representations for target user by preserving both the structural proximity and attribute proximity to recommend friends.

•
Attentionwalk (Abu-El-Haija et al., 2018) is an attention mechanism for learning the context distribution used in graph embedding network. We implement this model in social links graph to capture users' embedding to realize links prediction.

•
HAN (Wang et al., 2019) is a heterogeneous graph neural network on the top of GAT (Veličković et al.), which considers meta-path between users when predicting social links via citing node-level and semantic-level attention.

5.4. Implementation details
We implemented MutualRec model via Pytorch5, a well-known open-source deep learning platform. All the experiments are conducted on a Titan Xp GPU. For tuning the hyperparameters, we select the approaches that perform best on dataset sets based on precision, recall, and NDCG metrics and report their results. Model parameters are saved every 20 epochs. All approaches are trained until convergence, i.e., if the performance (i.e., precision, recall, and NDCG) does not improve after 50 epochs. All the approaches are trained for a maximum of 500 epochs. As for MutualRec, the number of neighborhood sampling in spatial attention layer is tuned amongst {10, 20, 30, 40, 50} (defaulted as 20). For the embedding size d, we test its value amongst {16, 24, 32, 64, 128}. Moreover, we empirically set the size of the hidden layer the same as the embedding size and the activation function as LeakyReLU. The learning rate is tested on {0.001, 0.002, 0.005, 0.01}. As for NJM and ELJP related with time variation, we mark four timestamps according to the fold of datasets, i.e., the test set is marked the 4th timestamp while the train set is randomly marked from 1 to 3 timestamps. For the remaining approaches, we follow the configuration stated in their works. The batch size and regularization parameter λ are fixed to 128 and 0.01 in the experiments.

5.5. Results
In this section, we evaluate our proposed MutualRec against other state-of-the-art algorithms. Our experimental evaluation is designed to answer the following research questions (RQs).

•
RQ1: Does MutualRec outperform other state-of-the-art approaches for both social recommendation and social links prediction (See Section 5 Experiments, 5.5.1 Social recommendation: RQ1.5.2)?

•
RQ2: How does the mutualistic attention layer affect the recommendation performance (See Section 5.5.3)?

•
RQ3: How does parameters influence the performance of MutualRec (See Section 5.5.4)?

5.5.1. Social recommendation: RQ1
For social recommendation task, we report the comparison results of MutualRec with other approaches shown in Table 3. As shown in Table 3 (N is set as 3), MutualRec outperforms other state-of-the-art approaches, and attains significant improvements because it hits the highest value in precision, recall, and NDCG. Compared with current best approaches: DANSER and HERS, MutualRec has at least 5.70% and 3.23%, 7.31% and 6.02%, 5.67% and 3.39% improvement in precision, recall and NDCG, respectively. Higher values in these three metrics indicate that MutualRec can efficiently predict users’ consumption preference in social recommendation.


Table 3. Overall comparison on real-world datasets for social recommendation.

Methods	Precision@3	Recall@3	NDCG@3	Precision@3	Recall@3	NDCG@3
Lastfm	Epinions
Crossfire (+)	0.1451	0.0381	0.1473	0.0256	0.0125	0.0239
NJM (+)	0.1483	0.0406	0.1491	0.0343	0.0182	0.0363
ELJP (+)	0.1321	0.0372	0.1383	0.0321	0.0154	0.0334
MGNN	0.1661
(↑ 7.41%)	0.0452
(↑ 9.85%)	0.1540
(↑ 11.32%)	0.0429
(↑ 10.02%)	0.0233
(↑ 6.84%)	0.0438
(↑ 7.08%)
TrustSVD (+)	0.1277	0.0361	0.1326	0.0293	0.0129	0.0312
GraphRec	0.1637
(↑ 8.97%)	0.0431
(↑ 19.58%)	0.1505
(↑ 12.35%)	0.0424
(↑ 11.31%)	0.0228
(↑ 9.88%)	0.0414
(↑ 13.29%)
PinSage	0.1552
(↑ 17.72%)	0.0427
(↑ 21.80%)	0.1520
(↑ 13.93%)	0.0405
(↑ 16.44%)	0.0223
(↑ 12.04%)	0.0403
(↑ 16.20%)
HERS	0.1705
(↑ 4.64%)	0.0486
(↑ 6.02%)	0.1636
(↑ 3.39%)	0.0449
(↑ 5.01%)	0.0236
(↑ 6.08%)	0.0445
(↑ 5.33%)
DANSER	0.1695
(↑ 5.01%)	0.0476
(↑ 8.29%)	0.1542
(↑ 9.65%)	0.0385
(↑ 22.48%)	0.0213
(↑ 17.37%)	0.0395
(↑ 18.80%)
MutualRec	0.1784	0.0515	0.1691	0.0472	0.0250	0.0469
Gowalla	Flixster
Crossfire (+)	0.1095	0.0632	0.1291	0.0920	0.0164	0.1025
NJM (+)	0.1172	0.0680	0.1327	0.1038	0.0185	0.1047
ELJP (+)	0.0967	0.0530	0.1061	0.0983	0.0156	0.0859
MGNN	0.1456
(↑ 3.98%)	0.1134
(↑ 6.26%)	0.1541
(↑ 7.27%)	0.1310
(↑ 3.36%)	0.0244
(↑ 9.43%)	0.1248
(↑ 2.64%)
TrustSVD (+)	0.0911	0.0929	0.1148	0.0310	0.0072	0.0295
GraphRec	0.1231
(↑ 22.98%)	0.0967
(↑ 24.61%)	0.1401
(↑ 17.99%)	0.1151
(↑ 18.33%)	0.0217
(↑ 15.01%)	0.1152
(↑ 11.67%)
PinSage	0.1374
(↑ 10.20%)	0.1052
(↑ 14.50%)	0.1448
(↑ 14.15%)	0.1204
(↑ 13.14%)	0.0212
(↑ 18.02%)	0.1205
(↑ 6.75%)
HERS	0.1467
(↑ 3.23%)	0.1144
(↑ 5.27%)	0.1562
(↑ 5.84%)	0.1276
(↑ 6.74%)	0.0242
(↑ 3.15%)	0.1246
(↑ 3.26%)
DANSER	0.1433
(↑ 5.70%)	0.1081
(↑ 11.40%)	0.1507
(↑ 9.65%)	0.1235
(↑ 10.24%)	0.0233
(↑ 7.32%)	0.1217
(↑ 5.67%)
MutualRec	0.1514	0.1205	0.1653	0.1354	0.0267	0.1281
Note: ‘+’ means that the performance of MutualRec exceeds more than 25% in precision@3 compared with other approaches. ‘↑’ means the improvement in accuracy.

Also, several insights can also be derived from Table 3: 1) In total, Graph neural networks based methodologies (i.e., GraphRec, PinSage, DANSER, HERS and MGNN) outperform better than those based on matrix factorization, i.e., Crossfire and ELJP. 2) Compared with joint models including Crossfire, NJM, and ELJP, those schemes mainly oriented to the social recommendation, i.e., GraphRec, PinSage, HERS, DSR, and DANSER generally achieve better recommendation performance. Based on the discussion above, MutualRec accomplishes more accuracy in social recommendation compared with current state-of-the-art methodologies from the evaluation.

5.5.2. Social links prediction: RQ1
In social links prediction, we aim to compare MutualRec with BayDNN, Graphsage, HAN, Attenwalk, and ASNE apart from Crossfire, NJM, ELJP and MGNN. As shown in Table 4 (N defaults as 3), MutualRec achieves better recommendation performance in social links prediction, which has 14.15%, 19.95% and 11.58% increase in precision, recall and NDCG on average. Compared with current best approaches, i.e., Attenwalk and MGNN, MutualRec also have at least 3.98% and 5.43%, 6.43% and 9.28%, 3.50%, and 6.78% increase in these three metrics, respectively. This improvement in metrics indicates that MutualRec can efficiently predict users’ social preference in link prediction task.


Table 4. Overall comparison on real-world datasets for links prediction.

Methods	Precision@3	Recall@3	NDCG@3	Precision@3	Recall@3	NDCG@3
Lastfm	Epinions
Crossfire (+)	0.0774	0.0672	0.1009	0.0405	0.0276	0.0437
NJM	0.0978
(↑ 15.96%)	0.0795
(↑ 16.02%)	0.1021
(↑ 24.51%)	0.0484
(↑ 27.88%)	0.0313
(↑ 29.51%)	0.0475
(↑ 28.70%)
ELJP (+)	0.0866	0.0652	0.0902	0.0381	0.0257	0.0412
MGNN	0.1052
(↑ 7.79%)	0.0891
(↑ 3.48%)	0.1213
(↑ 4.86%)	0.0594
(↑ 4.04%)	0.0388
(↑ 4.38%)	0.0585
(↑ 4.44%)
BayDNN	0.0908
(↑ 24.95%)	0.0695
(↑ 32.61%)	0.1110
(↑ 14.56%)	0.0506
(↑ 22.19%)	0.0319
(↑ 27.20%)	0.0539
(↑ 13.36%)
Graphsage	0.1047
(↑ 8.30%)	0.0767
(↑ 20.26%)	0.1076
(↑ 18.13%)	0.0532
(↑ 16.29%)	0.0327
(↑ 23.97%)	0.0549
(↑ 11.32%)
HAN	0.1062
(↑ 6.79%)	0.0824
(↑ 11.86%)	0.1183
(↑ 7.46%)	0.0587
(↑ 5.43%)	0.0348
(↑ 16.46%)	0.0572
(↑ 6.78%)
Attenwalk	0.1017
(↑ 11.51%)	0.0844
(↑ 9.28%)	0.1160
(↑ 9.65%)	0.0525
(↑ 17.70%)	0.0364
(↑ 11.33%)	0.0551
(↑ 10.81%)
ASNE (+)	0.0793	0.0674	0.0915	0.0398	0.0261	0.0428
MutualRec	0.1134	0.0922	0.1272	0.0618	0.0405	0.0611
Gowalla	Flixster
Crossfire (+)	0.1080	0.0947	0.1109	0.0655	0.0393	0.0616
NJM	0.1251
(↑ 13.17%)	0.0989
(↑ 26.98%)	0.1264
(↑ 16.77%)	0.0879
(↑ 25.39%)	0.0538
(↑ 22.90%)	0.0866
(↑ 13.31%)
ELJP (+)	0.1038	0.0854	0.1173	0.0837	0.0493	0.0842
MGNN	0.1278
(↑ 10.80%)	0.1024
(↑ 22.66%)	0.1328
(↑ 11.14%)	0.1017
(↑ 8.36%)	0.0640
(↑ 3.44%)	0.0960
(↑ 2.19%)
Bay’DNN (+)	0.1048	0.0893	0.1211	0.0613	0.0251	0.0622
Graphsage	0.1312
(↑ 7.95%)	0.1106
(↑ 13.56%)	0.1399
(↑ 5.48%)	0.0930
(↑ 18.52%)	0.0569
(↑ 16.26%)	0.0917
(↑ 7.04%)
HAN	0.1297
(↑ 9.21%)	0.1072
(↑ 17.13%)	0.1367
(↑ 7.97%)	0.0979
(↑ 12.58%)	0.0591
(↑ 11.88%)	0.0925
(↑ 6.12%)
Attenwalk	0.1328
(↑ 6.61%)	0.1130
(↑ 11.15%)	0.1419
(↑ 8.99%)	0.1026
(↑ 3.98%)	0.0622
(↑ 6.43%)	0.0948
(↑ 3.50%)
ASNE (+)	0.1083	0.0897	0.1204	0.0827	0.0476	0.0817
MutualRec	0.1416	0.1256	0.1476	0.1102	0.0662	0.0981
Note: ‘+’ means that the performance of MutualRec exceeds more than 30% in precision@3 compared with other approaches. ‘↑’ means the improvement in accuracy.

All of the findings illustrated by Table 4 show that MutualRec accomplishes better accuracy in social link prediction compared with other schemes from the evaluation.

5.5.3. Mutualistic attention analysis: RQ2
In this section, we mainly discuss the effect of our mutualistic attention mechanism. Because of pages limitation, we only show the results in Lastfm and Epinions datasets. To be persuaded, we conduct this comparison experiments from two aspects:

1
We design a concatenation layer to replace the mutualistic attention layer applied in MutualRec. The new MutualRec with concatenation layer is denoted as Simple_MutualRec. The parameters setting in Simple_MutualRec is as identical as that in MutualRec.

2
We add our mutualistic attention layer to the selected approaches. For social recommendation, we choose NJM, GraphRec, and DANSER; for links prediction, we choose NJM and GraphSage.

Concatenation Layer. We first state how to construct a concatenation layer in Simple_MutualRec. The main idea of the construction is that: We concatenate user preference embedding 
 and user social embedding 
 to get mixed embedding which is used to generate the new mutual preference embedding 
 
 and mutual social embedding 
 
. The new generated embeddings 
 
 and 
 
 will be the input part of predicted layers. The details of constructing a concatenation layer are formulated as:(24) 
 
 
 

The comparison results of MutualRec and Simple_MutualRec are shown in Fig. 3. In Fig. 3, it is obvious that MutualRec is superior to Simple_MutualRec in precision, recall and NDCG because the red line is over the blue one. Specially for social recommendation task, the difference between MutualRec and Simple_MutualRec are more remarkable in Epinions than that in Lastfm, which has more than 20% increase in precision and NDCG. Besides, several viewpoints are concluded from Fig. 3: 1) The variation tendency of precision is contrary to that of recall. 2) With an increase in N, the difference between MutualRec and Simple_MutualRec in these three metrics are becoming larger shown in Fig. 3 (b)–(d).

Fig. 3
Download : Download high-res image (971KB)
Download : Download full-size image
Fig. 3. Self-comparison between MutualRec and Simple_MutualRec.

In the next contexts, we continue to validate the effect of mutualistic attention layer from a second aspect, i.e., adding mutualistic attention layer into the selected approaches including NJM, GraphRec, DANSER, and GraphSage. We use “New_name” to represent the new constructed approaches, e.g., New_NJM or New_DANSER, etc. Considering the property and task of each selected approaches, the output of the mutualistic attention layer becomes one embedding to better adapt these approaches (e.g., NJM, GraphRec, and GraphSage) while the original output is two embeddings. To facilitate understanding, we adopt the same symbols utilized in the original approaches to explain how to construct New_approach as follows. More details about NJM, GraphRec, DANSER and GraphSage are elaborated in Wu et al. (2018), Wu et al. (2019), Fan et al. (2019), Hamilton et al. (2017).

•
New_NJM: NJM is a jointly neural framework to the model social recommendation and social links prediction. In NJM, latent embeddings 
 and 
 are related with the final outputs, i.e., predicted ratings derived from user feedback model and predicted links derived from social links model. NJM assumes that 
 is related with target user preference and her friends influence in the last timestamp, i.e., 
 and 
, which is computed as follows: 
. 
 is related with latent preference embedding and latent link embedding at timestamp t − 1, i.e., 
 and 
, which is computed as 
, 
 and 
 are activation function in Wu et al. (2018). In New_NJM, we user mutualistic attention layer (MAL) to compute 
 and 
. For 
, 
 and 
 are regarded as the input of MAL, the output of MAL in New_NJM becomes one embedding, then we use same activation function 
 in this embedding to obtain 
. So does the same process in 
. The construction details about New_NJM is shown in Fig. 4 (a).

Fig. 4
Download : Download high-res image (913KB)
Download : Download full-size image
Fig. 4. Construction Details (best viewed in color). ‘MAL’ marked by yellow square means mutualistic attention layer, some structures which are unchangeable in the original approaches are simplified by blue square. The dotted portion represents that the modification with mutualistic attention layer between original scheme marked by ‘(He et al., 2015)’ and New_approach marked by ‘(Ding et al., 2017)’, respectively. (For interpretation of the references to color in this figure legend, the reader is referred to the Web version of this article.)

•
New_GraphRec: GraphRec aims to leverage social links information to accomplish better social recommendation. In GraphRec, user latent factor is obtained by concatenating item space and user space. In New_GraphRec, item space and user space are the inputs of MAL to get the user latent factor shown in Fig. 4 (b).

•
New_DANSER: DANSER is another social recommendation oriented approach which incorporates graph attention networks. In DANSER, four factors obtained from Dual GCN/GAT layer are user statistic preference factor 
, user dynamic preference factor 
, item dynamic attribute factor 
 and item static attribute factor 
. Authors directly applied element-wise multiply operation on any two of them. In New_DANSER, we add MAL for 
 and 
 and get the new 
 (i.e., new_
) and new 
 (i.e., new_
). Then we do the same operation as DANSER for new_
, new_
, 
 and 
 shown in Fig. 4 (c).

•
New_GraphSage: GraphSage is an inductive representative learning for the link prediction task. In Hamilton et al. (2017), authors considered the graph structure of neighborhood and derived new user embedding by concatenating aggregated neighborhood embeddings and user embedding. In New_GraphSage, We use MAL to replace concatenation operation to get the new user embedding shown in Fig. 4 (d).

The results between approaches and New_approaches are shown in Table 5, where N is set as 3 and 5. It is obvious that New_approaches are much better than the original ones in recommendation performance: 1) For social recommendation task, the increases in precision, recall, and NDCG are at least 3.56%, 9.29% and 1.46%, respectively. 2) For social links task, the average increases in these three metrics are 7.61%, 7.72%, and 6.05%. Besides, we also find several points from Table 4: 1) The increase in social recommendation is more remarkable than that in link prediction because the improvement proportion in precision@3 is higher, i.e., the largest improvement in precision@3 is 13.61 in social recommendation while it only attains 9.03% in link prediction. 2) Among NJM, GraphRec, DANSER and GraphSage, The improvement of recommendation performance in NJM is more notable than other three New_approaches no matter on social recommendation or link prediction task, which has the greatest increase in precision, i.e., 12.22% and 13.59% for social recommendation, 9.03% and 8.04% for link prediction in precision@3 and precision@5, respectively.


Table 5. Comparison between approaches and New_approaches.

Methods	Precision@3	Precision@5	Recall@3	Recall@5	NDCG@3	NDCG@5
Social Recommendation on Lastfm
NJM	0.1483	0.1215	0.0406	0.0567	0.0638	0.0773
New_NJM	0.1664	0.1380	0.0444	0.0624	0.0680	0.0784
Improve (%)	12.22%	13.59%	9.29%	10.23%	6.61%	1.46%
GraphRec	0.1637	0.1399	0.0431	0.0610	0.1585	0.1554
New_GrapRec	0.1717	0.1505	0.0481	0.0695	0.1622	0.1645
Improve (%)	4.88%	7.57%	11.52%	13.88%	2.31%	5.82%
DANSER	0.1695	0.1472	0.0476	0.0617	0.1542	0.1483
New_DANSER	0.1857	0.1524	0.0539	0.0661	0.1607	0.1562
Improve (%)	9.57%	3.56%	13.22%	7.04%	4.22%	5.29%
Social Recommendation on Epinions
NJM	0.03425	0.032	0.0182	0.02624	0.03627	0.0396
New_NJM	0.03891	0.0346	0.0212	0.02754	0.03842	0.0416
Improve (%)	13.61%	7.91%	16.96%	4.97%	5.91%	5.17%
GraphRec	0.0424	0.0392	0.0228	0.0346	0.0414	0.0434
New_GrapRec	0.0468	0.0431	0.0251	0.0366	0.0432	0.0457
Improve (%)	10.45%	9.98%	10.40%	6.06%	4.34%	5.29%
DANSER	0.0385	0.0318	0.0213	0.0286	0.0395	0.0404
New_DANSER	0.0417	0.0341	0.0239	0.0312	0.0414	0.0430
Improve (%)	8.24%	7.31%	12.57%	9.07%	4.92%	6.59%
Link Prediction on Lastfm
NJM	0.0978	0.0688	0.0795	0.0988	0.1021	0.1138
New_NJM	0.1067	0.0736	0.0849	0.1061	0.1080	0.1164
Improve (%)	9.03%	7.03%	6.88%	7.39%	5.70%	2.28%
GraphSage	0.1047	0.0709	0.0767	0.1243	0.1077	0.1176
New_GraphSage	0.112	0.0802	0.0849	0.1334	0.1153	0.1280
Improve (%)	7.66%	13.11%	10.82%	7.32%	7.13%	8.82%
Link Prediction on Epinions
NJM	0.0484	0.0403	0.0313	0.0475	0.0475	0.0546
New_NJM	0.0512	0.0435	0.0331	0.0504	0.0510	0.0575
Improve (%)	5.82%	8.04%	5.68%	6.12%	7.49%	5.30%
GraphSage	0.0532	0.0475	0.0327	0.0532	0.0549	0.0622
New_GraphSage	0.0554	0.0503	0.0351	0.0585	0.0594	0.0644
Improve (%)	4.17%	5.98%	7.49%	10.08%	8.20%	3.51%
Based on the analysis of Fig. 5 and Table .5, we conclude that our designed mutualistic attention layer (MAL) does improve the recommendation performance for both social recommendation and link prediction. MAL is scalable because it is easy to be transplanted to other approaches and further improve recommendation performance.

Fig. 5
Download : Download high-res image (429KB)
Download : Download full-size image
Fig. 5. 𝓝’s effect on performance.

5.5.4. Parameters effect: RQ3
In this section, we mainly discuss the sensitivity of parameters applied in MutualRec, w.r.t., sampling size, denoted by 𝓝 in the spatial attention layer and embedding dimension d (The other hyper-parameters have little impact on performance, so we skip the discussion on them for page limits). We use a control variable method to accomplish the experiments on parameters. Control variable method is a scientific method which keeps one parameter changeable while other parameters hold unchangeable during experiments. For instance, when we study the effect brought by sampling size in the spatial attention layer, we fix the embedding dimension d. The datasets used in this section are Lastfm and Epinions, where N is set as 3. The results of sampling size 𝓝 and embedding dimension are indicated as shown in Fig. 5, Fig. 6.

•
𝓝’s effect: We test the sampling size 𝓝 amongst {10, 20, 30, 40, 50} (d is defaulted as 32). As shown in Fig. 5, the metric values become larger with the increase of 𝓝 because the larger 𝓝, the more information will be accumulated to form the consumption preference embedding and social preference embedding.

•
d's effect: We test the embedding size 𝓝 amongst {16, 24, 32, 64, 128} (𝓝 is defaulted as 20). As shown in Fig. 6. For the Epinions dataset, the performance increase with the increase of embedding size. For the Lastfm dataset, with the increase of the embedding size, the performance first increases and then decreases. When increasing the embedding size from 16 to 64, the performance becomes significantly better. However, when the embedding size is fixed as 256, the performance becomes lowered on Lastfm dataset. It demonstrates that using a large embedding size has remarkable representation. Nevertheless, if the embedding size is too large, the performance may be negatively lowered. Therefore, we need to find an appropriate embedding size when training model.

Fig. 6
Download : Download high-res image (423KB)
Download : Download full-size image
Fig. 6. d's Effect on Performance.

6. Conclusions
In this paper, we propose a novel framework, MutualRec, to jointly model users' preferences and social interactions in social networks. MutualRec first uses a spatial and spectral attention neural network layers to capture users’ preference features and social features from observed data, and then merges these two features via a mutualistic attention layer to solve social recommendation and link prediction tasks simultaneously. Our experiments on four real-world datasets demonstrate the superior performance of MutualRec to current state-of-the-art approaches in the two tasks. Our future work includes applying graph neural networks and merging knowledge graph into graph neural networks for recommendations.