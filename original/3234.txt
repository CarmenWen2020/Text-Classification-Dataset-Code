Regarding the advantages of video for communications, it is expected that video transmission becomes an inseparable part of most IoT solutions which raises numerous challenges especially over LLNs (Low Power Lossy Networks). Considering these challenges and video transmission requirements for IoT applications, utilizing a proper application layer protocol plays an important role consequently. In this paper, a new application layer protocol for IoT video transmission scenarios, named VE-CoAP is proposed. VE-CoAP, which is an extended version of CoAP, develops complementary methods for improving CoAP performance and overcoming its challenges for video transmission scenarios especially over IoT and LLNs. These methods include pushing data from server to client, supporting parallel flows for data transmission as well as preserving video quality with a combination of guaranteed and best-effort transmission and also a ULP (Unequal Loss Protection) method for prioritizing data partitioned H.264 video sequence. To compare the performance of VE-CoAP to CoAP and HTTP, an evaluation considering metrics such as PSNR, SSIM and video delivery duration is performed. The evaluation shows that VE-CoAP outperforms its rivals for burst data transmission in both lossy or low loss networks and it also preserves video quality at a superior level compared to other protocols over LLNs.

Keywords
Application layer protocols
CoAP
IoT
LLN
Multimedia
Video transmission

1. Introduction
Advances in many aspects of IoT from hardware (such as higher performance devices with lower energy usage) to software (such as performance upgrade in data processing and networking capabilities) make it possible to integrate various internet-connected devices in daily life. These devices which classify into three scopes (Minerva et al., 2015) named people, machines and information with unique identity (e.g. IPv6), can interact with each other globally and transfer any kind of data using any kind of network.

The transferred data can be in the form of simple text such as environment temperature or more complex forms such as audio and video (Sheeraz et al., 2015). Even though simple text-based data constitute major part of data transferred by IoT devices right now, considering the advantages of video data such as high variety of information which can be extracted from video sequence as well as simplicity of interaction with humans, it is expected that number of multimedia and video-enabled devices increase in the near future. Based on predictions, over 82% of internet traffic will be dedicated to video by 2022 with over 77.5 exabytes of video data per month (Cisco Visual Networking Index, 2019). Despite the advantages of video transmission, some characteristics such as high volume, unequal importance of different sections of video and sensitivity to delay (for some kind of videos), make it challenging for IoT video-enabled devices to transfer video through the network. Video transmission becomes even more challenging if devices use LLNs.

As mentioned before, IoT devices can use any kind of network to access the Internet. These networks and their protocols and standards can be divided based on their features. These features include I) coverage range (general grouping of networks which is based on the range they can cover including WPAN1 (e.g. Bluetooth), WLAN2 (e.g. IEEE 802.11-Wifi), WHAN3 (e.g. Zigbee, Z-Wave), LPWAN4 and etc), II) device types (low-end or high-end groups based on the type of devices they are suitable for (e.g. CoAP5 or IEEE 802.15.4 as light-weight protocol and standard for low-end devices) or III) specific network characteristics. The latest grouping divides networks based on their special characteristics. For example, when a device has hardware constraints or is used in a constraint environment (environment with lack of high-speed network infrastructure or lack of power source as well as difficulties for physical accessibility), it is advised to use a group of networks named LLNs (including LR-WPAN6). Some examples of LLNs standards and protocols are MQTT,7 CoAP and IEEE 802.15.4 with commercial names such as SigFox, NB-IoT, LoRA, ZigBee. LLNs are a group of networks that are suitable for low-end devices or conditions in which high-end devices being used on constraint environments. High loss rates (around 50%), low data rates (e.g. 250 kbps on IEEE 802.15.4), small MTU (e.g. 127 bytes on IEEE 802.1.5.4) and low power usage are some concrete examples of LLNs characteristics (Atzori et al., 2010; Al-Fuqaha et al., 2015; ThubertBrandtHui et al., 2012; Liekens, 2016; Deshpande, 2006).

Considering the diversity of IoT solutions, IoT devices can be used in vastly different locations ranging from urban to suburban areas. Deploying these devices in areas with the lack of a high-speed network or proper power source, led to the deployment of LLNs for data transmission. Also, as it is possible to use video data for solutions that use LLNs for data transmission (e.g. environment monitoring such as mountains or protected environments), video transmission in IoT becomes even more challenging as it may be transferred over both LLNs or high-speed low-loss networks. Therefore, it should be compatible with both of these network types.

Video transmission over LLNs is also considered in many WMSN8 and IoT researches from various aspects. These researches can be categorized as follows:

•
Studies of network loss rates for video streaming over IEEE 802.15.4 (Deshpande, 2006) or QoS requirements for MPEG video transmission over LLNs and ZigBee (Jasim and Çeken, 2015; Tanganelli et al., 2018) as well as studies of suitable configurations for H.264/SVC streaming over IEEE 802.15.4 (Pereira and Pereira, 2015a, 2015b)

•
Strategies for improving IEEE 802.15.6 for different applications such as video streaming (Alam and Ben Hamida, 2015)

•
Surveys about WMSN protocols, algorithms, standards and applications (including video streaming) (Akyildiz et al., 2007)

•
Methods such as dynamic adaptive streaming over IEEE 802.15.4 (and 6LoWPAN) (Pedro Martinez et al., 2013) as well as methods for MAC layer scheduling (Xu et al., 2014) or methods for increasing network lifetime, improving energy consumption and network utilization for H.264 streaming over IEEE 802.15.4 (Ammar et al., 2015; Farhad et al., 2016)

•
Framework for managing simultaneous video transmission over IEEE 802.15.4e (Taneja, 2015)

•
Evaluation of H.264 video transmission over IEEE 802.15.5 (Tanganelli et al., 2018)

However, utilizing a proper application layer protocol that fulfills IoT requirements and is compatible with video transmission can be influential on the performance of IoT video-enabled solutions. CoAP is one of the light-weight application layer protocols which is vastly used in many IoT applications and outperforms its rivals for video transmission scenarios (such as MQTT, HTTP, XMPP, DDS, Websocket and etc) (Ghotbou and Khansari, 2020). Supporting two-way communication as well as publish/subscribe and RESTful request/response service model, compatibility with low-end devices and LLNs (TSHC, 6LoWPAN, RPL) as well as high-end devices, supporting guaranteed delivery (despite being UDP-based), support bulk data transmission without fragmentation on lower layers and supporting proxies and caching with easy conversion to HTTP are some reasons for the popularity of this protocol (Silva, 2016; Bormann and Shelby, 2016; Shelby et al., 2010, 2014). CoAP is used for simple solutions such as temperature monitoring or on high-end devices for solutions such as video transmission. It is used as an application layer protocol for device management standards such as OMA LWM2M9 (OMA, 2019) and it also can be used on servers as an alternative to HTTP which can highly increase the performance and lowers the cost (Levä et al., 2014).

Various research studies consider CoAP as an application layer protocol for video transmission scenarios. These works include providing health management systems with the ability to transfer video using CoAP (IRehMo, All4Health) (Khoi et al., 2015; Plageras et al., 2016), studying applications of CoAP as well as standards for wearable things (including video transmission with CoAP) (Babu et al., 2015; Chu and Luptow, 2017), improving performance of CoAP block-wise transmission for video transmission (Choi et al., 2016), proposing architecture for guaranteeing QoS of video transmission using CoAP (Abu-Lebdeh et al., 2016), building smart mirror with video transmission ability using CoAP (Thandekkattu and Rao Vajjhala, 2017), methods such as dynamic adaptive streaming over CoAP (DASCo) (Krawiec et al., 2018) and sensing as a service with applications such as video streaming over CoAP (Wang et al., 2017).

Despite CoAP great features, there are some drawbacks for bulk-data transmission, especially video, which have not been appeared in the aforementioned related works to our knowledge. The problem in transferring bulk-data in multi-flows which cause HoL-blocking issue, lack of support for the combination of best-effort and guaranteed bulk-data transmission which is necessary for video transmission especially over LLNs and lack of ULP for video transmission are some of these issues (Bormann and Shelby, 2016; Shelby et al., 2010). But based on its complete set of features, it has great capability to be adapted to solve these issues and outperforms its rivals for video transmission.

In this paper, a CoAP-extended application layer protocol names VE-CoAP (Video Enabled CoAP) is introduced for IoT video transmission scenarios. By addressing the above challenges and considering video encoding contents as well as IoT requirements and LLNs constraints, VE-CoAP is developed to be compatible with these issues. VE-CoAP is also included with a sample newly proposed ULP method to prioritize data partitioned H.264 sequence and preserve the video quality up to an acceptable level. It also increases network bandwidth utilization by guaranteeing the quality of a video sequence with extremely large GoP10 (containing only inter-frames - around 300 frames and higher which decreases video size) over lossy networks (around 50% loss rate).

Due to emerging applications of video data in various IoT technologies and domains, including but not limited to smart homes, smart cities, smart environments and smart vehicles, there is still a lack of proper application layer protocol with increased video transmission performance. VE-CoAP can be amongst the first attempts to provide this solution considering IoT challenges. Regardless of employed network technology (existing ones such as Wi-Fi or LAN or more recent ones such as 5G) for transferring video data between two entities, VE-CoAP can replace the existing application layer protocol (e.g. original CoAP, HTTP, RTP, etc) to increase the performance of video transmission. Here entities can be any kind of devices such as user smartphone, middleware responsible for edge computing or high-end servers used for distributed video data analysis.

The rest of the paper is organized as follows: Section 2 reviews the requirements and challenges for IoT video transmission over CoAP. Section 3 introduces VE-CoAP and its features including push-based block-wise, repetitive best-effort and parallel data flow transmission as well as the ULP video packet prioritization method. Section 4 evaluates the performance of VE-CoAP for video transmission compared to some of its rivals. Finally, section 5 summarizes the pros and cons of VE-CoAP and proposes future works to compensate for the shortcomings of VE-CoAP.

2. Requirements and challenges of IoT video transmission using CoAP
Video traffic characteristics and IoT requirements bring up some challenges and requirements which a proper application layer protocol should address. As mentioned before, CoAP is one of the application layer protocols which can best fit these challenges and requirements. These challenges and requirements besides how CoAP faces them are discussed in the following section.

2.1. General challenges and requirements
In the following, challenges of video transmission over LLN are discussed and responses of CoAP to these challenges are presented.

1)
Packet header considerations: As video sequence consists high volume of data, fragmentation and packetization process led to a high number of packets to be transmitted. Also, if LLNs are used for video transmission, preventing fragmentation on lower network layers because of small MTU, cause even higher number of application layer packets to be transferred. Therefore, header size plays an important role in video transmission scenarios and therefore, a large header size would waste network bandwidth.

CoAP header: CoAP has a binary 4-bytes header which is one of the smallest headers compared to its rivals such as HTTP (Shelby et al., 2014).

2)
Compatibility with mesh topology: Many IoT networks specially LLNs, support mesh topology, in which motes without the ability to connect directly to the internet, can forwards their packets toward a border router with the help of their neighbors. Deploying mesh topology increases network hops and occupies neighbors' bandwidth. It also consumes neighbors' energy to buffer and forward a packet towards a desired route. Therefore, in the case of deploying mesh topology, packet transmission between the sender and receiver should be kept at a minimum or otherwise, delay and loss increase all over the mesh network. To minimize additional packet transmission at the application layer, routines such as asking for ACK messages should be kept at a minimum unless it is necessary. Also, for video transmission over mesh topology, it is suggested to utilize a proper video elements prioritization method which transfers unimportant elements of video using best-effort delivery.

CoAP mesh compatibility: CoAP supports both best-effort and guaranteed delivery and does not specify which method should be used for a certain kind of data. Therefore, it is up to the user/developer to choose what best fits the situations (Shelby et al., 2014).

3)
Preserving video quality over high packet loss rates: Video transmission over LLNs with high loss rates requires various methods such as special encoding/decoding or loss protection by network protocol to preserve video quality at an acceptable level. On the other hand, these methods should not cause high delay or waste network bandwidth by deploying flow or congestion methods such as TCP. Generally, loss in unimportant elements of a video sequence can be concealed at the receiver and loss in important elements such as SPS11 or PPS12 will cause decoding problems. Therefore, a proper application layer protocol should be able to protect video elements based on their importance. Also, as the video sequence is sensitive to the order of received packets, the protocol should also consider this issue.

CoAP ability for protecting video at high network loss rates: Although CoAP is a UDP-based application layer protocol, it can guarantee packets delivery by the help of CON13 messages. Also, its block-wise transmission can preserve packets order by the helping of BLOCK1/BLOCK2 options (Bormann and Shelby, 2016; Shelby et al., 2014).

4)
Support hybrid packet transmission by deploying a proper video elements prioritization method: Basically, a video sequence consists of elements related to each other. Some of them such as quantization parameters, motion vectors and intra sections are considered as important parts in which other elements such as inter-sections depend on them. The decoding process will be interrupted if error or loss happens at these important parts whereas problems in un-important elements can be concealed and will have a negligible impact on video quality. Therefore, an application layer protocol responsible for video transmission should support both reliable/best-effort delivery which should be selected by a proper prioritizing method based on the importance of a packet payload that is carrying the video elements.

CoAP support for hybrid packet transmission and elements prioritization method: As mentioned before, CoAP deploys two kinds of messages named CON/NON14 for both reliable and best-effort delivery. Its block-wise method for bulk-data transmission can be used with both of these message types. In block-wise transmission, a bulk-data is chunked into small blocks and requested individually by a client (individual request for each block) and therefore, the transmission method is selected by the client. As the video stream is buffered on the server, the client has no intelligence whether a requested block contains an important video element or not. Therefore, the selected block delivery method (reliable/best-effort) by the client does not fit the content of the packet requested from the server and it also is not possible to change the requested block delivery method at the server-side (Bormann and Shelby, 2016; Shelby et al., 2014). As a part of this study, a new prioritization method for prioritizing H264 video elements is proposed which preserves video quality at LLNs high loss rates.

5)
Light packetization process: Generally, video data contains a high volume of data which fragmenting and packetizing it results in a huge number of packets to be transferred. As IoT devices are different from power consumption, network connectivity and the environment being used, therefore a protocol responsible for video delivery in IoT applications should be compatible with both high-end and low-end devices (devices being used on constrained environments and deploying LLNs). Due to the high number of packets to be transferred, packing and unpacking the data by protocol should be light-weight which means complex header/payload encryption and compression should be avoided.

CoAP packetization process: CoAP is basically designed for constrained devices and despite its complete set of features, it is one the lightest application layer protocols with simple binary header encoding, simple variable length header options and binary payload encoding which supports transmission of any kinds of data without additional encoding (Shelby et al., 2014).

6)
Supporting two-way scalar data transmission: Based on IoT requirements, Things should interact with each other which means they need two-way scalar data transmission. Therefore, for video transmission scenarios in IoT, application layer protocols should also support two-way communication.

CoAP two-way scalar data transmission: Despite CoAP RESTful request/response service model, each device can act as client and server at the same time which means all devices can establish two-way communication to send and receive any kind of data at the same time (Shelby et al., 2014).

2.2. CoAP specific challenges and requirements
1)
Prioritized video transmission with CoAP block-wise method: As mentioned in the previous section, CoAP block-wise method transfer each block by a request from the receiver and therefore the transmission method is specified by the receiver and not the server. This is unsuitable for prioritizing video sequence and protecting it with ULP methods (A method named push-based block-wise transmission is proposed to overcome this issue).

2)
HoL-blocking15 and video transmission interruption of CoAP block-wise transmission: In case of reliable block-wise transmission using CON messages, the client request a specific block and piggybacks the corresponding ACK message to the server by the next request. Therefore, after the server sends the requested block, the flow of data has to be paused for the ACK message to arrive. This method causes an additional delay which is more intensive at high network loss rates and can disqualify CoAP as a suitable protocol for sending delay-sensitive videos over lossy networks (A method named parallel block-wise transmission is proposed for this challenge).

3. VE-CoAP
By considering the challenges of the CoAP for video transmission, a CoAP-extended protocol for video transmission in IoT named VE-CoAP is proposed. VE-CoAP is an extended version of the CoAP which leverages its features to address raised issues for video transmission. The main challenges and the proposed solutions which constitute VE-CoAP are summarized in Table 1 and are described with details in the following sub-sections.


Table 1. CoAP and LLNs main challenges for video transmission and the proposed VE-CoAP solutions.

CoAP and LLNs main challenges for video transmission	VE-CoAP Solutions	Method name in VE-CoAP
Impossibility of transmitting prioritized video sequence packets with CoAP block-wise method	1. Enabling pushing blocks from server to client (as HTTP/2.0) as well as enabling support for variable transmission modes selected by a prioritizing method	Push-based block-wise transmission
2. Enabling repetitive best-effort transmission for semi-important video sequence elements (e.g. last P frames in each GoP) to decrease the probability of losing them	Repetitive best-effort block-wise transmission
Necessity of hybrid packet transmission by deploying a proper elements prioritization method to protect important video parts and increase network bandwidth utilization (e.g. sending unimportant part with best-effort delivery)	3. Proposing a prioritization method to classify data partitioned H.264 video sequence elements base on their importance to consequently 4 priorities (ZP, LP, MP, HP)	4. Proposing a complementary method to preserve video quality at an acceptable level at high loss rates in case of extending GoP up to large values (e.g. 300 frames) with only inter-frames which decreases the video sequence size and increase network bandwidth utilization	Video (H.264) elements prioritization method
HoL-blocking and interruption in CoAP block-wise video transmission	5. Enabling parallel flow of data for CoAP block-wise transmission to prevent HoL-blocking issue (and also persevering packets order)	Parallel block-wise transmission (Push-based)
3.1. Push-based block-wise transmission
The proposed push-based block-wise transmission replaces the original CoAP block-wise routine to push all blocks from server to client by a single request from the client. As mentioned before, CoAP original block-wise transmission transfers blocks from server to client by sperate requests sent from the client for each block (Bormann and Shelby, 2016). Basically, there are two main reasons to replace this routine with push-based block-wise transmission:

1.
Original block-wise transmission on both best-effort/reliable delivery methods (using NON/CON messages) needs a request from the client (Bormann and Shelby, 2016). This routine causes additional overhead for requesting NON messages.

2.
Since a request starts from the client for video elements, the message type is selected by the client and not the server. Trivially, the client has no knowledge about the importance of video elements. As a result, it is not possible to prioritize the video elements neither by the client nor by the server with CoAP original block-wise transmission.

To solve these issues, VE-CoAP proposes two solutions as a “push-based block-wise transmission”:

•
In the case of best-effort delivery method, if the server sends all the data blocks by a single request, there would be no need to transfer an additional message for requesting each block. This also increases the network bandwidth efficiency and preserves packets order by the help of CoAP BLOCK1/BLOCK2 options which exists in the original routine.

•
By pushing blocks from server to client, the server will be able to select a proper method for each element and change it continuously during the transmission.

Fig. 1 depicts a comparison between CoAP original push-based block-wise transmission and the proposed one. As it can be seen for the push-based block-wise transmission, block #2 and #3 are transferred from server to client without any message being transferred from client to server. Compared to the original block-wise transmission, which transfers two messages for NON types, the push-based method has one message less transferred for NON types. This routine can increase the transmission speed up to 41% if only NON type is used (Ghotbou and Khansari, 2018).

Fig. 1
Download : Download high-res image (191KB)
Download : Download full-size image
Fig. 1. A sample for CoAP original block-wise transmission (Bormann and Shelby, 2016) (a) and the proposed push-based block-wise transmission (b).

To implement the proposed method, several considerations should be taken into account. The proposed push-based method temporally stores the offset of data at the server-side until the end of block-wise transmission and also prevents CoAP transactions to be cleared after successful transmission of a block during the block-wise transmission. A detailed version of Fig. 1 sample for push-based block-wise transmission is shown in Fig. 2 which demonstrates some elements such as the message header used for video transmission with VE-CoAP push-based block-wise transmission. MID (Message-ID) which is kept fixed through a block-wise transmission, is used for matching messages to a correct CoAP transaction which is responsible for handling packet transmission. The format element, VIDEO_ENCODED is also added to an empty spot of CoAP format options which specifies whether the original block-wise or push-based block-wise should be used (selecting this option for a block-wise initial packet, triggers CoAP to use push-based block-wise transmission (VE-CoAP), otherwise, the original block-wise transmission will be used).

Fig. 2
Download : Download high-res image (492KB)
Download : Download full-size image
Fig. 2. A detailed example of push-based block-wise transmission.

3.2. Repetitive best-effort delivery for block-wise transmission
Repetitive best-effort delivery is a simple and new feature added to CoAP original block-wise transmission which makes it possible to transmit a specific packet more than once during block-wise transmission based on the desired configuration at the CoAP engine. This feature decreases the probability of losing a packet and by selecting a proper repeat time, repetitive transmission can have lower overhead than reliable transmission. Repetitive best-effort block-wise transmission is useful for bulk data sequences in which some packet loss in the sequence does not have a high impact on the whole data. For example, in the case of encoded video transmission, loss of progressive macroblocks at nearly the end of each GoP does not disrupt the decoding process but if delivered successfully increases the overall quality of the decoded video. Therefore, these elements can have importance between important elements (e.g. sequence quantization parameter) and non-important elements (e.g. data partition ‘C’ at H.264 data partitioned sequence) and can be sent repeatedly to decrease the loss probability and on the other hand, prevent imposing additional overhead of reliable transmission to the network (pausing the data flow for ACK message transmission).

Eventually, VE-CoAP deploys 3 degrees of priority to transfer packets (HP, MP, LP)16 and an additional zero-priority (ZP) to prevent a packet to be sent by block-wise transmission. Selecting HP for a packet causes VE-CoAP block-wise to send it reliably, selecting MP causes repetitive best-effort transmission during the block-wise transmission and selecting LP will send a packet once with best-effort method. The latter zero-priority is useful for situations where algorithms such as congestion control detect congestion and thus can force some unimportant packets to drop from the buffer before being transferred with the block-wise method by assigning these packets a zero-priority and therefore decrease the congestion as well as delay for transmitting important packets.

3.3. Video element prioritization method
As mentioned in previous sections, due to high loss rates and low bandwidth of LLNs as well as devices constraints, it is beneficial to use a hybrid method (best-effort and guaranteed) for video transmission over these networks. Hybrid transmission requires a method to analyze and prioritize different video elements as well as a protocol to support this kind of combined transmission.

Generally, an encoded video sequence consists of various elements with different importance levels. Thus, it can be resistant to losses and errors up to a certain degree based on the importance of the lost element. For example, losses in certain parts of an I frame can be concealed spatially and losses and errors in B and P frames can be concealed temporally. On the contrary, a small error at motion vectors or quantization parameters can deteriorate video quality. For video transmission in IoT especially over LLNs, the size of the video sequence should be kept at a minimum due to LLNs low rate bandwidth and it also should be resilient to errors and losses because of LLNs high loss rates. Given that devices using LLNs for data transmission are either constrained in terms of power supply/processing capabilities or used in constrained environments, a proper video encoding mechanism should be compatible with all of these challenges which means it should provide a highly compressed video sequence with highest resilience to errors and losses with lowest encoding complexity. It is clear that these challenges conflict with each other as can be seen in Table 2.


Table 2. Characteristics of an encoded video sequence.

Resilience to errors and losses	Encoding complexity	Sequence size
–	↑	Lowest∗
↓	Lowest∗	↑
Highest∗	↑	↑
∗ Desired values.

H.264 is one of the encoding methods with a moderate encoding complexity (compared to H265, VP10, …) as well as features for handling losses and errors (e.g. data partitioning for extended profile) (Ozer, 2015; Advanced video, 2003). Despite these features, the existence of an application layer protocol to cooperate alongside encoding methods such as H.264 can further extend the capabilities of encoder and decoder to address the mentioned challenges. VE-CoAP alongside with H.264 encoding provides the following features to help address the above challenges:

1)
Deploying ULP: Protecting important elements of a video sequence from being lost during transmission by requesting ACK from the receiver as well as sending unimportant elements with best-effort delivery, can reduce network bandwidth usage (compared to the transmission of all elements with guaranteed delivery) and preserve video quality at an acceptable level. This kind of data protection called ULP requires a sequence analysis and a prioritization method as well as a protocol with prioritization support.

2)
Extending GoP size: One of the methods used for reducing video sequence size is extending the GoP (Lawrence, 2017). Extending GoP with only inter frames can reduce the total size of the sequence significantly especially when there are low changes and motions in the video scenes. This is due to the fact that each inter frame (e.g. P or B) is encoded temporally based on previous or subsequent frames within a GoP. Therefore, changes for videos with nearly similar frames can be predicted by motion vectors and therefore, the macro-block difference matrix will have values close to zero which results in a highly compressed frame and reduced overall video sequence size. For videos with high scene changes and motions, based on the encoding method, extending GoP with only inter frames cause less decrease in sequence size compared to the former case. For example, in case of a 100% difference between each frame of a GoP, it would not be possible to match any macro-block to the related frame via motion vectors and these frames would be encoded similar to I frame (will have nearly the same size). By the way, extending GoP with inter frames is beneficial on overall video sequence size as it is very rare to have a sequence with completely different induvial frames. To better understand the influence of GoP length on the overall size of a video sequence, an example for Foreman video sequence (CIF, 30 FPS, 10s, IPBPB …) is shown at Table 5. Despite this advantage, if loss happens at the important frames of a large GoP with only inter frames, it will propagate through the entire GoP until a new GoP and IDR frame appears. This issue causes more quality degradation of video transferred over LLNs as the loss rate may reach up to 50% or more. To address this issue, our VE-CoAP corresponding video elements prioritization method should discover and protect elements which are influential on error and loss propagation.

As mentioned before, based on “push-based block-wise” and “repetitive best-effort delivery” features introduced to CoAP, under VE-CoAP name, it is possible to deploy a custom prioritization method to protect video with ULP method which increases network bandwidth utilization, decreases nodes processing and power usage and also preserves video quality at a superior level.

In this section, a new video element prioritization method for data partitioned H.264 sequence, as one of the suitable encoding methods for IoT and LLNs video transmission scenarios is introduced. This method is explained in the following section and its prerequisites are explained in Appendix A.

3.3.1. The proposed prioritization method for data partitioned H.264 sequence
When a part of video data is read from the video buffer and get packetized by VE-CoAP, it passes through an additional step before being sent by the sender. First, the desired prioritization method analyzes the packet payload to find which video element is being sent by this packet. In the next step, the packet will be prioritized by the prioritization method into VE-CoAP priorities (HP, MP, LP or zero). Then the VE-CoAP selects the transmission method (reliable, repetitive, best-effort or drop from transmission buffer) based on these priorities. This routine is shown in Fig. 3.

Fig. 3
Download : Download high-res image (289KB)
Download : Download full-size image
Fig. 3. Video transmission steps in VE-CoAP.

Based on the VE-CoAP architecture, it would be possible to develop a prioritization method for any encoder. Here, the proposed prioritization method is adapted for H.264 as it is suitable for video transmission in IoT and especially over LLNs. Due to the high computation complexity of encoding methods such as HEVC, it is not suitable to deploy them on low-end or constraint devices, but they can be adapted for VE-CoAP in case of high-end devices (Ozer, 2015). The proposed method analyzes the packet payload by searching for the NALU header and specifies the corresponding element type and packet priority by the help of NALU type and NALU priority set by the encoder (nal_ref_idc). This method also takes care of loss propagation resulted from extended GoP length. The process of prioritization is as follows:

•
Loss of IDR, SPS, PPS and I slice elements of the video sequence can cause deterioration of decoded video quality as they are references for many subsequent frames. Therefore, they should be protected by the highest level of VE-CoAP priority (HP). This is also true for partition A of slices which are reference for other slices (e.g. partition A of P frames). Therefore, if nal_ref_idc for a NALU is 3 and it is not a partition B or C of a reference slice, the priority of packets containing this kind of NALU is set to VE-CoAP HP.

•
Partition B or C of a slice, whether it is a reference for other slices or not, gets VE-CoAP LP. The idea came from (Porter and Peng, 2011). Also, Partition A of a non-reference slice would get VE-CoAP LP.

•
As mentioned in previous sections, an H.264 video sequence does not specify NALU length in the NALU header. Therefore, the length should be calculated by measuring the distance between two NALUs start code or header (also used for decoding NALU). In case of losing these two, the decoder will not be able to use the corresponding NALU and this will cause deterioration of video quality if the loss happens at important NALUs. Therefore, packets carrying each of these two parts also get VE-CoAP HP.

•
Because of MTU limitations in LLNs, it is likely for a NALU to not to be fit in one single packet. Therefore, there will be situations where a packet contains part of an important NALU as well as part of an unimportant NALU. In this case, the packet gets VE-CoAP priority based on the priority of the most important NALU which exists in that packet.

•
The last part of the proposed prioritization method is dedicated to the case of extended GoP sequence transmission. As mentioned before, despite the great benefits of extending GoP with only inter-frames for video streaming, error or loss of key video elements can propagate through the end of GoP which highly decreases overall delivered video quality. To prevent this issue, a simple light-weight method is proposed which uses the size of NALUs and video elements for determining NALU priorities.

When a slice or frame has high similarities with its previous frame or slice (e.g. in slow camera movement), major differences of encoded frames will appear in motion vectors. Therefore, in this case, subsequent B or P slices will have nearly the same size. On the other hand, when a new moving object appears in the video scene or the camera turned quickly (in a way to cause scene change), the corresponding frame size will highly increase. In this case, the encoder cannot estimate blocks of new the frame based on blocks of its previous or subsequent frames and therefore, end up encoding them spatially which is the reason for an increase in frame size. Also, encoding these blocks temporally causes lots of various non-zero values in the difference matrix. Therefore, compression methods (such as entropy coding) cannot properly compress these various non-zero values which cause an increase in overall encoded slice or frame size. Based on observations, a scene change will at least increase a NALU size by 10 times compared to the mean size of previous NALUs that exists in a part of the video with less motion.

The next issue is succession and relation between encoded video frames as well as loss concealment methods. When a loss happens at a part of large GoP containing scenes with low motions, the concealment methods can use previous or subsequent frames to estimate the lost part with high accuracy. Instead, when a loss happens at a frame of GoP containing scene change or high amount of motions, the previous or successor frames cannot be used to estimate the lost part. Also, the subsequent B and P frames cannot be decoded correctly as their reference cannot be reconstructed based on its previous frames. This causes the propagation of error to the end of GoP even with the existence of an error concealment method.

To prevent this issue, some I-frames can be periodically placed in a GoP (Árpád and Imre, 2010). This will have the same effect as small GoP and will increase the total video size. Also, these frames are not beneficial in places where a low amount of motion happens in the video. The other preferred method (the proposed method) is to leverage VE-CoAP features and prevent losing frames marked as “frames with a sudden increase in size”. It is probable that these frames contain scene changes and will be protected by VE-CoAP HP.

To find these important NALUs (NALUs that cannot be concealed with their previous NALUs due to major scene differences), the size of ready-to-send NALU is compared with the mean size of previous NALUs calculated with a moving average (Formula 1). Our experiments show that in case of at least 10 times increase in the size of specified NALU, it should be marked as an important element and be assigned with VE-CoAP HP. The update equation of the mean value is shown in Eqn. (1) and is explained in Appendix B (M: NALUs mean length until time t, L: Ready-to-send NALU length).(1)Mt+1 = 0.8∗Mt + 0.2∗L

3.4. Parallel block-wise transmission (along with the push-based method)
Another issue for both CoAP original block-wise and the proposed push-based block-wise transmission is HoL-blocking issue. In HoL-blocking, if packets transferred using CON messages, the flow of data should be paused for each packet until the corresponding ACK delivers to the sender. Blocking the flow of data is unsuitable for video transmission specially delay-sensitive video delivery.

To solve this problem, a new parallel block-wise transmission method that leverages CoAP transactions to make VE-CoAP able to simultaneously transfer a video sequence through different lines or flows is proposed. The state diagram of a line in the parallel push-based block-wise transmission is present in Fig. 5. In the proposed method, if a block of a sequence needs to be transferred with a CON message, the line responsible for this block will change from transferring state to busy state after sending the block until the corresponding ACK arrives. Then it will get back to free state. In the meanwhile, other blocks will be transferred through other lines with free state. In this way, the flow of data will only be blocked if all lines are at busy state.

Fig. 5
Download : Download high-res image (293KB)
Download : Download full-size image
Fig. 5. State diagram of a line in VE-CoAP parallel push-based block-wise transmission.

An example of transmitting 8 blocks using VE-CoAP parallel push-based block-wise transmission with sample 4 lines is shown in Fig. 6. Also, to better clarify the difference between push-based block-wise transmission and parallel push-based block-wise transmission, Fig. 7 compares the transmission of 4 blocks using the original and the proposed methods. It is worth mentioning that by using CoAP block number (BLOCK options), the offset for every block of a sequence in the receiver buffer can be calculated. Therefore, using the proposed parallel push-based method is not affected by the out-of-order issue.

Fig. 6
Download : Download high-res image (364KB)
Download : Download full-size image
Fig. 6. Difference between push-based block-wise and parallel push-based block-wise transmission.

Fig. 7
Download : Download high-res image (590KB)
Download : Download full-size image
Fig. 7. Sample parallel push-based block-wise transmission (NON for best-effort and CON for reliable delivery).

4. Evaluation
For the evaluation, the performance of VE-CoAP is compared to its rivals. The evaluation is performed by considering metrics for measuring video quality and network bandwidth utilization. As video transmission can be delay-sensitive for video streaming scenarios or non-delay sensitive for on-demand access,17 evaluation is conducted for both scenarios. Also, as video can be transferred over mesh topology, the evaluation is also performed over 1 (Direct), 2 and 4 hops.

Protocols used for this evaluation are as follows:

•
CoAP as the base protocol for VE-CoAP

•
HTTP/1.1 as the most used protocol over the web and video streaming methods such as MPEG-DASH (Sodagar, 2011)

Metrics used for this evaluation are as follows:

•
PSNR and SSIM for measuring the quality of the delivered video (Hore and Ziou, 2010)

•
Normalized PSNR for evaluating the overall quality of delay-sensitive video sequences. In the case of delay-sensitive video transmission such as video streaming scenarios, frames exceeding the deadline will drop from the sender buffer. In these scenarios, protocols which cause fewer frames to drop from the buffer and deliver more decodable frames with higher quality to the receiver, have better performance. This is calculated as a portion of decoded frames PSNR to the total number of frames which is shown in Eqn. (2).(2)
 

•
Video delivery duration metric for measuring network bandwidth utilization. In the case of non-delay sensitive video transmission, it can be said that protocols which send a specified number of frames faster, have better utilized the network bandwidth. This metric should be considered along with the PSNR metric to evaluate the overall performance of a protocol for video transmission.

4.1. Network parameters
The evaluation is performed with Cooja motes on Contiki 3.1 (Contiki, 2019) with the Cooja IoT emulator/simulator (Simulator, 2019). Erbium implementation is used for CoAP and VE-CoAP (Kovatsch et al., 2011) and uIP implementation for TCP and UDP (Dunkels, 2001). As VE-CoAP and similar protocols can be used for both lossy and low loss networks, evaluation is performed from 0% to 50% packet loss rate with the step size of 5%, test videos are transferred 10 times at each step and over 100 k times for overall evaluations in various settings. To prevent fragmentation on lower layers and transfer an equal number of video packets by each protocol, the payload is considered 32 bytes. The next consideration is about the RDC18 algorithm which can interfere with the application layer protocols routine for video transmission and invalidate the evaluation results. As video is a bulk data and packets get transferred unstoppably, radio should be kept on during video transmission or on/off transition should be in a way to prevent additional delay which both require proper RDC algorithm to detect video transmission and act consequently. As this kind of RDC algorithm does not yet exist, in these evaluations, RDC has turned off during video transmission to prevent invalidating the results. For performance evaluation of delay-sensitive video transmission, a hypothetical delay of 2 s is considered (initial delay for buffering). Video delivery duration is calculated using a Cooja script, videos are encoded and decoded using JM (Software) and FFMPEG (FFmpeg) and video quality (PSNR and SSIM) is measured using VQMT (Vatolin et al., Osipov). Other network parameters are shown in Table 3 which are default parameters of Contiki 3.1.


Table 3. Network parameters used for evaluations.

Parameter	Value
Framer	Framer 802.15.4
MAC	CSMA
MAC ACK	On
Network	6LoWPAN
Internet protocol	IPV6
Routing	RPL
CoAP max transactions	4
Channel bandwidth	250 kbps
For video transmission using CoAP, its original block-wise routine with CON messages is used which sends each block and piggybacks the corresponding ACK message. For video transmission using HTTP, two methods are considered. In the first method, each block is requested with a GET message through URL query. Based on the default stateless routine of HTTP, for each request, a TCP connection is established, the corresponding chunk gets transferred and the TCP connection gets closed. In the second method, the whole video sequence is requested in one HTTP GET message and the process of splitting and merging chunks is delegated to TCP. In this method, a connection is made between the receiver and the sender and remains open until the end of the transmission. Therefore, HTTP doing nothing but preparing a GET request and receive its response as a single file.

In the case of video transmission over error-prone environments or lossy networks, preserving video quality can be done by protocols responsible for delivering packets or other error/loss recovery methods such as error concealment. To emphasize the importance of protocols responsible for preserving video quality, two sample videos are chosen for evaluations that have special characteristics such as scene change. These characteristics make it hard to preserve video quality using recovery methods and show the importance of a proper application layer protocol role.

The first video is Foreman which has a scene change in the middle of its sequence. The second video is 10 s of the Home documentary19 titration. In addition to scene change, it has other special characteristics that make it one of the most challenging video sequences to preserve its quality during transmission. This evaluation will show that if a method can preserve the quality of the Home video sequence at an acceptable level, can easily preserve the quality of video sequences such as Foreman. Some frames of this sequence are shown in Fig. 8 and its characteristics are as follows:

•
This sequence starts with a full black frame and scenes gently appear after a while. Generally, a frame with equal value for all pixels will result in small encoded frame size as compression methods such as entropy coding can highly compress this kind of data. This makes the first frame (which is usually an IDR) to be even smaller than subsequent P or B frames and therefore the size of this frame cannot be used by protocols to decide whether to protect this kind of frame or not. Also, as this frame is black, it cannot be used as an alternative frame for subsequent lost frames (e.g. P and B) within a GoP. This feature makes recovery methods that simply preserve video quality by protecting I frames from error and loss, fail to preserve the quality of this video sequence.

•
Until the middle of the video sequence (around 5 s), few changes can be seen in scenes (slight and gentle camera movement) which makes the first part of the sequence similar to the Waterfall video sequence.

•
The 5th and 6th seconds of this sequence is the most important and challenging part of this sequence. In these moments, a scene change happens (like Foreman) and a new highly dynamic scene with four special types of characteristics shows up which makes it extremely challenging for video quality preservation methods and protocols to deal with. These characteristics are as follows:

Major changes with slight movements (low dynamics): The moment when the trees scene changes to lake and mountain scene which are moving as the camera moves.

Major changes with high dynamics: For 1 s (5th to 6th second), the mountain below the scene moves very quickly and highly influences nearly half of the scene which just completely changed in the past few moments. As this change occurs in the middle of the GoP, it has increased the size of many corresponding B and P frames and thus, has made them a reference for subsequent B and P frames (similar to what was explained in 3.3.1).

Small changes with low dynamics: At the moment of the scene change, South Africa's name in the lower-left corner of the sequence, changes to Alaska and remains there until the end of the video sequence. This is a small change compared to other changes happening simultaneously. In case of error and loss, a proper video quality preservation method can correctly change this part of frames (change the name from South Africa to Alaska) and keep it readable until the end of the sequence (preserve this small part as well as major changes happening simultaneously).

Small changes with high dynamics: Again, at the moment of the scene change, there is a titration moving up quickly from the bottom of the page which consists small portion of the overall frame but should be kept readable in case of loss or error which also makes it challenging for quality preservation methods and protocols.

Fig. 8
Download : Download high-res image (1MB)
Download : Download full-size image
Fig. 8. Sample frames of Home video sequence.

Finally, to properly evaluate VE-CoAP performance, both Foreman and Home video sequences are encoded and transmitted from server to client. The encoding settings are shown in Table 4.


Table 4. Encoding settings of video sequences.

Video\ Parameter	Foreman	Home
Resolution	352x240 (CIF)	640x320
FPS	25	30
Frames Sequence	IPbPb …	IPbPb …
Duration	12s	10s
File Size	164 KB	176 KB
Codec	H.264	H.264
Profile	Extended	Extended
Data partitioning	✓	✓
GoP	300	300
# of packets with 32-byte payload	5267	5640
As this evaluation is performed on one of the LLNs standards (IEEE 802.15.4) with the network bandwidth of 250 kbps (Al-Fuqaha et al., 2015), these sequences are encoded below the specified bitrate to prevent packet loss due to exceeding network transmission capacity. The other important issue is the length of GoP. As mentioned before, increasing the GoP length decreases the overall size of the encoded video sequence (e.g. H.264) and increases the probability of error propagation through the GoP. Also, even if every frame has a completely different scene compared to its previous frame, increasing GoP length does not make the overall video size bigger than a video with a smaller GoP length. Here the reason is that the corresponding P or B frames have more intra-coded macroblocks or bigger difference matrices which makes the frame size nearly equal to an I frame. In other words, the overall video size is nearly equal to encoded video with a GoP length of one. To demonstrate the amount of compression that can be reached by increasing GoP length, Foreman and Home video sequences are encoded with different GoP lengths. The overall size of videos based on their GoP lengths are shown in Table 5. Lower video size can save costs and energy of motes deployed in remote areas (ability to transfer video for a longer time). It also makes it possible to encode videos with higher quality and transfer it through low network bandwidth of LLNs.

4.2. One-hop transmission
In this section, one-hop or direct video transmission over HTTP, CoAP and VE-CoAP is evaluated considering both delay and non-delay sensitive video transmissions. To transfer packets directly, local IPv6 addressing (fe80::) is used.

In the case of non-delay sensitive video transmission, protocols with packet delivery guarantee such as HTTP and CoAP have the same delivered video quality as packets won't drop from the network or video buffer due to exceeding deadline. In these scenarios, the “video delivery duration” metric can be used for comparing these protocols. Also, as packets transfer directly between the sender and receiver, the one-hop transmission has the shortest overall video delivery duration compared to a multi-hop transmission.

In the case of delay-sensitive video transmission, protocols with the shortest delivery duration can have better-delivered video quality as it is possible to deliver more packets before their deadline exceed.

4.2.1. Non-delay sensitive video transmission
As can be seen in Fig. 9.a and Fig. 10.a for Home and Foreman video sequence, increasing network loss rate increases the delivery duration of protocols with packet delivery guarantee (HTTP and CoAP) as well as hybrid delivery guarantee (VE-CoAP) as these protocols should retransmit the lost packets. As can be seen, requesting every video chunk with a GET message as well as opening and closing TCP connection for every request, made HTTP the worst protocol compared to others (first method of HTTP transmission). By comparing CoAP with the second method of HTTP transmission (using TCP for splitting and joining video chunks), it can be seen that increasing the network loss rate has increased CoAP delivery duration more than HTTP. This happens due to the CoAP retransmission mechanism which retransfers the lost packet based on a timer that has been set for each CoAP transaction. When a packet gets lost, CoAP waits for a pre-set timer which was randomly initiated at beginning of each packet transmission for each transaction (In Erbium implementation of CoAP in Contiki, it has large values around 1–3 s). Then, CoAP retransfers the lost packet and doubles the timer. If this packet gets lost again, the timer will be doubled again and this will continue until CoAP max retransmission value exceeds (the default value for Erbium implementation is 4). Based on the above discussion, it is clear that over networks with high loss rates which cause a packet to be retransmitted several times, the CoAP retransmission method causes a large amount of delay compared to the second method of HTTP transmission and will prolong video delivery duration. Changing some of these parameters can improve CoAP and VE-CoAP performance which will be discussed in 4.4.2. As can be seen in Fig. 10.b, as VE-CoAP prioritizes parts of the video sequence and use different transmission methods, it has a faster delivery duration compared to HTTP (second method – TCP used for chunks)/CoAP. To better illustrate the duration values, these charts without HTTP (first method) are shown in Fig. 9.b, Fig. 9.c and Fig. 10.b. Also, detailed values are shown in Table 6 and Table 7.

Fig. 9
Download : Download high-res image (434KB)
Download : Download full-size image
Fig. 9. a) Delivery duration for Home video sequence in One-hop transmission, b) HTPP excluded, c) up to 40% loss rate.

Fig. 10
Download : Download high-res image (298KB)
Download : Download full-size image
Fig. 10. a) Delivery duration for Foreman video sequence in One-hop transmission, b) HTPP excluded.


Table 5. Size of Foreman and Home sequences encoded with different GoP lengths.

GoP Length	4	8	16	32	64	300
Home	Video Size (KB)	787	543	421	365	332	303
% Increase (Compared to 300)	44.9%	29%	15.3%	9.9%	9.6%	–
Foreman	Video Size (KB)	333	259	220	204	193	186
% Increase (Compared to 300)	28.6%	17.7%	7.8%	5.7%	3.8%	–

Table 6. One-hop delivery duration for Home video sequence.

Network loss rate (%)	Mean delivery duration for Home (s)
CoAP	HTTP (TCP for chunks)	HTTP	VE-CoAP
0%	56.3	57	453.9	47.6
5%	68.3	68	544.9	60.6
10%	82.7	81.9	652.6	70
20%	120	118.1	943.9	96.6

Table 7. One-hop delivery duration for Foreman video sequence.

Network loss rate (%)	Mean delivery duration for Foreman (s)
CoAP	HTTP (TCP for chunks)	HTTP	VE-CoAP
0%	52.8	52.9	423.4	44.3
5%	63.9	63.6	508.4	56.3
10%	73.3	76.5	609.6	67.4
20%	111.9	110.2	883.3	90.8
In the next step, qualities of delivered video sequences that are measured by PSNR and SSIM for Home and Foreman are shown in Fig. 11.a, Fig. 11.b, Fig. 12.a and Fig. 12.b. As expected, because CoAP and both HTTP methods fully guarantee delivery of packets, PSNR and SSIM of delivered video with all of these protocols are the same and also the same as the original video which was coded with the extended profile of H.264. As some video elements such as SPS or reference frames are extremely important for video transmission (Advanced video, 2003), losing even one packet of these elements can cause the whole video to be undecodable or be damaged. This can be seen in Fig. 13 where only a single packet from PPS with nal_ref_idc of 3 from the Home video sequence is lost (0.0001% of overall video size) and the effect propagated through the end of the GoP. The upper frame is the original frame where the below frame is the one with the lost packet. This picture clearly demonstrates the importance of a single packet with important non-VCL data.

Fig. 11
Download : Download high-res image (258KB)
Download : Download full-size image
Fig. 11. Video quality based on two measures for Home video sequence in one-hop transmission, a) PSNR, b) SSIM.

Fig. 12
Download : Download high-res image (242KB)
Download : Download full-size image
Fig. 12. Video quality based on two measures for Foreman video sequence in one-hop transmission, a) PSNR, b) SSIM.

Fig. 13
Download : Download high-res image (431KB)
Download : Download full-size image
Fig. 13. Impact of single packet loss from PPS element (Appendix A) of Home video sequence (with nal_ref_idc: 3) - Original frame (right), Affected frame (left).

At last, for VE-CoAP, because of using the packet prioritization method, video quality has slightly degraded as the network loss rate increases but it is kept at an acceptable level even at 55% of network loss rate.

By reviewing video delivery duration and video quality in figures of this section, it can be seen that VE-CoAP mechanisms (ULP, Parallel block-wise transmission and push-based block-wise transmission) not only increased VE-CoAP packet delivery speed but also preserved the quality of the two most difficult video sequences (which have scene change, great amount of motions and also encoded with extremely large GoP with the value of 300) during transmission in one the most devastating network conditions up to loss rates of 55%. It is also can be seen that even in low loss rates, VE-CoAP has the best performance compared to other protocols and it can be used even in high-speed network infrastructures with low loss rates to improve video transmission performance.

4.2.2. Delay sensitive video transmission
To evaluate the performance of VE-CoAP and other protocols for delay-sensitive video transmission scenarios, a hypothetical 2-s deadline is assumed for each frame (more precisely for each NALU). Frames that are not sent within the deadline will be dropped from the video buffer without being passed to the network buffer. As mentioned at beginning of section 4, protocols able to deliver more decodable video frames with higher quality within the deadline (e.g. PSNR) can be selected as proper protocols for delay-sensitive video transmission scenarios. This is measured by a metric called normalized PSNR which was shown in (2) at the beginning of section 4.

As a video sequence usually has different elements with different importance levels, transferring sequences with bitrates higher than network bandwidth in delay-sensitive scenarios such as live video streaming may lead to loss of important elements and deterioration of video quality. Therefore, in this part of the evaluation, as video corresponds to the network bandwidth, it is expected that application layer protocols with lower video delivery duration have better performance for delay-sensitive scenarios at low network loss rates. In higher network loss rates, it is expected that faster protocols with full or selective packet delivery guarantee (e.g. VE-CoAP) show better performance. Therefore, as the first method of HTTP had the worst transmission duration amongst others with no additional benefit for video transmission, it is omitted from the rest of the evaluations.

The results for the Foreman video sequence is shown in Fig. 16. As it can be seen in Fig. 16.a, all of these protocols delivered nearly equal number of decodable frames at network loss rates below 20%. It can also be seen in Fig. 16.b that by expectations, VE-CoAP delivered more decodable frames with higher quality (PSNR) as it had the shortest video delivery duration compared to the second method of HTTP transmission (TCP used for chunks) and CoAP in the previous section. Finally, it is worth mentioning that as the network loss rate increases, the effective network bandwidth decreases for protocols with packet guarantee as lost packets should be retransmitted. This causes video bitrate to be higher than effective network bandwidth which led to loss of packets due to exceeding deadline. Thus, the need for concealment methods is increased. This is why VE-CoAP shows better performance than both HTTP (second method) and CoAP which can be seen in Fig. 16.c.

Fig. 16
Download : Download high-res image (364KB)
Download : Download full-size image
Fig. 16. a) Mean number of (mean of evaluation iterations) decoded frames (10 times iteration for each point), b) PSNR and c) normalized PSNR for Foreman video sequence transmitted within a delay-sensitive one-hop transmission.

4.3. Multi-hop transmission
In multi-hop transmission, video is transmitted through 1 and 3 middle motes (2 and 4 hops) and evaluated by the same metrics used in the previous section for both delay-sensitive and non-delay sensitive video transmission scenarios. Generally, increasing network hops in LLNs causes an additional delay for packet transmission especially for protocols that require transferring ACK messages. For example, transferring a packet directly from the sender to the receiver with TCP requires at least four steps in addition to establishing a connection. In the first step, TCP sends a packet, then the MAC ACK message should be delivered to the sender. In the next step, TCP should send an ACK message for the delivered packet to the sender and the sender should transfer the MAC ACK message to the receiver. In this procedure, increasing one hop requires an additional 4 ACK messages to be transmitted between the sender/receiver and the middle mote. This is illustrated in Fig. 17 which clarifies why increasing network hops increases packet transmission delay, decreases effective network bandwidth and decreases video delivery performance (both duration and quality).

Fig. 17
Download : Download high-res image (224KB)
Download : Download full-size image
Fig. 17. Comparing one-hop and two-hop transmission steps for TCP with enabled MAC ACK.

4.3.1. Non-delay sensitive video transmission
In multi-hop transmission scenarios, it is expected that protocols with full packet delivery guarantee such as CoAP and HTTP as well as protocols with “selective packet delivery guarantee” such as VE-CoAP have lower performance compared to their performance for direct transmission. Also, there is more performance reduction for protocols with a full packet delivery guarantee.

Video delivery duration of HTTP (second method), CoAP and VE-CoAP for Foreman video sequence and two-hop transmission is shown in Fig. 18.a and with four-hop transmission is shown in Fig. 19.a. Also, to better compare results with one-hop transmission, some of these values for two-hop transmission are shown in Table 8. For VE-CoAP, protecting important video elements prevents the video from being undecodable even at high network loss rates and multi-hop transmission. It also makes video transmission faster than HTTP (second method) and CoAP. The difference is more obvious as network hops increase. PSNR and SSIM of Foreman video sequence for two-hop and four-hop transmission are shown respectively in Fig. 18.b/c and Fig. 19.b/c. It is worth mentioning that the uIP default configuration for TCP in Contiki failed to keep the connection open at network loss rates higher than 50% and interrupted video transmission.

Fig. 18
Download : Download high-res image (430KB)
Download : Download full-size image
Fig. 18. a) Delivery duration, b) PSNR and c) SSIM for Foreman video sequence in the two-hop transmission.

Fig. 19
Download : Download high-res image (390KB)
Download : Download full-size image
Fig. 19. a) Delivery duration, b) PSNR and c) SSIM for Foreman video sequence in four-hop transmission.


Table 8. Two-hop delivery duration for Foreman video sequence.

Network loss rate (%)	Mean delivery duration for Foreman (s)
CoAP	HTTP (TCP for chunks)	VE-CoAP
0%	134	133	127.6
5%	159.8	158.4	133.4
10%	186.7	184.9	146.8
20%	237.8	247.5	196.5
For four-hop transmission, two unusual cases should be analyzed. First, it can be seen that video delivery duration for CoAP has enormously increased at 25% loss rates compared to HTTP (second method) (Fig. 19a). As formerly discussed, this issue is due to the CoAP retransmission mechanism and initial parameters values. After each packet loss, CoAP doubles the initial waiting time of its timer which is around 1–3 s and then retransmit the lost packet. This routine causes a massive delay for video data which is bulk data with a high number of packets. Changing the initial values and the retransmission routine can improve CoAP performance for video transmission.

The second case is about VE-CoAP video quality which is weirdly higher at 5% loss rate compared to 0%. This is due to a lack of congestion control for the VE-CoAP best-effort block-wise transmission part which is responsible for transferring non-important video elements. At 0% loss rate, the number of packets that are successfully delivered to middle motes is greater than the 5% loss rate. Due to delay caused by middle motes which has decreased effective network bandwidth, lack of congestion control causes congestion in middle motes which results in more packet loss due to buffer overflow of middle motes compared to 5% loss rate. At 5% loss rate, nearly 5% of packets which are being transferred by NON messages (best-effort block-wise) get lost which decreases congestion at middle motes and led to lower packet loss due to buffer overflow. This issue can be solved by deploying suitable flow and congestion control mechanisms for CoAP and VE-CoAP best-effort block-wise transmission.

4.3.2. Delay sensitive video transmission
Delay caused by middle motes increases video delivery duration for multi-hop transmission compared to one-hop transmission. Consequently, we expect more packets drop in multi-hop delay-sensitive video transmission scenarios compare to one-hop transmission. This causes video quality degradation which led to unaccepted levels of video quality at higher network loss rates. As it can be seen in Fig. 20, normalized PSNR of Foreman video sequence transferred with two hops with the same deadline as previous sections (2 s), decreased below 25 at network loss rates higher than 10% for HTTP (second method), CoAP and even VE-CoAP. This shows that in these scenarios, in addition to the proposed methods, it is required to use either a video sequence with a lower bitrate or methods to dynamically adapt video bitrate based on the network condition. By the way, even in these situations, VE-CoAP has outperformed other reviewed protocols.

Fig. 20
Download : Download high-res image (423KB)
Download : Download full-size image
Fig. 20. a) Mean number of (mean of evaluation iterations) decoded frames, b) PSNR and c) normalized PSNR for Foreman video sequence transmitted within a delay-sensitive two-hop transmission.

4.4. Complementary evaluations
So far, we dealt with several performance metrics including PSNR, SSIM and “video delivery duration” for comparing VE-CoAP with other protocols. For the complementary evaluations, we investigate VE-CoAP overhead and try to improve its parameters. Moreover, the role of VE-CoAP in other IoT emerging technologies is discussed.

4.4.1. Protocols overhead
In the last part of the evaluation section, it is worth mentioning another advantage of VE-CoAP for video transmission compared to its rivals (except CoAP). As video is a bulk data, it requires tons of packets to be transferred. As each packet has a header that causes overhead, a small increase in header size can cause massive overhead which wastes network bandwidth that is a precious resource especially in LLNs. As VE-CoAP is based on CoAP, header overhead for both is the same. Therefore, a side-evaluation is performed to compare the header overhead caused by CoAP, TCP and UDP. In this part, the size of data is measured using Wireshark (2019) at transport and physical layers. This measurement can show that how much overhead these protocols have for 32 bytes payload (actual video data). The results are shown in Table 9. It can be seen that despite CoAP is an application layer protocol, it has lower overhead than TCP itself, which is another benefit of CoAP and VE-CoAP compared to TCP and all TCP-based application layer protocols (It is clear that any application layer protocol has additional overhead (header) in addition to its transport layer overhead). Also, despite VE-CoAP have a larger header compared to UDP itself, it has lower overhead than many UDP-based application layer protocols such as RTP with exclusive features for video transmission in both low-loss and lossy networks.


Table 9. Comparison for header overhead at different layers.

Protocol/Layer	Application Layer (Payload)	Transport Layer Size	Physical Layer Size
UDP	32 bytes	40 bytes (+8 bytes header)	80 bytes (+40 bytes rest of the layers)
TCP	32 bytes	52 bytes (+20 bytes header)	92 bytes (+40 bytes rest of the layers)
CoAP	32 bytes	49 bytes (+17 bytes header)	89 bytes (+40 bytes rest of the layers)
4.4.2. VE-CoAP performance with improved settings
As mentioned in previous sections, CoAP block-wise retransmission routine and initial parameters are not suitable for video transmission scenarios. In this routine, the lost packet has to wait for its transaction timer to reach zero and then be retransmitted. The timer doubles after each transmission with initial values that was randomly selected approximately between 1 and 3 s which cause massive delay for video transmission especially at higher network loss rates. This issue affects VE-CoAP guaranteed transmission (for high priority video elements) and thus overall video transmission is highly increased.

Therefore, Erbium and Contiki 3.1 default parameters are improved and then VE-CoAP performance for Foreman video sequence and one-hop non-delay sensitive transmission is evaluated again. The first change is the maximum number of simultaneous or parallel flows of data allowed, which was changed from 4 to 10. It is worth mentioning that in contrary to expectations, a higher amount of parallel flows (more than network capacity) not only won't increase performance but may cause congestion because of more simultaneous in/ongoing packets and ACKs that should be transferred which led to network buffer overflow and increase in packet loss.

The second change is preventing the CoAP retransmission timer to be doubled after each video packet transmission failure (for CON messages). The results for this change are shown in Fig. 21 under the title of VE-CoAP 2. Fig. 21.a and Fig. 21.b show PSNR and SSIM of VE-CoAP 2 compared to VE-CoAP which are closely the same and Fig. 21.c shows the duration of video transmission which shows up to nearly doubled transmission speed at a 55% network loss rate. The difference is clearer at higher loss rates as more retransmission required due to more packet loss.

Fig. 21
Download : Download high-res image (374KB)
Download : Download full-size image
Fig. 21. a) PSNR, b) SSIM and c) delivery duration comparison between VE-CoAP with the default and improved settings (VE-CoAP 2) for Foreman video sequence.

4.4.3. VE-CoAP and other IoT emerging technologies
In recent years, the introduction of IoT to existing systems and technologies raises new challenges. These challenges such as the high density of varied IoT devices in a small area, high volume of data produced by these devices as well as the need for worldwide accessibility to devices and their associated services, require existing technologies to address these new demands. Considering the increasing importance of video data, utilizing this kind of data in IoT will lead to a further challenge. Here we clarify the role of VE-CoAP in IoT emerging technologies by reviewing a few related references and finding VE-CoAP placement in this regard. Basically, VE-CoAP is an application layer protocol that can be used with any device for transferring video data. These devices could be in various IoT application domains such as smart cities, smart homes, smart manufacturing or smart environment and can be benefited from VE-CoAP regardless of being high-end or low-end.

Cheng et al. addressed the issue of not considering the impact of network condition on transmitting real-time video (Cheng et al., 2014). Their solution involves getting feedback from network condition (possibly with the help of application layer protocols such as RTCP) and using a learning approach to set a proper bitrate for the video based on network conditions. With regard to this solution, as VE-CoAP supports multi-path transmission, it can be used along with their method to further increase the transmission speed and also can be customized to provide feedback channel. This channel can provide real-time feedback for loss rate and delay which can be used for any tuning such as optimizing the performance and congestion control. On the other hand, due to lower overhead and supporting hybrid transmission (continuously switching between best-effort and guaranteed delivery), VE-CoAP can better utilize the network bandwidth which means, video can be encoded with higher bitrate and better quality. At last, the VE-CoAP prioritization method can be customized to use feedback from Cheng et al. method (possibly from their network status collecting module) to change the priority ranking of NALUs based on this feedback. For example, in the case of a congested route, VE-CoAP can mark unimportant NALUs with ZP and drop them from the buffer and also lower the number of its repetitive transmission for MP NALUs. This will result in less congested routes and therefore, higher encoding bitrate and better video quality. In general, Cheng et al. adaptive video transmission control system and similar methods can be used alongside VE-CoAP to further increase the performance.

In the next work, Wu et al. used flow rate allocation to dynamically change the path and bitrate of video for heterogeneous wireless mobile streaming applications (Wu et al., 2015). They used the congestion status of each path and considered delay and loss to adaptively allocate flow rate and deliver high-quality video over SCTP. The main supported transport protocol for CoAP and VE-CoAP is UDP, however it may support other protocols such as SCTP but requires new implementation and customization. Despite the mentioned issue, VE-CoAP also lacks a dynamic congestion control based on network condition and therefore, some parts of Wu et al. method can help implement congestion control at the application layer. Same as Cheng et al. method, VE-CoAP can be used alongside with Wu et al. method to further increase performance and to help supporting IoT with video transmission. Regarding another work from Wu et al., a method for transmitting delay-constraint videos over a multipath heterogeneous network is proposed by considering energy metrics (Wu et al., 2017). As mentioned before, VE-CoAP is amongst few application layer protocols which provide a framework for supporting unequal loss protection (four priorities) with any custom prioritization methods. Therefore, Wu et al. method can both be benefited from VE-CoAP protocol and integrated with the VE-CoAP pipeline. On the other hand, VE-CoAP came up with a sample prioritization method which specifically considers the importance of NALUs for prioritizing video sequence. Therefore, despite it can be completely replaced by Wu et al. method, it can be mixed with it, specifically for delay-constraint unequal protection to further develop the proposed method by considering other metrics such as energy constraints.

On side of big data technologies, multimedia brings new challenges for various fields including edge analysis (e.g. fog computing), preprocessing, data filtering and pattern extraction. Therefore, various techniques such as image pattern recognition being proposed to increase the performance (Zerdoumi et al., 2018). Regardless of techniques used for processing, filtering and analyzing multimedia on servers or edge computers, data is required to be transferred from an end-device to/from processing and storage units. In the case of video data, VE-CoAP and the above-mentioned proposed methods can all help increase the performance of transferring video between these entities.

Last but not the least, is the emergence of high-bandwidth low-range connectivity protocols such as 5G. 5G has become one of the most promising technologies in various IoT domains such as smart cities and has had a great impact on the performance of big data and edge computing. Zhao et al. (2019) and Xu et al. (2019) proposed methods for edge computing over 5G. These are two examples of numerous efforts to utilize 5G in various IoT fields. Despite new applications and performance improvement are going to happen due to 5G, this technology won't replace all the existing network technologies. Cost and low range are two main reasons for this statement which make it more suitable for dense places like cities. For example, deploying IoT devices for monitoring large areas of jungles still requires high-range low-cost network technologies (preferably LLNs). Therefore, regardless of the type of network (5G, other cellular technologies or LLNs, etc.) VE-CoAP can be used on any network technology which supports open application layer protocol. As a result, VE-CoAP can be used on any video-enabled device using 5G to further increase performance compared to other existing application layer protocols such as HTTP.

5. Conclusion and future work
In this paper, a new CoAP-extended application layer protocol for video transmission was proposed which is also suitable for IoT applications and meets its requirements. This protocol, named VE-CoAP is designed to leverage CoAP features and improve video transmission performance even for networks with low bandwidth and high loss rates (LLNs). Despite being RESTful, VE-CoAP can push bulk data from the server to the client with parallel flows. In addition, it can benefits from custom ULP/UEP20 methods, since it supports four transmission routines and priority levels (HP, MP, LP, ZP). In this paper, VE-CoAP is fitted with a ULP method for data partitioned H.264 and it was indicated that it can guarantee video quality at an acceptable level even at extremely high loss rates (nearly 50%). By leveraging VE-CoAP features and the proposed ULP method, it is also possible to extend GoP up to extremely large values (300 frames for instance) and prevent loss and error propagation through GoP. As VE-CoAP has a shorter video delivery duration compared to its rivals such as most TCP-based protocols, HTTP and CoAP, it also has better performance for delay-sensitive video transmission scenarios. VE-CoAP features are summarized as follows:

•
Low header overhead for video transmission (17 bytes for each packet in this evaluation)

•
Hybrid packet delivery (best-effort and reliable)

•
Guarantying in-order packet delivery

•
The ability to push blocks from the client to the server in block-wise transmission

•
The ability of parallel data transmission for the block-wise model with custom number of flows

•
Supporting two-way scalar data communication (one of IoT requirements) and all CoAP features such as publish/subscribe and request/response service models

•
Supporting custom ULP/UEP methods for video transmission by allowing four priority levels (HP, MP, LP, ZP)

•
Equipped with a sample video sequence prioritizing method for data partitioned H.264 sequence with the following features:

Guarantying video quality at an acceptable level at high network loss rates.

Supporting extremely large GOP sequences and preventing error propagation through GOP

Decreasing utilization of middle motes for multi-hop transmission scenarios (compared to protocols with full packet delivery guarantee such as HTTP), resulting in increased performance and decreased energy usage

Increasing video delivery duration speed compared to full packet delivery methods

Table 10 present a summary of the performance improvement for VE-CoAP.


Table 10. Performance comparison for VE-CoAP.

Home Video	Foreman Video
One hop	One hop	Two hops
From 0% to 55% loss rate	Quality decrease	Change in PSNR (dB)	Max 13	Max 8.2	Max 8.43
Change in SSIM	Max 0.19	Max 0.13	Max 0.15
Speed increase	Compared to CoAP	15.5%–78.5%	16%–78.6%	4%–80.6%
Compared to HTTP (TCP used for chunks)	16.4%–60.1%	16.1%–60.9%	4%–53.6%
Compared to HTTP	9.5–22.7 times	9.5–22.9 times	–
Finally, for networks with low loss rates, it is possible to use UDP for transport layer protocol as well as protocols with no packet delivery guarantee. However, as the network loss rate increases, using these protocols cause deterioration of video quality and therefore, protocols with packet delivery guarantee such as CoAP and HTTP are preferred as these protocols at least can preserve video quality (despite increasing delivery duration and probably being unsuitable for delay-sensitive video transmission over LLNs). As VE-CoAP can be equipped with custom packet delivery methods, it can act as fast as UDP and UDP-based protocols for networks with low loss rates with the advantages of in-order packet delivery guarantee and it also can preserve video quality like TCP for networks with high loss rates with the advantage of lower overhead. This demonstrates the wide diversity of applications that VE-CoAP can be fitted with, making it a highly useful video transmission and application layer protocol for various IoT applications.

Despite VE-CoAP features and advantages, there are still some challenges that should be tackled in the future.

•
Congestion and flow control mechanisms for best-effort delivery as well as handling requests and responses for parallel data transmission

•
Method for considering network loss rate and congestion status to prioritize video elements (adaptive prioritization based on the network status)

•
Feedback mechanism for providing information about the network status to the video encoder to encode video at rates suitable for current network condition

•
Proper ULP method for other encoding methods (H265, VP9, etc.) and other video types (360°, scalable -SVC, etc.)

Furthermore, to increase the performance of VE-CoAP in LLNs, all network layers should be equipped with adapted methods for video content requirements such as adapted channel sharing, radio duty cycling, routing and video-enabled hardware (such as Intel Imote with MMX co-processor).

Finally, IoT video transmission over LLNs is still a new topic requiring further research, new methods and tools to be fully operable in real-life applications. Generally, the main steps of transferring a video are encoding/decoding and transmission while being stored efficiently, being viewable by the user and being easily accessible are other aspects that should also be considered. Regarding the high number of devices in most IoT application scenarios, analysis of many recorded videos and extraction of information from them can also be other important phases of this process. Therefore, by considering challenges of LLNs (e.g. low bandwidth and high loss rates), IoT requirements and device constraints, researches and methods such as providing a compatible encoder, offering various implementation of a video-enabled protocol for various applications (e.g. VE-CoAP on californium or servers) and proposing new edge computing methods with respect to constraints of LLNs are still required. Moreover, there is still a lack of a proper testbed and an emulator for video-enabled devices.

CRediT authorship contribution statement
Arvin Ghotbou: Conceptualization, Methodology, Formal analysis, Investigation, Resources, Data curation, Software, Writing - original draft, Visualization. Mohammad Khansari: Conceptualization, Methodology, Formal analysis, Investigation, Resources, Data curation, Validation, Writing - review & editing.

Appendix A.
A.1. Data partitioned H.264 video sequence elements
Generally, an encoded video consists of several pictures or frames being played sequentially at higher rates than human eyes can discriminate (10–12 FPS (Read and Meyer, 2000)). Storing these frames without compressing causes an extremely large overall video size. Therefore, there are many methods that compress these frames by considering their spatial and temporal similarities (searching for similarities spatially within a frame itself or temporally based on the frame reference frames). Processing these frames and finding the similarities is usually performed based on frame types at the macroblock level within a GoP. Common components of an encoded video sequence are shown in Fig. 22.

Fig. 22
Download : Download high-res image (307KB)
Download : Download full-size image
Fig. 22. General components of a typical encoded video sequence.

GoP: An encoded video sequence usually consists of several GoPs which each GoP contains several relating frames. Processes of encoding and finding similarities are limited within a GoP which means that there is no relationship between any two GoPs. Therefore, GoPs can be used for random access in a video sequence. A GoP is always started with an IDR frame and is followed by other kinds of frames. GoPs have no length limitation which means that the entire sequence of video can be one GoP while on the opposite, each frame can be a single GoP (length of one).

Frame types: An encoded video usually contains a different kind of frames. Different types of these frames and their importance for the decoding process are as follows (Haskell and Puri, 2012):

•
I-frames21: These are special kind of frames which are encoded only spatially like a single picture without any dependency to their previous or subsequent frames. These frames are not always, but usually larger than other kinds of frames (such as P or B) and they can be used as a reference for their previous or subsequent frames. Therefore, they are more important than other kinds of frames.

•
IDR-frames22: These frames are special kind of I-frames which prevent any subsequent frames to take frames before an IDR as a reference. In other words, all frames after an IDR-frame can only have references to the latest IDR-frames or frames that exist before it. IDR frame is the most important frame of each GoP and losing it can cause a GoP to be undecodable.

•
P-frames23: These frames are dependent on one of their previous frames (e.g. I, IDR or P). Their importance is based on encoding settings for the GoP hierarchy. Usually, depending on encoding settings, P-frames closer to the beginning of each GoP have more importance for decoding. The reason is that these frames are reference for many subsequent frames and the occurrence of an error in this frame can be propagated through the end of each GoP.

•
B-frames24: These frames have two references, one from their previous and another from their subsequent IDR, I or P frames. They also usually cannot be selected as a reference for other frames and have the least importance compared to other frames types if not selected as a reference. In the case of using B-pyramid feature and taking B-frames as a reference (e.g. in hierarchical GoP architecture), their importance is related to their position in each GoP.

•
SI/SP frames25: These frames are first introduced by Karczewicz and Kurceren in 2001 for the H.264 encoding method (Setton and Girod, 2005). They are used for multi-quality video sequences and make it possible to switch between each quality at the position of these frames. They also make it possible to continue decoding by switching between qualities if a frame loss happened at one stream of a video (streams with different qualities).

Block, Macroblock and Slice: Generally, each frame consists of several pixels. Few numbers of these pixels (e.g. 5x5) form a block and several blocks form a macroblock. Some macroblocks together form a slice. A video frame can be divided into several slices or be a single slice. These classifications are used in the process of encoding and compressing the video sequence such as determining motion vectors or increasing error resistance and compression ratio (Haskell and Puri, 2012). Similar to video frames, each macroblock can have one of the IDR, intra, inter or bi-directional predictive types. In some of the advanced video encoding methods such as H.264, each slice can contain different kinds of macroblocks. For example, a P frame can have intra macroblock types along with inter macroblock types. But an IDR frame can only have intra macroblocks. The type of macroblocks is selected based on encoding options (e.g. to further increase the compression or resistance to errors and losses).

A.1.1. Advanced video encoding features
In process of encoding and storing a video sequence, some advanced features may be used for purposes such as increasing resilience to errors and losses. Some of these features are as follows (Wenger, 2003):

Picture segmentation: Segmenting a frame into a single or more slice is called picture segmentation which was discussed in the previous section. These slices can be stored by different patterns such as raster scan.

Data partitioning: Encoding each frame, slice or macroblock results in a string of data that can be stored sequentially (e.g. raster scan) or in other ways. To further increase resilience to errors and losses as well as easing the process of analyzing a video sequence by different applications (such as user-defined applications or application layer protocols), encoded data sequence can be partitioned and stored in different sections. This feature is used by some of encoding methods such as H.264 (Advanced video, 2003). There are three types of partitions as follows:

Partition A: The header of each slice containing information such as motion vectors, macroblock types and quantization parameters are stored at the partition A. Partition B and C are dependent on Partition A and therefore, this partition is the most important partition compared to the other two.

Partition B: The information needed for decoding intra macroblocks of a slice is stored at the partition B. The importance of this partition is between Partition A and C.

Partition C: This partition contains information needed for decoding inter macroblocks of a slice and has the lowest importance compared to others as it can be compensated from other reference macroblocks.

In case of losing partition B and C in video streaming, partition A can be very useful to conceal the loss of these parts and prevent error and loss propagation. As partition A contains information such as motion vectors, the corresponding lost macroblock can usually be recovered precisely or with a minor color and shape differences.

Parameter sets: Parameter sets are one of the most important elements of a video sequence and contain common information about video frames. There are two types of parameter sets named SPS26 and PPS27 (Advanced video, 2003; Wenger, 2003). SPS stores type of profiles, GoPs length, frames resolution based on macroblocks count and PPS keeps the type of entropy coding (CAVLAC28 or CABAC29) as well as the number of slices in frames. Generally, PPS stores information of fewer frames compared to SPS and also PPS is dependent on SPS and has a reference to SPS. Based on the H.264 standard, the decoder will use information from the latest SPS and PPS to decode each subsequent frame and slice with common information until a new SPS or PPS appears in the sequence. Using parameter sets can increase compression of overall video sequence and be influential for video elements prioritization methods.

A.1.2 H.264 sequence analysis
Whether the aforementioned advanced features are used for encoding a video or not, the result is a string of data that should be stored in a memory (buffer or permanent storage). For encoding methods such as H.264, encoded video data can be stored in three different forms which is important for analyzing the sequence by prioritization methods (Advanced video, 2003; Chan, 2014).

Fig. 23
Download : Download high-res image (575KB)
Download : Download full-size image
Fig. 23. A sample SODB encoded video sequence.

Fig. 24
Download : Download high-res image (351KB)
Download : Download full-size image
Fig. 24. An example of SODB to RBSP conversion.

SODB30: Storing encoded video elements (such as macroblocks or frames known as video syntax elements) at the bit level, is called SODB. Storing video in this form causes the starting point of each element not to be aligned at the start of each byte. This will also cause massive overhead for video sequence analyzing methods (such as the proposed prioritization method in section 3.3) as they should analyze the sequence bit by bit to find the header of each element and discover its type. To clarify a SODB video string, Fig. 23 shows a sample encoded video with hypothetical 32-bit and 25-bit frames (frame size is extremely larger in reality). It is clear that if these frames place together in a SODB form, the header of frame 2 will end up in the middle of byte 4. Therefore, the video analyzer should look up every bit to find the header which is through bytes 4 and 5. (The header used for this example is just hypothetical. In reality, a syntax element may not use any header and be specified by other means).

RBSP31: Aligning each video syntax element at the beginning of each byte will result in an RBSP sequence. To convert a SODB sequence to RBSP, if the end of a syntax element does not align with the end of a byte, an RBSPSB32 which is a bit with value “1” will be added after the syntax element and the rest of the byte will fill with zero bits. To better demonstrate the conversion of SODB to RBSP, the example shown in Fig. 23 is converted to RBSP (Fig. 24).

EBSP33: In an RBSP sequence, if every syntax element being put in a package in which the package header specifies some brief information about the element, the RBSP sequence will be called EBSP. These packages which are called NALU34 in encoding standards such as H.264 and H265, are the fastest way to analyze a video sequence. For converting RBSP to EBSP, each syntax element should be inserted into a NALU with a specified header and also in H.264 standard, a start code which is 0x000001 or 0x00000001 should be added before each NALU. Therefore, finding NALUs and their info is just as easy as finding the specified NALU start code and read the next byte for header information. It worth mentioning that if the RBSP sequence itself contains data the same as the NALU start code, it should be added with an emulation prevention byte which is 0x03 in the H.264 standard. Therefore, any of the following data in an RBSP sequence will be added with an emulation prevention byte.

•
0x000000 → 0x00000300

•
0x000001 → 0x00000301

•
0x000002 → 0x00000302

•
0x000003 → 0x00000303

Fig. 25 summarizes the process of converting a SODB sequence to an EBSP sequence. It is clear that in video streaming scenarios in which sequence analysis is required for purposes such as ULP, using EBSP will highly decrease the overhead of analyzing the stream. EBSP is suitable for video streaming in IoT especially over LLNs and is adapted in this paper.

Fig. 25
Download : Download high-res image (209KB)
Download : Download full-size image
Fig. 25. Overview of converting binary encoded sequence to EBSP.

In the case of using the EBSP video sequence type, it would be possible to easily specify the type of each element and its importance by finding its corresponding NALU header and analyze it. NALU header consists of 1 byte of data including one forbidden bit that should always be zero, a 2-bit field that specifies the importance of subsequent video element which is determined by the video encoder (called nal_ref_idc) and a 5-bit field for determining the type of video element placed in the NALU and whether it is a VCL35 or not (called nal_unit_type). This structure is shown in Fig. 26.

Fig. 26
Download : Download high-res image (51KB)
Download : Download full-size image
Fig. 26. NALU structure for an H.264 video sequence

For prioritizing a video sequence for transmission, the importance of each element which is determined by the encoder can be useful but based on specific scenarios, the prioritization method may use different priority for each element. It is worth mentioning that different video encoders may specify different importance levels for each element. For example, a partition C of B type slices can have importance between 0 and 3 based on encoder settings whether B-pyramid is set or not. Nevertheless, there can be a relationship between NALU or the type of a video element and its importance for the decoding process. The different types of NALUs for the H.264 video sequence can be seen in Table 12 and the probable relationship between NALUs type and NALUs importance can be seen in Table 11.


Table 11. Probable relationship between NALU type and nal_ref_idc

NALU type	Probable nal_ref_idc
1–4	Non-zero if being a reference
5	Non-zero
7	Non-zero
8	Non-zero
13	Non-zero
15	Non-zero
6, 9, 10, 11, 12	Zero

Table 12. H.264 NALU types

NALU type	Details	Class
0	Undefined	Non-VCL
1	Non-IDR element of a non-partitioned sequence	VCL
2	Partition A	VCL
3	Partition B	VCL
4	Partition C	VCL
5	IDR element of a non-partitioned sequence	VCL
6	SEIa	Non-VCL
7	SPS	Non-VCL
8	PPS	Non-VCL
9	AUDb	Non-VCL
10	End of sequence	Non-VCL
11	End of stream	Non-VCL
12	Filler data	Non-VCL
13	Sequence parameter set extension	Non-VCL
14	Prefix NAL unit	Non-VCL
15	Subset sequence parameter set	Non-VCL
16–18	Reserved	Non-VCL
19	Coded slice of an auxiliary coded picture without partitioning	Non-VCL
20	Coded slice extension	Non-VCL
21	Coded slice extension for depth view components	Non-VCL
22, 23	Reserved	Non-VCL
24–31	Undefined	Non-VCL
a
Supplement enhancement information.

b
Access unit delimiter.

A.1.3 H.264 profiles
In the case of video transmission over LLNs in IoT scenarios, due to the high loss rate of LLNs, video sequence should be encoded in a way to have the best resilience to data loss. It should also provide methods for applications and protocols to help prevent video quality degradation in these situations. For the H.264 encoding method, using data partitioning makes it easier to protect video with the ULP scheme. To deploy data partitioning, a proper H.264 profile which supports this feature should be selected. Profiles define a set of features that an encoder is allowed to use. H.264 profiles are as follows (Advanced video, 2003) and some of the features supported by them are shown in Table 13.


Table 13. Some supported features by H.264 profiles (Advanced video, 2003)

Profile/Features	Baseline	Main	High	Extended
I & P Slices	✓	✓	✓	✓
B Slices	✗	✓	✓	✓
SI & SP Slices	✗	✗	✗	✓
FMOa	✓	✗	✗	✓
RSb	✓	✗	✗	✓
Data Partitioning	✗	✗	✗	✓
a
Flexible Macroblock Ordering.

b
Redundant Slices.

•
Baseline: This profile provides basic features for the H.264 encoder and can be used for simple applications. The only frame types available in this profile are I and P.

•
Main: This profile is used for storing or broadcasting HD media and allows the encoder to use I, P and B frames. With the creation of High profiles, the importance of the main profile decreased.

•
High: These profiles with names such as HiP, Hi10P, Hi422P and Hi444PP, developed for fulfilling main profile features and have the same functionally as the main profile.

•
Extended: This profile is famous for video streaming scenarios. It allows data partitioning and deployment of SI/SP frames as well as providing better resilience to loss and errors compared to other profiles (Mohd and Lattif, 2008).

•
Baseline: This profile provides basic features for the H.264 encoder and can be used for simple applications. The only frame types available in this profile are I and P.

•
Main: This profile is used for storing or broadcasting HD media and allows the encoder to use I, P and B frames. With the creation of High profiles, the importance of the main profile decreased.

•
High: These profiles with names such as HiP, Hi10P, Hi422P and Hi444PP, developed for fulfilling main profile features and have the same functionally as the main profile.

•
Extended: This profile is famous for video streaming scenarios. It allows data partitioning and deployment of SI/SP frames as well as providing better resilience to loss and errors compared to other profiles (Mohd and Lattif, 2008).

Appendix B. The logic behind the update equation of the mean value for the video element prioritization method
As it was shown in section 3.3.1, the update equation used for marking a frame as a frame containing scene change (with a sudden increase in the size of that frame) is as follows (Eqn. (1)).(1)Mt+1 = 0.8∗Mt + 0.2∗L → Mt+1 = (4∗Mt + 1∗L)/5

The logic behind choosing coefficients is that sudden changes (including scene changes) in the recent frame should be detected compared to the last few frames. This means that when sudden changes have happened in a frame, the size of this frame increases compared to the last few frames and the older frames have less to do with the sudden change in this new frame. For example, one way for making a scene change is by suddenly moving a camera while capturing a video. The sudden camera movement usually happens in less than a second which means it may affect 4 or 5 frames (can be more based on the frame rate of the video).

To make it clearer, two other cases can be considered. In the first case, the equation considers the same importance for the new frame as the older frames. Therefore, the equation will change as follows (Eqn. (2)) while n is the number of frames from the beginning of the sequence to the current frame (more specifically, the number of NALUs):(2)Mt+1 = ((n-1)∗Mt + L)/n

For example, if the new scene being captured after camera movement has a bigger frames size than the previous scene (e.g. consider the first scene as a sky where nothing is moving and consider the second scene a crowded shore where people are constantly moving. In this case, the mean size of frames from beginning to end of the first scene is significantly smaller than the mean size of frames from beginning to end of the second scene), using equation 2 will consider most of the frames of the second scene, as frames with sudden change while the purpose of the update equation was to help find few frames after the scene change. This is why the last few frames before scene change happens should be more influential on the updated mean value.

In another case, the equation can be considered as follows (Eqn. (3)):

(3)Mt+1 = (Mt + L)/2
In the case of using this equation for updating the mean value, the size of new frames (NALUs) have a deep impact on the updated mean value and if several frames with large size (frames happens during scene change) place sequentially, it is probable that the last frames containing scene change to not be considered as important frames which is not what is desirable for marking frames as frames containing scene change.

Regarding the explanation, it can be seen that the coefficients should be selected with values between equations 2 and 3. These values possibly can be affected by the frame rate of the video which can be further optimized and be evaluated with various video sequences.

Appendix C. Supplementary data
The following is the Supplementary data to this article:

Media player
0:00 / 0:10Speed: 1xStopped

Download : Download video (3MB)
Multimedia component 1.