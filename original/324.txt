Edge computing, as a new computing model, is facing new challenges in network security while developing rapidly. Due to the limited performance of edge nodes, the distributed intrusion detection system (DIDS), which relies on high-performance devices in cloud computing, needs to be improved to low load to detect packets nearby the network edge. This paper proposes a low load DIDS task scheduling method based on Q-Learning algorithm in reinforcement learning, which can dynamically adjust scheduling strategies according to network changes in the edge computing environment to keep the overall load of DIDS at a low level, while maintaining a balance between the two contradictory indicators of low load and packet loss rate. Simulation experiments show that the proposed method has better low-load performance than other scheduling methods, and indicators such as malicious feature detection rate are not significantly reduced.

Previous
Keywords
Reinforcement learning

Intrusion detection

Task scheduling

Q-learning

1. Introduction
In the field of computer networks, traditional cloud computing (CC) transmits data at the edge of the network to the cloud server for centralized processing. However, with the rapid development of the Internet of Things (IoT) and 5G, various types of sensor devices at the edge of the network show explosive growth. These sensors generate a lot of data. In some scenarios with high real-time requirements, the single computing resource model based on cloud computing can no longer meet the needs of real-time, security, and low energy consumption of big data processing. Therefore, edge computing (EC) came into being(Shi et al., 2017).

Edge computing is where compute resources are placed closer to information-generation sources, to reduce network latency and bandwidth usage generally associated with cloud computing (Zhang et al., 2018a). However, with the transfer of data control to edge nodes that are not easy to be controlled physically, coupled with the complexity of edge computing mode, the multi-source heterogeneity of data and the limited resources of terminals, the traditional security protection mechanism in cloud computing environment is no longer applicable to edge computing environment (He et al., 2018). As a result, the breadth and difficulty of access control and threat protection in edge computing environments has increased significantly.

In cloud computing environments, intrusion detection systems (IDS) can be used for security protection (Zhao et al., 2020). With the surge in complexity of network traffic and intrusion behaviors, traditional single-host IDS cannot meet the requirements of efficient and accurate detection (Zhao et al., 2019), so the distributed intrusion detection system (DIDS) is widely used. DIDS consists of a scheduler and multiple detection engines. The scheduler distributes traffic to multiple detection engines for detection according to certain rules, which improves detection efficiency and prevents overall paralysis due to a single point of failure.

However, DIDS in cloud computing relies on high-performance hardware devices (Liu et al., 2020), but in edge computing, the processing power of edge nodes is limited. This will directly cause a single detection engine to fail to detect network traffic that exceeds its detection capabilities, which will cause missed detections when the network speed is high (Zhao et al., 2016). Network traffic containing malicious information that escapes security detection will pose a security threat to the entire network (Hui et al., 2019). If this problem is not resolved, it will be determined that edge computing will not develop in a healthy manner.

Under the premise of limited performance of a single detection engine, how the DIDS scheduler optimizes task al location to each performance-limited detection engine becomes an important issue (Zhao et al., 2015). The reasonable optimization decision of the scheduler will lead to the reduction of the overall PLR of DIDS.

Although there have been some researches related to task scheduling in the field of edge computing (Lin et al., 2018) (Diddigi et al., 2018), and even an IDS framework for edge computing environments has been proposed (Hui et al., 2019), as far as the problems studied in this paper are concerned, the following problems still need to be solved:

1)
How to make the scheduler make scientific decisions and distribute the detection tasks to each detection engine reasonably, so that the load of the entire system is in a low state. In this way, low-load DIDS can adapt to the edge computing environment;

2)
The processing performance of the detection engines of DIDS running at the edge of the network is likely to be different. The load capacity of each detection engine needs to be objectively evaluated so that the scheduler can make scientific decisions;

3)
Low load and low packet loss rate (PLR) are two conflicting indicators. How to find a balance between these two contradictions to make DIDS run in a low load environment and keep the PLR within an acceptable range.

Because DIDS is faced with fast and randomly changing network traffic, this will cause some machine learning algorithms to be unsuitable. But reinforcement learning (RL) can take correct actions based on changes in the environment to maximize the expected benefits. Some scholars have used it to build a random model in a dynamically changing environment and achieved meaningful results (Diddigi et al., 2018). As a model-free algorithm in reinforcement learning, Q-learning (QL) algorithm can find the optimal strategy in the model established by the Markov decision process (MDP), so it has gradually gained attention.

On this basis, this paper uses Markov decision process modeling, and uses reinforcement learning to conduct research through rigorous mathematical theoretical reasoning. The key contributions in this paper are provided as follows.

1)
A task scheduling method based on Q-learning is proposed. The method takes low load as the optimization goal and finds the optimal policy to keep the DIDS low load state through the value function. In this way, the scheduler can performs reasonable scheduling between detection engines of different performance and packets of different lengths according to this policy.

2)
In order to avoid the problem that excessively low load may cause PLR increase, we have established an adjustment mechanism that allows the scheduler to adjust the probability of detection engines of different efficiencies being assigned tasks based on changes in PLR, thereby achieving a balance between two contradictory indicators;

3)
A scientific evaluation methods for the processing performance of the detection engine and the load generated by the packets is proposed.

The rest of this paper is organized as follows. Section 2 discusses the previous studies related to this paper. The detailed method design is introduced in Section 3. Section 4 describes how the method balances low load and packet loss. Section 5 provides the experimental results and discussions of our proposed method. Finally, Section 6 concludes this paper.

2. Related work
In this section, we will review the related research on IDS task scheduling and reinforcement learning in the edge computing environment.

2.1. IDS and task scheduling
In order to effectively utilize the resource constrained terminal devices, some literatures have studied the task scheduling and resource allocation in the environment related to edge computing. In order to improve the efficiency of resource allocation, In order to solve the problem of resource limitation of edge nodes, Hui et al. (2019) proposed a new resource allocation mechanism based on the deterministic differential equation model (DDEM), and obtained the existence, uniqueness and stability of the positive solution of the model by using Lyaponv stability theory. Lin et al. (2018) proposed an SDMMF allocation algorithm that uses the principle of dominant resource fairness to allocate all available resources, and then uses max–min fair to allocate the remaining resources. Arian et al. (2017) proposed a weighted task invocation method that provides proper traffic distribution among sensors by assigning specific weights to each sensor of the DIDS.

Some research focuses on load balancing (LB). For example, Wu et al. (2018) proposed a distributed intrusion detection model for big data based on lossless partitioning and balanced allocation, and implemented local load balancing through data allocation strategies based on capacity and workload. Puthal et al. (2018) proposed a new load balancing technology for authenticating Edge Data Center (EDC), and used the EDC with less load for task distribution. Ha et al. (2016) proposed a clustering-based flow grouping scheme that achieves a better load balance on the IDSs. Li et al. (2020) proposed a multi-objective evolutionary algorithm decomposed by Tchebycheff to achieve a trade-off between energy efficiency, latency and bandwidth.

Other studies focus on the trade-off between energy and efficiency. For example, Diddigi et al. (2018) used reinforcement learning to solve the problem of IDS state space and action space explosion under energy budget constraints. Kaur K et al. (Kaur et al., 2018) proposed a multi-objective evolutionary algorithm using Tchebycheff decomposition to handle large data streams in edge computing environments to achieve a trade-off between energy efficiency, latency and bandwidth. Prithi et al.(Prithi and Sumathi, 2020) proposed a new learning dynamic deterministic finite automaton with PSO algorithm for secure and energy-efficient routing in wireless sensor networks and eliminating intruders. In order to reduce the energy consumption and ensure the efficiency of IDS, Han et al. (Lansheng Hanet al., 2019) proposed an intrusion detection model based on game theory and an autoregressive model to obtain an optimal defense strategy that balances system detection efficiency and energy consumption. Francisco et al. (Colom et al., 2018) proposed scheduling framework for distributed intrusion detection systems over heterogeneous network architectures to minimize the cost of public computing.

Although scholars have obtained research results in different directions, the difference from this paper is that they did not consider how DIDS can complete traffic scheduling and detection in a low-load environment such as edge computing. If this problem cannot be solved, it will be difficult to deploy traditional DIDS, which can only rely on high-performance equipment, to the edge of the network. If this problem cannot be solved, it will be difficult to deploy traditional DIDS, which can only rely on high-performance equipment, to the edge of the network. However, our research aims to optimize both low load and low PLR of DIDS in edge computing. Table 1 lists the differences between our study and other current studies.


Table 1. Comparisons between our work and current job in DIDS/IDS.

Study	Object	Optimization Goal	method
Hui et al. (2019)	MEC-IDS	the random disturbance in the process of resource allocation	DDEM
Lin et al. (2018)	EC-IDS	the multiple resources fair allocation in multiple layers	SDMMF
Diddigi et al. (2018)	WSN-IDS	the optimal number and the right configuration of sensors	POMDP
Arian et al. (2017)	SN-DIDS	the improvement of overall performance of DIDS	LB
Wu et al. (2018)	BD-DIDS	the traffic distribution strategy based on capacity	LB
Puthal et al. (2018)	FOG-EDC	detection task assignment	LB
Ha et al. (2016)	SDN-IDS	traffic distribution based on routing information and flow data rate	Clustering
Prithi and Sumathi (2020)	WSN-IDS	the improvement of overall performance	DFA, PSO
Lansheng Hanet al. (2019)	WSN-IDS	the energy consumption and efficiency	Game theory
Colom et al. (2018)	CC-IDS	the cost of public computing	Framework
Ours	EC-DIDS	low load and low PLR	QL
2.2. Reinforcement learning
Reinforcement learning is an important area of machine learning. It is used to describe and solve the problem of maximizing returns through learning strategies during the interaction between the agent and the environment. Since the network environment is full of randomness, it is difficult for the scheduler to use a fixed strategy for scheduling to achieve the optimum of a certain index. After combining reinforcement learning with scheduling, the scheduler can learn from the network environment. Under the stimulation of the rewards or punishments given by the network environment, the scheduler gradually forms the anticipation of the stimulus, and produces the habitual behavior that can obtain the greatest benefit(Shuja et al., 2021).

Among various researches that use reinforcement learning for scheduling, some research aims to reduce the response time of the scheduling process. For example, in distributed computing (DC), considering the randomness of the network task arrival and execution process, Tong et al. (2014) use MDP to describe and model dynamic scheduling. In their subsequent research (Tong et al., 2019), they proposed the QL-HEFT task scheduling algorithm for cloud computing to reduce the response time of scheduling. Mostafavi et al. (Mostafavi and Hakami, 2020) proposed a solution to perform predictive task scheduling in a cloud environment, which can shorten the response time of the system. Li et al. (2019) formulated the resource management among MEC and IoT devices as a double auction game, and use reinforcement learning to solve it.

The purpose of other common research is to reduce the energy consumption of the equipment or maintain the balance between performance and power consumption. For example, Wei et al. (2019) proposed a Q-learning algorithm for task scheduling based on improved support vector machine in wireless sensor networks to optimize the application performance and energy consumption of the network. Lei et al. (2019) proposed a joint computation offloading and multi-user scheduling algorithm in NB-IoT edge computing system, which minimizes the long-term average weighted sum of delay and power consumption under stochastic traffic arrival. Abishi et al.(Chowdhury et al., 2019) studied the scheduling and resource allocation problem for dynamic user requests with varying resource requirements. Their goal is to generate an optimization strategy through RL to minimize overall energy consumption. In order to reduce the network delay of wireless sensor networks and improve the network lifetime, Ramadhani et al. (Sinde et al., 2020) proposed an energy-saving scheduling scheme using RL algorithms in wireless sensor networks.

Although the above researches are related to scheduling through RL, they are different from this paper in that their optimization goals are mainly the response time of scheduling and the energy consumption of the system, and they do not consider how to keep the load of the system which is not suitable for low load environment at a low level. However, our proposed RL-based scheduling method can not only keep the overall load of the system at a low level, but also maintain a balance between low load and low PLR to avoid the increase of PLR caused by too low load. Table 2 lists the differences between our study and other current studies.


Table 2. Comparisons between our work and current job in RL.

Study	Environment	Optimization Goal	Performance metrics
Tong et al. (2014)	DC	response time	response time, convergence
Tong et al. (2019)	CC	response time	makespan, speedup, efficiency, etc.
Mostafavi and Hakami (2020)	CC	response time	response time, waiting time, makespan
Li et al. (2019)	MEC	response time	convergence, price
Wei et al. (2019)	WSN	energy consumption	application performance, residual energy
Lei et al. (2019)	NB-IoT	delay and energy consumption	average delay, power consumption
Chowdhury et al. (2019)	IoT	energy consumption, response time	energy consumption, response time
Sinde et al. (2020)	WSN	energy consumption, network lifetime	energy consumption, network lifetime, throughput, delay
Ours	EC	low load and low PLR	system load, PLR, detection rate, memory usage, etc.
2.3. Other problems to be solved
As far as the problems studied in this paper are concerned, there are still some problems that need to be solved:

1)
Although various literatures usually use heuristic algorithms (Zhang et al., 2018b) and meta-heuristic algorithms to solve task scheduling and task offloading, both types of algorithms have their own disadvantages. For example, heuristic algorithms tend to fall into local minima and it is difficult to obtain global optimal results; meta-heuristic algorithms have too many parameters, it is difficult to reuse the calculation results, and parameter adjustments cannot be performed quickly and efficiently (Lu et al., 2020). But we build a model through MDP, and then quickly obtain the global optimum of the value function through value iteration, so as to solve the above contradiction.

2)
The existing load capacity measurement methods for DIDS need to be improved. For example, in Wu et al. (2018), the load capacity of the detection engine is measured by adding the number of CPUs, disk read-write capacity, memory usage and network bandwidth usage. In Li and Yu (2019), on the basis of Wu et al. (2018), the CPU processing capacity is subdivided into CPU model, CPU main frequency size, CPU three-level cache size, and then combined with memory frequency and hard disk I/O reading and writing speed in a weighted way to reflect the computing capacity of the detection engine. Although these parameters have a positive correlation with the processing capacity of the detection engine, this evaluation method is only obtained by adding the weights of different units, and the weighting method does not give a reasonable basis, so the existing load capacity measurement methods for DIDS need to be improved.

Based on this, this paper will scientifically evaluate the performance of the detection engine and the load generated by the packets, and study how to conduct distributed intrusion detection in a low load environments through reinforcement learning.

3. Problem model
3.1. Model workflow
There are multiple detection engines with different performance levels in DIDS to provide detection for randomly arrived packets of different load levels. The workflow of the model is described below with reference to Fig. 1.

Fig. 1
Fig. 1. Model workflow.

1)
After the DIDS is started, the performance of each detection engine is evaluated to get its performance level d (d = 1,2, …,D);

2)
When a packet arrives and needs to be detected, the scheduler first performs load evaluation on it, and get its load level k (k = 1,2, …,K); Fig. 9

3)
The scheduler makes a decision and decides which performance level detection engine to detect this packet;

4)
After a detection engine completes the detection, if the scheduler does not reassign other detection tasks, it will be temporarily idle;

5)
When a detection engine is also assigned other detection tasks, it will immediately complete another detection task assigned by the scheduler;

6)
When a detection request arrives, if there is no idle detection engine in the DIDS, the scheduler will record the detection request and put it in the queue. Once the queue is full, this new packet will have to be discarded. If there is an idle detection engine in DIDS, no packets will be put into the queue to wait.

Because the next incoming packet load level is uncertain and the queue length is limited, for a DIDS with a fixed number of detection engines, the optimal decision needs to be made to minimize the overall load while maintaining the PLR at acceptable level.

3.2. Evaluation of performance and load
In order to objectively evaluate the performance of each detection engine, we test the detection engine with the same traffic in advance, collect the detection time (DT) and memory usage (MU) information for the test traffic, and take the ratio of the two as the performance index pi of the detection engine, which is Eq. (1):(1)
 

After testing all detection engines, the detection engines are divided into different performance levels according to the order of pi value. The specific level is represented by d (d = 1,2, …,D) and the difference of pi value is within 10%, which can be classified as the same level.

Although it is proposed in (Lin et al., 2018) (Prithi and Sumathi, 2020) to combine related parameters such as CPU, disk, memory, and bandwidth to calculate detection engine performance, these devices in the computer do not contribute all resources to DIDS. Therefore, the calculation result can only show a positive correlation with the detection engine performance, and cannot be used as an accurate measurement method. However, the evaluation method we propose is obtained by testing and calculating actual traffic, so it is more rigorous and reasonable.

In (ZHANG et al., 2019) shows that DIDS spends up to 70% of execution time and 80% of instruction execution time on pattern matching, so the load evaluation of traffic mainly depends on the pattern matching time it takes, and the pattern matching time is proportional to the length of the string it detects, so the length of the string can be used as a measure of the load generated by the packet. For the packets captured by DIDS, the ratio of the packet length to the maximum transmission unit (MTU), such as 1500 Bytes, can be used to obtain the load level generated by the packet in the preprocessing stage.

3.3. Parameter definition
DIDS has D performance level detection engines for K load level packets. The detection time follows an exponential distribution. The arrival process of packets can be regarded as K independent Poisson processes. The evaluation criterion adopts the average load criterion. Considering the time when the packet arrives and the end of detection, the embedded chain is a Markov chain.

Table 3 shows the parameter settings to be used later.


Table 3. Parameter settings.

Notation	Description
nd	The total number of detection engines at d-level, d = 1,2, …,D;
ndk	Number of d-level detection engines that are detecting k-level packets, d = 1,2, …,D and k = 1,2, …,K;
nd0	Number of d-level detection engines that have not been allocated for use
μdk	Detection rate of d-level detection engine for k-level packets
λk	k-level packet arrival rate k, k = 1,2, …,K
ldk	Average load of k-level packets on d-level detection engine
lk	Minimum load of k-level packets on the detection engine
bk	Queue length of k-level packets waiting to be detected
T	The total number of all detection engines
3.4. State space
When a new packet arrives, the scheduler needs to allocate a detection engine for detection. At this time, the state of the system changes, so the scheduler needs to make a decision and execute the corresponding behavior. Similarly, when a detection engine completes the detection of a certain packet, the execution of this behavior causes the state of the system to change, and the current state of the system is transferred to another state in the state space.

Let s be the working state of DIDS, including the status of the detection engine with and without detection tasks assigned. When the limit b of the queue length is determined, a set X containing all possible states can be defined. Listed below are some typical possible states in set X.

1)
If there is an idle detection engine in the system, and exactly one packet arrives, and it is a j-level packet after the load evaluation, then X1 is a state in the set X as shown below Eq. (2)(2)
 

2)
All possible states X2 when no detection engine is available in the system can be expressed as Eq. (3)(3)

3)
All possible states in the system that still have an idle detection engine and no packets waiting to be detected can be expressed as Eq. (4)(4)

4)
There is only one idle detection engine in the system and there are all possible states of packets waiting to be detected,.as below Eq. (5).(5)

3.5. Action set
In the cases listed above, for the state in X1 the scheduler needs to choose which level of detection engine to assign to process this packet. For the state in X4, the system needs to consider which level of packets in the queue should be detected by the only idle detection engine at present. For states in X2 and X3, the system does not need to make a choice. So the action set A of the state space X is defined as Eq. (6)(6) 
 

0 in the action set means no decision is needed. Action k ∈ A(s)(s ∈ X4) indicates that the only idle detection engine in the system processes a waiting k-level packet, and d ∈ A(s)(s ∈ X1) indicates that the d-level detection engine is used to detect the packet that has just arrived.

3.6. Value function and optimal policy
lk is set to the minimum load that the k-level packet brings to the detection engine. lk depends on the load level of the packet to be detected; the average load ldk depends on the performance level d of the detection engine and the load level k of the packet. Considering that the distribution of detection time is usually exponential, then the expected load of action a in state s ∈ X1∪X4 can be given as Eq. (7)(7)
 

If the scheduler meets the low load expectations in a certain state, then it can be rewarded. Let r denote the reward, and let γ∈[0,1] denote the attenuation coefficient of the reward of the states after the state. The sum of all rewards since time t, which can be called the gain Gt, can be expressed as Eq. (8):(8)

Let vπ(s) denote the state value function based on the policy π, and vπ(s) is the expectation of the gain obtained by following the policy π from the state s. vπ(s) can be expressed as Eq. (9):(9)

Let Qπ(s,a) denote the state-action value function based on the policy π, and Qπ(s,a) is the expectation of the gains that can be obtained by performing a specific action a starting from the current state s. Qπ(s,a) can be expressed as Eq. (10):(10)

Then, among all the optional scheduling policies, there will be an optimal scheduling policy that minimizes the load generated by the DIDS and maximizes the cumulative gain Gt. At this time, the optimal state-action value function Q∗(s,a) will be generated, which can be expressed as Eq. (11):(11)

Conversely, the optimal strategy π∗(s,a) can be found through the above formula, which is defined as follows Eq. (12):(12)
 

Open the expectation in Equation (9) to get the following formula Eq. (13):(13)

3.7. Evaluation and improvement of policy
In order to find the optimal policy, we use policy evaluation to measure the quality of the strategy, and then use policy improvement to find a better policy. First, policy evaluation is used to obtain a state-value function vπ(s) based on policy π, and then a better policy π′ is obtained according to policy improvement, and then vπ’(s) is calculated to obtain a better policy π’’ until the relevant termination conditions are met. The pseudocode of the above process is as follows Algorithm 1 Evaluation and improvement of policy:


Algorithm 1 Evaluation and improvement of policy.

Input: policy π(s), state value function v(s)
Output: optimal policy π∗(s), optimal state value function v∗(s)
1 initialization π(s), v(s) arbitrarily for all s ∈ S
2 policy evaluation
3 repeat
4 Δ←0
5 for each s ∈ S do
6 temp←v(s)
7 

8 Δ←max(Δ,|temp-v(s)|)
9 end
10 until Δ<θ (a small positive number)
11 policy improvement
12 policy-stable←ture
13 for each s ∈ S do
14 temp←π(s)
15 

16 if temp≠π(s) then
17 policy-stable←false
18 end
19 end
20 if policy-stable then
21 stop and return π∗(s), v∗(s)
22 else
23 go to policy evaluation
24 end

4. Q-learning algorithm
The Q-learning algorithm is a modeless reinforcement learning algorithm. It provides the learning capability for scheduler to select optimal actions according to historical experience (Tong et al., 2014). Firstly, we construct all states in the state space and all corresponding actions in the action space into a Q_table to store the Q value, which is Q(s,a), and then select the action that can get the most benefit based on the Q value. In the Sarsa algorithm of reinforcement learning, the policy for selecting action and updating action value function is the same, but Q-Learning is an off-policy timing difference method. So we set the strategy of selecting actions to ε-greedy and the updated policy to greedy.

The time difference (TD) method combines the Monte Carlo sampling method and the bootstrapping of the dynamic programming method, that is, the value function of the subsequent state is used to estimate the current value function. This makes the TD method suitable for model-free algorithms, and the single step update speed is faster. According to the TD method, the value function is calculated as follows Eq. (14):(14)

In the above formula, 
 is TD target, and 
 is the TD deviation.

According to the above derivation, the Q value can be calculated, and the updated formula of Q-learning is Eq. (15):(15)

The pseudocode of the above process is as follows Algorithm 2 Minimum load scheduling based on Q-learning algorithm:


Algorithm 2 Minimum load scheduling based on Q-learning algorithm.

Input: state space S, action space A, discount rate γ, learning rate α
Output: policy 

1 random initialization Q(s,a)
2 
3 repeat
4 initialize state s;
5 repeat
6 choose action a from s using policy derived from Q(s,a)
7 take action a, observe r,s’
8 

9 s←s’
10 until s is terminal
11 until  convergence

5. Balance of contradictory indicators
Although the above state-action value function strives to minimize the overall load of DIDS, low load and low PLR are two contradictory indicators. Excessive low load will increase the PLR, especially when there is a sudden surge in network traffic. Therefore, these two indicators need to be balanced. In order to achieve this goal, it is necessary to first calculate the relevant parameters required for balance.

5.1. Related parameters
1)
Number of tasks assigned to the detection engine

The average number of packets (ANP) allocated by the scheduler to the d-level detection engine can be calculated by Eq. (16)(16)

In the above formula, n = 1,2, …,nd, d = 1,2, …,D.

2)
Probability of detection engine being assigned tasks

The probability that the n d-level detection engines are assigned detection tasks by the scheduler can be obtained as follows Eq. (17):(17)

In the above formula Eq. (18),(18)

For all n, d = 1,2, …,D, β is a parameter for adjusting the PLR, which is described in detail in section 5.2.

3)
Work efficiency

Using the above formula, it can be concluded that the working efficiency of the d-level detection engine can be given as Eq. (19)(19)
 

After mastering the working efficiency of a certain level of detection engine, the scheduler can adjust the decision based on the changes in traffic during the iteration.

Similarly, according to ANP(d), the overall efficiency of DIDS can be obtained as Eq. (20)(20)
 

5.2. Balance of two contradictory indicators
First, the efficiency of detection engines of different performance levels needs to be ranked. Our balancing principle is that when the PLR increases, the scheduler increases the probability of assigning tasks to the high-efficiency detection engine. Conversely, when the overall load of DIDS is high, the scheduler reduces the probability of assigning tasks to inefficient detection engines.

To achieve this, two parameters need to be added: the low threshold TL and high threshold TH of the PLR. The specific method of balancing is divided into the following situations:

1)
If the PLR is lower than TL, β in formula (18) (the probability of the task assigned to the detection engine) is set to 1. At this time, the scheduler performs task scheduling according to the principle of low load priority.

2)
When the PLR exceeds TL, set β to ηπ(d)/ηπ.This means that compared with the overall efficiency of DIDS, the higher the efficiency of a certain level of detection engine, the higher the probability of being assigned a detection task. Conversely, the lower the efficiency of the detection engine, the lower the probability of being assigned a detection task. At this time, the scheduler performs task scheduling according to the principle of both low load and low packet loss.

3)
When the PLR continues to rise, the tasks assigned to detection engines that are higher than the overall efficiency of DIDS have reached the processing limit (the PLR at this time is TH). In order to make the inefficient detection engine share the pressure, β will return to 1. At this time, the scheduler performs task scheduling according to the principle of low PLR.

Therefore, in order to adjust the balance between low load and PLR, the probability of detection engine being assigned tasks can be summarized as the follows Eq. (21)(21)
 
 

6. Experiments and analysis of results
The experiment tests the proposed method on DIDS through the simulation environment. During the test, the proposed method is compared with the algorithms of DDEM in (Hui et al., 2019), SDMMF(Lin et al., 2018), and LB (Arian et al., 2017) in recent literature. The purpose of the test is to answer the following questions:

•
Compared with other methods, can the overall load of DIDS be effectively reduced?

•
Will the proposed method increase the PLR?

•
How do detection engines with different performances be assigned tasks by the scheduler?

•
Will the proposed method increase the detection rate of malicious features?

6.1. Experimental environment
A simulation experiment system was built on EdgeCloudSim based on the DIDS framework shown in Fig. 1. EdgeCloudSim is a simulation tool dedicated to edge computing proposed by Cagatay Sonmez team based on CloudSim. In the EdgeCloudSim, there are five main basic modules, which are the Core Simulation, Networking, Load Generator, Mobility and Edge Orchestrator (Sonmez et al., 2018) (as shown in Fig.2).

Fig. 2
Fig. 2. The basic module of EdgeCloudSim (Sonmez et al., 2018).

In this experiment, after configuration, the role of each module is as follows (see Table 4):


Table 4. The role of each module.

Module	Role
Core Simulation	The core simulation is responsible for simulating the Edge Computing environment.
Networking	The networking is responsible for connecting the detection engines and rule bases, and processing the transmission queue
Edge Orchestrator	Edge Orchestrator includes scheduler and packet load evaluation module
Load Generator	Load Generator is responsible for sending the test data set
In order to simulate the low-speed environment of the edge network and the medium-to-high speed environment near the core of the network, different sending speeds were set for the test traffic during the test.

6.2. Test data set
There are two kinds of test data sets used in the experiment:

1)
NSL-KDD data set

The NSL-KDD dataset is an improved version of the well-known KDD99 dataset (Zhao, 2015) after removing a lot of redundant data. Therefore, the data set has fewer records than KDD99, which can reduce the testing overhead. Each sample in the NSL-KDD dataset consists of 41-dimensional features and one-dimensional labels (Dong et al., 2018). There are four types of attacks in the data set, including Probe, DoS, R2L, and U2R. In the edge computing environment, some types of attacks near WSN terminals can be reflected in the NSL-KDD data set (Dong et al., 2018), as shown in Table 5. In addition, the number of other attack features of this dataset is shown in the table below (see Table 6).

2)
WSN-DS data set


Table 5. Correspondence between attack types in WSN and NSL-KDD.

Attack types in WSN	Corresponding types in NSL-KDD
Wormholes and black holes	Probe
Tampering, cheating, Replay Attacks	R2L
Flood,U2R,Resource Consumption Attack	DOS

Table 6. NDD-KDD dataset distribution.

Data Set	Type
Dos	Probe	R2L	U2R	Normal	Total
KDDTrain+	45926	11655	995	52	67345	125973
KDDTrain_20%	9234	2289	209	11	13449	25192
KDDTest+	7458	2421	2754	200	9711	22544
The WSN-DS data set is generated by the NS-2 simulating the WSN environment. There are a total of 374,661 pieces of traffic data in the data set, each of which has 23 characteristics. The WSN-DS dataset has four types of attacks: black hole, gray hole, flood, and dispatch attack, which can be used exclusively for IDS research in the edge computing terminal environment. In addition, the number of other attack features of this dataset is shown in the table below (see Table 7).


Table 7. WSN-DS data set distribution.

Data Set	Types
Grayhole	Blackhole	Scheduling	Flooding	Normal	Total
Testing set 40%	402	583	132	266	13603	14986
Training set 60%	603	876	199	398	20404	22480
6.3. Evaluation index and result analysis
1)
System load

System load is obtained by adding up the average load of each detection engine in different states. In order to simulate the low-speed environment of the edge network and the medium-high-speed environment near the core of the network, test traffic is sent at different speeds. The test results are shown in the following figure.

It can be seen in Fig. 3 that the proposed QL-based method has a clear low-load advantage in the low-speed traffic of the edge network. When the network speed is lower than 100 Mbps, compared with DDEM, SDMMF and LB, the QL-based method can achieve a system load reduction of up to 28.3%, 19.4% and 23.7%. Only in high-speed traffic does the scheduler gradually tilt towards the principle of low packet loss, so the low-load advantage gradually disappears at this time. The system load at this time is close to the result of the SDMMF method, which also emphasizes lightweight task scheduling. In addition, it can be seen from Fig. 4 that as the network speed increases, because the PLR increases, DIDS gradually approaches the performance limit, so the load increase of each method gradually slows down.

2)
Memory usage

Fig. 3
Fig. 3. System load test.

Fig. 4
Fig. 4. Memory usage ratio.

In Section 5.2, a balance method of low load and low PLR is proposed. According to this method, the scheduling method has different processing principles under different network speeds. In the experiment, the low threshold TL is set to 20% which is more tolerant to the PLR. Then, when the PLR is less than 20%, the scheduler performs task scheduling according to the principle of low load first. When the PLR is higher than 20%, the scheduler performs task scheduling according to the principle of taking into account both low load and low PLR. When the PLR continues to rise and approaches the processing limit of the detection engine, the scheduler reverts to the principle of low PLR.

Therefore, in order to test the memory usage of detection engines of different performance levels, the following tests are performed at 5 typical network speeds, namely 25 Mbps (no packet loss occurs), 50 Mbps (packet loss just occurred), and 95 Mbps (the PLR reaches 20%), 175 Mbps and 250 Mbps (processing limit). The test result is shown in Fig. 4.

As can be seen in Fig. 4, no matter at which stage, according to the optimal policy, detection engines with high performance levels have higher processing efficiency, so they are assigned more tasks than the detection engines with low levels and occupy more memory. When the network speed is low, although there are differences in the memory usage of detection engines of different performance levels, this difference is not large compared to when the network speed is high. When the PLR reaches 95 Mbps, because the low threshold TL is reached, the memory occupancy difference of detection engines of different performance levels is very significant. When the network speed continues to increase, the high-performance detection engine is approaching its performance limit, the proportion of tasks allocated to the low-performance detection engine begins to increase, the memory usage increases, and the difference between them begins to shrink. On the other hand, it can also be seen that in the stage of very high network speed, the memory usage of each level of detection engine is increased compared with the low-speed stage.

It can be seen from the above three figures that, at different network speed stages, the proposed QL-based method has a smaller memory usage than other algorithms, or is equal to the lightweight SDMMF. However, the DDEM algorithm occupies the largest memory space because of its time and space complexity.

Through the above tests, it can be confirmed that the proposed method has advantages over other algorithms in terms of memory usage.

3)
Tasks assigned to the detection engine

In the simulation environment, under the QL-based method, the detection engine is divided into 5 performance levels, which respectively represent 5 kinds of low to high working efficiency.

It can be seen in Fig. 8 that when the network speed is low, the proportion of tasks assigned to detection engines of different performance levels is different. The difference between the highest value and the lowest value is 14%. As the network speed increases, the detection engines with high performance level have more tasks assigned because of their high processing efficiency. When the PLR reaches the set low threshold of 20%, the scheduler starts to switch the allocation principle, so this difference starts to grow, and the high-performance detection engines are assigned more tasks. At this time, the gap between the highest value and the lowest value is as high as 33%. When the speed continues to increase and reaches a high threshold, the high-performance detection engine has reached its limit. At this time, the proportion of tasks assigned to the low-performance detection engine starts to increase, and the difference between them begins to narrow. The difference between the highest and lowest values reaches 16% at the lowest.

4)
PLR

Fig. 8
Fig. 8. Proportion of tasks assigned to the detection engine.

This experiment is to test whether the proposed method will increase the PLR compared with the comparison algorithm. The actual PLR is the ratio of the number of detected packets to the number of all packets, which can obtained by dividing the number of undetected packets by the number of total packets. In the experiment, the two test data sets are sent to DIDS by the Load Generator respectively, and the low threshold TL is set to 20% that is more tolerant to PLR. The test results are shown in Fig. 9.

Fig. 9
Fig. 9. Actual PLR

As can be seen in the above figures, because the QL-based method focuses on the principle of low load in a low network speed environment, it does not show the advantage of low PLR when the PLR is lower than 20%. When the PLR exceeds 20%, the network speed at this time is about 95 Mbps, and the balance method we proposed begins to take into account the principle of low PLR, so the PLR begins to decline. The higher the network speed, the more inclined to the principle of low PLR, the more it has the advantage of low PLR. At 250 Mbps, the PRL of our method is more than 20% lower than that of the LB method, which requires high computing power and memory space. On the other hand, in order to reduce the PLR of the proposed method in the low-speed phase, it is only necessary to set the low threshold TL lower.

5)
Detection rate

We pre-calculate the total number of malicious features in each test data set. The malicious feature detection rate (DR) is obtained by dividing the number of detected malicious features by the total number of malicious features. The test data sets is still sent to DIDS by the Load Generator respectively.

As shown in Fig. 10 above, in the low-speed state, because the scheduler focuses on the low-load principle, the malicious feature detection rate of the proposed QL-based algorithm is 1.7–2.5% lower than other algorithms. When the network speed exceeds 100 Mbps, because the low threshold is reached, the scheduler switches to the low PLR principle. From this point on, the detection rate starts to rise compared with other algorithms, and it can exceed 15.2% of the LB method at the highest.

Fig. 10
Fig. 10. Detection rate.

7. Conclusion
Aiming at the problem of limited processing performance of devices in edge environments, this paper first scientifically evaluates the processing performance of each DIDS detection engine and the load generated by different packets, and then proposes a DIDS task scheduling method based on Q-learning algorithm. This method can keep DIDS in balance between the two contradictory indicators of low load and PLR. Finally, this paper compares and verifies the proposed method through a simulation platform. The results show that the proposed method has better low-load performance than other scheduling algorithms.

Due to the limitations of the Q-learning algorithm, in the experiment we found that sometimes there are problems such as too large Q table and too long update time. In future work, we will consider using a deep neural network to simulate the Q value to solve this problem, and combine the deep reinforcement learning method with the task scheduling of DIDS in the edge computing environment.