Cloud computing and big data are two technologies whose combination can yield many benefits and challenges. One of the most significant challenges is the traffic produced by data-intensive jobs within a data center. A possible way to manage the produced traffic is optimizing the placement of virtual machines (VM) on hosts. However, placing VMs compactly to reduce the communication cost can negatively impact on other aspects such as host utilization and load balancing. In this paper, we aim to make a balance between optimizing the host utilization and the communication cost while considering load balancing. We investigate the VM placement problem by modeling it as the minimum weight K-vertex-connected induced subgraph. We prove the NP-Hardness of the problem and propose a novel two-phase strategy for placing VMs on hosts. At first phase, in order to balance traffic and workload among racks, we rank all racks using a fuzzy inference system and select the best ones based on a linear programming model. At second phase, we introduce a novel greedy algorithm to assign each VM to a host regarding a proposed communication cost metric. We evaluate our approach using CloudSim simulator whose results show our two-phase strategy is able to make a balance between host utilization and network traffic. It keeps more than 80 percent of the traffic rack local while reduces the average network link saturation to almost 40 percent with a low variance. Besides, the number of used hosts increase linearly by increasing the number of VMs which leads to a higher host utilization.

Previous
Next 
Keywords
Cloud computing

Big data

Virtual machine placement

Traffic aware VMP

Host utilization

Fuzzy logic

1. Introduction
Data-intensive applications such as image processing and machine learning, play an increasingly important role in many domains of business and scientific research. They are required to efficiently analyze a huge amount of data which is known as big data. In terms of big data analysis, the data-parallel strategy is an efficient approach to process large amounts of data on a cluster of computing machines. On the other hand, big data processing tools require a host cluster such as private cloud whose maintenance is not feasible for a majority of companies and institutes. Therefore, utilizing a public cloud infrastructure is a common and acceptable solution (Assunção et al., 2014, Philip Chen and Zhang, 2014, Hashem et al., 2015).

As virtual machines running a data-intensive application transfer a large amount of data among each other, running big data analytical jobs on the public cloud highly effects network traffic. Based on Cisco global cloud index 2016–2021, big data as a significant driver of data center traffic will be responsible for 20 percent of all traffic within the data center by 2021 which was 12 percent in 2016. Inefficient management of this traffic results in various undesirable effects like data transferring latency, high job completion times, network failure , etc. (Mann, 2015, Cisco, 2018). In addition, by rising the popularity of some platforms like Kubernetes which uses a virtual cluster of VMs to provide a specific service including data analyzing, the importance of considering the communication between VMs has been increased.

An optimized VM placement can improve the network scalability and reduce the communication cost to a great extent. Nevertheless, a majority of the preposed placement schemas model the VMs as independent objects and assign them to the physical machines without considering consumption of network resources (Filho et al., 2017, Masdari and Zangakani, 2019). On the other hand, the proposed methods for reducing the communication cost either contain some assumptions which make them impractical or do not consider others essential aspects of infrastructure management such as resource utilization and load balancing.

In this work, in contrary to previous works, we do not consider any predefined traffic among virtual machines. The reasons for elimination of this assumption are (1) it is hard to predict traffic among each pair of virtual machines beforehand, (2) in real-world environments these simplifying assumptions do not hold due to the dynamic interactions between these virtual machines. Alternatively, we assume there is a high amount of traffic with an unknown pattern and search for a cluster of close hosts whose connecting links have the maximum available bandwidth. However, this assumption may lead to a suboptimal solution where the communication pattern among VMs is neither many-to-many nor one-to-many. Therefore, the optimal placement is achieved by dividing VMs in groups as a majority of previous works has done.

In addition to communication cost, we consider improving host utilization as the second optimization objective which is a main concern for cloud service providers. In order to address the problem of east–west traffic flow within a data center, we only consider big data processing batch jobs where data is stored inside the VMs and exchanged between them as network traffic.

We also consider a one-to-one mapping for the assignment of the VMs to the hosts. Some of the previous placement schemas try to assign all the VMs to a single host which causes a low degree of reliability in the case of host failure (Liu et al., 2019). To decide about how many VMs of an application are allowed to place on a single host is out of the scope of this research and is discussed in Section 6 as the future work. Therefore in this work, we consider a high degree of reliability which means no two VMs belonging to the same application are placed on the same host.

In summary, in this paper the virtual machine placement (VMP) problem is investigated to improve the data center network performance and resource usage. Our work aims to select a group of hosts to place a cluster of virtual machines on them in order to keep the traffic more local and balanced inside a rack as well as between different racks in a data center while considering the hosts’ utilization. For this reason, we first try to find a specific area of data center with regard to our objectives, and then select the final hosts from the specified area. The target area, in the ideal case, is a single rack with enough resources and low network traffic, and if it is not possible, a combination of a minimum number of racks which are more likely to handle the request efficiently. In this work, the term distance refers to the number of hops between the source and the destination in a data center.

The main contributions of this work are summarized as following:

•
A new metric, namely networking-cost, is defined to calculate the communication cost between hosts. It considers both the traffic and the number of hops between hosts.

•
A fuzzy inference system (FIS) is designed to rank the racks based on their total free resources and intra-rack traffic.

•
A linear programming (LP) model is defined to select a minimum number of close racks having high rank while the traffic on their uplinks is low.

•
A greedy method is implemented to assign VMs to the hosts inside the selected rack(s) based on the metric networking-cost in order to create a balanced traffic in the data center.

The remainder of this paper is organized as follows: In Section 2, we review the related works. In Section 3, we introduce our VM placement problem and prove its NP-Hardness characteristic. In Section 4, we present our strategy which solves the problem in two phases. In Section 5, the evaluation results are analyzed; and in the final Section 6, we conclude our research and discuss the possible future works.

2. Related work
In a cloud data center, one of the main challenges is deciding on where to place the newly requested VMs which has to be done autonomously. There are different goals for VM placement such as load balancing, energy saving, minimizing network traffic, and reducing SLA violations (Masdari et al., 2016, Filho et al., 2017, Masdari and Zangakani, 2019). In this section, we briefly review the related work which considers the network traffic as the main objective or tries to improve it alongside other criteria in a multi objective problem modeling.

2.1. Traffic-aware/bandwidth-aware proposals
The first work in this domain is Meng et al. (2010) which proposes the idea of using traffic-aware virtual machine placement to improve the network scalability instead of changing network architecture or the routing protocols. The authors consider the number of switches on the routing path between VMs as the communication cost and model the placement problem as a quadratic assignment problem (QAP). They try to place the VM pairs with heavy mutual traffic on the hosts with low-cost connections. Although the approach reduce the east–west traffic to a great extent, their model is limited to homogeneous environments where the hosts contain a specific number of slots. Moreover, by assigning several virtual machines to single host, the degree of failure tolerance is reduced.

Fang et al. (2013) extend the work in Meng et al. (2010) by mapping VM-groups to racks on a one-to-one basis instead of mapping VMs to individual servers on an n-to-one basis in order to reduce the total inter-rack traffic. Besides, they introduce a routing algorithm to optimize the power consumption by reducing the number of used switches. However, they consider the equal amount of resources in each rack to group VMs without considering the resource utilization state of the racks.

Another work which extends Meng et al. (2010) is Li et al. (2016) by D. Li et al. which changes the algorithm by adding pseudo slot occupied by pseudo VMs to adapt it for heterogeneous environments. Besides, they consider the situation when the requested VMs have dependency with some existing VMs. Nonetheless, they assume all the VMs of an application, including the existing ones, share the same size.

In another work, Chen et al. (2013) aim to reduce communication overhead by considering dependencies across virtual machines. They propose an affinity-aware grouping algorithm to analyze virtual machines and put the most dependent ones in the same group. In the next step, they define the problem of assigning groups to the minimal number of hosts as a bin packing problem and use existing heuristic algorithms to solve it. However, their method suggests a reassignment for running VMs which may raise several live migrations as they solve the bin packing problem independent from the current location of the VMs.

Biran et al. (2012) introduce a method which tries to place a virtual machine on a host in a way that, while respecting resource constraints on host CPU, host memory and network cuts, minimizes the maximum network cut load. In the first proposed algorithm, mathematical programming is used which can be applied only if the network topology is a tree. In the second proposed algorithm, they greedily place related virtual machines on available hosts based on their traffic flows in order to minimize the maximum cut load without considering hosts utilization. Besides, as the authors also point out, the proposed approach is more suitable for reassigning the running VMs.

Georgiou et al. (2013) propose two algorithms for virtual machine placement which exploit user-provided hints about required bandwidth in order to reduce network resource consumption. The first solution which is an opportunistic algorithm tries to place a pair of communicating VMs on the host with the most available resources. If there is no host available or there is an anti-collocation request, they place the most resource demanding VM on the most available PM and look for a nearby PM to accommodate the second one. In the second solution, they present a vicinity based search algorithm which first finds a fitting neighborhood and then uses the previous algorithm to place the VMs. In addition to being limited to PortLand-based clouds, the approach always select the area or host with maximum available resource which leads to a low host utilization.

In Tseng et al. (2017) decreasing communication time for service-oriented applications is studied by F. Tseng et al. where the authors model the data center as an undirected graph containing all the switches, hosts, and virtual machine role instances. They consider both transmitted data amount and link capacity and propose two algorithms in order to select appropriate hosts for virtual machines placement according to their service type. The method is limited to some specific types of cloud services and depends on the nature of the application.

X. Li and C. Qian in Li and Qian (2015) try to optimize the network traffic while satisfying reliability requirements in a multi-tenant cloud. They develop a function based abstraction model to demonstrate the relation between VMs where each VM serves only one function. They define a cost function to calculate the distance between any two positions inside the network and then formalize the VMP problem based on the formula. The cost function considers the subscription ratio of each link on the path in order to balance the traffic within the data center. However, the subscription ratio is based on possible routing paths and does not represent the actual traffic. They solve the problem using a greedy algorithm in two phases named Partition and Place. The trace-driven simulations show that the algorithm can effectively optimize the network traffic under various circumstances. Nevertheless, the approach is limited to hierarchical tree based networks and does not consider resource utilization.

Lian et al. (2017) aim to avoid network bottlenecks by minimizing the maximal link utilization. They model the VM requests as a weighted undirected graph, where the vertices are VMs, the edges are communication between VMs, and the weight of edges is the size of the traffic flow. They try to place all VMs of the same request on the same host, if it is not possible they split the graph based on its bridges. Then, they place the VMs in a way that the maximal link utilization is minimal while ignoring the distance between hosts. They model the problem assuming the hosts and the requested VMs are uniform.

Liu et al. (2019) propose an approximation algorithm which tries to make a balance between network traffic and reliability. They first define a reliability model which determines how many VMs are allowed to be placed on the same host. Then according to reliability degree and available resources, the requested VMs are divided into several distinct partitions based on their communication and min cuts approach. Finally, the VMs are mapped to corresponding hosts in a way that the most communicating partitions are placed on the closest hosts. In their model, the VMs are homogeneous and the mapping between the hosts and the partitions is based on the number of the VMs in each partitions.

Lin et al. (2020) propose a method which aims to reduce the network traffic in the long run. Their method first analyzes the free resources of all hosts in order to select the minimum required number of hosts belonging to the minimum number of racks to serve the current request. They also consider the running VMs’ end time to produce more concentrated free resources in future. Then they group the requested VMs according to the selected hosts and the traffic between VMs. Finally each group is assigned to the intended host. The proposed method assumes the VMs are homogeneous and consider free resources as a number of slots. Besides, when the requested VMs do not fit a single rack, the extra racks are selected without considering the distance.

There are several studies in the literature which are limited to the MapReduce programming model and architecture. Alicherry and Lakshman (2013) suppose that they already know the location of data sets and introduce an algorithm to assign a subset of deployed virtual machines to data nodes in order to minimize data access latency while satisfying system constraints. They also add additional constraints to take inter virtual machine communication into consideration. They consider a threshold for inter-VM distance and model the problem as a graph clique problem solved by a heuristic algorithm.

Kuo et al. (2014) consider the same specific scenario as Alicherry and Lakshman (2013) where the location of data nodes (DNs) and the access latency between nodes are known and fixed. The aim is assigning a VM to each DN which minimizes the maximum access latency between VMs and data nodes. They propose a 3-approximation algorithm by employing a threshold technique. Subsequently, they improve their method by introducing a 2-approximation algorithm.

Yan et al. (2012) propose a method to create a virtual cluster for MapReduce applications whose distance is minimized. Their goal is to decrease network traffic and model the problem with an integer linear programming. The model is solved using a greedy algorithm which places virtual machines on the hosts as close as possible. The first virtual machine is chosen randomly. They assume that each host is able to support some specific types of virtual machines.

The main purpose of work Xu et al. (2017) by C. Xu et al. is optimizing the topology of the virtual network for MapReduce jobs in order to create a balance between data transmission latency and data processing rate. They consider some communication agents to transfer data chunks, then introduce a performance optimization model for cloud-based MapReduce workflows whose goal is to minimize the total task execution time. In the evaluation phase, a centralized architecture is used to deploy the physical cluster and all the hosts are connected to a single switch.

In another scenario Ilkhechi et al. (2015) suppose there are a few network nodes named sink on which other virtual machines are dependent. In the scenario, the tendency to transmit massive traffic to sinks is so high that it is an important factor in measuring the efficiency of a virtual machine. Based on the subject scenario a satisfaction metric is defined to show how a host is appropriate for a given VM. The metric is calculated using the cost between hosts and sinks along with the virtual machines demand vector. They introduce two different approaches for solving the problem, namely Greedy-based which assigns the VM with highest demand to the host with highest satisfaction score and Heuristic-based which considers the minimum performance degradation over the remaining VMs. The proposed method, as also mentioned by the author, is an off-line approach which places all VMs in once.

2.2. Multiobjectives optimization proposals
Wang et al. (2014) aim to improve virtual machine placement mechanisms in software defined data center networks considering three objectives: (1) traffic balancing, (2) energy saving, and (3) SLA violation reduction. They propose a combination of energy-efficient and QoS-aware mechanisms to solve the VMP problem in three steps. First, VMs are partitioned into groups such that the number of VMs in each group is balanced and costs between different groups are minimized. Then, each group of VMs is assigned to a host by considering the resource utilization and giving a priority to power-on hosts. Finally, they use the advantage of software defined networking to achieve flow transmission in networks without congestion. The proposed approach does not offer an efficient method to calculate the number of groups and ignore the distance between hosts while placing each group of VMs to a host.

Duong-Ba et al. (2019) propose a multi-objective formulation for an optimal VM management in data centers in order to reduce resource usage and power consumption. They consider three optimization objectives, namely load balancing, cross traffic cost minimizing, and migration cost minimizing. They introduce a multi-level joint VM placement and migration (MJPM) algorithm which first partitions VMs by a community detection algorithm. Next, subset sum algorithms are used to group the communities. Finally, the placement and migration problem is solved for each group in parallel by applying an algorithm which tries to place VMs in the minimum number of hosts in several optimization iterations. The approach does not consider the distance between hosts; besides, it reorganizes the VMs which leads to several costly migrations.

Larumbe and Sanso (2017) try to optimize the quality of service and the power consumption of applications while taking into account the communication traffic between VMs. They model the VM assignment problem as a mixed integer programming model. The objective function consists of three variables, namely the communication delay among VMs, the server power consumption, and the overflow of each rack. A hierarchical heuristic algorithm is used to solve the model which works based on the Tabu search. The proposed approach has a high time complexity which makes it unsuitable for a real world scenario.


Table 1. The comparison between the reviewed approaches.

Reference	Limitations and assumptions	Optimization objectives
No.	Year	No prior info required	Heterogeneous VMs	Heterogeneous hosts	Host failure tolerance	Communication cost	Resource utilization	Load balancing	Traffic cost	Other
[35]	2010	×	×	×	×	Number of hops	×	×	✓	×
[13]	2013	×	×	×	✓	Number of hops	×	×	✓	Power optimization
[28]	2017	×	×	×	×	Not defined	×	×	✓	×
[36]	2018	×	✓	×	×	Not defined	×	×	✓	Power optimization
[12]	2019	×	✓	✓	×	not defined	✓	✓	✓	Power optimization
[30]	2019	×	×	×	✓	Number of hops	×	×	✓	Reliability guarantee
[29]	2020	×	×	✓	×	Number of hops	×	×	✓	Power optimization
[14]	2020	×	✓	✓	×	Number of hops	✓	×	✓	Power optimization
This work	–	✓	✓	✓	✓	New metric	✓	✓	✓	×
In Pahlevan et al. (2018) A. Pahlevan et al. aim to minimize the overall server and accordingly data center power consumption, and network traffic. They formulate the problem as an integer linear programming (ILP) model for the optimization. They propose a two-phase greedy heuristic algorithm which jointly minimizes power consumption and network traffic during the first phase and then allocates resulting traffic in a network topology-aware fashion during the second phase. At first, they determine the data communication patterns between VMs, the CPU utilization, and the memory requirements for the next time slot. Then, they use the VMs’ information along with data center network topology to re-allocate or migrate the VMs such that highly data-correlated VMs are placed together, while highly CPU-load correlated VMs are placed apart from each other. The proposed approach assumes a homogeneous data center and has a tight dependency to the nature of workload as it uses prediction techniques.

S. Farzai et al. in Farzai et al. (2020) propose a genetic-based meta-heuristic algorithm to minimize power consumption, resource wastage, and bandwidth usage in the data center. They first define a mathematical model for each objective and then merge them into a multi-objective communication-aware optimization model. To reduce the communication cost, they try to place the VMs with the highest volume of data transferring on either the same host or the ones with minimum hops in their connection. In the presented genetic algorithm the chromosome length is equal to the number of requested VMs and contains the index of the assigned host. The Fitness function is determined based on the mentioned objective functions. The proposed approach is an off-line approach which places all VMs in once.

2.3. Summary
The studies mentioned above have valuable contributions in this domain and are the basis of our work. Nonetheless, all of them assume to know the communication patterns among VMs in the time of the request arrival, except few works which are limited to some specific scenarios (Yan et al., 2012, Alicherry and Lakshman, 2013, Kuo et al., 2014, Xu et al., 2017). A majority of works consider traffic pattern between VMs as an input of the problem (Meng et al., 2010, Li et al., 2016, Biran et al., 2012, Tseng et al., 2017, Li and Qian, 2015, Lian et al., 2017, Liu et al., 2019, Ilkhechi et al., 2015, Wang et al., 2014, Farzai et al., 2020), while others use either prediction methods (Fang et al., 2013, Pahlevan et al., 2018, Lin et al., 2020) which limit them to a specific type of workloads, or user hints (Georgiou et al., 2013, Larumbe and Sanso, 2017) which is not practical in real world scenario. There are few works that use actual VMs traffic and offer a reassignment (Chen et al., 2013, Duong-Ba et al., 2019) which causes several costly live migrations.

Furthermore, most of the works try to place VMs as compactly as possible on a single host to save the traffic consumption (Meng et al., 2010, Fang et al., 2013, Li et al., 2016, Chen et al., 2013, Biran et al., 2012, Li et al., 2016, Lian et al., 2017, Wang et al., 2014, Duong-Ba et al., 2019, Pahlevan et al., 2018, Farzai et al., 2020). This technique places the VMs possibly together with their High-Availability (HA) replicas (forming a virtual cluster) on the same node which raises concerns with respect to a host failure and finally the availability of the service (Yang et al., 2017).

In addition, although the reviewed works consider the VMs as a group of communicating objects, they do not take the relation between hosts into account. They select a host based on individual factors such as its free resources which does not guarantee a suitable selection for the next host. In this work, we try to first find an area which guarantees having enough resources provided by close suitable hosts, and then place the requested VMs in an optimized way.

Table 1 shows a comparison between the proposed method with a subset of related works including 2 first highly cited works Meng et al. (2010), Fang et al. (2013) and the 6 most recent ones.

3. VM placement problem modeling
To model our problem, we first define the metric networking-cost for computing the communication costs between hosts, which considers the number of hops and the existing traffic in the route from source to destination in order to make a balanced traffic within the data center network. Then, we exploit this metric to define our VM placement problem using graph theory. All the symbols are explained in Table 2.


Table 2. Problem modeling key symbols.

Symbol	Definition
The source host for a traffic flow
The destination host for a traffic flow
The path from host  to host 
The networking-cost for 
The average of the normalized saturation percentage for 
The number of hops in 
The source node for a link in 
The destination node for a link in 
The saturation percentage of the link from node  to node 
The normalized value for 
The bandwidth of the link from node  to node 
The minimum bandwidth of data center links
The number of requested virtual machines
An undirected complete weighted graph representing the data center
The set of graph nodes representing the available hosts
The set of graph edges representing the route between hosts
The set of nodes value representing the resource utilization
The set of edges weight representing the networking-cost
A K-vertex-connected induced subgraph as the selected neighborhood
3.1. Networking-cost metric
In our scenario, users request services in the form of a cluster of virtual machines whose goal is running data-intensive applications. In this case, because of transferring large amounts of data, the distance between virtual machines and the network traffic plays an important role in the data center’s scalability and efficiency.

The reviewed works in the previous section only consider the number of hops as the networking cost between hosts. Therefore, all hosts with an equal distance have an equal chance to be selected. As a result, while there are some hosts with light traffic, a host with heavy traffic may be selected which increases the links saturation. In addition, due to the fact that big data batch jobs are usually throughput bound, it is preferable to select two hosts with low traffic on the route between them rather than two closer ones with a heavy traffic on their connection.

Besides, by considering network traffic instead of predefined traffic between newly requested virtual machines, the actual traffic generated by the current VM placement will impact on future VM placement decisions.

Accordingly, we introduce the metric networking-cost for the path from host  to host  to measure the cost of data transferring on this path as follows: (1)
 

Where 
 is the number of hops between hosts  and  encoding the distance between them, and 
 is the average value for the normalized saturation percentage of the links between nodes (switch or host) in the path from host  to host .

If we simply use the average saturation value, we lose the impact of distance between hosts. On the other hand, if we consider the total saturation percentage of the links without averaging, we have no control over preferring a shorter path with a higher traffic to a longer one with lower traffic. According to Eq. (1), the cost value 
 is raised by increasing the traffic on the route from  to  (
) while the distance (
) between VMs controls its impact.


Download : Download high-res image (134KB)
Download : Download full-size image
Fig. 1. The networking-cost value for various conditions.

The diagram in Fig. 1 depicts the cost function behavior for different distances while the average traffic is increasing.

As the diagram shows, with the same average saturation percentage (
 representing the traffic on the path), the closer hosts (having a lower value for 
) always have a lower networking-cost. On the other hand, if the traffic difference between two paths is notable, the far hosts with lower traffic in their connection path are preferred to the closer hosts with higher traffic in their connection path.

We normalize the saturation percentage of a link for comparing the values of the links with different bandwidth (e.g. comparing a 10 Gbps link with 50% traffic to a 1 Gbps link with 50% traffic). To calculate the 
 we first define  as the routing path from host  to host  and 
 as the saturation percentage of a link in the path. The value of 
 is calculated as the traffic of the link divided by the bandwidth of the link. Then, we consider 
 as the normalized 
 which is calculated as follows. Suppose the minimum bandwidth of data center links is 
, and the bandwidth of link  is 
. So we have: (2)
 
Subsequently, we define the average normalized saturation percentage 
 as follows: (3)
 

As there are several routes from host  to  (according to the network topology), at each node the link with minimum traffic is selected to send a packet.

3.2. VM placement problem definition
•
The hosts are as close as possible in the number of hops.

•
The network traffic on the path between hosts is low.

•
The host utilization is as high as possible.

•
The idle hosts have a low priority to be selected.

•
The traffic on the links and CPU load of the hosts are as balanced as possible.

We use graph theory to formulate our virtual machine placement (VMP) problem. The primary objective is finding a group of host with minimum networking-cost between each pair of them (which includes both traffic and distance). The secondary objective is achieving the maximum possible CPU utilization.

We consider the data center as an undirected complete weighted graph  where  is the set of the hosts with enough resources for at least one VM,  is the path between hosts,  is the utilized percentage of the hosts’ CPU cores, and  is the metric networking-cost as the weight of each edge.

Having a request of  virtual machines, we need a K-vertex-connected induced subgraph 
 with enough resources to serve the request while has the maximum total resource utilization and the summation of edge weights (networking cost) is minimized. A sample data center and its related graph are shown in Fig. 2, Fig. 3. The graph contains 6 nodes representing the 6 hosts in the data center. The edge weights are computed using Eq. (1). For example, the value on the edge from  to  is computed as follows. 
 
 

3.3. Proof of NP-hardness
To prove NP-Hardness of the problem, we first prove the decision version of the problem is NP-complete. We define our decision problem as follows: given a cost , a minimum average resource utilization , and total requested VMs , is there a group of  hosts such that its total utilized resources is greater than or equal to  and the summation of networking-cost values for all its pairs of hosts is less than or equal to ?

Having total cost, the available resources, and a list of hosts, the correctness of the solution can be verified in polynomial time, thus the problem belongs to NP class. For the NP-Hardness, we will show that a solution to our problem is also a solution to Quota-Based Prized Collection Steiner Tree (PCST) problem (Johnson et al., 2000) which is NP-hard. We consider the decision version of the Quota-Based Prized Collection Steiner Tree problem whose definition is:

Given a graph , a non negative edge cost  for each , a non negative vertex prize  for each , and a prize quota  and a cost , find a subtree 
 such that 
, subject to 
.

The simple graph  of our problem is the same graph in the Quota-Based PCST, and the amount of prize quota () is the total utilized resources (). For each  and ,  and  are equal to 
 and 
 respectively.

We consider 
 and 
 as the total weight and prize of subgraph 
. Similarly, we consider 
 and 
 as the total weight and prize of an arbitrary spanning tree of 
. According to the fact that the weight of links is positive, we have: (4)
 Thus, if 
 and 
 for the induced subgraph 
, then we have 
 and 
 for its spanning tree 
. Creating an arbitrary spanning tree from a given graph is done in polynomial time, which shows our decision problem is NP-complete.

Since the decision version of the problem is NP-complete, the original optimization placement problem is NP-Hard.

4. A two-phase VM placement strategy
The analysis in Section 3 shows that the defined VM placement problem is NP-hard. Therefore, in this section, we introduce a Two-Phase VM Placement (2PVMP) strategy to solve the described problem.

In the first phase, we aim to find the optimal subarea of the data center which can fulfill the defined objectives including close hosts with low traffic rate and high resource utilization. We also try to avoid selecting the idle hosts.

In the second phase, we introduce a greedy algorithm, namely Traffic-Distance-Balanced (TDB) algorithm, which places the VMs on the hosts belonging to the selected subarea in order to reduce the networking cost as our primary objective. The flowchart of the proposed strategy is shown in Fig. 4.

4.1. NEIGHBORHOOD-SELECTION algorithm
We define a neighborhood as a set of one or more racks in a datacenter. Therefore to find a suitable neighborhood, we first evaluate all racks using a fuzzy inference system (FIS) (Mamdani and Assilian, 1975) and assign them the rank  regarding their free resources, idle hosts, and intra-rack traffic. Then, we use linear programming (LP) to formulate the objective function and select the suitable rack(s) for VM placement based on their rank and uplink traffic. We assume the hosts which do not run any VM are in sleep mode to reduce energy consumption. We name the hosts in sleep mode as the idle hosts and the ones which run at least one VM as the active hosts.

To fulfill the optimization objectives named in the problem definition, we need to consider both host utilization and communication cost (consisting of both traffic and distance). For this reason, we include the resource utilization and the downlink traffic of an edge (also known as top of rack) switch in the ranking, where uplink traffic is considered as a constraint in the LP model. Since the LP model selects the minimum number of racks, the VMs belonging to the same request are not distributed among different racks as long as there is at least one rack to deploy the whole request. In the case that more than one rack is required, the LP model cannot guarantee selecting the closest racks. To overcome this drawback, we divide the data center into several zones and avoid selecting racks from different zones. The zones are defined while designing the data center and are based on the topology of the data center as well as the preferred locality for the traffic flow. Algorithm 1 selects the target neighborhood by applying the FIS followed by the LP model while the racks are limited to be selected from the zone with maximum total ranks.


Download : Download high-res image (464KB)
Download : Download full-size image
The time complexity of the algorithm for evaluating  racks with a total of  hosts and  zones is:  for evaluating all hosts and computing the rank of all racks, plus  for selecting the best zone and determining the allowed racks, plus 
 (worst case) for the LP model solved by the simplex algorithm (Spielman and Teng, 2004). Although the overall time complexity is equal to 
, we expect a polynomial time in practice since the simplex algorithm has shown a polynomial time complexity in a smoothed analysis.

4.1.1. Fuzzy Inference system definition
A Fuzzy Inference System (FIS) is developed to calculate the rank  for all racks using three metrics as the inputs. The 4 fuzzy variables, including the VM rank as the output, are illustrated in Fig. 5. The fuzzy rules set shown in Table 3 contains 8 fuzzy rules which take the minimum of three membership values as the rule’s output. The rules are aggregated using the max method and the final rank is achieved by applying the Center of Sums (COS) defuzzification technique (Rao and Saraf, 1996). The designed FIS is described in detail as follows.

Fuzzy Membership function: The inputs of the FIS are defined by three metrics: (1) the average percentage of the hosts’ free processors (APE) to optimize host utilization, (2) the rack utilization percentage (RKU) to avoid selecting idle hosts, and (3) the average percentage of the links saturation (LKS) to select a low traffic subarea.

The fuzzy variables are calculated as follows:

•
APE: The number of total free processors divided by the number of total processors of hosts inside the rack.

•
RKU: The number of active hosts divided by the total number of hosts inside the rack.

•
LKS: The mean value of saturation percentage for the intra rack links.

The membership functions of these attributes along with the output membership function are shown in Fig. 5. The numbers on the APE membership function (Fig. 5(a)) are based on work Buyya et al. (2010) which analyzes the values for the CPU utilization thresholds in terms of power consumption and QoS provided. Since our method aims to avoid selecting the idle hosts, by defining the RKU variable, a priority is assigned to selecting the racks which are able to provide enough resources using their already active hosts. The utilization of a rack whose active hosts are more than zero percent is considered high to a certain degree. On the other hand, the utilization of a rack whose active hosts are less than 50 percent is considered low to a certain degree. For determining the LKS membership function, we define the low saturation level for the links with less than 75% utilization since the links with an average utilization above 75% are considered the hot-spots in the network (Kandula et al., 2009). Besides, the links with having 50% utilization and higher are mostly considered as a link in the high traffic situation.

•
Low: When the situation is invalid like a rack with low utilization but also few free resources (R7,R8). Besides, the situations when too many hosts are communicating in high rate (R1) or too many hosts have heavy workload and low free resources (R5,R6).

•
Medium: When selecting the rack may lead to activating idle hosts (R4,R3).

•
High: When enough resources are provided with active hosts in a light traffic (R2).

To calculate the rank of a rack, the COS defuzzification technique (Rao and Saraf, 1996) is applied which is formulated with regards to the output fuzzy membership function in Fig. 5(d) as follows. (5)
 
 
 
 


Table 3. The fuzzy rules set.

Operation	Input	Output
APE	RKU	LKS	Result
R1	High	High	High	Low
R2	High	High	Low	High
R3	High	Low	High	Medium
R4	High	Low	Low	Medium
R5	Low	High	High	Low
R6	Low	High	Low	Low
R7	Low	Low	High	Low
R8	Low	Low	Low	Low
The values 
, 
, and 
 represent the firing area of aggregated rules for low, medium, and high classes respectively. Subsequently, 
 
, 
 
, and 
 
 are the center of the respective areas.

Example

Considering the rack  as an example with 4 hosts whose situation is shown in Table 4, we have the values 0.60 as the percentage of free cores, 0.40 as the average link saturation, and 0.75 as the utilized hosts percentage. To rank the rack, we first calculate the membership values of parameters for both high and low condition based on the functions in Fig. 5, then we apply the fuzzy logical operations as shown in Table 5 whose results are 0.48 for high state, 0.0 for medium state, and 0.25 for low state. Finally, using the output membership function and COS technique as the defuzzifier, the rank of the rack is 0.5904 computed based on formula (5) as follows: 
 
 


Table 4. An example for the situation of a rack containing 4 hosts.

IDLE	Free CPU cores	Total CPU cores	Uplink saturation
No	16	32	0.55
No	20	32	0.45
No	6	16	0.6
Yes	16	16	0.0

Table 5. The fuzzy rules result for the example rack in Table 4.

Operation	Input	Output
APE	RKU	LKS	Result
R1	0.75	0.75	0.22	Low:0.22
R2	0.75	0.75	0.48	High:0.48
R3	0.75	0.00	0.22	Medium:0.00
R4	0.75	0.00	0.48	Medium:0.00
R5	0.25	0.75	0.22	Low:0.22
R6	0.25	0.75	0.48	Low:0.25
R7	0.25	0.00	0.22	Low:0.00
R8	0.25	0.00	0.48	Low:0.00
4.1.2. Linear programming model definition
A linear programming (LP) model is designed to select the minimum number of the highest ranked racks with enough free bandwidth in their uplinks to place the requested VM cluster. All the symbols are explained in Table 6.

To measure if a rack has enough free bandwidth on its uplinks, we compare the average saturation percentage of the links with a threshold 
. Similar to the bandwidth evaluation for FIS in Section 4.1.1, we consider 
 to be 70 percent.


Table 6. Linear programming key symbols.

Symbol	Definition
The status of being allowed for the th rack
The selection percentage of the th rack
The rank of the th rack calculated by FIS
The number of hosts with enough resources in the th rack
The average used bandwidth of the th rack’s uplinks
The threshold for the network link saturation
The number of newly requested virtual machines
The number of racks in a data center
To model the rack selection problem, we consider a data center of  racks and define 
 as the number of hosts in each rack which have enough resources to serve at least one requested VM. Then, we define 
 as the rank of each rack and 
 as its status of being allowed which are computed based on the fuzzy inference system in previous phase. Finally, we define the objective variable 
 to show which percentage of the hosts inside a rack is selected.

As the maximization objective has a tendency to increase the value by increasing the number of selected racks, we define a minimization objective in order to keep the number of selected racks as minimal as possible. Therefore, we try to minimize the summation of 
 instead of maximizing the summation of 
. The proposed LP model is given by Eq. (6) which aims to select the minimum number of racks with maximum total rank subject to the constrains given in Eq. (7) to Eq. (10). (6)
(7)
(8)
(9)
(10)
 The constraint 1 forces exactly  hosts to be selected for  requested VMs which means selecting the minimum number of racks with enough resources to serve all the requested VMs. The constraint 2 forces the selected rack to have an uplink traffic less than or equal to considered threshold. The constraints 3 forces avoiding the situation of selecting hosts from a different zone than the active one and the constraint 4 guarantees a logical selection without negative values. We solve the model using the popular  algorithm which shows a polynomial time complexity in practice (Spielman and Teng, 2004). Then, send all the racks with at least one selected host as the selected neighborhood to the second phase.

4.2. Traffic and distance balanced (TDB) VM placement algorithm
Having a selected subarea of the data center as the outcome of the previous phase, we have to assign each individual VM a specific host in a way that, selected hosts are connected with the minimum possible number of low traffic links.

To implement our method, we use the weighted adjacency matrix () of the related graph where the elements are the networking-cost metric 
 which is described in Section 3.1.

Before beginning host selection, in line 8, we compute thenetworking-cost metric as the communication cost between each pair of candidate hosts. Then in the lines 9 to 12, we select the host which has the total minimum communication cost to all other candidate hosts as the first selected host. Subsequently, in the lines 13 to 19, we assign it a VM whose requested resources are less than the host free resources. In the following, for each VM, we find a host which has enough resources and the summation of its communication cost to all the selected hosts is minimum.

The time complexity of the algorithm for placing  requested VMs on  candidate hosts is 
 for filling the communication cost matrix, plus 
 for selecting the first host and its VM assignment, plus 
 for host assignment to the rest of VMs. Therefore, the overall time complexity is 
.

To describe the TDB-VM-PLACEMENT method using an example, we consider the data center in Fig. 2 and its weighted adjacency matrix (Fig. 6) as the selected neighborhood with a total of 6 candidate hosts for placing 3 requested VMs. Based on the algorithm 2, the first selected host is  whose total communication cost to other hosts is 3.93 which is the minimum. Then, the next selected host is  Which has the minimum communication cost 0.59 to the first selected host , and the last selected host is  which has the minimum summation of communication cost 1.6 to the hosts  and .

5. Simulation experiment
In this section, we analyze the performance of the proposed VM placement strategy (2PVMP), described in Section 4, using CloudSim 4.0 (Calheiros et al., 2011) (in particular the package “org.cloudbus.cloudsim.network.datacenter”) which is a toolkit to model and simulate cloud computing. To make generating workload with communication stages more flexible and independent from the code, we extended the simulation to dynamically create and submit the VMs in various time intervals based on a dataset in JSON format. The process of creating the dataset is explained in Section 5.2.

The performance is evaluated under two data center topologies, including K-ary three-layer fat-tree and two-tiered spine-and-leaf. The simulated data center architectures, explained in Section 5.1, contains 128 hosts to have the data center in high load state with regards to the workload dataset.

We investigate the host utilization and the network traffic and compare our approach to the following algorithms:

•
Greedy: The VMs belonging to the same request are placing as close as possible. The target area of the data center is selected randomly with a uniform distribution. This approach causes the minimum network traffic without considering host utilization.

•
Best Fit (BF): For each VM a host with minimum free resource which can serve the VM is selected. This approach causes the maximum host utilization without considering the network traffic.

•
RoundRobin (RR): For each VM the next available host is selected where the hosts order is based on their index. This approach causes a balanced workload distribution. In addition in our implementation, the hosts under the same switch have a sequential index number which causes a higher chance for placing the requested VMs close together.

•
Random: For each VM an available host is selected randomly. Although this approach does not consider any optimization objective, it is used in real world due to its scalability and low computational cost.

5.1. Environment setup
The first simulated network topology is a K-ary three-layer fat-tree (Al-Fares et al., 2008) with the K equal to 8. The data center consists of 128 hosts divided equally between 32 racks (8 pods). Each rack (edge switch) is connected to 4 aggregation switches, and each aggregation switch is connected to 4 root switches. There are a total number of 16 root switches, 32 aggregation switches, and 32 edge switches. The bandwidth of the links between hosts and edge switches is 10 Gbps, and other links have a capacity of 40 Gbps. We divide the data center into 8 zones where each zone includes one pod.

The second simulated network topology is a two-tiered spine-and-leaf (Cisco, 0000) where every lower-tier switch (leaf layer) is connected to each of the top-tier switches (spine layer) in a full-mesh topology. To create a data center with the same number of hosts as the first simulated topology (128 hosts), we consider 16 leaf switches connected to 8 spine switches. Each leaf switch is connected to 8 hosts forming a rack. The bandwidth of the links between hosts and leaf switches is 10 Gbps, and other links have a capacity of 40 Gbps. As in the spine-and-leaf topology the distance between each pair of racks are fixed and equal to one hop, we consider the whole data center as a single zone.

To have a heterogeneous environment, the designed hosts are HP Proliant servers under 3 different configurations shown in Table 7 which is provided based on an academic private cloud.


Table 7. Configuration of the data center physical hosts.

Processor	Logical cores	MIPS	RAM (GB)	proportion
4*10 core E7–4860	80	5200	385	13%
4*8 core X7550	64	4600	516	20%
4*8 core X7550	64	4600	256	67%
5.2. Workload structure
CloudSim gives the possibility of defining various kinds of applications. Each application contains several applets which themselves consist of several processing or data transferring stages. To create a workload as realistic as possible, we need two types of information about job execution in the real world. First, how resources are consumed and then how data is transferred. Thus, a dataset containing the mentioned information is generated using the following approach.

We created a virtual cluster of 10 KVM-based virtual machines with different kinds of resources which are captured in Table 8. Then we deployed a fully distributed Apache Hadoop 3.1.1, Apache HBase 1.4.7, Apache Hive 3.1.0 and Spark 2.3.1 installation on the cluster in order to run data intensive batch jobs. To achieve a realistic outcome, we run the big data benchmarks described below on the cluster and collect the information about their resource consumption and data transferring into a database in order to generate our final dataset. We chose InfluxDB for this purpose.


Table 8. Hadoop virtual cluster capability.

VM	VCores	Ram (GB)	Storage (GB)	OS
Master	2	4	40	CentOS 6.9
Worker001	4	4	40	CentOS 6.9
Worker002	2	8	40	CentOS 6.9
Worker003	2	8	40	CentOS 6.9
Worker004	2	8	40	CentOS 6.9
Worker005	2	8	40	CentOS 6.9
Worker006	2	8	40	CentOS 6.9
Worker007	4	16	80	CentOS 6.9
Worker008	4	16	80	CentOS 6.9
Worker009	4	16	80	CentOS 6.9
•
BIGDATABENCH: which contains seven different types of workloads (Gao et al., 2018). In this evaluation, we only use Hadoop benchmarks which are batch jobs.

•
Statistical Workload Injector for MapReduce (SWIM): which is provided based on running jobs at Facebook and Yahoo (Chen et al., 2011).

•
Yahoo! Cloud Serving Benchmark (YCSB): which contains HBase related workloads (Cooper et al., 2010).

•
Spark benchmark: which contains K-means and Logistic Regression workloads (Li et al., 2015).

•
Hive benchmark: which contains Hive queries workloads (Hortonwork, 0000).

CloudSim assigns each cloudlet to a single VM and considers a group of cloudlets communicating to each other as an application. In our simulation, each request is an application asking for one or more VMs. To collect real world information, we run each job in a specific time slot, then use the timestamp and the IP of the VMs to distinguish resource consumption and network traffic of different cloudlets and applications.

5.3. Performance results
In order to validate that our approach can indeed improve the cloud data center performance, we present the evaluation results regarding resource utilization and network traffic. We first evaluate our approach on data center resource utilization in Section 5.3.1 by measuring the total number of active hosts and their CPU and memory utilization rate. Then in Section 5.3.2, we examine the capability of our method to keep the traffic between VMs as local as possible inside a rack. Finally in Section 5.3.3, we investigate how the traffic is distributed among data center links. Besides, in Section 5.3.4 we investigate the scalability of the proposed algorithm.

5.3.1. Increasing the utilization of the cloud data center
To reduce power consumption of a data center, one way is to use as few numbers of hosts as possible to accommodate a certain number of VMs and put the remained hosts on idle mode. Besides, increasing a host CPU utilization causes a decrease in the power consumed per VM.

Fig. 7 depicts how the number of active hosts is changing when the number of submitted VMs is gradually increased. Subsequently, Fig. 8, Fig. 9 illustrate the change for CPU and memory utilization rates. The result of the BF policy is regarded as the optimal solution for the host utilization. Since the 2PVMP policy tries to keep the VMs of a requested cluster (i.e. a specific application) close together, it does not achieve the optimal result. Nevertheless, contrary to the other policies (Greedy, RR, and Random) which make almost all hosts active after a short time, the 2PVMP policy shows a stepwise increase because of giving a priority to deploy the newly requested VMs on the already active hosts and intelligently selecting an area which contains enough active hosts.


Download : Download high-res image (336KB)
Download : Download full-size image
Fig. 7. The number of active hosts in a data center with 128 total hosts under two different topologies.


Download : Download high-res image (374KB)
Download : Download full-size image
Fig. 8. The average CPU utilization of the active hosts under two different topologies.


Download : Download high-res image (408KB)
Download : Download full-size image
Fig. 9. The average memory utilization of the active hosts under two different topologies.

In addition, these characteristics lead to a better resource utilization even in early steps when the workload is light. Subsequently Fig. 10 illustrates how the average CPU utilization rate is distributed among active hosts where RR and Random methods show the most uniformly distributions. The results for 2PVMP and Greedy show an acceptable distribution while it is more diverse for BF method.

In compare to the recent schema proposed by Duong-Ba et al. (2019) which offers a rearrangement of all VMs and improve the Random policy by 30 percent, our method surpasses the Random policy by around 20 percent in high load situation (3500–4000 VMs). However the work (Duong-Ba et al., 2019) does not provide detailed information on resource utilization and workload condition.

5.3.2. Minimizing the inter-rack traffic
As it is unavoidable for several kinds of jobs such as video streaming applications to consume the bandwidth beyond the edge switches, the batch jobs in particular data-intensive ones should use these resources as little as possible.

So, we investigate the inter-rack communication traffic by gradually increasing the number of requested VMs which leads to a rise in the total amount of exchanged data within the data center whose results are shown in Fig. 11. The result of the Greedy policy is regarded as the optimal solution for inter-rack network traffic. Since the 2PVMP policy tries to use the already active hosts, it does not reduce the inter-rack traffic to the minimum possible value. Nevertheless, the inter-rack traffic is less than 10 percent of total produced traffic under Spine-Leaf topology and around 20 percent under 8-ary Fat-Tree topology. The RR method also shows an acceptable inter-rack traffic, since the order of host indexes leads to a higher probability for selecting neighbor hosts. The other policies (BF and Random) produce a high amount of the inter-rack traffic, as they ignore the communication between the VMs in the placement decision.


Download : Download high-res image (427KB)
Download : Download full-size image
Fig. 11. The amount of inter-rack data transferred among hosts in a data center with 128 total hosts with two different topologies.

In compare to the work (Duong-Ba et al., 2019), our method surpasses the Random policy by around 80 percent while the work (Duong-Ba et al., 2019) achieves 55 percent less traffic than Random placement.

5.3.3. Reducing the probability of network link congestion
To reduce the probability of packets loss and network failure, the traffic should be as balanced as possible among data center links to prevent a link congestion. Therefore, we analyze the impact of each placement policy on traffic distribution inside the data center.

We stored the maximum saturation percentage of all network links occurring during the whole simulating time whose distribution is shown in Fig. 12 as a boxplot. As the diagram indicates, The 2PVMP policy causes a low saturation percentage as it selects low traffic racks in the first phase and low traffic hosts in the second phase. The Greedy policy also shows a low saturation percentage as it uses more hosts and distributes the traffic among more links. However, because of prioritizing the network traffic for placing VMs in second phase, the median in 2PVMP policy is less than Greedy policy. The behavior of RR and Random methods is depended on the network topology. In addition, similar to Greedy, they use more links to transfer the same amount of data. Due to compacting many VMs on a single host to achieve the maximum utilization, the BF method shows the worst result where the maximum saturation percentage is higher and more diverse.


Download : Download high-res image (254KB)
Download : Download full-size image
Fig. 12. The distribution of the Data center links’ maximum saturation percentage.

5.3.4. Investigating the scalability of the proposed algorithm
The scalability means that the runtime of the algorithm increases linearly with the size of the problem given fixed amount of hardware resources. We examined the scalability of the algorithm using CloudSim simulator running on a Macbook with 2.7 GHz Intel Core i5 processor and 8 GB 1867 MHz DDR3 memory.

We consider the number of hosts in a data center as the problem size while it defines the solution search domain scale.


Download : Download high-res image (212KB)
Download : Download full-size image
Fig. 13. The scalability of the proposed algorithm in terms of the hosts number in a data center for placing 2000 VMs with two different topologies.


Table 9. The configuration used to examine the scalability.

Total hosts	Fat-tree	Spine-leaf
Racks	Hosts in a rack	Racks	Hosts in a rack
54	16	3	9	6
128	32	4	16	8
250	50	5	25	10
432	72	6	36	12
686	98	7	49	14
To investigate how scalable the algorithm behaves in practice, we increased the size of the simulated data center and measured the total computational running time of the placement algorithm for a fixed number of 2000 VMs. The results are shown in Fig. 13 which is expressed in milliseconds. The Table 9 shows the changes in data center configuration for both topologies. For the K-ary fat-tree topology, we increased the K from 6 to 14 which causes an increase in the both number of hosts and racks. To generate a similar situation under spine-leaf topology, we increased the number of leaf switches, spine switches and the ports of the leaf switches in order to have a similar growth in the number of hosts and racks. It can be seen that the required computational time grows linearly in both fat-tree and spine-leaf topology. The reduction in the time for 250 hosts with the spine-leaf topology happens due to the increase in the number of the hosts in each rack. By increasing the rack capacity, the selected neighborhood for the second phase is changed from mostly 2 racks to mostly 1 rack which reduces the search domain.

6. Conclusion and future works
This paper presents a novel strategy for solving VMP problem intending to decrease inter-rack and balance intra-rack traffic while utilizing the infrastructure’s resources. we first select a small subarea (one or more close racks) of the data center which is able to fulfill the optimization objectives, then the final hosts are selected from the subarea in order to optimize the main objective (reducing the communication cost).

We evaluate our method using CloudSim simulator whose results show a desirable tradeoff between traffic management and resources consumption. Our approach is able to keep a majority of traffic rack local without losing the host utilization. Besides, it keeps both traffic and workload balanced inside the data center.

Nonetheless, the proposed strategy needs to be improved in two different directions. First, in this work we assign VMs to hosts in a one-to-one mapping. Although this approach is host-failure tolerance and improves the service reliability, it does not benefit from consolidation advantages such as host local traffic. Therefore, providing a tradeoff between reliability and consolidation on top of the current objectives seems to be necessary. Second, we evaluate the hosts using a fuzzy logic inference system which is designed based on expert knowledge. There is an opportunity to improve this part using the reinforcement learning technique. Therefore, future work involves extending our strategy to cover more optimization objectives using the reinforcement learning technique. Furthermore, we will implement the proposed strategy in OpenStack to show it is a practical method.