Human-aware robot navigation is the key to facilitate the deployment of mobile robots into human–robot shared environments. Although many efforts have been made in this regard, almost all of the existing works focus on the constraints such as user comfort and social rules, while ignoring to consider user preferences. And it is expected that the mobile robot is able to navigate in the user-desired areas according to user preferences, e.g., navigating at low speed in the bedroom. To this end, this paper studies the problem of allowing users to control the robot’s working areas to ensure robot aware navigation based on their preferences. To address the intrusiveness of additional facilities or devices to users, we propose a non-intrusive solution by deploying the virtual areas that are non-physical areas but are respected by the mobile robot while performing navigation tasks. Farther, an interactive interface is developed to support the common (non-expert) users to flexibly define the desired areas and specify the navigation behavior of the robot according to their preferences. The proposed solution is fully evaluated through extensive experiments. Experimental results are presented to demonstrate the validity of our approach, and show that the mobile robot can change its navigation behavior in user-defined areas according to user preferences. Supplementary video can be available at https://youtu.be/oXtbNavLmMk.

Previous
Next 
Keywords
Human–robot shared environment

Mobile robot

Human-aware

Navigation

Virtual area

1. Introduction
Benefiting from the continuous technical advancements in the field of robotics, such as mapping (Cadena et al., 2016), localization (Feng et al., 2019), navigation (Tian et al., 2016), exploration (Wang et al., 2019), and human–robot interaction (Lemaignan et al., 2017), an increasing number of mobile robots are infiltrating into our daily life. For such mobile robots, the ability to navigate autonomously and safely in human–robot shared environments, such as home environment, is crucial (Zhang et al., 2019). In order for mobile robot to ‘coexist’ with humans, the robot has to take into consideration the presence of humans when performing navigation tasks (Charalampous et al., 2017). To this end, a new paradigm for robot navigation, referred to as human-aware navigation (Kruse et al., 2013), has emerged to cater for human comfort and social conventions. For instance, the robot can change its navigation behavior based on the modeled social rules to make the interaction more natural and comfortable. From the perspective of users, such solution is user-friendly, but may not be flexible enough (Sprute et al., 2018). In some realistic cases, users particularly expect the mobile robot to be able to navigate in the user-desired areas according to their preferences.

Let us consider a home scenario, as shown in Fig. 1, where a mobile robot is deployed to serve the user. In this scenario, if the robot wants to live in harmony with user, it has to respect the user-specified areas and perform navigation tasks according to user preferences. For example, due to privacy concerns, the user does not allow the networked robot to enter the bathroom (i.e., area ①). Also the user bought a carpet and spread it in the bedroom (i.e., area ②). In this case, the robot is instructed to avoid such area when navigating to prevent damage to the carpet. Besides, the robot should keep a certain distance from the glass area (i.e., area ③) when performing tasks to avoid collision. Last but not least, the robot is required to appropriately reduce its speed (to keep it quiet) while navigating in the bedroom (i.e., area ④). In the above home scenario, the user’s preferences (or needs) for the mobile robot are not harsh, which are common in most realistic scenarios. If the robot wants to be accepted by the user, it needs to consider and respect such situations, which is crucial for the deployment of mobile robot in the human-shared environments. Yet achieving this scenario is arduous for the mobile robot due to several uncertainties such as

•
diversity of preferences (e.g., the robot is expected to avoid the bathroom area ① and navigate at low speed in the bedroom area ④);

•
difficulty of area detection (e.g., the carpet area ② cannot be detected because it is below the laser rangefinder, and the glass area ③ is difficult to observe due to its transparency);

•
ambiguity of area boundaries (e.g., how far from user-desired areas ② and ③ is appropriate to change the robot’s navigation behavior).

One straightforward solution to address these uncertainties is to add the physical boundaries (Billard and Kragic, 2019) or deploy the additional detection devices (Zhang et al., 2020a) in the environment. But for users, this is intrusive. Additionally, an alternative is to use learning-based approach to predict user preferences (Chang et al., 2017). However, since users living in a home may have different preferences, it is usually impossible for the robot to measure and learn these preferences accurately. Therefore, the key challenge to enable the mobile robot to navigate according to user preferences is how to flexibly determine user-desired areas in a non-intrusive way and change the navigation behavior of the robot accordingly.


Download : Download high-res image (490KB)
Download : Download full-size image
Fig. 1. Examples of the user’s preferences for mobile robot in specific areas of the home scenario.

To address this challenge, this paper proposes a virtual area (VA) solution for robot aware navigation. The VA is a non-physical area, but is respected by the mobile robot when performing navigation tasks. The proposed approach is implemented based on an occupancy grid map (OGM) as OGM is especially suitable for robot navigation. More specifically, on the basis of the created OGM, an interactive interface is adopted to flexibly define user-desired VAs, in order to restrict the working area of the robot and ensure that the robot can navigate in an acceptable way in accordance with user preferences. Remarkably, the presented solution is unique in that the deployment of any physical boundaries or additional detection devices in the environment are not required. Hence it is non-intrusive to the users. The contributions of this paper are threefold:

(1)
A non-intrusive solution is proposed for robot aware navigation, which allows the common (non-expert) users to flexibly define the desired VAs and change the navigation behavior of the robot in a simple way according to their preferences. To the best of our knowledge, this work is the first to permit users to specify the behavior of a robot within the user-defined VAs.

(2)
A visual interactive interface is developed to facilitate the integration of OGM and VAs, in which a method based on unordered points is introduced to characterize the enclosed VA. The interface supports non-expert users to accurately define VAs of arbitrary shape and size, and easily specify restrictions on mobile robot while featuring little effort.

(3)
The presented solution is carried out in indoor environments, and evaluated by comparing it with other four methods in terms of accuracy, efficiency and correctness. Also, a practical application scenario is presented to further demonstrate the validity and practicability of the solution.

The remainder of this paper is structured as follows: Section 2 provides a brief literature review on robot aware navigation. Then the problem formulation is introduced in Section 3. In Section 4, the presented non-intrusive solution for robot aware navigation is elaborated. Section 5 presents the experimental evaluations. Finally, the conclusion and future work are given in Section 6.

2. Related works
Human-aware robot navigation is described as a novel robot navigation paradigm that takes into consideration of the presence of humans (Kruse et al., 2013). It tends to enable the mobile robot to navigate in a human-acceptable manner. Therefore, a large body of works have been carried out in an effort to accelerate the acceptability of mobile robots in human–robot shared environments (Charalampous et al., 2017).

In an early study investigated by Sisbot et al. (2007), a human aware robot planner was proposed, in which the safety criterion and visibility criterion are modeled with OGM. The purpose is to provide an acceptable robot path for guaranteeing human safety and comfort. Rios-Martinez et al. (2011) introduced a risk-based robot navigation strategy by considering social conventions, which enables the mobile robot to respect the personal space (Hall, 1966) while navigating toward a given goal. By estimating human localization, Kenk et al. (2019) designed a robot aware navigation framework for comfortable navigation. Based on social rules, Ferrer et al. (2017) presented a robot aware navigation framework to walk side-by-side with human in a safety and natural manner. Similarly, Truong and Ngo (2018) proposed a unified framework, with the goal to make the robot approach humans in a socially acceptable way. Above-mentioned robot aware navigation methods (or frameworks) are proceeded with human–robot proxemics (Rios-Martinez et al., 2015) and human behavior mainly, and attempt to make the mobile robot change its navigation behavior for ensuring human safety and comfort in human–robot shared environments. However, such methods have not considered the preferences of humans to restraint the navigation behavior of the robot. In other words, in some realistic scenarios, especially in home scenarios, users are particularly looking forward to being able to restrict the mobile (service) robot’s work area and control the robot navigation behavior according to their preferences.

The most direct approach to restrict the work area of robot and change its behavior is the deployment of physical markers (Park and Hashimoto, 2009) or landmarks (Lu et al., 2017). For instance, the artificial landmarks/beacons were deployed in the environment to change the robot navigation behavior (Curiac, 2016), whereas deploying a large number of landmarks can change the structure of environment or be intrusive to users. To reduce these concerns, the radio frequency identification (RFID) tags are considered as an alternative option (Sobral et al., 2018). Paredes et al. (2014) used RFID tags to design a signage system for robot navigation. But this solution may be inflexible. Alternatively, Nardi and Stachniss (2017) presented a robot navigation system by learning previous experiences to meet user preferences. Kretzschmar et al. (2016) modeled the navigation preferences of users to allow the mobile robot to navigate in a socially compliant way. Similarly, Zhang et al. (2020b) proposed a user preference learning method for mobile robots, which aims to guide the robot to perform services in a human compliant way. However, due to the diversity of user preferences, it is difficult to accurately establish the modeling. And additional measuring equipments are needed to observe user preferences.

To control the navigation behavior of the robot, Chestnutt et al. (2009) described a guiding approach according to the needs of operator. In this approach, the augmented reality system with pointing device is adopted to draw the desired paths for the robot. Coronado et al. (2017) introduced a gesture-based robot control method, where the user can control the robot navigation behavior with wearable devices. Nevertheless, these methods not only require special devices, but also need the operator to have certain operating skills. Boniardi et al. (2016) introduced a hand-drawn sketch method for robot navigation. Using the tablet interface, the user can define a desired path that should be followed by the robot to control the robot for task execution. Analogous, based on hand-drawn map, a robot navigation method was employed in Niu and Qian (2019). Although hand-drawn-based method seems convenient, it limits the robot’s autonomy to some extent and does not allow the user to define the robot’s work area. Toward this end, Sprute et al. (2019b) investigated four different ways to define the robot’s work area for avoidance. Among them, the Google Tango tablet is perceived to be best compromise of accuracy, flexibility and time cost (Sprute et al., 2018). Although the good performance can be achieved, the special hardware, i.e., Google Tango, needs to be deployed. For safety aware navigation, Luo et al. (2015) presented a motion planning based on the virtual obstacle. Similarly, Ueno et al. (2015) and Ravankar et al. (2019) implemented the virtual obstacle strategy to change the navigation behavior of the robot. In order to define the robot navigation zone, Ghorbel (2020) introduced a method of adding a virtual layer of obstacles to the costmap. PAL Robotics (Pal, 2020) developed a Map Editor based on RViz, which allows the user to define the zones of interest to constrain the robot’s navigation behavior. Although these methods allow the users to control the navigation behavior of the robot, they still suffer from essential weaknesses such as the needs for the professional technician to deploy the system, which poses challenges to non-expert users. To facilitate restriction of the workspaces of the robot, a teaching method based on a laser pointer was described in Sprute et al. (2019a) for the definition of virtual borders. Howard (2019) provided a map editing tool to permit the non-expert user to draw the expected boundary on a given OGM. However, modifying the defined boundary is tedious and does not support the motion restriction of the robot.

The representative works of above discussion are summarized in Table 1. Also, Table 1 shows that it is still unexplored to allow non-expert users to change the navigation behavior of the robot in the specific workspace according to their preferences, such as specifying the speed of the robot in the user-defined area.


Table 1. Comparison of related studies and this work.

Paper	Method	Special hardware	User-defined area	Velocity	Non-expert
Accessible	NO-GO	restriction	user
Sprute et al. (2018)	Virtual border	Tango device	–	✓	–	✓
Park and Hashimoto (2009)	RFID-based	No	✓	✓	–	–
Chestnutt et al. (2009)	Path-drawing	Augmented reality & pointing device	–	–	–	–
Coronado et al. (2017)	Gesture-based	Wearable device	–	–	–	–
Boniardi et al. (2016)	Hand-drawn	No	–	–	–	✓
Luo et al. (2015)	Virtual obstacle	No	–	–	–	–
Ravankar et al. (2019)	Virtual obstacle	No	–	✓	–	–
Ghorbel (2020)	Virtual layer	No	–	✓	–	–
Pal (2020)	Zones of interest	No	–	✓	–	–
Sprute et al. (2019a)	Virtual border	Laser pointer & cameras	–	✓	–	✓
Howard (2019)	Editing map	No	–	✓	–	✓
This work	Virtual area	No	✓	✓	✓	✓
3. Problem formulation
In this paper, an easy-to-implement non-intrusive solution is presented by the utilization of VAs for addressing the problem of controlling the working areas of the mobile robot to achieve the human-aware navigation based on user preferences. The solution allows non-expert users to flexibly define the desired areas and change the navigation behavior of the robot in a simple way. In the following, a formal description of the proposed methodology is provided for facilitating the formulation of the problem.

To facilitate robot navigation, the physical environment is modeled as an OGM, denoted by . The generation of OGM is formulated in the form of a joint posterior 
, as follows: (1)
where 
, 
 and 
 denote the sequence of robot pose, observation and odometry information, respectively. With this formulation, the OGM models the workspace of robot in terms of cells, where each cell is assigned an occupancy probability referring to the corresponding area of real-world environment. Formally, 
 represents the occupancy probability of cell  in . Besides, all possible coordinates 
 on  are defined as the domain of map 
. Since OGM models geometric properties of environment, the collision-free navigation is allowed in the environment.

Based on the OGM, our method allows users to define the desired area 
 and set corresponding restrictions according to the demand of users. The VA is an enclosed area. All possible coordinates  contained in 
 are defined as the domain of VA 
. Note that the robot navigation is implemented based on an OGM. Hence, in order to enable the robot to observe user-defined VAs with corresponding restrictions during navigation, it is desired to integrate 
 into . Toward this end, the user is allowed to interactively define a manipulation so that 
. This integrated map 
 includes VAs and the geometric properties of physical environment. Accordingly, after integration, 
 is composed of two domains, that is, (2)
 where 
 represents a domain of containing all possible coordinates of the 
, and 
 is the domain of the  except 
. In this context, the defined VAs can be estimated based on the localization information of the robot. Therefore, the integrated map 
 can be utilized for human-aware navigation and path planing.

4. Presented robot aware navigation method
The overall architecture of the proposed method for robot aware navigation is presented in Fig. 2. Intuitively, the whole framework consists of two parts. One is OGM creation and the other is VA creation. The part of creating VA is based on the established OGM. In the part of OGM creation, a robust simultaneous localization and mapping (SLAM) algorithm that uses an adaptive technique for Rao-Blackwellized particle filter (RBPF) is deployed (Grisetti et al., 2007). In such case, the reliable OGM can be built by the robot. On the basis of resulting OGM, users can easily define VAs and restrictions through the developed visual interactive interface according to their preferences. The user-defined VAs and restrictions are respected by the mobile robot. Thus the robot is able to navigate in a user-acceptable way on the basis of users’ preferences. The details are elaborated in the following subsections.

4.1. OGM creation
To create OGM, the popular approach of estimating Eq. (1) is the utilization of RBPF (Doucet et al., 2000). It is composed of a set particles 
, in which each particle 
 is presented by a robot pose 
, proposed grid map 
 and weight 
. Then based on odometry measurements 
, OGM is produced iteratively. However, this method suffers from the problems of computational complexity and particle depletion. In order to deal with that, by integrating the observations of laser rangefinder 
, an adaptive technique is adopted in this paper to create reliable OGM based on RBPF (Grisetti et al., 2007, Sidaoui et al., 2019), which can be summarized into the following five steps (or refer to the middle part of Fig. 2):

•
Pose propagation In each iteration, the new pose is propagated according to the previous pose 
 of that particle and the odometry measurement data 
 since the last filter update.

•
Scan matching Based on the map 
, the matcher  is performed starting from 
. The aim is to find the most likely pose 
 by matching current 
 against the current map.

•
Importance weighting According to the matching results, the importance weighting 
 is estimated for each particle based on importance sampling principle. If the matcher reports a failure, the motion model is used to calculate and update the pose and weight. Otherwise, the observation model is used.

•
Update map Based on the new pose 
 of each particle and the observation 
, the function  is executed to update each map 
.

•
Adaptive resampling After updating the next generation of particle set, whether to perform the resampling step is determined by an effective sample size 
. Only when 
 is below the pre-defined threshold 
, resampling function  is performed.

4.2. VA creation
In this paper, a VA is defined as a triple 
, where 
 denotes a polygonal chain, 
 and 
 represent the attribute restriction and velocity limit, respectively. These components can be specified by users using the visual interactive interface according to their preferences. Besides, it is allowed to define multiple VAs 
 on , where  is the number of VAs.

Based on OGM, the user can specify  points 
 that are used to generate the desired 
 enclosed by the polygonal chain 
 . Remarkably, this method (using specified points to automatically generate a closed area) is not only very convenient for users to learn but also saves a lot of efforts in drawing the area with curves or lines. According to Sprute et al. (2018), one can obtain (3)
 (4)
where  is a line segment operator, and 
 denotes a line segment between any two points. In this case, 
 can be represented by a closed polygonal chain 
.

For the convenience of the common (non-expert) users, they cannot be required to specify points used to generate the enclosed VA in a certain order. It is worth noting that the arbitrary polygon may be produced by unordered points. As an example, when we give five points, different areas or shapes can be obtained (see Fig. 3). But we want these unordered points to automatically generate only one closed area [refer to Fig. 3(b)]. To achieve this, the resulting polygon needs to satisfy the following conditions (5) 
 where 
 denotes the interior angle of the resulting polygon.


Download : Download high-res image (289KB)
Download : Download full-size image
Fig. 3. An example of the enclosed areas generated by five specified points. (a) The given five points. (b)–(f) Different enclosed areas can be obtained by the given five points. Colors are just for visualization.

This completes the definition of VAs. After that, users can specify the attribute restriction 
 of defined 
 to restrict robot behavior. In this work, there are two kinds of constraints: Accessible and NO-GO. Accessible means that this VA allows robot navigation and path planning, while NO-GO implies this VA cannot be navigated and planned through, similar to designating the area as occupied state. Meanwhile, when a 
 is marked by Accessible, users can also select or specify the velocity limit of robot 
, such as Low-speed, High-speed, Default and User-defined. These conventions are respected by the mobile robot. In order to convenient the users, a visual interactive interface is developed based on Python with TkInter1 (see Fig. 4) to define VAs and corresponding attributes. Using the interface, users can easily delete and add VAs. In addition, for the defined VA, its location is not movable. If the users want to change a VA, they can delete it and add the desired VA through the interactive interface.

A noteworthy aspect is that the creation of VA is based on OGM with the global coordinate frame. Thus, this ensures VA and OGM are always in the same coordinate frame without transformations, which plays an essential role in map integration.


Download : Download high-res image (382KB)
Download : Download full-size image
Fig. 4. Example screenshots of the procedure of defining virtual area through the visual interactive interface. (a) Specify arbitrary points. (b) Select or specify the corresponding constraints. (c) The resulting virtual area based on occupancy grid map.

4.3. Map integration
After creating OGM and VAs, it is desirable to get the integrated map 
 that allows the mobile robot to perform the navigation tasks in the user-acceptable way according to the preferences of user.

Essentially, the defined VAs 
 partitions OGM  into two parts with different properties, and that can be denoted as two sub-maps: (6)
which is the sub-map corresponding to the created VAs, and (7)
which is the sub-map corresponding to the complementary area of VAs. More specifically, 
 contains VAs and their attributes, while 
 contains the areas except VAs and inherits the attributes of OGM . In the end, the map 
 is constructed based on 
 and 
, as follows: (8)
 Based on 
, the robot can estimate the VAs according to the localization information. Hence, using 
, the human-aware navigation of mobile robot based on user preferences can be realized. Note that since the proposed solution is implemented based an OGM, the path planning and obstacle avoidance are permitted for robot aware navigation.

5. Experimental evaluation
The presented method is implement in the real-world environments. The performance is evaluated in terms of accuracy, efficiency, and correctness. Also, a practical application is provided to further demonstrate the validity of the proposed method. In this section, experimental results and performance comparison are elaborated. The video can be found in the supplementary material at https://youtu.be/oXtbNavLmMk.

5.1. Experimental setup
The experiments are performed on a robot operating system2 (ROS)-based mobile robot platform TIAGo (see the left side of Fig. 2). This robot is equipped with a SICK TIM561 laser rangefinder (field of view: 180°) for map creation and navigation. The resolution of OGM and the threshold of 
 are set as 0.025 m and 0.65, respectively. Besides, a flexible navigation stack (Marder-Eppstein et al., 2010) is adopted for robust robot navigation, in which an adaptive monte carlo localization (AMCL) method (Fox, 2003), D* algorithm (Stentz, 1994) and dynamic window approach (DWA) (Seder and Petrovic, 2007) are used for robot localization, global path planning and local path planning, respectively.

To demonstrate the effectiveness and feasibility of our proposal, a series of experiments are conducted, which are divided into comparative experiments and a practical application experiment. Specifically, comparative experiments are performed in a 
 indoor environment, with the purpose of testing the performance of the method. In this paper, the performance is evaluated on the basis of the following three criteria.

(1)
Accuracy. The accuracy of defined VAs is an important aspect for user acceptance. A high accuracy indicates that VAs are integrated where the user wants to be on OGM;

(2)
Efficiency. On the precondition of ensuring the accuracy, the efficient definition of VAs is what user expects. For this purpose, based on criterion (1), the time taken by the user to create VAs as desired serves as the measure of efficiency in this paper;

(3)
Correctness. This criterion is to examine whether the aware navigation of mobile robot is implemented according to user preferences. In other words, the user-defined VAs with restrictions should be correctly integrated into the robot navigation map.

Most notably, the comparative experiments of this paper are carried out under the same experimental conditions to obtain a fair performance comparison. Furthermore, in order to further verify the feasibility of preference-aware navigation of the robot, the practical application experiment is presented, and is implemented in a home-like indoor environment with an area of 
.

5.2. Comparison of VA creation
Experiments of VA Creation are performed by a non-expert participant. Before conducting the experiments, the participant is given a practice to become familiar with the interactive interface and defining method. In order to better examine the performance of the proposed method in creating VAs, the experiments with five different methods are designed for fair comparison: (1) tele-operation method (TOM) that is to let the participant use the joystick to control the robot to move along the boundaries of desired area, similar to the method of using additional devices to guide the robot for the definition of desired boundaries in Sprute et al. (2019b), (2) virtual layer method (VLM) that defines the desired area by adding a virtual layer of obstacles on the costmap (Ghorbel, 2020), (3) zones of interest method (ZIM) that is to define the zones of interest based on Rviz with a map editor plugin (Pal, 2020), (4) editing map method (EMM) which is let the participant to draw the expected boundary on the given map (Howard, 2019), and (5) the method proposed in this paper (Our). Among them, VLM and ZIM are not suitable for direct use by a non-expert user, thus the selected participant needs more time to get familiar with these two methods. Note that the practice time of all methods is not used as a factor in evaluating performance of the method for unbiased comparison.

Additionally, to fully test the performance, four different areas (i.e., Area 1: 0.2 
, Area 2: 0.75 
, Area 3: 2.25 
 and Area 4: 19.2 
) that are desired to be incorporated into OGM are considered in VA creation experiments. The resulting maps using different methods are illustrated in Fig. 5, where the desired area is called ground truth (GT), as shown in the first row. In the following, the analysis of performance in terms of accuracy and efficiency is elaborated.

5.2.1. Accuracy
To get a measure of accuracy of the created VAs by different methods, an evaluation metric of intersection-over-union (IoU) is introduced to measure whether the defined VA matches the GT. It is defined as (9)
 
where 
 and 
 denote the number of all cells of OGM contained in VA and GT, respectively.

Fig. 6 depicts the accuracy of creating VAs using different methods. As demonstrated in Fig. 6, the accuracy shows an overall upward tendency with the increase of the area of GT. This indicates the accuracy of the method has a certain correlation with the shape and length of the VAs. There is one exception, however: the accuracy of using ZIM and EMM to define Area 1 is higher than that of Area 2. This is because the areas of Area 1 and Area 2 are small and there is no reference information around them. Therefore, in this case, the definition accuracy cannot be guaranteed. For Area 4, it is a different story. With the help of reference information on OMG, the participant can consciously define VA that matches GT as closely as possible. As a result, VLM, ZIM, EMM and our method achieve an sufficient accuracy (about 90%). In all the scenarios, TOM presents the lowest accuracy. The reasons are mainly in two aspects: (i) for small areas of GT (e.g., Area 1, Area 2 and Area 3), using TOM, the robot needs to move along the outer boundary of GT. This leads to the fact that the defined VAs are always larger than the desired areas (refer to Fig. 5). (ii) While for large area (e.g., Area 4), TOM makes the robot move along the inner boundary of GT to define VAs, resulting in the defined VAs lying in the interior of GT. It is noteworthy that our method is less affected by the shape and size of GTs, and can always obtain the high accuracy. Therefore, the accuracy of the proposed method is impressive.

5.2.2. Efficiency
During the above experiments, the execution times have also been recorded for efficiency comparison. The results are presented in Fig. 7, which shows that the proposed method and ZIM can always create VAs with less time cost in all cases. The main reason is that using these two methods, the participant only needs to specify the vertexes of the area to implement the definition of the VA. In this context, the GT with complex shape will lead to more vertexes to be chosen for VA creation. Fortunately, there has only a subtle influence on efficiency of using our method and ZIM (see the red and green lines in Fig. 7). Although the similar definition is used in VLM, the vertexes are given in the form of coordinates through the terminal. Therefore, compared with our method and ZIM, VLM takes more time, and as the number of vertexes increases, the time spent is correspondingly longer (see the yellow line in Fig. 7). In addition, the efficiency TOM of and EMM tends to be dependent on the area of GT. This is because a large area means a long border. In this case, TOM needs a longer path to be traversed by the robot, and EMM needs to draw a longer boundary. In addition, the efficiency of TOM is also limited by the shape of the area. The reason for this is that the motion performance of mobile robot is prone to be affected by the shape of GT. As an example, in Area 3, the robot needs to make more turns with at low speed, which obviously takes more time (see the blue line in Fig. 7). Therefore, in terms of time, our method is significantly better than TOM, VLM and EMM, and is slightly superior to ZIM.

5.2.3. Statistical result
The results of VA creation experiments are summarized in Table 2, where the average accuracy and taken time using TOM, VLM, ZIM, EMM, and the presented method are presented, respectively. From the average values, the method proposed in this paper has the best performance in terms of accuracy and time (0.83 and 20.32 s), which outperforms TOM (0.58 and 79.89 s), VLM (0.73 and 53.96 s), ZIM (0.76 and 21.65 s) and EMM (0.75 and 56.72 s). Among them, TOM performs worst in both aspects. In comparison with VLM and EMM, ZIM takes less time to define VAs with higher accuracy. And in terms of time cost, the performance of TOM is similar to our method. However, TOM does not allow the non-expert user to define VAs and corresponding attribute restriction. Moreover, it is worth mentioning that, except for our method, the other four methods (i.e., TOM, VLM, ZIM and EMM) cannot specify velocity constraints to change the navigation behavior of the robot. Therefore, the experimental results of VA creation prove the effectiveness of our method, and show that it is be allowed for non-expert users to easily define VAs and specify corresponding constraints.


Table 2. Summary of average accuracy and time using different methods in VA creation experiments.

TOM	VLM	ZIM	EMM	Our
Accuracy	0.58	0.73	0.76	0.75	0.83
Efficiency (Time/s)	79.89	53.96	21.65	56.72	20.32
Velocity restriction	–	–	–	–	✓
Non-expert user	✓	–	–	✓	✓
5.3. Comparison of robot aware navigation
The robot aware navigation experiments are conducted to validate the correctness of the proposed method. The experiments here are carried out based on the generated maps of Section 5.2. More specifically, OGMs with defined Area 2 and Area 4 are used. In Area 2 the attribute restriction is specified as NO-GO by the non-expert participant through the visual interactive interface, which means the robot is expected to avoid Area 2 when navigating. While in Area 4 the attribute restriction is specified as Accessible in the same manner, and the velocity limit is selected as Low-speed, that is, the maximum velocity is set as 0.15 m/s (the default value of maximum velocity is 0.5 m/s). In other words, when the robot moves to Area 4, we expect the maximum speed of the robot to change from 0.5 m/s to 0.15 m/s.

In order to fully verify the correctness, the experiments are implemented in the following four experimental scenarios with different settings (i.e., Scenario 1, and Scenario 2, Scenario 3 and Scenario 4). In all scenarios, the mobile robot is tasked to navigate from starting position ‘S’ to target position ‘T’. Additionally, in each Scenario, the experiments of with and without the proposed method are performed for performance comparison, while the trajectory, linear velocity and angular velocity of the robot are recorded to facilitate comparative analysis.

5.3.1. Scenario 1
In Scenario 1, the defined Area 2 is deployed in an attempt to change the navigation behavior of mobile robot. Fig. 8 presents the results of the experiments, where the red and blue lines represent the results of using and non-using the proposed method, respectively. As shown in Fig. 8(a), without the proposed method, the mobile robot crosses the VA (green dotted box). The reason is that this is the shortest path to the target position ‘T’. As expected, using our method, the robot bypasses the VA to reach target position, even though the VA does not exist in real-world environment. The results demonstrate that the presented method successfully integrates VA into OGM, which is respected by robot during navigation. Thus the created VA is effective in changing the navigation behavior of the mobile robot.


Download : Download high-res image (411KB)
Download : Download full-size image
Fig. 8. Comparison of the results of Scenario 1. (a) Trajectory, (b) Linear and (c) Angular velocities of robot. ‘A’ of (a) corresponds to ‘A’ of (b) and (c). (Please see supplementary video for more details.) (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)

5.3.2. Scenario 2
Unlike Scenario 1, the defined Area 4 is used in Scenario 2. The experimental results are depicted in Fig. 9, where the red and blue lines show the results of using and non-using our method, respectively. In the case of without using our method, the navigation behavior of mobile robot does not change, similar to the navigation behavior of Scenario 1. Using the proposed method, the motion trajectory is analogous to that of without this method, but the velocity changes when the robot enters the VA (green dotted box) that is a non-physical area. As recorded in Fig. 9(b), the maximum speed changes at point ‘A’ from 0.5 m/s to 0.15 m/s. This experimental result is exactly what we expected. Therefore, the presented method allows the user to specify the navigation behavior of the robot within the user-defined VA.


Download : Download high-res image (440KB)
Download : Download full-size image
Fig. 9. Comparison of the results of Scenario 2. (a) Trajectory, (b) Linear and (c) Angular velocities of robot. ‘A’ of (a) corresponds to ‘A’ of (b) and (c). (Please see supplementary video for more details.) (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)

5.3.3. Scenario 3
According to the settings of Scenario 1 and Scenario 2, Scenario 3 is designed through the deployment of Area 2 and Area 4. Experimental results are reported in Fig. 10, where the red and blue lines represent the results of using and non-using the presented method, respectively. As shown in Fig. 10(a), there is significant difference between the two trajectories. Specifically, the utilization of our method allows the mobile robot to plan a path to avoid the virtual Area 2, and navigate to reach the target position. In contrast, the robot goes straight through the virtual Area 2 without using the method proposed in this paper. In addition to this difference, the robot deploying our method also changes the maximum speed from 0.5 m/s to 0.15 m/s when entering the virtual Area 4, see the point ‘A’ in Fig. 10(b). While this behavior of velocity change is not reflected in the non-using the proposed method (refer to the blue lines in Fig. 10). The results are in line with expectations. Consequently, the application of the proposed method enables the mobile robot to perform the navigation behavior as desired.


Download : Download high-res image (459KB)
Download : Download full-size image
Fig. 10. Comparison of the results of Scenario 3. (a) Trajectory, (b) Linear and (c) Angular velocities of robot. ‘A’ and ‘B’ of (a) correspond to ‘A’ and ‘B’ of (b) and (c), respectively. (Please see supplementary video for more details.) (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)

5.3.4. Scenario 4
The setting of Scenario 4 is different from that of Scenario 3 in that the positions of starting point and target point are interchanged. Thus in Scenario 4, it is expected that the mobile robot can not only avoid virtual Area 2, but also change the maximum speed from 0.15 m/s to 0.5 m/s when leaving virtual Area 4. Fig. 11 illustrates the results of the experiments, where the red and blue lines show the results of using and non-using the proposed method, respectively. Without using the presented method, the mobile robot still passes directly through the virtual Area 2, and the velocity is always maintained at about 0.5 to reach the target position. As expected, the use of our presented method allows the robot to successfully avoid virtual Area 2 and change the speed as required (see the red lines in Fig. 11). Hence, the navigation behavior of the mobile robot can be changed according to the specified needs.

In a word, the experimental comparison proves the correctness of our proposed method. And the user preference based robot aware navigation can be realized through the presented method.


Download : Download high-res image (481KB)
Download : Download full-size image
Fig. 11. Comparison of the results of Scenario 4. (a) Trajectory, (b) Linear and (c) Angular velocities of robot. ‘A’ and ‘B’ of (a) correspond to ‘A’ and ‘B’ of (b) and (c), respectively. (Please see supplementary video for more details.) (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)

5.4. Practical application
The practical application experiment is carried out in a home-like indoor environment, with 2 rooms of which one is a living room and one is a bedroom. In the experiment, the following service scenario is investigated: the user is in the bed and needs the robot to deliver an object for him/her. In this context, the robot is tasked with performing a delivery service from the living room to the bedroom. During the mission, the robot is required to avoid the carpet laid in the living room and the glass door of the bedroom, and reduce the speed to keep quiet while navigating the bedroom. Note that the carpet and glass door cannot be observed by the robot with its on-board sensors. To this end, the participant uses the proposed solution to define corresponding VAs with constraints according to demand. Specifically, three VAs are defined [refer to areas ①, ② and ③ in Fig. 12(a)]. Two of the VAs correspond to the carpet area and the glass door, respectively, and their attribute restrictions are specified as NO-GO. The other VA corresponds to the bedroom, the attribute restriction of which is specified as Accessible, and velocity limit is selected as Low-speed (i.e., 0.15 m/s).

The results of the practical application experiment are presented in Fig. 12, which demonstrate the proposed solution successfully integrates VAs on the OGM and changes the navigation behavior of the robot according to the defined constraints. As shown in Fig. 12(a), the mobile robot first bypasses the laid carpet in the living room when performing the delivery service. Although the carpet cannot be measured by the robot’s laser rangefinder, the presented solution can still allow the robot to avoid crossing the carpet. Because this area [see ① in Fig. 12(a)] is defined as a NO-GO area, even if it is non-physical area, it is still obeyed by the robot during navigation. Subsequently, the robot reaches the bedroom doorway safely [see ‘A’ in Fig. 12(a)], and then keeps a certain safe distance from the glass door for navigation. Normally, the robot cannot observe the glass door due to its transparency. Apparently, the robot respects the defined VA ② and corresponding constraint during this navigation. At the same time, when the robot enters the bedroom [see ③ in Fig. 12(a)], the maximum speed of the robot is changed from 0.5 m/s to 0.15 m/s, as shown in Fig. 12(b). This navigation behavior is in line with expectations. Finally, the delivery service is successfully completed. On the whole, the results further verify the feasibility and practicability of the solution proposed in this paper, and show that the solution allows non-expert users to easily control the working areas of the mobile robot and change the navigation behavior of the robot based on their preferences.


Download : Download high-res image (389KB)
Download : Download full-size image
Fig. 12. Results of the practical application experiment. (a) Trajectory, (b) Linear and (c) Angular velocities of robot. ①, ② and ③ represent the VAs of carpet, glass door and bedroom, respectively. ‘A’ of (a) corresponds to ‘A’ of (b) and (c), respectively. (Please see supplementary video for more details.).

6. Conclusion
This paper proposed a non-intrusive solution that allows non-expert users to control the working areas of the mobile robot in a convenient way to achieve the human-aware navigation according to their preferences. To cope with the intrusiveness of deploying additional facilities, a VA scheme was presented to define the desired areas. Moreover, to facilitate the integration of OGM and VAs, a visual interactive interface was developed, which supports non-expert users to flexibly define arbitrary VAs and specify the navigation behavior of a robot in the user-defined VAs. Extensive experiments were implemented to evaluate the performance and validity of the presented solution. Experimental results showed that our solution outperforms four state-of-the-art methods. Furthermore, a practical application further demonstrated the validity and practicability of the solution of this paper. Future work will investigate the transplantation of the interactive interface to smartphones and tablets, in order to improve the applicability of the scheme. We also plan to extend our approach to three-dimensional (3-D) map.

