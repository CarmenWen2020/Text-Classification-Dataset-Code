Software defined networking (SDN) is regarded as a new paradigm of flow management in data center networks (DCNs). In SDN, the existing methods of managing flows are centralized by controller in the control plane and highly rely on the switches in the data plane to get forwarding rules from controller. Therefore, it is of great significance to consider the mapping relationship between the controller and the switch based on flow types. To address this issue, this paper first designs a flow-based two-tier centralized management framework for software defined DCNs and formulates a dynamic mapping problem using integer programming called cost-effective adaptive controller provisioning (Cost_EACP) problem, which is proved to be NP-complete. Then, in order to solve this problem, we transform the integer programming problem into fraction programming problem. Subsequently, we design a rounding-based Cost_EACP algorithm (RC_EACP) and propose a switch controller mapping algorithm (SCM_EACP) based on RC_EACP algorithm to address the mapping problem. Furthermore, we analyze the approximation performance and time complexity of proposed algorithm. Finally, experimental results demonstrate that the proposed algorithm is more effective than existing algorithms in terms of reducing the control channel bandwidth cost and delay cost for setting up flow rules, and the gap between the proposed algorithm and the optimal one is less than 24%.

Previous
Next 
Keywords
Adaptive controller provisioning

Software defined networking (SDN)

Data center networks (DCNs)

Switch controller mapping

Fraction programming

1. Introduction
Software-Defined Networking (SDN) offers a new network paradigm to deploy, operate and manage communication networks by separating the network's control logic from the underlying switches, which has attracted much attention from both academia and industry (Kreutz et al., 2015; Mendiola et al., 2017). It is able to promote logical centralization of network control, collect the global information of the whole network, dispatch the whole network traffic service, realize the global management and introduce the ability to program the network (Xia et al., 2015), which contributes to run centralized management in cloud computing and 5G network. Initially, SDN is designed as a centralized architecture with a single controller responsible for managing entire networks (Yonghong et al., 2014; Liu et al., 2020; Thakur and Khatua, 2020). But it is almost impossible with the expansion of the network scale (Yao et al., 2014). Therefore, multiple controllers are deployed in the network and there are some existing approaches, which can be classified into three categories: the designing architecture of control plane (DACP), the controller placement problem (CPP) and the dynamically mapping between controller and switch (DMCS).

The most existing works on the DACP (Schriegel et al., 2018; Akanbi et al., 2019; Amiri et al., 2019; Togou et al., 2019; Fu et al., 2015) mainly discussed the logical connection between multiple controllers and used the multiple controllers to build a distributed, hierarchical or hybrid control plane. But when the network size and the number of flows increase, more information will be exchanged between the controllers for setting up flow rules, leading to high overhead of channel bandwidth and control delay. The most CPP works (Hu et al., 2018a; Han et al., 2016; Guo et al., 2019a; Obadia et al., 2015; Wang et al., 2019a; Suh et al., 2018) focused on seeking the optimal location to place controllers within a given network topology and a certain network goal, such as network reliability, and minimize flow setup delay or control overhead. But most of them ignores the unbalanced flows in the network(Yao et al., 2015), which will inevitably lead to the unbalanced load of some controllers. In (Guo et al., 2020a), the authors studied network load balancing by a dynamic flow scheduling scheme with the aim to achieve network power efficiency and QoS improvement. The existing DMCS works (Guo et al., 2019b, 2020b, 2020c; Wang et al., 2016; Bari et al., 2013; Gao et al., 2017) usually dynamically establish mapping relationship between controller and switches according to the load of the controller, traffic dynamics and network status in the network. These strategies can improve the load imbalance of an overloaded single controller, and reduce long response time and high maintenance cost. However, in cloud computing networks and 5G networks, most of traffic flows have the characteristics of short transmission time(80% last less than 10s), small volumes (small 10 KB in size), and delay sensitivity (Benson et al., 2010; Tang et al., 2019). However, the 80% traffic load of the network is the flows with long transmission time, large volumes and insensitive delay. Generally speaking, these delay-sensitive flows which do not need to update flow exchange rules are different from the flows with big size (Achleitner et al., 2018). For larger streams, the probability of path updates will inevitably increase due to the longer transmission time. Furthermore, this will result in that different class of flows have different transmission management costs. But the previous studies have not considered these traffic characteristics, which focused on achieving stability and resiliency with low communication overhead, but our work aims to reduce the bandwidth overhead and delay cost for setting up forwarding rules by considering traffic characteristics.

Actually, how to place controller and find the mapping between controller and switches still remains a challenging issue because (i) the utilization efficiency of the controller is becoming more and more challenging when the network scale and the number of streams continue to increase; (ii) the increasing in the number of controllers in the network will rapidly enhance the coordination information between the controllers, which makes it difficult to reduce the total bandwidth of the entire network; (iii) the size of the network and the mapping algorithm between controller and switches will inevitably have an impact on reducing flow rule set up time; iv) currently, more and more traffic services such as cloud service and 5G scenarios belong to multi-class traffic with different bandwidth and delay requirements (Wang et al., 2019b). They often groom into flow groups with different priorities and types of performance requirements; v) The mapping algorithm between the switch and the controller does not consider the traffic characteristics of different flows, which cannot guarantee to effectively balance the load of the controller in the entire network while reducing bandwidth overhead and delay cost.

Based on above considerations, this paper first designs a novel two-tier centralized control framework to manage mapping relationship for DCNs called multi-controller provision (MCP) framework. All controllers are divided into two categories: high-level controller and low-level controller, where high-level controller is responsible for managing mice flow transmission and adaptively selecting low-level controller for an elephant flow to manage transmission, while low-level controller is only responsible for managing elephant flow transmission in the network. When considering the mapping relationship between the controller and the switches, different from the previous studies which only focused on the position of the controller and the switch, we also take into account the flow types. Moreover, our MCP framework focus on reducing the total cost of network bandwidth overhead and delay cost, comparing with the typical algorithms, Dynamic Controller Provisioning with Simulated Annealing (DCP-SA) (Bari et al., 2013) and Dynamic Switch Assignment-Stable Matching algorithm (DSA-SM) (Wang et al., 2016). The main contribution of this paper is summarized as follows:

●
First, we design a novel two-tier centralized control framework to flexibly manage flow transmission, which consists of high-level controller and low-level controller, and main module functions of the access controller are introduced.

●
Second, to fully consider the transmission characteristics of each flow, we formulate a cost-effective adaptive controller provisioning (Cost_EACP) problem and prove it to be NP-complete.

●
Third, we design a rounding based Cost_EACP algorithm (RC_EACP) to solve the Cost_EACP optimization problem and propose a switch controller mapping algorithm based on RC_EACP (SCM_EACP) to address the mapping problem. Furthermore, we analyze the approximation factor of the proposed algorithm.

●
Finally, the simulation results show that the proposed algorithm can reduce the total cost by over 36% and 50% on average compared to DSA-SM and DCP-SA respectively. Moreover, the gap between proposed algorithm and the optimal one is less than 24%.

The rest of this paper is organized as follows. Section 2 surveys some related works. In section 3, we design a novel system framework and describe the operating mechanism. Then, we formulate the optimization model in section 4. Section 5 proposes a rounding-based algorithm, and section 6 analyzes experimental results. Section 7 concludes this paper.

2. Related work
In SDN, the control mode and the location of the controller have an important effect on the performance of managing flows. So in this section, we will survey some related works on the designing architectures of control plane (DACP), controller placement problem (CPP) and the dynamically mapping between controller and switch (DMCS).

2.1. Designing architecture of control plane
In terms of the architectures of control plane in large-scale networks, there are three control modes, i.e., distributed control plane, centralized hierarchical control plane, and hybrid hierarchical control plane. For distributed control plane architecture, the authors in (Egilmez and Tekalp, 2014; Schriegel et al., 2018; Akanbi et al., 2019) proposed to improve the scalability of control plane and reduce the delay caused by geographical distance. However, when a SDN network scales to large size, it lacks of the ability to solve the super-linear computational complexity growth of the control plane. Alternatively, the centralized hierarchical control plane of SDN (Amiri et al., 2019; Togou et al., 2019; Choi and Li, 2017) was proposed, which can handle the large-scale network with the global information of entire network. However, it has to obtain a forwarding rule command and network status information from top controller when flows are transmitted over whole network. It leads to high latency and path stretch problem once the reactive approach is used to assign forwarding rules to the corresponding switches. Based on above considerations, the authors in (Fu et al., 2015) and (Abdeldjalil Chekired et al., 2018) proposed the hybrid hierarchical control plane, which is able to effectively reduce the computational complexity of SDN control plane by several orders of magnitude. However, it is of little help for managing flows in terms of reducing transmission delay.

2.2. Controller placement problem
In terms of controller placement problem, there are a lot of previous works that focused on seeking the optimal location to place controllers within a given network topology and a certain network goal to achieve network reliability (Hu et al., 2018a), minimize flow setup delay (Han et al., 2016) and minimize control overhead (Obadia et al., 2015; Wang et al., 2019a; Suh et al., 2018). Hu et al. in (Hu et al., 2018a) designed Multiple Domain Partition (MDP) algorithm to allocate switches for controllers according to controller load balance rate, which can realize the reasonable domain planning. However, they did not consider specific situations, such as dynamic load balancing and switch migration. Han et al. (2016) designed a minimum-control-latency optimized algorithm based on greedy controlling pattern and obtained accurate control latency. But they did not analyze the impact of bandwidth cost in network and did not have universality for many network topologies. In (Obadia et al., 2015; Wang et al., 2019a; Suh et al., 2018), the authors achieved to find the mapping between controllers and switches under the bandwidth constraint with the optimization goal to minimize network overhead. But they did not consider network latency, which may lead to high computation time in large scale topologies.

2.3. Dynamically mapping between controller and switch
To address dynamic traffics problem, there are a lot of academics about multi-controller collaboration platform (Guo et al., 2019b, 2020b; Hu et al., 2018b, 2019) and dynamic controller assignment problem (Wang et al., 2016; Huang et al., 2019, 2020; Guo et al., 2020c; Gao et al., 2017). Guo et al., 2019b, 2020b used the control structure of multiple controllers to achieve control stability and resiliency with low communication overhead. In (Hu et al., 2018b, 2019), the authors formulated the slave controller assignment problem as a multi-objective mixed optimization problem. Most of them focused on the mapping between controller and switch once the controller failure occurs. In order to minimize the average response time of the control plane, Wang et al. (2016) formulated the dynamic controller provisioning problem to ensure minimal flow setup delay and communication overhead by dynamically adjusting the number of active controllers. In (Huang et al., 2019, 2020), the authors studied the joint problem of dynamic switch-controller association to minimize the total system cost and the average request response time with queueing stability guarantee. In order to balance the traffic load, the authors (Guo et al., 2020c) proposed a dynamic flow scheduling scheme to save the controller resource significantly, and the authors (Gao et al., 2017) made an exploration on the usage of devolved controllers, which can reduce the load of the single controller. Different from these previous works on the mapping between switch and controller which focused on achieving stability and resiliency with low communication overhead, our work aims to reduce the bandwidth overhead and delay cost for setting up forwarding rules by considering traffic characteristics.

The imbalance of traffic distribution brings unbalanced load among controllers, leading to the difficulty of controller reassignment. Although all of the dynamic algorithms are better than DACP and CPP in terms of bandwidth overhead, network delay and load balancing, but they did not fully consider the characteristics of flows that different flow types require different monitoring mechanisms and rule establishment modes.

3. Multi-controller provision framework
In this section, we first describe a new multi-controller provision (MCP) framework composed of two-layer control platform to manage mapping relationship for DCNs. Compared with traditional control platform, this framework is more flexible for dynamic traffic management especially for DCNs. Then, we will detail the functions of each component of proposed framework.

3.1. Description of framework
The network system is divided into two parts: one is the control plane composed of many controllers as indicated in part I of Fig. 1, and the other is data plane composed of Openflow switches as indicated in the part II of Fig. 1. We assume that each network infrastructure device can be connected to any controller. In order to centrally collect network information and achieve distributed traffic service management, we define two types of controller. One is the low-level controller called service controller which is used to implement elephant flow transfer management. And the other is special high-level controller called access controllers. It is used to classify the flows, calculate routing rules for mice flow, and choose an optimal service controller to manage the transmission of elephant flow.

Fig. 1
Download : Download high-res image (520KB)
Download : Download full-size image
Fig. 1. The multi-controller provision (MCP) framework.

The main function modules of the access controller shown in the Part I of Fig. 1 are described as follows.

●
Network Status Monitoring (NSM) Module: This module is responsible to periodically collect the network status in data plane including network topology, link status, flow requests and status information of service controllers.

●
Predetermined Transmission Path Decision (PTP) Module: This module calculates the Predetermined routing path for the requests determined by access controller based on default routing algorithm such as Hybrid Routing by Joint optimization of Per-flow routing and Tag-based routing problem (HR-JPT) (Zhao et al., 2018).

●
Flow Type Analysis (FTA) Module: Since different types of flow have different transmission requirements, such as transmission duration and status statistics (Benson et al., 2010), this module is used to classify flows into many categories by the existing flow classification technology (Tang et al., 2019).

●
Controller Provisioning (CP) Module: This module assigns service controller for elephant flows. Based on the results of PTP Module, it will calculate the total cost of each predetermined path managed by each service controller for the requested elephant flow, and then use the proposed SCM_EACP algorithm described in Section 5 to select an optimal service controller to manage the flow transmission.

●
Rule Calculation (RC) Module: This module will calculate the forwarding rules for mice flows and distribute the forwarding rules to the corresponding switches on the PTP Module.

3.2. Operations in framework
Fig. 1 gives the operations of the framework and flow requests between controllers in control plane and switches in data plane. When a new flow arrives, the first switch will send a request message to one of access controllers if there is no matching forwarding rules in the switch. According to the flow information composed of 4-tuple (source_id, destination_id, flow_class, flow_size) and the information of network status, the access controller will first calculate the preliminary path for this flow. If the flow is an elephant flow, the access controller will select the appropriate service controller to manage the subsequent transmission. Then, the selected service controller will receive the path information of this flow from access controller. Next, it will calculate the forwarding rules of all the switches on the path and send the forwarding rule to the corresponding switches. During the entire transmission process of elephant flow, the service controller is responsible for generating forwarding rules, updating links, collecting network information and monitoring the transmission status of the flow. It is noted that the management of this flow will be determined by selected service controller until the transmission of this flow is finished.

According to MCP framework, it can be observed that collecting topology information of entire network and monitoring the mapping state between controller and switches are executed separately for elephant flow. In other words, the access controller is responsible for calculating the routing path for an elephant flow while service controller is responsible for generating forwarding rules and assigning them to all corresponding switches on the routing path. The focus of this paper is to determine how to choose a service controller with minimum cost, which will be discussed in the following sections.

4. Problem formulation
This section formulates the optimization problem based on MCP framework of software defined DCNs by giving several cost models. The goal is to minimize the overhead of bandwidth management caused by flow transmission and the cost of rule set-up delay under the premise of satisfying the data flow set-up latency requirement. The main symbols used in this paper are listed in Table 1.


Table 1. List of important notations.

Notation	Definition
G(V, E)	The modeled graph of software defined DCNs.
CS	Set of service controllers.
CA	Set of access controllers.
Pf	The predetermined path set of a flow f entering the network from an ingress switch to an egress switch.
αf	Bandwidth demand weight for flow f.
f(vr, cj)	A function indicating the routing cost of signals from switch vr to controller cj.
Sj,p	Set of switches on path p that are linked to controller cj.
A function denoting the bandwidth cost in managing flow f.
A function denoting the delay cost in managing flow f.
T(vi, cj)	Transmission delay function between the switch vi and the controller cj.
Number of switches on the path p for transmitting flow f.
Xf(vi, cj, p)	A binary variable denoting whether a switch vi on path p is linked with a service controller cj for flow f.
4.1. Data center network model
The proposed MCP framework is modeled by graph G(V, E), where V represents the set of nodes of switches, controllers and servers, and E represents the set of network links between nodes.

In control plane, it contains two parts including the service controllers and the access controllers. Let M + L be the total number of controllers in the network, where M is the number of service controllers and L is the number of access controllers. The set of service controllers and the set of access controllers are denoted as CS = {c1, c2, …, cM} and CA = {cM+1, cM+2, …, cM+L}, respectively. Therefore, the set of all the controllers is represented as C = CS ∪ CA. The processing capacity of service controllers is denoted as QS = {q1, q2, …, qM}, it represents that the controller can handle the number of requests in one time unit. And the decay factor indicates the ratio of the spare capacity of a controller to the total capacity (Wang et al., 2016). We use the set BS = {β1, β2, …, βM} to represent the spare capacity of each service controller. Similarly, the processing capacity and decay factor of access controllers is respectively represented as QA = {qM+1, qM+2, …, qM+L} and BA = {βM+1, βM+2, …, βM+L}. We assume that M service controllers and L access controllers are sufficient to handle the maximal request rate for a DCN topology.

In data plane, a variety of traditional DCNs such as Fat-tree and VL2 are adopted. There are N switches denoted as S = {v1, v2, …, vN}, where vi is the i−th switch. Similar to (Yao et al., 2015), we define a function ω(vi) be the weight of i−th switch, which is related to node degree 
, i.e, the number of adjacent nodes that switch vi connected to, and the further description is described in Equ.(2). Generally, in cloud computing networks and 5G networks (Wang et al., 2019b), there are multi-class traffics. Most of these flows have the characteristics of short transmission time, small volumes, and are delay-sensitivity. However, the 80% traffic loads of the network (Tang et al., 2019) are the flows with long transmission time, large volume and insensitive delay. To simplify the analysis, we define the flow with long transmission time (i.e, greater than 10s) and large volume (i.e., large 10 KB in size) as an elephant flow (Tang et al., 2019), and we define the flow with short transmission time and small volume as a mice flow. Then, let F denote the set of flows in the network and Fe be the set of elephant flows. The set of mice flows is F − Fe.

When a flow f ∈ F from an ingress switch to an egress is arriving in the network, the Packet_In control messages will be sent to a controller via the ingress switch, and one of the access controllers will handle it and calculate the routing path. In this process, the control signal messages will be delivered and transmitted, which will consume small network bandwidth and generate large network transmission delay. Since each switch is able to communicate with the controller through multi-hop forwarding from other switches, the distance between i−th switch and j−th controller denoted as dij can be obtained through the shortest routing algorithm such as Dijkstra algorithm (Dijkstra, 1959). In addition, multiple path transmission is allowed in this paper. And we assume that a flow f is transmitted through multiple paths Pf. Let Xf(vi, cj, p) be a binary variable to denote whether a switch vi on path p ∈ Pf is linked with a service controller cj for a flow f being transmitted. If Xf(vi, cj, p) = 1, it means that the switch vi on path p ∈ Pf is connected with the controller cj, otherwise Xf(vi, cj, p) = 0.

As indicated in (Yao et al., 2014) and (Curtis et al., 2011), we can conclude that the main functions of control plane are mainly composed of following steps, (i) collecting network status information and exchanging the information with other controllers to maintain global information of entire network, (ii) processing the newly Packet_In events and calculating the transmission path for each flow, (iii) collecting accurate global view of network traffic which is useful for a variety of applications, (iv) calculating routing path for the newly arriving flow and assigning forwarding rules to the corresponding switches. In MCP framework, the step (i) and the step (ii) are executed in the access controller while the load of control plane is only related with the step (iii) and the step (iv). Our focus is step (iii) and (iv). Next, we will discuss the control overhead of transmitting newly arriving flows in the following sections.

4.2. Flow setup control overhead model
The control bandwidth overhead required to set up the rule for a flow is divided into the request rule update control overhead and the rule installation control overhead, where the request rule update control overhead is the control overhead caused by the Packet_In control messages of ingress switch sending to the controller. When the flow is being transmitted, there may be rule updates. So, we define the flow rule updated repetition factor (the repetition factor) rf, which indicates the flow rule updated time evaluated by the FTA Module. Normally, the transmission time of elephant flow is 10 − 100s or even longer (Tang et al., 2019). For the elephant flow, we assume that different flows have different repetition factor rf. Meanwhile, for the mice flow, we assume the repetition factor rf = 1.

1) The request rule update control overhead: It is defined as follows:(1) 
 
 Similar to (Yao et al., 2015), if the controller cj is selected for managing the transmission of flow f and rule update is assumed to occur on switch vi, we define a function f(vi, cj) to represent the channel bandwidth cost of transmission routing rules between switch vi and controller cj, where vr is the ingress switch of flow f,

To reflect the importance of each switch in the process of setting up forwarding rules, the value ω(vi) is the normalized node degree to determine the weight of the switch vi,(2)
 
where dmax is the maximum degree of all the switches in the network, i.e., 
 
.

2) The rule installation control overhead: The overhead of all control channel is generated when controllers assign the forwarding rules to the corresponding switches for the flow, which is denoted as follows:(3) 
 
 where Sj,p represents the set of switches on a predetermined path p ∈ Pf that is able to connect with controller cj for transmitting flow f, and vi is one of the switches that needs to update the data rule vi ∈ Sj,p.

4.3. Flow statistics overhead model
The statistic information collected by SDN controller usually includes the following three aspects: i) network state information such as network topology, link state, and controller state; ii) basic flow information such as flow type, flow size and duration; iii) the transmission status and statistics of the flow by regularly monitoring (Bari et al., 2013; Xu et al., 2017). In our model, the network state information and the basic information of a flow collected by the access controller are not our attention, instead, we mainly analyze the overhead of collecting statistical information when monitoring and collecting dynamic information of flows.

In order to detect whether a flow exists in the network, the controller needs to frequently collect flow statistics information from the switches. In this case, the control channel bandwidth and the collection delay should be taken into account. It is noted that both of them are a linear relationship with the number of flow statistics (Curtis et al., 2011). For a flow, we give the statistical overhead function as follows:(4)
 
 
where as represents the basic bandwidth overhead for collecting per-flow dynamic information at one time, Tf represents the traffic duration time of the flow f, and Ts is the interval time between two consecutive statistics. In Equ. (4), 
 indicates the number of switches on the path p for transmitting flow f, where 
.

In addition, if a flow f in Equ. (4) is an elephant flow, i.e., f ∈ Fe, then the selected controller cj belongs to one of service controllers, i.e., cj ∈ CS; otherwise, the selected controller cj belongs to the one of access controllers, i.e, cj ∈ CA and flow f is a mice flow, i.e., f ∈ F − Fe.

4.4. Flow setup response time model
Compared with the traditional network, the network delay in SDN is not only caused by transmitting flows in the data plane, but also by communicating between the controller and the switches called control signal delay. Strictly speaking, the control signal delay consists of three components: i) the propagation delay of the control signal over the control channel link; ii) the queuing delay caused by the control signal at each switch on the control channel link; and iii) rule processing delay for calculating forwarding rules for each switch by controller. Since the current data center topology such as Fat-tree has a high bandwidth (e.g., 1000 MB/S) and the propagation delay (in us (Wang et al., 2016)) is negligible, we mainly consider and analyze the queuing delay and the processing delay in detail.

1) The control signal queuing delay: This queuing delay caused by control signal on the control channel link. The queuing delay includes the rule request delay and the rule installation delay. Specifically, the rule request delay is the transmission time between the switches and the controller while the rule installation delay is the time from sending rules by the controller to receiving rules by all corresponding switches. We assume that the controller sends forwarding rules to all switches at the same time. Although the controller needs to install forwarding rules to all switches on the path, it is not hard to get that the rule installation delay is the maximum transmission delay between controller and switches.

We use l(vi, cj) to represent control channel link to connect the switch vi and the controller cj. Let vm be the switch on the link l(vi, cj), and l be the number of hops on the control channel link between the switch vi and the controller cj. Then we define a binary variable Z(vm) to indicate whether the node vm is on the controller channel link l(vi, cj), which is expressed by,(5)
 

Then, let τ(vi, cj) be the function of the transmission delay between the switch vi and controller cj, which is defined as follows:(6)
where τ(vm) represents the delay time of the control signal at node vm. Similar to Equ. (4), the selected controller cj will belong to one of service controller cj ∈ CS if the flow f is an elephant flow, i.e., f ∈ Fe; otherwise, cj ∈ CA and f ∈ F − Fe. Therefore, the queuing delay on the control channel link can be defined as follows:(7) 
 
 where the first part is the rule request delay and the second part is the rule installation delay.

2) The rule processing delay in the controller: The CPU processing time of the controller is usually in several milliseconds (ms), which is related to the controller processing capacity and the load of the processing object. We assume that each flow processed is modeled as a M/M/1 server queuing model (Stern, 1979). Let θf(cj, p) be the processing capacity of controller cj. Since the elephant flow and the mice flow are respectively managed by the service controller and the access controller, the load function θf(cj, p) is defined as follows:(8)
 where λf(vi) is the request rate of flow f served in switch vi. In Equ. (8), the first expression is the load of service controller where an elephant flow f must be handled while the second expression is the load of access controller for mice flow.

Then, the total load of controller cj can be obtained by,(9)

Therefore, the rule processing delay in the controller can be defined as follows:(10)
 
where |V| is the number of switches in the network.

4.5. Problem formulation
It is critical to minimize the overhead of flow set-up rule and monitor flow status, as well as reduce the flow rule setup delay. Therefore, in this paper, we design a flow-based management scheduling strategy, i.e., how to choose the optimal service controller to manage each flow so that the flow management cost can meet a given bound.

We use 
 to represent the cost of bandwidth overhead in handling flow f.(11)

After substituting Equs. (1), (3), (4) into Equ. (11), (12) can be rewritten as follow:(12) 
 
 
 

Then, let 
 be the delay cost of flow f.(13) 
 

If the flow is transmitted on the path p and managed by controller cj, the total cost Cf(cj, p) is expressed as,(14) 
 
 
 where αf is the bandwidth demand weight for flow f, BM is the maximum bandwidth overhead for flow f and DM is the maximum control signal delay that meets the transmission quality requirements for flow f.

Since the number of repetitions for setting up the forwarding rules is quite different for various flows, we define another binary variable Xf(cj, p). If Xf(cj, p) = 1 indicates that the flow f is transmitted on path p and managed by controller cj; otherwise, Xf(cj, p) = 0. The aim of cost-effective adaptive controller provisioning (Cost_EACP) problem is to achieve the cost balancing among controllers (or to minimize the maximum cost on all the controllers), i.e., minimize γ. Accordingly, the problem is formulated as follows:(15a)(15b)
(15c)
(15d)
(15e)

In the OPT-1, the inequality (15b) denotes that each switch on the path p must be linked with controller cj for transmitting flow f, the inequality (15c) means that the management cost should not exceed γ if controller cj manages the transmission of a flow f, the inequality (15d) ensures that controller cj is not overloaded, and the inequality (15e) shows Xf(cj, p) is a binary variable.

For the Cost_EACP problem, we first need to determine the controller that manages the flow and the path of the flow. The next step is to complete the mapping between the controller and all switches on the predetermined path. Then, the problem becomes to determine the value of binary variable Xf(cj, p), 1 or 0, where cj ∈ M or cj ∈ N, p ∈ Pf. When the number of flows and network nodes increases, the number of controllers and the predetermined paths also increases. This leads to a rapid growth in the number of binary variables Xf(cj, p). In this case, it is difficult to use polynomial time to find the optimal solution among Xf(cj, p). Therefore, in section 5, we use relaxation and rounding methods to obtain approximate solution.

4.6. NP-complete proof
In this subsection, we prove that the Cost_EACP problem is NP-complete. First, we prove that the Cost_EACP problem is an NP problem, and then we prove NP completeness of Cost_EACP by a reduction from unrelated processor scheduling (UPS) problem (Lenstra et al., 1987).

Theorem 1

The Cost_EACP problem is NP problem.

Proof

Given a solution of the Cost_EACP problem (a controller establishes a mapping with the switches for transmitting a certain flow), we can verify whether the bandwidth and delay overhead satisfies the constraints of the Cost_EACP problem in polynomial time. Now we construct an instance of Cost_EACP problem. Assume that a flow f passes through a set of switches on a path p, denoted by Sp = {v1, v2, …, vp}. It means that the selected controller cj will establish and update the rules for the flow f, and monitor the information of the flow f on these switches Sp. It is noted that the management of a flow f will only be scheduled by one of the controllers C. Thus, the management cost of the flow f on all of these switches SP is Cf(cj, p), if cj ∈ CS, f ∈ Fe or cj ∈ CA, f ∈ F − Fe; otherwise, its cost is ∞. As a result, for a flow f on a path p, there is a controller cj ∈ C which takes a minimum control management cost to be selected to manage the transmission of flow f. Therefore, the Cost_EACP problem belongs to NP problem.

Theorem 2

The Cost_EACP problem is NP-hard.

Proof

From paper (Lenstra et al., 1987), an instance of UPS problem is defined as: Suppose that there are M parallel processors and N independent tasks. Each task is assigned to one of the processors to complete. The time required for the processor j to process the task i is Ti,j. Then the objective of UPS problem is to find a schedule that minimizes the make span.

We regard the management of a flow on a path as a task and a controller as a processor. Then Cost_EACP problem can be considered as the UPS problem. Specifically, the management cost Cf(cj, p) of the flow f is regarded as the time required Ti,j in the UPS problem. If the path p used for transmitting the flow f is decided, then the set of switches Sp on the path p is also determined and the management cost Cf(cj, p) of the flow f on the controller cj can be calculated in the polynomial time. Obviously, the reduction is done within polynomial time, thus the Cost_EACP problem is reduced to the unrelated processor scheduling (UPS) problem. This completes the Proof.

According to the above two theorems, the Cost_EACP problem belongs to NP problem and is NP-hard. Therefore, the Cost_EACP problem is NP-complete. In general, solving NP-complete problems is time-consuming, especially in large-scale data centers where large number of flows are transmitting. In addition, latency is one of the most important performance metrics in the data center. Thus to solve Cost_EACP problem with low time complexity, we propose a flow management scheduling rounding-based Cost_EACP algorithm (RC_EACP).

5. Algorithm design
Due to the hardness of Cost_EACP, we propose a flow management scheduling rounding-based Cost_EACP algorithm (RC_EACP) to solve Cost_EACP problem in this section. Then, a switch controller mapping algorithm based on RC_EACP (SCM_EACP) is proposed to find the mapping between switches and controllers. Finally, the approximation performance and time complexity are respectively analyzed.

5.1. RC_EACP
Since the Cost_EACP problem is NP-complete, it is generally difficult to find the optimal solution in polynomial time. Instead, it is an integer programming problem, so we utilize relaxation to transfer integer programming into a fraction programming by replacing Equ. (15e) with Xf(cj, p) ≥ 0. Then, OPT-1 can be transformed as follows:(16a)(16b)
(16c)
(16d)
(16e)

The essence of managing flow transmission in SDN is to establish a mapping relationship between the controller cj and all the switches vi that pass through paths in Pf. Based on the existing routing algorithm (Zhao et al., 2018), the first step is to find out the path p that the flow passes through and choose a controller to manage it, i.e., Xf(cj, p) = 1. The second step is to establish the mapping between all the switches on the path p and the controller cj, i.e., Xf(vi, cj, p) = 1. We use 
 to denote the fractional solution. For simplicity, we use the set X and 
 to represent all the results of Xf(cj, p) and 
, respectively. Let the subset 
 represent the results of the same path p, the set Sp represent the switches on the path p. We assume that the set of all the switches the flow f passes by is denoted by Sf, then we have Sj,p ⊆ Sf and 
. Besides, the set of the switches unmapped with controller is denoted 
, which is initialized as all switches in Sf.

RC_EACP is described and summarized in Algorithm 1. We first arbitrarily choose a switch vi from set 
. We suppose that switch vi is on the path p and handled by the controller cj, so vi ∈ Sj,p and the variable 
 must have a value. Then, if the switch vi is on the different path and handled by different controllers, then 
 has different values and it belongs to the set Xf. Next, a 
 with maximum value among all these sets containing switch vi is selected, and set 
. Finally, the controller cj will map all of the corresponding switches belonging to Sj,p, and then 
 is updated by removing Sj,p from 
. The algorithm will terminate until the mapping between switches and controllers is achieved.


Algorithm 1. RC_EACP: Rounding-based Cost_EACP

Input: Graph G = (V, E), response time bound δ, the controller processing capacities QS and QA, and decay factor BA and BS; Sf, Pf, Sp;
Output: {
};
1: Step 1: Solving the Relaxed RC_EACP Problem;
2: Calculate bandwidth cost f(vi, cj) and delay cost τ(vi, cj) between each controller and each switch, based on network topology information;
3: Calculate the management cost Cf(cj, p) for each controller on every path p for flow f;
4: Solve the relaxed problem OPT-2;
5: Obtain the fraction solution X;
6: Step 2: Rounding to 0-1 Solution;
7: 
8: while 
 do
9: Arbitrarily choose a switch vi from set 
;
10: Choose a 
 with the maximum value among all these sets containing switch vi, and let 
, where j ∈ M.
11: Obtain the set of switches Sj,p = Sp, which is controlled by controller j on the path p;
12: 
;
13: end while
5.2. SCM_EACP
Based on above analysis, we can find that a controller to manage all the switches on the paths can be fully achieved, but those switches may not be only controlled by one controller. Therefore, a controller cj should be selected to map with some switches on one path p or multiple path Pf. The switch-controller mapping based on RC_EACP algorithm called SCM_EACP is designed and described in Algorithm 2.

According to the results of rounding, we first sort each management cost Cf(cj, p) when 
 for the flow f. Then, we choose the controller cj to manage the flow f, which contains a minimum value of Cf(cj, p) among all of 
. Next, we map the controller cj with the switches on the path p or other paths in Pf, i.e., Xf(vi, cj, p) = 1, vi ∈ Sp or vi ∈ Sf.


Algorithm 2. SCM_EACP: Switch Controller Mapping Based on RC_EACP

Input: Graph G = (V, E), Sf, Pf, Sp, and Cf(cj, p), {
};
Output: {Xf(vi, cj, p)};
1: Calculate The combination set of the management cost 
;
2: Find the mapping between some switches with the controller, and choose a minimum Xp,j from the set {
}, where 
 
;
3: cj = j;
4: 
;
5: p = 1;
6: while p ≤ P do
7: Sp = Sj,p;
8: 
;
9: Arbitrarily choose a switch vi from set Sp;
10: while Sp≠Φ and 
 do
11: Xf(vi, cj, p) = 1;
12: Sp = Sp − vi;
13: 
;
14: end while
15: p = p + 1;
16: end while
5.3. Approximation performance analysis
Since the switch vi is arbitrarily selected and the value of 
 is obtained by rounded 
, there exists a gap between the optimal solution and the rounded one. The approximation performance is analyzed and given in the following:

Similar to (Yang et al., 2018), we first give two famous lemmas for probability analysis.

Lemma 1

(Chernoff Bound): Given n independent variables:z1, z2, …, zn, where ∀zi ∈ [0, 1]. Let 
. Then, 
 
 where ε is an arbitrary positive value.

Lemma 2

(Union Bound): Given a countable set of n events: A1, A2, …, An, each event Ai happens with possibility Pr(Ai). Then, 

We then give the approximation performance for the RC_EACP algorithm.

Theorem 3

The approximation ratio provided by the RC_EACP algorithm is bounded by 
 
.

Proof

We use a variable Yp to denote whether a path p is managed by a controller or not. According to Line 10 of the RC_EACP algorithm, we set 
 to 1 with probability 
. Otherwise, 
 is set to 0. Thus, the expectation 
 is 
.

The expected number of switches is:(17) 
 where ρ ≥ 2 denotes predetermined paths for a flow f. This inequality shows that the number of selected paths cannot be more preselected paths. Here Yp is a binary variable, so we can directly apply Lemma 1. Assume that ε is an arbitrary positive value. It has:(18) 
 
 

Now, we assume that(19) 
 
 
 where Φ is the function of network-related variables (such as the number of controller, etc.) (Yang et al., 2018). When the network size grows, i.e., k → ∞, apparently Φ → 0.

From Equ.(19), we can get:(20) 
 
 
 
 
 

We set 
 
. According to inequality (20), we can further get(21) 
 
 

Due to 
, by applying Lemma 2, we have,(22) 
 
 
 
 

Then Equ.(22) is guaranteed with 
 
. That means that after the rounding process, the approximation factor of our algorithm is 
 
.

5.4. Time complexity analysis
Let η denote the maximum number between the service controller and the access controller, where η = max(M, L), let 
 denote the number of switches on the p path. For a flow f, we use ρ to denote the number of predetermined paths that a flow may pass, and let  
 
 to denote the maximum number of switches on the each predetermined path, respectively.

Theorem 4

The time complexity of RC_EACP is O(ηρ).

Proof

In Algorithm 1, it is to find the mapping between the controller and the path for one flow. We can see that the bandwidth cost f(vi, cj) and the delay cost τ(vi, cj) can be obtained based on the network topology information in line 2. The time complexity of the bandwidth cost and the delay cost are both O(1). In line 3, once the network is established and the path of the flow is determined, the management cost Cf(cj, p) are calculated for each controller, and the time complexity is O(sp). Line 10 is to find the maximum value from the η number of 
, and the complexity is o(η). For a flow f, there are ρ predetermined paths. It is not difficult to find that there are ρsubsets Sp in set Sf. So, the line 12 will be executed ρ times. For a flow f, the total complexity is O(1) + O(sp) + O(ηρ) = O(ηρ). Therefore, the time complexity of the RC_EACP algorithm is O(ηρ).

Theorem 5

The time complexity of SCM_EACP is O(nρ).

Proof

In this subsection, we analyze the complexity of the SCM_EACP algorithm. If we get Cf(cj, p) and 
 from algorithm 1, the algorithm complexity is O(1) in line 1. In line 2, it needs to find the minimum value 
 from ρ paths. So, the time complexity is O(ρ). From line 10 to line 14, there is the mapping between controller cj and all switches on the path p. According to the above definition, the number of switches on the path p is 
, and the maximum number of switches on the each predetermined paths is  
 
. So, the time complexity is O(n). It is not difficult to see that the algorithm complexity from line 6 to line 16 is O(nρ). For a flow f, the time complexity of the mapping algorithm between the switch and the controller is O(1) + O(ρ) + O(nρ) = O(nρ). Therefore, the time complexity of the SCM_EACP algorithm is O(nρ).

6. Performance evaluation
In this section, we first describe the experiment setting and give the performance metrics to evaluate algorithms, and then analyze the experimental results.

6.1. Experiment setting
●
Operating environment: The hardware configuration is 3.1 GHz CPU, i7-5557U processor and 4G RAM. We utilize Mininet (Mininet,) and Ryu controller (Ryu,) to build SDN network environment, and the communication protocol is OpenFlow 1.3 (Openflow). The algorithms are implemented in the ubuntu 14.04 system.

●
Network topology: The widely adopted classical Fat-tree data center topology (Al-Fares et al., 2008) and VL2 topology (Greenberg et al., 2009) depicted in Figs. 2 and 3 are utilized in the simulation. For Fat-tree, the number of pod is 8 and each pod connects with 16 servers. The number of edge switches and aggregation switches is both 4 in each Pod; The number of core switches is 16; The total number of hosts is 128. For VL2, the entire topology is divided into three layers, top of rack (ToR) switch, intermediate switch and aggregate switch. The number of ToR switch is 36, and each connects with 20 servers, and the number of intermediate switch and aggregate switch are both 12, so the total number of hosts is 720.

Fig. 2
Download : Download high-res image (322KB)
Download : Download full-size image
Fig. 2. Topology of Fat-tree.

Fig. 3
Download : Download high-res image (316KB)
Download : Download full-size image
Fig. 3. Topology of VL2.

●
Data Trace: Traffic requests follow Poisson distribution, and each request is from a source node to a destination node, where both nodes are randomly selected. We use iperf (Iperf and Cisco Nexus 2200 s, 2200) to generate actual network traffic, and evaluate the performance of algorithms by changing the size and number of flows.

●
Parameter Setting: Let O(N2) in Equ (10) be 104. The number of service and access controllers are all set to be 10. The capacity of each controller is 1800K flows/s (Dixit et al., 2013), which is enough to handle the maximum request rate in the simulation.

●
Algorithm Comparison: We compare the proposed algorithm with the DCP-SA (Bari et al., 2013) and the DSA-SM (Wang et al., 2016). The two algorithms typically studied how controller placement affects the bandwidth and latency to setup forwarding rules. DCP-SA tries to periodically evaluate the system's control cost while DSA-SM is to minimize the average response time of the control plane by dynamically allocating controller.

6.2. Performance metrics
This paper mainly focuses on the control channel overhead for flow rule setup which consists of the bandwidth overhead and delay cost on the control channel link when managing flows. They are respectively formulated as follows,

●
Bandwidth Overhead: According to Equ. (11), the control channel bandwidth overhead for a flow f on path p is 
. Hence, the total bandwidth overhead is expressed as 
.

●
Delay Cost: According to Equ. (13), it is expressed as 
 
.

●
Channel Total Cost: Based on Equs. (14), (15a), it is expressed as 
.

6.3. Effect of bandwidth overhead in Fat-tree and VL2
In the experiment, when the network topology is established, the shortest path between each switch and each controller can be determined, so we can easily get the bandwidth cost f(vi, cj) and the delay cost τ(vi, cj). According to the data trace and the existing routing algorithm (Zhao et al., 2018), we can obtain the predetermined paths Pf of a flow, and get the set of switches Sp on each path. Assume that every controller cj can manage switches Sp on any path p, and let Xf(cj, p) = 1, ∀cj ∈ M or cj ∈ N, p ∈ Pf. According to Equ.(12), we can use the exhaustive search method to calculate the every bandwidth overhead 
, and find the minimum one. In this way, the mapping between the controller and the path can be determined, which is defined as the optimal solution. In order to examine the performance, we compare the bandwidth overhead of SCM_EACP algorithm with the optimal solution and other algorithms.

As shown in Figs. 4 and 5, it is not difficult to observe that the bandwidth cost increases linearly as the number of flow increases. However, the bandwidth overhead of optimal algorithm and SCM_EACP increases less slowly than that of DCP-SA and DSA-SM. Through the bandwidth overhead of SCM_EACP is a little higher than that of optimal algorithm, it is always lower than that of DCP-SA and DSA-SM. The main reason is that SCM_EACP algorithm is applied to select an optimal controller for each elephant flow to manage its transmission.

As shown in Fig. 5, compared with Fig. 4, when the number of flow is small, such as 2000, the channel bandwidth cost in the VL2 network is less than that in the Fat-tree network. Besides, as the number of flows continues to increase, the channel bandwidth cost in the VL2 network grows faster than that in the Fat-tree network. This is because, in the VL2 network, the source_id and destination_id of a large number of elephant flows are not in the same Pod, when these elephant flows are transmitted, more control bandwidth needs to be occupied between the service controller and the switch.

6.4. Effect of latency cost in Fat-tree and VL2
We can observe from Fig. 6 that SCM_EACP always has less delay cost than DCP-SA and DSA-SM. Specifically, SCM_EACP can reduce delay cost by 9.87% and 28.02% on average compared to DSA-SM and DCP-SA. This is because SCM_EACP optimizes the allocation of controllers for each flow to prepare for transmission, which relies on the characteristics of elephant flow and mice flow, thus it can reduce the number of control path hops for elephant flows and cut down the rule calculation time for the elephant flows.

We can easily see from Figs. 6 and 7 that the maximum delay cost gap between SCM_EACP and optimal algorithm in the Fat-tree network and the VL2 network are 5.29% and 4.23%, respectively. Obviously, for SCM_EACP, the delay cost performance in VL2 is better than that in the Fat-tree network. Because there are more redundant links between intermediate and aggregation in VL2 with the number of flow increasing, and more and more elephant flows have fewer queue delay because the transmission of these flows can be managed by a controller with the shortest control path based on SCM_EACP.

6.5. Effect of total control cost in Fat-tree and VL2
In order to reflect the performance of Cost_EACP scheme, we evaluate how the total cost of the system will change as the number of flows increases.

In the experiment, we assume that the bandwidth is enough so that the propagation delay of control signal is neglected and α = 0.6. Figs. 8 and 9 depict the total cost of the four controller assignment algorithms over different number of flows. It is not difficult to observe that the total cost of SCM_EACP is almost close to the optimal result and has an almost constant gap. In addition, the total cost of our algorithm can reduce total cost by 36% and 50% on average compared to DSA-SM and DCP-SA. This is because that when controller is assigned to each flow, the feature of elephant flow is fully considered which may need to set up multiple flow rules in the transmission process.

Fig. 8
Download : Download high-res image (308KB)
Download : Download full-size image
Fig. 8. The total control costs of SCM_EACP in Fat-tree.

Fig. 9
Download : Download high-res image (295KB)
Download : Download full-size image
Fig. 9. The total control costs of SCM_EACP on VL2.

As shown in Fig. 9, in the VL2 network, as the number of flows increases, the total costs of all four algorithms grow faster than those in the Fat-tree network. The main reason is that with the number of flows increases, the increase of delay cost in the two networks is almost the same, but the bandwidth overhead of VL2 increases faster than that of Fat-tree network.

6.6. Effect of α parameters
In this part, we will evaluate the total cost gap percentage with different parameter α. Here, the total cost gap percentage indicates the extra total cost part of SCM_EACP compared to the optimal algorithm and it is expressed as 
 
, where CostA and CostB denote the total cost of SCM_EACP and optimal algorithm, respectively. In the experiment, the parameter α is set to 0.2, 0.4, 0.6 and 0.8.

Comparing Figs. 10 with 11, we can also draw the following conclusions: in the Fat-tree network, when the number of the flows in the network is determined, the larger α has a larger total cost gap. On the contrary, the total cost gap in the VL2 network decreases with the increase of the parameter α. This is because bandwidth cost has a greater impact on the total cost in the Fat-tree network, while the delay cost has a greater impact on the total cost in the VL2 network.

7. Conclusions
In this paper, we design a flow-based two-tier centralized control framework called multi-controller provision (MCP) framework for software defined DCNs, where the control mode is quite different from the traditional centralized control mode. The proposed MCP framework is more conductive to manage multi-service types of flows in the DCNs. In addition, we analyze the control mode of the flows in the proposed framework and formulate a cost-effective adaptive controller provisioning (Cost_EACP) problem, which is proved to be NP-complete. Next, a rounding-based Cost_EACP algorithm (RC_EACP) is designed to solve this problem. Besides, a switch controller assignment mapping algorithm based on RC_EACP (SCM_EACP) is proposed to find the mapping relationship between switches and controllers. Simulation results show that the proposed algorithm outperforms other flow management schemes in terms of bandwidth cost and delay cost.

