Fog computing extends Cloud-based facilities and stays in the vicinity of the end-users to provide an attractive solution to a diverse range of latency-sensitive applications. The applications are becoming more sophisticated, context-aware, and computation-intensive due to varying situational and environmental conditions in order to meet the ever-increasing users’ demands. Further, resource heterogeneity, dynamic nature, resource limitations, and unpredictability of the Fog environment make scheduling of application tasks while satisfying Quality of Service (QoS) requirements a challenging job. To overcome these issues various scheduling strategies have been proposed considering contextual information of different entities involved in Fog computing. This survey represents a comprehensive literature analysis pertaining to context-aware scheduling in Fog computing. It provides detailed comparison of existing scheduling approaches based on important factors such as context-aware parameters, case studies, performance metrics, and evaluation tools along with advantages and limitations. It also presents detailed taxonomy, performance metrics, and context-aware parameter analysis. Further, it list several issues and challenges. This study will aid the research community in exploring future research directions and essential aspects of scheduling approaches using different types of contextual information.

Previous
Next 
Keywords
Fog computing

Context-awareness

Scheduling

Resource management

Resource estimation

Resource provisioning

Contextual information

1. Introduction
The number of sensing and mobile devices geographically connected to the Internet, termed as the Internet of Things (IoT), would include almost 50 billion by the end of 2020 (Solutions, 2015). These large numbers of IoT devices/sensors are highly distributed and mobile, placed at the edge of the network, demand low latency and real-time data analysis (Anawar et al., 2018). This poses many challenges to the traditional network architecture of the Cloud computing framework where Cloud Data Centers (CDCs) are geographically centralized that often fail to deal with the processing and storage demands of billions of the geo-distributed IoT devices/sensors. As a result, high latency in service delivery, network congestion, and poor QoS are experienced (Mancas and Mocanu, 2013).

Cisco introduced a novel computational paradigm, named as Fog computing, that overcomes the shortcomings of Cloud computing by moving some of the core functionalities of Cloud towards the edge of the system (Bonomi et al., 2012). Fog computing performs efficiently in terms of response time, network traffic, service latency, and power consumption for the applications such as intelligent traffic control, health monitoring, and emergency services that demand to receive response in a small amount of time. It comprises entities having computing, storage, and networking capabilities that support context-awareness, real-time interactions, scalability, interoperability, and service-application execution (Di Martino et al., 2017; Dastjerdi and Buyya, 2016; Perera et al., 2013). The context-awareness of an entity implies an information relevant to its state that can be exploited to perform the desired operation(Sangulagi and Sutagundar, 2018; Krause et al., 2003). A Fog environment is said to be context-aware if it extracts, interprets, and uses the contextual information to produce the required results (Byun and Cheverst, 2004).

With the exponential increase in IoT devices, applications are becoming context-aware, more advanced, and computation-intensive to meet the growing users’ requirements (Murtaza et al., 2020; Assuncao et al., 2012). But on the other hand, Fog devices have constrained capacity in terms of storage and processing capabilities (Rabie et al., 2019). Consequently, this incongruity limits the level of QoS as well as Quality of Experience (QoE). Furthermore, application modules placed on several Fog nodes receive a large number of requests with a high rate of input tuples. The huge fluctuating load managed by Fog manager leads to an increase in communication and computation overhead, and operational inefficiencies. This results in network congestion as well as degradation in service delivery time and QoS (Roy et al., 2018).

Various scheduling approaches (Sun et al., 2018; Cardellini et al., 2015; De Benedetti et al., 2017; Kabirzadeh et al., 2017; Sun and Zhang, 2017; Ghobaei-Arani et al., 2019a) have been widely applied in the Fog computing environment to overcome aforementioned issues. However, these approaches narrowly exploit context-awareness while making task scheduling decisions. As a result, these are not able to allocate and provision resources efficiently that leads to unpredicted and unfair management of resources (Tran et al., 2019).

Context-aware scheduling helps in the proper selection of resources for the application tasks to satisfy the users' QoS requirements such as minimizing completion time and improving the throughput (Aazam et al., 2016a, 2016b). It can be employed for efficient resource allocation that considers different levels of context-awareness. The contextual information relevant to the user and application context can be applied in resource scheduling based on motivation and scenarios (Mahmud et al., 2018a). It can lead to efficient scheduling, allocation, and provisioning of resources and services in the Fog computing environment. By leveraging contextual information, the performance of services can improve drastically and the system could understand the user's situation to enhance the QoE (Mahmud et al., 2016). Therefore, it is of great practical significance to exploit context-aware scheduling in order to achieve higher performance and enhance the utilization of computational resources in the Fog computing environment.

1.1. Related surveys
There are several survey papers on context-aware in the area of IoT, mobile computing, and pervasive computing as discussed below:

Perera et al. (2013) presented a study of IoT based computing technologies on context-awareness for evaluating various context-aware systems. Vahdat-Nejad et al. (2016) discussed various context-aware applications designed on Vehicular Ad-hoc Networks (VANETs) that offer a broad range of services to drivers including collision avoidance, traffic management, and convenient support. Yürür et al. (2014) investigated the mobile systems based on context-awareness and how their implementation varies from the conventional context-aware systems. Sezer et al. (2017) investigated context-aware systems and how context information can be provided through different methods. Pradeep and Krishnamorthy (2019) compared numerous context-aware systems in terms of features and also highlighted their components. To the best of our knowledge, there is no survey that addresses context-aware scheduling in Fog computing. The proposed survey is the first one of its kind in the area of Fog computing. This survey differs from the aforementioned surveys in several means as summarized in Table 1. It classifies different types of context-aware information that can be used in the Fog environment. It also discusses, how contextual information can be leveraged for efficient resource management in general and scheduling in particular.


Table 1. Summary of related surveys.

Ref.	Year	Main focus/contributions	Study area
Perera et al. (2013)	2013	Surveyed context awareness, provided an in-depth analysis of the context life cycle, and discussed a wide range of techniques, methods, and middle-ware solutions related to context-awareness.	IoT
Yürür et al. (2014)	2014	Surveyed the literature of context-awareness and provided definition, representation, and inference of context. Also, pointed out the significance of context-aware middle-ware design, and the challenges faced during system design and integration.	Mobile Computing
Vahdat-Nejad et al. (2016)	2016	Reviewed and classified existing vehicular context-aware research projects and applications. Also, a classification framework is proposed including the environment, context-awareness, and system-and-application.	VANETs
Sezer et al. (2017)	2017	Reviewed context-aware computing, learning, and big data studies. Also, focused on context-awareness, inferences from context, context reasoning, and learning algorithms to make profiling, predictions, and data analysis using big data.	Ubiquitous/pervasive computing
Pradeep and Krishnamorthy (2019)	2019	Outlined the basic elements required for a context-aware system. Also, evaluated multiple context-aware systems based on context modeling, context organization, and context middle-ware approach	Context-aware systems
PS	–	Systematic literature review of context-aware resource scheduling. Provides analysis and classification of context-awareness. Also, presents an in-depth discussion of the utilization of context-awareness in resource scheduling approaches in order to improve QoS.	Fog computing
PS-Proposed Survey.

1.2. Contribution
This work presents the comprehensive systematic literature survey on context-aware scheduling in Fog computing. It presents detailed analysis of the existing scheduling strategies that exploits context-awareness of different entities of Fog computing. The purpose of this paper is to benefit researchers in identifying future research directions and gaining insights for developing scheduling techniques by leveraging contextual information in order to reduce resource wastage while enhancing QoS requirements. The main contributions of this paper are as follows:

(i)
An extensive analysis of the context-awareness is carried out and its classification is exposed.

(ii)
A comprehensive review of the existing resource scheduling approaches that utilize different levels of contextual information is presented.

(iii)
A comparative analysis of scheduling approaches based on important factors such as context-aware parameters, performance metrics, case studies, and evaluation tools with their advantages and limitations is given.

(iv)
A detailed taxonomy and analysis of context-aware parameters as well as performance metrics are provided.

(v)
A list of issues and challenges along with future research directions are compiled for further research exploration.

1.3. Paper organization
The rest of this paper is organized as follows: Section 2 provides a background that discusses the Fog computing paradigm. Section 3 presents the review technique that includes research questions, motivation, different sources of information, search criteria and quality assessment. Section 4 discusses scheduling in Fog computing and explores the fundamentals of context-awareness. It also presents context-aware scheduling architecture and provides a literature analysis of various context-aware scheduling approaches. Section 5 presents analytical discussions. This section also provides a detailed taxonomy, parameter analysis, challenges of context-aware scheduling and future research directions. Finally, the conclusion is presented in Section 6.

2. Background
This section discusses the basic concepts and architecture of the Fog computing paradigm.

2.1. Fog Computing paradigm
Fog computing is a distributed computing environment that serves as an intermediate between CDCs and IoT devices/sensors. It extends Cloud to devices on the network edge to cope with computational requirements of massive numbers of IoT devices, and storage needs of real-time low-latency applications (Evans, 2011; Yousefpour et al., 2019). The OpenFog Consortium defined Fog Computing as: “A horizontal, system-level architecture that distributes computing, storage, control, and networking functions closer to the users along a Cloud-to-thing continuum” (O. C. A. W. Group, 2017). In March 2018, the National Institute of Standards and Technology released another definition: “Fog computing is a horizontal, physical or virtual resource paradigm that resides between smart end-devices and traditional Cloud computing or data center that supports vertically-isolated, latency-sensitive applications by providing ubiquitous, scalable, layered, federated, distributed computing, storage, and network connectivity” (Iorga et al., 2018). It has been formulated to bolster up latency constraints, a stipulation for geo-distribution, and mobility of applications. It provides resource scheduling, storage-capabilities, and data processing to accomplish tasks locally instead of transferring them to the Cloud (Mouradian et al., 2017; Bonomi et al., 2014; Chiang and Zhang, 2016). The services requested by the user can be disintegrated into a collection of tasks. Consequently, the scheduler prepares an optimal task assignment that is submitted across the Fog devices for execution to fulfill the user requirements while meeting QoS objectives such as latency, cost, energy utilization, and scalability.

Fog Computing has a three-layer architecture comprising layers, namely, IoT device/sensor layer, Fog layer, and Cloud layer (Hu et al., 2017; Ghobaei-Arani et al., 2019b) as shown in Fig. 1. The IoT device/sensor layer is the bottom layer that incorporates many smart internet-connected IoT devices and sensors. These devices generate a workload that is collected by Fog gateways and then transfer it to the Fog layer. The processes are carried out by Fog nodes to diminish bandwidth and execution time (Ghobaei-Arani et al., 2018). The Fog layer is the middle layer consisting of intelligent intermediate devices called Fog nodes such as routers, gateways, and bridges (Di Martino et al., 2017). This layer is responsible for the management of data flow and task scheduling. The tasks may be assigned to some high-performance Fog nodes or to a dedicated node. A dedicated Fog node, Fog manager, is also responsible for determining whether a task should be processed in the Fog layer or the Cloud layer. The decisions of the Fog manager are crucial for efficient scheduling and depend on the availability of resources, computational power of Fog nodes, and latency constraints. The Fog nodes are connected to the Cloud layer via Cloud gateways that are responsible for transmitting workload to the Cloud servers at given intervals of time. The Cloud layer is a top layer, comprising thousands of inter-connected high-end servers, that provides infrastructure, platform, and software services on a pay per usage basis (Dastjerdi and Buyya, 2016).

Fig. 1
Download : Download high-res image (340KB)
Download : Download full-size image
Fig. 1. Fog computing environment.

3. Review technique
This section discusses about the review technique followed to carry out this systematic literature survey. It also discusses about the electronic database sources used for searching and retrieving articles pertaining to the area of context-aware scheduling. Further, it describes the general and specific keywords used for finding suitable articles and their quality assessment procedure.

The systematic survey guidelines described in Kitchenham et al. (2007) and Kitchenham et al. (2009) are followed in this survey. The processes involved in writing the proposed survey are framework review creation, survey execution, review results examination, review results recordation, and research challenges exploration. The research questions framed to successfully execute this survey are presented in Table 2.


Table 2. Research questions and motivation.

Research Questions	Motivation
1.	What are the different types of contexts considered in the Fog computing paradigm?	Researchers have envisioned context-awareness in the Fog computing environment, but it is still at its initial stage. Finding a deficiency of survey papers in the area of context-aware scheduling based Fog computing was a vital motivational factor. Context-awareness can lead to efficient scheduling in Fog computing. Contextual information can be received in many forms that still needs an in-depth investigation. It helps in efficient scheduling of tasks including resource estimation, provisioning, and allocation as well as application, data, and network management. Various resource scheduling techniques are developed so far in the context of user and application. However, several other contexts are either not discussed or narrowly exploited.
2.	What are the various context-aware parameters exploited in scheduling approaches?	
3.	What is the life cycle of context-aware engine and how scheduler utilizes the contextual information?	
4.	What are the different techniques that utilized contextual information in scheduling approaches?	
5.	How different contexts can be utilized to allocate resources efficiently?	
6.	How the resources are estimated efficiently in the context of mobility?	
7.	What are the techniques to enhance QoS such as minimizing latency, response time, and cost in scheduling policies, by exploiting different contextual information?	
8.	How utilization of resources can be enhanced by through context-aware scheduling?	The research challenges in terms of research questions explore the existing literature. Also, many essential aspects and features of contextual-information are yet to be explored. The study of several techniques to utilize contextual-information in scheduling techniques can be a possible research area in the Fog computing paradigm.
9.	How to utilize the application contextual information such as task processing requirements (CPU speed, storage, memory) and networking requirement for resource and service previsioning?	
10.	Which case studies are implemented in the context-aware scheduling methods?	
11.	What evaluation tools are practiced for evaluating context-aware scheduling approaches?	
12.	What are the performance metrics utilized to evaluate context-aware scheduling approaches?	
13.	What are the open issues and challenges, and future research directions in context-aware Fog scheduling?	
3.1. Source of information
Recommendations of (Kitchenham et al., 2007, 2009) have been followed for searching research articles pertaining to the area of context-aware resource scheduling in Fog computing. The following electronic database sources have been used for searching:

●
Google Scholar (<www.scholar.google.co.in > )

●
ScienceDirect (<www.sciencedirect.com > )

●
Springer (<link.springer.com > )

●
IEEE eXplore (<www.ieeexplore.ieee.org > )

●
ACM Digital Library (<www.acm.org/dl > )

●
Wiley Interscience (<www.Interscience.wiley.com > )

●
Taylor & Francis Online (<www.tandfonline.com > )

3.2. Search criteria
This systematic literature survey includes research articles written in the English language from the year 2012 to mid-2020. The search process based on the search strings given in Table 3 retrieved about 200 research papers from various conferences, journals, book chapters, and workshops. The search process is explicitly applied on some journals of IEEE, Science Direct, Wiley, Springer, etc. to cross-verify the e-search. The review methodology for filtering quality research papers based on the exclusion and inclusion criteria at various stages is depicted in Fig. 2. The number of research articles was reduced to 100 in the first stage on the basis of their titles and to 80 in the second stage based on their abstracts and conclusions. Then, the 50 research papers were selected in the third stage based on the full-text examination. Finally, in the fourth stage, these 50 articles were extensively studied to obtain a concluding compilation of 32 articles via reference investigation and excluding common challenges on the basis of the principle of inclusion and exclusion.


Table 3. Keywords used for searching research articles.

S#	General Keywords	Specific Keywords	Duration	Content Type
1	Context-aware scheduling	Mobility-aware, Latency-aware, QoS-aware, Location-aware		Journal, Conference, Book Chapter, Workshop, Magazine, and Transactions
2	Context-aware resource management	Resource provisioning, Resource estimation, Resource allocation, Application placement	2012-mid-2020	
3	Context in Fog	User, Device, Network, Application contexts		
Fig. 2
Download : Download high-res image (320KB)
Download : Download full-size image
Fig. 2. Review methodology.

3.3. Quality assessment
The quality evaluation was performed on research papers in order to segregate the research articles by applying the principle of inclusion and exclusion. Each research paper was investigated for unfairness, external and internal validation of results according to Center for Reviews and Dissemination (CRD) guidelines given by (Kitchenham et al., (2009)) to obtain quality research articles pertaining to the area of context-aware scheduling in Fog computing.

4. Scheduling
This section discusses about basic concepts of scheduling, context-aware fundamentals, and different types of context-aware scheduling. It also presents the critical analyses of existing literature based on type and parameters of context employed and performance metrics used for evaluation.

In the Fog computing environment, the problem of task scheduling is to allocate suitable resources to the tasks submitted according to scheduling goals. But, the heterogeneity, resource limitation and mobility of Fog nodes, and QoS constraints makes scheduling a challenging problem (Ghobaei-Arani et al., 2019b). High-performance, low-latency, and critical-applications such as surveillance, online gaming, telecommunications, digital signal processing, intelligent transportation, emergency, and healthcare with increasing user's requests demand immediate actions for processing while ensuring ultra-responsiveness and ultra-reliability (Mastoi et al., 2020; Name et al., 2017; Liu et al., 2018). This results in network congestion, computational overhead, increase in communication, and prolonged service delivery time; hence degrades QoE. Therefore, to deal with increasing requirements for computational resources requested by users to perform a large number of tasks and providing smooth QoE, the need for an efficient task scheduling arises in Fog computing. Selection of appropriate resources for the tasks, meeting the minimum completion time, and improving the throughput in order to satisfy the users' QoS requirements can be defined as a scheduling problem. A scheduling problem is searching an optimal solution in an enormous solution space for scheduling a set of n tasks T= {t1, t2, t3, …,tn} with multiple QoS demands such as deadline, response time, and cost on a set of m Fog nodes F= {f1, f2, f3, ….,fm} with different capabilities like network usage, memory usage, and processing power for optimizing scheduling objective function such as reducing execution time (Ghobaei-Arani et al., 2019b; Hosseinioun et al., 2020). The task scheduling is an NP-complete/hard problem (Garey and Johnson, 1979; Ullman, 1975), and thus requires heuristic methods for finding near optimal solutions.

In the Fog paradigm, service requests coming from end-users can be segregated into a collection of tasks that are served by several Fog nodes. The scheduler plans an optimal distribution of these tasks submitted for execution on the Fog nodes. It intends to make the resources available to the tasks in order to serve the user requirements, resulting in efficient sharing of available resources among the multiple service requests. Furthermore, it keeps track of available resources to identify the best device for hosting an application module and allocating the device resources to the module (Souza et al., 2018; Lin and Yang, 2018; Zeng et al., 2016; Fan et al., 2017; Bitam et al., 2018).

4.1. Context-awareness fundamentals
Context-awareness has been applied in computing systems since early 1990s. It is regarded as a key feature of ubiquitous and pervasive computing systems that provide convenient and intelligent services/applications to the users by leveraging the context (Mark, 1991). The definition of the term Context has been proposed by many researchers (Schilit et al., 1994; Brown, 1995; Franklin and Flaschbart, 1998; Ryan et al., 1998), but the widely accepted one given by Dey and Abowd (Abowd et al., 1999; Dey, 2001) is: “Context is any information that can be used to characterize the situation of an entity. An entity can be a device, a network or an environment that is considered relevant to the interaction between a user and an application, including the user and the applications themselves.” The classification of context pertaining to different entities such as User, Application, Environmental, Network and Device involved in Fog computing is shown in Fig. 3 (Nguyen et al., 2018; Mahmud et al., 2018a). These are discussed as follows:

(i)
User Context: It is a type of context that provides user-related information. It describes user's characteristics such as mobility, activities, and social interactions (Yan et al., 2016; Hou et al., 2016). In addition to these characteristics, customer type, user relinquish probability or relinquish rate, usage history, service type, service price, and Net Promotor Score (NPS) can also be considered as user-level contextual information (Aazam et al., 2016a, 2016b). The user contextual information can be exploited in resource scheduling approaches to improve QoS and QoE. For example, mobility of both IoT devices and Fog nodes leads to frequent change of locations from one subnetwork to another resulting in an excess delay that directly has an impact on QoS of invoked service. This requires migration of the Fog service in proximity of the user or consumer in order to maintain the desired QoS (Puliafito et al., 2020). Similarly, usage history of the existing customers can be exploited for resource prediction and estimation in order to improve resource utilization.

(ii)
Application Context: Application context refers to operational requirements of an application. It includes application architecture, latency sensitivity, and application type (Mahmud et al., 2018b; Pešić et al., 2017). The current task load of an application and its processing/networking requirements are also considered as application-level contextual information (Dsouza et al., 2014; Cirani et al., 2015; Cardellini et al., 2015; Shi et al., 2015; Aazam and Huh, 2014; Gazis et al., 2015). Based on the amount of computation required, the application can be heavyweight such as computer vision and speech recognition, or lightweight such as word count and log processing applications (La et al., 2019; Cardellini et al., 2015; Mahmud et al., 2005). The scheduling polices need to consider application type contextual information so that optimal resources can be provisioned to achieve desired QoS. Further, several latency related issues such as node-to-node communication delay, increased service delivery time, and high service access frequency that occur while executing applications in a distributed fashion impact QoS and resource utilization. Thus, prioritization of applications based on latency-sensitivity context of an application can be incorporated in scheduling policies to meet QoS requirements (Afrin et al., 2015; Mahmud et al., 2018b). Services can also be ranked using context-aware parameters such as processing/networking requirements to enable on-demand service provisioning mechanism to identify a service instance that matches the application's requirements.

(iii)
Environmental Context: Environmental context represents environment-centric properties related to surrounding conditions between user and resource such as location from where the access request has been originated. It includes the location and time stamp of the request (Jayaraman et al., 2014; Minh et al., 2017; Minh et al., 2018; Garcia-de-Prado et al., 2017). On-demand tasks and data analytics could be performed by utilizing this type of contextual information for better CPU, and memory management that eventually lead to improved energy efficiency (Jayaraman et al., 2014). Further, Fog computing provides plethora of complicated services such as data organization, indexing, routing, and allocating computing resources. These services can be scheduled effectively to optimize service decentralization in order to reduce latency, energy consumption, and network load by considering contextual information such as location and time (Minh et al., 2017). Similarly, different contextual parameters like location, time, available resources, and estimated response time can be utilized for efficient processing of tasks and delivering delay-sensitive services while optimizing virtualized resources(Minh et al., 2018).

(iv)
Network Context: Network context encompasses information related to network-oriented attributes such as bandwidth of the network, connection signal (cellular signal/(Wi-Fi), and traffic (Tran et al., 2019; Santos et al., 2019; Minh et al., 2018; Truong et al., 2015). It also contains network condition such as topology, communication quality (packet loss and delay) between nodes, and available resources at each node as network-level contextual information (Alam et al., 2016). The main issue of task placement algorithm is maximizing utilization of Fog devices while minimizing latency, energy, and operational cost. To overcome this issue, an efficient task placement algorithm can be developed by taking full advantage of context-aware information such as network conditions, node's resource availability, communication quality, network-topology, and service type (Tran et al., 2019). Further, the scheduler can be made to allocate resources based on contextual information about the current status of the network infrastructure for reducing the energy consumption of the Fog network (Santos et al., 2019).

(v)
Device Context: Device context gives information related to device-centric characteristics such as remaining battery lifetime, available resources, and mobility (Iqbal et al., 2018; Alam et al., 2016; Tran et al., 2019; Minh et al., 2018; Aazam et al., 2018). Further, data size, data flow, and data type are also considered as device-level contextual information (Mahmud et al., 2018a; Mononen et al., 2018). Device contextual parameters such as battery level along with both traffic type and channel condition of network context can be utilized in the scheduling process in order to reduce energy consumption of power-hungry applications such as video streaming and interactive gaming that drain out the battery quickly. Further, data size and data flow contextual information can be utilized in application placement algorithms in order to improve service reliability and service delivery time by coordinating contextual information with the capacity of Fog nodes (Mahmud et al., 2019).

Fig. 3
Download : Download high-res image (453KB)
Download : Download full-size image
Fig. 3. Classification of context in Fog paradigm.

4.2. Context-aware scheduling
“A scheduler is context-aware if it uses context to provide relevant information and/or services to the user, where relevancy depends on the user's task” (Dey, 2001). It should have potential to use contextual information of distinct entities communicating with it at any given time and change its scheduling-policy accordingly in order to manage the resources efficiently (Gu et al., 2019). The architecture of the context-aware scheduling presented in Fig. 4 has been derived from (Assuncao etal., 2012, Guo and Ma, 2017, Mahmud et al., 2019, Perera etal., 2013, Roy etal., 2018). It comprises of context-aware scheduler and context-aware engine as the main components as described below:

(i)
Context-aware Engine: It acts as a core component, serves from acquiring to analyzing the contextual data, and follows a general procedure called context life cycle. The context life cycle consists of four stages that are:

(a)
Context Acquisition: It is the first stage of the context life cycle, where the contextual information of various involved entities like user, application, device, etc. is acquired periodically and then stored in a repository for further processing. The methods adopted for acquiring context varies based on context source, sensor type, responsibility, frequency, and acquisition process.

(b)
Context Repository: It represents the second stage, that stores the contextual information of various types of contexts in the database including their changes as well. It makes the contextual information available for further use.

(c)
Context Reasoning: In this third stage of the life cycle, several methods are exploited to deduce information from the data available in context repository for better understanding of the state of the entities involved in Fog environment.

(d)
Context Analyzer: This is the last and final stage of the context life cycle that operates by analyzing the contextual information received from context reasoning stage and sends recommendations to the scheduler. Its analysis includes cross-relating the present context to historic data, situational information of the system, and existing jobs.

(ii)
Context-aware Scheduler: This is the second core component that provisions resources to the tasks as per their priority. It utilizes the contextual information to derive recommendations for modifications of task priorities and process them accordingly for efficient utilization of resources. It provides effective and fair management of resources by combining context-awareness and scheduling policies. This component intends to rationalize resource estimation and utilization in the Fog computing environment in response to variations of the context-awareness available in the network, leading to the improvement of QoS and QoE. Furthermore, it reduces resource-wastage by rescheduling the tasks considering variations in the user's context. Moreover, the tasks having low priority can be preempted to save computing resources so that more resources can be provisioned to execute high priority tasks.

Fig. 4
Download : Download high-res image (429KB)
Download : Download full-size image
Fig. 4. Architecture of context-aware scheduling.

Several resource scheduling strategies for Fog computing have been proposed to cater to the resource needs of the applications (Sun et al., 2018; Cardellini et al., 2015; De Benedetti et al., 2017; Kabirzadeh et al., 2017; Sun and Zhang, 2017; Ghobaei-Arani et al., 2019a; Truong et al., 2015). Most of the scheduling strategies have not considered context-awareness while making task placement decisions for applications. As a result, they are not able to manage the resources efficiently (Tran et al., 2019). Further, due to the lack of dynamic resource scheduling in applications like intelligent transportation systems, VANETs, etc. have inconsistent behavior because of the mobility of devices. This leads to unpredicted need and utilization of resources which may cause unfair resource management (Aazam et al., 2016a, 2016b). It can be improved by utilizing user or device context. Furthermore, for applications with fluctuating load, an end-to-end manageability of Fog infrastructure can be delivered by considering the application context such as operational/task processing requirements. It can be helpful in coping with various challenges such as improving the operational efficiencies by putting intelligence (contextual information) in applications while guaranteeing an effective utilization of resource (Gazis et al., 2015). Moreover, the prolonged application service delivery time due to heterogeneous, distributed, and constrained computational capacity of Fog nodes can be improved by coordinating contextual information among them. This can be useful not only in effective scheduling of high input data rate to the applications placed on multiple Fog nodes but also in mitigating communication overhead and balancing the computation load of devices (Mahmud et al., Buyya). Therefore, context-aware scheduling in the Fog paradigm can prove beneficial in providing efficient solutions.

Based on the various types of contexts, scheduling approaches exploiting contextual information are categorized and reviewed according to the employed systematic literature review technique. These approaches are discussed as follows:

4.2.1. User context-aware scheduling
Zhu et al. (2013) presented a novel concept of automatically optimizing the web pages dynamically by applying the methods at the web server/Content Delivery Network (CDNs) using user context information about a client that is known or can be accurately measured. The contextual information includes the user's condition for instance network status, device computational load, and traffic type. As a result, web page rendering performance was improved by leveraging contextual information with Fog computing architecture.

Aazam and Huh (2015) proposed a service-oriented resource management model for efficient, effective, and fair management of resources in the Fog paradigm. This model focuses on the type of customer, device-based resource estimation as well as pricing. It overcomes the issues related to customer-type resource estimation and reservation, resource prediction, and pricing for new and existing customers based on user characteristics.

Datta et al. (2015) proposed an architecture based on Fog for connected vehicles with Machine to Machine (M2M) gateways and Road Side Unit (RSUs) for consumer-centric IoT applications and services (connected vehicles, smart grid). The authors examined M2M data processing with semantics, management, and discovery of connected vehicles by utilizing user context (user requirements) in the Fog paradigm. RSUs and M2M gateways act as Fog nodes which facilitate automatic management of connected vehicles to keep track of mobility via the Fog computing platform.

Aazam et al. (2016b) formulated MEdia FOg Resource Estimation (MeFoRE) methodology to offer better resource estimation and management using user contextual information (UCI). It copes with the issue when the nodes are mobile and have fluctuating behavior in resource consumption. The UCI includes the historical records of the Cloud Service Customers (CSCs) service give-up ratio (Relinquish Rate) and NPS. This method improves QoS using prior QoE and NPS histories, and diminish resource underutilization. MeFoRE aids in the effective and fair resource management by attaining necessary QoS and deliver reliable service. The algorithms were implemented and performed using CloudSim on real-IoT traces employing Amazon EC2 resource pricing.

Aazam et al. (2016a) proposed Probabilistic Resource Estimation (PRE) model for the Fog system that takes into account user context to perform resource management beneficially and fairly. It deals with inconsistent connectivity behavior leading to unpredicted need and utilization of resources. The different user characteristics considered were customer type, relinquish probability, usage history, service type, and service price. This model predicts and pre-allocates resources based on the user's behavior and probability of using those resources in the future. Based on customer type and traits like relinquish probability, usage history, price, etc., resources were estimated and defined in units of Virtual Resource Value (VRV). The VRV was then mapped to actual resources (storage space, memory, CPU, etc.) according to service type and policies of Cloud service provider. The PRE model was simulated in real-time on Crawdad trace with various Amazon EC2 services and historical record of CSCs.

Hou et al. (2016) presented a novel concept of Vehicle Fog Computing (VFC) that utilizes vehicle-as-a-infrastructure for computation and communication. The authors discussed VFC architecture that exploits a combined mass of near-user edge devices or end-user clients to carry out communication and computation based on improved exploitation of resources of each vehicle. In this architecture, vehicles act as Fog nodes, and hence no additional deployment cost is required. Because of the mobility of nodes, VFC uses user location as contextual information for computation and communication to manage resources efficiently. For instance, VFC consumes near-located vehicle resources and allows them to cooperate. The authors revealed: i) relationship among the communication ability, connectivity, and mobility of vehicles and ii) features of the parking behavior pattern helped in utilizing the vehicular resources.

Yan et al. (2016) analyzed the coverage probability and ergodic rate for both device-to-device users and Fog computing-based-access points users by considering the user context such as cache sizes, different node locations, access modes, and user density. Also, a called an adaptation user access mode selection mechanism was presented to enhance Fog radio access Networks system performance. They achieved the user-centric objectives via the adaptive technique resulting in load release and lighten the burden of the baseband unit pool. The results produced in the simulation environment validated the author's analysis.

Bittencourt et al. (2017) investigated the scheduling problem in Fog computing considering the impact of user mobility on application performance, and analyzed three different scheduling algorithms, namely concurrent, first come first serve, and delay-priority, to be used to enhance execution based on application characteristics. The various factors such as distributed capacity, range and context awareness (types of user applications, and the mobility of smart devices) were used in resource management and scheduling strategies to improve efficiency.

Mastoi et al. (2020) proposed a system for health care applications based on the Fog-Cloud paradigm. The system implements devised algorithm, named as Health Care-awareness Cost-Efficient Task Scheduling (HCCETS) that determines critical tasks of heartbeat application in order to schedule and executes them with minimum cost while respecting their deadlines. The authors developed the algorithm in multiple phases - task prioritizing phase, resource search phase, and task scheduling phase to achieve the cost-effectiveness of tasks during allocation under QoS requirements. The performance evaluation of the proposed task scheduling algorithm outperformed the previously existed algorithm methods in terms of cost.

Naha et al. (2020) proposed the resource provisioning and allocation algorithms to satisfy the user's dynamic requirements considering deadline parameter in hierarchical and hybrid pattern. The authors ranked the resources on the basis of processing power, available bandwidth, and response time. The ranked resources are then provisioned according to the deadline-driven requests. The simulation results showed reduction in cost and average processing time by 15% and 12% respectively.

4.2.2. Application context-aware scheduling
Dsouza et al. (2014) presented policy-based management of resources to upkeep safe cooperation and interoperability between dissimilar user-requested resources in Fog computing. It supports huge data usage demands for robust real-time data analytics and resource provisioning. The authors also discussed the severe issues of policy-management concerning secure sharing, collaboration, and data reuse by considering application context in terms of networking requirements in the heterogeneous environment.

Aazam and Huh (2014) proposed an architecture that uses a smart gateway complemented with a smart Fog network in order to pre-process and trim data before sending it to the Cloud. It leads smart communication and lesser load to the Cloud. The proposed architecture utilizes the network requirements as an application context for quick resource/service provisioning. The authors considered various factors such as upload delay jitter, synchronization delay, bulk data synchronization delay, and bulk data upload delay for improving resource utilization and service provisioning.

Cirani et al. (2015) proposed a Fog node, represented as IoT-Hub, placed at the edge of the various networks to improve their capabilities of the network by implementing the various functions such as border router, cross-proxy, cache, and resource directory. IoT-Hub functions on the protocol stack at two distinct layers - link layer and application layer. It bridges different physical networks and merges them into a single all IP network at the link layer and provides various functions used for enabling resource discovery and interoperability among applications at the application layer. The performance and feasibility of the IoT-Hub was evaluated by performing experimentation on real-world IoT testbed including various heterogeneous devices.

Shi et al. (2015) used the inter-level communication between the end-user/IoT devices and the Fog nodes to facilitate device clouds and sensor networks to seamlessly share their resources/services. A dedicated Web transfer protocol named as Constrained Application Protocol (CoAP) was used for communication between constrained nodes. This work facilitated the nodes to share services and capabilities in the Fog level and devices of the end-users by using IoT CoAP and REST protocol. The authors conceded both low-energy and high-end sensors of the end-users/IoT devices that were considered as CoAP clients. Further, they applied a machine with Mac OS to send requests to the CoAP servers for experimentation purposes where CoAP servers act as the Fog nodes in the fFog level. This level involves application components that were accountable for delivering pre-processing of the data produced by the sensors. The application context is considered that includes the task load of various applications.

Cardellini et al. (2015) extended standard Storm architecture by adding key modules that are permitted to execute the distributed QoS-aware scheduler. This scheduler gives self-adaptation abilities to the system and can scale the number of applications by increasing the network resources with the help of application context that includes the task load of different applications. The authors provided an absolute experimental assessment of the presented solution using two data stream processing applications: one is the word count and the other is log processing, characterized by distinct requirements with simple topology. The results showed that the proposed scheduler outperforms the standard Storm system by enhancing the application performance and improving the extended system with run-time modification abilities.

Gazis et al. (2015) proposed an Adaptive Operations Platform (AOP) to deliver end-to-end manageability to improve the operational efficiencies by putting in intelligence in industrial applications. This guaranteed an effective utilization of the communication and computing resources of the Fog infrastructure. It aided the Fog computing infrastructure by considering application context such as operational requirements including task processing requirements (CPU, memory, storage, speed) of the industrial processes.

Pešić et al. (2017) presented a software framework solution for Fog computing systems that manage resources and provides services. The proposed solution uses context-aware decision-making methods shared between IoT gateways and IoT Cloud platform. Based on topology changes, it makes decisions for smart actuation by analyzing sensory data-streams, and context-aware management of resources and services to improve performance and efficiency. The authors used the MQTT protocol for data and command transportation. The experiment was performed on a Fog computing testbed that validated the performance of the solution in enhancing flexibility and responsiveness in the context of topology changes.

Mahmud et al. (2018b) proposed a policy for the Fog landscape, termed as Latency-aware Application Module management, that meets data processed per unit time and multiple service delivery latency for different applications. The purpose of the policy is to satisfying service delivery deadlines and optimize resource utilization. The proposed policy includes two algorithms: placement algorithm that places application modules on nodes to ensure QoS, and module forwarding algorithm that forwards applications for the optimization of resources. The application context between various Fog nodes is shared to identify the node as highly-occupied or under-occupied. Both the algorithms were separately evaluated on the parameters such as placement time, and deadline satisfied percentage.

Roy et al. (2018) proposed a well-organized methodology for cross-domain services of different applications by context sharing in Fog computing irrespective of communication protocols. The model is consistent and effective in terms of computing and offers an encouraging solution for low latency service between different applications in real-time and interoperability. The authors also presented context sharing and delay-tolerant load balancing algorithms for optimizing context-sharing time among Fog nodes.

4.2.3. Environmental context-aware scheduling
Jayaraman et al. (2014) introduced Context-aware Real-time Data Analytics Platform (CARDAP) that can perform distributed data analytics in Fog/Cloud environment effectively and efficiently. The authors designed a paradigm for scattered mobile data analytics by implementing CARDAP, which was developed as generic application domain analytics agnostic for monitoring activities of the citizens in a smart city. It captures environmental context-aware information such as walking, jogging, sitting, and standing. CARDAP performs a task on demand, local data analytics, stores analyzed data locally in the Fog, queried later by the Cloud, and utilized an energy-efficient approach for constant monitoring and data upload to the Cloud. The empirical evaluations showed that the CARDAP achieves notable advantages in terms of memory, CPU, and energy efficiency over baseline methods.

Minh et al. (2017) devised a multi-tier Fog computing architecture that helps in IoT service provisioning. The authors proposed a novel cost-effective service placement mechanism that optimizes service decentralization on Fog landscape by leveraging context-aware information such as location, time, and QoS. They conducted an experiment on smart grid applications to evaluate the effectiveness of the proposed method in terms of reducing latency, energy consumption, and network load in comparison with the traditional Cloud computing model.

Garcia-de-Prado et al. (2017) presented context-aware data processing and service-oriented architecture (COLLECT) for the IoTs to expedite the integration of IoT heterogeneous domain context data. The main purpose of the proposed architecture is to aid the Fog devices in publishing and subscribing the scattered events of concern in the context of application. Furthermore, it enhances intelligent decision-making in a real-time context.

Sharif et al. (2017) devised an context-aware architecture and an optimized methodology for creating advanced and useful services. The resources are estimated at each level of the architecture for better management of resources. By using these estimates, the executed code is redistributed across the network to optimize the efficiency and cost of the IoT environment. The current methodology employs environmental and user level context and multi-objective optimization based on key performance indicators such as processing, memory, power consumption, and communication to efficiently exploit the available resources of nodes.

Minh et al. (2018) proposed a new context-aware framework for intelligent transportation systems (CFC-ITS). They systematically model a Fog computing system services into multiple intelligent tiers: IoT tier, Fog service tier, and global Cloud service tier. Different contexts like location, time, available resources are sensed at each tier for efficiently processing delay-sensitive services, estimating response time, and optimizing virtualized resources. The effectiveness of the proposed framework was thoroughly evaluated in terms of energy, cost minimization, and latency by applying to real ITS applications.

4.2.4. Network context-aware scheduling
Truong et al. (2015) proposed FSDN architecture as a potential solution that combines an evolving network and a computing model, one is Software Defined Networking (SDN) and another is Fog Computing. FSDN utilized global knowledge from SDN and context-aware information from Fog paradigms such as location awareness, network status, and task processing requirements to fulfill the demands of future VANETs. The proposed architecture resolved some of the key challenges of VANETs by enhancing Vehicle-to-Infrastructure, Vehicle-to-Vehicle, Vehicle-to-Base Station and SDN centralized control while optimizing resources utility, and dropping latency by incorporating Fog Computing. They also presented two use-cases - data streaming and Lane-change assistance for non-safety and safety services.

Alam et al. (2016) presented a scheduling algorithm based on context-aware (CSA) for 5G based on LTE-A. CSA takes advantage of network, devices, application, environment, and user context in real-time. The scheduling process utilizes the contextual information such as battery level, traffic type, and channel condition to reduces the energy consumption of user equipment and guarantee QoS. The authors also discussed the context-aware framework along with the information model for resource management.

Santos et al. (2019) proposed a Network-Aware Scheduling (NAS) algorithm as an addition to the default scheduling feature existing in Kubernetes (KS). It enables resource-allocation decisions based on the contextual information of the current status of the network infrastructure. NAS showed improvement in service provisioning by attaining a decrease of 70% in network latency in comparison to the default KS algorithm, and an increase in scheduling decision time by 1.22 ms per pod.

4.2.5. Device context-aware scheduling
Mahmud et al. (2019) proposed a context-aware application placement policy for Fog environment to place fourth-generation Industry oriented applications over Fog nodes to minimize the service delivery time. This policy utilizes the context-awareness and coordinates device-level contextual information with the capacity of Fog nodes. The authors considered data size and sensing frequency as device-level context information due to its direct influence on the functionalities of a Fog node and application characteristics. The devised policy considers networking capacity and computation of Fog nodes, QoS requirements, and service delivery deadline of applications. It also prevents the streams of input data from congesting the network and withstand the increasing computing overhead on the host Fog nodes. Evaluation was done on simulation and FogBus enabled real-time environment, and results showed improved service reliability and service time, and less network congestion.

Iqbal et al. (2018) presented a Fog-based framework for data analytics to overcome the issues posed by Internet of Vehicles (IoV) environment by offering context-aware real-time, near real-time, and batch services at the edge of a network. The framework contains five layers namely data-collection, pre-processing, analytics, service, and application. The data-collections-layer collects IoV data from multiple sources such as road sensors, vehicles, pedestrians, roadside units, and autonomous cars. The pre-processing-layer performs in data-trimming, extraction, reconstruction and modeling to aid the layers above, which extract a meaningful-information from the provided datasets. The analytics-layer benefits in bringing data intelligence close to the Fog computing based user network. The service layer handles and reveals the results of analytics layer, whereas the application-layer deals with various IoV applications based on their QoS requirements. The suitability of the framework was assessed through two essential use-cases. The authors used user and device context for productivity, safety, and dependability of IoV systems, for instance, it can help: drivers in understanding their driving behavior, enhancing fuel efficiency by providing less-traffic routes, allowing officials in managing parking slots, and directing traffic laws in densely populated urban areas.

Mononen et al. (2018) presented an algorithm for context-aware Fog computing architecture to filter the data, and map data traits to the frequency of occurrence in order to avoid network congestion in real-time based sensor network applications. The authors also proposed a Fast Fourier Transform (FFT) based algorithm that uses a mapping technique considering both past data and alongside sensor data decided by the requirements of the receivers to improve the latency of data transfer for real-time systems.

Aazam et al. (2018) published a book chapter on Resource estimation challenges and modeling in Fog-IoT. The authors discussed resource allocation by estimating the resources using contextual information such as user type, data type, and device type. They concluded that the overall estimation of the resources is very essential element for reliable resource allocation, leading to better resource scheduling and efficient service provisioning.

Tran et al. (2019) proposed a novel approach for Fog computing frameworks to overcome the issue of efficient task placement in order to maximize utilization of Fog devices while minimizing latency, energy and operational cost. The authors presented a novel model of intelligent multi-tier Fog infrastructure framework that supports effective IoT service provisioning. They devised an efficient service placement algorithm by taking full advantage of context-aware information for maximizing the Fog device utilization, and reducing response time, energy consumption and cost. The contextual information utilized by the proposed task placement solution includes location, network condition containing nodes’ resource availability, communication quality, network-topology, service type, response time. The results evaluated confirmed the robustness of the proposed system.

4.2.6. Summary and analysis
Form the critical analysis of literature, various scheduling approaches have been identified based on the type and parameters of context employed, and performance metrics used for evaluation. A side-by-side comparison of context-aware scheduling approaches in terms of method, type of context, context-aware parameters, performance metrics, case study and tool utilized along with their advantages and limitations is given in Table 4. It is observed that most of the scheduling approaches utilized user and/or application-level context, and were evaluated using either iFogSim or CloudSim simulation tool. Further, it is observed that numerous studies have presented the estimation and utilization of resources to overcome the under-utilization, and achieve a fair, effective and efficient scheduling. In addition to it, some of the research studies in resource scheduling have focused on minimization of latency, response time, and effective management of resources. Also, few research studies have shown low scalability, high cost and energy consumption.


Table 4. Side-by-side comparison of context-aware scheduling approaches.

Ref./Year	Method	Context	Parameters	Case Study	Performance Metrics	Tool	Advantages	Limitations
Zhu et al. (2013)	General	User	Network status	Website rendering	
●
Response time

–	
●
Improved web page rendering performance

●
Network-level contextual information not considered

Aazam and Huh (2014)	General	Application	Task process-ing requirements	–	
●
Upload delay

●
Synchroni-zation delay

–	
●
Efficient service provisioning

●
Better utilization of resources

●
Energy efficiency

●
No study on impact of heterogeneous storage

●
Low overall performance in diverse applications


Dsouza et al. (2014)	Policy-based management of resources	Application	Networking requirements	Smart transportation system	
●
Interoper-ability

●
Secure communication

STS	
●
Real-time data analytics and resource provisioning to clients

●
No sophisticated means to identify policy conflicts for complex use-cases along with diverse devices

Jayaraman et al. (2014)	CARDAP	User, Environmental	Activities, Location	Smart city	
●
Energy

●
Cost

●
Scalability

Android SDK v4.2.2	
●
Delivers significant benefits in terms of CPU, memory and energy efficiency

●
Utilizing context information from the user and his/her environment

●
Task assignment functionality of the scheduler based on capabilities and deadlines has not been discussed

Aazam and Huh (2015)	Fog-based IoT resource management	User	Customer type, User relinquish probability, Pricing	Smart heath-care system	
●
QoS

●
Resource estimation

●
User satisfaction

Cloud-Sim	
●
Efficient in scheduling time and management of resources

●
Allows data centers to perform according to the situations

●
Low scalability

●
No device-level mobility

Alam et al. (2016)	CSA	Device, Network	Battery level, Traffic type, Channel condition	5G	
●
Energy consumption

●
QoS

System level simulator	
●
Increased throughput and energy efficiency

●
Utilization of contextual information in the scheduling policy

●
Cost of energy per bit and the required energy factors not considered

Cardellini et al. (2016)	Extended Storm architecture, QoS-aware scheduling	Application	Task load of different applications	Word count, Log processing	
●
Scalability

●
QoS

●
Latency

JAVA	
●
Improved application performance

●
Enhanced runtime adaptation capabilities

●
Instability in complex topologies involving many operators that can decrease application availability

Cirani et al. (2015)	IoT Hub	Application	Networking requirements	IoT testbed	
●
CPU usage

●
Memory

Califo-rnium	
●
Resource discovery

●
Interoperability among applications

●
Lower Delays

●
Cost not analyzed

●
Scalability not evaluated

Datta et al. (2015)	M2M standard architecture	User	Mobility	Smart vehicular management	
●
Latency

●
QoS

–	
●
Low service latency

●
Provides smart mobility services

●
Lack of powerful communication and computational support

●
Not cost efficient

●
Case study not evaluated

Gazis et al. (2015)	AOP	Application	Task load of different applications	Industrial domain IoT applications	
●
CPU utilization

●
Network traffic

–	
●
Improved operational efficiencies

●
Good utilization of resources

●
High computational complexity

Shi et al. (2015)	General	Application	Task load of different applications	–	
●
Through-put

●
Average round-trip-time

●
Time out probability

Erlang, Raspberry Pi	
●
Reduces the number of packets sent to the Cloud

●
Heterogeneity

●
Low scalability and latency

●
Mobility not considered

Truong et al. (2015)	FSDN	Application, Network, Environmental	Location, Mobility, Latency sensitivity, Application architecture	VANETs	
●
Response time

●
Latency

●
QoS

–	
●
Contextual information utilized to reduce latency of services

●
No backup mechanism for unreliable connections and connection failure between vehicles and SDN controller Less suitable resource management model

Aazam et al. (2016a)	PRE	User	Service type, Service price, Customer type, Usage history, User relinquish probability	Healthcare application	
●
Resource Utilization

●
Latency

Cloud-Sim	
●
Fair management of resources

●
Less effective in terms of latency

●
Underutilization of resources by the node due of its heterogeneity

Aazam et al. (2016b)	MeFoRE	User	Relinquish rate, NPS, Previous QoE	VANETs, Healthcare applications	
●
Resource estimation

●
Latency

Cloud-Sim	
●
Reliability

●
Utilization of historical data as contextual information

●
Heterogeneity

●
Single QoE parameter considered

Hou et al. (2016)	VFC architecture	User	Mobility	Intelligent transportation system	
●
QoS

●
Resource utilization

–	
●
Improves the computational performance/capacity and communication

●
Lack of better mobility model to provide valuable information such as vehicular speeds

●
No network and application-level context considered

Yan et al. (2016)	F-RAN	User	User density, User access mode, Location of nodes, Cache size	5G	
●
Coverage probability

●
Ergodic rate

●
QoS


MATLAB	
●
High performance

●
High interference scenarios not covered

●
Low scalability

Bittencourt et al. (2017)	Mobility aware scheduling algorithm	User	Mobility	Surveillance and gaming application	
●
Delay

●
QoS

●
Network traffic

iFogSim	
●
Improved resource allocation and estimation

●
No backup for scheduling policies for mobility prediction misses

●
Uncertainty in bandwidth availability and application modules processing

●
Lack of network-level contextual information

Minh et al. (2017)	Service placement mechanism	User, Environmental	Location, Time, QoS	Smart grid management system	
●
Latency

●
Energy consumption

●
Network load

iFogSim	
●
Effective in terms of reducing latency, Energy consumption, Network load

●
Less effective resource provisioning

Pešić et al. (2017)	Context Aware Resource Management	Application	Topology changes	–	
●
Resource provisioning

–	
●
Scalability

●
Improved resilience and responsiveness

●
Lack of an appropriate simulation

●
Overhead of the proposed approach not examined

Garcia-de-Prado et al. (2017)	COLLECT	User, Environment	Patient alerts, Hospital alerts	Healthcare application	
●
Response time

●
Data flow

Java, Raspberry Pi	
●
Easy data delivery

●
Improves decision making

●
Less Scalability

●
Cannot process a huge number of events

●
No user profile considered within the system

Sharif et al. (2017)	Context-aware optimization architecture	User, Environmental	Location, Health conditions, Preferences	Healthcare application	
●
Cost

●
Energy

●
Memory

–	
●
Cost optimization

●
Resource usage optimization

●
Optimization of resources for statically defined contexts

●
Lack of implementation

Aazam et al. (2018)	Reliability-based dynamic resource estimation	User, Device	User activity, Location, Power, Battery life	–	
●
Resource estimation

●
Service price

●
Latency

–	
●
Minimizing resource under-utilization

●
No implementation

Iqbal et al. (2018)	Context aware framework for Fog-based data analytics	User, Device	Mobility, Fuel level	VANETs, Intelligent transportation systems	
●
Data Quality

●
QoS

–	
●
Context aware services

●
No implementation of the framework

Mahmud et al. (2018b)	Latency aware model	Application	Latency sensitivity	Healthcare, Smart-home based system	
●
QoS

●
Latency

iFogSim	
●
Optimizes resource usage

●
No evaluation of proposed policy in a real-case study

●
Mobility and users customized settings not considered

Minhet al. (2018)	CFC-ITS	User, Environmental, Device, Network	Location, Time, Available resources, Topology	Smart city	
●
Latency

●
Energy consumption

●
Cost

–	
●
Cost efficient

●
Resource optimization

●
Less flexible resource provisioning

●
Low scalability and openness

Mononen et al. (2018)	Context aware filtering, FFT based algorithm	Device	Data type	Industrial hydraulic application	
●
Latency

●
Bandwidth

–	
●
Improved system performance by monitoring energy and fault diagnosis

●
Low scalability

Roy et al. (2018)	Smart context sharing algorithm, Delay tolerant load-balancer	User, Application	Mobility, Latency sensitivity, Application architecture	Intelligent transportation, Healthcare, Smart-home applications	
●
Delay

●
Latency sensitivity

●
QoS

NS-2	
●
Reduces service delay

●
Context sharing among Fog nodes

●
No focus on minimization of storage capacity of Fog nodes

●
Energy consumption of the network architectures is not considered

Mahmud et al. (2019)	Application placement algorithm	Device	Data sensing frequency, Data size	Industry oriented applications	
●
Service delivery time

●
Computing resource overhead

●
Relaxation ratio

iFogSim	
●
Reduces service delivery time and relaxes network

●
Manages computing overhead and increases service reliability

●
Application placement irrespective battery level of devices

●
Less contextual information exploited for context sharing

Santos et al. (2019)	NAS	User, Network	Network status, Bandwidth, Latency	Smart city application	
●
CPU usage rate

●
RAM usage rate

Kubeadm	
●
Reduction in terms of network latency

●
Algorithm does nor support mobility

●
No mechanism for bandwidth fluctuations and delay changes

Tran et al. (2019)	Service/Task provisioning	User, Network, Device	Location, Topology, Service type, QoS(response time), Available resources	Intelligent transportation system	
●
Latency

●
Energy consumption

●
Network load

iFogSim	
●
Energy consumption reduction

●
Fog device utilization

●
Cost saving

●
Degree of task dependency not considered

●
Task provisioning irrespective of power of devices.

Mastoi et al. (2020)	HCCETS	User	Deadline	Smart healthcare application	
●
Deadline

●
Execution costs,

●
Bandwidth utilization costs

Java, Arduino	
●
Cost efficient

●
No support for mobility awareness

●
Energy, security, and Fault-tolerance cost not considered

●
Device contextual information not considered in resource search phase

Naha et al. (2020)	Resource ranking and provisioning algorithm	User	Deadline	Smart healthcare, Smart city, Smart transportation applications	
●
Delay

●
Processing time

●
Processing cost

iFogSim	
●
Improved performance in terms of instance cost, network delay, and data processing time

●
Scalability

●
Resource ranking done on single parameter.

●
Lack of user contextual information while resource allocation like budget and response time

5. Analytical discussion
This section presents the analytical discussion about existing literature from different perspectives. It also presents detailed taxonomy and parameter analysis of context-aware scheduling. Further, it discusses issues and challenges as well as future research directions.

The existing research in the area of context-aware scheduling in Fog computing has been critically analyzed from the point of view of research questions presented in Table 2. And the analytical discussion about the same is presented as follows:

Fig. 5, a two-level pie chart, depicts the percentage of research papers from different journals, conferences, and book chapters of various publishers examined in this systemic literature review. The inner circle of the pie chart shows that 44%, 55%, and 3% of the research articles that discussed context-aware scheduling in the Fog paradigm are published in journals, conferences, and book chapters respectively. The outer circle shows the various publishers considered for publishing the research articles. It is found that IEEE published the most of these research papers in both journals (44%) and conferences (47%). However, 7%, 6%, 3%, and 3% were published in Elsevier, Sensors, ACM, and Wiley respectively. Further, 6% research papers each in journal and book chapter, and 3% studies in conferences were published by Springer.

Fig. 5
Download : Download high-res image (279KB)
Download : Download full-size image
Fig. 5. Percentage of journals, conferences, and publishers of research papers reviewed.

Fig. 6 is a multi-level pie chart consisting of two concentric circles. The inner circle shows the percentage of the different contexts exploited in context-aware scheduling strategies. It is observed that the user context (31%) is the most utilized in scheduling policies followed by the application context (28%), but the network context (9%) is less utilized. Additionally, 16% of the study contributes to each environmental and device context-aware scheduling policies. Whereas the outer circle depicts the percentage of distinguished context-aware parameters considered in different research articles. It is also identified that most of the context-aware scheduling policies have utilized activities (19%) and application type (19%) parameters that comes under user and application context, respectively. But only 3% of each network condition, bandwidth and traffic parameters of network context, social interaction parameter of user context, data parameter of device context, and application architecture parameter of application context were utilized in scheduling policies. Further, 6% of the studies discussed different scheduling policies for each parameter that include battery life and available resources of device context, time of environmental context, and latency sensitivity of application context. Furthermore, 10% in each mobility and location-based context-aware scheduling was proposed in research papers under user and environmental context respectively.

Fig. 6
Download : Download high-res image (349KB)
Download : Download full-size image
Fig. 6. Percentage of context types and context-aware parameters exploited in scheduling approaches.

Fig. 7 displays the percentage of performance metrics employed for evaluating the various context-aware scheduling approaches. The results drawn from the figure shows that the latency (18%) followed by QoS (16%), time (9%), energy (9%), and cost (9%) parameters are widely used in the evaluation of context-aware scheduling policies. However, scalability (3%), bandwidth (3%), deadline (3%), data flow (3%), throughput (1%), and ergodic rate (1%) are less exploited parameters. Further, delay (6%), estimation (6%), traffic (6%), and utilization (7%) parameters have moderate usage in the evaluation of scheduling policies.

Fig. 7
Download : Download high-res image (192KB)
Download : Download full-size image
Fig. 7. Percentage of performance metrics for evaluating scheduling approaches.

Fig. 8 shows various tools used for the implementation of context-aware scheduling approaches. The inner circle of the pie chart represents usage percentage of tools such as simulation environments (48%), hardware deployments (9%), and programming languages (24%) utilized for evaluating context-aware scheduling approaches. Whereas the outer circle of the pie chart depicts that under the simulation environment category, the authors of 29% research articles have evaluated their proposed methods using iFogSim, 14% of the research articles utilized CloudSim, and 5% of the studies have used NS-2. Further, under the programming languages category, JAVA and MATLAB have been put to use for evaluating 19% and 5% of studies, respectively. In addition to these, under hardware deployment category Raspberry Pi has been used in 9% of research papers to assess the various techniques. About 19% of the studies have neither used nor mentioned any evaluation tool.

Fig. 8
Download : Download high-res image (232KB)
Download : Download full-size image
Fig. 8. Evaluation tools for context-aware scheduling.

Fig. 9 shows the percentage of case studies based on context-aware scheduling approaches. Many case studies such as smart cities, industrial domain IoT, VANETs, 5G, surveillance and gaming, and smart grids have been implemented in the existing literature. It is observed that case study on health care and intelligent transportation system have been implemented by 26% and 20% of the research papers, respectively. The application areas such as surveillance and gaming (3%), 5G systems (6%), and smart cities (3%) are rarely explored.

Fig. 9
Download : Download high-res image (163KB)
Download : Download full-size image
Fig. 9. Distribution of case studies based on context-aware scheduling.

5.1. Taxonomy of context-aware scheduling
In this paper, context-aware scheduling in Fog computing has been taxonomized based on the careful analysis of the existing literature. Fig. 10 presents the taxonomy and classification of context-aware scheduling in Fog computing. The first level of figure shows different type of contexts such as user, application, environmental, network, and device context that are utilized in the context-aware scheduling approaches. Second level of the figure represents subtype of the context, for instance, latency sensitivity, application type and application architecture are the subtypes of application context. The parameters of a particular subtype of the context are represented by the third level of the figure, for example, task processing requirements, task load of application, and networking requirements are the parameters related to application type.

Fig. 10
Download : Download high-res image (382KB)
Download : Download full-size image
Fig. 10. Taxonomy of context-aware scheduling approaches.

5.2. Parameter analysis of context-aware scheduling
The existing literature has been critically analyzed in order to identity context-aware parameters based on type of context, and performance metrics used for evaluation that were considered for achieving the objectives of research articles. The details parameter analysis is give in Table 5.


Table 5. Parameter analysis of context-aware scheduling approaches.

User Context	App. Context	Env. Conx.	Network Context	Device Context	Performance Metrics
References	Activities	Mobility	SI	LS	App. Type	App. Arch.	Location	Time	Bandwidth	Traffic	NC	Data	Battery Life	Avl. Resources	Latency	Time	Delay	QoS	Energy	Cost	Scalability	Utilization	Estimation	Throughput	Traffic	Data Flow	Ergodic Rate	Deadline	Bandwidth
Zhu et al. (2013)	✓	–	–	–	–	–	–	–	–	–	–	–	–	–	–	✓	–	–	–	–	–	–	–	–	–	–	–	–	–
Aazam and Huh (2014)	–	–	–	–	✓	–	–	–	–	–	–	–	–	–	–	–	–	–	–	–	–	–	–	–	–	–	–	–	–
Dsouza et al. (2014)	–	–	–	–	✓	–	–	–	–	–	–	–	–	–	–	–	–	✓	–		–	–	–	–	–	–	–	–	–
Jayaraman et al. (2014)	✓	–	–	–	–	–	✓	–	–	–	–	–	–	–	–	–	–	–	✓	✓	✓	–	–	–	–	–	–	–	–
Aazam and Huh (2015)	✓	–	–	–	–	–	–	–	–	–	–	–	–	–	–	–	–	✓	–	–	–	–	✓	–	–	–	–	–	–
Alam et al. (2016)	–	–	–	–	–	–	–	–	–	✓	✓	–	✓	–	–	–	–	–	✓	✓	–	–	–	–	–	–	–	–	–
Cardellini et al. (2015)	–	–	–	–	✓	–	–	–	–	–	–	–	–	–	✓	–	–	✓	–	–	✓	–	–	–	–	–	–	–	–
Cirani et al. (2015)	–	–	–	–	✓	–	–	–	–	–	–	–	–	–	–	–	–	–	–	–	–	✓	–	–	–	–	–	–	–
Datta et al. (2015)	–	✓	–	–	–	–	–	–	–	–	–	–	–	–	✓	–	–	✓	–	–	–	–	–	–	–	–	–	–	–
Gazis et al. (2015)	–	–	–	–	✓	–	–	–	–	✓	–	–	–	–	–	–	–	–	–	–	–	✓	–	–	–	–	–	–	–
Shi et al. (2015)	–	–	–	–	✓	–	–	–	–	–	–	–	–	–	✓	✓	–	–	–	–	–	–	–	✓	–	–	–	–	–
Truong et al. (2015)	–	–	–	✓	–	✓	✓	–	–	–	–	–	–	–	✓	✓	–	✓	–	–	–	–	–	–	–	–	–	–	–
Aazam et al. (2016a)	✓	–	–	–	–	–	–	–	–	–	–	–	–	–	–	✓	–	–	–	–	✓	–	–	–	–	–	–	–	–
Aazam et al. (2016b)	✓	–	–	–	–	–	–	–	–	–	–	–	–	–	✓	–	–	–	–	–	–	–	✓	–	–	–	–	–	–
Hou et al. (2016)	–	✓	–	–	–	–	–	–	–	–	–	–	–	–	–	–	–	✓	–	–	–	✓	–	–	–	–	–	–	–
Yan et al. (2016)	–	–	✓	–	–	–	–	–	–	–	–	–	–	–	–	–	–	✓	–	–	–	–	–	–	–	–	✓	–	–
Bittencourt et al. (2017)	–	✓	–	–	–	–	–	–	–	–	–	–	–	–	–	–	✓	✓	–	–	–	–	–	–	✓	–	–	–	–
Minh et al. (2017)	✓	–	–	–	–	–	✓	✓	–	–	–	–	–	–	✓	–	–	–	✓	–	–	–	–	–	✓	–	–	–	–
Pešić et al. (2017)	–	–	–	–	–	✓	–	–	–	–	–	–	–	–	–	–	–	–	–	–	–	–	✓	–	–	–	–	–	–
Garcia-de-Prado et al. (2017)	✓	–	–	–	–	–	✓	–	–	–	–	–	–	–	–	✓	–	–	–	–	–	–	–	–	–	✓	–	–	–
Sharif et al. (2017)	✓	–	–	–	–	–	✓	–	–	–	–	–	–	–	–	–		–	✓	✓	–	✓	–	–	–	–	–	–	–
Aazam et al. (2018)	✓	–	–	–	–	–	–	–	–	–	–	–	✓	✓	✓	–	–	–	–	✓	–	–	✓	–	–	–	–	–	–
Iqbal et al. (2018)	–	✓	–	–	–	–	–	–	–	–	–	–	✓	–	–	–	–	✓	–	–	–	–	–	–	–	✓	–	–	–
Mahmud et al. (2018b)	–	–	–	✓	–	–	–	–	–	–	–	–	–	–	✓	–	–	✓	–	–	–	–	–	–	–	–	–	–	–
Minh et al. (2018)	✓	–	–	–	–	✓	✓	✓	–	–	–	–	–	✓	✓	–	–	–	✓	✓	–	–	–	–	–	–	–	–	–
Mononen et al. (2018)	–	–	–	–	–	–	–	–	–	–	–	✓	–	–	✓	–	–	–	–	–	–	–	–	–	–	–	–	–	✓
Roy et al. (2018)	–	✓	–	✓	–	✓	–	–	–	–	–	–	–	–	✓	–	✓	✓	–	–	–	–	–	–	–	–	–	–	–
Mahmud et al. (2019)	–	–	–	–	–	–	–	–	–	–	–	✓	–	–	–	✓	–	–	–	–	–	✓	–	–	–	–	–	–	–
Santos et al. (2019)	✓	–	–	–	–	–	–	–	✓	✓	–	–	–	–	–	–	–	–	–	–	–	✓	–	–	–	–	–	–	–
Tran et al. (2019)	–	–	–	–	–	✓	✓	✓	–	–	–	–	–	✓	✓	–	–	–	✓	–	–	–	–	–	✓	–	–	–	–
Mastoi et al. (2020)	✓	–	–	–	–	–	–	–	–	–	–	–	–	–	–	–	–	–	–	✓	–	✓	–	–	–	–	–	✓	–
Naha et al. (2020)	✓	–	–	–	–	–	–	–	–	–	–	–	–	–	–	✓	✓	–	–	✓	–	–	–	–	–	–	–	✓	–
App., Application; Env. Conx., Environmental Context; SI, Social Interaction; LS, Latency Sensitivity; Arch., Architecture; NC, Network Condition; Avl., Available.

5.3. Issues and challenges
The critical analysis of the existing literature has been carried out to identify various issues and challenges. These are given below:

5.3.1. Context-aware scheduling mechanisms
Most of the existing context-aware scheduling mechanisms narrowly exploit context-aware parameters (e.g., activities, application type) and largely overlook other parameters such as bandwidth, network condition, traffic, application architecture, data, and social interaction. Also, these mechanisms consider a few performance metrics (e.g., time, latency) and ignore other parameters such as cost, energy, deadline, and throughput (Bittencourt et al., 2017; Naha et al., 2020; Tran et al., 2019; Mastoi et al., 2020; Alam et al., 2016). Therefore, there is a need to design and develop new context-aware scheduling mechanisms considering a combination of different types of context-aware parameters/performance metrics for providing optimal solutions and improving QoS as well as QoE.

5.3.2. Mobility-aware management
Mobility, an inherent characteristic of the Fog paradigm, is considered as either user or device or environmental level context-awareness. Many of the existing works considered Fog nodes/IoT devices fixed (Tran et al., 2019; Alam et al., 2016; Aazam and Huh, 2015). However, in studies that considered mobility, prediction misses and application delays can occur due to user unpredictable behavior or lack of information (Bittencourt et al., 2017; Datta et al., 2015; Aazam et al., 2018; Hou et al., 2016). Therefore, impact and benefit of using contextual information in resource management operations needs to be thoroughly investigated. Hence, designing new techniques for mobility management becomes crucial to tackle issues such as optimal use of available resources, resource discovery, offloading, resource provisioning, and security and privacy.

5.3.3. Resource estimation
User characteristics and QoE are utilized as contextual information in various resource estimation models to cope with fluctuating resource requirements, customer relinquish probability, resource prediction, and customer type based resource evaluation and reservation to improve QoS (Aazam et al., 2016a, 2016b, 2018; Aazam and Huh, 2015). However, attaining low latency and efficient utilization of resources is challenging because of heterogeneity of Fog nodes. Further, these models have low scalability. Therefore, there is a need for resource estimation models that consider dynamic behavior of entities in order to minimize latency and enhance QoS.

5.3.4. Context-aware resource and service provisioning
Several existing resource and service provisioning approaches exploited contextual information, but have low flexibility, limited scalability, and inability to process a huge number of events in the Fog nodes (Minh et al., 2017; Garcia-de-Prado et al., 2017; Pešić et al., 2017; Minh et al., 2018; Tran et al., 2019; Santos et al., 2019). Further, in these approaches the degree of task dependency was not considered, and the overhead of the proposed solutions has not been examined. Therefore, the investigation of diverse techniques to apply context-awareness in resource and service provisioning becomes essential for overcoming these limitations.

5.3.5. Energy-aware management
Minimizing the overall consumption of energy in the Fog environment has not been extensively studied in the literature. Existing work improved the energy consumption of the Fog system by using contextual information (remaining battery life) of the devices only (Alam et al., 2016). Hence, there is a requirement to improve the overall performance of the Fog system. This can be achieved by exploiting contextual parameters of different types of context, for example, traffic type, battery level, and channel condition can be exploited to enhance the battery life and reduce bandwidth requirements.

5.3.6. Context sharing within Fog nodes
The collaboration of cross-domain applications, for instance, parking monitoring, road status, traffic guidance, etc. can be combined into a full-fledged service traffic management system (Roy et al., 2018). In such systems, Fog nodes act as an interface between the applications to provide cross-domain services. Contextual information needs to be exchanged between the Fog nodes to allow the applications to perform intelligently in order to realize low latency. The sharing of context among Fog nodes is a potential challenge to achieve interoperability between real-time applications in order to meet the QoS requirements of unified services.

5.3.7. Context-aware application placement policies
The streams of tuples rushing towards the applications placed over distributed, heterogeneous, and resource-constrained Fog nodes congest the network and therefore increase the computing overhead of Fog manager. When the input rate of tuples becomes high, the processing destination of tuples changes leading to an increase in computation and communication load of Fog nodes. As a result, service delivery time increases sharply (Mahmud et al., 2019). Therefore, it is necessary to incorporate context-awareness (e.g., device-level, application-level) in the application placement policies to make it more efficient and improve service reliability and delivery time.

5.4. Future research directions
The possible future research directions extracted from the extensive analysis of the literature are:

●
Designing a resource scheduling algorithm through effective utilization of resources based on various levels of context-awareness.

●
Improving communication and computational support in mobile devices/users through contextual information.

●
Optimizing resource usage and satisfy service delivery deadlines by utilizing user-customized settings and mobility.

●
Designing an efficient methodology to obtain cross-domain services by using context sharing concept and optimizing context sharing time delay between Fog nodes.

●
Designing effective task scheduling by considering task dependency and power of device.

●
Improving application placement policies using combination of device and application-level contexts.

●
Ranking of resources in resource allocation algorithms using context information such as budget/cost and response time.

●
Utilizing contextual information in resource search phase of an algorithm for efficient resource allocation.

●
Achieving interoperability and satisfy QoS requirements using context-awareness.

6. Conclusion
In this survey, a systematic comprehensive literature analysis is provided on context-aware scheduling in the Fog computing paradigm. This survey analyzed the results in numerous ways. It classifies the context in Fog computing into five categories: user, application, environmental, network, and device contexts. Further, the context-aware parameters and performance metrics for utilization as well as evaluation of different scheduling strategies and detailed analysis of overall parameters are presented. Also, the number of case studies applied, and tools used for evaluation are given as well. The advantages and limitations are discussed for each scheduling approach. Based on the extensive study, context-aware scheduling taxonomy has been provided that will benefit the research community to attain a better understanding of context-aware scheduling approaches. Based on the literature studies, this survey puts forward numerous innovative research directions by extracting open issues and challenges. The current work gives the research community an in-depth insight into the existing developments of this field.

