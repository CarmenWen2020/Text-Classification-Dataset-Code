Machine learning is becoming increasingly popular in modern technology and has been adopted in various application areas. However, researchers have demonstrated that machine learning models are vulnerable to adversarial examples in their inputs, which has given rise to a field of research known as adversarial machine learning. Potential adversarial attacks include methods of poisoning datasets by perturbing input samples to mislead machine learning models into producing undesirable results. While such perturbations are often subtle and imperceptible from the perspective of a human, they can greatly affect the performance of machine learning models. This paper presents two methods of verifying the visual fidelity of image-based datasets by using QR codes to detect perturbations in the data. In the first method, a verification string is stored for each image in a dataset. These verification strings can be used to determine whether or not an image in the dataset has been perturbed. In the second method, only a single verification string is stored and can be used to verify whether an entire dataset is intact.

Previous
Keywords
Adversarial machine learning

Cyber security

QR code

Visual fidelity

Watermarking

1. Introduction
The widespread popularity of Machine Learning (ML) in modern technology and its rapid advancements in recent years have paved the way for its adoption in a variety of application areas. ML techniques empower a range of diverse applications, including malware detection (Gibert et al., 2020), self-driving cars, network intrusion detection, natural language processing, behavioral analytics, speech recognition, and so on. Nevertheless, researchers have demonstrated that ML models are vulnerable to adversarial examples in its inputs. The purpose of these adversarial techniques is to employ malicious inputs to fool a ML model into producing erroneous outputs (Hu et al., 2019; Papernot et al., 2017). Consequently, this has given rise to a field of research known as adversarial machine learning (Biggio and Roli, 2018).

There are a number of different adversarial attacks that can be deployed against ML techniques, for instance, an adversary can poison a dataset by perturbing samples in the training data (Biggio and Roli, 2018). Over the years, researchers have examined and demonstrated the effectiveness of such poisoning attacks (Biggio et al., 2012; Rubinstein et al., 2009; Xiao et al., 2015). Adversarial attacks are a serious threat to the success of ML in practice, as small and subtle perturbations in its inputs can mislead a ML model into outputting incorrect predictions, thereby negating the usefulness of the ML model. For ML in computer vision, such perturbations made to images in the ML dataset are often imperceptible to the human visual system (Akhtar and Mian, 2018).

This paper focuses on methods of protecting image-based datasets by verifying the visual fidelity of the data. The effectiveness of ML techniques is affected by the availability of training data. In general, ML models require large amounts of training data to be effective. In light of this necessity for significant amounts of training data, many ML models are trained using public datasets that are freely available online. These public datasets are often copied and distributed without any mechanism for protecting the integrity of the data. This makes these datasets vulnerable to alterations by an adversary.

To address this problem, the research presented here investigates two methods of verifying the visual fidelity of image-based datasets by detecting perturbations in the data using QR codes. The advantage of the proposed methods is that a copy of the original dataset does not have to be stored or used for verification. In the first method, a verification string is generated for each image in a dataset using a QR code. The size of a verification string is much smaller than the original image, and it can be used to verify the visual fidelity of the image. To verify image fidelity, a verification process is used to recover a QR code for each image. If the resulting QR code is noisy or cannot be recovered through this verification process, this indicates that the dataset has been altered.

However, since this method requires a verification string for each image in a dataset, the storage requirement increases linearly with the number of images in the dataset. While this is fine if storage space is not an issue, it may not be an attractive solution for applications with limited storage capacity, e.g., for mobile computing and pervasive social networking that may have storage restrictions (Yan et al., 2017). Therefore, a second method is proposed where only a single verification string is required, and can be used to verify the fidelity of images in an entire dataset. The limitation of the second method is that if the dataset has been altered, one cannot determine which image has been altered, only that the dataset is not intact as it is different from the original.

1.1. Contribution
The research presented in this paper investigates the problem of protecting image-based ML datasets against alteration by an adversary. The proposed methods, which were first presented in Chow et al. (2019), attempt to provide mechanisms for verifying the visual fidelity of images in a dataset without the need to store and use the original dataset for this purpose. To do this, verification strings are generated from the visually important content of the images and these are associated with QR codes. The reason for using QR codes for this purpose is due to its inherent data capacity and error correction properties, which are inbuilt in the QR code structure. Two methods for creating verification strings to verify the fidelity of an image dataset are presented. The advantage of the first method, i.e. the Linear Verification String (LVS) method, is that the fidelity of each image can be verified individually. However, this comes at the cost of higher storage requirements. The advantage of the second method, i.e. the Aggregate Verification String (AVS) method, is that only a single verification string is required to determine whether an entire dataset is intact. Nevertheless, it does not allow for one to determine the visual fidelity of individual images.

2. Background
The proposed methods are based on several concepts, including the QR code structure, the Discrete Wavelet Transform (DWT), the Arnold transform and trapdoor permutation. This section presents a background to these concepts followed by a description of related work.

2.1. Preliminaries
2.1.1. QR code
The Quick Response (QR) code is a two-dimensional (2D) barcode, which was invented by the company Denso Wave (Denso Wave Incorporated). Since its inception, the QR code has seen widespread adoption in a variety of diverse application domains, ranging from electronic payments to product advertisements, social media, and so on. In this research, the reason for using the QR code is to exploit its inbuilt error correction mechanism, which is an inherent part of the QR code structure. This mechanism enables a QR code to be correctly decoded even if part of the QR code symbol is corrupted.

A QR code is made up of light and dark modules, which are organized into function patterns and an encoding region (International Organization for Standardization, 2006). The size and data capacity of a QR code is determined by its version and error correction level. There are forty different QR code versions and four error correction levels, namely, low (L), medium (M), quartile (Q) and high (H). These four error correction levels correspond to error tolerances of approximately 7%, 15%, 25% and 30%, respectively. The higher the error correction level, the greater the capacity for data recovery. However, conversely the higher the error correction level, the larger the amount of error correction redundancy that must be encoded in the QR code. Hence, QR codes with higher error correction levels have lower capacity for encoding actual data as compared with the same QR code version with a low error correction level.

2.1.2. DWT
The Discrete Wavelet Transform (DWT) is a frequency domain transform technique that is widely used in signal processing. For 2D images, it involves decomposing an image into frequency channels of constant bandwidth on a logarithmic scale (Mallat, 1989). An image is decomposed into four sub-bands, which are labeled low-low (LL), low-high (LH), high-low (HL) and high-high (HH). The sub-bands can be further decomposed at the next level, and this process can continue until the desired number of levels is achieved. A depiction of how an image can be decomposed into two levels of DWT sub-bands is shown in Fig. 1. The LL sub-band contains most of the information of the original image (Liu and Yan, 2014). As such, it represents the highest visual fidelity of an image, as the human visual system is more sensitive to its contents. Data from the LL2 sub-band was used in experiments conducted in this study.

Fig. 1
Download : Download high-res image (53KB)
Download : Download full-size image
Fig. 1. Sub-bands of a two-level DWT.

2.1.3. Arnold transform
Adjacent pixels in an image have strong correlation to each other. The Arnold transform, shown in Eq. (1), is an invertible transform that can be used to disrupt the correlation between adjacent pixels (Guan et al., 2005). An original image that undergoes a number of Arnold transform iterations results in a chaotic image. The transform is invertible as the original image can be recovered after a certain number of transform iterations are applied to the chaotic image. The reason for using this transform in this study is because image perturbations introduce noise to an image. Applying the Arnold transform to an image scrambles the pixels, and thus, scatters the noise over the image. This increases the potential of detecting perturbations made to the image and of being able to recover the QR code despite the perturbations.(1) 
  
  
 

2.1.4. Trapdoor permutation
Let D be a finite set. A permutation family Π over D specifies a randomized algorithm for generating (descriptions of) a permutation and its inverse, denoted as 
; an evaluation algorithm Evaluate(s, ⋅); and an inversion algorithm Invert(t, ⋅). We require that for all (s, t) produced by the algorithm Generate, Evaluate(s, ⋅) be a permutation of D and Invert(t, Evaluate(s, ⋅)) be the identity map. A trapdoor permutation family is one way if it is hard to invert, given just the forward permutation description s. Formally, a trapdoor permutation family is (t, ε)-one way if no t-time algorithm 𝓐 has advantage greater than ε. Hence,𝓐
𝓐
where the probability is over the coin tosses of Generate and 𝓐.

2.2. Related work
Researchers have previously proposed the use of QR codes for a number of diverse applications in information security. Examples of information security techniques that adopt the use of QR codes, include methods for authentication in mobile cloud computing (Alizadeh et al., 2016), authentication for wearable devices (Liu et al., 2016), storing private and public information (Tkachenko et al., 2016), secret sharing (Chow et al., 2018), visual cryptography (Fu et al., 2018), digital image watermarking (Chow et al., 2017) and so on.

The methods presented in this research are related to zero-based image watermarking. Unlike traditional image watermarking techniques, which necessitate that a watermark be embedded within an image, zero-based approaches do not require the embedding of a watermark (Ishizuka et al., 2014). For example, Liu and Yan (2014) proposed a secret sharing scheme based on zero-based watermarking. In their approach, the watermark was not embedded within the images, but rather cover images were used to create shares that were to be distributed to participants, with another share that was to be registered with a certification authority. Ownership of the images could then be verified when information in the shares were combined with contents in the original images.

Zero-based image watermarking schemes that adopt the use of QR codes have also previously been proposed by the research community. As an example, an authentication method for medical images using zero-based watermarking and QR codes has been investigated. In this scheme, a patient's identification details and a link to their data was encoded in the form of a QR code that served as the watermark (Seenivasagam and Velumani, 2013). Similarly, Li et al. (2016) proposed a QR code based zero watermarking scheme in conjunction with visual cryptography for authenticating identification photos.

In addition, QR codes have previously been used in conjunction with traditional digital image watermarking. For instance, a watermarking scheme based on the combination of discrete cosine transform, QR codes and chaotic theory was proposed by Kang et al. (2014). In other work, a digital rights management technique for protecting documents by repeatedly inserting a QR code into a DWT sub-band of a document was investigated (Cardamone et al., 2018). Others have also proposed different QR code watermarking approaches, for example, an approach that incorporates an attack detection feature to detect malicious interference by an attacker (Thulasidharan and Nair, 2015), another approach that embeds QR code watermarks using a just noticeable difference model to increase imperceptibility (Lee et al., 2013), and a hybrid approach that uses the discrete cosine transform in conjunction with the discrete wavelet transform for QR code watermarking (Chow et al., 2017).

3. Proposed methods
The two proposed methods are described in this section. The first method allows for the verification of each image in a dataset. It provides a mechanism whereby a user can check whether individual images in a dataset have been perturbed. However, the first method comes at a cost of storing a verification string for each image. This may not be an appealing solution in situations where storage space is an issue. As such, a second method is proposed with very low storage requirements and provides a mechanism to verify whether an entire dataset is intact. Nevertheless, using the second method, while a user can determine whether or not a dataset is intact, the user cannot determine which of the images have been perturbed in the event that the dataset has been altered from its original version.

3.1. Method 1 - Linear Verification String (LVS) method
The notion behind this method is to generate a verification string for each image in a dataset. Hence, it is called the Linear Verification String (LVS) method. The purpose of a verification string is to be able to ascertain whether an image has been altered from the original image. An advantage of generating a verification string is so that respective images in a dataset do not have to be compared with their original image. Moreover, the verification strings will require much less storage space when compared with the size of an entire image dataset. This is due to the fact that the size of a verification string, which is a bit string consisting of 0s and 1s, is much smaller than the size of an image.

The motivation behind this approach is based on zero-based image watermarking (Liu and Yan, 2014). Unlike traditional watermarking techniques that requires a watermark to be embedded within an image, zero-based approaches do not alter the image. This is in line with the objective of the proposed approach, which is to detect whether images in a dataset have been altered. Hence, the method itself must not change the images.

3.1.1. Generating verification strings
Fig. 2 gives an overview of the process used to generate verification strings for all images in a dataset. It can be seen from the figure that to create the verification strings, a QR code that contains a secret message, S, and a key, K, for encryption is required. K is a random bit string, which can be generated in a number of different ways, for example, by using a pseudorandom number generator or by using the hash of a password. Encryption involves performing S ⊕ K for all modules in S. As such, the length of K must equal the number of modules in S. For instance, a QR code version 1 which consists of 21 × 21 modules requires the key to contain 441 random bits.

Fig. 2
Download : Download high-res image (287KB)
Download : Download full-size image
Fig. 2. Overview of the process to generate a verification string for each image in a dataset.

The purpose of performing encryption is so that an adversary will not be able to obtain any information about S from the verification strings. Arnold transform will then be performed on the encrypted secret, SE, to scramble the pixels. The reason for doing this is to scatter any noise that may result from perturbations to an image over the entire image. The scrambled secret, ST, will be used for creating the verification strings. Note from Fig. 2 that the same ST can be used for all images, as this will avoid the necessity of having to generate multiple QR code messages and keys.

Each image in the dataset then undergoes the same process. Each image is decomposed into DWT components at the desired level (DWT at level 2 was used to obtain the experiment results presented in Section 4). The LL sub-band, which represents the highest visual fidelity, will be binarized using a dithering process. The objective of dithering is to binarize the image into black and white bits (i.e. 0s and 1s), while maintaining the average gray level distribution. In the results presented in Section 4, the Floyd-Steinberg dithering technique, which is an approach that is based on error diffusion (Floyd and Steinberg, 1976), was used for the experiments.

After the dithering process, each image is upsampled using nearest neighbor sampling to match the size of ST. The upsampling process is the reason why S can be of any size larger than IDWT. Finally, a verification string, Vi, is produced for each image in the dataset by XORing ST with their respective dithered and upsampled DWT sub-band.

For grayscale images, this process can be carried out directly. For color images, there are two options. The first is to convert the color images into grayscale images, prior to running the algorithm. The second is to separate the color channels, e.g., into red, green and blue color channels, and perform the process on each individual color channel, in which case the verification string's length will increase based on the number of color channels. Nevertheless, the advantage of the second option of generating verification strings for each color channel allows for the method to detect alterations in individual color channels.

Details of the algorithm to generate verification strings for all images in a dataset is provided in Algorithm 1. Algorithm 2 in turn, modifies the original verification string generation algorithm by performing the process on separate color channels for color images. Inputs to both algorithms are a secret QR code, S, an encryption key, K, and all images in a dataset. The algorithm outputs a verification string for each image in the dataset.


Algorithm 1. Algorithm for generating verification strings.

Input: A QR code, S, a key, K, and images in a dataset, Ii, where i ∈{1, 2, …, n}.
Output: Verification strings, Vi, where i ∈{1, 2, …, n}
Step 1. Encrypt information in S by XORing the random bits in K with the modules in S to produce SE.
Step 2. Generate a chaotic image ST by scrambling the bits in SE using Arnold transform for a number of iterations.
For each image, Ii, in the dataset, do
Step 3. Convert Ii to IDWTi by performing DWT to the desired level.
Step 4. Exact the LL sub-band from IDWTi.
Step 5. Dither the pixels to binarize the extracted LL sub-band into black and white bits (i.e. 0s and 1s).
Step 6. Produce IDi by upsampling the dithered LL sub-band to match the size of ST.
Step 7. Generate the verification string Vi using IDi ⊕ ST.

Algorithm 2. Algorithm for generating verification strings for separate color channels.

Input: A QR code, S, a key, K, and images in a dataset, Ii, where i ∈{1, 2, …, n}.
Output: Verification strings, Vi, where i ∈{1, 2, …, n}
Step 1. Encrypt information in S by XORing the random bits in K with the modules in S to produce SE.
Step 2. Generate a chaotic image ST by scrambling the bits in SE using Arnold transform for a number of iterations.
For each image, Ii, in the dataset, do
Step 3. Split Ii into its corresponding color channels, Iij, where j ∈{r, g, b} for red, green and blue color channels.
For each image color channel, Iij, do
Step 4. Convert Iij to IDWTij by performing DWT to the desired level.
Step 5. Exact the LL sub-band from IDWTij.
Step 6. Dither the pixels to binarize the extracted LL sub-band into black and white bits (i.e. 0s and 1s).
Step 7. Produce IDij by upsampling the dithered LL sub-band to match the size of ST.
Step 8. Generate components of the verification string Vij using IDij ⊕ ST.
Step 9. Produce the verification string Vi by appending the components Vij for all j ∈{r, g, b}, i.e. Vi = Vir + Vig + Vib.
3.1.2. Verifying image fidelity
The verification strings can be used to verify the visual fidelity of images in the dataset, and to determine whether any of the images have been perturbed. An overview of this process is depicted in Fig. 3. Similar to the process for generating verification strings, each image in the dataset is decomposed into DWT components. The LL sub-band at the pre-determined level is dithered and upsampled using nearest neighbor sampling to match the size of the verification string. Each pair of these are XORed, i.e. Vi ⊕ IDi, to produce S′Ti. Note that if an image was perturbed, S′Ti≠ST for that image.

Fig. 3
Download : Download high-res image (234KB)
Download : Download full-size image
Fig. 3. Overview of the process to verify each image in a dataset.

To recover the QR code, Arnold transform is inversed and K is used for decryption to produce a recovered QR code for each image, SRi. If no images in the dataset were altered, the QR code can be recovered perfectly for all images in the dataset. However, if an image was perturbed, SRi for that image will result in a noisy QR code.

Algorithm 3 details the steps involved in verifying the fidelity of images in a dataset. The required inputs to the algorithm are the key, K, that was used for encryption, the verification strings and the images in the dataset. For each image in the dataset, the algorithm outputs the recovered QR code. The visual fidelity of each image can be determined based on whether the resulting SRi is a clean or a noisy QR code. If SRi is noisy, a clean reconstructed QR code can be obtained by averaging the black and white pixels per module. If there are more white pixels, the module should be white, and vice versa. If the verification strings were generated based on separate color channels, Algorithm 4 presents steps in the process to reconstruct QR codes for verifying the color channels of the images.


Algorithm 3. Algorithm for verifying image fidelity.

Input: The key, K, verification strings, Vi, and images in a dataset, Ii, where i ∈{1, 2, …, n}.
Output: Recovered QR codes, S′Ri, where i ∈{1, 2, …, n}
For each image, Ii, in the dataset, do
Step 1. Convert Ii to IDWTi by performing DWT to the desired level.
Step 2. Exact the LL sub-band from IDWTi.
Step 3. Dither the pixels to binarize the extracted LL sub-band into black and white bits (i.e. 0s and 1s).
Step 4. Produce IDi by upsampling the dithered LL sub-band to match the size of ST.
Step 5. Generate S′Ti using Vi ⊕ IDi.
Step 6. Inverse the Arnold transform on S′Ti to produce S′Ei.
Step 7. Decrypt S′Ei using K to recover the QR code, SRi.
3.2. Method 2 - Aggregate Verification String (AVS) method
The main drawback of the previously described LVS method, is its requirement to have to store the verification strings of all images in a dataset. Storage requirements are proportional to the number of images in a dataset. While the LVS method may seem impractical, it can be used to independently identify any alteration in an individual image Ii, because the verification string Vi must be stored. Hence, when storage space is not an issue (such as the use of cloud storage), this method is feasible and attractive.


Algorithm 4. Algorithm for verifying image fidelity for separate color channels.

Input: The key, K, verification strings, Vi, and images in a dataset, Ii, where i ∈{1, 2, …, n}.
Output: Recovered QR codes, S′Ri, where i ∈{1, 2, …, n}
For each image, Ii, in the dataset, do
Step 1. Split Ii into its corresponding color channels, Iij, where j ∈{r, g, b} for red, green and blue color channels.
For each image color channel, Iij, do
Step 2. Convert Iij to IDWTij by performing DWT to the desired level.
Step 3. Exact the LL sub-band from IDWTij.
Step 4. Dither the pixels to binarize the extracted LL sub-band into black and white bits (i.e. 0s and 1s).
Step 5. Produce IDij by upsampling the dithered LL sub-band to match the size of ST.
Step 6. Extract Vij from Vi.
Step 7. Generate S′Tij using Vij ⊕ IDij.
Step 8. Inverse the Arnold transform on S′Tij to produce S′Eij.
Step 9. Decrypt S′Eij using K to recover the QR code, SRij.
In the event that storage space is limited, the second method address this issue. The aim of the second method is to reduce the required storage size to the size of a single verification string. In addition, the use of trapdoor permutation (which can be implemented using an RSA algorithm for example) and a one way hash function, is used to provide additional layers of security.

An overview of the algorithm is depicted in Fig. 4. The initial process is similar to the LVS method. After the dithering process, IDi is obtained, which will be XORed with the key Ki and the QR code S. Hence, IEi ← S ⊕ Ki ⊕ IDi. However, unlike the LVS method, 
, where the input of the Evaluate algorithm is obtained from the hash value of the IE(i−1), which is the output of IEi from the previous image block. For K1 of the first image block, an initial string such as the hash value of S or any other initial vector can be used. With this chaining mechanism, the final output block will only comprise of a single verification string, V, regardless the number of images involved. The algorithm for the AVS method is provided in Algorithm 5.


Algorithm 5. AVS algorithm.

Input: A QR code, S, the first key, K1, and images in a dataset, Ii, where i ∈{1, 2, …, n}.
Output: A verification string, V
For each image, Ii, in the dataset, do
Step 1. Convert Ii to IDWTi by performing DWT to the desired level.
Step 2. Exact the LL sub-band from IDWTi.
Step 3. Dither the pixels to binarize the extracted LL sub-band into black and white bits (i.e. 0s and 1s).
Step 4. Produce IDi by upsampling the dithered LL sub-band to match the size of S.
Step 5. Generate IEi using S ⊕ Ki ⊕ IDi. If i≠1, Ki = hash(IE(i−1)).
For the last image, In, output the verification string, V = IEn.
Fig. 4
Download : Download high-res image (222KB)
Download : Download full-size image
Fig. 4. Overview of the AVS method.

Since trapdoor permutation is used in the construction, the verification will require the use of the Invert(t, ⋅) algorithm, which cannot be executed without the value of the trapdoor. Therefore, when verifying the fidelity of images in a dataset, if the QR code cannot correctly be recovered at any image block, this means that the image has been altered and the dataset is not intact. On the other hand, if the QR code can be recovered in all image blocks, the dataset is completely intact.

In summary, to highlight the differences between LVS and AVS, in the AVS method, a constant size output is achieved regardless the number of the image blocks. Furthermore, the security level is enhanced by incorporating a symmetric encryption scheme in LVS with a trapdoor permutation algorithm in AVS.

3.3. Security
The security of the AVS algorithm is outlined as follows. First of all, let's recap the algorithm involved.

1.
IDWTi ← DWT(Ii).

2.
IDi ←Dither and Up-sample(IDWTi).

3.
IEi ← S ⊕ Ki ⊕ IDi, where S is obtained from the QR code, and the key is set to be Ki.

4.
Ki ← Evaluate(s, H(IE(i−1))), where (s, t) is a pair of public/private key of the user.

Note that the Evaluate(⋅, ⋅) function is a trapdoor permutation. The security of this system relies on the difficulty of finding the inverse of this Evaluate algorithm, and hence, the trapdoor permutation. If the trapdoor permutation is implemented with RSA, then≔for a modulus N. This means, the attacker must be able to find the Invert(t, ⋅) function, which is defined as≔where st ≔ 1 (mod φ(N)). Hence, as long as the RSA problem is secure, then the above algorithm is secure.□

It can further easily be deduced that given the final verification string V (see Fig. 4), it is infeasible to backtrack and obtain all of the other verification parts for each image Ii without knowledge of the secret trapdoor. Hence, the security of the scheme is ensured. ▪

4. Experiments and discussion
This section presents example results obtained from experiments that test the proposed methods. To perform the tests, experiments were conducted by implementing the algorithms and a number of different test images were used in conjunction with the implementations. The test programs were implemented using the OpenCV library.

Fig. 5(a) shows an example of S, i.e. a QR code containing a secret message. The QR code is of version 3, which contains 29 × 29 modules, with error correction level H. An example of S after encryption, i.e. SE, is shown in Fig. 5(b). Fig. 5(c) was produced by scrambling the bits in SE using Arnold transform to generate ST. Note that in line with Algorithm 1 and Algorithm 2, the same ST was used in the experiments to obtain the results shown in Table 1, Table 3.

Fig. 5
Download : Download high-res image (398KB)
Download : Download full-size image
Fig. 5. Example of S, SE and ST (a) QR code containing a secret message; (b) the encrypted QR code; (c) result after Arnold transform.


Table 1. Example images that were produced as a result of Algorithm 1.




Table 2. Examples resulting from Algorithm 3 on perturbed images.




Table 3. Example images resulting from Algorithm 2, which separates the image color channels.



Table 1 shows examples of some test images that were used in the experiments. These images are commonly used for testing image processing techniques; namely, the lena, peppers and mandrill images, respectively. The table also shows the DWT LL2 sub-band after the dithering and upsampling processes for the respective test images, i.e. IDi. Finally, the table shows visual depictions of the verification strings Vi for the respective test images, which were generated as a result of IDi ⊕ ST. Note that the verification strings are bit strings consisting of 0s and 1s, the table depicts these in a visual form (i.e. 0s are represented using black pixels, and conversely, 1s are represented as white pixels).

Table 2 shows example results that were obtained when attempting to verify the image fidelity of perturbed test images. This shows results of Algorithm 3, when tested on images that underwent basic perturbations. The test image was then perturbed using three common techniques; namely, JPEG compression, noise and blurring. For JPEG compression, a value of 95% quality was used in the experiments. To test perturbations resulting from the introduction of noise, 5 random pixels in the test images were altered. For blurring, Gaussian blurring was used with σ = 0.5. These are small perturbations that typically go unnoticed from the human visual perspective.

The first column in the table shows the test images that were used, while the second column shows the type of perturbation that was performed on the image before running the verification process. The third column in Table 2 depicts the results obtained when attempting to recover the QR code (i.e. SRi) from the image that underwent perturbation. Note that the recovered QR codes are all noisy, hence, this shows that the image was perturbed and is different from the original image. It can be seen that even small changes to the image (e.g., 5 random pixels), results in a noisy recovered QR code, which demonstrates the effectiveness of the proposed approach in ascertaining whether an image in a dataset has been perturbed. The final column shows QR codes that were reconstructed from SRi, which were obtained as a result of cleaning up the noisy QR code by averaging the black and white pixels per module (i.e. if there are more white pixels in the noisy QR code constituting a module, the module is reconstructed as a white module, and vice versa). Note that the gray modules represent modules that were incorrectly reconstructed when compared with the original QR code, S.

Table 3 shows example images obtained from running Algorithm 2 on a color image. The first column of the table shows the color image that was used, and the second column shows images of the respective color channels. Results of dithering and upsampling the DWT LL2 sub-band for each color channel of the image is shown in the third column, and a visual depiction of the generated verification strings is shown in the last column.

The purpose of Algorithm 4 was to verify whether a color image was perturbed using different color channels. Example results obtained from an experiment to test this is presented in Table 4. The previously described perturbations were done on the color image, prior to the verification process. The images of the different color channels can be seen in the first column of Table 4, while the second column shows the type of perturbation that was done on the image. Columns three and four show the recovered QR code, SRi, and reconstructed QR code, respectively. It can be seen from the results in the third column that the recovered QR codes are all noisy as a result of perturbations made to the image. Hence, it can be determined that the image is not intact.


Table 4. Examples for different image color channels resulting from Algorithm 4 on a perturbed color image.



5. Conclusion
In this paper, the problem of protecting image-based machine learning datasets against alteration by an adversary was examined. Two methods were presented, namely, the Linear Verification String (LVS) method and the Aggregate Verification String (AVS) method. The purpose of these methods is to provide mechanisms for verifying the visual fidelity of images in a dataset without the need to store and use the original dataset for verification.

In both the LVS and AVS methods, verification strings were generated from the important visual content of the images and associated with QR codes. In the LVS method, a verification string is generated for each image in a dataset. For verification, a verification process is used whereby each verification string is used to verify the visual fidelity of individual images in the dataset. However, the drawback of the LVS method is that the storage requirement increases linearly with the number of images in a dataset. To overcome this problem, the AVS method was proposed. The AVS method only requires the storage and use of a single verification string to verify whether an entire image dataset is intact. Nonetheless, in the AVS method, one can only ascertain whether the dataset has been altered, but cannot determine the visual fidelity of individual images.
