MapReduce is a common framework that effectively processes multi-petabyte data in a distributed manner. Therefore, MapReduce is widely used in heterogeneous environments, such as cloud, to provide performance adequate for system needs. Despite the MapReduce benefits, tweaking the system configuration to achieve the maximum performance is still challenging and needs deep expertise. Besides, some new MapReduce security issues, which has not been well-addressed yet, are recently raised. In this paper, we present a performance-aware and secure framework, named , to minimize the makespan of the tasks while considering task security constraints. Inspired by the  algorithm, first, we introduce , which proposes a two-stage static scheduler in Map and Reduce phases, respectively, to minimize makespan while considering network traffic. Plus, 
 introduces a mathematical optimization model of the proposed scheduler aiming to estimate the system performance while considering security constraints with an error of less than 2%. The experimental results demonstrate that  outperforms Hadoop-stock in terms of makespan and network traffic by 29% and 31%, respectively, for the tasks running in heterogeneous environments.

Previous
Next 
Keywords
Bigdata

Hadoop

MapReduce

Scheduling

Makespan

Security

Optimization model

Heterogeneity

1. Introduction
Rapidly growing data production necessitates the non-classical processing approach to handle the processing of such massive data volume. Hadoop (2011) as a processing asset of cloud computing is a well-known open-source implementation of the MapReduce programming model for handling Very Large Scale Data (so-called VLSD) in parallel (Dean and Ghemawat, 2008). Each MapReduce job consists of two dependent tasks, Map and Reduce tasks, where the former feeds the latter to provide output (White, 2012). Since MapReduce is a data parallelism level framework, the input dataset is chunked into equal parts known as splits in the disposal to the Map tasks. Each Map task performs three activities on the data: Mapping, Partitioning, and Sorting and then produces a list of intermediate key/value pairs results as Map Output Files (so-called MOF). Similarly, each Reduce task performs three activities on the intermediate values related to the same key: Shuffling/Copying, Merging, and Reducing. The values are shuffled through the network, grouped and processed by the same user-defined Reduce function, and produces the intermediate key with the list of its values and generate the final results as Reduce Output Files (so-called ROF) (Hadoop, 2011, Dean and Ghemawat, 2008, White, 2012).

There is extensive work exploiting the performance of MapReduce. Task scheduling with considering flow scheduling (network traffic) are two essential factors to achieve the high performance, i.e., maximizing the number of jobs that can be run in a distributed system by meeting the time requirements for all jobs (Wang et al., 2018, Al-Fares et al., 2010). A preferred task scheduling results in higher task throughput, measured by the Job Completion Time (JCT). Moreover, in the shuffle phase, the data transmission time from a source to a destination across the network, i.e., Flow Completion Time (FCT), directly influences JCT (Guo et al., 2016). Therefore, in a data-intensive distributed system like Hadoop (2011) with multiple jobs, designing efficient scheduling is the main concern (Ghodsi et al., 2011, Grandl et al., 2016, Bodík et al., 2012, Gao et al., 2016).

Hadoop offers three schedulers including, FIFO (First In First Out) (Hadoop, 2020), HCS (Hadoop Capacity Scheduler) (Hadoop, 2020), and HFS (Hadoop Fair Scheduler) (Hadoop, 2020). However, the homogeneity assumption in the proposed schedulers degrades the performance of the MapReduce framework. Nowadays, in favour of hardware technology advancement, there is a specific diversity in the hardware of real-world systems that contributes to a high-performance environment for MapReduce execution. Therefore, working with heterogeneous clusters would be a major goal to increase the scope of MapReduce (Naik et al., 2019).

Task scheduling studies usually focus on only the assignment of Reduce tasks with the belief that Map scheduling is determined by the initial data distribution of the file system hosted on the MapReduce compute nodes (Verma et al., 2013, Tian et al., 2016, Zhu et al., 2014, Jiang et al., 2019, Jiang et al., 2017, Hashem et al., 2018). However, in the cloud-based MapReduce or high-performance environments, this assumption is not valid since the input data often resides in a remote shared file system such as Amazon S3 (Cloud, 2011). Therefore, since all the data is loaded from a remote location, the scheduling of Map tasks also becomes important (Selvitopi et al., 2019). Additionally, by growing MapReduce application in sensitive fields such as economic, health, and scientific research, MapReduce’s security aspect has aroused more and more attention while a few studies have been devoted to investigating MapReduce’s security (Reddy et al., 2020, Naisuty et al., 2020, Perwej, 2019). If a hundred servers linked together in a typical application stack, many questions will arise about how to protect such a valuable asset (Spivey and Echeverria, 2015). It is apparent that managing petabytes of data in a single centralized cluster can be dangerous (Alapati, 2016).

We encounter with an emergent need to exploit the operation of security services to protect security-sensitive applications from attacks if we intend to deploy MapReduce as a data processing service over open systems such as service-oriented architecture, cloud computing, and volunteer computing (Ahmad et al., 2018). Although adding the security mechanisms increases an overhead, i.e., the computation time of tasks but, it guarantees a security level for MapReduce tasks execution. Hadoop assumes that the entire cluster of machines and all users accessing it are part of a trusted network by establishing a secure stance (Wei et al., 2009). This effectively means that Hadoop did not have strong security measures to enforce, well, much of anything. The researchers have underestimated these problems in most of the conventional MapReduce scheduling algorithms. The main contributions of our work are as follows:

1.
Our study is the first one to the extent of our knowledge that simultaneously addresses both Map and Reduce tasks scheduling while considering makespan and security.

2.
 leverages performance-aware two-stage scheduling forMapReduce tasks in heterogeneous environments.

3.
 is an optimization model proposed for reducing total completion time of tasks (makespan) and increasing the security level of their execution. It is robust against diverse types of attacks.

4.
 considers the network traffic of transferring the output of Map tasks and applies a flexible Reduce task-partition binding rather than static scheduling of Reduce tasks.

We organize the paper as follows: Section 2 introduces the Hadoop core components. Section 3 presents a review of current literature. Section 4 describes the parameters and models of the proposed methodology. The proposed framework is discussed in Section 5. The results and discussion are presented in Section 6, and the conclusion and future work are provided in Section 7.

2. Background
Apache Hadoop assists the distributed storing and processing of big datasets using Google’s MapReduce and Google File System (GFS) models. The prevalence of Hadoop in industries and academic communities is due to its open-source solution (Shabestari et al., 2019). The Hadoop framework is classified as described in Section 2.3. Section 2.1 to Section 2.3.

2.1. Hadoop file system
Hadoop Distributed File System (HDFS) as a fault-tolerant and scalable system has developed for running the big data on the commodity hardware (Rao and Reddy, 2012). Fig. 1 shows the master–slave architecture of HDFS. The NameNode, as a master node, controls the access of clients to data and performs the namespace operations to determine the location of blocks on the DataNodes (Rao and Reddy, 2012). The DataNodes, as the slave nodes, are responsible for the clients’ read and write operations (Kao and Chen, 2016). The default replication factor of Hadoop is three and the HDFS’s placement policy is to put one replica on one node in the local rack, another copy on a node in an off-rack (remote) node, and the last one on a different node in the same remote rack to establish the fault tolerance (Rao and Reddy, 2012, Maleki et al., 2018). Besides, Hadoop employs a backup NameNode called JournalNode as the checkpointing server to take checkpoints of the file system metadata present on NameNode where the single point of failure occurs (Kao and Chen, 2016).

2.2. MapReduce programming model
MapReduce programming model is expressed as two computational functions: Map and Reduce (Glushkova et al., 2019). The Map stage is partitioned into Map tasks, and the Reduce stage is partitioned into Reduce tasks (Verma et al., 2011b). Fig. 2 illustrates the MapReduce architecture. A pair of key and data value, with a type in one data domain (so-called Split) is fed as input to the user-defined Map function, and a list of pairs in a different domain (so-called Partition) is produced as MOF output: Map (k1, v1)  list (k2, v2). The values/sub-partitions belonging to the same key k2 are grouped, form the partition, and then passed to the user-defined Reduce function. The Reduce stage is divided into three phases: Remote-fetch (so-called Shuffle), Merge and Reduce (Pansare et al., 2011). Shuffle phase collects intermediate key k2 with a list of values all over the machines hosting k2, Merge phase sorts them, and Reduce phase makes a new list of values in the same domain as ROF output: Reduce (k2, list (v2))  list(v3). The ROFs are generally written back to HDFS.

2.3. YARN cluster manager
In 2013, the second version of Hadoop MapReduce 2.0, Yet Another Resource Negotiator (YARN), was proposed by Apache to overcome limitations such as scalability, reliability, and resource utilization (Inoubli et al., 2018). Despite the Hadoop MapReduce version1, where the centralized resource management and job/task scheduling are utilized, YARN allows a global ResourceManager (RM) and per-application ApplicationMaster (AM) (Cai et al., 2017) as detached services. Besides, YARN adopts a resource abstract called container for resource provisioning, which encapsulates multi-dimensional resources of a node, including CPU and memory. As shown in Fig. 3, the Resource Manager receives and runs MapReduce jobs. The per-application Application Master obtains resources from the ResourceManager and works with the Node Manager(s) to execute and monitor the tasks (Inoubli et al., 2018). YARN provides flexibility to run any application in a cluster (Fu et al., 2017). Many programming frameworks have validated that they have already attracted by YARN. For example, batch processing is performed using MapReduce, online processing through Apache Storm (Spark, 2020), and in-memory processing using Apache Spark (Storm, 2020) via YARN (Soualhia et al., 2017).

3. Related work
The MapReduce tasks scheduling approaches are divided into three categories according to the solving strategy, the number of optimization objectives, and the heterogeneity of the environment (workload and workers) considered by the scheduler designer (Maleki et al., 2019b). We focus on the scheduling methods aiming at minimizing makespan and/or increasing security in heterogeneous environments. Therefore, we extrapolate Fig. 4 for our discussion based on and survey the current work in each category. Section 3.1 to Section 3.3 explain the related papers in these three categories.

3.1. Entity-level scheduling
Numerous studies proposed different task schedulers to improve MapReduce performance. According to the taxonomy shown in Fig. 4, Hadoop offers three scheduling levels:  User-level,  Job-level, and  Task-level. HFS and HCS as user-level scheduling and FIFO and Priority scheduler as job-level scheduling are the Hadoop built-in scheduling algorithms. The scheduling tasks of a job by considering different criteria such as performance (makespan), data locality, network traffic, cost, and so on, is the third level and fine-grained of scheduling. There are three levels of tasks scheduling including: Map, Reduce, and Speculative tasks scheduling. The default Hadoop scheduler works based on the data locality criteria at the Map task scheduling level, i.e., it selects the local Map tasks for a given resource by enquiry the meta-data service to find the hosted data chunks. Besides, Hadoop randomly selects the Reduce tasks of the selected job to be scheduled on the available resource. However, the built-in Hadoop schedulers are designed for homogeneous environments and do not consider the performance of the resources.

To reach better performance, we chose the Heterogeneous Earliest Finish Time (HEFT) algorithm (Topcuoglu et al., 2002) as an inspiring algorithm in the context of MapReduce. The HEFT algorithm is one of the most classic algorithms in static task scheduling with an effective solution for the DAG scheduling problem on heterogeneous systems. HEFT has robust performance, low complexity time, and the ability to give stable performance over a wide range of graph structure (Topcuoglu et al., 2002, Tong et al., 2019, Sumathi and Poongodi, 2018, Bittencourt et al., 2010). Some researches (Bittencourt et al., 2010, Dubey et al., 2018, Samadi et al., 2018, Xu et al., 2014) have worked on the improvement of the HEFT algorithm, and few papers (Tong et al., 2019, Tang et al., 2016, Caruana et al., 2017) have applied the idea of the original HEFT algorithm in the context of the MapReduce scheduling. We also benefit the idea of original HEFT algorithm for two reasons; (1) improving the HEFT algorithm using extra information, i.e., learning and meta-heuristics mixed techniques (Tong et al., 2019, Bittencourt et al., 2010, Samadi et al., 2018, Xu et al., 2014), and so on, increases the scheduling time complexity (Bittencourt et al., 2010), (2) we propose a mathematical model based on the original HEFT for our framework, SPO, that lets us analyse the system accurately (analyzability). However, although the learning algorithms work well, it is well-known that analysing their good behaviour is a challenging problem because the interaction of their components is highly nonlinear, complex, and stochastic (Yang, 2011). Since our model is designed for processing massive data-intensive jobs, adopting the idea of HEFT is the excellent choice for SPO.

HEFT schedules a set of dependent tasks onto a network of heterogeneous resources taking communication time into account. HEFT consists of two phases: (1) Prioritizing tasks; (2) Assigning tasks to resources. In the first phase, each task is given a priority, and in the second phase, tasks are assigned to resources. The task with the most priority of all dependent tasks that are finished is scheduled on the worker. It will provides the earliest finish time for that task. This finish time depends on the communication time to send all necessary inputs to the resource, the computation time of the task on the resource, and the time when that processor becomes available. HEFT utilizes an insertion-based policy sufficiently that fills sized gaps between scheduled tasks.

Sumathi and Poongodi (2018) proposed an improvement in the HEFT algorithm based on the fact that the tasks of a workflow have different resource requirements for successful execution. Therefore, the authors included different factors such as RAM, storage memory, and inter-node bandwidth in addition to the CPU resource in the proposed algorithm for efficient scheduling with promising results. Bittencourt et al. (2010) provide an improvement of Heterogeneous Earliest Finish Time (HEFT) where rather than considering only the estimation of a single task for the locally optimal decisions, it considers look-ahead in the schedule and includes the information that affects the children of the task by the decisions made. The key idea of the paper is to boost the process of scheduling tasks in the HEFT algorithm by considering information about the descendants of a task. In Dubey et al. (2018), the authors modified the HEFT algorithm in terms of load balancing called Modified-HEFT. The Modified-HEFT distributes almost evenly the workload among the processors and reduces the makespan time of applications. Samadi et al. (2018) have proposed an enhancement variant of the heterogeneous earliest finish time algorithm (E-HEFT) under a user-specified financial constraint. To achieve a good load balancing across the virtual machines while trying to minimize the makespan, the authors defined the load threshold of each machine based on both processing speed and storage capacity. Afterwards, they define dataset clustering based on their dependency, prioritize the tasks, and select the resources for tasks based on the Matching Game theory. A stochastic search method based on the genetic algorithm is proposed by Xu et al. (2014) to enhance the HEFT algorithm. The authors considered multiple priority lists to prioritize sub-tasks and then map the tasks to the heterogeneous computing resources based on the heuristic-HEFT search.

Tong et al. (2019) have proposed a task scheduling algorithm called QL-HEFT that combines Q-learning with HEFT algorithm to reduce the makespan. The algorithm uses the upward rank (
) value of HEFT as the immediate reward in the Q-learning framework. The agent can obtain better learning results to update the Q-table through the self-learning process. However, an excessively large Q-table producing an excessive update time which increases the schedule order time. Tang et al. (2016) have proposed MRWS (MapReduce-enabled workflow scheduler), which is a HEFT-based job level scheduler for minimizing performance in heterogeneous environments. The jobs are modelled as a MapReduce workflow and scheduled in two phases, including, prioritizing jobs and resource allocation. The priority scheduler works based on the type of the jobs in prioritizing phase and in the second phase, the data locality criteria are considered for scheduling the jobs on the resources. Caruana et al. (2017) have proposed a resource-aware job scheduler, gSched, that takes into account both the resource heterogeneity and running costs in cloud computing environments. Caruana et al. (2017) benefits the idea of the HEFT algorithm and form an Estimated Computing Time (ECT) table that schedules the CPU-bound and IO-bound tasks on the resources based on the task bias identification using machine learning. However, the scalability of gSched is low, and it requires enhancements for being applicable in large-scale environments.

In all the work mentioned above, HEFT has been employed in a coarse-grained problem, i.e., the job-level scheduling while we apply the HEFT algorithm in a fine-grained problem, i.e., the Map and Reduce task-level scheduling. The red highlighted path in Fig. 4 shows our choice in this category.

3.2. Optimization approach
The scheduling methods try to find the solution/solutions aiming at the optimization of an objective or multiple conflicting objectives with/without constraints. Since the objectives of the current study are makespan and security, we have considered the studies which leastwise optimize makespan, security, or both.

Nita et al. (2015) have proposed a multi-objective MapReduce tasks scheduler called MOMTH to fulfil both user’s objective (hit the deadline of jobs) and cloud provider’s goals (resource efficiency) through the constraints (deadline and budget). The best assignment between jobs and resources is selected based on the summation of the predefined weighted service functions. Authors in Hashem et al. (2018) have proposed a user-benefit two-objective scheduler that minimizes the job completion time and usage cost of cloud services in MapReduce-based environments. The scheduler defines the priority of jobs based on the earlier finish time of their tasks. Voicu et al. (2014) have proposed MOMC, a multi-objective and multi-constrained scheduling algorithm that optimizes the workload of the cluster while meeting the budget and deadline constraints. In Wang et al. (2014a), authors have proposed a double-fitness Adaptive Genetic Algorithm that minimizes makespan and balances the load of the cluster. Authors have used a greedy algorithm to initialize the population and calculated the final results using weighted objective-functions. Zuo et al. (2015) have proposed a multi-objective optimization scheduler based on the Ant Colony algorithm. The proposed scheduler uses a resource cost model to optimize the user’s objectives, i.e., makespan and resource usage cost based on the user-defined constraint, i.e., budget.

A Security-Oriented MapReduce (SOMR) infrastructure has been proposed by Zhao et al. (2017) that integrates the big-data processing framework, key management system, and trusted computing infrastructure to guarantee the persistent security on the user data and processing. Hu et al. (2017) have proposed a multi-objective heuristic algorithm with three components. The proposed scheduler optimizes the three objectives, i.e., makespan, data locality, and security. In Chhabra and Singh (2016) authors have presented a data leakage detection, S-MAX, that provides a secure environment by avoiding the data leakage. Authors reduce the size of data to identify data leakage and faulty agent of the system with high probability. Roy et al. (2010) have proposed a new mandatory access control integrated with differential privacy. The proposed secure solution enables the privacy-preserving of MapReduce computations without the need to audit untrusted code and prevents information leakage through the data provider’s policy. Azizi et al. (2019) has proposed a methodology of security analysis that locates and extracts data probably related to attacks made by malicious users who intend to compromise a system. Through a process of learning using the collected data, authors can identify, predict attacks, or detect intrusions. Authors have presented a security and privacy layer model between the Hadoop file system (HDFS), and MapReduce Layer called Secured MapReduce (SMR) Layer in Jain et al. (2019). The main advantage of this model is to promote data sharing for knowledge mining. This model creates a privacy and security guarantee, resolve scalability issues of privacy and maintain the privacy-utility tradeoff for data miners. In this SMR model, running time, information loss, and CPU and memory usage have a remarkable improvement.

Table 1 shows the pros and cons of the mentioned optimization solutions. All the methods lack to comprehensively consider the time spent in all three stages of MapReduce, i.e., Map, Shuffle, and Reduce while minimizing the makespan. Moreover, although the papers that addressed security as the objective, they have not considered the risk-limitation as a constraint. Therefore, we focus in particular on the mono-objective MapReduce scheduling with constraints by introducing a mathematical model aiming at optimizing makespan as objective while considering the security and risk-limitation as constraints. Our mathematical model, 
 is a Mixed Integer Non-Linear Programming (MINLP). The blue highlighted in the taxonomy shows our choice in this category. Table 1 shows that 
 has more advantages compared to the other models.


Table 1. Comparison of optimization approaches.

Optimization approach	Advantages	Disadvantages
No considering Shuffling time in makespan minimization.
Optimizing makespan with budget and deadline constraints.	No considering security constraints.
MOMTH (Nita et al., 2015)	Load balanced.	No considering contradicting objectives.
Static model (known number of Map and Reduce tasks).
No considering of job type (CPU/IO-intensive).
No considering constraints.
Minimizing makespan and cost.	No considering network time in makespan minimization.
TARGIO (Hashem et al., 2018)	Scalable.	No considering contradicting objectives.
Static model (known number of Map and Reduce tasks).
No considering of job type (CPU/IO-intensive).
No considering Shuffling time in makespan minimization.
Optimizing makespan with budget and deadline constraints.	No considering security constraints.
MOMC (Voicu et al., 2014)		No considering contradicting objectives.
Static model (known number of Map and Reduce tasks).
No considering of job type (CPU/IO-intensive).
Minimizing makespan and optimizing load balancing.	No considering security constraints.
JLGA (Wang et al., 2014a)	Solving using Adaptive Genetic Algorithm (AGA).	Static model (known number of Map and Reduce tasks).
No considering of job type (CPU/IO-intensive).
Scheduling time complexity.
It is assumed that security is met.
Optimizing performance and cost.	No considering network time in makespan minimization.
PBACO (Zuo et al., 2015)	Solving using Improved Ant Colony Algorithm (IACA).	Static model (known number of Map and Reduce tasks).
Considering job type.	Scheduling time complexity.
No considering heterogeneous environment.
Minimizing makespan with data locality and security constraints.	Only confidentiality security mechanism has been applied.
SMSO (Hu et al., 2017)	Load balanced.	No considering the network time.
Static model (known number of Map and Reduce tasks).
No considering of job type (CPU/IO-intensive).
Time minimization is not considered.
Maximizing security with load balancing constraint.	No considering heterogeneous environment.
S-MAX (Chhabra and Singh, 2016)	Load balanced.	No Integrity security mechanism has been applied.
Static model (known number of Map and Reduce tasks).
No considering of job type (CPU/IO-intensive).
No risk analysis.
No considering makespan minimization.
Maximizing security.	No considering heterogeneous environment.
SMR (Jain et al., 2019)	All three security mechanism have been considered.	Static model (known number of Map and Reduce tasks).
Scalable.	No considering of job type (CPU/IO-intensive).
Considering all three stages of time for the makespan	
minimization, i.e. Map, Shuffle, and Reduce phase.	
Considering the security and risk rate.	
SPO*	Considering contradicting objectives, i.e., minimize makespan	Performance miss due to
while maximize security with risk-limitation constraint.	the data locality on the Reduce side.
Considering data locality on Reduce side.	Not Load balanced.
A dynamic model since the number of	
Reduce tasks are not known in prior.	
Scalable.	
3.3. Heterogeneity
A large number of studies (Verma et al., 2013, Tian et al., 2016, Zhu et al., 2014, Jiang et al., 2019, Jiang et al., 2017, Hashem et al., 2018, Yao et al., 2019, Wang et al., 2014b, Jeyaraj et al., 2019, Kalra and Singh, 2015) have been tried to minimize the makespan of tasks, and also improve Hadoop performance. We categorized the works into two classes:  Studies that ignore resource and workload heterogeneity,  Studies that consider the heterogeneity in terms of workload and resource.

First Category. Verma et al. (2013) and Tian et al. (2016) proposed a Johnson-based static scheduler method aiming to reduce the makespan of MapReduce jobs. The proposed scheduler is inspired by the two-flow shop problem where the Map and Reduce execution stage is known in prior. Verma et al. (2013) proposed a heuristic Johnson-based method where separated pools (named Balanced Pools) are utilized to minimize the jobs of each pool makespan. Nevertheless, their proposed method is not optimal, and it cannot reduce the overall makespan. Tian et al. (2016) proposed an extended Johnson algorithm that tries to minimize the overall makespan of users’ jobs. The drawback of the proposed scheduler is it puts all types of users’ jobs in one work queue to achieve the minimum makespan, and it divides all the capacity of the cluster among jobs while ignoring the type and data size of jobs. Zhu et al. (2014) proposed an approximation task scheduling algorithm to minimize the total completion time and makespan. Zhu et al. (2014) assumes Map tasks are parallelizable in a homogeneous Hadoop cluster, whereas Reduce tasks are non-parallelizable. The preemption of jobs has been considered to achieve fairness of jobs. Jiang et al. (2019) proposed an online scheduler with the goal of minimizing the makespan of MapReduce jobs. Jiang et al. (2019) has considered preemptive, and non-preemptive Reduce tasks in a homogeneous Hadoop cluster, both. The proposed scheduler is optimal for cluster up to two nodes, while the heterogeneity and scalability are ignored. Besides, the proposed method considers neither the heterogeneity of resources nor jobs.

Second Category. Inspired by the bin packing problem, Jiang et al. (2017) proposed a static task scheduler in order to reduce the makespan while considering the heterogeneity of the cluster. To this goal, first, Reduce tasks with more execution time (Large Reduce First) and their related Map tasks are assigned to the faster nodes. Then, the same process is repeated for other remained Map and Reduce tasks gradually to minimize the makespan. To provide a reduced makespan, Jiang et al. (2017) assumed that Map tasks are parallelizable and can be executed on multiple machines.

Hashem et al. (2018) proposed a multi-objective scheduling algorithm in MapReduce-based cloud environments. The cost of cloud services and the job completion time have been considered into account as the optimization objectives to minimize the makespan. The proposed scheduler achieves higher tasks throughput and cost efficiency, in terms of resource usage by cloud users, compared to FIFO and Fair schedulers. The proposed scheduler is designed for only one job while there are several and different jobs in the MapReduce cluster.

With the hope of improving Hadoop performance in terms of makespan, Yao et al. (2019) proposed a novel batch scheduler for MapReduce jobs. The proposed scheduler uses the information of requested resources, resource capacities and tasks dependency, which makes the tasks’ fitness function for scheduling. Yao et al. (2019) managed to conduct experiments with various workloads but has not considered the resource heterogeneity. Wang et al. (2014b) considered the Map tasks scheduling problem of MapReduce jobs to obtain network traffic and tasks throughput optimally in the heterogeneous environment. The scheduler is based on the MaxWeight policy and the ShortestQueue, that achieves the full capacity region and minimization of the expected number of backlogged tasks in the considered heavy-traffic regime. However, Wang et al. (2014b) has not addressed the Reduce tasks scheduling problem that is the reason for network cost in the shuffling phase.

Jeyaraj et al. (2019) proposed a Map tasks locality-aware scheduler, named TSMJS, to minimize the makespan by alleviating the amount of produced intermediate data in the shuffle phase. The idea is minimizing then on-local Map tasks on a node to provide the least number of combiners because for combining the records in the shuffle phase, a per-combiner memory reservation is required. Jeyaraj et al. (2019) considered the Map tasks scheduling problem in the cloud environment with heterogeneous workloads. However, the Reduce tasks scheduling is not considered in this study. Kalra and Singh (2015) reviews metaheuristic-based scheduling algorithms proposed for the MapReduce jobs. The schedulers such as Genetic Algorithm(GA), Ant Colony Optimization (ACO), League Championship Algorithm (LCA), Particle Swarm Optimization (PSO), and BAT algorithm can find near-optimal solutions in many environments such as Cloud, Grid, and distributed environments for minimizing makespan of jobs. Nevertheless, we leveraged the greedy heuristic solution which is suitable for scheduling problem in a short time since the metaheuristic solutions take a long time to converge, due to the insufficient optimization space or non-optimal fitness function.

The objective of  is different from the methods above, where we aim to minimize the MapReduce job execution time under the security and risk-limitation constraints. We employ the HEFT algorithm to develop a two-level MapReduce scheduling optimization framework by focusing on the heterogeneous resources. The yellow highlighted in the taxonomy shows our choice in this category.

4. Proposed framework preliminaries
Our problem consists in scheduling the MapReduce tasks with security constraints to minimize the execution time, i.e., makespan. This section starts with the presentation of the target system model of a MapReduce-based system environment considered in this paper, followed by a detailed description of the makespan model of the MapReduce jobs. Besides, we consider the security constraint of the execution of jobs and introduce a security model for that. Moreover, we consider the risk analysis, and then we formulate the problem. All the indices, parameters, and variables used in the model are represented in Table 2 for ease of reference.


Table 2. Notations used in the model.

Symbols	Description
Indices
Map task index of job , 
, and 
 is the th Map task
Reduce task index of job , 
, and 
 is the th Reduce task
Job index, 
Core index, 
Partition index, 
Security level index, 
Parameters
Number of Map tasks of job 
Number of Reduce tasks of job 
Number of jobs
Number of total cores
Number of Partitions
The execution time of Map task  of job  on core 
The processing rate of 
The computational degree of Map task function of job 
The distance between core 
 and core 
Maximum number of Reducers that should be assigned to a partition on a node
Data volume of key     generated by Map task  
The traffic from Map task  to Reduce task  for key partition 
The input data traffic of partition  to Reduce task 
The propagation delay between 
 and 
The data transfer time between  and 
The bandwidth between 
 and 
The execution time of Reduce task  of job  on core 
Required level of th security service
Security requirement set of task 
Obtained level of th security service
Security level set of task 
Security overhead of task 
Risk rate constraint
Risk rate of job 
Risk rate of the th security service of task 
Variables
 
 
4.1. System model
A scheduling model consists of  multiple Tasks,  a target computing environment, and  one or more performance criteria for scheduling. Let us suppose that there is a set of Map and Reduce tasks of different jobs in a MapReduce-based environment. The Map tasks run in parallel on a set of heterogeneous resources. Reduce tasks start only once the entire Map tasks finished for the logical correctness of the MapReduce programming model. Makespan is the same Job Completion Time (JCT) by considering one MapReduce job. Therefore, we have four options to minimize the makespan as presented in the following (Fig. 5): (1) Defining the Map tasks optimal number; (2) Defining the Reduce tasks optimal number; (3) Diminishing the last Map task execution time; and (4) Diminishing the execution time of the last Reduce task.

The number of Map tasks is set according to the size of the input split (i.e., HDFS chunks). For example, if we have “”  of the input file and the chunk size of the HDFS is “” , then the number of input splits is , that accordingly, the number of Map tasks of the job 
 is set to . In the case of Hadoop-stock, the number of Reduce tasks (second alternative) is defined based on a user-defined parameter. Thus, the total number of Mappers and Reducers is fixed before the application submission. However,  defines the number of Reducers dynamically based on the size of the Map output partitions. We need to recall that there is a trade-off between performance in terms of reducing storage accesses (due to large sequential I/O) and fault-tolerance in terms of the amount of computation that must be re-performed when a Reduce task is failed. Diminishing the last Map task execution time (option3) and the last Reduce task (option 4) simultaneously, are targeted by , that are explained in Section 5.


Download : Download high-res image (259KB)
Download : Download full-size image
Fig. 5. The minimization problem.

In this study, we consider several common assumptions due to the high complexity of MapReduce job scheduling. The problem is formally described as follows. Assume a set of  different jobs 
, which must be processed on  heterogeneous computing nodes 
 organizing total cores . Each job is assigned to a logical container that is physically distributed among the cores of the nodes. A job is fractional meaning that it can be arbitrarily split between the nodes of its associated container. Therefore, the splits of the same job can be simultaneously processed on different nodes. These splits are known as Map tasks and Reduce tasks that are independent in advanced, executed in parallel, and requires only one core at any given time. Reduce tasks can be launched only if all the Map tasks have been completed.

Consider 
 and 
 are the sets of Map tasks and Reduce tasks of 
 () such that the Map tasks generate a set of partition sizes from 
 as the set of partitions on their local disk after execution. 
 and 
 are the number of tasks in 
 and 
, respectively. The summation of 
 and 
 shows the job size. The number of Map tasks 
 is defined by the size of input dataset 
 uploaded on the HDFS and the size of  while the number of Reduce tasks 
 is specified in the run-time subject to the partition sizes. In our model, we assumed the execution task of a given application is non-preemptive meaning that the Map task or Reduce task is not interrupted (paused or killed) during its processing (Jiang et al., 2017). Besides, the data transfer rate (network bandwidth) between the cluster nodes is stored in matrix 
 with the size of , and the propagation delay of nodes is given in an m-dimensional vector .

Let 
 be the completion time of Reduce tasks of job 
. The goal of this paper is to minimize the makespan, i.e., the time when the last server completes its last task, 
, 
, (1) where there is any security constraint; (2) where there is security constraint.

4.2. Makespan model
The MapReduce execution flow, as shown in Fig. 2, consists of three parts, i.e., Mapping, Networking, and Reduce. For minimizing the makespan of the batch of tasks in a MapReduce-based system, we should consider the execution time of Map tasks, the time required to transfer the intermediate data produced in middle stage across the network, i.e., shuffle phase, and the execution time of the Reduce tasks. Therefore, 
 is the completion time of all MapReduce tasks in a system that can be modelled by a schedule length (makespan) function as Eq. (1): (1)
where 
 is the total execution time of all Map tasks in the Map stage. 
 denotes the time required for transferring the intended data including, the time that the sub-partitions related to a specific partition should be transferred to the resource based on the partition placement strategy and the delay time between two resources. Moreover, 
 is the total execution time for aggregating the partition related to each Reduce task in the Reduce stage.

4.2.1. Map makespan minimization
We use  to represent the objective function i.e. makespan minimization. To formulate the Map makespan minimization problem, we define a binary variable 
 as follows: (2)
 

Since each Map task is non-fractional, i.e., it cannot be split and executed on more than one resource, we define the following constraint. The limitation states that among all the processing cores, there is only one that runs the Map task  of job , and all Map tasks are certainly scheduled on the resources. (3)

The required time for executing the Map task i of job j (
) on resource  with the speed of 
 is calculated by Eq. (4): (4)
 

Since the different jobs have a different computational degree, we consider its effect on Map execution time. Therefore, we define parameter 
 and enter in the Eq. (5). (5)
 

Since the number of resources is less than the number of Map tasks and the Map tasks may run in more than one wave, so having the average time required to process the first wave of Map tasks, we can calculate the required time to process all Map tasks. We show the average time needed to process the first wave of the Map tasks with 
 and is calculated using Eq. (6): (6)
 

Now, with the number of waves of each job that is obtained by dividing the total number of the Map tasks by the total number of resources, we can calculate the end time of Map stage from the following equation (Eq. (7)): (7)
 

Therefore, in the first part of the objective function we should minimize the Eq. (8): (8)

4.2.2. Shuffle time minimization
To formulate the traffic minimization problem in the shuffle phase, we first consider the data forwarding between the Map stage and the Reduce stage. We define a binary variable 
 as described in Eq. (9): (9)
 

But among all the Reduce tasks, there is only one for which 
 equals one, i.e., it receives the key , and the other Reduce tasks will not receive that key. So, there is the following constraint (Eq. (10)): (10)

Therefore, we use 
 to show the number of Reduce tasks which process on this partition and are placed on the same host, i.e., 
. Thus, the constraint is converted as Eq. (11): (11)

We let 
 denote the traffic from Mapper 
 to Reducer 
 for key 
, which can be calculated by Eq. (12): (12)

The data transfer time between the two resources comes from the Eq. (13): (13)
 
where 
 is the propagation delay between two resources and 
 is the bandwidth between two resources. Since the phase of the shuffling is part of the Reduce task, and the Reduce task starts only after the Map tasks are completed, the average time of traffic transfer of Map tasks of a job (
) is calculated using Eq. (14): (14)
 

And for  job, the total network traffic time is calculated from the Eq. (15): (15)
 

Therefore, in the second part of the objective function, we should minimize the following statement (Eq. (16)): (16)

4.2.3. Reduce makespan minimization
We define a binary variable called 
 that if the Reduce task  of job  is executed on resource  its value is one, otherwise it is zero (Eq. (17)). (17)
 

The following constraint (Eq. (18)) states that among all the  resources, there is only one that executes the Reduce task  of jab , and unquestionably all the Reduce tasks are scheduled on the resources. (18)

The required time for executing the Reduce task  of job  (
) on resource  with the speed of 
 is calculated by Eq. (19): (19)
 
where 
 determines the effect of the computational degree of job in Reduce task execution time.

Similarly, we can calculate the average time required to process the first wave of Reduce tasks represented with 
 by Eq. (20): (20)
 

Now with the number of waves obtained by dividing the total number of Reduce tasks by the total number of processing elements, we can calculate the end time of the Reduce tasks of jobs from the Eq. (21): (21)
 

Therefore, in the third part of the objective function, we should minimize the Eq. (22): (22)

4.3. Security model
Since various security threats are a big concern of distributed computing system, it is mandatory to deploy security services to protect security-critical MapReduce applications from being attacked (Perwej, 2019, Spivey and Echeverria, 2015). Snooping, change, and spoofing are three frequent attacks in distributed systems that we can protect MapReduce applications against these common threats by employing the security services, i.e., authentication service, integrity service, and confidentiality service (Xie and Qin, 2006). Therefore, it became apparent that at first, there should be a mechanism for Hadoop users to authenticate to prove their identities strongly.

The mechanism chosen for the Hadoop project is Kerberos (Sitto and Presser, 2015), a well-established service that today is common in enterprise systems such as Microsoft Active Directory. After strong authentication, the authorization is provided by applying BAT algorithm (Parmar et al., 2017). Secure authorization defined what an individual user could do after they had been authenticated. Initially, authorization was implemented on a per-component basis, meaning that administrators needed to establish authorization controls in multiple places (Perwej, 2019). Another aspect of Hadoop security that is still evolving is the protection of data through encryption and other confidentiality mechanisms. In the trusted network, it was assumed that data was inherently protected from unauthorized users because only authorized users were on the network (Spivey and Echeverria, 2015). Since then, Hadoop has added encryption for data transmitted between nodes using Blowfish and SALS (Sharma and Navdeti, 2014) algorithms, as well as data stored on disk using DES and AES algorithms (Parmar et al., 2017, Sharma and Navdeti, 2014). With these security services in place, users can flexibly combine them to construct integrative security protection against a variety of threats and attacks. Fig. 6 illustrates a MapReduce workflow execution with security services.

First, to prevent the spoofing attacks, it is required to deploy the authentication services for authenticating the users who intend to access input data. Then, the output data of the Map task is transferred to the Reduce task, and the Reduce task can be executed by using the input data, i.e., Map output data (MOF). After that, since integrity services ensure the detection of a tampered dataset, they are used to cope with alteration threats while executing tasks. Finally, to avoid unauthorized interception of the Reduce task’s output data, a confidentiality service is applied to counter the snooping attack to ensure that data is not available to unauthorized persons. Therefore, as a task may be under various attacks, Hadoop users should select a set of security levels to form integrated security protection in MapReduce-based environments. Thus, the security overhead to the MapReduce job execution time is considered by augmenting 
 that indicates the overall overhead in each phase (Eq. (23)). (23)


Download : Download high-res image (283KB)
Download : Download full-size image
Fig. 6. A secure-enabled MapReduce execution Flow.

We assume that each Map or Reduce task may need all three types of security service with different security levels specified by the user. For example, 
 is the set of security requirements of task 
, which can be specified as a q-tuple 
, where 
 represents the demanded security level of  security service and . For simplicity, we use a, g and c to represent the authentication service, integrity service, and confidentiality service, respectively. Table 3 shows the security levels and cryptographic overheads for confidentiality service (Li et al., 2016). Based on the performance of cryptographic algorithms, each algorithm is assigned a security level from 0.08 to . For example, we assign security level 1 to the strongest yet slowest encryption algorithm, IDEA. Note that the security level of each algorithm is inversely proportional to its time overhead.

Let the set 
 represent the levels of security services provided for task 
, where 
 indicates the level of  security service that task 
 has obtained. Security services introduce overheads to the existing computing systems. For integrity and confidentiality services, the amount of security overhead mainly depends on the service level and the size of data to be protected (Parmar et al., 2017). Hence, the security overheads of these two services are denoted as Eq. (24): (24)
where 
 is the data of task 
 to be protected. However, the security overhead of authentication service is a constant value that only depends on the service level. Hence, the security overhead of authentication service is computed by Eq. (25). (25)


Table 3. Cryptographic algorithms for confidentiality (Li et al., 2016).

Cryptographic algorithms	Security levels	Overhead (KB/ms)
SEAL	0.08	168.75
RC4	0.14	96.43
Blowfish	0.36	37.50
RC5	0.46	29.35
DES	0.90	15.00
IDEA	1.00	13.50
Then, the overall security overheads 
 experienced by task 
 can be calculated by Eq. (26). (26)

In the Hadoop cluster, we assume that each server can offer all the security services, and Hadoop users can run their jobs with various security levels to meet their QoS requirements.

In the execution process of the MapReduce jobs owned by different users in the cloud, tasks are no risk-free (Derbeko et al., 2016). Therefore, for evaluating the security services quantitatively, we perform the risk analysis for the entire MapReduce job, i.e., the workflow of Map and Reduce tasks. We calculate the risk probability of a task when the th security service is provided using the exponential distribution as follows: (27)
where 
 is the risk coefficient that can be different in each type of attack over the time interval. Hence, we can calculate the risk probability of task 
 by including all the security services using Eq. (28): (28)

After all the Map and Reduce tasks risk rate have been defined, we can calculate the risk probability of MapReduce job based on Eq. (29): (29)

We add the risk rate as a constraint in our mathematical model and evaluate the results.

4.4. Two-Tier HEFT scheduling
To schedule Map and Reduce tasks, since Reduce tasks can only be started after the Map tasks are completed, we will have a workflow from the Map and Reduce tasks. Map tasks as input tasks have the same rank and, their rank is higher than that of Reduce tasks as exit Tasks. The proposed framework uses a two-tier HEFT algorithm to schedule Map tasks and Reduce tasks of a Job. To do this, in the dramatic workflow of the MapReduce, we divide it into two parts, according to Fig. 7 and use the HEFT algorithm at each separate level.

4.5. Problem formulation
In MapReduce scheduling, for each task, we attempt to find the corresponding resources to execute a job on the distributed system such that the total execution time, i.e., makespan is minimized and the task is completed within the security constraint is met. Therefore, the overall mathematical model to minimize the objective function makespan is equal to: (30)

which is equal to the unnumbered equations are given in Box I.

Box I

 
 
 
 
 
 
 
 
 

5. Optimization framework
Double-layer architecture scheduling of the optimization framework,  is shown in Fig. 8. In the first layer, jobs are assigned an ApplicationMaster (in short, AppMaster) by the admin-customized resource allocation reside on ResourceManager. At the second layer, tasks are scheduled on the resources in the format of distributed containers by their corresponding AppMaster (Cai et al., 2017). SPO focuses on scheduling in the second layer while benefiting the default YARN job scheduler in the first layer. According to Fig. 8, Client first calibrates the required levels of security services for his/her jobs and then submits them for the execution. The Clients’ jobs are queued, and one AppMaster is assigned to each. Then, a logical container which is physically expanded on the nodes of the system is allocated to the AppMaster. The yellow logical container of AppMaster2 is shown as an example in Fig. 8.

Next, the Client copies the input data files into the Hadoop file system (HDFS) where the files are divided, encrypted, and replicated on the nodes as the data blocks. The Preprocessing component makes into the disposal the information of Map and Reduce task execution time obtained by profiling to the ResourceManager. The queried Map tasks information is sent to the Performance-aware Map and Reduce task scheduler components, respectively. SPO schedules the Map tasks using the Map scheduler (Algorithm 1). After finishing execution, the Map tasks write their output on the local hard disk of the host node as Map Output File (MOF). At this time, the NodeManager daemon active on the node reports the status of the node to the ResourceManager. Fig. 9, shows an MOF consisting of key/data-bytes as sub-partitions (partition 1, partition 2, …, partition p) (Condie et al., 2010).


Table 4. The execution time of map tasks on different resources.

Job ID	Job type	Number of map tasks	The processing time of a task (s)
CPU-intensive	20	7	6	5
IO-intensive	14	7	6	5
CPU-intensive	30	7	6	5
IO-intensive	26	7	6	5
We note that there is an asynchronous state between Map and Reduce stages where during the Map stage, the complete I/O disk resources will be reserved to the Map tasks to keep a track on all the intermediate partition data. At the Reduce stage, the number of Reduce tasks are determined based on the partition sizes produced as MOFs. Each partition size is measured based on its constitutive scattered sub-partitions belonging to a specific key-range. Therefore, by querying the information of Reduce tasks execution time from the Preprocessing stage, the Reduce stage scheduler schedules the Reduce tasks (Algorithms 2 and 3).


Download : Download high-res image (280KB)
Download : Download full-size image
Fig. 9. Map Output File (MOF) format (Condie et al., 2010).

5.1. Preprocessing
The availability of necessary information about the execution time of the Map and Reduce task for making the decision is the fundamental of static scheduling. In a heterogeneous cluster, there are different slots in term of processing rate. Besides, for a heterogeneous hardware structure, the processing time can also be different even on the same slot subject to the various jobs, i.e., CPU-intensive or IO-intensive type. Hence, for all MapReduce jobs in the system, the corresponding computing time of tasks on each resource is known, as shown in Table 4. To measure the execution time of a job during its Map and Reduce phases, we have set up a MapReduce-enabled Spark infrastructure (Maleki et al., 2019a) where the different types of jobs with different input data sizes run. We did profiling using the built-in monitoring tool of Spark, i.e.,  and measured the time duration of each phase of the job, including, initialization, Map, Shuffle, and Reduce time. We store the obtained information into the  and , respectively and apply the Matrixes in the Map and Reduce stage of scheduling, respectively. This stage responsible for the initial calculation called  stage. Map and Reduce stage schedulers are fed by the Map and Reduce Matrixes as input, respectively. We note that the consideration of job type under the mixture of co-running jobs workload is necessary since the partition size produced by the Map tasks of a job is not a good indicator for estimating the execution time of its Reduce tasks to efficiently define the partition placement on heterogeneous resources.

5.2. Map scheduler: Algorithm 1
The ratio of Map task output size and input size is defined as Map selectivity, i.e., the average number of produced records per the input split records (Herodotou and Babu, 2011). Map selectivity is the same and invariant for all Map tasks of a given job (Guo et al., 2016) since they process the same amount of data and do the same functionality. Therefore, in a heterogeneous environment with the heterogeneous processing capacities, the execution of the Map tasks will be affected by the server speed. Supposing the split data are already residing on the nodes, the Map scheduler schedules the Map tasks based on the HEFT algorithm using the information of Map task execution time obtained in the  stage. The Map tasks are scheduled in the way that minimum execution time is achieved (see Algorithm 1).


Download : Download high-res image (321KB)
Download : Download full-size image

Table 5. Algorithm variables.

Variable	Description
Resource on which the execution time of reduce tasks is minimized.
Reduce tasks finish time.
Reduce task start time.
Execution time of Reduce tasks.
Minimum of Reduce tasks finish time.
Number of reduce tasks.
Number of resources of the cluster.
5.3. Reduce Scheduler: Algorithms 2 and 3
SPO schedules Reduce tasks when all Map outputs are produced. At this stage, to minimize the Reduce tasks completion time, we need to decrease the last Reduce task execution time (See Algorithm 2). We can determine the Reduce tasks finish time by specifying the Reduce tasks start time and Reduce tasks execution time. After defining the finish time, the Reduce tasks are scheduled on the resources that minimize the execution time (see Algorithm 3). SPO calculates the partition size by adding the related sub-partitions volume scattered on the nodes of the cluster. Since in SPO, the number of Reduce tasks is defined dynamically subject to the partition size; thus we define the number of Reduce tasks for each partition size as a coefficient of the variable 
 which is a system parameter. Table 5 presents the variables used in the Algorithms.


Download : Download high-res image (317KB)
Download : Download full-size image

Download : Download high-res image (647KB)
Download : Download full-size image
According to Algorithm 3, we propose a performance and traffic-aware partition placement, Largest Partition First (LPF). LPF ranks the partitions by size and decides which node will minimize its related Reducers finish time while considering the server performance and data transfer time across the network. To find the Reducers finish time, we need to know the Reducers start time and Reducers execution time, according to Eq. (31). (31)

Reducers Execution Time: We can calculate the execution time of Reducers related to a partition since we have obtained the Reduce tasks execution time on each node of the cluster in the Preprocessing stage and maintain the information in  (line 2).

Reducers Start Time: To find this time, we need to calculate the maximum time between the time elapsed to transfer the sub-partitions related to a partition from other nodes to a specific node and the time a slot on the specific node becomes available for the execution of the first Reducer. We calculate Reducers start time using Eq. (32):

(32)

For calculating Reducers start time, we have to find the first Reduce start time since all other Reducers will be executed on the same node in parallel on the free cores or waited in the resource queue until it becomes free. Therefore, Reducers start time depends on the maximum time of two factors: (a) Time elapsed to transfer the sub-partitions related to a partition from other nodes to a specific node; (b) Time at which the resource will be available.


Download : Download high-res image (576KB)
Download : Download full-size image
Fig. 10. The flow diagram of the proposed framework, SPO.

Partition Transfer Time: We calculate the transfer time of data between two nodes by Eq. (33): (33)
where the 
 is the propagation delay between two resources 
 and 
. Notably, since for starting the Reducers execution time, the total data related to a key-range is required, the maximum time needed for transferring all the sub-partitions of a partition to a resource is taken into account.

Resource Available Time: This is the earliest time that the CPU resource becomes available to execute the Reduce tasks. After production of all partitions and when the resources become load-free, we decide where to place the Reduce tasks so that the finish time is minimized. We update the CPU available time of resources each time that the corresponding Reducers of partition scheduled on the right node (line 5). Fig. 10 represents the flowchart of the SPO.

6. Validation of the framework
In this section, we present the implementation and the validation details of our proposed framework, 
. We also discuss the results obtained from the 
. Since designing, prototyping, and evaluating new resource allocation and job scheduling algorithms in large-scale distributed systems such as Hadoop is a challenging, labour-intensive, and time-consuming task (Verma et al., 2011a), we simulate and assess the overall performance and security of SPO via simulation. According to Maleki et al. (2019b) many MapReduce papers (Cai et al., 2017, Kathiravelu and Veiga, 2014) have evaluated their work using Cloudsim (Calheiros et al., 2011) or its derivations CloudsimRT (Kao and Chen, 2016), CloudsimEX (Alrokayan et al., 2014), and CloudsimMR (Jung and Kim, 2012). We design and implement all the required java classes, including TaskDispatcher,TaskSchedule, KeyValuePair, JobSpec, MapTaskInfo, PartitionInfo, ReduceTaskInfo, and NetworkInfo. At the same time, we only benefit from the basic entity java classes of Cloudsim such as Datacenter, Host, Cloudlet, and Space/Time-shared scheduler.

6.1. Experimental design
6.1.1. Setup
We follow the test environment framework proposed in Tiwari et al. (2015) that was commonly used to MapReduce job for performance evaluation. We give an overview of the SPO evaluation setup, according to Fig. 11 and explain it in the following subsections.

We implemented all strategies on a computer with Intel Core   quad-core processors with  RAM running a  version of Linux Ubuntu 16.04 which are coded in Java with Eclipse Photon Released JDK 1.8. We create a YARN environment in Java with heterogeneous physical servers in small, medium, and large scale of , and  resources and the network speeds among each machine are randomly generated between  and . To simulate heterogeneity of servers, we consider a different capacity computation power for each server, in a range of  MIPS. Since the focus of this work is on CPU utilization, we assume each YARN resource container has unlimited memory space.

6.1.2. Dataset and benchmark
We simulated the input data, and the produced intermediate data according to the benchmarking and testing tools included in the Apache Hadoop distribution like Wordcount, TeraSort, Grep, etc. We have set up the YARN-SLS simulator (YARN, 2020) and run some jobs to see the partition sizes using the included simulated dataset ranging from 1 GB to . According to Guo et al. (2016) we have generated the Map intermediate partition data sizes based on the simulator pattern using uniform distribution between  and  GB as shuffle-light and shuffle-heavy jobs, respectively. We define the shuffle degree of jobs based on the MapSelectivity (in short, ) since the Map output data size is an application-specific parameter and also depends on the input data size. Thus, the  and  are the shuffle-light and shuffle-heavy Map output data, respectively. For instance, by applying , for a shuffle-heavy job with  dataset, the  intermediate data will be generated by  Map tasks.

6.1.3. Performance metrics
We measure the following parameters as evaluation criteria:

1.
Makespan: The total elapsed time required to execute the entire MapReduce job. We use the following equation to calculate the makespan: 
. Where 
 as the Reduce task finish time is calculated by 
 (Eq. (33)).

2.
Security Overhead: The time required for processing data encryption–decryption, task execution integrity, and user authentication.

3.
Network Overhead: It is the time needed for remotely fetching the data produced by Map tasks, then process it on the intended node.

4.
Scalability: Performance evaluation by leveraging more resources and different input data size in homogeneous and heterogeneous environments.

6.1.4. Comparing conditions
In general, to evaluate the performance of the proposed model and the proposed framework, 
, we perform various experiments. First, we compare our proposed model with 
 in terms of makespan minimization and security overhead and evaluate the performance gap between them. Second, to demonstrate the effectiveness of 
, we analyse the experiments in two parts from two perspectives to consider the performance:  
 is evaluated under different environments (homogeneous and heterogeneous) and dataset sizes,  
 is compared to Hadoop-stock in terms of makespan and intermediate data processing time. We mention that in the experimental results when we call the proposed framework, , it means 
 without considering security constraint while 
 is the framework with the objective of makespan minimization and security constraint.

6.2. Experimental results
6.2.1. Proposed model vs. 
We evaluate the performance gap between our proposed framework, 
, and the optimal solution obtained by solving the proposed MINLP optimization model using  toolkit (Cplex, 2009) under two situations:  Performance as the objective function with any constraint;  Performance as the objective function with the security constraint in the following subsections.

Comparison of performance.
We consider small-scale, medium-scale, and large-scale problem instances in this set of simulations. To evaluate the accuracy of our performance model regarding the typical MapReduce jobs covering all parts of a MapReduce flow, we decided to use Map-and-Reduce-input heavy jobs  that process large amounts of input data and also generate extensive intermediate data (Glushkova et al., 2019). The input data is set to , respectively, and the parameter  is set to , i.e., the input data size of Map tasks. To generate corresponding Map outputs, due to the high computational complexity of the  formulation, we consider four partitions, each key associated with random data size of  and the sub-partition size with random data size are generated on each node within  for Map Selectivity of . We create the number of Map tasks based on the input data size and set it to , and , respectively. Plus, we configure the bandwidth among the cluster to  and the propagation delay between two resources is one second. The results of the scenario with five resources and  input data both by the proposed model and  are represented in Figs. 12, and 13, respectively.


Table 6. Results of the performance-aware two-stage MapReduce task scheduling algorithm,  on different scenarios.

Scenario	Objective Function  F(minimize makespan)
#Resources	Input size (GB) , #Map  tasks 	Homogeneous environment	Heterogeneous environment
3	1,8	30.90	37.5	6.6	25.27	28	2.73
5	1,8	22.48	15	0.02	20.82	25	4.18
10	1,8	14.28	15	0.72	15.82	16	0.18
15	1,8	14.28	15	0.72	12.82	13.5	0.68
20	1,8	14.28	15	0.72	10.32	11	0.68
25	1,8	14.28	15	0.72	10.32	11	0.68
30	1,8	14.28	15	0.72	10.52	11	0.48
3	3,24	81.82	82.5	0.68	70	70	0
5	3,24	51.44	52.5	1.06	55.04	60	4.96
10	3,24	37.24	37.5	0.26	31.75	32	0.25
15	3,24	29.44	30	0.56	26.48	27	0.52
20	3,24	29.94	30	0.06	21.46	22	0.54
25	3,24	22.44	22.5	0.06	19.54	20	0.46
30	3,24	14.44	15	0.56	19.54	20	0.46
3	5,40	139.33	142.5	3.17	114.47	117.5	3.03
5	5,40	90	90	0	86.92	90	3.08
10	5,40	58.90	60	1.10	50	50	0
15	5,40	50.90	52.5	1.10	40.97	44	3.03
20	5,40	43.40	45	1.60	35.10	38	2.90
25	5,40	43.40	45	1.60	32.23	35	2.77
30	5,40		45			30	
3	10,80	262.07	262.5	0.43	220	222	2
5	10,80	170.10	172.5	2.40	170	170	0
10	10,80	111.60	112.5	0.9	94.64	95	0.36
15	10,80	96.5	97.5	1	79.82	80	0.18
20	10,80	81.80	82.5	0.70	64.94	65	0.06
25	10,80	81.60	82.5	0.90	64.79	65	0.21
30	10,80		75			59	
Average	1.09		1.32
As shown in Fig. 13, the performance of  is very close to the optimal solution. Table 6 shows a comparison of the results of the  with the results obtained by the solver for different scenarios. The first two columns define the scenario. More precisely, column1 () gives the number of resources in the cluster, whereas column2 () shows the size of the input data and the number of tasks for processing the data, respectively. In the next columns, we show , , and  values for the objective function  in the homogeneous and heterogeneous environment, respectively.  is a relative error from the optimum value, and it is calculated as  (where  and  stand for the optimum value and the , value, respectively). A zero  means that the solution found by the proposed scheduler is proven optimal, whereas an open gap means that there may exist a better solution in the range between  and .

Observing Table 6, we notice that in both the heterogeneous and homogeneous environment, the solutions obtained for  are near-optimal because the gaps to optimal are far less than 2%. Besides, the solution is scalable, i.e., the performance scales linearly with the number of resources for a specific input data. More experiments are provided in Section 6.2.2.

Comparison of security overhead.
In this section, we consider the security overhead epitomized in time. Fig. 14a to d shows the security overhead by 
, i.e., when the security constraint is met, in terms of makespan in heterogeneous environments. We have conducted the experiments for , and  input data size in the environment with , and  resources. We applied the  benchmark as a CPU-intensive and Shuffle-heavy application with Map-Selectivity of . The blue bar shows the makespan of tasks while the red bar indicates the overhead of fulfilling security of tasks. As seen, compared to , 
 provides a secure environment with slightly overhead for satisfying the maximum level of security.

Risk analysis.
To analyse the risk, we conduct and compare the experiments in terms of execution time in the homogeneous and heterogeneous environments while the risk rate is changing. For comparison, we use the  and  to show the execution time in the homogeneous and heterogeneous environments, respectively, and define the Minimum Service Level () and Maximum Service Level () algorithms as the baselines. In , there is any security level for the tasks, which means the risk rate of the MapReduce job is always 1. In contrast,  sets the security level of 1 for all tasks, which means the risk rate is always 0. We change the risk rate from 0.1 to 1 with an increment of 0.1 and set the maximum risk rate as a middle value to 0.5. Fig. 15a and b shows the results of the makespan.  and  have the best and worst makespan, respectively since the security services are fixed (0 and 1, respectively), and the flat line indicates that the execution time is not depending on the risk rate of the job in these algorithms. We can see that for the risk rate values less than 0.5, the makespan is high and then set to drop slowly in both  and . The reason is that when the risk rate of a job is set to a high value, each task will demand a lower risk rate that means that the higher levels of security services are demanded which results in more makespan. Since in the heterogeneous environments the stronger resources (higher processing capacity in our case) cost more, there would always be a trade-off between cost and security and users should consider this trade-off for their job execution and protection. Under the same parameter setting, the K-means has a higher time cost than Wordcount which is related to the job property where K-means is a high CPU-intensive job, and hence, it takes more CPU resource capacities.


Download : Download high-res image (422KB)
Download : Download full-size image
Fig. 14. Security overhead of the (a) 1 GB input data size, (b) 3 GB input data size, (c) 5 GB input data size, and (d) 10 GB input data size. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)

Since the risk coefficient has a high correlation with the risk rate; we vary its value from 0.25 to 3 with the increase step of 0.25. If we set the risk coefficient to zero, according to Eq. (27), the probability risk of a task will be zero meaning that a task will not demand the 
 service security as it does not suffer the attack. Fig. 15.c shows that if we apply the authentication service, the execution cost will be lowest, and since it does not depend on the risk rate, the curve becomes flat. By considering the confidentiality and integrity services, we can see where the risk coefficient has a lower value 
 the graph steadily grows because the risk rate changes accordingly and increases the execution cost.


Download : Download high-res image (383KB)
Download : Download full-size image
Fig. 15. Impacts of risk rate and risk coefficient on Makespan (a) WordCount, (b) K-means, and (c) Grep.


Download : Download high-res image (324KB)
Download : Download full-size image
Fig. 16. (a) The execution time of Wordcount by varying the size of data in homogeneous environment. (b) The execution time of Wordcount by varying the size of data in heterogeneous environment, on 10, 20, and 30 nodes, respectively.


Download : Download high-res image (328KB)
Download : Download full-size image
Fig. 17. (a) The execution time of K-means by varying the size of data in homogeneous environment. (b) The execution time of K-means by varying the size of data in heterogeneous environment, on 10, 20, and 30 nodes, respectively.


Download : Download high-res image (307KB)
Download : Download full-size image
Fig. 18. (a) The execution time of Sort by varying the size of data in homogeneous environment. (b) The execution time of Sort by varying the size of data in heterogeneous environment, on 10, 20, and 30 nodes, respectively.

6.2.2.  vs. Hadoop-stock
To evaluate , since the objective is makespan minimization while considering the network traffic, we consider both the heterogeneity of workload and environment. The jobs are different in terms of shuffle-light/heavy and CPU/IO-intensive; plus, the heterogeneous environment contains resources with different processing power. For the simulations in the homogeneous and heterogeneous environments, we adopted the following settings. Each host in the homogeneous environment is an  processor. Each host in the heterogeneous environments consists of the following Intel Xeon processors in the round-robin distribution: , , , ,  (Maleki et al., 2020). The processing power of the homogeneous system is  since it is the average computing power in comparison with the heterogeneous system resources and results in a fair comparison condition. Besides, we practically assess the scalability of , by various data sizes in a range of  to  in three different environment sizes, i.e., small, medium, and large. For the small environments, we assumed that they have ten hosts, these numbers are  hosts for medium-sized environments, and in large environments, these values are  hosts.

We build a YARN environment with the homogeneous/heterogeneous resources and interconnects the all resources by a Gigabit Ethernet (). We consider each YARN resource container has unlimited memory space since the focus of this paper is on CPU utilization. The input file size determines the number of Map tasks, and the HDFS block size 128 MB is set for all the scenarios. Table 7, Table 8 summarize the benchmark characteristics and scenarios that we use in these experiments, respectively.

Makespan.
Part 1: Fig. 16, Fig. 17, Fig. 18, Fig. 19 represent the job completion time for Wordcount, K-means, Sort, and Grep in homogeneous and heterogeneous systems, respectively.


Table 7. Application characteristics.

Job	Description	CPU/IO-intensive	Shuffle-light/heavy
Wordcount	Counts the occurrence of	CPU-intensive	Shuffle-heavy
each word in the input data		
TeraSort	A popular benchmark to sort one	IO-intensive	Shuffle-heavy
terabyte of randomly distributed data		
K-means	A clustering analysis algorithm for multi-dimensional	CPU-intensive	Shuffle-light
numerical samples in data mining		
Grep	Counts the number of occurrences of	IO-intensive	Shuffle-light
strings matching the target in a text file		

Table 8. Scenario description.

Workload type	Input data size small, medium, large 	# of Map tasks
Scenario 1	Job is CPU-intensive and the produced intermediate	1 GB , 3 GB , 5 GB , 10 GB 	8, 24, 40, 80
data is low in the range of [10,30] GB.		
Scenario 2	Job is IO-intensive and the produced intermediate	1 GB , 3 GB , 5 GB , 10 GB 	8, 24, 40, 80
data is low in the range of [10,30] GB		
Scenario 3	Job is CPU-intensive and the produced intermediate	1 GB , 3 GB , 5 GB , 10 GB 	8, 24, 40, 80
data is high in the range of [30,300] GB .		
Scenario 4	Job is IO-intensive and the produced intermediate	1 GB , 3 GB , 5 GB , 10 GB 	8, 24, 40, 80
data is high in the range of [30,300] GB .		
Scenario 5	Mix of CPU-intensive and IO-intensive jobs.	100 GB , 190 GB , 250 GB , 375 GB 	800, 1500, 2000, 3000

Download : Download high-res image (303KB)
Download : Download full-size image
Fig. 19. (a) The execution time of Grep by varying the size of data in homogeneous environment. (b) The execution time of Grep by varying the size of data in heterogeneous environment, on 10, 20, and 30 nodes, respectively.


Download : Download high-res image (264KB)
Download : Download full-size image
Fig. 20. Completion time of co-running jobs (a) WordcountGrep, (b) WordcountSort.


Table 9. Completion time of co-running jobs.

Workload mix		Hadoop-stock		SPO
A		B		A	B		A	B
WordCount		Grep		1198	2185		1028.4	1550
Sort		WordCount		3168	2828		3028	1504
K-means		Grep		39 493	11 308		27 250	9725
Sort		K-means		16 281	39 188		10 420	27 040
a.
WordCount: According to Fig. 16.a,  provides less execution time by increasing the number of resources in the Wordcount application for all different input sizes.  also achieves more performance for a significant amount of input data sizes by increasing the number of resources. The performance gain in the large environment compared to the small environment with 3 GB and 10 GB input size are equal to ＋2.5X and ＋1.5X, respectively. We should mention that in the homogeneous environment with 1 GB dataset, there is no performance in all cluster sizes since the number of Map tasks are less than the resources. In Fig. 16.b, the performance gain is more considerable in the heterogeneous environment, and makespan is about ＋1.2X less compared to the homogeneous system. The performance gain is ＋1.4X and ＋1.6X in the large environment compared to the small environment with 1 GB and 10 GB input size, respectively. The reason is that in the heterogeneous environment, with greedy behaviour of , the fastest resources are preferred in the decision-making process which makes earlier Map tasks finish time and subsequently results in less overall makespan.

b.
K-means: It is divided into two main phases. The first phase is the iteration phase, while the second phase is the clustering phase. In the earlier phase, the task is a CPU-bound meaning that the performance will be increased if there is an improvement in processing power, e.g., an increase in the number of resources. This is perceptible in Fig. 17a., and b with 2X and 2.11X performance gains in a large environment compared to the small environment with 10 GB input size in a homogeneous and heterogeneous environment, respectively. However, the performance gain of the heterogeneous system compared to the homogeneous system is not considerable (about 6%). The reason is that the performance is IO-bound in the clustering phase of K-means, meaning the performance is limited by IO data communication within a cluster. Since K-means is a shuffle-light job, the produced intermediate partition sizes are small, the overhead of network traffic in all cases is almost the same and low in both environments. Thus, it indicates that the slightly higher makespan of K-means in the homogeneous environment compared to the heterogeneous environment is due to its computational degree.

c.
Sort: As shown in Fig. 18.a  provides less execution time by increasing the number of resources in the Sort application for all different input sizes in the homogeneous environment. The performance gain in the large homogeneous environment in comparison with the small environment with 3 GB and 10 GB input size is ＋1.7X and ＋1.5X, respectively. Fig. 18.b indicates the performance in the heterogeneous environment is roughly the same with all input data sizes, and it reaches to better performance when the input data is large (10 GB). We achieve 1.2X and 1.3X in the small and large environment, respectively, compared to the homogeneous environment.

d.
Grep: Grep application has the minimum runtime among other applications, ＋6.1X, ＋35.4X, and ＋2X faster compared to Wordcount, K-means, and Sort benchmarks in the large heterogeneous environment, respectively (Fig. 19.b). The reason for less Grep runtime is that Grep is an IO-intensive job with light shuffling which based on , makes a smaller number of Reduce tasks for processing the produced partitions and consequently less makespan. Moreover, the performance improvement of the heterogeneous environment in comparison with the homogeneous environment with 10 GB input size in large, medium, and the small environment is equal to ＋1.2X, ＋1.3X, and ＋1.2X, respectively.

e.
Mixed Jobs: In this scenario, we have conducted complex experiments where multiple CPU and IO-intensive jobs are running with different input data sizes and different intermediate data sizes in the heterogeneous environment. We did these experiments by pair-combining the jobs including, Wordcount with 100 GB, Sort job with 190 GB, Grep job with 250 GB, and K-means job with 300 GB input data, respectively. In the presence of multiple jobs, we use the default Hadoop job scheduler, i.e., FIFO, giving priority to jobs for the run. However, in the future, we plan to apply the Analytic Hierarchy Process (AHP) model to determine the best permutation of jobs to run based on attributes including, job size, CPU/IO-intensive, shuffle light/heavy, and so on. We did the first experiment with the combination of a shuffle-heavy job and a shuffle-light job (WordcountGrep). Fig. 20.a shows the result of the workload mix of Wordcount and Grep in the heterogeneous environment. The results show that SPO outperformed the Hadoop-stock by 28% and 15% for Wordcount and Grep, respectively. On the one hand, since SPO is based on the asynchronous schema where the Reduce tasks start after finishing all the Map tasks, it allows the Reduce tasks of one job to use more resources. On the other hand, if we prioritize the Grep job that produces less intermediate data to start earlier than Wordcount, the execution time of their Reduce tasks takes a short time, and consequently, its performance will be significantly boosted. Next, we experimented with two shuffle-heavy jobs. Fig. 20.b shows the performance of Sort and Wordcount. It shows that SPO improved job execution times by about 29% and 7% over the Hadoop in these two benchmarks. Since the Wordcount job has a smaller shuffle volume, therefore, its Reduce tasks can be started earlier as their partitions required less time to shuffle. The performance of Sort is less in this scenario because its Reduce tasks start time is delayed by the Wordcount job. Another reason is that since the Sort is a shuffle-heavy job, the number of required Reducers responsible for processing the partition is large. Therefore, based on the LPR algorithm, which tries to reduce the network traffic by preserving the data locality in the side of Reducers through sacrificing the performance, the Sort execution takes longer. Table 9 shows more results of SPO with heterogeneous workloads compared with Hadoop-stock. SPO was able to reduce the job completion time for both jobs in the workload mixes with two jobs. Since SPO considers the heterogeneity of resources, the performance gain depends on the Map tasks scheduling and the placement of the amount of shuffled data in these co-running jobs.

Part2: For comprehensive performance analysis of SPO, we compare it with Hadoop-stock in three separated stages; Map, Shuffle, and Reduce by applying all the benchmarks defined in Table 6 including, WordCount, Grep, Sort, and K-means with 100 GB input data per application on 16 heterogeneous nodes. The -axis shows the latency of each stage. We report the average value of ten times running each simulation to demonstrate the confidence of the results. The deviation of results is negligible for the jobs Grep and K-means, where the random intermediate data size is generated. Plus, the deviation is less than 1% for the Wordcount and Sort jobs. As Fig. 21 shows, SPO outperformed Hadoop-stock in terms of makespan. In Map stage, SPO has much less makespan because it considers the resource performance when scheduling the Map tasks while Hadoop-stock is not performance-aware and selects Map tasks based on the data locality. If the head-on-the-line Map task is not local, it will be randomly placed on one of the cluster resources leading to worse makespan. Moreover, although, Hadoop-stock is already good enough at overlapping the shuffle phase with the Map stage since it follows the slow-start mechanism (starting shuffling when only 5% of Map tasks are completed) however, the network traffic in Hadoop-stock is considerably high. The reason is that the large volume of data is transferred across the network towards the randomly scheduled Reduce tasks leading to repetitive merges and disk accesses. The Reduce phase starts, which takes 30% of the time to complete, after finishing the rest of the shuffling (the small grey part which is about 4%). However, SPO schedules partitions using the data locality-based partition placement algorithm (LPF) and mitigates the network data traffic on average by 31% compared to Hadoop-stock. Despite starting the shuffling process along with Map, the Hadoop-stock performance is less than SPO. This is due to the asynchronous Map and Reduce scheme where SPO starts the shuffle phase after all Maps are completed. Therefore, here the complete resources are in the disposal to the Map tasks which fastens the Map execution while in Hadoop-stock, the Map tasks and Reduce tasks will compete for the resources. The asynchronous Map and Reduce schemes make a trade-off between improving the data locality along with input data size fair distribution for Reducers (achieved by LPF algorithm) and concurrent MapReduce, i.e., Map and Reduce concurrent phase execution. The results adequately demonstrate that SPO is able to accelerate Wordcount, Sort, Grep efficiently, and K-means job execution on average 29%, 36%, 14%, and 31% respectively, meanwhile achieves competent scalability for large-scale data processing. The time complexity of SPO is O(nm), where n and m represent the number of tasks and resources, respectively.


Download : Download high-res image (247KB)
Download : Download full-size image
Fig. 21. Per-stage latency of SPO compared to Hadoop-stock with four benchmarks.

Intermediate data size.
Fig. 22 shows the comparison of job completion time of SPO and two Hadoop schedulers, FIFO and HCS. For the shuffle-light jobs, where the produced intermediate data is small, the shuffle delay is unimportant. Thus, we measured the job completion time with intermediate data size ranging from  to  to understand the performance of the scheduler under the different size of intermediate data. The Figure shows that the job completion time of all three algorithms scales linearly with the intermediate data size. The job completion time of  is consistently less than Hadoop-stock scheduler in which the Reduce tasks are statically scheduled, leading to prolong the shuffle/network phase.

7. Discussion and future work
In this paper, we proposed a performance-aware and security-aware scheduling framework, SPO, for MapReduce tasks in heterogeneous environments. Besides, we presented a mono-objective multi-constraint mathematical model and compared the efficiency of our model and SPO in terms of makespan and security overhead. SPO is based on the HEFT algorithm, aiming to minimize the total MapReduce tasks execution time while meeting the security constraints using a two-level optimization strategy. Moreover, extensive experiments using the synthesized real-world applications demonstrate the effectiveness and practicality of SPO compared to Hadoop-stock.

Regarding the cloud-based computing, the MapReduce-enabledcloud environments are already provided as a service to the cloud users. Therefore, the cost of execution will become an essential factor. As a part of our future work, we plan to refine the proposed mathematical modelling and consider cost minimization as an optimization objective.

