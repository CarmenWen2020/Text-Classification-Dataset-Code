The emergence of the Internet of Things (IoT) paradigm has led to the rise of a variety of applications with different characteristics and Quality of Service (QoS) requirements. Those applications require computational power and have time sensitive requirements. Cloud computing paradigm provides an illusion to consumers with unlimited computation resource power. However, cloud computing fails to deliver on the time-sensitive requirements of applications. The main challenge in the cloud computing paradigm is the associated delays from the edge IoT device to the cloud data center and from the cloud data center back to the edge device. Fog computing extends limited computational services closer to the edge device to achieve the time sensitive requirement of applications. This work proposes a scheduling solution which adopts the three-tier fog computing architecture in order to satisfy the maximum number of requests given their deadline requirements. In this work, an optimization model using mixed integer programming is introduced to minimize deadline misses. The model is validated with an exact solution technique. The scheduling problem is known to be an NP-hard, and hence, exact optimization solutions are inadequate for a typical size problem in fog computing. Given the complex nature of the problem, a heuristic approach using the genetic algorithm (GA) is presented. The performance of the proposed GA was evaluated and compared against round robin and priority scheduling. The results show that the deadline misses of the proposed approach is 20%–55% better than the other techniques.

Previous
Next 
Keywords
Fog-cloud computing

Task scheduling

Mixed integer programming

Number of deadline misses

Genetic algorithm

1. Introduction
In the new era of Internet of Things (IoT) and smart lifestyle multiple computing paradigms have emerged as the suitable solution for the computing needs of enterprises and consumers alike. Cloud computing, in particular, has served and continues to serve a wide variety of computing applications. However, with the emergence of 5G and IoT infrastructure, cloud computing will likely encounter a severe limitation when the applications require small end-to-end delay. This is due to fact that the cloud computing infrastructure might be located far away from end users and, therefore, the encountered networking delays might be prohibitively large. Applications with stringent time delay requirements require that the computing services be located close to the edge of the network. Such a paradigm is referred to as fog computing (Mouradian et al., 2018).

With the introduction of the fog layer, the computing paradigm is commonly modeled as a three-layer architecture which consists of the edge layer, the fog layer, and the cloud layer (Mukherjee et al., 2018), (Hu et al., 2017). The edge layer includes end users IoT devices such as sensors, actuators, smart phones, and smart vehicles. Nodes in the edge layer are generally viewed as having limited resources in terms of storage and computing capabilities. The fog layer, on the other hand, is the intermediate layer that interconnects the edge and the cloud. It is commonly modeled to house thousands of networking devices such as base stations, routers, and gateways which do not only support different communication technologies, such as Wi-Fi and 5G, but also have the storage and computational capabilities that can accommodate the computational needs of many edge nodes. Finally, the cloud layer represents cloud datacenters that have very high computational and storage capabilities. The cloud layer is expected to deliver on-demand computing services with high availability and rapid elasticity (Gong et al., 2010), (Puthal et al., 2015).

The fog layer composed of a mix of software and hardware that is generally regarded as less capable than the cloud. It is an intermediate, with distributed computing capabilities that reside between the IoT devices and the cloud data centers (Mahmud, Buyya). Fog computing is often viewed as an extension of the cloud computing infrastructure that resides at the edge of the network (Yi et al., 2015). Being close to the edge devices is beneficial especially for latency-sensitive applications. In order to meet the latency requirements of the numerous requests, originating from the edge layer and forwarded to the fog layer, a scheduling solution is usually required to allocate the computational requests to the resources with the adequate capability. Typically, each request is divided into several tasks. Therefore, the scheduling solution usually involves finding the proper sequence for task execution under specific constraints to meet the scheduler objectives (Kittipittayakorn and Ying, 2016).

Task scheduling can be categorized as static or dynamic based on the environmental characteristics (Zhao et al., 2009). In static scheduling, all the tasks, resources, objectives, and constraints are known before the execution of the scheduler. The scheduler then allocates the tasks to the adequate resources at a specific time. In dynamic scheduling, the tasks and resources parameters can change within the environment. This can have an impact on the objective and the constraints at runtime which makes previous scheduling solution not optimal and, in some cases, infeasible. For example, tasks can appear, and new resources can be introduced or removed at runtime. To find the best schedule for a given static or dynamic environments, scheduling methods often rely on one of the optimization methods, which can be either exact or heuristic. The exact approaches such as Branch-and-Bound and Simplex techniques assure finding the optimal solution but may require very long computational times, especially for large-size problems. On the other hand, the heuristic approaches may achieve a near-optimal solution, but they provide a relatively short execution time. Examples of heuristic techniques include the genetic algorithm (GA), tabu search (TS), and simulated annealing (SA). The scheduling problem is known to be an NP-hard (Non-deterministic Polynomial-time Hardness), and hence, exact solutions are inadequate for large size problems. A full analysis and proof of the complexity of similar class of problem can be found in (Burkard et al., 1998). Heuristic approaches provide a reasonable execution time and quality.

In our previous research work (Aburukba et al., 2020), we proposed a GA approach for scheduling IoT requests with the objective of minimizing latency in a Fog-Cloud computing environment with hard deadline constraint. In this work, we propose a GA-based optimization model for scheduling computational requests from stationary IoT devices in a hybrid fog-cloud computing environment with the objective of minimizing deadline misses with soft deadline constraints. These requests have different deadlines depending on the nature of the application. Real-time applications such as patient health monitoring have very stringent deadline requirements, while non-real time applications such as infrastructure monitoring may tolerate relatively longer deadlines. Requests sent by IoT devices at the edge tier are received in the fog tier and then allocated to fog-cloud resources according to the scheduling model with the objective of minimizing deadline misses. The model accounts for such delays as the transmission delays, the queuing delays at the various networking devices, and the tasks execution delays at the fog and the cloud layers. In this work, the IoT request scheduling is modeled as an optimization problem using mixed integer programming (MIP), with the objective to minimize the number of requests deadlines misses. Given the nature of the presented problem being stochastic, nonlinear, and mixed-integer programming, the GA is a good heuristic candidate. The developed GA obtains a feasible solution with a good quality in a reasonable computational time. The GA is customized for the proposed model with the objective to minimize deadline misses. The work presents an adequate representation of a possible solution (chromosome) and a well-designed crossover operator. In addition, the GA includes a procedure to penalize possible solutions that do not satisfy the problem constraints. This will make the infeasible solutions to become less likely to be selected for producing new offspring chromosomes. The results of the proposed GA-based optimization technique are verified using a Branch and Bound exact solution technique for small size problems to verify the quality of the solution. GA is implemented for both static and dynamic scheduling. The solution for the static GA scheduling is compared to other scheduling algorithms such as round robin (RR) and priority scheduling (PS). The results show that the deadline misses of the proposed approach is 20%–55% better than the other techniques.

The rest of this paper is organized as follows: section 2 presents a review of related work, section 3 presents the formulation of the MIP model, along with a discussion about result validation, section 4 illustrates the proposed genetic algorithm heuristic, section 5 presents the GA parameters experimentation, section 6 presents the various simulation results, section 7 presents the results of a case study where the proposed model is applied, section 8 presents the conclusions and future work.

2. Literature survey
Scheduling is the allocation of tasks to capable resources at a specific time (Hoos and Stützle, 2005). Task scheduling involves finding the proper sequence for tasks execution under specific constraints and objectives that must be fulfilled to guarantee the quality of the solution (Zheng et al., 2018) (Ding and Wu, 2019) (Du et al., 2019). In most cases, scheduling problems consist of four elements (Aburukba et al., 2006) (Aburukba et al., 2006):

•
Jobs: A set of physical/logical operations to be processed by the resources.

•
Resources: A set of physical/logical devices with the ability to process the assigned jobs.

•
Objective: The evaluation metric used to assess the scheduling performance; it can be modeled as minimization or maximization functions.

•
Constraints: A set of conditions related to jobs, resources, or both. These conditions should be satisfied in the scheduling process.

Scheduling is classified as an NP-hard problem. A full analysis and proof of the complexity of similar class of problem can be found in (Burkard et al., 1998). Hence, heuristic approaches provide a reasonable performance and quality. The presented literature addresses such techniques in solving the scheduling problem within the fog computing related domain.

The work in (Aburukba et al., 2020) proposed a GA approach for scheduling IoT requests with the focus of minimizing latency in a Fog-Cloud computing environment. The performance of the approach is compared against the waited-fair queuing (WFQ), priority-strict queuing (PSQ), and round robin (RR) techniques. The results showed that the overall latency for the proposed GA approach is 21.9%–46.6% better than the other algorithms. It also showed an improvement in meeting the requests deadlines by up to 31%. The work in (Kumar Naha et al., 2020) proposed a resource allocation algorithm that deals with satisfying requests’ deadline. The algorithm uses a resource ranking, and the provisioning of resources are done in a hybrid and hierarchical fashion. The experiments conducted considered the algorithm performance and network delay, as well as the cost of when the number of requests increase. The results showed that the average processing time and cost were reduced by 12% and 15% respectively when compared to other techniques. Li et al. in (Li et al., 2020) proposed a scaling strategy to optimize resource costs based on load fluctuations. Their proposed approach utilizes a deep belief network to predict the workload of the IoT devices. Furthermore, they utilized a genetic algorithm heuristic to provide an efficient resource scaling strategy for executing IoT applications. Ma et al. (2019) designed a framework to estimate fog computing nodes for serving the dynamic requests of a mobile user. They examined various requirements of QoS in the mobile requests to resolve the problem of providing resource via multiple cloud instances. Determining the computation capacity of edge, hosts can be assessed using piecewise convexity of the problem so that it is possible to evaluate the optimal cloud tenancy method. X. Xu et al. proposed in (Xu et al., 2018) a dynamic resource allocation algorithm for load balancing in fog computing. Their proposed system architecture consisted of four layers: IoT application layer, service layer, fog layer and cloud layer. Load balancing problem formulated mathematically as a variance of the resource utilization, hence, was solved by minimizing the variance. The results showed that the proposed method has a good performance especially in the case of many fog services. L. Yin et al. proposed in (Yin et al., 2018) an algorithm for task scheduling and resource allocation in the context of smart manufacturing, their algorithm for resource allocation relies on containers as a lightweight virtualization technique with a relatively short start-up delay. Resource manager problem was formulated as a linear programming problem and the results showed increasing numbers of accepted tasks. The authors in (Choudhari et al., 2018) introduced a priority assignment module to minimize overall delay depending on tasks priorities. Their fog layer entailed fog server manager and virtual machines, prior to applying the assignment module, a decision is taken by the fog server manager, which decides between processes the task in the fog layer or forwards it to the cloud layer. The manager module is then followed by the priority assignment module depending on the time requirements of the received requests. The proposed modules improved the performance and increased the number of satisfied requests.

Genetic algorithm is considered as one of the most popular meta-heuristic techniques that have an empirically proven ability and adaptability in generating appropriate solutions (Montana et al., 1998). In (Liu et al., 2017), the authors used the genetic algorithm for solving a fog computing problem. The proposed framework objective is to minimize the overall latency that consists of three main components; the distributing latency, the processing latency and return latency. The adopted GA method resulted in latencies that are significantly improved for all test cases. In (Keshanchi et al., 2017), the authors proposed an improved genetic algorithm for task scheduling in cloud environments using priority queueing. The proposed approach benefits from the use of evolutionary algorithms and heuristic techniques. For algorithm verification, authors employed labeled transition system (LTS) method. Evaluation metrics included fairness and reachability. A comparison of the proposed algorithm to other well-known heuristic algorithms showed a significant improvement in makespan performance.

One of the reasons GA is an attractive option for large scheduling problems is the fact that it is highly parallelizable. Parallel implementation of GA has been extensively studied since it promises a significant improvement in the execution time of the solution implementation. The authors in (Wen et al., 2017) implemented parallel GA for a fog orchestration model considering data shuffling and communication cost. The implementation was based on portioning the individual solution into separate parallel compute nodes with a centralized master. A parallel GA implementation was compared to a standalone one, where the evaluation metrics included the execution time and the fitness score. The results showed that the parallel implementation consumed around 50% of the time consumed by the standalone one, and its solution quality was always 30% better. In (Shonkwiler, 1993), the authors presented a theoretical parallelizing method referred to as independent and identical processing (IIP). The analysis showed that the speed up for the parallel implementation grows exponentially with the number of parallel processors. Another study presented in (Pospichal et al., 2010) showed that implementing the GA in GPU environment using CUDA can achieve three orders of magnitude in speed improvement while still maintaining high quality solutions. Such an improvement makes it attractive to use GA in highly complex problems.

3. System model
The scheduling problem is mapped to an optimization model for allocating tasks to capable resources at a specific time in the hybrid fog-cloud computing environment. In this work, we propose to study the case of non-collaborating fog schedulers with stationary IoT edge devices. The overall system environment is shown in Fig. 1.

Fig. 1
Download : Download high-res image (285KB)
Download : Download full-size image
Fig. 1. Model for non-collaborating schedulers with stationary IoT devices.

In this setting, stationary IoT devices have some computational tasks that require processing at the fog tier. A given request is composed of several jobs and a job has a set of instructions to be executed by one of the processors (CPU). These jobs have properties such as size, priority, and deadline. Once a request is sent by the IoT device it experiences a fog networking delay, denoted 
, before reaching the fog tier where the fog scheduler resides. The fog tier contains a set of CPU resources where each CPU has a queue in which jobs may experience CPU queuing delay, 
, before being executed. The execution time is denoted 
. The sum of the execution times of all the jobs belonging to a given request constitute the request processing delay.

The scheduler must allocate any given job to the available adequate CPU to meet the deadlines of the maximum number of requests. Therefore, the objective of the scheduler is to find a suitable job schedule that minimizes deadline misses of requests. The cloud computing resources are also available to the fog scheduler. Although they have much higher processing capabilities than their fog counterparts, the cloud networking delay, 
, might be orders of magnitude higher than the networking delay of the fog, 
. The scheduler must, therefore, find the right tradeoff between the networking delays and the processing delays.

In this work, we consider the case of requests with soft deadlines. In such scenario, meeting the deadlines of a fraction of the total number of jobs in a given request is enough to satisfy the entire request. This fraction depends on the nature of the application that the IoT device is running. Table 1 shows the nomenclature adopted in the proposed model.


Table 1. System parameters and nomenclature used in the proposed model.

Parameter	Description
Number of stationary IoT devices denoted as 
Number of requests issued by a given node denoted as 
Number of jobs denoted as 
Size of job , belonging to request , coming from device 
Deadline of job , belonging to request , coming from device 
Priority of job , belonging to request , coming from device 
Number of fog CPUs denoted as 
Fog CPU speed defined as the number of cycles that can be performed per time unit.
Number of queue slots in the CPU buffer denoted as 
Logical variable: TRUE if job  of request  from device  is allocated to slot  at CPU 
Time that job  spends in queue of fog resource before execution
Time that job  requires to be executed by CPU 
In the proposed model, we note that the job priority, 
, can assume one of two priority levels: a low level 
, or a high level 
. We defined 
and 
such that the sum of the priorities of all the jobs that belong to a given request should be equal to one. The fraction of jobs with a low priority is denoted by . It is the fraction of jobs that can miss their deadlines as tolerated by the application and defined by the soft deadline approach.

The total delay experienced by a given request before its completion, denoted by 
 is presented in equation (1), comprises the networking delay, 
, experienced by the packets carrying the request over the network, the job queuing time
, and the job execution time, 
:(1)

The networking delay may be either 
 if the request is processed in the fog tier, or 
 if the request is processed in the cloud tier. The execution time 
 is proportional to the job's size 
 and inversely proportional to the CPU  speed, 
. Hence, the execution time for job , belonging to request , coming from device  is presented in equation (2).(2)
 

The job queuing time, 
, is the time spent in the queue of the CPU resourcebefore being executed. It is equivalent to the summation of the execution times of the jobs assigned to that CPU prior to it. Hence, the queuing time depends on the position  of the queue slot where job  resides. Hence, the job queuing time is presented in equation (3).(3)
 

The logical variable  in equation (3) is used to indicate whether a job is allocated to a given CPU. It is TRUE (
) if job  belonging to request  coming from device  is allocated to queue slot  at CPU , and it is FALSE (
) otherwise.

Therefore, using (1), (2), (3), the completion time is given by equation (4).(4)
 

If we let 
 be the deadline of job , belonging to request , coming from device , then that job is said to have missed its deadline if the time difference verifies: 
.

The model objective is to minimize the deadline misses of requests. As requests are composed of several jobs, meeting the request soft deadline depends on the priority of the missed jobs. Therefore, achieving the largest number of requests that meet their soft deadlines requires the minimization of the following objective function presented in equation (5).(5)
subject to(6)
 

From equation (6), the variable 
 assumes a value between 0 and 1 based on the priority of the job that missed its deadline.

To ensure that that each job  belonging to request  coming from device  is allocated to queue slot  at CPU  must be processed only once, the decision variable 
 must satisfy constraint in equation (7).(7)

In addition, to ensure that only one job is allocated to a CPU queue slot , the variable 
 must also satisfy constraint 8.(8)

Finally, the sum of priorities of all the jobs for a given request , coming from device , should be equal to one as presented in constraint 9.(9)

3.1. Model validation
For the model validation, a small-scale scheduling problem was examined using Lingo optimization solver (Lindo Systems and Inc. (2019, 2019). The problem consists of one end device that sends four requests of the same size. The attributes of these requests are presented in Table 2. Each request has five jobs (sub-requests) with different deadline requirements and priority levels to be allocated by the scheduler. Table 3 presents three CPU resources in the fog tier and a single resource in the cloud tier considered in this study. The table presents the CPU resources processing speeds, 
, and average delays from the edge device, τ. Here, τ is the sum of the average transmission delay 
 and queuing delay 
 experienced by a request in the network before reaching the fog tier. The propagation delay is neglected since the distance between the edge and fog tiers is assumed short. The resource 
 represents a cloud resource that has a relatively high processing speed when compared to the fog resources. However, the average delay from the edge tier to reach this cloud resource is much higher. This is attributed to the substantial propagation delay component that can no longer be neglected as was the case for the fog tier. In this model validation study, the requester's tolerance ratio is 0.8. This means that, for each request, if 80% of the jobs met their deadlines then the overall request is considered a success. Moreover, for each request, four out of the five jobs were set with a high priority level and one job with a low priority level. With this setup, missing a low priority-level job means missing 20% of the request's jobs, which satisfies the constraint imposed by the acceptable tolerance ratio .


Table 2. Parameters of requests 
 coming from one device with five jobs each used for model validation with Lingo.

2	2	2	2	2	2	2	2	2	2	2	2	2	2	2	2	2	2	2	2
50	70	40	30	30	50	40	60	30	60	50	50	40	70	50	40	80	60	70	40
0.2	0.2	0.1	0.2	0.2	0.2	0.2	0.1	0.2	0.2	0.2	0.2	0.1	0.2	0.2	0.2	0.2	0.1	0.2	0.2

Table 3. Resource parameters used in the model validation study.

Resource	
Processing speed 
100	130	150	500
Average delay, 	1	2	3	30
In the search for the exact optimal scheduling solution a decision variable 
 was used. Its value is set to 1 when allocating job of request  that belongs to device  to CPU resource , and set to 0 otherwise. Fig. 2 presents a visual of the produced optimal schedule with the order of allocated jobs to the CPU resources. The labels inside the blocks refer to the job index. For example, the label (
) means that the 4th job of the 1st request, allocated to the 1st CPU has a start time of 1, and a completion time of 22 time units. Note that the deadlines for each job are presented in Table 2. In this study, two jobs 
 and 
 have missed their deadlines. However, since they have a low priority, the model is assumed to have found the overall optimal solution while satisfying all the constraints.

Fig. 2
Download : Download high-res image (294KB)
Download : Download full-size image
Fig. 2. Optimal schedule produced by Lingo for the scenario used in model validation. The time units are relative to the resource processing speed.

For resource 
, the deadline for (
) was set to 30 time units. The average delay associated with CPU resource 
 in Fig. 2 was the lowest with only 1 time unit. However, its relatively low processing speed led to the lowest number of scheduled jobs. Moreover, two of the scheduled jobs at 
 (the latest two) were the low priority missed jobs (
; 
). For resource 
 the associated average delay was more than 
 by 1 unit. Nevertheless, it had a higher processing speed. Hence, all the scheduled jobs at the 
 resource met their deadlines. The execution order of the jobs is compatible with their deadlines. This means jobs with smaller deadline requirements are scheduled in the queue prior to jobs with larger deadline requirements. This validates the proposed model since the target is to meet the deadline rather than minimizing jobs execution time. A similar scenario was applied to resource 
 where all jobs met their deadlines. The resource 
 represents the cloud with a relatively higher computational speed. Given the provided experimentation setup, the cloud resource has six high priority jobs where all met their deadlines. Fig. 3 illustrates the differences between the jobs completion times and deadlines. The results shown validate the proposed model of minimizing the deadline misses. As described earlier, 
 and 
 marked with red arrows on the figure, were the requests from device 
 that had jobs with low priority, that missed their deadlines but still met the tolerance ratio requested by the device. Moreover, Fig. 3 shows that the completion time of the jobs in most cases is compatible with their deadlines, where the completion time increases with the increase of job's deadline requirement since the objective is to meet the deadline and not to minimize the completion time.

Fig. 3
Download : Download high-res image (487KB)
Download : Download full-size image
Fig. 3. Comparison between the job deadline and the completion time for the optimal schedule.

4. Proposed solution
Real-time scheduling for large scale problems encounters several challenges due to the complexity of the search space, the dynamic nature of the problem, and the nature of the constraints (Montana et al., 1998). Exact solution techniques imply an exponential growth which makes it infeasible to solve latency-sensitive scheduling problems. Therefore, a GA-based technique is adopted in order to provide a near-optimal solution to the proposed model in reasonable computational time. The proposed GA implementation is further described in the upcoming subsections.

4.1. GA implementation
GA is a meta-heuristic approach that generates a solution to the optimization problem. GA relies on the principles of biological evolution and natural selection. It starts with several initial possible solutions called the initial population. The population is represented as individuals (i.e. chromosomes) based on a representation scheme. New individuals are then generated via bio-inspired operators including crossover and mutation. Parents are selected from the population randomly with a higher chance of selecting the better ones based on the evaluation criteria which is represented by the fitness and the feasibility functions. The new generated population goes through the same process of the parents’ selection followed by crossover and mutation. This results into a new generation with a chance to have a better fitness value. This process is repeated until satisfying some predefined termination criteria.

a.
Individual representation scheme: Each individual in the population represents a candidate scheduling solution. It is represented as a binary array where each index represents a possible allocation of a request to a corresponding CPU, as shown in Fig. 4. For example, the array represented in Fig. 4 exhibits 4 requests (
) and 2 CPU resources (
), each CPU has 3 possible queue slots. The allocation of these 4 requests is represented by the individual binary array, where “0” means not allocated and “1” means allocated. Hence, in Fig. 4, 
 is allocated at the first queue slot of 
. Similarly, the allocation for each request is indicated by the “1” within the corresponding CPUs.

Fig. 4
Download : Download high-res image (574KB)
Download : Download full-size image
Fig. 4. GA implementation: a) Individuals representation and request allocation. b) GA crossover and mutation.

b.
Fitness function: GA is considered a flexible approach where the fitness function is problem dependent. It is calculated based on the objective function of the optimization model as presented in Fig. 5b. As the model objective is to minimize the deadline misses, the fitness value is inversely proportional to the priorities of the missed requests found in equation (4). This means that the smaller the number of deadline misses results in a better fitness value. Fitness is calculated for everyone in the population (i.e. each possible solution). In this function, GA iterates through all requests. For each request, it extracts the corresponding CPU and the queue slot at that CPU for the allocation. After that, the GA calculates the completion time that includes network delays, queuing time, and execution time, given in (2), and compares it to the request deadline. If the comparison result is larger than zero; it adds the priority of the missed request to the total number of misses. Finally, the fitness value is equal to the inverse of the total number of misses.

Fig. 5
Download : Download high-res image (406KB)
Download : Download full-size image
Fig. 5. GA implementation algorithms: a) Pseudocode for GA main function. b) Pseudocode of the fitness function.

c.
Feasibility function: The feasibility function presented in Fig. 6a depends on the problem constraints, where only feasible solutions can join the new generation. The feasibility in this model assures the satisfaction of the model constraints. The first constraint in equation (7) stated that each request should be executed by one CPU at a time; this is done by iterating through each request segment. The request segment represents a portion of the array that belongs to one request as shown in Fig. 4a. Request allocation is represented by “1” as illustrated in the scheme architecture, hence, for each request segment, the summation of all indices should be equal to 1. The second condition presented in equation (8) stated that for each CPU queue slot, one request should be allocated at a time. Hence, the GA iterates through a CPU queue slot for all requests and calculate the summation. This summation can be equal to “0”, means that this queue slot is free, or it can be equal to “1” means that the CPU queue slot is occupied by one request. Fig. 4a displays the first queue slot for 
.

Fig. 6
Download : Download high-res image (358KB)
Download : Download full-size image
Fig. 6. GA implementation algorithms: a) Pseudocode for the feasibility function. b) Pseudocode of the population initialization function.

d.
Initial population: The initial population presented in Fig. 6b is created by generating several individuals (i.e. possible scheduling solutions) through allocating each request to a corresponding random CPU slot. This number of generated individuals represents population size. GA iterates through each request of the individual and for each request it selects a CPU queue slot randomly for the allocation.

e.
Crossover and mutation: The population reproduction process starts from selecting parents with best fitness values. The selection is then followed by the crossover and mutation operators. The selection method adopts the tournament selection (TS) algorithm (Goldberg and Deb, 1991). TS starts by choosing a number of individuals randomly from the population. This number of individuals is the tournament size. Then, GA runs a tournament among the chosen individuals by iterating through them and checking their fitness values. The winner individual (i.e. the one who had the highest fitness value) is then selected for the following genetic operators. TS is called twice to select two parents for reproduction. The crossover operator aims to combine genetic information to generate a child individual that carries genetic information better than the parents according to the biological evolution. GA adopts single point crossover. After selecting two parents' individuals by TS algorithm, parents get crossed over the selected cross point. The new child is then generated by swapping parents' opposites upon the selected point as shown in Fig. 4b. It is important to note that the cross point should be selected carefully as it should reside at the end of the request segment. Selecting a cross point within a request segment will corrupt the individual scheme. Mutation is a GA operator that is meant to keep the diversity of genetic information across generations. Mutation prevents generations from being very similar to each other by altering a small number of bits in the mutated individual before adding it to the new population. Mutation should be applied in low rates, using high mutation rates can turn the search process into primitive non-adaptive one. The mutation rate adopted in this work is 15% which means that only 15% of population's individuals will incur a mutation process. After that, random number of requests within the chosen individuals is reallocated. This is implemented in the GA by iterating through population individuals, then selecting 15% of them for the mutation process. Within the individual, a reallocated number of requests is defined with a predefined variable.

f.
Termination criteria: The GA saves the best-so-far chromosome (the one that is feasible and has the least fitness value) of all generated populations. The algorithm stops when the termination criterion is satisfied. According to our experiments, this is done when the fitness did not change for 4 consecutive iterations or when the maximum number of iterations (determined as shown in the following section 5) is reached.

4.2. Dynamic GA scheduling algorithm
The scheduling problem addressed in the work is dynamic. Hence, the GA is further tuned to accommodate the new arriving requests. Initially, dynamic GA starts scheduling the first set of arriving requests into the available CPU resources, CPU resources in the simulation start as free resources. The scheduling process follows Algorithm 1 presented in Fig. 5a starting from population initialization followed by feasibility check, fitness evaluation and genetic operators (selection, crossover, and mutation). The process is repeated until the termination criteria is satisfied. Dynamic GA performs scheduling shown in Algorithm 1 periodically upon specific predefined scheduling intervals, but with considering the allocated requests from the previous intervals. Hence, the algorithm in Fig. 7 is the updated algorithm used to handle new arriving requests in addition to requests from previous scheduling intervals.

Fig. 7
Download : Download high-res image (353KB)
Download : Download full-size image
Fig. 7. Dynamic GA algorithm.

To handle previous requests in the CPU queues; for each following scheduling interval, GA considers and simulates the elapsed time during the previous scheduling interval (i.e. scheduling interval duration). This elapsed time affects previous requests in one of two ways:

1)
if the elapsed time is larger than the request completion time, it means that the request has finished and should leave the CPU. Hence, GA deallocates it from the CPU queue.

2)
if the elapsed time is smaller than the request completion time, it means the request is not finished with its execution. Hence, the GA keeps it in the queue with updating its completion time after subtracting the elapsed time.

To handle new arrivals, GA performs Algorithm 1 presented in Fig. 5a taking into consideration the unfinished requests from the previous scheduling intervals. These unfinished requests will affect the feasibility check as they still occupy CPU queue slots. Furthermore, this affects the fitness function for the new arrivals, as they represent an additional queuing delay. Fig. 8 illustrates the flow chart of the GA dynamic implementation. This algorithm finds the solution for every new scheduling interval.

Fig. 8
Download : Download high-res image (394KB)
Download : Download full-size image
Fig. 8. Dynamic GA scheduling flowchart.

5. GA experimentation
GA experimentation is meant to specify GA parameters including the population size and the maximum number of generations. A detailed discussion about the design of experiment approach followed to conduct the experiments in this section is given in Appendix A.1.

5.1. Population size
As a non-deterministic technique, GA is greatly influenced by the population size, where increasing the number of possible solutions increases the possibility of finding better solutions. However, population size also affects the runtime. Therefore, experimentations shown in Fig. 9 were conducted to determine the proper population size that results in quality solutions. Various setups were adopted in these experimentations using Intel R Xeon R Gold 5118 CPU @ 2.3 GHz, 8 Core(s) machine under Windows 10 (64 bit). The sizes of the experimented problems are namely: 20 requests and 3 CPUs (20/3), 30 requests and 5 CPUs (30/5), 40 requests and 7 CPUs (40/7), 50 requests and 8 CPUs (50/8). It should be noted that requests had soft deadline requirements, and the number of deadline misses in the figures represents request misses rather than job misses. Initially, the number of generations was set to 30 (number of generations are studied in Section 5.2). From the experimentation results shown in Fig. 9a; it can be noticed that the population size affects the behavior of the algorithm significantly. With a small population size, the algorithm tends to miss number of requests deadlines. This number decreases with the increase of the population size until it reaches a stabilized point at population size of 70. Hence, the population size of 70 can be considered for further exportation as it is found to represent a convergence point for the genetic algorithm. Moreover, as shown in Fig. 9b, the execution time of the algorithm is sensitive to problem size rather than population size.

Fig. 9
Download : Download high-res image (365KB)
Download : Download full-size image
Fig. 9. GA Experimentation: a) Deadline misses vs. population size. b) Run-time vs. population size.

5.2. Number of generations
The other important parameter in the genetic algorithm is the termination criteria which constitute the number of generations. As an adaptive algorithm, the quality of the solution enhances along with the number of generation before convergence. In addition, increasing the number of generations increases the run-time of the algorithm. Hence, the experimentation on the number of generations was encountered for similar size of problems presented in section 5.1 with a population size of 70 to specify a suitable number of generations before terminating the process. Fig. 10 shows the result of the experiments. Similarly, the behavior of the algorithm is refined along with increasing the number of generations until it reaches 20 iterations. Therefore, using 20 as the number of generations is acceptable for algorithm convergence.

Fig. 10
Download : Download high-res image (412KB)
Download : Download full-size image
Fig. 10. GA Experimentation: a) Deadline misses vs. number of generation. b) Run-time vs. number of generations.

5.3. Comparison of GA solution with the exact solution
Exact solution techniques provide an optimal solution for the scheduling problems. Nevertheless, the execution time of the algorithm increases significantly with the problem size. This makes it inadequate for real-time applications. On the other hand, heuristic techniques can provide a near-optimal solution in a moderate period of time. We have examined both the exact and heuristic techniques for validating the quality of the solution of the implemented GA, in addition to the algorithms run-times. For the exact solution, we used the Lingo software (Lindo Systems and Inc. (2019, 2019). Fig. 11 presents a comparison between Lingo (exact solution) with the proposed GA (heuristic solution). The figure shows the quality and execution time in finding a scheduling solution. In the experimentation, the requests have soft deadline requirements, where each request was split into 5 jobs with a system tolerance ratio of 80%. The problem size in Fig. 11 represents the number of jobs to be allocated to the corresponding number of available CPUs (jobs/CPUs). From Fig. 11, it can be observed that the solution quality of the GA algorithm is very close to the exact solution. Moreover, the jobs with missed deadlines are from the low priority level in all the cases, which implies that both GA and Lingo reached the optimal solution while meeting the system tolerance ratio. In the same figure, the labels show the runtime of each experiment. Clearly, the execution time of Lingo increases with the problem size.

Fig. 11
Download : Download high-res image (228KB)
Download : Download full-size image
Fig. 11. Lingo vs. GA Experiment.

6. Simulation and results
In this section, we consider a static scheduling problem where all the requests are known beforehand and scheduled at once, and a dynamic scheduling problem where requests are periodically scheduled at predefined intervals. In static scheduling, we compare the cases of soft and hard deadlines of IoT requests; study the impact of CPU speed and the delays encountered between the edge layer and the fog layer. In dynamic scheduling, we add to the previous study the impact of requests arrival rate. We refer the reader to Appendix A.2 for a detailed discussion on the design of experiment approach followed to conduct the experiments in this section.

6.1. Simulated model
The simulated model assumes the three-layered architecture, in which requests are initiated at the edge, and are then sent to the fog layer where the GA is run by the scheduler. GA is responsible for dispatching requests among the available CPU resources in accordance with the model objective of minimizing deadline misses. The simulation studies the impact on GA performance when varying the attributes of the requests, namely their deadlines, sizes, and priorities, and the computational capability of the CPU resources. The proposed GA results were compared against two algorithms that are typically used: Round robin (RR) and priority scheduling (PS). For all experiments, the evaluation metric is the number of deadline misses.

6.2. Simulation environment
In this simulation, the requests have different sizes, measured in the number of CPU cycles needed to execute all their tasks. They also have different deadlines, and different encountered delays in the fog and cloud networks. The CPU resources had different processing speeds, measured in CPU cycles per second.

The delays experienced in the network connecting the edge layer to the fog layer, as well as the network connecting the fog to the cloud layers are drawn from a folded normal distribution, 
, with a mean  and a standard deviation . In such distribution, only the positive values of the delays are accepted. Such assumption is commonly used in modeling randomness in many physical phenomena (Burkardt, 2014). Additionally, to have a more realistic simulation environment, the scheduling algorithm runtime was considered in the overall delay budget. To assess the run time impact, we ran experiments with 20 generations using an Intel Xeon® machine with a CPU clock speed of 2.3 GHz, with 8 physical cores, running 64-bit Windows10 operating system. In these experiments RR and PS runtimes were one order of magnitude smaller than that of GA, since they are non-iterative, and their solution is generated from the first run.

6.3. Impact of deadline
The objective of this study is to compare the number of deadline misses when deploying hard and soft deadline approaches. To highlight the merit of the proposed GA-based algorithm, GA results were compared to those of RR and PS. In this study, a

fixed number of requests was scheduled at once. Initially, the CPUs were assumed to be idle, resulting in a null initial queuing delay at the CPUs.

The experiment used 8 fog CPUs with processing speeds, 
, ranging between 2 and 6 GHz. The processing speeds of all the cloud CPUs were set to 100 GHz. The delay from the edge layer to the fog layer CPUs was modeled as 
 ms (Ye et al., 2002), (Li et al., 2017). The delay from the edge layer to the cloud layer was modeled as 
 ms (Li et al., 2017). The additional runtimes were set to 1000, 100, 70 ms for GA, PS and RR algorithms, respectively. These values correspond to  requests. In this experiment, the size of the requests was increased progressively to stress the scheduler. Each request was split into  jobs with acceptable tolerance ratio of  in the case of the soft deadline approach, and  in case of the hard deadline one. Requests’ deadlines 
 were drawn from a folded normal distribution as 
 ms. Such values are adequate for such applications where the mean tolerable delay is in the range of seconds, and where the standard deviation is small enough to capture the similar nature of the requests.

Fig. 12 shows the results of the experiment. The -axis represents the number of deadline misses, and the -axis represents the request size in units of million cycles (M-cycles). Fig. 14a is for the soft deadline case and Fig. 14b is for the hard deadline case. Clearly, GA had a superior performance compared to RR and PS. For the soft deadline case, it can be noted that GA provided a feasible solution (without deadline misses) all the way to request sizes of about 3000 M-cycles. The number of deadline misses started to increase afterwards, reaching 9 misses at around 4000 M-cycles. At the same time, PS and RR started to miss deadlines with relatively smaller request sizes. RR started to miss at 1700 M-cycles only and then started to miss deadlines rapidly till it exceeded 30 missed deadlines at the size of 4000.

Fig. 12
Download : Download high-res image (314KB)
Download : Download full-size image
Fig. 12. Comparison of the number of deadline misses of GA, PS, and RR algorithms a) using soft deadline case b) hard deadline case.

Fig. 13
Download : Download high-res image (348KB)
Download : Download full-size image
Fig. 13. Deadline misses vs. request sizes for CPU speeds 
 GHz. a) case of GA, b) case of PS, and c) case of RR algorithms.

Fig. 14
Download : Download high-res image (433KB)
Download : Download full-size image
Fig. 14. Deadline misses vs. request sizes for delay ratios . a) case of GA, b) case of PS, and c) case of RR algorithms.

The reason behind this decline in performance for RR is the way requests are allocated, which does not consider the requests’ priorities and does not attempt to minimize the number of deadline misses in the first place. The result for the strict priority algorithm shows that PS was better than RR and worse than GA also since PS does not consider the objective of minimizing the number of deadline misses. PS provides a feasible solution till the request size of around 2000 M-cycles and yields slightly more than 20 misses when the request size is 4000 M-cycles.

The hard deadline results show the same superior behavior of the GA. However, the performance declined compared to the soft deadline system where it managed to provide a feasible solution until the size of 2700 M-cycles and ended with more than 10 missed requests at the request size of 4000 M-cycles. Comparably, PS showed deteriorated performance where it started to lose requests at the size of 1700 M-cycles and ended at around 40 requests at the size of 4000 M-cycles.

It is noticeable that PS behavior was like RR in the case of hard deadline systems. The justification of this result is that in case of the hard deadline systems, job priorities are no longer beneficial because each missed job even if it is of a low priority will result in request failure, and for the same reason, soft deadline shows better overall performance.

6.4. Impact of CPU processing speed
The objective of this study is to investigate the impact of the fog CPUs processing speed, 
, in delivering a feasible solution by minimizing the deadline misses using the proposed GA.

The experiment used the same setting for the requests, CPUs, and delays considered in the previous experiment with soft deadline. The CPUs speeds are varied in order to study their impact on the solution quality. The results shown in Fig. 13, compare the impact of the fog CPU speed for GA, PS, and RR algorithms.

The results show that varying the CPU speed had a noticeable impact on the quality of the solution. In Fig. 13a, GA showed a high ability to exploit the increasing speeds to enhance the provided solution. The number of feasible solutions without deadline misses increased from 3000 M-cycles at speed of 4 GHz to around 6000 M-cycles at the average speed of 8 GHz and up to 9000 M-cycles when adopting the speed of 12 GHz. It can be noticed that the request size for the feasible solution increases linearly with the CPU speed. On the other hand, the number of deadline misses decreases with the CPU speed, faster than the linear fashion. For a request size of 12000 M-cycles, the number of missed deadlines dropped from 49 to 25 at the CPU speed of 8 GHz, and to just 10 missed deadlines at the CPU speed of 12 GHz.

In Fig. 13b, PS shows a performance decline when compared to the GA case. For the PS algorithm, the ability to provide feasible solution reached 2000 M-cycles only for average CPU speed of 4 GHz, around 4000 M-cycles for average CPU speed of 8 GHz and around 6000 M-cycles for average CPU speed of 12 GHz. Like GA, PS exhibits a linear relationship between CPUs speeds and the request size of feasible solution. However, for the size of 12000 M-cycles, PS missed all the requests using average speed of 4 GHz and it missed around 50% of the requests for the speed of 12 GHz.

In Fig. 13c, RR algorithm suffered from high percentage of deadline misses despite the relatively high processing speeds. RR achieved feasible solution at low requests sizes, it reached 2000 M-cycles without deadline misses for an average speed of 4 GHz, around 4000 M-cycles for an average speed of 8 GHz and around 5000 M-cycles only for an average speed of 12 GHz. Furthermore, with a speed of 12 GHz, RR missed more than 60% of the requests at the size of 12000 M-cycles.

6.5. Impact of network delays between the edge and fog tiers
This experiment aims to study the impact of the network delay, , encountered by a given request during its trip to the CPU resources at fog tier. The average network delay in the experiment was set as a fraction of the request deadline, 
. With the same previous setup, the deadlines were modeled as 
 ms. The impact of the network delay was studied for the following values of : 10 ms, 100 ms, 1000 ms, 3000 ms, and 5000 ms. This means that the ratios 
 were set to 0.1%, 1% 10%, 30%, and 50% respectively.

In Fig. 14a, GA results show almost the same behavior for ratios 0.1%, 1% and 10%. Hence, it can be concluded that the delay to the fog tier has a small impact if it is less than 10% of the request deadline. For the range of 10%, GA generated a feasible solution for request sizes reaching 2700 M-cycles.

Furthermore, a relatively low number of deadline misses was recorded during the experiment. GA performance starts to decrease with delay ratios greater than 30%. The feasible solution size declined from 2700 to 2000 M-cycles, and the number of deadline misses jumped from around 10 to 30 at the size of 4000 M-cycles. With a delay ratio of 50%, the feasible solution size declined from 2000 to around 1000 M-cycles, and the number of deadline misses increased to around 40 at the size of 4000 M-cycles. Thus, it can be noticed that for delay ratios beyond 10%, GA performance declines with the increase of the encountered delay ratio.

In Fig. 14b, PS results show an overall decrease in the performance compared to GA results. In the range of 0.1% and 1%, PS behaved in the same manner, however, it started to decline in the range of 10%, where it missed around 25 requests at the size of 4000. With the increase of the average delay, the deadline misses increased greatly, at the range of 50%. PS missed all the deadlines at the size of 4000 M-cycles. In Fig. 14c, RR had the same trend but with a higher number of deadline misses during the experiment and a relatively small size for feasible solution.

7. Case study: leak detection in gas pipelines
In this section, we propose to study the suitability of our proposed GA-based scheduling technique in a case of a practical interest. Gas leaks are a major concern for the Oil and Gas industry in general and for the power utility companies, in particular. Leaks, if undetected, usually lead to a substantial financial loss, environmental pollution, and possible human injuries (Murvay and Silea, 2012). Gas leaks are more challenging to detect than fluid leaks. This is due in part to the poor signal to noise ratio encountered when measuring gas flows (Adnan et al., 2015). Among the many techniques that have been developed for gas leak detection(Murvay and Silea, 2012), acoustics techniques are considered the most suitable due to their high accuracy and adaptability (Liu et al., 2015). Acoustic signals are first sensed with specialized sensors then processed and analyzed. Features of processed signals are extracted and compared against the normal conditions to detect any leakage conditions.

The three-tier conceptual model of the gas leak detection application is shown in Fig. 15. It consists of a pipeline with micro-controllers connected to acoustic sensors and actuators residing at the edge tier. The scheduler resides at the fog tier along with the CPU resources and the conventional cloud centers at the cloud tier. The acoustic sensors installed along the pipeline are sampled by the microcontrollers and sampled data is sent to the fog tier. The data received by the scheduler at the fog layer is dispatched to CPU resources with the adequate capabilities. The received data are then processed by the fog node while taking into consideration the near-real-time latency requirements to prevent further leakages.

Fig. 15
Download : Download high-res image (165KB)
Download : Download full-size image
Fig. 15. Three-tier conceptual model of the gas leak detection application.

The decision to close the vanes of the defective pipeline section is reached by the fog node after the computationally intensive signal processing and feature extraction activity. The actuating signal is sent back to the microcontrollers which in turn instruct the actuators circuitry the shutdown the leakage sources.

The following experiment simulated a soft deadline scheduling scheme for the gas leakage detection case study. The simulation considered a scheduler with 5 fog CPUs, each having a CPU ranging from 5 to 7 GHz. The network delay to the fog tier was modeled as 
 ms. The cluster of cloud CPUs were assumed to have an aggregate processing speed of 100 GHz. The delay to the cloud was modeled as 
 ms.

The experiment used the dynamic scheduling scheme for 5 intervals of 1-s duration each. In addition, the following parameter values were used: An average arrival rate of 10 requests per second; the request deadlines were modeled as 
 ms; the queuing delay at the CPU was modeled as 
 ms; and the runtime delay was set to 200 ms.

The results in Fig. 16 show that hybrid fog-cloud architecture had the best performance. Nevertheless, fog and hybrid fog-cloud architecture shared the same feasible solution size recorded at around 5000 M-cycles. On the other hand, the cloud-only architecture started to miss deadlines at around 4000 M-cycles and continued to miss deadlines until reaching 50% at the size of 6000 M-cycles.

Fig. 16
Download : Download high-res image (248KB)
Download : Download full-size image
Fig. 16. Gas leak detection results for hybrid fog-cloud (HFC), fog-only (F), and cloud-only (C) architectures.

8. Conclusion and future work
This paper addressed the scheduling problem of the time-sensitive IoT requests in the hybrid fog-cloud architecture with soft deadline. The scheduling was modeled as a mixed integer programming problem with the objective of minimizing deadline misses. In order to meet requests deadlines, the model considered delays encountered by the requests during its round trip through the three layers of the environment, namely the edge layer, the fog layer, and the cloud layer. Delay components included transmission delay, queuing delay, networking delay, and the task execution delay. The proposed model considered requests’ characteristics such as the request size, deadline, and priority. The proposed model considered proposing a genetic algorithm for scheduling requests generated from stationary IoT devices. The Fog computing CPU resources are associated with each the fog scheduler and characterized by their processing speeds, and the average delay it takes an IoT request to reach the resource. The genetic algorithm was used to provide near optimal solutions, since it has an empirically proven ability to provide high quality solutions for large search space problems. The implemented GA solution was compared to the exact one for small scale problem size. The proposed GA was evaluated and compared against round robin and priority scheduling and the results showed that the deadline misses of the proposed approach is 20%–55% better than the other techniques. Future work is to model the mobile IoT devices and extend on the proposed scheduling model in Fog-Cloud computing to support the mobility characteristic.