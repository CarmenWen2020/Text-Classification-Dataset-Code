Runtime environments for IoT data processing systems based on the actor model often apply a thread pool to serve data streams. In this paper, we propose an approach based on Reinforcement Learning (RL) to find a trade-off between the resource (thread pool in server machines) usage and the quality of service for data streams. We compare our approach and the Thread Pool Executor of Akka, an open-source software toolkit. Simulation results show that our approach outperforms ThreadPoolExecutor with the timeout rule when the thread start times are not negligible. Furthermore, the tuning of our approach is not tedious as the application of the timeout rule requires.

Previous
Next 
Keywords
IoT

Actor

Resource management

Reinforcement Learning

1. Introduction
In the past few years, we have experienced a significant rise in networking devices and the global network infrastructure behind them called the Internet of Things (IoT). Gartner (2017) predicted that the number of IoT units would exceed twenty billion and hardware spending from the consumer and the business segment together would reach three trillion USD. There are many use cases regarding the IoT technology: RFID tags can be attached to products to track their origin; data through vehicle-to-vehicle and vehicle-to-infrastructure communications can be utilized to avoid congestion and reduce incidents; sensors could monitor the state of a vehicle, the health of a specific person, or other environmental parameters (Bandyopadhyay and Sen, 2011, Xu et al., 2014, Albahri et al., 2021).

Systems that serve and process data from IoT devices are often implemented with the actor model programming paradigm (Diaz Sánchez et al., 2015, Haubenwaller and Vandikas, 2015). The Akka toolkit is a framework built on the JVM that supports the development of actor-based systems (Roestenburg et al., 2015, Bonér, 2010). Actors are designed for distributed computing and can handle highly concurrent IoT data streams. Also, the hierarchical structure of an actor-based system can be directly mapped into the relationships between IoT groups and devices. Actors in Akka share a set of threads in a thread pool, and the performance of the IoT application heavily depends on the size of the thread pool. A common way to set a static thread pool size is by trial-and-error. The system operator either increases or decreases the thread count based on measurement results. In a dynamic environment, e.g. under varying traffic, Akka allows the thread pool to change its size dynamically. We looked into the timeout rule used by Akka’s thread pool managing entity. In this rule, the operator may set a timeout parameter to adjust the system to varying traffic. However, finding this value requires expert knowledge and may need a tedious trial-and-error process.

Motivated by the need for a good resource management solution, we propose an approach based on reinforcement learning (RL) for controlling the thread pool size to minimize resource usage while maintaining a certain QoS level. Our contributions are as follows:

•
We defined a threshold-based multi-objective reward for the reinforcement learning problem. It considers the service waiting times for Qos and the thread pool size for efficiency.

•
We used population-based training (Jaderberg et al., 2017) and grid search in combination to find the hyperparameters for the RL algorithm. These hyperparameters influence how close we could get to optimal operation and the stability of the algorithm.

•
We compared the performance of the RL algorithm with the timeout rule used by the Akka actor framework. Through simulations, we showed that when threads start instantaneously, the RL algorithm performs similarly to the timeout rule. When the thread start time is not negligible, the RL algorithm performs better than the timeout rule.

The rest of the paper is organized as follows. In Section 2 we overview the related literature. In Section 3 we describe a scenario in which an actor based system is built for an IoT use case. In Section 4 we present our approach. In Section 5 we review the RL algorithm we used in our experiments. In Section 6 our experiment results are discussed. Finally, Section 7 contains our conclusions and future directions.

2. Related works
2.1. Resource management in for IoT
The problem of optimizing resources has always been central for many. Lately, resource management involving IoT has gained more focus.  Musaddiq et al. (2018) investigated the resource managing capabilities of various operating systems in IoT. Zhang et al. (2017) studied the resource management between network slices in a 5G system, where IoT applications would get a separate slice. Alazab et al. (2021) used an enhanced rider optimization algorithm to find the optimal head nodes in IoT clusters and Alazab et al. (2020) used long short-term memory (LSTM) to predict resource usage in smart grids.

Furthermore, numerous studies have been published on cloud or fog computing resource management with IoT applications in mind. Aazam and Huh (2015) devised a dynamic pricing algorithm based on resource usage estimation in a fog computing system.  Skarlat et al. (2016) constructed a framework with fog cells and fog colonies for IoT and created a resource provisioning algorithm for the framework. Barcelo et al. (2016) formulated the service distribution problem in IoT clouds and provided a solution through linear programming.

2.2. Thread pools
Approaches to control threads can be classified into the reactive and the proactive policy (Singh et al., 2019). The reactive policy takes actions to respond to the environmental changes, whereas the proactive approach predicts these changes and adjusts the number of threads accordingly.

In Ogasawara (2008), each service’s load is continuously monitored, and thread counts of various thread pools were adapted dynamically according to load. The adaptive algorithm moves idle threads from thread pools with a low load to thread pools with a higher load. Kang et al. (2008) used the average exponential scheme to predict the number of required working threads and started other threads if necessary. Threads are stopped after a certain timeout period. Chen and Lin (2010) used the M/M/c/K//FCFS queuing model to estimate mean waiting times and queue lengths to adjust the thread pool size in a middleware system dynamically. Lee et al. (2011) used a trendy exponential moving average scheme to predict and set the worker thread count and used the M/M/c queuing model to find an adequate minimum thread pool size. García-Valls (2016) identified the thread pool as one of the critical resources to manage in middleware applications. The author proposed an interception layer between the middleware and the operating system to control the upper and lower bounds of the thread pool’s size.

2.3. Reinforcement learning for resource management
Machine learning (ML), or more specifically, reinforcement learning (RL), could provide an excellent alternative to managing resources by hand. RL is a technique for solving Markov Decision Processes (MDPs) (Sutton and Barto, 2018). In RL, an agent interacts with an environment described by the MDP, and over time it learns the optimal actions to pick to maximize the long term reward. RL algorithms like Q-learning (Watkins and Dayan, 1992) or policy gradient methods (Sutton et al., 2000) have been around for decades. However, only in the past few years have they received more use and attention due to hardware power developments that allowed computationally more demanding function approximation techniques like deep neural networks (DNN). When combined with DNN, RL is also referred to as deep reinforcement learning (DRL). Recently various feats have been achieved with DRL, such as machines learning to play ATARI games (Mnih et al., 2015) or Go (Silver et al., 2016). DRL has also found numerous practical applications like power consumption management (Tesauro et al., 2008), resource management (Mao et al., 2016), or robotics (Levine et al., 2018).

Cheng et al. (2018) investigated a deep RL based resource provisioning and task scheduling algorithm for large scale cloud service providers. Their goal was to minimize energy consumption using deep Q-networks (DQN) with two stages of action selection. They showed that the RL algorithms could outperform heuristic baseline methods. Wei et al. (2019) used Q-learning to rent on-demand and reserved virtual machines dynamically for a SaaS provider.  Bibal Benifa and Dejey (2019) created an auto-scaling algorithm based on SARSA for clouds.

Jin et al. (2018) used fuzzy Q-learning to control a cloud-based web application. They defined the system’s state using the workload and the virtual machine count and used the long term profit as the reward. They carried out experiments with simulations and in a real testbed. Jin et al. (2019) argued that exploiting some domain knowledge might be a better approach than tackling the problem in an entirely model-free manner.

Arteaga et al. (2017) dealt with scaling network functions. They combined Q-learning with a Gaussian process to estimate the system’s future states to avoid executing erroneous actions. Mao et al. (2019b) introduced a new baseline to reduce the variance in environments with random input. They used their method on a load balancing task and a bitrate adaptation problem. Mao et al. (2019a) summarized the RL problems related to computer systems and created a framework for training RL algorithms for these problems.

Tahsien et al. (2020) provided a review of ML approaches for the security of IoT against different types of possible attacks. They also identified some research challenges on the application of ML to the security of IoT systems. Chowdhury et al. (2019) proposed a drift adaptive deep RL algorithm for scheduling and allocating resources in an IoT application. However, the performance of an IoT application is approximated with the simple M/M/1 queue.

To our best of knowledge, there have been no works that dealt with the application of RL to manage the resource of Actor-based systems.

3. Problem description
3.1. An implementation of an IoT application using the actor model
Akka is a free and open-source toolkit for the development of distributed applications (Roestenburg et al., 2015, Bonér, 2010). It uses the computational actor model, in which the basic building blocks are called the actors (Hewitt, 2010). An application that connects to IoT devices sets up and manages sessions and processes the sensor data in a server would employ actors.

Actors communicate through messages only. Upon receiving a message, an actor may either send a finite number of messages to other actors with known addresses; spawn new actors; or decide how to handle the following message. The actor may execute any amount of these actions in any order. Furthermore, these actions may be carried out in an asynchronous manner enabling a highly concurrent system. The actor model makes building distributed systems more convenient for the developer as it hides away lower-level elements like threads or locks. Note, though, that these low-level objects may significantly impact the IoT system’s performance.

The main focus of Akka is to support actor-based concurrency. It is worth emphasizing that Akka can be used to build IoT platforms to process data from IoT devices (for example, ThingsBoard). An IoT platform’s job is to provide a safe and secure connection to different IoT devices and lay the groundwork for applications and analytics. Fig. 1 outlines a simplified architecture of an IoT platform similar to the one used by ThingsBoard. We can see that IoT devices send data from the outside world through the Message Queuing Telemetry Transport (MQTT) protocol. If there is no connection established yet, the Session Manager Actor creates a Session Actor to manage the IoT device session. Meanwhile, the Device Manager Actor makes a Device Actor to process the incoming data stored in persistence for further analytics.


Download : Download high-res image (191KB)
Download : Download full-size image
Fig. 1. IoT platform.

3.2. Threads in Akka
Akka provides a runtime environment for the execution of distributed applications on the Java Virtual Machine (Akka documentation). The management of resources, i.e. the mapping of actors and system-level threads (operating system’s resources), is based on the solution of the ThreadPoolExecutor of Java (Java TM Platform).

In Akka, the coordination of processing the messages is done by the dispatcher (Akka documentation). If an actor has a message in its message queue, the dispatcher may take that actor and assign it a thread to process that message. Threads are then mapped to CPU cores by the scheduler in the operating system. Fig. 2 shows us the relationship between actors, the thread pool and the CPU. Note that some actors may need to wait for available threads if there are more actors than threads. Similarly, if there are more threads than the number of CPU cores, the cores will be shared between the threads by the scheduler.

The assignment and the execution of the actor’s code are done through the ExecutorService. In Akka, the two most common executors are the ForkJoinExecutor and the ThreadPoolExecutor. The former is the default executor and utilizes a static, fixed-sized thread pool, which the developer can configure manually. The latter can scale the thread pool dynamically depending on the system’s demand. Since Akka is written in Scala and runs on the JVM, these executors are identical to those described in Java-related literature.


Download : Download high-res image (278KB)
Download : Download full-size image
Fig. 2. The dispatchers in Akka assigns threads to the Akka actors. The mapping between the threads and the CPU cores is done by the operating system’s scheduler.

By default, the ThreadPoolExecutor uses the timeout rule as a policy (Java TM Platform). In this rule, threads are started on-demand. If a new task arrives and all other threads are busy, a new thread is created in the thread pool to process it. After finishing the task, the thread becomes idle until there is another task to be processed. If the thread remains inactive for a specific amount of time called the timeout (or keep-alive time), the thread is terminated to free up capacity. This allows the system to scale down the pool size when the traffic is low and scale up if traffic is high. The default timeout period set by Akka is 60 s.

3.3. A need to control the number of threads depending on traffic load
When a new IoT device connects to the system, an actor is created to open and manage a session, and another actor processes the data coming from the device. We will only consider data processing actors and assume that the thread and CPU time every other type of actors need is negligible.

Theoretically, there is no limit on how many actors we can spawn to execute these tasks simultaneously. However, the performance highly depends on the size of the dispatcher’s thread pool beneath the actors. If there are not enough threads, the actors might end up waiting for each other. Unfortunately, we cannot just increase the thread pool size endlessly because only one thread can run on a CPU core, and a high number of threads would increase the time of context switching. Furthermore, idle threads may also use up resources. Therefore it is crucial to keep their number as small as possible. One of the developer’s challenges is finding the optimal number of threads for a given application.

Suppose sessions arrive randomly with rate 
. We will denote the number of sessions at time  with . The value of  is limited by the maximum number of sessions in the system , that is , where  depends on the capacity of the physical machines. During the session, the IoT data arrive through packets with 
 fixed interarrival times, which the actor then processes. We assume the length of a session is also random with mean . When the IoT device disconnects, the session gets closed, and the related actors get removed from the system. Note that the processing of a packet may be finished after its session is expired, and the length of a session does not necessarily equal the actor’s lifetime.


Table 1. Notations used to describe the thread pool environment.

Arrival rate of sessions
Number of sessions
Number of threads
Number of busy threads
Maximum number of sessions
Maximum number of threads
Interarrival time of packets
Mean length of a session
Base service time of a packet
Real service time of a packet
Thread start time
Number of CPU cores
Capacity of an actor’s message queue
Let  denote the number of threads at time . The actors share these threads to process their packets. A thread may be in one of the following three states: booting, if it was started but has not finished the initialization process yet; busy, if it is processing an actor’s message; or idle, if it is initialized, but is currently not working. Let  denote the number of busy threads at time .

In this paper, to show the applicability of the RL technique, we assume that packets have the same size and that the service time of the packets is a fix value 
 until the number of busy threads  reaches a limit 
 after which the service time of the tasks increases due to the threads resorting to sharing CPU resources. In practice, 
 is the number of CPU cores itself. Fig. 2 shows us a system with  actors,  threads, and 
 CPU cores. We can see that an actor is assigned to a thread if there is an available thread in the pool. A thread can be assigned to a CPU core to execute its actor’s task.

Threads are assigned to the actors in a round-robin manner by the dispatcher. This means that actors are ordered, and thread assignments follow this order. After a thread finishes its job, the dispatcher assigns it to the next waiting actor to process its packet. If no actors are waiting, the thread becomes idle until the dispatcher assigns it again to an actor. If all threads are busy upon the arrival of a packet, the packet is placed into the actor’s message queue. We assume a limit of  to the number of packets waiting in the message queues, which means that new incoming packets get thrown away if the number of waiting packets in the system is  upon their arrival.

The number of threads in the thread pool may be changed dynamically through the dispatcher. This thread count is upper bounded by  which can be set in a configuration file and depends on the capabilities of the physical machine, that is, . We assume termination of threads is instantaneous, but turning them on requires 
 time. To satisfy specific QoS requirements, we need to have a sufficient amount of threads in the pool.

Using the ThreadPoolExecutor the operator may tweak the timeout value to adjust the thread pool size indirectly. This also alters the trade-off between QoS and performance. High timeouts result in a large thread count and better QoS, whereas a lower timeout comes with a smaller thread count and degrades QoS. Finding the correct value may be a tedious task for the operator, and this process may be repeated every time QoS requirements change. Furthermore, if 
 is high, we need higher timeouts too. Otherwise, turning off threads would result in high waiting times.

Our goal is to find a more efficient control of the thread pool size. Unfortunately, we cannot assume that the traffic is constant, which means that the optimal number for thread count may change throughout the system’s lifetime.

For the list of notations describing the thread pool, see Table 1.

4. A formulation of a problem
To apply RL, first, we formalize the problem as a Markov Decision Process (MDP) with a -tuple , where  is the set of states in the system;  is the set of actions;  describes the transition probabilities between states;  describes the immediate reward between state transitions; and  is the discount factor.


Table 2. Notations used for describing the MDP.

Set of states in the MDP
Set of actions in the MDP
Reward function in the MDP
Discount factor in the MDP
Decision times ()
Time gap between two decisions
Policy function
Value function following policy 
State
State at time 
Action at time 
Reward at time 
Approximated arrival rate
Approximated mean waiting time
Approximated blocking rate
QoS threshold for the mean waiting time
QoS threshold for the blocking rate
Reward multiplier hyperparameter
In the MDP framework, an agent interacts with the system. At timestamp 
 () the agent observes the state of the environment and decides on executing an action. As a result, the agent receives reward feedback from a system. In our case observations are made periodically with period , that is 
.

At each timestep, the agent decides according to a policy  which is a mapping between the set of states  and a probability distribution  over the set of actions . The agent’s goal is to find the optimal policy that maximizes the long term expected reward when followed. One of the main advantages of RL lies in its model-free property, which means that it is not expected of the RL agent to possess knowledge of the environment. The agent does not require the  transition probability function to reach the optimal policy.

One of the most critical steps in solving MDPs with RL is defining the state space, the action space, and the reward function. The description of a state has to include every information that can be essential for making a decision. However, it also needs to be compact so that the RL agent could explore the state space in a reasonable time. Similarly, having a smaller action space can make it easier for the RL agent to learn, but it also limits the possible moves it can choose. Lastly, the reward function drives the RL agent into the desired operating point when optimized. Usually, this is a difficult task, and a poorly defined reward may result in the RL agent being stuck in unwanted states.

4.1. State space
The observation of the agent is compiled into a state representation . The definition of  is crucial for the RL since including too many variables would explode the state space. It would unnecessarily elongate the training process, whereas too few variables may make it impossible for the agent to differentiate otherwise distinct states.

At any time  the state can be represented by a -tuple 
, where  is the number of active sessions,  is the number of threads (including booting, busy, and idle threads),  is the number of busy threads in the thread pool, and 
 is the measured arrival rate during the previous  time period. Note that the upper limit for the number of sessions  and the number of threads  are  and  respectively, that is ,  and . We will denote the states observed by the agent with 
 (), where 
.

4.2. Action space
At each decision point 
 the agent observes a state 
, from which it needs to take an action 
 with probability 
.  describes the set of actions that the agent may decide on. Note that the state of the system may change immediately after the execution of action 
, but the new state is not necessarily identical with 
, as 
 is the state that would be observed at time 
. We considered two cases for the set of actions (we will denote them with 
 and 
).

4.2.1. On/off actions
In the first scenario, we consider only actions that either turn on or turn off one thread. Let us define the following actions:

•
: a new thread is initialized in the thread pool;

•
: an idle thread in the thread pool is stopped, if all threads are busy working, then the first to finish its job will be stopped;

•
: no change is made.

That is 
. Obviously not all three actions are possible in every state. For action selection the following rules apply:

•
if 
, we cannot start any more threads, which means that  cannot be selected;

•
if 
, there are no threads to stop, that is  cannot be selected.

4.2.2. Multi-actions
The advantage in using 
 is that it is a small action space, which is easy to explore. Thus the RL algorithm can learn and converge faster. However, it leaves little freedom to the operator since it can only react to changes by either stopping or starting precisely one thread. Therefore, we experimented with another, larger action space, where multiple threads could be turned on or off at once.

In the second case we define only one type of action, 
, where 
 is the number of busy threads at decision time 
. In this setting the agent could select any value between 
 and  and start or terminate multiple threads in order to set the thread pool size to that value. The action 
, at time 
 would set 
 and would be the thread count for the time period 
. Obviously this is a much larger action space than 
, but in this case the agent has more flexibility in making adjustment in the system.

Note that the name multi-action scenario refers to the fact that we can start or terminate multiple threads in one action and not that we may execute multiple actions at a decision point 
.

4.3. Reward function
The reward function  describes the RL agent’s objective to optimize. When the system reaches a decision point at time 
, the agent receives the immediate reward 
 which is calculated based on the performance measures collected between the previous and the current decision points.

A well-calibrated thread pool should minimize resource usage while maintaining the quality of service (QoS). Here we are facing a multi-objective optimization problem as we want to minimize the number of threads in the thread pool while also adhering to the constraint of keeping the mean waiting time of the packets below a threshold level. A common way to tackle this is to aggregate the components into a single real reward value. This can be a very challenging task as different components may have different measures and numerical ranges. For more on multi-objective reinforcement learning see (Liu et al., 2015).

Motivation for a threshold-based objective for the RL algorithm is the provisioning of QoS for applications, i.e., the processing time requirement (QoS requirement) of a message carrying IoT data. The rationale behind the QoS provisioning is that processing data sensors that monitor critical events (like fire, …) needs a short time, while processing data of a greenhouse may not be so critical. We used the following reward function inspired by threshold-based algorithms: (1)
 where

•
 and 
 are the mean waiting time and blocking rate of packets measured between the previous and the current decision points 
 and 
, ,

•
 and 
 are the threshold values determined by the QoS,

•
parameter  is simply a multiplier to solve the issue of 
 when 
. Without  the 
 value would numerically outweigh 
 in the expectation of the reward when the waiting time is close to 
.

The intuition behind reward function (1) is that if 
 or 
 is too high and higher than the threshold value, then we need to minimize 
 which is the same as maximizing 
. However, if 
 and 
 are low enough and do not exceed the threshold 
 and 
, then we should minimize the number of threads to save resources. This is done by maximizing . Note that the reward having a negative value can be interpreted as a cost.

We also experimented with other possible reward functions, like Kleinrock’s power function (Kleinrock, 2018), but this objective cannot be adjusted for different QoS requirements. We tried a simple weighted sum of rewards too, but found that the reward function was too sensitive to the weights and setting the process of finding the weights was not as intuitive as our reward function. For the list of notations used to describe the MDP, see Table 2.

5. Reinforcement learning
Reinforcement learning finds the optimal policy 
 to be followed that maximizes the long term expected cumulated reward (2)
for the trajectory 
, .


Table 3. Notations used for the learning algorithms.

tr_st	Number of training steps
Policy neural network parameter vector
Value neural network parameter vector
Mean reward
Epoch
Batch size
Generalized advantage estimator
Generalized advantage estimation parameter
Entropy function
Entropy multiplier coefficient
Policy function learning rate
Value function learning rate
Reward averaging factor
Clipping ratio
par	Degree of parallelism
Population based training steps
Population based training repetitions
Reward multiplier perturbing factor
Entropy multiplier perturbing factor
Reward multiplier perturbing range
Entropy multiplier perturbing range
Grid search run count
Here the notation 
 means that we compute the expectation considering an MDP under the policy , and 
, often called the value function, is signifying the expectation of the total discounted reward if we follow policy  starting from state . Note, that the optimal policy does not depend on the starting state.

Fig. 3 shows us a model of a RL agent integrated into an ExecutorService that controls a thread pool. In this setup, threads in the thread pool would send statistics on message delay, thread state, traffic load etc. A monitoring service would interpret these observations and calculate the current reward and state. The RL agent could decide to start or terminate threads in the thread pool based on its policy from the state information. Using the reward, the agent could improve its policy with the learning algorithm.


Download : Download high-res image (208KB)
Download : Download full-size image
Fig. 3. Interaction between the RL agent and the thread pool.

In this work, we used the proximal policy optimization (PPO) (Schulman et al., 2017) method to find the optimal policy. We also experimented with other methods like the actor-critic algorithm, or deep q-learning (Sutton and Barto, 2018) and found them often not as accurate or stable as the PPO algorithm.

5.1. A general learning framework
Algorithm 1 shows the learning framework we used for simulations and training. It displays the interaction between the learning agent and the simulation environment. The training runs for  amount of steps, where in each step the agent executes an action and then stores the state, the action, the probability of the actions, the reward, and the next state to improve its policy.


Download : Download high-res image (300KB)
Download : Download full-size image
Here 
 is the policy function parameterized with vector . It maps the state 
 to a probability distribution over the possible actions, thus outputs a vector of size . We denote this vector with 
. We get action 
 by sampling an action from  using this distribution given by the policy.

5.2. The proximal policy optimization algorithm
In the PPO algorithm, the RL agent uses the  policy to interact with the environment and adjusts the  parameter vector during training to find the optimal policy. Here  is implemented with a non-linear approximator, a neural network. Aside from the policy , the algorithm also keeps track of the value function by estimating it with , a function approximated with a second neural network with  parameter vector. Both the policy function’s and the value function’s neural network receive the state as an input, but the former outputs a vector of size , whereas the latter results in a single real value.

Algorithm 2 is similar to the one presented by Schulman et al. (2017). The algorithm keeps track of 
, an estimate of the mean reward using soft updates. In each simulation step it runs  number of update steps and it stores a history of size  before running the update. Also, generalized advantage estimation (GAE) (Schulman et al., 2016) was used for estimating the advantage 
 and the reward scheme was modified for the average reward scenario for continuing tasks. We also used 
 as a regularization function. Note, that we used bold symbols to signify vector values and lower index  to represent their components (e.g.: 
 for a vector v of size ). Here 
 and 
 are the learning rates of the neural networks approximating the policy and the value functions.  is the coefficient that scales the entropy. The hyperparameter 
 is the reward averaging factor,  is the GAE’s hyperparameter, which implicitly contains the discount factor , and  is the clipping ratio used by PPO. The vector 
 is the so-called probability ratio of the actions  and the symbol  is the elementwise vector product. The notation  signifies the probability of action  in state  when the policy is parameterized with .


Download : Download high-res image (374KB)
Download : Download full-size image
5.3. Setting hyperparameters
In general, hyperparameter tuning is a costly operation in machine learning, and it is even more expensive in RL as it requires training data and comparisons in each tuning iteration. Two popular methods are grid search and random search (Bergstra and Bengio, 2012, Bergstra et al., 2011). In the former, hyperparameters have a set of fixed candidate values and training is run on every combination of them. In the latter, hyperparameters are given a range of values from which they are sampled randomly.

During the training, we found that for most hyperparameters, the literature’s default values, recommended by Schulman et al. (2017), were sufficient for the learning process. We identified two hyperparameters to which the algorithm was sensitive. The  reward coefficient and the  entropy weight. In the more simple experiments, we applied grid search for a few candidate values, but for the later experiments we used population based training (PBT) (Jaderberg et al., 2017).

PBT is a hyperparameter search method based on evolutionary algorithm principles. In short, it runs multiple () simulation instances in parallel for  steps, after which it executes so-called exploitation and exploration steps. Each simulation trains an RL agent, each with different sets of hyperparameters to tune. The algorithm evaluates each of the  agents during exploitation and sorts them according to a performance measure. A copy of the best agent replaces the worst agent. In the exploration phase, the hyperparameters of the new replacements are slightly perturbed. Finally, the training resumes for another  steps, and this is repeated  times.

This study used PBT to identify a small set of candidate  and  values. After that, we ran a grid search on these values and picked the hyperparameter set with the best performance. Algorithm 3 contains the hyperparameter search we used with PBT. In line 7 we rank the agents by performance, which means that first, we sort them by whether they can keep the mean waiting time below 
 and if they can, we sort them by increasing mean idle thread count, otherwise, we sort them by increasing mean waiting times. This results in 
 being the best performing agent, an agent that can keep the QoS level while having the lowest idle thread counts. In lines 9–11 we random sample 
 and 
 variables from uniform distributions and use them to perturb the  and  values of the new agent. Finally, we select the  and the  best performing  and  values, respectively and run a grid search on every combination of them.


Download : Download high-res image (516KB)
Download : Download full-size image
Note that the PBT involves one continuous training process. We initialized the agents and the environments initially and did not reset the training process after the exploitation and exploration steps. However, during the grid search, training was performed for each combination of 
 and 
 values for  times and the final hyperparameters were chosen based on the performance measures averaged from these  runs.

This search also introduces new hyperparameters, in this case , , , 
, 
, . However, we found that setting these hyperparameters was much easier, and the training was less sensitive to their changes. For the complete list of notations used to describe RL, see Table 3.

6. Design of investigation and results
For our experiments, we created simulated environments to evaluate the timeout rule and the RL method. We want to answer the following questions: is there an advantage in using multiple actions; can the RL method compare to the timeout rule; and under what conditions does RL outperform the timeout rule.

6.1. Simulation setup
We assumed sessions arrive according to a Poisson process with arrival rate 
. This is a reasonable assumption when working with a high number of IoT devices (Metzger et al., 2019). We considered a case with fixed arrival rates and another case with arrival rate varying through time. In the case of fixed arrival we considered a low traffic scenario with 
 
 and a high traffic scenario with 
 
.

For the case of varying arrival rate we used (3)
 
 
 
Note that equation (3) is borrowed from Wang et al. (2015) and scaled to 
. The purpose of scaling is to have the range of arrival rates comparable to the fixed arrival rate experiments. It is worth mentioning that function (3) was created using mobile network data, however, we believe it represents traffic created by human activity well and therefore is also suitable for our IoT experiments. We also assumed that the length of the session is distributed exponentially with mean . The maximum number of sessions in the system was set to .

When a session arrives, an actor is spawned for it with its own message queue. During their lifetime, each session sends packets with inter-arrival time 
. The service time 
 of a packet follows (4)
 
 where 
 is the base service time of each packet. We assumed the system has  CPU cores, that is, 
. While the number of busy threads  is below 
, the service time is no different from the base value. However, once  exceeds the available number of CPU cores, a contention starts for CPU time slots resulting in lower service times. The processing of the packet starts immediately on arrival if the queue is empty. Otherwise, it is stashed in the queue in a FIFO manner. We used bounded message queues and limited the queue sizes to .

The worker threads do the processing of the packets in the thread pool. When a worker finishes its job, it looks for the next session waiting for a thread in a round-robin manner. The maximum size of the thread pool was set to . Furthermore, we also set a minimum thread pool size of  so that the RL method would not terminate all threads while exploring the state space. We assumed turning off a thread happens immediately. However, turning them on takes 
.

6.2. Comparison of the timeout rule and the RL method at fixed arrival rates
When running the timeout rule we considered three timeout cases: , , and . For both the fixed and the varying arrival rate case, we simulated a  hour period and collected the performance measures for this period.

When RL is applied, we investigated the on/off and the multi-action action selection alternatives. For the reward function, we set the blocking threshold to 
 and we considered two cases for the delay threshold 
: a case with a lower QoS requirement 
; and a case with a higher QoS requirement 
. In case of 
 we set  and in case of 
 we set .

We used a neural network (NN) to approximate the policy and the value function. The NN has an input layer accepting state vectors. This is fully connected with the hidden layer, where the activation function is a rectified linear unit (ReLU). These two layers are shared between the policy and the value functions. For the former, the output is a layer fully connected to the hidden layer with nodes for each action. The result is converted to probabilities with a sigmoid activation function. A layer with a single node is fully connected to the hidden layer for the latter, and no activation function is used. Values for the training hyperparameters can be found in Table 4.

We ran training for  time steps and then evaluated the resulting RL agent for  time steps with . Fig. 4 shows us the number of threads during training for the first  steps for the on/off and the multi-action schemes. We can see that the RL agent could converge to the optimal policy under both action selection schemes. With the on/off actions, it took around  steps, whereas with multi actions it took  steps to converge because the multi-action action selection involves a much larger action space than the on/off action scheme.

Table 5, Table 6 show the results of the simulation runs under the timeout scheme and under the PPO agent. We displayed the mean number of total threads and idle threads for each run and the mean waiting time of the packets. We chose to omit the blocking rate. Blocking seldom happened due to the message queue being large enough most of the time.


Download : Download high-res image (248KB)
Download : Download full-size image
Fig. 4. Thread counts during the training of the AC algorithm for different environments at 
 
, 
, 
, and .


Table 4. Hyperparameters of PPO runs.

Name	Value
Neural network hidden layers	1
Hidden node count	50
Value function learning rate (
)	0.001
Policy function learning rate (
)	0.001
Reward averaging (
)	0.1
Batch size ()	32
Epoch count ()	5
Clipping parameter ()	0.1
GAE parameter ()	0.9
PBT parallel instances (par)	8
PBT iterations ()	50
PBT training steps ()	43200
PBT  perturbation range (
)	0.35
PBT  perturbation range (
)	0.25
Grid search run count ()	10
Looking at the results of the timeout rule, we can see the trade-off between the number of threads and the mean waiting time. We can see that the RL method could minimize the idle thread count while keeping the waiting times below 
.


Table 5. Comparison of the timeout rule and the PPO with  at 
 
.

Timeout rule
Timeout	Mean thread count	Mean idle threads	Mean waiting time
1 s	2.501584	1.520005	0.003406
10 s	3.906392	2.951080	0.000626
60 s	5.276596	4.330228	0.000087
RL (PPO, )
 [s]	Action	Mean thread count	Mean idle threads	Mean waiting time
0.1	on/off	2.126357	1.126818	0.013988
multi	2.388725	1.392581	0.012177
0.01	on/off	3.665027	2.672725	0.000503
multi	3.779104	2.788579	0.000412

Table 6. Comparison of the timeout rule and the PPO with  at 
 
.

Timeout rule
Timeout	Mean thread count	Mean idle threads	Mean waiting time
1 s	15.433190	5.664380	0.000854
10 s	19.102335	9.360207	0.000136
60 s	22.163054	12.402659	0.000019
RL (PPO, )
 [s]	Action	Mean thread count	Mean idle threads	Mean waiting time
0.1	on/off	11.595852	1.853114	0.056434
multi	10.933337	1.190067	0.058022
0.01	on/off	16.200938	6.449216	0.000554
multi	14.945809	5.186084	0.000618
6.3. Comparison of the timeout rule and the RL method at varying arrival rates
For the case of varying arrival rates we used a simple sine function 
 
, and ran  of training steps with a time step of  which is  weeks in simulation time. Then we used (3) to evaluate the RL agent for  time steps which is  day of simulation time. Fig. 5 shows us the graphs for the 
 used during training and evaluating.

In Table 7 we can see the results with the varying arrival rates. In the case of the RL experiments, we can see that using the multi-action selection method performed worse than the on/off actions and in the case of 
 the agent could not keep the main waiting time below the threshold in some of the runs. This is because the change in 
 through time was slow enough compared to  that turning on and off single threads were enough to react. Another reason is that the probability of the actions cannot go down to zero. This is caused by the entropy regularization, which prevents degenerate distributions for the action selection. As a consequence, the mistake the agent can do with the multi-action space can be much bigger. We can also see this in Fig. 6 which shows that controlling the number of threads was easier with the on/off actions. By restricting the action space, we could reduce the error the PPO agent made. Also, the results provided by the RL method are just slightly worse if we compare the results for the  timeout with the  threshold, where the timeout rule gave fewer idle threads and slightly lower waiting times. However, if we did not need such strict waiting times but wanted to further decrease resource usage, we could do it easily with the RL method. In case of the timeout rule, we would need to search again for a timeout value in a lower region. As we can see in Table 7, by setting a  threshold we could much lower the mean thread count.


Table 7. Comparison of the timeout rule and the PPO with  with varying 
.

Timeout rule
Timeout	Mean thread count	Mean idle threads	Mean waiting time
1 s	23.275186	7.099843	0.000773
10 s	27.675422	11.723045	0.000221
60 s	32.305623	16.138733	0.000108
RL (PPO )
 [s]	Action	Mean thread count	Mean idle threads	Mean waiting time
0.1	on/off	18.200269	2.108050	0.062803
multi	22.775796	6.554406	0.037386
0.01	on/off	23.408005	7.277181	0.002342
multi	22.819045	6.759416	0.065778
6.4. Performance under different 
 values
Looking at the results of the timeout scheme in Table 5, Table 6, Table 7 we see that the lower the timeout value, the lower the idle thread count, whereas the mean waiting time is still below the threshold 
. One might ask, why not further decrease the timeout value? In this particular case, it would be possible to decrease the timeout to get better results. However, after a point, the improvement would stop. If the timeout was very low, threads would be stopped shortly after finishing a job. This means that new incoming packets would arrive into a system without idle threads, and they would need to wait for a new thread to start to get processed. Thus, waiting time would depend on 
, the time it takes a thread to start. In Table 8 we can see the results of our experiments with the timeout scheme with different 
 values. It is obvious that decreasing the timeout value would increase the waiting time. However, at higher 
 values the mean waiting time reaches the threshold much sooner. For example if we set 
, at 
 the ideal timeout, where the mean waiting time is just below the threshold level, would be between  and  seconds according to Table 8. At 
 this value would be between  and  seconds. Thus, we would need an algorithm, like a binary search, to find an ideal timeout value for our system.

For comparison, we trained the PPO agent on the environment under different 
 values, Table 9 shows the results for on/off actions and Table 10 shows them for multi-actions. For these runs, we used Algorithm 3 to find the  and  hyperparameters. Looking at the results of the on/off actions, we can see that the agent could keep the mean waiting time below the threshold level while minimizing the number of idle threads. The RL agent could still maintain the mean waiting time below the threshold level with multi-actions and still performed better than the timeout method. However, it could not minimize the number of idle threads as well as with the on/off actions due to the much bigger action space the agent needs to explore and the entropy regularization that prevents zero probability actions. Another thing we can notice is that for higher 
 values, the timeout scheme needs a lot more threads to meet the QoS requirement than the RL agent.


Table 8. Results for the timeout scheme under different 
 values with varying 
.

Timeout	Mean thread count	Mean idle threads	Mean waiting time
1 s	23.183291	7.004011	0.038398
10 s	28.113262	11.971182	0.006512
60 s	32.163872	16.023724	0.001145
1 s	22.476157	6.929937	0.326515
10 s	28.394714	12.386796	0.056084
60 s	32.191699	16.075730	0.007978
1 s	22.336707	6.743414	0.661729
10 s	28.560191	12.544235	0.124329
60 s	32.270030	16.157706	0.017190

Table 9. Results for PPO agent with on/off actions under different 
 values with varying 
, 
.

Mean thread count	mean idle threads	Mean waiting time
0.0033	25.4578	21.9863	5.8618	0.003700
0.0137	19.2551	22.8223	6.7079	0.006921
0.1000	10.0000	23.9722	7.8588	0.004386

Table 10. Results for PPO agent with multiple actions under different 
 values with varying 
, 
.

Mean thread count	mean idle threads	Mean waiting time
0.0752	15.9797	23.4559	7.3053	0.006267
0.0116	4.6571	28.4877	12.3906	0.007994
0.0104	9.3111	42.4358	26.2788	0.003966
Fig. 7 shows that the RL method performs much better during the evaluation phase. A few idle threads can still maintain waiting times below the threshold. Note that the time between two decisions is  s. If 
 is larger than  s, a thread will not be ready at the next decision timestep and the recent decision will not have immediate impacts. Nevertheless, RL can still deal with these delayed rewards because the value function of state 
, denoted by 
, contains the next reward 
 and recursively contains the value function of the next state 
. This endless recursion guarantees that if effects of 
 only appear in a later reward 
, it will still be included in the value of 
. We can observe that with the timeout method, higher timeout values can guarantee low waiting times; however, they result in higher idle thread counts. The reason is that the policy of the RL decides based on mean traffic measurements, and it learned through averaging multiple rewards in a given state.


Download : Download high-res image (399KB)
Download : Download full-size image
Fig. 7. Comparison of the mean waiting time and the mean idle thread count during the evaluation phase at 
 and 
.

7. Conclusion
We have investigated the dynamic scaling of the size of a thread pool for an actor-based IoT application. We have formulated the problem as an MDP by defining its state space, action space, and reward function. We have used the PPO algorithm to find the optimal policy and used PBT to find the proper hyperparameter set under various scenarios. We have found that broadening the action space by allowing the agent to turn multiple threads on or off at the same time did not improve results. Results show that the timeout scheme falls off when threads take longer, whereas the RL approach still holds in these scenarios.