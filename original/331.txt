The cache-enabled network architecture is very promising to improve the efficiency of content distribution and reduce the network congestion. In this paper, we propose a maximizing weighted throughput (MWT) algorithm for joint request forwarding, cache placement and flow control to dynamically optimize network performance. Specifically, a dual queue system that includes requests and data is established to retrieve the global content demands and traffic congestion information. In order to improve throughput performance as well as stabilize the queue, we formulate the flow-level throughput and design the request-level control policy by optimizing the throughput function and Lyapunov drift. The request forwarding policy adaptively allocates request forwarding rates for every link according to the differences among adjacent request queue backlogs. A novel cache priority function and a threshold-based request-dropping policy are used in the caching policy and flow control respectively to alleviate network overload. In addition, we prove the MWT algorithm achieves throughput near-optimal performance, and testify the dual queue system is stable by deducing the upper bound of the all queue backlogs. The experimental results verify the MWT algorithm stability and demonstrate the superiority of the MWT algorithm, compared to the state-of-the-art caching algorithms which are combined with back-pressure algorithm.

Previous
Next 
Keywords
Content caching

Flow control

Lyapunov optimization

Queueing analysis

1. Introduction
With the rapid growth of mobile devices and wireless devices, we have seen traffic surges for working, entertainment and socializing. Notably, lots of repeated deliveries for popular contents would aggravate traffic burdens and cause huge resource wastes. Thereby, the wireless caching is developed to address the repeated deliveries for popular contents as one of the attractive candidate techniques for 5G communications (Luo et al., 2017, Poularakis et al., 2016, Long et al., 2016). Wireless caching technology allows base stations (BSs) and mobile devices to cache the popular contents, which reduces energy consumption and delivery delay. At the same time, the diversity of content distribution brought by caching brings some new challenges to the content distribution efficiency and network congestion issues in the cache-enabled network architecture, which is also drawing considerable interest from academia and industry.

Content caching can significantly relieve the heavy traffic burden on backhaul links and reduce service latency (Tao et al., 2016, Song et al., 2017, Shanmugam et al., 2013, Yang et al., 2018, Ali et al., 2020, Zhang et al., 2020). A smart collaborative video caching strategy is proposed in Tao et al. (2016) to improve energy efficiency in cognitive content-centric networking. The file download time is minimized in Shanmugam et al. (2013) by caching at the helpers and the cellular base station. Yang et al. (2018) analyzed and deduced the globally optimal probabilistic content placement strategy from the perspective of maximizing the hit probability under the constraint of document security. However, these caching strategies are designed based only on the known content requirements without regard to data forwarding issues, such as network congestion due to excessive traffic as well as link shared by multiple data objects.

In order to solve the above problems, significant research has also been done on jointly content caching and data forwarding. A novel jointly flow forwarding and rule caching decision is designed in Luo et al. (2020) for multiple user flows in software defined network. Liu and Shi (2019) proposed the online forwarding and real-time caching algorithms by jointly optimization request forwarding and data caching to minimize the average transmission cost, and utilized Lyapunov optimization to stable the network. Wang et al. (2018) designed a distributed scheme that combined request forwarding and content caching in order to reduce the traffic load, however no corresponding control measures can be taken when congestion occurred. These existing studies only relieve the traffic burden by caching and forwarding policies, but are invalid to solve the network congestion problems caused by malicious users who frequently request contents or misconfigured nodes that forward everything they receive. Therefore, when the system is on the verge of collapse, it is extremely important to take congestion control measures.

Congestion control can significantly improve the performance gain with caching in traffic load (Cui et al., 2016a, Lu et al., 2017, Li et al., 2020, Liu et al., 2019, Lee et al., 2015, Song et al., 2020, Huang et al., 2019). As one of the congestion control methods, flow control is effective to meet QoS requirements in practical applications or balance network load. A novel flow control mechanism is designed in Lu et al. (2017) to meet different QoS requirements and improve the utilization of network resources. In data enter networks, Lee et al. (2015) proposed a switch-based approach that identifies the target flow and provided different congestion control schemes using explicit congestion notification. In addition, consider a situation like this: if the transport path of a content object from its cache node to its destination node is severely congested, it is important to control traffic at the same time as forwarding requests. Therefore, it is necessary to jointly optimize content caching, request forwarding and flow control, which are intrinsically coupled.

In this work, we present a maximizing weighted throughput (MWT) algorithm for joint request forwarding, cache placement and flow control to improve the throughput performance of the network. To indirectly obtain the content demands and traffic congestion information, we design a dual queue system that controls the transmission rate of data by controlling the forwarding rate of requests. Moreover, we optimize the combination of throughput maximization and Lyapunov drift minimization to improve the throughput performance while stabilizing the queue. On the basis of optimization, we propose three policies including the request forwarding policy, request dropping policy and caching replacement policy. In the request forwarding policy, we schedule request for multiple data objects at a finer granularity so that they can be transmitted in the same slot, which can achieve better performance than the conventional back-pressure algorithm. The correspondence between data transmission rate and request forwarding rate is given with the help of virtual data. We adopt the threshold-based request-dropping policy to control the data flow, because of the data flow congestion caused by excessive requests. Under the dual queue system mode, a novel cache priority function based on request queue backlog and request forwarding rate is used in the caching replacement policy.

Our algorithm has the following three advantages. Firstly, requests injected into the network do not require any computation, i.e., the nodes do not need to calculate the available request receiver rates. Secondly, the algorithm has no need for explicitly exchanging the information of system time-varying parameters such as network congestion and link quality when it tracks the optimal solution. Finally, we provide delay guarantees for the network by deducing the upper bound of all queues. Our contribution is summarized as follows:

•
We formulate the weighted throughput maximization problem and combine it with the Lyapunov drift minimization to propose a MWT algorithm. The proposed algorithm can dynamically provide the request forwarding rates, the request discarding rates of all data objects, and update cache contents of all nodes in each slot.

•
We prove that the backlogs of request queues and data queues are deterministically upper bounded, the virtual data backlogs are stochastically upper bounded and the MWT algorithm achieves the throughput near-optimal performance.

•
We provide some numerical experiments to show the stability of the MWT algorithm and demonstrate the superiority of the MWT algorithm over the state-of-the-art caching algorithms which are combined with back-pressure algorithm.

The remainders of this paper are organized as follows. Section 2 discusses the related work and Section 3 presents the network model and the queue system model. In Section 4, we propose the MWT algorithm and prove the boundedness of all queues and throughput near-optimal performance of MWT algorithm. Finally, simulation results and conclusion are given in Sections 5 Simulation, 6 Conclusion, respectively.

2. Related work
Back-pressure algorithm (Awerbuch and Leighton, 1993) is a common method to obtain network state information in stochastic network optimization (Neely, 2010). In the back-pressure algorithm, the forwarding rate of a link is based on the differences of queue backlogs at transmitting and receiving nodes. The results show that, within the network stability region (Cui et al., 2016b), the throughput performance of the general multi-hop queueing networks can be optimized by adopting the back-pressure algorithm to ensure the stability of the network under any arrival rate vector. Back-pressure algorithm has been widely used in resource scheduling, routing and forwarding, but the back-pressure algorithm for forwarding are mostly operated on the data queues. To reduce the delay, a new virtual queue-based back-pressure scheduling algorithm is designed in Jiao et al. (2015) for wireless sensor networks.  Hai et al. (2018) proposed an improved back-pressure algorithm to enhance the delay performance by introducing a novel delay metric called the sojourn time backlog for multi-hop wireless networks. More efficient routing schemes are proposed in Li and Modiano (2015) and Paschos et al. (2016) according to the back-pressure-based forwarding in order to reduce the waste of resources and improve delay performance.

Unlike the work above, which is initiated by the source nodes, data transmission is usually initiated by the data requester in the cache-enabled network. Therefore, the back-pressure algorithm that operates on data queues fails to take into account dynamic content requirements. To solve this problem, a back-pressure algorithm for request queue operation is proposed in Yeh et al. (2014), which uses local request queue information to provide the demand information of content caching and data forwarding. Wang et al. (2018) designed a distributed scheme that combined request forwarding and content caching by considering both the information of network congestion and content requirements can be obtained from local queues. However, our problem is different because we consider the coupling between content caching, data forwarding, and flow control. To this end, we set up a queue system consisting of request control sub-system and data queue sub-system, such that the joint information of content requirement, forward decision and network congestion can be obtained from the queue system.

The problem of joint cache and routing optimization has been extensively studied for caching networks in recent years. In Chu et al. (2018), a joint caching and request routing algorithm that include a cache partitioning approach and a content-oblivious request routing algorithm is proposed in view of maximizing the sum of utilities over all content providers. Over an arbitrary network topology, Ioannidis and Yeh (2018) equated the routing costs minimization problem to maximizing cache gain problem by jointly caching and routing approaches. Kulkarni and Seetharam (2020) designed a caching and routing strategies by utilizing model-based and machine learning approaches in view of improving the delay and hit rate performance and showed the main factors that affect caching and routing performance. However, due to the multi-source problem (Mau et al., 2015) in the cache-enabled network, more and more researches focus on jointly content caching and request forwarding strategies.

A novel jointly flow forwarding and rule caching decision considering the heterogeneous transmission performance and the limited cache space is designed in Luo et al. (2020) for multiple user flows in software defined network.  Yeh et al. (2014) proposed a throughput-optimal algorithm for named data networks, in which forwarding and caching were decoupled by local optimization. The performance of the throughput-optimal algorithm (Yeh et al., 2014) is further improved in Lai et al. (2016) by obtaining the traffic of interest packets more accurately under interest suppression in named data networks. However, this decoupling method reduces the performance improvement by loosening the bound of the Lyapunov drift. These existing studies have greatly simplified the problem by using some technical means or heuristic algorithms to avoid the coupling between content caching and data forwarding. Therefore, we combine network utility maximization and Lyapunov drift to solve the coupling between caching and forwarding head-on.

There are several common methods to deal with stability-based delay-aware resource allocation (Wang and Lau, 2014). In general, the Markov decision process (Hui et al., 2020) can minimize the delay, but due to the brute force value iteration, it will lead to the dimensional disaster. Large deviation (Cui et al., 2012) is an effective approach to convert delay constraints into equivalent rate constraints. But this method can only get better delay performance under a large delay region. Lyapunov optimization (Asheralieva and Niyato, 2020) can ensure the queue stability of network system as long as the average arrival rates are in the stable region of the network. By utilizing Lyapunov optimization theory, we optimize the delay performance by reducing the average request queue backlog. We also improve the throughput performance while stabilizing the queues by combining throughput maximization and Lyapunov drift minimization.

3. System model
3.1. Network model
The cache-enabled network is modeled as a directed graph  with  nodes and  links. If a request transmits over a link , the corresponding data will be transmitted on the opposite link . Let 
 and 
 be the sets of ingress and egress nodes of node , respectively. Time is slotted. Let 
 be the time-varying link capacity information during slot . We have summarized the notations of main symbols in Table 1.

Each node  has a storage space which can be used to cache the popular data. The cache size of each node  is denoted as 
. Assumed that the cache sizes are limited. The contents in the cache-enabled network are identified as  data objects, denoted as . These data objects may arise from the application naturally, and are related to the amount of control state the network can hold. For convenience, assume that the cache sizes of all data objects are the same (Dehghan et al., 2017, Ioannidis and Yeh, 2018, Lai et al., 2016). We consider the case 
, i,e., no node can cache all data objects.1 Meanwhile, a server node denoted as ħ that can store all data objects permanently is designed for system stability. We define 
 as the cache status information of the whole network at time slot , where 
 denotes that node  caches data objects  in slot . Therefore, the set of available sources of content object  can be defined as 
.


Table 1. List of major notations.

Notation	Description
Network graph with nodes  and links 
The sets of ingress nodes of node 
The sets of egress nodes of node 
The time-varying link capacity during slot .
The cache size of each node 
Data objects
Cache status information at time slot 
The set of available sources of content object 
Request queue for data object  of node 
Data queue for data object  of node 
Drop queue for data object  of node 
Virtual data queue for data object  of node 
Request forwarding rate for object  over link 
Data rate for data object  over link 
Virtual data rate for data object  over link 
Drop rate for data object  of node  in slot 
The global request arrivals vector
3.2. Design of queue system
Let 
 and 
 be the request queue and the data queue for data object  of node  in slot . Unsatisfied requests and data for object  waiting to be transmitted are stored in 
 and 
, respectively. Let 
 and 
 be the request drop queue and the virtual data queue for data object  of node  in slot . Dropped request and virtual data for object  are stored in 
 and 
, respectively. Notice that 
 contains not only the content demands information, but also the potential network congestion information because data is triggered by the request. Therefore, we combine request forwarding with a threshold-based request-dropping strategy to reduce network load. Let 
 be the request forwarding rate for object  over link  in slot . With requests forwarding, the unsatisfied requests are dispersed to downstream nodes and the request queue backlog has a descending gradient from node  to the content sources. When the length of the request queue exceeds the system-defined threshold, i.e., the length of drop queue 
, the excess requests will be discarded into the drop queue 
 in slot , which is effective in reducing network traffic load.

The concept of the dynamic correspondence is that the data transmission rate of link  remains the same as the request forwarding rate over the adverse link . However, the length of the data queue 
 may not be sufficient for transmission. Therefore, to make the dynamic correspondence valid, the virtual data queue 
 is introduced. Let 
 and 
 are the data rate and the virtual data rate for data object  over link , respectively. Before determining the values of 
 and 
, we need to discuss the relationship between 
, 
 and 
. We assume that 
, i.e., the request forwarding rate over link  is not zero, and 
. The dynamic correspondence between request rate and data rate is as follows:

•
If the length of the data queue 
 is greater than or equal to the sum of the request forwarding rates, i.e., 
, then the data queue 
 has enough data to transmit on the opposite path without the help of the virtual data queue 
. Therefore, in order to ensure the balance between the request flow and the data flow on the two-way link, it is only necessary to set the data transmission rate that is equal to the request forwarding rate, and the virtual data transmission rate is 0. In other words, for all 
, set the data transmission rata 
 and the virtual data transmission rate 
.

•
If the length of the data queue 
 is less than the sum of the request forwarding rates, i.e., 
, then the length of data queue 
 is not sufficient for transmission on the opposite path. For this reason, the virtual data queue 
 is used. Due to the lack of data, each link will be allocated a certain amount of virtual data to ensure flow balance. In order to distribute the insufficient amount of data equally across each link, we define an auxiliary variable to represent the amount of data that is insufficient on each link 
, where 
 and  denotes the cardinality of the set . Therefore, the data transmission rate is equal to the request forwarding rate minus the amount of insufficient data allocated on average, and the virtual data transmission rate is equal to the request forwarding rate minus the data transmission rate, i.e., for all 
, set the data transmission rata 
 and the virtual data rate 
, where 
 represents .

When the requests are forwarded to the node which has cached the corresponding data objects, the requests exit the system and then the same number of data copies will be generated to satisfy the requests. In addition, the data and the virtual data exit the system at a rate 
, where 
 denotes the request arrival of data object  at node  in slot . The difference between virtual data and data is that when virtual data is received by the node which requests the data, additional requests are added to the request queue because the requests are not met.

3.3. Queueing structure in queue system
3.3.1. Structure of the request queues
There is a random request arrival 
 at each node , where 
 is a constant. The global request arrivals vector from the applications is denoted as 
. We assume that 
 are independent across content objects  and nodes , and are i.i.d. over slots with mean 
. The structure of the request queue 
 evolves according to (1)
 

The link capacity constraint 
 must be satisfied in every slot. To solve the potential congestion, the request queue 
, after forwarding requests to neighbor nodes in slot , discards 
 requests from the remaining backlog. Under given control policy, the drop rate 
 takes values in 
, where 
 is a finite parameter depends on the system. In the first term, 
 shows that if node  has cached the object , then many data copies will be generated to satisfy the requests during slot . That is, the request queue at time slot  is emptied. At the same time, an equal amount of data is generated and added to the data queue. 
 represents the request arrival rate at node , and 
 is the number of additional requests added to the request queue due to virtual data consumption. The last term is the forwarding request from all neighbor nodes received by node .

For convenience, 
 is defined as the maximum request forwarding rate allowed by the channel. The following assumption will be used throughout the paper.

Assumption 1

We assume 
.

Similar to Assumption 1 in Li and Modiano (2015), according to Eq. (1), we can obtain the request arrival rate of each node is not greater than 
 because of 
. Therefore 
 is also an upper bound on the maximum request queue overflow rate at any node. Assumption 1 guarantees that the maximum request-dropping rate is great than or equal to the maximum request queue overflow rate, so the request-dropping rates can be always used to reduce the network congestion.

3.3.2. Structure of the drop queues
To control the request-dropping rate directly, we establish a drop queue 
 associated with each request queue 
. Before leaving the system, the requests dropped from 
 are moved to 
. Then they will be discarded permanently according the system policy. The dynamics of the drop queue 
 satisfies (2)
where 
 is a decision variable based on the backlogs of request queue 
 and drop queue 
, and 
 is the actual requests dropped from the request queue 
 in slot . From Eq. (1), we have (3)
 
which can be less than 
 if there are insufficient requests to be discarded. Assume 
 for all  and , which avoids unnecessary request dropping at the beginning of the system.  is a system parameter that will be defined in Section 4.2.

3.3.3. Structure of the data queues
In addition to request queue 
, each node also contains a data queue 
 that is used to store corresponding data on reverse links. The structure of the data queue 
 evolves according to (4)
Data are generated only on these nodes that have cached specific data object, so the request queues of these nodes are cleared and the same amount of data is generated. The term 
 represents the request queue backlog for the cache corresponding data node is added to the data queue. And the term 
 represents the data consumed by the local requests.

3.3.4. Structure of the virtual data queues
The structure of the virtual data queue 
 evolves according to (5)

If node  does not cache object  and the backlogs of 
 and 
 are less than the sum of endogenous request arrival rates 
, then the virtual data are generated. In addition, the virtual data is also consumed to satisfy the local requests when virtual data queue 
 is not empty. As the virtual data is consumed, the additional requests are added to the request queue.

Focusing on a content object , for example see Fig. 1, our queueing system works as follows. Similar to the back-pressure algorithm, node  has a downward gradient to the data source. The length of the request queue implies the urgency of the node’s data requirements. In the request queue control sub-system, 
 determines the number of requests for data object  transmitted on link , 
 decides the number of requests that internally moved from request queue to the drop queue of node , and 
 is the number of requests that are permanently discarded from the drop queue of node . In the data queue sub-system, 
 and 
 are the numbers of data and virtual data transmitted over the reverse link , respectively. The data queue of node  is empty and 
 mean that the data is insufficient, so the virtual data is used for transmission. Notice that the virtual data is not actually transmitted over the real link, hence it does not provide real data to satisfy the requests and the additional requests are added to the request queue.

4. Throughput maximization
We consider the throughput maximization of all content objects in cache-enabled network. Firstly, we formulate the flow-level throughput and get the average request dropping rate that affects the throughput. Secondly, a request-level control policy is obtained by optimizing the throughput function and Lyapunov drift. Finally, the MWT algorithm is proposed and the performance of MWT is analyzed.

4.1. Flow-level throughput
Before the request-level control policy is given, it is beneficial to understand the flow conservation of the queue system. We prove the optimality of the proposed MWT algorithm by using the flow-level solutions.

For convenience, we consider a scenario where the content sources of content object  have not changed over a long period of time. Let 
 be the average data object  request forwarding rate over link  and 
 is the average request dropping rate of node . Therefore, all the variables 
 and 
 satisfy the link constraints and flow conservation: (6)
ħ
(7)
ħ(8)
 where 
 is the mean of random request arrivals, 
 represents the average additional request arrivals and 
 is defined as the average link capacity of link . The average request dropping rate 
 is considered feasible if the flow variables 
 satisfy Eqs. (6)–(8).

Summing Eq. (7) over 
, the average incoming request rate of all content sources 
 is given by (9)
ħ
ħ
The content sources will generate the same amount of data to be transmitted on opposite links, so the average incoming request rate become also the average data arrivals of all content sources of data object . Since the rate of data and virtual data on the adverse path is controlled by the request rate and the average additional request arrivals 
 is, in effect, equal to the mean of the virtual data consumption rate, the throughput of data object  is equal to the average incoming request rate of all content sources less the sum of the average virtual data consumption rate 
ħ
. Therefore, the throughput of data object  is given by (10)
ħ
ħ
ħ
which shows that the throughput of a data object is equal to the sum of exogenous request arrival rates less the sum of request dropping rates.

Definition 1 Capacity Region

Given a request arrival rate matrix 
, let the capacity region 
 be the set of feasible throughput vectors 
 under the request forwarding rate allocation and request dropping rate control algorithm that satisfies the link capability constraint, i.e., (11)

We assign a utility function 
 to each data object . The problem of maximizing the weighted sum throughput of all data objects is considered in the cache-enabled network. Define 
 for each data object , where 
. Therefore, we have the optimization problem (12)
Substituting Eq. (10) into Eq. (12), we get (13)
Since 
 are unknown constant, maximizing 
 is the same as minimizing the weighted request dropping rate. Consequently, the maximizing weighted throughput problem Eq. (12) is equivalent to the minimization problem (14)
Next, a request-level control policy is designed to stabilize all queues and achieves optimal request dropping rates in the cache-enabled network. The maximum total weighted throughput is also achieved because of the equivalence of Eqs. (12), (14).

4.2. Request-level control
Considering the stability of all request and drop queues, we use the Lyapunov optimization method (Neely, 2010) to set up a strictly increasing function of the 
 and 
, i.e., we use the following Lyapunov function (15)
 
 
Let 
 be the vector of request queues and drop queues in slot . Then define the Lyapunov drift (16)The control strategy of minimizing Lyapunov drift in each slot is sufficient to limit queue backlogs and stabilize the network.

In addition, the minimization problem (14) is also considered. Therefore, we have (17)
≔
 
 
According to queueing theory, if a drop queue 
 is stable, then its request arrival rate must be less than or equal to its time average service rate, i.e., from Eq. (2) we have (18) 
 
 
 
 
Our method forces the bound to be tight, hence if all 
 queues are stable, we can minimize the upper bound in Eq. (18) in order to minimize Eq. (17). In fact, it is sufficient to minimize the sum 
 in each slot, where 
 is a compact representation of the condition expectation .

Minimizing the sum 
 make the network more congested because less requests are discarded, which conflicts with minimizing Lyapunov drift . Therefore, we minimize the following formula (19)
where  is pre-defined by the system, and represents the relative importance of the queue stability and the minimum request dropping. In Section 4.4, we also conclude that  determines a trade-off between the performance gap of the MWT algorithm from optimal solution and the finite queue length required in the request queues 
.

4.3. The proposed algorithm
By minimization of Eq. (19), we get the request forwarding, caching and congestion control policies. The upper bound of Eq. (19) is derived in Appendix A. (20)
 
 
 
  
 
 
  
 
 
  
 
 
 where 
 
is a constant. Next, we propose the MWT algorithm.

The details of the MWT algorithm are provided in Algorithm 1. And the pseudo-code starts at the beginning of each time slot. The request forwarding policy, request dropping policy, and cache replacement policy in Algorithm 1 are detailed in Sections 4.3.1 Request forwarding policy, 4.3.2 Request dropping policy, 4.3.3 Cache replacement policy respectively.


Download : Download high-res image (476KB)
Download : Download full-size image
4.3.1. Request forwarding policy
For each data object , the request forwarding rate over link  is allocated as follows (21)
where 
 can be attained from the link capacity constraint (22)
and 
 is a weight which depends on the difference between the two adjacent request queues 
 and 
 for data object . The request forwarding policy is based on the Theorem 2 in Wang et al. (2018), which can achieve better performance than the conventional back-pressure algorithm.

4.3.2. Request dropping policy
Each request queue 
 moves 
 requests to the drop queue 
 at the end of slot , where (23)
 The drop queue 
 moves 
 requests from the system according to (24)
 

4.3.3. Cache replacement policy
Let 
 be the available caching set of node  in slot . Compute the priority function 
 for each data object 
.

•
If 
, then node  caches the object  and sets 
, and sets 
.

•
If 
, let 
 and sort 
 from big to small. Then node  caches the data objects corresponding to the first 
 elements in 
.

The MWT algorithm uses the threshold-based request-dropping policy, therefore, the requests injected into the network do not require any computation. In addition, the information of content popularity and network congestion are presented in the form of request queue backlog and data queue backlog respectively. Therefore, the MWT algorithm has no need for explicitly exchanging the information when it tracks the optimal solution. Finally, the MWT algorithm provides the delay guarantees for the network because of the boundness of all queues which is confirmed in Theorem 1, Theorem 2.

4.4. Performance evaluation
Theorem 1 Boundness of Request Queues, Drop Queues and Data Queues

For each data object , 
 and 
 are upper bounded by (25)
Here 
≔
 and 
≔
. Moreover, the drop queue 
 is true for all  and . In addition, the average data queue backlogs satisfy (26) 
 
 

The proof is provided in Appendix B. In Theorem 1, the terms 
 and 
 are the finite buffer sizes sufficient at queue 
 and queue 
 respectively. The MWT algorithm gives a larger buffer to a data object that produces a better reward to avoid request dropping in that data object.

Theorem 2 Bound of the Virtual Data

According to the Chebyshev’s theorem, the generated virtual data is bounded and satisfies (27)
ɛ
ɛ
 
where  denotes the variance and ɛ denotes the distance between the total generated virtual data and the threshold. We can figure out the values of 
 and 
 according to their density functions.

The proof is provided in Appendix C. From Theorem 2, the probability that the total generated virtual data tends to infinity approaches , indicating that the amount of virtual data has a random upper bound, and adding virtual has no effect on the stability. From Theorem 1, Theorem 2, we conclude that a finite buffer size 
 is sufficient for all queues. The next theorem demonstrate that the performance of the MWT algorithm approaches optimality as the buffer sizes 
 and 
 increase.

Theorem 3 Optimality of MWT Algorithm

Under MWT algorithm, the total weighted throughput satisfies (28)
 
where 
 
 
is the total throughput of data object  and 
 is a solution to Eq. (12). An arbitrarily small performance gap 
 
 can be obtained by setting  large enough.

The proof of Theorem 3 is provided in Appendix D. From Eq. (28), we prove the near-optimal performance of the MWT algorithm by choosing a sufficiently large parameter , where the large parameter  implies the large buffer sizes of 
 and 
.

Finally, we analyze the performance overhead, including queue system overhead and communication overhead, caused by the MWT algorithm allowing nodes to forward requests.

Queue system overhead of the MWT algorithm: To make the dynamic correspondence valid, we introduce the virtual data queue in the data queue sub-system, which will bring some additional overhead to the queueing system. Theorem 2 shows that the amount of virtual data is bounded in probability and the addition of virtual data does not affect the stability of the MWT algorithm.

Communication overhead of the MWT algorithm: The communication overhead of the MWT algorithm increases slightly by allowing nodes to forward requests. In order to clearly describe this effect, we consider the case that the forwarding rate of node  is not 0, i.e., it is assumed that the request queue length of node  is larger than that of the neighbor node, and the length of the frame is . The signaling process consists of three stages: (1) Request broadcasting. It takes 
 for node  to broadcast the request queue length information to neighbor nodes. (2) Request forwarding. It takes at most 
 for node  to forward the requests to neighbor nodes, where  is defined as the maximum degree of the cache-enabled network graph. (3) Data transmission. It takes 
 for node  to transmit data or virtual data to node .

5. Simulation
We provide the numerical simulation results that show the performance of MWT algorithm. In this paper, our algorithm and compared algorithms are implemented in MATLAB 2018a. Moreover, we run all experiments on a personal computer with an Intel(R) Core(TM) i7-7700 CPU @  with  memory. The simulation network is a randomly generated connected graph with  nodes. We utilize Shannon formula to calculate the link capacity 
, where 
 is the SNR in the Rayleigh channel and  denotes the channel bandwidth. Set the cache size 
 for each node  and the throughput weight 
 for each data object . In addition, only a node that can cache all objects is designed to stabilize the network. In this experiment, we assume that the average request arrival rate for all nodes is the same and is a Poisson process.

We adopt , and 
 for each data object and node. Fig. 2(a) shows that the impact of parameter  on average request queue backlog and average throughput. In a particular network environment, the value of  is very important to the network performance. Under the network environment in Fig. 2(a), we can obtain the highest average throughput and the lower average queue backlog when . Therefore, we will use  throughout the experiment. The stability performance of the MWT algorithm is shown in Fig. 2(b). The policies with and without request dropping queue are stable but the average request queue backlog of request dropping policy is smaller than that of the policy without request dropping queue. From  to  time slot, the average backlog of request queues falls oscillatorily. The reason for this phenomenon is that: (1) if a node has cached a data object, many data copies will be generated to satisfy the requests, which leads to the decline; (2) Several time slots may be taken to transmit a data object from a node to another because of the channel capacity constraint, which is the reason for the occasional increase. The average backlog of all request queues fluctuates in a limited region after  time slot, which supports the stability analysis of the MWT algorithm. From Fig. 2(c), we observe that the request dropping policy can obtain almost the same throughput and the lower average queue backlog than the policy without request dropping queue. Fig. 2(c) further validates that the request dropping policy can reduce queue stress without significantly reducing average throughput. This important finding demonstrates that the MWT algorithm can reduce network congestion while maintaining high throughput requirements.

We compare our MWT algorithm with other four cache algorithms. (1) DFC Algorithm: Distributed forwarding and caching (DFC) algorithm (Wang et al., 2018) considers the request forwarding and caching based the request queue backlogs. (2) VIP Algorithm: VIP algorithm (Yeh et al., 2014) combines back-pressure interest forwarding and caching based the virtual interest count (VIP). (3) LRU Algorithm: LRU algorithm combines the least recently used (LRU) caching policy (Yang et al., 2018) with the back-pressure request forwarding (Neely, 2010). (4) LCE Algorithm: LCE algorithm combines the leave copies everywhere (LCE) caching policy (Long et al., 2016) with the back-pressure request forwarding (Neely, 2010).

In Fig. 3, our MWT algorithm always outperforms the four cache algorithms, since we consider the problem for joint request forwarding, cache placement and flow control to dynamically optimize network performance. Under the MWT algorithm, the request forwarding policy is based on the differences of adjacent request queue backlogs and the content placement policy is based on the local information about queue backlog and request forwarding rate.

As shown in Fig. 3(a), as the average request arrival rate increases, the average request queue backlog for these five algorithms also increases. When , the performance gaps are small because low average request arrivals do not cause traffic overloads. However, as  increases, the performance gap between MWT algorithm and the other four algorithms also increases. When  is very large, a large number of requests are injected into the network in a short period of time, which may lead to network congestion caused by link overload. Because the algorithms (DFC, VIP, LRU, LCE) cannot deal with the network congestion, the number of requests in the request queue is very large, which implies that the average request queue backlog is very high. However, the MWT algorithm considers the problem of network congestion and adopts the request discarding strategy. When the length of the request queue exceeds the system-defined threshold, the request queue will discard part of the requests according to the request drop policy to reduce the link load.

Fig. 3(b) demonstrates the importance of a better caching algorithms when the cache size is limited. When the cache size 
, the performance of these five algorithms varies greatly. This is because the small cache capacity is not enough to satisfy the entire network’s requests for data objects. Cache-enabled networks allow routing nodes to cache data objects and respond to data requests, thereby increasing the rate of content delivery. Therefore, a good cache replacement policy is the key to increasing the rate of content delivery. The other four algorithms used by the comparison are not based on the request queue form, but the general cache algorithm and the back pressure algorithm combined. However, the cache replacement policy in MWT algorithm is to design a cache priority function based on the backlog value of request queue. The MWT algorithm caches data objects with a high priority function, and periodically changes the data objects to be cached based on the cache priority function. Because the backlog of the request queue reflects the demand for data objects, the cache replacement policy based on the request queue backlog has real-time effects. Therefore, the MWT algorithm performs well with limited cache capacity. The performance gap gradually decreases as the node cache capacity increases, indicating that the average request queue backlog is very low when the node cache capacity is infinite. In this case, the average request queue backlog is lower regardless of the algorithm used. However, the cache capacity of nodes cannot be infinite, so under the limited cache capacity, the proposed MWT algorithm has superior performance.

Fig. 3(c) and (d) show the average request queue backlog differences of these five algorithms under different nodes and different data objects, respectively. As the number of nodes and data objects increases, so does the average request queue backlogs for all five algorithms. The trend of the all lines gradually flattens out of these two figures. The average request queue backlog of MWT algorithm under different nodes and different data objects is smaller than that of the other four algorithms, which is because MWT algorithm combines request forwarding, request discarding and cache replacement to optimize network performance. Although the other four algorithms all adopt the request forwarding policy based on the back-pressure algorithm, the MWT algorithm adopts the request forwarding strategy based on the upgraded back-pressure algorithm which can realize the forwarding of multiple data objects simultaneously. Therefore, MWT algorithm is superior to other algorithms in request forwarding, which is also one of the reasons why MWT algorithm has better performance.

6. Conclusion
In this paper, we presented a distributed algorithm that maximizes the weighted throughput of all data objects. Firstly, we introduced a dual queue system to retrieve the global content demands and traffic congestion information. Secondly, to improve throughput performance as well as stabilize the queue, we formulated the flow-level throughput and designed the request-level control policy by optimizing the throughput function and Lyapunov drift. Finally, to alleviate network overload, we gave a request forwarding policy, a novel caching policy and a threshold-based request-dropping policy in the MWT algorithm. The performance of MWT algorithm was analyzed and proved to be near-optimal. The experimental results showed the MWT stability and that the performance of MWT is superior to the existing ones. In future work, we are prepared to design the optimal algorithm for cache-enabled networks.


