With the rapid increase of human activities in cyberspace, various network intrusions are tended to be frequent and hidden. Network intrusion detection (NID) has attracted more and more attention from industrial and academic fields. Over the years, researchers have developed artificial intelligence methods to tackle them. However, most existing methods are usually not feasible and sustainable when faced with the demands of current NID systems. To alleviate this problem, this paper proposes a novel convolutional neural network (CNN) named RANet for NID automatically. In RANet, we not only introduce a Group-Gating module but also apply the overlapping method to the last max-pooling layer. Based on the hyper-parameter settings of our RANet, a lot of performance comparison experiments are conducted. The results demonstrate that RANet achieves better NID performance than strong baselines and existing state-of-the-art methods on five publicly available NID benchmarks. For example, the RANet improves accuracy with approximately 5.3% on NSL-KDD 
 dataset through comparisons to state-of-the-art baselines. Moreover, the results of RANet also indicate that it has the great potential or use in current NID systems.

Previous
Next 
Keywords
Network intrusion detection

RANet

Group-Gating module

Convolutional neural network

1. Introduction
A network intrusion detection system (NIDs) can be a hardware or software-based system to monitor malicious activities (malicious traffic) on a network system. With the rapid development of information technology, it is challenging and necessary to construct a robust and efficient NIDs to protect network security in dynamic network environment. Given the increasing significance of NID techniques in the network defense system, researchers have proposed massive machine learning methods (Yuan et al., 2017, Hacibeyoğlu et al., 2016, Khammassi and Krichen, 2017, Papamartzivanos et al., 2018, Wu et al., 2020, Hasan et al., 2014, Ramaiah et al., 2021) to address the NID problem and have achieved great improvements over the years. However, it is a bottleneck for existing machine learning methods to work well when massive network attacks are produced, and network attacks vary quickly in the period of big data. Thus, it is impossible to label every unknown network attack manually in advance.

Given limitations of machine learning methods in feasibility and sustainability facing the growth of network attacks in cyberspace. Deep learning methods have recently achieved great success in many domains, such as image classification, video analysis, natural language preprocessing (NLP), medical data mining, etc. Researchers also have gradually used deep learning methods to tackle the NID problem, such as artificial neural networks (ANNs) (Ingre and Yadav, 2015, Singh et al., 2015), auto-encoder (AE) (Shone et al., 2018), deep neural networks (DNNs) (Kim et al., 2017), recurrent neural networks (RNNs) (Yan and Han, 2018, Xiao et al., 2019), and convolutional neural networks (CNNs) (Lin et al., 2018, Wu and Guo, 2019, Vinayakumar et al., 2017, Xiao et al., 2019, Li et al., 2020, Cui et al., 2018, Chawla et al., 2018, Li et al., 2017, Liu et al., 2019). Although deep learning methods achieved better NID performance than machine learning methods, the generalization ability of deep learning methods in predicting unknown network attacks is also poor.

To alleviate this problem and improve the performance of NID, this paper presents a Convolutional Neural Network (CNN) named RANet for NID. In RANet, inspired by group decision-making mechanism (Song and Hu, 2019), we introduce a novel convolution module named Group-Gating module. The proposed Group-Gating module is the core component of RANet, which is comprised of a group part and a gating part. It not only can learn diverse feature representations but also can learn to remember important feature representations and forget unimportant feature representations. To improve the NID results furtherly, the max pooling operation with overlapping method (overlapping max pooling) is used to select more important feature representations while lowering the computational burden. Based on hyperparameter setting experiments, a series of performance comparison experiments are conducted to verify the effectiveness of our RANet on public NID datasets. The experimental results demonstrate that our RANet significantly outperforms strong baselines and previous state-of-the-art methods. The main contributions of this paper are summarized as follows.

1.
This paper proposes a novel CNN model named RANet to detect multi-class network intrusions efficiently. Inspired by group decision-making mechanism, we design a novel Group-Gating module, which not only captures diverse feature representations but also learns to remember important feature representations and filter out unimportant feature representations.

2.
We test three different pooling methods for the last pooling layer based on RANet, and results show that the proposed overlapping max pooling method works better than the other two.

3.
Extensive experiments are conducted on five public NID benchmarks. The results show that our RANet surpasses strong baselines and existing advanced methods.

The rest of this paper is organized as follows. In Section 2, we briefly review existing works of NID methods and recent advances in CNNs. Section 3 details the general framework of NID based on the proposed RANet architecture. Section 4 introduces public NID datasets, feature normalization methods, experiment setup, and evaluation measures. Section 5 provides hyper-parameter configuration analysis, and the result and discussion of our RANet and comparable methods. We conclude this paper in Section 6.

2. Background
In this section, we briefly survey existing works of NID and recent advances in CNNs.

2.1. Existing works of NID
Machine learning based NID. In the past decades, researchers have proposed advanced machine learning methods to address the NID problem. Al-Yaseen et al. (2017) combined an improved K-means with a support vector machine (SVM) to construct a hybrid NIDs, and they achieved good NID results. Hasan et al. (2014) proposed a support vector machine (SVM) model to enhance the performance of NIDs. Ambusaidi et al. (2016) proposed a NIDs based on SVM and an improved feature selection method, and the results showed that proposed hybrid methods achieved good accuracy results. Yuan et al. (2017) combined the C5.0 decision tree (DT)method with the Naive Bayes (NB) and achieved good detection results based on the public KDD CUP1999 dataset. Hacibeyoğlu et al. (2016) used the C4.5 DT to construct a multilevel hybrid classifier to improve the low detection accuracy of low-frequency attacks, and results showed that the proposed method worked well. Papamartzivanos et al. (2018) took advantage of DT and genetic algorithms to detect network intrusions by integrating heuristic methods in the evolutionary process. Khammassi and Krichen (2017) used three different DT classifiers to evaluate the performance of selected subsets on the public KDD CUP1999 dataset. Parsaei et al. (2016) combined the SMOTE method with the k-nearest neighbor (KNN) to improve NID accuracy, and results showed that the proposed method achieved better classification results than KNN and SMOTE. Tavallaee et al. (2009) and Vinayakumar et al. (2019) systematically compares the classification performance of classical machine learning methods on public KDD datasets.

Deep learning based NID. Apart from classical machine learning methods, researchers have increasingly used deep learning methods to detect network intrusions over the past years (Sriram et al., 2020). Ingre and Yadav (2015) used an artificial neural network (ANN) model to detect multiple network attacks and surpassed machine learning methods on the public NSL-KDD dataset. Alrawashdeh and Purdy (2016) used the Restricted Boltzmann Machine (RBM) model to detect different network intrusions, and the experimental results showed that the proposed RBM outperformed previous works on KDD Cup1999 dataset. Given the feasibility and sustainability of modern NIDS, Shone et al. (2018) proposed a nonsymmetric DAE model for NID, and the results demonstrated that the nonsymmetric deep autoencoder (DAE) model obtained improvements through comparisons to existing advanced methods. Kim et al. (2017) proposed a deep neural network (DNN) model for NID and achieved excellent classification performance. Literature (Yan and Han, 2018) uses a Gated Recurrent Unit (GRU) neural network to classify different network attacks and outperformed previous state-of-the-art methods. Researchers also have applied CNNs to network-related tasks like network traffic analysis (Aceto et al., 2019, Aceto et al., 2019) and network intrusion detection (Wu and Guo, 2019, Vinayakumar et al., 2017, Xiao et al., 2019, Li et al., 2020, Cui et al., 2018, Chawla et al., 2018, Li et al., 2017, Liu et al., 2019). Xiao et al. (2019) presented a CNN model for NID, and the results showed the CNN model worked better than DNN and RNN. In literature (Vinayakumar et al., 2017), researchers also used CNN models for NID, and the results show CNN models achieved better detection results than classical machine learning methods. Wu and Guo (2019) proposes a hierarchical CNN+RNN network called LuNet to classify different types of network intrusions, and results showed that LuNet worked better than state-of-the-art NID methods.


Table 1. Notations used in this paper.

Label	Definition
x
, y
Feature representations
The weight in the convolution kernels
W, V	Weight matrices
The proposed gating convolution equation
Sigmoid activation function
Dot product
Derivative sign
Softmax function
The output of the last dense layer
L
Softmax loss
K and N	Total number of classes and total number of samples
2.2. Advances in CNNs
Convolutional neural network (CNN) is one of the most widely-used deep learning architectures, which have achieved excellent performance in many domains such as computer vision (CV) (Krause et al., 2015, Wang et al., 2015, Xiao et al., 2015), natural language preprocessing (NLP) (Collobert and Weston, 2008, Collobert et al., 2011, Dauphin et al., 2017, Kalchbrenner et al., 2014), and medical data analysis (MDA) (Zhang et al., 2019). CNN can be applied to process one-dimensional (1D) data, two-dimensional (2D) data, three-dimensional (3D) data, and even four-dimensional (4D) data. CNN usually comprises convolutional layers, pooling layers and fully-connected layers. Recently, many excellent methods have been proposed to design state-of-the-art CNN architectures, such as residual connection in ResNet (Szegedy et al., 2017), small stacking convolution kernels in VGGNet (Simonyan and Zisserman, 2014), Inception module in GoogleNet (Szegedy et al., 2016), group convolution method in IGCNet (Zhang et al., 2017) and AlexNet (Krizhevsky et al., 2012), depthwise convolution method in MobileNet (Howard et al., 2017), pointwise convolution method (NIN) (Hua et al., 2018), and gating convolution method in GatedNet (Dauphin et al., 2017). Recently, group convolution and gating convolution methods have become prevalent methods to be used to design different powerful CNN architectures, since they enable CNN architectures to achieve good classification performance. To our best knowledge, no previous works have utilized both group convolution method and gating convolution method mutually to construct powerful CNN models. A novel CNN model named RANet is presented based on two above-mentioned convolution methods for NID in this paper.

3. Methodology
We illustrate the framework of network intrusion detection based on the RANet model, as shown in Fig. 1. The detailed introduction of the proposed RANet is classified into three parts: the Group-Gating module, the over-lapping max pooling, and explored RANet architectures. Table 1 summarizes notations used in this paper.

3.1. Network intrusion detection framework
Fig. 1 presents a general network intrusion detection (NID) framework based on the CNN model. First, massive network data is collected from the computer system. Afterward, the partial network data is manually labeled as the training data, and the rest is labeled as the testing data. Then the CNN model is constructed, and the training data is used as the input for training the CNN model. Based on the detection performance of the testing data, we gradually improve the construction of the CNN model and set proper hyperparameters. Once the trained CNN model obtains the desired results, we would stop training. Finally, we deploy the trained CNN model on the network-based computer defending system to detect different types of network attacks and validate the performance of the CNN model in real systems.

3.2. Group-gating module
In daily life, we usually form a discussion group to discuss important decisions called group decision-making mechanism. The group decision-making mechanism consists of two stages: discussion stage and decision-making stage. First, in the discussion stage, every member has the right to propose various advice based on different perspectives. Then in the decision-making stage, all members discuss together, keep important advice, and delete useless advice based on the group decision-making rules, such as feasibility, risk, and profit. Getting inspiration from the group decision-making mechanism, we introduce a novel convolution module named Group-Gating module. It is comprised of a group part and a gating part, as shown in Fig. 2. The group part and the gating part correspond to the discussion stage and the decision-making stage of the group decision-making mechanism, respectively. To understand the Group-Gating module easily, we introduce group convolution method and gating convolution method before detailing the construction of the Group-Gating module.

3.2.1. Group convolution and gating convolution
Fig. 3 provides a simple implementation of group convolution method, where we divide a convolution partition into two disjoint convolution partitions in a convolutional layer. Each disjoint convolution partition can learn different feature representations from the disjoint convolution partitions from the previous layer independently. Two disjoint convolution partitions have the same number of convolution kernels. The above operations are repeated in continuous convolutional layers (two convolutional layers at least). Finally, all convolution partitions are concatenated together along with the channel axis.

3.2.2. Construction of Group-Gating module
The Group-Gating module comprises a group part and a gating part, as shown in Fig. 2. In the group part, we divide one convolution partition into two disjoint convolution partitions in a convolutional layer and set the same number of convolution kernels for each convolution partition. The above-mentioned operation is repeated in continuous convolutional layers (two continuous convolutional layers are required at least). Each convolution partition is capable of learning different feature representations respectively and through group convolution method efficiently. In the gating part, this paper utilizes two independent pointwise convolution partitions to learn and cluster two feature representations from the previous group part correspondingly. Then we apply a gating operation to filter out unimportant feature representations and keep important feature representations based on the gating convolution method. Pointwise convolution denotes the convolution kernel size with 1 × 1. Moreover, the gating convolution operation used in this paper is different from the gating convolution method in previous works, because this paper adopts two different feature representations as the inputs for gating convolution operation. In contrast, previous works use the same feature representations as the inputs.

To understand the key difference between the improved gating convolution operation and previous gating convolution operation, Fig. 5 presents an overview of the improved gating convolution operation in this paper. The improved gating convolution operation is expressed as follows: (1)
 
 and 
 represent two different inputs, respectively. W, b, V, and c are learned parameters. The gradient of the gating convolution operation can be represented by Eq. (2). It can avoid the gradient vanishing problem of LSTM-style gated operation as you can see from Eq. (3), because of the downscaling factors 
 and 
. (2)
 (3)


Download : Download high-res image (278KB)
Download : Download full-size image
Fig. 5. An illustration of the improved gating convolution operation.

According to the above introduction, the advantages of the proposed Group-Gating module can be concluded as follows: In the group part, we apply two disjoint convolution partitions to learn independent feature representations through two non-shared weights, which can help to get diverse feature representations while reducing convolution redundancy. In the gate part, we utilize pointwise convolution method to cluster correlated feature representations for enhancing and clustering high-level feature representation extraction. It is followed by the gating operator, which can filter out unimportant abstract feature representations and preserve significant feature representations by updating parameters in training.

3.3. Max pooling with over-lapping method
In this paper, max pooling with over-lapping method (over-lapping max pooling) is used to select more important feature representations. Fig. 6 shows a simple comparison of standard max pooling layer and over-lapping max pooling. The key difference between two pooling operations is that the over-lapping max-pooling operation utilizes over-lapping method to select important feature representations as much as possible.


Download : Download high-res image (310KB)
Download : Download full-size image
Fig. 6. Comparison between standard max pooling (top) and max pooling with over-lapping method (bottom).

3.4. RANet architecture
RANet is comprised of an input layer, a convolutional layer, a Group-Gating module, two max-pooling layers, fully-connected layers, and an output layer. The input layer is used to input matrix vectors for the RANet. Two max-pooling layers are used to lower the computational cost by reducing the number of connections between convolutional layers. For two max-pooling layers, the first max-pooling layer is standard max pooling, and the second max-pooling layer is over-lapping max-pooling. The output layer (classification) is used to output predicted classification results. We use softmax function as the classification layer, which can be represented as the following equation: (4)
where i and j represent th input network traffic data and its prediction of j-th class. 
 is the output of the last dense layer and can be written as: (5)
where 
 denotes learnable weights, 
 denotes output of last fully-connected layer, and 
 denotes the bias.


Table 2. Explored RANet architectures.

Model	RANet
Architecture	A	B	C	D
Dataset	NSL-KDD	KDD Cup1999
Input	1 × 122
Conv1	[1,3] × 8	[1,3] × 8	[1,3] × 8	[1,3] × 8
Pooling	Max-pooling
Group-Gating module	Group part	[1,2]x 8, [1,2]x8, [1,2]x12, [1,2]x12
Gate part	[1,1]x12, [1,1]x12
Pooling	Overlapping Max pooling	Max pooling	Max-pooling	Global average pooling
Fully-connected	160
Dropout	1.0	1.0	0.5	1.0
Fully-connected	80
Softmax	5
Based on softmax function, softmax loss is used as loss function. It is essentially a combination of multinomial logistic loss and softmax, which can be computed as follows: (6)
 
where ,  denotes ground truth and predicted probability of each label, respectively.

Table 2 presents four explored RANet architectures, named RANet-A, RANet-B, RANet-C, and RANet-D. The difference among RANet-A, RANet-B, and RANet-D is that three RANet architectures use different pooling methods: overlapping max pooling, max pooling, and global average pooling. To test how the dropout (Srivastava et al., 2014) method affects the classification performance, we set two different dropout rates for RANet-B and RANet-C, respectively. We add batch normalization (BN) (Ioffe and Szegedy, 2015, Wang et al., 2020) operation between convolution operation and activation operation in this paper, which can alleviate the overfitting problem and accelerate the convergence of our method.

Fig. 7 provides the parameter comparison of the RANet and six advanced CNN models. Compared with advanced CNN models, RANet only contains about 0.02 million parameters, much less than the parameters in advanced CNNs like ALexNet and VGGNet. The total parameters of RANet, LeNet, and GatedNet are similar because they belong to shallow CNN models. The required memory to deploy the trained RANet model on the real NID systems is approximately 4MB. Hence, it is feasible to embed it into various security systems. Adam (Kingma and Ba, 2014) optimizer is the most popular back-propagation algorithm in deep learning, which requires less tuning. Thus, Adam optimizer is used as an optimizer to update the parameters in the gradient descent. Algorithm 1 describes how the trained parameters of our RANet update based on Adam optimizer.


Download : Download high-res image (338KB)
Download : Download full-size image
4. Dataset and experimental setup
4.1. Datasets
This paper mainly uses NSL-KDD dataset to validate the performance of the proposed RANet, because it is one of the most widely-used datasets to evaluate the overall performance of methods on NID. NSL-KDD dataset is generated from the KDD Cup1999 dataset. It solves the inherent redundant example problems of the KDD Cup1999 dataset efficiently. The NSL-KDD dataset contains KDD 
 dataset, KDD 
 dataset and KDD 
 dataset. KDD 
 dataset is used as the training dataset, while KDD 
 dataset and KDD 
 dataset are used as testing datasets. KDD 
 dataset is generated from KDD 
 dataset, which does not include any of the records that were classified correctly by all 21 base classifiers on KDD 
 dataset. Hence, it is more difficult and challenging for methods to achieve good detection results on KDD 
 dataset. NSL-KDD dataset has 41 features and 1 label. The label distributions of these two datasets are different as well as imbalanced, as you can see from Table 3.


Table 3. Label distribution of NSL-KDD dataset.

Attack type	NSL-KDD 
NSL-KDD 
NSL-KDD 
Normal	67343	9711	2152
DoS	45927	7458	4342
Probe	11656	2421	2402
R2L	995	2754	2754
U2R	52	200	200
Total	125973	22544	11850
To further verify the performance of RANet comprehensively, KDD Cup1999 dataset, Kyoto University’s Honeypots traffic dataset (Kyoto dataset) (Song et al., 2006), UNSW-NB15 dataset (Moustafa and Slay, 2015, Moustafa et al., 2019), and CICIDS2017 dataset are used in this paper. KDD Cup1999 dataset is collected from DARPA’98 IDS evaluation program. KDD Cup1999 dataset has approximately 4,900,000 single connection records, and 10% of the full dataset is used as the testing dataset. In the KDD Cup1999 dataset, about 78% and 75% of the records are duplicated in the train and test sets, hence, it has massive redundant records. The Kyoto University’s Honeypots traffic dataset (Kyoto dataset) is collected from the honeypot systems and comprises 24 statistical features: 14 conventional features and 10 additional features. It contains several subsets of different years and has been the benchmark data for NID (Ferrag et al., 2020, Song et al., 2010, Malaiya et al., 2018, Aliakbarisani et al., 2019, Garg et al., 2020). In this paper, we use the 2013 Kyoto University’s Honeypots traffic dataset and use the methods in literature (Agarap, 2018) to preprocess data. The training dataset and the testing dataset contain 1,898,240 and 420,608, respectively. The Australian Center for Cyber Security (ACCS) collected and shared the UNSW-NB15 dataset. It is collected from real-world systems, and the synthetic environment is used to generate a hybrid of normal activities and attack activities. The UNSW-NB15 dataset contains 257,673 records: nine attack activities and one normal activity.

CICIDS2017 dataset (Sharafaldin et al., 2018) is shared by The Canadian Institute for Cybersecurity (CIC). It contains benign and up-to-date common attacks, depicting the real-time work traffic. This benign traffic has the characteristics of 25 users based on the HTTP, HTTPS, FTP, SSH, and email protocols. The network traffic for five days was collected and dumped with normal activity traffic on one day, and attacks injected on other days. The implemented attacks include Brute Force FTP, Infiltration, Botnet, Brute Force SSH, DoS, Heartbleed, Web Attack, and DDoS. The detailed introduction of CICIDS2017 dataset in Sharafaldin et al. (2018).

4.2. Dataset preprocessing
According to previous works (Shone et al., 2018), the features of these two KDD datasets can be categorized into two types: continuous features and attribute features (38 continuous features and 3 symbolic features). The input values for the RANet model should be numeric. We convert symbolic features into continuous features and can get 41 continuous features. Moreover, to differentiate the meaning of attributes in symbolic features, we use the one-hot encoding method to map these attributes into binary numeric features; thus we extend 41 features into 122 features. For example, the protocol_type feature has three types of attributes, ‘tcp’, ‘udp’, and ‘icmp’ and we can use (1,0,0), (0,1,0) and (0,0,1) to represent these three attributes. In the experiments, we compare the performance between 41 features and 122 features based on the same RANet. Because the values of different features are not in the same order of magnitudes, Min–Max normalization method is utilized to linearly map the feature value into the range [0,1], and can be represented as follows: (7)
 
where  denotes the arbitrary value of each feature. x
 and x
 denote the maximum value and the minimum value of each feature. For three other datasets, we also use the same preprocessing method.

4.3. Experimental setup
We use TensorFlow platform, scikit-learn tool, and Python language to implement all methods. We run experiments on a machine with Intel (R) 3.20 GHz CPU (i5-6500) and 16 GB RAM. We get hyper-parameter configurations by running a series of hyperparameter experiments manually. Based on experimental results, we set batch size for 500 and epochs to 150. The learning rate is set to 0.00035 according to experimental results, as shown in Table 4. To test which activation function works well in RANet, Tanh, relu, elu, relu6, and sigmoid are used for comparison. Early stop trick is utilized in the training when the proposed model achieves better results than setting values in advance. The best results are reported for comparison in this paper.


Table 4. Parameter settings for RANet.

Parameter	Setting
Normalization method	Min–Max
Batch size	500
Training epochs	150
Learning rate	0.00035
Activation function	relu6
Optimizer	Adam
4.4. Evaluation measures
Four common evaluation measures are used to evaluating the overall performance of methods, including accuracy, recall, F1, and false positive rate (FPR). (8)
 

Accuracy is utilized to measure that the proportion of total number of instances are correctly classified. (9)
 

Recall is utilized to measure that the number of correctly classified instances penalized by the number of incorrectly classified instances. (10)
 

F1 is the harmonic means of precision and recall, and assesses the overall performance of methods efficiently. F1 is also an important measure the verify the performance of a method on an imbalanced dataset. (11)
 

FPR represents how many normal records are misclassified as network attacks. In the above equations, TP, FP, TN, FN represent positive, false positive, true negative and false negative correspondingly.

5. Results analysis and discussion
5.1. Hyperparameter result comparison
Impact of learning rates. Fig. 8(a) presents the classification results of different learning rates. It can be observed that RANet achieves the best classification performance when the learning rate is 0.00035 and gets the worst classification performance when the learning rate is 0.1. Two possible reasons can explain the classification results: (1) we cannot get good parameter initialization for RANet. (2) RANet is more inclined to get good classification results at small learning rates.

Impact of batch sizes. Fig. 8(b) shows the classification results of different batch size settings. It can be seen that when batch size increases, the performance of RANet gradually gets better. However, when the batch size is over 500, the performance of RANet degenerates. Hence, we can conclude that it is difficult and challenging to set proper batch size for RANet, because batch size is associated with other hyper-parameters such as learning rate and epochs.

Impact of activation functions. Fig. 8(c) presents the classification comparison of five activation functions. We can see that relu6 achieves the best classification results while elu activation function gets the worst performance. Thus, according to the classification results, relu6 is chosen as the activation function for our method.


Download : Download high-res image (424KB)
Download : Download full-size image
Fig. 8. Performance comparison of different hyperparameter setting.


Table 5. Five-classification results of RANet and advanced deep learning methods on NSL-KDD datasets.

Method	NSL-KDD 
NSL-KDD 
Accuracy	Recall	F1	Accuracy	Recall	F1
LeNet	79.54%	79.54%	77.33%	61.22%	61.22%	61.89%
GatedNet	80.81%	80.81%	79.62%	63.41%	63.42%	63.72%
AttentionNet	81.16%	81.16%	79.64%	64.98%	64.98%	65.02%
IGCNet	80.58%	80.58%	78.65%	63.16%	63.17%	63.53%
AlexNet	80.45%	80.45%	78.35%	63.36%	63.36%	62.99%
VGGNet	80.49%	80.49%	79.40%	62.96%	62.96%	64.09%
GoogleNet	80.77%	80.77%	79.33%	63.79%	63.79%	64.75%
MobileNet	79.82%	79.82%	77.24%	62.02%	62.02%	62.22%
LSTM	81.27%	81.27%	79.58%	65.32%	65.32%	65.48%
CharCNN (Lin et al., 2018)	79.05%	79.05%	N/A	60.86%	60.86%	N/A
RNN (Yin et al., 2017)	81.29%	N/A	N/A	64.67%	N/A	N/A
RANet-A	83.23%	83.23%	82.57%	69.04%	69.04%	69.85%
RANet-B	81.75%	81.76%	79.69%	65.54%	65.54%	65.63%
RANet-C	82.24%	59.18%	62.03%	66.66%	66.66%	67.00%
RANet-D	81.34%	81.34%	78.63%	64.54%	64.54%	63.58%
5.2. Performance comparison on NSL-KDD dataset
Table 5 presents five-classification results of four explored RANet architectures and state-of-the-art CNNs on NSL-KDD 
 and NSL-KDD 
 datasets. It can be seen that RANet-A gets better classification results than RANet-B and RANet-D, which demonstrates that the overlapping max-pooling works better than standard max pooling and global average pooling. One possible reason to account for the classification performance is that overlapping max pooling is more able to keep important feature representation. For dropout rate, RANet-C gets worse NID accuracy than RANet-B, verifying that proper dropout rate setting may improve the accuracy of NID, but it is challenging to find a good dropout rate.

Compared with existing advanced CNN models and RNN models, RANet-A receives 83.23% and 69.04% accuracies on NSL-KDD 
 dataset and NSL-KDD 
 dataset, respectively. It outperforms existing advanced CNN models with about 2.5% on NSL-KDD 
 dataset. Furthermore, RANet-A improves approximately 5.3% on all evaluation measures on NSL-KDD 
 dataset. As previously introduced, NSL-KDD 
 dataset is more able to assess the generalization ability of a method. RANet outperforms GatedNet and IGCNet on two NSL-KDD testing datasets, verifying group-gating module works better than gating convolution method and group convolution method. Moreover, the classification results indicate it may be a potential research field to design efficient CNN models by combining multiple convolution methods. According to Table 3, we can see that NSL-KDD dataset is an imbalanced dataset. RANet achieves 82.57% and 69.85% of F1 on two NSL-KDD testing datasets and outperforms advanced methods with about 3%, demonstrating that our method is more robust than other advanced methods.

Fig. 9(a) provides five-classification accuracy results of RANet and previous state-of-the-art methods on NSL-KDD 
 dataset (


) and NSL-KDD 
 (

) dataset, respectively. RNN achieves accuracy with 81.29%, and MLP achieves 77.41% on NSL-KDD 
 dataset. RANet achieves the best accuracy with 83.23% on NSL-KDD 
 dataset and surpasses other existing methods over 2%. For NSL-KDD 
 dataset (

), accuracies of all methods are less than 70%. Our RANet achieves the best accuracy with 69.04% and outperforms existing methods by approximately 5.3%. To further verify the effectiveness of the proposed RANet, Fig. 9(b) shows the results of binary classification accuracy of RANet and previous state-of-the-art methods on NSL-KDD 
 (

) and NSL-KDD 
 (

) datasets. We can see that RANet achieves 87% and 75.39% accuracy on two testing datasets and improves approximately 4% accuracy than other methods, such as RNN, SVM, MLP, and RF. RNN, RF, and DT obtain over 80% accuracy on NSL-KDD 
 dataset. However, no existing methods can reach 70% on NSL-KDD 
 dataset (RNN achieves an accuracy with 68.55%). Deep learning methods achieve better accuracy than machine learning methods, indicating that deep learning methods have great potential in the NID field.

Download : Download high-res image (339KB)
Download : Download full-size image
Fig. 9. Performance comparison of RANet and start-of-art methods on two NSL-KDD testing datasets.

To evaluate the overall classification results of our RANet comprehensively, we provide two confusion matrices of RANet on NSL-KDD 
 and NSL-KDD 
 datasets, respectively, as shown in Fig. 10. Fig. 10(left) gives the confusion matrix of RANet on NSL-KDD 
 dataset, RANet achieves 82.75% F1, 83.23% recall, and 83.23% accuracy, respectively. Fig. 10(right) reports the confusion matrix result of RANet on NSL-KDD 
 dataset, RANet achieves 68.77% F1, 69.04% recall, and 69.04% accuracy. Table 6 details the classification results of normal type and four other intrusion types based on our RANet. U2R gets the worst recall with only 6% and 5.5% accordingly on two testing datasets. Dos gets the best recall with 87.11% and 87.05% correspondingly on two testing datasets. However, U2R only achieves 0.2% and 0.4% FPRs on two testing datasets and surpasses three other intrusion types. Dos achieves the worst FPR. According to Table 6, the overall five-classification results of RANet are good, but there is a further improvement for deep learning methods in the NID field, e.g., imbalanced data problem.


Download : Download high-res image (357KB)
Download : Download full-size image
Fig. 10. Confusion matrices of RANet on two NSL-KDD testing datasets.

In addition, we also use t-SNE visualization method (Ravi et al., 2021, Vinayakumar et al., 2020) to see where the features are learned by the penultimate layer is shown in Fig. 11(a)–(b), where can help us enhance the interpretability of our method.

According to the classification results of the above figures and tables, our RANet achieves better performance on NSL-KDD 
 dataset than NSL-KDD 
 dataset through comparisons to state-of-the-art methods. It also demonstrated that NSL-KDD 
 dataset does not contain any of the records that were classified correctly by all 21 base classifiers on the KDD 
 dataset, which makes it more difficult to be correctly classified, as previously introduced.


Download : Download high-res image (948KB)
Download : Download full-size image
Fig. 11. t-SNE visualization results of NSL-KDD 
 dataset, NSL-KDD 
 dataset, Kyoto dataset, and UNSW-NB15 dataset.

Generally, the reasons to explain the classification results of our RANet are summarized as follows.

•
Firstly, group-gating module as the core component of RANet, which not only enhances latent feature representation discrimination by group convolution method, but also can keep important feature representations and filter out unimportant feature representations based on pointwise convolution method and gating convolution method.

•
Secondly, overlapping max-pooling helps RANet-A to select more important feature representation information compared with max-pooling and global average pooling.


Table 6. Detailed classification results of five types on two NSL-KDD testing datasets.

Metric	Normal	DoS	R2L	U2R	Probe
Recall	
92.06%	87.11%	85.02%	6.0%	49.42%
84.81%	83.51%	72.23%	8.5%	43.46%
F-Score	
86.23%	83.16%	89.17%	9.76%	56.59%
55.59%	83.51%	82.20%	14.19%	53.65%
FPR	
17.5%	3.2%	3.2%	0.2%	3.9%
26.70%	4.2%	2.03%	0.18%	0.56%
5.3. Performance comparison on other four public datasets
Table 7 provides a comparison of our RANet and previous advanced methods on KDD Cup1999 dataset. Our RANet achieves 99.78% accuracy, 99.5% F1, and 0.005% FPR, respectively. It achieves better performance than previous methods. Additionally, all methods can achieve accuracy and recall over 90% for the five-classification of NID except for DBN and LR. As previously introduced in the KDD Cup1999 dataset, massive redundant records can explain why most methods can achieve excellent classification results of NID because redundant records can make methods pay more attention to frequent records than infrequent ones. Furthermore, the classification results also demonstrate that the KDD Cup1999 dataset cannot verify the overall performance of a method better than the NSL-KDD dataset.

Table 8 presents the results of our RANet and previous methods on the Kyoto dataset. All methods achieve over 80% accuracy except GRU. RANet achieves the best classification performance among all comparable methods. It improves about 1.2% on accuracy and recall, while reducing about 0.5% of FPR accordingly. Fig. 12 shows the accuracy comparison of RANet and six advanced methods based on the UNSW-NB15 dataset. All methods achieve over 70% accuracy, and the RANet achieves an accuracy of 85.36%. It outperforms other methods with about 0.5% at least.


Table 7. Performance comparison of RANet and existing methods on KDD Cup1999 dataset.

Method	Recall	FPR	Accuracy
SVM (Hasan et al., 2014)	N/A	3.46%	99.79%
TLMD (Yuan et al., 2017)	93.11%	0.761%	93.32%
MHCVF (Hacibeyoğlu et al., 2016)	95.57%	1.38%	98.04%
LR (Vinayakumar et al., 2019)	80.10%	12.8%	80.10%
S-NADE (Shone et al., 2018)	97.85%	2.15%	97.85%
DNN (Vinayakumar et al., 2019)	93.5%	6.6%	93.5%
DBN (Alrawashdeh and Purdy, 2016)	97.47%	2.10%	80.58%
D-ONN (Ramaiah et al., 2021)	98.00%	2.00%	98.00%
SNN (Ramaiah et al., 2021)	93.00%	7.00%	93.00%
RANet	99.78%	0.005%	99.78%

Table 8. Performance comparison of RANet and existing methods on Kyoto dataset.

Method	Recall	F1	Accuracy
GRU-SVM (Agarap, 2018)	84.15%	83.60%	84.15%
GRU (Agarap, 2018)	70.75%	69.68%	70.75%
DNN (Vinayakumar et al., 2019)	96.4%	93.8%	88.5%
SVM	63.93%	51.3%	63.93%
DT	94.08%	94.11%	94.08%
RF	96.64%	96.62%	96.64%
MLP	63.75%	50.91%	63.75%
NB	89.95%	90.07%	89.95%
AdaBoost	93.36%	93.31%	93.36%
KNN	96.42%	96.41%	96.42%
RANet	97.55%	97.54%	97.55%

Table 9. Performance comparison of RANet and existing methods on CICIDS2017 dataset.

Method	Recall	F1	Accuracy
MLP (Sharafaldin et al., 2018)	83.00%	76.00%	83.00%
Adaboost (Sharafaldin et al., 2018)	84.00%	77.00%	84.00%
LR (Sharafaldin et al., 2018)	87.00%	86.80%	87.00%
RF (Sharafaldin et al., 2018)	94.40%	95.30%	94.40%
KNN (Sharafaldin et al., 2018)	90.90%	92.20%	90.90%
SVM (Aksu and Aydin, 2018)	69.79%	65.00%	70.00%
DNN (Vinayakumar et al., 2019)	95.60%	95.70%	95.60%
RANet	96.73%	96.59%	96.73%
5.4. Discussion
Although our method achieves the best performance for NID in contrast to strong baselines and previous state-of-the-art methods, this paper still has some limitations. We summarize limitations in the following. Firstly, previous works (Sriram et al., 2019, Venkatraman et al., 2019, Ravi et al., 2021) have conducted NID experiments to verify the robustness of methods in an adversarial environment and help NID methods classify unknown attacks; however, this paper has not considered it and just took into account that NID performance. Secondly, the interpretability of RANet is poor, which is a bottleneck to deploy it in current NID systems. Thirdly, both machine learning and deep learning methods obtain poor detection results on infrequent network attacks.

To address the mentioned limitations, we would conduct a series of experiments to validate the robustness of our method in an adversarial environment and analyze shortcomings of our method for enhancing robustness of it in an adversarial environment. Furthermore, the interpretability of deep learning methods is significant and helpful for people to know how they work and make decisions. To improve the interpretability of our method and take actual requirements of NID systems, we plan to use visualization analysis methods to explain the mechanism of our method and apply the mathematical model to demonstrate the proposed method. To detect infrequent network attacks as accurately as possible, we can take the following solutions: (1) Designing advanced artificial intelligence methods and cost sensitive methods. (2) Generating infrequent network attacks for training. (3) Conducting experiments in an adversarial environment.

6. Conclusion and future work
This paper proposes a CNN model named RANet for NID by introducing a Group-Gated module. The Group-Gating module comprises a group part and a gating part, and it can enhance latent feature representation discrimination and keep important feature presentations. To further improve the accuracy of NID, we apply over-lapping max-pooling to select key feature representations as much as possible. Based on the hyperparameter setting, a series of performance comparison experiments are conducted on public NID datasets. The results demonstrate that the RANet achieves better NID results than previous works and strong baselines.

Although the RANet has achieved promising results and has great potential for use in real NID systems, there is much room for further improvement. In the future, we will focus on studying the imbalanced data problem of NID datasets, and to further improve the overall NID results.

