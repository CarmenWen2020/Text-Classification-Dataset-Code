Link prediction in complex networks is to discover hidden or to-be-generated links between network nodes. Most of the mainstream graph neural network (GNN) based link prediction methods mainly focus on the representation learning of nodes, and are prone to over-smoothing problem. This paper dedicates to the representation learning of links, and designs an edge convolution operation so as to realize the link representation learning. Besides, we propose an normalization strategy for the learned link representation, for the purpose of alleviating the over-smoothing problem of edge convolution based link prediction model, when constructing the link prediction graph neural network EdgeConvNorm with stacking edge convolution manipulations. Lastly, we employ a binary classifier sigmod on the Hadamard product of two nodes representation parsed from the final learned link representation. The EdgeConvNorm can also be employed as a baseline, and extensive experiments on real-world benchmark complex networks validate that EdgeConvNorm not only alleviates the over-smoothing problem, but also has advantages over representative baselines.

Previous
Next 
Keywords
Link prediction

Complex network

Graph neural network

Edge convolution

Normalization

Residual connection

1. Introduction
Many real-world systems in nature and society can be described as complex networks or graphs, such as the Internet, World Wide Web, social networks and knowledge graphs, in which the nodes or vertices denote the entities while the interactions between entities can be described as edges or links (Zhang and Wang, 2015). As a novel interdisciplinary field, complex network science is not only a natural extension of classical graph theory and random graph theory in mathematics, but also an innovative development of system science and complexity science (Goyal and Ferrara, 2018; Zhou et al., 1812). Link prediction in complex networks, as one of the core scientific issues across multiple disciplines, is to discover hidden or to-be-generated links between network nodes, including not only the prediction of existent yet unknown links, but also the prediction of future links. Link prediction can help to improve the efficiency of biomedical experiments, can be used for personalized production recommendations in e-commerce, and can even be employed to predict the votes of judges of the U.S. Supreme Court. As an abstraction of large category of universal problems, link prediction can be applied to any systems that can abstract entities and their relationships into a network form (Zhou et al., 1812; Boronat et al., 2021). Because of its far-reaching practical application significance, link prediction has received extensive attention from scientists with different backgrounds in different fields, and plays an increasingly significant role in future science and engineering research (Gori et al., 2005; Zhang et al., 1812; Li et al., 2021a).

However, the traditional link prediction models and techniques mainly analyze the topological characteristics of different types network data (i.e. graph data) from the spatial perspective, and require manual construction of feature engineering and selection of heuristics, as shown in Fig. 1 (a). Unfortunately, due to different types of network have different characteristics and attributes, the generalization ability of traditional link prediction models are poor, and the inherent topological and dynamic characteristics of the network cannot be learned adaptively. Therefore, research on algorithms that can automatically and adaptively learn link formation features and mechanisms from complex networks will have high practical connotation in present and future.

Fig. 1
Download : Download high-res image (753KB)
Download : Download full-size image
Fig. 1. Graph data analysis ideas comparison. (a) Traditional link prediction models. (b) Graph neural network based approaches.

In fact, with the rapid development of deep learning technology, data in the form of image, speech, text and so on have all been well deep learned and the task related features are automatically obtained. Thus, it is very natural and necessary to transfer the successful experience of deep learning technology to the learning on complex network data represented in the form of graph data1 (Scarselli et al., 2009; Li et al., 2019; Li et al., 2018; Zhuang and Ma, 2018; Huang et al., 2018; Zhang et al., 2018a; Ying et al., 2018; Zhang et al., 2018b; Wang et al., 2019; Beck et al., 2018; D'Angelo and Palmieri, 2021). Fortunately, the emergence of graph neural network (GNN) has realized the effective fusion of graph data mining and deep learning techniques, which can perform deep learning effectively on relational data in non-Eucliden space such as complex network data (Huang et al., 2018; Zhang et al., 2018a, 2018b; Ying et al., 2018; Wang et al., 2019; Li et al., 2021b). However, Most of the mainstream graph neural network (GNN) based link prediction methods mainly focus on the representation learning of nodes, they first learn the representation of nodes, and then perform operations, such as Hadamard product, on the representation of node pairs to predict whether there is a link between two nodes. Besides, existing GNN-based link prediction approaches are prone to over-smoothing problem, which fail to meet the desired performance.

Our objective with this paper is to build an edge convolution based graph neural network, which can learn the link presentation from the complex network, so as to complete the task of link prediction in complex networks. To accomplish this goal, we first propose the edge convolution operation to learn and update the link representation with the aim of establishing the convolutional layer of EdgeConvNorm. Then, the graph neural network EdgeConvNorm for link representation learning is constructed by stacking the edge convolutional layers, combined with the link representation normalization strategy proposed in this paper. Finally, we extract the representation of the adjacent nodes from the link representation learned by EdgeConvNorm, and perform a ‘0–1’ binary classification on the Hadamard product of the extracted nodes representation, thereby realize the prediction of whether there is an link between that two nodes.

Thus, two main contributions this paper makes include:

•
We propose the edge convolution operation and build the corresponding edge convolutional layer of EdgeConvNorm, which first learns and updates the node representation by concatenating the node and its neighbors representation, and then concatenates two nodes representation to indicate the representation of a link.

•
Combined with the link representation normalization proposed in this paper, the link prediction graph neural network EdgeConvNorm is constructed by stacking edge convolutional layers for learn and update the link representation, and the strategy of link representation normalization could better alleviate the over-smoothing problem of EdgeConvNorm.

The remainder of this paper is organized as follows. Section 2 illustrates the research status in academia and works related to the topic of this paper. Section 3 introduces preliminaries of link prediction, and outlines the edge convolution and normalization based link prediction model. In section 4 purpose experimental analysis are reported and final section offers concluding remarks and sheds light on future research directions.

2. Related works
This section will introduce the research status and development trends related to link prediction, including graph representation learning and representative link prediction methods.

2.1. Graph representation learning
The goal of graph representation learning is to acquire discriminative graph features from graph data, so as to automatically construct feature engineering for downstream graph data mining tasks, as shown in Fig. 1 (b). The random walk algorithm DeepWalk proposed by Bryan et al. is one of the most representative algorithms in graph representation learning (Bryan et al., 2014), which basic idea is to map the relationship and structural properties of the nodes into a new vector space, thereby nodes with similar attributes and structural properties will have closer vector distance. Then, Grover et al. proposed node2vec (Grover and Leskovec, 2016) by further abstracting DeepWalk in a more general sense, they introduced Biased-Random walks to characterize whether each random walk is partial depth search or breadth search. Since node2vec is a more flexible model with two hyper-parameters, it has a better performance for graph node classification (Grover and Leskovec, 2016).

Moreover, the SDNE (Structural Deep Network Embedding) proposed by Wang et al. (2016) takes the similarity vector of nodes as the input of model directly, and compresses that vector through an Auto-Encoder to obtain the vectorized results. The main difference between the DNGR (Deep Neural Graph Representations) proposed by Cao et al. (2016) and SDNE is the definition of similarity vector. DNGR hires the “common path” obtained by the random walk of two nodes as an index to measure their similarity while SDNE directly utilizes the first-order relationship as the input of similarity. However, what DNGR needs to be further improved is that the dimension of its input vector is limited by the square of the number of nodes. Therefore, DNGR has certain restrictions on the network scale, and its acceptance for new coming nodes is also poor. In general, SDNE and DNGR have two disadvantages (Hamilton et al., 1709): On the one hand, there are no parameters sharing between nodes in the encoder, thus the number of parameters will increases linearly with the number of nodes, which will lead the low computational efficiency. On the other, the direct embedding strategy lacks generalization ability and can not handle dynamic graphs, so then the performance is not very stable when evaluating the link prediction task (Hamilton et al., 1709).

2.2. GNN-based link prediction methods
Link prediction is one of the key issues of complex network analysis, which mainly includes two types of mainstream methods, one is simple but effective heuristic methods, and the other is GNN based approach.

The traditional heuristic methods for link prediction focus on directly calculating several heuristic node similarity scores as the likelihood of links (Zhang and Chen, 2018). Among the existing heuristic-based link prediction approaches, the more commonly employed methods are common neighbor (CN) and its variants, such as Jaccard, Adamic-Adar (AA) and resource allocation (RA). The basic idea of this type of algorithm is that nodes with more common neighbors are more likely to have links, and the actual implication is that two people who have more common friends are more likely to be friends in reality. Because of their simplicity, interpretability, and scalability of some of these heuristics, they have been widely used in practical applications (Zhang and Chen, 2018). Unfortunately, although the heuristic-based link prediction methods working well in practice, each heuristic has a strong assumption on when two nodes when are likely to link, which leads to their poor link performance when these assumption fail. For instance, the common neighbor assumption may be proper in social networks, but is shown to fail in user-item network – two users sharing many neighbors (items) are actually less likely to interact (Zhang and Chen, 2018). Thus, s more reasonable idea should be to automatically learn a suitable heuristic from a given network instead of using a predefined heuristic.

As a deep learning framework on graphs, graph neural networks follow the neighborhood aggregation scheme, which can not only automatically learn topological features from the network, but also learn the attributes and characteristics related to nodes and links. In literature (Zhang and Chen, 2017), Zhang et al. first studied how to automatically learn network features from the network, they extracted the local enclosure subgraphs surrounding the links as training target, and employed a fully connected neural network to learn which enclosure subgraphs correspond to the existence of links. Based on the above theoretical basis, Zhang et al. further proposed the SEAL framework (Zhang and Chen, 2018), they first utilized a graph neural network GCN to replace the fully connected neural network to obtain better subgraph feature learning capability, and then allowed SEAL not only to learn link features from the subgraph structure, but also from Implicit and explicit node features, so as to absorb more comprehensive characteristics. In addition, IGMC (Zhang and Chen, 2020) also extracted an enclosure subgraph for each (user, item) pair, and utilized a graph neural network to train a regression model that maps the structure of the subgraph into the user's rating of the product, and according to this learn regression model, the link prediction in the complex network is realized. The main advantage of above approaches is that the extraction and learning of local enclosure subgraphs around links are independent of each other, so parallel and distributed programming models can be used to efficiently process large-scale complex networks. However, due to these methods only consider the local characteristics of the link and ignore the corresponding global features, the performance improvement of these methods is limited, especially on complex networks with strong sparseness.

In this paper, the representation of two nodes associated with an edge are concatenated, and the average aggregation strategy is employed to construct the convolution operation on the edge. Then, the normalization and residual idea of the edge embedding are combined to learn links representation. In addition, the representation of nodes is restored from the embedding of edges, and Hadamard product operation is performed on nodes representation to realize the link prediction.

3. Link prediction model
In this section, we first introduce the preliminaries related to link prediction and graph neural networks. Then, we outline the edge convolution operation, link representation normalization, and residual concatenation. Finally, we present a link prediction framework based on edge convolution.

3.1. Preliminaries
Notations. In order to introduce and explain the issues related to link prediction, notations and symbols employed in this paper are shown in Table 1.


Table 1. Notations employed in this article and their corresponding descriptions.

Notations	Description
G = (V, E)	G is an undirected graph, V is the set of nodes and E is the set of edges in G, respectively.
N = |V|	The number of nodes in G.
𝓔	The number of edges in G.
V = {v1, v2, …, vN}	V stands for the nodes set of G, and vi represents the i-th node.
𝓔
E indicate the observed edges set of G, and ei denotes the i-th edge.
eij	the edge between node vi and vj.
𝓝	the neighbors of i-th node in G.
U = V × V	U indicates the universal edges set consisting of |V|∗(|V|− 1)/2 node pairs.
 represents the unobserved edges in G.
Link Prediction. Intuitively, for any undirected graph G(V, E) and its corresponding universal edges2 set U, each pair of nodes vi and vj is assigned to a score by a specified link prediction technique, this score could be understood as a kind of proximity, which is positively correlated with the connection probability of two nodes. Then ranking the scores of all pairs of nodes in non-ascend order, thus the top node pairs have the higher probability of connecting with each other. Moreover, for the purpose of evaluating the performance of link prediction methods, the observed edges E is usually divide into two parts, training set ET and test set EP, however, only the information in training set can be employed to calculate that score for each pair of nodes. Obviously, E = ET ∪ EP and ET ∩ EP = ∅. Here, the edges belonging to U but not to E are called unobserved edges , while the edges belonging to U but not to ET are named unknown edges EU. And the task of link prediction is to predict the edges connection probability in EU according to the information learned from ET.

Graph Neural Network. Various entities (nodes) and their relationships (links) in complex networks are usually represented in form of graph data and stored as adjacency matrix. Graph data is an irregular and unordered data in non-Euclidean space, and traditional machine learning methods failed to perform well on it. Fortunately, graph neural networks (GNN) can be employed for non-Euclidean spatial data modeling and captures the inherent dependencies and heuristics of link formation between pairs of nodes. GNN essentially aggregates neighbor nodes features in an iterative manner to learn the representation of the target nodes. Employing this end-to-end learning advantage, we combine the representation of any edge adjacent nodes to uniformly learn the representation of edges, and finally parse the node representation from the acquired edge embedding for link prediction.

3.2. Edge convolution operation
To the best of our knowledge, the edge convolution operation EdgeConv was first presented by Wang et al. (Wang et al., 1145) and applied to the point cloud learning on dynamic graph, as shown in Eq. (1). Since point clouds inherently lack topological information, while EdgeConv incorporates local neighbors information and can be stacked applied to learn global shape properties so as to recover point clouds topology and further enrich the representation power of point clouds (Wang et al., 1145).(1)
𝓝
 

Where 
 stands for the representation or embedding of target node vi, 𝓝 represents the neighbors of vi, Θ and Φ are linear layers. The EdgeConv in multi-layer systems affinity in feature space captures semantic characteristics over potentially long distances in the original embedding and restored well the topology of point cloud. However, the maximum aggregation utilized in Eq. (1) is similar to the preferential attachment in bib_Price_1965Price, 1965, 1976bib_Price_1976, which may cause the phenomenon that the rich get richer in the scenarios of graph data. Therefore, in view of the inherent topological characteristics and application of graph data, an improved output of edge convolution operation at i-th node is given by Eq. (2).(2)
𝓝
 

For simple and practical purpose, Eq. (2) could be simplified to Eq. (3), and the edge convolution manipulation for the representation learning of edge eij is shown in Eq. (4) accordingly.(3)
𝓝
 
(4)

Where ‖ expresses the concatenate manipulation in this paper. Here, we take advantage of the mean aggregation strategy instead of the previous max aggregation to comprehensively measure the representation of the target node through the neighbor's embedding of the target node, so as to prevent the representation of target node from being strongly influenced by its ‘strongest’ neighbor, especially for scale-free social networks.(5)

Moreover, instead of using ReLU as the activation function, as shown in Eq. (5), we employ the LeakyReLU to improve the expression ability of neural network. Because as a kind of nonlinear activation function, when the input value of ReLU is negative, the output is always 0 and its first-order derivative is always 0, which will cause the neuron to stop updating parameters, that is, the neuron does not continue to learn. For the sake of solving that shortcoming of ReLU, a leaky value is introduced in the negative half interval of ReLU by assigning a non-zero slope λ to all negative values, thus the LeakyReLU activation function comes into being, as shown in Eq. (6), where λ is a learnable hyper-parameter. In addition, combined with the normalization strategy detailed in section 3.3, the LeakyReLU can make the model more stable and robust, which is verified in the experimental consequences in section 4.4 Results and discussion, 4.5 Stability and robustness.(6) 
 

3.3. Link representation normalization and residual connection
We then conduct further investigation on how transformation operations in GNN layers impact the model performance. Intuitively, the performance of graph neural network decreases with the increase of network depth, revealing that increasingly severe model over-smoothing, which can not be addressed by existing regularization techniques such as Dropout (Zhou et al., 2020a). Besides, as illustrated in (Zhou et al., (2020a)), the performance of deep GNN models oscillate rapidly at the later stage of training, showing severe training instability. Because of training instability, the training process of deep GNN models are sensitive to regularization.

Without considering the depth of GNN model, we alleviate the problem of over-smoothing and training instability to a certain extent by employing the edge convolution based link prediction model EdgeConvNorm, inspired by the NodeNorm strategy illustrated in (Zhou et al., (2020a)), and further propose an edge representation normalization strategy EdgeNorm to normalize the embedding of each edge by means of its own mean 
 and standard deviation 
, as shown in Eq. (7) and Eq. (8), respectively.(7)
 
(8)
 

Let 
 denotes the embedding vector of edge eij with dl-dimension features. Hereby, the proposed EdgeNorm manipulation is given in Eq. (9).(9)
 

Where 
 and 
 are edge-wise mean and deviation of edge eij specified in Eq. (7) and Eq. (8), respectively. No extra hyperparameters are introduced like NodeNorm (Zhou et al., 2020a), denoting that reduces the risk of model over-smoothing and enhances the stability of model training. Thus, an EdgeConvNorm layer equipped with EdgeNorm and residual connection becomes Eq. (10).(10)

Obviously, the EdgeConvNorm has a simple form and can be easily implemented on mainstream deep graph learning platform, such as pytorch_geometric.3 Besides, the loss function adopted in this article is binary_cross_entropy_with_logits imported from pytorch_geometric.

3.4. Link prediction framework
In this section, the whole framework for link prediction in complex networks is built on top of edge convolution operation and a binary classifier sigmod, as shown in Fig. 2. Firstly, we construct the graph neural network EdgeConvNorm for link representation learning by stacking the edge convolution layers described in section 3.3. Then, we employ a binary classifier sigmod on the Hadamard product of two nodes representation, parsed from the final learned link representation by Eq. (11), to implement a link predictor as depicted in Eq. (12). Finally, the loss function binary_cross_entropy_with_logits imported from pytorch_geometric is adopted for review and optimize the performance of EdgeConvNorm. The details of the above process can be obtained from Fig. 2.(11)
(12)

Fig. 2
Download : Download high-res image (491KB)
Download : Download full-size image
Fig. 2. An edge convolution and normalization based graph neural network framework for link prediction in complex networks, named EdgeConvNorm.

Where Parser can parse the corresponding representations of the adjacent nodes vi and vj from the embedding of edge eij, and ⊗ indicates the Hadamard product operation. In addition, f(vi, vj) is a binary classifier, such as the sigmod employed in this paper, which can determine whether there is a link between nodes vi and vj.

4. Experiments and discussion
Our extensive experiments begin with the introduction of benchmark datasets, baselines and evaluation metric used for link prediction, then we examine the performance of EdgeConvNorm and compare it with baselines so as to analyze the stability and robustness of EdgeConvNorm.

4.1. Datasets
Three different types of widely used benchmark datasets are employed, including Cora, Citeseer and Pubmed (Daniel et al., 2018; Zhou et al., 2020b; Sen et al., 2008), these datasets are public citation networks in transductive learning task. Nodes of a graph represent documents while edges express citation links. Besides, documents are classified into several classes, thus each node is attached with a label and each graph is associated with a ceratin amount of features. The detailed statistics are illustrated in Table 2.


Table 2. Dataset statistics. Following (Daniel et al., 2018; Zhou et al., 2020b; Sen et al., 2008), we only consider the largest connected component as the experimental data.

Dataset	#nodes	#edges	#classes	#features
Cora	2708	5429	7	1433
Citeseer	3327	4732	6	3703
Pubmed	19,717	44,338	3	500
As a note, for each dataset splitting, we follow the mask preassigned by torch_geometric for Cora, Citeseer and Pubmed.

4.2. Baselines
To evaluate the performance of EdgeConvNorm, we compare it with the state-of-the-art GNN models, including GCN (Kipf and Welling, 2017), GAT (Veličković et al., 2018) and EdgeConv (Wang et al., 1145). A brief description of these baselines employed for comparison are as follows.

•
GCN (Kipf and Welling, 2017) is one of the most representative graph neural network model, which core idea is to aggregate the node information by utilizing edge weights so as to generate a new node representation. While there exist a mount of GCN variant models, we just focus on the common one (Kipf and Welling, 2017).

•
GAT, a graph attention network (GAT) is composed of several attention layers which can learn various edge weights to different nodes in its neighborhoods. GAT is usually employed as a baseline model to compare the performance of different GNN models (Veličković et al., 2018).

•
EdgeConv was first proposed for point cloud analysis. EdgeConv has several appealing properties, including it incorporates local neighborhood information and can be stacked applied to learn global shape properties(Wang et al., 1145). In this article, we add a classifier on the EdgeConv so as to implement the task of link prediction and served as a baseline for comparison.

In addition, all baselines applied in this paper are implemented by PyTorch and pytorch_geometric.

4.3. Evaluation metric
We use AUC, namely area under the receiver operating characteristic curve, as the performance evaluation metric for all models. What calls for special attention is that AUC is not only widely used in traditional link prediction models such as Jaccard (1901) and HPI (Ravasz et al., 2002), but also utilized in currently GNN based link prediction methods such as SEAL (Zhang and Chen, 2018) and GAT (Veličković et al., 2018). Without loss of generality, we also only consider AUC to measure the performance of link prediction baselines and EdgeConvNorm. As for other metrics, we will conduct in-depth explorations in our consequent work. In addition, we run all the models 10 times and report the average AUC with standard deviation, as shown in Eq. (13) and Eq. (14), respectively.(13)
 
 
(14)
 
 

Where n = 10 and we use 
 
 as the final evaluation results.

4.4. Results and discussion
In this section, we report the comparative evaluation results for all baselines in Table 3, and the experimental settings that need to be emphasized are as follows: the learning rate, the number of hidden channels, the number of output channels, the epoch and the run number is set to 0.001, 256, 256, 5000 and 10, respectively. GCN, GAT, EdgeConv and EdgeConvNorm are simply implemented by pytorch_geometric without strict optimization tuning. By adjusting the probability of Dropout, we only report the best performance highlighted in bold. The probability p of Dropout for Cora, Citeseer and Pubmed is assigned to 0.6, 0.6 and 0.7, respectively; the number of head for GAT is set to 1 default. In addition, the best performance is highlighted in bold.


Table 3. Experimental results conducted on different benchmark network by different baselines.

Dataset	Model
GCN	GAT	EdgeConv	EdgeConvNorm
Cora	Val.	0.9089 ± 0.0022	0.9043 ± 0.0047	0.8759 ± 0.0069	0.9231 ± 0.0119
Test	0.9050 ± 0.0026	0.8979 ± 0.0019	0.8528 ± 0.0082	0.9178 ± 0.0088
Citeseer	Val.	0.8816 ± 0.0045	0.8808 ± 0.0052	0.8174 ± 0.0060	0.8896 ± 0.0080
Test	0.8701 ± 0.0039	0.8731 ± 0.0037	0.8294 ± 0.0084	0.8754 ± 0.0066
Pubmed	Val.	0.9708 ± 0.0006	0.9436 ± 0.0012	0.8675 ± 0.0026	0.8930 ± 0.0066
Test	0.9694 ± 0.0004	0.9436 ± 0.0006	0.8665 ± 0.0018	0.8911 ± 0.0025
From Table 3, we can make the following observations.

(1)
Our proposed method EdgeConvNorm outperforms its predecessor EdgeConv on benchmark dataset Cora, Citeseer and Pubmed, revealing that the edge convolution manipulation illustrated in Eq. (3) is more suitable for the situation of complex network. Combined with the model performance under different disturbances as shown in Fig. 3, it can also reveal that the normalization strategy can make our proposed model more stable. That's owing to the normalization strategy can make the link representation smoother, thus suppressing the disturbance caused by abnormal links, such as the links that associate nodes with a large degree difference.

Fig. 3
Download : Download high-res image (337KB)
Download : Download full-size image
Fig. 3. The average AUC of EdgeConvNorm tested on different dataset under Different Dropout Probabilities.

(2)
Although compared with classical GNN model GCN and GAT, the performance of EdgeConvNorm is not much improved, but EdgeConvNorm has the best performance on the dataset Cora and Citeseer, and it also performs better than its predecessor EdgeConv conducted on all benchmark datasets listed in Table 2. Moreover, EdgeConvNorm is only slightly lower than the classical GAT and GCN; The main reasons why EdgeConvNorm's performance on large-scale complex networks such as Pubmed is not as good as that on medium-sized networks such as Cora and Citeseer are as follows.

•
The large-scale networks including Pubmed tend to have lower network density, indicated by the ratio of the actual number of edges in a network to the maximum number of edges that can be formed between nodes. However, Pubmed's network density is only 0.00023, which is much lower than Cora's 0.00148. Therefore, the neighbors aggregation based EdgeConvNorm can not fully learn the link representation the from Pubmed with a higher sparsity.

•
Moreover, compared with Cora and Citeseer, Pubmed only has 500 features, which results in link prediction models including GCN, GAT and EdgeConvNorm unable to fully learn the representation of links.

(3)
Fortunately, although there are different Dropout probability perturbations, EdgeConvNorm's performance is very stable, all above 86%, as shown in Fig. 3, and there is no any over-smoothing. However, GCN and GAT are suitable for large-scale complex networks, although it also performs well on medium-scale datasets such as Cora and Citeseer. On the contrary, the EdgeConvNorm has more stable performance and higher AUC value on different scale of datasets.

4.5. Stability and robustness
To the best of our knowledge, Dropout makes the activation value of a neuron stop working with a certain probability p when the graph neural network propagates forward, which can further promote the generalization ability of the model, due to it does not rely too much on local characteristics, so it can also significantly reduce the over-smoothing problem of the EdgeConvNorm.

To further show the stability and robustness of the proposed EdgeConvNorm, we identify a scenario where the probability p of Dropout (Srivastava et al., 2014) is adjusted in steps of 0.1 range from 0.1 to 0.9 on dataset Cora, Citeseer and Pubmed, respectively. Then the corresponding AUC is obtained to analyze the stability and robustness of the model EdgeConvNorm. From Fig. 3 and Table 4, without considering the situation of p = 0.9, we obviously observe that the mean AUC on different benchmarks with different p changes smoothly and the gap is almost within 0.05, that is, under different perturbation rates, the change of p only has little effect on the performance of EdgeConvNorm so as to prove that our model is more robust.


Table 4. AUC on validation dataset and test dataset under different Dropout probabilities. The notation Val. and Test of the dataset name indicate the validation dataset and the test dataset, respectively.

P	Dataset
Cora	Citeseer	Pubmed
Val.	Test	Val.	Test	Val.	Test
0.1	0.8978 ± 0.0085	0.8675 ± 0.0079	0.8432 ± 0.0149	0.8629 ± 0.0087	0.8848 ± 0.0025	0.8854 ± 0.0010
0.2	0.8951 ± 0.0064	0.8692 ± 0.0089	0.8514 ± 0.0122	0.8701 ± 0.0066	0.8893 ± 0.0021	0.8880 ± 0.0024
0.3	0.8904 ± 0.0085	0.8706 ± 0.0062	0.8520 ± 0.0094	0.8598 ± 0.0079	0.8895 ± 0.0025	0.8888 ± 0.0024
0.4	0.9028 ± 0.0085	0.8728 ± 0.0030	0.8474 ± 0.0066	0.8623 ± 0.0088	0.8877 ± 0.0016	0.8895 ± 0.0015
0.5	0.9074 ± 0.0108	0.8737 ± 0.0063	0.8454 ± 0.0114	0.8539 ± 0.0069	0.8881 ± 0.0033	0.8868 ± 0.0041
0.6	0.9231 ± 0.0119	0.9178 ± 0.0088	0.8896 ± 0.0080	0.8754 ± 0.0066	0.8897 ± 0.0025	0.8874 ± 0.0053
0.7	0.9131 ± 0.0119	0.8744 ± 0.0087	0.8476 ± 0.0104	0.8552 ± 0.0109	0.8930 ± 0.0066	0.8911 ± 0.0025
0.8	0.8936 ± 0.0177	0.8652 ± 0.0164	0.8360 ± 0.0169	0.8480 ± 0.0155	0.8824 ± 0.0066	0.8820 ± 0.0060
0.9	0.7944 ± 0.0145	0.7881 ± 0.0164	0.7988 ± 0.0111	0.7892 ± 0.0133	0.8327 ± 0.0064	0.8351 ± 0.0052
In addition, without considering the situation of p = 0.9, the mean AUC with standard deviation reported in Table 4 are above 0.86, and the standard deviation is also small, no matter it is on the validation datasets or on the test datasets, which all revealing the performance of EdgeConvNorm on different datasets are stable.

As for the case of p = 0.9, the average AUC are lower than 0.8 and the corresponding standard deviation is almost all above 0.01, mainly due to the larger p can lead to fewer neurons for training so as to make the EdgeConvNorm unable to fully propagate messages forward and learn related features, thus resulting in unsatisfactory correspondences.

5. Conclusions and future directions
In this work, we proposed a edge convolution based graph neural network EdgeConvNorm and presented a normalization strategy of link presentation, for link prediction in complex network. Besides, we also combined the mechanism of residual connection for the purpose of eliminating the model over-smoothing. First, we focus on the representation learning of edges by employing the edge convolution. Then, the strategy of link representation normalization and residual connection are applied on the learned link presentation. Finally, we parse the representation of the nodes from the learned link representation, and apply the binary classifier sigmod to the Hadamard product of two nodes representation to determine whether there is a link between these two nodes. Extensive experiments on real-world benchmark networks validate that EdgeConvNorm performs well, and has advantages over representative baselines.

Although the EdgeConvNorm achieved a better AUC performance on several benchmark complex network datasets, there are at least two directions in which this work could be extended. First, despite the AUC is often engaged to assess the performance of link prediction by mainstream link prediction techniques, including the baselines employed in this paper, it still needs to explore other evaluation metrics, such as F-value and accuracy, thereby estimate the capability of link prediction methods comprehensively. Then, applications of the EdgeConvNorm with large-scale and dynamical complex works are also needed in future. Just as the performance of EdgeConvNorm on Pubmed is not ideal, we need to further explore the intrinsic mechanisms that affect the performance of EdgeConvNorm in large-scale complex networks, especially those with strong sparseness, and make targeted improvements and perfection.