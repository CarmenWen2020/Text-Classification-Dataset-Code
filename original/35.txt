Abstract
The number of Mild Cognitive Impairment (MCI) older adults is increasing; thus, it becomes more and more important to provide them with support to avoid, or at least slow down, their cognitive decline. To this end, interactive serious games can play an important role. So far, most of them have been deployed through tablets, which represent a cost-effective solution, yet offering only limited possibilities for truly engaging such users in a multimodal manner. However, emerging humanoid robots, through their physical embodiment and human-like attributes, including facial expressions and body language, may open up new possibilities in more effectively engaging MCI older adults during repetitive cognitive training. We present a study aiming to better understand the impact of humanoid robots in supporting serious games for such users. In particular, we investigate how seniors with Mild Cognitive Impairments relate to and perceive serious games accessed through humanoid robots, as part of a training programme aimed to improve their cognitive status. For this purpose, two versions of a music-based memory game have been designed by a multi-disciplinary team, one for humanoid robots and one for tablets. We report on its use during a between-subject study that involved MCI seniors, and discuss their experience. The results show that the robot was received with more enthusiasm by the older adults, thus improving their level of engagement.

Keywords
Mild cognitive impairment
Serious games
Interactive technologies
Humanoid robots

Introduction
With a senior population that is foreseen to more than double by 2050 worldwide1, an increasing demand for high-quality elderly support is likely to be expected in the coming years. Among the various disabilities typically associated with ageing, cognitive impairments are those affecting a significant part of people aged 65 plus: the Alzheimer Association reported that approximately 15 to 20 per cent of people aged 65 or older have Mild Cognitive Impairments (MCI).2 MCI is an intermediate stage between the cognitive decline associated with normal ageing and more serious forms of dementia. Seniors with MCI often show memory loss or forgetfulness and may have issues with other cognitive functions such as language, attention and visuospatial abilities. The potential evolution of this disease makes it necessary to increasingly provide aid to such people over time, and it is frequently associated with an increased burden on their caregivers, and in the worst case, with the institutionalisation of such patients. Thus, it is especially important to provide people who are potentially at risk of developing dementia with timely and engaging cognitive training to slow the progression of their decline, while significantly cutting down the associated socio-economic costs.

Typically, cognitive training consists of a series of repeated and standardised tasks with challenges that target specific cognitive domains (e.g. memory, attention, information processing speed). Currently, cognitive training for seniors with an MCI diagnosis is generally administered by professional caregivers who often use paper-based material. However, with the advent of mobile technologies, such as tablets and smartphones, the easiness of computerising traditional cognitive interventions has greatly increased due to the several advantages that computer-based cognitive interventions have over traditional ones. Among them, we mention e.g. homogeneous administration of stimuli, real-time automatic evaluation and comparison of performances, reduced workload for caregivers, cost-effectiveness, possibility of providing a flexible and personalised approach, easiness with which cognitive interventions can be delivered by caregivers to older adults using various types of media, devices, and modalities.

Thus, the introduction of computerised serious games for cognitive support has received several proposals (Chan et al., 2016; Savulich et al., 2017; Vaportzis et al., 2017). However, at the same time, some criticism has been raised, one challenge could be in the acceptance of technology by elderly (Holthe et al., 2018). Indeed, for seniors with MCI, technology not only can provide opportunities, but may also present challenges. This can be due to their difficulties, for example in visual attention and memory amongst other cognitive abilities, and, more in general, because they could be unfamiliar with technology. Thus, to allow older adults with cognitive impairments to interact comfortably with technology and actually benefit from it, it is important to understand how they approach and perceive the cognitive interventions provided through various types of assistive interaction technologies. In this work, we focus on two kinds of interactive technologies: humanoid robots and tablets.

On the one hand, the tablet has been the interactive device most considered until now, also due to its low cost. Some work (Joddrell and Astell, 2016) suggested that even people affected by dementia may be able to use effectively touch screen devices, which may provide some benefits to them and their caregivers. On the other hand, we also found it interesting to investigate the impact of the use of humanoid robots on interventions targeting people with cognitive disabilities for several reasons. They are becoming increasingly affordable and popular since they can support users in a variety of real-world tasks. In addition, the varied humanlike, multimodal emotional expressiveness and body language they can display seems promising to stimulate and empathetically motivate MCI subjects to stay engaged during repetitive cognitive training sessions. Indeed, some initial studies highlighted that robots can serve as a powerful technology for improving motivation and compliance in settings where repetitive exercises need to be carried out. This seems especially connected to their physical embodiment and ability to employ also nonverbal cues (Andrist et al., 2015). These features seem to indicate them as more effective to motivate people, compared to what can be offered by interactions supported by traditional computers (Tapus et al., 2009b).

In this paper, we report on a study whose goal was to investigate how seniors with Mild Cognitive Impairments relate to and perceive serious games accessed through humanoid robots, as part of a training programme aimed to improve their cognitive status. To this end, we designed and developed a quiz game to help the subjects to train their memory while receiving encouraging feedback from the application. We introduced not only the version of the game for a humanoid robot but also a version for tablets, whose purpose was to represent a useful reference point for our analysis, given that the tablet is currently the most used device in cognitive training.

The structure of the paper is as follows. In the next section we analyse the state of the art, then we provide the context in which the described study was carried out, and identify relevant requirements for applications supporting cognitive stimulations targeting MCI seniors. Then, we describe the music quiz game designed to support cognitive stimulation of seniors. We also report on the user study in which 14 users interacted with two different versions of the application (tablet-based and robot-based). Finally, we provide a discussion of lessons learnt and conclude with some remarks and indications for future work.

Related work
MCI is characterised by a cognitive decline greater than expected as compared to that typically associated with an individual's age, but which does not significantly interfere with daily activities (Gauthier et al., 2006). Due to the increasingly high number of ageing people expected to suffer from cognitive impairments in the coming years (Prince et al., 2013), many interventions have been promoted to aid this population under a wide variety of perspectives. For instance, Mentis et al. (2019) discuss the security and privacy risks associated with online services (e.g. banking) to which seniors suffering from MCI are increasingly exposed nowadays. However, one of the areas that have received the most attention is related to interventions aiming to delay the cognitive decline of MCI population, which is at higher risk of developing dementia. A large body of literature has focused on the development of Serious Games (SG), digital applications aiming to go beyond mere entertainment, thereby helping assessment, stimulation, treatment and rehabilitation of patients suffering from cognitive disorders (Robert et al., 2014; Manera et al., 2015). A literature review of studies on the use of SG in various neurodegenerative disorders, MCI and Alzheimer's Disease (AD) is reported in McCallum and Boletsis (2013). This paper emphasises that, while physical games (i.e. games promoting physical fitness) can positively affect several health areas of players with MCI and mild AD (such as balance, gait, and voluntary motor control), cognitive games can improve attention, memory, and visuo-spatial abilities.

The approaches that improve cognitive aspects include cognitive training and cognitive engagement. Cognitive training targets specific domains with the expectation that improvements observed in one domain will be potentially transferred to other ones. In contrast, cognitive engagement involves acquiring new skills, which may simultaneously train several cognitive abilities including executive function, reasoning, and memory (Vaportzis et al., 2017; Chan et al., 2016). While cognitive engagement potentially offers more significant opportunities to produce broader benefits compared to more focused cognitive training, it has received limited attention due to the cost and complexity of assessing users for prolonged periods in experimentally controlled real-world environments. Indeed, the type of cognitive intervention that has shown most benefits is the one that engages individuals in a focused, repetitive task, usually targeting the improvement of one specific cognitive ability (e.g. memory, executive functioning): this is also the kind of intervention we address in this work. However, cognitive training packages are typically repetitive, thereby they present two major challenges: overcoming the incidence of dropout rates in patient groups, and increasing their engagement during such activities (Cohen-Mansfield et al., 2009). Savulich et al. (2017) conducted a randomised controlled trial of cognitive training using an iPad-based memory game in 42 patients with amnestic MCI assigned to either the cognitive training group or the control one. The cognitive training group maintained good levels of enjoyment and motivation to continue playing, with self-confidence, self-rated memory ability, and episodic memory improvement over time. Differently from this work, they did not include robots in training.

Several studies showed that elderly people with cognitive disorders might have problems in using many SG currently available. This is due to poor familiarity with game technology, and often derives from the fact that most SGs have been developed for entertainment (e.g., Nintendo Wii Fit, Big Brain Academy), and with a "typical healthy user" in mind. To understand this problem, Manera et al. (2015) investigated the acceptability of a tablet-based SG to stimulate executive functions and praxis. The results concerning game performance (e.g. time spent playing, number of errors), as well as the self-reported data, confirmed the acceptability of the game for patients with MCI and related disorders, and the usefulness of employing it for training purposes.

Another work (Holthe et al., 2018) analyses the usability and acceptability of technology for older adults with MCI, providing a review that analyses various studies in which different technologies/devices have been used (including tablets or robots). This review indicates that several studies have considered interactive technologies for training people with MCI, for example using tablets for various purposes, ranging from meaningful engagement, to cognitive stimulation (using photos, music, and games) to social communication, even to entertainment and joy. However, few studies reported findings on the perceived acceptability and usability of technologies by people affected by MCI, and trials with end users are needed for this purpose.

The tablet is the device that has been more considered in this context, also because of its limited cost. Low-cost interactive technologies can provide positive results in the cognitive rehabilitation of older individuals. For instance, De Oliveira Assis et al. (2010) found that 50 minutes of cognitive stimulation programs twice a week positively influenced cognitive functioning, as indicated by pre-post measures on the Mini-Mental State Examination. Various apps for smartphone/tablets have been developed for this purpose, and some studies about them have been reported. Eichhorn et al. (2018) explored the use of an iPad to propose some 2D games to older adults and received some initial positive preliminary feedback in some trials in a nursing home. Kerkhof et al. (2017) have investigated the user requirements for apps for people with mild dementia through a qualitative exploratory study. Some of such requirements concern usage, such as the minimal amount of typing required to navigate within the apps. However, they also found that often such older adults still need support to learn how to use the tablet and its apps. Indeed, despite the availability of several apps, such solutions have been adopted to a limited extent, probably also because older adults do not feel sufficiently engaged and motivated to use them.

Another emerging area for improving cognitive skills in older adults involves robots. Some preliminary work (Churamani et al., 2018) explored the use of emotion recognition within dialogues with the NAO robot to make cognitive training more emotionally engaging for seniors. Other work explored whether robot-based SGs can be beneficial even for people with dementia. In particular, in Tapus et al. (2009) and Martín Rico et al., 2013, robots were exploited in combination with music-based games targeting people with dementia. In Tapus et al. (2009) the robot engages users in a musical game, which adaptively adjusts its difficulty to the abilities of the player. Results show that this approach may engage patients and keep them interested in interacting with the robot. In Martín Rico et al., 2013 the authors present an application for dementia patients, in which the behaviour of the humanoid robot and the therapy session are visually programmed in a script that allows music playing, physical movements, speech synthesis and the activation of lights/LEDs on the humanoid robot. Initial results of experiments with institutionalised patients affected by moderate and severe dementia showed a slight improvement in neuropsychiatric symptoms compared to other traditional methods but without significant statistical differences compared to a baseline group. Both of such works used music and humanoid robots to activate patients affected by dementia, whereas we focus on MCI patients.

Focusing on older adults with MCI, Stogl et al. (2019) investigated the use of a mobile robot for motor stimulation of people with MCI. They used an omnidirectional robot with handlebars and a force-torque sensor to support interaction with users. Ten older adults with MCI evaluated the device and the training. The results show that, during training, users tend to be more precise and faster in controlling the device, which indicates an improvement in their motor skills. They also felt safe and managed to adapt to changes in the device's behaviour, suggesting that this device can be suitable for training. While they mainly address patients' motor skills via non-humanoid robots, we aim to improve seniors' memory using social humanoid robots, which seem more promising since they can support more engaging interactions with users. Pino et al. (2019) present an approach for slowing the progression of cognitive decline in MCI patients by using a humanoid robot that supports tasks from usual memory-training programs. Subjects either had the support of the NAO robot or only that of a psychologist. The resulting data indicated that memory training with NAO increased the patients’ visual gaze and reinforced the therapeutic behaviour compared to the other condition. An aspect highlighted in that work was the users’ positive reaction to reinforcement phrases provided by the robot as feedback after completing a task, and the importance of providing such feedback in a personalised manner (i.e. by including the name of the user who replied correctly). However, this was rather problematic in that work: the interaction with the robot was carried out in groups, and NAO was expected to recognise people by looking at their faces, which did not always work well. In our work, each user interacts with the Pepper robot individually: this can support more reliable opportunities for personalised training.

When planning cognitive interventions including robots, it is important to analyse the attitude of older adults towards this technology. Wu et al. (2016) explore the needs of older adults with MCI and their attitudes toward an assistive robot. Although participants reported difficulties in managing some daily activities, they did not see themselves as needing an assisting robot. However, they considered it potentially useful either for themselves in the future, or for others who are very old, frail, alone or lonely. The goal of Bechade et al. (2019) was to find an objective procedure for evaluating dialogues with a social robot, by collecting data from end-users: they first collected data through a Wizard of Oz system with Nao, next through an autonomous system with the Pepper robot. Although users positively perceived the dialogues with robots, some potential users declined to participate since they did not want to interact with a robot: this shows that seniors are not used to talking to devices, thus robots should be introduced carefully.

The integration of social robots in a smart environment is the subject of Schroeter et al. (2013), which presents a socially assistive robot companion, which supports reminders, recommendations, video contacts with relatives/friends, and a cognitive stimulation game. Six trials involving patients suffering from MCI, AD and Front-Temporal Dementia were conducted. The authors report that, although speech recognition turned out to be insufficiently accurate for intuitive use, the subjects, despite some initial scepticism, became convinced by the idea of support obtained through the combination of a smart home with a social robot. The robot was valued for its embodied interactions, its ability to give reminders/suggestions, the fact that it comes physically to the users, talks to them, shows initiative and has some ‘personality’. While in that paper the authors’ aim was mainly to get qualitative feedback on how a robot can assist people in their life, our study is more focussed, addressing cognitive, memory-related abilities of MCI people.

In this work, we discuss the impact of two technologies (tablet and humanoid robot) in supporting serious games for seniors with MCI, by reporting on a multi-disciplinary work that involved a group of such users in testing a game that exploited musical quizzes to provide engaging cognitive training. While in the literature there has been some previous work analysing the usability of various technologies including tablets or robots (Holthe et al., 2018), to the best of our knowledge, this is the first time that tablets and social robots have been exploited in parallel for improving memory abilities in MCI seniors, who individually interacted with one version of the developed application.

Context
This research is the result of a collaboration between two groups working in two Institutes of the same campus: one group belonging to an Informatics institute and working in the area of interactive technologies, and the other one working in the Neuroscience field. The neuroscience group participates in a local project called Train the Brain (TTB) aiming to help aged MCI people to keep active their mind and body, and to prevent/slow down cognitive decline. They recruited several patients at risk or already suffering from a slight cognitive deficit. All participants were firstly evaluated by memory-disorders specialists at the local university clinic, since the diagnosis of MCI was based on the assessment of relevant neurological and clinical tests/examinations, as well as of personal data (e.g. socio-demographic data, medical history, pharmacological drug use and lifestyle habits).

In the TTB programme, participants use games, social activities, and physical activity with a series of increasingly complex exercises as well as face-to-face meetings, music therapy, group stories, bike and stretching exercises in order to tone at the same time their body and their brain. The programme is structured into 8 cycles, and each one is divided into 18 sessions of activities designed to stimulate various cognitive functions: auditory and visual attention, visual-spatial memory, imagination, space-time and personal orientation, verbal memory, lexical skills, and affective memory. Two 60-minute sessions a day are scheduled, three times a week. Each cycle lasts three weeks, then the same sessions are proposed again, increasing the difficulty. In past editions of such project, the caregivers tried to introduce computers for cognitive training, but it was a failure. They used desktop computers with touch screens, however the older adults had no familiarity with them, did not know how to manage window-based user interfaces, and found it very difficult to perform the training exercises with such platform. Patients explicitly expressed their dislike of carrying out the training games through PCs. Thus, it was decided to propose the use of different interactive technologies to investigate whether they could be more useful and stimulating for MCI older adults. The decision was to use a tablet and a humanoid robot. Since most users had experience with smartphones, the tablet seemed a type of device more consistent with their experience with technologies; in addition, touch-based devices make less demand of hand-eye coordination when compared with a desktop computer using a mouse and cursor. The humanoid robot was something completely new for them: thus, it was deemed interesting to investigate the reactions it can stimulate in such an audience. Thus, the two groups agreed to co-design a serious game for stimulating the older adults in some cognitive aspects and implement it in two versions (tablet and robot).

Design
Considering the previous experiences of the TTB psychologists with older adults, it was decided to design an application requiring users to recognise songs from the years when they were younger. The song recognition exercise is a task of retrograde memory (for familiar songs) and anterograde memory (for unknown songs) to which also a component of autobiographical (retrograde) memory is potentially added when the familiar songs evoke personal memories of the individuals.

The "songs" stimulus was chosen based on the experience conducted over the years with the local Train the Brain program on a group of elderly people of age between 65 and 89 years. Songs proved to be a very effective stimulus in eliciting a reaction of involvement and emotional activation in the participants.

The known songs also showed how, from the recognition of the stimulus, it was possible to switch to a more extensive retrograde memory set than the song itself, which included particular moments of past life (autobiographical memory) and long past periods associated with the stimulus (episodic and emotional memory). The component of affective memory (long periods of life linked to the stimulus) was therefore linked to the autobiographical memory component (specific episodes) elicited by the recalled stimulus (retrograde declarative memory). In addition, while other types of stimuli used in the past with MCI patients tended to involve only one specific cognitive resource, the song quiz proposed, although mainly aimed to stimulate memory, also indirectly involves other capabilities such as attention and even reasoning (i.e. stimulating deductive reasoning such as “it was a song from my youth, then the singer could be…”). Moreover, the subjects work in a program which also includes other kinds of exercises (e.g. gym), thus in a context of social exchange, the music may provide positive emotions.

In order to choose the songs, we considered that the age when people most elaborate musical stimuli is around 15-20 years old; afterwards, musical stimuli can continue to accompany people significantly but, from an emotional point of view, subjects tend to remain tied to the musical genres of their youth. The study to which the "songs" exercise was applied included people between 65 and 89 years of age (which means born between 1930 and 1955). Therefore, the songs were chosen in a range from 1940 to 1970, to cover the different age groups of users while ensuring that there was at least one song that belonged to the youth of each participant. The songs were also chosen from among the best known of the chosen time period.

The difficulty associated with the memory task was not related to the recognition of the stimulus, but rather to recollecting titles and singer names. Recollecting the titles of the chosen songs could have been difficult since, over time, the songs could have become more famous by a different name. An example of this is the Italian song “Nel blu dipinto di blu”, whose title is often erroneously named ”Volare” (which is actually its main refrain). Recollecting the names of the singers required further effort by the older adults because, especially until the 70s, the most famous songs were reinterpreted by different musical singers.

The final task was therefore thought to be sufficiently approachable in order to avoid fears and disincentives, but also complex enough to activate concentration and commitment, and elicit the processing of declarative memory stimuli. Thus, the designed application plays the initial part of songs, and the older adults have to select the singer or the song title in a limited amount of time from a list of three potential answers provided by the application.

Various meetings with the psychologists of the TTB program were necessary for co-designing several aspects of the game for the two devices. In such meetings it was decided that each question must show three options to choose from: the correct answer, one that can be misleading (i.e. because it shares some similarities with the correct one), and one that can be easily recognised as wrong.

Users have to listen to a small segment of a song (20 seconds), and then they have to guess the title or the singer. In order to maintain a high attention level and not tire the older adults, we decided that each exercise would have proposed 15 songs. After the question is displayed and vocally synthesised by the application, a musical piece is played, starting from the beginning, and the users are asked to recognise the title or the singer. They have 20 seconds to respond, and then the correct answer is shown. The time limit was designed to give a fast-paced exercise rhythm, thus avoiding the tendency to become distracted, typical of elderly people. We judged that allowing longer response times could have triggered comments, requests and jokes from subjects, thus hampering the required level of concentration.

After this, the user can decide when to continue the game, by pressing a button in the UI: this was done to let users have the time to memorise the correct answer, especially when they replied incorrectly to a question. When the users respond, if they indicate a wrong answer, the right one is shown by highlighting it over a green background, and the wrong answer is highlighted using a red background. No response after 20 seconds is considered an error. The application shows the answer either after the user replies or when the time available is over.

In total, 80 potential songs were identified and, from this list, 15 were selected for the game sessions. In each game session such songs were randomly proposed. The three possible answers for each of them were always the same but randomly presented, to avoid using spatial memory to choose the answer.

The interaction was designed to take place in a multimodal way, and the game provides different feedback, depending on whether the user answers correctly or not. The two versions of the game support the same tasks, in the same order, and with the same constraints. They differ in the way users receive feedback during their sessions, as the robot also uses body movements, eye colouring and ear LEDs, its synthesised voice, and hand and arm gestures while speaking.

For the graphical part, we considered guidelines to design interfaces for elderly users (Hoffman and Hancock, 2015; W3C, 2019; Johnson and Finn, 2017; De Almeida et al., 2015; Park et al., 2014), such as using clear icons, simple and common words, minimal keyboard use, maximum contrast, fonts bigger than 12 pt, labelled icons to reinforce the concept, few colours, and large buttons.

Finally, the system was enhanced in such a way to log significant information associated with user interactions: it was decided to record data regarding users’ reaction times for answering each question, the time needed to complete the entire game in each session, the number of correct and incorrect responses, the number of answers not provided within the time interval of 20 seconds, and the number of sessions needed by users to respond correctly to (at least) 12 questions (which correspond to 80% of the total number of questions in each game session). This last measure was considered by the psychologists as providing indications about users’ learning rate.

The Music Quiz game
We identified four main states for the Music Quiz game: login, menu, play and results. The starting state is login, where users access the game, the next state is menu, where an introduction vocally presents the instructions. At the beginning of the introduction, users are greeted by using their first names, to make them perceive the communication as more personalised. From the menu state, the user can go to the results or the play state. After the users play the game, they enter into the results state, where the results of the game are shown (number of right/wrong answers, mean reaction times and total session time).

The play state is divided into two sub-states, question and answer. The question state corresponds to displaying and vocally synthesising the question, a progress bar with the remaining time to answer the question, and the three possible answers proposed in the form of large buttons, disabled at the beginning. After the question has been completely presented, the song and the countdown start, and the possible answers are enabled (see Fig. 1a).

Fig 1
Download : Download high-res image (338KB)
Download : Download full-size image
Fig. 1. The application presenting a question (a), providing feedback for an incorrect answer (b), and for an occurred timeout (c).

The user interface associated with the answer state changes depending on the answer chosen by the user. On the one hand, if the answer is wrong, it highlights the choice over a red background and displays the thumbs down icon to reinforce the negative feedback (see Fig. 1b); it also shows the right answer (to allow users to learn it), with a green background and a thumbs up icon (see Fig. 1c). On the other hand, if the user provides the right answer, a reinforcement sentence is shown (i.e. “Very good, you gave the right answer!”), and the singer and the song title are shown with a green background and thumbs up icon.

After the user provides the answer, for each type of answer (right, wrong or timeout), there are some possible feedbacks, randomly chosen from time to time. The possible feedbacks for correct answers are: “Congratulations! Correct answer” or ”Very good! Right answer”. The feedback messages for the wrong answers can be: "I'm sorry, you gave the wrong answer" or "Error". In case of timeout, the provided feedback is “Sorry. Time is up”.

While all the above-mentioned sentences are also vocally synthesised in the two considered cases, one difference between the two devices is the additional feedback that is provided after a user selects the answer to a specific question. Indeed, the humanoid robot also provides feedback in terms of additional modalities such as head, body, hand and arm movements, as well as sounds and coloured LEDs positioned on shoulders and eyes (which become green in case of a right answer, red in case of mistake, and blue before the answer is provided). In particular, if a question is correctly answered, the (positive) feedback provided by Pepper is one of the following three “happy” animations:

•
•
It raises the right arm with a closed fist, its eye LEDs are green and a “hey!” expression is vocally provided;

•
It nods its head, and the LEDs on the eyes and the shoulders become green.

Either in case of a question incorrectly answered or a timed-out question, the feedback provided by Pepper is one of these three animations:

•
This animation is composed of two gestures: It shakes the head and it raises both arms up to the level of the torso;

•
It groans and moves the arms up and down, the LEDs on the eyes and the shoulders become highlighted (see Fig. 2, right part);

•
It shakes the head and it vocally renders an “oh!” expression, while the eye LEDs become yellow and red.

When the number of correct answers exceeds 12 (which is the 80% of the total number of questions) one of the following animations is chosen:

•
Pepper raises the right arm twice, the eye LEDs become yellow, and the sound of a celebrating crowd is rendered;

•
It raises both arms to simulate a “win” gesture, the eye LEDs become yellow, and the same sound used in the previous animation is rendered.

Such animations were identified in such a way to make the robot provide expressions that to some extent resemble the humanlike ones, so that users interpret the robot's behaviour as empathic as possible (according to the user's current game result). We chose not to use negative animations in this context, following the suggestions of psychologists for designing feedback encouraging the elderly to do increasingly better.

The game starts the next song only after explicit user request; in this way users have time to memorise the right answer.

Users can end the game at any time. At the end of a session, a feedback message is displayed and synthesised depending on the percentage of correct answers:

•
If the game is interrupted before the end, the message "See you next time" is presented.

•
If the percentage of right answers is less than 50%, the following sentence is presented: “You did a good job. See you next time to do even better!”;

•
If that percentage is between 50% and 80% the sentence is: “Congratulations, you responded well to more than half of the stimuli. See you next time!”;

•
If that percentage is higher than 80%, the phrase is: “Exceptional, great job! Keep it up!”

The tablet used for the experiment was a Samsung Galaxy Tab A SM-T580, with Android 8.1. The robot was a Pepper, by SoftBank Robotics, a 1.2-m-tall wheeled humanoid robot, with 20 degrees of freedom for motion in the body (17 joints for body language) and three omnidirectional navigation wheels to move around smoothly. It has a range of sensors to allow it to perceive objects and humans in its surroundings.

NAOqi OS is the robot's operating system. It is a GNU/Linux distribution based on Gentoo, developed specifically to meet the needs of SoftBank Robotics robots. It provides and manages several programs and libraries required to control the robot sensors and movements. The NAOqi framework supports different programming languages (Java, Python, C++): we decided to use Python because it is the most supported, and it can run both on PCs (for debugging purposes) and on the robot.

The graphical part of the game has been implemented with the Ionic framework v3.9.3 (version 4.0 was not compatible with the Pepper support for the tablet-like display located on its chest). The part of the applications running on the robot display is a Web application (HTML5 + JavaScript + CSS) generated by Ionic, and within the JavaScript part, it is possible to raise events that can then be caught by the Python back-end module, which then calls the native functionalities of the robot (text to speech, led management and animations). The MusicQuiz Python module manages the robot during the execution of the games. The main goal of this component is to wait for events generated from the graphical user interface and then call the native functions which control the robot. The application running on the tablet is a native Android application generated by Ionic as well.

The study
The study was organised in twelve sessions conducted in the Train the Brain clinic during May-June 2019. Our participants were recruited by the local Train the Brain project, and all of them have had a diagnosis of Mild Cognitive Impairment. All participants were invited to provide their informed written consent before the beginning of the study. For each participant, demographic and computer experience data were formally collected through a questionnaire submitted to users before the first session.

The users were enrolled according to the following inclusion criteria: diagnosis of MCI, age over 65 years, Italian speaking participants.

Participants
14 participants (9 females) with age ranging between 69 and 84 (mean=75.3; std. dev.=4.5) were involved in the experiment. The users have different levels of education: 4 users held an elementary school degree, 2 middle school degree, 7 a high school degree, 1 an academic degree.

At the beginning, the participants needed to be divided into two groups associated with the two conditions considered in the test (“pepper” vs “tablet”). In order to do this, the psychologists of the clinics asked them to rate their familiarity with technological devices by referring to the following four levels: (1) Very low familiarity: very low experience with technologies and devices (call or sending messages), (2) Low familiarity: know the basic functionalities of devices (sending message and call, have internet connection), (3) Medium familiarity: able to use a smart device, (4) Good familiarity: able to use the functionality of smart device (navigate on the internet, using social networks and applications). Seven users rated their familiarity as “good”, three users rated it as “medium”, one user rated it as “low”, the remaining three rated it as “very low”. The fourteen participants were then divided into two groups in such a way that the two groups were homogeneous according to such values. More specifically, one group was composed of three users with good familiarity, two users with medium familiarity, one user with low familiarity, one user with very low familiarity. The other group was composed of four users with good familiarity, one user with medium familiarity, two users with very low familiarity. Then the two groups were randomly assigned to the two conditions (“pepper” or “tablet”).

Once the two groups were formed, before the first test session, the participants had to fill in a questionnaire to gather both socio-demographic information and additional data about the use of technological devices in their life. The devices most used by them were mobile phones (all of them), then TVs, then computers (either desktop or portable ones). Only two users declared to use tablets (the type of device least used by the involved subjects). None of them had interacted with a robot before. Regarding the frequency with which the elderly use technological devices, five users declared using them 3-4 times a week, at maximum; two users declared using them once per day; three users declared using them 3-4 times per day; four declared using them more than 3-4 times per day. As for the goal of using technological devices, the most mentioned one was to make a call, followed by getting information, then sending messages, then using apps. No colour blindness was reported by the participants.

Test organisation
The experiment was organised in two different stages: (1) familiarising with the humanoid robot Pepper and the tablet; (2) playing the music game with the humanoid robot and the tablet. Table 1 summarises the two conditions considered.

In the first phase, participants received a brief introduction to the study, the devices used, and the main goals and motivations. Then, the participants saw for the first time the robot and could interact with it and the tablet in order to try the main game functionalities and familiarise themselves with them. While both devices were available when the study was introduced, it was interesting to note that all the participants were attracted by the robot and curious to see its reactions, how it would move, what it says, while the tablet prompted very little interest. We also offered participants the opportunity to try the vocal interaction with Pepper: although the users were very excited, it did not work very well in the end, probably due to the vocal features of some elderly users and also the difficulties that Pepper exhibited recognising the Italian language spoken by people who (even occasionally) may have language-related difficulties. Therefore, in order to achieve fluent interaction, we decided not considering vocal input in this version of the application. During such initial tests, MCI patients showed a high level of interest and curiosity towards Pepper, and enthusiasm towards the possibility of interacting with a robot for their training. Fig. 3 was taken during one of such tests: as you can see, the users’ body postures reflect the general user interest towards Pepper.

Fig 3
Download : Download high-res image (482KB)
Download : Download full-size image
Fig. 3. Older adults with the humanoid robot Pepper.

'Our experimental test was included in the routine training session in the clinic. The game was played two days a week with a total of 12 sessions over a month. During this phase, each individual participant interacted and played the music quiz game one by one. As already mentioned, before the start of the first test session, we asked the older adults to provide (through a questionnaire) some demographic information as well as further information about their experience and use of technological devices. In addition, after the first session, they had to fill in the User Engagement Scale questionnaire (O'Brien et al., 2018), which was also repeated after the last game session. After the compilation of the two questionnaires we asked them to log in, then listen to the instruction given by the device and then recognise the title or the singer of the 15 songs proposed by the game.

For the test, the users were called one by one in a dedicated room for their own session training class where they were observed by one moderator. The Pepper and the Tablet sessions were done simultaneously in different rooms. In the sessions, the users were at a distance of 30-40 cm from the robot, so that they were close enough to interact with it while the robot's animations with its arms did not bother the participants. The participants who interacted with the tablet were seated in a chair next to a desk where the tablet was located. The speed of Pepper's speech was initially left to the default value of 100% of speed (“normal” speed 3). However, during some initial informal trials before the training sessions, some elderly expressed difficulties in easily perceiving the robot's speech while it was presenting the rules of the game: thus, the robot's speech rate was decreased to 80%.

We measured: number of correct answers, wrong answers, number of answers not provided within the time limit, the time needed to complete the whole game, the time needed to respond to each answer and the number of trials needed to provide 12 correct answers (which corresponds to 80% of the total number). For each group, a moderator was available during the various sessions and took note of user feedback or any significant events occurring during user interactions with the robot and the tablet.

Results
Completion Time and Task Success
Fig. 5 shows the average task times over the 12 sessions in the two conditions (Tablet vs Pepper). The X-axis indicates the sessions, whereas the Y-axis indicates the mean task time in seconds. As can be seen from such figure, under both conditions, the tendency was that the average completion time decreased over time, assuming quite stable values approximatively in the middle of the training.

Fig 5
Download : Download high-res image (342KB)
Download : Download full-size image
Fig. 5. Task Time Means over sessions in the two conditions (Tablet vs. Pepper). The error bars show the 95% confidence interval.

The number of the correct answers provided during the interaction with the game was slightly better for the tablet. On average, in the Pepper group, each user correctly answered 10.6 questions, whereas in the Tablet group each user correctly answered 11.2 questions. In both versions, the minimum number of correctly answered questions by users was 4, and the maximum number was 15.

Regarding the answers that users were not able to provide within the given time, they were on average 0.7 answers on the tablet and 0.4 on Pepper. As for the answers that were provided but they were wrong, they were 3.1 on tablet and 4 on Pepper, on average.

We also consider the number of users who were able to correctly answer at least 80% of the provided questions (=12 questions). For each of such users, we calculated the number of trials needed to answer such 12 questions correctly. For the tablet version users on average needed 5 trials, whereas on the Pepper platform, users on average needed 4.4 trials, which is slightly better.

User engagement
In the User Engagement Scale questionnaire, the subscales (1 = strongly disagree, 5 = completely agree) considered are: FA= Focused Attention (to what extent the application is able to receive focused attention by users); PU= Perceived Usability (the usability of the application as perceived by the user); AE= Aesthetics (to what extent the application is aesthetically attractive); RW= Reward (to what extent the user experience of interacting with the application is rewarding for the user).

Table 2 reports the user engagement values collected at the beginning of the test (i.e. after the end of the first session). As it can be seen, the subscale that received the highest value at the beginning of the test was Reward for the tablet and Perceived Usability for Pepper. At the beginning of the test, the overall, composite engagement score on the tablet was calculated as 16.1, for Pepper it was 16.2.


Table 1. Summary of the two conditions considered.

‘Tablet’ condition	‘Pepper’ condition
User input: touch-based (through the tablet)	User input: touch-based (through the robot display)
While using the tablet, users were seated at a table	Users were in front of Pepper, at a 30-40 cm distance
Feedback: visual, sounds, verbal (synthesised voice)	Feedback: visual, sounds, verbal (synthesised voice) + body animations + LEDs

Table 2. UES values collected at the beginning of the test (after the first session).

Subscale	M(SD) Tablet	M(SD) Pepper
FA	3.5 (0.8)	3.4 (1.1)
PU	4.1 (0.7)	4.5 (0.8)
AE	4.2 (1.0)	4.2 (0.5)
RW	4.3 (0.3)	4.1 (0.7)
Table 3 shows the user engagement values collected at the end of the test (i.e. after the end of the last session): the subscale that received the highest value at the end of the test was still Reward for the tablet and (again) Perceived Usability for Pepper. At the end of the test, the overall composite engagement score was calculated as 16.2 for tablet, 16.7 for Pepper.


Table 3. UES values collected at the end of the test (after the last session).

Scale	M(SD) Tablet	M(SD) Pepper
FA	3.3 (0.5)	3.1 (0.8)
PU	4.2 (0.8)	4.6 (0.3)
AE	4.2 (0.7)	4.5 (0.5)
RW	4.5 (0.6)	4.5 (0.5)
Thus, participants rated their engagement as higher in the robot condition than in the tablet condition. Thus, the more empathically/emotionally rich feedback provided by the robot seems to have affected the level of their engagement.

User's feedback
During the test, we took note about how users felt while interacting with the devices. At the beginning, some of the participants that discovered to be assigned to the ‘only-tablet’ group displayed some disappointment for not having the possibility to interact with the robot. Nonetheless, the caregivers reported that, after starting playing with the game, this feeling disappeared and the participants of both groups showed interest in interacting with it.

Before starting the game, participants in both groups received an introduction in which the application greeted the user and then presented the goal of the game. On the one hand, as soon as such introduction finished, the participants in the ‘tablet’ group immediately started the game session. On the other hand, users in the robot group responded to the robot's greeting spontaneously saying things such as “Good morning Pepper, you are so cute” and after the short game introduction given by the device they typically answered “Thank you Pepper” and several of them touched the robot's hands. It is worth noting that this situation did not occur only in the first session, but it was repeatedly observed by the moderator over various sessions. In addition, some patients talked to the robot as it was one of their best friends, i.e. telling it information about their current physical/cognitive status, and/or expressing the willingness/pleasure to meet it. For instance, one participant said: “Hello Pepper, this morning when I woke up I had a severe back pain and I was about to decide not coming here, however, in the end, I came because I knew I would have played with you”. Furthermore, the patient said that the robot is very ‘cute’ and its behaviour encourages him to continue playing the game. So, in this case, the robot was stimulating in making the older adults go out their house and more properly adhering to the training program, and it was approached in a way that resembles the kind of communication occurring among humans.

Even during the actual game, users of the robot group, differently from the others, continued considering the robot as having human traits, e.g. they said that it has a cute face, also touching the hands and the arms of the robot. As another example, when a user was wrong in giving an answer, he told the robot: “I'm a bit slow, Pepper, please be patient with me”. Occasionally, when they realised that the answer they chose had some similarities with the correct answer (for instance, it was a longer version of the correct title, or it was part of the refrain of the song but not the correct answer), they reacted as the robot itself would have tried to be misleading towards them (“You are trying to trick me”). On the other hand, one user, after correctly replying to an answer and after seeing the robot as expressing great enthusiasm, interpreted the robot's reaction as really empathic towards him (“Can you see? The robot is happy! If the robot is happy, I'm happy too”).

The game was appreciated by all the users, and resulted in a stimulating atmosphere. This was also perceived by caregivers who noticed that users sometimes, just after the test, commented about the results they got at the end of the test session. Indeed, as it has been previously mentioned (see “Game” section) at the end of each session, as a feedback, the game provided the users with a summary message about their performance. Indeed, it sometimes happened that users compared their current performance with the feedback they received during the previous session (when they were able to remember it), other times they compared this summary feedback with the one received by the others.

In addition, during the test, some patients were observed using compensatory strategies for their memory lacks. For instance, one user, after realising to have provided an incorrect answer, declared that, for the next round, he would have tried to memorise that the correct answer was the one having the longest title between the possible options. One patient of the ‘tablet’ group even brought to the training session a piece of paper where she had written down a few answers she declared unable to recall (of course, she was told not to use it during the game). Caregivers reported that the robot significantly contributed to making the atmosphere in the training program very dynamic, activating and stimulating, even in a part of the training period (i.e. towards the end) when, traditionally, some drops in patients’ attendance were registered during the previous editions. According to what caregivers reported, patients also appreciated the fact that, differently from previous editions of the TTB project where the games were basically group-based ones, this edition considered single-user games; therefore they could engage individually with them, and at their own pace. The inclusion of a highly innovative device like the robot, and the fact that each of them was asked to interact with it individually, increased their sense of ‘importance’ and self-esteem (generally low in this kind of patients). Finally, all the users successfully completed the training sessions, which was also reported as a good result per se.

Discussion
Overall the test results were encouraging: users were enthusiastic about participating, and also caregivers expressed satisfaction about how this experimental training went and the level of engagement that users showed. As a result of this study, a number of design-related considerations can be put forth.

Often it is challenging to motivate MCI users to train frequently and for a sufficiently long time using traditional training exercises, which after a while participants typically find repetitive, and thus feel disengaged. The provision of a digital game through a humanoid robot seems to successfully stimulate MCI patients in better committing to their training throughout its entire duration. In addition, the fact that users were positive toward Pepper and open to interacting with it, shows that older age is still compatible with robot use. Furthermore, although not having specific previous experience in interacting with robots, all the subjects were able to individually interact with Pepper, and successfully complete the training without experiencing particular difficulties. This shows that this type of device and training game can be considered as a viable and usable opportunity for stimulating MCI elderly, also because it provides some emotional support, which is an aspect to which older adults are sensitive.

Another aspect we noted was the fact that the level of engagement in the group of people who interacted with the robot also seemed to affect the group that used the tablet. The game created quite a socially active and ‘competitive’ environment. Users of both groups were observed as interested to know and improve their own performance over time (when they received the final feedback at the end of a session they tried to compare it with what they had got in the previous session, when they were able to remember it). However, especially at the beginning of the training, the users of the robot group were observed to be more interested in prompting discussions with other users about the game results obtained, to compare them with other users, which subsequently also affected users of the other group. Thus, the game also offered a good opportunity for socialisation among the recruited patients.

Another result of the study was that the more empathic feedback provided by the robot seemed to improve the level of user engagement. Regarding the relationships between user engagement and performance in carrying out the submitted tasks, our initial hypothesis was that more engaged users would score higher in terms of correct answers. Instead, the results gathered in this evaluation indicate that more engaging systems do not necessarily foster better task success outcomes, as users of the (less-engaging) tablet version, on average, performed slightly better than the robot group (see “Results” sub-section) in terms of correct answers provided.

Regarding task completion times, in the robot condition, Pepper adds some more empathic/emotional feedback (through body movements and lights) to better emphasise and reinforce the results. Therefore, taking into account the kind of target users involved in the test, it is worth pointing out that in the robot condition, users were able to stay focused on the tasks, even though they often needed to switch their attention between the two parts of the robot: the one providing graphical information (the chest) and the parts providing lively and emotional expressions (i.e. a mix of eye led colours, head postures, robot's gestures and vocal feedback). Thus, we can argue that the emotional feedback provided by the robot did not distract them from their main task.

In order to further improve the empathic aspects of the training and make it even more stimulating for the target subjects, further improvements can be envisaged. For instance, a more personalised interaction could be designed, starting from the feedback messages that the application provides to the user. Indeed, in our study, we observed that people were very interested in their (and in the others’) performance, thus, more personalised feedback can be provided to keep users motivated and engaged. For instance, the feedback could include reinforcement phrases referring to previous user results or the average performance of the class, encouraging subjects to maintain and progress beyond their last played level, and therefore stimulate users in better adhering to the cognitive training. Finally, as mentioned, both versions of the game supported the same tasks and had the same application logic, the main difference was in the way the application provided feedback (with the robot version that exploited its human-like form and possible expressions). However, it is worth noting that it would have been possible to include an avatar with human form in the tablet version as well. As a possible limitation of the study, we mention the fact that, without proper empirical validation, we cannot conclude that an avatar with human form rendered on a tablet could not bring similar empathic effects as the humanoid robot.

Conclusions and future work
In the context of interventions for reducing cognitive decline in the elderly population, technologies have been increasingly conceived as a support for patients, their caregivers and the clinicians. The presented study aims to investigate how seniors with Mild Cognitive Impairment relate with and perceive serious games accessed through humanoid robots, as part of a training programme aimed to improve their cognitive status. One limitation of this work is that the interaction period was limited and the sample of subjects was not very large, therefore for future work we plan further evaluations over more extended periods of time and considering larger trials, which can provide data suitable for statistical analysis as well. Another aspect that we plan to investigate in future work is the possibility to consider additional, more varied multimodal interactions to further increase the level of empathic support provided by the robot.

