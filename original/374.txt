Wireless connectivity has traditionally been regarded as an opaque data pipe carrying messages, whose context-dependent meaning and effectiveness have been ignored. Nevertheless, in emerging cyber-physical and autonomous networked systems, acquiring, processing, and sending excessive amounts of distributed real-time data, which ends up being stale or useless to the end user, will cause communication bottlenecks, increased latency, and safety issues. We envision a communication paradigm shift, which makes the semantics of information (i.e., the significance and usefulness of messages) the foundation of the communication process. This entails a goal-orient-ed unification of information generation, transmission, and reconstruction, by taking into account process dynamics, signal sparsity, data correlation, and semantic information attributes. We apply this structurally new, synergetic approach to a communication scenario where the destination is tasked with real-time source reconstruction for the purpose of remote actuation. Capitalizing on semantics-empowered sampling and communication policies, we show significant reduction in both reconstruction error and cost of actuation error, as well as in the number of uninformative samples generated.

Wireless connectivity has traditionally been regarded as an opaque data pipe carrying messages, whose context-depen-dent meaning and effectiveness have been ignored. Nevertheless, in emerging cyber-physical and autonomous networked systems, acquiring, processing, and sending excessive amounts of distributed real-time data, which ends up being stale or useless to the end user, will cause communication bottlenecks, increased latency, and safety issues.

Introduction
Today's communication technology offers a cornucopia of wireless connectivity solutions and is the foundation of our hyperconnected society and automated economy. The interconnection of myriad autonomous smart devices (robots, vehicles, drones, etc.) empowered with advanced sensing, computing, and learning capabilities is forecast to generate a staggering amount of data (on the order of zettabytes). For example, data gathered by an autonomous car starts from 750 MB/s. A swarm of mobile robots may involve transmission of 1 GB aggregated data per second for target tracking or collaborative sensing.

In this expanding ecosystem, wireless networks are evolving to cater to emerging cyber-physi-cal and mission-critical interactive systems, such as swarm robotics, self-driving cars, and smart Internet of Things (IoT). These systems call for reliable real-time communication, autonomous interactions, and automated decision making. Their successful operation entails the processing and exchange of massive volumes of multimodal, often high-dimensional, distributed data in an efficient, effective, and timely manner. Simply generating and communicating data traffic, which often ends up being outdated or irrelevant to the end user's application, will cause severe communication bottlenecks. These bottlenecks could inevitably jeopardize the proper functioning of wireless networks, leading to unnecessary network congestion, wasteful resource utilization, and excessive energy consumption.

The End of the Current Communication Paradigm?
Virtually all of today's wireless systems are built upon fundamental principles of reliable communications over noisy channels, first developed in the locus classicus of information theory [1]. Despite various endeavors [2]–[3][4][5][6], most existing communication paradigms are content-agnostic, in particular at the lower protocol layers where the significance and the effectiveness of transmitted messages have been set aside. The main objective has been to optimize conventional key performance indicators, such as throughput, delay, and packet loss; quality of service is usually provided through network over-provisioning and resource reservation control.

The dichotomy of information content and significance was a conceptual advance, which has been suitable for classical data communication targeting error-free high-speed data transmission. In sharp contrast, this approach comes short of meaningfully scaling and of supporting the needs of emerging networked intelligent systems and machine-type communication. Consider, for example, a large number of autonomous mobile robots communicating with one another to reach timely consensus in the negotiation for collision avoidance. Achieving this goal is neither simply a question of understanding the throughput-reli-ability-delay trade-off, nor of delivering streams of “random” bits from one robot to another while maximizing throughput or minimizing delay. For safe and successful operation, it is crucial to factor into the communication process the urgency and the value of messages provided by each robot. That way, transmissions are prioritized efficiently, and the application demands are met with greater accuracy.

Recent work on status update systems (e.g., environmental monitoring, news reports, web crawlers) and networked control systems has started addressing data prioritization issues. Therein, first steps toward importance-aware communication have harnessed the concepts of age of information (Aol) [7], value of information (Vol) [8]–[9][10][11], and quality of information [12]. Several application-driven technologies have recently emerged for the upper layer network management and orchestration, including named-data networking [13], semantic-plane protocols [14], zero-touch networks, as well as software-defined and intent-based networking. Most of these networking paradigms rely on high-level abstractions and leverage machine learning to configure and continuously maintain the network in a desired state according to business intents.

Toward Goal-Oriented Semantic Communication
At the source level, semantics refers to the relative importance of different, often equiprobable, events, outcomes, or observations from a stochastic source of information or a process. For instance, these primary information sources could represent sensor measurement data, patterns of a physical phenomenon (e.g., a vehicle's trajectory), or the state of a dynamical system.
Looking beyond the aforementioned confined view of wireless connectivity, we place ourselves in a setting where communication is not an end in itself but a means to achieving specific goals. Our vision entails a communication paradigm shift to enable the generation and timely provisioning of the appropriate information to the right processing point. This can be realized, in a nutshell, by making the semantics of information the foundation of the entire communication process. Different from its common use in linguistics, logic, or computer science (Semantic Web, databases, ontologies, etc.), semantics is employed here with its etymological meaning, that of significance. Semantics here is a measure of the usefulness of messages with respect to the goal of data exchange. Concretely, we propose a semantics-empowered communication system, whose foundations entail a goal-oriented unification of data generation, information transmission, and usage. Our approach capitalizes on the largely untapped innate and contextual attributes (semantics) of information. That way, the entire communication process can be tailored to meet the networked applications' requirements, thus enabling the achievement of specific goals. Considering a simple actuation-oriented real-time reconstruction scenario, we show that this paradigm shift has the potential to entirely transform several prevailing design principles. We showcase its potential to significantly reduce the number of uninformative data samples, the real-time reconstruction error, and the cost of actuation error.

Semantics-Empowered Communication
Defining the Semantics of Information
A first natural question is how to define and measure the information semantics. We advocate for assessing and extracting the semantic value of data at three different granularity levels.

Microscopic Scale
At the source level, semantics refers to the relative importance of different, often equiprobable, events, outcomes, or observations from a stochastic source of information or a process. For instance, these primary information sources could represent sensor measurement data, patterns of a physical phenomenon (e.g., a vehicle's trajectory), or the state of a dynamical system. Imagine two equally rare events, occurring with very low probability, one of which carries a major safety risk, while the other is just a peculiarity. Although they provide the same high amount of information, the information conveyed by the first event is evidently of higher significance. This disparity in importance can be incorporated into key information measures (e.g., entropy rate, mutual information) and statistical similarity metrics (e.g., f-divergences) using weight functions. These functions may be context-dependent and could incorporate various temporal variations and spatial patterns in information utility. Semantics can also be captured using Rényi's information measures [15], which are instrumental in assessing compressability, sparsity, and trackability of stochastic processes, signal complexity, as well as information gain efficiency in decision making (e.g., robotic exploration, importance sampling, multi-goal reinforcement learning).

Mesoscopic Scale
At the link level, semantics of information is a composite nonlinear multivariate function of the vector of information attributes. These qualitative attributes of information can be either innate (objective) or contextual (subjective). The former are attributes inherent to information regardless of its use; they depend on the information generated by a source and on its transformations (e.g., compression). Representative innate attributes include freshness or Aol (the time elapsed since the newest successfully received sample was generated) and precision (a measure of the degree of closeness of measured values to each other and of the reproducibility of the measurement). The latter are attributes that depend on the particular context or goal for which information is being used. The most relevant ones are timeliness, that is, the time instant by which information has to be available for use at a point of computation or decision making, and completeness, an information relevance attribute that measures the difference between the information amount and the total information of the real world. For example, an image delivered by the network for remote monitoring has a certain freshness, field of view, and resolution (precision). Mission-critical applications impose stringent requirements on timeliness and availability of data.

A widely used attribute is accuracy, which describes the degree of correctness and can be perceived as both intrinsic and contextual. It is related to distortion, which is the distance between the measured or estimated value or state and the true value or state. As stated above, information semantics can be formally defined as a composite function, where a context-dependent, cost-aware function is applied to a multidimensional function of information attributes. In a simple example, semantics can be a weighted sum of accuracy and timeliness, where timeliness (con-textual) could in turn be an exponentially decreasing function of information freshness (innate).

Information Value Dualism
It is important to highlight that information may have a value per se, in addition to its “utilitarian,” context-depen-dent value. For instance, the precision of a sensor measurement has an intrinsic value related to the quality of how accurately it represents a phenomenon, whereas this same measurement has different value depending on its context of use and the application requirements (e.g., whether it monitors temperature in a smart home or in a nuclear plant). In a later section, we introduce the cost of actuation error, which may cast the reconstruction error to the context of the application.

Macroscopic Scale
At the system level, semantics of information is related to the end-to-end, effective distortion and timing mismatch between information generated at a point or region in space-time and its reconstructed or estimated version at another point in space-time, factoring in all sources of variability and latency (sensing latency and accuracy, data gathering, transmission latency, decoding, processing, etc.). The spacetime coordinates represent an event. For example, the original information may represent the system state of the physical world at a certain area, while its estimation may represent the perceived information about this physical world at a remote unit (virtual world, digital twin). In many real-time networked systems, the objective is to provide the observer at the receiver side with an instantaneous and accurate estimate of the information generated at the transmitter side. This calls for a relativistic information transfer theory that allows us to synchronize the state evolution at both communication ends. In other words, the evolution of reconstructed information is aligned to the temporal dynamics of the original information source, while at the same time the closeness between the estimated and the true state is maximized. Roughly speaking, that could minimize the time duration in which a remote unit remains in an erroneous and/or time mismatched state, via necessary transformations compensating for the system timing dilation (to draw an analogy with relativistic clock synchronization). This holds the promise to provide the theoretical foundations for applications targeting real-time experience, such as extended reality, tactile Internet, and holographic communication.

Semantics-Empowered Communication Model
In this section, we present the envisioned semantic communication model. In sharp contrast to most prevailing communication systems that assume uncontrolled exogenous traffic arrivals, the communication process in our proposed architecture starts from information generation and data acquisition. This radical departure capitalizes on smart devices' ability to control their traffic via seman-tic-aware active sampling, in which samples are generated at will or trigger-based. Furthermore, the entire communication process extends up to goal-oriented signal reconstruction, and information usage and exploitation.

The generale-nd-to-end semantic communication model is depicted in Fig. 1 and mainly includes the following building blocks:

Multiple continuous or discrete time, possibly correlated, signals (stochastic processes) and information sources, which represent a time-varying real-world physical phenomenon in space, are observed by spatially distributed smart devices. In general, these devices may have heterogeneous sensing, computational, and learning/inference capabilities.

Smart devices access a shared communication medium to send data samples (e.g., observations, measurements, updates) to one or multiple destinations (e.g., fusion center, control unit). Samples are generated at will using process-aware, non-uniform active sampling, according to the source variability (e.g., changes, innovation rate, autocorrelation, self-similarity), the communication characteristics, and the semantics-aware applications' requirements. That way, only the most valuable and informative samples are generated and prioritized for transmission.

Source samples could be preprocessed prior to being encoded and scheduled for transmission over noisy and delay-/ error-prone communication channels. This operation may include quantization, compression, and feature extraction, to name a few. Scheduling is performed according to semantic information value and priority, extracted from data.

The input signals (sources) are finally reconstructed at the destinations from causally or non-causally received samples to serve the application purpose, including collision avoidance, remote state estimation, control and actuation, situation awareness, and learning model training, to name a few. In general, the reconstructed signals may alter the recipients' states and may initiate specific actions at the receiving ends (actionable intelligence).


Figure 1.
End-to-end goal-oriented semantic communication model.

Show All

Joint Sampling, Communication, and Reconstruction Under Real-Time Constraints
A fundamental element of the proposed communication paradigm is the cohesion of the entire process of information generation, transmission, and reconstruction, which has to be synergistically redesigned under the prism of semantic information. Let us highlight this with an example from networked robotics. A mobile robot generates and sends updates of a continuous stochastic process (e.g., a vehicle's trajectory) to a remote tracking unit for real-time causal reconstruction. Conventional approaches decouple sampling from transmission, resulting in simple but suboptimal solutions. Sampling is optimized based on a signal's changes; therefore, samples might become stale before being successfully received. Transmission is optimized based on quality of service metrics (e.g., delay, rate, timeliness) ignoring the source variations; samples may be received on time but contain no useful information or could even be misleading about the system's true state. This simple example reveals the structural links between sampling and communication, which are generally non-separable in semantic communication. This means that one cannot just take the best sampling policy, place it before the best communication scheme, and expect to get the best out of both. The key challenge is to develop a theory of optimal, semantics-aware joint active sampling, transmission, and reconstruction of multidimensional signals, in particular under stringent timing constraints. This is of cardinal importance for enabling timely decision making and for efficiently meeting the requirements of real-time networked applications.

An Illustrative Example
We consider an end-to-end communication system in which a device monitors a two-state Markovian source. The source initiates certain actions to a robotic object at the transmitter side, and the goal is to have a digital twin of that robotic object at the receiver side (Fig. 2). We consider a slot-ted-time system in which the monitoring device samples the process and transmits updates on the source's status to a remote actuator. Status update packets are transmitted over a wireless erasure channel in which realizations are independent and identically distributed over time slots. We consider two cases, one with low channel quality, in which the probability of successful transmission is 0.4, and one with high channel quality and success probability 0.9. Real-time source reconstruction is performed at the endpoint, upon receipt of status updates, as a means to achieve the real-time actuation goal of the digital twin.


Figure 2.
The setup for the illustrative example.

Show All

Sampling and Transmission Policies
We consider the following four policies for information generation and transmission.

Uniform
In this source-agnostic policy, sampling is performed periodically, independent of the evolution of the source process. Thus, there could be several state transitions (changes) between two collected samples, especially for rapidly varying sources. If transmission fails, the most recently acquired measurement (sample) is communicated.

Age-Aware
In this policy, acquisition and transmission of a new sample is triggered by the receiver once the Aol reaches a given threshold. Note that this could also model the case where the transmitter knows the Aol at the receiver side (using feedback acknowledging, or not, receipt of the sample). Since Aol can be viewed as a concrete, quantitative surrogate for semantics, the age-aware policy can be considered as a first, simple semantics-aware scheme. Whenever a transmission fails, the receiver tries to anticipate the update based on the statistics of the source process. In that case, the receiver, given its current state, tries to predict the next state based on the state transition probabilities, which are assumed to be known (or can be learned after a period of time).

Semantics-Aware
This is a source change-triggered policy; that is, sample generation is triggered at the transmitter side whenever a change at the state of the source is observed (since the previous sample). In a way, this can be seen as a Vol-aware sampling policy. Consider that in a given time slot, the source is in a certain state, in which it remains for a certain period of time. At the end of that period, the state changes; hence, the transmitter generates and transmits a new status update sample. Note that this policy takes into account only changes occurring and tracked at the source side (transmitter).

E2E Semantics
In this end-to-end policy, sample acquisition is triggered whenever there is discrepancy between the states at the two communication ends. It extends the semantics-aware policy so that the amount of change is not solely measured at the source, but is tracked by the difference in state between the two ends. Let us clarify what is meant by difference here. Assume that in a given time slot, both source and destination are in the same state. Then a change in the state at the source occurs in the next slot; hence, a new sample is generated and transmitted. In the case of erasure, the reconstructed source will remain in the previous state. In the next slot, the original source returns back to the state that was two time slots before. This means that no discrepancy exists now between the original and the reconstructed source; thus, there is no need to send an update.

Metrics and Performance Evaluation
Performing sampling and transmission at each slot could evidently provide the best result for the application. However, this approach does not scale; an excessive number of (not necessarily useful) samples is generated, which require a tremendous increase in communication resources for their transmission. Our semantic approach primarily aims at reducing or even eliminating the generation of uninformative sample updates, thus improving network resource usage.

Performance is assessed using the following metrics of interest: real-time reconstruction error and cost of actuation error. The reconstruction error measures the discrepancy in real time of the values between the original and the reconstructed source as time evolves. The cost of actuation error captures the significance of the error at the actuation point given the fact that some errors can be non-commutative and may have higher impact than others. We consider here two cases for the error occurrence: when the original source is in the first state, but the reconstructed source believes that is in the second state, the cost of actuation is low (e.g., set to one), while in the opposite case, the cost is assumed to be relatively high (e.g., equal to five). The latter corresponds to a case where the penalty or the loss from taking a wrong action upon a misconceived system's state is high. When the sources are in the same state, there is no actuation error.

We have two scenarios regarding the source variability, the first being when the source is slowly changing, depicted in Fig. 3, and the second being when the source is rapidly changing, depicted in Fig. 4. We observe that in the case of low source variability, the age-aware policy outperforms the semantics-aware one when the communication channel quality is poor. This is due to the fact that if a transmission error occurs and the receiver fails to anticipate the right state, it leads to high reconstruction error. This is the case where a uniform sampling policy could perform better. However, the performance from the perspective of cost of actuation error is different; the semantics-aware policy outperforms uniform sampling.


Figure 3.
The case of a slowly varying source: a) Real-time reconstruction error; b) Cost of actuation error.

Show All

Figure 4. - The case of a rapidly changing source: a) Real-time reconstruction error; b) Cost of actuation error.
Figure 4.
The case of a rapidly changing source: a) Real-time reconstruction error; b) Cost of actuation error.

Show All

For slow varying sources, E2E semantics significantly outperforms the semantics-aware scheme due to the fact that the system manages to eliminate the discrepancy fast, even in the case of a low-quality channel. On the other hand, for rapidly varying sources, both semantics-empowered policies exhibit similar reconstruction error performance, while E2E semantics provides the lowest actuation error without wasting resources transmitting uninformative samples.

Uninformative Samples Reduction
We provide here the percentage of uninformative samples each policy generates. E2E semantics, by definition, does not create redundant samples, since it accounts for the end-to-end discrepancy among sources at both ends. Uniform and age-aware policies generate, in most cases, the highest percentage of redundant (uninformative) samples. Despite being expected for uniform sampling, the explanation for the age-aware scheme is as follows. For the purpose of real-time remote reconstruction, metrics based solely on information freshness are inefficient since baseline Aol does not take into account the source variability. The performance of semantics-aware policy is relatively good, despite operating only at the transmitter side. The percentage of uninformative samples is less than 10 percent when the channel is good for a slow varying source and less than 5 percent for a rapidly varying source.

In a nutshell, semantics-empowered policies basically generate informative samples, that is, samples conveying the most valuable information for the purpose of real-time reconstruction and actuation, for which the timing when this information has been acquired is crucial. Note that additional gains, mainly in terms of savings in communication load and in the number of samples generated, could be achieved by learning the patterns of the source evolution, for instance, via reinforcement learning.

Semantics-Aware Networking
In this section, we present key functionalities required for reliable and timely communication of concisely represented valuable information in semantics-aware networks. The main operations span from local goal-oriented information acquisition, representation, and semantic value inference, up to data prioritization mechanisms and in-network processing (e.g., fusion, compression). Specifically, they will allow the following to be performed.

Semantic filtering for avoiding unnecessary redundancy during data acquisition and information encoding using active sampling and censoring. That way, only useful and relevant information is generated and transmitted. Moreover, seman-tics-aware data acquisition can significantly reduce the average sampling rate (sub-Nyquist limit) and channel utilization without affecting reconstruction accuracy.

Semantic preprocessing, which enables goal-oriented sparse representations (e.g., feature extraction, labeling, embedding, segmentation) and computations on information manifold. For example, a robot could compute local estimates of the state (tracked target's velocity and location) from visual features or scene labeling extracted from an image instead of sending raw data. Another instance is in distributed learning, where only data samples that are semantically representative (core set selection) or informative are processed and transmitted.

In a nutshell, semantics-em-powered policies basically generate informative sanples, that is, samples conveying the most valuable information for the purpose of real-time reconstruction and actuation for which the timing when this information has been acquired is crucial. Note that additional gains could be achieved by learning the patterns of the source evolution, for instance, via reinforcement learning.
Semantic reception for fast partial or approximate source reconstruction, as well as goal-de-pendent information recovery, fusion, and querying. Reconstruction quality is conventionally measured by distortion metrics, such as the time-averaged mean square error (MSE) between the estimated and the original input signal, or via entropy-based measures. Depending on the end user's objectives, approximate results with different distortion or perceptual quality could be sufficient for achieving a specific goal. For example, a low-quality, highly compressed video may be sufficient for remote surveillance during non-alert mode. Furthermore, in many scenarios involving, among others, images, patterns, and machine learning, low distortion does not necessarily mean high perceptual quality. In that case, reconstruction efficiency could be assessed using a semantic quality indicator based on divergence measures or distance functions (e.g., Kantorovich-Wasserstein). In the illustrative example, we saw that a scheme can have higher cost of actuation error (low perception quality) in spite of achieving lower reconstruction error (low distortion) than others.

Semantic control, which enables agile orchestration of multi-quality multimodal information gathering and fusion and efficient resource utilization. Metadata processing is required for scalability whenever the amount of metadata carrying information on semantic attributes and the number of network nodes become prohibitively large.

Future Challenges
We now discuss key open problems and technical challenges associated with this promising avenue of research.

Semantic Metrics
A key challenge is to establish concrete metrics, which incorporate qualitative attributes of information in the existing communication-theoretic edifice. These new semantics-based metrics should capture both source and network dynamics, as well as potential nontrivial interdependencies among information attributes.

Semantics-Aware Multiple Access
Consider a large number of heterogeneous devices that transmit, in either a time- or event-triggered pro-cess-aware manner, signals conveying multi-quality information (not necessarily from the same codebook) to a remote destination. For optimally utilizing the shared medium, devices have to adapt their access patterns and transmission attempts not only based on exogenous traffic arrivals and the other nodes' status, but also based on the source or process variability, the information semantics, and the applications' demands.

Goal-Oriented Resource Orchestration
Semantics-empowered communication will significantly improve network resource usage, energy consumption, and computational efficiency, thus supporting the scalability of future massive networked intelligent systems. It will pave the way for the design of next-generation real-time data networking and will provide the foundational technology for a plethora of socially useful services.
Semantic real-time data networking requires efficient scheduling and resource allocation policies for gathering multi-source multimodal - often correlated - information, acquired at different levels of quality. The objectives of emerging networked applications could be achieved by utilizing one of multiple alternative sets of multi-quality data objects. For example, a remote monitoring system could normally operate in non-alert mode using sensing data (e.g., images) with precision above a percent and freshness above β. These problems fall in the realm of real-time scheduling with multiple choices, for which online algorithms may select which piece of information, from where and when, to gather and transmit under communication and processing resource constraints.

Multi-Objective Stochastic Optimization
Semantics-aware data gathering and prioritization require multi-criteria optimization with goal-ori-ented, end-user-perceived utilities, which assess the relative degree of priority among different information attributes. A multi-objective stochastic optimization framework based on cumulative prospect theory, which incorporates semantics via risk-sensitive measures and multi-attribute entro-py-based utility functions, and performs rank-de-pendent nonlinear semantic weighting, seems to be a promising endeavor.

Epilogue
Supporting autonomous, real-time, and connected intelligence applications in future wireless networks necessitates fundamental theoretical advances in communication, information theory, and signal processing. It requires transforming commonly held design assumptions and prevailing communication paradigms. We propose a structurally new, synergistic approach that accounts for the information semantics and aims at harnessing the high potential benefits of a goal-oriented unification of information generation, transmission, and usage, which have hitherto been treated separately. Semantic networking will enable carrying around only the most informative data samples, thus conveying to the end user only information that is timely, useful, and valuable for achieving its goals. Semantics-empowered communication will significantly improve network resource usage, energy consumption, and computational efficiency, thus supporting the scalability of future massive networked intelligent systems. It will pave the way for the design of next-generation real-time data networking and will provide the foundational technology for a plethora of socially useful services, including autonomous transportation, consumer robotics, environmental monitoring, and telehealth.