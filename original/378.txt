To facilitate the deployment of machine learning in resource and privacy-constrained systems such as the Internet of Things, federated learning (FL) has been proposed as a means for enabling edge devices to train a shared learning model while promoting privacy. However, Google's seminal FL algorithm requires all devices to be directly connected with a central controller, which limits its applications. In contrast, this article introduces a novel FL framework, called collaborative FL (CFL), which enables edge devices to implement FL with less reliance on a central controller. The fundamentals of this framework are developed and a number of communication techniques are proposed so as to improve CFL performance. An overview of centralized learning, Google's FL, and CFL is presented. For each type of learning, the basic architecture as well as its advantages, drawbacks, and operating conditions are introduced. Then four CFL performance metrics are presented, and a suite of communication techniques ranging from network formation, device scheduling, mobility management, to coding are introduced to optimize the performance of CFL. For each technique, future research opportunities are discussed. In a nutshell, this article showcases how CFL can be effectively implemented at the edge of large-scale wireless systems.
Introduction
Machine learning (ML) finds a wide range of applications in wireless networks ranging from data analytics to network monitoring and optimization [1]. However, centralized ML requires edge devices to transmit their data to a central controller for learning. In practical deployments of ML in wireless systems, such as the Internet of Things (IoT), due to privacy issues and stringent resource (e.g., bandwidth and power) constraints, edge IoT devices may not be able or willing to share their collected data with other devices or a central controller. To enable edge devices in a wireless network in training a shared ML model without data exchanges, federated learning (FL) was proposed by Google [2].

FL is a distributed ML scheme that allows IoT devices to collaboratively perform on-de-vice training of a shared ML task while only exchanging model parameters with a central controller. Keeping the data at IoT devices not only promotes privacy but may also reduce network congestion. Due to the unique features of FL, a number of existing works (e.g., [3]–[4][5][6]) have studied its use for wireless network optimization.

In practice, to implement FL over IoT networks, edge devices must repeatedly transmit their trained ML models to a central controller via wireless links. Due to limited wireless resources, say in an IoT, only a subset of devices can use FL. Meanwhile, ML models that are transmitted from IoT devices to a central controller (e.g., a base station) are subject to errors and delays caused by the wireless channel, which affects the learning performance. Therefore, it is necessary to consider the optimization of wireless networks to improve FL performance, as pointed out in [7]–[8][9]. This emerging “communications for FL” research area is the key focus of this work.

Recently, several surveys and tutorials related to FL over wireless networks have appeared in [4]–[5][6], [10]. First, the works in [4]–[5][6] investigated the use of FL for communications, rather than the impact of wireless networking on FL. Moreover, the prior works in [4]–[5][6], [10] focused on the original FL from Google in [2] (called original FL hereinafter), which requires all IoT devices to transmit their ML models to a central controller. Hence, these existing surveys did not consider the implementation of FL with less or even no reliance on the central controller. Furthermore, they did not analyze how to use wireless techniques to optimize FL performance.

The main contribution of this article is a novel FL framework, dubbed collaborative FL, that combines collaborative learning [11] with FL to enable edge devices to perform FL without a central controller. We first provide a detailed overview of centralized learning (CL), original FL (OFL), and collaborative FL (CFL), and summarize their advantages, drawbacks, and operating conditions. Then we introduce four important performance metrics to quantify the CFL performance over IoT systems. Further, we introduce several important communication techniques to optimize the CFL performance metrics. For each communication technique, we first introduce the motivation for optimizing CFL performance and then present future research opportunities. We end the article with some conclusions.

Preliminaries and Overview
Centralized Learning
As shown in Fig. 1, CL needs only one ML model located at a central controller. All devices must be connected and send their data to the base station (BS) for training this ML model. Then the BS will transmit the trained ML model to all devices. Hence, CL only requires the BS to communicate with all devices once to collect all devices' datasets.

Figure 1. - Architectures of CL, OFL, and CFL. Here, to implement CFL, the BS is not necessary.
Figure 1.
Architectures of CL, OFL, and CFL. Here, to implement CFL, the BS is not necessary.

Show All

Table 1 summarizes the advantages, disadvantages, and operation conditions of CL. The key advantage of CL is that it enables the BS or cloud to directly find a global ML model that minimizes the learning loss function value. Since the entire training process is completed by the BS, the ML training will not be affected by wireless network performance. However, imperfect transmissions may introduce errors to the data used for training. Moreover, CL requires devices to transmit their private data with the BS. Also, significant overhead and resources are needed at the network and device levels to execute CL.

Table 1. Summary of the advantages, drawbacks, and operation conditions of ML over wireless networks.

Original Federated Learning
To maintain a degree of privacy, Google's OFL framework allows each edge device to cooperatively train a shared ML model without data transmission. In OFL, both devices and the BS own an ML model with the same architecture, as in Fig. 1. OFL is trained by an iterative learning process. First, all devices use their local data to train their local ML models and transmit their trained models to the BS. Then the BS aggregates the received ML models, generates a new aggregate ML model, and transmits it back to all devices. Hereinafter, the ML model trained by an edge device is called the local FL model, while the ML model generated by the BS is called the global FL model. At convergence, the global FL model is equal to all local models, which means that devices find a shared FL model, and the local FL model at convergence can be used to analyze all devices' datasets. The number of iterations that the FL needs to converge depends on the ML model architecture, learning tasks, devices' datasets, and the number of involved devices.

The key advantage of OFL is that it promotes data privacy and can be implemented over devices with less overhead than CL. However, OFL still requires all devices to transmit their local FL model parameters to a BS. Hence, imperfect and dynamic wireless transmission will significantly impact the OFL convergence time and performance.
The advantages, disadvantages, and operating conditions of OFL are summarized in Table 1. The key advantage of OFL is that it promotes data privacy and can be implemented over devices with less overhead than CL. However, OFL still requires all devices to transmit their local FL model parameters to a BS. Hence, imperfect and dynamic wireless transmission will significantly impact the OFL convergence time and performance.

Collaborative Federated Learning
OFL requires all devices to send their local models to a BS; however, in an IoT, devices may not be able to be connected to the BS due to energy limitations or a potentially high transmission delay. To overcome this challenge, we propose the concept of CFL, using which devices can engage in FL without being connected to a BS or a cloud.

In CFL, some devices are directly connected to the BS, while others are associated with a certain number of neighboring devices. For example, as shown in Fig. 2, for OFL, device b cannot be directly connected to the BS and perform FL due to a potentially high transmission delay. However, in CFL, device b can be connected to device a for performing FL. Since each device in CFL can use broadcast techniques to transmit its local FL model parameters to multiple other devices, CFL does not occupy more resources compared to OFL. CFL is also trained iteratively. First, each device transmits its trained local FL model to its connected devices or the BS. Then the BS generates the global FL model and transmits it to the associated devices. Finally, each device updates its local FL model based on the local FL models received from other devices or the BS. In OFL, each device trains its local FL model using gradient descent (GD) methods, while the BS aggregates the local FL models. However, in CFL, each device must both aggregate the local FL models received from other devices and train its local FL model.

Figure 2. - Simulation system for the implementation of CFL and OFL. In this figure, a quantity in red indicates the distance between two adjacent devices.
Figure 2.
Simulation system for the implementation of CFL and OFL. In this figure, a quantity in red indicates the distance between two adjacent devices.

Show All

To show the difference between CFL and OFL, we implemented a preliminary simulation for a network having one BS and six devices, as shown in Fig. 2. The local FL model of each device consists of a shallow feedforward neural network with 50 neurons. The MNIST dataset [12] is used for training the local models at each device, and each device has 500 data samples. We assume that device a is the farthest device that could be connected directly to the BS for FL training.

Figure 3 shows how the identification accuracy changes over time. Figure 3 demonstrates that CFL outperforms OFL. This is because, for OFL, only four devices can participate in FL, and the other two devices have a delay larger than 0.23 s. Since CFL allows devices to be connected to other devices and the transmission delay between any two neighboring devices is smaller than 0.23 s, six devices can participate. In fact, CFL can also reduce the energy consumption for device b since it only needs to transmit its ML model parameters to device a instead of the BS. The energy consumed by device a in CFL is similar to that of device a in OFL. This is because although CFL device a needs to exchange its local FL model with device b and the BS, it can broadcast its FL model. Meanwhile, it is known that the energy consumed by any wireless device for receiving data is much smaller than the energy needed for data transmission. Therefore, we can reasonably assume that the energy consumed by device a to receive the FL model of device b is negligible.

Figure 3. - Simulation results to show the performance of CFL and OFL.
Figure 3.
Simulation results to show the performance of CFL and OFL.

Show All

Table 1 summarizes the advantages, disadvantages, and operating conditions of CFL. The key advantage of CFL is that it enables the devices to perform FL without transmitting local FL models to the BS, as shown in Fig. 4. We can now remark:

Choosing between CL or FL depends on willingness to share data, ML model data size, and size of the collected data of each device. For example, when devices agree to share the data, and the size of the collected data is smaller than the ML model data size, CL is recommended.

Choosing between OFL or CFL depends on whether the BS performs FL, and the connection and transmission delay between devices and the BS. For example, if all wireless devices need to implement FL without the BS, CFL is more suitable.

OFL can be considered as a special case of CFL. In a network, if each device is connected to all other devices, CFL is equivalent to OFL.


Figure 4.
Number of iterations needed to converge for a CFL algorithm with different topologies [13].

Show All

Performance of CFL Over Wireless Networks
Loss Function Value
The value of a loss function is used to evaluate the CFL performance. The goal of CFL training is to find an ML model that minimizes the loss function, which depends on the local FL models of all participating devices. When those models are transmitted over wireless links, they experience transmission errors and delays, which can negatively impact the loss function during training. Due to limited energy and computing resources, only a subset of devices can be engaged in CFL, which decreases the total number of training data samples used for training the local FL models and increases the loss function value. Table 2 summarizes the wireless factors that affect the FL loss function along with suggested solutions.

Table 2. Summary of the wireless factors that affect the performance metrics and suggested solutions.
Table 2.- Summary of the wireless factors that affect the performance metrics and suggested solutions.
Convergence Time
For CFL, the convergence time has three components:

FL model parameter transmission delay

Time needed by each device to train its local FL model

Number of iterations for FL convergence (i.e., the number of global FL model updates)

The model transmission delay depends on the FL model data size and the data rate of the wireless link. The time used to train each device's local FL model depends on the FL model data size, the computational resources of each device, and the number of iterations (called the number of local FL model updates hereinafter) that each device uses to train its local FL model (using GD) at each FL iteration. Note that as the number of local FL model updates increases, the number of global FL model updates decreases. The number of global FL model updates also depends on the limited spectrum resources that restrict the number of devices that engage in FL. Table 2 summarizes the wireless factors that affect the convergence time and the suggested solutions.

Energy Consumption
The energy consumption needed for training a CFL algorithm has four components:

Local FL model transmission

Local FL model update

Global FL model transmission

Global FL model aggregation

Each device will spend energy for local FL model transmission and updating, while the BS needs to spend energy for global FL model transmission and aggregation. A trade-off exists between the energy consumption of the local FL model update and the transmission energy. The energy consumption of CFL depends on the FL model data size, the distance between the BS and the devices, the convergence time requirement, and the target loss function value. Table 2 summarizes the wireless factors that affect the energy consumption along with suggested solutions.

Reliability
For CFL, we define reliability as the probability that a CFL algorithm achieves a target loss function value. At each iteration, erroneous local FL models caused by imperfect wireless transmission must be discarded by the devices. Hence, the number of local FL models used to generate the global FL model will decrease, thus increasing the CFL convergence time and the loss function value. Hence, a CFL algorithm may not be able to achieve a target FL loss function value due to imperfect wireless transmissions. Therefore, the reliability of CFL depends on the wireless channel conditions. As the transmit power of each device increases, the number of erroneous local FL models decreases, thus increasing CFL reliability. Table 2 summarizes the wireless factors that affect the reliability and the suggested solutions.

Communication Techniques for Collaborative Federated Learning
We now review key techniques for improving the performance of CFL over wireless networks.

Network Formation
A tradeoff exists between the energy consumption of the local FL model update and the transmission energy. The energy consumption of CFL depends on the FL model data size, the distance between the BS and the devices, the convergence time requirement, and the target loss function value.
The first fundamental step toward deploying CFL is to analyze the process of network formation using which devices could be connected to one another to engage in a CFL task. In CFL, devices can form different network topologies. For example, IoT devices can form a grid topology for CFL, as shown in Fig. 4. Naturally, the training complexity and the FL convergence time directly depend on the formed topology. Hence, for any given scenario, one must investigate the optimal CFL network topology using the metrics described above.

Figure 4 shows an upper bound on the number of iterations for CFL convergence when assuming that the upper bound is derived while assuming that each device updates its local FL model using the Lazy Metropolis method and the GD method [13]. Figure 4 shows that when the number of links of each device increases, the number of iterations decreases because having more links increases the frequency of local FL model sharing.

CFL yields interesting network formation research questions.

CFL Network Formation Fundamentals
An IoT device may update its local FL model using the local models of a subset of devices, thus decreasing the CFL convergence time. Therefore, we must find an optimal device scheduling policy that can determine which iterations and the frequency with which each device performs CFL, thus optimizing the CFL performance metrics.
The optimal CFL network topology depends on the CFL performance metrics being optimized. Therefore, a fundamental CFL question is that of network formation: How can the devices interact to form an optimal network topology that maximizes the various CFL performance metrics and tradeoffs? To find the optimal topology, the first step is to define a proper utility function that jointly considers multiple dependent CFL performance metrics and network topologies. Given the defined utility function, one must develop network formation algorithms to optimize the utility function. Both centralized and distributed solutions can be developed. Centralized solutions such as search-based algorithms may be able to find a globally optimal network topology. However, the implementation of centralized solutions requires the collection of devices' information such as locations, which is impractical in large-scale IoT. For distributed solutions, one can adopt a game-the-oretic approach [14]. In a network formation game, each device is treated as an agent whose goal is to form a graph with neighboring devices to optimize the CFL performance metrics. The CFL performance (e.g., utility) depends on the entire graph and the decisions of all agents, which makes the use of mean field game theory suitable. When dealing with large-scale networks, one can rely on the tools of mean-field game theory, which allow capturing the presence of a very large number of agents in a game.

Network Formation with Asynchronous Training
Under asynchronous FL training, IoT devices will update and transmit their local FL models at different time slots. Due to limited computing and wireless resources, each device may not want to update its local FL model until it receives all local FL models of its associated devices. Using asynchronous training can increase local FL model update frequency and the data rate of each device, which reduces the convergence time. In asynchronous training, the number of devices that need to transmit the local FL models is time-varying. Hence, the network topology must be adapted to the changes in the number of devices that must transmit local FL models. Here, one must determine the frequency with which the network topology must be updated according to the number of participating devices. Each network topology update will change the wireless resource allocation and device association schemes so as to improve CFL performance metrics such as convergence time. However, network topology updates will also introduce communication overhead such as network state information sharing.

Network Formation with Partial Network Information
In a practical IoT network, each device may not completely know the network architecture, device locations, and network composition. Due to this limited information, the number of devices to which each device can be connected is limited, and hence devices may not be able to form a network topology that satisfies the CFL operating conditions (Table 1). Therefore, there is a need to investigate a globally optimal network formation for IoT devices with partial information. Since most existing complexity results on network formation (e.g., [13]) assume that each device has complete information, they cannot be used for devices with partial network information. Meanwhile, due to partial network information, devices may form several unconnected small device groups. Hence, a multi-layer network formation must be designed. For example, in the first layer, devices exchange their local FL model parameters in their own groups, while the local FL model parameters are exchanged over multiple groups in the second layer. The designed scheme must balance communication overhead and training complexity across layers.

Device Scheduling
Due to energy constraints and wireless resource limitations, the number of devices that can engage in CFL is limited. Hence, an IoT device may update its local FL model using the local models of a subset of devices thus decreasing the CFL convergence time. Therefore, we must find an optimal device scheduling policy that can determine with which iterations and the frequency with which each device performs CFL, thus optimizing the CFL performance metrics.

We envision several research problems related to device scheduling.

Data-Importance-Aware Device Scheduling
In CFL, the contribution of each device's dataset to the update of a local FL model can be seen as the data importance of that device's dataset. The data importance of each device depends on the number of training data samples and the data distribution. For instance, if a device has a large number of training data samples, its local model will be allocated a large weight during local model updating. Since only a subset of devices can perform FL at each iteration, it is necessary to design data-importance-aware device scheduling policies for faster convergence. In particular, one must first build a data importance model that jointly considers the number of training data samples, data distribution, and data uniqueness. In CFL, devices cannot share data; hence, each device may not be able to directly know the data importance of other devices. Therefore, there is a need to find a method to learn the data importance of other devices from their transmitted local FL model parameters. One must determine the frequency of local FL model updating for devices with different data importance. Note that increasing the update frequency of the devices with high data importance can improve convergence speed, but it also increases the loss function value.

Device Scheduling for Multiple FL Tasks
In IoT, a device may perform multiple FL algorithms simultaneously. Therefore, we must design a device scheduling policy that enables devices to efficiently train multiple FL models and transmit them to other devices simultaneously. Since each task has its specific convergence time requirement and target loss function value, the developed device scheduling policy must determine which FL model must be trained first and which FL model must be transmitted first so as to satisfy the requirements of each FL task. Since the convergence time of each FL task is different, the designed scheduling policy must be adapted to changes in the number of incomplete FL tasks.

Coding
During the CFL training process, source coding, channel coding, and gradient coding can be used to improve the FL performance. Source coding can compress the high-dimensional FL model parameters so that they can be represented by a small number of bits, hence reducing the FL parameter transmission delay [15]. Channel coding can protect the transmitted FL model parameters against noise and interference, thus reducing packet errors and improving CFL reliability. Gradient coding is used to encode the GD parameters of ML algorithms to improve the ML performance.

Obviously, source, channel, and gradient coding can significantly improve CFL performance. However, a number of research questions still exist.

Heterogeneous Source Coding Design
In IoT, the wireless link characteristics-of each device are different (e.g., different rates). To efficiently use wireless resources for FL model transmission, each device may encode its local FL model using different numbers of bits or coding techniques. This type of coding scheme is called heterogeneous source coding. For example, some devices can use 15 bits to represent their local FL models, while another can use 7 bits. Heterogeneous source coding can significantly reduce the coding energy consumption and decrease the loss function value. However, in CFL, a device must transmit its local FL model to multiple devices. Therefore, one must determine the number of local FL models that each device must encode and the number of bits used for encoding the corresponding local FL models.

Gradient Coding for Avoiding Stragglers
Due to limited wireless resources, an IoT system can have devices with extremely high transmission delays or computational delays. Such devices (called stragglers) may not be able to complete the local FL model transmission within the time duration required by the system. If a network has a large number of stragglers, the number of devices that can perform CFL will significantly decrease. Therefore, there is a need to design gradient coding schemes for addressing the problem of stragglers. However, traditional gradient coding methods require devices to share their datasets with other devices so as to remove stragglers; thus, they cannot be used for CFL since CFL does not allow devices to share their data. Hence, we need new gradient coding schemes without data sharing.

Conclusion
This article has proposed a novel wireless CFL framework and introduced the challenges and opportunities of using wireless communication techniques for optimizing CFL performance. The introduced techniques provide guidance for reliably deploying CFL across IoT devices. The discussed research opportunities identify important open problems that must be considered when designing and deploying CFL for IoT systems. We expect that the proposed CFL framework will fundamentally change the original FL architecture, allowing it to be deployed for several future applications such as mobile keyboard prediction, device identification and monitoring, and extreme event detection for autonomous vehicles.