Abstract
Knowledge about the expected interaction duration and expected distance from which users will interact with public displays can be useful in many ways. For example, knowing upfront that a certain setup will lead to shorter interactions can nudge space owners to alter the setup. If a system can predict that incoming users will interact at a long distance for a short amount of time, it can accordingly show shorter versions of content (e.g., videos/advertisements) and employ at-a-distance interaction modalities (e.g., mid-air gestures). In this work, we propose a method to build models for predicting users’ interaction duration and distance in public display environments, focusing on mid-air gestural interactive displays. First, we report our findings from a field study showing that multiple variables, such as audience size and behaviour, significantly influence interaction duration and distance. We then train predictor models using contextual data, based on the same variables. By applying our method to a mid-air gestural interactive public display deployment, we build a model that predicts interaction duration with an average error of about 8 s, and interaction distance with an average error of about 35 cm. We discuss how researchers and practitioners can use our work to build their own predictor models, and how they can use them to optimise their deployment.

Previous
Next 
Keywords
Pervasive displays

Users behaviour

Audience behaviour

1. Introduction
Interactive public displays can today be found in airports, universities, shopping malls and more. Although it is generally known that users interact with public displays for very short amounts of time, interaction durations vary widely (Davies et al., 2014; Müller et al., 2012; Parker et al., 2018). The same is true for interaction distances, particularly when using interaction techniques such as mid-air gestures (Müller et al., 2012; Nancel et al., 2015). Knowing the expected interaction duration and distance upfront can bring in a plethora of benefits to the different stakeholders of a public display. For instance, if a system is aware that the current situation will result in the user interacting at a certain distance, it can dynamically determine which interaction modality to employ (Dingler et al., 2015b). Furthermore, depending on the expected interaction duration, the system could, for example, dynamically choose which version of an advertisement video to show according to the video length. In addition to run-time benefits, knowing which factors influence interaction duration and distance can be of great value to stakeholders who would then be able to tweak their setups to achieve the optimal user experience.

To address such issues, designers developed methods that try to keep users interacting longer by, for example, showing autopoiesic content (Memarovic et al., 2011), or presenting new content immediately after the user has finished interacting with current content (Alt et al., 2016). Moreover, previous works reported that users interact at different distances, particularly when using at-a-distance interaction techniques such as mid-air gestures (Müller et al., 2012; Nancel et al., 2015). The surrounding environment also affects how users position themselves: for instance, benches surrounding the display potentially result in larger audiences, which in turn discourage users from positioning themselves close to the display (Gentile et al., 2017a). These facts have led researchers to study how to guide users into positioning themselves in the optimal sweet spot (Alt et al., 2015; Zhang et al., 2014).

In contrast to display-centred approaches discussed in previous work, in this paper we focus on the people near the display, and in particular on estimating how the audience could affect possible interactions with the display itself. In more detail, here we present a method for building models to predict the duration and distance of interactions based on the behaviour of users (i.e., people who actually interact with the display) and audience (i.e., people who sit or stay around the display), and on the relationship between them.

To this end, we first analysed the behaviour of users and audience in a real-world deployment of a public display. We chose a deployment in which mid-air gestures were employed for interaction because this modality often results in varying interaction durations (Ackad et al., 2016) and interaction distances (Müller et al., 2012). Data analysis revealed that the users’ interaction duration and distance from the display are significantly influenced by the number of users, the size of the audience, the user-audience relationship, and the audience’s gaze towards the user(s). We then used the collected data to train predictor models based on our method, able to estimate the probability density functions (PDFs) for interaction duration and distance (see Fig. 1), using audience-related information. Our solution estimates the interaction duration with a mean absolute error (MAE) of about 8 seconds, and the interaction distance with a MAE of about 35 cm. We describe our approach in details and discuss how the outcomes of our study, which include the predictor model and a visualisation tool, can be reused by researchers and designers to build predictor models for their public display setups.

Fig. 1
Download : Download high-res image (585KB)
Download : Download full-size image
Fig. 1. In this work we study the influence of the passive audience’s size and behaviour on interactions with a public display, and accordingly build models to predict the distance from which users will interact, as well as the interaction duration. The figure shows a sample of our prediction model: users position themselves close to the display in the absence of an audience (A), a bit farther away in case of an audience of size two (B), and even further in case of an audience of size four (C). The graphs represent the predicted probability density functions of the interaction duration in seconds with the respective audience sizes.

We discuss how this work can help space owners and designers to optimise user engagement and experience, e.g., by (a) building similar models and (b) using them to predict how settings will influence users to (c) ultimately optimise the setup. We also explain how to leverage this knowledge to adapt content and interaction modalities in real time.

The contribution of this work is threefold:

•
we report our findings from a 35-day long observation of our deployment to identify factors that influence users’ behaviour;

•
we propose and evaluate a machine learning approach based on expectation maximisation, aimed at generating predictor models using the collected data;

•
we describe how similar models can be developed for predicting user behaviour on public displays.

To help visualise the predictions, we implemented a visualisation tool that can be adapted to other similar deployments. The tool is freely available and open source1

2. Related work
Our work builds on previous work on the analysis, modelling, and prediction of the behaviour of users of interactive public displays. In particular, we focused on display applications that use mid-air gestures in order to provide interactivity to the users.

In this section, we provide an overview of the most significant previous works on these main topics.

2.1. Interacting with public displays
Today’s public displays feature interactivity in many forms. Some displays offer implicit interaction, such as reacting to the user’s natural behaviour when they approach the display. For example, many previous deployments aimed at making the display noticeable by showing silhouettes that mimic users’ movements (Khamis et al., 2018a; Müller et al., 2012).

Apart from implicit interaction, a large body of work investigated explicit input through modalities beyond keys and buttons, such as touch (Peltonen et al., 2008), mid-air gestures (Gentile et al., 2017b), feet-input (Steinberger et al., 2014), mobile devices (Ng et al., 2017), eye gaze (Khamis et al., 2017b), or multi-modal combinations of them (Mäkelä et al., 2018).

2.1.1. Touch-based interaction
Touch interfaces were a significant improvement over physical hardware (e.g., keypads, buttons and joysticks) since they expand the entropy of interaction possibilities, and allow faster software-based updates of the user interface (Davies et al., 2014). A downside of touch interfaces is that they have to be physically reachable, while public displays are often mounted above user’s height for visibility, or placed behind shop windows. As a result, distances from which users interact with displays via touch do not change significantly.

2.1.2. Mobile device interaction
Mobile devices, such as smartphones or smartwatches, can be used to interact with displays. Researchers explored their use for gesture-based interaction with remote displays (Dingler et al., 2015a; Reyes et al., 2018). Some displays allow for interaction using mobile devices via Bluetooth (Mäkelä et al., 2017a), HTTP (Khamis et al., 2017a), or NFC (Broll et al., 2011). Interaction distances could vary depending on the used technology. For example, NFC requires close proximity to the display, while Bluetooth’s range can span several meters around the display. HTTP allows for interaction from anywhere as long as the user is connected to the Internet.

2.1.3. Touchless interaction
Interaction is said to be touchless if does not require mechanical contact between the human and any part of the artificial system (de la Barré et al., 2009). According to this definition, both eye gaze and mid-air gestures are touchless interaction modalities.

Eye gaze has been gaining attention as an alternative modality for interaction with public displays. Gaze has been considered a form of hygienic interaction since it can be used at-a-distance. Although gaze is fast (Sibert and Jacob, 2000), designing highly accurate gaze interfaces often results in significantly longer input time (Khamis et al., 2016), thereby increasing interaction durations. Interaction distances are governed by the range of the cameras used for tracking the eyes. With the exception of active eye-tracking systems (Khamis et al., 2017b), most commercial eye trackers require users to position themselves 60cm-90cm away from the display (Khamis et al., 2018b).

The use of mid-air gestures provides an alternative at-a-distance interaction modality. Compared to other modalities, mid-air gestures were shown to extend interaction durations due to their playful nature (Ackad et al., 2016). Opposed to gaze, mid-air gestures often require larger distances between the user and the display to allow the sensors to capture the user’s movement (e.g., Microsoft Kinect requires users to be between 0.5 and 2.5 m (Microsoft 2014)).

2.2. Mid-air gesture interaction with public displays
The variance in durations and distances of interactions is not the only reason why we focused on mid-air gestures. Indeed, especially in the context of public displays, this interaction modality has been widely adopted and used due to many advantages. Among them, touchless gestural interaction limits vandalism by placing displays in unreachable places (Sorce et al., 2017), maintains hygiene as users no longer need to touch the display (Gerba et al., 2016), and removes constraints to the display size – see, for instance, works on media façades (Dalsgaard and Halskov, 2010; Fatah gen Schieck et al., 2013). Several authors focused their work on user representation in mid-air gestural applications. Many prior work suggested to adopt users’ silhouettes or avatars (Bottoni et al., 2008; Gentile et al., 2016; Müller et al., 2012; Walter et al., 2014), since they have proven to be very effective in solving some common pervasive display issues, namely interaction blindness – i.e. the inability of the users to recognise the interactive capabilities of a display (Ojala et al., 2012) – and affordance blindness – i.e., the inability to understand the interaction modality of the display (Coenen et al., 2017). Gentile et al., showed also that the presence of an avatar makes two-handed interactions more “natural” in the sense that it contributes to a reduction of the cognitive workload while interacting with public displays (Gentile et al., 2017b).

On the other hand, mid-air gestures have also some drawbacks. If not properly designed, such interaction modality might indeed require teaching passersby how to perform gestures (Ackad et al., 2015; Walter et al., 2013). In terms of ergonomics, using gestures for long periods of time might lead to arm fatigue, an issue often referred as the Gorilla-arm problem (Hincapié-Ramos et al., 2014). To solve these issues, it is crucial to incorporate users’ preference when defining the gesture sets (Aigner et al., 2012). In this sense, gesture elicitation studies might help, along with specific measures of arm fatigue, used to filter out potentially bad gestures (Morris et al., 2010; Ruiz and Vogel, 2015). Social acceptability of gestural interfaces has also been investigated by prior work. In particular, Ahlström et al. showed that users are sensitive and selective regarding where and in front of whom they would feel comfortable using mid-air gestures (Ahlström et al., 2014). They also showed that acceptance and comfort are strongly linked to gesture characteristics, such as gesture size, duration and in-air position. Finally, an additional limitation of mid-air gestures is in the technologies that enable for mid-air gesture recognition, which are usually not suitable for being used in outdoor deployments. This is especially the case in very sunny environments as most depth sensors rely on infrared reflections that are impacted by sunlight (Mäkelä et al., 2017b; Microsoft, 2014).

Based on the above considerations, we decided to focus on a gestural interface based on the use of an avatar, continuously replaying user’s movements. Although we are aware that this choice might lead to fewer interactions compared to touch-based displays – e.g. due to social embarrassment of performing gestures in public (Mubin et al., 2009) – the use of gestural interfaces has many advantages. As further explained in (Section 3.1), this solution allows us to reduce interaction blindness (Müller et al., 2012; Walter et al., 2014), makes interactions more natural (Gentile et al., 2017b) and does not require learning specific gestures in order to interact properly (Gentile, 2017; Gentile et al., 2016) – thus supporting immediate usability. Moreover, technical limitations, such as sunlight sensitivity, do not apply in indoor deployments, like the one we took into account. Furthermore, given the greater variance in durations and distances of interaction via mid-air gestures compared to other modalities, prediction of interaction distance and duration of public displays that employ this modality is particularly valuable.

2.3. Behaviour of public display users
Previous works studied how several aspects impact the behaviour of active public display users, passive audience, and passersby.

For example, researchers looked into how the setup influences users’ behaviour. In Looking Glass, users were reported to be influenced by traffic lights; passersby were observed waiting expectantly at pedestrian crossings, and then interacting with the display as soon as they are on the other side (Müller et al., 2012). In other words, the waiting situations make passersby notice the display and hence increase interactions. Similarly, ten Koppel et al., found that the configuration of multiple screens influences how users position themselves to interact with a display (Ten Koppel et al., 2012). They found that arranging the displays in a hexagonal configuration encourages users to interact with neighbouring displays, while in flat configurations users maximise the distance to other users. The authors attributed this to the users’ desire to maintain their personal space. As a result, configurations with a large interaction space had more simultaneous users. Fatah gen Schieck and colleagues found that the layout of the building influence the behaviour of spectators, passersby, and audience and suggested that spatial configuration can be manipulated to encourage certain behaviour (Behrens et al., 2013; Fatah gen Schieck, Al-Sayed, Kostopoulou, Behrens, Motta, 2013, Fatah gen Schieck, Biones, Mottram, 2008). Fatah gen Schieck et al., used displays as a socialising platform that triggers shared encounters among passersby, which in turn encouraged interactions (Fatah gen Schieck et al., 2008). Dalton et al., found that the architecture of the building in which a display is deployed has a strong influence on whether or not users notice displays (Dalton et al., 2015). For example, users followed ceilings and wall edges with their gaze before looking at a display. Gentile et al., found that the presence of seats around the display attracts a larger audience, which causes users to interact at a further distance from the display (Gentile et al., 2017a). This was attributed to discomfort resulting from having a large audience gazing at users as they interact. CityWall (Peltonen et al., 2008) and MyPosition (Valkanova et al., 2014) showed that displays can encourage communication among strangers.

Social aspects were also shown to influence interaction in public. Research showed that people might interact only to be noticed by others (Dalsgaard and Hansen, 2008), or resist interaction due to social embarrassment (Dalsgaard et al., 2016). Multiple works observed and investigated the honeypot effect (Beyer et al., 2014; Brignull and Rogers, 2003; Khamis et al., 2015; Marshall et al., 2011; Müller et al., 2012; Wouters et al., 2016), which refers to a social affordance in which the presence of users interacting with a display encourages surrounding passersby to come forward and interact with the display as well. Brignull and Rogers described the honeypot effect as a “social buzz” in which those interacting with a public display seemed to signal to others that they are willing to engage in discussions and meet new people (Brignull and Rogers, 2003). Brignull and Rogers also discussed how attempts to reduce social embarrassment when interacting with a display, such as allowing users to interact remotely with the displays, might be counterproductive as they could reduce the honeypot effect which could in turn result in less interactions with the display. Observations from field deployments indicate the honeypot effect is a powerful cue to attract attention to displays; those who observed others interacting with a display often return to interact with it themselves, sometimes even days afterwards (Memarovic et al., 2016; Müller et al., 2012). It is expected that the honeypot effect evokes curiosity thus encouraging users to interact, and counteracts social embarrassment by suggesting engagement (Wouters et al., 2016). Indeed, in Dalton et al.’s eye tracking study (Dalton et al., 2015), they found that 12% of fixations at displays were preceded by noticing someone interacting with the said display. In summary, as users interact, others are attracted to the display.

In these cases, the user influences the audience. In contrast, Gentile et al., studied how the audience influences the user (Gentile et al., 2017a). They found that the larger the audience, and the more focused they are on interacting users, the farther users position themselves from the display. While the work of Gentile et al., simply shows an effect of the audience on user’s behaviour, in this work we significantly build over that by leveraging knowledge about the audience behaviour to predict the user’s interaction duration and interaction distance.

2.4. Modelling and predicting public display users’ behaviour
Several works modelled users’ behaviour. For example, multiple spatial (Streitz et al., 2003; Vogel and Balakrishnan, 2004) and temporal models (Brignull and Rogers, 2003; Michelis and Müller, 2011; Müller et al., 2010) were proposed for public displays. While these works contributed to discovering the existence of interaction zones, our work builds significantly over that by identifying factors that impact the transition between these zones and leveraging these factors to predict user’s interaction duration and distance.

Unlike our predictive model, existing models are static and mostly qualitative, and they cannot easily be adapted to, for example, the interaction modality, application, or display context. Moreover, our model aims to provide numerically quantifiable predictions, which would increase its practical applicability in many cases.

Prediction of users’ behaviour has been researched extensively in HCI and psychology. For example, previous work predicted user’s mouse (Pasqual and Wobbrock, 2014) and touch interactions (Buschek, De Luca, Alt, 2015, Buschek, Schoenleben, Oulasvirta, 2014; Hinckley et al., 2016) to improve accuracy. Several works proposed predicting the user’s gaze based on user’s behaviour (Frintrop et al., 2010; Huang et al., 2012). Erazo and Pino proposed a model for predicting execution time of mid-air gestures (Erazo and Pino, 2015). Other works predicted the applications that will be used next (Shin et al., 2012; Xu et al., 2013). In the context of public displays, Huber et al., analysed passerby’s feet position to detect the user’s intention and accordingly adapt the rendered content (Huber et al., 2015).

In contrast to these previous works, our approach is the first to provide quantitative predictions of the behaviour of public display users in terms of interaction duration and distance, based on audience size and behaviour.

3. Audience's impact on interaction
To study the impact of audience presence and behaviour on interaction in our deployment, we conducted a longitudinal study in order to observe users’ behaviour while interacting with a public display. In this section, we provide a brief description of the deployment we analysed, the data collection process and the outcome from our data analysis.

3.1. Description of the deployment
The public display we studied is located in a 150 square-meters-large indoor space inside a building within the campus of the University of Palermo. It consists of a 32-inches LCD monitor placed at eye-level, with a Microsoft Kinect sensor placed below it. The display is situated next to several benches where students often sit while waiting for lectures to start.

The display acts as an information provision system, which is one of the most common applications for public displays (Clinch et al., 2016; Valkanova et al., 2015). In particular, it provides university-related information for students of different disciplines and ages (mostly between 19 and 35 years), lecturers, and other university staff members. The display runs an Avatar-Based Touchless Gestural Interface (named ABaToGI, Gentile, 2017), based on the prototype described in Gentile et al. (2016). It consists of an animated avatar shown in the middle of the screen, with other interactive tiles arranged around it (see Fig. 2).

Fig. 2
Download : Download high-res image (240KB)
Download : Download full-size image
Fig. 2. The display allows users to access different types of information via mid-air gestures reflected by an on-screen user representation.

To make the application interactive, the aforementioned avatar is shown every time a user approaches the display, and remains visible in the middle of the screen, continuously reflecting user’s movements. This was done to reduce interaction blindness and affordance blindness (Coenen et al., 2017; Müller et al., 2012; Walter et al., 2014), and to make interactions more natural (Gentile et al., 2017b). Users can interact using in-air direct manipulations by placing a hand on a tile to trigger a selection event. By not requiring symbolic gestures (e.g., a gesture to convey a certain meaning), this system alleviates the need to learn which gestures to use in order to interact, greatly reducing the learning curve and allowing immediate usability (Gentile et al., 2016).

3.2. Data collection
We needed to collect as much information as possible about the area of interest for our study, such as the behaviour of people within it and, in particular, people’s interest toward the display. For this reason, we needed to use an RGB camera to allow us to inspect the setup and the behaviour of the passersby. Since we were not allowed to install any new RGB cameras in the test environment, we had to use one of the available WiFi surveillance cameras. The WiFi camera we used was placed in front of the display, in an unreachable position, to observe users’ interactions and audience behaviour. This way we were able to remotely observe: (1) users, (2) audience (e.g., people sitting on the benches next to the display), and (3) the display status. The University’s institutional review board allowed us to access the camera’s streaming video for the time the experiment has been carried out. It is worth noting that even though we had a Kinect device installed for gesture detection, we could not use it for data collection because its range is not wide enough and it does not allow for gathering contextual data (e.g. audience gaze, and whether the user interacted due to the honeypot effect).

Fig. 3 shows an example of the camera view, and a subset of the variables and measurements we collected. We recorded interactions between 10 am and 5 pm on workdays.

Fig. 3
Download : Download high-res image (813KB)
Download : Download full-size image
Fig. 3. The wide view of the camera enabled us to identify users who arrived in groups, or communicated with each other or with the audience before interacting with the display. This allowed us to identify relationships. The figure shows a sample of the variables and measurements that were logged in the recordings.

Since we started collecting videos after one year from the first installation of the display, we can reasonably assume the absence of significant biases due to novelty effect.

For this work, we analysed 200 hours and 47 minutes of recorded video feed collected along 35 days. During this period of time, we observed 123 total interaction events. For our study, an event was relevant when there is at least an interaction attempt with the display by one or more people. Users interacted for 1 to 136 seconds (  see Fig. 4b). It is worth noting that the average duration of interaction events is in line with data reported in other public display deployments (Müller et al., 2012). Same considerations apply for the average number of interaction events per hour (0.615), which is in line with data reported for other indoor public display deployments (Parker et al., 2018).

Fig. 4
Download : Download high-res image (165KB)
Download : Download full-size image
Fig. 4. Histograms show the distributions of (a) interaction distance and (b) interaction duration.

There are infinitely many factors that might impact users’ behaviour around displays. We decided to focus more on the human factors than on technical ones, in particular on how some audience-related factors affects the behaviour of display’s users-to-be. Furthermore, some of the audience-related variables, such as size, distance, side, and gaze, can be easily influenced by the deployment layout (presence/absence of benches, number and position of seats, etc.). Therefore, space owners might have more control on such changes, for instance in terms of cost, to achieve the desired values for duration and distance of interactions. Indeed it is likely that a space owner could easily add, remove or move some furniture around. We acknowledge that factors such as time (e.g., weekday and time of the day), weather, screen size as well as the user’s gender, culture, age and body orientation might impact the users’ behaviour (Ballendat et al., 2010; Müller et al., 2012; Zielinska-Dabkowska and Fatah gen Schieck, 2018). Indeed we provided an overview of the most significant factors that impact the users’ behaviour around displays in Section 2.3. Here, we decided to take into account the variables on which space owners may have an easier, and cheaper, control, and leave the others for future work (see Section 5.6). Thus, for each interaction event, we collected 11 variables, summarised in Table 1. Two researchers separately reviewed all the recorded videos, based on the following rules:

•
Number of Users (Ucount) was tracked because it influences interaction due to the honeypot effect, which we discussed at length in Section 2.3. Ucount was coded by counting the number of users simultaneously involved in each interaction event.

•
Audience Size (Asize) was tracked as it was shown to influence the behaviour of users of public displays (Gentile et al., 2017a). Asize was coded by counting the number of persons that, during an interaction event, were not involved in the interaction

•
Audience-Display Distance (ADD) is likely to impact the users’ awareness that they are being observed, which could in turn cause social embarrassment (Dalsgaard et al., 2016) or, for some, even encourage interaction (Dalsgaard and Hansen, 2008). We coded ADD by counting the number of tiles on the floor from the display to the closest person in the audience, or the number of seats in case the closest person in the audience was seated. In cases where the audience were considerably moving during the interaction event (i.e., the distance between audience and the display varied, and it was not possible to identify a single constant value for ADD during the whole interaction event), we coded this value as N/A.

•
Audience Side (Aside) refers to which side the audience was at, and was coded by checking whether the audience was on the left, on the right or on both sides with respect to the user(s). In those cases when the person(s) in the audience were not clearly placed on the left or on the right (i.e., they were moving around the joining line between the user and the display), we coded this value as N/A

•
Audience Gaze (Agaze) was suggested to influence interaction distance (Gentile et al., 2017a). Agaze was coded by looking whether at least one person from the audience looked at the user(s) during the interaction event. In those cases where this determination was dubious or unclear, and when such determination was useless (i.e., when 
) we coded this value as N/A.

•
User(s) Gaze (Ugaze) can be an indicator of whether the user noticed the audience. This can be valuable in further distinguishing cases where the user was aware of the surrounding audience. Ugaze was coded by looking whether the user (or one of the users, in case of groups of users) looked back to a person from the audience who were looking at her/him during the interaction event. In other words, this variable represents if the user(s) noticed to be looked by someone in the audience. In those cases were this determination was dubious or unclear, we coded this value as N/A.

•
User(s)-Audience Relationship (UAR) can be influential as previous work showed that users are sometimes more comfortable interacting in front of friends (Khamis et al., 2015). UAR was coded by looking at interactions between user(s) and persons in the audience before and after the interaction events. For instance, if a user and someone in the audience talked before or after the interactions, or if they arrived together next to the display, we assumed they were acquainted. Otherwise, if a user arrived alone at the display, and went away with no other social interactions with the audience, we assumed they were strangers. In those cases were this determination was dubious or unclear, we coded this value as N/A.

•
User(s) was/were in the Audience (
): if the user was among the audience earlier, this can be considered a result of the honeypot effect as it was shown that users return, sometimes even days, after observing someone interacting with the display (Müller et al., 2012). 
 was coded by checking if the user(s) was/were part of the audience during a previous interaction event.

•
User(s) Interacted due to Honeypot Effect (HE): tracking this is useful in determining whether the user was motivated by the honeypot effect or not which is known to influence user behaviour (see Section 2.3). HE was coded by checking if an honeypot effect occurred (i.e., at least one user from the audience approached the user during an interaction event, and then started a new interaction event).

•
Interaction Distance (d): this is one of the dependent variables that we wanted to investigate. Knowing which factors impact the interaction distance can be very valuable for space owners. For example, a display can dynamically determine which interaction modality to employ, or the size of the content depending on the anticipated interaction distance. d was coded by counting the number of tiles on the floor between the user(s) and the display.

•
Interaction Duration (t): this is another dependent variables that we investigated. It is also very valuable to understand and be able to manipulate the factors that impact interaction duration. For example, if shorter interaction durations are expected, the display can play shorter versions of advertisements, or decide to employ more aggressive approaches for keeping the user. t was coded by measuring the time between an action that showed user(s)’ intention of initiating the interaction (e.g. stopping in front of the display and raising arm or moving body), until user(s) left the display.


Table 1. Summary of variables that we tracked while annotating the videos. We used these variables both for the statistical analysis and for building our predictor model.

Variable name	Symbol	Type	Values	Description
Input	Number of Users	Ucount	Discrete	≥  1	Number of interacting users
Audience Size	Asize	Discrete	≥  0	Number of persons acting as passive audience while one or more users interact
Audience-Display Distance	ADD	Continuous	≥  0	Distance between the display and the closest person in the audience
Audience Side	Aside	Categorical	left, right, both	Which side is the audience, with respect to the display
Audience Gaze	Agaze	Categorical	yes, no	Whether or not someone in the audience looked at the user during interaction
User(s) Gaze	Ugaze	Categorical	yes, no	Whether or not a user noticed to be gazed from the audience
User(s)-Audience Relationship	UAR	Categorical	acquainted, strangers, mixed	Relationship between user and the audience
User(s) was/were in the Audience	
Categorical	yes, no	Whether or not the user(s) acted as part of the audience before interacting
User(s) interacted due to Honeypot Effect	HE	Categorical	yes, no	Whether or not the user(s) interacted due to the honeypot effect
Output	Interaction Distance	d	Continuous	≥  0	Distance between the user(s) and the display
Interaction Duration	t	Continuous	≥  0	Duration of the interaction session
All distances coded by counting the number of tiles or seats were later converted to centimetres. In a few cases, when variables changed only slightly and shortly during an interaction event (e.g., changes in audience size amid interaction), we coded the most representative value for the specific interaction analysed (e.g., the audience size during the major part of the interaction session).

Both the researchers who were involved in the video reviewing and coding process did record the same number of interaction events. We also computed three different agreement scores, based on the variable type reported in Table 1. In particular, based on the recommendations by Ranganathan et al. (2017), we used Cohen’s kappa for categorical variables, weighted Cohen’s kappa for discrete variables, and intra-class correlation coefficient for continuous variables. Agreement scores are reported in Table 2. In those cases where the two researchers disagreed, both the researchers reviewed again the videos together. Then, we resolved the disagreements as follows:

•
for continuous variables (i.e., ADD, d, t), if an agreement was not reached, we computed the average value between the values coded by the two researchers

•
for categorical and discrete variables, if an agreement was not reached, the values were set to N/A (i.e., treated as dubious/unclear cases)


Table 2. Agreement scores: intra-class correlation coefficient (ICC) has been used for continuous variables (ADD, d, t), while Cohen’s kappa has been computed for all the other variables (weighted in case of discrete variables Ucount and Asize).

Ucount	Asize	ADD	Aside	Agaze	Ugaze	UAR	
HE	d	t
ICC or x †	0.985	0.977	0.979	0.994	0.991	0.975	0.978	0.981	1.000	0.960	0.994
†
p < .001 for all the above results

3.3. Limitations
The results of our statistical analysis are valid for the group of users that we considered in our longitudinal study. Results could be different in other deployments based on the setup, user groups, and interaction modality. However, the procedure described for data collection, statistical analysis and predictor training can be replicated in other deployments with different settings. Another limitation is that although we carefully reviewed the situation before and after each considered case to note any external influences on users’ behaviour (e.g., indications that the audience and users know each other), however we are not fully aware of what happens beyond the camera’s range. Finally, we do not know how often each user interacted with the display before each recorded interaction attempt. Consequently, we have not included information about previous experiences in interacting with the display. While an alternative would be to collect and recognise faces to identify users and collect such information, this was not feasible due to privacy concerns.

3.4. Statistical analysis
From the 123 collected interactions, we excluded exceptional cases that occurred very few times to be significant for drawing general conclusions. Namely, we excluded five interactions where there was an audience of six or more people, and three cases where the audience was more than 2.5 m away from the display. This means we ended up with 115 valid interactions to analyse.

Out of the 115 considered interactions, 43 occurred in the presence of an audience that the users were acquainted to, 17 cases were logged where the audience and the users were strangers, 23 cases had a mixture of strangers and acquaintances among the audience, and in 32 cases there was no audience at all. A one sample binomial test confirmed that there is a statistically significant tendency for more interactions when there are acquaintances among the audience or when there is no audience, with a proportion of 0.85 (larger than expected 0.75, p < 0.001).

3.4.1. Interaction duration
We found an effect of the number of users on interaction duration. An ANOVA test revealed a significant effect of the number of users on interaction duration (
 p < 0.001). A post-hoc Tukey test showed that interaction duration in the case of a single user ( ) is significantly shorter than in the case of two users (  p < 0.05) and three users (  p < 0.005). This means when multiple users interact, they are likely to interact for longer durations.

The size of the audience also had an influence on interaction duration. An ANOVA test showed that there is a significant effect of the audience size on interaction duration (
 p < 0.05). Post-hoc Tukey tests revealed significant differences between interaction duration when there is an audience of size 4 or more ( ) compared to cases where there is no audience (  p < 0.001) and cases where there is a smaller audience of size 1 to 3 (  p < 0.05). This means that the bigger the audience, the shorter the duration users interact.

We also found an effect of the user-audience relationship on the interaction duration. An ANOVA test showed a significant effect of the relationship on interaction duration (
 p < 0.05). A post-hoc Tukey test showed that interaction duration in the presence of at least one stranger among the audience ( ) is significantly shorter than in the case of an audience composed of acquainted people (  p < 0.05), and also significantly shorter than the case of no audience (  p < 0.005). There is no significant difference in the interaction duration between the cases of no audience vs presence of acquaintances. In other words, this means that interaction durations in front of strangers are 1) shorter than in front of acquaintances, and 2) shorter than in the absence of an audience.

3.4.2. Interaction distance
Our findings with respect to interaction distance are illustrated in Fig. 6. An independent-samples t-test was run to determine if there were differences in user-display distance between presence of an audience compared to the absence of an audience. A significant main effect was found (). Users position themselves significantly farther away from the display when an audience is present ( ), compared to cases where there is no audience ( ). When further analysing the cases where an audience is present, an ANOVA showed that the audience size has a significant effect on the interaction distance (
 p < 0.001). Post-hoc Tukey tests revealed that users position themselves significantly farther away from the display in the presence of an audience of size four or more ( ) compared to cases where there is no audience (  p < 0.001) and compared to the presence of an audience of size between one and three (  p < 0.05). This means that users position themselves significantly farther away from the display in the presence of an audience, and in particular in the case of a large audience.

Finally, an ANOVA revealed that the audience’s gaze at the user has a significant main effect on user-display distance (
 p < 0.001). Post-hoc Tukey tests showed that users position themselves significantly farther away from the display when the audience gazes at them ( ) compared to when they do not gaze at the user (  p < 0.05), and compared to when there is no audience (  p < 0.005). This means that when the audience gazes at the user, the user keeps a larger distance to the display.

4. Predicting interaction duration and distance
In the previous section, we described the process of observing users’ behaviour and collecting data related to a set of surrounding information, e.g., the audience’s presence and behaviour, in order to understand the relationship between such audience-related data and the users’ behaviour while interacting with a display. Using this information, we concluded that some of the observed variables have a more significant influence than others.

In this section, our goal is to understand how to use such data in order to automatically predict the impact of audience and users’ behaviour with respect to the deployment setup. To this end, here we present a machine learning approach for supporting display owners in predicting users’ behaviour in a specific deployment. Such predictions can then be used for increasing the use of the display, improving the experience, and making public display deployments more successful.

4.1. Predictor model: Definition
The predictor model proposed in this section estimates the probability density function (PDF) of a continuous output variable y, using a set X of N input variables xi, i.e., 
. In particular, we estimate the PDF of an output variable given the observed variables (namely p(y|X)), which implies the estimation of the joint probability p(y, X).

In the practice, a perfect joint probability estimation generally requires a number of samples that grows exponentially with the number of input variables. For instance, suppose that our predictor model would require nine input variables, each of which can assume only two values. Thus, a proper joint probability estimation would require at least 29 training samples, which is often not practical in actual deployments. This is particularly true when considering video-coded data, which is very often the case of observations in-the-wild and longitudinal studies in pervasive display research (Davies et al., 2014).

To overcome this limitation, we leverage a standard technique that implies the assumption of a Naive Bayesian dependence between output and input variables. This is a machine-learning stratagem that was proven effective and eases the estimation of complex joint probabilities, even with relatively few data samples, unlike other approaches – e.g., neural networks, SVMs, etc... (Bishop, 2006). In particular, it allows considering the input variables conditionally independent given the output.

Thus we can write down the conditional as follows:(1)
 
 
 
 

We can group the constant parts p(xi) and p(X), which form a constant scaling factor K, defined as follows:(2)
 

Thus, it holds that Eq. 1 can be rewritten as follows:(3)
 

In practice, p(y|xi) and p(y) can be estimated from the observed data (i.e., training samples) as Gaussian mixtures, using the expectation-maximisation (EM) algorithm (Bishop, 2006).

The calculus of K can be avoided because it is equivalent to a simple normalisation operation of p(y|X).

Thus, the p(y|X) estimation all comes down to estimating p(y|xi) for each i-th input variable, and p(y). As we explain in Section 4.2, such estimations represent the whole training process of our model, which can be implemented by means of the available (free and open source) implementations of the EM algorithm (Bishop, 2006; Pedregosa et al., 2011).

Estimating p(y|X) would then allow to predict the most probable value of y, according to the input variables xi.

4.1.1. Choice of prediction technique
This approach has several significant advantages compared to other machine learning approaches, such as regression analysis. Although the higher the number of samples are, the more accurate the estimation can be, unlike neural networks and SVMs this model can be used with fewer samples. Moreover, it features incremental training which allows improving precision with newly collected data, without the need to retrain the whole model.

Furthermore, unlike classical regression, this approach allows to train and use the model even if some of the input variables are not available. For instance, an automatic data collection process might provide uncertain values in some cases (e.g., due to occlusion or low video resolution). Even if some input values are missing, this approach would still be useful for training a predictor model. While in this paper we have manually coded data from videos, we envision that future work will collect the data automatically in the wild, which means that it will be more likely that certain data points will be missing (e.g., due to inaccuracies, obstacles, etc.). We discuss how to automate the data collection process in Section 5.5.

The feature of managing missing values is also useful while using the model as a predictor (i.e., after the training stage): even if some of the input variables are not available, the PDF estimation is still possible. Of course, the more the known inputs are, the more precise the estimation will be.

4.2. Predictor model in practice
As stated before, our aim is to predict how the distance between the users and the display varies according to audience behaviour, as well as how long users interact with the display. In order to put the above methodology into practice, we have thus built two different predictor models: one for predicting the user-display distance (i.e., p(d|X)), and another one for predicting the interaction duration (i.e., p(t|X)). The variables d and t are therefore our outputs.

We have then considered a total of nine possible input variables xi, which correspond to the data collected for the aforementioned data analysis (see Table 1).

Obviously, if there is no audience (i.e., 
) then many variables (namely ADD, Aside, Agaze, Ugaze, UAR) are irrelevant, since they are subsumed by Asize in this particular case. For this reason, we have not built two models (i.e., one for predicting p(d|X), and another one for p(t|X)) but actually four, doubling the original two by splitting the cases for 
 and for Asize > 0. This distinction is depicted in Fig. 7. However, for the sake of simplicity and clarity, in the rest of the paper we will often refer to a single model that predicts both p(d|X) and p(t|X).

Fig. 7
Download : Download high-res image (178KB)
Download : Download full-size image
Fig. 7. Overview of the variables and the predictor models. Because some variables, namely ADD, Aside, Agaze, Ugaze, UAR, would be meaningless if there is no audience, we built a model per interaction duration (t) and interaction distance (d), and we further split each to two models to account for 
 and for Asize > 0.

Training such models basically means that we have to estimate p(d), p(t) and each p(d|xi) and p(t|xi) by applying EM-based Gaussian mixture estimation from the data collected during our observations.

We have implemented this procedure using the Python programming language, exploiting some of the features available from the scikit-learn package (Pedregosa et al., 2011). This allowed us to quickly estimate probability distribution as Gaussian mixtures, and to easily operate on them. Our predictor model was built using the data gathered during the observations described previously. In particular, we used the same dataset analysed during the statistical analysis.

In practice, interactions from distances of 2.5 m or farther from the display are not detected reliably by the Kinect device used in our deployment (Microsoft, 2014). This suggests that interaction experiences from such distances might be skewed. Furthermore, analysing the distribution of the interaction durations revealed that there is a significant drop in number of cases per interaction durations that are longer than 60 seconds (see Fig. 4b). For these reasons, we used the 104 interactions with distances and durations below these criteria to train our model.

4.3. Performance analysis
We evaluated the performance of our methodology by using the Leave-One-Out Cross Validation (LOOCV), over the 104 observed samples. Thus, we measured a mean absolute error (MAE), and its standard deviation σMAE for the two output variables (i.e., interaction distance and duration).

We computed the MAE by considering all the possible combinations of 8 input variables (i.e., all the variables except Asize, which was always included in order to select the right model, as depicted in Fig. 7).

According to our evaluation, the best input variables combination for predicting user-display distance is 
 (MAE = 0.3518, σMAE = 0.3525), while for the interaction duration, the best input variables combination is  < Asize, Ucount, Agaze, Ugaze, UAR, 
 HE >  (MAE = 7.8786, σMAE = 5.5776).

In order to rank all the input variables combinations for minimising the MAE of both the outputs, we computed the following metric for each k-th input variable combination:(4)
 where y can be d or t depending on the considered output. Thus, the error vector representing the k-th input variable combination can be defined as follows:(5)

We adopted the Pareto dominance as an order relation (Deb et al., 2002) for all the error vectors Ek. In general, if E1 and E2 are two error vectors, then the Pareto dominance of E1 with respect to E2 is expressed by the following equation:(6)

Thus, an error vector E* can be considered Pareto optimal if it is not worse than (i.e., dominated by) any other error vector:(7)

Using the Pareto sorting algorithm proposed in Deb et al. (2002), the input variable combination  < Asize, Ucount, Agaze, UAR, 
 results in the best outcome with respect to both interaction distance and duration (MAE(d) = 0.3575, σMAE(d) = 0.3530, MAE(t) = 7.8793, σMAE(t) = 5.9246), since it produced a Pareto optimal error vector.

It is worth noting that, among all the input variables listed in Table 1, HE and 
 require knowledge on the behaviour of user(s) before the interaction. This means that such variables may result tricky to use, and somehow not particularly useful for display providers or space owners, that are the final users of this predictor model. For this reason, we report here also the best combination of input variables excluding HE and 
 which is  < Asize, Ucount, Agaze, UAR >  (MAE(d) = 0.3497, σMAE(d) = 0.3588, MAE(t) = 7.9180, σMAE(t) = 6.0094).

This means that our model can predict interaction duration with a MAE of 7.9180 seconds, and interaction distance with a MAE of 0.3497 m.

4.3.1. Prediction errors and statistical significance
The variables  < Asize, Ucount, Agaze, UAR >  resulted to be the best combination to predict both user-display distance (d) and interaction duration (t). This is in accordance with the findings of the statistical analysis (see Section 3.4), which showed a significant effect of audience size (Asize) and audience’s gaze (Agaze) on user-display distance, as well as a significant impact of audience size (Asize), number of users (Ucount) and relationship between them and the audience (UAR) on the interaction duration. These results, along with the MAEs reported above, confirm the effectiveness of our proposed model in automatically (and correctly) predicting the impact of audience and user’s behaviour with respect to the deployment setup.

4.4. Validation against a different dataset
In order to further confirm the results discussed in the previous sections, we decided to perform an additional validation by using a different dataset. To this end, we collected an additional dataset from the same deployment, after several months from the first data collection period. In particular, we collected data for a total of 20 interaction events, coded over 2 days for 14 hours and 20 minutes. It is worth noting that such number of interaction events represents a sufficient percentage with respect to the size of the dataset, according to the common practices adopted for sizing validation sets (Krogh and Vedelsby, 1994). The data collection procedure followed the same rules explained in Section 3.2.

Using this additional dataset, we computed the MAE by considering a combination of all the input variables, as well as using only the combination  < Asize, Ucount, Agaze, UAR > , as this was the more suitable (based on considerations in Section 4.3. For the combination of all the variables, we computed MAE(d) = 0.3753 m (σMAE(d) = 0.3119) and MAE(t) = 7.700 s (σMAE(t) = 7.4364). Considering instead the combination  < Asize, Ucount, Agaze, UAR > , our results were MAE(d) = 0.3654 m (σMAE(d) = 0.3147) and MAE(t) = 8.0500 s (σMAE(t) = 6.1192). Since the results are in line with the ones reported in Section 4.3 after the LOOCV analysis, we can reasonably confirm the average prediction errors.

5. Discussion and uses of the model
In this section, we discuss the results of the field study and the subsequent statistical analysis, and we describe how display providers and space owners can benefit from the proposed model.

5.1. Performance assessment
In Section 4.3 we computed a mean distance error of 0.3497 m, which is roughly 1.5 times the average length of a human foot (Wunderlich and Cavanagh, 2001). Moreover, based on Hall’s work on proxemics and interpersonal interaction, this error is less than the size of intimate space (Hall, 1992). As discussed in Section 2.4, to the best of our knowledge prior works do not provide quantitative estimations that are comparable to these results, but only qualitative interaction zones (Brignull and Rogers, 2003; Michelis and Müller, 2011; Müller et al., 2010; Streitz et al., 2003; Vogel and Balakrishnan, 2004). Consequently, to our knowledge, our model is the first providing quantitative predictions, and therefore we cannot entirely compare our results to similar models. In future work we may expect that new predictor models will improve the aforementioned prediction error, but based on the above considerations, distance error we obtained here can be considered acceptable.

As for the duration error, our results showed a mean duration error of 7.9180 seconds. Prior work, albeit in a variety of different contexts and scenarios, showed an average interaction duration with public displays of about 26 seconds (Müller et al., 2012) for which, approximately 8 seconds, represents a non-negligible proportion. Nonetheless, such circa 8-sec an error might be useful to qualitatively predict if interactions will be “short” or “long”, with respect to some threshold – e.g. the average duration defined in Müller et al. (2012). It is worth noting that up to our knowledge, no prior works have provided predictor models that are even vaguely able to predict interaction duration.

5.2. Impact of the audience on interaction
In Section 3.4, we found that interaction duration increases as the number of users increases. This is in line with previous work which showed that when users interact, they encourage others to interact too in what is referred to as the honeypot effect (Brignull and Rogers, 2003). On the other hand, we also found that an increase in audience size results in a decrease in interaction duration. This is also the case when people in the audience are strangers; if the users do not know the audience, they interact for a short time compared to when they know them. This is a novel finding that we attribute to social embarrassment resulting from the presence of strangers and a large audience. Social embarrassment is known to influence interaction (Brignull and Rogers, 2003; Perry et al., 2010). Previous works have also reported cases where users felt more comfortable when interacting in front of friends (Khamis et al., 2015), which is inline with our results that show that users interact longer when in front of an acquainted audience.

Another factor that was found to be influenced by the audience size and behaviour is the interaction distance. We found that the presence of an audience results in significantly larger distance between the user and the display. This effect is amplified in case of a large audience; users position themselves significantly farther as the size of the audience increases. Furthermore, the audience’s gaze towards the user has a significant effect on the interaction distance as well. These results can be interpreted as follows: users compensate for the audience’s eye contact, which is a form of “increasing closeness”, by keeping a larger distance to the audience. This can be explained by the compensation model of interpersonal distancing, which states that individuals will increase the distance to others if others are too close in terms of physical proximity, eye contact, etc. in an attempt to reach an equilibrium (Argyle and Dean, 1965).

Although previous work showed that there is a relationship between the audience size and how far users distance themselves from the display (Gentile et al., 2017a), our work is the first to find statistically significant results to support this finding. This is mainly due to our larger sample size; our sample covers 35 days while previous work coded only 5 days. Another difference is that previous work found a trend showing that audience-display distance might influence the user-display distance. Although we noticed a similar trend in our data, we could not find any significant effects to support the existence of an influence (p > 0.05).

5.3. Data visualisation
In order to highlight the main advantages that our predictor model can provide to researchers and practitioners, we developed an interface to easily set the inputs and visualise the outputs of our predictions based on the model2 Through this visual tool, a user can easily set any of the input variables (see the upper part of the GUI, shown in Fig. 8), in order to see their effects on the outputs. This provides users with a straightforward visualisation of the predictions.

Fig. 8
Download : Download high-res image (908KB)
Download : Download full-size image
Fig. 8. GUI of our proposed visualisation tool. Researchers and practitioners can use their data to feed the prediction model, and thus visualise the estimated interaction distance and duration through this tool.

This visualisation tool shows the most probable interaction areas in front of the display. The interface (shown in Fig. 8) was implemented in Python, by means of some packages for GUI programming (Tkinter - Lundh, 1999), graph plotting (matplotlib - Hunter, 2007), and image processing (Pillow - Lundh, 2011). Our tool uses a dataset file to train the predictor model on startup, and then uses the model to generate distance heat maps based on the input values set by the user. Such heat maps overlay an orthogonal top view of the deployment.

Along with such visualisation, the probability density functions (PDFs) generated by our predictor model, both for user-display distance and interaction duration, are shown in the bottom area of this visual tool. The heat maps are generated by expanding the estimated user-display distance PDF within a 2-D wedge-shaped area, corresponding to the field of view of the Kinect installed below the display.

This means that, in order to adapt our model to different deployments and to suitably visualise the corresponding predicted values, users would only need to change two files: the deployment orthophoto, and the dataset used for training the model.

5.3.1. How to adapt the visualisation tool to different deployments
The tool can be run by means of the Python script file gui.py. When started, it loads the two files representing the current deployment, and that can be used for customisation in other contexts:

•
a JPEG image file, representing an orthogonal top view of the deployment;

•
a plain text file, formatted as CSV (i.e., comma-separated values), representing the whole dataset.

The CSV file contains all the coded data used for training the predictor model. The structure of this file is described in Fig. 9. The training time depends on many factors, such as the number of entries in the dataset and the available computational resources.

Fig. 9
Download : Download high-res image (386KB)
Download : Download full-size image
Fig. 9. Structure of the CSV file, along with a sample file content. Note the presence of NA (i.e., not available) values: when Asize is equal to 0, many variables cannot be measured (namely ADD, Aside, Agaze, Ugaze, UAR), which is in line with our choice of defining two models instead of one. Moreover, there are some cases where the right values of some variables cannot be correctly assessed (e.g., due to low video resolution or other uncertain situations): in such cases, NA values can still be used for training, due to the features of the machine learning approach adopted in this work.

The tool can be used by public display researchers and practitioners to visualise the estimated interaction distances and durations in different settings of their deployments. Adapting the tool to other deployments is straightforward; the tool’s users only need to convert their data to CSV format in order for them to be used to train the model, and replace the JPEG file with a top view of the deployment. By visualising interaction distances, display owners can better understand the behaviour of their users, and the impact of their setup. Furthermore, by changing the variables set and values, they can easily estimate the effect of layout changes on distance and duration of interactions.

5.4. Use cases
We envision three possible use cases where our prediction model can be helpful.

Use Case 1: Planning and Improving Setups.Space owners can build a model for their layout according to our proposed method and then use it, along with the visual tool, for decision support when planning and improving public display setups. For example, based on the knowledge acquired from a model, a space owner who wants to achieve certain values for interaction durations or distances, could use the visual tool to experiment with different arrangements of seats around the display (which mainly affect the audience size), without actually making any changes in the real deployment, until the estimated values are satisfactory. This can be done as follows: the space owner should first collect data from a period of observations on the actual layout to generate a dataset, and use it to feed the model built according to our method.

The space owner can then use the visualisation tool to visualise PDF graphs and a heat map over the top-view picture of the layout, similar to those shown in Fig. 8, which would help understanding how audience behaviour affects the interactions. Here the space owner uses the model to understand the behaviour resulting from the current layout, in terms of observed values for distance and duration of interactions.

The trained model can then be used to predict the performance of different setups before actually implementing them. For example, if a space owner would like to test the impact of changing the audience size, they can just set different values for the audience size variable in the visualisation tool, thus simulating a different layout (e.g. a different number of benches around the display), and examining the resulting estimated values of interaction distance and duration. In other words, once the model has been trained for a given actual layout, which results in a given set of input variables and data, space owners can simulate different possible layouts, which means different arrangements of variables and data, without actually changing the layout itself. This allows for quicker and cheaper trials based on simulations and estimations from the proposed model, instead of expensive experimentation on different layout arrangements. This opportunity will empower the space owners to make key decisions to optimise their deployments. According to the result, the space owner can decide to add/remove seats around the display. Such a decision, which is based on the model predictions, would, in turn, affect the expected interaction distance and duration as desired.

Use Case 2: Real-time Prediction and Uses in Applications. A second use case is that the model could be used to forecast the users’ behaviour before they approach the display. For example, assuming the input variables are collected automatically (e.g., a camera estimates the size of the audience and their gaze direction – as described in Section 5.5), the system could dynamically decide to show the user different types of content (e.g., shorter videos) depending on the expected interaction durations. The system could also dynamically decide which input modality is better to enable – if users are likely to come close to the display, the system could allow for input via touch and disable other at-a-distance supported modalities (e.g., mid-air gestures).

Use Case 3: Facilitating Qualitative Comparisons across Deployments. Another possible use case is of particular interest for display providers. Typically, providers manage several deployments with different setups, and usually need tools for measuring performance of their deployments. Our visualisation tool would allow them to train multiple models (one per deployment), and easily compare the outputs by visually inspecting the differences in the heatmaps. Display providers can then use these estimations to provide qualitative data to their clients about their users’ behaviour, allowing them to take informed decisions about arrangements setup or displayed content as explained in the previous use cases.

5.5. Data collection strategies
In the previous sections, we described the process of collecting data and how to use them in order to train our models and make predictions. However, such a data collection process may be non-trivial and very time consuming, in particular if the coding is done by watching videos.

However, all the variables we coded can be collected automatically or semi-automatically. In particular, the number of users, the audience size and the side at which they were present, as well as the audience-display distance, can be all collected using video processing tools specifically aimed at monitoring audience’s behaviour, such as the AudienceMonitor (Elhart et al., 2017) and/or the pedestrian tracker (Williamson and Williamson, 2014). The same information can be collected over time to estimate if the honeypot effect occurs (e.g., by looking for peaks in the number of interactions), or if users were present during previous interactions (i.e., estimating variables HE and 
). Users’ and audience’s gaze can be also captured by visual computing techniques such as head pose estimation (Fanelli et al., 2011), and appearance-based gaze estimation (Wood et al., 2015; Zhang et al., 2015). Furthermore, the relationship between the user(s) and the audience can be estimated by tracking the arrival of users to the display in a way similar to how Block et al., identified if a user is a member of a group (Block et al., 2015). Indeed, recognising such situation would allow to code an interaction event where the audience and the user are acquainted. Another possibility is to detect if a group of users interact with each other. Marin-Jimenez et al., demonstrated that this can be done using computer vision algorithms (Marin-Jimenez et al., 2014). In those cases where some of the aforementioned variables cannot be identified with sufficient precision, they can be treated as N/A values, which means they are ignored during the fitting process due to the EM algorithm.

5.6. Extending the use of the model
The proposed approach is straightforward to extend by including other types of variables. We discuss some of the most promising ones below.

5.6.1. Time of the day
Müller et al., observed a very diverse audience over the day (Müller et al., 2012): school children in the morning, business people during lunch, and people shopping in the evening. In such cases, including the time of the day as an additional input variable would likely result in more accurate predictions. This can be done in two ways: one approach is to build different models for different periods of the days (e.g., one for the morning, one for the afternoon, and another one for the evening). In this case, the additional variable can be used as we did with the audience size Asize for selecting the right model accordingly (see Fig. 7). Alternatively, the time of the day can be added as an additional discrete variable, and the EM algorithm can be used to estimate its probability density function as we described and did for all the other variables.

5.6.2. Interaction modality
Another interesting consideration is about the interaction modalities. In our work, we considered a display that employs interaction via mid-air gestures. Hence we had the possibility of focusing on both the duration and distance. However, the same approach can be used for estimating interaction duration only, for instance in the case of touch-based interfaces, or eye gaze-based ones (where usually the user-display distance is required to be constant). Moreover, multimodal interface designers can use predictions in order to understand users’ behaviours, and thus design different attracting sequence for different supported modalities, e.g., based on audience size and/or audience-display distance.

Similarly, instead of guiding users to the sweet spot (Alt et al., 2015; Zhang et al., 2014), a system that can estimate the interaction distance could dynamically determine the sweet spot, according to the input variables used for the predictions. For example, EyeScout adapts the position of an eye tracker based on the position of the user of a large public display as detected by a Kinect (Khamis et al., 2017b). This can be improved by preemptively adapting the system according to the expected position of the incoming user.

5.6.3. Promising variables to explore in future work
Previous work suggested that variables that could impact the user’s behaviour around displays include the screen’s size and position (Zielinska-Dabkowska and Fatah gen Schieck, 2018), user’s properties like gender, age and body orientation (Ballendat et al., 2010), as well as the time of the day (Müller et al., 2012), the weather, and the presence of nearby events (Mäkelä et al., 2017b). Future work should explore incorporating these factors into the model building. Note that this would require very long deployments to collect enough data (e.g., how weather over the years impact attention to displays). Additionally, prior work showed that building layouts (Fatah gen Schieck et al., 2013) and building architecture (Dalton et al., 2015) influence user behaviour. Thus, future work can investigate how models can be trained to predict the impact of a building’s layout and architecture on the behaviour of passersby and users of public displays.

6. Future work
As discussed in Section 3.2, the data collection process can be very time consuming if not automated. A direction for future work is to automatically detect the situation around the display. Tools such as those proposed by Williamson and Williamson (2014) and by Elhart et al. (2017) already track some of the relevant aspects of audience behaviour (e.g., audience size). These tools can be extended to collect additional information such as the audience’s gaze by, for example, using head pose estimation (Fanelli et al., 2011; Murphy-Chutorian and Trivedi 2009) or appearance-based gaze estimation (Zhang et al., 2015). Another tool proposed by Block et al. (2015) infers the relationship between user groups, which can also be extended to estimate the relationship between the user and the audience in public display deployments. Collecting the aforementioned data automatically and feeding it into the model has the potential to result in more accurate models.

In our work we focused on predicting interaction distance and duration, which neither requires nor produces information about the number or frequency of interactions. Often, researchers and practitioners are interested in interaction frequency since it provides quantitative information about the passersby interest in the display’s content. Another interesting direction for future work is to predict the number of expected interactions. This requires continuous tracking of the audience to pinpoint situations that led to interactions, and would thereby be practically feasible by using an automated tracking tool like the ones discussed earlier.

Finally, interaction distance and duration are not the only variables that can be predicted. The model may be extended to be trained in order to predict, for instance, the kind of gestures used (e.g. two-handed vs single-handed, or large movements vs subtle ones) according to a particular audience configuration.

7. Conclusion
In this work, we studied the impact of audience size and behaviour, along with contextual information, to predict duration and distance of interactions via mid-air gesture on public displays. Through a field study, we found that the interaction duration is influenced by the number of users, the audience size and the relationship between the users and the audience. We also found that the interaction distance is influenced by the audience size and whether they gaze at the user(s). We used the collected data along with the expectation maximisation algorithm to build two predictor models to estimate the probability density functions of interaction duration and distance. Our results show that our models predict the interaction duration with a mean absolute error (MAE) of about 8 seconds and the interaction distance with a MAE of about 35 cm. We found that more accurate predictions can be achieved by using the variables that showed to have a significant influence on user behaviour. We also developed and presented a publicly available tool for visualising the results of our predictor models, making them more useful for researchers and practitioners. Our methodology can be also extended for including other contextual information (e.g., time of the day), and can be applied to many other situations by integrating it with many available systems for automatic data collection.