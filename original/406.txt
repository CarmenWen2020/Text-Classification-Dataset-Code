With the development of deep neural networks, the smart edge-cloud scenario is expected to meet the diverse and personalized requirements of users. However, the exiting edge-cloud architecture requires the deployment of manually designed neural networks. Providing adaptive network architecture in this environment, especially for heterogeneous edge devices, becomes the major challenge. To this end, we design the customized network architecture with neural architecture search for adaptively neural network search. Specifically, our architecture can be identified in three parts: cloud layer, equipped with huge amounts of computing resources, conducts multi-objective architecture search on a proxy dataset; the edge layer aims to train the optimal architecture based on the target dataset from scratch, which makes full use of the computing resource of the edge server; and the user layer deploys the multi-objective model in diverse devices for inference in real time. Furthermore, we deploy an industrial sensor monitoring scenario as a case study to search for the temporal convolutional network, demonstrating the effectiveness of the proposed architecture. Experimental results show the feasibility of the proposed architecture.

Introduction
In the past few decades, with the widespread application of deep neural networks (e.g., image classification, object detection, and semantic segmentation), the edge-cloud architecture has been widely used to provide computation resources and reduce inference latency [1]. Traditionally, a hand-designed neural network is trained on a target dataset in cloud, and then the model is deployed to the edge server for inference. However, the dramatic increase of neural-network-based applications in edge devices poses significant burdens on designing specific network architectures for users. It becomes difficult to meet the requirement of extremely diverse edge devices based on designing neural networks manually, particularly in scenarios where the computational costs of edge devices are constrained. Furthermore, the privacy issue makes data transfer from user to cloud server more challenging, attracting much attention recently. Fortunately, neural architecture search (NAS) has been proposed recently as an effective network architecture design method to solve the above problem [2].

NAS, as a part of AutoML, has achieved impressive performance in neural architecture design that is superior to those developed manually. Generally, NAS consists of three factors: search space, search strategy, and evaluation method. Specifically, the search strategy first samples a large number of architectures from the search space, and then the selected architecture is evaluated through the evaluation method. Finally, the search strategy is updated through the evaluation results. As a pioneering work, [3] first adopted reinforcement learning (RL) methods to search for network architecture so that the method reaches the state of the art on the image classification problem. This method demonstrates that the idea of NAS is feasible in practice. Furthermore, the work proposed in [4] using evolutionary learning to search architecture achieves similar results as RL methods, further demonstrating the feasibility of the idea.

However, despite the effectiveness of NAS in designing high-quality architectures, the searching process for optimal architecture is computationally demanding (e.g., tens of thousands of GPU days [3]), which impedes the practical deployment of NAS in various applications.

To alleviate the above issues, great efforts have been undertaken to reduce the architecture search cost in recent years. The remarkable effort is the differentiable search methods, represented by DARTS [5], which relaxes search space as continuous architecture parameters, reducing search cost to about one GPU day. Early NAS work trained the sampled architectures from scratch, and the trained child architecture was removed. Recent work [6] has tried to reuse the weights of previously optimized architectures, which reduces the search cost by several orders of magnitude. Based on a review of the literature, we note that recent achievements mainly focused on optimizing the NAS algorithm, for example, reducing the time consumption. Although NAS has made great breakthroughs in search time, from the previous tens of GPU days to the current few GPU hours, there is still a great challenge in actual deployment. We take the deployment of the NAS process in cloud as an example. Typically, to search the network architecture for an edge device, we need to upload a training dataset to cloud, which causes data leakage problems. Moreover, when the target dataset is large, uploading the dataset to cloud may introduce additional transmission delay [7]. Therefore, we need an effective architecture to meet the requirements of various edge devices.

Motivated by such facts, we first redesign an adaptive NAS for collaborative edge-cloud computing. Precisely, cloud layer obtains the multi-objectives from the edge to search for the network architecture on proxy dataset [8], then sends the optimal network architecture to the edge server to train the architecture from scratch based on target dataset, and finally deploys the trained model to edge devices. Additionally, previous work on NAS mainly achieved great success in various computer vision tasks. There was a wide research space in the time series model. Based on this observation, we further propose a new search space with the temporal convolutional network (TCN) [9] to achieve the multi-step time series forecasting for sensor data.

Our major contributions are as follows:

To automatically design neural architecture for heterogeneous edge devices, we propose an adaptive NAS in the collaborative edge-cloud scenario. Specifically, a searching architecture with the proxy dataset in cloud avoids data leakage, and training the searched architecture in the edge server makes full use of edge resources.

Based on the novel NAS, we present a new search space with the temporal convolutional network for industrial sensor data forecasting, filling the gaps of NAS in the time series forecasting model.

Some challenging open issues related to the proposed architecture are summarized, indicating potential research directions for the future.

NAS in the Collaborative Edge-Cloud Scenario
This section first presents the collaborative edge-cloud architecture used for NAS and then discusses our motivation.

Explanation of NAS in the Edge-Cloud Scenario
Figure 1 shows the collaborative edge-cloud with a three-layer architecture, including cloud layer, edge layer, and user layer. Different from the traditional edge-cloud architecture that trains hand-designed neural networks, we reconstruct the three-layer architecture to adapt better heterogeneous edge devices with NAS. The composition of each layer is shown as follows.

Figure 1. - Neural architecture search in collaborative edge-cloud.
Figure 1.
Neural architecture search in collaborative edge-cloud.

Show All

Cloud Layer
The cloud layer contains various computation resources, for example, GPU, TPU, and field programmable gate array (FPGA), which can speed up the process of architecture search.

Edge Layer
The edge layer containing massive resources (e.g., edge servers and base station) is closer to the edge device compared to cloud layer. Thus, latency can be reduced by processing the data in the edge server.

User Layer
Edge devices such as vehicles and smartphones are contained in the user layer. Various artificial-intelligence-based applications run on these devices, such as face recognition, objective detection, and automatic translation. The trained models are deployed in these devices for inference.

Challenges and Motivations
Recently, NAS has become one of the most popular directions for AutoML. Although NAS receives exceptional performance on computer vision and language modeling tasks, we still face some challenges when deploying NAS in the edge-cloud scenario. The first challenge is that the process of architecture search consumes huge computational power, especially when deployed in the heterogeneous edge. Even though cloud layer can provide computing resources [10], transferring data from the generator to cloud may cause privacy issues. Meanwhile, cloud layer is far from the data generator, which causes large transmission delay. Another challenge is that additional objectives such as latency and energy consumption can be considered in a real deployment.

Since the NAS process is time-consuming, it is hard to conduct NAS research without computing resources. Therefore, the research on NAS in recent years mainly focuses on reducing the search space so that the research can be continued in academia. Meanwhile, more search strategies have been proposed and achieved state-of-the-art performance. Unfortunately, we find that the many efforts on NAS in recent years are mainly in algorithm innovation. An integrated architecture that can handle the entire NAS process in practice is still missing from the state-of-the-art literature. To bridge this gap, we aim to propose such an integrated architecture.

In the next section, we discuss how to tackle the above challenge with the proposed NAS for collaborative edge-cloud architecture.

Proposed Architecture for Collaborative Edge-Cloud
In this section, we describe NAS for the heterogeneous edge in collaborative edge-cloud. The main goal of the architecture is to design neural networks dynamically to satisfy edge requirements while providing computing resources for architecture search. In the following, we use NAS-RL [3] as the search strategy to describe the proposed architecture. Note that other search strategies, such as evolutionary algorithms (EA) [4] and gradient-based methods [5], are also compatible with the architecture. As shown in Fig. 2, the process consists of three parts: NAS based on the proxy dataset, training from scratch in the edge server, and deployment in the edge device, each of which is specified below.

Figure 2. - Illustration of the proposed architecture.
Figure 2.
Illustration of the proposed architecture.

Show All

Neural Architecture Search Based on Proxy Dataset
As shown in Fig. 2, RL-based NAS generates neural networks through a controller in cloud. Specifically, the architecture of a neural network can be described as a variable-length string. To generate such a string, we exploit recurrent neural network (RNN) as the controller and use gradient policy to optimize it.

Generally, to obtain the optimal network architecture in the specific dataset, the most effective method is to search the network architecture directly on the target dataset [11]. Unfortunately, uploading the dataset directly to cloud may cause leakage of dataset privacy and additional transmission delay. To avoid this problem, we use NAS to search the network architecture based on the proxy dataset rather than directly based on the target dataset. This idea is similar to [12], in searching the network architecture on the proxy dataset (e.g., CIFAR-10) and then deploying the optimal architecture on a large dataset (e.g., ImageNet) to reduce the search cost. Although searching on the proxy dataset can avoid data uploading, the generation gap can be introduced as the inconsistent performance of architectures on different datasets. To solve this inconsistency, we adopt AdaptNAS [13] to improve the generalization of architecture. The key concept behind AdaptNAS is to optimize network weights with both proxy training dataset and the A-distance, where A-distance denotes the domain distance constraint during the search.

When deploying the model in edge, a suitable DNN architecture is needed to optimize a multi-objective efficiency metric (e.g., accuracy, latency, and energy), achieving a satisfying user experience for edge inference. To achieve this purpose, as shown in Fig. 2 (left), the controller first obtains the search target from the edge user before the architecture search process. Since the search strategy is based on RL-NAS, it is convenient to customize multi-objective optimization.

Specifically, the RL-based strategy scalarizes the multi-objective into one reward using a weighted sum, where the trade-off among various objectives is controlled by tuning the corresponding weights. Thus, multi-objective optimization can be converted to a single-objective problem. In summary, the process of architecture search can be conducted as follows:

The controller samples a batch of candidate architectures.

The candidate architectures are trained on the proxy dataset for fixed steps.

The controller obtains the multi-objective reward.

The controller parameters are updated.

Repeat until reaching the predefined number of candidate architectures.

Training from Scratch
After the optimal architecture based on proxy data is found in cloud, the next stage is to transfer the optimal architecture to the edge server for training on the target dataset from scratch. Compared to the traditional edge-cloud scenario, which needs to upload edge data to cloud for model training, the edge server is located between cloud and the edge device, providing massive computing resources for the edge device [14]. Furthermore, as the user data are usually stored in the edge server, directly training the model on the edge server avoids data leakage and transmission delay. As shown in Fig. 2 (middle), the edge server trains the model on the target dataset after obtaining the optimal architecture. Therefore, the data privacy of edge devices can be protected without uploading target data to cloud. The key point of the proposed strategy is protecting data privacy and reducing the delay caused by uploading the target dataset to cloud.

Deployment
Next, we deploy the optimal architecture trained from the target dataset to inference on edge devices (e.g., smartphones). Considering the limitations of computation and communication resources in edge devices, we make edge devices only responsible for online inference to extend their battery life. In addition, edge devices may continuously upload the latest data to edge servers. When the performance is lower than a predefined threshold, the edge servers update the deployed model using the latest dataset to improve the performance. Therefore, the model can provide higher inference accuracy.

Case Study: DRL-Based Temporal Convolutional Network Search
In this section, we present an industrial IoT scenario as a case study of applying the proposed NAS architecture for multi-step time series forecasting based on a temporal convolutional network. As shown in Fig. 3, the environment consists of three components: (1) the edge device aims to predict the time series data in real time; (2) the edge server is used to train the optimal architecture from scratch; and (3) cloud server is applied for searching the neural architecture based on DRL. The configuration of each component is as follows:

Edge Device: A Nvidia Jetson Nano, equipped with Quad-Core ARM Cortex-A57 64-bit@1.42 GHz and NVIDIA 128 CUDA cores, is used as an edge device for inference.

Edge Server: A MacBook Pro equipped with Intel 6 Core i7@2.6 GHz CPU and 32 GB RAM is used as an edge server for providing architecture training from scratch.

Cloud Server: The cloud server is equipped with 8 vCPU 2.5 GHz Intel Xeon Platinum 8163 and 32 GB RAM, having NVIDIA V100 with 32 GB graphic memory.


Figure 3.
The use case of NAS in a smart factory.

Show All

Time Series Modeling
Time series modeling consists of predicting the next T values yN+1, …, yN+T given an input sequence x1,…,xN at each time, where T denotes the forecasting steps. If T=1, it is a one-step prediction; if T>1, it is a multi-step prediction. Generally, many forecasting strategies can be used for multi-step time series forecasting, for example, direct forecasting and iterative forecasting. In this work, we use the direct forecasting strategy for convenience. For time series forecasting, the key restriction is that the predicting value Yt only uses the input obtained from x1,…,xN. Formally, the target forecasting step yt+h is generated by function f learned from the time series xt,…,xt−d+1. Note that h means forecasting horizon, and d denotes the history time steps.

Although machine learning models can be used for time series forecasting [15], the most famous approach is realized on long short-term memory (LSTM), a variant of RNN. However, the disadvantage of LSTM is that the later timestep prediction can wait for the prior cell to complete, which results in disadvantage parallelism. Fortunately, the recently proposed TCN consisting of a 1D convolutional network outperforms the RNN models in sequence tasks [9]. Unlike in RNNs, convolutions in TCN can be calculated in parallel because the filter is shared in each layer, which speeds up both training and evaluation cycles. Moreover, the receptive field size of TCN can be changed in multiple ways (e.g., stacking more dilated convolutional layers and using larger dilation factors), which make it easy to adapt to different domains. We detail the structure and the architecture search for TCN in the following.

The Neural Architecture Search for TCN
This article intends to search the optimal architecture of TCN for time series forecasting. TCN was first proposed by [9] for sequential data, which was composed of three components: causal convolutions, dilated convolutions, and residual connections. For causal convolutions, the output at time step t only consists of elements from t and earlier in the previous time step. The causal convolution is very important for time series prediction because it can effectively avoid the leakage of information in the future. However, to achieve a long history size, simply stacking causal convolutions may cause the network to be extremely deep, which is not feasible in real use. To solve this problem, TCN applies dilated convolution to enable an exponentially large receptive field. Specifically, dilated convolution contains a dilation factor, which refers to the steps between every two adjacent filter taps. Based on the dilation convolution, the output of the top layer can represent a wider range of time steps, which effectively expands the receptive field of the network. Therefore, the receptive time step of TCN depends on three parts: network depth, filter size, and dilation factor. To stabilize the training process with the deep network, TCN uses residual module identifying mapping rather than the entire transformation.

Based on the above concept, we define the following component to search for the best TCN.

Search Space
For simplifying the search process, we use a fixed-size network structure to find the optimal TCN architecture. Specifically, there are four residual blocks in the entire architecture, and each one contains two dilated causal convolution layers. For each convolutional layer, we search optimal kernel size (ks) and dilated factor (df). Note that each convolutional layer follows a rectified linear unit (ReLU) and a dropout layer. Therefore, the search space for dilated causal convolution can be summarized as follows:

Convolutional kernel size: 3×1, 5×1,7×1

Dilated factor: 1, 2, 3

Thus, there are 94×2≈4.3×107 potential architectures. To find the optimal network architecture, we adopt the RL search strategy, since it is easy to customize the reward and is well established in traditional NAS.

Searching with Reinforcement Learning
After defining the search space, we can use a search strategy to find the optimal architecture. The main search strategy used in this work is based on RL-NAS proposed by [3], where the agent is the LSTM controller and the environment is the search space. During the search process, we follow the sample-eval-update loop to train the controller. After model m is sampled, we train it on the proxy dataset to get its loss value and calculate the model size. Finally, we calculate the reward by combining both loss value and model size. Similar to [12], we follow the idea that maps each potential architecture in the search space to a string list.

Also, the controller is updated using proximal policy optimization (PPO). The controller first obtains the encoding architecture through the input unit, then outputs the prediction result through softmax. The unit at the next time step reads the output of the previous layer for prediction. This process continues until four residual blocks in the entire architecture is generated.

Experimental Results
Dataset
In this subsection, we use a real dataset collected from factory substations to verify the effectiveness of the proposed method. This dataset contains four months of the sensory dataset from December 2019 to March 2020 collected at five-minute intervals and is composed of hundreds of features. For the sake of convenience, we only use three features – Current, Power, and Volt – with approximately 34,000 pieces of each to perform multi-step time series forecasting. Similar to [15], we adopt the direct strategy that develops different forecasting models for each forecast interval independently and exploit a sliding window with multi-step to segment the sensor data to obtain training, validation, and testing samples.

Training Details
We deploy the neural network in PyTorch and use mean square error (MSE) as the loss function. For the controller, we use a single-layer LSTM with 64 hidden units. It is trained with stochastic gradient descent (SGD) with a learning rate of 0.001. We exploit NVIDIA V100 with 32 GB graphic memory and sample 20 architectures in each step to speed up training. For the sampled architecture, we train the model for 40 epochs with a learning rate of 0.001.

To verify the efficiency of our proposed search space, we first compare the architecture performance of a manual design to that obtained by NAS. We further compare the generalization performance of the searched architecture on different datasets. Note that the searched TCN architecture is shown in Fig. 4. In particular, unlike architectures developed manually by human experts, the residual block in the search result contains different ks and df. The skip connections exploit 1×1 convolution to match the input-output tensor shape.

Figure 4. - The optimal architecture under search space.
Figure 4.
The optimal architecture under search space.

Show All

Figure 5a illustrates the response time of three different schemes in multi-step forecasting. Note that the response time includes the time for data transmission and inference. As shown in Fig. 5a, the time for inference in cloud is much longer than execution on edge servers and edge devices. Specifically, when forecasting the series for the next 24 hours, it takes 30.7 ms on the edge device, which is 28.9 and 16.8 percent shorter than on cloud and edge servers, respectively. The reason is that cloud and edge servers can provide powerful computing power, but data transmission takes up most of the time.

Figure 5. - Performance comparison under multi-step ahead: a) Response time with the increment of time step.; b) MSE and params size on three models with the increment of time step.
Figure 5.
Performance comparison under multi-step ahead: a) Response time with the increment of time step.; b) MSE and params size on three models with the increment of time step.

Show All

Table 1. MSE value under different domains.
Table 1.- MSE value under different domains.
Figure 5b shows the tendency of performance metrics (i.e., MSE and model parameter size) to increase the forecasting step. When the forecasting horizon increases from 30 mins to 24 hours, the MSE value increases as well. The reason is that the uncertainty increases with the increase of prediction interval, which leads to the decrease of prediction accuracy. Obviously, the MSE based on TCN is lower than the LSTM, where NAS-TCN improves the MSE value by 6.1 and 5.3 percent compared to LSTM and hand-TCN in 12 hours, respectively. In Fig. 5b, the params size is also shown to demonstrate the effectiveness of the multi-objective search. Since the aim of architecture search is to balance loss value and model size, the params size of NAS-TCN is lower than that of manually designed TCN. Specifically, in 24 hours, NAS-TCN reduces the params size by 38.9 and 16.9 percent compared to LSTM and hand-TCN, respectively. As a result, the effectiveness of NAS for TCN is verified, which makes the architecture more accurate and efficient.

Table 1 shows the generalization performance of searched architecture on target dataset. We deploy two approaches: search on proxy and search on target. Specifically, search on proxy means the architecture search is conducted on the proxy dataset, and then the MSE value is tested based on the target dataset. Specifically, the value 0.074 indicates that the network architecture is searched on Currentproxy, and the model is finally tested on Volttarget. For search on target, however, both the architecture search and test are performed on the target dataset. Since the search method is based on AdaptNAS, it is competitive to directly search on the target dataset. Especially in the middle column, the MSE value of search on proxy is even lower than search on target. The reason is that we retrain the parameters from scratch after completing the architecture search. Based on this observation, we demonstrate the availability of searching on proxy data to avoid data uploading in cloud.

While we only examine our framework with TCN, other sequence models (e.g., LSTM) and convolutional networks can easily be exploited. We believe this framework is general enough to be adapted to collaborative edge-cloud computing.

Open Issues
In this section, based on the proposed NAS architecture, we propose the following open issues for further study:

Search Strategy Selection: In this article, the search strategy based on RL can effectively complete the lightweight model search with a small dataset. However, when edge devices deploy deep image recognition models (e.g., face recognition and target detection), the search strategy based on RL is very time-consuming. Therefore, a faster search strategy is imminent. There are many interesting directions in one-shot NAS that treat all architectures as different subgraphs of a supergraph and share weights.

Multi-Objective Optimization: The search process considers loss value and model size, which have certain limitations. In fact, model deployment on edge devices requires consideration of various metrics, such as latency, energy consumption, and accuracy. These metrics need to be considered during the search process and a good trade-off among them achieved, which is also a hot topic.

Privacy Preservation for NAS: Generally, the search process is time-consuming and requires a lot of hardware acceleration equipment. Although cloud platform can provide many computing resources for NAS, directly uploading sensitive target data to cloud may encounter the risk of data leakage. To effectively search the network architecture on the target dataset, ensuring the private dataset safely in cloud is also an important research direction.

Conclusion
In this article, we introduce adaptive NAS in the collaborative edge-cloud scenario to automatically design neural architecture for heterogeneous edge devices. Our proposal prevents the data leakage and transmission delay by conducting architecture search on the proxy dataset and fully utilizes the computing resources of the edge server by training the searched architecture from scratch. As a case study with this architecture, we design a novel NAS search space based on TCN to predict time series datasets in a smart factory. Experimental results indicate the availability of the proposed architecture and the efficiency with multiple objectives.