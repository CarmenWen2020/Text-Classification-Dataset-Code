The Industrial Internet of Things (IIoT) enables intelligent industrial operations by incorporating artificial intelligence (AI) and big data technologies. An AI-enabled framework typically requires prompt and private cloud-based service to process and aggregate manufacturing data. Thus, integrating intelligence into edge computing is without doubt a promising development trend. Nevertheless, edge intelligence brings heterogeneity to the edge servers, in terms of not only computing capability, but also service accuracy. Most works on offloading in edge computing focus on finding the power-delay trade-off, ignoring service accuracy provided by edge servers as well as the accuracy required by IIoT devices. In this vein, in this article we introduce an intelligent computing architecture with cooperative edge and cloud computing for IIoT. Based on the computing architecture, an AI enhanced offloading framework is proposed for service accuracy maximization, which considers service accuracy as a new metric besides delay, and intelligently disseminates the traffic to edge servers or through an appropriate path to remote cloud. A case study is performed on transfer learning to show the performance gain of the proposed framework.

Introduction
The Industrial Internet of Things (IIoT), as a subset of IoT, interconnects a multitude of industrial devices, actuators, and people at work. IIoT will deliver new insights into industrial applications, reduce manual labor and time, and pave the way for Industry 4.0. According to GE Digital, IIoT is estimated to unlock manufacturing savings and benefit 46 percent of the global economy [1].

IIoT incorporates artificial intelligence (AI) technologies to process and analyze data from various sources and make advanced predictive analytics, such as fault class prediction, predictive maintenance, demand forecasting. Since smart manufacturing is a large connected and complex industrial process, which produces a large amount of multi-feature data, it is difficult to construct its operation process with an accurate mathematical model. AI algorithms are able to extract critical features without in-depth physical understanding of the concerned system. Therefore, AI algorithms fit IIoT for the ability of self-organization, self-adaptation, and self-learning. IIoT usually has time-varying monitoring parameters, and highly dynamic system state and data structure; hence, AI technologies play a significant role in IIoT. For example, in IIoT, predictive maintenance relies on machine learning to detect anomalies in systems and then predict the failure of devices by correlating and analyzing the change in the pattern.

AI-enabled IIoT typically requires prompt and private computing service to process and aggregate the manufacturing data. Thus, integrating intelligence into the edge is without doubt a promising development trend [2]. Edge computing provides distributed computing service through small-scale data centers near the edge of the network. As compared to mobile cloud computing, edge computing provides real-time data analytics with privacy preserving, increases network capabilities, and avoids congestion of backbone networks and the Internet core.

Edge intelligence for IIoT exhibits the advantages of personalization, responsiveness, and privacy.

Personalization
Customized AI models can be developed at the edge servers, which are tailored to individual users' behaviors and requirements to deliver accurate results to the users.

Responsiveness
While the industrial process is time-varying and unpredictable, the computing service must be prompt and more adaptive and feasible to the new situation.

Privacy Preserving
Especially for IIoT, the processing information owned by industrial companies may not be willing to transmit to the remote cloud for privacy issues; thus, the edge server provides private service.

Existing works mostly discuss how to deploy (compress and reduce) neural networks on edge servers, ignoring the impact of AI-enabled service on offloading [3]. Actually, the new businesses spawned by IIoT have extremely strict requirements for delay and accuracy of computing results, which is the main characteristic of IIoT. The AI service deployed on edge servers exhibits heterogeneity in terms of service accuracy due to the limited and heterogeneous computing capability of edge servers. Most works [4] on offloading try to find the best power-delay trade-off. With the involvement of accuracy, IIoT mobile devices (MDs) should determine the appropriate edge servers according to their accuracy and delay requirements. The impacts of edge intelligence on computing offloading remains untouched.

In this vein, in this article, we introduce an intelligent computing architecture with cooperative edge and cloud computing for IIoT. Then an AI enhanced offloading framework for service accuracy maximization is developed, which considers service accuracy as a new metric besides delay, and intelligently disseminates the traffic to edge servers or through appropriate paths to remote cloud. A case study is performed on transfer learning to show the performance gain of the proposed scheme.

Table 1. The comparison of existing works on network related areas.
Table 1.- The comparison of existing works on network related areas.
The organization of the remaining article is as follows. In the following section, we overview the basics of AI in IIoT and edge computing. Then we propose intelligent traffic control in edge intelligence for service accuracy maximization. A case study is then performed on transfer learning. Finally, we conclude the article.

Edge Intelligence in IIoT
Machine Intelligence in IIoT
Machine learning, as an application of AI, gives devices or computer systems the ability to “learn” with data without being explicitly programmed [2]. Machine learning tasks are classified into supervised learning, unsupervised learning, and reinforcement learning, depending on whether there are training samples available for the learning system. Particularly, supervised learning conducts classification or regression tasks from labeled data, while unsupervised learning categorizes the unlabeled data into clusters. Reinforcement learning indicates agents to take actions so as to maximize the cumulative reward. Typical AI algorithms include artificial neural network (ANN), convolutional neural network (CNN), binary decision tree, Q-learning, and so on.

Machine learning algorithms have found their applications in networking, since they can extract features from huge amounts of data with complex system states. First, machine learning can learn the network patterns to help with optimal decisions, for example, traffic patterns for routing decisions [5], [6]. Second, the resource allocation and network optimization can be formulated as a decision making problem [7]. For example, He et al. [8] exploit reinforcement learning to optimize resource allocation in trusted social network with edge computing, caching, and device-to-device (D2D) communications. Moreover, network adaptation is possible with machine learning schemes to adjust the network parameters adaptively to the network dynamics [9]. Table 1 compares the existing works on AI applications in networks.

In IIoT, machine learning algorithms are leveraged to analyze the complex manufacturing data and believer insights about predictive maintenance, industrial prognostics, and demand forecasting. For example, Bisio et al. [10] studied the role of context awareness in IIoT applications such as smart health, smart factory, and smart home scenarios. Their main contribution is that the system can intelligently perceive the environment and react to specific events or conditions according to the typical situational awareness principles and behaviors. Wang et al. [11] explored the trade-off between energy consumption and service latency in IIoT. They proposed a new model that is constructed using the reinforcement learning framework to describe the willingness of users to share resources. Then a dynamic reinforcement learning scheduling algorithm and a deep dynamic scheduling algorithm are proposed to solve the offloading decision problem. Wu et al. [12] applied random forest for machinery prognostics in tool wear in dry milling operations. The prognostic model is run on the Amazon Elastic compute cloud. Generally, cloud computing is employed for data processing. However, it is difficult to transmit huge amounts of data to the remote cloud; thus, approximate and distributed computing service becomes necessary.

Machine Learning in Edge Computing
Deploying AI-based computing service at the edge servers could effectively extend the computing capability of MDs and enable MDs' access to machine intelligent service with low latency. There are edge servers that are actually designed for AI-enabled computing tasks such as the NVIDIA DGX workstation. As a practical example, Nokia has developed a system running on their Liquid MEC product (Intel also has a similar offering) that analyzed video streams recorded on a number of surveillance cameras. The data was then processed by a neural network executing on an MEC server deployed at the base station.

The MEC application examined the video streams, classified what were normal and abnormal patterns, and then only needed to send the stream to the backbone when a potential security issue was identified.

Typically, AI models need to be compressed and reduced for applications on edge servers, as the computation-demanding and time-consuming AI tasks laid a heavy burden on edge servers with limited resources. Li et al. [13] applied deep learning for IoT in the edge computing environment. They designed a scheduling algorithm to maximize the number of tasks in edge computing with guaranteed quality of service (QoS) requirements. In [3], edge servers learn model parameters from data distributed at the edge nodes, using the gradient-descent method based on distributed learning, instead of sending data to the centralized cloud. They proposed a control algorithm for the trade-off between local update and global parameter aggregation to minimize loss function and under a given resource budget.

IIoT with Edge Intelligence
In this section, we first introduce the architecture of edge intelligence in IIoT and then present the offloading framework for IIoT MDs.

Edge Intelligence Architecture
Figure 1 depicts an example of IIoT consisting of industrial devices, base stations (BSs) equipped with edge server functionality, and remote cloud. Industrial MDs monitor the industrial parameters, and deliver the collected data to the data center for aggregation. The AI-enabled IIoT service includes self-monitoring, demand forecasting, fault detection, and workforce management. The decision is fed back to the IIoT devices and executed automatically.


Figure 1.
Illustrating an example of IIoT devices with intelligent computing service, consisting of edge layer and cloud layer. Each BS is equipped with edge server functionality, on which AI-based computing service is processed. IIoT MDs determine the appropriate edge servers according to their requirements and the provided accuracy of the edge servers.

Show All

However, the training phase of some machine learning algorithms, such as CNNs, poses much pressure on the computational capability of edge servers, as well as the communication resource in the backbone network where training data need to be downloaded from remote cloud. Hence, the intelligent computational architecture needs to be reshaped. In this regard, we introduce an intelligent computing architecture for IIoT, consisting of a two-layer intelligent data center, that is, edge layer and cloud layer. Edge and cloud computing work cooperatively to serve IIoT devices with proximate and prompt computing service through edge layer as well as powerful and comprehensive computing service through cloud layer.

Edge Layer
It accommodates lightweight intelligent computing service for IIoT, while the computing service of edge servers may be differentiated from each other by computing application and service accuracy.

Cloud Layer
It provides powerful and comprehensive computing service for IIoT at the cost of latency and communication burden.

The edge layer and cloud layer interact with each other. The edge layer may need the assistance of the cloud layer to train its machine learning model, while the edge layer also works as a relay to transfer data from IIoT devices to the remote cloud. The interaction between the edge layer and the cloud layer is at the cost of additional communication on the backbone network. Thus, how to deploy the computing service between the edge layer and cloud layer, and afterward assign the computing tasks of IIoT devices according to their requirements as well as the characteristics of heterogeneous edge servers and remote cloud, needs serious consideration.

Offloading in Edge Intelligence for IIoT
In heterogeneous edge computing, the determination of traffic offloading actually relies on the particular delay and accuracy requirements of a certain computing task, the heterogeneous capability and congestion condition of the edge servers, and the network topology. Assume there are m IIoT MDs that need to offload tasks and n AI embedded edge servers with different accuracy. MD i submits its computing task requirements with the priority of the task, accuracy, and delay, that is, (ui, ai, di), where ui is the degree of urgency of MD i, that is, priority (a scalar value within (0, 1)), and ai is the acceptable accuracy of MD i, and di is the acceptable delay of MD i (unit: millisecond). In addition, the computing and storage resources required by the task can be expressed as (ci, si), where ci is the computing resource required by MD i (unit: CPU cycle) and si is the storage resource required buy MD i (unit: byte). In the same way, the computing and storage resources of server j can be denoted as (Cj, Sj), and the accuracy of the AI application deployed in server j is expressed as Aj. To choose an optimal offloading decision for MD i, it is imperative to estimate the delay and accuracy of offloading to available edge servers, and determine the optimal offloading option according to its requirement. Let σij∈{0,1} represent the offloading decision of MD i where σij=0 when MD i does not choose edge server j for offloading, and σij=1 when MD i determines edge server j for offloading, Different from the previous research, the offloading decision depends not only on the estimated access delay but also on the accuracy the edge server can provide.

Definition 1
Accuracy Maximization Offloading with Latency Constraints (AMOLC): Given the information of edge intelligence, such as the computing capability of edge servers, the deployed computing service on edge servers, the network topology, as well as the information from MDs, such as delay requirement, accuracy requirement, the location of MDs, the problem in AMOLC is to find an optimal decision for all computing tasks of MDs to achieve the maximum accuracy for MDs under the latency constraints.

In this article, accuracy refers to the ratio of the correct number of test results to the total number of test samples. Network training generally consists of two processes: training and testing. In the testing process, we utilize test samples to check the performance of the network, and whether the test results are accurate is the key indicator to judge the robustness of the network. Therefore, it is appropriate that the ratio of the correct number of test results to the total number of test samples is used as the accuracy of the network.

However, solving AMOLC is nontrivial, as the traffic offloading and traffic path decision of MDs interact with each other. For the tagged edge server, if its peer MDs all select an edge server for offloading, the estimated processing delay will be significantly increased. Actually, the problem of AMOLC is NP-hard, as we can easily transform it into a maximum cardinality bin packing problem [14]. In order to intelligently disseminate the traffic of MDs in edge intelligence, we propose a near-optimal offloading framework for accuracy maximization offloading with latency constraints (AMLC). Therefore, through the proposed offloading framework, the accuracy of MDs can be maximized, while the traffic to the backbone can be effectively reduced. The proposed offloading framework is described in the following four steps.

Step 1: (Estimate the accuracy of computing task from IIoT MD i.) According to the computing and storage capabilities (Cj, Sj), service j will load different AI-enabled computing applications on various edge servers. Then server j trains the network with an appropriate set of training images and predicts the assess network accuracy Aj. The accuracy is affected by the size and quality of training data and network training mode. Therefore, the accuracy of AI applications deployed on different edge servers will vary even with the same network structure. If the sample size from server j is large, it may have higher Aj, but the increasing sample size would definitely consume more computing resource of the edge server, excluding other computing applications. For the tagged edge server, given the loaded computing application, deep learning algorithm, neural network architecture, and sample size, the accuracy can be determined. If the tagged edge server does not hold a computing application, the accuracy is set as zero.

(Estimate the access delay of computing tasks from MD i.) For computing tasks of MD i, we first estimate the latency if offloading to the available edge servers or through some paths to the remote cloud, respectively, and drop those computing tasks whose delay requirements cannot be satisfied (i.e., the access delay is greater than di) by offloading to edge servers or remote cloud. In particular, the processing delay in access delay at the edge server reflects the congestion situation at the edge servers, while the propagation delay relies on the distance between MD i and the edge servers or the remote server and also the network topology. In other words, by jointly minimizing the propagation delay, an appropriate path can be selected toward the remote server.

(Offload to the appropriate edge servers.) According to the estimated accuracy and delay, as well as the (ui, ai, di) of MD i, we determine the offloading edge servers for MD i. In particular, for the computing tasks that are determined to be processed at the edge servers, for each MD i, we sort all the available edge servers according to their estimated accuracy in descending order. Then from the edge server with the highest accuracy, we examine whether the predicted delay attained from Step 1 is lower than its delay requirement. If satisfied, we determine the current edge server to the tagged MD. Otherwise, we check the delay of the second edge server. For all IIoT MDs, the higher priority IIoT MD is served first.

For those MDs that did not find an appropriate edge server or remote server, it can be accomplished locally at the CPU of MDs. For those computing tasks with predicted delay to the remote cloud lower than its delay requirement, we tend to route the computing tasks to the remote cloud, since the remote cloud is most powerful and can provide the highest accuracy. Thus, through Step 3, the accuracy of MDs is determined to be the maximum among all the available edge servers under the constraints of latency. By the proposed offloading framework, traffic will be disseminated intelligently, according to its requirement, to the optimal edge servers or to the remote cloud through an appropriate path so that the pressure on the backbone network will be effectively alleviated.

The cost of our approach consists of two parts: downloading the pretrained network from the remote cloud and uploading the target domain data to the specific edge server. Since only a small amount of parameter data about the neural network needs to be transmitted to the edge server from the remote cloud, there is not much burden on the backbone network. In the training phase of transfer learning, we need to upload training samples from the target domain. The target domain data is only uploaded to nearby edge servers and therefore does not consume much bandwidth. Unlike large-scale training in remote cloud, transfer training requires only a small amount of targeted training data to achieve high accuracy of the network.

A Case Study on Transfer Learning
In the case study, deep learning, for example, DL-based transfer learning, computing applications are run on edge servers in IIoT. IIoT MDs turn to the edge servers for image processing to monitor the industrial process. In this section, we present the numerical results of the proposed intelligent offloading framework for edge intelligence in IIoT.

Figure 2 shows the diagram for transfer learning. The transfer-learning-based computing and offloading framework is done following five phases, that is, training the source neural network with large-scale data in the remote cloud, loading the pretrained neural network, customizing the predictive model, training the predictive model with small-scale data in the edge, and offloading tasks to appropriate edge servers. First, the source neural networks are trained in large-scale data in remote cloud. Then an edge server loads the pretrained neural network from remote cloud. Assume the source model consists of early layers that learned low-level features and last layers that learned with specific features. Third, the pretrained network then transforms to a customized predictive model by employing new layers to learn features specific to the target domain data instead of the last layers, and then trains from the concerned edge server. Finally, we assess the service accuracy of the predictive model and offload the computing tasks of IIoT devices according to AMLC.


Figure 2.
A case study in IIoT edge intelligence using transfer learning. The edge server loads pretrained network from remote cloud, and then customizes its predictive model by replacing the last layers and train with the target domain data. IIoT devices offload to the appropriate edge server after assessing the service accuracy following AMLC.

Show All

In the training phase of the proposed framework, there are two information sources for the training model, that is, the large-scale training data at the remote cloud to attain the pretrained neural network, and the small-scale training data at the edge server to customize the predictive model in the transfer learning. In the offloading phase, we need to know the particular delay and accuracy requirements of a certain computing task, the heterogeneous capability and congestion condition of the edge servers, and the network topology. In addition, time-consuming network training occurs before the IIoT devices request the services. Therefore, the mobile learning network deployed on the edge server meets the real-time requirements of IIoT devices well.

Experiment Settings
In order to develop lightweight machine learning technologies on edge servers in IIoT, transfer learning is adopted. Transfer learning is a popular approach in deep learning where pre-trained models are used as the starting point to learn a new task. Using transfer learning to fine-tune a pretrained network significantly reduces the computational complexity of the training phase of CNN from training the network from scratch.

The simulation runs on NVIDIA TITAN V GPU. We use the pretrained AlexNet CNN in MATLAB. In the training phase, we use a total of 13,000 images in 10 categories from the ILSVRC 2012 data set, with 1300 images in each category, of which 1000 images were used as training data and the remaining 300 images were used as test data. The learning rate was set to 0.006. Batch size is 1250 and epoch was set to 1. In the simulation, we consider the edge network, consisting of 10 edge servers and 100 IIoT MDs randomly deployed in an application area with a radius of 200 m. On the edge servers, AI-enhanced applications are run with different service accuracy. IIoT MDs require computing tasks from edge servers with specific delay requirements. Only edge servers that meet the delay requirements can provide services to MDs.

Numerical Results
Figure 3 depicts the service accuracy over the varying training set size. With the increase of training data size, the classification accuracy also increases. Under a relatively small amount of training data, the accuracy increases proportionally to the amount of training data. But as the amount of training data continues to grow, service accuracy begins to increase slowly. This shows that at the edge layer, the customized predictive model on edge servers differs from each other due to different local data, even when the pretrained networks are the same. The direct result is the heterogeneity of the edge servers in terms of served application and service accuracy, which has impacts on the offloading framework.

Figure 3. - Accuracy varying with different sample size.
Figure 3.
Accuracy varying with different sample size.

Show All

Figure 4 depicts the average accuracy over the varying number of MDs under delay constraints between AMLC and that not considering accuracy (i.e., MLC). As can be seen, the average accuracy of the proposed AMLC is higher than that of MLC. This is because considering accuracy will lead the offloading tasks to the edge servers with higher service accuracy. Note that the performance improvement is higher when the number of IIoT MDs is small, because in this case the edge servers with high accuracy get crowded, and the other MDs will turn to the edge servers with less accuracy.

Figure 4. - The average accuracy of computing tasks using AMLC and MLC.
Figure 4.
The average accuracy of computing tasks using AMLC and MLC.

Show All

For the sake of comparison, we introduce a classic offloading algorithm, AI-centric cloud computing (ACCC). ACCC is a task offloading algorithm where tasks that satisfy the delay and energy constraints are randomly assigned to the AI service deployed on the remote cloud for processing. Actually, ACCC is a variant of the algorithm in [15], which is more adaptive to compare with our proposed algorithm. The served traffic refers to the total traffic that MDs are served from the networks within a unit time period. As can be seen from Fig. 5, AMLC through AI-enabled computing service could serve more traffic from MDs as compared to that in ACCC when AI-enabled computing service is centered at the remote cloud. This is because AI-enabled edge servers could effectively process the delay-intolerant traffic at the edge of networks.

Figure 5. - The served traffic of MEC using AMLC and ACCC.
Figure 5.
The served traffic of MEC using AMLC and ACCC.

Show All

Discussion
Through the above experimental analysis, we find it feasible to deploy machine learning applications to edge servers and employ service accuracy as a metric in the traffic offloading of MEC, while it also faces many challenges, such as the storage of training and test data, model training, and parameter updates. These issues are not covered in this article but are unavoidable problems in the training process of machine learning applications. Intuitively, training and testing models can take a lot of time for large-scale machine learning applications. This process can be done in the remote cloud as long as the trained parameters are passed back to the edge servers, and the storage of data can also be migrated to the remote cloud. The final model of deep learning does not require many resources, which is why deep learning applications can be deployed on edge servers. The computation-intensive traffic will be processed at the edge of networks.

In this work, we assume the AI-enabled computing resource has been deployed in networks. Actually, in AI-enabled edge computing, it is also challenging to appropriately tailor the AI-based computing service to trade off between accuracy and the constrained computing resources. According to the computing capability of edge servers and the demands from MDs, how to cache different computing applications and prune the computing service among different edge servers needs serious consideration.

Conclusions
In this article, we have proposed an intelligent computing architecture in IIoT with cooperation between edge servers and remote cloud. Then we have proposed an AI-driven offloading framework considering service accuracy as a new metric, intelligently disseminating traffic to edge servers or remote cloud. A case study was performed utilizing transfer learning in image recognition in IIoT. In future work, we will appropriately tailor the AI-based computing service to trade off between accuracy and the constrained computing resources. According to the computing capability of edge servers and the demands from MDs, allocating caching and computing applications among different edge servers will be jointly considered.