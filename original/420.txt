Computing in the network (COIN) exploits the sparce computing power of network nodes to offload applications' computations. This paradigm benefits computation-demanding applications, such as source separation for acoustic anomaly detection. However, wider adoption of COIN has not occurred due to intertwined challenges. The monolithic design of the source separation algorithms and the lack of a flexible transport layer in COIN hinders its exploitation. This article presents network joint independent component analysis (NJICA), leveraging COIN to recover original acoustic sources from a mixture of raw sensory signals. NJICA redesigns the monolithic algorithm for source separation into a distributed one to unleash the offloading capability to an arbitrary number of network nodes. Furthermore, NJICA develops a message-based transport layer that allows aggregating application data at network nodes and differentiating message types. Extensive evaluations of the practical implementation of NJICA using a realistic dataset shows that NJICA significantly reduces both the computation and service latencies.
Introduction
Influenced by the demands to decrease capital expenditure (CAPEX) and operation expenditure (OPEX), communication networks have transformed toward a programmable infrastructure, leveraging network function virtualization (NFV) and software defined networking (SDN). NFV allows network functions running on commodity hardware to benefit from mature virtualization technologies, such as virtual machines and containers. SDN orchestrates multiple routers simultaneously from a centralized controlling entity. The interaction between NFV and SDN allows allocating and orchestrating a chain of network functions (called a service function chain, or SFC) since practical applications often require several network functions, including firewall and packet inspection, at the same time. Thus, the spare computing capability of the NFV infrastructure can be used for computation offloading. This transition has given rise to the concept of computing in the network (COIN), the integration of in-network computation and network processing in a common framework. COIN can benefit many verticals, including the Industrial Internet of Things (IIoT).

One of the central building blocks in IIoT is collecting data from embedded sensors, analyzing, and responding quickly to devices' abnormal behaviors to maintain uninterrupted operations. Using acoustic signals for anomaly detection offers significant gains. Compared to temperature and humidity signals, acoustic ones allow detecting incidences early. Compared to image analysis techniques, distorted acoustic signals often emerge before an abnormal event. Additionally, audio-based detection methods can avoid blocking and angular distortion problems. Audio-based anomaly detection requires separating source signals from a mixture of collected audio signals and has three main characteristics. First, the observed raw sensory acoustic signal is inevitably altered (mixed) due to multiple objects' presence and operation. Second, there is significant interference from different objects, affecting the detection accuracy of a particular object. Third, the communication and computation demands are massive to deliver and process the collected raw data [1]. There are a large number of deployed sensors in a typical IIoT application. Their acoustic signals are often collected continuously at rapid sampling rates for extended periods.

Potentially, source separation applications can benefit from a COIN-capable infrastructure by offloading some of the computation to COIN. However, there are significant challenges when combining a source separation application with COIN. All algorithms for source separation have a monolithic design, which is only suitable for centralized computation. To exploit the offloading capability of COIN, source separation algorithms require a modular design that enables independent execution of computing functions. Next, source separation typically computes a large amount of data that far exceeds the caching capacity of routers. Last but not least, current COIN frameworks lack flexible transport with aggregation and differentiation functionalities. Therefore, to facilitate data-intensive applications such as source separation, COIN needs to provide a flexible transport layer. Such a layer has to allow COIN's network nodes to tap into the application's data flow to perform computing functions.

In this article, we introduce network joint independent component analysis (NJICA), leveraging COIN to recover original acoustic sources from a mixture of raw sensory acoustic signals. First, NJICA designs a distributed computing model of the source separation algorithm, which enables offloading the computation to an arbitrary number of network nodes. Second, NJICA introduces ahead-of-time data extraction to avoid communication bottlenecks. Last but not least, NJICA develops a message-based transport layer, which allows aggregating application data at network nodes and differentiating message types. The transport layer minimizes the latency to the forwarded traffic flows. We extensively evaluate the practical implementation of NJICA using a realistic dataset. Results show that NJICA reduces the computation latency drastically up to two orders of magnitude while imposing a negligible increase in the overall service latency.

Even though we design NJICA for one particular blind source separation (BSS) algorithm, NJICA can benefit other works in many ways. The family of independent component analysis (ICA) algorithms can benefit from NJICA's modular and distributed design. Extracting data is crucial when offloading computation needs a large chunk of data. The offloading functions need to operate only on the extracted data. Second, a terminating criterion is essential for each offloading function to fine-tune its performance while preventing overload of COIN resources. A generic application can also benefit from the message-based transport layer of NJICA. It is the enabler to tap into application data, enabling computation at network nodes. To simplify the deployment of other applications on COIN-enabled infrastructure, we release NJICA as an open source library, which is available at [2].

The rest of the article is structured as follows. The following section introduces fundamental principles in acoustic source separation and discusses state-of-the-art methods. Following that, we describe NJICA's architecture and implementation in detail. Then we cover the evaluation and discuss the results. Finally, we conclude the article and sketch directions for future work.

Background and Related Work
This section briefly reviews the background knowledge of BSS for a mixture of acoustic signals. After describing respective algorithms for BSS, we discuss the open challenges when offloading such algorithms to leverage COIN. To clarify notations and their meaning, we summarize all notations used in this and subsequent sections in Table 1.

Blind Source Separation
BSS [3] is the process of recovering source signals from the mixture of observed ones, such as recorded sounds of machines operating simultaneously in a factory hall. The separation is required to understand each source individually, for example, for acoustic anomaly detection.

Assume that k acoustic sensors record acoustic signals from n sources S∈Rn×m. The observed mixture of signals at each sensor, throughout m time slots, is the addition of n acoustic components from sources over that period. These n components of sources are statistically independent and non-Gaussian distributed. The observed mixtures, denoted as X∈Rk×m, at each time instant is the linear combination of the sources, the mixing process is presented as a mixing matrix A∈Rk×m. The size of X is usually enormous because there are often tens of thousands of samples per second for each acoustic signal. Furthermore, a typical Internet of Things (IoT) setup consists of numerous k of sensors, which increases further the length of observed data and the mixing matrix's size. Even though the number of observers k can be more significant than the number of sources n to perform BSS, k should be equal to n for two reasons. First, a minimum number of observers is resource-efficient for both transmission and computation. Second and more importantly, the n×n square matrix is the smallest possible n-rank matrix to invert A [4]. The task of the given BSS in Fig. 1 is to estimate the source signals by multiplying the observed mixtures with a separation matrix W, which is an inverse of the mixing matrix.

Figure 1. - Model of BSS with the sampling in $m$ time slots.
Figure 1.
Model of BSS with the sampling in m time slots.

Show All

Table 1. Notation.
Table 1.- Notation.
Independent Component Analysis
One of the well-known methods to estimate the separation matrix W is ICA. Conventional ICA algorithms (e.g., FastICA [4]) maximize the non-Gaussianity of observed data to separate the mixtures. Newton's iteration can approximate the process of solving the non-Gaussianity maximization. Round-by-round Newton's Iteration updates the separation matrix toward an optimal point resulting in correct separation of source signals. Consequently, ICA-based algorithms are complicated and computation-intensive.

Adaptive extraction-based ICA (AeICA) reduces traditional ICA algorithms' computation demand. AeICA [5] accelerates the separation with high accuracy by two main improvements. First, it exploits convergence tolerance, guaranteeing reliable separation quality. Second, AeICA uses an adaptive extraction distance to increase the amount of extracted data after each iteration. One step of Newton's Iteration only needs a subset of the original data. A partial Newton's Iteration reduces the need for computing resources and avoids transmitting raw data in the network as it extracts data locally.

Related Work
Distributed ICA
None of the ICA algorithms can exploit distributed computing, let alone leverage computing in the network capability. They have a monolithic design and therefore require centralized execution. The initial extraction distance is highly dependent on the input data (i.e., the observed mixing matrix). For each different input data, the whole separation system's parameters have to be changed individually. There is a tight coupling between the source signals as input data and Newton's Iteration computation. One of the challenges to computing ICA algorithms in a distributed manner is that each subset's data extraction distance always depends on the previous iteration, which leads to a reduction in collaboration efficiency.

Computing in the Network
State-of-the-art COIN systems [6]–[7][8] process a few IP packets at a time, which are the data units of routers. Routers operate at the network layer and process individual IP packets with a minimal buffering capacity. When the computation requires a large amount of data, existing COIN systems face several challenges. Each processing unit operates on the observed mixture matrix whose size usually is two orders of magnitude larger than the maximum transmission unit (MTU) of IP packets. The processing needs to collect stateful flow IP packets for this matrix to perform ICA algorithms. This requires transport-layer functionalities, which are missing in state-of-the-art COIN systems. Subsequent challenges include the modification of the end-to-end principle [9] for the traditional transport layer protocols. To reduce latency and overhead, COIN needs to minimize the amount of data to process at network nodes. For the data that require processing, COIN needs to handle the transmission at the transport layer itself.

All in all, there are two main unsolved challenges for offloading the computation of ICA algorithms to COIN systems. First, ICA algorithms need a modular design for distributed computing to leverage the offloading capability of COIN systems [10]. Second, the COIN systems need to provide a transport layer to tap into the flow of a large amount of input data [9].

Network Joint ICA
In this section, we present for the first time NJICA, a co-design between distributed computing and computing in the network with two innovations:

NJICA designs a distributed computing model of the centralized ICA algorithm. NJICA decomposes functional components of the algorithm into modules and deploys them as network functions.

NJICA develops a novel message-based transport layer to handle application data, allowing aggregating application data at network nodes and differentiating traffic types.

ICA as Network Functions
For source separation algorithms to work in a distributed manner, NJICA decouples ICA into three main functionalities, namely data extraction (DE), separation matrix estimation (SME), and data reconstruction (DR). Altogether, DE extracts a small fraction of the observed mixture and sends it to SME. Then SME uses the fraction to estimate the separation matrix. If the estimation result approaches the desired tolerance, DR starts reconstructing separated sources. Otherwise, SME requests DE to extract more data for the estimation. Figure 2 depicts the three main functionalities and their dependencies as a flowchart. In order to offload the computation of AeICA, we apply the divide-and-conquer approach. Toward this goal, we decouple the dependency between DE and SME.

Figure 2. - Decomposing AeICA into three sub-components: DE, SME, and DR: As an example, two sensors send the observed mixture $\mathbf{X}$ of two sources over eight time slots. Initially, the extraction distance $\mu_{1}$ equals four, meaning that DE extracts every fourth time slot from the mixing matrix to construct $_{\mu_{1}}\mathbf{X}$ and sends it to SME for newton's iteration. As the estimated separation matrix $_{\mu_{i}}\mathbf{W}$ does not meet the desired tolerance, SME asks DE for additional data. DE reduces the extraction distance by half, meaning to extract every second time slot from the mixing matrix. The process continues until the estimated separation matrix $\mathbf{W}$ satisfies the desired tolerance. Finally, DR reconstructs the separated sources from $\mathbf{W}$ and $\mathbf{X}$.
Figure 2.
Decomposing AeICA into three sub-components: DE, SME, and DR: As an example, two sensors send the observed mixture X of two sources over eight time slots. Initially, the extraction distance μ1 equals four, meaning that DE extracts every fourth time slot from the mixing matrix to construct μ1X and sends it to SME for newton's iteration. As the estimated separation matrix μiW does not meet the desired tolerance, SME asks DE for additional data. DE reduces the extraction distance by half, meaning to extract every second time slot from the mixing matrix. The process continues until the estimated separation matrix W satisfies the desired tolerance. Finally, DR reconstructs the separated sources from W and X.

Show All

Data Extraction
DE receives the mixture X observed from n sources throughout m time slots. The goal of DE is to downsample X and extract a minimum subset of X that permits successful estimation of the separation matrix. To converge quickly to an optimal set, DE exploits the following heuristic. Initially, DE starts with a substantially large subset and then increases the subset at a reasonable speed. We characterize this process by a parameter, namely the extraction distance μi, where i is the iteration number of the extraction operations. In each iteration i, DE extracts every μith column from X to form a new matrix. Initially, μi equals (m/n). As long as the extracted matrix does not meet the tolerance condition, DE performs a new extraction iteration and reduces μi+1 by half. DE terminates when the extracted matrix meets the desired tolerance, or the extraction distance reaches zero, meaning to take the whole mixture matrix X.

Separation Matrix Estimation
The goal of SME is to estimate the separation matrix W. SME applies Newton's Iteration on the extracted data from DE, denoted by μiX. SME terminates when the outcome of Newton's Iteration approaches the desired tolerance. Since the procedure to calculate the separation matrix W is computation-intensive and can take a long time, SME adds an intermediate step to check the reduction rate of gradient after each iterative operation. Only after sufficiently reducing the gradient of the intermediate separation matrix μiW does SME check its tolerance. Otherwise, SME continues with Newton's Iteration. We illustrate the process as the decision of Slow Gradient Reduction in Fig. 2 When the intermediate separation matrix μiW has a sufficiently reduced gradient, SME verities the tolerance condition. If μiW does not meet the desired tolerance, SME requests a larger μiX from DE and reiterates the process. As soon as μiW meets the desired tolerance, SME passes the found separation matrix W to the last step, reconstructing separated sources S^.

Data Reconstruction
DR is the final step of the ICA process and is responsible for reconstructing the separated sources S^. Therefore, the ICA process needs to execute DR only once. DR performs a matrix multiplication between the observed mixture X and the final separation matrix W. DR then outputs the separated source signals. The volume of DR's output data is significantly larger than that of its input data. DR has a relatively small computing demand.

Distributing Functional Components of NJICA
This process decides where to distribute which components of the ICA algorithm to maximize the benefit of computation offloading and to minimize the amount of transmitted data over the network. First, we decide not to offload DR since the benefits are marginal otherwise. DR has a weak demand for computing and remains at the server. This also means localizing DR's output, which is typically large, to decrease traffic demand and transmission delay. Second, to reduce the computing load on the server significantly, NJICA allows offloading of DE and SME, which are computing-intensive and require multiple executions. NJICA allocates each pair of DE and SME to the same network function due to their frequent interactions. To fine-tune the offloading workload, we define δ as the maximum number of executions between DE and SME within each network function. Last but not least, NJICA implements the so-called ahead-of-time data extraction feature. DE can always extract the next subset from the observed mixture matrix, even before SME requests, given an initial extraction distance as the input. This allows executing DE and SME virtually in parallel, minimizing the processing delay.

Computing NJICA in the Network
To incorporate data-intensive applications into the COIN paradigm, NJICA introduces two key innovations, namely data differentiation and a message-based transport layer.

Data Differentiation
To enable parallel processing at network nodes, NJICA needs to forward the received data immediately without waiting for intermediate results at each node. Along the data stream from the client to the server, each network node may tap into and process the data, and forward the intermediate results to the next node down the stream. To decouple the transmission of extensive data X and intermediate result μiW, NJICA uses tags to differentiate two types of messages, one for the observed mixture X and the other for intermediate results μiW. Figure 3 illustrates the fragmentation process. The client sends messages to the server encapsulating unprocessed data. They traverse, along the way, a chain of virtualized network functions, being interconnected and coordinated dynamically by an SDN controller, and span one or more physical network nodes. Each node performs multiple iterations of DE and SME operations. Since the processing only requires the intermediate result μiW and the extracted data μiX, network nodes fast-forward all messages carrying the data of X downstream. At the same time, the network node keeps a copy of X for DE and SME to compute μiW and then sends the results to the next network node.

Figure 3. - Computation offloading in NJICA: DE and SME can run as network functions on network nodes, while DR remains entirely on the server.
Figure 3.
Computation offloading in NJICA: DE and SME can run as network functions on network nodes, while DR remains entirely on the server.

Show All

A Message-Based Transport Layer
This layer is responsible for delivering application-level data through the service function chain. In NJICA, each network function works on the transport layer for data collection, processing, and forwarding. In a BSS application, the size of X is considerable even when the number of sources n and the signal duration is small. Therefore, the transmission of X requires fragmenting X into multiple IP packets. To simplify the packet processing logic on the network nodes, we employ a message-based protocol for NJICA. Message-based data packaging is a reasonable and flexible solution to deliver a wide range of application data. Network nodes can work cooperatively to reduce the end nodes' workload based on each message's metadata. Subsequently, NJICA encapsulates each message chunk into a UDP datagram. Each has a specific preamble header containing metadata information such as message type, message flags, message sequence number, chunk sequence number, chunk length, and current loop number. Network nodes exploit this metadata in the header to collect and reassemble X and μiW.

The network node can change the message flag depending on verification of its intermediate μiW. If it meets the desired tolerance, the network node changes the message flag to fully processed to inform the next node to fast-forward this μiW without any further processing. When the server receives the message of X and its corresponding μiW. it processes them based on the message flag of μiW. If data processing fully completes, the server only executes the DR function. Otherwise, it needs to perform more iterations of DE and SME functions from the partially processed data from the network.

Since NJICA involves assembling and processing data packets in batches, we leverage Data Plane Development Kit (DPDK) version 19.11 to implement network functions (NFs). DPDK bypasses the kernel space to avoid costly data-copying operations and increase packet processing. Note, however, that DPDK's alternative data plane processing technologies (e.g., XDP [11]) are eligible, too.

Performance Evaluation
This section compares, side by side, NJICA to conventional cloud computing in offloading capability and the cost introduced by leveraging COIN. We first describe the metrics, the experiment setup, and parameters.

Metrics
To measure the computation load at the server, we introduce the computation latency metric. It is the duration the server needs to compute until the source separation completes. Low computation latency means that the network nodes already offload a large portion of the computation. Thus, the remaining computation load is small. On the contrary, significant computation latency means that the server still needs to compute a large portion of the workload.

To assess the quality of service (QoS) of the NJICA service, we introduce the second metric, which is the service latency observed at the client. The metric measures the time difference between a request message sent by the client and its respective response from the server received at the client. The service latency includes both transmission and computation delays introduced by network nodes and the server. The metric directly addresses the concern about the overhead introduced by offloading the computing among network nodes. Together with the computation latency, the service latency allows us to assess both the benefit and cost of NJICA collectively.

Experiment Setup
We conducted an evaluation on the network emulator, Communication Networks Emulator (ComNetsEmu) [12]. The unique advantage of ComNetsEmu is its capability to provide functionalities of both NFV and SDN. It allows quick prototyping of COIN applications virtually at any scale. As illustrated in Fig. 3, we set up a multihop topology with routers as intermediate network nodes. Each router can perform one or both operations of forwarding and data processing. A client connects to the network and sends the observed mixtures X using UDP traffic to the server. All topology links have the same homogeneous bandwidth of 1 Gb/s and a fixed propagation delay of 10 ms. These specifications are typical for commercial off-the-shelf (COTS) devices at the network edge. The purpose is to examine the benefits of offloading in COIN without superior specifications of high-end infrastructure. We also assume that all devices and connections are general-purpose and available as COTS. We implement the client and server functions in Python. For each configuration of a parameter set, we performed 100 measurements in which the client sends the mixture matrix to the server. We calculated the mean results together with their 95 percent confidence intervals. Each network node forwards packets to the server at the fastest possible store-and-forward speed. When operating in the COIN-enabled offloading mode, the SDN controller inserts rules into the flow table of each router to forward the data traffic through each network node to build an SFC.

To compare the proposed NJICA with conventional ICA schemes, we implemented and deployed FastICA [4] and store-and-forward. While FastICA is the state-of-the-art centralized ICA algorithm with a monolithic design, store-and-forward factorizes the internal functionality of the ICA algorithm to achieve a modular design. We deploy FastICA and store-and-forward using a conventional cloud computing model without leveraging COIN.

All source code in our emulation is publicly available [2]. We deploy the emulation on a single COTS server with an i7-6700T CPU with 16 GB RAM running the Ubuntu 18.04 LTS operating system.

Workload and Parameters
In our evaluation, we use the collected dataset Google Audio Set [13]. This large-scale and widely used dataset enables us to cover diverse audio data types. We vary the number of sources n between 2 and 12. For each value of n, we randomly select n sources from the dataset to construct the source signals S. We mixed the source signals S following the standard normal distribution to simulate as many mixing scenarios as possible. By doing that, our evaluation covers various data types and mixing cases.

We examine two scenarios of the server's workload, namely a moderate workload and a heavy one. In a moderate workload scenario, the server has 100 percent of its computing power to fulfill clients' requests, whereas in the heavy workload scenario, the server has only one-third of its computing capacity. The two workload scenarios allow us to understand in which circumstance NJICA provides the most benefits for offloading.

Results and Discussions
In this section, we compare the computation latency on the server tc and the service latency ts on the client between NJICA, store-and-forward, and state-of-the-art FastICA.

Impacts on Computation Latency
Figure 4 depicts the impacts of the offloading parameter δ and the computation workload represented by the number of source signals n to the computation latency. When the amount of input data is small, such as with two sources, the computation latency of both NJICA and store-and-forward is equally low. As the amount of input data increases, as a result of the increased number of sources n, the computation latency increases for both store-and-forward and NJICA. However, the computation latency increases faster for store-and-forward. For particular configurations, such as when δ=4, the computation latency of NJICA is between one or two orders of magnitude lower than that of store-and-forward. Within NJICA, δ has significant impact on the computation latency. As expected, larger δ tends to reduce the latency more drastically since network nodes already offload a large portion of the total computation workload. When δ is small, network nodes offload only the few first executions of Newton's Iteration process, which tends to be a light workload. Therefore, the impact of offloading on the computation latency is insignificant.

Figure 4. - Computation latency on the server, with the setups of different available computing resources of each network node.
Figure 4.
Computation latency on the server, with the setups of different available computing resources of each network node.

Show All

For all workloads, NJICA shows better computational performance than FastICA. For the case without any computational offloading, NJICA takes only half the computation time on average compared to FastICA.

All in all, the evaluation results show that NJICA reduces the computational demands of BSS significantly on the end server, especially when the dataset is large.

Impacts on Service Latency
We would like to understand the impact of offloading on the overall service latency of the client under the heavy workload condition. For comparison, Fig. 5 plots the service latency with regard to the number of sources n for NJICA, FastICA, store-and-forward, and No Compute. As its name implies, No Compute depicts the service latency when there is no computation on the client's data. As expected, the service latencies of all schemes increase significantly as the number of sources grows. More sources means a more extensive mixture of data, which requires more time for fragmenting data at the client, transmitting through several network nodes, and finally reassembling the data at the server. As the number of sources increases from 2 to 12, the service latency increases from around 500 ms to approximately 4200 ms. As emphasized earlier, BSS is a typical computing-intensive data analysis task. Even on a dedicated machine, its processing latency can often reach several seconds (e.g., as measured in [5]). Compared to conventional FastICA and store-and-forward, NJICA presents more negligible service latency when the source number is greater than 4. Compared to FastICA, NJICA can reduce the service latency by about 65 percent (2836 ms vs. 8153 ms) in a heavy workload scenario with a source number of 12. For this scenario, NJICA presents better latency performance by about 32 percent (2836 ms vs. 4142 ms) compared to the store-and-forward approach. The results show that NJICA can significantly reduce the service latency for heavy workload scenarios.

Figure 5. - Service latency on the client, with the setups of different available computing resources of each network node.
Figure 5.
Service latency on the client, with the setups of different available computing resources of each network node.

Show All

Conclusion and Future Work
This article presents NJICA, a joint network computation system to separate acoustic source signals from a recorded mixture. NJICA leverages COIN to offload the computation workload to network nodes along the transmission path. To that end, NJICA introduces decentralized computation modules and redesigns the ICA algorithm into a distributed computing model. Second, NJICA introduces a message-based transport layer. It categorizes and tags data flows, which allows dispatching them for either computing or forwarding. NJICA can offload computation to an arbitrary number of network nodes. Additionally, the ahead-of-time data extraction feature allows parallel computing to further reduce computation latency. The practical implementation evaluation using a realistic workload demonstrates that NJICA significantly reduces computation latency compared to the conventional cloud computing approach. Our results are promising, leading us to believe that other communication-intensive and computation-demanding applications can also benefit from COIN.

NJICA benefits other research work in many ways. The family of ICA algorithms can benefit from a modular design. Extracting data is crucial for COIN leveraging applications whose offloading computation needs a large chunk of data. The offloading functions need to operate only on the extracted data. Second, a terminating criterion is essential for each offloading function to fine-tune offloading performance while preventing COIN resources from being overloaded. To benefit a broad range of applications, a message-based transport layer is an enabler to tap into application data, enabling computation at network nodes.

One immediate direction for future work is to explore the performance bound of the family of ICA algorithms on COIN. The study requires the development of a mathematical model as well as a more comprehensive parameter study. The second potential research direction following NJICA is to support a wider variety of applications, which requires a more generic but sophisticated transport layer. It requires protocols for loss recovery, congestion control, and addressing.