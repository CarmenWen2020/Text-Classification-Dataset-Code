Volumetric video (or hologram video), the medium for representing natural content in VR/AR/MR, is presumably the next generation of video technology and a typical use case for 5G and beyond wireless communications. To realize volumetric video applications, efficient volumetric video streaming is in critical demand. This article responds to the challenges of and proposes solutions to wireless transmission systems of point cloud video, which is the most popular and favored way to represent volumetric media and significantly differs from the other types of videos. In particular, we first introduce point cloud video technology and its applications, and then discuss the challenges of and solutions to point cloud video streaming, including encoding, tiling, viewing angle prediction, decoding, quality assessment and transmission optimization. Furthermore, we explain a prototype of a MPEG DASH-based point cloud video streaming system as a preliminary study, along with more simulation results to verify its performance. Finally, we identify future research directions for providing high-quality point cloud video streaming.
Introduction
With the capability of providing an immersive viewing experience and low-latency high-bandwidth network requirements, virtual reality (VR) video [1] has become the quintessential application of 5G wireless communication and has been widely studied in both academia and industry. However, VR video users are unable to move forward or backward freely, which degrades the immersive viewing experience. Volumetric video (or hologram video) can conquer this limitation by providing users with a six degrees of freedom (6DoF) immersive viewing experience, that is, users can freely move forward/backward (surging), up/down (heaving), or left/right (swaying) to select the favorite viewing angle of the 3D scene, and hence enjoy three more degrees of freedom in comparison with 3DoF VR video users.

By enabling viewing from different angles, volumetric video can be widely used in many areas, including education, healthcare and industry, as shown in Fig. 1, and thus becomes a research hotspot. It is expected to be the next generation of video technology and is considered a typical application of 5G and beyond wireless communications. The global holography market for industrial applications alone is expected to reach 22.5 billion USD by 2024 [2]. Note that the pandemic of COVID-19 draws further research interests on new holographic technique and its applications, and the market is expected to further expand. To further enhance volumetric video applications, networked volumetric video is in critical demand, which allows users to untether themselves.

Figure 1. - Examples of typical point cloud video applications.
Figure 1.
Examples of typical point cloud video applications.

Show All

However, transmitting volumetric video over the current wireless network is challenging due to the lack of transmission-friendly encoding, adaptive transmission methods, as well as effective quality metrics and accurate viewing angle prediction. In general, volumetric media can be represented in an image-based manner (e.g., light fields) or point cloud videos according to the adopted capturing and processing methods. As the most popular and favored representation of the volumetric media, point clouds consist of 3D points, each with multiple attributes, such as coordinates and color. The focus of this article is to discuss the challenges of and solutions to point cloud video streaming and to provide a prototype of a MPEG Dynamic Adaptive Streaming over HTTP (DASH)-based point cloud video streaming system as a preliminary study. As shown in Fig. 2, a typical point cloud video streaming system mainly consists of cameras (capturing the scene of interest from different angles simultaneously), servers (generating the point cloud video from these captured videos and encoding/compressing the generated point cloud video), base stations or access points (allocating the available network resources to efficiently deliver the video content to the users), and users (receiving, decoding and watching the video content).

Figure 2. - Illustration of a typical point cloud video streaming system and the inherent tasks. In particular, the scene is captured by multiple RGB-depth or RGB cameras from different angles simultaneously. These cameras are connected with a server that generates the point cloud video from the captured videos. The point cloud video is then encoded/compressed for storage. The server is connected with a base station (or access point) using a cable with sufficient bandwidth. The base station predicts the viewing angle of the user and correspondingly manages the network resources to optimize the transmission performance (with/without the support of transcoding, which could generate the video at a lower rate). The wireless communications can be based on cellular (e.g., 5G) or WiFi networks, with limited bandwidth and dynamic channel conditions. Each client first buffers the video, then decodes it for playback and recovers the missing packet if required.
Figure 2.
Illustration of a typical point cloud video streaming system and the inherent tasks. In particular, the scene is captured by multiple RGB-depth or RGB cameras from different angles simultaneously. These cameras are connected with a server that generates the point cloud video from the captured videos. The point cloud video is then encoded/compressed for storage. The server is connected with a base station (or access point) using a cable with sufficient bandwidth. The base station predicts the viewing angle of the user and correspondingly manages the network resources to optimize the transmission performance (with/without the support of transcoding, which could generate the video at a lower rate). The wireless communications can be based on cellular (e.g., 5G) or WiFi networks, with limited bandwidth and dynamic channel conditions. Each client first buffers the video, then decodes it for playback and recovers the missing packet if required.

Show All

The significant difference between the point cloud video data structure and traditional video data structure distinguishes the point cloud video streaming from the usual video streaming. Research on point cloud video systems has focused on point cloud compression [3], [4], with the goal of optimizing the coding efficiency so as to achieve low source rate, which, however, results in high complexity. Academia and industry (such as the Fraunhofer Institute for Telecommunications) have also made great efforts on point cloud video streaming solutions. These recent point cloud video streaming schemes [5]–[6][7][8][9] are based on the VR video streaming approach [1]. In other words, these schemes do not fully exploit the point cloud video features, which degrades the system performance. For example, the high decoding complexity is not fully considered in these schemes. This brings huge computation burden to the user devices. Overall, research on point cloud video streaming systems is still in its infancy and requires much effort to investigate the unknowns.

This article responds to the challenges of and proposes solutions to point cloud video streaming. In particular, we first introduce point cloud video and its applications, and then discuss the challenges of and solutions to point cloud video streaming from the aspects of encoding, tiling, viewing angle prediction, decoding, quality assessment and transmission optimization. Furthermore, we propose a DASH-based point cloud video streaming solution and prototype it as a preliminary study, along with more simulation results to verify its performance. Finally, we identify future research directions for providing high-quality point cloud video streaming.

Characteristics, Challenges and Solutions
This section discusses the characteristics and challenges of point cloud video streaming systems, followed by alternatives to solve these inherent technical challenges.

Characteristics
Point cloud video is generated by synthesizing the videos captured simultaneously by multiple RGB-depth cameras or RGB cameras located at different angles; the cases with RGB cameras require extra processing to estimate the depth maps. A denser camera setup results in a higher point cloud video quality. As a typical example, to capture the point cloud video sequence Longdress, shown in Figs. 1 and 2, 14 clusters of 42 RGB cameras are used, each as a logical RGB-depth camera at 30 frames per second. This sequence uses a spatial resolution with 1024 x 1024 x 1024 voxels as a cube, within which only the surface voxels are taken. These voxels are characterized with three colors, that is, red, green and blue. Each cube is adjusted as small as possible to contain the whole video according to its size [10].

Point cloud video enables a 6DoF immersive viewing experience and is very different from the other types of videos in many aspects, including the data volume, encoding, decoding, delay tolerance and inherent key research issues. As shown in Table 1, a point cloud video streaming system requires an extremely high transmission rate (at the Gb/s level [10]), which may sometimes surpass the capacity of a 5G wireless communication network. Moreover, the encoding tools (e.g., MPEG-VPCC-TMC2) for point cloud video are very different from those for other types of videos. These tools encounter very high encoding and decoding complexity, which is one of the critical issues in implementation. In addition, the 6DoF immersive viewing experience and high degree of user interaction require even lower network latency than a VR video streaming system. These characteristics bring new and unique challenges for designing point cloud video streaming systems.

Technical Challenges and State-of-the-Art Alternatives
These aforementioned characteristics lead to unique research issues. Next, we discuss these research issues and alternatives to solve each of them.

Point Cloud Video Encoding
Point clouds consist of 3D points, each with multiple attributes, such as coordinates and color. As the current transmission and storage technologies face great pressure caused by the large raw point cloud video frames, efficient encoding and compression of point cloud videos thus are becoming vital [10] and are attracting great attention from both academia and industry. ISO/IEC MPEG is also working on point cloud video encoding for its international standard. To the best of our knowledge, two groups of point cloud encoding methods are usually adopted. Based on the different distributions of point cloud data, we tend to use well-established 2D video technologies to project the points into 2D frames for those with uniform distribution in 3D space, while we allocate those with a sparse distribution into hierarchically structured cubes and encode each point as an index of the cube [9].

Note that the existing point cloud video encoding methods require higher computation complexity than the methods for traditional video encoding [6], [7]. On the one hand, this issue poses a new computation constraint on the point cloud video streaming system design and might even require mobile edge computing or cloud computing to provide extra computation capability for encoding. On the other hand, this issue affects the point cloud video encoding design. Thus, designing a computationally friendly encoding method becomes an important and urgent research issue. Another option is to parallelly render, encode, transport and decode the point cloud video to reduce the overall streaming latency. To further cope with adaptive and scalable point cloud video transmissions, efficient scalable encoding and multiple description coding for point cloud video are essential.

Table 1. Features of different types of videos [1], [3], [4], [6], [7], [10]. Here the low or high data volume is compared with a 2D video, which has a higher data volume than normal data. The delay tolerance is also compared with the other types of videos. Note that VR video is usually projected into 2D video and then encoded using state-of-the-art 2D video codec such as HEVC.
Table 1.- Features of different types of videos [1], [3], [4], [6], [7], [10]. Here the low or high data volume is compared with a 2D video, which has a higher data volume than normal data. The delay tolerance is also compared with the other types of videos. Note that VR video is usually projected into 2D video and then encoded using state-of-the-art 2D video codec such as HEVC.
Tiling
Similar to VR video users, a point cloud video user may only watch a part of the whole video each time, and this viewing area is called the field of view (FoV). Instead of sending the whole video content, we may only transmit the FoV each time to avoid wasting precious communication resources. Note that unlike a VR video system, where the tiles play the same role, tiles in point cloud video streaming may have different impacts on the received video quality due to the different distances between the user and the scene.

To enable efficient streaming, the point cloud video needs to be partitioned into smaller segments by sacrificing the coding efficiency, and each segment can satisfy the FoV or partial FoV requirements and is called a tile. The most popular way to partition a point cloud video is through equal 3D tiling [5], [7], which evenly divides the whole point cloud video into cuboids, and each cuboid is one tile. However, uniform tiling may sacrifice the coding efficiency and ignore the video content itself and its associate semantics, where different parts are associated with different impacts on the user experience.

Viewing Angle Prediction
Predicting user behavior can serve users more effectively with limited computation and communication resources by avoiding transmitting unnecessary video tiles, providing more efficient video predownloading/buffering, and so on. Viewing angle prediction has been well studied in VR and 360 video streaming systems, especially machine learning-driven viewing angle prediction. However, user behaviors in point cloud video streaming systems become more complicated and difficult to predict due to the enriched 6DoF; that is, point cloud video streaming system users change not only the viewing angles but also the distances from the scene.

In general, the prediction can be classified into model-based schemes (e.g., linear prediction based prediction) and machine learning-based schemes (e.g., long short-term memory (LSTM) or gated recurrent units (GRUs) based prediction). Both require viewing logs, but unfortunately, the public dataset of user viewing logs for point cloud video streaming systems is still not available. Note that collecting the viewing logs is labor intensive, and generative adversarial network (GAN) may be useful for effectively increasing the dataset, which has been successfully used in increasing the quantity of the samples in other scenarios, such as indoor localization.

Hou et al. provide a pioneering study for the viewing angle prediction of such 6DoF systems [11]. This work considers head movements and body movements separately; an LSTM model is used for body motion prediction, and a multilayer perceptron model is used for head motion prediction with high precision. The authors generate viewing logs and achieve a better prediction performance than the baseline schemes. However, whether it is good to separate head motions from body motions is unclear, although this separation simplifies the prediction. Moreover, video information such as saliency has been used in VR viewing angle prediction and has boosted prediction accuracy. How this can help the viewing angle prediction in point cloud video system requires further study.

High Decoding Complexity
The high decoding complexity [6], [7] makes point cloud video streaming systems differ greatly from other video streaming systems, the decoding in which is trivial. The most straightforward solution to handling the high decoding complexity issue is to reduce the decoding complexity, which requires revisiting point cloud video encoding and decoding. Mobile edge computing is popular in 5G and beyond wireless systems, which can provide augmented computation capability and may become another option to satisfy the computation requirement by the decoding.

Li et al. [7] provide another alternative to solve the high decoding complexity, which transmits the uncompressed tiles at different quality levels in addition to compressed tiles. The uncompressed tiles do not require decoding, and thus, can reduce the computation requirement by the decoding. However, these uncompressed tiles consume more channel bandwidth, and trade-off between the user device's computation capability and channel bandwidth should be carefully conducted.

Quality Assessment
Quality assessment studies how to effectively measure video quality, and it directly affects how video streaming is conducted. Considering the different distances between the user and the scene and the different visual effects between the foreground and background, the existing quality assessment methods cannot be directly adopted in a point cloud video streaming system. Current quality assessment tools for point cloud video streaming systems are basically variants or extensions of counterparts from conventional approaches. In particular, the point cloud video PSNR can be calculated from the point-to-point distortion (if no corresponding point at the exact location can be found, the point at the nearest position is used instead) [12] or the angular similarity to measure the objective quality. This calculation is also supported by MPEG PCC quality metric software [13], which can compare an original point cloud video with an adapted model and provide numerical values for the point cloud video PSNR. We find that these schemes basically inherit the state-of-the-art quality metrics for traditional videos and do not fully consider the features of point cloud videos, especially the visual difference caused by the different distances between the scene and the viewer.

Another domain of quality assessment is quality of experience (QoE), which directly reflects how users perceive the video and is affected by many factors, including the video quality, the amount of quality level switches and stalls during playback. Different from VR video systems, in which each viewer only observes a part of the video each time, the point cloud video streaming system is more complicated due to the 6DoF. Viewers may enjoy a part of or the entire point cloud video according to the viewing angle and relative distances between the scene and the user. To the best of our knowledge, how to measure the QoE of point cloud video streaming systems is still in its infancy. An effective and efficient objective or subjective quality assessment scheme for point cloud video streaming systems, which takes the various features of the point cloud video into consideration, remains an open issue.

Optimizing Point Cloud Video Streaming
Most of the existing point cloud video transmission schemes [5]–[6][7][8] are extensions of the VR video streaming schemes [1]; that is, the point cloud video is encoded, stored and divided into smaller tiles at the server, and only tiles inside the user's FoV are transmitted under the motion to photon latency requirement. Optimization can be conducted following this method to optimize the defined objective function [5], jointly considering the point cloud video features (such as the high data rate and decoding complexity).

These optimization problems can be solved mathematically or using AI techniques. For example, [7] considers the high computational complexity of the point cloud video decoding and solves the inherent transmission optimization problem mathematically. Note that these are model-based schemes and are not adaptive to scenarios with dynamic network conditions, which are hard to be modeled. With the development of artificial intelligence, reinforcement learning has become increasingly popular in resource allocation for video streaming systems with satisfactory performance [14]. Reinforcement learning along with other similar approaches are capable of dynamically adapting its behaviors by interacting with the environment to optimize the received video quality. The difficulties of reinforcement learning-based point cloud video streaming optimization mainly include how to properly define the reward and how to efficiently train the reinforcement learning model to achieve good performance.

Moreover, point cloud videos have three modes of distributions: live transmission, on-demand transmission and telecommunication. Different distributions have different details (e.g., buffer management), and how these should be considered in transmission optimization is an important yet challenging research issue.

Prototype and Performance Evaluation
MPEG-DASH is an adaptive HTTP-based streaming solution that is an international standard. This section first overviews the DASH-based solution and explains the prototype with its experimental results. Then more simulation results are introduced to verify the performance of the proposed scheme.

Case Study: A Dash-Based Solution (Fig. 3)
Figure 3. - System architecture of the case study.
Figure 3.
System architecture of the case study.

Show All

System Overview
The DASH-based video streaming system of interest considers a server and a subscriber, as shown in Fig. 3 and Fig. 4. The server evenly partitions the point cloud video into 3D tiles and encodes each tile into different quality levels with different source rates. The compressed tiles are decoded at the server so that the uncompressed tiles at different quality levels are also available for transmission. The uncompressed tiles have larger source rates than the compressed tiles at the same quality level but do not require decoding at the user side, which releases the computation burden and serves as an alternative to solve the high decoding complexity issue. A media presentation description (MPD) file is then recorded for DASH-based point cloud video streaming. The MPD file includes the information on the file size, computation resource requirement for decoding, the number of points within a frame and the URL of each file.

Figure 4. - Overview of the prototype and the transmission performance.
Figure 4.
Overview of the prototype and the transmission performance.

Show All

The subscriber has an HTTP interface, a tile-reordering module, a decoding module, a fusion module, a tile selection module and a buffer. The tile selection module is the “brain” of the DASH-based point cloud video system. It calculates the tiles residing in the subscriber's FoV and selects the proper quality level for each tile to maximize the viewing experience. In this case study, the selection is formulated as an optimization problem to optimally allocate the available computation and communication resources according to the status of the subscriber's FoV, available bandwidth and computation resources. This optimization problem is a non-linear integer programming problem. After the continuous relaxation of discrete variables, the new relaxed problem can be solved easily based on KKT-conditions, and branch and bound method. The key point is that we take into consideration the user device's limited computation capability, which is insufficient for decoding, and thus include transmitting the uncompressed tiles to reduce the computation requirement for decoding at the cost of higher bandwidth consumption. Please refer to [7] for details.

Prototype and Results
The prototype of this DASH-based point cloud video streaming solution uses a laptop as a server and a PC as a client in the laboratory, as shown in Fig. 4. Another PC is used as the playback device and the same visual range parameters (43∘×29∘) with HoloLens2 are used. An off-the-shelf network router is used for wireless transmission with a bandwidth of up to 1200 Mb/s.

VS2017, PCL1. 9.1, and Qt5.12 are used to build a point cloud video player on the client. The MPEG officially recommended encoder VPCC-TMC2-v7 is used as the encoding and decoding tool for the server and player, respectively. An HTTP web server nginx1.16 is built at the server in order to realize the video transfer. The point cloud video sequence used is selected from Long-dress, which consists of 300 frames and each frame has about 780K points. Each 10 consecutive frames form a group of frames (GoF), which is divided into 2×2×3 tiles. The player buffers up to five GoFs before starting playback in order to avoid stalls,

Figures 4a-4e show the visual experimental results. In particular, Fig. 4a shows one view watched by the user, and Fig. 4c shows the corresponding original frame. Both figures demonstrate the video from the user's viewing perspective, and their quality levels are almost the same. With the aforementioned setup, the PSNR achieved by this system reaches up to 71 dB in terms of the point-to-point PSNR. We also exhibit the viewing angle in Fig. 4b and we can observe that only the tiles that intersect the cone (i.e., the tiles inside the FoV) are streamed to the client. These results show the effectiveness of the proposed solution. As a reference for comparison, the original video is shown in Fig. 4d.

Simulation Setup and Results
To further verify the performance of the core algorithm used in the aforementioned prototype, the following simulations are conducted with different network bandwidth conditions and numbers of CPU cores. The same Longdress with 300 frames is used, and each GoF contains 10 frames. The video is partitioned into 3×3×4 tiles, and each tile is encoded into five quality levels. A higher quality level is associated with a higher video quality, but requires more bandwidth resources. Both compressed versions and uncompressed versions are provided for each tile. The player buffers up to five GoFs before the user starts playing in order to avoid stalls. This article considers the baseline schemes, which only consider a compressed version or uncompressed version. These schemes are optimized to select the tiles with the most proper quality levels. The performance results are from the average of 20 trials.

Figures 5a-5c show the simulation results. In particular, Fig. 5a shows that the proposed scheme provides the highest video quality (highest average quality level) given that it can optimally utilize the merits of both compressed and uncompressed tiles. With the increase of CPU cores, there will be more available computing resources, which results in better overall performance by enabling the user to decode more compressed tiles. With larger bandwidth, our scheme achieves better performance by allowing transmitting more high quality level tiles. Figure 5b shows the number of freezes during the playback. The proposed scheme optimally uses the bandwidth and computing resources and thus has the fewest freeze. With more CPU resources available, the schemes considering compressed tiles have fewer freezes given the higher capability to decode the compressed tiles. With more bandwidth resources available, the schemes considering uncompressed tiles have fewer freezes given the higher capability to transmit the video sources at higher quality levels. Figure 5c shows the waiting time before the user can playback the video, where the proposed scheme provides the smallest waiting time. With more computing or network bandwidth resources, the waiting time decreases.

Figure 5. - Simulation results at different bandwidth conditions and CPU cores: a) Simulation results in terms of the average quality levels obtained; b) Simulation results in terms of the times of freeze during the playback; c) Required time before user can start watching the video; d) Average quality levels obtained with different network traces and buffer lengths. In (d), buffer = x means the buffer length limit is x GoFs.
Figure 5.
Simulation results at different bandwidth conditions and CPU cores: a) Simulation results in terms of the average quality levels obtained; b) Simulation results in terms of the times of freeze during the playback; c) Required time before user can start watching the video; d) Average quality levels obtained with different network traces and buffer lengths. In (d), buffer = x means the buffer length limit is x GoFs.

Show All

We also run more simulations using 4G/LTE bandwidth logs [15] with different buffer lengths, and the results are shown in Fig. 5d. Bandwidth logs at three scenarios (car, bus and tram) with different variations are selected, and scaled to have the same average bandwidth with Fig. 5a for comparison. Although the performance drops slightly compared with the case without bandwidth variation (as shown in Fig. 5a), the proposed scheme can still provide satisfactory performance. While different buffer lengths result in different performances, a longer buffer provides better performance as shown in Fig. 5d, especially when the channel bandwidth variation is large.

Future Research Directions
We envision that the point cloud video streaming system will play a vital role in future society by offering an enriched, 6DoF immersive viewing experience. This area opens up many exciting and critical future research directions.

Error-Resilient Point Cloud Video Encoding and Streaming
Point cloud video streaming is extremely sensitive to delays, and wireless communication is lossy in nature due to shadowing, channel fading and intersymbol interference. How to deal with transmission errors while maintaining smooth playback is vital. The possible solutions include error-resilient point cloud video encoding, source/channel coding for transmission, or even error recovery at the user end, which are still open yet very important research issues for error-resilient point cloud video streaming systems.

AI-Empowered Resource Allocation
Studies of resource allocation mainly solve how to effectively utilize the network resources for a high-quality viewing experience. In particular, physical-layer resource allocation studies the modulation and coding scheme selection, transmission power and time allocation, subcarrier allocation in OFDMA systems, and so on, to maximize a defined objective function such as optimizing the received video quality in terms of the PSNR or minimizing the power consumption. A point cloud video streaming system brings extra constraints (such as computation) and variables, and makes resource allocation more complicated.

In addition, most existing resource allocation schemes are mode based. How to use AI techniques to design dynamic resource allocation methods for point cloud video streaming systems, accounting for the computation constraint and other point cloud video features, is an interesting topic for future studies.

Performance Boost with Edge Computing
Point cloud videos are dense in terms of the number of points and require more computation resources and time for encoding, transcoding and decoding than traditional videos. Low-latency computation capability hence becomes important in a high-quality point cloud video transmission system. As edge computing has already greatly accelerated the development of conventional video streaming, it can serve as an alternative to provide the required computation capability with low latency. The inherent research issue is how to explicitly use edge computing to boost point cloud video streaming, including where to place the edge server and when and how to offload the tasks.

Point Cloud Video Streaming in Various Types of Networks and Scenarios
Point cloud video is expected to be used in various scenarios, for example, in vehicular ad hoc networks (VANETs) or information-centric networking (ICN). How to design point cloud video transmission to cope with the features of these scenarios to promote transmission performance is still open. As an example, for point cloud video streaming in VANETs, the driving routing, relative distances between neighboring vehicles and network infrastructures, and user behaviors can be jointly investigated to improve the transmission performance.

Moreover, social media platforms such as TikTok and YouTube have become increasingly popular and bring new requirements and opportunities. These live-streaming systems have more stringent delay, real-time encoding, decoding and transmission scheduling requirements. How to efficiently serve point cloud video users in these services remains unsolved. Moreover, these systems have strong connections with social networks, exploring which could provide performance enhancement.

Conclusion
Point cloud videos enable 6DoF viewing experiences and are expected to be the next-generation video technology. Point cloud video streaming is one fundamental research topic to facilitate point cloud video applications. In this article, we discussed the challenges of and solutions to point cloud video streaming systems, followed by a proposed DASH-based point cloud video streaming system. We prototyped this system with off-the-shelf devices and achieved satisfactory performance compared with the baseline schemes. Future research directions were also introduced to help further understand and study this topic.