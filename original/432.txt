The arrival of big data and the Internet of Things (IoT) era greatly promotes innovative in-network computing techniques, where the edge-cloud continuum becomes a feasible paradigm in handling multi-dimensional resources such as computing, storage, and communication. In this article, an energy constrained unmanned aerial vehicle (UAV)-aided mobile edge-cloud continuum framework is introduced, where the offloaded tasks from ground IoT devices can be cooperatively executed by UAVs acts as an edge server and cloud server connected to a ground base station (GBS), which can be seen as an access point. Specifically, a UAV is powered by the laser beam transmitted from a GBS, and can further charge IoT devices wirelessly. Here, an interesting task offloading and energy allocation problem is investigated by maximizing the long-term reward subject to executed task size and execution delay, under constraints such as energy causality, task causality, and cache causality. A federated deep reinforcement learning (FDRL) framework is proposed to learn the joint task offloading and energy allocation decision while reducing the training cost and preventing privacy leakage of DRL training. Numerical simulations are conducted to verify the effectiveness of our proposed scheme as compared to three baseline schemes.

Introduction
The ever-increasing growth of connected devices has brought large amounts of data generated in wireless networks, and inspires innovative applications and services (autonomous driving, virtual reality, etc.) that require complicated computation and analysis. Fortunately, the integration of advanced computing and communication technologies (e.g., 5G techniques and cloud/edge computing) can greatly support in-network computing and offer commendable resource provisioning techniques to ensure necessary computing services for IoT devices [1], [2].

Motivation
Edge-Cloud Continuum
To facilitate efficient resource provisioning for the massive resource demands of mobile applications, cloud computing has been presented as a viable solution, which offers powerful computing capability and sufficient resources. However, as a remote computing mode, cloud computing can result in heavy response delay and burden on backhaul links. Therefore, mobile edge computing (MEC) has emerged as a cost-effective paradigm that enables cloud-like computing/storage capabilities at the edge of mobile networks, thus providing responsive resource sharing and computing services. Nevertheless, MEC servers are always supposed to have limited resources, posing great challenges to meet the ever-growing resource demands. Consequently, the coexistence of MEC and cloud computing has attracted wide attention, where how to realize the reasonable cooperation among edge and cloud servers to meet heterogeneous applications requirements becomes urgent and critical. For example, a MEC server can offload delay-insensitive tasks to a cloud server to reserve resources for delay-sensitive tasks via designing appropriate task scheduling mechanisms [3].

UAV-Aided Computing Network
Most existing works associated with task offloading via edge-cloud continuum cooperation mainly focus on cellular networks with stable communication links and sufficient energy supply [1]. However, sustainable computing services in remote or disaster regions are still challenging due to the shortage of network equipment and power grid. Therefore, the exploration of flexible mobile devices in the sky becomes an attractive field that deserves attention. Unmanned aerial vehicles (UAVs, also known as drones) present a popular topic due to the flexibility and maneuverability, as well as the potential value in next-generation wireless networks (6G and beyond), which provide effective platforms for air-ground communications [4]. Nowadays, embedded advanced communication equipment, sensors, and processors enable a UAV to offer efficient and flexible services for on-ground users. For example, UAVs can act as relays that assist the wireless communication procedure. Furthermore, many existing works also consider UAVs as MEC servers that offer fast and reliable computing services to users.

WPT-Aided Dynamic Energy Transfer
The limited energy capacity of UAVs becomes a pivotal disincentive that may hinder sustainable and reliable services. To alleviate the possible performance degradation, wireless power transfer (WPT) technology can be applied. However, conventional WPT generally relies on radio frequency (RF) band and can only transfer low-power energy within a short distance, which presents drawbacks in UAV networks [5]. Consequently, laser-based WPT technology has been introduced to power UAVs, where UAVs can harvest energy from laser beams transmitted from ground energy stations [6]. UAVs can also offload a certain amount of workloads to on-ground stations that are connected to a cloud server via a high-speed fiber link, due to limited on-board computing and storage capabilities. To extend the fight duration and provide sustainable computing services, UAVs need to optimize the energy allocation strategy to maximize long-term performance. Thus, the joint optimization of task offloading and energy allocation in a dynamic UAV-aided mobile edge-cloud continuum presents a noteworthy problem.

The above mentioned discussions present our major motivations. In this article, a joint task offloading and energy allocation problem is investigated in a dynamic UAV-aided mobile edge-cloud continuum. Considering the diversity of task requirements and the dynamics of the environment (e.g., time-varying wireless channels and energy level of the UAV), conventional static optimization methods (e.g., convex optimization, iterative optimization) face difficulties in solving the aforementioned problem in a timely and cost-effective manner. Also, since the long-term performance of task offloading represents another major objective in this article, deep reinforcement learning (DRL) is adopted as a feasible way.

Generally, training a DRL framework is always resource-consuming. Furthermore, limited by the ergodicity of DRL state, training performance of DRL can be affected by the quality and quantity of training data. For example, a DRL agent may take a long time to collect enough high-quality training data for DRL training, which leads to a long convergence delay. Moreover, a model trained by local training data is only suitable for the local environment, which limits the adaptability of the model to be deployed in a new environment. To overcome these drawbacks, cooperative training based on training data sharing among different UAVs can improve the adaptability of UAVs in different environments and guarantee the convergence performance. However, sharing training data directly will incur excessive communication overhead and severe privacy leakage problems, especially in networks with malicious attacks [4], the details of which are discussed below.

Thus, federated learning (FL) has been introduced as a popular distributed learning approach, which trains a global model via utilizing training data from different clients. During the training process, only the weights or gradients of clients are uploaded to the central server, which can effectively protect the privacy and reduce the amount of transmitted raw data [7]. Driven by the advantages brought by FL, we investigate a federated DRL (FDRL) framework in this article as a good solution to solve the joint optimization problem of task offloading and energy allocation.

Contribution and Organization
Major contributions in this article are summarized as follows:

We study the joint task offloading and energy allocation problem in a novel UAV-aided mobile edge-cloud continuum based on RF and laser WPT.

We propose an FDRL framework to jointly optimize both task offloading and energy allocation, while reducing the training cost and preserving privacy.

We conduct comprehensive simulations to evaluate the effectiveness of the proposed framework as compared to three baseline schemes.

The remainder of this article is organized as follows. The following section illustrates the framework of the UAV-aided mobile edge-cloud continuum. The FDRL framework is proposed following that. Simulation results are then demonstrated before concluding the article in the final section.

UAV-Aided Mobile Edge-Cloud Continuum Description
Network Architecture Overview
Figure 1 illustrates the network architecture of the UAV-aided mobile edge-cloud continuum, where a UAV is embedded with a lightweight MEC server and a rechargeable battery. The UAV moves at constant speed on a preset trajectory. A ground base station (GBS) is connected to a cloud server (through a fiber link) and equipped with a laser beam director, which can transmit a laser beam at a constant power to charge the UAV. There are N∈N on-ground IoT devices within the coverage area of the UAV. Each IoT device can generate various tasks at the beginning of each time slot t∈T, which can be modeled as a finite-state discrete Markov chain. Assume that IoT devices are suffering from unsatisfying computing capability and resources, and thus we do not consider local computing cases (e.g., IoT devices cannot process tasks locally by themselves). The tasks of IoT devices can be offloaded to the UAV for execution, and can be further offloaded to the cloud server when the UAV is overloaded. If a task is offloaded within the generated time slot, the UAV can obtain a reward; otherwise, the task will expire. For simplicity, the transmission delay on fiber link and task execution delay at the cloud server are ignored. We assume the UAV uses orthogonal channels to communicate with IoT devices and the GBS, where the co-channel interference can thus be ignored.

Figure 1. - The illustration of UAV-aided mobile edge-cloud continuum integrated with WPT: a) The overall architecture; b) The energy flow and task offloading process of the whole system.
Figure 1.
The illustration of UAV-aided mobile edge-cloud continuum integrated with WPT: a) The overall architecture; b) The energy flow and task offloading process of the whole system.

Show All

Communication Model and Execution Model
Assume that each IoT device can only occupy one channel to communicate with the UAV, and the channel state is stationary during each time slot. At the beginning of each time slot, the UAV can estimate the channel gains to the IoT devices and the GBS based on its current location by adopting the air-to-ground channel model [8]. The channel gains on the uplink and downlink are reciprocal for simplicity. The transmission data rate from each IoT device to the UAV can be calculated via applying Shannon's formula based on the sensed channel gain and transmission power of the IoT device. Similarly, data rate from UAV to GBS can also be calculated in the same way.

Each IoT device may have different task requirements at each time slot. Each task can be offloaded to the UAV or updated by the IoT device at the next time slot. Let binary variable otn indicate the binary offloading state of IoT device N∈N at time slot t, where otn=1 indicates that the task can be offloaded to the UAV; otherwise, otn=0. Notably, a task will be dropped when the data transmission procedure has not been finished within a time slot. The task of IoT device n at time slot t can be described as (ltn, Btn), where ltn denotes the input data size (bit), and Btn represents the required CPU cycles for task execution. At the beginning of each time slot, each IoT device can send a task offloading request to the UAV, while the offloaded tasks can be cached by the UAV's task buffer. Therefore, the total offloaded data lttotal and workload Bttotal are the sum of otnltn and otnBtn for all IoT devices, respectively.

The transmission delay from IoT device n to the UAV is denoted as τtn,tran as calculated by the known task size and data rate. The relevant energy consumption of transmission is represented by etn,tran. Since each lot device can transmit task data to the UAV independently, and in parallel, the actual transmission delay τtl,final is defined by the maximum value among τtn,tran.

In the proposed framework, the UAV can help with further task offloading to a cloud server, since IoT devices are facing challenges to contact with the cloud server directly. The cache size of the UAV can be denoted as Cu (bits), and the computation resource is Fu (CPU cycles/s). Thus, the total task offloaded to the UAV at time slot t should not exceed the remaining cache size of the UAV. Let 0≤φt≤1 denote the partial off-loading rate to the cloud server. Thus, the task execution delay at the UAV is τtu,exe, which can be calculated by the partial offloading rate, the total workload, and the computation resource of the UAV. The energy consumption of task execution can be denoted as etu,exe, which is defined by the task size and the unit energy consumption to process one cycle.

Similarly, the partial offloading transmission delay can be calculated as τtu,tran, and the relevant transmission energy consumption is etu,tran, which depends on the transmission power of the UAV and the transmission delay. The task execution delay at the cloud server is ignored since the cloud server has powerful computing ability.

Furthermore, the energy consumption of UAV flight can be estimated by the flight model in [6], which is mainly related to the speed of the UAV. The task transmission and execution can be performed in parallel; thus, the final delay at the UAV is τtu,final=max(τtu,exe, τtu,tran). The final delay of the offloading process is τtfinal, which is the sum of τtl,final and τtu,final.

Energy Allocation Model
In this article, a UAV can harvest energy from a laser beam of a GBS. Suppose the total amount of harvested energy within one time slot is Etu,har, which is determined by the length of the time slot, the power of the laser, and the energy conversion efficiency. The UAV can further transfer energy to the IoT devices with an energy allocation strategy. Similarly, the total harvested energy at IoT device n during one time slot is Etn,har, which can be calculated by the length of the time slot, the allocated transmission power from the UAV, and the RF energy conversion efficiency. Therefore, the energy level of the UAV at the beginning of each time slot can be decided by the previous harvested energy and total energy consumption.

Problem Statement
Considering the dynamics and causality of the system, the UAV has to make feasible decisions on jointly optimizing task offloading and energy allocation. In particular, definitions of energy causality, task causality, and cache causality are detailed as follows.

Energy Causality
To highlight the importance of energy harvesting, IoT devices can only use the energy harvested at the previous time slot, which is decided by the previous energy allocation decision of the UAV. Thus, the transmission energy consumption of IoT device n at time slot t should satisfy etn,total≤Et−1n,har. The UAV harvests the energy from the GBS; then the energy can be used for task transmission and execution, transferring energy to the IoT devices, and UAV flight. Thus, the energy level of the UAV at each time slot is related to the remaining energy of the previous time slot and energy harvested at the current time slot, which can be modeled as a Markov decision process (MDP).

Task Causality
The UAV needs to decide the binary offloading decision by selecting the IoT devices and accepting the task offloading request. When all the tasks finish transmission and are cached in the UAV, the partial offloading decision can be made.

Cache Causality
When part of the workload is offloaded to the cloud server, the corresponding occupied cache will be released. In addition, we assume that each task is splittable. All the off-loaded tasks can be regarded as a general task and the individual task execution ignored. Furthermore, the remaining cache size will only be updated at the beginning of each time slot. Thus, the remaining cache size is decided by the cache size of the previous time slot and current task off-loading decision, which can also be modeled as an MDP.

Based on the above causalities, we want to maximize the long-term reward, which is subject to the executed task size and execution delay with T time slots:
max∑t=1Tγt−1(Φ(Ittotal)−αdτtfinal),(1)
View Sourcewhere γ is the discount factor of future reward, and ϕ(lttotal) is the reward function related to the executed task size. For example, we can set ϕ(lttotal)=lttotal.αd as the unit cost of the execution delay.

Federated DRL-Based Task Offloading and Energy Allocation
Hybrid DRL-Based Solution
At the beginning of each time slot, the UAV makes the joint task offloading decision and energy allocation decision based on the observed state, consisting of the current energy level, the current spare cache size, the predicted harvested energy, the channel gains vector of the IoT devices and the GBS, and the task requirements vector of IoT devices. Apparently, the current state is only related to the previous state and the action made by the UAV, which can be modeled as an MDP problem. Then a DRL-based task offloading and energy allocation framework is proposed to optimize the long-term offloading performance. The basic idea of DRL is to find the optimal action policy to maximize the discounted total future reward expectation by mapping the states to the actions through the neural network, just like the state-of-the-art deep Q-network (DQN) [9] and deep deterministic policy gradient (DDPG) [10] algorithm do. However, it is noteworthy that the action involved in this article is a hybrid joint action that contains the discrete action (binary offloading decision) and continuous action (energy allocation decision and partial offloading decision), which cannot be tackled by the DQN or DDPG alone. Therefore, we propose a hybrid DRL framework by combining the DQN and DDPG algorithm [11].

Figure 2 demonstrates the overall framework of the hybrid DRL-based joint task offloading and energy allocation, which is composed of an actor network and a critic network. Both the actor and critic networks are composed of fully connected neural networks. At the beginning of the time slot, the UAV can observe the current state; then the observed state is input into the actor network and outputs the continuous action (i.e., energy allocation and partial offloading). Then the state and continuous action are input into the critic network. The critic network evaluates the action (Q) value of the generated action, and then outputs the discrete action and the binary offloading decision with Bellman equation iteration. Thus, continuous action and discrete action can be obtained to formulate the joint action. The UAV can receive a reward when the joint action is executed in the environment, and the UAV will turn to the next state at the next time slot. The tuple composed of the current state, the joint action, the received reward, and the next state will be stored in an experience buffer which makes the training process more sample-efficient. Finally, a mini-batch can be sampled from the replay buffer to update the weights of the neural networks of the action network and critic actor via stochastic gradient descent, respectively.


Figure 2.
The framework of hybrid DRL-based joint task offloading and energy allocation.

Show All

Federated DRL Framework
Recalling the drawbacks of DRL training mentioned earlier, different DRL training schemes are discussed in this subsection. Figure 3 illustrates three different DRL training schemes, namely, independent DRL training, centralized DRL training, and FDRL training. Although independent DRL training will not incur extra communication cost, it may cause performance degradations. For example, independent DRL training allows each UAV to train the DRL model independently, which can occupy excessive computation resources of the UAV. Also, training performance may be affected by low quality and insufficient trading data. Centralized DRL training is computation-re-source-friendly for UAVs, and UAVs only need to share the training data. However, two deficiencies may exist in this training process:

Large communication overhead: The training data will increase exponentially with the number of IoT devices and task requirements, and large scale training data transmission will bring severe pressure to the link from the UAV to the GBS.

Privacy leakage: As an aerial platform, UAVs are vulnerable to malicious attack (jamming or eavesdropping) [4], and frequent training data transmission will cause privacy leakage, which is fatal for a DRL-based task offloading model. The states and actions obtained from the leaked training data will enable the attacker to predict the UAV's offloading strategy, leading to task offloading failures.

Figure 3. - Different DRL training schemes: a) Independent DRL training; b) Centralized DRL training; c) Federated DRL training.
Figure 3.
Different DRL training schemes: a) Independent DRL training; b) Centralized DRL training; c) Federated DRL training.

Show All

The merits and demerits of independent DRL and centralized DRL can be compromised by FDRL. Figure 3c presents the basic training process of FDRL. First, a DRL model (i.e., actor network and critic network) is initialized at the cloud server. Then the UAVs at different areas can download the global model from the cloud server. Each UAV trains the model with local training data after several rounds. Then all the UAVs upload the weights of the model to the cloud server. The uploaded weights will be used to update the global model via a specific aggregation method (e.g., FedAvg [12]). The updated global model will be distributed to the UAVs and repeat the above training process until reaching the training target. Finally, each UAV can obtain the final trained model.

Overcoming the drawbacks associated with independent DRL and centralized DRL (as discussed previously) presents the major pro of the proposed FDRL framework, while the potential cons of FDRL are summarized as follows.

Network Dimension Mismatching
When the training process of FDRL is completed, each UAV obtains the same trained model. Thus, the state dimension and action dimension of each UAV must be the same. In other words, the number of IoT devices and state of the task requirements in different areas must be the same, which limits the application of FDRL. This problem can be tackled with other learning techniques (e.g., federated transfer learning [13]), which can overcome the dimensional mismatch problem of training data.

Non-Independent Training Data
When two or more UAVs work in the same area, the training process of each UAV is not more independent, while the UAVs form an interdependent multiagent system. Thus, the training data of each UAV is non-independent, which will degrade the global training performance.

In FDRL, overheads on computation and communication may still exist since each UAV has to perform local training and upload/download models. In FL, communication overhead is much larger than computation overhead [14]; thus, we mainly discuss the communication overhead in this article. As indicated in [14], the size of a generic K-layer neural network can be approximated on the order of O(linloutK)≪D, where lin and lout are the dimension of the input layer and output layer, respectively, while D is the size of raw training data. Thus, compared to centralized DRL training, communication overhead of FDRL can be greatly reduced. Moreover, the DRL model applied in this article is lightweight. For example, the model size is only tens of kilobytes according to the simulation settings below, which also indicates that the communication overhead of the proposed FDRL is acceptable.

Numerical Simulation and Discussion
Simulation Setup
Numerical evaluations are conducted to evaluate the performance of our proposed FDRL-based task offloading and energy allocation framework. We consider a circular area with a radius of 50 m; the location of the GBS is (50, 50). The transmission power of the laser is 50 dBm, and the other laser-related parameters are set according to [6], [8]. The IoT devices are randomly distributed within the area. The constant transmission power of IoT devices is set to 20 dBm. The flying height of the UAVs is independently selected from [10, 20] m with a constant speed of 5 m/s. The maximum transmission power of the UAV is 30 dBm. The UAV flight energy consumption is set according to [6], [8]. The bandwidth of each channel is 200 kHz. The task requirement of IoT devices can be modeled as a Markov chain model and picked up independently from the set {200, 400, 600, 800} kb. The Bn can be decided accordingly by defining the required cycles per bit as 500 cycles/bit. The unit cost αd is set to 1. The time slot duration length is 500 ms. The cache size of the UAV is 5 Mb. The computing ability of the MEC server is 5 GHz.

As for the DRL framework settings, the actor network and critic network have the same network architecture with three hidden layers with 64, 64, and 32 neurons, respectively. The size of the replay buffer is 4000; the mini-batch size is 32. The discount factor is 0.95, and the learning rates of the actor network and critic network are 1e-4 and 1e-3, respectively. The maximum training episode is 500 and 500 steps per episode.

Three baseline schemes are considered to evaluate the performance of the proposed framework, namely, MEC only, Cloud only, and Random. Specifically, MEC only denotes that the offloaded tasks are only executed at the UAV, while Cloud only represents that all the tasks are fully offloaded to the cloud server. Furthermore, Random means the UAV randomly offloads its workload to the cloud server.

Simulation Results
Figure 4 evaluates the training performance of the proposed FDRL and centralized DRL upon considering three agents (UAVs) with five IoT devices in each area. Figures 4a and 4b present the average reward and average execution delay averaged over the time slots of each training round, respectively. It can be seen that both the proposed FDRL and centralized DRL converge within five training rounds and then fluctuate in a small range. In addition, the average reward of FDRL is smaller (about 7 percent) than that of the centralized method, while the average execution delay of the FL and centralized methods has similar performance.

Figure 4. - Training performance comparison of the proposed FDRL and centralized DRL: a) Average reward of the UAV over the time slots; b) Average task execution delay over the time slots.
Figure 4.
Training performance comparison of the proposed FDRL and centralized DRL: a) Average reward of the UAV over the time slots; b) Average task execution delay over the time slots.

Show All

Figure 5 compares the system performance of four schemes having different numbers of IoT devices. It can be seen that the proposed scheme outperforms the other three schemes with the highest reward and lowest delay, which proves the relevant effectiveness of the proposed scheme. Note that except for cloud only, the average reward of other schemes increases with the growing number of IoT devices. The rationale is that with the increasing number of IoT devices, the transmission delay increases sharply when all the tasks are offloaded to the cloud, which exactly embodies the shortcoming of cloud computing.

Figure 5. - Performance of different schemes under different numbers of IoT devices: a) Average reward; b) Average task execution delay.
Figure 5.
Performance of different schemes under different numbers of IoT devices: a) Average reward; b) Average task execution delay.

Show All

Figure 6 compares the performance of four schemes under different system configurations with five IoT devices. Figures 6a and 6b illustrate the average reward, and average execution delay vs. the computation resources of the MEC (UAV). From Figs. 6a and 6b, the proposed scheme still outperforms the other three schemes, and the average reward/execution delay increases/decreases with the increasing computation resources except for cloud only. The intuitive reason is that with the enhancement of MEC computing ability, the task execution delay will be significantly reduced at the MEC side, while the computation resources of the MEC are no longer needed in cloud only. In Figs. 6c and 6d, we evaluate the effect of communication resources on performance, considering different values of channel bandwidth. As can be seen from Fig. 6c, the average reward of different schemes increases with the increasing value of channel bandwidth. Interestingly, the average execution delay of different schemes in Fig. 6d rises with the increase of channel bandwidth at the beginning, and then decreases after reaching a peak value at 400 kHz. The rationale behind this is that IoT devices tend to offload more tasks to the UAV as the bandwidth increases, which leads to the decline of execution delay. When continuing to raise the value of bandwidth, the UAV tends to offload more tasks to the cloud server; thus, the execution delay decreases. The above evaluated performance reveals the effectiveness and necessity of the cloud-edge continuum in improving the amount of offloaded tasks and reducing execution delay.

Figure 6. - Performance of different schemes under different system configurations: a), b) Average reward and average execution delay vs. the computation resources of MEC (UAV); c), d) Average reward and average execution delay vs. channel bandwidth.
Figure 6.
Performance of different schemes under different system configurations: a), b) Average reward and average execution delay vs. the computation resources of MEC (UAV); c), d) Average reward and average execution delay vs. channel bandwidth.

Show All

Conclusions and Future Work
In this article, we investigate the task offloading and energy allocation problem in a novel UAV-aided mobile edge-cloud continuum.

Considering the time-varying wireless environment and various task requirements of IoT devices, we propose a hybrid DRL framework to enable the UAV to jointly optimize task offloading and energy allocation decisions. Furthermore, to improve the adaptability of the UAV to different environments and guarantee convergence performance, we propose an FDRL framework to reduce communication overhead and prevent privacy leakage. Comprehensive simulations demonstrate the effectiveness of the proposed scheme under different system configurations as compared to three baseline schemes.

Several future directions can be discussed, such as edge-cloud continuum via integrating space, air, ground, and ocean networks. Interesting problems such as how to realize in-network computing through the collaboration of various edge-cloud continuums based on heterogeneous hardware platforms in such an integrated network, and perform intelligent resource management and scheduling according to different service requirements may be worth attention.