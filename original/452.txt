Real-time intelligent video processing on embedded devices with low power consumption can be useful for applications like drone surveillance, smart cars, and more. However, the limited resources of embedded devices is a challenging issue for effective embedded computing. Most of the existing work on this topic focuses on single device based solutions, without the use of cloud computing mechanisms for parallel processing to boost performance. In this paper, we propose a cloud platform for real-time video processing based on embedded devices. Eight NVIDIA Jetson TX1 and three Jetson TX2 GPUs are used to construct a streaming embedded cloud platform (SECP), on which Apache Storm is deployed as the cloud computing environment for deep learning algorithms (Convolutional Neural Networks - CNNs) to process video streams. Additionally, self-managing services are designed to ensure that this platform can run smoothly and stably, in the form of a metric sensor, a bottleneck detector and a scheduler. This platform is evaluated in terms of processing speed, power consumption, and network throughput by running various deep learning algorithms for object detection. The results show the proposed platform can run deep learning algorithms on embedded devices while meeting the high scalability and fault tolerance required for real-time video processing.
SECTION 1Introduction
Embedded devices equipped with impressive computing capacities are being increasingly adopted in various domains. For example, some drones can monitor public places using aerial photography to help to maintain public safety, or can be used for goods delivery, and even for power grid monitoring to ensure their targets are working reliably. For such applications, there are challenging issues in the way of achieving good quality services for embedded video or image processing in real-time:

Low real-time recognition accuracy. Only light-weight network structures [1] can run on drones in real-time due to hardware limitations; this limits the possibility of achieving high accuracy in a complex environment compared with deep network structures [2].

Bad performance of on-line recognition. At this time, usually only simple tasks such as video collection and transferring are performed on board by drones and robots, and then the collected video is processed on a remote cloud server to conduct analysis. This makes it difficult to fulfill real-time requirements, and suffers latency that may cause loss of both life and assets. For example, the real-time recognition of an early stage fire around a power line may help to reduce potential damages.

Deep learning is currently applied successfully in various domains, such as natural language processing [3] and computer vision [4], and can even exceed human performance in some cases [5]. It has been applied for aerial video analysis due to its high accuracy in many computer vision tasks [2], [6], [7], [8]. However, these high-performance deep neural networks have large-scale weight parameters and require billions of floating point operations, which result in high power consumption. For example, AlexNet has 61M parameters (249 MB of memory) and performs 1.5B high precision operations to classify one image. These numbers are even higher for deeper CNNs, e.g., VGG [7]. Even though heavy-weight tasks can be migrated to cloud servers [9], bandwidth, latency, and network availability are still major factors hindering the responsiveness of near real-time applications.

Efforts to address these issues have taken two directions: 1) network structure modification, and, 2) platform construction.

Binarization is an effective approach to reduce neural network size. In Binarized Neural Networks (BNNs) [10], weights and activations are constrained to either +1 or âˆ’1, which requires 32 times less memory but gives 7 times the performance acceleration on MNIST data compared with the non-optimized model. XNOR-Net [11] approximates both weights and inputs to convolutional and fully connected layers with binary values, which offers a 58 times speed up in CPUs.

Google proposed MobileNets [12], which is a small, low latency model which can be easily matched to requirements of mobile and embedded vision applications. Apples Core ML1 is a foundational machine learning framework used by Apple products, enabling developers to build smart applications on embedded devices. Despite successful network size reduction, these models still cannot achieve as high accuracy as 32-bit DNNs [6] in processing color images.

Other work has focused on building embedded platforms and adopting deep learning algorithms. For example, Mao et al. [13] ran the Fast R-CNN on a Jetson TK1 platform.2 Although additional modifications on the Fast R-CNN were made to fit TK1, the detection speed was very low (1.85 frames per second; hereafter, fps). Another study by Jagannathan et al. [14] ran a 7-layer CNN on TDA3x SoC for object classification, and the overall system performance was 15 fps. The majority of existing work (Ratnayake et al. [15], Bako et al. [16], Nikitakis et al. [17], Chen et al. [18], Arva et al. [19]) used only a single embedded device to conduct video processing, which is obviously not capable of fulfilling real-time video processing (24 fps). Gao et al. [20] built an embedded cluster for video processing, but it lacked a complete solution to address system stability and fault tolerance. Therefore, a powerful software/hardware platform is needed to support efficient embedded deep learning based real-time video processing.

We propose a streaming embedded cloud platform (SECP) for real-time intelligent video processing using embedded GPUs, supported by cloud computing. We apply both NVIDIA Jetson TX1 and TX23 to build the streaming embedded cloud platform for real-time video intelligent processing. On SECP, Apache Storm4 is deployed as the runtime environment for deep learning algorithms to provide parallel processing. In order to make services both stable and fault-tolerant, some self-managing services are designed to conduct sensing, detecting, and scheduling functionalities, including a metric sensor, a bottleneck detector, and a scheduler. The metric sensor collects metrics from Storm to monitor the health status of the platform. The bottleneck detector finds computing bottlenecks, and the scheduler executes the decision to manage resources.

We conducted comprehensive evaluations of an SECP, including on-line video processing based on deep learning, capability to find bottlenecks, and ability to make reasonable decisions when there are abnormal situations. The contributions for this paper include:

A novel streaming embedded cloud platform-SECP for real-time intelligent video processing is proposed that combines the power of stream processing and embedded GPUs parallel processing capabilities, which can run deep neural networks efficiently.

Self-managing services are designed to ensure SECP fault-tolerance for continuous stream processing by monitoring CPUs, GPUs and network throughput of the platform.

A scheduler service applies a video frame emitting strategy to respond to faults or failures while minimizing data loss.

The rest of this paper is organized as follows. Section 2 gives a detailed description of the proposed SECP platform and its implementation. Section 3 evaluates the SECP. Section 4 presents related work. Conclusion and future work are given in Section 5.

SECTION 2SECP Platform Overview
The SECP platform aims to build a streaming embedded cloud for intelligent real-time video processing, consisting of hardware, deep learning algorithms, and a streaming processing framework. In addition, self-managing services with a monitor and a scheduler are designed to ensure appropriate decisions on faults.

2.1 Hardware Design of SECP
2.1.1 Embedded GPUs used in SECP
Deep neural networks are computing intensive for their heavy floating point calculation, which can be accelerated by GPUs. To ensure the performance of SECP, we use commercial embedded GPUs. The NVIDIA Jetson is an industry-leading embedded computing device, which currently has two versions: TX1 and TX2. Table 1 lists their main properties related to our work.

TABLE 1 Jetson TX1 versus Jetson TX2
Table 1- 
Jetson TX1 versus Jetson TX2
2.2 SECP Hardware Design
We designed two platforms for each version of NVIDIA Jetson device. A baseboard is designed for each platform to integrate three modules: power supply, communication, and computing. Then TX1 or TX2 GPUs can be plugged into the baseboard.

The computing module consists of three interfaces for each of the TX1/TX2 cores. The computing module enables horizontal scaling, as its computing cores can be added and removed arbitrarily according to computing demands. The communication module uses a Gigabit Ethernet RTL8370 switching chip to achieve rapid communication between computing cores. We also include a separate USB3.0 interface and a HDMI interface for platform debugging purposes.

2.3 Designing the SECP Software Components
2.3.1 Choosing Deep Learning Algorithms
The platform runs video object detection algorithms to process real-time videos. We compare a set of algorithms in terms of accuracy and speed running on an NVIDIA TITAN X using the Visual Object Classification Challenge 2007 (VOC2007) dataset. The results are listed in Table 2.

TABLE 2 Performance of Various Algorithms
Table 2- 
Performance of Various Algorithms
The R-CNN (Girshick et al. [21]) based object detection algorithms are very slow because they have complex steps: 1) coarse-grained region proposal generation; 2) CNN feature extraction and object detection; and 3) fine-grained bounding box compression and regression. Although they fused multiple stages together and avoided redundant computations in later works [22], [23], [24], these algorithms are too slow to fulfill real-time requirements.

SSD [27] can achieve both high accuracy and high speed using an NVIDIA TITAN X GPU. Although the TITAN Xs computing capacity exceeds that of NVIDIA Jetson, it is not practical to reproduce such performance on an embedded platform.

YOLO [25] considers object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network is used to predict bounding boxes and class probabilities directly from full images in one evaluation. This method achieves both good accuracy and high speed so that it is suitable to be transformed in an embedded platform. In the rest of the paper, we build the object detection algorithms based on YOLO v2.

2.3.2 Parallel GPU Processing using the Storm Framework
In order to overcome the limitations of a single embedded GPU, in our work we built an embedded cloud to overcome the computing capacity on embedded devices so that devices can process videos in parallel. In such an embedded cloud, each device is responsible for only one portion of the video data partitioned over the SECP, and the final results from different devices are merged.

We utilize a real-time stream processing framework (Apache Storm) to design the parallel video processing capabilities for SECP. The reason we chose Storm as the underlying cloud computing infrastructure is that it can handle an event with a sub-second latency, which is much faster than other options like Spark Streaming5 which can incur a second-order delay. A Storm job called topology consists of spouts, bolts, tuples, streams and grouping specifications.

In the SECP, spouts receive data and emit it to other computing nodes called bolts. First, a spout pulls video stream from a camera. Then a spout coverts the video stream to video frames. A spout also has a message queue to emit the video frames to other bolts. Bolts are computing nodes. Since the processing time varies between different video frames, by default, every bolt receives the next video frame upon the completion of the current video frame. Video frames are emitted from the message queue to three bolts randomly. Then the video frames are processed in parallel by three bolts. Finally, to reconstruct the video correctly we add time stamps to every frame in the message queue to maintain their order in the video. All processing steps on Storm are called Storm tasks.

Given this architecture, we built a video object detection job as a Storm topology. We deploy the YOLO v2 as our object detection algorithm on three bolts, and the three bolts are deployed on three computing GPUs respectively. Finally, the results are merged on the bolt in the third layer. Please refer to Fig. 2 which demonstrates how this works.

2.3.3 SECP Metric Sensor
To monitor the status of a video processing job, we designed a metric sensor to collect status of Storm tasks. The collected measurement data are stored for analysis and decision making. We mainly consider the following metrics:

Latency

Latency represents the delay between the starting time from the video frame emitted from a spout, and the ending time when the results are output. Latency measures our platforms video processing speed.

Queue length

The length of message queue indicates if the video data collection speed matches the video processing speed. An empty queue indicates the processing platform is idle and waiting for video data to be collected, and a long queue shows the processing platform is overwhelmed and cannot cope with the current speed of the data collection. Both situations require the platform to be tuned.

CPU utility

CPU utility is the ratio of the CPU time consumed in one task to its total time interval.

GPU utility

Similarly, GPU utility is the ratio of the GPU time consumed in one task to its total time interval.

Traffic

Traffic measures the data bulk passing through the topology in a given time period.

Storm provides APIs for accessing topology-level and cluster-level metrics. Hardware level metrics (such as GPU utility) are obtained by Jetsons APIs. To develop a general metric sensor, for all the metrics at different levels, we designed an API model of the metric sensor as shown in Fig. 1.

Fig. 1. - 
The API model of the metric sensors.
Fig. 1.
The API model of the metric sensors.

Show All

First, an interface called IMetric is defined for collecting metrics, which is implemented by GPUMetric, CPUMetric, and TrafficMetric. CPUMetric and TrafficMetric extend BaseRichBolt and BaseRichSpout of Storm APIs respectively. GPUMetric extends TegraStates of the NVIDIA Jetson APIs. Second, we set TopologyMetricConsumer as the default metric consumer to receive metric messages in a time interval. It calls TopologyMetricEvent that composes ComponentMetricEvent to get metrics. All the metrics are collected in an interval of one second.

2.3.4 The Bottleneck Detector
The bottleneck detector detects abnormal events. A self-learning detection algorithm is ideal, but not practical for two reasons. First, the detection algorithm needs to be light-weight. Second, the training data requires a significant sample size with manual labels of abnormal events. This prevents use of a supervised learning algorithm. Instead, we adopted a simple statistical method.

We calculate the mean value for each metric by
MetricMeank=âˆ‘Ti=1MetrictkMaxMetricValuekT.(1)
View SourceRight-click on figure for MathML and additional features.

Where k represents the kth metric and t is the time point. We conduct normalization that divides each metric by its theoretical max value to make sure each metric has the same scale. T is the number of time interval and usually set to 10, which can achieve quick and accurate response. Another statistic method is variance calculated by
Ïƒ2k=âˆ‘Ti=1(Metrictkâˆ’MetricMeank)2T(2)
View SourceRight-click on figure for MathML and additional features.

We set two thresholds, MeanThreshold=0.9 and VarianceThreshold=0.0025 for the bottleneck detector. Only when MetricMeank>MeanThreshold and Ïƒ2k<VarianceThreshold, we determine the kth metric is facing a bottleneck. Ïƒ2k>VarianceThreshold represents the cluster is not stable enough. The platform should make suitable decisions according to the bottleneck detection results.

2.3.5 Task Scheduler
The scheduler acts as the decision executor that calls Storm APIs to configure Storm parameters and adjusts the video frame emitting strategy under the situation of a bottleneck. The aim is to keep the SECP available with the least loss of data.

Fig. 2 shows an example. In part (a), a computing core crashes causing a bolt to shut down. The video frames in the spout message queue are blocked and the latency increases.

Fig. 2. - 
A sample of decision execution. (a): A computing core is crashed. (b): Our strategy to emit video frames in message queue. Red grids represent the blocked and dropped video frames, and green grids represent the video frames that will be emitted to other nodes.
Fig. 2.
A sample of decision execution. (a): A computing core is crashed. (b): Our strategy to emit video frames in message queue. Red grids represent the blocked and dropped video frames, and green grids represent the video frames that will be emitted to other nodes.

Show All

More seriously, if the blocked video frames keep accumulating in the message queue, the spout will end up with an out of memory exception. Although a defender is designed to empty the memory by force when the message queue is full, video frames are lost. In this situation, the bottleneck detector identifies the abnormalities in memory, GPU utility, and traffic, and the scheduler drops one frame while emitting two frames, as shown in part (b). The idea of this strategy is that continuous frames contain more information than single frames, especially in an object detection task.

The schedule scheme is described in Algorithm 1. The video frames are divided into blocks of fixed length. If the computing nodes are powerful enough to process the video data, we do not need to drop any frames. But for the situation of low computing capacity (e.g., some computing nodes crash), as the length of message queue increases, the number of dropped frames will go up. When the crashed computing nodes recovered, the dropped frames will decrease. This scheme can guarantee that SECP can run stably with the lowest data loss using dynamic adjustment of dropped frames.

2.4 SECP Platform
The SECP platform is shown as a component diagram in Fig. 3. The metric sensor and scheduler depend on Storm directly. The metric sensor calls Storm APIs to get each task's metrics and the scheduler calls Storm configuration to configure it. The bottleneck detector reads metrics obtained by the metric sensor and provides an interface for the scheduler to decide on which strategy is to be executed. Storm calls camera APIs to pull video streams from the camera. A video processing interface is designed for Storm to call video processing algorithms such as image recognition and object detection.

Fig. 3. - 
Component diagram of overall architecture.
Fig. 3.
Component diagram of overall architecture.

Show All

Algorithm 1. Schedule Algorithm of SECP
Require:

CL: Current length of message queue

MaxL: Maximum length of message queue

MinL: Minimum length of message queue

Block=Bn: A block is a set of video frames

BL: Length of a block

df = 0 : Number of dropped frames

Ensure:

for all BnâˆˆBlock do

if CL>MaxL and df<BL then

df+=1

end if

if CL<MinL and df>0 then

dfâˆ’=1

end if

for t=0 to t=df do

Bn.delete()

end for

for t=0 to t=BLâˆ’df do

Video Process Bn.pop()

end for

end for

The SECP platform has multiple nodes with the same computing capability and configuration, which means that it is easy to make these nodes work in parallel. When there is a video input, the platform first clips it into frames and each frame is given a unique time stamp to facilitate the final result merging process. We use fn to denote the video frames where n is the time stamp. The clipped video frames are put into a queue and all computing nodes retrieve video frames from this queue in parallel. Because of a tiny time difference between different frames caused by network fluctuation or image complexity, there would be variance between the input order and the output order. For example, the input is f1,f2,f3 and the output is r2,r1,r3. The time stamp will help to correct this problem to have a result of r1,r2,r3.

The SECP platform is designed for running deep learning algorithms, such as YOLO. The main computing of YOLO is convolutional computing, which occupies over 90 percent of the whole process. Therefore, accelerating convolutional computing can speed the running up. According to our former experiments [28], GPU is suitable for computation intensive tasks, which motivate us to use GPU-accelerated approach for improving the processing performance. The computing intensive portion (e.g., convolutional computing) can run a large number of GPU cores in parallel using CUDA.6 The Jetson TX1 module has 256 CUDA cores. This is able to dramatically speed up computing applications. In addition, we installed a deep learning environment and deployed YOLO on each module, and made each module run independently but able to run in parallel to boost performance.

We further show the relations of these components as a package diagram in Fig. 4. The monitor package contains three sub-package and uses the streaming processing framework (Storm) APIs to get metrics. The Storm stream processing framework depends on two packages directly. One is a video capture package that contains camera class and pre-processing class such as resize, and uses HIKVISION camera APIs to pull video stream. The second is the video processing package, which provides an intelligent processing interface and a traditional processing interface. These two interfaces are implemented by deep learning algorithms, such as image recognition, and conventional algorithms such as canny edge detection. We have applied a set of toolkits on the algorithms including darknet7 and OpenCV.8

Fig. 4. - 
Package diagram of overall architecture.
Fig. 4.
Package diagram of overall architecture.

Show All

Fig. 5 shows the deployment of the platform architecture. The platform has three computing cores and a switch. Each computing core has a GPU and a CPU cluster, on which we arrange different types of software, as shown in Table 3.

Fig. 5. - 
Deployment diagram of overall architecture.
Fig. 5.
Deployment diagram of overall architecture.

Show All

TABLE 3 Software Deployment
Table 3- 
Software Deployment
Supervisor3, Supervisor4 and Supervisor5 run object detection algorithms, so they are deployed on GPUs on three computing cores. Supervisor1, deployed on the second computing Cores CPU, merges results and stores them. Supervisor2 and Monitor are deployed on the other two computing cores CPUs respectively. The video capture node is an IP camera. All the nodes are connected with a switch to swap data.

SECTION 3Evaluation
In this section, we evaluate the performance of two types of SECPs built with Jetson TX1 and Jetson TX2. And we focus mainly on testing the scalability and fault tolerance of the platforms using YOLO V2. All the network models are trained on NVIDIA TITAN X in advance. The video stream is pulled from HIKVISION IP camera with a resolution of 1920 Ã— 1080. The running environment is given in Table 4.

TABLE 4 Running Environment
Table 4- 
Running Environment
3.1 Jetson TX1 Platform
In this experiment, we used eight Jetson TX1 cores to construct the platform. This is bigger than the proposed three cores platform because we aimed to explore the scalability and the bottleneck of the SECP. Three kinds of YOLOs with different network scales were running on the platform. Table 5 shows the properties of three kinds of YOLOs.

TABLE 5 Properties of three kinds of YOLOs

When the platform is started, all the computing cores are started in standby mode. Every computing core has one worker, which is in charge of running tasks. We add these running workers gradually and the three YOLOs results are shown in Tables 6, 7, and 8 respectively. The SECP platform in standby mode has a power consumption of approximate 20W, which is the running cost of basic system. Fig. 6 shows the processing speed of three YOLOs. Obviously, it goes up continuously as the numbers of workers increase, and Tiny YOLO is much faster than the other two YOLOs. We can also get the same conclusion from Figs. 7 and 8. However, in these three figures, Tiny YOLO changes more significantly than Medium YOLO and Large YOLO, and Medium YOLOs measurements are very similar with Large YOLOs. In addition, we found that the GPU utility was near 100 percent when running Medium YOLO. That is to say, this platform cannot run larger network models than Medium YOLO without latency. This experiment demonstrates that the platform has a high scalability, and that the bottleneck results from its computing capability.

Fig. 6. - 
Processing speed of three kinds of YOLOs.
Fig. 6.
Processing speed of three kinds of YOLOs.

Show All

Fig. 7. - 
Power consumption of three kinds of YOLOs.
Fig. 7.
Power consumption of three kinds of YOLOs.

Show All

Fig. 8. - 
Network throughput of three kinds of YOLOs.
Fig. 8.
Network throughput of three kinds of YOLOs.

Show All

TABLE 6 Performance of Tiny YOLO on TX1 Platform
Table 6- 
Performance of Tiny YOLO on TX1 Platform
TABLE 7 Performance of Medium YOLO on TX1 Platform

TABLE 8 Performance of Large YOLO on TX1 Platform

3.2 Jetson TX2 Platform
We applied three Jetson TX2 cores to construct SECP and ran Tiny YOLO on it. TX2 supports five kinds of running modes for various applications and we can configure it using NVIDIAs command tool called NVPModel. Table 9 lists the details of the configuration of five modes. This tool does not support hot switching, so we have to configure the platform before launching tasks. The results are shown in Table 10

TABLE 9 NVPModel Mode Definition
Table 9- 
NVPModel Mode Definition
TABLE 10 Performance of SECP in Five Modes
Table 10- 
Performance of SECP in Five Modes
Figs. 9 and 10 show the performance of the platform in five modes. Mode 0 had the highest processing speed because of its highest GPU frequency, but it also consumes the most energy, so its energy efficiency is not high. Mode 1 is the most power-saving mode, but the speed is a little bit low. Mode 3 has the highest energy efficiency with relative high processing speed, which is the best choice.

Fig. 9. - 
Power consumption and speed in five modes.
Fig. 9.
Power consumption and speed in five modes.

Show All

Fig. 10. - 
Five modesâ€™ energy efficiency (fps/W).
Fig. 10.
Five modesâ€™ energy efficiency (fps/W).

Show All

Another experiment on the Jetson TX2 platform evaluates fault tolerance using three time intervals. In the first 1-10 minutes, the platform runs normally. At the 11th minute, core2 is shut down manually to simulate an accident. This condition lasts 10 minutes until the scheduler executes the repair strategy described in Section 2.3.3 at 21th minute. We configured Jetson TX2 to work in mode 0 and run Tiny YOLO on the SECP.

As shown in Table 11, in the first 10 minutes, the average latency to process a frame is 86.5 ms, which means three cores can reach a processing speed of 35 fps. When core 2 is shut down, the average latency increased to 102.0 ms due to decreased computing resources. After 10 minutes, the scheduler executes the repair strategy, bringing the average latency down to 75 ms, but the SECP could process only 2/3 of the data of a normal situation.

TABLE 11 Overall Performance of Fault Tolerance
Table 11- 
Overall Performance of Fault Tolerance
We can analyze the changes of three cores by the four metrics (message queue, CPU utility, GPU utility, latency) that are shown in Figs. 11, 12, 13, and 14 respectively. For normal status, the platforms processing speed is faster than the video frame pulling speed (30 fps), so the message queue is empty. Core1 is the data transfer node which pulls video stream from the camera, decodes it and emits video frames to the other two cores GPUs as well as to its own GPU, and these operations run on CPU. Therefore, core1s CPU utility is always the highest but its processing latency is always the lowest. When core2 is shut down, all the tasks including data swap, object detection, result merging etc. on it are moved to core3, causing core3 to have both high CPU utility and high GPU utility scores. In addition, core3s processing latency increases because two object detection tasks are competing for core3s computing resources. The reduced computing capacity causes video frames to block the message queue and the length of the queue would keep increasing if nothing is done to fix it. At the 20th minute, when the scheduler executes the repair strategy, all core2s previous tasks on core3 are killed and the message queue is emptied. According to this strategy, the frame emitting method will be adjusted to make the platform run smoothly. Although the computing capacity became weaker due to the (simulated) hardware fault, the SECP platform recovers to a normal status.


Fig. 11.
Length of message queue (frame).

Show All


Fig. 12.
CPU utility (%).

Show All

Fig. 13. - 
GPU utility (%).
Fig. 13.
GPU utility (%).

Show All

Fig. 14. - 
Processing latency (ms).
Fig. 14.
Processing latency (ms).

Show All

In order to verify that TX1 and TX2 are efficient for object recognition, the YOLO v2 Tiny network and the VOC 2007 data set are used to test and compare the performance of the three platforms (including GTX Titan X). The specific results are shown in the following Fig. 15:

Fig. 15. - 
Comparing TX1, TX2 and GTX Titan X.
Fig. 15.
Comparing TX1, TX2 and GTX Titan X.

Show All

As can be seen from Fig. 15, although the performance of GTX TITAN X is very good, its single high energy consumption is very unsuitable for low power scenarios. TX1 and TX2 are more energy efficient with the same accuracy as GTX TITAN X, and TX2 is the best.

3.3 Discussion
In the experiment on the Jetson TX1 SECP, three kinds of YOLOs have different number of convolutional layers but the same number of fully connected layers. Because the main calculation overhead in CNNs comes from convolution, we modified only the convolutional layers of the YOLOs. In the experiment on the Jetson TX2 SECP, an abnormal status lasted 10 minutes until the scheduler executes repair strategy. This is too long for practical applications, but this was adopted on purpose for this experiment so that we could observe the results clearly. In practical applications, the scheduler can execute the strategy as soon as possible (i.e., within 10 seconds). We designed two kinds of system architectures for two main reasons: 1) we demonstrate the generalization ability of the platform because two kinds of platforms have the same software environment and both of them can run stably; 2) the experiment results can give some guidance in choosing a suitable platform in practical applications, since TX2 based platform is stronger but TX1 based platform is more economical.

In the SECP platform, only limited number of bolts is used to process video (one bolt on each node), and all these bolts are running in full capacity from our tests. Our results show the performance - speed, power consumption and network throughput - increases linearly with increasing module number, and the SECP platform has no bottleneck issue with 8 modules. Theoretically, the most likely cause of bottleneck is the network bandwidth. The last single bolt in the SECP is used to collect all the video frame information to organize the recognition results in the chronological order of the original video frames, where one single bolt is enough. The amount of calculation in this single bolt is very small (which takes much less than 1 ms). Therefore, the last single bolt will not be a bottleneck. In the real-world environment, SECP must determine the number of bolts doing parallel processing during deployment. Usually, we reserve a standby node for fault recovery. The SECP is monitoring the status of worker nodes and will reassign the task to the standby node if a node is down.

In this research, a scale-out approach could be the right answer because we have many small nodes. In addition, object detection is not a simple task. It needs lots of memory and computation at each node. This redundant deployment of neural network models may add cost in higher power consumption, but it does not need to pass copies of the data, which involves additional network serialization and de-serialization.

There are various kinds of deep neural network architectures, such as CNNs, Recurrent Neural Networks (RNNs), and Deep Belief Networks (DBNs), and many kinds of input data (e.g., video, audio and text), but they are all essentially tensor calculation. In this work, we take object detection as an example to evaluate the performance of the platform, and the results show the platform can support CUDA well, which means the SECP platform can support a large variety of deep learning algorithms, and other computing intensive applications that require the corresponding software (e.g., CUDA). Our work shows that SECP can be a general platform for AI applications.

SECTION 4Related Work
There is considerable research about embedded platforms for video processing. Ratnayake et al. [15] proposed a resource and power optimized FPGA-based configurable architecture for video object detection by integrating noise estimation, Mixture-of-Gaussian background modeling, motion detection and thresholding. Their Virtex-5 FPGA-based embedded platform achieved real-time processing of HD-1080p video streams at 30 fps. However, their work was actually video background subtraction instead of object detection like YOLO as we used. Our platform can not only support conventional video processing algorithms such as background subtraction, but also support complex deep learning algorithms.

Bako et al. [16] presented an embedded implementation of a hardware-efficient method for motion information extraction from video signal, using an FPGA circuit-based system for on-line Sobel edge detection and edge displacement-based optical flow computation. The total execution time on a Xilinx Spartan-6 FPGA was approximately 1.6 ms. This research only used a single embedded device to conduct Sobel edge detection, which is much simpler than the intelligent video processing like the object detection that we are doing.

Mao et al. [13] transferred and tailored deep neural networks to embedded devices by using an embedded Jetson TK1 CPU+GPU platform to conduct real-time object detection. They paid much attention to improving Fast R-CNN to accelerate the algorithm. However, the hardware they used was too weak to support real-time object detection and they did not use multiple TK1s to construct a platform as we did, and so they achieved a performance of only 1.85 fps. Our SECP is both more powerful and more flexible because it can not only run deep neural networks efficiently but also has high scalability and fault tolerance, making it suitable for practical applications.

Jagannathan et al. [14] introduced TIs low power TDA3x Soc which was based on a heterogeneous, scalable architecture to detect objects from images. They used HOG features and AdaBoost cascade classifier to detect objects, and CNN to classify the detected objects. Compared with our work, this research had two main drawbacks: 1) the object detection method was multi-step rather than end-to-end, and, 2) their single computing core was not powerful. The overall system performance was 15 fps, but the input images were very low resolution (32 Ã— 32). Our platform can solve HD-1080p video with the highest speed of 79 fps running on three TX2s.

Gao et al. [20] proposed a scalable distributed object detection framework based on an embedded manycore cluster architecture. The basic object detection algorithm was parallel process images by cascade classifier and local binary pattern operator. The framework was implemented on a Xilinx Zynq SoC and Adapteva Epiphany combined heterogeneous manycore platform named Parallella. The application of an embedded cluster made the object detection algorithm run in parallel, providing 7.8 times speedup over a dual-core ARM. However, they could only achieve 2 fps when solving 1920 Ã— 1080 resolution images. In addition, they did not consider fault tolerance and stability of the system as we have done in this paper.

There is also work that took advantage of cell phones to build mobile clouds called Mobile Storm [29], in which the work flow of a real-time stream processing job was modeled, then decomposed into several tasks so that the job can be executed concurrently and in a distributed manner on multiple mobile devices. It was implemented on Android phones and conducted on real-time HD video processing applications. This work is very similar with ours as it deployed Storm on embedded devices for real-time video processing. In addition, it also has high scalability by adding worker nodes. The results showed that the cluster with 5 nodes could handle a high resolution video stream at 19 fps. However, this cluster was evaluated using only face detection.

Some work is targeting elastic management of cloud resources. For example, Aljawarneh et al. [30] optimized the performance of big spatial data queries on top of Apache Spark, which also supported advanced management functions including a self-adaptable load-balancing service to self-tune framework execution. A comprehensive survey is conducted for stream processing by Assuno et al. [31] for achieving efficient resource management decisions based on current load, especially for edge and cloud computing, where our proposed SECP can serve as edge nodes. These works motivate our design of the self-management services in order to improve the reliability of SECP.

Kang et al. [32] proposed NOSCOPE targeting querying large scale of videos. It can reduce the cost of neural network video analysis by up to three orders of magnitude via an inference-optimized model search. Its purpose is to scale to thousands of hours of video, possibly from thousands of data feeds, for the purposes of large-scale video classification, but real-time performance is not its target. It used powerful GPUs to achieve good performance, which is not possible for the embedded devices we are testing.

Han et al. [33] demonstrated that they can achieve even higher accuracy while significantly reducing the computation and the size of models. However, this approach is not flexible enough. Some domain specific models have performed well and could be used directly on the proposed platform. It is unwise to rewrite every model using a compression way. We used a simple but effective method to find bottlenecks of our SECP, which calculates the mean and variance of each metric, rather than adopting complex methods as the work by Zhang et al. [34] and Chen et al. [35]. Because the Storm cluster is small, light-weight algorithms are best for achieving energy efficiency.

SECTION 5Conclusions and Future Work
Real-time video processing on embedded devices can be applied to many domains, including drones and smart cars. However, the limited resources of embedded devices make this a big challenge. The existing work usually use a single device without the support of cloud computing technologies. In this paper, we design a deep learning platform on embedded devices using stream processing cloud computing, which is called streaming embedded cloud platform, where NVIDIA Jetson TX1 and TX2 development boards are used, and Apache Storm is deployed as cloud computing environment for deep learning algorithms (Convolutional Neural Networks) to process video streams. Some self-managing services are designed to make sure the platform can run smoothly and stably. When facing accidents such as a computing core crash, a strategy can be executed to keep the platform running continuously with the least data loss.

We have evaluated the SECP platform in terms of processing speed, power consumption, and network throughput by running different kinds of deep learning algorithms for object detection. The results show the proposed platform can meet the requirement of real-time video processing with high scalability and fault tolerance, and is usable for running deep learning algorithms on embedded devices to realize real-time video processing.

In the future, we will pay more attention to video processing algorithm modification rather than using existing algorithms directly. We are considering designing algorithms specifically for the platform in order to take full advantage of computing resources to achieve higher speeds. Furthermore, it is advantageous that the SECP can communicate with cloud servers, where more tasks and decisions can be made.