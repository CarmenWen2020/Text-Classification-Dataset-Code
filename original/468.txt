Mobile edge computing is emerging as a new computing paradigm that provides enhanced experience to mobile users via low latency connections and augmented computation capacity. As the amount of user requests is time-varying, while the computation capacity of edge hosts is limited, Cloud Assisted Mobile Edge (CAME) computing framework is introduced to improve the scalability of the edge platform. By outsourcing mobile requests to clouds with various types of instances, the CAME framework can accommodate dynamic mobile requests with diverse quality of service requirements. In order to provide guaranteed services at minimal system cost, the edge resource provisioning and cloud outsourcing of the CAME framework should be carefully designed in a cost-efficient manner. Specifically, two fundamental issues should be answered: (1) what is the optimal edge computation capacity configuration? and (2) what types of cloud instances should be tenanted and what is the amount of each type? To solve these issues, we formulate the resource provisioning in CAME framework as an optimization problem. By exploiting the piecewise convex property of this problem, the Optimal Resource Provisioning (ORP) algorithms with different instances are proposed, so as to optimize the computation capacity of edge hosts and meanwhile dynamically adjust the cloud tenancy strategy. The proposed algorithms are proved to be with polynomial computational complexity. To evaluate the performance of the ORP algorithms, extensive simulations and experiments are conducted based on both the widely-used traffic models and the Google cluster usage tracelogs, respectively. It is shown that the proposed ORP algorithms outperform the local-first and cloud-first benchmark algorithms in system flexibility and cost-efficiency.
SECTION 1Introduction
With the popularity of intelligent devices, substantial computation-intensive delay-sensitive mobile applications keep emerging and have drawn enormous attentions, e.g., face recognition and augmented reality [1], [2]. To meet the quality of service (QoS) requirements of those applications, sufficient computation capacity should be provided. Nevertheless, mobile devices are usually resource-constrained in terms of computation capacity and battery lifetime. Mobile cloud computing has been proposed to deal with this conflict [3], [4]. In mobile cloud computing, mobile requests are transmitted through the wireless access network, and are further delivered to remote clouds through the Internet backhaul. However, the wide area network delay and jitters incurred during computation offloading are unpredictable, which may violate the QoS requirements of delay-sensitive applications, such as online gaming [9]. In addition, with the prevalence of mobile computing (e.g., virtual reality and augmented reality), offloading all the mobile requests to remote clouds will induce substantial communication and computation burden, which is beyond the capacity of the Internet core network and centralized data centers, respectively [6].

Mobile edge computing (MEC) is an ideal paradigm to address these problems. By deploying edge hosts within the wireless access network, mobile users are able to access sufficient computation resources without suffering from the uncontrollable Internet delay [7]. Due to the advantage of low latency, extensive efforts have been devoted to the potential applications of MEC [13], [14], [15], [16], [17], [18], [19]. In MEC, computation resources of edge hosts are pre-configured, yet computation requests of mobile users fluctuate significantly both in the short and long run [20]. Conventionally, computation resources are usually over-provisioned to provide guaranteed performance, leading to high cost and low resource utilization. For example, more than 10,000 computers should be provisioned to guarantee the operation of the online game World of Warcraft [21]. How to achieve cost-efficient resource provisioning for dynamic requests is crucial to mobile edge operators.

To cater dynamic requests with cost-efficiency, this work adopts the CAME framework, in which fixed edge hosts are deployed and elastic cloud resources are dynamic provisioned to accommodate dynamic requests [11]. Typically, commercial public computation resources are provided in elastic encapsulated instances (e.g., Amazon EC2,1 Microsoft Azure2 and Google Cloud Platform3). On-demand instances, reserved instances and spot instances are the main cloud instances, which are differentiated by pricing schemes [29]. The system cost of the CAME framework consists of two parts: edge provisioning cost, cost of deploying edge hosts; cloud tenancy cost, cost of tenanting instances from public clouds. As computation capacity of edge hosts is fixed and may not be sufficient, which directly affects system cost and QoS performance, two fundamental questions should be answered to achieve cost-efficiency: (1) What’s the optimal edge computation capacity configuration? (2) What types of cloud instances should be tenanted and what’s the amount of each type?

This paper addresses the above issues by investigating the resource provisioning problem with dynamic requests. Mobile requests are considered to be with diversified QoS requirements (including delay-sensitive and delay-tolerant requests), as shown in Fig. 1. Edge hosts execute all delay-sensitive and part of the delay-tolerant requests, and cloud instances are dynamically tenancted to serve the outsourced delay-tolerant requests. Among the three types of cloud instances, spot instances are not taken into consideration due to the possibility of service interruption [29]. Thus the resource provisioning problem in CAME can be divided into three categories: with on-demand instances, with reserved instances and hybrid strategy.


Fig. 1.
Resource provisioning in CAME.

Show All

We solve the resource provisioning problem by formulating an optimization problem that aims at minimizing the long-term average system cost. Resource provisioning with single-type cloud instances (i.e., with on-demand instances and with reserved instances) is first investigated, based on which resource provisioning with hybrid strategy is further explored. Based on queuing analysis, it is proved that this optimization problem is piecewise convex. By exploiting this property, the Optimal Resource Provisioning (ORP) algorithms are developed to deal with different categories, i.e., Optimal Resource Provisioning with On-Demand instances (ORP-OD), Optimal Resource Provisioning with Reserved instances (ORP-R) and Optimal Resource Provisioning with Hybrid Strategy(ORP-HS), respectively. In specific, the optimal computation capacity of edge hosts is determined by solving convex optimization problems in each segment of the resource provisioning problem. Then, the cloud tenancy strategy is further devised according to users’ QoS requirements.

The main contributions of this work are as follows.

The CAME framework is exploited to achieve cost-efficient resource provisioning for dynamic mobile requests. Considering different types of cloud instances, the resource provisioning problem is divided into three categories: with on-demand instances, with reserved instances and hybrid strategy. Based on queuing analysis, the resource provisioning problem is formulated as an optimization problem, which is proved to be piecewise convex.

By leveraging the piecewise convex property, the Optimal Resource Provisioning (ORP) algorithms are designed. Convex optimization problems are solved in each segment to determine the optimal computation capacity of edge hosts, and cloud tenancy strategy is further optimized based on the QoS requirements of mobile requests.

To evaluate the ORP algorithms, extensive simulations based on two widely-used traffic models and Google cluster tracelogs are implemented. Results demonstrate that the ORP algorithms outperform other benchmark algorithms in system flexibility and cost-efficiency.

This paper is organized as follows. Section 2 discusses the related work. In Section 3, the system model is presented and problem formulation is further described. Algorithm design is provided in Section 4, and simulations and analysis are presented in Section 5. Finally, the conclusion is given in Section 6.

SECTION 2Related Work
Mobile cloud computing provides an ideal solution to alleviating the conflict between resource-intensive mobile applications and resource-constrained mobile devices. Design and implementation of systems that support code computation or computation offloading have been presented in [3], [4]. Computation offloading problems have then been studied in extensive works. A user-oriented offloading framework has been proposed in [22] to implement efficient online computation offloading and the conditions in which mobile users can be benefited from computation offloading have been investigated in [23], [24]. However, uncontrollable WAN delay and jitters induced in conventional mobile cloud computing can significantly degrade the QoS for mobile users.

MEC is envisioned as an effective paradigm to address this challenge, by deploying edge hosts within wireless access network and in close proximity to mobile devices. Many efforts have been devoted to fully exploiting the advantages of MEC. Workload scheduling problems in MEC have been well studied in substantial existing works. Tong et al. have designed a hierarchical edge cloud architecture to schedule the peak mobile workloads [25]. Workload placement and resource provisioning decisions have further been made to achieve high resource utilization. Sun et al. have proposed the LEARN algorithm for the replica placement of Avatar virtual disks in the cloudlet network [12]. With this algorithm, adaptive Avatar handoff can be achieved with low E2E latency.

In MEC, the resource provisioning problem is also a critical issue since the computation capacity of edge hosts is fixed and constrained. In [15], a service-oriented resource management framework has been proposed for fog computing, in which resource prediction, resource reservation and pricing can be addressed. Kiani et al. have explored hierarchical capacity provisioning in edge computing [17]. A 2-tier edge computing architecture has been proposed and the capacity provisioning problems have been solved by stochastic ordering and optimization problems. Enayet et al. [26] have presented Mobi-Het, a mobility-aware resource allocation architecture, to execute tasks at remote cloudlets with high efficiency and reliability. In existing works of resource provisioning for MEC, dynamics of mobile requests have not been taken into consideration, which can incur overprovisioning of edge hosts and low resource utilization. Federating the internal infrastructures with public clouds provides an effective solution to augment system computation capacity and provision elastic resources for dynamic requests [27], [28]. Cloud-fog interoperation has been explored in an SDN enabled framework, which aims at improving the QoS and optimizing the utilization of network resources [30]. In our previous work, the CAME framework has been proposed to augment the computation capacity of MEC by tenanting computation resources from public clouds [11]. In this paper, to achieve cost-efficient resource provisioning for dynamic requests, we investigate the resource provisioning problem in CAME.

In the CAME framework, the flexibility of MEC with dynamic requests is enhanced by exploiting the scalability of cloud resources. Different from most existing works in which the computation capacity of edge hosts is assumed to be given, we aim to investigate the optimal edge computation capacity and further dynamically tune the usage of cloud resources. The preliminary results of resource provisioning with on-demand instances have demonstrated the effectiveness of the CAME framework in enhancing the scalability of system resources [32]. In this paper, the resource provisioning problem for both delay-tolerant and delay-sensitive mobile requests is investigated. Furthermore, to fully exploit the advantages of different cloud instances (e.g., availability, economy and scalability), three resource provisioning categories (i.e., with on-demand instances, with reserved instances and hybrid strategy) are studied. By addressing this resource provisioning problem, different QoS requirements of mobile applications can be satisfied at the minimum system cost.

SECTION 3System Model and Problem Formulation
In the resource provisioning problem, Mobile requests are considered to be with different QoS requirements (including delay-sensitive and delay-tolerant requests). As shown in Fig. 1, all delay-sensitive requests are transmitted to edge hosts via the wireless access network, while part of delay-tolerant requests can further be outsourced to the remote cloud through Internet backhaul [10]. The request scheduling issue has been well studied in [11], [12], which is not the focus of this work. Delay-tolerant requests are allocated to edge hosts and cloud instances in proportion to the available computation capacity of each part. A set of T intervals is considered as a cycle (e.g., there are 24 intervals in a cycle for daily requests). Non-homogeneous Poisson traffic model is adopted to characterize the dynamic delay-sensitive and delay-tolerant requests, with the average arrival rates of λt1, λt2 (t∈{1,2,…,T}) respectively [33]. Then the CAME framework can be modeled as a queuing network, as presented in Fig. 2. When offloading mobile requests to edge hosts or further outsourced to the cloud, additional communication requests (program codes or data) are incurred. μa represents the transmission rate of communication requests in the wireless access network, and d is the WAN latency from mobile edge to the cloud. Denote by μe the computation capacity of edge hosts and μtc represents the serving rate of cloud instances. Then the system delay can be calculated based on queuing theory [34]. The notations are listed in Table 1.


Fig. 2.
Analytical model.

Show All

TABLE 1 Table of Notations
Table 1- 
Table of Notations
3.1 System Delay
In the wireless access network, the communication traffic is related to mobile computation requests. For example, when computation requests of a face recognition request arrive at edge hosts, the figure or data including features extracted from the figure should also be transmitted via the wireless access network. Thus, the arrival rate of communication requests in the access network can be represented as ρ1(λt1)+ρ2(λt2), where ρ1(λt1) and ρ2(λt2) are functions increasing with λt1 and λt2, respectively (ρ1(λt1) can be a linear function of λt1, which means c1 units of data should be transmitted when one unit of delay-sensitive requests is offloaded to edge hosts from mobile devices [11]). Denote by Dtcomm the communication delay in the wireless access network. Then, Dtcomm can be calculated as
Dtcomm=1μa−[ρ1(λt1)+ρ2(λt2)],(1)
View Sourcewhere μa>ρ1(λt1)+ρ2(λt2) to ensure the system stability.

To satisfy the QoS requirements, all delay-sensitive requests should be executed at edge hosts, with higher priority compared with delay-tolerant requests. Suppose the service time of edge hosts and cloud instances are exponentially distributed, then the service processes of mobile edge and cloud can be modeled as M/M/1 queues in each time interval [31]. Thus the serving rate demanded to execute delay-sensitive requests in the tth time interval is
μts=1D1−1μa−[ρ1(λt1)+ρ2(λt2)]+λt1,(2)
View Sourcewhere D1 represents the QoS requirement of the delay-sensitive mobile requests. As all delay-sensitive mobile requests are executed at mobile edge, it holds that
μe≥max{μts,t=1,2,...T}.(3)
View Source

When no computation capacity is provisioned for delay-tolerant requests at edge nodes (i.e., μe=μts), all delay-tolerant requests are outsourced to the cloud and the computation delay is 1μtc−λt2+d. When no cloud instances are tenanted (i.e., μtc=0), all delay-tolerant requests are executed at edge hosts and the computation delay is 1μe−μts−λt2. Otherwise (μe>μts, μtc>0), delay-tolerant requests scheduled to edge hosts and cloud instances are proportional to the available computation capacities of each part, and the arrival rates are μe−μtsμe−μts+μtcλt2 and μtcμe−μts+μtcλt2, respectively. The computation delay is the weighted sum of the delay at each part with the weights of μe−μtsμe−μts+μtc and μtcμe−μts+μtc. Based on above analysis, the computation delay of delay-tolerant requests can be summarized as follows:
Dtcomp=⎧⎩⎨⎪⎪⎪⎪⎪⎪⎪⎪2μe−μts+μtc−λt2+dμtcμe−μts+μtc,  1μtc−λt2+d,  1μe−μts−λt2,  μe>μts,μtc>0,μe=μts,μtc>0,μe>μts,μtc=0,(4)
View SourceRight-click on figure for MathML and additional features.where λt2<μe−μts+μtc to ensure the stability of queues.

The overall system delay of delay-tolerant requests includes the saverage computation delay (when executed at mobile edge or further offloaded to the cloud) and the communication delay in the wireless access network. Thus, the system delay Dt can be represented as
Dt=Dtcomp+Dtcomm.(5)
View Source

3.2 System Cost
In the resource provisioning problem of CAME, the system cost includes edge provisioning cost and cloud tenancy cost. On-demand instances, reserved instances and spot instances are the main cloud instances, with respect to different pricing schemes. Spot instances allow users to access spare computation resources for even greater discount compared to on-demand price. Nevertheless, applications scheduled to spot instances are required to have flexible start and end times in case of interruption. Furthermore, the pricing rate of spot instances varies with the relationship between supply and demand of cloud resources. To ensure the QoS of the CAME framework, spot instances are not considered in this work. Thus, the resource provisioning problem is divided into three categories: with on-demand instances, with reserved instances and hybrid strategy. In these cases, the system cost can be calculated as follows.

3.2.1 Edge Provisioning Cost
The dominating monetary cost of deploying a mobile edge goes to server purchasing [35]. In addition, the computation rates of mobile edge increases with the number of servers [36]. Therefore, the edge provisioning cost can be expressed as
C(μe)=ceμe.(6)
View SourceA typical server with 9.6 GHz costs $3000, with an amortization of 3 years [35], [37].

In practical application, provisioning larger computation capacity at mobile edge yields much more expenses on parallelism. A convex pricing model is also employed in this work to approximate the edge provisioning cost as
C(μe)=ca(μe)θ,(7)
View Sourcewhere ca and θ are constants, and ca>0, θ>1. Notice that the linear pricing model is a special case of convex pricing model with θ=1.

3.2.2 Cloud Tenancy Cost
On-demand instances are charged by per hour or even per second without long-term reservation. Denote by cp and μto the pricing rate and serving rate of on-demand instances, respectively. Then, the cost of on-demand instances can be represented as
Ct(μto)=cpμto.(8)
View SourceReserved instances provide users with a significant price discount (up to 75 percent [29]) compared with on-demand instances, while a one-year or three-year commitment is demanded. Therefore, the cost of reserved cloud instances is given by
C(μr)=rcpμr,(9)
View Sourcewhere r represents the discount ratio of reserved instances over on-demand instances and μr denotes the serving rate of reserved instances.

3.3 Problem Formulation
In the resource provisioning problem of CAME, the computation capacity of edge hosts and cloud tenancy strategy should be jointly optimized to achieve minimum system cost, formulated as follows: \begin{align*} & {\mathbf (P1)}: \min\; S(\mu _{\mathrm {e}},\mu _{\mathrm {c}}^t)=C({\mu _{\mathrm {e}}})+\frac{1}{T}\sum \limits _{t = 1}^T {C^t({\mu _{\mathrm {c}}^t})}\\ &\qquad s.t.~~{\mu _{\mathrm {e}}-\mu _{\mathrm {s}}^t} \geq 0 \qquad\qquad\quad t \in \lbrace 1,2,\ldots,T\rbrace\\ &\qquad\qquad\ {\mu _{\mathrm {c}}^t} \geq 0 \qquad\qquad\qquad\;\;\ t \in \lbrace 1,2,\ldots,T\rbrace\\ &\qquad {\mathbf C1}:{\mu _{\mathrm {e}}-\mu _{\mathrm {s}}^t} + {\mu _{\mathrm {c}}^t} > {\lambda _2^t}\quad\;\ t \in \lbrace 1,2,\ldots,T\rbrace\\ &\qquad {\mathbf C2}:{D^t}(\mu _{\mathrm {e}},\mu _{\mathrm {c}}^t) \leq D_2\quad\quad\;\;\; t \in \lbrace 1,2,\ldots,T\rbrace, \tag{10} \end{align*}
View SourceRight-click on figure for MathML and additional features.where D_2 represents the delay requirement of delay-tolerant requests. In this problem, the objective is to minimize the system cost. As the usage of cloud resources varies with different time intervals, the long-term average cloud tenancy cost is \frac{1}{T}\sum \limits _{t = 1}^T {C^t({\mu _{\mathrm {c}}^t})}. C1 constraints serving rates to ensure the stability of system, and C2 guarantees the system delay to meet the QoS requirements.

As the computation capacity of edge hosts \mu _{\mathrm {e}} is unchanged through all time intervals, (P1) can be solved as follows:

Given the computation capacity of edge hosts \mu _{\mathrm {e}}, obtain the optimal cloud tenancy strategy \mu _{\mathrm {c}}^t (t \in \lbrace 1,2,\ldots,T\rbrace) satisfying \begin{align*} {\mathbf (P2)}:\; &\min {\mathrm {\ }}C^t({\mu _{\mathrm {c}}^t})\\ & s.t.{\mathrm {\ \ \ }}{\mu _{\mathrm {c}}^t} \geq 0\\ & {\mathbf C3:}{\mathrm {\ \ }}{D^t}({\mu _{\mathrm {c}}^t}) \leq D_2\\ &\quad {\mathrm {\ \ \ \ \ \ }}{\mu _{\mathrm {c}}^t} > {\lambda _2^t} - ({\mu _{\mathrm {e}} -\mu _{\mathrm {s}}^t}). \tag{11} \end{align*}
View SourceDenote by \mu _{\mathrm {c}}^{t*}(\mu _{\mathrm {e}}) the optimal solution of (P2), then the cloud tenancy cost can be represented as C^t(\mu _t^{\mathrm {c*}}(\mu _{\mathrm {e}})).

Determine the optimal computation capacity of edge hosts \mu _{\mathrm {e}} satisfying \begin{align*} {\mathbf (P3)}:\;& \min \frac{1}{T}\sum \limits _{t = 1}^T {{C^t}(\mu _t^{\mathrm {c*}}({\mu _{\mathrm {e}}}))} + C({\mu _{\mathrm {e}}})\\ & \quad s.t.\;\;\;{\mu _{\mathrm {e}}-\mu _{\mathrm {s}}^t} \geq 0 ~~~~ t \in \lbrace 1,2,\ldots,T\rbrace . \tag{12} \end{align*}
View Source

When \mu _{\mathrm {e}} is given, the optimal cloud tenancy strategy can be represented as a function of \mu _{\mathrm {e}} by solving P2. Then the optimal computation capacity of edge hosts \mu _{\mathrm {e}} can be further determined by solving the piecewise convex optimization problem (P3). Based on above analysis, the ORP algorithms are designed to address the three categories of the resource provisioning problem.

SECTION 4Algorithm Design
This part provides algorithm design for the resource provisioning problem in CAME. Three optimal resource provisioning algorithms, i.e., ORP-OD, ORP-R and ORP-HS, are designed to deal with the optimal resource problem with on-demand, reserved instances and hybrid strategy, respectively. The details and analysis are as follows.

4.1 Resource Provisioning with On-Demand Instances
In the category of on-demand instances, cloud resources are tenanted on demand without long-term reservation [29]. Thus, in the problem formulation of (P1), \begin{align*} & \mu _{\mathrm {c}}^t=\mu _{\mathrm {o}}^t,\\ & C^t(\mu _{\mathrm {c}}^t)=C^t(\mu _{\mathrm {o}}^t). \tag{13} \end{align*}
View SourceThe relationship between the optimal usage of on-demand instances and computation capacity of edge hosts is first explored. Then the computation capacity of edge hosts is further optimized based on the above results.

4.1.1 Optimize Usage of On-Demand Instances
The optimal usage of on-demand instances can be obtained by solving (P2) when the computation capacity of edge hosts is given. As the cloud tenancy cost {{C^t}({\mu _{\mathrm {o}}^t})} increases with the serving rate of on-demand instances \mu _{\mathrm {o}}^t, solving (P2) is equivalent to searching for the minimum \mu _{\mathrm {o}}^t within the constraints.

To ensure C3, it should be satisfied that \begin{equation*} \alpha \cdot {(\mu _{\mathrm {o}}^t)^2} + \beta ({\mu _{\mathrm {e}}})\mu _{\mathrm {o}}^t + \gamma ({\mu _{\mathrm {e}}}) \geq 0, \tag{14} \end{equation*}
View SourceRight-click on figure for MathML and additional features.where \alpha, \beta (\mu _{\mathrm {e}}) and \gamma (\mu _{\mathrm {e}}) are given by \begin{align*} & \alpha=D(\lambda_2^t)-d,\\ &\beta (\mu _{\mathrm {e}})=(2D(\lambda _2^t)-d)({\mu _{\mathrm {e}}-\mu _{\mathrm {s}}^t})+ \lambda _2^t d - D(\lambda _2^t)\lambda _2^t - 2,\\ & \gamma (\mu _{\mathrm {e}})=D(\lambda _2^t)({\mu _{\mathrm {e}}-\mu _{\mathrm {s}}^t})^2 - D(\lambda _2^t)\lambda _2^t ({\mu _{\mathrm {e}} -\mu _{\mathrm {s}}^t})\\ & \qquad\quad - 2({\mu _{\mathrm {e}}-\mu _{\mathrm {s}}^t}), \tag{15} \end{align*}
View SourceRight-click on figure for MathML and additional features.where D(\lambda _2^t) represents the computation delay requirement of delay-tolerant applications, i.e., D(\lambda _2^t)=D_2-{D_{\mathrm {comm}}^t}. The round trip delay d < D(\lambda _2^t) to guarantee the QoS of delay-tolerant requests.

Define \begin{equation*} \Delta ({\mu _{\mathrm {e}}}) = {[\beta ({\mu _{\mathrm {e}}})]}^2 - 4\alpha \gamma ({\mu _{\mathrm {e}}}). \tag{16} \end{equation*}
View SourceRight-click on figure for MathML and additional features.It can be proved that for any \mu _{\mathrm {e}}, there exists \Delta ({\mu _{\mathrm {e}}}) > 0. The details are as follows.

Proof.
\begin{align*} \Delta ({\mu _{\mathrm {e}}})& = {[\beta ({\mu _{\mathrm {e}}})]}^2 - 4\alpha \gamma ({\mu _{\mathrm {e}}})\\ &=d^2({\mu _{\mathrm {e}}-\mu _{\mathrm {s}}^t})^2+\delta ({\mu _{\mathrm {e}}-\mu _{\mathrm {s}}^t})+\epsilon, \tag{17} \end{align*}
View Sourcewhere \begin{align*} \delta &=2d\lambda _2^tD(\lambda _2^t)-2\lambda _2^td^2-4d\\ \epsilon &=[{D(\lambda _2^t)}]^2({\lambda _2^t})^2+d^2({\lambda _2^t})^2-2dD(\lambda _2^t)({\lambda _2^t})^2\\ & \quad +4D(\lambda _2^t)\lambda _2^t-4d\lambda _2^t+4. \tag{18} \end{align*}
View SourceSince d^2 > 0 and {\delta }^2-4d^2\epsilon =32d^2\lambda _2^t(d-D(\lambda _2^t)) < 0, there exists \begin{equation*} \Delta ({\mu _{\mathrm {e}}})=d^2({\mu _{\mathrm {e}}-\mu _{\mathrm {s}}^t})^2+\delta ({\mu _{\mathrm {e}}-\mu _{\mathrm {s}}^t})+\epsilon > 0, \tag{19} \end{equation*}
View Sourcefor any \mu _{\mathrm {e}}.

As a > 0 and \Delta ({\mu _{\mathrm {e}}}) > 0, Eq. (14) can be rewritten as \mu _{\mathrm {o}}^t\leq f_{\mathrm {d}}^t(\mu _{\mathrm {e}}) or \mu _{\mathrm {o}}^t\geq f_{\mathrm {u}}^t(\mu _{\mathrm {e}}), where \begin{align*} & f_{\mathrm {d}}^t(\mu _{\mathrm {e}})=\frac{-\beta (\mu _{\mathrm {e}})-\sqrt{ \Delta ({\mu _{\mathrm {e}}})}}{2\alpha }\\ & f_{\mathrm {u}}^t(\mu _{\mathrm {e}})=\frac{-\beta (\mu _{\mathrm {e}})+\sqrt{ \Delta ({\mu _{\mathrm {e}}})}}{2\alpha }. \tag{20} \end{align*}
View SourceRight-click on figure for MathML and additional features.

Theorem 1.
When the computation capacity of edge hosts \mu _{\mathrm {e}} is given, the optimal usage of on-demand instances is given by \begin{equation*} \mu _{\mathrm {o}}^{t*}({\mu _{\mathrm {e}}}) = \left\lbrace \begin{array}{l}0 \qquad\qquad\qquad {\mu _{\mathrm {e}}} \geq \mu _{\mathrm {s}}^t+{\lambda _2^t} + \frac{1}{D(\lambda _2^t)}\\ f_{\mathrm {u}}^t({\mu _{\mathrm {e}}}) \qquad \mu _{\mathrm {s}}^t < {\mu _{\mathrm {e}}} < \mu _{\mathrm {s}}^t+ {\lambda _2^t} + \frac{1}{D(\lambda _2^t)}\\ {\lambda _2^t} + \frac{1}{{D(\lambda _2^t) - d}}\qquad\qquad\qquad\quad {\mu _{\mathrm {e}}} = \mu _{\mathrm {s}}^t. \end{array} \right. \tag{21} \end{equation*}
View SourceRight-click on figure for MathML and additional features.

Proof.
Define F(\mu _{\mathrm {o}}^t) as \begin{align*} F(\mu _{\mathrm {o}}^t)&=(D(\lambda _2^t) - d)({\mu _{\mathrm {o}}^t})^2\\ & \quad + (2D(\lambda _2^t){\mu _{\mathrm {e}}}+ \lambda _2^t d - D(\lambda _2^t)\lambda _2^t - {\mu _{\mathrm {e}}}d - 2){\mu _{\mathrm {o}}^t}\\ & \quad + (D(\lambda _2^t)({\mu _{\mathrm {e}}})^2 - D\lambda _2^t {\mu _{\mathrm {e}}} - 2{\mu _{\mathrm {e}}}). \tag{22} \end{align*}
View SourceRight-click on figure for MathML and additional features.Then, \begin{align*} & F(\lambda _2^t-(\mu _{\mathrm {e}}-\mu _{\mathrm {s}}^t))\\ &=(D(\lambda _2^t) - d)(\lambda _2^t-(\mu _{\mathrm {e}}-\mu _{\mathrm {s}}^t))^2\\ & \quad +(2D(\lambda _2^t){(\mu _{\mathrm {e}}-\mu _{\mathrm {s}}^t)}+ \lambda _2^t d - D(\lambda _2^t)\lambda _2^t - {(\mu _{\mathrm {e}}-\mu _{\mathrm {s}}^t)}d - 2)\\ & \quad *(\lambda _2^t-(\mu _{\mathrm {e}}-\mu _{\mathrm {s}}^t))\\ & \quad +D(\lambda _2^t)({\mu _{\mathrm {e}}-\mu _{\mathrm {s}}^t})^2 - D(\lambda _2^t)\lambda _2^t {(\mu _{\mathrm {e}}-\mu _{\mathrm {s}}^t)}-2{(\mu _{\mathrm {e}}-\mu _{\mathrm {s}}^t)}\\ &=-2\lambda _2^t < 0 \tag{23} \end{align*}
View SourceRight-click on figure for MathML and additional features.Since there is F(\mu _{\mathrm {o}}^t)\geq 0 when \mu _{\mathrm {o}}^t\leq f_{\mathrm {d}}^t(\mu _{\mathrm {e}}) and \mu _{\mathrm {o}}^t\geq f_{\mathrm {u}}^t(\mu _{\mathrm {e}}), and F(\lambda _2^t-(\mu _{\mathrm {e}}-\mu _{\mathrm {s}}^t)) < 0, the following holds: \begin{equation*} f_{\mathrm {d}}^t(\mu _{\mathrm {e}}) < \lambda _2^t-(\mu _{\mathrm {e}}-\mu _{\mathrm {s}}^t) < f_{\mathrm {u}}^t(\mu _{\mathrm {e}}). \tag{24} \end{equation*}
View SourceTo ensure the constraints in (P2), it should satisfy that \begin{align*} & \mu _{\mathrm {o}}^t \geq 0\\ & \mu _{\mathrm {o}}^t \geq \lambda _2^t-(\mu _{\mathrm {e}}-\mu _{\mathrm {s}}^t)\\ & \mu _{\mathrm {o}}^t\geq f_{\mathrm {u}}^t(\mu _{\mathrm {e}})~\mathrm {or}~\mu _{\mathrm {o}}^t\leq f_{\mathrm {d}}^t(\mu _{\mathrm {e}}). \tag{25} \end{align*}
View SourceCombining Eqs. (24) and (25), when given \mu _{\mathrm {e}}, the optimal usage of on-demand instances can be obtained as presented in Eq. (21).

Therefore, in the resource provisioning with on-demand instances, the optimal usage of on-demand instances can be represented by a function of \mu _{\mathrm {e}}, as shown in Eq. (21).

4.1.2 Optimize Computation Capacity of Edge Hosts
According to Theorem 1, we can conclude that the objective function of (P3) is a piecewise function over \mu _{\mathrm {e}}, and the boundaries are (\mu _{\mathrm {s}}^t+{\lambda _2^t} + \frac{1}{D(\lambda _2^t)}), t\in \lbrace 1,2,\ldots,T\rbrace. Sort the boundaries and let \begin{equation*} b_j=\mu _{\mathrm {s}}^{t(j)}+\lambda _2^{t(j)}+\frac{1}{D(\lambda _2^{t(j)})}, \tag{26} \end{equation*}
View SourceRight-click on figure for MathML and additional features.where t(j) represents the time interval with the jth smallest (\mu _{\mathrm {s}}^{t}+\lambda _2^t+\frac{1}{D(\lambda _2^t)}). Define b_0=\max {\lbrace \mu _{\mathrm {s}}^t,j\in \lbrace 1,2,\ldots,T\rbrace \rbrace } and b_{T+1}=+\infty. Then there is the following conclusion.

Theorem 2.
The long-term average system cost \frac{1}{T}\sum \limits _{t = 1}^T {{C^t}(\mu _{\mathrm {o}}^{t*}({\mu _{\mathrm {e}}}))} + C({\mu _{\mathrm {e}}}) is convex over \mu _{\mathrm {e}} in each domain bounded by b_j, j\in \lbrace 0,1,2,\ldots,T,T+1\rbrace.

Proof.
As indicated in Eq. (21), the minimum cloud tenancy cost C^t(\mu _{\mathrm {c}}^{t*}(\mu _{\mathrm {e}})) equals c_pf_{\mathrm {u}}^t({\mu _{\mathrm {e}}}) when {\mu _{\mathrm {e}}}\in (\mu _{\mathrm {s}}^{t}, \mu _{\mathrm {s}}^{t}+{\lambda _2^t} + \frac{1}{D(\lambda _2^t)}). Thus, there exists \begin{equation*} C^t(\mu _{\mathrm {c}}^{t*}(\mu _{\mathrm {e}}))=\frac{c_p*(-\beta (\mu _{\mathrm {e}})+\sqrt{ \Delta ({\mu _{\mathrm {e}}})})}{2\alpha }. \tag{27} \end{equation*}
View SourceBased on the results of Eqs. (15) and (17), the second derivative of C^t(\mu _{\mathrm {c}}^{t*}(\mu _{\mathrm {e}})) over \mu _{\mathrm {e}} can be given by \begin{equation*} [C^t(\mu _{\mathrm {c}}^{t*}(\mu _{\mathrm {e}}))]^{^{\prime \prime }}_{\mu _{\mathrm {e}}}=\frac{2c_p\lambda _2^td^2}{(\sqrt{{\beta (\mu _{\mathrm {e}})}^2-4\alpha \gamma (\mu _{\mathrm {e}})})^3}. \tag{28} \end{equation*}
View SourceFor each t\in \lbrace 1,2,\ldots,T\rbrace, there exists [C^t(\mu _{\mathrm {c}}^{t*}(\mu _{\mathrm {e}}))]^{^{\prime \prime }}_{\mu _{\mathrm {e}}} > 0 when {\mu _{\mathrm {e}}}\in (\mu _{\mathrm {s}}^{t}, \mu _{\mathrm {s}}^{t}+{\lambda _2^t} + \frac{1}{D(\lambda _2^t)}), thus C^t(\mu _{\mathrm {c}}^{t*}(\mu _{\mathrm {e}})) is a convex function over \mu _{\mathrm {e}}. In addition, as revealed in Eq. (7), C(\mu _{\mathrm {e}}) is a convex function over \mu _{\mathrm {e}}, (\mu _{\mathrm {e}}\geq 0). Thus the long-term system cost \frac{1}{T}\sum \limits _{t = 1}^T {{C^t}(\mu _{\mathrm {cloud}}^{t*}({\mu _{\mathrm {e}}}))} + C({\mu _{\mathrm {e}}}) is the sum of several convex functions in each segment bounded by b_j, j\in \lbrace 0,1,2,\ldots,T,T+1\rbrace. Thus, Theorem 2 can be proved.

It can be inferred from Theorem 2 that (P3) is a piecewise convex optimization problem over \mu _{\mathrm {e}}. Thus, to obtain the optimal computation capacity of edge hosts, we just need to solve convex optimization problems in each domain of \mu _{\mathrm {e}} bounded by b_j (j\in \lbrace 0,1,2,\ldots,T,T+1\rbrace) [38], and the optimal usage of on-demand instances can be further determined based on Theorem 1. Above all, the resource provisioning with on-demand instances can be solved by ORP-OD, as shown in Algorithm 1.

As the gradient of system cost over \mu _{\mathrm {e}} can be expressed in a closed form (as shown in step 4), the optimal solutions of the convex optimization problems can be obtained by bisection method, as illustrated in step 5 to step 13. Then, (P3) can be solved by selecting from the solutions of these convex optimization problems. Since the sorting of boundaries induces the main calculation, the computation complexity of this algorithm is O(T \log T).

4.2 Resource Provisioning with Reserved Instances
In the resource provisioning with reserved instances, the serving rate of cloud instances remains unchanged through the T time intervals, due to the long-term commitment of reserved instances [29]. Denote by \mu _{\mathrm {r}} the serving rate of reserved instances, and we have \begin{align*} & \mu _{\mathrm {c}}^t=\mu _{\mathrm {r}},\\ &C^t(\mu _{\mathrm {c}}^t)=C(\mu _{\mathrm {r}}), \tag{29} \end{align*}
View SourceRight-click on figure for MathML and additional features.in the problem formulation (P1).

As the serving rate of reserved instances remains unchanged, the provisioned computation capacity of both edge hosts and cloud instances should ensure the QoS of mobile requests in the long run. Thus, when the computation capacity of edge hosts \mu _{\mathrm {e}} is provided, the optimal usage of reserved instances can be given by \begin{equation*} \mu _{\mathrm {r}}^*(\mu _{\mathrm {e}})=\mathop {\max }\limits _{t\in \lbrace 1,2,\ldots,T\rbrace }\lbrace \mu _{\mathrm {o}}^{t*}(\mu _{\mathrm {e}})\rbrace, \tag{30} \end{equation*}
View Sourcewhere \mu _{\mathrm {o}}^{t*}(\mu _{\mathrm {e}}) is as shown in Eq. (21). Then (P3) is transformed into \begin{align*} {\mathbf (P4)}:\; &\min \lbrace C(\mu _{\mathrm {r}}^*(\mu _{\mathrm {e}})) + C(\mu _{\mathrm {e}})\rbrace\\ & s.t.~~{\mu _{\mathrm {e}}-\mu _{\mathrm {s}}^t} \geq 0 ~~~~ t \in \lbrace 1,2,\ldots,T\rbrace . \tag{31} \end{align*}
View Source

Remark.
According to the proof of Theorem 2, \mu _{\mathrm {o}}^{t*}(\mu _{\mathrm {e}}) is convex in each domain of \mu _{\mathrm {e}} bounded by (b_{j-1},b_j), (j\in \lbrace 1,2...,T,T+1\rbrace). Then \mathop {\max }\limits _{t\in \lbrace 1,2,\ldots,T\rbrace }\lbrace \mu _{\mathrm {o}}^{t*}(\mu _{\mathrm {e}})\rbrace is convex in each domain of \mu _{\mathrm {e}}. In addition, C(\mu _{\mathrm {e}}) is a convex function over \mu _{\mathrm {e}}. Thus, (P4) is a piecewise convex optimization problem and can be solved as summarized in Algorithm 2.

Algorithm 1. ORP-R: Optimal Resource Provisioning with On-Demand Instances
Input:

The arrival rates of delay-sensitive and delay-tolerant requests in each time interval [\lambda _1^t,\lambda _2^t], t\in \lbrace 1,2,\ldots,T\rbrace.

Output:

The optimal computation capacity of edge hosts \mu _{\mathrm {e}}^*;

The optimal usage of on-demand instances in each time interval \mu _{\mathrm {o}}^{t*}, t\in \lbrace 1,2,\ldots,T\rbrace.

Define \mu _{\mathrm {e\_0}}=\mathop {\max }\limits _{t \in \lbrace 1,2,\ldots,T\rbrace } \lbrace {\mu _t}\rbrace, representing the original solution of \mu _{\mathrm {e}}, and \mu _{\mathrm {e\_j}} as the optimal solution in the jth segment.

Sort (\mu _{\mathrm {s}}^t+\lambda _2^t+\frac{1}{D(\lambda _2^t)}), t \in \lbrace 1,2,\ldots,T\rbrace and denote as b_j, (j\in \lbrace 0,1,2,\ldots,T,T+1\rbrace).

for each segment of \mu _{\mathrm {e}} bounded by (b_{j-1},b_j), where j \in \lbrace 1,2,\ldots,T\rbrace, b_{j-1},b_j\geq \mu _{\mathrm {e}\_0} do

Obtain the gradient of system cost by

[S(\mu _{\mathrm {e}})]^{^{\prime }}=\frac{c_p}{T}\sum \limits _{i = j}^T {[f _{t(i)}^{\mathrm {u}}({\mu _{\mathrm {e}}})]^{^{\prime }}} + c_0\theta (\mu _{\mathrm {e}})^{\theta -1}.

if [S(b_{j-1})]^{^{\prime }}*[S(b_j)]^{^{\prime }} > 0 then

if [S(b_{j-1})]^{^{\prime }} > 0 then

\mu _{\mathrm {e\_j}}=b_{j-1}+\varepsilon.

else

\mu _{\mathrm {e\_j}}=b_{j}-\varepsilon.

end if

else

Search the extreme point \mu _{\mathrm {e}\_{\mathrm {ex}}} ([S(\mu _{\mathrm {e}\_{\mathrm {ex}}})]^{^{\prime }}=0) with bisection search method.

\mu _{\mathrm {e\_j}}=\mu _{\mathrm {e}\_{\mathrm {ex}}}.

end if

end for

\mu _{\mathrm {e}\_{\lbrace T+1\rbrace }}= b_{T}+\varepsilon

\mu _{\mathrm {e}}^* = \mathop {\arg \min }\limits _{{\mu _{\mathrm {e}}} \in \lbrace \mu _{\mathrm {e}\_1},\ldots,\mu _{\mathrm {e}\_{\lbrace T + 1\rbrace }}\rbrace } \lbrace S({\mu _{\mathrm {e}}},{\mu _{t}^{\mathrm {o*}}({\mu _{\mathrm {e}}})})\rbrace.

Compute \mu _{\mathrm {o}}^{t*} according to Eq. (21).

As shown in Algorithm 2, in each domain of \mu _{\mathrm {e}} bounded by (b_{j-1},b_j), j \in \lbrace 1,2,\ldots,T\rbrace, the optimal solution of \mu _{\mathrm {e}} can be obtained by Gradient Descending from Step 3 to Step 15, as (P4) is a convex optimization problem over \mu _{\mathrm {e}} in each domain. It is intuitive that the computation complexity from Step 3 to Step 7 is O(T). From Step 8 to Step 14, gradient descending takes O(\log (1/\sigma)) iterations, with each iteration yielding the complexity of O(T) to calculate the gradient \varrho. Thus the computation complexity from Step 8 to Step 14 is O(T\;\log (1/\sigma)), and Algorithm 2 yields the computation complexity of O(T^2\;\log (1/\sigma)).

Algorithm 2. ORP-R: Optimal Resource Provisioning with Reserved Instances
Input:

The arrival rates of delay-sensitive and delay-tolerant requests in each time interval [\lambda _1^t,\lambda _2^t], t\in \lbrace 1,2,\ldots,T\rbrace.

Output:

The optimal computation capacity of edge hosts \mu _{\mathrm {e}}^*;

The optimal usage of reserved instances \mu _{\mathrm {r}}.

Sort (\mu _{\mathrm {s}}^t+\lambda _2^t+\frac{1}{D(\lambda _2^t)}), t\in \lbrace 1,2,\ldots,T\rbrace and denote as b_j, (j\in \lbrace 0,1,2,\ldots,T,T+1\rbrace).

for each domain of \mu _{\mathrm {e}} bounded by (b_{j-1},b_j), where j \in \lbrace 1,2,\ldots,T\rbrace, b_{j-1},b_j\geq \mu _{\mathrm {e}\_0} do

Set the original value \mu _{\mathrm {e}}=b_{j-1}.

Find the time interval i that has the largest f_{\mathrm {u}}^t({\mu _{\mathrm {e}}}), i.e., i = \mathop {\arg \max }\limits _{t \in \lbrace 1,2,\ldots,T\rbrace } f_{\mathrm {u}}^t({\mu _{\mathrm {e}}}).

\varrho =r*c_p*[f_{\mathrm {u}}^i({\mu _{\mathrm {e}}})]^{^{\prime }}+c_0*\theta *({\mu _{\mathrm {e}}})^{\theta -1}, representing the local gradient.

Let the learning rate \tau =1.

Let the learning precision \sigma =10^{-5}.

while \mu _{\mathrm {e}} < b_j~\&\&~\mu _{\mathrm {e}} > b_{j-1}~\&\&~\phi > \sigma do

\mu _{\mathrm {e}}^{\mathrm {old}}=\mu _{\mathrm {e}}.

\mu _{\mathrm {e}}=\mu _{\mathrm {e}}-\mathrm {\tau *\varrho }.

\phi =(\mu _{\mathrm {e}}-\mu _{\mathrm {e}}^{\mathrm {old}})^2.

i = \mathop {\arg \max}\limits _{t \in \lbrace 1,2,\ldots,T\rbrace } f_{\mathrm {u}}^t({\mu _{\mathrm {e}}}).

\varrho =r*c_p*[f_{\mathrm {u}}^i({\mu _{\mathrm {e}}})]^{^{\prime }}+c_0*\theta *({\mu _{\mathrm {e}}})^{\theta -1}.

end while

\mu _{\mathrm {e}\_{j}}= \mu _{\mathrm {e}}^{\mathrm {old}}.

end for

\mu _{\mathrm {e}\_{\lbrace T+1\rbrace }}=b_{T}+\varepsilon

\mu _{\mathrm {e}}^* = \mathop {\arg \min }\limits _{{\mu _{\mathrm {e}}} \in \lbrace \mu _{\mathrm {e}\_1},\ldots,\mu _{\mathrm {e}\_{\lbrace T + 1\rbrace }}\rbrace } \lbrace S({\mu _{\mathrm {e}}},{\mu _{\mathrm {r}}^*(\mu _{\mathrm {e}})})\rbrace.

Compute \mu _{\mathrm {r}}^* according to Eq. (30).

4.3 Resource Provisioning with Hybrid Strategy
In the resource provisioning with hybrid strategy, a combination of on-demand and reserved instances are leased to complement the computation capacity of edge hosts. Thus, in the problem formulation of resource provisioning with hybrid strategy, (P1) is transformed into \begin{align*} &\mu _{\mathrm {c}}^t=\mu _{\mathrm {o}}^t+\mu _{\mathrm {r}},\\ &C^t(\mu _{\mathrm {c}}^t)=C^t(\mu _{\mathrm {o}}^t)+C(\mu _{\mathrm {r}}), \tag{32} \end{align*}
View Sourcewhere \mu _{\mathrm {o}}^t \geq 0, \mu _{\mathrm {r}} \geq 0.

According to the conclusion of Theorem 1, to ensure the QoS of mobile requests, when \mu _{\mathrm {e}}, the tenanted cloud instances \mu _{\mathrm {c}}^t should satisfy \begin{equation*} \mu _{\mathrm {c}}^t\geq \mu _{_t}^{\mathrm {o*}}({\mu _{\mathrm {e}}}), \tag{33} \end{equation*}
View Sourcewhere \mu _{_t}^{\mathrm {o*}}({\mu _{\mathrm {e}}}) is presented as in Eq. (21). In addition, as \mu _{\mathrm {c}}^t=\mu _{\mathrm {r}}+\mu _{\mathrm {o}}^t\geq \mu _{\mathrm {r}}, we have \begin{equation*} \mu _{\mathrm {c}}^t \geq \max \lbrace \mu _{\mathrm {o}}^{t*}(\mu _{\mathrm {e}}),\mu _{\mathrm {r}}\rbrace . \tag{34} \end{equation*}
View SourceRight-click on figure for MathML and additional features.Therefore, in the resource provisioning problem with hybrid strategy, the minimum provisioned on-demand instances can be summarized as \begin{equation*} \mu _{\mathrm {o}}^{t**} = \left\lbrace \begin{array}{l}0 \qquad\qquad\qquad\qquad\qquad\qquad\quad\ {\mu _{\mathrm {e}}} \geq \mu _{\mathrm {s}}^t+{\lambda _2^t} + \frac{1}{D(\lambda _2^t)}\\ \max \lbrace f_{\mathrm {u}}^t,\mu _{\mathrm {r}}\rbrace -\mu _{\mathrm {r}},\qquad\quad \mu _{\mathrm {s}}^t < {\mu _{\mathrm {e}}} < \mu _{\mathrm {s}}^t+{\lambda _2^t} + \frac{1}{D(\lambda _2^t)}\\ \max \lbrace {\lambda _2^t} + \frac{1}{{D(\lambda _2^t) - d}},\mu _{\mathrm {r}}\rbrace -\mu _{\mathrm {r}}\qquad\qquad\quad \quad \quad {\mu _{\mathrm {e}}} = \mu _{\mathrm {s}}^t. \end{array} \right. \tag{35} \end{equation*}
View SourceRight-click on figure for MathML and additional features.It has been proved that f_{\mathrm {u}}^t is a convex function over \mu _{\mathrm {e}} (see Eq. (28)), thus, \max \lbrace f_{\mathrm {u}}^t,\mu _{\mathrm {r}}\rbrace -\mu _{\mathrm {r}} is convex over \langle \mu _{\mathrm {e}},\mu _{\mathrm {r}} \rangle, and it is not difficult to conclude that \mu _{\mathrm {o}}^{t**}({\mu _{\mathrm {e}}},{\mu _{\mathrm {r}}}) is piecewise convex over \langle \mu _{\mathrm {e}},\mu _{\mathrm {r}} \rangle. By substituting (32), (35) into (P1), the resource provisioning problem with hybrid strategy can be reduced into \begin{align*} {\mathbf (P5)}:\; &\min \lbrace C({\mu _{\mathrm {e}}}) + C({\mu _{\mathrm {r}}}) + \frac{1}{T}\sum \limits _{t = 1}^T {C^t(\mu _{\mathrm {o}}^{t**}({\mu _{\mathrm {e}}},{\mu _{\mathrm {r}}}))} \rbrace\\ & s.t.~~{\mu _{\mathrm {e}}-\mu _{\mathrm {s}}^t} \geq 0\qquad\quad\qquad\quad\quad\quad\;\; t \in \lbrace 1,2,\ldots,T\rbrace\\ &\qquad\quad {\mu _{\mathrm {r}}} \geq 0\qquad\qquad\qquad\qquad\quad\ \ t \in \lbrace 1,2,\ldots,T\rbrace\\ & {\mu _{\mathrm {e}}-\mu _{\mathrm {s}}^t} + {\mu _{\mathrm {r}}} +{\mu _{\mathrm {o}}^{t**}({\mu _{\mathrm {e}}},{\mu _{\mathrm {r}}})} > {\lambda _2^t}~~~~t \in \lbrace 1,2,\ldots,T\rbrace, \tag{36} \end{align*}
View SourceSince C(\mu _{\mathrm {e}}) is convex (or linear) with \mu _{\mathrm {e}}, and C(\mu _{\mathrm {r}}) is linear with \mu _{\mathrm {r}}, the objective function of (P5) is piecewise convex over \langle \mu _{\mathrm {e}},\mu _{\mathrm {r}} \rangle. Therefore, (P5) is a piecewise convex optimization problem over \langle \mu _{\mathrm {e}},\mu _{\mathrm {r}} \rangle, and can be solved by Gradient Descending, similar to Algorithm 2, and we call this algorithm Optimal Resource Provisioning with Hybrid Strategy (ORP-HS).

As (P5) is a piecewise convex optimization problem over a bivector (i.e., \langle \mu _{\mathrm {e}},\mu _{\mathrm {r}} \rangle), solving the convex optimization problems in each segment can induce O(\log ^2(1/\sigma)) iterations. Thus the total computation complexity of ORP-HS is O(T^2\;\log ^2(1/\sigma)). Denote by f the calculation that can be completed in each time interval. It is intuitive that to ensure the feasibility of ORP-HS, the calculation time should not exceed one time interval, i.e., \begin{equation*} \frac{{\varpi {T^2}\;\log ^2 (1/\sigma)}}{f} < 1, \tag{37} \end{equation*}
View Sourcewhere \varpi is a constant that depends on the algorithm setting. Thus the precision \sigma should satisfy that \begin{equation*} \sigma > {{\mathrm {e}}^{ - \sqrt{\frac{f}{{\varpi {T^2}}}} }}. \tag{38} \end{equation*}
View SourceIt is shown in Eq. (38) that to ensure the feasibility of the algorithm, the precision \sigma is lower bounded. To achieve higher precision (smaller value of \sigma) in ORP-HS, one can either increase the computing capability during each time interval, or reduce the number of time intervals in a cycle.

SECTION 5Simulation Results
This section evaluates the proposed ORP algorithms by conducting extensive simulations. Simulation results based on two widely-used traffic models are first presented to validate the benefits of the ORP algorithms in term of cost-efficiency and flexibility. Then, to evaluate the ORP algorithms in practical, trace-driven experiments based on Google cluster tracelogs are further performed.

In these simulations, there is a base station with both computation resources (edge hosts) and communication resources (the wireless access network). Edge hosts are deployed within or in close proximity to the base station. The average transmission rate of the wireless access network is 100 Mbps. The cloud instance M4.large of Amazon EC2 [29] is leased to augment the system computation capacity, and pricing rate (when tenanted on demand) is 0.0208 dollar/GHz per hour. The reserved instances are charged with a discount of 50 percent over on-demand instances. The average round-trip delay from edge hosts to the cloud is set to 50 milliseconds [8], [9]. The face recognition application in [1] is considered as the representative of delay-tolerant applications, with the computation requests of 1000 Mege CPU cycles and transmitted block of 420 KB. To guarantee the QoS of the face recognition application, the system delay when executing delay-tolerant requests should not exceed 400 milliseconds [2]. The online game First Person Avatar in [9] is the representative of delay-sensitive applications, of which the processing delay should not exceed 100 milliseconds.

The local-first and the cloud-first algorithms are selected as the benchmark algorithms in these simulations. In the local-first algorithm, both the delay-sensitive and delay-tolerant requests are served at mobile edge. Sufficient edge hosts should be deployed to ensure the QoS of these diversified requests through all time intervals. In the cloud-first algorithm, edge hosts only provides sufficient computation resources for delay-sensitive applications, and all delay-tolerant requests are outsourced to the cloud.

5.1 Evaluation Based on Two Traffic Models
Simulations based on different traffic models are conducted to evaluated the ORP algorithms. Two widely-used cellular traffic models in Fig. 3 are employed in these simulations, as the daily traffic pattern of mobile requests at mobile edge is similar to the communication traffic in cellular network. The Two-peak Traffic Model is constantly adopted to characterize the dynamics of daily traffic at public places (e.g., bus stations), and the peaks represent the requests at rush hours. The Sine Traffic Model is frequently used to validate the effectiveness of network deployment algorithms [39]. Based on these traffic models, the performance of the ORP algorithms is evaluated, in term of both cost-efficiency and system flexibility.

Fig. 3. - 
Traffic patterns of mobile requests [39].
Fig. 3.
Traffic patterns of mobile requests [39].

Show All

In the resource provisioning problem, the composition of mobile requests can significantly influence the results of the ORP algorithms. In the resource provisioning problem with delay-sensitive dominant requests, sufficient edge hosts should be deployed to guarantee the QoS of delay-sensitive requests. Yet for the delay-tolerant dominant resource provisioning problem, more scalable cloud instances can be leased to execute the outsourced delay-tolerant requests. The ORP algorithms with delay-tolerant dominant requests is first evaluated, then the results with delay-sensitive dominant requests are further analyzed.

5.1.1 Delay-Tolerant Dominant Resource Provisioning
The delay-tolerant dominant resource provisioning problem is investigated, in which the arrival rate of delay-tolerant requests is much larger than that of delay-sensitive requests. The performance of the ORP algorithms is compared to the two benchmark algorithms in terms of cost-efficiency and flexibility, with the results illustrated in Figs. 4 and 5, respectively.

Fig. 4. - 
System cost over expectation of traffic pattern 1, with delay-tolerant dominant requests.
Fig. 4.
System cost over expectation of traffic pattern 1, with delay-tolerant dominant requests.

Show All

Fig. 5. - 
System cost over fluctuation of traffic pattern 1, with delay-tolerant dominant requests.
Fig. 5.
System cost over fluctuation of traffic pattern 1, with delay-tolerant dominant requests.

Show All

Fig. 4 shows system cost of different algorithms with increasing mobile computation load, i.e., expectation of arrival rates. The variance of mobile request arrival rates is 2 Giga CPU cycles/second. In this simulation, the Two-peak Traffic Model is adopted. Compared to the benchmark algorithms, the ORP algorithms can always yield the minimum system cost, as shown in Fig. 4. Fig. 4a illustrates the system cost when provisioning resources with on-demand instances. Due to the relatively low dynamics of mobile requests and high pricing rate of on-demand instances, the system cost of cloud-first algorithm is the highest among the three algorithms. Fig. 4b shows the results in resource provisioning with reserved instances. The system computation resources of the three algorithms remain unchanged through all time intervals, since the computation capacity of edge hosts and reserved instances are fixed. As the pricing rate of edge hosts increases with the serving rate, thus the system cost of local-first algorithm grows faster with increasing mobile computation load than cloud-first algorithm. In the OPR-R algorithm, the system cost can always be minimized by provisioning the computation resources (edge hosts or reserved instances) with lower pricing rate.

In Fig. 4c, with increasing arrival rates of mobile requests, the system cost of ORP-HS approximates that of the cloud-first algorithm. As when the arrival rates of requests increase, the pricing rate of edge hosts becomes larger than reserved instances. To reduce the system cost, more reserved instances are tenanted in the ORP-HS algorithm rather than increasing the computation capacity of edge hosts. The results of the proposed ORP algorithms are compared, as shown in Fig. 4d. As the expected arrival rates increase, mobile requests become less fluctuated. Nevertheless, the pricing rate of the reserved instances is much lower than on-demand instances. Thus, the system cost of ORP-OD increases faster than the ORP-R algorithm. In the ORP-HS algorithm, the system cost can be significantly reduced by exploiting the advantages of both on-demand instances (scalability) and reserved instances (economy).

Flexibility of the ORP algorithms is evaluated by comparing the system cost with changing fluctuations of mobile requests. As illustrated in Figs. 5a, 5b, and 5c, the proposed ORP algorithms can always yield the minimum system cost compared to the benchmark algorithms. In the resource provisioning with on-demand instances (Fig. 5a), the system cost of ORP-OD and the cloud-first algorithm increase much slower (or even decrease) with higher dynamics of requests than that of local-first algorithm. For the local-first algorithm, overprovisioning of edge hosts is incurred to guarantee the QoS at rushing hours, while for the cloud-first and ORP-OD algorithms, cloud instances can be leased on demand to deal with fluctuated requests, resulting in higher resource utilization and lower system cost. In the resource provisioning with reserved instances (Fig. 5b), both the mobile edge and cloud have fixed computation capacities. Substantial computation resources should be provisioned both at mobile edge and cloud to guarantee the QoS in the long run. Since the reserved instances have a fixed pricing rate while the pricing rate of edge hosts varies with the serving rate, the ORP-R algorithm always provisions the computation resources with lower pricing rate between edge hosts and cloud instances. Therefore, ORP-R always yields the lower system cost of the two benchmarks algorithms. As shown in Figs. 5c and 5d, the ORP-HS outperforms all the other algorithms (including the benchmark algorithms in Fig. 5c and the two proposed ORP algorithms in Fig. 5d. In ORP-HS, the system cost can be minimized by achieving an optimal balance between lower pricing rate and higher resource utilization.

Therefore, the local-first and ORP-R algorithms can not scale well with dynamics of mobile requests, yet the ORP-OD and ORP-HS algorithms can achieve higher flexibility by leasing elastic on-demand instances.

5.1.2 Delay-Sensitive Dominant Resource Provisioning
According to above simulation results and analysis, ORP-HS can always outperform ORP-OD and ORP-R in reducing system cost and dealing with dynamics of mobile requests. Thus, in the resource provisioning for delay-sensitive dominant mobile requests (1/6 delay-tolerant requests and 5/6 delay-sensitive requests), the performance of ORP-HS is mainly evaluated. Simulation results are shown in Fig. 6.


Fig. 6.
System cost over fluctuation of traffic pattern 2 with delay-sensitive dominant requests.

Show All

In this scenario, substantial edge hosts should be deployed at mobile edge to serve delay-sensitive requests. Thus to ensure the QoS at rushing hours, the computation capacity of edge hosts increases rapidly with dynamics of requests. For the ORP-HS and cloud-first algorithms, cloud instances can be leased on demand to achieve high flexibility, while for the local-first algorithm, the QoS can only be ensured by increasing edge hosts, resulting in significantly increasing system cost. According to ORP-HS, when the variance of requests exceeds 1.5 Giga CPU cycles/second, all delay-tolerant requests are outsourced to cloud as the pricing rate of edge hosts is larger than cloud instances. Then ORP-HS introduces the same system cost with the cloud-first algorithm in this case. Therefore, in the resource provisioning with delay-sensitive dominant requests, the performance of ORP-HS is similar to the cloud-first algorithm, especially for the requests with higher dynamics.

5.2 Trace-Driven Evaluation
In this part, trace-driven experiments are preformed to evaluate the performance of ORP algorithms in real life. As MEC has not yet been widely deployed in practical life, computation requests at mobile edge can not be traced precisely. This work employs Google cluster usage traces [40] to evaluate the ORP algorithms. In a Google cluster, a Google compute cell typically consists of a set of machines that are connected by a high-bandwidth network. Since mobile edge can be considered as a cloud of small range, user data of a Google compute cell is used to approximate the user data of mobile edge. The Google cluster tracelogs4 record the information of tasks in a Google compute cell. The tasks are described by task event tables and task resource usage tables. Task event tables record task event information, such as event types (submit, schedule, fail, finish, etc.), job IDs, task indexes and timestamps when these events happen. With task resource tables, average CPU usage of tasks (denoted as U^{{\mathrm {cpu}}}) can be calculated.

According to [41], the computation requests of the tasks can be computed as \begin{equation*} {R^{{\mathrm {comp}}}} = ({t^{{\mathrm {finish}}}} - {t^{{\mathrm {schedule}}}}) \cdot {U^{{\mathrm {cpu}}}} \cdot {C^{{\mathrm {cpu}}}}. \tag{39} \end{equation*}
View SourceHere, {t^{{\mathrm {finish}}}} and {t^{{\mathrm {schedule}}}} represent the timestamps the task is finished and scheduled to machines respectively. {C^{{\mathrm {cpu}}}} is average computation capacity of CPU in the Google cloud. Then, the arrival rate of computation requests can be calculated as \begin{equation*} \lambda = {R^{{\mathrm {comp}}}} \cdot {a^{{\mathrm {task}}}} \tag{40} \end{equation*}
View Sourcewhere {a^{{\mathrm {task}}}} represents the arrival rate of the tasks. In MEC, computation requests are much more delay-sensitive and have less computation requirements than those in conventional cloud computing. Thus, the arrival rates of computation requests at mobile edge can be obtained by slightly modifying the results of Google cluster tracelogs, as shown in Fig. 7. Five groups of trace results are illustrated in this figure. It can be observed that Trace-data 2, 3, 4 are greatly fluctuated while Trace-data 1 and 5 are less fluctuated.


Fig. 7.
Arrival rates of mobile requests based on Google Cluster Tracelogs.

Show All

Experiments are performed based on these trace data, and the results are illustrated in Fig. 8. As shown in Figs. 8a, 8b, and 8c, the proposed ORP algorithms always yield the minimum system cost compared with the benchmark algorithms. When arrival rates of mobile requests are fiercely fluctuated (Trace-data 2, 3, 4), the ORP-HS and ORP-OD algorithms yield similar results to the cloud-first algorithm. As in ORP-OD and ORP-HS, cloud resources are tenanted on demand to achieve elastic matching of resource provisioning and dynamic requests. Nevertheless, in the local-first algorithm, edge operators have to deploy substantial edge hosts to meet QoS requirements at rushing hours, resulting in much higher edge provisioning cost (as shown in Figs. 8a and 8c).


Fig. 8.
Performance of different algorithms in trace-driven experiments.

Show All

Fig. 8d shows the comparison among the three ORP algorithms. The results show that the ORP-R algorithm requires much more system cost than the other algorithms. This is straightforward, as fixed computing capacities are provisioned both at mobile edge and cloud in ORP-R. Low resource utilization and high system cost are incurred when dealing with dynamic requests. In ORP-HS, on-demand instances are mainly leased to serve fluctuated requests, thus introduce similar results to ORP-OD.

Insight. When mobile requests are fiercely fluctuated (Fig. 8) or mobile requests are dominated by delay-sensitive mobile requests (Fig. 6), the results of the ORP algorithms are similar to the cloud-first algorithm, which are much smaller than the local-first algorithm. In these cases, directly offloading all delay-tolerant mobile requests to remote clouds can achieve desired performance. When considering more general cases, extensive simulation results demonstrate that the proposed ORP algorithms can constantly reduce the system cost compared with the benchmark algorithms. In addition, the ORP algorithms show high flexibility with increasing dynamics of mobile requests. Among the three ORP algorithms, the ORP-HS algorithm outperforms the ORP-OD and ORP-R algorithms, as the optimal combination of on-demand and reserved instances can be achieved by solving (P5).

SECTION 6Conclusions
In this paper, CAME framework has been introduced to enhance the scalability of MEC when dealing with time varying mobile requests. The resource provisioning problem with multiple cloud instances has been solved, considering different QoS requirements of mobile requests. By leveraging piecewise convexity of this problem, ORP algorithms have been proposed to determine the optimal computation capacity of edge hosts, based on which the optimal cloud tenancy strategy is further explored. The proposed ORP algorithms have been shown to outperform the local-first and cloud-first benchmark algorithms in system flexibility and cost-efficiency. In addition, when dealing with greatly fluctuated mobile requests or delay-sensitive dominant mobile requests, desired performance can be achieved by directly offloading all delay-tolerant mobile requests to remote clouds. For the future work, we will investigate the multiple edge nodes case and jointly address the resource provisioning and workload scheduling issues, where multiplexing gain will be explored.

