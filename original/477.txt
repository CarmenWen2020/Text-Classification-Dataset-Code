Cloud providers have started to deploy various FPGA accelerators in their datacenters because the performance of many applications can be significantly improved by implementing their core routines in FPGAs. In conventional datacenters with FPGA accelerated servers, if a tenant wants to use FPGA accelerators, it requests for a VM instance residing in a server equipped with an FPGA accelerator. This paradigm to integrate FPGA into Cloud leads to poor resource sharing of the precious FPGA resources. In this paper, we propose FPGAPooling, an FPAG-enabled Cloud system where all FPGA accelerators are managed as a single resource pool and shared among all VMs. For a VM, instead of requesting the Cloud to run the VM on an FPGA accelerated server, at run time, when a VM needs to use FPGA acceleration, it requests an FPGA accelerator from the pool. After the VM finishes using the FPGA accelerator, it releases the FPGA accelerator back to the pool. We design a centralized scheduler to handle acceleration requests from VMs and assign each request to an idle FPGA accelerator at run time; We implemented a system prototype on IBM's OpenPower Cloud system. The key challenging of FPGAPooling is scheduling. We designed and implemented a group of scheduling algorithms for the FPGAPooling system. With extensive evaluations on both a small testbed and a large-scale simulation, we found that our algorithms can improve the average and tail job completion time by up to 7 and 4 times, respectively.
SECTION 1Introduction
1.1 Motivation
Cloud providers have been increasingly deploying various Field Programmable Gate Array (FPGA) accelerators in their datacenters. An FPGA chip is a semiconductor component that can be reprogrammed after manufacturing. FPGA-based accelerators bring two key benefits to datacenters. First, they can significantly boost the performance of many applications by accelerating some core computation-intensive routines, such as encryption and decryption of network data, web page ranking, and signal processing [13]. Second, they are re-programmable, which allows them to adapt to new application needs. Thus, FPGA-based accelerators have been increasingly deployed in datacenters. For example, Intel has integrated FPGAs with CPUs to provide heterogeneous architecture [25]. Microsoft has deployed FPGAs in datacenters to accelerate its Bing search engine [13], [41]. IBM has utilized FPGAs in its data engine to achieve large-scale and ever-growing NoSQL data stores [9].

FPGA resources are often not deployed on every server in a datacenter mainly for three reasons. First, FPGA accelerators are expensive. Second, it is difficult to program FPGA devices. FPGAs are programmed using languages such as VHDL or Verilog, which are hardware description languages operating at a very low level of abstraction using logic elements, such as digital gates, registers, and multiplexers. Third, not every application needs FPGA accelerators. FPGA devices are usually used to speed up tasks that involve heavy data-processing, such as data encryption/decryption and parameter inference in deep learning.

In conventional datacenters with FPGA accelerated servers, if a tenant wants to use FPGA accelerators, it requests for a VM instance residing in a server equipped with an FPGA accelerator. This paradigm to integrate FPGA into Cloud leads to poor resource sharing of the precious FPGA resources. In this paper, we consider the problem of scheduling limited FPGA based accelerators for a large number of different applications in a datacenter.

1.2 Limitations of Prior Art
The most common approach to sharing FPGA resources is to virtualize one physical FPGA chip into multiple virtual FPGA accelerators using paravirtualization. If a tenant wants to use an FPGA accelerator, it requests a Virtual Machine (VM) instance residing in a machine equipped with an FPGA accelerator at extra cost for this premium service. However, such sharing is too coarse-grained and inefficient. As a VM may only need the FPGA accelerator for a short period of time, for a physical server with a physical FPGA card, for all time instances that none of the VMs running on the physical server is using the physical FPGA card, the FPGA accelerator cannot be utilized by VMs running on other physical servers, and therefore the FPGA resource is wasted.

Another approach to sharing FPGA resources among multiple tenants is to group multiple physical FPGA chips as one virtual FPGA accelerator, and each FPGA group is equipped with a hardware switch fabric [28]. Tenant applications communicate with this switch fabric via APIs in order to use this FPGA group. This method shares FPGA resources in an even coarser granularity than the above approach because multiple physical FPGA chips in the same group can only serve one application at the same time. Moreover, the reconfiguration time of one FPGA group takes several seconds, which is unbearable for latency-sensitive tasks.

1.3 Proposed Approach
In this paper, we propose and implement an FPAG-enabled Cloud system called FPGAPooling. In our system, a physical FPGA chip is divided into multiple slots where each slot works as a virtual FPGA accelerator, and all FPGA accelerators are managed as a single resource pool shared among all VMs. More specifically, when a VM needs to use a specific FPGA acceleration functionality, it requests an FPGA accelerator from the pool. We design a centralized scheduler to handle requests from VMs and assign each request to an idle FPGA accelerator at run time. After a VM finishes using an FPGA accelerator, the accelerator is released back to the pool.

Our system has two features. First, it supports the abstraction of FPGA resources. Users from any node in the cloud can access and release FPGA accelerators by calling FPGAPooling API functions at any time. An FPGA accelerator will not be occupied by VMs who do not need it. Second, our system allows the efficient use of precious FPGA resources. We decoupled FPGA services from VMs and provided an FPGA as a Service (FaaS) structure, and cloud users can send requests to the FPGAPooling scheduler for acceleration services at runtime. This scheduler will track the status of all FPGA accelerators and recollect the idle ones after their job is finished, so that the FPGA accelerators will not be occupied by an VM once it finishes its acceleration service.

1.4 Technical Challenges and Solutions
The first challenge is on providing users with easy and efficient access to FPGA acceleration services. On one hand, programming on FPGA needs expertise in hardware description languages, such as VHDL or Verilog, making FPGA service forbidding to common cloud users. On the other hand, the compilation of an FPGA based program is time consuming, making conventional systems that rely on dynamic code generation not suitable for FPGA applications. Our solution is to provide programming abstraction in the form of API functions, so that users can call different API functions in their own applications for requesting and utiliziing different FPGA accelerators at run time.

The second challenge is on scheduling FPGA resources under multi-dimensional resource constraints. The system is facing two resource constraints: DMA-pipe throughput and network throughput. Due to FPGA’s partial reconfigurability, a physical FPGA chip can serve as multiple FPGA accelerators that work independently and simultaneously. These multiple accelerators fetch and push data through a shared DMA-pipe, and share the same Network Interface Controller (NIC) of their physical server. If all accelerators on the same FPGA chip are processing large workload, each accelerator gets only a tiny portion of the total DMA-pipe throughput; If all accelerators on the same FPGA chip are serving for requesting from remote tenants, there can be intensive contention on the NIC ingress/egress ports when accelerated data are transferred between requesters and the FPGA server. Either of the above cases may prolong the processing time of the job requests from tenants. To address this challenge, we design another contention-aware algorithm called Resource-Aware algorithm. The Resource-Aware algorithm assigns a job to an accelerator in the suitable location at the suitable time, taking both network and Direct Memory Access (DMA) contention into consideration.

The third challenge is on scheduling FPGA resources under various workload distributions. In modern datacenters, the distribution of workloads follows either long tail or short tail patterns. Traditional heuristic scheduling schemes include First-In-First-Out (FIFO) and Shortest-Job-First (SJF). FIFO is only suitable for light-tailed workload and has poor average completion time [22]. SJF, on the other hand, is suitable for long-tailed workload but prolongs the tail completion time [18]. The scheduling of FPGA accelerated jobs should be robust to various workload distributions. To address this challenge, we propose a multi-queue based algorithm called Workload-Aware algorithm. We prove that by imitating SJF for jobs among different queues and FIFO for jobs in the same queue, our Workload-Aware algorithm improves both the average and tail completion time under different workload distributions. Moreover, we integrate the Resource-Aware algorithm and the Workload-Aware algorithm into a hybrid called Workload-and-Resource-Aware algorithm, which combines the advantages of both and therefore achieves an optimal performance in terms of job completion time and job locality rate.

1.5 Summary of Experimental Results
We implement a system prototype over an OpenStack Cloud. The system contains 5 worker nodes and one controller node, where the three worker nodes are attached with a Xilinx FPGA chip. We adopt four metrics in our experiments: Average Completion Time, Tail Completion Time (TCT), System Average Response (SAR), and Data Locality Ratio (DLR). We use FIFO and SJF scheduling as the baselines. Experimental results show that the Workload-and-Resource-Aware Algorithm performs the best, and it improves the average job completion time by up to 7 times, and the 95-percentile tail completion time by up to 4 times, respectively.

1.6 Advantages of FPGAPooling over Prior Art
FPGAPooling is advantageous in terms of abstraction, heterogeneity, easy accessibility, and robust performance. For abstraction, FPGAPooling allows cloud users to request FPGA acceleration from anywhere at any time, while existing approaches only allows uses to request FPGA acceleration from VMs in an FPGA-attached machine. For heterogenity, FPGAPooling supports five different FPGA application functionalities, which can run simultaneously on the same physical FPGA chip, while existing schemes do not. For easy accessibility, FPGAPooling implements a set of API for cloud users to request, utilize, and release FPGA accelerators, so all the details of FPGA scheduling are transparent to cloud users. For robust performance, FPGAPooling scheduling algorithms are designed based on M/G/1 queuing theory to be adaptive to different job distributions, whereas no existing schemes consider multiple job distribution problems.

SECTION 2Related Work
2.1 FPGA Sharing
FPGA sharing aims to make one physical FPGA chip shared by multiple applications. There are two kinds of approaches to achieve FPGA sharing. In the first approach, FPGAs have no direct network connections and are attached to processors through a PCIe link. The host processor is involved when the FPGA resource is needed for computation [11], [14], [27], [32], [41], [52]. In the second approach, FPGAs have direct network connections and work independent of any host processors, if there is even a host processor [13], [54], [55].

The first approach are dominating the ways that FPGAs are integrated into Cloud. Prior work along this line further falls into two categories. In the first category, an FPGA chip is treated as a monolithic device, and the partial-reconfigurable ability of FPGA is not supported [27], [41], [52]. The most representative system in this category is Microsoft’s Catapult [41]. The Catapult fabric attaches an FPGA chip to its server through a PCIe connection, and each FPGA is wired directly to other FPGAs via a torus network. In their system, a server only runs one specific application at a time on its local FPGA. Besides their work, Wang et al. built a para-virtualized environment using Xen VMM [52]. They used a privileged VM as the agent to communicate with other VMs via memory sharing, and forwards their requests to the FPGA host driver. Huang et al. built a system called Blaze, which provide FPGA accelerators as service by integrating their APIs to Hadoop YARN [27].

In the second category, an FPGA chip is divided into multiple regions, and each region can be reconfigured with a particular circuit design as an independent virtual chip [4], [11], [14], [23], [32]. Our system falls into this category. Byma et al. proposed FPGA abstractions that allows tenants to access virtualized FPGA chips with a command similar to what they would use to boot an VM [11]. Fahmy et al. achieved FPGA virtualization by providing virtualized communication interfaces for each virtual FPGA [23]. The limitation of these approaches is that a FPGA resource will continually being occupied by its tenant even when the tenant no longer need it, which leads to inefficient FPGA usage. Chen et al. [14] and Asiatici et al. [4] addressed this problem by providing FPGA acceleration services in the form of API functions in each FPGA-attached server machine. Tenants access different accelerator services by calling different API functions, and release their FPGA resources immediately after calling. The limitation of their work is that FPGAs are accessed by applications only from the local server, although their design can be potentially used in cloud scale. Another system proposed by Knodel et al. supports both virtualized and physical FPGA resources [32]. However, information about the FPGA type and a free predefined virtual FPGA region are necessary to utilize FPGA services, so the system design are not transparent for cloud users. Li et al. proposed a CPU-FPGA co-design for fast packet processing [36]. The idea is to offload computation-intensive Network Functions (NFs) to FPGAs as accelerator modules, and provided API libraries for FPGA developers to access those accelerator modules. They use partial reconfiguration to enable concurrent FPGA sharing. However, their approach did not involve scheduling on multiple accelerator modules of the same physical FPGA.

The second approach can be represented by Microsoft’s Configurable Cloud [13]. In their system, a layer of FPGAs is placed between server NICs and the Ethernet network switches, so the data path of cloud communication is accelerated with FPGA hardware. A drawback of this design is that during the time that an FPGA chip is being reconfigured, the network link will be broken because the FPGA chip is placed between server NICs and the Ethernet network switches. Weerasinghe et al. [54], [55] also proposed FPGA accelerators as standalone resources with direct connections to the data center network. To enable FPGAs with direct network connections, they implemented a Network Interface Controller (NIC) on part of the FPGA logic. This approach has the limitation that implementing a NIC on the FPGA chip occupies the precious FPGA function logic, which could be used for user application acceleration otherwise.

2.2 Heterogenous Scheduling
Heterogenous systems that are equipped with hardware accelerators, such as GPU and FPGA, are becoming mainstream. Automatic task scheduling across different accelerators are therefore critical for the effectiveness and efficiency of a heterogeneous system. In this paper, we summarize the prior art along this line from two aspects: FPGA scheduling and GPU scheduling.

2.2.1 FPGA Scheduling
Most prior work considers assigning FPGA resource to jobs in a single machine environment [4], [20], [40], [48]. Papadimitriou et al. proposed a FPGA cost-model to calculate the expected FPGA reconfiguration time and FPGA throughput [40]. Steiger et al. designed a two-dimensional resource cost model to divide an FPGA chip [48] into multiple regions, and assign different FPGA regions to different size of jobs. Dai et al. proposed a benefit based algorithm in order to save computing resources [20]. They assigned jobs to either FPGAs or CPUs, based on the highest throughput of each job. Asiatici et al. [4] proposed a runtime manager on a FPGA-attached server, which consists of different threads for managing different low-level hardware resources, such as memory and FPGA region. An application sends a request to the manager by performing a system call, and the manager dynamically decides which and how many resources can be used by the application. Iordache et al. considers FPGA scheduling in a cloud environment [28]. However, they schedule FPGA resources in a coarse granularity. They schedule the entire FPGA chip and assign it to different user groups, instead of scheduling multiple reconfigurable regions inside one FPGA chip. Huang et al. proposed heuristic scheduling to improve accelerator locality by avoiding launching mixed FPGA workloads on the same cluster node as much as possible, so that each node will run the same kind of workloads [27].

2.2.2 GPU Scheduling
One typical form of GPU scheduling is to treat GPU as a alternative resource to CPU [16], [42], [45], [57]. Scogland et al. [45] proposed an OpenMP extension as a scheduling service to assign OpenMP tasks across both CPUs and GPUs. Some similar work can be found along this line, which conducts GPU scheduling in a single node with heterogenous computing cores. [2], [34] Ravi et al. [42] proposed two separate schedulers to deal with a single-node scenario and a multi-node scenario respectively. Zhang et al. [57] proposed a fine grained CPU-GPU scheduling for cloud gaming. They decompose each gaming workload into lightweight tasks. When the resource usage of one physical machine becomes saturated, they will dispatch smaller tasks to different physical machines, rather than conducting a whole VM migration. However, their work does not consider network overhead involved in task scheduling. Some other work aims to address the issue of GPU scheduling from the perspective of energy saving [8], [12], [37].

SECTION 3System Design
In this section, we present our approach to turn the FPGA chip from a single-node device to pooled resource. First, we present the architecture of the whole FPGAPooling platform. Second, we present the architecture of one FPGA node; Third, we describe the workflow for cloud users to use our FPGAPooling system.

3.1 Platform Architecture
There are three major components in our FPGAPooling system: clients, scheduler, and FPGAPooling service layer, as shown in Fig. 1.

Clients: Clients are processes that are served by the FPGAPooling acceleration services. They could be processes residing in a host server, a Virtual Machine (VM), or a Linux Container. Client processes are aware of the general size of the data to be accelerated. We will illustrate how a client process access an FPGA acceleration service in Section 3.3.


Fig. 1.
FPGAPooling architecture.

Show All

FPGAPooling service layer: This layer acts as an agent among the clients, the scheduler, and the hardware-level FPGA resources. Clients interact with accelerators by calling the API functions provided by this layer.

Scheduler: The scheduler is a critical component to enables real-time FPGA sharing in the cloud scale. It runs on a control node and and is integrated with Nova in the OpenStack-based Cloud [46]. We use an SQL database to maintain the hardware status of FPGA resources, such as the host IP address and compatibility of each accelerator slot. The scheduler keeps in-memory information about all the FPGA resources. It only needs to load information from database when it is rebooted, or when the system settings have been modified. Examples of system modification include a new FPGA chip being added to or removed from a Cloud, or a chip being re-partitioned into different physical regions. The scheduler will only receive hands-shake information, such as a request for accelerators or a notification of a completed acceleration. Once a scheduling decision is made, the heavy data for acceleration will not go through the scheduler but be sent to the according FPGA node directly.

3.2 Node Architecture
As shown in Fig. 2, a compute node with an FPGA chip holds four logic layers:

Hardware Layer: In this layer, an FPGA chip is pre-partitioned into multiple Reconfigurable Regions (RRs). We give an example of partitioning FPGA in Fig. 3. To achieve the partial reconfiguration and management on FPGA RRs, we design the FPGA chip to be a three-sublayer stack from low to high levels: the platform sublayer, the service sublayer, and the user sublayer.


Fig. 2.
Four logical layers of FPGA compute node.

Show All


Fig. 3.
Hardware layer design on FPGA.

Show All

The user sublayer is implemented on the programmable area which is pre-partitioned into standard RRs. Resources inside each RR, such as the programmable logics, are exclusive to other RRs. Resources outside each RR, such as the DMA engine and PCIe engine, are shared by all RRs so that accelerated data can be fetched from/to each RR. An RR can be considered as a virtual FPGA chip which holds one accelerator core at a time. Each accelerator core has a corresponding bit-file for configuration, and to verify whether an accelerators core fit into a RR, a pre-compilation of the accelerator’s bit-file on that RR is necessary. A failure of pre-compilation means that there is no enough resource for the RR to hold such an accelerator core.

The service sublayer is also implemented on the programmable region, which works independently with the reconfiguration process in the user sublayer. It uses a shared DMA engine to fetch and store data to/from accelerator cores, and provides interfaces for upper software layers by exposing registers, so that software layers can access and manage the physical FGPA resources through memory mapped IO (MMIO). It also contains a reconfiguration controller, which receives reconfiguration commands from the hypervisor layer. The service sublayer is separated from the user sublayer, so that a reconfiguration executed in the user sublayer will not affect the operation in the service sublayer. The platform sublayer is implemented on fixed-functional circuits. It enables RRs to access outside resources attached to the FPGA chip, such as memory and Ethernet controllers.

Hypervisor Layer: This layer provides two modules: Host Driver Module (HDM) and Host Control Module (HCM). The HDM implements the dynamic reconfiguration of the physical RR, while the HCM provides interface to the library layer to control and track each RR’s usage status. More related details about this layer can be found in our previous paper [14].

Library Layer: This layer has two main functionalities. First, it manages all the bit-files that are compatible on RRs, and each bit-file corresponds to an accelerator core. In the bit-file library, the profile of a bit-file includes the following information: an unique identifier for indexing the accelerator core, the address of RRs that the bit-file is compatible with, and the maximal acceleration throughput that its corresponding accelerator core can provide. A bit-file is compatible with a RR if it can be pre-compiled successfully on that RR. The size of a bit-file is proportional to the size of its compatible RR. An over-large RR leads to the waste of FPGA resources and long configuration time, because it needs a large bit-file to be loaded and compiled. An over-small RR, on the other hand, can not hold enough accelerator cores because most of the bit-files are incompatible with it. In our system, we carefully choose the size of each RR, and the size of a bit-file is 30 - 40 MB in average. A bit-file to be configured on a RR is stored in CPU’s DRAM, and will be loaded to the RR through the PCIe pipe between the CPU and FPGA hardware.

Second, it provides built-in API functions for the application layer to configure and access local FPGA RR. Details about how application layers interact with these built-in functions will be presented in Section 3.3.

Application Layer: This layer is where the clients, the scheduler, and the FPGAPooling service layer reside. There is an FPGAPooling service layer on each compute node that provides a set of FPGAPooling API functions to its clients. Each client interacts with the FPGAPooling service layer to request accelerators from the scheduler by calling provided API functions. The scheduler resides in a central node. It interacts with the FPGAPooling service layer on each compute node to distribute, recollect, or track the status of an accelerator resource. More details of this layer will be illustrated in Section 3.3.

3.3 FPGAPooling Workflow
We first introduce the principles of tenant assignment, and then describe how tenants use APIs functions to complete an FPGA acceleration job.

Tenant Assignment. Users can specify their accelerator needs during the instance boot time, and tenant assignment will use a “best-effort” policy. The modified Nova scheduler in OpenStack cloud will create an instance (a Virtual Machine or a Container) on a compute node attached with an FPGA chip, as long as there are adequate resources to boot the instance. Otherwise, if the user doesn’t specify any requirements, or if there is no enough resource, the scheduler intends to allocate an instance on a normal compute node. There is a list of default accelerator bit-files available in the system. If no existing accelerator matches the demand, the tenant can also upload his self-designed bit-files.

Client Behavior. After an instance is booted, a client residing in the instance can interact with the scheduler via the following three API functions to request, utilize and release accelerator resources, respectively.

Request: On the client side, a client can request for an acceleration service by calling the API function named acc_open. Calling this function will trigger a socket request to be sent to the FPGAPooling scheduler. This request will be stored in a job queue until a suitable resource is available.

On the scheduler side, the scheduler deals with in-queue requests in a priority order. When the scheduler deals with a request, there are two possibilities: the scheduler may assign the request to a RR on a remote node, or to a RR on the same node as the request com from. In the first case, the scheduler acts as a agent between client and the remote node. It first communicates with a daemon process running in the service layer of that remote node. The daemon triggers a service thread which listens on a socket port to wait for coming connections. Next, the thread loads the requested accelerator module onto the assigned RR, and responds to the scheduler with the listening port number. Afterwards, the scheduler responds client with the IP and port number of that service thread. By now, the agent task for the scheduler is complete. After the client sets up a connection with the service thread, the acc_open function returns successfully with a pointer to a fixed-size open buffer in the user space, which will later hold the clients’s job workload. In the second case, there is no remote connection process.

Acceleration: Once the client fills the open buffer with input data, it starts FPGA acceleration service by calling the acc_run API function. For a local assignment, this function will invoke an I/O device simulator in Qemu. It will transfer data from the open buffer to a host buffer, which will be directly accessed by the FPGA’s DMA engine. For a remote assignment, data in the open buffer will be delivered through the set up socket, and finally get processed by the accelerator hardware. Due to physical memory limitation and Linux kernel constraints, the fixed buffer size is no larger than a given threshold. This data transfer process will automatically repeat, until all job workload has been executed.

Release: For a local assignment, the acc_close API function frees the buffer in user space, and deletes associated records in the HCM; for a remote assignment, it also clears up the connections with the remote service thread. Finally, it sends a notification to the scheduler indicating the idle state of the released RR resource.

Example Workflow. We illustrate the FPGAPooling workflow using an example in Fig. 4: A user wants to use FPGA accelerators to speed up AES encryption on 1G Bytes of data, and the user has no advanced knowledge about FPGA hardware coding. He will write a C program, which calls the acc_open function with two input parameters: acc_open(‘AES’, 1024*1024*1024), to request for an AES accelerator. Once the function call returns a address pointer pin, the user program uses a repetition loop to copy input data to the pointed address, and call the acc_run(pin) function to conduct acceleration. This step will not involve the scheduler, as the encrypted data will be transmitted to the FPGA node directly. The acc_run function will return another pointer pout which points to the encrypted output. Finally, after all data has been encrypted, the user program will release the AES accelerator by calling acc_close function: acc_close(pin, pout).


Fig. 4.
Workflow of remote FPGA acceleration.

Show All

SECTION 4Resource Aware Scheduling
We have designed three different scheduling algorithms for the FPGAPooling system. In this section, we focus on one of the three scheduling algorithms: Resource-Aware Algorithm. It is designed for addressing the challenge of multiple resource constraints faced in FPGAPooling scheduling. We first describe this challenge, and then provide our Resource-Aware algorithm as the solution. Next, we give a detailed analysis of our algorithm.

4.1 Scheduling Challenge
One of the most critical challenges in FPGAPooling scheduling is multiple resource constraints. We face two kinds of resource constraints in our system:

FPGA Throughput. The total FPGA throughput in the system is limited because of two reasons. First, the number of computing nodes with physical FPGA chips in the cloud is limited. Second, each physical FPGA chip has an identical processing throughput limit U, which is the highest amount of data that can be processed per time unit. The total throughput available in system is therefore U×N, N being the number of FPGA chips in the system.

It is worth noting that a physical FPGA chip is pre-divided into a fixed numbers of slots, and each slot acts as different virtual accelerators. For a given virtual accelerator, its peak processing throughput u is pre-determined by its bit-file design as well as its logic implementation, with u≤U. In practice, the actual throughput among k active slots on a physical FPGA chip is automatically shared by max-min fairness [14], where
∑1≤i≤kui≤U.
View Source

Network Throughput. Network throughput can be a bottleneck for data computation, especially when applications are data-intensive and do not run in pipeline[24]. Heterogeneous accelerators, such as GPU and FPGA, are mainly used for data intensive jobs [3], [33], so network throughput are crucial for the performance of job delivery/processing. For the FPGAPooling scheduler, we abstract the network as a fat-tree topology [35]. Fat-tree is one of the most popular architectures in modern data centers for its simple topology and good performance on aggregated bandwidth [7]. Based on fat-tree architecture, the network bottleneck resides mainly on end ports.

4.2 Resource-Aware Algorithm
To address the challenge of resource constraints mentioned above, we introduce the Resource-Aware algorithm. Resource-Aware algorithm assumes that the scheduler maintains a queue of waiting jobs. When an accelerator is available, the scheduler traverses each of the jobs and categorize them into three categories:

Jobs from the same node as the available accelerator.

Jobs from a node without FPGA resources.

Jobs from another FPGA-available node which has no idle resources at the meantime.

In general, the Resource-Aware algorithm uses a “delay scheduling” strategy: it prefers to to assign the accelerator to the first two kinds of jobs. For a third kind of job, the algorithm tends to let it wait for a local accelerator, rather than assign it with a remote one. Because the delay caused by waiting can be repaid by the time saved from network transfer [56]. Next, we introduce how the Resource-Aware algorithm improves this simple delay scheduling to address different resource constraints.

4.2.1 FPGA Contention Awareness
Simple delaying scheduling ignores the problem of FPGA contention. Due to the system’s tenant assignment policy, nodes with FPGA resources typically contain more potential accelerator users, which together generate more frequent job requests. Meanwhile, an FPGA-available node may become a hot spot, i.e., a node busy with accelerating large jobs for a long time. In that case, jobs requested from such a node may stay blocked in queue. We need a strategy to detect such hot node and relieve the blocked job from over-waiting. To address this issue, we set a timer and a skipped counter for each job. The rationale behind is that, if a third kind of job has waited for a long enough time, it is probably from a busy-node, i.e., a node busy with large jobs. In that case, this job will be assigned with a high priority and get launched immediately to alleviate FPGA contention in the hot spot.

4.2.2 Network Contention Awareness
Simple delaying scheduling also ignores the problem of network contention. A network contention occurs when a hot spot is serving a large amount of remote jobs. Simple delay scheduling only reduces unnecessary network transfer, but does not address the network contention problem. We need a strategy to balance the load of remote jobs equally among all FPGA-available nodes. Therefore, we use a counter for each node to record the number of remote jobs running on this node. If the remote job number on a node outnumbers a threshold, the scheduler will constrain more remote jobs from launching on this node. The constraints for the second kind of jobs is looser than the third kind of jobs: in case of network contention, a third kind of job has to wait enough time to get an accelerator. Otherwise, it means the node of the third kind of job is not busy enough to shed its workload to other nodes.

4.2.3 Partial Reconfiguration Latency Ignorance
Partial reconfiguration enables different RRs on the same physical FPGA chip to work on different acceleration services independently, so that the reconfiguration of one RR will not impact the status of other RRs. Counter-intuitively, the Resource-Aware algorithm does not consider the latency caused by reconfiguring an accelerator slot while making scheduling decisions. Our rationale is that the reconfiguration latency is independent from job size, and it is insignificant compared to potential latencies caused by resource contention. As mentioned in Section 3.2, bit-files of different accelerators are stored in CPU’s DRAM. On reconfiguring a RR, a bit file will be loaded to the RR through the PCIe pipe between CPU and FPGA, so the reconfiguration latency of each accelerator slot depends on the size of its according bit-file. In our system, this latency is in level of microseconds [14], which is neglectable compared with job’s execution time and potential network overhead. Reconfiguration latency matters only when the acceleration job size is less than several KBs, which is unlikely to happen in our system, as those jobs may gain better performance by simply using software acceleration. Moreover, partial reconfiguration is one of the most important features for FPGA accelerators, we should leverage this property instead of avoiding it.

The pseudocode of Resource-Aware algorithm is shown in Algorithm 1. Please note that a pre-compilation of all available bit-files on all size of FPGA slots is conducted beforehand, so the bit-file of an accelerator is compatible with any available FPGA slot in our system. Therefore, we do not need to check the bit-file compatibility everytime when we make a scheduling decision.

Algorithm 1. Resource-Aware Algorithm
Input: queue; Output: True or False

/* remotek=1 if slot k is running a remote job, otherwise 0 */

/* accID=1 If FPGA resource available on node ID, otherwise 0 */

/* C: Contention Threshold */

When an accelerator slot i on node loc get released:

for each job j in queue do

/* if job j is from the same node as the accelerator slot i (Job kind 1)*/

if j.location==i.location then

assign job j on slot i

remotei=0

return True

/* if this node still has quota to serve more remote jobs*/

if ∑k.location=i.locationremotek<C then

/*If the node is from a non-FPGA node (Job kind 2),

or from a FPGA node but has waited for a long time (Job kind 3) */

if accIDj==0 or wait_long_time(i,j)==True then

assign job j on slot i

remotei=1

return True

return False

Function: wait_long_time

Input: {i,j} slot ID and job ID

Output: True or False

/*D: skipCount threshold*/

/*W: timeout weight*/

if j.waitTime≥W∗j.size or j.skipCount≥D then

return True

else

j.skipCount←j.skipCount+1

return False

4.3 Analysis of Resource-Aware Algorithm
We set the value of D in Resource-Aware algorithm as the threshold number of times for any job to be skipped by the scheduler. If we define the preference node for each job as it local node, the locality rate can be denoted as the ratio of jobs executed on its preference node to the total number of jobs.

We set the value W as the weight of timeout. Generally, the larger a job is, the more it will gain from launching on its preference node, meanwhile the longer a job awaits, the less performance-cost ratio it will achieve.

It is critical for Resource-Aware algorithm to set proper values of D and W to make a tradeoff between the gain and lose generated by waiting. Suppose we want to achieve a certain locality rate in a N-node cloud with N′ FPGA-available nodes, and the estimated skipped rounds for a job from an FPGA-available node to launch on its preference node is D, then we denote the locality rate r a function of D
r(D)=N′N(1−(1−1N)D).
View Source

When the scheduler makes a scheduling decision, the probability for a job to not launch on its preference node is 1−1N. The probability for a job to skip its preference node for D times is therefore (1−1N)D, which also means that a job from an FPGA-available node has at least 1−(1−1N)D probability to launch on its preference node, given at most D times of being skipped. The final job locality can be estimated as N′N(1−(1−1N)D), where N′N is the prior knowledge about the ratio of FPGA-available nodes. Therefore, to achieve locality rate of μ, D must satisfy
D≥log1−1N(1−μNN′).
View SourceSuppose the maximum network bandwidth is B, then for a job with size s, its time saved on network is at least s/B. We set a job’s maximal waiting time twait∝s/B. If we set this proportion as k, then we have the value of W as: W=k/B.

SECTION 5Workload Aware Scheduling
In this section, we first describe the scheduling challenge of different workload distributions. Second, we provide our solution to this challenge. Third, we provide a model to formulate the scheduling problem in order to get optimal parameters for our algorithm. Finally, we analyze and compare different parameter settings in Workload-Aware algorithm based on the given model.

5.1 Scheduling Challenge
A critical challenge faced by the FPGAPooling system is to design a scheduling algorithm that deals with different workload distributions. On one hand, we may not have statistics for the hot workload patterns in heterogeneous clouds. On the other hand, scheduling strategy designed for one kind of workload pattern may not be suitable for the other. For example, first-in-first-out (FIFO) strategy is suitable for light-tailed workload but leads to poor average completion time, whereas Shortest-job-first (SJF) strategy can shorten the average completion time for long-tailed workload but prolongs the tail completion time. A workload-sensitive scheduler may lead to unpredictable system performance when the workload pattern changes.

5.2 Workload-Aware Algorithm
We make the following two assumptions:

Intervals between FPGA acceleration requests follow Poisson distribution. Poisson distribution is used to predict the number of events (i.e. job requests in our case) occurred within a fixed time interval [31]. It is a commonly observed pattern in datacenters [29]. With the Poisson distribution assumption, we can safely design scheduling algorithms for the average case, and focus on the distribution of job size.

FPGA acceleration job sizes follow either the power law distribution (i.e. long-tail distribution), or the exponential distribution (i.e. short-tail distribution). Different cloud applications can have large variances in terms of workload distributions. Despite these variances, we can still classify them into two common categories: short-tail distribution workloads and long-tail distribution workloads [5], [29], [30], [38]. It is reasonable to assume that workload distributions of FPGA acceleration also fall into the two categories.

To address the challenge of different workload distributions, we provide our solution called Workload-Aware algorithm. In this algorithm, the scheduler maintains multiple priority queues. Jobs with larger workload will be assigned to lower priority queues, whereas jobs with smaller workload will be assigned to higher ones and get scheduled earlier. By imitating SJF for jobs among different queues, and imitating FIFO scheduling within the same queue, Workload-Aware algorithm performs well under either long-tail or short-tail workload distribution.

5.2.1 Priority Queues
We assume that the scheduler maintains K priority queues with threshold (t0,t1,t2,…,tK−1,tK), where t0<t1<⋯<tK−1<tK. The priorities decrease from queue 1 to queue K, so that the first queue with threshold t1 maintains the highest priority. A job with workload size between tk−1 and tk will enter the kth queue. Within each queue, jobs are scheduled in the FIFO order. By separating jobs to different priority queues, this schemes make the scheduler approximates FIFO scheduling for nearly-identical jobs, and approximates SJF scheduling among large and small workloads. Therefore, it performs well under both short-tail and long-tail distributions.

5.2.2 Queue Thresholds Selection
Some researches have been done on formulating and choosing the optimal demotion thresholds for multiple queues. Bai et al. estimated the queue threshold tk as a function of t1,t2,…,tk−1. Their approach leads to large amounts of parameters [6]. Alizadeh et al. designed a two-priority queue structure with only one threshold to consider [1]. Their approach can be considered as a special case of [6], and it schedules jobs in a coarse-granularity. Chowdhury et al. compared both linearly and geometrically demotion thresholds, and heuristically pick queue thresholds and queue numbers [17].

For the FPGAPooling scheduler, we want to give finer grained scheduling for most of the acceleration jobs while not maintaining an over-large queue set. Therefore, we consider a piecewise, hybrid queue structure to combine both linear and geometric demotion thresholds, given as below:
tk=⎧⎩⎨⎪⎪⎪⎪Eqk−1,1≤k≤k1,q>1,E>0Eqk1+E(k−k1)qk2−1−qk1k2−k1,k1+1≤k≤k2Eqk−1,k2+1≤k≤K−1.(1)
View SourceHere E is the base of the smallest threshold, q denotes how much bigger the threshold in one queue is from that in another, and K is the maximum queue number, with tK=∞. Consequently, it is expressed as a Θ(logE(N)) function of the maximum workload N. For queues within [Q1, Qk1] and [Qk2+1, QK], their shresholds increase geometrically with queue ID, while the threshold of queues between k1+1 and k2 increase linearly with queue ID.

When K=1, this algorithm performs the same as FIFO, and when q approximates to 1, this algorithm is close to SJF. The key challenge to this algorithm is to find a suitable combination of (E, q, k1, k2, K) to include enough jobs into the finer-grained priority queues, while maintaining an acceptable queue set size. We will show later in Sections 5.3 and 5.4 that the optimal parameters for Workload-Aware algorithm can be achieved by formulating the total job completion time (JCT), and compare different JCTs under different parameter variables.

The pseudocode for Workload-Aware algorithm is shown in Algorithm 2.

Algorithm 2. Workload-Aware Algorithm
Input: wj

/*wj = workload of job j */

if job j arrives: then

idj←get_priority_queue(wj)

add j to the tail of queue idj

if accelerator i requests for a new job: then

for each q in priority queues (in decreasing order) do

if q≠ϕ then

assign accelerator slot i to job q[0]

return

Function: get_priority_queue_id

Input: wj

Output: idj

/*idj = queue id of job j */

/*wj = workload of job j*/

/*K = maximum queue id*/

/*k1/2 = begin/end of linear queue id*/

/*E = threshold of the first queue*/

When job j arrives:

if wj≤E.qk1−1 or wj≥E.qk2−1 then

id←1+max(⌈logq(wjE)⌉,K)

else

id←k1+⌈(wj−E.qk1−1)/(E.qk2−1−E.qk1−1k2−k1)⌉

return id

5.3 Scheduling Problem Modeling
In order to find optimal parameter settings for Workload-Aware algorithm through systematic analysis instead of heuristically tuning, we derive a model for the FPGAPooling scheduling problem based on the classic M/G/1 queuing theory. In this model, we consider that the jobs arrival follows Markovian distribution, so that the arrival of previous jobs will not impact that of future jobs. Job service time has a general distribution, and there is one single server [26]. Our target is to optimize the total Job Completion Time (JCT) based on this model. We start with the following notations and definitions:

λ: average job arrival rate (number of jobs per second).

B: total output bandwidth.

f_s(\cdot): Probability Density Function (PDF) of the job size.

F_s(\cdot): Cumulative Density Function (CDF) of the job size.

s_0: minimum job size. We also consider the job size distribution has a finite mean and variance.

K: number of priority queues. We need K-1 thresholds to assign the K queues.

(t_0, t_1, t_2, \ldots, t_{K-1}, t_K): job size thresholds associated with each priority queue, where s_0=t_0<t_1 <\cdots <t_{K-1}<t_K=\infty.

\lambda _k: job arrival rate of queue k with \lambda _k(t_{k-1}, t_k)=\lambda \big (F_s(t_k)-F_s(t_{k-1})\big).

\mu _k: job service rate of queue k with \mu _k(t_0^{k-1})=B-\lambda \mathbb {E}[s] \sum _{i=1}^{k-1}\beta _i(t_{i-1}, t_i), and \begin{equation*} \beta _i(t_{i-1}, t_i) =\int _{t_{i-1}}^{t_i} x f_s(x) dx \Big / \mathbb {E}[s]. \tag{2}\end{equation*}
View Sourceis the fraction of traffic incoming to the ith queue.

\rho _k: traffic load for queue k \begin{equation*} \rho _k(t_0^{k-1})=\lambda \mathbb {E}[s]\frac{\beta _k(t_{k-1}, t_k)}{\mu _k(t_0^{k-1})}. \end{equation*}
View Source

Using the Pollaczek-Khintchine formula which provides the average waiting time for a M/G/1 queue [49], we have \begin{align*} w_k(t_0^{k})&=\mathbb {E}[W_k] = \frac{\lambda _k}{2(1-\rho _k)} \int _{t_{k-1}}^{t_k} \left(\frac{x}{\mu _k}\right)^2\frac{f_s(x)}{F_s(t_k)-F_s(t_{k-1})} dx\\ &= \frac{\lambda }{2\mu _{k+1}\mu _k} \int _{t_{k-1}}^{t_k} x^2f_s(x) dx. \tag{3}\end{align*}
View SourceTherefore, the total JCT is given by \begin{align*} JCT(t_1^{K-1}) & =\sum _{k=1}^K \int _{t_{k-1}}^{t_k} \left(\frac{x}{\mu _k} + w_k\right) f_s(x) dx\\ & =\sum _{k=1}^K \left(\frac{\beta _k(t_{k-1}, t_k)}{\mu _k(t_0^{k-1})} \mathbb {E}[S]+w_k(t_0^{k})(F_s(t_k)-F_s(t_{k-1}))\right). \tag{4}\end{align*}
View Source

Our goal is to minimize the total Job Completion Time, i.e., \begin{equation*} \mathop{\min} _{t_1^{K-1}} \ JCT(t_1^{K-1}), \end{equation*}
View Sourcewhere the queue threshold t_1^{K-1} is given in (1).

5.4 Analysis of Workload-Aware Algorithm
5.4.1 Relations Between JCT and Workload Distributions
The total JCT depends on value of \beta _i and w_k, which further depend on the PDF of job distribution. Due to the deficiency in surveys about FPGA workloads in datacenter environment, we do not use workload traces from other fields like key-value stores [5]. Instead, we classify FPGA workload distributions into two general categories:

Exponential Distribution (Short Tail). We consider exponentially distributed job size with minimum job size s_0 and average job size \alpha +s_0. Especially, we give the following PDF and CDF \begin{align*} &f_s(x)=\frac{1}{\alpha }\exp \left(-\frac{x-s_0}{\alpha } \right)\\ &F_s(x)=1-\exp \left(-\frac{x-s_0}{\alpha } \right),\quad x \geq s_0. \end{align*}
View SourceIn order to calculate JCT for this kind of distribution, we need to calculate \beta _i and w_k. Specifically, using (2), we have the following equation to calculate \beta _i, with t_0=s_0 and t_K=\infty \begin{align*} & \beta _i(t_{i-1}, t_i)=\\ & \quad \left\lbrace \begin{array}{l}1-\frac{t_1+\alpha }{s_0+\alpha }\exp \left(\frac{s_0-t_1}{\alpha }\right),i=1\\ \frac{t_{i-1}+\alpha }{s_0+\alpha }\exp \left(\frac{s_0-t_{i-1}}{\alpha }\right)- \frac{t_{i}+\alpha }{s_0+\alpha }\exp \left(\frac{s_0-t_i}{\alpha }\right),\quad 1<i<K\\ \frac{t_{K-1}+\alpha }{s_0+\alpha }\exp \left(\frac{s_0-t_{K-1}}{\alpha }\right), i=K \end{array}\right. \end{align*}
View Source

Next, we calculate w_k using (3) \begin{align*} & \quad \ w_k(t_0^{k})=\\ & \left\lbrace \begin{array}{l}\frac{\lambda }{2\mu _1\mu _{2}} \Big ({(\alpha +s_0)}^2 +\alpha ^2 \Big .\\ \quad - \exp (\frac{s_0-t_1}{\alpha })(\alpha ^2+(\alpha + t_1)^2) \Big . \vphantom{{(\alpha +s_0)}^2 +\alpha ^2 - } \Big),k=1\\ \frac{\lambda }{2\mu _k\mu _{k+1}}\left(\exp {\frac{s_0-t_{k-1}}{\alpha }} \right. ((\alpha +t_{k-1})^2+\alpha ^2)\\ \quad - \exp {\frac{s_0-t_k}{\alpha }} ((\alpha +t_k)^2+\alpha ^2) \left. \vphantom{ \exp {\frac{s_0-t_{k-1}}{\alpha }}} \right),1<k<K\\ \frac{\lambda }{2\mu _K\mu _{K+1}}\exp (\frac{s_0-t_{K-1}}{\alpha })(\alpha ^2+(\alpha + t_{K-1})^2), k=K \end{array}\right. \end{align*}
View Source

Substituting \beta _i and w_k with f_s(x) and F_s(x), We can obtain the closed-form JCT expression.

Power-Law Distribution (Long Tail). For power-law distribution, we give the following PDF and CDF \begin{equation*} f_s(x) =\frac{\alpha s_0^\alpha }{1-(s_0/s_{max})^\alpha } x^{-\alpha -1}, F_s(x) =\frac{1-(s_0/x)^\alpha }{1-(s_0/s_{max})^\alpha }, \end{equation*}
View Sourcewhere s_0 \leq x \leq s_{max}, and 0<\alpha \leq 2. \mathbb {E}[s] is given as below [19] \begin{equation*} \mathbb {E}[s]=\frac{\alpha (s_0^\alpha s_{max}^{1-\alpha }-s_0)}{(1-\alpha)(1-(s_0/s_{max})^\alpha)}. \end{equation*}
View SourceNote that if 1<\alpha \leq 2, the second order moment will be infinite, which leads to infinite waiting time for the Kth priority queue and also the overall JCT. In order to avoid this uninteresting case, we add an upper limit on the s_{max} on the job size so that the JCT is bounded. The \beta _i and w_k in f_s(x) and F_s(x) of Power Distribution can also be derived similarly and is omitted here for simplicity. It is worth noting that our approach can be extended to other types of distributions by substituting the corresponding distribution expression.

5.4.2 Hyper-Parameter Selection
The Workload-Aware algorithm is based on the prior knowledge of hyper-parameters that are related to the workload distribution pattern. The value of those hype-rparameters, including \lambda (job arrival rate), s_{0} (minimal job size), and s_{max} (maximal job size), will affect how much workload our system will serve as well as how busy our system will be. We carefully choose those hyper-parameters to make sure that the total workload per time unit is larger than the system’s total throughput (B), so that the system needs scheduling to handle job requests. Otherwise, the system will be idling with few jobs waiting in queues. For simplicity, we use a ratio r to make the traffic load per time unit to be proportional to B \begin{equation*} \frac{\mathbb {E}[s]}{\lambda } = r B, \end{equation*}
View Source

where r > 1, and \mathbb {E}[s] is the expectation of the average job size.

5.4.3 Optimization Through Tuninig
The formulated JCT is non-convex and hence impossible to obtain a viable solution. However, we can get a numerical relationship between JCT and the control variables. Especially, with a certain value of K, the optimal value of q, k_1, k_2 in Equation (1) can be achieved by traversing through a range of variables to find the optimal parameterization. For both exponential and power law distributions, we plot in Fig. 5 the numerical relationship between JCT and K (the number of queues) with three queue structures: linearly threshold, geometrically threshold, and our hybrid queue structure (1). We can observe from the figure that, larger Ks lead to smaller JCTs, and among the three different queue structures, the hybrid queue performs the best under both distributions.


Fig. 5.
Estimated total job completion time under different K.

Show All

For exponential distributions, a purely linear structure needs more queues to achieve the similar performance as hybrid structure. Our hybrid queue also outperforms the purely geometric queue with more than 27 percent improvement on JCT when K is 7, and with a more than 7 percent improvement when K is 20.

For power-law distribution, hybrid queue still also outperforms the linear queue structure. The optimal parameterization for hybrid queue is achieved when k_1=1, k_2=2, i.e., the queue is actually a geometrical queue. For both distributions, the hybrid queue will always guarantee an optimal performance compared to the its complements.

SECTION 6Workload and Resource Aware Scheduling
Resource-Aware Algorithm addresses the challenge of resource constraints, but it is workload-agnostic. Workload-Aware, on the other hand, addresses the challenges of different workload distributions but it is resource-agnostic. To design an algorithm competent for both challenges, we combine the two algorithms as a hybrid, which is called Workload-And-Resource-Aware algorithm. We will show later that, by adopting the workload-aware and resource-aware scheduling, the Workload-And-Resource-Aware is robust to various workload and resource settings, and performs the best compared with single algorithms.

In Workload-And-Resource-Aware Algorithm, the scheduler maintains multiple priority queues. A newly arrived job will be assigned to a certain priority queue based on the Workload-Aware algorithm. Once an accelerator is released, the scheduler will traverse each priority queue, skipping unsuitable jobs until it finds the suitable one based on the Resource-Aware algorithm. If all jobs has been searched once but no job satisfies the requirement to be launched, scheduler will assign the accelerator to the first waiting job within the highest priority queue.

The pseudocode is in Algorithm 3.

SECTION Algorithm 3.Workload-and-Resource-Aware Algorithm
Input: w_j

/*w_j = workload of job j */

if job j arrives: then

id_j \leftarrow get\_priority\_queue(w_j)

add j to the tail of queue id_j

if accelerator i requests for a new job: then

for each q in priority queues do

if Resource-Aware Algorithm (q) == True then

return

assign the first job in q_0 on slot i

return

SECTION 7Performance Evaluation
In this section, we evaluate the FPGAPooling system using both a system prototype and a simulator for large-scale experiments. We implemented the system prototype on an OpenStack Cloud at IBM China Research Lab to verify our system; we also built a fine-grained simulator to conduct large scale evaluation of different scheduling algorithms. First, we conducted the same experiments both on the simulator and on the system prototype to validate the system prototype and the accuracy of the simulator. Next, we conducted different experiments to evaluate the efficiency and robustness of the scheduling algorithms under different workload distributions and resource settings.

7.1 Implementation Setup
Prototype Implementation. We deployed a system prototype on 6 nodes in an OpenStack Cloud. We use 5 worker nodes (IBM S822L) and one controller node, and each node is equipped with a 24-core processor and 512 GB memory. The OS running on each server is Ubuntu 15.10, and the Qemu used for the VM is a modified qemu-kvm-1.0. Each worker node is installed with FPGAPooling middleware, with three of them attached with a Xilinx xc7vx690t FPGA chip on Alphadata ADM-PCIE-7V3 board. A central scheduler using FIFO scheduling is running on the controller node. All of the three FPGA-attached nodes are within the same rack connected over Broadcom 5,720 Dual Port 10 Gb Ethernet Card.

Simulator Implementation. We build a fine-grained, event-driven simulator using Python language. It is designed to emulate the real system performance in large scale. There are four key components in this simulator: jobs, nodes, the FPGA card associated with each node (if any), and the NIC card associated with each node. Initial parameters of these hardware settings, such as NIC bandwidth and FPGA chip throughput, are pre-loaded from a configuration file. We adopt max-min fairness to share both NIC and FPGA-DMA bandwidth. The network topology is emulated as a fat-tree model, so the only bottleneck is in each NIC’s ingress/egress port.

The simulator maintains an event list with events associated with job-delivery (on network) and job-processing (on FPGA), sorted by the event time. Each time a meta workload of a job starts/ends delivery/processing, it corresponds to an event in the list, which will generate new events or modify the time of existing events. In our real system, jobs are transferred and conducted by 4 MB chunk each, so we take the same value as the meta workload in our simulator. The original events in the event-list are job-arrival events, which are pre-loaded from a pre-defined script. We can update this script content to emulate different job distributions.

7.2 Evaluation Benchmarks
We provide five accelerator benchmarks for our FPGAPooling platform:

Advanced Encryption Standard (AES) is an encryption specification which comes with three standard key sizes: 128, 192, and 256 bits. It is a symmetric-key algorithm that uses the same key for both data encryption and decryption [10], [21].

Erasure Coding (EC) is an algorithm that adds redundancy to large storage systems for failure tolerance [15], [39]. It takes input data fragments, expand and encode them with redundant data pieces. It has been recognized as a good replacement for data replication strategy due to its low storage overhead.

Dynamic Time Warping (DTW) algorithm has been widely used for processing sequential signals, including audio and graphical data [44], [53]. It receives sequential samples as inputs and generate the similarity between them as the output.

Fast Fourier Transform (FFT) is a widely used signal processing approach [43], [51]. It maps a sequential input from the time-domain to the frequency domain. A standard implementation of FFT-1D has the following form: \begin{equation*} X[k]=\frac{1}{N} \sum _{n=0}^{N-1} x[n] e ^{-j \frac{2\pi }{N} kn}. \end{equation*}
View Source

Secure Hash Algorithm (SHA) is a widely used hashing algorithm for secure data compression. It takes an input and generates a 20-byte hash value as the output [47], [50].

In our system prototype, each benchmark has been implemented as an accelerator bitfile and can be accessed by calling the acc_open function. As shown in Table 1, each benchmark accelerator requires certain amount of FPGA resources (LUTs, registers, and memory) to be implemented and has a peak acceleration throughput.

TABLE 1 Accelerator Profiles

7.3 Evaluation Metrics
We adopt four metrics in our experiments:

Average Completion Time (ACT): the mean value of all job’s completion time. We denote a job’s completion time as the time interval from its arrival to its finish.

Tail Completion Time (TCT): we mainly evaluate the 95 percent jobs’ completion time. The lower TCT and ACT, the better performance the system achieves.

System Average Response (SAR): the mean value of jobs’ Performance Ratio (PR). We denote PR as the value of a job’s execution time divided by its completion time. Larger SAR means less time spent on waiting, and therefore means better performance.

Data Locality Ratio (DLR): We denote DLR as the value of job workloads that get implemented on its local node divided by the total job workloads. Its optimal value is close to the FPGA resource ratio (i.e., the proportion of number of nodes in the FPGA pool to that of the total worker nodes in cloud).

7.4 System Verification
Our system verification experiments aim to answer the following questions:

Is our system valid in practice? To validate the feasibility of FPGAPooling, we implemented the system prototype in a openStack-based testbed. We used the benchmark applications mentioned above to design FPGA acceleration jobs. Each job request a random acceleration type and claims a data size that follows Gaussian distribution. Job requests from different nodes are submitted to the scheduler, with requesting interval following a Poisson distribution.

How accurate are our simulation results? Due to the fact that we have limited number of FPGA hardwares, we conducted small scale experiments in the real prototype, and use simulated environments for large-scale evaluations. To validate the effectiveness of our simulation, we conducted evaluation on a real prototype, then repeat experiments on a simulated platform with the same workload and topology settings.

Does remote FPGA sharing bring better system performance than the sate of the art? Especially, we compared our system performance with Chen’s work [14], a comparable system structure that enables FPGA local sharing.

For prototype evaluation, we designed three experiments, with parameters shown in Table 2. In the table, local mode means all jobs are from FPGA-equipped nodes, remote mode means all jobs are from non-FPGA-equipped nodes, and global mode means the mixture of the above. We use three experiments to test the simulator accuracy when: sharing network throughput, sharing FPGA-DMA throughput, and sharing both resources. In real settings, the FPGA-DMA throughput is 2.1 GB/s, the NIC bandwidth is 10 Gb/s, and FIFO scheduling strategy is used for each of the experiments.

TABLE 2 Job Patterns
Table 2- 
Job Patterns
As the results shown in Fig. 6, the simulation results are closely matched with those of implementation, with less than 8 percent differences for jobs over 400 MB in each mode. This demonstrates that our simulator is a good predictor of performance that one may expect in a real implementation.

Fig. 6. - 
Comparison of simulator and prototype results.
Fig. 6.
Comparison of simulator and prototype results.

Show All

We compared the performance of FPGAPooling system with Chen’s system. In Chen’s system, FPGA accelerators are scheduled per node and remote FPGA sharing is not considered. We used two FPGA-attached nodes, and each node generates 100 jobs with random job intervals and acceleration requests. To implement Chen’s system, we used Shortest-Job-First algorithm on each node, so that each node only answers local job requests. Next, with the the same job settings, we used the Workload-and-Resource-Aware algorithm to enable cloud-scale scheduling. We explored different size of jobs in both scenarios. Experiment results are shown in Fig. 7. We also observed that the FPGAPooling performance is generally better than local FPGA sharing, although with some jitters in the improvement ratio. Compared with Chen’s system, our FPGAPooling improves the ACT by up to 45.35 percent, and TCT by up to 56.05 percent.


Fig. 7.
Comparison with prior work, scheduling enabled.

Show All

We can also observe that central scheduling algorithms are critical for a system which involves remote resource sharing. Fig. 8 shows the result when we used FPGAPooling system mechanism, but only implemented Shortest-Job-First algorithm in the central scheduler. Compared with local FPGA sharing, the cloud-scale FPGA sharing using a simple SJF scheduling leads to higher job completion times. Therefore, Workload-and-Resource-Aware algorithm is essential to improve FPGA’s performance in cloud-scale.


Fig. 8.
Comparison with prior work, scheduling disabled.

Show All

7.5 Scheduling Performance under Different Job Distributions
This experiment is based on a simulated cluster of 100 nodes, and from each node we generate 50 jobs. We simulate the experiment settings with 50 percent accelerator resource ratio. The NIC bandwidth and FPGA throughput is 10 Gb/s and 2.1 GB/s.

We consider two distributions: exponential distribution and bounded power law distribution, to emulate short-tail and long-tail workloads. The PDF of each distribution is given in Section 5.3. We set the interval among jobs to follow Poisson distribution. For each distribution, we explore different parameters in the PDF to generate groups of workloads, and run simulations under each group of workload. The settings of distribution parameters of size and interval are shown in Table 3. The CDFs are shown in Fig. 9.


Fig. 9.
CDF of different job workloads.

Show All

TABLE 3 Parameter Settings for Workload-Aware Algorithm

Scheduling results under two different workload distributions are shown in Figs. 10 and 11. For both figures, the ACT and TCT improvements of an algorithm are those over the FIFO algorithm, and WA, RA, WRA stands for the Workload-Aware, Resource-Aware, and Workload-and-Resource-Aware algorithm.


Fig. 10.
Performance under exponential distribution.

Show All


Fig. 11.
Performance under power distribution.

Show All

In Fig. 10 (short-tailed workloads), mean is the expectation of the workload size. We can see that locality-aware algorithms (RA and WRA) performs better than locality-ignorant algorithms (SJF and WA), in terms of job completion time and job locality rate. Especially, WRA algorithm improves ACT by 5× to 7×, and improves TCT by 2× to 4× respectively. It also achieves high locality rates (DLR) close to the FPGA resource proportion (50 percent in our case). The advantage of WRA is more obvious when mean is small, because in that case the main resource bottleneck resides in network instead of FPGA resource, so locality is more important for the overall performance. In terms of System Average Response (SAR), WRA algorithm achieves comparable performance to non-delay algorithms (SJF and WA), whereas the RA algorithms achieves lower SAR.

In Fig. 11 (long-tailed workload), smaller \alpha means the longer tail in its CDF graph. We can see that WRA still outperforms non-delaying algorithms (WA and SJF) in terms of job completion time and locality rate. It is worth noting that in Fig. 11, delaying-involved algorithms (RA and WRA) has lower SAR improvement compared with non-delaying algorithms (Fig. 11c). This means that by average, jobs spend more time on waiting, compared to their total completion times. Given the fact that WRA algorithm has the highest improvement on ACT and TCT, we can come to the conclusion that waiting for suitable FPGA resources is worth the time, especially for workload with long-tailed distribution. At the meantime, WRA algorithm outperforms RA algorithm in terms of SAR because it uses knowledge about FPGA contention to avoid over-waiting.

For both long- tail and short-tail distributions, the Workload-and-Resource-Aware algorithm produces the best performance with up to 7× improvement on ACT, and 3× improvement on TCT, respectively. As Table 3 illustrates, we tune the parameters of Workload-Aware algorithm using our formulation based on different job distributions and resource settings. For Resource-Aware algorithm, we set D= 5 and W = 0.01.

7.6 Scheduling Performance under Different Resources Settings
In this experiment, we traverse different resource ratios for network and accelerator to evaluate how different resource settings impact the scheduling performance. The experimental results are shown in Fig. 12, where the FPGA Resource Proportion denotes the number of FPGA-available nodes divided by the number of all worker nodes.


Fig. 12.
Performances under different resource ratios, exponential distribution.

Show All

We can see that when accelerator resource is rare (\leq 40\%), the order of job queueing is crucial, and therefore Workload-Aware algorithm performs better than Resource-Aware algorithm. When the FPGA resource ratio reaches a plateau (\geq 50\%), Resource-Aware algorithm exerts much more importance than Workload-Aware algorithm. Under either scenario, the hybrid algorithm Workload-and-Resource-Aware algorithm is generally optimal by imitating the better one between Workload-Aware algorithm and Resource-Aware algorithm.

7.7 Scheduling Performance under Suboptimal Hyper-Parameters
Note that even if we choose suboptimal parameters, the Workload-and-Resource-Aware algorithm can still achieve satisfying performance. As shown in Figs. 13 and 14, we explore different values of q and k_1 with fixed values for k_2 and K to evaluate the scheduling performance. For the exponential distribution, the theoretically optimal value for q and k_1 should be 1.41 and 5. For the power law distribution, the optimal value for q and k_1 should be 1.8 and 10. We can observe that the ACT and TCT improvements for Workload-and-Resource-Aware algorithm are still generally higher than the Workload-Aware algorithm, which does not involve queue scheduling. For workload that follows power distribution, we achieve at least 3.85× improvement on ACT, and at least 1.82× improvements on TCT. For workload that follows exponential law distribution, the minimal improvements for ACT and TCT are 1.98× and 2.20×. This means that the parameters of Workload-and-Resource-Aware algorithm can be estimated with more heuristics when in lack of information of resource or job settings without too much deterioration on its performance.


Fig. 13.
Performances under different algorithm parameters, exponential distribution.

Show All


Fig. 14.
Performances under different algorithm parameters, power distribution.

Show All

SECTION 8Conclusions
In this paper, we design and implement FPGAPooling, an FPGA-based Cloud system to enable transparent FPGA-based accelerations. Our approach provides FPGA accelerators as a resource pool, allowing application users from any VM to obtain various accelerator implementations by calling our FPGAPooling API functions. This platform frees users from VM resource constraints, making FPGA accelerators to have the illusion of infinite and immediate availability to Cloud tenants. We implement a central scheduler for FPGA pooling, and design online scheduling algorithms that can be applied in cloud-scale. The system prototype has been implemented on a small testbed in an OpenStack Cloud, and performance of our scheduling algorithms is analyzed by fine-grained simulation. We make three key contributions. First, we achieve full FPGA abstraction and sharing on an OpenStack Cloud. We have implemented a system prototype on IBM’s OpenPower Cloud system. Second, we formulated the FPGAPooling scheduling problem as a variant of the M/G/1 model [26], which is adaptive to different job distributions. This model can further serve other general resource scheduling scenarios in modern datacenters. Third, we introduce FPGAPooling scheduling algorithms, which are robust to different workload distributions and resource settings. Compared with the state of the art in FPGA sharing, our system can achieve better performance because of remote sharing and cloud-scale scheduling. we conducted extensive evaluations on both a small testbed and a large-scale simulation. We found that our scheme can improve the average job completion time by up to 7×, and 95 percent tail job completion time by 4×, respectively.

