Protection on end users’ data stored in Cloud servers becomes an important issue in today’s Cloud environments. In this paper, we present a novel data protection method combining Selective Encryption (SE) concept with fragmentation and dispersion on storage. Our method is based on the invertible Discrete Wavelet Transform (DWT) to divide agnostic data into three fragments with three different levels of protection. Then, these three fragments can be dispersed over different storage areas with different levels of trustworthiness to protect end users’ data by resisting possible leaks in Clouds. Thus, our method optimizes the storage cost by saving expensive, private, and secure storage spaces and utilizing cheap but low trustworthy storage space. We have intensive security analysis performed to verify the high protection level of our method. Additionally, the efficiency is proved by implementation of deploying tasks between CPU and General Purpose Graphic Processing Unit (GPGPU) in an optimized manner.
SECTION 1Introduction
With both the development of computers and Cloud computing technology, the trend in recent years is to outsource information storage and processing on Cloud-based services [1]. The Cloud-based services for individual end users [2] are gaining popularity especially for data storage. Relying on large storage space and reliable communication channel, Cloud-based service providers such as Dropbox, Google Drive, or Amazon Drive just to name a few, are providing individual users with almost infinite and low-cost storage space.

This situation raises the question of the trustworthiness of Cloud service providers. Many data security and privacy incidents are observed in todays Cloud services [3], [4], [5], [6]. On the one hand, Cloud service providers deal with a large number of external attacks. In 2018, a total of 1.5 million SingHealth patients non-medical personal data were stolen from the health system in Singapore [7]. On the other hand, Cloud service providers cannot be entirely trusted either [8]. Personal data may be exploited in a malicious way such as in the Facebook and Cambridge Analytica data scandal which affected 87 million users in 2018 [9]. Thus, it becomes increasingly important for end users to efficiently protect their data (texts, images, or videos) independently from Cloud service providers.

One reasonable solution is to protect data on a safe end user’s machine before outsourcing to Clouds which naturally becomes traditional ciphers such as AES [10]. However, encryption algorithms are transferring protection on data to protection on keys which in turn, introduces key management problems. Once the key is exposed, data security will be threatened. Worse, if the end users have no cryptography good practice and try to reuse the same key for different data protection, one key exposure will lead to a large range of data leakage. Thus, in addition to ciphers, other data protection schemes are necessary to support such scenarios.

One previous research direction is the Selective Encryption (SE) [11] which is normally seen as lightweight encryption methods dedicated for multimedia data formats. They exploit redundancies of multimedia data and are mostly based on compression algorithms. Usually, they separate data into two fragments. A private fragment contains most of the information, such that this fragment is usually sufficient to understand the original data. A second fragment, called public fragment, is supposed to contain a much smaller amount of information while taking a large storage space. These two fragments are selectively protected with different approaches according to their respective confidentiality levels [11], [12]. SE methods’ state of art shows that each SE method is designed to protect a specific type of multimedia data and information loss are inevitable [13].

In this paper, we aim at presenting a novel data protection scheme by combining fragmentation, encryption, and dispersion with high performance and enhanced level of protection [14]. Fragmentation methods are introduced for data storage in a cost-effective manner using a public Cloud for the less confidential data fragments. For this matter, we expect the method to split data into a first private fragment that is small in size but important in content, while the other public fragments fulfill most of the storage space and leak little information related to the original data. Then, the private fragment should be well protected and stored in a trusted area such as an end user’s personal computer in the end user to Cloud scenario. The public fragments are stored in public Clouds and cannot be used to rebuild the original data. Thus, this method can provide both, data storage cost effectiveness and prevention of any information leak from the storage in a public Cloud.

In order to extend the concept of SE methods from supporting specific format of multimedia data (with a information loss) to supporting any data format, data integrity must be considered. In fact, a set of SE designs did ignore the data loss caused by the rounding between integers and floating point numbers [15]. In this paper, we chose to use the Discrete Wavelet Transform (DWT) with Le Gall 5/3 filter, for its invertibility property and its losslessness. This will be further explained in Section 3.

Efficiency is critical and is another key rationale. In fact, the overall performance gain of previous SE methods is not that obvious, as some of these approaches did add a pre-processing phase of data analysis which could lead to far worse performance than a full encryption [16]. In this paper, we propose to introduce General Purpose Graphic Processing Unit (GPGPU) [17] as an accelerator for better performance. In Section 5, we will compare our method with an implementation of full encryption AES-like on two typical hardware environments: a laptop platform (with a limited computing resource environments and a low-end GPGPU) and a desktop platform (with a high-end GPGPU).

Also, a long list of security analysis is performed to show that our method can withstand in-front of a number of known attacks with details is shown in Section 4.

In summary, our contribution is mainly threefold: (1) by designing a novel data protection scheme supporting data integrity, extending the old SE concept. This data integrity allows protecting any format of data. We say in the following that the scheme is agnostic, (2) by implementing this design on different practical hardware environments with GPGPU acceleration, we can favorably compare performance with a classical AES encryption ; (3) by using this proposed method in a cost effective storage manner with an end user to cloud scenario which can protect data against different threats.

In Section 2, we briefly introduce some work related to SE methods. In Section 3, we present our method, in particular, introducing a design that can fit for arbitrary data and key implementation details. In Section 4, a detailed security analysis with various tests on different data formats is presented. In Section 5, we evaluate the performance of our implementation regarding the two hardware environments. We discuss and list the future works in Section 6. We conclude this paper in Section 7.

SECTION 2A Brief Review of Related Works
In this section, we will briefly introduce the existing SE methods and point out the shortcomings. Some new criteria will be also mentioned to compare our results and existing solutions. In [15], the most two common criteria for the multimedia SE methods are shown as histogram analysis and correlation analysis. However, the criteria for evaluating data protection methods should be extended according to the practical use cases such as the secure data storage from the end users to Clouds described in this paper. For instance, the execution speed must be measured on practical hardware platforms and compared with encryption algorithms (AES-128 in this paper). The security level must be also evaluated according to the design purpose. Data integrity, as a basic requirement for realizing format agnostic, is also important to be evaluated. For the secure data storage from end users to Clouds use case, considering the storage allocation optimization and resistance to error propagation are also necessary. The brief comparison is shown in Table 1.

TABLE 1 A Brief Comparison between Previous SE Methods and the Proposed Method

For evaluating the execution speed, it is important to first consider whether in the design level there are additional preprocessing steps such as the DCT process shown in [18]. For this method, only the preprocessing step based on DCT is slower than using AES on the entire data found on a modern CPU as pointed in [15], leading to performance issues that are not taken into consideration. Such problem is always ignored by transformation-based SE method such as [19], [20], [21]. In our method, we use GPGPUs to accelerate the calculation tasks and the execution times are evaluated to prove the efficiency compared with AES or AES-NI.

The security level is always based on the design purpose. For instance, some multimedia SE methods are designed to only reduce the visual effects which are normally seen as low level considering security such as [18]. More specifically, if the protection is only done on the private fragments, we consider it as low security level as there are many related works to show the direct recovery from the public fragments such as [13], [22]. Thus, the only previous works qualified high security levels in Table 1 are [15], [21], [23]. In this paper, intensive security analysis is performed to prove a high security level is achieved with protecting both the private fragments and public fragments.

Data integrity is an important criteria but is always ignored in previous SE methods. For instance, in [20], a fractional Wavelet-based SE method is used to degrade the image quality. But data integrity cannot be guaranteed as the rounding errors of calculations between integers and floating point numbers are ignored which will cause serious issues as shown in [22]. For the SE methods based on compression and coding, the data integrity could be guaranteed. However, SE methods designed based on compression and coding techniques are always relying on the details of specific compression and coding algorithms which lead to error propagation and format reliance. For instance, in [23], a protection method for JPEG2000 images is presented consisting in permuting the MQ lookup table. This will lead to error propagation in the decoding process when there are tiny errors in the transmission and also make this method only available when MQ coding is used. Such issue is avoided in our method with processing data as matrices of bytes in an agnostic manner and designing the allocation of data fragments according to communication channel status.

Storage optimization is considered in this special use case of secure storage from end users to Clouds. For most SE methods, the fragmentation concept is not designed based on the storage usage of public Clouds which optimize the storage space usage of the trusted area. In this brief review, only the work shown in [15], [23], [24] could be used to optimize the trusted storage area by uploading the public fragments to the Clouds. In this paper, we defined the confidential levels of the fragments and the public fragments are also protected. Thus, the small private fragment with high confidential level can be stored in an area trusted by the end users while the public and protected fragments can be stored on public Clouds with resistance to attacks.

SECTION 3Design and Implementations
In this section, we first introduce the general concept of selective encryption and then combine it with the idea of fragmentation and dispersion. Afterward, several key design and implementation elements will be given in order to illustrate with our practical protection method.

3.1 General Concept
The initial idea of existing SE methods is to protect a piece of information by encrypting only a part of it. This could increase the overall performance for multimedia contents by reducing the data amount that has to be encrypted.

In our design, additional concepts are introduced: fragmentation and dispersion. As shown in Fig. 1, input data d is separated into two fragments, d1 and d2: d1 must take important elements of original information and little storage space. d1 will become the private fragment after encryption. d2 is the public fragment. It is intended to take up most of the storage space, while carrying little pertinent information from d. Then, d1 can be stored in local and private storage which could have limited storage space and d2 can be stored in a public Cloud with lightweight protection.

Fig. 1. - 
Concept of SE with a dispersion into two independent locations optimizing trust area storage usage.
Fig. 1.
Concept of SE with a dispersion into two independent locations optimizing trust area storage usage.

Show All

3.2 Discrete Wavelet Transform
In previous works [18], [29], Discrete Cosine Transform (DCT) was used to support fragmentation decision before performing SE for bitmap protection. As pointed out in [15], DCT cannot guarantee the losslessness due to conversions between integers and floating point numbers which will result in rounding errors [22]. These rounding errors can be reduced with a precise design with more storage space, but cannot be totally avoided. This issue makes DCT unusable and unable to provide the necessarily required integrity in order to deal with agnostic data formats.

DWT is a signal processing technique used to extract information that is mostly used in multimedia compression standard such as JPEG2000. It can represent data by a set of coarse and detail values in different scales. Naturally, it is a one-dimensional transform, but it can also be used as a two-dimensional transform applied in both the horizontal and vertical directions. For the two-dimensional case, DWT will generate four sub-matrices where each sub-matrix is a quarter size of the original matrix. Results for one level 2D-DWT are: one with low resolution (LL), one with high vertical resolution and low horizontal resolution (HL), one with low vertical resolution and high horizontal resolution (LH), and one with all high resolution (HH). Then, the second level transform will only be done for the LL part which is called dyadic decomposition as shown in Fig. 2.


Fig. 2.
An example for a two level DWT which generates two-dimensional coarse and detailed values.

Show All

In our design, a two-level 2D-DWT is chosen and is illustrated in Fig. 2. The Le Gall 5/3 filter [30] is being used as it has an important lossless property. The DWT-2D based on Le Gall 5/3 filter fits best for our design by providing both data integrity and efficiency as it can enjoy the acceleration brought by GPGPUs [16], [31]. The selected coefficients used in order to build the private fragment are the 2nd LL which takes about 1/16 of the storage space and carries the basic elements (coarse information) of the input matrix. The reason for using two-level DWT is that the one-level DWT still has a large part (1/4 of the whole DWT-2D result) of LL coefficients in order to be able to protect, and three or more DWT levels in order to make the range value of the high frequency coefficients too large, leading to the unnecessary consumption of storage space.

Performance of DWT must be considered based on comparing the execution time against full encryption. In some scenarios, the preprocessing steps of SE can legitimately be ignored, as SE and compression are integrated, such transformation is being used by both applications [32]. However, our use case that encompasses any kind of data, will have to take into account the entire process when it comes to performance evaluation. This will lead us to implement DWT on a GPGPU in order to benefit from the acceleration supplied by the parallel architecture of a GPGPU [33] (more implementation details in Section 5).

3.3 System Designs
Any kind of data can be seen as a sequence of data chunks Di were each chunk Di will be defined in turn as a square matrix with a tunable size (512×512 or 1024×1024 bytes), depending on the accommodation of transformation or the hardware implementation. Every element of Di is a byte which can be seen as an 8-bit integer. Then every chunk Di will simply be processed using the SE method block by block with a block size of 8×8 as shown in Fig. 3. The chunk size can be changed according to the implementation details, especially the GPGPU hardware configuration. The block size is supposed to be always 8×8, which fits best our design as indicated in Section 3.4. This tiling step is used not only for fitting with the GPGPU architecture but also for the best design of the three fragments in Fig. 3.


Fig. 3.
Proposed SE method for the single 8×8 block.

Show All

For every 8×8 block, the first step is to perform the 2D Discrete Wavelet Transform (DWT-2D). In our work, two successive levels of the DWT-2D were performed with the Le Gall 5/3 filter. The low frequency coefficients (2ndLL coefficients) are considered as the private fragment. This fragment takes only 4 out of 64 coefficients but carries most of the information according to an energy viewpoint. The AES-128 bit [10] will be used to protect this fragment. In our design, the code is structured such that another cipher algorithm can easily replace AES-128 if needed. The other two coefficients levels are considered as the two “public and protected fragments” (PPFs) as shown in Fig. 2.

The private fragment of each 8×8 block will then be used to generate a 256-bit sequence, by using SHA-256 [34]. This will guarantee the generation of different bit sequences, even when the corresponding private fragments in the neighboring blocks are very similar (encryption key is also involved to guarantee the key sensitivity as shown in Fig. 3). This bit sequence is used to protect the 1st PPFs (the remaining coefficients of 2nd level DWT are shown in Fig. 2) by performing an XOR operation. This fragment is defined as the 1st PPF. For the 2nd PPF which contains the remaining DWT coefficients, protection is done by XORing it with a bit sequence generated from SHA-512 [34] based on the inputs of 1st PPF and the encryption key.

The protection of the PPFs is provided by XOR operations and is based on the randomness guaranteed by the SHA functions. For example, in some cases like bitmaps, as long as there are redundancies due to similar neighboring pixels, the frequency coefficients could be very similar, especially between neighbor blocks. However, the SHA-256 and SHA-512 function will generate a totally different bit sequence, even when there is only one different bit as input (e.g., 2ndHL as input for SHA-256; 2ndHL, 2ndLH and 2ndHH as input for SHA-512). This randomness will then be added to the PPF by XORing similar frequency coefficients along random hash values of the last layer of fragments. In addition, this design also has a good effect to resist against redundancies of any kind of input files. More security analysis will be shown in Section 4. For the defragmentation and decryption, it is simple to just reverse the process in Fig. 3 since all steps are symmetric.

3.4 Numerical Precision to Enable Agnostism
The preprocessing step used to separate data may lead to integrity problems, where data before and after reversing protection could be different. In previous SE methods, this is normally due to rounding errors of conversions between integers and floating point numbers. One way to solve this problem is described in [35], where the authors proposed to declare all variables with a double precision based on their bit-length of 64 bits. This leads to a large increase in the usage of storage space without being able to totally avoid such rounding errors. However, it is not optimal in terms of footprints to use larger storage space in order to float precision numbers, especially if the input data is stored as an integer with a bit-length of 8 bits, where the resulting storage space will require 8 times the storage space of the original data. In [15] an optimized value representation in terms of the footprint was designed, yet it was not possible to totally avoid rounding errors caused by the DCT.

In this paper, the preprocessing step is the DWT based on “LeGall 5/3” filter which is designed to be an integer-to-integer map, such that this DWT is lossless. As a result, on one hand, any rounding error is avoided; on the other hand, the extra storage space usage caused by the int to float conversion does not exist either. The only possible extra storage usage can be caused by the different range value of the input 8-bit int, and the output int coefficients. The output value range can be calculated as long as the input values are always stored Byte by Byte. The input value range (seen as unsigned value) will be then from 0 to 255 which can be considered as from −128 to +127 (the range is seen as from −128 to +128 during the following calculation). Then the storage methods can be designed according to the value range distribution.

The first level DWT-2D transformation is actually calculated by twice DWT-1D transformations on the 8×8 block in horizontal and vertical directions sequentially. The first horizontal transformation generates two sub-matrices which are 1stL and 1stH that take each half of the result matrix horizontally. The vertical transformation is done on each of the two sub-matrices which generates four sub-matrices like in Fig. 2 (1stLL,1stHL,1stLH,1stHH).

In the first horizontal transformation, the range for 1stH is -255 to +255 (did double the input range), and the range for the 1stL is -192 to +192 (did increase the input range by 1.5 times). Then the transformation in the vertical direction, which transformed the 1stL and 1stH blocks respectively, gets the following results: 1stHH is from −511 to +511 and the range for 1stLH is from −384 to +384. All the coefficients in the three sub-matrices of the first level of DWT-2D transformations can be stored using 10-bits space.

The value range of second level DWT-2D coefficients is generated by the same two directions DWT-1D that transforms the 1stLL sub-matrices coefficients. The range of the second level DWT coefficients can be estimated by simplifying the equations of DWT-2D with “LeGall 5/3” filter, and then directly get results from calculating the final formula of each element in the four sub-matrices in Fig. 2 (2ndLL,2ndHL,2ndLH,2ndHH). And the max values and min values for each of the value estimated are shown in the following matrices. The storage method for the second level DWT-2D coefficients is: 11-bits for each of the lower left corner, four coefficients (2ndHH), and 10-bits for remaining of the coefficients.
⎡⎣⎢⎢⎢338260468468260200360360468360648648468360648648⎤⎦⎥⎥⎥⎡⎣⎢⎢⎢−338−260−468−468−260−200−360−360−468−360−648−648−468−360−648−648⎤⎦⎥⎥⎥.
View SourceRight-click on figure for MathML and additional features.

3.5 Storage Space Usage
As pointed out in Fig. 3, the private fragment that we selected is a sub-matrix (2ndLL of Fig. 2). According to the design, this private fragment is supposed to be stored in a trusted area (locally in this paper), and the remaining two fragments are supposed to be stored in public Cloud servers. In fact, this storage configuration for the three fragments could be flexible according to the quality of the communication channel. The storage space taken by the private fragment will be only 7.8 percent of the input data size (for each block, only 40 bits compared with the original storage space 512 bits). However, if the transmission errors are considered, the avalanche effect [36] must be avoided. In such a situation, the 1st PPF will also be stored in a trusted area with the private fragment. This design is also chosen as the one evaluated by us for the storage space design in this paper. The fragments stored locally will take 164 bits in total (40 bits for private fragment and 124 bits for the 1st PPF as shown in Fig. 3). The fragment dispersed to the Cloud servers will take 480 bits. The total storage space usage is 644 bits, at least which is about 26 percent more than initial input 512 bits, but the 2nd PPF (480 bits) can be stored on Clouds without leaks, and the private fragment is at most 32 percent according to the stronger storage design.

In summary, the preprocessing step is the DWT-2D based on “LeGall 5/3” filter which is designed to be an integer-to-integer map is lossless. In our design, we consider any kind of data type as int with bit-length of 8 bits which means regardless of the type of original data, we read the data by taking one byte at a time and reading it as an 8-bit integer. Then, the input bytes will form a 2D matrix of a configurable size. As a result, on one hand, any rounding error can be avoided; on the other hand, the extra storage space usage that could be caused by the int to float conversion does not exist. Also, the numerical data type of variables involved in DWT computation is carefully designed, in a way that can provide the lossless property, thereby providing necessary agnosticism for any data formats.

SECTION 4Security Analysis
In this section, different security tests on the proposed method are performed in order to establish its level of security. The private fragment of one data chunk is supposed to be securely protected. We assume it is encrypted with AES-128 but can be replaced with other encryption algorithms as the flexibility shown in Fig. 3. Thus, the security property of the private fragment will not be analyzed in this paper.

Therefore, the remaining challenge is that if it is possible to recover the input data chunks directly from the PPFs. More specifically, the threat model is that an attacker could get the PPFs stored in the Clouds and try to recover the original data from these 2nd fragments. As mentioned in Section 3, either the 2nd or both the 1st and 2nd PPFs could be stored in Clouds which can be guided in particular by considering the quality of communication channel. Therefore, both fragments need to be analyzed. The goal of our protection is to make these fragments as similar as possible to random values for any kind of data type. It is shown in [16] that the security properties of the two PPFs are very similar. In this section, we will only show the analysis results of the 2nd PPF.

In this section, we present the figures used for the security analysis, revealing all statistical results for a bitmap image, video files with mp4 format, and English texts in Table 2 respectively. There are other two video formats (rmvb, mkv) that were tested and the results are very similar. We have also tested more than 10 common seen data formats with a large number of files and the results are all similar. For the video and text files, the security analysis results are from the average of 100 randomly picked chunks (chunk size 512 × 512 bytes). Some criteria like PSNR are more suitable to measure images than other data types.

TABLE 2 Statistical Sensitivity Results for 2nd PPF for Image, an English Text, and an mp4 File

4.1 Uniformity Analysis
Immunity against statistical attacks requires that the PPFs must attain a high level of randomness [37]. In order to validate the robustness of this method, different statistical security tests are being applied with independence between the input data and the PPF. We started by showing the visual effects compared between an input image and the corresponding 2nd PPF as shown in Fig. 4. Then, accordingly, the Probability Density Function(PDF) of the PPF is used for testing and the results should be as uniform as possible. In this case, this means for any kind of data formats as input, each symbol (byte in the matrix) has a probability occurrence close to 1n, where n is the number of symbols (1256=0.0039 in byte level). As the input chunk size is 512 × 512 bytes, which means the matrix is 512 × 512, the uniform distribution of the symbol occurrence should be 1024 times for each symbol value from 0 to 255.


Fig. 4.
Visual results comparison between: (a) original image, (b) 2nd PPF.

Show All

The PDF for the input image and its corresponding 2nd PPF is shown in Figs. 5a and 5b, respectively. It can be observed that the PDF of the 2nd PPF is close to the uniform distribution with each symbol value occurs around 1,000 times. The distribution of the PDF for the input image and its corresponding 2nd PPF are very different, especially with the original one tending to be uniform.


Fig. 5.
Image: (a) PDF of an input image, (b) PDF of the corresponding 2nd PPF. Text file: (c) PDF of a text file, (d) PDF of the corresponding 2nd PPF. Mp4 file: (e) PDF of a mp4 video chunk, (f) PDF of the corresponding 2nd PPF. Rmvb file: (g) PDF of a rmvb video chunk, (h) PDF of the corresponding 2nd PPF.

Show All

Then, in Figs. 5c and 5d, the same comparison of English text is shown. As the English text is an ASCII file so the input value range is from 0 to 127. However, the PDF of its 2nd PPF is still close to uniform within the range from 0 to 255. We also show such analysis results for two kinds of video chunks (a mp4 data chunk and a mkv data chunk) in Figs. 5e and 5f for mp4 and in Figs. 5g and 5h for mkv. The obtained result indicates that the PDF of the 2nd PPF for video files possesses a uniform distribution.

We have more tests for different data formats. The results indicate the PDF distribution of the 2nd PPF tends to be a uniform one no matter what the data input type is. For the video cases, as long as the input chunks are video file contents which are already compressed and encoded, the original PDF distributions have no obvious pattern but the PDF of the PPFs still have shown a uniform distribution.

4.2 Test Correlation for PPFs
Lower correlation between input data and PPFs is an important factor that allows the validation of the independence between them. Having a correlation coefficient close to zero means that the high degree of randomness is obtained. The correlation coefficient of ρ is calculated according to the same method used in [15].

In this section, we show the correlation coefficient of ρ for images, text files, and mp4 files as input to analyze any correlation. The results are shown in Table 2. The distribution value ρ2, ρ−h, ρ−v, and ρ−d for the image case indicate the correlation relationship for the three directions (horizontal, vertical, and diagonal) are all close to zero. For the text and video cases, we only show the ρ−h value here as the ρ for other directions are very similar. The obtained result indicates that the coefficient correlation varies in a small interval very close to 0. This means that a low correlation coefficient is attained by employing the proposed method, and consequently, the independence between the input data chunk and its 2nd PPF is therefore also attained.

4.3 Information Entropy Analysis
The information entropy of a data sequence M is a parameter that measures the level of uncertainty in a random variable [38], and is expressed in binary which is defined by the Equation (1).
H(m)=−∑i=1np(mi)log21p(mi),(1)
View SourceRight-click on figure for MathML and additional features.

where p(mi) denotes the probability of symbol mi. It is easy to calculate a random source emitting 2N symbols, that show that the entropy is equal to N. In this design, as the data input matrix is always seen as 8-bits per element, the pixel data have 28 possible values. As such, the entropy for a random information source must be 8. For the image case, the entropy of the PPFs for different images are always more than 7.999, which proves a high level of randomness.

The entropy tests are also applied to the PPFs of English text files and three different types of video formats. For each file formats, 100 data chunks (each one is 1024 × 1024 bytes) are randomly chosen for the entropy tests. We calculate the entropy values of the input data chunks and their 2nd PPFs. As shown in Table 3 the PPF of text chunks always has high randomness with entropy values between 7.9992 to 7.9995 compared with the entropy value range for the input data chunks between 4.5961 and 4.6938.

TABLE 3 Entropy Test Results of 100 Random Chunks for Three Kinds of Video Formats and English Texts
Table 3- 
Entropy Test Results of 100 Random Chunks for Three Kinds of Video Formats and English Texts
As long as video files are compressed, encoded and formatted, the entropy values of the original video data chunks are already very close to 8 (always more than 7.9). However, in Table 3, there are still randomness improvements for the three video formats with the minimum entropy values for video data chunks are always more than 7.999. Therefore, the proposed method can achieve a very low entropy that is capable of resisting against an attack based on entropy analysis for the given tested file formats.

4.4 Difference Test
The difference ratio between the input data chunk and its PPFs is tested. The PPF must be statistically different from the original data (50 percent) at the bit level. The proposed method has achieved a high value of difference before and after processing all the tested data formats. For example, the image is tested and the obtained result in Table 2 revealed that 50 percent of bits (half the bits) are being changed between the PPF and the input image. Additionally, a similar result was obtained for text and video files, and the statistical value for the text file and mp4 file are tested (see Dif in Table 2).

To confirm the results of difference tests, we also applied the Normalized Mutual Information (NMI) test [39] between the input data chunks and their PPFs. The obtained results for different image data files are shown in Table 2 and the NMI values are always smaller than 0.02. For the text and video files, we test 100 randomly picked data chunks and the results show that the NMI values are always close to 0 (see NMI in Table 2). As NMI values should be in an interval of (0,1), a small NMI (close to 0) indicates that no detectable information can be extracted from the PPF which can be further used to recover the input data chunk.

4.5 Sensitivity Test
Differential attacks are based on studying any possibly existing differences between two encrypted data, which can result from any particular changes in the original plain-image or in the key. A sensitivity test shows how much a slight change will affect largely the encrypted data which could avoid the pattern being exploited by such side channel attacks. Ideally, with 1-bit difference in the input data chunk, a 50 percent change in bits of its PPFs is able to resist against a differential attack. In this section, we analyze two types of sensitivity: plain text sensitivity and key sensitivity.

Plaintext sensitivity tests permit to quantify the sensitivity against any slight changes in the input data chunk. Due to the randomness introduced by Hash functions, a one-bit change in the input data chunk will lead to 50 percent difference in the Hash values which will be used XORed with the PPFs. Thus, the slight change of the input data chunk will lead to a significant difference in the PPFs. In fact, as long as most files transmitted on the Internet are compressed and formatted, it is rare to see the same blocks in the same chunk. For the specific case with repeated input data chunks or blocks, a counter could be added as the input of the SHA function in Fig. 3 to generate different PPFs.

Key Sensitivity tests permit to quantify the sensitivity against any slight changes in the key. In fact, for the private fragment, the encryption algorithm used (AES-128) can meet such sensitivity requirements. In order to test the key sensitivity for the PPFs, two secret keys were used: SK1 and SK2, with only one random bit difference between the two keys. The two input data chunks are processed separately, and the Hamming distance of the two corresponding PPFs is computed. Our results averaging 100 tests for the key changes are shown in Table 2 where KS stands for Key Sensitivity. It can be observed that the obtained values are always close to the optimal value (50 percent bits changes as one sole bit changes in the key). This indicates that the proposed method ensures a high level of sensitivity against any tiny change in the key. Similar results have been obtained for video files and other test files.

4.6 Visual Degradation
This test is specific for images and permits in order to quantify the visual degradation that is reached by employing our proposed scheme. In fact, the degradation operated on the original image must be done in a way that the visual content presented in the protected image must not be recognized. Two well-known parameters are studied in order to measure the visual quality degradation: Peak Signal-to-Noise Ratio (PSNR) [40] and Structural Similarity (SSIM) [41].

PSNR is derived from the Mean Squared Error (MSE), while MSE represents the cumulative squared error between two images. A low PSNR value [40] indicates that there is a high difference between the original image and the PPF.

SSIM is defined after the Human Visual System (HVS) and evolved afterward which allows extraction of structural information from the scene. SSIM is in the interval [0,1] and a value of 0 means that there is no correlation between the two images, while a value close to 1 means that the two images are approximately the same. PSNR and SSIM are measured between the input image files and its PPF and presented in Table 2, respectively. The mean PSNR value is 9.23 dB which validates that the proposed method provides a high difference in visual effects between the input images and their PPFs. The SSIM value never exceeds 0.042 proved a hard visual distortion obtained. As a conclusion, the proposed method ensures a hard visual degradation which means that no useful visual information or structure about the input data will be revealed from the PPFs.

4.7 Error Propagation
Indeed, an important criteria that should be ensured for any data protection scheme is to measure the error propagation. The interference and noise in the transmission channel might cause errors. Bit error means that substitution of ’0’ bit into ’1’ bit or vice versa. This error may propagate and lead to the destruction of data recovery at the receiver ends. It is a big challenge since a trade-off between avalanche effect and error propagation was shown in [11]. In fact, network coding schemes exist and are being used in order to deal with such kind of transmission errors. However, in this paper, we still consider it necessary to prove that our method can resist the avalanche effect in the application layer. In fact, if a bit error takes place in the PPF of a chunk, this error will propagate randomly only in its corresponding blocks and will not affect the other blocks in this data chunk. If we assume the 2nd PPFs are the only ones being transmitted, any error occurred in this 2nd PPFs will not propagate as the XOR operation will not propagate errors. However, if the 1st PPFs and 2nd PPFs are both transmitted and an error occurs in the 1st PPFs, the Hash function will lead to 50 percent of the difference of the 1st PPFs which will endanger the recovery for the 2nd PPFs. Thus, for resisting the avalanche effect in the transmission process, we have different schemes for transmission. Only the 2nd PPFs can be transmitted through an unreliable communication channel. Thus, 1-bit error in the 2nd PPF will lead to 1-bit error in the recovery results and resist to error propagation is achieved.

4.8 Cryptanalysis Discussion
In this section, typical published cryptanalytic cases are considered and a brief analysis of the proposed method against several cryptanalytic attacks is provided from a cryptanalysis viewpoint. The proposed method is considered to be public and the attacker has complete knowledge to all steps but no knowledge about the secret key. But the strength of the proposed method against attacks is based on the existing cipher systems we deployed.

For the private fragment, AES has keyspace that can be 2256, which is sufficiently large to make the brute-force attack almost infeasible. Furthermore, differential and linear attacks would also become ineffective. For the PPFs, SHA-256 and SHA-512 guarantee the randomness. In fact, any change in any bit of the key causes a significant difference in the produced PPF as can be seen in Table 2. Hence, a key is used for every block (shown in Fig. 3) and as the difficulty of cipher-text-only attack is equal to one of the brute force attacks, it becomes impossible for a cipher-text-only attack to retrieve useful information from the PPFs.

The spatial redundancy between adjacent elements of input plain data are removed and a high randomness degree of all fragments is achieved. Different statistical tests such as the entropy analysis, PDF analysis, and correlation analysis are applied to validate the independence and uniformity property. Consequently, these results indicate that no useful information can be detected from the PPFs to rebuild the input data. This validates the robustness of the proposed method and their high resistance to statistical attacks.

For further cases like a single plain-text failure, Initialization Vector (IV) or counters could be introduced to generate dynamic keys for each of the chunk. In such a case, we increase the difficulty levels for an attacker to recover the dynamic keys that are changed for every input chunk. For accidental key disclosure, the attacker must be able to retrieve all the fragments to recover the input data chunk due to the relations we build among three fragments. For the worst case, we assume an end user is using the same key all the time and this key has been leaked to the attacker, and the attacker managed to get all fragments for data chunk Dk to recover the input data chunk Dk. However, for the other data chunks such as Dk+1, the attacker will still need to steal all the fragments for the Dk+1 and the knowledge to the key will not help to recover other data chunks.

SECTION 5Performance Evaluations
In order to achieve the best performance, all calculation resource of a hardware platform, both CPU and GPGPU, should be utilized. In this section, the evaluation of the performance will be presented. First, we listed the two GPGPUs tested in this paper with the detailed hardware information. Then, all the calculation tasks are evaluated on these GPGPUs including the corresponding CPUs, respectively. An architecture of overlapping the calculation tasks is given to evaluate the execution speed of our method.

5.1 Hardware Architecture Considerations
In our implementations, two different platforms are evaluated respectively. The laptop platform is assumed as a CPU with a GPGPU with limited calculation capacity (seen as a low-end GPGPU). Then, the desktop platform is assumed as a CPU with a GPGPU of powerful calculation capacity (seen as a high-end GPGPU). In summary, there are three GPGPUs tested in this paper which one of them is seen with the laptop platform and the other two are seen with the desktop platform. We tested two of the GPGPUs and compare the performance in this section. First, an Nvidia Nvs 5200M GPU is listed as a low-end GPGPU available on laptops with only 96 CUDA cores and 1 GB GPU memory. Then, a GeForce gtx 780 GPU is listed as one of the high-end GPGPUs with 2,304 CUDA cores and 3 GB GPU memory.

CPUs usually have a handful of powerful physical cores (4 or 8 on an Intel CPU for PC) that allow only a limited number of threads to actually run in parallel. However, GPGPUs can contain hundreds even thousands of calculation threads executed in physical parallel. The most commonly used GPGPU architecture is the Compute Unified Device Architecture (CUDA) from Nvidia [42]. The GPGPU calculation capacity has had and kept on having a remarkable growth rate [43] making this resource more and more attractive for general purpose computations.

As pointed out in [33], DWT-2D on tiled 8×8 blocks fits better on a GPU than a CPU architecture as the calculations naturally fits the physical parallel computing with a huge number of threads. In [33], a CUDA algorithm performs the one level DWT-2D Daubechies 9/7 DWT [44] in 45 ms (without considering data transfer between GPU and host memory) for a 4096×4096 image using Nvidia Tesla C870 which is about 20 times faster than a PC CPU of the same hardware generation. In this paper, we tested the performance of DWT-2D Le Gall 5/3 on two different GPGPUs and the results compared with CPUs settle the choice for implementing DWT on GPU, even a low-end GPGPU on a laptop platform. The results of DWT-2D Le Gall 5/3 on three GPGPUs are shown in Table 5. The difference between the low-end GPGPU and the high-end GPGPU is huge.

The other main calculation task is the two kinds of Hash functions (SHA-256 and SHA-512). As pointed in [45], the Hash functions can also enjoy the parallel computing architecture brought by the GPGPUs. Thus, we tested the execution speed of Hash functions on three GPGPUs and the results are listed in Table 5. According to our tests, the execution speed of Hash functions are very different on different GPGPUs: the GeForce gtx 780 is about 10 times faster than the Nvs 5200M to process Hash functions of SHA-256 and SHA-512.

5.2 Evaluation for the Proposed Method
In this section, we evaluate the performance of the whole protection process. As calculations are allocated on the PC platform, all the available computing devices (CPU and GPU) are considered. Thus, we design the architecture of parallel computing the calculation tasks on CPU and GPGPU. As pointed out in Section 5.1, DWT-2D, SHA-256, and SHA-512 can benefit from the GPU acceleration. So the design is based on the parallel execution of CPU with GPU while the GPU will execute DWT-2D, SHA-256, and SHA-512 while AES-128 used in order to protect the private fragment, to run on the CPU. The basic idea of this architecture for both low-end and high-end GPGPU scenarios aims at keeping the GPU always busy while the CPU would have time for other tasks as shown in Fig. 6.

Fig. 6. - 
Time overlapping architecture of the implementation.
Fig. 6.
Time overlapping architecture of the implementation.

Show All

As the calculation capacity of GPGPUs could be very different, the execution time of SE methods could be very different as well. The execution speeds of the SE method are evaluated in two typical scenarios with two GPGPUs respectively. The laptop has an Intel I7-3630QM CPU and an Nvidia Nvs 5200M GPU. The desktop has an Intel I7-4770K CPU and an Nvidia GeForce gtx 780 GPU.

In order to verify the optimality of our task allocated decisions, we evaluate the execution time for each task on the laptop, then on the desktop using different input data chunk sizes. The results are presented in Table 4. The selected matrix sizes are: 2048×2048Bytes, 3200×3200Bytes, and 4800×4800Bytes. If the series of input data chunks are of the same size, then, according to the test results shown in Table 5, the execution time of GPU for the second input data chunks can always overlap the execution time of CPU for the selected DWT-2D coefficients of the first input data chunk. Thus, for a series input of data chunks with the same size, we evaluate one time period in Fig. 6 as the speed of executing one data chunk with the proposed method.

TABLE 4 Performance Evaluation for Every Calculation Tasks of SE for Two Platforms

TABLE 5 Hash and DWT Speeds on Two Platforms

Basically, the speed of the SE method relies on how fast the GPU can process its calculation tasks on input data, as long as there are many chunks (D1, D2, etc) as input. In other words, in these two cases used, the time consumed by GPU is evaluated as the benchmark for our SE method. The execution speed of our SE method, evaluated for the laptop platform is about 360 MB/s, for the desktop with GeForce gtx 780 GPU scenario is about 2.8-3.2 GB/s.

As shown in Fig. 7, the performance of the SE method is compared with the traditional CPU-only AES-128 speed. The SE methods on laptops and desktops as shown in this figure, corresponding to the performance listed in Table 4 and the AES-128 speed is tested based on the implementation shown in [10]. It is worth noticing that [46] pointed out that the CPU-only AES can also be very fast with the support of the New Instructions extension (NI) brought by Intel. This AES-NI can also accelerate the AES on CPU by more than 5 times, which also achieved almost 3 GB/s on a NI-enable Intel CPU (shown in Fig. 7) which is almost the same speed as our method tested with the GeForce gtx 780. For the smaller data chunk size, the AES-NI is even faster than the proposed SE method due to the chunk size did not fit the workflow size of the GPGPU. When chunk size increases, the execution speed of our proposed SE method is faster than AES-NI shown in Figs. 7b and 7c.


Fig. 7.
Performance evaluations for SE on two different scenarios compared with AES-128 on CPU and AES-NI.

Show All

SECTION 6Discussions and Future Works
As mentioned in Section 5.1, the desktop GPU that we tested contains 2,304 CUDA cores compared with the 96 CUDA cores on the laptop GPU. It is easy to conclude that the execution speed of the desktop GPU is much faster than the laptop GPU which is common for different GPGPUs. As a consequence, SE Speed is very different for the two scenarios. This very difference did occur on respectively two typical hardware cases: low-end GPUs normally for laptops and high-end GPUs for desktops or gaming PCs. In fact, as long as GPU architectures are rapidly evolving, the hardware configuration of a GPU strongly influence software applications’ architectural choices, which are necessary to derive the best possible implementation [17]. This point could even invert the results of our evaluation since a large number of cores could very well favor the GPU calculation tasks that finally change the overlay design.

Our method is also tested with a more recently released GPGPU: Nvidia GeForce gtx 1080 (released in late 2017). The main calculation tasks in this paper have been tested on this GPGPU and the comparison with the Nvidia GeForce gtx 780 is shown in Table 6. The GeForce gtx 1,080 is about 6 times faster than the GeForce gtx 780 to process SHA-256 but 10 times faster to process SHA-512. Therefore, the overlay design in Fig. 6 will not work as the execution speeds of GPGPUs are increasing many times faster while the AES-128 on CPUs with traditional implementation cannot be overlapped by the execution of the GPGPU tasks. There are two further choices which are either deploying AES-NI for the AES processor or using GPGPU to calculate all tasks including the AES-128. If we assume the AES-NI is deployed and the overlapping design still works, the desktop equipped with GeForce gtx 1080 will theoretically achieve a speed of more than 8 GB/s which is three times faster than the results we got in this paper with GeForce gtx 780. As it is only fair to say that the gain in performance can be larger by simply employing the state-of-the-art GPGPU, and such rapid evolving of hardware is not seen in the recent years’ CPU manufacturing. Based on this fact, the rapid increase of GPGPUs calculation speed will our proposed method achieve better performance compared with AES-NI. However, as pointed by [47], when a GPU calculation capacity is very high, the bottleneck will be the memory transfer speed between the GPGPU memory and the host memory. This is mainly due to the limitation of transmission speed through the PCIe bus [48] between host memory and GPU memory. This problem may be avoided by arranging more calculation tasks on the GPU, instead of too much I/O usage or overlapping the transmission by execution time. Such solutions to increase the final execution speed of our method will be discussed in future work.

TABLE 6 Hash and DWT Speeds on Two Desktop GPGPUs

As this paper considers the basic scenario as the computer platform, the energy consumption is not tested. In fact, for the desktop platform with a powerful GPGPU, the initial design of such GPGPU is to consume more energy than CPU for better performance. If the method is deployed on mobile devices in future work, the energy consumption will be considered [49]. In fact, there are previous works trying to accelerate the transformation algorithms on smartphones. In [50], the authors showed that by implementing Fourier transform on a smartphone, the execution speed is faster with less energy consumed. In [51], wavelet transformation is implemented on a mobile phone platform and the acceleration is proved. Thus, deploying the concept of our method on a smartphone platform is definitely promising as one of our future works. Moreover, for the fragmentation methods, we used the traditional way based on the viewpoint of energy and visual sensitivity to select the low frequency coefficients as the private fragment which should be further discussed [52]. More practical deployment of our proposed method with use cases will also be considered in future work [53].

SECTION 7Conclusions
In this paper, we proposed a solution for end users to exploit the usage of cheap Cloud storage services while keeping their data safe. Our method can be applied on many different data formats which significantly improved the concept of selective encryption by introducing fragmentation and dispersion methods. The experimental and theoretical results have verified that our method can provide a high level of protection with resistance against propagation errors. We also provided a fast runtime on different PC platforms with practical designs and implementations based on GPGPU acceleration. In summary, we proposed a secure and efficient data protection method for end users to securely store the data on Clouds.