Cloud storage has been in widespread use nowadays, which alleviates users' burden of local data storage. Meanwhile, how to ensure the security and integrity of the outsourced data stored in a cloud storage server has also attracted enormous attention from researchers. Proofs of storage (POS) is the main technique introduced to address this problem. Publicly verifiable POS allowing a third party to verify the data integrity on behalf of the data owner significantly improves the scalability of cloud service. However, most of existing publicly verifiable POS schemes are extremely slow to compute authentication tags for all data blocks due to many expensive group exponentiation operations, even much slower than typical network uploading speed, and thus it becomes the bottleneck of the setup phase of the POS scheme. In this article, we propose a new variant formulation called “Delegatable Proofs of Storage (DPOS)”. Then, we construct a lightweight privacy-preserving DPOS scheme, which on one side is as efficient as private POS schemes, and on the other side can support third party auditor and can switch auditors at anytime, close to the functionalities of publicly verifiable POS schemes. Compared to traditional publicly verifiable POS schemes, we speed up the tag generation process by at least several hundred times, without sacrificing efficiency in any other aspect. In addition, we extend our scheme to support fully dynamic operations with high efficiency, reducing the computation of any data update to O(log n) and simultaneously only requiring constant communication costs. We prove that our scheme is sound and privacy preserving against auditor in the standard model. Experimental results verify the efficient performance of our scheme.
SECTION 1Introduction
Cloud computing has been widely accepted and deployed in our daily life due to the great benefits that it brings about such as decreasing infrastructure costs, providing high scalability and availability. More and more people rely on cloud storage services to reduce their local storage burden. Namely, data is outsourced to the cloud server and can be accessed on demand later. Meanwhile, how to ensure the security and integrity of the outsourced data without keeping a local copy for data owners is an imperative concern to address. One of the main solutions is to apply proofs of storage (POS) that is also referred to proofs of retrievability (POR) [1] or proofs of data possession (PDP) [2], in which the integrity of data stored in cloud server can be verified without having to download all the data. The basic idea is dividing the whole data file into multiple blocks, each of which is used to generate a homomorphic verifiable tag (HVT) sent to the cloud server together with the data file. Later, the verifier selects a set of data blocks rather than the whole file to audit the outsourced data from the cloud server (prover) with the help of those HVTs, which can significantly reduce the communication overheads.

Since the first POR and PDP schemes are presented in 2007, there have been lots of efforts devoted to constructing proofs of storage schemes with more advanced features such as public key verifiability [3], data dynamics [4], [5] (i.e., modifying/inserting/deleting data blocks), multiple cloud servers [6] and data sharing [7]. We focus on the first two features—public verifiability and support of data dynamics. As to the former, we observe that most of existing publicly verifiable POS schemes employ expensive operations (e.g., group exponentiation) to generate HVTs for data blocks. Consequently, it is prohibitively expensive to generate HVTs for medium or large-size data files. For instance, one of the most popular POS schemes, proposed by Wang et al. [8], achieves throughput of data pre-processing at speed 17.2 KB/s with an Intel Core 2 1.86 GHz workstation CPU, which means it will take about 17 hours to generate HVTs for a 1 GB file. Even if the user has a CPU with 8 cores, it still requires more than 2 hours’ heavy computation. Such amount of heavy computation is not appropriate for a laptop, not to mention tablet computer or smart phone.

Public verifiability of POS enables any third party to verify the integrity of data in cloud storage, which significantly eliminates the burden from data owner. Nevertheless, in practice, it is not desirable to allow anyone to audit the data at their will, and instead, delegation of the auditing task has to be in a controlled and organized manner. Otherwise, the following two extreme cases may happen: (1) some data files could attract too much attention from public, and are audited unnecessarily too frequently by the public, which might actually result in distributed denial of service attack against the cloud storage server; (2) on the contrary, some unpopular data files may be audited by the public too rarely, so that the possible data loss event might be detected and alerted to the data owner too late and no effective countermeasure can be done to reduce the damage. Instead, the data owner could delegate the auditing task to some semi-trusted third party auditor, and this auditor is fully responsible to audit the data stored in cloud storage on behalf of the data owner, in a controlled way, with proper frequency. We call such an exclusive auditor as Owner-Delegated-Auditor or ODA for short. In real world applications, ODA could be another server that provides free or paid auditing service to many cloud users.

The second feature we take into consideration in a POS scheme is supporting dynamic operations, in which data owners may request to modify, insert, or delete data blocks after outsourcing its original data to a cloud server. This is a desired property when designing new POS schemes. Upon generating an HVT, the block index information i has to be part of the inputs. This is to prevent a cloud server using the same HVT for different blocks while still passing the verification. As a consequence, if a new block m∗ is inserted after the ith block mi, then the indices of all the following blocks after mi have to be changed accordingly and all the corresponding HVTs need to be recomputed, which is impractical if the number of blocks is huge (for a 1 GB file, if we set one data block be 4 KB, then the number of data block is about 218≈ 1 million). To avoid this problem, we can manage the indices instead of the HVTs upon data updating. There have been a number of researchers working on techniques supporting dynamics for POS schemes, which results into two main classes of solutions: index table-based and tree-based methods. The former employs an index table to manage the block indices which significantly reduces the communication cost but taking O(n) computation for each data update, while the latter one utilizes tree-based structures such as the Merkle Hash Tree and the rank-based authenticated skip list that need only O(logn) computation but brings in extra O(logn) communication overhead for block auditing, where n is the number of data blocks.

To address the issues of existing publicly verifiable POS schemes, we propose a new variant formulation called Delegatable Proofs of Storage (DPOS) [9], which on one hand supports delegation of data auditing task, like publicly verifiable POS schemes, and on the other hand is as efficient as a privately verifiable POS scheme. As an extension of our conference paper [9], in this article, we design a new approach to enabling fully dynamic operations which include block modification, block insertion, and block deletion. The proposed method reduces the computation of a data update to O(logn) and simultaneously only constant communication costs are required. The extended scheme also provides privacy-preserving property for the outsourced data against ODA. In addition, we implement our protocol and the experimental results show that the proposed POS scheme is indeed highly efficient especially for the tag generation process.

1.1 Overview of Our Scheme
Our scheme generates two pairs of public/private keys: (pk,sk) and (vpk,vsk). The verification public/private key pair (vpk,vsk) is delegated to the ODA and will be updated (or re-randomized) once an ODA is revoked and a new ODA is chosen. The master public/private key pair (pk,sk) will always keep unchanged. We propose a novel linear homomorphic authentication tag function [10], which is extremely lightweight, without any expensive operations (e.g., group exponentiation or bilinear map). The tag function generates two tags (σi,ti) for each data block, where tag σi is generated in a similar way as Shacham and Waters’ scheme [3], and tag ti is generated in a completely new way. The size of all tags {(σi,ti)} is 2/m-fraction of the whole file size, where system parameter m can take any positive integer value and typical value is from a hundred to a thousand. ODA is able to verify data integrity remotely by checking consistency among the data blocks and both tags {(σi,ti)} that are stored in the cloud storage server, using the verification secret key vsk. The data owner retains the capability to verify data integrity by checking consistency between the data blocks and tags {σi}, using the master secret key sk, in a way similar to Shacham and Waters’ scheme [3]. When an ODA is revoked and replaced by a new ODA, all authentication tags {ti} will be updated (or re-randomized) together with the verification key pair (vpk,vsk) without having to download the data blocks, and tags {σi} will keep unchanged.

We combine our new linear homomorphic authentication tag function with existing techniques, in order to reduce communication cost and achieve privacy protection. We customize the polynomial commitment scheme proposed by Kate et al. [11] and integrate it into our scheme, in order to reduce proof size from O(m) to O(1). We also customize the “generalized Okamoto identification scheme” [12], [13] and integrate it in our scheme to prevent information leakage to the ODA during the auditing process.

We emphasize that: (1) The naive method that runs an existing private key POS scheme on an input file twice to generate two key pairs and two authentication tags, is unsatisfactory. This naive method does not support efficient updating of verification key (i.e., switching auditors), and the data owner has to download the whole data file to refresh the verification key pair, for every time that an auditor is revoked. (2) If the data file is encrypted using some semantic-secure cipher, and the POS scheme is applied over the ciphertext, then the privacy-preserving feature of the POS is not necessary. However, in some application scenario, the semi-trusted cloud server has to access the data file, in order to provide more kinds of services (e.g., analyzing/querying the data) rather than pure storage backup service. It is not storage efficient to require cloud server to keep both ciphertext copy and plaintext copy, where the ciphertext copy is audited by third party auditor and the plaintext copy is used to provide other services.

1.2 Contributions
Our contributions can be summarized as below:

TABLE 1 Performance Comparison of Proofs of Storage (PORPOR/PDPPDP) Schemes

We propose a new formulation called DPOS, as a relaxed version of public key POS. We design a new scheme under this formulation. Our scheme is as efficient as private key POS: the tag generation throughput is about 25 MB/s in our experiment. On the other hand, our scheme allows delegation of auditing task to a semi-trusted third party auditor, and also supports switching auditor at any time, like a publicly verifiable POS scheme. In addition, our scheme provides the data privacy against the third party auditor. We compare the performance complexity of our scheme with the state of arts in Table 1.

We extend our scheme to support fully dynamic operations by designing a new tree-based method. The proposed scheme reduces the computation of a data update to O(logn) while at the same time only needs constant communication costs for block verification.

We prove that our scheme is sound under Bilinear Strong Diffie-Hellman Assumption, and privacy preserving under Discrete Log Assumption, both in the standard model.

We implement our protocol together with several selected schemes and the experimental results verify the better performance of the proposed scheme over previous publicly verifiable POS schemes. In particular, the tag generation speed of our scheme is more than hundreds of times faster than the state of art of publicly verifiable POS schemes.

1.3 Organization
The rest of the paper is organized as follows. Section 2 revisits the related work. Section 3 describes our new formulation denoted as DPOS, as well as the system model and trust model. The concrete schemes are given in Section 4, while the corresponding security proof is demonstrated in Section 5. The performance analysis is shown in Section 6. Finally Section 7 makes the conclusion of this paper.

SECTION 2Related Work
There have been a large number of papers looking into auditing the integrity of data stored at untrusted servers since last decade [1], [2], [3], [4], [5], [6], [7], [8], [10], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41]. In 2007, Ateniese et al. [2] presented the PDP model and proposed the first publicly verifiable PDP scheme which employed RSA-based homomorphic authenticators and sampled a number of data blocks rather than the whole data file to audit the outsourced data. The sampling method significantly reduces the communication complexity. However, their scheme exposed a linear combination of sampled blocks to the third party auditor (TPA) at each auditing, which may leak the data information to TPA. At the meantime, Juels and Kaliski [1] described a similar but stronger model: POR, which enables auditing of not only the integrity but also the retrievability of remote data files by employing spot-checking and error-correcting codes. They proposed a concrete POR scheme which supports a bounded number of auditing services and does not support public verification.

In 2008, Shacham and Waters [3], [15] proposed a publicly verifiable POR scheme with a comprehensive proof of security under the POR model [1]. Similar to [2], their scheme utilized homomorphic authenticators built from BLS signatures [42]. Subsequently, Zeng et al. [36], Wang et al. [16], [17] gave similar constructions for publicly verifiable remote data integrity check, which adopted the BLS based homomorphic authenticators. With the same reason as [2], these protocols do not support data privacy. In [8], [18], Wang et al. designed a privacy-preserving POR scheme. The idea is to mask the linear combination of sampled blocks in the server's response with some random value. With the similar masking technique, Zhu et al. [19] introduced another privacy-preserving public auditing scheme. Later, Hao et al. [20] and Yang et al. [21] proposed two privacy-preserving public auditing schemes without applying the masking technique. Yuan et al. [22] gave a POR scheme with public verifiability and constant communication cost.

However, all of these publicly verifiable PDP/POR protocols require the data owner to do a large amount of computation of exponentiation on big numbers for generating the authentication tags upon preprocessing the data file. This makes these schemes impractical for file of large size, especially limiting the usage on mobile device.

Similar to our work, [43] constructed a publicly verifiable POR scheme from privately verifiable POR by employing indistinguishability obfuscation technique [44]. However, indistinguishability obfuscation is currently impractical yet to be deployed. In particular, Apon et al. [45] implemented the scheme of [44] and showed that, obfuscating a simple function containing 15 AND gates required about nine hours, and the obfuscated program had size 31.1 GB.

The concept of data dynamics for POS was first introduced by Ateniese et al. [28]. Their scheme supports block updates but not insertions. Erway et al. [4] proposed a fully dynamic scheme with a skip list structure. Simultaneously, Wang et al. [16] constructed a Merkle Hash Tree based structure for enabling fully dynamic POS. Later, Zhu et al. [19] presented an index-hash table based fully dynamic POS and Yang et al. [21] and Yan et al. [23] presented schemes with similar structure. Zhang et al. [37] proposed a block update tree structure where each tree node is associated with a range of data blocks rather than one block. In applications that users access/update files in sequentially, their scheme becomes more efficient and requires less complexity and communication overheads compared with other tree-based schemes. Cash et al. [41] employed oblivious RAM techniques to achieve full dynamic POR. Shi et al. [38] proposed a practical dynamic POR scheme with constant client storage. Very recently, Tian et al. [39] proposed to support full dynamics based on dynamic hash table which is essentially a single linked sequence table, while Shen et al. [40] proposed a new dynamic structure consisting of a doubly linked info table and a location array.

SECTION 3Formulation
We propose a formulation called DPOS, based on existing POR and PDP formulations. We provide the system model in Section 3.1, the trust model in Section 3.2 and the security definitions in Section 3.3.

3.1 System Model
Definition 1.
A Delegatable Proofs of Storage (DPOS) scheme consists of three algorithms (KeyGen, Tag, UpdVK), and a pair of interactive algorithms ⟨P,V⟩, where each algorithm is described as below

KeyGen(1λ)→(pk,sk,vpk,vsk): Given a security parameter 1λ, this randomized key generating algorithm generates a pair of public/private master keys (pk,sk) and a pair of public/private verification keys (vpk,vsk).

Tag(sk,vsk,F)→(ParamF,{(σi,ti)}): Given the master secret key sk, the verification secret key vsk, and a data file F as input, the tagging algorithm generates a file parameter ParamF and authentication tags {(σi,ti)}, where a unique file identifier idF is a part of ParamF.

UpdVK(vpk,vsk,{ti})→(vpk′,vsk′,{t′i}): Given the current verification key pair (vpk,vsk) and the current authentication tags {ti}, this updating algorithm generates the new verification key pair (vpk′,vsk′) and the new authentication tags {t′i}.

⟨P(pk,vpk,{F⃗ i}),V(vsk,vpk,pk,ParamF)⟩→Accept or Reject: The prover algorithm P interacts with the verifier algorithm V to output a decision bit Accept or Reject, where the input of P consists of the master public key pk, the verification public key vpk, and file blocks {F⃗ i}, and the input of V consists of the verification secret key vsk, verification public key vpk, master public key pk, and file information ParamF.

A DPOS system is described as below and illustrated in Fig 1a and Fig 1b.


Fig. 1.
Illustration of system model of DPOS.

Show All

Definition 2.
A DPOS system among three parties—data owner, cloud storage server and auditor, can be implemented by running a DPOS scheme (KeyGen, Tag, UpdVK,⟨P,V⟩) in the following three phases, where the setup phase will execute at the very beginning, for only once (for one file); the proof phase and revoke phase can execute for multiple times and in any (interleaved) order.

Setup Phase. The data owner runs the key generating algorithm KeyGen(1λ) for only once, to generate the master key pair (pk,sk) and the verification key pair (vpk,vsk). For every input data file, the data owner may choose to apply some error erasure code [46] on this file, and runs the tagging algorithm Tag over the (erasure encoded) file, to generate authentication tags {(σi,ti)} and file parameter ParamF. At the end of setup phase, the data owner sends the (erasure encoded) file F, all authentication tags {(σi,ti)}, file parameter ParamF, and public keys (pk,vpk) to the cloud storage server. The data owner also chooses an exclusive third party auditor, called Owner-Delegated-Auditor (ODA, for short), and delegates the verification key pair (vpk,vsk) and file parameter ParamF to the ODA. After that, the data owner may keep only keys (pk,sk,vpk,vsk) and file parameter ParamF in local storage, and delete everything else from local storage.

Proof Phase. The proof phase consists of multiple proof sessions. In each proof session, the ODA, who runs algorithm V, interacts with the cloud storage server, who runs algorithm P, to audit the integrity of data owner's file, on behalf of the data owner. Therefore, ODA is also called verifier and cloud storage server is also called prover.

Revoke Phase. In the revoke phase, the data owner downloads all tags {ti} from cloud storage server, revokes the current verification key pair, and generates a fresh verification key pair and new tags {t′i}. The data owner also chooses a new ODA, and delegates the new verification key pair to this new ODA, and sends the updated tags {t′i} to the cloud storage server to replace the old tags {ti}.

A DPOS scheme (KeyGen, Tag, UpdVK, ⟨P,V⟩) is complete, if for all keys (pk,sk, vpk,vsk) output by KeyGen, for all files F, if all parties follow our scheme exactly and the data stored in cloud storage is intact, then interactive proof algorithms ⟨P,V⟩ will always output Accept.

3.2 Trust Model
In this paper, we aim to protect data integrity and privacy of data owner's file. The data owner is fully trusted, and the cloud storage server and ODA are semi-trusted in different sense: (1) The cloud storage server is trusted in data privacy (We assume the server has to access plaintext to provide additional services to the data owner), and is not trusted in maintaining data integrity (e.g., the server might delete some rarely accessed data for economic benefits, or hide the data corruption events caused by server failures or attacks to maintain reputation). (2) Before he/she is revoked, the ODA is trusted in performing the delegated auditing task and protecting his/her verification secret key securely, but is not trusted in data privacy. A revoked ODA could be potentially malicious and might surrender his/her verification secret key to the cloud storage server.

We assume that all communication among the data owner, the cloud storage server and ODA is via some secure channel (i.e., channel privacy and integrity are protected). Framing attack among these three parties can be dealt with existing technique and is out of scope of this paper.

3.3 Security Definitions
3.3.1 Soundness
Based on the existing Provable Data Possession formulation [2] and Proofs of Retrievability formulation [1], [3]. The DPOS soundness security game Gamesound between a probabilistic polynomial time (PPT) adversary A (i.e., dishonest prover/cloud storage server) and a PPT challenger C w.r.t. a DPOS scheme E=(KeyGen, Tag, UpdVK, ⟨P,V⟩) is as below.

Setup. The challenger C runs the key generating algorithm KeyGen(1λ) to obtain two pair of public-private keys (pk,sk) and (vpk,vsk). The challenger C gives the public key (pk,vpk) to the adversary A and keeps the private key (sk,vsk) securely.

Learning. The adversary A adaptively makes polynomially many queries, where each query is one of the following:

Store-Query(F): Given a data file F chosen by A, the challenger C runs tagging algorithm (ParamF,{(σi,ti)})←Tag(sk,vsk,F), where ParamF=(idF,n), and sends the data file F, authentication tags {(σi,ti)}, public keys (pk,vpk), and file parameter ParamF, to A.

Verify-Query(idF): Given a file identifier idF chosen by A, if idF is not the (partial) output of some previous Store-Query that A has made, ignore this query. Otherwise, the challenger C initiates a proof session with A w.r.t. the data file F associated to the identifier idF in this way: The challenger C, who runs the verifier algorithm V(vsk,vpk,pk,ParamF), interacts with the adversary A, who replaces the prover algorithm P with any PPT algorithm of its choice, and obtains an output b∈{Accept,Reject}. The challenger sends the decision bit b to the adversary as feedback.

RevokeVK-Query: To respond to this query, the challenger runs the verification key update algorithm to obtain a new pair of verification keys (vpk′,vsk′, {t′i}) :=UpdVK(vpk,vsk,{ti}), and sends the revoked verification secret key vsk and the new verification public key vpk′ and new authentication tags {t′i} to the adversary A, and keeps vsk′ private.

Commit. Adversary \mathcal {A} chooses a file identifier \mathtt {id}^{*} among all file identifiers it obtains from \mathcal {C} by making Store-Queries in Learning phase. Let {\mathtt {F}}^{*} denote the data file associated to identifier \mathtt {id}^{*}. \mathcal {A} also chooses a subset \mathbf {C} \subset [0, n_{F^{*}}-1], where n_{F^{*}} is the number of blocks in file F^{*}. \mathcal {A} commits identifier \mathtt {id}_F^{*} and subset \mathbf {C} of indices to \mathcal {C}.

Retrieve. The challenger \mathcal {C} initiates polynomially many proof sessions with \mathcal {A} w.r.t. the data file {\mathtt {F}}^{*} and subset \mathbf {C}, where challenger \mathcal {C} plays the role of verifier and \mathcal {A} plays the role of prover, as in the Learning phase. Let \mathtt {transcript}_{\mathcal {A}} denote all random coins chosen by the adversary \mathcal {A}, and \mathtt {chall-response} denote all pairs of challenges and responses between the challenger and the adversary \mathcal {A}, during these proof sessions. The challenger \mathcal {C} extracts file blocks \lbrace \mathtt {F}_i^{\prime }: i \in \mathbf {C} \rbrace from \mathcal {A}'s storage by applying some \mathsf {PPT} knowledge extractor on input (\mathtt {transcript}_{\mathcal {A}},\ \mathtt {chall-response}, pk, sk, vpk, vsk).

The adversary \mathcal {A} wins this \mathsf {DPOS} soundness security game, if the challenger \mathcal {C} accepts \mathcal {A}'s responses in these proof sessions with some noticeable probability 1/\lambda ^{\tau } for some positive integer \tau. The challenger \mathcal {C} wins this game, if the extracted blocks \lbrace (i, \mathtt {F}_i^{\prime }): i \in \mathbf {C} \rbrace are identical to the original \lbrace (i, \mathtt {F}_i): i \in \mathbf {C} \rbrace.

Note: Events “adversary \mathcal {A} wins” and “challenger \mathcal {C} wins” are not mutual exclusive.

Definition 3 (Soundness).
A \mathsf {DPOS} scheme is sound against dishonest cloud storage server, if for any \mathsf {PPT} adversary \mathcal {A}, the probability that \mathcal {A} wins the above \mathsf {DPOS} security game is negligibly close to the probability that \mathcal {C} wins the same security game. That is \begin{equation*} \mathsf {Pr}[ \mathcal {A} \text{ wins } \mathsf {Game}_{\tt sound}] \leq \mathsf {Pr}\;[ \mathcal {C} \text{ wins } \mathsf {Game}_{\tt sound}] + negl(\lambda), \end{equation*}
View Sourcewhere \lambda is the security parameter.

In case of \mathsf {POR} formulation, the knowledge extractor does not require \mathtt {transcript}_{\mathcal {A}} as input.

3.3.2 Privacy-Preserving Against TPA
The \mathsf {DPOS} privacy security game \mathsf {Game}_{\tt private} between a \mathsf {PPT} adversary \mathcal {A} (i.e., dishonest verifier/auditor) and a \mathsf {PPT} challenger \mathcal {C} w.r.t. a \mathsf {DPOS} scheme \mathcal {E} = (\mathsf {KeyGen}, \mathsf {Tag}, \mathsf {UpdVK}, \langle \mathsf {P}, \mathsf {V} \rangle) is as below.

Setup. The challenger \mathcal {C} runs the key generating algorithm \mathsf {KeyGen}(1^{\lambda }) to obtain two pair of public-private keys (pk, sk) and (vpk, vsk), gives the public key (pk, vpk) and verification secret key vsk to the adversary \mathcal {A} and keeps the secret key sk private. It then chooses a random file F^{*} with bit-length \geq m \lambda, and computes (\mathtt {Param}^{*}, \lbrace (\sigma _i, t_i) \rbrace) \leftarrow \mathsf {Tag}(sk, vsk, \mathtt {F}^{*}), where \mathtt {Param}^{*} = (\mathtt {id}^{*}, n^{*}).

Learning. The adversary \mathcal {A} adaptively makes polynomially many queries, where each query is one of the following:

Store-Query(\mathtt {F}): the same with \mathsf {Game}_{\tt sound}.

Verify-Query(\mathtt {id}_F): Given a file identifier \mathtt {id}_F chosen by \mathcal {A}, if \mathtt {id}_F \ne \mathtt {id}^{*} and it is not the (partial) output of some previous Store-Query that \mathcal {A} has made, ignore this query. Otherwise, the adversary \mathcal {A} initiates a proof session with the challenger \mathcal {C} w.r.t. the data file {\mathtt {F}} associated to the identifier \mathtt {id}_F in this way: The adversary \mathcal {A}, who replaces the verifier algorithm \mathsf {V}(vsk, vpk, pk, \mathtt {Param}_F) with any \mathsf {PPT} algorithm of its choice, interacts with the challenger \mathcal {C}, who runs the prover algorithm \mathsf {P}(pk, vpk, \lbrace \vec{F}_i \rbrace), and obtains an output b \in \lbrace {\mathtt {Accept}}, {\mathtt {Reject}}\rbrace.

RevokeVK-Query: To respond to this query, the challenger runs the verification key update algorithm to obtain a new pair of verification keys and update authentication tags t_i's for all files: (vpk^{\prime }, vsk^{\prime }, \lbrace t_i^{\prime } \rbrace) := \mathsf {UpdVK}(vpk, vsk, \lbrace t_i \rbrace), and sends the all verification keys (vpk, vsk, vpk^{\prime }, vsk^{\prime }) to the adversary \mathcal {A}.

Guess. The adversary outputs a tuple (i, j, B), wins this game if B = \vec{F}_{i}^{*}[j], i.e., B equals to the jth dimension of the ith data block \vec{F}^{*}_{i}.

Definition 4 (Privacy-Preserving).
A \mathsf {DPOS} scheme is privacy-preserving against TPA, if for any \mathsf {PPT} adversary \mathcal {A} (dishonest verifier/auditor), \mathcal {A} wins the above privacy security game \mathtt {Game}_{\tt private} with only negligible probability. That is, \begin{equation*} \mathsf {Pr}\;[\mathcal {A} \text{ wins } \mathtt {Game}_{\tt private} ] \leq negl(\lambda), \end{equation*}
View Sourcewhere \lambda is the security parameter.

SECTION 4Our Proposed Scheme
We first briefly introduce the preliminaries that will be used to construct the proposed scheme. Then we present the basic DPOS scheme that only supports static data integrity auditing. Finally, we extend it to a fully dynamic one.

4.1 Preliminaries
Let \mathbb {G} and \mathbb {G}_T be two multiplicative cyclic groups of prime order p. Let g be a randomly chosen generator of group \mathbb {G}. A bilinear map is a map e:\mathbb {G} \times \mathbb {G} \rightarrow \mathbb {G}_T with the following properties:

Bilinearity: e(u^a,v^b)=e(u,v)^{ab} for all u,v \in \mathbb {G} and a,b\in \mathbb {Z}_p.

Non-degeneracy: If g is a generator of \mathbb {G}, then e(g,g) is a generator of \mathbb {G}_T, i.e., e(g,g)\ne 1.

Computable: There exists an efficient algorithm to compute e(u,v) for all u,v \in \mathbb {G}.

In the rest of this paper, the term “bilinear map” will refer to the non-degenerate and efficiently computable bilinear map only.

For vector \vec{a} = (a_1, \ldots, a_m) and \vec{b} = (b_1, \ldots, b_m), the notation \langle \vec{a}, \ \vec{b} \rangle \;{{\stackrel{{{\mathrm{def}}}}{=}}} \;\sum \limits _{j=1}^{m} a_j b_j denotes the dot product (a.k.a inner product) of the two vectors \vec{a} and \vec{b}. For vector \vec{v}= (v_0, \ldots, v_{m-1}) the notation {\mathsf {Poly}}_{\vec{v}}(x) \;{{\stackrel{{{\mathrm{def}}}}{=}}}\; \sum \limits _{j=0}^{m-1} v_j x^j denotes the polynomial in variable x with \vec{v} being the coefficient vector.

4.2 The Basic \mathsf {DPOS} Scheme
We define our DPOS scheme (\mathsf {KeyGen}, \mathsf {Tag}, \mathsf {UpdVK}, \langle \mathsf {P}, \mathsf {V} \rangle) as below, and these algorithms will run in the way as specified in Definition 2 (on page 4).

{\mathsf {KeyGen}(1^\lambda)\rightarrow (pk,sk, vpk, vsk)}

Choose at random a \lambda-bits prime p and a bilinear map e: \mathbb {G} \times \mathbb {G} \rightarrow \mathbb {G}_T, where \mathbb {G} and \mathbb {G}_T are both multiplicative cyclic groups of prime order p. Choose at random a generator g \in \mathbb {G}. Choose at random \alpha, \beta, \gamma, \rho \in _R \mathbb {Z}_p^{*}. For each j \in [1,m], define \alpha _j := \alpha ^j \;{\rm mod}\;{p} and \beta _j := \beta ^{j} \;{\rm mod}\;{p}, and compute g_j := g^{\alpha _j},\ h_j := g^{\rho \cdot \beta _j}. Let g_0 = g^{\alpha ^{0}} = g, h_0 = g^{\rho \cdot \beta ^{0}} = g^{\rho }, vector \vec{\alpha } := (\alpha _1, \alpha _2, \ldots, \alpha _m), and \vec{\beta } := (\beta _1, \beta _2, \ldots, \beta _m), Choose two random seeds s_0, s_1 for pseudorandom function \mathcal {PRF}_{\tt seed}: \lbrace 0,1\rbrace ^{\lambda } \times \lbrace 0,1\rbrace ^{\mathbb {N}} \rightarrow \mathbb {Z}_p.

The secret key is sk=(\alpha, \beta, s_0) and the public key is pk=(g_0, g_1, \ldots, g_m). The verification secret key is vsk= (\rho, \gamma, s_1) and the verification public key is vpk = (h_0, h_1, \ldots, h_m).

{\mathsf {Tag}(sk, vsk, F) \rightarrow (\mathtt {Param}_F,\lbrace (\sigma _i, t_i) \rbrace)}

Split file1 F into n blocks, where each block is a vector of m elements from \mathbb {Z}_p: \lbrace \vec{F}_i=(F_{i,0},\ldots, F_{i,m-1}) \in \mathbb {Z}_p^m \rbrace _{i \in [0, n-1]}. Choose a unique identifier \mathtt {id}_F \in \lbrace 0,1\rbrace ^\lambda. Define a customized2 pseudorandom function w.r.t. the file F: {{\mathsf {PRF}}}_{s}(i) = \mathcal {PRF}_{s}(\mathtt {id}_F, i).

For each block \vec{F_i}, 0 \leq i\leq n-1, compute \begin{align*} \sigma _i =\;& \left\langle \vec{\alpha }, \ \vec{F_i} \right\rangle + {{\mathsf {PRF}}}_{s_0}(i) = \alpha \cdot {\mathsf {Poly}}_{\vec{F_i}}(\alpha) + {{\mathsf {PRF}}}_{s_0}(i) \;{\rm mod}\;{p};\\ t_i =\;& \rho \left\langle \vec{\beta }, \ \vec{F_i} \right\rangle + \gamma {{\mathsf {PRF}}}_{s_0}(i) + {{\mathsf {PRF}}}_{s_1}(i) \;{\rm mod}\;{p}\\ =\;& \rho \cdot \beta {\mathsf {Poly}}_{\vec{F_i}}(\beta) + \gamma {{\mathsf {PRF}}}_{s_0}(i) + {{\mathsf {PRF}}}_{s_1}(i) \;{\rm mod}\;{p}. \tag{1}\end{align*}
View SourceThe general information of F is \mathtt {Param}_F := (\mathtt {id}_F, n). \mathsf {HVT}_i=(\sigma _i, t_i) is the homomorphic verifiable tag for \vec{F_i}.

{\mathsf {UpdVK}(vpk, vsk, \lbrace t_i\rbrace) \rightarrow (vpk^{\prime }, vsk^{\prime }, \lbrace t_i^{\prime }\rbrace)}

Parse vpk as (h_0, \ldots, h_m) and vsk as (\rho, \gamma, s_1). Verify the integrity of all tags \lbrace t_i \rbrace where i \in [0,n-1] (We will discuss how to do this verification later), and abort if the verification fails. Choose at random \gamma ^{\prime } \in _R \mathbb {Z}_p^{*} and choose a random seed s_1^{\prime } for pseudorandom function {{\mathsf {PRF}}}. For each j \in [0,m], compute h_j^{\prime } := h_j^{\gamma ^{\prime }} = g^{\left(\rho \cdot \gamma ^{\prime } \right) \cdot \beta _j} \ \in \mathbb {G}. For each i \in [0, n-1], compute a new authentication tag \begin{align*} t_i^{\prime } :=\;& \gamma ^{\prime } \left(t_i - {{\mathsf {PRF}}}_{s_1}(i) \right) + {{\mathsf {PRF}}}_{s_1^{\prime }}(i) \;{\rm mod}\;{p}\\ =\;& \gamma ^{\prime } \cdot \rho \left\langle \vec{\beta }, \ \vec{F_i} \right\rangle + \left(\gamma ^{\prime } \cdot \gamma \right) {{\mathsf {PRF}}}_{s_0}(i) + {{\mathsf {PRF}}}_{s_1^{\prime }}(i) \;{\rm mod}\;{p}. \end{align*}
View SourceRight-click on figure for MathML and additional features.

The new verification public key is vpk^{\prime } := (h_0^{\prime }, \ldots, h_m^{\prime }) and the new verification secret key is vsk^{\prime } := (\gamma ^{\prime } \cdot \rho, \ \gamma ^{\prime } \cdot \gamma, \ s_1^{\prime }).

\langle \mathsf {P}(pk, vpk, \lbrace \vec{F}_i \rbrace _{i \in [0, n-1]}), \mathsf {V}(vsk, vpk, pk, \mathtt {Param}_F) \rangle

Verifier parses \mathtt {Param}_F as (\mathtt {id}_F, n), and sends the file identifier \mathtt {id}_F to the prover to initiate a proof session.

Prover locates the file F corresponding to \mathtt {id}_F and will proceed on the following procedures based on file F and its associated tags \lbrace \sigma _i, t_i \rbrace. The prover chooses at random a vector \vec{y} = (y_1, \ldots, y_m) \in _R \mathbb {Z}_p^{m} and two elements y_{\sigma }, y_{t} \in _R \mathbb {Z}_p, and computes (Y_{\alpha }, Y_{\beta }, Y_{\sigma }, Y_{t}) \in \mathbb {G}^{4} as below \begin{align*} &Y_{\alpha } := \prod _{j=1}^{m} g_j^{y_j} = g^{\alpha {\mathsf {Poly}}_{\vec{y}}(\alpha)};\\ &Y_{\beta } := \prod _{j=1}^{m} h_j^{y_j} = g^{\rho \cdot \beta {\mathsf {Poly}}_{\vec{y}}(\beta)};\\ &Y_{\sigma } := g^{y_{\sigma }}; \quad Y_{t} := g^{y_{t}}. \end{align*}
View SourceProver sends (Y_{\alpha }, Y_{\beta }, Y_{\sigma }, Y_{t}) to the verifier, in order to commit the secret values (\vec{y}, y_{\sigma }, y_{t}).

Verifier chooses at random r, r_{\sigma }, r_{t}, \xi \in _R \mathbb {Z}_{p}^{*}, and a random subset \mathbf {C} \subset [0, n-1] of size \ell. For each i \in \mathbf {C}, choose at random a weight w_i \in _R \mathbb {Z}_p. Verifier sends (r, r_{\sigma }, r_{t}, \xi, \lbrace (i, w_i): i \in \mathbf {C} \rbrace) to the prover.

Prover computes \vec{\bar{F}} \in \mathbb {Z}_p^{m}, and \bar{\sigma }, \bar{t} \in \mathbb {Z}_p as below, where the random numbers (\vec{y}, y_{\sigma }, y_{t}) are used to blind secret information and prevent data leakage to the verifier. \begin{align*} &\vec{\bar{F}} := r \sum _{i \in \mathbf {C}} w_i \vec{F}_i + \vec{y} \ \;{\rm mod}\;{p};\\ &\bar{\sigma } := r_{\sigma } \sum _{i \in \mathbf {C}} w_i \sigma _i + y_{\sigma } \ \;{\rm mod}\;{p};\\ &\bar{t} := r_{t} \sum _{i \in \mathbf {C}} w_i t_i + y_{t} \ \;{\rm mod}\;{p}. \end{align*}
View Source

Evaluate polynomial {\mathsf {Poly}}_{\vec{\bar{F}}}(x) at point x=\xi to obtain z:={\mathsf {Poly}}_{\vec{\bar{F}}}(\xi) \;{\rm mod}\;{p}. Divide the polynomial (in variable x) {\mathsf {Poly}}_{\vec{\bar{F}}}(x) - {\mathsf {Poly}}_{\vec{\bar{F}}}(\xi) with (x-\xi) using polynomial long division, and denote the resulting quotient polynomial as \vec{v} = (v_0, \ldots v_{m-2}), that is, {\mathsf {Poly}}_{\vec{v}}(x) \equiv \frac{{\mathsf {Poly}}_{\vec{\bar{F}}}(x) - {\mathsf {Poly}}_{\vec{\bar{F}}}(\xi)}{x-\xi }. (Note: (x-\xi) can divide polynomial {\mathsf {Poly}}_{\vec{\bar{F}}}(x) - {\mathsf {Poly}}_{\vec{\bar{F}}}(\xi) perfectly, since the latter polynomial evaluates to 0 at point x=\xi.)

Compute (\psi _{\alpha }, \psi _{\beta }, \phi _{\alpha }) \in \mathbb {G}^{3} as below \begin{align*} &\psi _{\alpha } :=\prod \limits _{j=0}^{m-1} g_j^{\vec{\bar{F}}[j]} = \prod \limits _{j=0}^{m-1} \left(g^{\alpha ^j} \right)^{\vec{\bar{F}}[j]}\\ & \quad\quad= g^{\displaystyle \mathop{\sum} \limits _{j=0}^{m-1} \vec{\bar{F}}[j] \alpha ^j} = g^{ {\mathsf {Poly}}_{\vec{\bar{F}}}(\alpha) } \ \ \in \mathbb {G};\\ &\psi _{\beta } := \prod \limits _{j=0}^{m-1} h_j^{\vec{\bar{F}}[j]} = \prod \limits _{j=0}^{m-1} \left(g^{\rho \cdot \beta ^j} \right)^{\vec{\bar{F}}[j]}\\ &\qquad = g^{\rho \cdot \displaystyle\mathop{\sum} \limits _{j=0}^{m-1} \vec{\bar{F}}[j] \beta ^j} = g^{ \rho {\mathsf {Poly}}_{\vec{\bar{F}}}(\beta) } \ \ \in \mathbb {G};\\ &\phi _{\alpha } := \prod \limits _{j=0}^{m-2} g_j^{v_j} = \prod \limits _{j=0}^{m-2} \left(g^{\alpha ^j} \right)^{v_j}\\ &\qquad = g^{\displaystyle\mathop{\sum} \limits _{j=0}^{m-2} v_j \alpha ^j} = g^{ {\mathsf {Poly}}_{\vec{v}}(\alpha) } \ \ \in \mathbb {G}. \end{align*}
View Source

Prover sends (z,\phi _{\alpha }, \bar{\sigma }, \bar{t}, \psi _{\alpha }, \psi _{\beta }) to the verifier.

Verifier checks whether the following equalities hold: \begin{equation*} e(\psi _{\alpha }, g) \ \stackrel{?}{=} e(\phi _{\alpha },\ g^{\alpha }/g^{\xi }) \cdot e(g,g)^{z} \tag{2}\end{equation*}
View Source\begin{equation*} \left(\frac{e(\psi _{\alpha },\ g^{\alpha }) }{e(g,\ Y_{\alpha } \cdot (g^{\bar{\sigma }}/Y_{\sigma })^{ r_{\sigma }^{-1} \cdot r})}\right)^\gamma \ \stackrel{?}{=} \frac{e(\psi _{\beta },\ g^{\beta })}{e(g,\ Y_{\beta } \cdot (g^{\bar{t}}/Y_{t})^{ r_{t}^{-1} \cdot r} \cdot g^R)}, \tag{3}\end{equation*}
View Sourcewhere R={ -r\sum \limits _{i\in \mathbf {C}} w_i {{\mathsf {PRF}}}_{s_1}(i)}. If all of the above equalities hold, then output {\mathtt {Accept}}, otherwise output {\mathtt {Reject}}.

We remark that the data owner retains the capability to audit the data integrity by checking \begin{align*} (e(\psi _{\alpha }, g^{\alpha }/g^{\xi })e(g,g)^z)^\alpha \ &\stackrel{?}{=}\ e(g, g^{\bar{\sigma }}) \cdot e(g,g)^{{-\sum \limits _{i\in \mathbf {C}} w_i {{\mathsf {PRF}}}_{s_0}(i)}}, \tag{4}\end{align*}
View Sourceusing the secret key sk=(\alpha, \beta, s_0). The detailed completeness (or correctness) proof is given in the full paper [9].

4.3 Dynamic DPOS Scheme
As mentioned before, upon updating one data block, a new \mathsf {HVT} corresponding to that tag needs to be regenerated and sent to the cloud server. To avoid impacting on other tags, we can manage the indices instead of the \mathsf {HVT}s upon data update. Specifically, each block is bound with a unique index that will not be reused for other blocks. We call this unique index a syntactic index, which is determined upon the tag generation. However, in practice, if we insert a new data block m^* before m_i, then m^* will become the ith block and m_i will become the i+1th block. We call this kind of index a semantic index which may change due to dynamic operations. For example, suppose the cloud server stores n blocks (m_1,m_2,\ldots, m_n), and now the data owner inserts a new block m^* after m_2, then the semantic index of m^* is 3, and the semantic indices of blocks following m^* will be incremented by 1 respectively. However, the syntactic index of m^* is n+1 and the syntactic indices of all other blocks will keep unchanged.

We propose a balanced binary tree structure, named as index management tree (\mathsf {IMT}). Each node of the tree stores the number of offspring nodes which are leaves and we call this value the weight of the node. For example, in Fig. 2, the weights of v_1, v_2, v_3, v_6 are 6, 4, 2, 1 respectively. Obviously, a node with weight 1 must be a leaf node. The syntactic index of a data block is stored in a leaf node. The semantic index of this data block is implicitly determined by the order of the leaf node. Namely, from left to right of the tree, the ith leaf node is implicitly labeled with the semantic index sem_i=i, while the syntactic index syn_i is stored in the leaf node. Therefore, the number of leaf nodes is equal to the number of data blocks.

Fig. 2. - 
Tree-based index management. We treat the leaf nodes as the left-to-right sequence.
Fig. 2.
Tree-based index management. We treat the leaf nodes as the left-to-right sequence.

Show All

The proposed structure borrows the idea of \mathsf {AVL} tree [47] which is a binary search tree such that for each internal node v, the heights of its left subtree and right subtree differ by at most 1. Once there exists a node v and the heights of its two subtrees differ by 2 due to some insertion or deletion operations, this tree can be rebalanced to a new \mathsf {AVL} tree through single or double rotations. This makes any operation, such as retrieving a node or inserting a new node, done with complexity of O(h) where h is the height of the tree. Our goal is to mange the syntactic indices efficiently, so that whenever the data owner inserts, deletes or modifies a data block of a file, the computation and storage overhead due to these updates should be minimized. We propose to manage the indices so that there is no need to regenerate other \mathsf {HVT}s except for the updated one. This can significantly reduce both the computation and communication complexity.

The proposed dynamic DPOS scheme is similar to the basic DPOS scheme, while the tag generation is different and three new algorithms \mathsf {UpdateRequest}, \mathsf {UpdateIndex} and \mathsf {UpdateConfirm} for supporting dynamic operations are required. We will introduce the details as follows respectively.

Tag Generation Phase. In the dynamic DPOS scheme, the tag generation algorithm adopts the same method to generate \mathsf {HVT}s as that of the basic DPOS scheme. However, besides these works, the data owner also needs to initialize the \mathsf {IMT} as described above, in which from left to right the ith leaf node (sym_i=i) stores the syntactic index of ith block (syn_i). Initially, the semantic indices and the syntactic indices are identical. Only when data update happens sym_i could become distinct with syn_i. The prepared \mathsf {IMT} is sent to \sf {ODA} for auditing purpose.

Data Update Phase. Data update is implemented by three algorithms \mathsf {UpdateRequest}, \mathsf {UpdateIndex} and \mathsf {UpdateConfirm}. The \mathsf {UpdateRequest(M,i,UT)} algorithm, run by the data owner, takes input of the new data block M, the position i and the update type (\mathsf {UT}) including {\mathtt {m}odify/insert/delete}, and generates the update request information (\mathsf {URI}) that will be sent to the cloud server for dynamic operations. According to different update types, the corresponding implementation of the algorithm will be different shown as follows.

\mathsf {UpdateRequest(M,i,{\mathtt {m}odify})\rightarrow URI}

In the {\mathtt {m}odify} operation, the data owner produces a new syntactic index syn^{\prime }_M=syn_{max}+1 for M where syn_{max} is the maximum value of all syntactic indices in the index tree, and replaces the original syntactic index of syn_M with syn^{\prime }_M in the index tree. syn_M can be retrieved from the ith leaf node. As an example, in Fig. 3a, the fourth block is modified. Correspondingly, the fourth leaf node V_{11} is updated by replacing the syntactic index value with 7. Then, the data owner generates the new \mathsf {HVT}s (\sigma ^{\prime }_i, t^{\prime }_i) according to Eq. (1). Note that syn^{\prime }_M (in case of Fig. 3a syn^{\prime }_M=7) is used in calculating the \mathsf {HVT}s in order to prevent the forge attack introduced by Yang et al. [21], which will be discussed in Section 5. Finally, the data owner sends the request information \mathsf {URI}=(M,\sigma ^{\prime }_i, t^{\prime }_i, i,{\mathtt {m}odify}) to the cloud server and the \sf {ODA} which will update the \mathsf {IMT} with \mathsf {URI}.


Fig. 3.
Dynamic operations with our proposed scheme. Nodes with red edge could be affected by dynamic operations. Note that each leaf node stores the weight and a syntactic index while the semantic index is implicitly indicated by the leaf node's position from left to right. Here for simplicity, we list both the semantic index (Sem) and the syntactic index (Syn) of each leaf node in the figures.

Show All

\mathsf {UpdateRequest(M,i,{\mathtt {i}nsert})\rightarrow URI}

In the {\mathtt {i}nsert} operation, M is the new block that will be inserted into the ith position. Similarly, the data owner also generates a syntactic index syn_M=syn_{max}+1 for M, and creates the new \mathsf {HVT}s (\sigma ^{\prime }_i, t^{\prime }_i) with syn_M. The request information \mathsf {URI}=(M,\sigma ^{\prime }_i, t^{\prime }_i, i,{\mathtt {i}nsert}) is sent to the cloud server as well as the \sf {ODA} which will rebuild the \mathsf {IMT} with \mathsf {URI}. Fig. 3b shows an example of inserting a new block at position 6.

\mathsf {UpdateRequest(\mathtt {null},i,{\mathtt {d}elete})\rightarrow URI}

In the {\mathtt {d}elete} operation, the data owner requires to delete the ith block. The request information \mathsf {URI}=(\mathtt {null},\mathtt {null}, \mathtt {null}, i,{\mathtt {d}elete}) is sent to the cloud server as well as the \sf {ODA}. Fig. 3c shows the result of deleting the third block associated with node V_{10} of Fig. 3b.

\mathsf {UpdateIndex (URI)\rightarrow \lbrace \mathtt {null}\rbrace }

This algorithm is executed by \sf {ODA} which updates the \mathsf {IMT} with the information of \mathsf {URI}. In case of {\mathtt {m}odify}, \sf {ODA} only needs to replaces the value of the syntactic index of ith leafnode with syn_M=syn_{max}+1. In case of {\mathtt {i}nsert}, \sf {ODA} needs to inserts a new leafnode at position i and update the weight values of the added node's ancestors. In case of {\mathtt {d}elete}, \sf {ODA} deletes the ith leafnode and updates the weight values of the deleted node's ancestors. Note that in the latter two cases, the \mathsf {IMT} may need to be rebalanced since {\mathtt {i}nsert} or {\mathtt {d}elete} operation could affect the balance property of the tree. Optionally, if the data owner hopes to audit his data on his own, he can also keep and manage the \mathsf {IMT} as a local copy which will not cost much storage (for example, the size of \mathsf {IMT} for a 1 GB data file with 16 KB block size is less than 1 MB).

\mathsf {UpdateConfirm (URI)\rightarrow \lbrace Success, Fail\rbrace }

The cloud server will update the file blocks and the corresponding \mathsf {HVT}s once it receives the update request from the data owner. In case of {\mathtt {m}odify}, the cloud server replaces the current block together with its \mathsf {HVT}s at position i with the new block and new \mathsf {HVT}s. In case of {\mathtt {i}nsert}, the cloud server inserts the new block and \mathsf {HVT}s to the current data file and tag list respectively at position i. In case of {\mathtt {d}elete}, the cloud server deletes the block and the associated \mathsf {HVT}s at position i. If the procedure is executed correctly, the cloud server returns ‘Success’ to the data owner; otherwise it returns ‘Fail’.

With above algorithms, data dynamics can be achieved with the following steps shown in Fig. 4: first, the data owner runs \mathsf {UpdataRequest} to create the \mathsf {URI} which is then sent to the cloud server and \sf {ODA}. On receiving \mathsf {URI}, the cloud server update the data file and \mathsf {HVT}s correspondingly, while \sf {ODA} update its \mathsf {IMT}. Finally, \sf {ODA} confirms the dynamic operations by auditing the updated data and sends the result to the data owner.

Rebalancing \mathsf {IMT}. Inserting or deleting data blocks will modify \mathsf {IMT} as well and may cause the unbalance of \mathsf {IMT}. Similar with an \mathsf {AVL} tree, \mathsf {IMT} can be reblanced through single or double rotations within \log n time. The difference is that we need to update each node's weight when doing rotations.


Fig. 4.
The workflow of the dynamic operations.

Show All

4.4 Discussions
Discussion 1. How to verify the integrity of all tag values \lbrace t_i \rbrace in algorithm \mathsf {UpdVK}?

The data owner can verify the integrity of all tag values \lbrace t_i \rbrace using his/her secret keys. Note that she/he can replace the auditor to run the interactive proof algorithm \langle \mathsf {P}, \mathsf {V} \rangle with the cloud storage server, since she/he also holds key vsk. The data owner downloads all tags \lbrace t_i \rbrace and audits all data blocks at once by running \langle \mathsf {P}, \mathsf {V} \rangle with the cloud server using sampling set \mathbf {C}=[0, n-1]. She/he also checks an additional equality: g^{\bar{t}} \stackrel{?}{=} g^{r_{t} \sum _{i \in [0, n-1]} w_i t_i} \cdot Y_t, where r_t and w_i's are chosen by the data owner, \bar{t} and Y_t come from the server's response, and t_i's on the right hand side are downloaded from server before this interactive proof. If and only if \langle \mathsf {P}, \mathsf {V} \rangle outputs {\mathtt {Accept}} and the additional equality check succeeds, the data owner will consider that the downloaded tags \lbrace t_i \rbrace are intact.

Discussion 2. How to reduce the size of challenge \lbrace (i, w_i): i \in \mathbf {C} \rbrace?

Dodis et al. [31]'s result can be used to represent a challenge \lbrace (i, w_i): i \in \mathbf {C} \rbrace compactly as below:

Choose the subset \mathbf {C} using Goldreich [48]'s (\delta, \epsilon)-hitter,3 where the subset \mathbf {C} can be represented compactly with only \log n + 3\; \log (1/\epsilon) bits. Assume n < 2^{40} (sufficient for practical file size) and let \epsilon = 2^{-80}. Then \mathbf {C} can be represented with 280 bits.

The sequence (\ldots, w_i, \ldots) of \ell weights w_i, i \in \mathbf {C}, ordered by increasing i, forms a simple geometric sequence (w^1, w^2, \ldots, w^{\ell }) for some w \in \mathbb {Z}_p^{*}.

SECTION 5Security Analysis
5.1 Soundness
For ease of exposition, we clarify two related but distinct concepts: valid proof and genuine proof. (1) A proof is genuine, if it is the same as the one generated by an honest (deterministic) prover on the same query. (2) A proof is valid, if it is accepted by the honest verifier. In our scheme, for each query, there exists only one genuine proof, and there exist many valid proofs. To be secure, our scheme has to ensure that it is computationally hard to compute a valid but not genuine proof. Due to the limited space, we only display parts of proofs in this paper and the full proof of all theorems and lemmas can be found in the full version [49].

Definition 5 (m-Bilinear Strong Diffie-Hellman (m-BSDH) Assumption).
Let e: \mathbb {G} \times \mathbb {G} \rightarrow \mathbb {G}_T be a bilinear map where \mathbb {G} and \mathbb {G}_T are both multiplicative cyclic groups of prime order p. Let g be a randomly chosen generator of group \mathbb {G}. Let \varsigma \in _R \mathbb {Z}_p^{*} be chosen at random. Given as input a (m+1)-tuple T=(g, g^{\varsigma }, g^{\varsigma ^2} \ldots, g^{\varsigma ^m}) \in \mathbb {G}^{m+1}, for any \mathsf {PPT} adversary \mathcal {A}, the following probability is negligible \begin{equation*} \mathsf {Pr} \left[ d = e(g, g)^{1/(\varsigma +c)} \text{ where } (c,d)=\mathcal {A}(T) \right] \leq negl(\log p). \end{equation*}
View Source

Theorem 1.
Suppose m-BSDH Assumption hold, and \mathsf {PRF} is a secure pseudorandom function. The \mathsf {DPOS} scheme constructed in Section 4 is sound, according to Definition 3.

Game 1. The first game is the same as soundness security game \mathtt {Game}_{\tt sound}, except that the pseudorandom function {{\mathsf {PRF}}}_{s_0} outputs true randomness. Precisely, for each given seed s_0, the function {{\mathsf {PRF}}} is evaluated in the following way:

The challenger keeps a table to store all previous encountered input-output pairs (v, {{\mathsf {PRF}}}_{s_0}(v)).

Given an input v, the challenger lookups the table for v, if there exists an entry (v, u), then return u as output. Otherwise, choose u at random from the range of {{\mathsf {PRF}}}, insert (v, {{\mathsf {PRF}}}_{s_0}(v) := u) into the table and return u as output.

Game 2. The second game is the same as Game 1, except that the pseudorandom function {{\mathsf {PRF}}}_{s_1} with seed s_1 outputs true randomness. The details are similar as in Game 1.

Lemma 1.
Suppose m-BSDH Assumption holds. In Game 2, any \mathsf {PPT} adversary is unable to find two distinct valid tuples T_0 \ne T_1 and the last four elements of tuple T_0 and T_1 are equal, where T_0=(z,\phi _{\alpha };\ \ \bar{\sigma }, \bar{t}, \psi _{\alpha }, \psi _{\beta }) and T_1 = (z^{\prime },\phi _{\alpha }^{\prime };\ \ \bar{\sigma }, \bar{t}, \psi _{\alpha }, \psi _{\beta }). Precisely, for any \mathsf {PPT} adversary \mathcal {A} in Game 2, there exists a \mathsf {PPT} algorithm \mathcal {B}, such that \begin{align*} & \mathsf {Pr} \left[\substack{T_0 \ne T_1 \text{ are both valid and } T_0[2,3,4,5] = T_1[2,3,4,5],\\ \text{ where } (T_0, T_1)=\mathcal {A}^{\mathbf {Game 2}}} \right]\\ &\quad \leq \mathsf {Pr} \left[ \mathcal {B} \text{ solves}\ m-\mathrm{ BSDH} \;\text{Problem } \right]. \end{align*}
View Source(Proof of Lemma 1 is given in [49])

Lemma 2.
Suppose m-BSDH Assumption holds. In Game 2, any \mathsf {PPT} adversary is unable to find two distinct valid tuples T_0 \ne T_1 such that the last four elements of T_0 and T_1 are not equal, where T_0 = (z,\phi _{\alpha };\ \ \bar{\sigma }, \bar{t}, \psi _{\alpha }, \psi _{\beta }) and T_1 = (z^{\prime },\phi _{\alpha }^{\prime };\ \ \bar{\sigma }^{\prime }, \bar{t}^{\prime }, \psi _{\alpha }^{\prime }, \psi _{\beta }^{\prime }) and (\bar{\sigma }, \bar{t}, \psi _{\alpha }, \psi _{\beta }) \ne (\bar{\sigma }^{\prime }, \bar{t}^{\prime }, \psi _{\alpha }^{\prime }, \psi _{\beta }^{\prime }). Precisely, for any \mathsf {PPT} adversary \mathcal {A} in Game 2, there exists a \mathsf {PPT} algorithm \mathcal {D}, such that \begin{align*} & \mathsf {Pr} \left[ \substack{ T_0 \ne T_1 \text{ are both valid and } T_0[2,3,4,5] \ne T_1[2,3,4,5],\\ \text{ where } (T_0, T_1)=\mathcal {A}^{{\mathbf {Game 2}}} } \right]\\ & \quad \leq 2\mathsf {Pr} \left[ \mathcal {D} \text{ solves}\ m-\text{BSDH Problem } \right]. \end{align*}
View Source(Proof of Lemma 2 is given in [49]).

Proof 1 (Sketch proof of Theorem 1).
Since \mathsf {PRF} is a secure pseudorandom function, the soundness security game \mathtt {Game_{sound}} and Game 1 are computationally indistinguishable. So are Game 1 and Game 2. Therefore, Lemmas 1 and 2 also hold in \mathtt {Game_{sound}} with negligible difference in success probability.

Using proof of contradiction, one can show that: For any adversary \mathcal {A} that wins \mathtt {Game}_{sound}, there exists noticeable fraction of possible weights W:=\lbrace w_i \in \mathbb {Z}_p: i \in \mathbf {C} \rbrace over the domain of weights W, such that for each weight W, there exists noticeable fraction of points \xi \in \mathbb {Z}_p over the domain \mathbb {Z}_p, the adversary \mathcal {A} can provide the correct response to the challenge (\lbrace (i, w_i): i \in \mathbf {C} \rbrace, \xi) where \lbrace w_i: i \in \mathbf {C} \rbrace = W.

Therefore, from sufficient number of points z={\mathsf {Poly}}_{\vec{\bar{F}}}(\xi) along the same polynomial {\mathsf {Poly}}_{\vec{\bar{F}}}(\cdot), the knowledge extractor can find the coefficient vector \vec{\bar{F}} by solving linear equation system over \mathbb {Z}_p. From \mathtt {transcript}_{\mathcal {A}}, the knowledge extractor can recover the blinding randomness \vec{y}, and thus recover the weighted sum of file blocks: \sum _{i \in \mathbf {C}} w_i \vec{F}_i. Furthermore, from sufficient number of weighted sum w.r.t. different weights W's, the knowledge extractor can recover each file block \vec{F}_i, i \in \mathbf {C}, by solving a linear equation system over \mathbb {Z}_p. Consequently, the challenger of \mathtt {Game}_{\tt sound} wins the game, as we desire.

5.2 Privacy Preserving
Definition 6 (Discrete Log Assumption).
Let e: \mathbb {G} \;\times \mathbb {G} \rightarrow \mathbb {G}_T be a bilinear map where \mathbb {G} and \mathbb {G}_T are both multiplicative cyclic groups of prime order p. Let g be a randomly chosen generator g \in \mathbb {G} and h \in _R \mathbb {G} be a randomly chosen group element. For any \mathsf {PPT} adversary \mathcal {A}, the probability that \mathcal {A} can output x such that g^{x} =h is negligible. That is, \begin{equation*} \mathsf {Pr} \left[ g^{x} = h \text{ where } x= \mathcal {A}(g, h) \right] \leq negl(\log p). \end{equation*}
View Source

Theorem 2.
Suppose Discrete Log (DL) Assumption holds in group \mathbb {G} and \mathsf {PRF} is a secure pseudorandom function. The \mathsf {DPOS} scheme constructed in Section 4 is privacy-preserving according to Definition 4.

Game 4. This game is identical to the privacy security game \mathtt {Game_{\tt private}}, except that the pseudorandom values \lbrace \mathsf {PRF}_{s_0}(i) \rbrace with seed s_0 are replaced by true random values, as in Game 1.

Note: We emphasize that the values \mathsf {PRF}_{s_1}(i) with seed s_1 is not replaced, and it is still the actual “pseudorandom” function output. In the privacy security game, the adversary is the dishonest verifier, who will obtain the seed s_1 as a part of the verification secret key vsk.

Game 5. This game is identical to Game 4, except that adversary \mathcal {A} (i.e the dishonest verifier/auditor) is provided with extra information (g^{\langle \vec{\alpha }, \ \vec{F}_i \rangle }, g^{\rho \langle \vec{\beta }, \ \vec{F}_i \rangle }, g^{\sigma _i}, g^{t_i}) for each i and for each file.

Lemma 3.
Suppose the Discrete Log Assumption holds in group \mathbb {G}. In Game 5, any \mathsf {PPT} adversary \mathcal {A} (i.e., the dishonest verifier/auditor) is unable to output file sector \vec{F}_i[j] for any i \in [0, n-1], j \in [0, m-1]. Precisely, for any \mathsf {PPT} adversary \mathcal {A}, there exists \mathsf {PPT} algorithm \mathcal {E}, such that \begin{equation*} \mathsf {Pr} \left[ \mathcal {A} \text{ wins}\ {\mathbf {Game 5}}\right] \leq \mathsf {Pr} \left[ \mathcal {E}\ \text{ solves}\ {{\mathbf DL}}\ \text{problem} \right]. \end{equation*}
View Source(Proof of Lemma 3 is given in [49])

Proof 2 (of Theorem 2).
Since \mathsf {PRF} is a secure pseudorandom function, we have

\mathsf {Pr} \left[ \mathcal {A}\ \text{ wins }\ \mathtt {Game}_{\tt private} \right] \leq \mathsf {Pr} \left[ \mathcal {A}_2 \text{ wins}\ {{\mathbf {Game \;4}}} \right] + negl(\lambda) for any \mathsf {PPT} algorithm \mathcal {A}_2. Since Game 5 is identical to Game 4, except that the adversary will obtain extra information, we have \mathsf {Pr} \left[ \mathcal {A}_2 \text{ wins} \;{{\mathbf {Game\;4}}} \right] \leq \mathsf {Pr} \left[ \mathcal {A}_3 \text{ wins}\ {{\mathbf {Game \;5}}} \right] for any \mathsf {PPT} algorithm \mathcal {A}_3.

Combining the above two inequalities and Lemma 3, Theorem 2 is proved.

5.3 Security Analysis of the Dynamic DPOS
Since the dynamic DPOS scheme does not change the tag generation and block verification techniques, it is consequently able to be proven secure and privacy-preserving with the same proof methods as above. In addition, according to Yang et al. [21], dynamic operations may cause the auditing protocol insecure since the cloud server may launch two attacks: 1) Replay attack, where the server does not update the data owner's data correctly and tries to pass the auditing with previous version of the data. 2) Forge attack, where the server could obtain sufficient information from the dynamic operations on some specific block to forge a valid tag.

In our scheme, the verifier \sf {ODA} keeps the \mathsf {IMT} that stores a unique syntactic index considered as the abstract information for each block. A new syntactic index different from any of the previous ones will be created in the modification operation and thus a new \mathsf {HVT} that takes the syntactic index as part of the inputs is produced as well. Whenever the data owner completes a data dynamic operation, it will send the update information to \sf {ODA} for updating \mathsf {IMT}. Therefore, the abstract information on \sf {ODA} side is always up-to-date. If the server launches the replay attack, that is, returns the previous version of the data and the corresponding \mathsf {HVT}s, the verifier can detect and reject it trivially.

The reason we require to generate a new syntactic index when the data owner modifies a data block is to save storage compared with Yang et al.'s table-based scheme while keeping resistant to the forge attack. Otherwise, if the syntactic index keeps untouched upon modification update, the server may successfully launch the forge attack as following. It waits the data owner to request modification update of a data block F_i to F^{\prime }_i when the data owner will recompute \sigma ^{\prime }_i = \langle \vec{\alpha }, \ \vec{F^{\prime }_i} \rangle + {{\mathsf {PRF}}}_{s_0}(i) \;{\rm mod}\;{p} and send the updated \mathsf {HVT} \sigma ^{\prime }_i together with F^{\prime }_i to the server. The server can obtain \sigma _i-\sigma ^{\prime }_i = \langle \vec{\alpha }, \ (\vec{F_i}-\vec{F^{\prime }_i}) \rangle \;{\rm mod}\;{p}. Obviously, if the server gets enough times of this kind of information, it can recover the secret key \alpha and thus can forge a valid \mathsf {HVT} \sigma ^*_i for any forged data block F^*_i.

To prevent the forge attack, we propose to take the syntactic index as one component of generating the \mathsf {HVT}s. Now, upon modifying a block, the new \mathsf {HVT} is computed as \sigma ^{\prime }_i = \langle \vec{\alpha }, \ \vec{F^{\prime }_i} \rangle + {{\mathsf {PRF}}}_{s_0}(i^{\prime }) \;{\rm mod}\;{p}. The cloud server can reveal nothing from \sigma _i and \sigma ^{\prime }_i if {\mathsf {PRF}} is secure. With this method Yang et al.'s [21] index table can be reduced to half of the previous size without requirement of the timestamp T_i and the version number V_i.

SECTION 6Performance Evaluation
We evaluate the performance of our scheme and some selected popular schemes for comparison including publicly verifiable \mathsf {POS} schemes like Wang et al.'s scheme [8] and Yang et al.'s scheme [21], and privately verifiable \mathsf {POS} scheme like Shacham and Water's protocol [3]. Our experiment is conducted with a Laptop PC with a 2.5 GHz Intel Core i5-3210M CPU (with two cores and four threads) and 8 GB of RAM using Java language. The Java Pairing-Based Cryptography Library (JPBC) with version of 1.2.0 is used in the codes and the Type-A pairing constructed on the curve y^2=x^3+x with the embedding degree of 2 is chosen. Our test files are randomly generated and of size from 10 to 100 MB with block size of 4 KB. The reason that we test on files with size of at most 100 MB is that it is really slow for the other publicly verifiable \mathsf {POS} schemes to generate the \mathsf {HVT}s (for the 100 MB file, it takes more than twenty minutes for each trial). All the experimental results are obtained from the average value of 20 trials.

6.1 Data Pre-Processing Time
In this experiment, the time of generating the \mathsf {HVT}s is measured which does not include the I/O time to load or store data to disk. As we mentioned in Section 1, the data pre-processing is done by the data owner, so it is an important factor of the quality of service (QoS) for cloud clients.

Fig. 5 shows the experimental results. For the sake of easy view, we set the y-axis as the logarithm of time to the base 10 (i.e., \log _{10} (time)) where time is the seconds required to do the data pre-processing. To deal with a 100 MB file for data pre-processing, the private \mathsf {POS} needs 1.71 seconds, and our scheme takes 3.99 seconds, while both Wang et al. and Yang et al.'s schemes cost about 1,373 seconds. Thus, we achieve a throughput of data preprocessing at speed about 25 MB/s, about half of the speed of the private \mathsf {POS} scheme with 58 MB/s, compared with the other two publicly verifiable \mathsf {POS} schemes at speed around 75 KB/s. Namely, the data pre-processing in our scheme is more than 300 times faster than [8], [21]. Thus, our scheme is more suitable for mobile devices than existing public \mathsf {POS} schemes. The reason that our scheme is slower than the private scheme is that we generate two \mathsf {HVT}s for each block.

Fig. 5. - 
Comparison of data pre-processing time. $y$y-axis is $\log _{10} (Time)$log10(Time), where $Time$Time is the seconds required to do the data pre-processing. Note that performances of [8] and [21] are very close to each other.
Fig. 5.
Comparison of data pre-processing time. y-axis is \log _{10} (Time), where Time is the seconds required to do the data pre-processing. Note that performances of [8] and [21] are very close to each other.

Show All

6.2 Computation Cost
In this experiment, we look into the performance of the server that generates a proof of storage and the auditor \sf {ODA} that verifies the proof. According to Table 1, most of existing publicly verifiable \mathsf {POS} schemes have similar computation complexity in terms of the server and the auditor/verifier. Without loss of generality, we test two representative publicly verifiable schemes [8], [21] to compare with our scheme. We consider each block divided into m=10 sectors. The experiments are conducted for ten settings with different number of sampling blocks. According to [2], random sampling 460 blocks will enable the verifier to detect the corruption of 1 percent of the data file with probability larger than 99 percent. Therefore, in our experiments the number of sampled blocks is up to 500. As shown in Fig. 6, both the server and the auditor of our scheme perform much more efficiently than other publicly verifiable \mathsf {POS} schemes. Intuitively, this is because our scheme employs constant number of exponentiations and pairings during the proofing and verification phase once the number of sectors in each block is fixed.

Fig. 6. - 
Comparison of computation cost.
Fig. 6.
Comparison of computation cost.

Show All

6.3 Communication Cost
The communication cost includes the cost caused by the auditor and the server, namely, consisting of the challenge and the proof transcripts. According to Table 1, all existing \mathsf {POS} schemes result in communication costs increasing linearly with the number of sampled blocks, while our scheme only induces constant communication costs by employing the polynomial commitment scheme. To make it easier to compare, we plot the selected protocols according to Table 1 in Fig. 7.

Fig. 7. - 
Comparison of communication cost, with parameters of $|F|=100\;\mathrm {MB}$|F|=100 MB , $\lambda =160$λ=160, $m=10$m=10 and $\ell \in [1,500]$ℓ∈[1,500].
Fig. 7.
Comparison of communication cost, with parameters of |F|=100\;\mathrm {MB}, \lambda =160, m=10 and \ell \in [1,500].

Show All

SECTION 7Conclusion
We proposed a novel \mathsf {POS} scheme which is lightweight and privacy preserving. On one side, the proposed scheme is as efficient as private key \mathsf {POS} scheme, especially very efficient in authentication tag generation. On the other side, the proposed scheme supports third party auditor and can revoke an auditor at any time, close to the functionality of publicly verifiable \mathsf {POS} scheme. Compared to existing publicly verifiable \mathsf {POS} schemes, ours improves the authentication tag generation speed by hundreds of times. Our scheme also prevents data leakage to the auditor during the auditing process. Finally, we designed a new \mathsf {AVL}-tree based fully dynamic mechanism for our \mathsf {POS} scheme. The experimental results verified the performance efficiency of our scheme.