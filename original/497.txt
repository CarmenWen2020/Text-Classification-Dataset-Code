The size and number of cloud data centers (CDCs) have grown rapidly with the increasing popularity of cloud computing and high-performance computing. This has the unintended consequences of creating new challenges due to inefficient use of resources and high energy consumption. Hence, this necessitates the need to maximize resource utilization and ensure energy efficiency in CDCs. One viable approach to achieve energy efficiency and resource utilization in CDC is task scheduling. While several task scheduling approaches have been proposed in the literature, there appears to be a lack of classification-based merging concept for real-time tasks in these existing approaches. Thus, an energy-efficient dynamic scheduling scheme (EDS) of real-time tasks for virtualized CDC is presented in this paper. In the scheduling scheme, the heterogeneous tasks and virtual machines are first classified based on a historical scheduling record. Then, similar type of tasks are merged and scheduled to maximally utilize an operational state of the host. In addition, energy efficiencies and optimal operating frequencies of heterogeneous physical hosts are employed to attain energy preservation while creating and deleting the virtual machines. Experimental results show that, in comparison with existing techniques, EDS significantly improves overall scheduling performance, achieves a higher CDC resource utilization, increases task guarantee ratio, minimizes the mean response time, and reduces energy consumption.
SECTION 1Introduction
Cloud data centers (CDCs) are a collection of interconnected and virtualized heterogeneous resources (including hosts, storages, applications, networks, and services), which facilitate ubiquitous, on-request network access to (shared) computing resources [1], [2], [3]. CDCs generally host large number of physical hosts, for example hundreds of thousands of physical hosts. The scale of such operations, such as those related to the hosts and equipments, also incurs significant energy costs [4], [5]. This clearly has environmental implications. For example, CDCs are estimated to contribute 2 percent of the overall carbon emission [6]. There are also additional energy-related costs [7], which can have an implication on the reliability of hosts’ performance [8]. In other words, we are facing an environmentally unsustainable situation [6]. This necessitates the design of energy consumption reduction or “green” CDCs.

In parallel to the excessive energy consumption, CDCs also face other challenges such as (very) low resource utilization. For example, it was estimated that completely idle hosts consume approximately 50 percent additional energy in comparison to fully utilized hosts [9]. In other words, lower utilization of computing resources (estimated to be between 10 and 50 percent) indicates the massive waste of energy [10], [11]. Therefore, improving resource utilization by decreasing the number of on-run hosts, for example via dynamic arrangement of virtual machines (VMs) on a host, is one potentially viable strategy to minimize energy consumption [12]. In practice, most applications in CDCs are heterogeneous submitted dynamically by customers, with defined deadlines. In other words, we need to have sufficient number of active hosts in the CDC at any time to execute incoming or upcoming tasks successfully prior to their deadlines. Failing would affect energy consumption and violate the Quality of Service (QoS), which has significant financial and reputation implications.

In this paper, we design an energy-efficient dynamic scheduling scheme (EDS), to achieve energy-efficiency, and optimize task guarantee ratio, mean response time, and resource utilization for CDCs. More precisely in our EDS, tasks are classified using task classification method and then mapped to the most suitable VMs based on the task's requirement and the host's capacity. EDS uses task classification based on Bayes classifier and historical scheduling record (HSR), which allow one to find both task type and VM type. The task merging strategy is developed to merge similar type of tasks and schedule the most suitable VM until the resource condition is satisfied. Therefore, the scheduler can dynamically scale up and consolidate the resources of CDC, according to resource requirement.

A summary of our contributions in this paper is as follows:

The design of a dynamic task scheduling scheme with resource provisioning in the context of virtualized CDC.

The design of a task classification method to classify tasks and map them to the most suitable VM, in order to minimize mean response time.

The design of a task merging strategy to merge similar type of tasks, when are then scheduled in the same physical host to maximize utilization ratio.

The design of policies to create, migrate and delete VMs dynamically to adjust the scale of CDC and satisfy real-time requirements.

In the next two sections, we will present related literature and relevant mathematical background materials. In Sections 4 and 5, we present our proposed approach and describe our evaluation of the proposed approached. Specifically, the proposed approach is simulated using CloudSim toolkit, and we benchmark the performance of the proposed approach with existing schemes, in terms of task guarantee ratio, resource utilization, total energy consumption, and mean response time. Section 6 presents the conclusion.

SECTION 2Related Work
Topics relating to minimizing/optimizing energy consumption and scheduling for CDCs are active research areas, since the late 2000's [13], [14], [15]. For example, there have been extensive research efforts on designing energy efficient scheduling approaches, such as virtualization-based energy-efficient approaches and dynamic voltage and frequency scaling (DVFS) enabled approaches, for both homogeneous and heterogeneous environments [16], [17], [18]. The survey of energy efficient strategies for data centers presented in [17], for example, classified existing research efforts into those focusing on operating system, hardware, data center and virtualization. A multi-dimensional partition model based on VM placement technique was proposed in [19]. In a similar work, a greedy algorithm was proposed to deal with quadratic assignment and bin packing problems to improve utilization of resources by reducing the number of active hosts [20].

Energy-aware Resource Allocation Method (EnReal), proposed in [21], focuses on VMs dynamic deployment for scientific workflow executions. The authors proposed an energy consumption model to study cloud application requirements, and an energy-aware resource allocation method for VM allocation; thus, supporting scientific workflow execution based on their energy consumption model.

A balanced VM Scheduling method for energy-performance trade-offs in cyber-physical cloud system is presented in [22]. In this particular work, a joint optimization model for energy consumption and performance degradation of VM migration for cyber-physical cloud systems was formulated. Using the model, the authors designed a corresponding VM scheduling method for trade-offs between energy and performance to achieve both energy saving and performance degradation mitigation.

Several algorithms have been proposed for VM dynamic consolidation in CDCs, in order to optimize the number of hosts by consolidating VMs dynamically and powering off idle hosts to decrease additional energy consumption [23], [24], [25]. The main objective of these algorithms is mainly to improve the utilization of computing resources and reduction of energy consumption under existing service level agreement (SLA) constraints. Overloading host detection, power and SLA-aware VM selection, and two-phase VM placement algorithms are developed using an iterative weighted linear regression method to determine two utilization thresholds and avoid performance degradation [24]. These algorithms can be utilized for the effective placement of new VMs, where the selected VMs can be further processed for consolidation. Similarly, an enhancing energy-efficient and QoS dynamic virtual machine consolidation (EQVC) method is proposed in [25], which consists of four algorithms that correspond to different stages in VM consolidation. Based on their method, redundant VMs from the hosts are selected, and then they are migrated to other hosts before they get overloaded. In this work, the host-model with adaptive reserved resources were further introduced to prevent re-overload of hosts, guaranteeing QoS requirements.

A parameter-based VM consolidation solution is proposed in [26], which aims to mitigate issues with reservation-based and demand-based solutions. This parameter-based VM consolidation exploits the range between the demand-based and reservation-based ways of finding VM to host allocations that strike a delicate balance according to cloud providers’ goals. In another work, a self-adaptive approach, called SAVE, is proposed by [27]. This approach makes the decision of the assignment and migration of VMs by using probabilistic processes that exclusively use local information.

A workload prediction-based reconfiguration algorithm is proposed to allocate VMs and hosts dynamically [28]. Similarly, a scheduling algorithm is proposed in [29] to select the most suitable physical host to a VM, based on the future power consumption's forecast. An online scheduling system is also developed by [30] for distributed computing platforms to minimize the energy consumption.

The brownout enabled system considering application components is presented in [31], which are either mandatory or optional. The authors consider component-level control in the system that can also be applied to container or micro services architecture. In this work, they proposed an algorithm to determine brownout time and reduced utilization of host. Their results showed that the application utilization and total energy consumption could be reduced.

In another work, a utilization-based migration algorithm is proposed to minimize the energy consumption of host by decreasing the migrations [32]. The authors developed a performance function for utilization-based migration scheme to optimize the VM placement, which, in turn, consolidates VMs to surpass the energy efficiency and guarantee the QoS. Experimental results showed about 10 percent of hosts has low utilizations.

A multi-objective dynamic scheduling system is proposed to minimize the energy consumption [30]. A real-time system that combines execution time and energy consumption to execute task-based applications on cloud computing platforms based on energy performance important factor. A prototype of scheduler has been implemented and tested with real task-based COMPSs applications. The system is evaluated with different kind of DAG and size instances which provides a better solution.

Energy-aware resource provisioning mechanism is proposed in [33] for cloud data centers, which are capable of serving real-time periodic tasks following the Software as Service model. The three-tier energy-aware hierarchical scheduling approach is presented, where the scheduler of the guest OS constitutes the first tier, the placer component of the hypervisor located on top of servers forms the second tier, and the third tier is a server-level scheduler that allocates the VCPUs onto physical CPUs (PCPUs)within each multi-core server. Similarly, an online energy-aware resource provisioning algorithm (EAICA) for real-time cloud services is proposed in [34] to reduce the deadline miss ratio for real-time cloud services. The proposed provisioning framework not only considers the energy consumption of servers but it also takes the energy consumption of the communication network into account, to provide a holistic solution. EAICA is inspired from a swarm intelligence technique based on the Imperialist Competitive Algorithm (ICA).

A migration technique is used to move VMs from underutilized hosts to utilized hosts and makes under-utilized hosts idle; then idle servers get powered off [35], [36], [37]. From a critical point of view, migration techniques could bring delay with extra energy consumption. Maximum (rate) Utilization (MaxUtil) has compared two consolidation approaches which added migration techniques to former approaches aiming at maximizing resource utilization [38]. In MaxUtil's system, neither adaptive nor predictive techniques have been proposed. A power utility based energy-aware scheduling (EAS) algorithm is proposed in [39] to reduce the energy consume by workflow. The proposed EAS algorithm covered two sub-algorithms named as task merging algorithm and task mapping algorithm. First, all tasks (merged and non-merged task) are mapped to an optimal VM-type based on the power utility concept; then second, two tasks from task sequence are merged as one.

An algorithm based on Dynamic Voltage and Frequency Scaling (DVFS) technology has been proposed to reduce the amount of energy consumed by parallel jobs [40]. This algorithm widens the non-critical task's execution time and reduces energy cost by lowering the frequency and voltage of processor. However, the given algorithm does not examine parallel scheduling operation for other resource demands in addition to the CPU. An energy-aware scheduling for real-time tasks based on cooperative two-tier strategy was also proposed by [41], which aimed to be beneficial to both cloud users and their service providers. Similarly, a new task scheduler for cloud computing has been proposed in [42] by focusing on dependency between tasks and aiming to achieve energy savings. A cloud-based energy consumption model was presented in [43] to improve the energy efficiency based on a statistical method.

SECTION 3Problem Formulation and Modeling
This section formulates a complete scheduling model for CDC to define the system model, resource model, task model, energy consumption model, optimization problems and constraints, task and virtual machine mark model, task merging model and migration model. Table 1 shows the key terms and their descriptions, which have been used in the rest of the paper.

TABLE 1 Key Terms and Descriptions

3.1 System Model
The scheduler consists of task observer, historical scheduling record (HSR), VMs data, resource monitor, and resource allocator. When the user submit tasks, the tasks join the queue of the entire system in descending order according to deadlines [30]. Upon the arrival of a task in scheduling process, the task observer classifies the task and determines the types of VM. The complete task classification process is given in Algorithm 1 (Section 4), which is based on Bayes classifier. A merging queue is generated when the task observer finalizes the task types and VM types from the HSR. In case of no similarity, new task types and VM types are created. If the VMs show any inability to satisfy scheduling requirements, such as deadline, resource requirements (CPU, RAM, Network Bandwidth, and Disk Storage), the resource allocator will scale up resources. The resource monitor checks the status of hosts in terms of capability to see whether it can accommodate the first and second task of the same type or not. The task merger prefers the completion of merging similar types of tasks together based on satisfactory conditions including deadline, resource requirements, and energy optimization until the same type of tasks are all processed in the merging queue. Furthermore, the hosts report their execution status of scheduling a task and its resource utilization to the scheduler directly. The resource allocator and resource monitor manage the status of all the physical hosts in CDC. In addition, if the host is under-loaded, it may lead to having some VMs stay idle for a long period of time. In this scenario, the resource allocator decides which VMs should be migrated to other hosts to boost resource utilization by scaling down resources. The whole system diagram is shown in Fig. 1.


Fig. 1.
System diagram.

Show All

3.2 Resource Model
A CDC consists of unlimited set of hosts H={H1,H2,…,Hi}, providing the physical infrastructure for creating virtualized resources to satisfy user's requirements. An active host set Hactive={Hactive1,Hactive2,…,Hactivei};Hactivei⊆Hi is characterized by Hi=((fi,ϑi),ci,mi), where (fi,ϑi)={(f1i,ϑi1),…,(fmaxi,ϑimax)} is discrete pairs of frequency and voltage of Hi, ci represents the CPU capability, mi represents the memory capacity, which are computed. A set of VM on Hi can be modeled as Vij={Vτj,Vφj}, where Vτj represents VM types defined in Section 3.6, and Vφj is VM requirement which is modeled as Vφj={Vpj,Vmj,Vnj,Vsj}, where Vpj,Vmj,Vnj and Vsj represent the parameters of processor, memory, network bandwidth and disk storage of VM, respectively.

3.3 Task Model
The task is a medium or container that expresses or holds requirements of users. It can be observed that there are primarily two types of task. The first one is a computing task which requires high CPU and low memory, and the second task that needs low CPU and high memory. Here, we have defined a set of task as Ŧ={Ŧ1,Ŧ2,Ŧ3,…,Ŧk} which includes a set of parameter Ŧφk={Ŧpk,Ŧmk,Ŧnk,Ŧsk}, where Ŧpk, Ŧmk, Ŧnk and Ŧsk represent the processing capacity, primary memory, network bandwidth and disk storage, respectively, and they are needed to execute a given task set. Similarly, HŦ, Ŧτk, Ŧζk represent the historical task record, task type and type count, respectively. In CDC environments, service providers get independent real-time tasks submitted by end-user in random time frame, and then the system accepts the service without understanding the complexity of computing infrastructure. An independent real-time task can be modeled as Ŧk=(tak,tlk,tdk), where tak,tlk and tdk represent the task arrival time, task work volume (i.e., total computing units), and task deadline, respectively. Cloud providers specify task parameters when users submit their tasks to them.

3.4 Energy Consumption Model
Generally, there are several components that consume energy, including hosts, electrical and network components, cooling system, storage devices. The main cause of energy consumption in a CDC is hosts [23], which mainly are determined by CPU, RAM, and disk storage. The energy consumed by Hi is categorized into static power (Pstatic) and dynamic power (Pdynamic), where idle host consumes approximately 70 percent of the energy consumed by the host running at the full CPU speed [23]. The total consumed power is the summation of both static and dynamic powers as shown below.
 Ptotal=Pstatic+Pdynamic.(1)
View SourceThis paper mainly focuses on the dynamic power, Pdynamic of CPU like [8], which is directly proportional to supply voltage squared (ϑdi)2 and its frequency fdi. Let αi be a coefficient of proportionality, Pdynamic of Hi which can be presented as follows, where ϑdi and fdi is proportional to the Pdynamic [23].
 Pdynamic=αi⋅(ϑdi)2⋅fdi=αi⋅(fdi)3.(2)
View SourceLet, Υi be the power ratio for the idle host HIdlei, fmaxi is Hi 's maximal frequency and Pmaxi is the maximum power consumed by Hi. The proportionality coefficient αi of Hi can be computed as
 αi=(1−Υi)⋅Pmaxi(fmaxi)3.(3)
View Source

The dynamic power Pdynamic can be computed from Equations (2) and (3) as follows:
 Pdynamic=(1−Υi)⋅Pmaxi(fmaxi)3⋅(fdi)3.(4)
View SourceRight-click on figure for MathML and additional features.So, the power of Hi can be expressed as
 Pi=Pstatic+Pdynamic=Υi⋅Pmaxi⋅Sti+(1−Υi)⋅Pmaxi(fmaxi)3⋅(fdi)3,(5)
View SourceRight-click on figure for MathML and additional features.where, Sti ⊆ (1, 0) indicates the status of running Hi at t, where Sti is 1 if Hi is active, and is 0, otherwise at time t. The total energy consumption Etotali of Hi from start time tsi to finish time tfi can be approximated as follows.
 Etotali=∫tfitsi(Υi⋅Pmaxi⋅Sti+(1−Υi)⋅Pmaxi(fmaxi)3⋅(fdi)3)dt.(6)
View SourceTherefore, the total energy consumption of physical hosts can be determined as follows.
Etotal=∑i=1nEtotali.(7)
View Source

3.5 Optimization Problems and Constraints
Let tskij and tfkij of Ŧk be the start time and finish time on Vij of Hi, and tek be the Ŧk execution time, then the finish time tfkij of Ŧk on Vijk of Hi can be calculated as follows.
 tfkij=tskij+tek.(8)
View SourceThe parameter Xkij is used to represent the mapping relation between Ŧk and Vj on Hi. The assignment variable Xkij is 1 when Ŧk is scheduled to Vj, otherwise, Xkij is 0. This is formally represented as follows.

Xkij={1,  if unicodex0166 k assigned to Vij0,  Otherwise.

The optimization problem can be formulated with aforementioned analysis as:

Maximum Guarantee Ratio =∑k=1m∑i=1n∑j=1|Vij|Xkijm

Minimum Total energy Consumption =∑i=1n∫tfitsi(Υi⋅Pmaxi⋅Sti+(1−Υi)⋅Pmaxi(fmaxi)3⋅(fdi)3)dt

Minimum Response Time =(Waiting Time + Execution Time + Migration Time)

Maximum Resource Utilization

=(∑|T−|k=1∑|Hactivei|i=1∑|Vj|j=1tlk⋅Xkij∑|Hactivei|i=1Ci⋅(tfi)−tsi)
View Source
3.5.1 Constraints
(fdi,ϑid) ⊆ (fi,ϑi), ∀ Hi⊆H

∀Ŧτk⊆Ŧk, ∀Ŧk⊆Ŧ

Xkij=1,0,∀VijΥi⊆Vj,∀Hi⊆H

∑mk=1∑ni=1∑|Vij|j=1Xkij≤1,∀Ŧk⊆Ŧ

tfkij≤tdk

Here, we consider four optimization objectives to minimize the total energy consumption and the mean response time, while maximizing the task guarantee ratio and the utilization ratio.

3.5.2 Energy Optimization
Suppose, ξ is the computation size of the task Ŧk having utilization of frequency fdi of the host Hi. Then, the execution time tek can be calculated as the ratio of ξ and fdi, i.e., tek=ξ/fdi. Thus, Etotali of Hi can be expressed as follow.
 Etotali=Υi⋅Pmaxi⋅ξfdi+(1−Υi)⋅Pmaxi(fmaxi)3⋅(fdi)3⋅ξfdi.(10)
View Source

For optimization,
 −Υi⋅Pmaxi⋅ξ(fdi)2+2⋅(1−Υi)⋅Pmaxi(fmaxi)3⋅fdi⋅ξ=0.(11)
View Source

Based on the above equations, the optimal frequency of Hi can be calculated as
fdi=3Υi2.(1−Υi)(fmaxi)3−−−−−−−−−−−−−−−√.(12)
View SourceRight-click on figure for MathML and additional features.Optimal frequency (foi) of Hi can execute in set of discrete frequency, i.e., fdi⊆fi={f11,f22,…,fmaxi}, which can be defined with maximum power Pmaxi in the following manner.

foi=f1i, if fdi≤f1i,

foi=|fdi|, if f1i≤fdi≤fmaxi, where |fdi| represents the frequency that is more than foi and nearest of f_{i}^{max}, f_{i}^o=f_{i}^{max}, if f_{i}^d \geq f_{i}^{max}.

The ratio of power frequency (R_{f}^P) can be computed as below. \begin{equation*} \ R_{f}^P=\frac{f_{i}^o}{E_{i}}=\frac{f_{i}^o}{\Upsilon _{i} \cdot P_{i}^{max} + \frac{((1-\Upsilon _{i})\cdot P_{i}^{max})}{(f_{i}^{max})^3} \cdot (f_{i}^o)^3}. \tag{13} \end{equation*}
View SourceRight-click on figure for MathML and additional features.

The observed scenario indicates that the larger R_{f}^P of host denotes the higher energy efficiency in which case the system schedules to execute the real-time heterogeneous tasks when its workload decreases. This, in turn, results in powering off the host with lower R_{f}^P first.

3.6 Task and Virtual Machine Mark Model
The task and virtual machine mark model has two operations, i.e., categorizing randomly arriving tasks to define the best VM types for mapping with the most suitable VMs [44] [45], and obtaining historical data set from HSR to classify the tasks, and generating VM types. Let H_{{\unicode{x0166}}} be the historical task set that is extracted from HSR in CDC. Let H_{{\unicode{x0166}}}^{\zeta } represents the historical task type count, then the ratio P(H_{{\unicode{x0166}}}^{\zeta }) is given as \begin{equation*} \ P(H_{{\unicode{x0166}}}^{\zeta })=\frac{|H_{{\unicode{x0166}}}^{\zeta }|}{|H_{{\unicode{x0166}}}|}. \tag{14} \end{equation*}
View SourceRight-click on figure for MathML and additional features.Based on our resource model and task model, the matching degree P({\unicode{x0166}}_{k}^\varphi |V_{j}^\varphi) is calculated as \begin{equation*} \ P({\unicode{x0166}}_{k}^\varphi |V_{j}^\varphi)= \left\lbrace \begin{array}{lc}(V_{j}^\varphi /{\unicode{x0166}}_{k}^\varphi)^2, If ({\unicode{x0166}}_{k}^\varphi > V_{j}^\varphi) \\ (V_{max}^\varphi - V_{j}^\varphi + {\unicode{x0166}}_{k}^\varphi)/V_{max}^\varphi, Otherwise \end{array}\right., \tag{15} \end{equation*}
View SourceRight-click on figure for MathML and additional features.where V_{max}^\varphi =._{\tau \subseteq U}^{max} V_{\tau }^\varphi.

For us suppose that V_{j}^1 =12, V_{\tau }^1=8, V_{max}^1=100, and {\unicode{x0166}}_{k}^\varphi =10.

According to Equation (15),

\ P({\unicode{x0166}}_{k}^1|V_{j}^1)= (V_{max}^1 - V_{j}^1+ {\unicode{x0166}}_{k}^1)/V_{max}^1=0.98

\ P({\unicode{x0166}}_{k}^1|V_{\tau }^1)=(V_{\tau }^1/V_{max}^1)^2=0.64

\ P({\unicode{x0166}}_{k}^1|V_{max}^1)=(V_{max}^1 - V_{max}^1 +{\unicode{x0166}}_{k}^1)/V_{max}^1=0.1

We can see in the above outcome, 0.98>0.64>0.1. Note that if P({\unicode{x0166}}_{k}^\varphi |V_{j}^\varphi)=1, it means we have a perfect match. Thus, the result is that {\unicode{x0166}}_{k}^1 has a better match with V_{j}^1, rather than V_{\tau }^1 and V_{max}^1. By using the Bayes Classifier [46], we can calculate the probability of {\unicode{x0166}}_{k} belonging to {\unicode{x0166}}_{k}^\tau, \begin{equation*} \ P({{\unicode{x0166}}_{k}^\varphi }^{^{\prime }}|{V_{j}^\varphi }^{^{\prime }})= P({\unicode{x0166}}_{k}^p|V_{j}^p)P({\unicode{x0166}}_{k}^m|V_{j}^m). \tag{16} \end{equation*}
View SourceThen, the decision function of {\unicode{x0166}}_{k} is \begin{equation*} \ \lbrace (j^{^{\prime }},k^{^{\prime }})=arg \; max \; P({{\unicode{x0166}}_{k}^\tau }^{^{\prime }}|P({{\unicode{x0166}}_{k}^\varphi }^{^{\prime }}) \rbrace . \tag{17} \end{equation*}
View Source

3.7 Task Merging Model
We define two tasks sequence, the same type {\unicode{x0166}}_{k}^{\tau }, and {\unicode{x0166}}_{k+m_{n}}^{\tau } the same VM types. Here, k represents the sequence number of task, and m=\lbrace m_{1}, m_{2}, \ldots m_{n} \rbrace represents the queue length from one task to another task of the same type, which is not necessarily has to be equal. The task series of the same task type is given as {\unicode{x0166}}_{k}^{\tau }, {\unicode{x0166}}_{k+m_{1}}^{\tau }, {\unicode{x0166}}_{k+m_{2}}^{\tau }, ..., {\unicode{x0166}}_{k+m_{n}}^{\tau }, as shown in Fig. 2.


Fig. 2.
Task series of same task types.

Show All

The system keeps all tasks in merging queue which were generated by the task observer. The task observer categories the task based on HSR, then it maps them with the most suitable VM. The merging queue is generated by the task observer with task types and VM types. Merging queue is sorted with the decreasing order of deadline.

Let t_{k}^e, t_{k+m_{1}}^e, t_{k+m_{2}}^e, ...be the execution time of task series of tasks of the same type, as shown in Fig. 3. When two tasks {\unicode{x0166}}_{k}^1 and {\unicode{x0166}}_{k+m_{1}}^1 get merged, then the execution time of merged tasks {\unicode{x0166}}_{k}^{^{\prime }} is given as {\unicode{x0166}}_{k}^e = {\unicode{x0166}}_{k+m_{1}}^e. The deadline of task {\unicode{x0166}}_{k}^{^{\prime }} will be t_{k}^d because tasks {\unicode{x0166}}_{k}^1 must be finished before the deadline t_{k}^d.


Fig. 3.
The process of task merging.

Show All

Example.
Majority of the applications in cloud environments are dynamically submitted by end users and are specified with deadlines to ensure their timeliness requirements in many domains, including scientific visualization, real-time signal processing and so forth. Suppose, {\unicode{x0166}}_{k}^1, {\unicode{x0166}}_{k+m_{1}}^1, {\unicode{x0166}}_{k+m_{2}}^1, \ldots are the task series of task type 1 from Merging Queue (MQ) and V_{j}^1, V_{j+n_{1}}^1, V_{j+n_{2}}^1, \ldots are the VM series of the same type. The deadline of {\unicode{x0166}}_{k}^d is very near because MQ is sorted with decreasing order of deadline. First, if {\unicode{x0166}}_{k}^1 can be schedule to V_{j}^1 with the deadline t_{k}^d, it checks the same task type successor {\unicode{x0166}}_{k+m_{1}}^1. If {\unicode{x0166}}_{k}^1 + {\unicode{x0166}}_{k+m_{1}}^1 can finish its execution before t_{k}^d, it checks for another successor of the same type {\unicode{x0166}}_{k+m_{2}}^1. If {\unicode{x0166}}_{k}^1 + {\unicode{x0166}}_{k+m_{1}}^1 + {\unicode{x0166}}_{k+m_{2}}^1 is possible to be scheduled within t_{k}^d, the process continues until the same task type finishes. If {\unicode{x0166}}_{k}^1 + {\unicode{x0166}}_{k+m_{1}}^1 cannot be finished before t_{k}^d, it scales up resources and schedule. If {\unicode{x0166}}_{k}^1 can not be scheduled within t_{k}^d, it scales up resources and check whether the scheduling is possible or not, if it is possible, it performs the scheduling, otherwise, it rejects the tasks.

3.8 Migration Model
Migration is widely used in CDCs to optimize resource utilization at the host level for cloud resource management [45]. Basically there are two types of migrations, offline migration: it refers to moving a suspended VM from one host, and live migration: it refers to moving a running VM from one host to another.

In this paper, the live migration is solely considered due to two main reasons: first, it is more efficient from a performance point of view as it is able to transfer a VM between hosts with a close to zero downtime [47]. Second, it is the most adopted VM migration type in modern VM managers. The length of a live migration depends on the total CPU size, memory size, disk storage used by VM, and the available network bandwidth. During live migration, the CPU state context of the VMs are switched from the source host to destination host. As a result, there will be small data to be transferred, and represents the lowest limit for minimizing the migration time. Similarly, the VMs memory state also needs to be transferred to the destination host. This information may be greater than the CPU state, which includes the memory state of guest OS, and all the running processes within the VM.

Let f_{i}^{th} be the threshold frequency of hosts. If the host's resources are utilized to a very low extend (frequency of host (f_{i}^{d}) is less than or equal to threshold frequency (f_{i}^{th}) and greater than optimum frequency f_{i}^{o}, i.e., f_{i}^{o} < f_{i}^d \leq f_{i}^{th}), then a migration mechanism can be used to migrate running virtual machine from less utilized hosts to high utilized hosts (frequency of host (f_{i}^d) is greater than or equal to threshold frequency (f_{i}^{th}) and less then or equal to maximum frequency (f_{i}^{max}), i.e., f_{i}^{th} < f_{i}^{d} \leq f_{i}^{max}).

The VM migration time is very important factor for energy dissipation. Therefore, time must be considered before taking any migration-related decision. The total migration time, depends on the number of VMs to be migrated, and the size of VMs. However, the total number of VMs to be migrated, and the size of each VM cannot be known in advance except in a probabilistic sense [48]. The time cost of migrating of a single VM can be expressed in terms of the statistics of the CPU content size, RAM size, and storage size of this VM, as well as the available bandwidth at the time of migration. It is important to stress that the RAM size is the main parameter that affects the VM migration time. The migration time is given in Equations (18) and (19).

The VM Migration Time (t_{V_{j}}^{Mig}) can be computed as \begin{equation*} t_{V_{j}}^{Mig} = (V_{j}^p + V_{j}^m)/ V_{j}^n, \tag{18} \end{equation*}
View Sourcewhere, V_{j}^n is the available bandwidth for VM V_{j}, in Bytes/Seconds between the source and destination hosts.

The total VM Migration Time can be computed as \begin{equation*} t_{V}^{Mig}=\sum \limits _{j=1}^{|V^{Mig}|} t_{V_{j}}^{Mig}. \tag{19} \end{equation*}
View Source

In evaluating the total VM migration time t_{V}^{Mig}, the values of t_{V_{j}}^{Mig} are statistically independent for all VMs, where V^{Mig} denotes the number of migrated VMs.

The energy consumed for VM migration can be computed by \begin{equation*} E_{V_{j}}^{Mig}=P^{Mig} * t_{V_{j}}^{Mig} =P^{Mig} \ast (V_{j}^p + V_{j}^m)/ V_{j}^n, \tag{20} \end{equation*}
View Sourcewhere, P^{Mig} denotes a unit power consumption for migrating a VM.

Let Cost^E be the energy cost per unit, cost^P is the SLA violation fixed penalty value, \omega _{V_{j}} is the portion of k task's served by H_{i}^{target} and \lambda _{H_{i}} is the average request rate for H_{i}^{target}. Fig. 4 indicates the migration cost model for the proposed system. Then, the migration cost, Cost^{Mig}, can be calculated using \begin{align*} & Cost^{Mig}= Cost^E * P_{{dynamic}_{i}} \sum \limits _{j=1}^{|V^{Mig}|} \phi _{j} \\ & + \sum \limits _{j=1}^{|V^{Mig}|} cost_{ij}^P * \omega _{V_{j}} * \lambda _{i} * e^{-(((V_{j}^p + V_{j}^m)/ V_{j}^n) * \phi _{j} * \mu _{i} - \lambda _{i})} * t_{{V_{ij}}^{contract}}. \tag{21} \end{align*}
View Source


Fig. 4.
The migration cost model.

Show All

SECTION 4Algorithm Design
4.1 Overview
Task scheduling and resource management are the striking means to optimize energy consumption for CDCs to ensure the timing requirements of tasks. The scheduling strategies are used to schedule real-time tasks dynamically. Scheduling process deals with various disruptions during system operation, including the arrival of new tasks or just-completed tasks. In such events, the workload of CDC becomes heavy and will lead to a scale-up situation to consolidate the available physical and virtual resources based on the requirements of tasks.

The main method energy-efficient dynamic scheduling EDS () is presented in Algorithm 2, which has four methods. MARKVM () is used to calculate various task and virtual machine types. Similarly, TASKMERGE () is used to merge same type of tasks. The SCALEUPRESOURCES() and CONSOLIDATION() methods are called multiple times when system dynamically scale up and scale down computing resources, respectively.

4.2 Description
When users submit their tasks {\unicode{x0166}}_{k}, EDS gathers the information in Algorithm 2, which includes the arrival time t_{k}^a and deadline t_{k}^d. Then, these tasks join the Task Queue of the entire system according to the decreasing order of deadlines, and MARKVM () method (Algorithm 1) will be called. Algorithm 1 is based on historical scheduling records and Bayes classifier. It mainly consists of two parts. The first part is classifying the upcoming tasks, and second part is marking the virtual machines. Within Algorithm 1, lines 3-4 gather the information for tasks from historical scheduling records, and then count the number of task types. Task observer checks whether the task type for a given task is available or not. If the task type was found, it would mark the task type and VM types (lines 9-11), otherwise it would mark {\unicode{x0166}}_{k} as new {\unicode{x0166}}_{k} ^{\tau } and V_{j}^{\tau }, and update HSR (Line 13).

Algorithm 1. Task and Virtual Machine Types
procedure MarkVM()

Input: Task {\unicode{x0166}}_{k}, HSR

H_{{\unicode{x0166}}} \leftarrow Historical Task Set

\zeta \leftarrow Task type count

For each task T_{k} in \zeta

Compute P({H_{{\unicode{x0166}}_{k}}})

End for

a \leftarrow Choose High P(H_{{\unicode{x0166}}_{k}})

For each VM V_{j} in a

If V_{j}^\tau found of {\unicode{x0166}}_{k}^\tau type then

V_{j}^\tau \leftarrow VM types based on P(H_{{\unicode{x0166}}_{k}})

Else

Create New V_{j}\lbrace V_{j}^p, V_{j}^m, V_{j}^n, V_{j}^s \rbrace, Create New V_{j}^\tau, Update HSR

End If

End For

end procedure

In Algorithm 2, Line 6, the virtual machines are sorted based on the ratio of power frequency, R_{f}^P. If V_{j}^\varphi > {\unicode{x0166}}_{k}^\varphi is satisfied, all the tasks of {\unicode{x0166}}_{k}^\varphi type will be sequencing to Merging Queue (MQ) (Lines 8-11). Following this step, method TASKMERGING() will be invoked in Line 13.

In the task merging method, as shown in Algorithm 3, similar type of tasks can be merged and scheduled to VM of the same VM type. We merge as many the same typed tasks as possible and assign them to a VM that can satisfy the resource requirement. If the execution time of tasks {\unicode{x0166}}_{k}^{\tau } and {\unicode{x0166}}_{k+m_{1}}^{\tau } can be scheduled within the deadline t_{k}^d and resource requirements are satisfied, two tasks {\unicode{x0166}}_{k}^{\tau } and {\unicode{x0166}}_{k+m_{1}}^{\tau } will be merged and scheduled to the VM, as shown in Fig. 3. Then, the new execution time and deadline for a merged task {{\unicode{x0166}}_{k}^\tau }^{^{\prime }} becomes {t_{k}^e}^{^{\prime }}=t_{k}^e+t_{k+m_{1}}^e, {t_{k}^d}^{^{\prime }}=t_{k}^d, respectively (Lines 4-7). Otherwise, the resources will be scaled up, and accordingly scheduled (Lines 9-11).

In Algorithm 4, while scaling up the resources, all the active hosts are sorted based on R_{f}^P (Lines 3-6). If the requirements of the task can be accommodated by active hosts, V_{ij} of V_{j}^{\tau } is created and HSR is updated (Lines 6-8). Otherwise, a new host will be powered on from H_{List}^{off} (power off host list) based on the decreasing order of R_{f}^P, then new V_{j} of V_{j}^{\tau } will be created and HSR will be updated as well (Lines 14-17).

The workload of CDC naturally decreases if the arrival ratio of real-time tasks from end-users declines. When the idle time of VMs t_{V_{ij}}^{Idle} are higher than the t^{Idle}, the consolidation method will apply to minimize the running VMs and hosts, as shown in Algorithm 5. If t_{V_{ij}}^{Idle} of V_{ij} exceeds t^{Idle}, V_{ij} will be deleted (Lines 2-7), and if H_{i}^{active} for deleted V_{ij} becomes idle, H_{i} will be powered off (Lines 11-13). In addition, the virtual machines can be migrated from less utilized hosts (hosts having f_{i}^d \geq f_{i}^o AND f_{i}^d \leq f_{i}^{th}) to high utilized hosts (hosts having f_{i}^d > f_{i}^{th} AND f_{i}^d \leq f_{i}^{max}), and idle hosts H_{i}^{Idle} (hosts having f_{i}^d < f_{i}^o) are powered off to reduce energy consumed by hosts in the CDC (Lines 8-27).

Algorithm 2. Energy-Efficient Dynamic Scheduling
procedure EDS()

Input: Task {\unicode{x0166}}_{k}, a set of VM V_{j}, and set of host H_{i}

V_{j}^\tau \leftarrow MARKVM()

\tau \leftarrow Choose {\unicode{x0166}}_{k}^\tau

Set \delta \leftarrow 0

Order VMs of V_{j}^\tau types with descending order of R_{f}^P

For V_{j}^\tau in list

If {V_{j}^\varphi > {\unicode{x0166}}_{k}^\varphi } then

P({{\unicode{x0166}}_{k}^\tau }^{^{\prime }}|{{\unicode{x0166}}_{k}^\varphi }^{^{\prime }})= ({\unicode{x0166}}_{k}^p|V_{j}^p)({\unicode{x0166}}_{k}^m|V_{j}^m)

\delta \leftarrow 1

MQ \leftarrow List all the task of \tau types

{{\unicode{x0166}}_{k}^\tau }^{^{\prime }} ={\unicode{x0166}}_{k}^\tau, {t_{k}^e}^{^{\prime }}=t_{k}^e, {t_{k}^d}^{^{\prime }}=t_{k}^d

TASKMERGING()

Schedule {{\unicode{x0166}}_{k}^\tau }^{^{\prime }} to V_{ij}, Update HSR, Remove all merged tasks from merging queue

End If

Else

SCALEUPRESOURCES()

If V_{j}^\varphi > {\unicode{x0166}}_{k}^\varphi then

Go to line 9

End If

End If

End For

If \delta =0 then

V_{j}^\tau \leftarrow VM types [Create new VM types V_{j}^\tau], Update HSR

If t_{k}^e \leq t_{k}^d then

TASKMERGING()

Else

Close Created VM

Reject {\unicode{x0166}}_{k}

End If

End If

CONSOLIDATION()

end procedure

Algorithm 3. Task merging
procedure TaskMerging()

Input: List all the task of \tau types {{\unicode{x0166}}_{k}^\tau }^{^{\prime }} ={\unicode{x0166}}_{k}^\tau, {t_{k}^e}^{^{\prime }}=t_{k}^e

For each {\unicode{x0166}}_{k}^\tau in MQ

Calculate the execution time of task {\unicode{x0166}}_{k+m_{n}}^\tau

If ({t_{k}^e}^{^{\prime }}+t_{k+m_{n}}^e) \leq t_{k}^d AND Satisfy resource requirements then

{{\unicode{x0166}}_{k}^\tau }^{^{\prime }}= {{\unicode{x0166}}_{k}^\tau }^{^{\prime }}+ {\unicode{x0166}}_{k+m_{n}}^\tau, {t_{k}^e}^{^{\prime }}={t_{k}^e}^{^{\prime }} + t_{k+m_{n}}^e

Else

SCALEUPRESOURCES()

If ({t_{k}^e}^{^{\prime }}+t_{k+m_{n}}^e) \leq t_{k}^d then

{{\unicode{x0166}}_{k}^\tau }^{^{\prime }}= {{\unicode{x0166}}_{k}^\tau }^{^{\prime }}+ {\unicode{x0166}}_{k+m_{n}}^\tau, {t_{k}^e}^{^{\prime }}={t_{k}^e}^{^{\prime }} + t_{k+m_{n}}^e

Else

Break

End If

End If

End For

end procedure

Algorithm 4. Scale up Resources
procedure ScaleUpResources()

Select a V_{ij} that can finish task {\unicode{x0166}}_{k} before task deadline

If V_{ij} != NULL then

H_{List}^{active} \longleftarrow Active hosts

Sort H_{List}^{active} by R_{f}^P in decreasing order

For each H_{i} in H_{List}^{active}

If H_{i} can accommodate V_{ij} then

Create V_{ij} of V_{j}^{\tau } on H_{i}^{target}, Update HSR

Break

End If

End For

End If

Select a VM V_{ij} that satisfy V_{j}^{{\varphi }{^{\prime }}} \geq{\unicode{x0166}}_{k}^{{\varphi }^{{\prime }}} and t_{k}^e \leq t_{k}^d

If V_{ij} != NULL, then

H_{List}^{off} \leftarrow all the host being powered off

Sort H_{List}^{off} by R_{f}^P in decreasing order

Power on H_{i} from H_{List}^{off}, and Create new VM, H_{List}^{active} \leftarrow H_{i}, Update HSR

End If

end procedure

Algorithm 5. Scale Down Resources
procedure Consolidation()

V_{j}^{active} \longleftarrow all the active VMs

For V_{j} in V_{j}^{active} list

If t_{V_{ij}}^{Idle} \leq t^{Idle}

Delete V_{ij}

End If

End For

H_{List}^{active} \longleftarrow All the active hosts

List H_{List}^{active} by R_{f}^P in decreasing order

a \longleftarrow Select all H_{List}^{active} having f_{i}^d \leq f_{i}^{th}

For each host H_{i} in a

If H_{i} is idle then

Power off H_{i}

Else

b \longleftarrow Select all the VMs from H_{i}

For VM V_{j} in b

H_{i}^{target} \longleftarrow Select all the active hosts having f_{i}^d > f_{i}^{th} AND f_{i}^d \leq f_{i}^{max}

For each in H_{i}^{target}

If H_{i}^{target} can accommodate V_{j} then

Migrate V_{j} from H_{i} to H_{i}^{target}

End If

Break

End For

Remove V_{j} from H_{i}

End For

Power off H_{i}

End For

end procedure

Example.
Suppose users submit their tasks, {\unicode{x0166}}_{k}, to the cloud, for which these tasks have various deadline and heterogeneity. Scheduling scheme sequences these tasks to the task queue based on decreasing order of deadlines. A task set in the task queue is {\unicode{x0166}}=\lbrace {\unicode{x0166}}_{1}, {\unicode{x0166}}_{2}, {\unicode{x0166}}_{3}, \ldots, {\unicode{x0166}}_{k} \rbrace . The task observer checks similar information in historical scheduling record and VMs data, such as task arrival ratio, deadline, execution time, virtual machine information (CPU, RAM, Network Bandwidth, Disk Storage), and resource requirements (CPU, RAM, Network Bandwidth, Disk Storage). As mentioned earlier, the task classification is performed based on Bayes classifier. If the historical scheduling record is found, the decision will be to sequence these tasks in the merging queue with task type. If the task observer is unable to find any historical scheduling information which matched with tasks, the decision will be to create a new virtual machine, mark it as new task type and virtual machine types, and update HSR. Assume {\unicode{x0166}}_{1} has task type 1 and VM type 1, {\unicode{x0166}}_{2} has task type 3 and VM type 3, {\unicode{x0166}}_{3} has task type 1 and VM type 1, {\unicode{x0166}}_{4} has task type 1 and VM type 1, as so on. Then, scheduler takes {\unicode{x0166}}_{1}^1 ( {\unicode{x0166}}_{1} with type 1) for the merge process. The scheduler sorts all the virtual machine of type 1 with descending order of R_f^P. Then, it checks whether V_{1}^1 resources can accommodate and schedule {\unicode{x0166}}_{1}^1 within the deadline or not. If V_{1}^1 can accommodate {\unicode{x0166}}_{3}^1, scheduler sees the next task of the same task type 1. It also checks again whether V_{1}^1 can accommodate both tasks or not. If both tasks can be accommodated, it will merge these two tasks and the deadline for merged task { {\unicode{x0166}}_{1}^{1}}^{^{\prime }} of {\unicode{x0166}}_{1}^1 and {\unicode{x0166}}_{3}^1 becomes the deadline of {\unicode{x0166}}_{1}^1 (shortest deadline). If both tasks cannot be accommodated by V_{1}^1, then the scheduler scales up resources and checks again whether V_{1}^1 can accommodate both tasks or not. If yes, the merging will take place based on the schedule of {\unicode{x0166}}_{1}^1 and {\unicode{x0166}}_{3}^1 to V_{1}^1. Otherwise, it just schedules only {\unicode{x0166}}_{1}^1 to V_{1}^1.

4.3 Time Complexity Analysis
There are four methods running at different times during the scheduling scheme of Algorithm 2, i.e., the classification method described by Algorithm 1, the task merge method described by Algorithm 3, the Scale-up method that when some new host needs to be powered on in order to host new tasks described by Algorithm 4, and the consolidation method described by Algorithm 5.

Algorithm 1 has time complexity of O(n), where n is the number of tasks, since each task needs to be checked and given a task type and a VM type according to its resource requirements. Let m denote the number of the active hosts, and the complexity of Algorithm 3 is O(n*m\;log\;m). This is because Algorithm 3 may call upon Algorithm 4 for each task in the merging queue, and the complexity of Algorithm 4 is O(mlogm), since it needs to sort the active hosts in line 5 and the powered off hosts in line 16 of Algorithm 4.

After running for a period of time, consolidation needs to be performed, using Algorithm 5 Scale Down Resources, such that energy efficiency can be achieved. The hosts can be considered in three states. The first part is idle hosts, where they have nothing to do while still powered on; the second part is the less utilized hosts (i.e., they have very few tasks to execute, and the tasks can be migrated to the third part of hosts); and the third part of the host is the one that can accept newly migrated tasks. Let b be the number of VMs that need to be migrated from part two to part three. Since we need to find a suitable host in the third part of hosts for each VM in the second part of the hosts to migrate, the complexity of Algorithm 5 is O(m*b).

The complexity of ETVMC [49] is O(n*m*b), where n is the number of tasks, m is the number of hosts, and b is the number of VMs. Similarly, the complexity of CEVP [50] and UMA [32] are O(n^2) and O(n^2), respectively.

SECTION 5Experiments
This section describes the overall experimental setup, performance metrics, results, and analysis to evaluate the EDS. We compared the performance of EDS with ETVMC [49], CEVP [50], and UMA [32] algorithms as all of them are designed for energy optimization.

5.1 Environment Setup
Experimental environment includes CPU (Intel(R) Core(TM) i5-3230 M, 2.60 GHz), Memory (8.0 GB), Hard Disk (750 GB), Windows 10 operating system, NetBeansIDE, JDK 8.0 and CloudSim. Powered by CloudSim toolkit [51], our simulation carried out the dynamic scheduling process, the mechanism of merging similar type of tasks and energy-aware techniques.

5.2 Performance Metrics
To simulate task's heterogeneity and dynamic nature in the CDC environment, we have used a total of 898,766 continuous tasks, and 8500 hosts. In the simulation, each host is modeled to have only one CPU core with having 1,000 MIPS, 1,500 MIPS, and 2,000 MIPS CPU performance. The host specifications are presented in Table 2. Based on our simulation setup, we have identified four performance metrics which can be computed by Formula (9). These metrics include

TABLE 2 Simulated Host Specification

Task guarantee ratio: The proportion of the number of tasks finished before their deadlines to the total number of tasks × 100 percent

Total energy Consumption: It represents the total energy consumed by the physical hosts while scheduling tasks.

Mean Response Time: Optimal time taken to response to user.

Resource Utilization: It represents the total utilization of resources while scheduling tasks.

5.3 Experimental Results and Analysis
Table 3 describes the CPU and memory demands for each type of tasks, and Table 4 describes the CPU and memory capacity of each type of VMs. It is important to note that, in the fiber optic network as well as hard disk provision in CDCs, the network bandwidth and the hard disk capacity are not critical elements, and hence they are typically treated in a way that they will have the perfect match with all the tasks. For this reason, the hard disk and the network bandwidth were not considered in our analysis.

TABLE 3 Task Types with Parameter

TABLE 4 VM Types with Parameter

As mentioned earlier in Section 2, once again, the existing approaches for energy-efficient task scheduling in CDCs do not investigate the problems of combining different tasks of the same type in their underlying techniques. These approaches mainly focus on either a particular task type, or simply do not take into consideration a different kind of application that assumes a homogeneous task.

The metrics under study were measured and experiment results are shown in Figs. 5 to 14. A comparative summary of the total energy consumption of the proposed EDS and three peer algorithms is presented in Fig. 5, which shows that EDS has an optimal total energy consumption followed by CEVP, ETVMC, and UMA. Specifically, EDS saves about 28.49, 21.48, and 31.04 percent of energy compared to ETVMC, CEVP, and UMA, respectively. This performance is achieved because EDS uses a consolidation technique that performs simultaneous optimizations on the active number of hosts and the operating frequencies to minimize energy consumption.


Fig. 5.
Total energy consumption.

Show All


Fig. 6.
Total energy consumption with task count.

Show All


Fig. 7.
Total energy consumption of task type with task count.

Show All


Fig. 8.
Task guarantee ratio.

Show All


Fig. 9.
Task guarantee ratio with task count.

Show All


Fig. 10.
Task guarantee ratio of task types with task count.

Show All

Fig. 11. - 
Total mean response time.
Fig. 11.
Total mean response time.

Show All

Fig. 12. - 
Total mean response time with task count.
Fig. 12.
Total mean response time with task count.

Show All


Fig. 13.
Total mean response time of task types with task count.

Show All


Fig. 14.
Resource utilization.

Show All


Fig. 15.
Resource utilization with task count.

Show All

Fig. 6 exhibits the total energy consumption of the four algorithms with respect to task count. As shown in the figure, the computation length of task required to be processed is linear to the number of tasks. Similarly, the total energy consumption of the CDC is nearly linear to the computation length of the system.

In every task count, EDS achieves better energy optimization. For example, as shown in Fig. 7, type 1 consumes the lowest energy while task count reaches 1.5\times 10^5. Similarly, type 1 and type 8 consume the highest energy while task count reaches 9\times 10^5.

Task guarantee ratios of the physical and virtual resources of the four algorithms are shown in Fig. 8. The guarantee ratio for the EDS, ETVMC, CEVP, and UMA are stable in 96.12, 88.79, 93.78 and 87.57 percent with respect to the rise of task counts. Fig. 9 shows the task guarantee ratio with the task count of the four algorithms where the EDS shows to have a higher task guarantee ratio followed by CEVP, ETVMC and UMA. Similarly, Fig. 10 indicates the task guarantee ratio with the task count, in which type 6 has the highest guarantee ratio when task count reaches about 9\times 10^5.

This particular observation can be associated to the fact that there are adequate computing resources in CDC. Hence, if the available resources can not accommodate the requirements of a task, accordingly more VMs will be created and then added to run the workload in excess with extra hosts powered-on as necessary. On the other hand, the task guarantee ratio is not perfect as 100 percent. This phenomenon can be attributed to the delay of scaling up and consolidating resources.

Fig. 11 indicates the mean response time of the four algorithms where the EDS apparently showed to have the optimal mean response time. The reason for this out-performance is that EDS benefits from a classification method to minimize iterative communication time for selecting the most suitable physical hosts and VMs. The total mean response time with the task count are shown in Fig. 12. EDS has the lowest mean response time when task count reaches 3\times 10^5.

Fig. 13 shows the mean response time of task types with different task counts. The mean response time of type 3 is lowest when the task count reaches 3\times 10^5. Similarly, the mean response time peaks when type 7 is at task count 6\times 10^5.

We also observed that the resource utilization of the four algorithms ascend accordingly. Fig. 14. shows that the EDS has a higher resource utilization compared to its peers. This may be due to the fact that EDS employs task merging strategies to allocate similar type of tasks to most suitable physical hosts and VMs, and it uses a consolidation technique to enhance host operating frequencies and the number of active hosts simultaneously that will result in reducing unnecessary resource wastage.

Fig. 15 illustrates the resource utilization percentages with the task count. As observed from the figure, the resource utilization is at a higher level when the task count reaches 7.5\times 10^5 and 9\times 10^5.

We will now provide a comparative summary of the waiting time of the proposed scheduling scheme and other existing algorithms. As shown in Fig. 16, EDS outperforms the other schemes, as their waiting times are linear to the number of tasks. The waiting time of EDS is lowest when the task count reaches 1.5\times 10^5 and 7.5\times 10^5. Similarly, the waiting time is at a higher level when the task count reaches 9\times 10^5.

Fig. 16. - 
Waiting time with task count.
Fig. 16.
Waiting time with task count.

Show All

SECTION 6Conclusion and Future Work
Energy consumption and optimization are ongoing (operational) challenges faced by CDCs. In this research, the proposed EDS aims to optimize resource utilization and energy consumption in CDCs. In the EDS, the dynamic scheduling scheme makes use of HSR, classifies incoming tasks, creates VMs with different features, and assigns tasks in accordance with the matching features of the tasks and VMs; hence, the resource utilization ratio can be improved. The merging mechanism of same task types minimizes the mean response time and reduces total energy consumption. In addition, the scheme distributes and migrates tasks in accordance to the energy consumption function and the states of the computing resources (active or idle). The experimental results indicated that the EDS has a higher task guarantee ratio, shorter response time, higher resource utilization ratio, and a minimal energy consumption, in comparison to existing schemes.

In the future, we intend to investigate how task failure prediction based on machine learning approach, and task scheduling can be jointly optimized for better performance in the context of fault-tolerant mechanism.