The ability to support multitasking becomes more and more important in the development of graphic processing unit (GPU). GPU multitasking methods are classified into three types: temporal multitasking, spatial multitasking, and simultaneous multitasking (SMK). This article first introduces the features of some commercial GPU architectures to support multitasking and the common metrics used for evaluating the performance of GPU multitasking methods, and then reviews the GPU multitasking methods supported by hardware architecture (i.e., hardware GPU multitasking methods). The main problems of each type of hardware GPU multitasking methods to be solved are illustrated. Meanwhile, the key idea of each previous hardware GPU multitasking method is introduced. In addition, the characteristics of hardware GPU multitasking methods belonging to the same type are compared. This article also gives some valuable suggestions for the future research. An enhanced GPU simulator is needed to bridge the gap between academia and industry. In addition, it is promising to expand the research space with machine learning technologies, advanced GPU architectural innovations, 3D stacked memory, etc. Because most previous GPU multitasking methods are based on NVIDIA GPUs, this article focuses on NVIDIA GPU architecture, and uses NVIDIA's terminology. To our knowledge, this article is the first survey about hardware GPU multitasking methods. We believe that our survey can help the readers gain insights into the research field of hardware GPU multitasking methods.
SECTION 1Introduction
Graphic processing unit (GPUs) have been widely used in various general-purpose computing systems, such as data center [1], cloud computing platform [2], etc. These systems need to provide computing services for multiple concurrent applications. If the application that does not occupy the entire GPU shares a GPU with other applications, the utilization of GPU resources will be dramatically improved. Moreover, it is critical to release GPU resources occupied by low-priority applications to high-priority applications for achieving the quality-of-service (QoS) target. Therefore, GPU multitasking is essential and significant. Many application areas that need to share GPUs can benefit from GPU multitasking. For instance, the inference efficiency of multiple concurrent neural networks is improved by spatial sharing of the GPU in edge clouds [3]. GPU multitasking becomes more and more important in the evolution of GPU architecture, and attracts much attention from both academia and industry. This paper gives a survey of GPU multitasking methods supported by hardware architecture.

A GPU is composed of multiple streaming multiprocessors (SMs). Based on the manners that kernels from diverse applications share SMs, GPU multitasking methods can be classified into three types: temporal multitasking, spatial multitasking, and simultaneous multitasking (SMK). Temporal multitasking partitions GPU time among different kernels; in other words, the GPU is time-multiplexed. Spatial multitasking splits the GPU resources at the SM granularity. SMs of a GPU are divided into several groups; and each group executes a kernel. SMK runs multiple kernels concurrently on an SM. These types of GPU multitasking are shown in Fig. 1. The main difference between temporal multitasking, spatial multitasking and SMK is the granularity of resource allocation. In temporal multitasking, a GPU is exclusively occupied by a kernel. Spatial multitasking only allows the resources outside SM (e.g., interconnection network, L2 cache) to be shared by multiple kernels, while SMK enables multiple kernels to share the resources within an SM. Spatial multitasking and SMK are also called inter-SM and intra-SM slicing, respectively [4]. Preemption can release GPU resources and prevent low-priority applications from blocking high-priority applications, which is essential and critical for temporal multitasking. Therefore, temporal multitasking is also called preemption multitasking. In addition, preemption can also be used to adjust GPU resource allocation in spatial multitasking and SMK GPUs.


Fig. 1.
Types of GPU multitasking. It is assumed that two kernels run concurrently on the GPU including two SMs.

Show All

To support multitasking, NVIDIA Kepler GPU architecture [5] introduces hyper-Q technology that adopts leftover policy. With leftover policy, a running kernel can acquire as many resources as possible, and only remaining resources can be assigned to other kernels. As a result, concurrent kernel execution usually occurs during the period when the first kernel is about to finish and the second kernel just gets started [6]. In addition, the order of kernel submission incurs a significant impact on GPU system performance [7], [8], [9]. Obviously, it is not easy to run multiple kernels concurrently on a GPU. Based on NVIDIA pre-Volta GPU architectures, implementing spatial multitasking usually needs GPU code transformation, such as [10] and [11]. NVIDIA Volta GPU architecture [12] adopts hardware accelerated multi-process service (MPS). Volta MPS can prevent a kernel from occupying all the SMs in a GPU, which provides support for implementing spatial multitasking. Although the ability of GPU to support SMK is still lacked, some researchers try to implement SMK on real GPUs with transforming GPU program codes [13], [14], [15], [16], [17]. These software GPU multitasking methods run on real GPUs, and can improve the concurrency and system performance, but they need to modify the original application programs, which results in additional overhead and may not be always feasible.

The hardware GPU multitasking methods support GPU multitasking without program code transformation, and can be implemented by improving GPU architecture, which is interesting and concerned by many researchers. As far as we know, this paper is the first survey about the research of hardware GPU multitasking methods. Our main contributions are as follows:

We collect as many existing GPU multitasking methods as possible, and group them into three types. We briefly review the software GPU multitasking methods, and introduce the key idea of each hardware GPU multitasking method. The readers can quickly familiarize themselves with the GPU multitasking research.

We discuss the main issues that need to be concerned in each type of hardware GPU multitasking methods, and compare the characteristics of hardware GPU multitasking methods belonging to the same type. The readers can easily acquire a considerable insight into the research of hardware GPU multitasking methods.

We indicate that an enhanced GPU simulator is needed to bridge the gap between academia and industry, and give some interesting and promising directions that are worthy of exploring, which is beneficial to promote the future research of hardware GPU multitasking methods.

This paper is organized as follows. Section 2 provides some basic knowledge of GPU multitasking research. Section 3 reviews the related research of GPU multitasking methods, and focuses on the hardware GPU multitasking methods. The challenges and opportunities of the research of hardware GPU multitasking methods are proposed in Section 4. Finally, Section 5 summaries this paper.

SECTION 2Background
2.1 GPU Architecture
The GPU baseline architecture, as illustrated in Fig. 2, is constituted of a number of SMs. In the SM, arithmetic and logic units (ALUs) and special function units (SFUs) are computing units; load-store unit (LSU) includes shared memory, L1 data cache (L1D), etc., and executes memory access operations. A multi-banked L2 cache is shared by multiple SMs. One or more L2 banks are backed up with a memory controller to communicate with off-chip dynamic random access memory (DRAM). The L2 cache and SMs communicate through an interconnection network. NVIDIA provides compute unified device architecture (CUDA) [18] programming platform. The CUDA kernel (a set of instructions) is loaded to GPU for execution. An instruction is fetched and decoded for a group of threads constituting a warp, and then executed in the single-instruction multiple-threads (SIMT) style. A warp is the basic execution unit, and has a private space in the register file. A number of warps constitute a thread block (TB). All threads in a TB can synchronize and share data through shared memory. There are four types of resources that can limit the number of active TBs on an SM: register file, shared memory, warp scheduler slots and TB slots.


Fig. 2.
GPU architecture.

Show All

2.2 Features of GPU to Support Multitasking
NVIDIA is the dominant GPU vendor, especially in the general-purpose computing area. From Fermi architecture [19], NVIDIA GPU begins to support concurrent kernel execution. There is only one hardware work queue between host and GPU in Fermi architecture, which may result in false intra-stream dependencies. After Fermi, several advanced GPU architectures have been introduced, and are shown in Fig. 3. Kepler architecture [5] introduces hyper-Q and software-based MPS features to improve the ability to support multitasking. Hyper-Q increases the total number of work queues between host and GPU, and requires all the work queues to belong to a single CUDA context. MPS maps multiple clientsâ€™ CUDA contexts to a single CUDA context and then leverages Hyper-Q to build hardware work queues for the clients [20]. The software-based MPS is inherited by Maxwell [21] and Pascal [22] GPU architectures. Volta GPU architecture [12] provides hardware acceleration of MPS, which enables MPS clients to submit work directly to the work queues within the GPU. Volta MPS can restrict each client to only a fraction of the GPU execution resources, which can prevent all the GPU execution resources from being occupied by one client and provides the fundamental hardware support for spatial multitasking. Turing GPU architecture [23] inherits the hardware-accelerated MPS feature introduced in the Volta architecture. In the latest Ampere GPU architecture [24], Multi-Instance GPU (MIG) feature can divide a single GPU into multiple GPU partitions; each partition behaves like a smaller, fully capable independent GPU. MIG can avoid the interference between concurrent applications. Preemption is critical and indispensable for temporal multitasking. Pascal GPU architecture begins to support preemption at the instruction-level granularity, rather than TB granularity as in prior Maxwell and Kepler GPU architectures [22]. In summary, the ability of NVIDIA GPU supporting multitasking is getting stronger, but the technical details are not disclosed.


Fig. 3.
Evolution of NVIDIA GPU architecture.

Show All

In addition to NVIDIA, AMD is also an important GPU vendor. AMD GPU is the main competitor of NVIDIA GPU, and provides some features to support multitasking. For instance, AMD Polaris GPU architecture [25] supports partitioning execution resources for concurrent compute tasks; meanwhile, AMD RDNA GPU architecture [26] can completely suspend execution of tasks to free up all compute units (compute unit is like the SM in NVIDIA GPU) for a high-priority task.

2.3 System Modeling and Performance Metrics
Some open-source GPU simulators are used to model hardware GPU multitasking methods, such as GPGPU-Sim [27], MGPU-Sim [28], Multi2-Sim [29], gem5-gpu [30], MacSim [31], etc. Among these simulators, GPGPU-Sim is most widely used. In addition, to our knowledge, there are not yet open-source benchmarks for GPU multitasking. Two or three applications are usually used to construct a benchmark for evaluating GPU multitasking methods; these applications are primarily collected from Rodinia [32], Polybench-GPU [33], NVIDIA Computing SDK [34], Longstar [35], Mars [36], SHOC [37], Parboil [38], etc.

The system performance metrics usually used for evaluating GPU multitasking methods include system throughput (STP), average normalized turnaround time (ANTT), fairness and harmonic mean of the individual speedups (HSP) [39]. These metrics are calculated as Equations (1), (2), (3) and (4). N is the number of concurrent applications (i.e., concurrent kernels). Both i and j are integers, and range between 1 to N. IPC refers to instruction per cycle. IPCSPi is the IPC when application i runs on the GPU alone; IPCMPi is the IPC achieved by application i when multiple applications run simultaneously on the GPU. STP and ANTT quantify system-perceived and user-perceived performance, respectively. STP is the-higher-the-better; on the contrary, ANTT is the-lower-the-better. STP is also called weighted speedup (WS). Fairness is a value that ranges between 0 and 1; 1 means perfect fairness. HSP is a balanced metric for both system throughput and fairness [40], while WS is the metric for system throughput
STP=âˆ‘i=1NIPCMPiIPCSPi(1)
View Source
ANTT=1Nâˆ‘i=1NIPCSPiIPCMPi(2)
View Source
fairness=mini,j((IPCMPiIPCSPi)/(IPCMPjIPCSPj))(3)
View Source
HSP=N/(âˆ‘i=1NIPCSPiIPCMPi).(4)
View Source

SECTION 3GPU Multitasking Methods
We review as many existing GPU multitasking methods as possible, and group them into three types, as shown in Table 1. These typical GPU multitasking methods are categorized based on their concerns. For instance, [52] implements spatial multitasking, but mainly discusses preemption technologies; so [52] is in the preemption multitasking group. This paper focuses on the hardware GPU multitasking methods.

TABLE 1 GPU Multitasking Methods
Table 1- 
GPU Multitasking Methods
3.1 Preemption Multitasking
In temporal multitasking GPU, preemption swaps out low-priority applications, and reduces the pending time of high-priority applications as much as possible. Preemption technologies include context switching [54], SM draining [52] and SM flushing [53]. Context switching preserves the context of a running kernel, and releases GPU resources to a high-priority kernel. In addition to preemption latency, because no progress is made during context switching, preemption overhead also include system throughput. For context switching, in order to properly restore a warp or TB, its architectural states should be preserved. The architectural state of a warp includes its registers and SIMT stack. For a TB, besides the contexts of its warps, the architectural state also includes shared memory and barrier states. The SIMT stack and barrier states are very small compared to registers and shared memory [54]; therefore, the context of GPUs mainly includes registers and shared memory. Compared to the lightweight context switching in central processing unit (CPU) [91], GPU context is large. For instance, in the NVIDIA Kepler architecture, each SM has a 256KB register file and up to 48KB shared memory. Saving such large context will result in high overhead in both preemption latency and wasted throughput [53].

SM draining issues TBs from the new incoming kernel to an SM after all the running TBs on the SM finish, which can alleviate the throughput overhead. The process does not stop during the preemption of draining. Therefore, compared to context switching, throughput overhead of SM draining is little. Because the preemption latency of draining dependents on the remaining execution time of TBs in the SM, the drawback of this method is that the preemption latency can be very high. For the persistent kernel [92], [93], the lifetime of TBs may be as long as that of the kernel. Therefore, such kernels will result in the worst scenario, and cannot be preempted by draining method.

SM flushing flushes the running TBs without context saving and then executes a new kernel. The dropped TBs can be rerun from the beginning. This method only works on the idempotent kernels [94], [95]. For an idempotent kernel, atomic operations should not exist; meanwhile, overwriting a global memory location that is read in the kernel should also be avoided. Therefore, each TB of an idempotent kernel can be re-executed multiple times without affecting the results. This is obviously a strict limitation. In addition, although the preemption latency of flushing is almost zero, the useful work is wasted when a TB is flushed. The main advantages and drawbacks of the three preemption technologies introduced above are summarized and shown in Table 2; and these preemption technologies can also work at TB granularity. It is obvious that context switching, SM draining and SM flushing result in different overhead. The preemption research focuses on how to reduce overhead. The following of this subsection will introduce the preemption methods supported by GPU hardware architecture.

TABLE 2 Advantages and Drawbacks of Preemption Technologies

Menon et al. [51] partition GPU kernels into idempotent regions. The boundaries between regions fall at locations that contain relatively little amounts of live state. The instruction set architecture (ISA) is extended with a special instruction to mark the boundaries between idempotent regions. When a boundary instruction is encountered, the architectural state is saved immediately. After an exception has been handled, GPU program can be re-executed from the entry of an idempotent region. Because the code regions contain little or no live register state at the boundary, the overhead of context switching can be minimized. Although this method is proposed for handing exception, it can also be used to implement preemption.

Tanasic et al. [52] evaluate preemption technologies on the spatial multitasking GPU. Context switching and SM draining are used to adjust SM allocation dynamically. The experiment results show that hardware preemption mechanisms are necessary to obtain lower and more deterministic turnaround times; meanwhile, context switching can acquire better turnaround times and system fairness compared with SM draining. In addition, context switching can also acquire better STP than draining mechanism when running multiple workloads concurrently, which is different from the intuition. Although the reason of this scenario is not discussed in [52], it may be relative to the characteristics of benchmarks.

Park et al. [53] propose a collaborative preemption method called Chimera. Given the number of SMs to preempt, Chimera decides which SMs to preempt and selects proper preemption technique for each running TB to minimize total preemption overhead. First, for each running TB in every SM, Chimera evaluates the latency and throughput overhead of each preemption technology, and chooses the preemption technology with the least throughput overhead that satisfy the given preemption latency. Then, the total preemption cost of each SM can be calculated. Finally, while the required preemption latency is satisfied, SMs which can minimize throughput overhead are selected. In addition, Chimera relaxes the idempotence condition. Even for a non-idempotent kernel, the SM can still be preempted with flushing before the atomic or global overwrite operations, which expands the application scope of flushing mechanism.

Lin et al. [54] propose a lightweight context switching method consisted of three techniques. The first technology is in-place context switching, which allocates the unused register file and shared memory to the new kernel. The number of TBs de-allocated by the old kernel can be reduced; therefore, the amount of data to be saved and restored can be reduced. The second technique analyzes register liveness. Only the values in live registers can be saved when executing a preemption, which results in architectural state reduction. Based on the patterns of GPU registers reported by [96], the third technique compresses register states at both warp-level and TB-level to reduce architectural state.

Shieh et al. [55] propose a fast preemption method to support a dual-kernel execution model on the SMK GPU. A candidate victim set contains a set of running TBs, and can be preempted for allocating new TBs of the preempting kernel. To avoid resource fragmentation, resource allocation alignment is adopted. This method takes two steps to select a candidate victim set. First, for each TB in a candidate, its preemption overhead respect to different preemption techniques is estimated; then, while meeting the preemption latency constraint, a candidate with the least throughput overhead is selected to be the final victim. Similar to [53], this method adopts proper preemption technology for each TB to minimize total preemption overhead. Meanwhile, instead of preempting all the selected running TBs in an SM, this method dispatches new TBs once there are enough SM resources, which is beneficial to reduce the preemption cost on the SMK GPU.

Li et al. [56] propose a proactive preemption method that is like the conventional checkpointing mechanism [97]. A preemption can be anticipated before the actual request arrives. When the actual preemption is invoked, an incremental update relative to the previous saved state is performed. For a predicated preemption, if the draining time of running TBs is shorter than the time to perform a base checkpoint, draining is selected; otherwise, a checkpointing (i.e., context saving) is executed. Meanwhile, this method uses the in-place context switching [54] to reduce context saving size. In addition, this method is introduced in more detail in [57].

The preemption technologies adopted by these typical preemption methods are shown in Table 3. [51] discusses handing exception with context switching. [52] compares context switching with draining on spatial multitasking GPU. [54] focuses on reducing the overhead of context switching. The remaining methods try to reduce the preemption overhead by adopting several preemption technologies in combination. In addition, the resources swapping method for GPU virtualization proposed in [58] can also be used for preemption.

TABLE 3 Preemption Technologies Adopted by Hardware Preemption Methods

Besides these hardware preemption methods, some software preemption methods are also proposed to complete the high-priority applications within the deadline. [41], [42], [43] and [44] slice kernels into multiple sub-kernels, and execute preemption at the sub-kernel boundaries. [45] supports preemption by a run-time resource management algorithm. [46] proposes a preemptive GPU kernel scheduling scheme that harnesses the idempotence property of kernels. A GPU programming model which executes preemption at the boundary of TB is introduced in [47]. [48] implements a software prototype system which can enable flexible and efficient GPU kernel preemption. [49] enables low latency context switching with reasonable overhead. [50] tries to find the minimum number of preemptions when scheduling a set of independent tasks.

3.2 Spatial Multitasking
Spatial multitasking allocates a set of SMs to each one of concurrent kernels; and the off-SM GPU resources, which mainly include interconnection network, L2 cache and DRAM, are shared by multiple kernels. Spatial multitasking methods try to implement optimal SM partitioning for improving system performance. Meanwhile, interference between concurrent kernels, especially the contention on the lower memory system constituted of L2 cache and DRAM, may degrade system performance, which should also be concerned when designing spatial multitasking methods. The key ideas of typical hardware GPU spatial multitasking methods are introduced in this subsection.

As far as we know, Adriaens et al. [62] implement spatial multitasking on GPUs for the first time. The applications are classified into compute-bound applications, interconnect/memory-bound applications, and problem size-bound applications. SMs are evenly allocated to each application to evaluate GPU system performance. In addition, other SM partitioning methods, such as oracle best, oracle worst, rounds, etc., are also evaluated. The experimental results shows that spatial multitasking is beneficial to improve system performance.

Jog et al. [63] analyze the interference between current applications on the spatial multitasking GPU. The applications demanding high memory bandwidth consume most of the bandwidth, which damages the performance of other applications on the same GPU, and results in severe fairness problem. To alleviate this trouble issue, the First-Ready Round-Robin First-Come-First-Serve (FR-RR-FCFS) memory scheduling mechanism is proposed to replace widely known FR-FCFS memory scheduler. FR-RR-FCFS memory scheduling chooses memory requests from different applications in round-robin manner, which can prevent the bandwidth-intensive application from starving its co-scheduled applications.

Jog et al. [67] make a deeper analysis about the interference between multiple applications on the spatial multitasking GPU compared to their previous work [63]. It is illustrated that prioritizing lower misses-per-kilo-instruction (MPKI) applications and lower bandwidth applications can improve system instruction throughput (IT) and WS, respectively. Based on these observations, two memory scheduling schemes named IT targeted scheme and WS targeted scheme are proposed to improve system IT and WS, respectively.

Aguilera et al. [66] propose a dynamic resource scheduling method to satisfy the QoS requirements. This method is an iterative approach. First, the QoS application acquires half SMs of a GPU, and its performance is evaluated. Then, increasing or decreasing the SMs allocated to the QoS application is executed based on the result of comparing the acquired performance and the QoS requirements. After the goal that satisfying the QoS requirements with minimum SMs is implemented, the idle SMs can be disabled to reduce power consumption or allocated to a co-executing application to increase system performance.

Aguilera et al. [64] evaluate the impact of different fairness polices on SM allocation, and propose a run-time resource partition approach based on their previous work [66] to satisfy the chosen fairness metric. The initial SM allocation is created based on the isolation profiling information of applications; if the isolation profiling data is lacking, an equal SM allocation is conducted. Then, the performance of each application is evaluated during concurrent execution of applications. Based on the evaluated performance, SMs are reallocated to meet the desired fairness metric, such as equal throughput, equal speedup, etc.

Within-die process variations result in different maximum operating frequencies (Fmax) of the SMs in a GPU. Aguilera et al. [65] propose a process variation-aware SM partitioning method to support the per-SM clocking (PSMC) [98] scheme (i.e., each SM works at its own Fmax). Their SM partitioning method assigns compute-bound applications and memory-bound applications to faster SMs and slower SMs, respectively. SMs are allocated at the beginning, and will not be reallocated during the execution of concurrent applications. The simulation experiments illustrate that speedups are best when executing combinations of compute- and memory-bound applications.

Hu et al. [68] also study the interference between concurrent applications, and propose a dynamic application slowdown estimation (DASE) mode to estimate the application slowdowns on the spatial multitasking GPU. DASE can predicate the slowdowns of applications at runtime without the isolated application profiling information. Based on DASE, they propose a dynamic SM allocation policy called DASE-Fair to minimize the overall system unfairness. At the initial phase, DASE-Fair evenly allocates SMs to concurrent applications. Then, the SM allocation is adjusted according to the fairness estimation result at runtime until a best SM partitioning with the maximum improvement in fairness is selected.

Saha et al. [74] demonstrate that GPU applications exhibit a stark variety of frequency sensitivities, and propose an energy-efficient SM allocation method for the spatial multitasking GPU. The SMs with the same architecture in a GPU works at different voltage-frequency (VF) domains. The applications types are acquired based on online profiling. This method tries to allocate compute-intensive applications and memory-intensive applications to higher VF domains and lower VF domains, respectively, and implements optimal SM partitioning with a simultaneous perturbation stochastic approximation algorithm to improve the energy efficiency of the spatial multitasking GPU.

Zhao et al. [75] find that searching an effective SM partitioning based on workload classification can significantly reduce the search space. Through offline profiling, the workload is accurately classified into three categories based on the off-SM bandwidth model that takes interconnection network, last level cache and main memory bandwidth into account. Then, the optimum SM partitioning is determined by following a different search strategy for each of workload mixes. If the workload includes compute-intensive and memory-intensive applications, system performance is maximized by assigning more SMs to the compute-intensive application, while assigning a small fraction of SMs to the memory-intensive applications. If the workload contains only memory-intensive applications, allocating a fraction of the available SMs can not only acquire the optimum performance but also save power. For the workload consisted of compute-intensive applications, it is difficult to improve performance or save power only through SM partitioning.

Wang et al. [69] gives a detailed analysis of the thread-level parallelism (TLP) management technology at the warp granularity on the spatial multitasking GPU. The effective bandwidth (EB) is the ratio of attained DRAM bandwidth to the product of L1 and L2 miss rates. The application's EB changes with different TLP configurations are directly related to the changes of its performance. Therefore, EB can be used as the performance indicator of an application. In addition, for the concurrent applications, maximizing their total EB and balancing their EB are important to obtain better WS and fairness, respectively. Based on these observations, pattern-based searching (PBS) mechanisms are proposed to optimize WS, fairness and HSP. PBS can avoid searching all the different combinations of TLP configurations, and find the appropriate TLP combination efficiently.

Kim et al. [70] shows that the utilization of SM sub-resources, such as cache, scheduling unit, execution unit, etc., is obviously different for various applications. Therefore, some sub-resources are underutilized when multiple applications run on the spatial multitasking GPU, which limits fully exploiting TLP and improving system performance. To alleviate this problem, they propose the GPU Weaver which is a dynamic sub-resource management system for the spatial multitasking GPU. GPU Weaver can share scheduling units, execution units and L1 data caches between neighboring SMs. For an application running on an SM, if a type sub-resource is not sufficient but idle in neighboring SMs, this SM will borrow the sub-resource from neighboring SMs to accelerate executing the application. Compared to other spatial multitasking methods, GPU Weaver implements sharing fine-grained sub-resources between SMs.

Zhao et al. [76] propose a kernel slowdown model (KSM) to predict the slowdowns of concurrent applications. KSM periodically collects the hardware event statistics caused by each application, and adopts a pre-trained neural network to predict the application's slowdown online. The neural network is trained offline based on the key factors that affect kernel slowdown; these key factors mainly include the number of SMs assigned to a kernel, contention on shared resources and the kernel's sensitivity to resource contention. Based on KSM, Zhao et al. propose an enhanced slowdown prediction model called Themis in [72]. Besides the tried SM allocation, Themis uses a piecewise linear regression method to predict the slowdowns of concurrent applications with an un-tried SM allocation. Based on Themis, [72] implements an SM allocation engine to rein the slowdowns of concurrent applications for satisfying the purpose of QoS target or fair sharing. Wei et al. [71] introduce Themis and the SM allocation engine based on Themis in more detail. In addition, a software two-stage model to predict application slowdown on real GPU is also proposed in [71].

Zhao et al. [73] classify the GPU workloads into lean-TLP and thick-TLP kernels. Lean-TLP kernel performance does not degrade when given less per-SM TB resources; thick-TLP kernel performance obviously improves when running more TBs on each SM. Some TB resources are migrated from half SMs to another half, which converts the baseline GPU to HeteroCore GPU. HeteroCore GPU includes two type SMs, big-SM and small-SM. Compared to an SM in the baseline GPU, the big-SM and small-SM provide more and less TB resources, respectively. Based on the HeteroCore GPU architecture, a dynamic TLP resource-aware scheduling method is proposed for balancing STP and ANTT on the spatial multitasking GPU. This method determines the types of two concurrent kernels with a profiling phase. If two kernels are thick-TLP types, both big-SMs and small-SMs are evenly allocated to each kernel. Otherwise, all the big-SMs are allocated to the kernel that benefits most from running on the big-SMs, and another kernel gets to run on the small-SMs.

The SM partitioning strategy proposed by [77] is like that introduced in [75]. First, the application type is determined based on profiling. Then, if a workload includes both compute-intensive and memory-intensive applications, more SMs are allocated to the compute-intensive application; otherwise, SMs are evenly partitioned. The main difference between [77] and [75] is that [77] adopts TB throttling when the workload only includes memory-intensive applications.

Marangoz et al. [78] enhance GPU architecture to mitigate memory bandwidth contention. SMs are evenly allocated to each application; meanwhile, L2 caches are also evenly partitioned. A predefined memory bandwidth is assigned to each application at the begin of an epoch. For an application, if all its bandwidth is consumed, any following memory requests of this application become low-priority. The memory scheduler services low-priority requests when no high-priority requests are waiting for service. [78] optimizes memory scheduling to avoid that most of DRAM bandwidth is consumed by the application issuing many memory requests, which is similar to [63] and [67].

Some characteristics of these hardware GPU spatial multitasking methods introduced above are shown in Table 4. [63], [67], and [78] try to enhance DRAM controller to mitigate DRAM bandwidth contention and improve system performance. [75] and [77] reduce SMs occupied by the memory-intensive applications, which decreases the memory accesses leaving SMs, and should also be beneficial to ameliorate the GPU lower memory system contention between concurrent applications. Besides these hardware spatial multitasking methods, some researchers try to implement spatial multitasking on real GPUs. [10] and [11] implement efficient spatial multitasking on GPUs with code transformation. [59] improves the utilization of GPU resources while guaranteeing the QoS requirements. [60] uses a software-only mechanism to partition both compute and memory resources of a GPU to avoid the interference between concurrent applications. [3] and [61] accelerate concurrent neural network inferences: [3] discusses how to manage multiple spatial multitasking GPUs in the edge cloud; [61] try to reduce data transfer overhead by overlapping data transfer with computation in the context of GPU multitasking.

TABLE 4 Characteristics of Hardware GPU Spatial Multitasking Methods

3.3 Simultaneously Multitasking
SMK method runs multiple kernels concurrently on an SM; compared to spatial multitasking methods, SMK methods implement fine-grained resources sharing. For SMK GPUs, how to allocate intra-SM resources, i.e., TB partitioning, should be concerned, which has a significant influence on the system performance. Meanwhile, the interference between concurrent kernels, especially the contention for L1D, damage system performance. [87] and [89] have observed that the performance of compute-intensive kernel is reduced dramatically because L1D and memory pipeline is blocked by the memory-intensive kernel. Although many works have been proposed for mitigating L1D contention when running a single kernel on the GPU, such as [99], [100], [101], etc., how to reduce the performance loss caused by L1D contention on SMK GPUs is still challenging. Moreover, running kernels with complementary characteristics can acquire better performance; therefore, if there are many candidates, selecting proper kernel combinations is an interesting problem. The following will introduce the key ideas of these typical hardware SMK methods.

To our knowledge, Awatramani et al. [80] propose the first hardware SMK GPU method. Each kernel is mapped to all the SMs; on each SM, warps of concurrent kernels are interleaved. The experiment results show that overall throughput can be improved when running compute-intensive and memory-intensive applications concurrently. In addition, the application with high L1D miss rate can significantly reduce the throughput of the other applications.

Lee et al. [81] propose a lazy TB scheduling method to determine the optimal number of TB occupied by a kernel in an SM. The maximum TBs are initially allocated to each SM. In an SM, after the first TB completes its execution, the total number of instructions issued across all the TBs is divided by the number of instructions issued from the first finished TB; then, the result is the optimal TB number. After an SM acquires its optimal TB number, the maximum number of TBs that can be assigned to this SM is determined. Lazy TB scheduling limits the number of TBs running in an SM, which can improve performance, but will result in wasted resources (e.g., registers, shared memory). To improve resource utilization, TBs from other kernels are allocated to use the remaining resources in the SM; so, multiple kernels can run on the GPU simultaneously in SMK manner.

Wang et al. [82] propose a resource partitioning method based on the dominant resource fairness policy [102] to execute concurrent applications on the SMK GPU. The resources include registers, shared memory, number of active threads and number of TBs. A kernel's dominant resource share refers to the maximum share that this kernel requires of any resource. The aim of this resource partitioning method is to equalize the dominant resource share for all kernels on an SM. The resource allocation method proposed in [82] is enhanced in [83]. The computing cycles are also fairly allocated to concurrent kernels via warp scheduling for performance fairness. A kernel has a share of cycles that is proportional to the amount required when the kernel is executed alone on an SM. In addition, the applicability of this SMK method is also discussed when increasing the number of concurrent kernels.

The intra-SM resources are sliced to concurrent applications at TB granularity on SMK GPU, and it is impossible to find a resource slicing strategy that can acquire the best performance for all application pairs. Xu et al. [4] propose Warp-Slicer, which tries to find the optimal intra-SM resources partitioning for different application pairs. Based on a set of profiling, Warp-Slicer evaluates the performance variation of each kernel when the number of TBs running on an SM changes. Then, the water-filing algorithm [103] is used to find the sweet point TB partitioning, where the performance degradation of each application is minimized when running concurrent applications. Finally, after that the resources of each SM are allocated to concurrent applications with the sweet point TB partitioning, multiple applications can begin to run on the GPU with SMK manner.

Park et al. [84] propose GPU Maestro that performs dynamic resource management. Maestro tries spatial multitasking and SMK on different SMs; and different resource partitioning possibilities for SMK are tested during some epochs. Finally, the resource partitioning, which acquires the best performance and may be spatial multitasking, is adopted by the whole GPU. Meanwhile, Maestro employs 2-way allocation method to mitigate the resource fragmentation problem on SMK GPU. In addition, although greedy-then-oldest (GTO) warp scheduling policy is widely used, if the kernel does not occupy the front warp slots, it will have lower possibility of issuing instructions, and may be starved. To solve this problem, Maestro adopts kernel aware warp scheduling to provide equal chance of issuing instructions to each kernel.

Liang et al. [6] propose an efficient kernel management method including static TLP modulation, dynamic TLP modulation and cache bypassing. Based on the off-line profiling, optimal TB number of each kernel running alone in an SM and kernel type can be acquired. Then, static modulation determines the initial TLP configuration (i.e., TB partitioning) to run multiple kernels on the SMK GPU. The pipeline utilization is used as the performance metric by on-line learning. The dynamic modulation leverages on-line learning to adjust the TB resource allocation to improve system performance. In addition, to mitigate L1 cache contention, cache bypassing finds the kernel to bypass and adjusts the number of TBs bypassing cache. The aim of cache bypassing is that letting the kernel with good locality use the cache.

Liang et al. [85] try to mitigate cache contention on SMK GPU with cache bypassing and partitioning. If there are N concurrent kernels, N SMs are used as profiling SMs, and remaining SMs are used as followed SMs. The cache resources are allocated to multiple kernels at the granularity of cache way. During each sampling period, every profiling SM employs an untried cache resource partitioning strategy; meanwhile, the winner cache partitioning, which is used by the SM that acquires the maximum system throughput in the previous sampling period, is employed by followed SMs. The cache partitioning is used to mitigate the cache contention between concurrent kernels. At the same time, the technique proposed by Xie [104] that combines instruction and TB level cache bypassing is also used to alleviate the cache contention problem.

Wang et al. [86] discuss how to satisfy QoS requirements on the SMK GPU. A QoS kernel is allocated to every SM; meanwhile, SMs are allocated to non-QoS kernels evenly. This method makes effort to meet the QoS goal of each QoS kernel; meanwhile, it also tries to maximize the total throughput of non-QoS kernels. The quota indicates the number of instructions that should be executed per epoch on an SM. At the beginning of each epoch, the quotas of QoS kernels and non-QoS kernels are determined. Before the end of an epoch, if the quotas of QoS kernels are finished, the remaining cycles will be assigned to non-QoS kernels. At the end of an epoch, if the goal of a QoS kernel is not reached, the number of remaining instructions is added to the quota of this kernel in next epoch. In addition, the number of idle warps for all kernels in an SM is used as a metric to determine whether to execute run-time adjustment of TB allocation.

Lin et al. [88] divide kernels into bandwidth-sensitive (BS) kernel and latency-sensitive (LS) kernel, and propose a coordinated TB combination and bandwidth partitioning method to improve concurrent kernel execution on SMK GPUs. This method allocates more TB resources and more bandwidth resources to LS kernel and BS kernel respectively; meanwhile, it also prevents BS kernel from occupying too much bandwidth resources to avoid serious damage to the performance of LS kernel. This method receives kernel types and the relationship between the TB number and the corresponding bandwidth utilization as input parameters, and returns TB combination and bandwidth quotas. The bandwidth quotas are converted to the L1D miss request issue rate quotas (i.e., how many L1D misses can be sent to interconnection network per time unit) to control bandwidth consumption. The input parameters can be acquired by either online or offline profiling.

Dai et al. [87] illustrate that compute-intensive kernels may be starved because other memory-intensive kernels block the memory pipeline on SMK GPUs; then, they propose a dynamic memory instruction limiting (DMIL) method to mitigate L1 cache contention and accelerate concurrent kernel execution. The reservation failures per memory request (RFMR) indicates how severe cache contention and cache-miss-related resource congestion are. During a sampling interval, the RFMR and maximum number of active memory instructions of a kernel are collected and used to determine the memory instruction limiting number of this kernel for the next sampling interval.

Zhao et al. [89] also show that the performance of compute-intensive kernels is significantly reduced when memory-intensive kernels block memory pipeline and occupy most L1D resources. They propose a fair and cache blocking aware warp scheduling (FCBWS) approach to ameliorate the contention on data cache and improve system performance. FCBWS adopts kernel aware warp scheduling used by [84] to provide equal chance of issuing instructions to each kernel. In addition, for a ready memory instruction to be issued, if it is predicated that this instruction will block the data cache, FCBWS will select and issue another ready instruction of the same kernel; otherwise, this memory instruction will be issued to the memory pipeline. Both [87] and [89] focus on warp scheduling, which can be coordinated with various TB partitioning methods to improve the system performance of SMK GPU.

Kim et al. [90] propose a lookup-table-based multi-kernel scheduler called GPU Navigator. The GPU Navigator is mainly constituted of a single kernel table and a multiple kernel table, and selects the best kernel pair from multiple candidates to maximize the system performance of SMK GPU. GPU Navigator first executes the single kernel profiling, and records the IPC of each kernel in the single kernel table. Then, concurrent kernel profiling is carried out to acquire STP of each kernel pair; and the STPs of all the kernel pairs are stored in the multiple kernel table. Finally, The GPU Navigator can easily find the kernel pair that acquires the maximum STP, and executes this kernel pair on the SMK GPU. If a new kernel appears, both the single kernel profiling and concurrent kernel profiling will be executed again to update the single kernel table and the multiple kernel table.

The characteristics of these hardware GPU SMK methods are shown in Table 5. Only [90] discusses how to select the kernel pair that can acquire best performance, while other methods focus on improve system performance based on TB partitioning or more fine-grained resource scheduling in an SM. Even if only one TB is allocated to the memory-intensive kernel, L1D may still be blocked, which reduces other concurrent kernelsâ€™ throughput significantly. As a result, although STP and the throughput of memory-intensive kernel may be improved, other system performance metrics, such as ANTT and fairness, are damaged seriously. Therefore, acquiring balanced performance improvement on SMK GPUs cannot dependent on only TB partitioning, which is also illustrated in [87], [88] and [89]. In addition, there are some software SMK methods implemented on real GPUs. Kernel slicing is proposed in [17], and is used by [16] and [14] to implement SMK. [15] executes multiple kernels in SMK manner based on kernel stretching. [13] implements SMK by adopting a hybrid technique constituted of kernel slicing and kernel stretching. [79] illustrates that SMK is beneficial to reduce execution time and power consumption.

TABLE 5 Characteristics of Hardware GPU SMK Methods

SECTION 4Challenges and Opportunities
To efficiently utilize GPUs in various computing platforms, such as data centers, edge cloud, etc., supporting multitasking becomes an important development direction of GPU architecture. Although hardware GPU multitasking methods have been studied in depth, there are some challenges to surmount. Meanwhile, there are also some opportunities to explore and expand the research space of GPU multitasking methods. The main challenges and opportunities are as follows.

Demand Enhanced Simulator. The quick development of GPUs is promoted by industry, but the GPU simulation tools used by industry are not open-source; meanwhile, no one GPU vendor has disclosed their exact details of architecture design. All the open-source GPU simulators are implemented based on limited open technical documents and the assumptions of GPU architecture. In addition, GPGPU-sim [27] models most previous hardware GPU multitasking methods, but this simulator can only models early NVIDIA GPU architectures. The gap between opaque innovations of industrial GPUs and academic research cannot be ignored. To bridge this gap, a simulation framework Accel-Sim is proposed to model NVIDIA's Turing and pre-Turing GPU architectures [105]. Although Accel-Sim provides an open-source powerful GPU simulation platform, it still lacks the ability of supporting the features of NVIDIA latest Ampere architecture, such as MIG [24]. It is a challenge that making GPU simulators used by the academic research to keep up with industry, which is also a promising opportunity to give contributions to the research of hardware GPU multitasking methods.

Incorporate Machine Learning. Machine learning techniques, especially neural network, have been successfully used in many fields, such as video surveillance, medical image analysis, natural language processing, etc. Meanwhile, machine learning methods have also been adopted in some research based on real GPU hardware systems. For instance, a neural network is used to estimate GPU performance and power [106]; a wrapper feature selection method is used to select metrics for GPU kernel classification [107]; the performance gain of concurrent kernel execution over single kernel execution is predicated based on machine learning techniques [108]. In addition, machine learning techniques are also used in the research of hardware GPU multitasking methods. For example, a pre-trained neural network is used to predicate the application's slowdown online on the spatial multitasking GPU [72]. Machine learning techniques are inherently suitable to extract the features of massive GPU applications, and improve GPUsâ€™ ability of managing concurrent execution of multiple applications. The future GPU should be intelligent. It is interesting and promising that using machine learning techniques to promote the GPU-related research, especially the hardware GPU multitasking method.

Explore Design Space. The previous hardware GPU multitasking methods are mainly implemented based on NVIDIA Fermi architecture. It should be an interesting topic to investigate multitasking methods on other vendorsâ€™ GPU architecture. Fermi architecture has been introduced for more than 10 years. NVIDIA has introduced several GPU architectures after Fermi, and many architectural innovations are adopted. For instance, both L1 an L2 caches support 32B sub-line fetching in 128B cache lines after Maxwell architecture, which is beneficial to accelerate executing irregular applications. Modern GPUs employ advanced architecture techniques to improve L1 cache throughput, and shift the memory bottleneck from the L1 cache to the lower levels of the memory hierarchy [105], which signifies that mitigating L1 cache contention between concurrent applications may not be urgent for GPU multitasking. Meanwhile, high bandwidth memory (HBM) has begun to be employed as the main memory of GPU [109], [110]. With the features of HBM such as independent memory channels and single-bank refreshing [111], it is critical to investigate the interaction between memory scheduling and L2 cache to accelerate concurrent kernel execution on GPUs. In brief, advanced GPU architectural innovations and 3D stacked HBM give a huge exploring space to the research of GPU multitasking methods supported by hardware architecture.

SECTION 5Conclusion
This paper mainly reviews the hardware GPU multitasking methods. The main issues that need to be concerned in each type of hardware multitasking methods are illustrated; meanwhile, the characteristics of hardware multitasking methods belonging to the same type are compared. In addition, for the future research, both main challenges to be resolved and promising research directions are provided. This paper helps the readers quickly and deeply understand the research of hardware GPU multitasking methods, and is beneficial to promote the related research. GPU architectures of different vendors are like to a certain extent; in addition, some problems faced by multitasking methods based on NVIDIA GPUs still bother the multitasking method research based on other vendorsâ€™ GPUs, such as the interference between concurrent applications. Therefore, although this paper focuses on NVIDIA GPU architecture, it can still provide some valuable inspiration to the research of hardware GPU multitasking methods based on other vendorsâ€™ GPUs.