Abstract
The haptic illusion literature suggests that individuals’ haptic experiences rely not only on tactile signals, but on visual and auditory stimulation as well. Given that mixed reality (MR) technologies often harness realistic visual and auditory but not haptic stimulation, it is important to understand how visual and auditory signals shape haptic experiences. We examined whether visual and auditory cues in MR can enhance tactile sensation, including perceptions of roughness and stiffness on virtual gadgets, in the absence of haptic signals. A laboratory experiment showed that tactile perception of mobile gadgets in motion was influenced by both visual and auditory cues when the motion of the gadget was generated by participants (i.e., active touch), but only by auditory cues when participants observed the gadget moving (i.e., passive touch). The results indicate that visual and auditory cues can be employed to facilitate haptic experiences of virtual objects in virtual reality, with auditory cues being potentially applicable to a broader context than visual cues.

Previous
Next 
Keywords
Haptic illusion

Visual cues

Auditory cues

Active and passive touches

Mixed reality

1. Introduction
Continuous development of computing technologies has accelerated developers’ and users’ interests in virtual reality (VR), augmented reality (AR), and mixed reality (MR) (Ladwig and Geiger, 2018). CeArley et al. (2016) identified immersive experiences in VR, AR, and MR as one of the top 10 strategic technology trends of 2018. With the capacity to accommodate a display with a higher resolution, a larger-capacity battery, and a faster-performing central processing unit in the limited space of a smartphone, computing technologies for MR have turned to high-fidelity simulation (Majumder et al., 2017; Mukhopadhyay, 2014). Amid a market shifting from desktops to mobile platforms, Chi et al. (2013) has predicted that the next significant platform would be a form of wearable MR.

When interacting with virtual objects in MR, users sense objects and surrounding environments through multiple sensory channels. An MR system is therefore required to present coordinated multisensory information that allows users to perceive virtual objects realistically and perform tasks effectively in virtual environments. Crison et al. (2005) emphasized the importance of multisensory feedbacks in interactive manipulation of a virtual device, with realistic sounds and visual assistance in VR. Zahariev and MacKenzie (2003) showed that auditory and graphical cues influenced task performance in reaching, grasping, and placing in AR. Similarly, Bork et al. (2015) demonstrated the importance of coordinated visualization when using MR for medical purposes (e.g., AR surgery). These studies indicate that using coordinated auditory and visio-temporal stimulation for three-dimensional perception can improve users’ subjective feelings of being in a virtual environment and interacting with virtual objects.

Despite the potential of MR to provide users with realistic sensory experiences in a virtual environment, current MR systems face challenges in providing users with natural experiences, including intuitive interfaces, tracking, latency, depth perception, and overload (Azuma et al., 2001; Rolland and Fuchs, 2000). The practical use of haptic displays could be limited due to defective implementation of multisensory stimulation, and physical contact with an object should be offered to overcome such limitations (Bau and Poupyrev, 2012). Given that providing haptic feedback for complex interactions on a variety of platforms is challenging (Miyazaki et al., 2010), it is essential to investigate the feasibility for haptic feedback in MR from a practical perspective. Several options for haptic displays with limited or no skin tactility have been suggested, including volumetric (Blundell and Schwarz, 2002) or ultrasonic displays (Hoshi et al., 2010), but these techniques are still in development.

Among the alternatives, we propose that manipulating visual and auditory cues in MR is an effective way to improve users’ tactile experiences. The current study tests this proposal by examining the effect of visual and auditory cues in tactile sensing of physical characteristics of virtual objects in an MR environment. We employed an MR environment in which users interact with virtually implemented user-interface (UI) gadgets. By superimposing virtual gadgets on the real world, technologies can turn a physical environment into an integrative computing environment. While visual and auditory cues can theoretically enhance users’ tactile experiences in controlling virtual gadgets, little research has been conducted to test this possibility. In the current study, the effects of visual and auditory cues on tactile experiences of virtual gadgets were examined through active touch and passive touch (Gibson, 1962). Distinguishing active and passive touch in controlling UI gadgets in MR presents implications for our understanding of how users operate interfaces. These two distinctive situations were employed to examine the “cross-modal effect” and “haptic illusion,” which have been suggested as substitutes for physical contact (Hayward, 2008). The experimental findings provide insights into the potential for, and limitations of, touch illusion in MR.

2. Background
2.1. Mixed reality and virtual objects
MR can be defined as a merged space (Milgram and Kishino, 1994), in which the virtual and the real augment each other (Barfield, 2015). The coexistence of real and virtual environments has been anticipated for several decades (Milgram and Kishino, 1994) and it is now widely assumed that individuals will live their lives to some degree in MR platforms at some point in the future (Chi et al., 2013). User experiences in mobile environments have improved in recent years and now offer more immersive and contextual interactions with multiple sensory channels (Flintham et al., 2003). The integration of VR and AR with conversational platforms extends immersive applications beyond isolated and single-person experiences (Sally and Michael, 2012). Google Inc., for example, has created a smartphone-friendly MR world, emphasizing mobile, economic, practical, and universal values of virtual experience (Fenu and Pittarello, 2018). Virtual objects in MR are typically accompanied by visual, auditory, and haptic stimuli that let users interact with the virtual objects. (Wiedenmaier et al., 2003).

Previous studies have primarily emphasized the technical feasibility of long-term immersion in MR. However, research that considers comfort and usefulness in interaction with applications is lacking. The haptic display of virtual objects is now among the most important concerns in MR development. Ochiai et al. (2016) presented design guidelines for long-term interaction with an MR application that emphasized the importance of providing users with as much cue information. In the case of volumetric display, virtual objects could become superimposed on the real environment, transcending the limitations of mobile displays. It is also expected that the improvement of a haptic display with virtual objects will satisfy the need for collaborative work in MR and enable the exchange of data and information among individual users or groups (Billinghurst and Kato, 2002).

2.2. Cross-modal effects
The cross-modal effect is generated in response to multisensory interactions based on reciprocal effects among sensations and integration of sensorimotor systems (Driver and Spence, 2000). Such multisensory interactions include the McGurk effect, double-flash illusion, and ventriloquist illusion (Calvert et al., 2004). Freides (1974) emphasized the potentials of cross-modal functions in human information processing. When users cannot adequately handle input from a situation, they can utilize a combination of perceptual functions (Lindeman et al., 2006). Somatosensory and kinesthetic systems can produce various types of sensory haptic information through physical movement. Haptic illusions, or tactile feedback for a virtual object, can be triggered by multisensory interactions (Hayward, 2008). The relationships between visual and/or auditory cues and haptic feedback are an important element of cross-modal effects in MR.

Fig. 1 shows the cross-modal effect generated by multisensory interaction. In normal situations, multisensory information is available and multiple sensory channels are employed, and users may experience haptic input from haptic stimuli, visual input from visual stimuli, and auditory input from auditory stimuli. While each perceptual process is dominated by a corresponding sensory channel, they are also associated with each other through reconfirmation of their perceptions using information obtained from other channels. For example, a user may judge a surface of a virtual object to be soft by touching the object and also by seeing the surface of the object being squeezed by fingers. When information delivered through sensory channels is inconsistent, this cross-modal association can create a perceptual illusion. In the absence of haptic stimuli, for example, people may infer the perception from visual and auditory stimuli.

Fig. 1
Download : Download high-res image (232KB)
Download : Download full-size image
Fig. 1. Cross-model effects – haptic illusion by visual feedback and auditory feedback.

2.2.1. Effects of visual cues on haptic perception
MR merges real and virtual worlds mainly in a visual sense (Milgram and Kishino, 1994; Ohta and Tamura, 2014). When a virtual object appears in MR, the object not only informs users of its existence, but indicates its physical characteristics. Visualization of an object in MR may portray texture on the object's surface, for example, and allows users to infer the material of which the object is made. Individuals may have accumulated sufficient experience on an object using both visual and tactile channels and therefore achieve cross-modal integration on the particular object. This integration may allow individuals to automatically infer how they would feel when touching an object from visual perceptions of the object, resulting in an illusory tactile experience.

Supporting this line of reasoning, previous studies have demonstrated that visual cues of virtual objects influence users’ tactile sensations in MR. Biocca et al. (2001), for example, found that users felt physical resistance (e.g., gravity and inertia) when they moved virtual objects connected to a virtual spring using their hands in a VR environment. Although they did not receive any haptic feedback from the VR system, the sight of an elastic spring being extended caused participants to feel resistance. Nakahara et al. (2007) analyzed the sensory properties of visual and haptic cues in MR. They addressed the sharpness of a cube's edge, which was strongly affected by other visual and haptic senses. Their results showed that a haptic illusion by visual facilitation was augmented when a discrepancy existed between the visual and haptic cues. In addition, Kagimoto et al. (2009) examined how visual and audio stimuli influence tactual perceptions of the roughness of a touchable object when a computer-generated image was superimposed on the object in MR. The results showed that haptic perceptions could be replaced by visual stimulation. Hirano et al. (2011) examined the psychophysical influence of visual stimulation on the sense of hardness in MR. The tactile sense of virtual urethane was compared with that of real urethane. The results showed that, regardless of the haptic sense of the actual physical characteristics, touch sensations were influenced by visual stimulations, but the reverse phenomenon occurred occasionally when the virtual objects were significantly different from the actual ones.

Regarding mass perception in MR, Hashiguchi et al. (2014) examined dynamic illusions, the sensory illusion of weight by MR visual stimulation of moving objects. In an MR environment, a crossing sense could induce better recognition beyond actual physical characteristics. Punpongsanon et al. (2015) proposed a pseudo-haptic mechanism for operating a surface-deformation effect and body-appearance effect on perceptions of softness when pushing a soft physical object in AR. These results indicate that the perception of softness can be visually manipulated. In addition, Issartel et al. (2015) utilized visual cues to simulate the reaction force from virtual objects in MR. In their experiment, participants perceived mass through pseudo-haptic rendering of light and heavy virtual objects. Rosa et al. (2015) addressed the effects of visual cues on passive-touch sensations in a multi-modal virtual environment. According to the results, participants’ perception of a relatively simple touch could be manipulated by a visually rich stimulus and a more complex haptic experience. Gaffary et al. (2017), who investigated the psychological influence of haptic perception of stiffness in AR and VR, found that a virtual object was perceived to be softer in AR than in VR. This finding suggests that a visual-haptic illusion can also be observed in MR.

2.2.2. Auditory cues on haptic perception
Auditory cues also affect users’ perceptions of the physical properties of virtual objects. In an early study, for example, Jousmaki and Hari (1998) put forward the “parchment-skin illusion,” by demonstrating that sound synchronized with hand-rubbing could strongly affect users’ tactile sensations. As the average level of auditory feedback increased, the perceptions of roughness or moisture decreased, while perceptions of smoothness or dryness increased. Similarly, in Kagimoto et al. (2009), the effect of MR visual stimulation on haptic impressions of roughness was enhanced when auditory stimulation occurred at the same time.

In addition, Ternes and Maclean (2008) employed a “rhythm” combined with frequency and amplitude in designing and utilizing haptic icons. By analyzing how users perceptually organize tactile rhythm, they proposed distinguishable tactile stimuli that can be used in haptic icons in large quantities. Rausch et al. (2012) showed that auditory feedback can substitute for haptic feedback. In their experiment with a virtual drilling task, auditory feedback could compensate for a lack of haptic feedback, especially when both feedbacks were simulated simultaneously. Lee and Choi (2013) proposed a perception-level audio-to-vibrotactile translation algorithm. By focusing on the perceptual characteristics of audio and tactile stimuli, they demonstrated that audio-tactile feedback could be synchronized through the suggested algorithm for a richer user experience in immersive applications. Similarly, Lugtenberg et al. (2018) showed that users’ perceptions of thickness could be changed by auditory-tactile feedback. Specifically, thick and flexible objects with distinctive tactile features and vibratory responses in a tapping action could be perceived as being thin.

2.3. Active touch and passive touch
Haptic perception can be defined as the “perceptual processing of inputs from multiple subsystems including those in skin, muscles, tendons, and joints” (Wolfe et al., 2006). Haptic perception is employed in various tasks, such as rubbing, scratching, rolling, and vibration. Different tasks generate distinctive haptic experiences depending on the way perceivers touch an object. In this regard, Gibson (1962) distinguished “touching” (active touch) from “being touched” (passive touch). He emphasized the difference between the motion of an object and human movement relative to the object, specifically in sensory psychology. For active touch, variations in skin stimulation are caused by human movements and object features (left panel in Fig. 2). Passive touch is involved only with the excitation of skin receptors and underlying tissues caused by object movements and features (right panel in Fig. 2). In this vein, Gibson demonstrated the superiority of active touch over passive touch in object-discrimination tasks. Heller (1984) also demonstrated that active touch showed better recognition performance for tactual forms than passive touch did.

Fig. 2
Download : Download high-res image (123KB)
Download : Download full-size image
Fig. 2. Active touch and passive touch.

However, Loomis and Lederman (1984) argued that, in a narrow sense, there was a lack of evidence supporting the superior performance of active touch over passive touch. Similarly, Hughes and Jansson (1994) questioned whether active and passive touch offered equivalent information in perceptions of texture, and argued that this concern should be empirically addressed in information availability and utilization within perceptual-action subsystems. Wagner (2016) offered a similar view. Although the debate over active and passive touch should be addressed along with consideration of other factors, the distinction between the two appears to be meaningful. This distinction has not been properly addressed in connection with cross-modal effects, in particular the effects of visual and auditory cues on haptic perception in MR. To better understand users’ perceptions in MR, the concepts of active touch and passive touch should be applied to virtual objects.

3. Research questions
Building on the evidence regarding the effects of visual and auditory cues on haptic perception, the current study investigated the haptic illusion in a human-computer interaction context implemented in MR. In particular, we employed two interface gadgets as testbeds for a haptic illusion: a cylindrical date/time picker and a vibratory cube (Fig. 3). The date/time picker allows users to feed date and time information to a system by rotating the cylinder, whereas the vibratory cube notifies users by vibration. These functions are commonly used in contemporary mobile user interfaces, allowing users to experience the haptic illusion in the two distinctive contexts of active and passive touch.

Fig. 3
Download : Download high-res image (604KB)
Download : Download full-size image
Fig. 3. Stimuli for active touch (top) and passive touch (bottom).

Using these gadgets, we investigated how visual and auditory cues simulating the gadgets’ motions influence users’ experience of tactile sensations, including roughness of the cylinder and stiffness of a cube on a palm. Roughness and stiffness are fundamental characteristics of a physical object. Individuals judge the roughness of a physical object in the real world by processing visual and auditory cues as well as tactile signals generated by their touch on the object. When all sensory channels are available, individuals predominantly harness tactile signals to evaluate the roughness of the object and pay less attention to signals obtained through other channels (Spence et al., 2001). When tactile information is not available, people rely on cues of alternative modalities such as visual and auditory information. Considering that tactile feedback is neither sufficient nor realistic in an MR environment, individuals may harness visual and auditory signals in evaluating haptic sensations of a virtual object.

Note that visual cues are directly associated with roughness perception. For example, visual cues on the surface of an object (Kagimoto et al., 2009) and its shape (Nakahara et al., 2007) provide information that allows users to infer how rough an object is. In the case of the picker, which is a laterally oriented cylinder, the visual cues of the rotation may inform users of the roughness of the object, as the speed of rotating is associated with mass, which is in turn related to the roughness of the material. Particularly when the motion of the object is triggered by users, users may have expectations regarding how fast the cylinder would rotate based on visual cues of the cylinder. The visual rotation cues triggered by users may allow users to adjust their expectations of the roughness of the materials.

In addition, auditory information can influence perceptions of roughness. The conventional sound effect generated by the picker is a clicking sound, which is generated by a rotating gear inside the picker hitting a rod. This sound may inform users of the roughness of the object. Previous studies on auditory effects showed that people harness sound pitches generated by an object in judging the roughness of an object (Altinsoy, 2008; Jousmäki and Hari, 1998). Considering that rough materials create sounds with low pitches, user perception of roughness in active touch was inversely proportional to the pitches of the inducing sounds.

The haptic illusion in roughness will therefore depend on visual and auditory cues. How visual and auditory cues on perceived roughness will interact is our first research question.

RQ1: Do users’ perceptions of the roughness of a rotating picker in active touch differ by visual and auditory cues?

We predicted that visual and auditory cues would influence the haptic experiences of a vibratory cube. According to the findings of Gaffary et al. (2017), Hirano et al. (2011), and Rosa et al. (2015) a virtual object can be perceived to be stiffer in a low-speed movement than in a high-speed movement. That is, the stiffness illusion can be affected by visual cues involving speed. In addition, Rausch et al. (2012) and Lugtenberg et al. (2018) showed that users perceived an object as stiffer as the frequency of the sound generated by the object was attenuated. For a virtual object in MR, it can be expected that users will perceive it to be stiffer when associated with a low-pitched sound than with a high-pitched sound. No studies on the interaction between visual and auditory cues on perceived stiffness are available, and this knowledge gap needs to be addressed, which produced our second research question.

RQ2: Do users’ perceptions of the roughness of a vibratory cube in passive touch differ by visual and auditory cues?

4. Method
4.1. Study design and participants
We conducted a laboratory experiment to test our research questions. In the experiment, participants interacted with virtual objects in an active (i.e., active-touch task) and passive manner (i.e., passive-touch task), while the objects demonstrated different movement (fast vs. slow speed) and sounds (low-frequency accentuated, high-frequency accentuated, or non-manipulated). We examined how these visual cues of movements and auditory cues of sounds influenced participants’ haptic perception of roughness (active-touch task) and stiffness (passive-touch task) of the virtual objects.

Forty undergraduate or graduate students were recruited in a large private university in South Korea as participants (13 males and 27 females; aged between 19 and 30 years). Before the main experiment, participants stated that they had no difficulty in processing visual and auditory stimuli. Although they were not familiar with MR equipment, they did not report any specific problems in wearing an MR headset or making a finger or arm movement. They were compensated with 5000 Korean won (about 4.50 USD).

4.2. Stimuli and manipulations
For the experimental stimuli, a virtual date/time picker and vibratory cube were designed (Fig. 3). Both gadgets are commonly used in mobile applications and widely applicable to MR platforms. The date/time picker was used in the active-touch task. The motion of the object can be triggered by a user turning the cylinder-shaped picker with their finger. The vibratory cube was created for the passive-touch task, in which the cube would be placed on one's palm and move up and down in a certain angular frequency with a phone-vibrating sound.

The visual and auditory cues were manipulated by applying different motion speeds and sounds to the virtual objects (see Table 1). A pilot study was conducted to determine the appropriate levels of manipulation. Results of the pilot study indicated that any changes in the angular speed of the picker became indiscernible at the speed of 5 π/3 rad/s or below, thus we used this as the slow speed for the visual cues of the cylinder. Similarly, any changes in graphical vibration at an angular frequency of 0.4 π rad/s or below were undetectable, and thus we used this as our slow frequency of the visual cues for the cube. Further, we set the visual cues of the fast speed and frequency to be three times as fast as the slow speed and frequency, following the just-noticeable-differences (Gaffary et al., 2017; Li et al., 2008).


Table 1. Vision and audio manipulations.

Active-touch task (date/time picker)
Visual cue: Angular speed	Auditory cue: Frequency accentuated
Slow	5 π/3 rad/s	Non-manipulated	Not changed
Fast	5 π rad/s	High-frequency accentuated	High-Pass Filter (Cutoff Frequency: 3713 Hz)
Low-frequency accentuated	Low-Pass Filter (Cutoff Frequency: 936.9 Hz)
Passive-touch task (vibratory cube)
Visual cue: Angular frequency	Auditory cue: Frequency accentuated
Slow	0.4 π rad/s	Non-manipulated	Not changed
Fast	1.2 π rad/s	High-frequency accentuated	High-Pass Filter (Cutoff Frequency: 349 Hz)
Low-frequency accentuated	Low-Pass Filter (Cutoff Frequency: 108.8 Hz)
The size of the picker was set to 3.0 cm in radius and 4.2 cm in length, and that of the cube to 4.2 cm in length. The picker and cube were implemented in HoloLens, a headset manufactured by Microsoft Inc. They appeared completely in the view range1 (30° horizontal and 17.5° vertical) of HoloLens. The vibrating cube travels from its stationary position to the extreme position (5 cm). We set the experiment without any tracking system for recognizing participants' hands but located the virtual gadgets. The gadgets were located 50 cm far from the headset2 so that participants would reach out their hands to interact with them in a manner of active and passive touch.

In the current study, the audio sources of the clicking sound3 for the date/time picker and the vibrating sound4 for the vibratory cube were obtained from an online sound distributor. Previous studies did not present any agreed way of manipulating auditory cues, but they increased or decreased sounds in certain bands (Altinsoy, 2008; Guest et al., 2002; Jousmäki and Hari, 1998).

Following these previous studies, we distorted the fundamental frequency (at 1832 Hz for the clicking sound; at 179.3 Hz for the vibrating sound) of original sounds into −1 or +1 octave pitch to give their distinctive material aspects (Blattner et al., 1989). As audio preprocessing, we manipulated low / high-frequency accentuated sound with low-pass filter and high-pass filter, respectively. To apply a cutoff frequency for low-pass filter, we set the pitch as around −1 octave pitch (at 835 Hz for the clicking sound; at 108.8 Hz for the vibrating sound), and for high-pass filter, we set the pitch as around +1 octave pitch (at 3374 Hz for the clicking sound; at 349 Hz for the vibrating sound). After filtering each sound, we finally adjusted its volume to a similar level of each original sound volume (MATLAB, 2018).

4.3. Procedure and tasks
Upon arrival, participants were welcomed and given an introduction to the experiment. They were told that they would experience an MR environment as a test and interact with virtual objects in the environment. They are also noted that the experiment was not designed to gauge how well they conduct tasks or how fast they give a correct answer. After the introduction, participants were asked to put on HoloLens and perform the active- and passive-touch tasks (Fig. 4). The order of the tasks was counterbalanced across participants.

Fig. 4
Download : Download high-res image (706KB)
Download : Download full-size image
Fig. 4. Active-touch and passive-touch tasks.

The active-touch task was devised to allow participants to initiate the motion of the virtual cylinder volitionally. Participants first underwent a training session, in which they practiced their hand gestures following an oral instruction offered by HoloLens.5 Participants learned that they can adjust their head to put a cursor on the cylinder (the image 1 in the upper panel in Fig. 3). Then, by putting their thumb and index finger together (i.e., air-tap gesture, see the image 2 in the upper panel in Fig. 3), they are ready to control the object. They turned upward the cylinder by dragging their fingers up (the image 3) and downward by dragging them down (the image 4 in the upper panel in Fig. 3). When the cylinder spanned, it also generated the clicking sound. The cylinder stopped spinning and sounding when participants opened their two fingers. After they felt comfortable in controlling the object, participants moved on to the main session. In the main session, a set of three cylinders was displayed in sight of participants (see Fig. 4). The task was to rotate the set of the cylinders in turn using the participant’s index finger and report the roughness on the cylinders on a 7-point Likert scale (1 = not rough at all; 7 = extremely rough). For the set of the three cylinders, the three auditory cues were implemented in a random order but the same visual cues were. After participants tried the three cylinders in the set, they performed this trial two more times, with the order of the auditory cues being changed at random. This procedure allowed us to collect participants’ ratings on roughness three times for each condition. After being performed these trials in one condition of the visual cues, the other condition was implemented. The order of the visual cues was counterbalanced across participants.

The passive-touch task was designed to provide participants with an experience of passively sensing the vibratory cube moving upon their palm. A training session for the passive-touch task was also offered in a similar way to the training session for the active-touch task (see the lower panel in Fig. 3). In the main session, the virtual cube and a set of three buttons were displayed to participants. Participants were asked to place their left hand under the cube with their palm facing the cube and their right hand near the buttons. Participants randomly pressed each button using the air-tap gesture and experienced the cube moving on their left hand. The cube generated the sound when every position at the initial vibration point. They then reported the stiffness of the cub on a 7-point Likert scale (1 = not stiff at all; 7 = extremely stiff). This trial was repeated three times. The buttons in a set were randomly associated with three auditory cues, while the same visual cues were used for all buttons for the three trials. The other visual cues were implemented, once participants finished the first three trials. The order of the visual cues was counterbalanced.

Approximately two minutes were given for a break between the active-touch and passive-touch tasks. No time limit to perform the tasks was imposed.

5. Results
This section analyzed the results on the research questions: (1) haptic illusions in roughness for active touch; (2) haptic illusions in stiffness for passive touch. The descriptive statistics of the data are presented in Table 2. The number of the responses for each condition was 120 (= 40 participants × 3 trials). One may maintain that participants’ responses measured on a Likert-type are ordinal, and thus not appropriate for ANOVA (Gaito, 1980). Yet, it has been argued that measuring interval latent variables (i.e., true responses) using an ordinal variable leads to a violation of the normality assumption, to which tests of central tendency (e.g., ANOVA) are robust (Norman, 2010). Therefore, we performed ANOVAs in this experiment.


Table 2. Basic statistics for roughness (active touch) and stiffness (passive touch) – mean (standard deviation).

Slow speed	Fast speed
High-frequency accentuated	Non-manipulated	Low-frequency accentuated	High-frequency accentuated	Non-manipulated	Low-frequency accentuated
Active-touch task	3.975(1.707)	4.975(1.399)	5.892(1.114)	1.883(1.132)	3.175(1.465)	4.025(1.867)
Passive-touch task	3.550(1.833)	4.000(1.347)	4.250(1.941)	3.433(1.846)	4.042(1.405)	4.167(1.784)
5.1. Research question 1 – Active-touch task
To address RQ1, concerning the haptic illusions in roughness caused by visual and auditory cues in active touch, a two-way analysis of variance (ANOVA) was performed with visual and auditory cues as independent variables and roughness perception as a dependent variable. The main effect of visual cues on roughness perception was significant, F (1, 714) = 305.474, p < 0.001, and that of auditory cues was also significant, F (2, 714) = 114.433, p < 0.001. However, the visual cues by auditory cues interaction was not significant, F (2, 714) = 0.646, p = 0.525. Specifically, the ratings of perceived roughness were higher for the virtual objects with a slower motion speed and a low-frequency accentuated sound. Tukey's post-hoc tests for auditory cue revealed that any pair of the three sound frequencies was different in perceived roughness (all p values < 0.001).

5.1. Research question 2 – Passive touch experiment
For RQ2, concerning the haptic illusions in stiffness by visual and auditory cues in passive touch, we performed a two-way ANOVA with two independent variables (visual and auditory cues) and one dependent variable (stiffness perception). The main effect of auditory cues on stiffness perception was significant, F (2, 714) = 10.862, p < 0.001. but the main effect of visual cues, F (1, 714) = 0.108, p = 0.743 and the visual cues by auditory cues interaction were not significant, F (2, 714) = 0.108, p = 0.898. The ratings of perceived stiffness were higher for the virtual objects with a sound of low-frequency accentuated. Tukey's post-hoc tests were conducted to examine the differences among the three different-frequency accentuated sound levels. The results revealed that significant differences were found between the high level and the medium level (p = 0.003) and between the high level and the low level (p < 0.001).

6. Discussion
The results of the current study suggest that users are engaged in cross-modal information processing when interacting with virtual objects in MR. Manipulation of visual and auditory cues employed in the current experiment influenced participants’ perception of the physical characteristics of the virtual gadgets in MR. This finding of multisensory information processing broadens our understanding of users’ perception of virtual objects in MR. That is, information delivered by different sensory channels is not processed as separate sources but combined to form a holistic interpretation of a virtual object in MR. Thus, visual and auditory information allows users to develop imaginary perception of haptic stimulation. The current study underlines the potential of MR technologies creating haptic experience without implementing actual haptic stimulation. Given that current MR technologies are often provided without a realistic tactile device (e.g., haptic gloves), visual and auditory cues can be utilized to enrich MR users’ sensory experience. Users’ interactions with MR applications can be improved through multiple sensory channels based on cross-modal effects.

6.1. Haptic illusions by visual and auditory cues
While the current study found that visual and auditory cues influenced users’ perceptions of tactile characteristics of virtual gadgets, it also revealed nuanced differences in the way participants experienced the haptic illusion through active and passive touch. For active touch in MR, haptic illusions were found to be generated by visual and auditory cues. In the rotating task, the manipulation of visual and auditory cues influences perceived roughness. Given the manipulated visual cues, participants perceived the virtual objects with a slower motion speed and a low-frequency accentuated sound to be rougher. These findings are consistent with those of previous studies, revealing the cross-modal effects of visual cues (Hashiguchi et al., 2014; Issartel et al., 2015) and auditory cues (Altinsoy, 2008; Jousmäki and Hari, 1998).

The current study also found that the interaction effect between visual and auditory cues was not significant. This indicates that visual and auditory cues can influence the haptic illusion via independent channels. We propose this independence originates with individuals inferring the roughness of a material from its distinctive physical features. That is, study participants may have used the visual cues of rotating speed to infer roughness by considering that speed generated by a given force is associated with the mass or density of the material. In contrast, the sound generated by the rotating cylinder may be assumed to be indicative of the hardness of the object, which helps participants estimate roughness. The lack of a significant interaction in the effect of visual and auditory cues suggests that these two inference processes are independent of each other.

Regarding passive touch in MR, the current study showed that haptic illusions are generated only by auditory cues, and not by visual cues. In the vibrating movements of the virtual object, participants perceived that a virtual object generating a low-frequency accentuated sound was stiffer. This finding is consistent with those of Rausch et al. (2012) and Lugtenberg et al. (2018). However, the main effect of visual cues was not significant, and neither was the interaction effect between visual and auditory cues.

One may suspect that manipulation of the visual cues may not have been strong enough to influence participants’ perceptions of stiffness. We proposed that the null effect of the visual cues was due to the mode of interaction applied in the experiment, i.e., the passive touch. Given the significant effect of the visual cues in active touch, we suggested that the visual cues generated by active tough were informative of the physical characteristics of an object, as one can infer them from the association between the force they apply to the object and the visual feedback to their input. Yet for users of the passive touch, this inference was not possible as they merely observed an object moving by itself. Visual cues in a passive touch context therefore may not generate effects strong enough to influence the haptic experience.

6.2. Practical utilization of haptic illusions
Based on our findings, haptic displays have practical applications in various MR services. Although interactive technologies with external devices are available, visual and haptic rendering remains challenging. These concerns include instability due to poor object tracking, error accumulation, and coincidence errors of visual and tactual stimuli. To address these concerns, it may be possible to focus on multisensory cues that shape illusionary haptic senses rather than on the haptic equipment generating the stimulation that users experience as close to real but awkward. Haptic displays are more effective when no limitation in haptic sensation is provided (Bianchi et al., 2006), and the tactile internet can provide an interface that can be touched by users (Fettweis, 2014).

One domain in which virtual tactile displays can be widely used is the textile fashion industry (Gray, 1998). The application of a 3D virtual space for clothing products has achieved wide-scale popularity. The use of information and communication technology is key to the design of modern fashion textile products (Pascal and Nadia, 2000). In addition, virtual-space applications can be utilized in the form of audio-haptic displays in a virtual environment that allows for virtual fitting of clothes, as the technologies are promising (Lang et al., 2003; Saha et al., 2014). However, the demand for virtual haptics or digital sensory technologies is still in an early stage of development.

The perceptual illusion from multisensory channels can be used in various other industries and markets, including videos, music, movies, and games, all of which can be promoted in MR. This utilization can be beneficial in a ubiquitous and integrated environment in which content is directly produced and shared.

6.3. Directions for future research
Future research can be conducted based on the findings of the current study. First, our results on the haptic illusion are based on the simple interaction, in which they were asked to turn in a virtual date/time picker and have a virtual cube on a palm. While these simple tasks presented participants with well-controlled conditions in which they experienced haptic illusion, one may want to examine how haptic illusion can occur in more complicated MR environments. For example, users of an MR system may perform multitasking, as such practice is common among media users (e.g., TV viewing and computer use; (Brasel and Gips, 2011)). Haptic illusion in this context may be experienced to a greater extent as they may pay attention less to making distinctions on the sources of sensory channels but more to processing the holistic aspect of virtual stimuli.

Also, future research can consider employing a more sophisticated way of handling user's inputs. In the current study, a detecting module equipped with HoloLens was used to capture participants’ inputs. The detecting module identifies only whether participants moved their finger, not how forcefully the finger moves. Regardless of the force participants applied to the object, the virtual gadgets in our experiment demonstrated the predetermined visual and auditory cues. When people evaluate the physical characteristics of an object, however, they often try the object by applying different forces and sense how the object feels (e.g., pressing forcefully and lightly to check the stiffness of a surface). Thus, one may expect that people trying a virtual object in an MR environment use a similar strategy if the environment affords such interaction. A future study can test this prediction, by employing an MR system that incorporates the forces users apply to a virtual object and presents visual and auditory cues corresponding to users’ input force.

Lastly, a future study may use a research methodology complement to that of the current study. We collected self-reported quantitative data measured on a Likert-type scale. While this type of measurement allows us to analyze them using rigorous statistical models (e.g., ANOVA), one may miss complicating aspects of user experience in interacting with virtual objects. Thus, we propose a future study employing a qualitative research method for complementing the quantitative analysis as shown in previous studies (Lee and Choi, 2013; Pusch et al., 2009).

7. Conclusion
Recent information and communication technologies have given MR users interactive experiences with virtual contents similar to those of actual objects. The present study employed active and passive touch to test multisensory effects on roughness and stiffness, respectively. It is our belief that our understanding of haptic illusion can be extended to a various sensation of physical characteristics of virtual objects, such as audiotactile interaction for dryness/moisture (Jousmäki and Hari, 1998), visual effects on softness (Punpongsanon et al., 2015), or weight aspects based on size for "Charpentier illusion" (Charpentier, 1891). Implementing audio and visual cues to simulate the virtual tactual senses of those characteristics will allow users to enjoy enhanced experiences of a virtual object in an MR environment.