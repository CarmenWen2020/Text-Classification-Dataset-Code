To enable the large scale and efficient deployment of Artificial Intelligence (AI), the confluence of AI and Edge Computing has given rise to Edge Intelligence, which leverages on the computation and communication capabilities of end devices and edge servers to process data closer to where it is produced. One of the enabling technologies of Edge Intelligence is the privacy preserving machine learning paradigm known as Federated Learning (FL), which enables data owners to conduct model training without having to transmit their raw data to third-party servers. However, the FL network is envisioned to involve thousands of heterogeneous distributed devices. As a result, communication inefficiency remains a key bottleneck. To reduce node failures and device dropouts, the Hierarchical Federated Learning (HFL) framework has been proposed whereby cluster heads are designated to support the data owners through intermediate model aggregation. This decentralized learning approach reduces the reliance on a central controller, e.g., the model owner. However, the issues of resource allocation and incentive design are not well-studied in the HFL framework. In this article, we consider a two-level resource allocation and incentive mechanism design problem. In the lower level, the cluster heads offer rewards in exchange for the data owners' participation, and the data owners are free to choose which cluster to join. Specifically, we apply the evolutionary game theory to model the dynamics of the cluster selection process. In the upper level, each cluster head can choose to serve a model owner, whereas the model owners have to compete amongst each other for the services of the cluster heads. As such, we propose a deep learning based auction mechanism to derive the valuation of each cluster head's services. The performance evaluation shows the uniqueness and stability of our proposed evolutionary game, as well as the revenue maximizing properties of the deep learning based auction.

SECTION 1Introduction
Today, the predominant approach for Artificial Intelligence (AI) based model training is cloud-centric, i.e., the data owners transmit the training data to a public cloud server for processing. However, this is no longer desirable due to the following reasons. First, privacy laws, e.g., the General Data Protection Regulation (GDPR) [1], are increasingly stringent. In addition, the privacy-sensitive data owners can opt out of data sharing with third parties. Second, the transfer of massive quantities of data to the distant cloud burdens the communication networks and incurs unacceptable latency especially for time-sensitive tasks. As such, this necessitates the proposal of Edge Computing [2] as an alternative, in which raw data are processed at the edge of the network, closer to where data are produced.

The confluence of Edge Computing and AI gives rise to Edge Intelligence, which leverages on the storage, communication, and computation capabilities of end devices and edge servers to enable edge caching, model training, and inference [3] closer to where data are produced. One of the enabling technologies [4] of Edge Intelligence is the privacy preserving machine learning paradigm termed Federated Learning (FL) [5]. In FL, only the updated model parameters, rather than the raw data, need to be transmitted back to the model owner for global aggregation. The main advantages of FL are: (i) FL enables privacy preserving collaborative machine learning, (ii) FL leverages on the computation capabilities of IoT devices for local model training, thus reducing the computation workload of the cloud, and (iii) Model parameters are often smaller in size than raw data, thus alleviating the burden on backbone communication networks. This has enabled several practical applications, e.g., in the development of next-word-prediction models for text messaging [6], healthcare [7], Unmanned Aerial Vehicles (UAV) sensing [8], and mobile edge computing [9].

However, the FL network is envisioned to involve thousands of heterogeneous distributed devices, e.g., smartphones and Internet of Thing (IoT) devices [10]. In this case, the communication inefficiency remains a key bottleneck in FL. Specifically, node failures and device dropouts due to communication failures can lead to inefficient FL. Moreover, workers, i.e., data owners, with severely limited connectivity are unable to participate in the FL training, thus adversely affecting the model's ability to generalize. As such, solutions from edge computing have recently been incorporated to solve the communication bottleneck in FL. In [4], [11], [12], a hierarchical FL (HFL) framework is proposed in which the workers do not communicate directly with a central controller, i.e., the model owner. Instead, the local parameter values are first uploaded to edge servers, e.g., at base stations, for intermediate aggregation. Then, communication with the model owner is further established for global aggregation. Besides reducing the instances of global communications with the remote servers of the model owner, this relay approach reduces the dropout rate of devices.

While [11] discusses convergence guarantees and presents empirical results to show that the HFL approach does not compromise on model performance, the challenges of resource allocation and incentive mechanism design have not yet been well-addressed in the HFL framework. In 5G and Beyond networks, the resource sharing and incentive mechanism design for end-edge-cloud collaboration is of paramount importance to facilitate efficient Edge Intelligence [4].

In this paper, we consider a decentralized learning based system model inspired by the HFL. In our system model, there exist data owners, hereinafter referred to as workers, that participate in the FL model training facilitated by different cluster heads, e.g., base stations that support the intermediate aggregation of model parameters and efficient relaying to the model owners (Fig. 1). We consider a two-level resource allocation and incentive design problem as follows:

Lower level (Between workers and cluster heads): Each worker can freely choose which cluster to join. To encourage the participation of workers, the cluster heads offer reward pools to be shared among workers based on their data contribution in the cluster. For example, a worker that has contributed more data1 during its local training will receive a larger share of the reward pool. Moreover, the cluster heads offer the workers resource blocks, i.e., bandwidth, to facilitate efficient uplink transmission of the updated model parameters. However, as more workers join a cluster, the payoffs are inevitably reduced due to the division of rewards over a larger number of workers and the increased communication congestion. Thus, the cluster selection strategies of each worker can affect the payoffs of other workers. Accordingly, the workers may slowly adapt their strategies in response to other workers. In contrast to conventional optimization approaches, we use the evolutionary game theory [14] to derive the equilibrium composition of the clusters. Our game formulation enables the bounded rationality and worker dynamics to be captured. Specifically, the workers gradually adapt their strategies in response to other non-cooperative workers. To achieve their objectives, they observe each others’ strategies and gradually adjust their strategies accordingly. The solution is therefore, not immediately derived.


Fig. 1.
An illustration of our system model involving two populations of workers. Within each population, all workers have the same data quantities. The workers may choose to join either cluster head. The dynamics is modeled at the first level using the evolutionary game. Cluster head 2 eventually has higher data coverage across the network given that it offers the workers higher rewards. Thus, the services of cluster head 2 are valued higher at the auction.

Show All

Upper level (Between cluster heads and model owners): There may be multiple model owners in the network that aim to train a model for their respective usage collaboratively with the participation of the workers and cluster heads. However, at any point of time, each worker and cluster head can only participate in the training process with a single model owner. To derive the allocation of cluster head to the model owner, as well as the optimal pricing of the services of the cluster head by the competitive model owners, we adopt a deep learning based auction mechanism which preserves the properties of truthfulness of the bidders, while simultaneously achieving revenue maximization for the cluster heads.

The main contributions of our paper are as follows:

We propose a joint resource allocation and incentive design framework for the HFL. The “Edge for AI” [15] approach supports decentralized Edge Intelligence, i.e., FL at the edge with reduced reliance on a central controller.

We model the cluster selection decisions of the workers as an evolutionary game. Then, we provide proofs for the uniqueness and stability of the evolutionary equilibrium. In contrast to conventional optimization tools which assume that the players are perfectly rational, our model enables us to capture the dynamics and bounded rationality of player decisions.

To assign the cluster heads to model owners, we use a deep learning based auction mechanism. In contrast to conventional auctions, the deep learning based auction ensures seller revenue maximization while satisfying the individual rationality and incentive compatibility constraints.

The organization of our paper is as follows. In Section 2, we provide a review of related works. In Section 3, we discuss the system model and problem formulation. In Section 4, we study and analyze the evolutionary game. In Section 5, we discuss the deep learning based auction mechanism. Section 6 provides the performance evaluation and Section 7 concludes the paper.

SECTION 2Related Work
FL is a privacy-preserving machine learning paradigm first proposed in [5]. In distributed learning schemes such as FL, the communication cost often dominates the computation cost. In particular, the uplink transmission rate of workers is a major bottleneck in the training process and can lead to the straggler's effect [16]. Several works have proposed a variety of solutions, e.g., model compression techniques such as quantization and subsampling [17], client selection protocols to reduce the occurrences of stragglers [13], as well as Broadband Analog Aggregation (BAA) with over-the-air computation [18].

However, despite the above measures, the FL process is still susceptible to device dropouts. In addition, devices that are located at geographically distant locations are unable to participate in the FL model training process. This affects the model's ability to generalize well. Recently, edge computing-inspired solutions have been proposed to further enhance the communication efficiency of FL. These methods generally attempt to reduce the reliance of the FL training process on a central controller. In [11], the HFL framework is proposed in which the workers do not communicate directly with a central controller, i.e., the model owner. Instead, the intermediate aggregation of parameters is first conducted at the edge, e.g., with the aid of cluster heads, such as, base stations or other devices. Then, communication with the central controller is established only when there is a need for global aggregation.

Similarly, [19] proposes that mobile devices can form clusters to participate in self-organized FL. Besides improving communication efficiency, it reduces the likelihood that the training fails due to the unexpected malfunctioning of the central controller. In [19], the cluster head selection algorithm is studied, and the cluster head is chosen based on its social relationship with the other devices. The studies in [4], [12] also propose a collaborative FL, in which the device-to-device (D2D) and device-to-edge (D2E) communication is leveraged to ensure the efficient transmission of model parameters to the model owner.

While the aforementioned studies validate the feasibility of HFL and highlight the advantages of conducting FL in a decentralized manner, thereby reducing the reliance on a central controller, resource allocation and incentive mechanism design have not been addressed in the HFL framework. For example, in a network, the workers are free to join any cluster. In addition, they need to receive some rewards as a compensation for the resources expended during training. Given that the worker decisions are dynamic, it is important to develop a framework that can potentially serve to guide each cluster head's incentive design. As such, in this paper, we propose the evolutionary game to incorporate and analyze the dynamics of cluster selection in HFL.

In addition, the services of the cluster head have to be compensated. Given the competing model owners within a network, we utilize a deep learning based auction mechanism [20] to match the cluster head according to the varying valuations of the model owner.

SECTION 3System Model and Problem Formulation
3.1 System Model
We consider a network that consists of a set N= {1,…,n,…,N} of N workers. There exist L distinct model owners, each of which desires to develop an AI model for its own purposes, e.g., traffic crowdsensing [21] or location based recommendation [22]. Given the communication constraints of individual workers, the central controller-reliant conventional FL architecture is prone to high device dropout rates [12]. Moreover, we have J cluster heads, e.g., base stations, employed across the network to facilitate the HFL task, the set of which is denoted by J={1,…,j,…,J}. Each worker can choose to associate with any one j∈J cluster head.

Without loss of generality, each cluster head j∈J can only serve a single model owner l∈L at a time and facilitates the HFL process for a cluster j of pjN workers, where ∑Jjpj=1.

In HFL, a cluster head j first receives an initial global model, i.e., parameters denoted by the vector w, from a model owner that has chosen its services. It then relays the global model to its workers. The FL model training takes place over K iterations to minimize the global loss FK(w) where K is stipulated by the model owner. Each kth iteration, k∈{1,…,k,…,K}, consists of three steps [17] namely:

Local Computation: Each worker trains the received global model w(k) locally.

Wireless Transmission: The worker transmits the model parameter update to its cluster head j.

Intermediate Model Parameter Update: All parameter updates received from its pjN workers are aggregated by the cluster head to derive an updated intermediate model w(k+1)j, which is then transmitted back to the worker for the (k+1)th training iteration.

After K iterations, the intermediate model w(K)j is transmitted to the model owner for global aggregation with the intermediate parameters collected from other clusters. A new set of updated global parameters is derived by the model owner which sends it out to its cluster heads for another round of local model training.

In this paper, we assume that the cluster heads are pre-determined, e.g., through the cluster head selection algorithms based on energy efficiency [23], [24], trust [25], and social effects [26]. Instead, we focus our study on a two-level optimization problem as follows: i) in the lower level, we adopt an evolutionary game approach to study the dynamics of cluster selection by the workers to derive the dynamics of the composition of each cluster, and ii) in the upper level, we adopt a deep learning based auction to value each cluster head's worth to a model owner.

3.2 Lower-Level Evolutionary Game
In the lower level, the cluster formation is derived given J cluster heads. Each cluster head has the objective of attracting more workers to join its cluster, since this ensures that the cluster will have a larger data coverage across the network. With a larger data coverage, the cluster value is increased, e.g., due to the fact that the model performance increases with more training data [16].

To encourage the participation of the workers, each cluster head offers a reward pool to be shared by all workers in the cluster. The reward to be distributed to each worker is based on the proportion of the worker's contributions in the cluster, i.e., its data quantity relative to the total amount of data in the cluster. On one hand, a cluster that offers a high reward pool is more attractive to workers. On the other hand, when more workers join that cluster, the reward pool has to be shared among a larger number of workers. Thus, the worker decisions as to which cluster they choose to join are interrelated with the decisions of other workers. We adopt an evolutionary game theory approach to model the dynamics of cluster formation.

3.3 Upper-Level Deep Learning Based Auction
The lower-level evolutionary game gives us the data coverage of each cluster. For example, a cluster head that has greater data coverage will be deemed more valuable to an FL model owner, since the model performance, e.g., inference accuracy, is improved [27]. However, recall from Section 3.1 that there exists more than one model owner in the network seeking to secure the services of the cluster head to facilitate the HFL. In consideration of the competition among model owners, we adopt an auction mechanism in which L model owners bid for the services of each cluster head j∈J. Specifically, we utilize the deep learning based auction mechanism which has the attractive properties of ensuring the truthfulness of the bidders, as well as revenue maximization for the seller (i.e., cluster head), as discussed in Section 5.

SECTION 4Lower-Level Evolutionary Game
4.1 Evolutionary Game Formulation
In the following, we formulate the cluster selection as an evolutionary game:

Players: The set of workers N={1,…,n,…,N} in the FL network are the players of the evolutionary game. For clarity, we use the terms “workers” hereinafter.

Population: We partition the workers into M={1,…,m,…,M} populations based on the data quantities2 that each worker owns or the data coverage proportion of the worker using conventional data mining tools, e.g., k-means. The data coverage proportion can be a reflection of the workers’ market share in the case when organizations are considered, e.g., based on the proportion of users a bank has, or usage frequency in the case in which individuals are considered, e.g., based on how often the worker uses an IoT device. Each worker of population m owns dm training data samples, whereas the total data quantity in a population is denoted Dm. We denote the number of workers in each population as nm=pmN where pm∈[0,1] and ∑Mm=1pm=1. In other words, we have M populations of workers in the network, where all workers within a population own the same number of data samples.

Strategy: The strategy of each worker in population m is the selection of a cluster to join so as to achieve utility maximization. The strategy space of each worker n in population m is denoted by S(m)n={a(m)n,1,…,a(m)n,j,…,a(m)n,J} in which a(m)n,j is a binary variable where a(m)n,j=1 represents that the worker n in population m chooses the cluster j, whereas a(m)n,j=0 indicates otherwise.

Population Share: We denote the fraction of population m that selects strategy j, i.e., cluster j, by x(m)j where ∑Jj=1x(m)j=1. The population state [29] is denoted by the vector x(m)=[x(m)1,…,x(m)j,…,x(m)J]T∈ X.

Payoff: The expected payoff of each worker is determined by its net utility, which is the difference between the reward that it derives from joining a cluster, and the cost of participating in the FL model training. We further discuss payoffs in Section 4.2.

As an illustration, the system model and game formulation are illustrated in Fig. 1, for the case of two populations. Each worker in population 1 has fewer data samples than each worker in population 2. Each worker in the population can also choose to join either cluster heads. Eventually, each cluster head is associated with a certain level of data coverage, and has its worth evaluated using the auction mechanism discussed in Section 5.

Note that in this paper, we consider that each worker can only join a single cluster, as the worker device is unable to support two instances of model training in parallel. However, our model can be extended to the situation in which each worker can join more than one cluster at a time. In this case, the worker can be modeled to decide, in an evolutionary process, on how its limited resources can be divided among the model owners. Then, x(m)j∈[0,1] is denoted to represent the share of resources a worker from population m contributes to cluster head j, where ∑Jj=1x(m)j=1.

4.2 Worker Utility and Replicator Dynamics
The rewards derived by workers of population m, from joining a cluster j for K iterations of FL model training, is given by:
p(m)j=αjx(m)jDm∑Mm=1x(m)jDm+Rj,(1)
View Sourcewhere αj is the reward pool to be divided across all workers in cluster j based on their data contributions, x(m)jDm∑Mm=1x(m)jDm is the share of rewards based on the worker's data contribution3, and Rj is a fixed reward offered to workers in cluster j based on the compensation for the workers’ participation costs.

The cost of workers of population m incurred from joining a cluster j is given by an addition of the computation and communication cost over the K iterations of the model training. The computation cost is as follows:
ccmpm=ηκθmf2m,(2)
View Sourcewhere η is the unit cost of energy consumption, κ is the coefficient of the value that is determined by the circuit architecture of the worker Central Processing Unit (CPU) [30], θm is the number of CPU cycles required to perform local computation, i.e., model training, and f2m refers to the computation capability of the worker which is determined by the clock frequency of the worker CPU. Without loss of generality, we have the computation cost held constant throughout for all workers, i.e., ccmpm=ccmp, ∀m∈M. To account for the varying computation and communication capabilities, we can straightforwardly extend our work to include multiple heterogeneous populations with varying computation capabilities. For example, if there are Λ varying computation costs, we can have ΛM populations accordingly.

The main benefit of HFL is that devices with communication constraints are able to participate in FL. To facilitate the communication of parameters, the cluster head, e.g., a base station, distributes communication resource blocks to all participants within the cluster. On the one hand, clusters with more communication resources are attractive to participants since the participants can benefit from a higher achievable uplink transmission rate. On the other hand, with more participants attracted to join the cluster, the increased competition for resource blocks leads to more congestion. As such, following [31], we model the disutilities arising from network congestion effects as
ccomm,j(t)=ζj(∑m∈Mx(m)j(t))2,(3)
View Sourcewhere ζj is the congestion coefficient determined by the resource constraints of the cluster head [31], whereas (∑m∈Mx(m)j)2 represents the usage profile across populations in the network for a particular cluster. Specifically, a cluster head with more resources will have a lower congestion coefficient. Moreover, workers in a less-populated cluster experience less congestion.

The total cost of participation incurred by worker nm (of population m) in cluster j is obtained as
c(m)j(t)=ccmp+ccomm,j(t).(4)
View SourceRight-click on figure for MathML and additional features.

At time t, the net utility that the workers in class m receive for their participation in cluster j is:
u(m)j(t)=U(p(m)j(t)−c(m)j(t)),(5)
View SourceRight-click on figure for MathML and additional features.where we assume U(⋅) to be a linear utility function indicating the risk neutrality of workers without loss of generality [8], [16].

Accordingly, at time t, the average utility of workers in population m across all J clusters is
u¯(m)(t)=∑j=1Jx(m)ju(m)j(t).(6)
View Source

In practice, information regarding the utility derived from joining different clusters can be exchanged and compared among workers within the network [32]. Workers may thus switch from one cluster to another to seek higher utilities. To capture the dynamics of the cluster selection and model the strategy adaptation process, we define the replicator dynamics [33] as follows:
x˙(m)j(t)=f(m)j(x(m)(t))=δx(m)j(t)(u(m)j(t)−u¯(m)(t)),∀m∈M,∀j∈J,∀t,(7)
View Sourcewhere δ refers to the positive learning rate of the population that controls the speed at which workers adapt their strategies. For example, in a network with communication bottlenecks [32] or negative network effects [26], the learning rate tends to be slower as the worker requires more time to collect the information required to change its decision.

The replicator dynamics is a series of ordinary differential equations (ODEs) with the initial condition x(m)(0)∈X [34]. Specifically, based on the replicator dynamics, workers in population m can adapt their strategy, i.e., switch from one cluster to another if their utilities are lower than the expected utility. The evolutionary equilibrium is a fixed point in (7) that is reached in a particular t when x˙(m)j(t)=0,∀m∈M,∀j∈J. In other words, at the evolutionary equilibrium, workers from all clusters derive an identical payoff such that there is no longer a need to deviate from their prevailing clusters.

In a dynamic system, it is of paramount importance that the equilibrium is stable and unique. In terms of stability, an evolutionary equilibrium remains to be x˙(m)j(t)=0 for all time periods after the equilibrium is first reached. In terms of uniqueness, the same evolutionary equilibrium is reached regardless of the initial conditions. In Section 4.3, we prove the existence, uniqueness, and stability of the solution to (7).

4.3 Existence, Uniqueness, and Stability of the Evolutionary Equilibrium
In this section, we first prove the boundedness of (7) in Lemma 1.

Lemma 1.
The first order derivatives of f(m)j(x(m)(t)) with respect to x(m)v(t) is bounded for all v∈J.

Proof.
For ease of presentation, we omit the notations of t and (m) in this proof. The first order derivative of fj(x) with respect to xv, where v∈J, is given by
dfj(x)dxv=δ[dxjdxv(uj−u¯)+xj(dujdxv−du¯dxv)].(8)
View SourceFor ease of notation, denote A(xj)=∑Mm=1xjDm. Next, we derive dujdxv as follows:
dujdxv=αj(dxjdxvDmA(xj)−xjD2mA2(xj))−2ζj(∑m∈Mxj).(9)
View Source

It follows that ∣∣dujdxv∣∣ and thus ∣∣du¯dxv∣∣ are clearly bounded ∀v∈ J. Therefore, this represents that ∣∣dfj(x)dxv∣∣ is bounded. This proof also applies to all M populations and T time periods.

Theorem 1.
For any initial condition x(m)(0)∈X, there exists a unique evolutionary equilibrium to the dynamics defined in (7).

Proof.
From Lemma 1, we have proven that the replicator dynamics f(m)j(x(m)(t)) is bounded and continuously differentiable ∀x(m)(t)∈X,∀m∈M,∀j∈J, ∀t. Therefore, the maximum absolute value of its partial derivative given in (8) is a Lipschitz constant. According to the Mean Value Theorem, there exists a constant c between x(m)1(t) and x(m)2(t) such that ∣∣f(m)j(x(m)1(t))−f(m)j(x(m)2(t))∣∣(x(m)1(t)−x(m)2(t))=dfj(c)dxv. Therefore, we can define the relation
∣∣f(m)j(x(m)1(t))−f(m)j(x(m)2(t))∣∣≤Γ∣∣x(m)1(t)−x(m)2(t)∣∣,∀(x(m)1,x(m)2)∈X,∀m∈M,∀t.
View Sourcewhere Γ=max{∣∣dfj(c)dxv∣∣}. Following the Lipschitz condition [35], this implies that the replicator dynamics, i.e., an initial value problem with x(m)(0)∈X, in (7) has a unique solution x(m)j⋆∈X.

Next, we prove the stability of the evolutionary equilibrium in the following theorem.

Theorem 2.
For any initial condition x(m)(0)∈X, the evolutionary equilibrium to the dynamics defined in (7) is stable.

Proof.
We define the Lyapunov function
G(x(m)(t))=(∑m=1M∑j=1Jx(m)j(t))2,(10)
View Sourcewhich is positive definite since:
G(x(m)(t)){=0>0 if x(t)=0 otherwise. 
View Source

Taking the first-order derivative with of G(x(m)(t)) with respect to t,
dG(x(m)(t))dt=2(∑m=1M∑j=1Jx(m)j(t))(∑m=1M∑j=1Jx˙(m)j(t)).(11)
View SourceNote that at any point of time ∑Mm=1∑Jj=1x(m)j(t)=M. Thus, the replicator dynamics have to equate to zero for this to hold, i.e., the net movements and strategy adaptations across clusters are zeroed out in order for the population to remain constant. Specifically,
∑m=1M∑j=1Jx˙(m)j(t)=0,∀t.(12)
View SourceTherefore, (12) ensures that dG(x(m)(t))dt=0, which satisfies the Lyapunov conditions required for stability, as defined in the Lyapunov's second method for stability [36].

As such, we have proven the uniqueness and stability of the evolutionary equilibrium.

Next, we discuss the procedures to derive the equilibrium cluster data coverage based on the replicator dynamics in (7). In contrast to the population evolution algorithm [32] which involves the intervention of a centralized controller, e.g., in disseminating information of potential payoffs that can be derived from joining a particular cluster, we consider the implementation of a decentralized cluster selection algorithm in Algorithm 1.

At the initialization phase, workers in each population m are randomly assigned to j clusters, where m∈M, j∈J. At each time period t, the workers compute their utilities and the average utility of workers in the population. Note that in practice, the workers may not have the complete information of all workers belonging to the same population in a large network. As such, its knowledge of the average utility in the population is based on the worker's “best guess”, i.e., the expected average utility. This procedure is simply a comparison between (i) the worker's own utility from joining a particular cluster j, i.e., u(m)j(t) and (ii) the expected average utilities of other workers from the same population which have chosen to join other clusters, i.e., E(u¯(m)(t)). Thereafter, the evolution of population state can be derived following the replicator dynamics.

The output of Algorithm 1 is the population state that is observed after tmax iterations. Then, we are eventually able to derive the data coverages of the cluster head, i.e., the proportion of data across the network that each cluster head can cover, as follows:
Dj=∑m=1Mx(m)j(tmax)Dm.(13)
View Source

Algorithm 1. Cluster Selection for HFL
Input: Worker and cluster characteristics

Output: Dj,∀j∈J

Initialization: Workers in population m each assigned to a random cluster

while t<tmax do

for m∈M do

Payoff Computation

for j∈J do

Derive u(m)j(t)=p(m)j(t)−c(m)j(t)

end for

Compute E(u¯(m)(t))=E(∑Jj=1x(m)ju(m)j(t))

Cluster Selection

for j∈J do

Derive x˙(m)j(t)=δx(m)j(t)(u(m)j(t)−u¯(m)(t)) and x(m)j(t)

end for

end for

end while

for j∈J do

Compute Dj=∑Mm=1x(m)j(tmax)Dm

end for

SECTION 5Deep Learning Based Auction for Valuation of Cluster Head
5.1 Auction Formulation
Based on the cluster formations from the evolutionary game, we are able to derive the data coverage Dj of the cluster head j∈J.

As each cluster head can only offer its services to a single model owner, i.e., the workers’ participation in the FL model training, the model owners need to compete for the services of the cluster heads. Each model owner l∈L has different preference for the accuracy of their models, e.g., applications for accident warning and prediction [37] require higher accuracy than the route planning and navigation systems. Following the work in [38], the FL model accuracy Al of model owner l, can be expressed as a power law function that is denoted as follows:
Al(μl)=σ−vμ−rl,(14)
View SourceRight-click on figure for MathML and additional features.where σ, v, and r are calibrable parameters depending on the model to be trained. μl is the data coverage required by model owner l to achieve its required model accuracy, σ is the upper bound of the accuracy that can be derived from historical data, whereas v and r are the fixed parameters of the function.

In general, when the requirement for data coverage of the model owner μl is larger, the model owner has more incentive to pay a higher price for the services of the cluster heads which has more data coverage. In contrast, a model owner that already has some pre-existing training data dl will have less incentive to bid for the services of a cluster head. Therefore, the valuation bl of model owner l for the services offered by the cluster heads can be expressed as bl=μl−dl.

In order to maximize the revenue of the cluster heads and to ensure that the services from the cluster heads are allocated to the model owners that value them most, we model the allocation problem as multiple rounds of single-item auctions. In this auction, the cluster heads are the sellers, i.e., auctioneers, while the model owners are the buyers, i.e., bidders. The cluster head with the highest amount of data coverage is the first to auction its services to the model owners. All model owners submit their bids to compete for the service. Then, the cluster head collects the bid profile (b1,…,bl,…,bL) to decide on the winning model owner l⋆ and the corresponding payment price θl⋆. After each round of auction, the winning model owner has higher data coverage, i.e., higher dl. Thus, its valuation in the next round of auction naturally has to be updated and decreases. The auction ends when all cluster heads have been allocated to the model owners. Note that the model owners may participate in more than one round of auction to fulfill their data coverage requirement, e.g., if the data coverage that single cluster has is insufficient to fulfill its needs.

Accordingly, the utility of the model owner in each round of the auction is as follows
ul={bl−θl⋆0if the model owner wins the bid,otherwise.(15)
View Source

An optimal auction [39] has two characteristics:

Individual Rationality (IR): By participating in the auction, the model owners receive non-negative payoff, i.e., ul≥0.

Incentive Compatibility (IC): There is no incentive for the model owners to submit bids other than their true valuations, i.e., the bidders always bid truthfully.

In each round of the auction, in order to determine the payment price θl⋆ of the winning model owner, traditional auction schemes such as the first-price auction and second-price auction (SPA) may be adopted. However, each of these traditional auction schemes has its own drawbacks.

The traditional first-price auction, in which the highest bidder pays the exact bid it submits, maximizes the revenue of the seller but does not ensure that the bidders submit their true valuations. On the other hand, the SPA, in which the highest bidder pays the price offered by the second highest bidder, ensures that the bidders submit their true valuations, i.e., ensures IC, but does not maximize the revenue of the seller. Therefore, in order to ensure that both conditions of truthfulness and revenue maximization of the seller are satisfied, we design an optimal auction using the Deep Learning approach with reference to the study in [20].

5.2 Deep Learning Based Auction for Valuation of Cluster Heads
In this section, we illustrate the neural network architecture for the design of an optimal auction. Following the procedure in [20], we describe the neural network architecture (Fig. 2) which renders the design of an optimal auction. Then, we elaborate on the proposed implementation of multiple round single-item auctions for the valuation of the services of cluster heads.


Fig. 2.
Neural network architecture for the optimal auction.

Show All

By adopting the SPA scheme to determine the payment price of the winning model owner, the revenue of the cluster head is not maximized, especially when the bid of the second highest bidder is low. Thus, in order to maximize the revenue of the cluster heads, the monotonically increasing functions are applied to the bids of the model owners to transform the bids into transformed bids, which are used to determine the allocation and the corresponding payment of the model owners in the network. The input bids and the transformed bids of model owner l are denoted as bl and b¯l respectively. The transform function for the bids submitted by model owner l is denoted as ϕl. In order to determine the allocation and conditional payment of the model owners, the SPA with zero reserve price (SPA-0) is applied to transform the bids. The reserve price is the minimum price that the cluster head requires to offer its service. The SPA-0 allocation rule and the SPA-0 payment rule of the model owner l are represented by g0l(b¯) and θ0l(b¯), respectively, where b¯ is the vector of the transformed bids. The SPA-0 allocation rule g0l(b¯) determines the winning model owner which has the highest bid if the bid is greater than zero. The SPA-0 payment rule θ0l(b¯) determines the conditional payment price θl of model owner l by applying the inverse transform function which is represented by ϕ−1l.

Theorem 3.
For any set of strictly monotonically increasing function {ϕ1,…,ϕl,…,ϕL}, an auction which is defined by the allocation rule gl=g0l∘ϕl and the payment rule θl=ϕ−1l∘θ0l∘ϕl satisfies the properties of IC and IR, where g0 and θ0 represent the SPA-0 allocation rule and the SPA-0 payment rule, respectively [20].

Based on the theorem, for any choices of strictly monotonically increasing transform functions, the proposed auction with the allocation rule gl and conditional payment rule θl satisfy the characteristics of the optimal auction, i.e., IR and IC. Therefore, the monotone transform functions are used in the neural network to ensure the IR and IC properties of the auction. In addition to this, the cluster heads want to maximize their revenues. Based on the allocation and the conditional payment rules, the revenue of the cluster head is determined. In particular, the revenue of the cluster head is equivalent to the payment price of the winning model owner. Thus, the objective of the cluster heads is to maximize their individual revenues while fulfilling the properties of IR and IC of the optimal auction. In order to do so, the neural network architecture learns the appropriate transform functions for the optimal auction to minimize the loss function which is defined as the expectation of the negated revenue of the cluster head. The minimization of the loss function is equivalent to the maximization of the revenue of the cluster head. With this, the optimal auction design based on the neural network architecture maximizes the revenues of the cluster heads while satisfying the necessary and sufficient conditions for IC and IR.

The algorithm for the implementation of the optimal auction based on the neural network architecture is illustrated in Algorithm 2.

In the following, we discuss the three important functions in the neural network architecture

the monotone transform function ϕl,

the allocation rule gl,

the conditional payment rule θl.

Algorithm 2. Algorithm for Deep-Learning Based Optimal Auction.
Input: Set of cluster heads J={1,…,j,…,J}, bids of model owners bi=(bi1,…,bil,…,biL)

Output: Revenue of the cluster heads

while J≠∅ do

Identify the cluster head the highest data coverage, Dj

Initialization: w=[wlqs]∈RI×QS+, β=[βlqs]∈RI×QS

Deep-Learning Based Optimal Auction:

while Loss function R^(w,β) is not minimized do

Compute the transformed bids b¯il=θl(bil)=minq∈Qmaxs∈S(wlqsbl+βlqs)

Compute the allocation probabilities gl(b¯)=softmaxl(b¯1,…,b¯l,…,b¯L+1;γ)

Compute the SPA-0 payments θ0l(b¯)=ReLU(maxs≠lb¯s)

Compute the conditional payments θl=ϕ−1l(θ0l(b¯))

Compute the loss function R^(w,β)

Update the network parameters w and β using the SGD solver

end while

Update the data coverage of the winning model owner dnewl=doldl+Dj

Update the valuation of the winning model owner bl=μl−dl

Remove the cluster head from set J

end while

return The revenue gain by the cluster heads

5.3 Monotone Transform Functions
In the auction, the valuation, i.e., bid bl of each model owner l is the input to the transform function ϕl. The transform function maps the input to its transformed bid, bl¯=ϕl(bl). Each transform function ϕl is modelled as a two-layer feed forward network which consists of the min and max operators over several linear functions, as shown in Fig. 3. There are Q groups of S linear functions hqs(bl)=wlqsbl+βlqs, Q={1,…,q,…,Q}, S={1,…,s,…,S}, wlqs∈R+ and βlqs∈R are the positive weight and bias respectively. With these linear functions, the transform function ϕl of each model owner l is defined as follows:
ϕl(bl)=minq∈Qmaxs∈S(wlqsbl+βlqs).(16)
View SourceBased on the parameters for the forward transform function ϕl, the inverse function ϕ−1l can be derived as follows:
ϕ−1l(y)=maxq∈Qmins∈S(wlqs)−1(y−βlqs).(17)
View Source


Fig. 3.
Monotone transform functions.

Show All

5.4 Allocation Rule
The allocation rule in the neural network architecture is based on the SPA-0 allocation rule. In particular, the data coverage Dj of the cluster head j is allocated to the model owner with the highest transformed bid if the transformed bid is more than zero. Otherwise, the cluster head does not sell its service to any model owner. In order to model the competition among the model owners, we use a softmax function on the vector of transformed bids b¯=(b¯1,…,b¯l,…,b¯L) and the additional dummy input b¯L+1=0 in the allocation network. The output of the softmax function is a vector of allocation probabilities, which is represented by g=(g1,…,gl,…,gL). The softmax function used in the neural network architecture is defined as follows:
gl(b¯)=softmaxl(b¯1,…,b¯l,…,b¯L+1;γ)=eγb¯l∑L+1l=1eγb¯l,γ>0,∀l∈L.(18)
View SourceThe parameter γ in the softmax function measures the quality of the approximation where the higher the γ, the more accurate the approximation of the function. However, a better quality of approximation results in a less smooth allocation function.

5.5 Conditional Payment Rule
The conditional payment rule determines the price θl that needs to be paid by the winning model owner l. The conditional payment rule is carried out in two steps. First, the SPA-0 payment θ0l for each model owner l is calculated. Specifically, the SPA-0 payment θ0l is the maximum of the transformed bids of other model owners and zero which is determined by using the ReLU activation unit function as follows:
θ0l(b¯)=ReLU(maxs≠lb¯s),∀l∈L.(19)
View SourceRight-click on figure for MathML and additional features.The ReLU(x)=max(x,0) activation function guarantees that the SPA-0 payment of each model owner is non-negative. Second, based on the SPA-0 payment θ0l, the conditional payment θl of model owner l is calculated as follows:
θl=ϕ−1l(θ0l(b¯)),(20)
View SourceRight-click on figure for MathML and additional features.where the inverse transform function from Equation (17) is applied to the SPA-0 payment of the model owner l.

5.6 Neural Network Training
The aim of the neural network is to optimize the weights and biases of the linear functions in the neural network such that the loss function is minimized. In the neural network, the loss function is defined as the expectation of the negated revenue of the cluster head. The loss function of the neural network is formulated based on the inputs, i.e., the training dataset and the outputs, i.e., the allocation probabilities and the conditional payments of the model owners. The training dataset of the neural network consists of the bidders’ valuation profiles of which the bidders’ valuation profile i is denoted as bi=(bi1,…,biL), I={1,…,i,…,I} where I is the size of the training dataset. bil is the valuation of model owner l for the data coverage of cluster head is drawn from a valuation distribution function fB(b). Since the valuation bil of the model owner l depends on the data coverage requirement μl and the current amount of data coverage dl of the model owner, i.e., bil=μil−dil, the distribution function fB(b) can be determined based on the distribution of the data coverage requirement, which is represented by fμ(μ). In our work, we assume that the data coverage requirement of the model owners follows a uniform distribution, i.e., μ∼U[μmin,μmax].

The parameters of the monotone transform functions, i.e., weights wlqs and biases βlqs are the entries of matrices w and β. The matrices are needed to determine the allocation probability and conditional payment of model owner l, which are represented by g(w,β)l and θ(w,β)l respectively.

The objective of the training is to find the optimal weight w∗ and bias β∗ matrices that minimize the loss function of the neural network, i.e., the expectation of the negated revenue of the cluster head j. Specifically, the approximation of the loss function, R^ is defined as follows:
R^(w,β)=−1I∑l=1Ig(w,β)l(bi)θ(w,β)l(bi).(21)
View Source

For the optimization of the loss function R^(w,β) over the parameters (w,β), a stochastic gradient descent (SGD) solver is used.

SECTION 6Performance Evaluation
In this section, we present the performance evaluation of the evolutionary game based cluster formation and deep learning based auction for the valuation of cluster data coverage. Unless otherwise stated, the simulation parameters are as shown in Table 1. Note that we use the terms “cluster” and “cluster heads” interchangeably.

TABLE 1 Simulation Parameters [38], [40].
Table 1- 
Simulation Parameters [38], [40].
6.1 Lower-level Evolutionary Game
For the first part of our simulations, we analyze the lower-level evolutionary game. We consider a network which consists of 90 workers. The workers have different data quantities that follow a uniform distribution. Using the k-means algorithm, we derive 3 populations4 of 30 workers each based on the data quantities that they possess. In the first population m=1, each worker has 80 data samples. In the second population m=2, each worker has 100 data samples. In the third population m=3, each worker has 160 samples. Without loss of generality, the data samples that each worker owns are assumed to be characterized by the populations that they belong to. Thus, the populations are arranged in the ascending order based on the data samples each worker has.

Besides, there exist 3 cluster heads in the network with each offering different reward pools αj, as well as congestion coefficients ζj. Recall from Section 4.2 that a higher value of αj indicates that a cluster offers a larger reward pool for the workers to share, whereas a higher value of ζj represents that a cluster head has more limited communication resources. The clusters are arranged in the ascending order based on the reward pool they offer, i.e., cluster 3 offers the highest reward pool to its workers.

Accordingly, in each time period, workers in the populations choose one of the clusters to join. Then, following Algorithm 1, the strategy adaptation is performed and evolved such that the workers evaluate their payoffs and churn to another cluster with higher payoffs with some probabilities. Eventually, the evolutionary equilibrium is achieved.

6.1.1 Stability and Uniqueness of the Evolutionary Equilibrium
To demonstrate the uniqueness of the evolutionary equilibrium, i.e., the solution to the replicator dynamics defined in (7), we first derive the phase plane of the replicator dynamics in Fig. 4. For ease of exposition, population 3 is excluded initially. As such, only the first and second populations of workers are considered to choose among the three clusters to join. Fig. 4 shows the population states of populations 1 and 2, i.e., the proportion of workers in each population that join cluster 1. We consider varying initial conditions in Fig. 4 and plot the corresponding dynamics. For example, for the first condition, we have 10 percent of the workers from both populations choose cluster 1. Clearly, despite varying initial conditions, the evolutionary equilibrium always converges to a unique solution.


Fig. 4.
Phase plane of the replicator dynamics.

Show All

To evaluate the stability of this evolutionary equilibrium, we consider that 30 percent and 70 percent of workers in populations 1 and 2 respectively are initially allocated to cluster 1. Then, we plot the evolution of population states in Fig. 5. We observe that the proportion of workers from population 2 joining cluster 1 declines as more workers from population 1 joins the cluster. This is due to the division of rewards and congestion effects as more workers join the cluster. Eventually, the evolutionary equilibrium is reached and the population states no longer change.


Fig. 5.
Evolutionary equilibrium of population states for cluster 1.

Show All

Next, we consider the situation in which there are three populations and three clusters. We set the initial conditions such that a third of the workers from each population are assigned to each cluster initially. Then, we plot the evolution of utilities in Fig. 6. Specifically, within each population, we plot the utilities derived by workers which have chosen each of the three clusters. We observe that for each population, the utilities derived from choosing the varying clusters eventually converges with time. This implies that an evolutionary equilibrium is reached whereby at the equilibrium, the workers no longer have the incentive to adapt their cluster selection strategies. Moreover, at the equilibrium, workers which belong to population 3 derive the greatest utilities, given that they are compensated for having larger data shares in the clusters.


Fig. 6.
Evolution of population utilities.

Show All

In Figs. 7, 8, and 9, we plot the evolution of population states of each population respectively. We observe that cluster 3, which offers the highest reward pool for distribution across workers, has the largest proportion of workers from population 3. The reason is that the workers from population 3 have the largest number of data samples. Hence, they can have the largest shares of the large reward pool if they join cluster 3. In contrast, the lowest proportion of workers from population 1 joins cluster 3 because they have the lowest reward shares. However, there is an upper limit to how many workers can join cluster 3. Even though cluster 3 offers the highest reward pool, the distribution of rewards and congestion effects set in eventually when more workers have joined the cluster. Thus, workers of population 3 may also join clusters 1 and 2 as well when this occurs.


Fig. 7.
Evolution of population states for population 1.

Show All


Fig. 8.
Evolution of population states for population 2.

Show All


Fig. 9.
Evolution of population states for population 3.

Show All

6.1.2 Evolutionary Equilibrium Under Varying Parameters and Conditions
In this section, we vary the simulation parameters to study the evolutionary equilibrium under varying conditions. In Fig. 10, we vary the learning rate of the population, which controls the speed of strategy adaptation. Naturally, when the learning rate is low, the evolutionary equilibrium can only be reached after a longer time period. However, we can observe that the stability of the evolutionary equilibrium is not compromised. The speed of convergence depends how fast the workers can observe and adapt their strategies, e.g., they have more accurate information about the system.


Fig. 10.
Evolutionary dynamics under different learning rates.

Show All

In Fig. 11, we vary the reward pool offered by cluster 1 between [50,350] while keeping the reward pool for clusters 2 and 3 constant at 200 and 300 respectively. Then, we plot the changes in data coverage of each cluster, i.e., how much data coverage a cluster has as a result of worker contribution. Naturally, the data coverage of cluster 1 increases with an increment of the reward pool. The reason is that the cluster is more attractive to workers as shareable rewards increase. We further note that at the points α1=200 and α1=300, the data coverage for cluster 1 is identical to those of clusters 2 and 3 respectively. The reason is that at these points, the cluster 1 is identical to the corresponding clusters and thus, workers are indifferent between choosing the cluster to join.


Fig. 11.
Data coverage versus varying fixed rewards in cluster 1.

Show All

In Fig. 12, we vary the congestion coefficient of cluster 1 between [2,18] while keeping the other clusters’ constant. Then, we plot the changes in data coverage of each cluster. We observe that as the congestion coefficient increases, the cluster has lower data coverage. Instead, the workers that used to join the cluster 1 adapt their strategies and churn to clusters 2 and 3. This is given that with a large congestion coefficient, the cluster head has more limited communication resources and can no longer support as many workers without them having to incur larger communication costs, e.g., due to device interference.

Fig. 12. - 
Data coverage versus varying congestion coefficient in cluster 1.
Fig. 12.
Data coverage versus varying congestion coefficient in cluster 1.

Show All

In Fig. 13, we vary the data quantities of workers in population 1 while keeping the other populations constant. Then, we plot the population states of all populations with respect to participation in cluster 3. Specifically, the figure shows the proportion of workers from each population which have joined cluster 3 as the data quantities of population 1 vary. Clearly, as the data owned by workers in population 1 increases, more workers from population 1 are able to join cluster 3, which is the cluster with the largest reward pool as mentioned in Section 6.1.1. The reason is that with more data, the workers in the population can gain a larger proportion of the pooled rewards, relative to that of workers from other populations.


Fig. 13.
Population states in cluster 3 versus varying population data for population 1.

Show All

6.2 Upper-Level Deep Learning Based Auction
In this section, we perform simulations to evaluate the performance of the Deep-Learning based auction. For comparison, the classic SPA is chosen as the baseline scheme. The TensorFlow Deep Learning library is used to implement the optimal auction design. We consider a network with the formations of 3 clusters and 10 model owners. We first evaluate the performance of the Deep-Learning based auction against the traditional SPA. Then, we proceed to study the impacts of (i) data coverage of the cluster heads, (ii) model owners with varying distribution for data coverage requirement, and (iii) different quality of approximation, on the revenues of the cluster heads.

6.2.1 Evaluation of the Deep Learning Based Auction
From Figs. 14, 15, and 16, we observe that the revenue of the cluster heads determined by the deep-learning based auction is consistently higher than that of the conventional SPA scheme. The reason is that the SPA scheme guarantees IC, but does not guarantee that the revenue of the seller is maximized. While preserving the properties of IC and IR of the traditional auction, the deep-learning based auction maximizes the revenue earned by the cluster heads by providing their services to the model owners that value their services the most.


Fig. 14.
Revenue of cluster head 1 under different distribution of model owners.

Show All


Fig. 15.
Revenue of cluster head 2 under different distribution of model owners.

Show All


Fig. 16.
Revenue of cluster head 3 under different distribution of model owners.

Show All

The revenues of the cluster heads are affected by the amount of data coverage. In Fig. 17, we observe that when the data coverage of the cluster head is higher, the revenue earned is also higher. In particular, when the model owners have high requirement for data coverage, i.e., μl∼U[0.5,0.9], cluster head 3 with the highest data coverage of 0.4 earns a revenue of 8944 whereas cluster head 1 with the lowest data coverage of 0.26 earns a revenue of 7182. This is due to the fact that the cluster head with the highest data coverage is allowed to conduct auction its services first. Hence, it will be able to offer its service to the model owner with the highest data coverage requirement in which the model owner is willing to pay the highest price as compared to other model owners in the network. Intuitively, this serves to compensate the cluster head for the higher rewards expense it incurs.


Fig. 17.
Revenue vs data coverage of cluster heads.

Show All

We examine the impacts on the cluster heads’ revenues when they are presented with model owners with data coverage requirement of different distribution ranges. Specifically, model owners can take on two distribution ranges, i.e., μl∼U[0,0.4] and μl∼U[0.5,0.9]. In Fig. 14, cluster head 1 which has a total data coverage of 0.26 earns a revenue of 2724 when model owners have data coverage requirements that range between 0 and 0.4. On the other hand, when model owners have higher range of data coverage requirements, i.e., between 0.5 and 0.9, cluster head 1 earns a higher revenue of 7182. Since the model owners with higher data coverage requirement value the cluster head more, they have more incentive to pay a higher price which results in the higher revenue earned by the cluster head. Similar trends are observed for cluster head 2 and cluster head 3 in Figs. 15 and 16 respectively.

To further evaluate the performance of the deep-learning based auction, we consider the cluster heads’ revenues under different quality of approximation, γ. The quality of approximation is used in the softmax function in the determination of the winning model owner. We consider two values for the quality of approximation, i.e., 1000 and 2000. We observe in Figs. 18, 19, and 20, the revenue of the cluster head increases slightly when the quality of approximation is higher. Specifically, given the model owners with a high requirement for data coverage, i.e., μl∼U[0.5,0.9], the revenues of cluster head 3 are 8944 and 8969 when the values of γ are 1000 and 2000, respectively. This follows that with a greater value of approximation quality, the neural network is able to solve the optimization problem which maximizes the revenue of the cluster heads.


Fig. 18.
Revenue of cluster head 1 under different approximation qualities.

Show All


Fig. 19.
Revenue of cluster head 2 under different approximation qualities.

Show All

Fig. 20. - 
Revenue of cluster head 3 under different approximation qualities.
Fig. 20.
Revenue of cluster head 3 under different approximation qualities.

Show All

SECTION 7Conclusion
In this paper, we proposed a resource allocation and incentive mechanism design framework for HFL. We considered a two-level problem and leveraged on the evolutionary game theory to derive the equilibrium solution for the cluster selection phase. Then, we introduced a deep learning based auction mechanism to value the cluster head's services. The performance evaluation shows the uniqueness and stability of the evolutionary equilibrium, as well as the revenue maximizing property of the auction mechanism. In the future work, we will consider social network effects and their impact on the cluster selection decisions of the workers, as in [26]. Moreover, we may also account for the existence of malicious workers.