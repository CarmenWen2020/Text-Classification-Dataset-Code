The Deep Neural Network (DNN), Recurrent Neural Network (RNN) applications, rapidly becoming attractive to the market, process a large amount of low-locality data; thus, the memory bandwidth limits their peak performance. Therefore, many data centers actively adapt high-bandwidth memory like HBM2/HBM2E to resolve the problem. However, this approach would not provide a complete solution since it still transfers the data from the memory to the computing unit. Thus, processing-in-memory (PIM), which performs the computation inside memory, has attracted attention. However, most previous methods require the modification or the extension of core pipelines and memory system components like memory controllers, making the practical implementation of PIM very challenging and expensive in development. In this article, we propose a Silent-PIM that performs the PIM computation with standard DRAM memory requests; thus, requiring no hardware modifications and allowing the PIM memory device to perform the computation while servicing non-PIM applications' memory requests. We can achieve our design goal by preserving the standard memory request behaviors and satisfying the DRAM standard timing requirements. In addition, using standard memory requests makes it possible to use DMA as a PIM's offloading engine, resulting in processing the PIM memory requests fast and making a core perform other tasks. We compared the performance of three Long Short-Term Memory models (LSTM) kernels on real platforms, such as the Silent-PIM modeled on the FPGA, GPU, and CPU. For (p ×512) ×(512 ×2048) matrix multiplication with a batch size p varying from 1 to 128, the Silent-PIM performed up to 16.9x and 24.6x faster than GPU and CPU, respectively, p=1, which was the case without having any data reuse. At p=128, the highest data reuse case, the GPU performance was the highest, but the PIM performance was still higher than the CPU execution. Similarly, at (p ×2048) element-wise multiplication and addition, where there was no data reuse, the Silent-PIM always achieved higher than both CPU and GPU. It also showed that when the PIM's EDP performance was superior to the others in all the cases having no data reuse.
SECTION 1Introduction
In the von Neumann architecture, CPU reads code and data stored in memory, executes the program, and stores the result back in memory. This traditional execution method incurs the amount of unnecessary data movement, especially in recently emerged data-intensive applications that process large amounts of low locality data, such as DNNs/RNNs [1], in-memory databases [2], [3], and graph algorithms [4], [5], and so on. Eventually, even if the CPU performance increases, the memory bandwidth limits the overall system peak performance. Thus, data centers actively apply the high bandwidth memory devices like HBM2/HBM2E to their computing platforms. However, this approach would not provide the performance-optimal solution when considering the low data locality applications, especially recently used. Therefore, the tight integration of CPU with memory has attracted attention. One of the promising techniques is to place CPUs or accelerators close to memory (called near-data processing, NDP) [6], [7], [8], [9], [10] or inside memory (called processing-in-memory, PIM) [11], [12], [13], [14], [15], [16].

Most of the previous studies require to redesign and change the computing system that we currently use. Currently, JEDEC [17] does not define the PIM protocol and highly likely to define only the basic PIM protocols in the future; thus, limiting to developing and providing new PIM functions and making it difficult to use PIM in real systems. In order to solve this problem, all PIM operations must comply with the standard DRAM interface, and therefore we should express them as the standard memory requests.

To the best of our knowledge, our study in this paper is first to propose a PIM architecture that does not require any modification of hardware components except for the PIM DRAM device itself. It implies that we can control the PIM device and perform the PIM computation only with standard memory requests by complying with the standard DRAM interface [17], i.e., preserving the DRAM standard operation semantics and their timing constraints.

Using the standard memory requests for the PIM execution makes it impossible for the memory controller to distinguish the memory requests from PIM and non-PIM applications; thus, the PIM device should differentiate them. The PIM device driver stores the PIM source and destination operand addresses and their read/write attributes in the PIM memory device’s configuration area before starting the PIM execution. The PIM device compares all incoming memory requests with the stored addresses and identifies the PIM requests. The operations for the PIM and non-PIM memory requests are the same except for handling the datapath. Also, all the PIM operations should satisfy the timing constraint for using the standard memory requests. A burst operation accompanies the DRAM RD/WR command operation, and there is timing constraint, called tCCD, between commands as many cycles as necessary for the burst operation. We developed a 2-stage pipelined 8-way vector datapath per bank to support the bfloat16 data type for DNN/RNN applications [18], supporting the burst data and not violating the timing constraint.

By preserving both the DRAM standard operation semantics and their timing constraints, we could execute both PIM and non-PIM applications simultaneously. It implies that the PIM process does not alter or interfere with the execution state of other non-PIM processes, and consequently does not affect any behavior of the system. We represent this behavior as “silent” execution. To perform operations on a von Neumann-based computing platform such as a CPU or GPU, we need to read source operands and write the destination operand at least once. Therefore, our approach can provide better performance when executing applications that exploit poor data locality than any computing platform, since our execution needs only to read source operands from memory and write destination into memory by using the standard memory requests.

To achieve the design goal and deliver the best performance, various levels of architectural components, such as applications, libraries, operating systems, and PIM device engines, were carefully designed with minimal modification and extension. For example, we configured the kernel-specific PIM datapath only once before starting the kernel, like a configurable processor [19]. The PIM applications repeatedly perform the same operation only with different data inputs and outputs. Therefore, sending the command for every computation is very inefficient [20]. In addition, the memory model was also carefully studied for efficiently developing PIM programming and execution models.

We measured and compared the performance of the three kernels of LSTM [21] such as matrix multiplication and an element-wise multiplication/addition on real machines, such as the PIM modeled FPGA platform, Nvidia Titan Xp GPU/cuBLAS, and AMD Ryzen 7 CPU/OpenBLAS.

The major contributions of this paper are the followings:

We propose a Silent-PIM that performs the PIM computation only with standard DRAM memory requests; thus, requiring no change of any hardware components.

We describe in detail how to develop various levels of architectural components, from applications to the PIM device, to achieve our goal.

The Silent-PIM achieved the speedup of 24.6x and 16.9x at the vector-matrix multiplication and up to 1.4x and 23.2x at the element-wise operations over CPU and GPU execution, respectively, while PIM’s EDP performance was superior to the other platforms.

The remainder of this paper consists of the following: In Section 2, we describe the overall Silent-PIM architecture and its experimental platform. In Sections 3 and 4, we describe the PIM device interface with a host and the device design. In Section 5, we describe the PIM software stack with an example, and in Section 6, we evaluate our PIM design’s performance. Section 7 introduces the previous works of PIMs, and finally, Section 8 concludes the paper.

SECTION 2Overall Architecture and Experimental Platform
We designed the Silent-PIM device and emulated its execution on the FPGA board. We also developed a full software stack on a host platform connected with the board via PCI-e and verified all the operational concepts at a system level.

Fig. 1 shows the overall architecture. We configured the DDR4 DRAM on the FPGA board as uncacheable system memory; thus, we regarded the memory controller on the FPGA as the memory controller on the processor side. We used the Xilinx DDR4 memory controller IP without any modification. Since our Silent-PIM complied with the standard memory interface, we did not need to add any DRAM state. However, we extended the functionality of the existing states, as shown in Fig. 2, for providing the read data to the PIM datapath and the computation result to the bank. We placed the PIM device between the memory controller and the DRAM, which emulated the DRAM and performed the PIM execution. The data granularity fetched from each DRAM bank was 128-bit, and the burst length was 4. Our memory can be thought of as one die of 3D-stacked memory. Since we did not modify the FPGA memory controller, the PIM device was designed to comply with the DDR4 specification mounted on the board.


Fig. 1.
Overall architecture and its experimental platform. The gray-colored boxes were added or modified.

Show All


Fig. 2.
The Silent-PIM state diagram identical to the JEDEC standard [17] except for adding the PIM functions to the reading and writing states.

Show All

For emulating the DRAM-die PIM, our FPGA-based PIM device performs the same as DRAM while supporting the computation. For this purpose, the emulated device should 1) match the DRAM states in Fig. 2 with the memory controller’s states by recognizing the commands issued by the memory controller to DRAM and 2) satisfy the state transitions’ timing constraints. We made the PIM device correctly preserve the DRAM states by recognizing all the DRAM command. We implemented the 64-byte vector register (a burst size) per bank. In the case of a PIM read command (the same as the standard read command), the PIM device intercepts data transferred from DRAM to the memory controller and stores them in the vector register. In the case of a PIM write command (the same as the standard write command), the PIM device transfers the vector register data to DRAM instead of the data from the memory controller. By these methods, our PIM device can satisfy all the read/write command related timings, like tRCD, tCCD, tRAS, etc. In addition to the data management, the PIM engine also should satisfy the DRAM timings while performing the computation. Our PIM engine supports our argument since the DRAM column command can be issued sequentially at 4-cycle (tCCD) intervals. To validate the proposed design to be implemented in DRAM die, we used the 65 nm logic process, similar to the DRAM fabrication characteristics [20]. We verified that our design met the DRAM internal operating frequency of about 800 MHz, and the available space near each bank of about 40,000μm2.

We used a memory-mapped I/O for the PIM interface, i.e., accessing the control registers inside the device. We implemented the PIM interface unit to identify the PIM memory requests from standard memory requests (Section 3.1) and configure the PIM engine (Sections 3.2 and 4.3). We designed a 2 stage pipelined 8-way vector ALU to support the bfloat16 data type for our target applications like DNN [18] and attached it to each bank (Section 4.1).

The slow PCI-e made it difficult to study how the developed PIM device affected the overall system performance. Additionally, an application that used the FPGA memory was hundreds of times, or even more seriously, thousands of times slower than those using the system memory. Also, if the CPU issues the PIM memory requests, it becomes very busy. Therefore, we applied Direct Memory Access (DMA) to the PIM execution as an offloading engine to solve this problem (Section 3.4), and this was possible because our method uses only the standard memory requests for the PIM execution. Our experiment showed that the CPU took only about 2.68 percent of computing resources by using the system DMA in our host platform. Therefore, the system DMA usage for the DRAM-die PIM as the offloading engine will allow the CPU to perform other tasks while executing the PIM kernel.

The PIM software consists of several components: 1) an application that is the highest level of PIM architecture layers (Section 5.1), 2) a PIM wrapper that provides the PIM execution interface to the application (Section 5.2), 3) a math library that supports various PIM kernel operations (Section 5.3.2), and 4) a device driver to manage the PIM device (Section 3.3).

SECTION 3Interfacing the Silent-PIM Device
We made OS reserve one memory-mapped page at boot time for the PIM management, as shown in Fig. 3. The PIM application stores its operand and kernel-specific engine configuration information on the reserved page using the PIM driver before starting the PIM execution. The PIM device compares the address of the incoming memory request with the stored operand information for identifying it as the PIM request. The PIM engine computes using the configuration information at every incoming PIM request.


Fig. 3.
Silent-PIM device interface for identifying the PIM requests and configuring the PIM device.

Show All

3.1 Identifying the PIM Requests
To realize the Silent-PIM, a programmer should provide the information about the PIM operands to hardware, i.e., the PIM device. Then, the PIM device distinguishes the PIM requests from the standard memory requests to perform the PIM operation.

For the PIM programming, we aligned all PIM operands to physical pages in order to simplify the request identification at the device interface and recognize their bank placement easily at the PIM programming. Also, we modified the OS to contiguously allocate the physical pages for reducing the configuration overhead and row buffer misses. Whenever we start to use a page not physically contiguous, we should guarantee that all previously issued memory PIM requests finish being serviced. After that, we need to configure the operand control registers for identifying the new contiguous pages as the PIM operands. It would incur significant overhead.

Before starting the PIM execution, the programmer stores the start physical page address of each operand (20 bits) and its size (20 bits) in its corresponding 64-bit control register using the PIM device driver (Fig. 4b). The PIM device includes two source registers (REG A and B) and one destination register (REG C). We did not define the operands’ valid bits in operand control registers, but in LSB 3 bits of the configuration register (REG D). The reason was that we could not store all the operand and configuration information using one memory request. We could make the PIM engine ready to execute after configuring the PIM datapath. The memory request should not be identified as the PIM command using the stored operand information before configuring the datapath. The Request Identification Unit (RIU) identifies the PIM RD command (a bank to PIM datapath) and the PIM WR command (PIM datapath to a bank) using the information stored in the operand control registers. RIU is located in a preprocessing unit for the incoming request to the PIM device so that all banks share the results from RIU.

Fig. 4. - 
Identifying the Silent-PIM commands in RIU. (a) Physical to DRAM address mapping. (b) Structure of operand registers. (c) Address matching logic.
Fig. 4.
Identifying the Silent-PIM commands in RIU. (a) Physical to DRAM address mapping. (b) Structure of operand registers. (c) Address matching logic.

Show All

We cannot compare the incoming request’s address with the address stored in the control register straightforwardly due to the following characteristics of DRAM operations: 1) Row and column addresses are transmitted to memory devices by separate DRAM commands. 2) ACT and RD/WR commands issued to one bank can be mixed with those to another bank. Also, 3) OS pages and DRAM rows are misaligned, as shown in Fig. 4a. Therefore, the address match process consists of two steps, one for row match and the other for column match. For the row match, we compare the ACT command’s row address with the stored PIM operand address and store the match result at a 1-bit space per bank (called a row match table). With the row match information, we can decide whether the incoming RD/WR request entered later to the device is the PIM command by comparing their column addresses. Fig. 4b shows the logic for the address match, and there is one logic for each operand. It performs as follows: Initially, clear all entries of the row match table.

Row address match (ACT and PRE commands): If the operand’s bit is valid (❶), compare the address latch’s row address [A31:A14] with the operand address range calculated from the stored operand’s base address and size (❷). If the incoming row address is in the range, set the row match table[bank id] at the ACT command or unset the row match table[bank id] at the PRE command. The bank id is defined as [A9:A6].

Column address match (RD/WR commands): If the row match table[bank id] is 1 (❸), compare the incoming memory request’s column address [A13:A12] in the address latch with the operand’s page number (❹). If they are matched, the incoming memory request is decided as the PIM command.

We provided 3-bit global output lines (two for sources and one for destination) from RIU to express the match result for providing a flexible and simple design to a control unit. This will is explained in detail in Section 4.3.

3.2 Configuring the PIM Engine
In the case of data-intensive applications targeting by PIM, the same sequence of operations is repeatedly performed on a large amount of data. Therefore, repetitively sending the PIM operation commands from the host to the PIM device incurs unnecessary significant overhead [20].

Consequently, we solved this problem by configuring the PIM engine only once before starting the kernel execution, i.e., before sending its related memory requests for source and destination operands. We store the configuration information in the configuration control register (REG D) of the PIM device. For the experiment, we prepared for the configuration information for several PIM kernels, such as matrix-matrix multiplication, vector-matrix multiplication, element-wise addition/multiplication, and so on. We will explain the process in detail in Section 4.3.

3.3 PIM Device Driver
The PIM device driver reserves one unswapped and uncached page in the FPGA board’s memory space at boot time, receives the operand and configuration information from PIM applications, and stores them in the control registers memory-mapped to the page. The driver also manages the PIM device ownership from multiple processes using semaphores for PIM devices. The spill from the PIM process scheduling complicates implementation because it requires modification of the process scheduling-related part of the OS with storing/restoring the PIM device states. Eventually, we used only one semaphore for each PIM device and did not rely on the OS scheduling for controlling the PIM device.

The device driver supports the following two functions.

sys_pim_acquire (operand’s base address & size, function configuration): 1) Convert the operand’s virtual base address to a physical address and calculate its page sizes. 2) Acquire a semaphore of the PIM device before staring the PIM operation. If it fails, the driver waits until the semaphore is released. 3) When the semaphore is acquired, the PIM device driver stores the operands’ base address and size in the corresponding operand control registers and kernel configuration information.

sys_pim_release (): 1) Invalidate all the operand’s information (LSB 3-bits in REG D) for disabling all the PIM operations. 2) Release the semaphore.

3.4 Using Direct Memory Access
Our FPGA-based experimental platform has the following two performance issues. First, the host should continuously send memory requests to the PIM device for operand accesses, which can incur the overhead at the entire memory system and make the host processor busy. Second, due to the overhead of the PCI-e interface, as a result of executing various PIM kernels and analyzing the performance, we found that about 94 percent of the FPGA memory controller’s total execution time was measured as idle.

To solve this problem, we used Direct Memory Access as the PIM device’s offloading engine. Our Silent-PIM can use DMA without any modification because it performs with standard memory requests. Also, in general, the array-typed source and destination operands are used in the PIM kernels, which is very suitable for the DMA operation because it requests continuous burst data to the slave device.

SECTION 4Silent-PIM Engine Design
In this section, we describe how to design the datapath in a limited space and the control unit that can support various PIM kernel execution. We also explain how to design an ALU that supports the bfloat16 data type attracting attention in DNN, while satisfying the DRAM timing constraints.

4.1 Datapath
Fig. 5 shows our PIM datapath by assuming that the DRAM bank data bit-width is 128-bit, and the burst length is 4 for the standard memory request. We could not afford abundant computing resources due to the space limitation in DRAM. Nevertheless, we needed to support the burst read/write operations. The datapath per bank includes 128-bit × 4 general vector registers (vecA[3:0]), 128-bit general vector registers (vecB), 176-bit × 4 accumulator registers (vACC[3:0]), and bfloat16 8-way vector ALUs for DNN computation. The datapath consists of 4 stages: FE stage for fetching operands from a bank, EX0/EX1 stages for their computation, and WB stage that writes vACC to the bank. The control unit manages the datapath by decoding the information in the configuration register (REG D).


Fig. 5.
Silent-PIM datapath.

Show All

Table 1 shows the Silent-PIM ISAs, which are divided into five categories, as follows:

TABLE 1 Silent-PIM ISAs

Read and write data from a bank (RD, WR)

Clear registers (CLR)

Register data transfer (MOV)

Scalar broadcasting (CSTB)

Arithmetic operations (MAC, ADD, SUB, MUL).

The PIM RD/WR command performs the standard memory request service also with the PIM execution. If the PIM RIU recognizes the PIM RD command, it turns on the bus switch connected to the PIM datapath. For example, the RD command matched to src0 operand (REG A) or src1 operand (REG B) stores the data read from the bank in vecA and vecB, respectively. Also, in PIM WR (matching the address of the destination operand in REG C), the PIM RIU turns on the bus switch and stores the accumulator registers to the bank instead of the data expressed in the write memory request. MOV ISA defines data movement from the general registers into vACC with mantissa expansion. CSTB ISA broadcasts one scalar from a vector register to the source of the entire MACs for vector-matrix multiplication.

4.2 Bfloat16 MAC Unit
We designed a 2-stage (multiplication and addition stages) pipelined bfloat16 MAC unit capable of satisfying the DRAM timing constraint. We also supported ADD, SUB, and MUL operations based on MAC. We developed the following optimization techniques.

First, to reduce complexity from mantissa alignment due to the exponent difference between two operands, we extended 7-bit to 16-bit mantissa using the lower 3-bit exponent [22]. The mantissa extension allowed us to move the logic of calculating the exponent difference to the multiplication stage for being hidden by the multiplier delay. Second, we assumed that the exponent calculated in the multiplication stage does not change in the addition stage. The assumption made it possible to move the circuit to handle the exponent difference to the multiplication stage. If an overflow occurs due to adding mantissa, an alignment process that adds 1 to the exponent is required, so the assumption is wrong. Therefore, the addition stage’s overflow signal is forwarded to the multiplication stage to correct the number of cases for the pre-calculated exponent difference. Finally, the result stored in vACC did not meet the format of bfloat16 since we resized the exponent and the mantissa bit widths. It was not necessary to perform the normalization whenever a new result was stored in vACC. Therefore, we normalized vACC only when written to the bank, thereby reducing the critical path delay by removing the normalization unit from the accumulation path.

4.3 Control Unit
Since most PIM applications like DNN performs the same computation repeatedly to process a lot of data, the datapath could be configured only once before starting the PIM execution for reducing the command transmission overhead. It also reduces power consumption to decode the command. Fig. 7 shows the specification of the configuration register (REG D). Each configuration bit field corresponds to one control signal for the data path, which allows us to reduce the decoding hardware overhead and add PIM ISAs easily.

Fig. 6. - 
Vector addition instruction sequence timing diagram.
Fig. 6.
Vector addition instruction sequence timing diagram.

Show All


Fig. 7.
Format of the configuration register (REG D) and its corresponding control signals to the Silent-PIM datapath.

Show All

We designed a hardwired control unit and decoupled the data movement control from the pipelined ALU control in order to efficiently support the burst operation. We took the following design and operation methods for the design simplification:

We used the operand match information from RIU to determine which source or destination is related to the incoming PIM memory request. For the design simplification, we used 3 global bit-lines for two sources and one destination.

The standard memory request involves 4 burst operations (i.e., 128-bit × 4). Since only vecA register can store all of them, the data from the first identified PIM read memory request is stored in vecA. One burst (128-bit) by the second identified PIM read memory request is stored in vecB, and the operation using two source operands starts whenever the new data is stored in vecB.

When the PIM write memory request is identified, the data in vACC is written to its attached bank, and the control states, including burst counters, a duplication counter, etc. are cleared.

In order to support burst operation in the pipelined MAC unit, we designed a burst counter (i=0,1,2,3) corresponding to the burst length for each stage. We execute the datapath defined in the configuration register as many as the burst length at each pipeline stage. CSTB is an ISA that represents which scalar in vecA, containing 32 scalars, is broadcast to all MAC units. We used a 6-bit duplication counter to indicate the scalar position. When vecA is newly updated, the counter value is initialized.

Fig. 6 shows a timing diagram that represents how to perform the vector addition at the burst mode in our datapath. As shown in Fig. 5, the PIM engine consists of 4 pipeline stages: FE (fetching data from a bank into registers), EX0/EX1 (MAC execution), and WB (normalizing the result in vACC and storing it to a bank). The vector addition can be performed by the following five ISAs: RDvecA, RDvecB, MOV, ADD, and WR. We can configure the datapath for MOV and ADD in advance by setting REG D8 and REG D11. The control unit reads and stores the first operand into vecA burst-by-burst (FE stage), and moves it to vACC using the datapath pre-configured by MOV ISA (EX0/EX1 stages) in a pipelined manner. The control unit reads one burst data of the second operand and stores it in vecB (FE stage). Then, the unit performs the addition at the MAC unit pre-configured by ADD ISA and stores its result in vACC (EX0/EX1 stages). The unit performs all the operations in a pipelined manner. In the WB stage, the results stored in vACC are normalized and stored in the DRAM bank.

SECTION 5Silent-PIM Software Stack
The PIM software consists of the followings: 1) an application that is the highest level of PIM architecture layers, 2) a PIM wrapper that provides the PIM execution interface to the application, 3) a math library that supports various PIM kernel operations, and 4) a device driver that plays the role of PIM device management and configuration (Section 3.3).

In the following subsections, the software implementation is described in detail with vector-matrix multiplication as an example.

5.1 Application
The PIM application allocates the PIM operands onto FPGA DRAM. We modified the OS for managing the FPGA DRAM as a part of the system memory and extending the mmap() function to make the PIM operand allocated as contiguous, unswapped, and uncached physical pages. The swapping and migrating the PIM operand pages would incur the inconsistency with the operand information stored in the operand control registers. We replace an application code section suitable for the PIM execution with a PIM wrapper whose arguments are the PIM kernel identifier and its operands. After allocating and initializing the PIM operands, the application calls the PIM wrapper.

5.2 PIM Wrapper
The PIM wrapper provides the same interface for executing all PIM kernels to applications. The wrapper function performs:

Call __pim_acquire of the PIM device driver to acquire the PIM device ownership and configure the PIM engine.

Call the incoming PIM kernel with its argument for the execution.

Call __pim_release to release the PIM device ownership and clear the PIM engine states.

5.3 Case Study: Vector-Matrix Multiplication
5.3.1 Algorithm
There are two methods for the multiplication of a vector and a matrix: vector-matrix multiplication (column-wise manner) in Fig. 8a, and matrix-vector multiplication (row-wise manner) in Fig. 8b. Each method has its advantages and disadvantages. The row-wise method requires an adder-tree that sums the multiplication results. In DRAM, which is the basis of PIM, as the number of banks and channels increases, it is challenging to implement hardware capable of gathering and summing the banks’ results. Its associated overhead would be significantly increased. On the other hand, in the column-wise method, one column data of matrix B tend to be stored in different DRAM rows, which would incur significant row buffer misses.


Fig. 8.
(a) Vector-matrix multiplication (column-wise). (b) Matrix-vector multiplication (row-wise).

Show All

The DRAM burst operation would involve data parallelism, so it is imperative to exploit the parallelisms well in the execution. Therefore, we chose the vector-matrix multiplication method based on the column-wise way and relied on the programmer’s manual data layout for reducing the row buffer misses.

5.3.2 PIM Kernel
Fig. 9 shows the PIM kernel code for the vector-matrix multiplication by the column-wise manner, where we can execute each column of matrix B independently. We assume that vector A is already stored in all banks before the calculation for a simple explanation.


Fig. 9.
The PIM kernel code for the vector-matrix multiplication.

Show All

Each PIM engine includes 64-byte vACC storing 32 bfloat16 elements. Therefore, we can execute 32 consecutive columns of matrix B without spilling vACC into a memory, which we define as one chunk. We used the interleaved scheduling across all the banks, and executed 512 columns of matrix B (16 banks × 32 elements/bank) in parallel across all the banks (Line 4). The register vecA holds 32 elements, which can execute 32 rows of matrix B. Therefore, new 32 elements of vector A should be fetched from the bank every 32 rows of matrix B (Line 6). NUM_A_REGS represents the number of vecA entries (i.e., 32). After executing the last row of matrix B, the result is stored in the bank (Lines 26∼31).

SECTION 6Performance Analysis
6.1 Experimental Methodology
We evaluated the performance of three kernels (matrix multiplication and element-wise addition/multiplication of A and B) popularly used in LSTM [18] on real machines: the proposed PIM architecture (Xilinx Virtex UltraScale board/XCVU190), CPU (AMD Ryzen 7 1700/3,000MHz), and GPU (NVIDIA TITAN Xp/1,582MHz). We also used 16 logical cores for the multi-threading execution on the CPU, and the number was equal to the number of banks in the PIM device. For the matrix multiplication, we used the OpenBLAS for CPU [23], cuBLAS for GPU, and the extended PIM kernel in Fig. 9. For the element-wise kernel, we developed a simple program for CPU/GPU and compiled them with -O2. All the matrix sizes and the batch sizes were determined by [18].

The PIM performance would be better than CPU and GPU with low data reuse applications. The higher the kernel’s data reusability, the better it is to compute on the CPU or GPU using a cache. For example, the CONV kernel has very high data reuse, so its PIM execution is less efficient than the CPU and GPU executions. Also, the CONV kernel requires data sharing between banks; thus, it would incur high overhead for implementing it inside DRAM. Therefore, our experiment focused on only the LSTM kernels.

The goal of our experiment is to provide the design and performance insight of the DRAM-die PIM. However, the FPGA memory controller’s operating frequency is 156.25 MHz, much lower than that of our host AMD Ryzen 7’s system memory controller, 1,200 MHz. We multiplied the FPGA-based PIM’s performance by the frequency difference for predicting the DRAM-die PIM performance, which is acceptable due to the following reason. The host CPU and FPGA use the same DDR4 timing constraints. Since the DRAM performs in cycles (for example, tCL-tRCD-tRP-tRAS: 17-17-17-39 CK), the DRAM execution time directly depends on its operating frequency.

We used the column-wise access algorithm in Fig. 9; thus, a row buffer miss would occur whenever accessing the next row of matrix B. Therefore, to be friendly to the DRAM operational behaviors, matrix B was transposed in the matrix multiplication, and matrices A and B were interleaved by 64 bytes, and its performance was observed. All operations related to the data layout was excluded from the performance measurement because we could layout the data when generating them, like tensors in PyTorch.

As a result, we compared the performance of the following six experiments: ① CPU serial execution, ② PIM (No DMA) without using DMA, ③ PIM (DMA) using DMA without the contiguous memory allocation, ④ PIM (DMA+Layout) using both DMA and DRAM-friendly data layout with the contiguous memory allocation, ⑤ CPU (OMP) using OpenMP, and ⑥ GPU performance. The non-contiguous memory allocation requires to set the operand control registers whenever fetching a new page. Therefore, the performance was measured, including the overhead of writing to the configuration registers except for the PIM (DMA+Layout) experiment.

6.2 Execution Time
Fig. 10 shows the speedup by varying the batch size in three kernels, based on the CPU serial execution.


Fig. 10.
Speedup by varying the batch size, p. (a) Matrix multiplication of (p×512)×(512×2048). (b) Element-wise addition/multiplication of (p×2048). ① CPU (Serial), ② PIM (No DMA), ③ PIM (DMA), ④ PIM (DMA+Layout), ⑤ CPU (OMP), and ⑥ GPU.

Show All

6.2.1 Matrix Multiplication
Our PIM (DMA+Layout) performance was superior to CPU performance in all the cases, regardless of batch sizes. When the batch size, p, was 1, the speedups of PIM (DMA+Layout), PIM (DMA), and PIM (No DMA) were about 24.6x, 1.8x, and 0.3x, respectively.

In the case of PIM (No DMA), the slow PCI-e interface caused performance degradation significantly. The non-contiguous physical pages in PIM (DMA) limited the DMA burst size, requiring frequent interactions between the host and the DMA engine. The contiguous memory allocation of PIM (DMA+Layout) could reduce the interaction ideally by 32 times (the number of elements in vecA) and row buffer misses simultaneously. The overhead of writing to the configuration register was measured by 3.3 and 21.4 percent in PIM (No DMA) and PIM (DMA) on average.

When the batch size is 1, the computation becomes a vector-matrix multiplication that does not include any data reuse. The operand copy between system memory and GDDR degraded the GPU performance, about 1.5x better performance than the CPU. Since PIM did not require the operand copy, the speedup gain of PIM (DMA+Layout) was much higher than the GPU, i.e., 16.9x. We will discuss the memory copy overhead more in detail in Section 6.3. When the batch size increases, the data reuse also increases. Therefore, the speedup of PIM gradually decreased compared to the GPU. When the batch size was 128, the speedups of GPU, PIM (DMA+Layout), PIM (DMA), and PIM (No DMA) were about 55.7x, 10.8x, 1.1x, and 0.1x, respectively. The proposed PIM structure can fully exploit the bank/data-level parallelism by a burst operation and vector execution for each bank. However, the execution time linearly increased with the batch sizes since the PIM device could not hold abundant data in registers for their reuse. GPU could reuse the data using its GDDR memory and utilize the massive computational parallelism. In the case of CPU (OMP) using multi-threading, there was data reuse, but as the batch size increased, the performance was saturated due to capacity cache miss.

6.2.2 Element-Wise Computation
Except when the batch size is 1, the PIM (DMA+Layout) performance and the CPU performance were similar. Intrinsically, the element-wise computation does not involve any data reuse; thus, the number of DRAM accesses directly affects the execution time. Moreover, the addition and multiplication occurred the same execution behavior. When the batch size was 1, the PIM (DMA+Layout) speedup was 1.4x since the CPU execution incurred the row buffer miss at every operand access. Also, when the operand size was small, it was more affected by DRAM behavior. So the PIM (DMA+Layout) speedup seems to be larger than CPU. The overhead of writing to the configuration register was measured by 2.0 and 9.2 percent in PIM (No DMA) and PIM (DMA) on average.

The PIM (DMA) execution incurred frequent interactions between the host and DMA due to the non-contiguous page allocation. The GPU execution copied operands with no locality from system memory into GDDR. Therefore, their performance was lower than the CPU performance. The CPU (OMP) using multiple cores had the same characteristics in execution as when using a single core. However, its performance increased linearly, reducing each core’s workload by the number of cores.

6.3 Memory Behavior and Overhead
Fig. 11a shows the profiling result of the FPGA memory controller’s execution cycles, showing how much overhead occurs due to the PCI-e interface. The idle cycle implies when there is no memory request in the controller. In the matrix multiplication, the idle cycle in PIM (No DMA) occupied about 95.0 percent of the total execution cycle regardless of the batch sizes. The access interval between memory requests, especially read requests, was very long due to the slow interface; thus, the controller spent most time waiting for the request. In the PIM (DMA) execution, the idle cycle was about 92.4 percent of the total execution time, spent for the frequent interactions between the host and the DMA engine since the DMA burst size was limited to 16 bank units (512-bit × 16). In the case of PIM (DMA+Layout), the DMA burst size could be increased by 32 times; thus, the idle cycle could be reduced significantly, i.e., about 38.1 percent. In the element-wise operation, the idle cycle occupied about 48.5 percent in all the cases, which was less than the matrix multiplication. The element-wise execution involves the ratio of the number of read and write requests is 2:1. Since the write request has little overhead at the PCI-e interface, so the idle cycle ratio was small compared to the matrix multiplication.

Fig. 11. - 
PIM execution profiling. (a) Cycles. (b) The ratio of the DRAM commands. (c) Row buffer behaviors. ② PIM (No DMA), ③ PIM (DMA), and ④ PIM (DMA+Layout).
Fig. 11.
PIM execution profiling. (a) Cycles. (b) The ratio of the DRAM commands. (c) Row buffer behaviors. ② PIM (No DMA), ③ PIM (DMA), and ④ PIM (DMA+Layout).

Show All

Fig. 11b shows the ratio of the DRAM commands issued by the memory controller, and Fig. 11c shows the row buffer hit/miss ratio, which allows us to know how to allocate pages to the DRAM row. It should be noted that the number of RD and WR commands in all the experiments with the same batch size is the same since the Silent-PIM executes with the commands, and the memory behavior is almost the same for all ps. The ratio of RD and ACT commands are similar in PIM (No DMA) in both kernels since the read request interval was too long. The long interval occurred a refresh before the next read command was received, closing the row continuously. In the matrix multiplication, most column commands are RD commands, which incurred about 53.6 and 92.1 percent hit ratio in PIM (DMA) and PIM (DMA+Layout), respectively. The high hit ratio in PIM (DMA) without the data layout resulted from that the matrix sizes were sufficiently small, possibly mapping the different rows of the matrix to the same DRAM row. The element-wise execution showed a lower row hit ratio than the matrix multiplication. When the batch size was 1, the hit ratios of PIM (No DMA), PIM (DMA), and PIM (DMA+Layout) were 11.2, 24.9, and 33.2 percent, respectively. In particular, when the batch size was 128, the hit ratios of PIM (No DMA) and PIM (DMA) were 0 percent, which implied that row misses occurred every time. The low hit ratio resulted from the element-wise computation’s characteristics. As their sizes increase, all of the operands would be placed onto different DRAM rows, which incur the row buffer misses whenever accessing the operands. Even though the source operands are interleaved, since the row buffer miss occurs again when accessing the destination operand. Therefore, the row hit ratio in PIM (DMA+Layout) was measured as 32.5 percent regardless of the batch sizes.

Fig. 12 shows the GPU execution time divided into computation and memory copies. Regardless of the kernel types and batch sizes, the copy overhead was significant in all the cases, more than 95.0 percent of the total execution time. In the application execution to exploit very low data locality, the PIM performance would be always superior to GPU.


Fig. 12.
GPU execution profiling at the batch size, p. (a) Matrix multiplication of (p×512)×(512×2048). (b) Element-wise addition/multiplication of (p×2048).

Show All

6.4 EDP Analysis
We estimated each platform’s power consumption by summing its components as follows: CPU (CPU+DRAM), GPU (CPU+GPU+DRAM), and PIM (CPU+DRAM+PIM engine). We assumed that all the platforms used the same system DRAM. We did not include the FPGA power in the analysis result by assuming that we implemented the PIM engine inside the system DRAM.

For the accurate power measurement, we ran the kernels on real CPU and GPU machines and collected their average power consumption of 22W and 60W, respectively, by using the performance profiler tools: the Running Average Power Limit (RAPL) interface [24] for the CPU power measurement and the Nvidia System Management interface [25] for the GPU power measurement. The AMD processor did not provide a method to measure the DRAM power through the RAPL interface, so we used the DRAMPower tool [26] and achieved its average power consumption of 3.4W. Also, we used the Synopsys design compiler with 65 nm PDK for the PIM engine’s performance metrics, which has similar characteristics to the current DRAM process [20], for the Verilog-modeled PIM engine’s power measurement. The PIM power consumption was evaluated as 0.03W.

Fig. 13 shows the power consumption in each platform. DRAM consumed 5.6 percent, and the PIM engine consumed about 0.04 percent of the GPU power consumption. As we predicted, the power consumption in the execution of a single core and PIM execution was measured almost similarly. The multicore execution (CPU (OMP)) consumed more power than a single core execution, and GPU consumed about 15.0 percent less than the multicores.


Fig. 13.
Average power consumption at the batch size, p. (a) Matrix multiplication of (p×512)×(512×2048). (b) Element-wise addition/multiplication of (p×2048). ① CPU (Serial), ④ PIM (DMA+Layout), ⑤ CPU (OMP), and ⑥ GPU.

Show All

Fig. 14 shows the EDP performance in each platform. In the matrix multiplication, when the batch size was 1, the PIM’s EDP performance was about 676x and 992x better than those of the CPU and the GPU, respectively. The reason was that PIM consumed less power and executed faster than the CPU and the GPU. The GPU’s power consumption was more significant than CPU, so its EDP performance was 1.4x worse. At the larger batch size, GPU’s execution became much faster, so its EDP performance improved, finally better 1064x and 8.2x than the CPU and the PIM. The PIM’s execution time linearly increased, so the EDP performance gap between PIM and CPU decreased to 128x. In the element-wise kernel, GPU performance was not always good at any batch size since GPU’s copy overhead was dominant in its total execution time due to no data reuse. The CPU consumed more power than the PIM, but the larger the batch size, the smaller the speedup gap from the PIM, so the EDP gap decreased. PIM’s EDP performance was superior to any platform.

Fig. 14. - 
EDP at the batch size, $p$p. (a) Matrix multiplication of $(p \times 512) \times (512 \times 2048)$(p×512)×(512×2048). (b) Element-wise addition/multiplication of $(p \times 2048)$(p×2048). ① CPU (Serial), ④ PIM (DMA+Layout), ⑤ CPU (OMP), and ⑥ GPU.
Fig. 14.
EDP at the batch size, p. (a) Matrix multiplication of (p×512)×(512×2048). (b) Element-wise addition/multiplication of (p×2048). ① CPU (Serial), ④ PIM (DMA+Layout), ⑤ CPU (OMP), and ⑥ GPU.

Show All

SECTION 7Related Work
Recent PIM studies have been extensively studied with near-data processing and in-memory processing. The NDP places the hardware logic like general cores and accelerators very near memory, in general, on the logic die of the 3D-stacked memory. The in-memory processing uses computing logic inside the memory to exploit its internal bandwidth and power-efficient computation.

The NDP design using the using 3D-stacked memory [6], [7], [8], [9], [10] is not different from the typical accelerator design. However, there are important design and performance issues in offloading PIM tasks [27], [28] on the die and supporting PIM-specific ISAs [6], [8], [9]. The former issue is related to which code sections should be executed on the PIM die. The latter is related to how the PIM accelerators cooperate with a host. Unfortunately, most of the previous studies needed to modify hardware components like address translations [6], [7], [10], core pipelines [8], [9], memory controllers [6], [7], [9], [10], and so on.

An in-situ analog-based in-memory processing using ReRAM [14], [15], [16], where memory cells store not only values but also operations, and most of them used ReRAM as a memory platform. These papers implemented in-situ analog-based memristors [29] as a processing unit. It would simplify the computing the data by the analog operation, so the data need to be handled by ADC/DAC. Therefore, the ADC/DAC area overhead would be significant in memory. Ambit [12] and DrAcc [13] implemented basic logic operations based on charge sharing among different rows, which can exploit full internal bandwidth inside DRAM. However, the charge sharing mechanism requires activation of several DRAM rows at once, which would lead to high power consumption and become unreliable with a variation.

McDRAM [11] is similar to our architecture that placed MAC units in the region of the DRAM column decoder, i.e., inside DRAM. More recently, they proposed a systolic array architecture to make the use of the internal bandwidth and share data from banks. However, they did not consider the design at the system level, thus not showing the software execution.

SECTION 8Conclusion
In this paper, we proposed the Silent-PIM architecture, performing inside DRAM with standard DRAM memory requests, and presented how the entire architecture layers, including applications, operating systems, and PIM devices to perform the computation. Also, only using the standard memory requests allowed us to use the popular DMA as the Silent-PIM offloading engine, which significantly reduced the memory request overhead. Furthermore, our proposal did not require any modification of hardware components except for our PIM device; thus, we would apply our PIM device to current commodity platforms.

We analyzed the performance from the real execution of the Silent-PIM modeled FPGA platform, CPU, and GPU using the three kinds of LSTM kernels by varying the batch size: a matrix multiplication, an element-wise multiplication, and an element-wise addition. The Silent-PIM achieved significant performance in all the performance metrics when the kernel did not exploit any data locality. In the execution without data reuse, i.e., the (p×512)×(512×2048) matrix multiplication with p=1, the Silent-PIM achieved 24.6x and 16.9x speedup and 676x and 992x EDP improvement over CPU and GPU, respectively. In (p×2048) element-wise multiplication and addition kernels, the Silent-PIM showed a similar performance gain. On the other hand, in the execution with data reuse, i.e., the matrix multiplication with p=128, the GPU performed very fast, which resulted in its superior EDP performance than the others.

We are sure that this work provides great insight into PIM design as the main memory in the real system for research and development in our community and widens PIM’s applicability to the real world.