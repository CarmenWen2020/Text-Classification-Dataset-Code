Federated Learning is the current state-of-the-art in supporting secure multi-party machine learning (ML): data is maintained on the owner's device and the updates to the model are aggregated through a secure protocol. However, this process assumes a trusted centralized infrastructure for coordination, and clients must trust that the central service does not use the byproducts of client data. In addition to this, a group of malicious clients could also harm the performance of the model by carrying out a poisoning attack. As a response, we propose Biscotti: a fully decentralized peer to peer (P2P) approach to multi-party ML, which uses blockchain and cryptographic primitives to coordinate a privacy-preserving ML process between peering clients. Our evaluation demonstrates that Biscotti is scalable, fault tolerant, and defends against known attacks. For example, Biscotti is able to both protect the privacy of an individual client's update and maintain the performance of the global model at scale when 30 percent adversaries are present in the system.
A COMMON requirement in machine learning (ML) applications is the collection of massive amounts of training data. This data is frequently distributed, such as among hospitals or devices in an IoT deployment. However, when training ML models in a multi-party setting, users must share their potentially sensitive information with a centralized service [1], [2], [3], [4]. Such sharing is problematic for users or companies who are not willing to trust a third party. For example, pharmaceutical companies compete with each other in drug discovery and rarely share data.1 And, internet users are increasingly aware of the value of their data and would like to retain control over their data.2 To avoid directly sharing sensitive data, federated learning [5], [6], [7] is a prominent solution for large-scale secure multi-party ML: clients train a shared model through a trusted aggregator without revealing their underlying data or computation [8], [9], [10]. But, doing so introduces a subtle threat: clients, who previously acted as passive data contributors, are now actively involved in the training process [11], [12], [13], presenting new privacy and security challenges to multi-party ML systems [14].

Prior work has demonstrated that adversaries can attack the shared model through poisoning attacks [11], [12], [15], [16], [17], [18], [19], in which an adversary contributes adversarial updates to shared model parameters. Adversaries can also attack the privacy of other clients in federated learning: in an information leakage attack, an adversary poses as an honest data provider and attempts to infer properties of the sensitive training data of a target client through observation of the target's shared model updates [13], [20].

Both poisoning and information leakage attacks have individually been defended in prior work through centralized anomaly detection [21], differential privacy [22], [23], [24], [25] or secure aggregation [26], [27], but to the best of our knowledge, a private and decentralized solution that solves both threats concurrently does not yet exist. Furthermore, these approaches are inapplicable in decentralized contexts that lack a trusted central authority. In this work, we focus on the decentralized P2P setting, which enables a stronger data ownership model for distributed ML.

Because ML does not require strong consensus or consistency to converge [28], [29], traditional strong consensus protocols such as Byzantine Fault Tolerant (BFT) protocols [30] are overly restrictive for machine learning workloads. Distributed ledgers (blockchains) [31] have emerged as a more appropriate system to facilitate private, verifiable, crowd-sourced computation. Through design elements such as publicly verifiable proof of work, eventual consistency, and ledger-based consensus, blockchains have been used in decentralized multi-party settings, such as currency management [31], [32], archival data storage [33], [34], financial auditing [35], privacy-preserving computation [36] and IoT edge computation [37], [38], [39].

As designed, federated learning relies on a trusted aggregator, and is unsuitable for peer-to-peer (P2P) ML settings. In this work, we propose Biscotti, a decentralized public P2P system that co-designs privacy-preserving multi-party ML with a blockchain ledger. In contrast to on-blockchain, layer-2 ML applications [40], we propose proof-of-federation (PoF), a layer-1 blockchain consensus protocol that combines the state-of-the-art in defenses for federated learning and makes them applicable in decentralized P2P settings. Biscotti coordinates ML training between peers who are weighed by the value, or stake, that they provide to the system through PoF. Inspired by prior work [32], Biscotti uses consistent hashing based on PoF in combination with verifiable random functions (VRFs) [41] to select key roles for peers who will help coordinate the privacy and security of model updates. PoF prevents groups of colluding peers from overtaking the system without a sufficient stake ownership.

With Biscotti, our primary contribution is to adapt several prior techniques into one coherent system that provides secure and private multi-party machine learning in highly distributed P2P settings. In particular, Biscotti prevents peers from poisoning the model through the Multi-Krum defense [42], provides privacy through differentially private noise [22], [23], and uses Shamir secrets for secure aggregation [43].

We evaluated Biscotti on Azure and considered its performance, scalability, churn tolerance, and ability to withstand different attacks. We found that Biscotti can train an MNIST softmax model with 200 peers on a 60,000 image dataset in 266.7 minutes, while withstanding up to 30 percent of adversarial peers. In addition, we show that Biscotti's design is resilient to information leakage attacks [13] that require knowledge of a client's SGD update, and that Biscotti is resilient to poisoning attacks [44] from prior work. Biscotti is also fault tolerant, coping with nodes that churn every 1.875 s and provides model training that converges even with node churn.

SECTION 2Challenges and Contributions
We now describe the key challenges in designing a P2P solution for multi-party ML and the key pieces in Biscotti's design that resolve each of these challenges.

Sybil Attacks: Consistent Hashing Using VRF's and PoF.In a P2P setting adversaries can collude or generate aliases to increase their influence in a sybil attack [45]. Biscotti uses a consistent hashing protocol based on the latest block hash and verifiable random functions (VRF's) (see Appendix D, which can be found on the Computer Society Digital Library at http://doi.ieeecomputersociety.org.ezproxy.auckland.ac.nz/10.1109/TPDS.2020.3044223) to select a subset of peers that are responsible for the different stages of the PoF protocol: adding noise to updates, validating an update, and securely aggregating the update. To mitigate the effect of sybils, PoF selects peers proportional to their stake. A peer's stake is the reputation that the peer acquires by positively contributing to the shared model. This ensures that an adversary cannot increase their influence in the system by creating multiple peers without improving the model. We evaluate the security of our VRF mechanism and PoF in Appendix K, available in the online supplemental material.

Poisoning Attacks: Update Validation Using Multi-Krum. In multi-party ML, peers possess a disjoint and private subset of the training data. As mentioned above, adversaries can exploit the private P2P setting to stealthily execute poisoning attacks [12], [18], [46], [47], [48].

In multi-party settings, we assume that baseline validation models are not available to peers, so Biscotti validates an SGD update by evaluating it with respect to the updates submitted by other peers. Biscotti validates an SGD update using a Byzantine-tolerant aggregation scheme called Multi-Krum [42]. Multi-Krum is only one of several algorithm that Biscotti could use; others include median/trimmed mean and Bulyan [49], [50]. Although Multi-Krum and related aggregation schemes do not protect against all poisoning attacks [47], they are effective against some classes of attacks and Biscotti can generally support any aggregation scheme that operates only on model updates.

Multi-Krum rejects model updates that differ heavily from the direction of the majority of the updates. In our implementation of Multi-Krum, described in Section 4.5, in each round a committee of validation peers is selected by a majority vote and each member of the committee uses Multi-Krum to remove these anomalous updates. Multi-Krum guarantees convergence against f Byzantine adversaries in a system with n total clients (when 2f+2<n). We demonstrate in Section 6.1 that by using Multi-Krum, Biscotti can handle poisonous updates from up to 30 percent malicious clients.

Information Leakage Attacks: VRF Verifier Peers and Differentially-Private Updates Using Pre-Committed Noise. By observing a peer's model updates from each verification round, an adversary can perform an information leakage attack [13], [20], [51], [52], [53], [54] and recover details about a victim's training data. (See Appendix E, available in the online supplemental material, for background.)

Biscotti prevents such attacks during update verification in two ways. First, the latest block hash is used to select the verifiers, ensuring that malicious peers cannot deterministically select themselves to verify a victim's gradient. Second, noising peers send differentially-private updates [22], [23] to verifier peers: before sending a gradient to a verifier, pre-committed ε-differentially private noise is added to the update, masking the peer's gradient such that no individual peer can influence or observe the model update process. (See Appendix I, available in the online supplemental material, for details). By verifying noised model updates, verifier peers in Biscotti can verify the updates of others without directly observing their un-noised counterparts.

Utility Loss With Differential Privacy: Secure Update Aggregation and Cryptographic Commitments. The blockchain-based ledger of model updates allows for auditing of state, but this transparency is counter to the goal of privacy-preserving multi-party ML. For example, the ledger trivially leaks information if we store SGD updates directly.

Using differentially private updates during verification is one technique for preserving data privacy in Biscotti. Another technique used is secure aggregation: a block in Biscotti does not store updates from individual peers, but rather an aggregate that obfuscates any single peer's contribution in a single round. Biscotti uses a verifiable secret sharing scheme [55] to aggregate the updates so that any individual update remains hidden through cryptographic commitments (See Appendix C, available in the online supplemental material, for background).

However, secure aggregation can be either done on the differentially-private updates or the original updates with no noise. This design choice represents a privacy-utility tradeoff in the final model. By aggregating differentially-private updates the noise is pushed into the ledger and the final model has lower utility. We illustrate this trade-off experimentally in Appendix H, available in the online supplemental material, and choose to aggregate the un-noised model updates, providing stronger utility guarantees that match the performance of federated learning. Furthermore, we evaluate the potentital for information leakage attacks in Appendix I, available in the online supplemental material, and show the the probability of a successful attack on Biscotti is negligible.

SECTION 3Assumptions and Threat Model
Like federated learning, Biscotti assumes a public ML system in which peers can join/leave anytime. Biscotti assumes that users are willing to collaborate on training ML models, but are unwilling to share their data [5].

3.1 Design Assumptions
Proof of Federation. Peers in Biscotti use proof-of-federation (PoF) to arrive at a consensus on the state of the model for each round of training. PoF is a consensus protocol for secure and private federated learning based on a recently proposed blockchain consensus mechanism called proof-of-stake (PoS) [32], [56]. Consensus using PoS is fast because it delegates the consensus responsibility to a subset of peers in each round, assigned proportionally based on their stake (see Appendix F, available in the online supplemental material, for background). In cryptocurrencies, the stake of a peer refers to the amount of value (money) that a peer holds. PoS relies on the assumption that a subset of peers holding a significant fraction of the stake will not subvert the system.

In Biscotti's PoF, we define stake as a measure of a peer's contribution to the system. Peers acquire stake by providing beneficial model updates or by facilitating the consensus process. Thus, the stake that a peer accrues during training is proportional to their contribution to the model being trained. We assume that the stake ownership of any peer is publicly available from the current state of the blockchain.

In addition, we also assume that at any point in time, at least 70 percent of the stake in the system is honest and is properly bootstrapped. The initial stake distribution at the start may be derived from an online data sharing marketplace, a shared reputation score among competing agencies, or auxiliary information from a social network.

Blockchain Topology. Each peer is connected to some subset of other peers in a topology that allows flooding-based dissemination of blocks to eventually reach all peers. For example, this could be a random mesh topology with flooding, similar to the one used for block dissemination in Bitcoin [31]. Peers that rejoin the system after going offline during training can recreate the latest state of the blockchain from other peers in the system.

Machine Learning. We assume that all information required for P2P training is disseminated to all peers in the first block, including the model, hyperparameters, optimization algorithm, and learning objective. In a non-adversarial setting, peers have local datasets that they wish to keep private. When peers draw a sample from this data to compute a model update, we assume that this is done uniformly and independently. This ensures that the Multi-Krum [42] is accurate. In our design of Biscotti we assume stochastic gradient descent (SGD) [57] as the optimization algorithm. SGD is a general learning algorithm that can be used to train a variety of models, including deep neural networks [57].

3.2 Attacker Assumptions
Adversarial peers may perform a poisoning attack by sending malicious updates or an information leakage attack by observing a target peer's updates. In doing so, we assume that the adversary may control multiple peers in a sybil attack [45] but does not control more than 30 percent of the total stake. Although adversaries may be able to increase the number of peers they control in the system, we assume that adversaries cannot artificially increase their stake in the system except by providing valid updates that pass Multi-Krum [42].

When adversaries perform a poisoning attack, we assume that their goal is to harm the performance of the final global model. Our defense relies on filtering out malicious updates that are sufficiently different from the honest clients and push the global model towards an sub-optimal objective. For the purposes of this work, we limit adversaries to poisoning attacks [44] in which data is mislabelled to a different class, causing the trained model to misclassify it. This does not include poisoning attacks that leverage unused parts of the model topology, like backdoor attacks [12], attacks based on gradient-ascent [58], or adaptive attacks based on knowledge of the poisoning defense [47]. However, Biscotti is compatible with any other aggregation approach on SGD updates, including future approaches that may handle backdoor and gradient-ascent attacks.

When adversaries perform an information leakage attack, we assume that they aim to learn properties of a victim's local dataset. Specifically, we provide record-level privacy, which protects against the de-anonymization of a single example from the user's dataset. Due to the vulnerabilities of secure aggregation, we do not consider information leakage attacks with side information [51], [59] or attacks against class-level privacy [20], which attempt to learn the properties of an entire target class.

SECTION 4Biscotti Design
Biscotti's design has the following goals: (1) converge to the optimal global model (the same model trained without adversaries in a federated learning setting), (2) prevent poisoning by verifying peer model updates, (3) keep peer training data private by preventing information leakage attacks, and (4) prevent colluding peers from gaining influence without acquiring sufficient stake. Biscotti meets these goals with a custom blockchain design that we now describe.

Design Overview.Peers join Biscotti and collaboratively train a global model. Each block in the distributed ledger represents a single iteration of SGD and the ledger contains the state of the global model at each iteration. Fig. 1 overviews the Biscotti design with a step-by-step illustration of what happens during a singleSGD iteration in which a single block is generated.


Fig. 1.
The ordered steps in a single iteration of the Biscotti algorithm, from step 1 (local SGD) to step 8 (block generation).

Show All

In each iteration, peers locally compute SGD updates (step ① in Fig. 1). Since SGD updates must be kept private, each peer first masks their update using differentially private noise. This noise is collected from a set of noising peers unique to each client and is selected by a VRF [41] (step ② and ③).

The masked updates are validated by a verification committee to defend against poisoning. Each member in the verification committee signs the commitment to the peer's unmasked update if it passes Multi-Krum (step ④). If the majority of the committee signs an update (step ⑤), the signed update is divided into Shamir secret shares (step ⑥) and given to a aggregation committee. The aggregation committee uses a secure protocol to aggregate the unmasked updates (step ⑦). All peers who contribute a share to the final update along with the peers chosen for the verification and aggregation committees receive additional stake in the system. The aggregate of the updates is added to the global model in a newly created block which is disseminated to all the peers and appended to the ledger (step ⑧). Using the updated global model and stake, the peers repeat (step ①). Next, we describe how we bootstrap the training process.

4.1 Initializing the Training
Biscotti peers initialize the training process using information in the first (genesis) block. We assume that a trusted authority facilitates and bootstraps the training process by publicly distributing the genesis block out of band to all peers in the system. The authority is only trusted for this step: they are not entrusted with the individual SGD updates of the peers, which could potentially leak private information of a peer's data. Each peer obtains the following information from the genesis block: (1) the initial model state w0 and expected number of iterations T, (2) the public key PK for creating commitments to SGD updates, (see Appendix C, available in the online supplemental material), (3) the public keys PKi of all other peers in the system, which are used to create and verify signatures during verification, (4) pre-commitments to differentially private noise for T SGD iterations ζ1..T by each peer, (see Fig. 3 and Appendix B, available in the online supplemental material), (5) the initial distribution of stake among peers, and (6) a stake update function that executes when new blocks are appended.


Fig. 2.
Block contents at iteration t. Note that wt is computed using wt−1+Σwu where wt−1 is the global model stored in the block at iteration t−1 and j is the set of verifiers for iteration t.

Show All


Fig. 3.
Peers commit noise to an N by T structure. Each row i contains all the noise committed by a single peer i, and each column t contains potential noise to be used during iteration t. When committing noise at an iteration i, peers execute a VRF and request ζki from peer k.

Show All

4.2 Blockchain Design
Distributed ledgers are constructed by appending read-only blocks to a chain structure and disseminating blocks through a gossip protocol. Each block holds a pointer to its previous block as a cryptographic hash of its contents.

Each block in Biscotti (Fig. 2) contains, in addition to the previous block hash pointer, an aggregate (Δw) of SGD updates from multiple peers and a snapshot of the global model wt at iteration t. Newly appended blocks to the ledger store the aggregated updates ∑Δwi of multiple peers. To verify that the aggregate was honestly computed, individual updates need to be included in the block. However, storing them individually leaks information about individual private training data [13], [20]. We solve this problem by using polynomial commitments [55]. Polynomial commitments take an SGD update and map it to a point on an elliptic curve (see Appendix C, available in the online supplemental material, for details). By including a list of commitments for each peer i's update COMM (Δwi) in the block, we can provide both privacy and verifiability of the aggregate. The commitments provide privacy by hiding the individual updates yet can be homomorphically combined to verify that the update to the global model by the aggregator ∑Δwi was computed honestly. The following equality holds if the list of committed updates equals the aggregate sum
COMM(∑Δwi)=∏iCOMM(Δwi).
View SourceRight-click on figure for MathML and additional features.

The training process continues for a specified number of iterations T, at which point the learning process stops and each peer extracts the global model from the final block.

4.3 Using Stake for Role Selection
In each iteration in Biscotti, a consistent hash weighted by peer stake designates roles (noiser, verifier, aggregator) to some peers in the system. PoF ensures that the influence of a peer is bounded by their stake (i.e., adversaries cannot trivially increase their influence through sybils). Peers may be assigned multiple roles in a given iteration but cannot be a verifier and aggregator in the same round. The verification and aggregation committees are the same for all peers but the noising committee is unique to each peer.

The initial SHA-256 hash of the last block is repeatedly re-hashed: each new hash result is mapped onto a hash ring where portions of the ring are proportionally assigned to peers based on their stake. The hash is repeated until verifier/aggregator committees of the correct size are obtained. This provides the same stake-based property as Algorand [32]: a peer's probability of winning a lottery is proportional to their stake. Since an adversary cannot predict the future state of a block until it is created, they cannot speculate on outputs of consistent hashing and strategically perform attacks. Unlike verification and aggregation, the noising committee is different for each peer for additional privacy. Each peer can arrive at a unique committee for itself via consistent hashing by using a different initial hash. The hash computed should be random yet globally verifiable by other peers in the system. In Biscotti, a peer computes this hash by executing their secret key SKi and the SHA-256 hash of the last block through a verifiable random function (VRF) (see Appendix D, available in the online supplemental material). By virtue of the peer's secret key, the hash computed is unique to the peer resulting in distinct committees for different peers. The VRF hash is also accompanied by a proof that can be combined with a peer's public key PKi, allowing other peers to determine that the correct noising committee is selected by the peer.

At each iteration, peers run the consistent hashing protocol to determine whether they are an aggregator or verifier. Peers that are part of the verification and aggregation committees do not contribute updates for that round. Each peer that is not an aggregator or verifier computes their noising committee, obtains the noise, and hides their updates by following the noising protocol.

4.4 Noising Protocol
To prevent information leakage attacks, peers use differential privacy to hide their updates during verification by adding noise sampled from a normal distribution. This ensures that each step is (ϵ,δ)-differentially private [23]. (See Appendix B, available in the online supplemental material, for formalisms).

Using Pre-Committed Noise to Thwart Poisoning.Attackers may maliciously use the noising protocol to execute poisoning or information leakage attacks. For example, a peer can send a poisonous update Δwpoison, and add noise ζp that unpoisons this update to resemble an honest update Δw, such that Δwpoison+ζp=Δw. By doing this, the honest update Δw passes verification, but the poisonous update Δwpoison is applied to the model since the noise is removed in the final aggregate.

An information leakage attack may also occur when a noising peer A and a verifier B collude against a victim peer C. The noising peer A can commit a set of zero noise that does not hide the original gradient value at all. When the victim peer C sends its noised gradient to the verifier B, B can perform an information leakage attack on client C's gradient to reconstruct C's training data.

To prevent such attacks, Biscotti requires that every peer pre-commits the noise vector ζt for every future iteration t∈[1..T] in the genesis block, and that each iteration uses a private VRF to select a group of noising peers based on the victim C's secret key. In doing so, an adversary cannot pre-determine whether their noise will be used in a specific verification round by a particular victim, and also cannot pre-determine if the other peers in the noising committee will be malicious in a particular round. We analyze this probability in Appendix I, available in the online supplemental material, and show that the probability of an information leakage is negligible given a sufficient number of noising peers.

Protocol Description. For an ML workload that may be expected to run for a specific number of iterations T, each peer i generates T noise vectors ζt and commits these noise vectors into the ledger, storing a table of size N by T (Fig. 3). When a peer is ready to contribute an update in an iteration, it runs the noising VRF and contacts each noising peer k, requesting the noise vector ζk pre-committed in the genesis block COMM(ζk). The peer then uses a verifier VRF to determine the set of verifiers. The peer masks their update using this noise and submits to these verifiers the masked update, a commitment to the noise, and a commitment to the unmasked update. It also submits the noise VRF proof that attests to the verifier that its noise is sourced from peers that are a part of their noise VRF set.

4.5 Verification Protocol
Verifier peers run Multi-Krum on the received set of updates and filter out malicious updates by accepting the top majority of the updates received in each round. Each verifier receives the following from each peer i: (1) the masked SGD update: (Δwi+∑kζk), (2) a commitment to the SGD update: COMM(Δwi), (3) a set of k noise commitments: {COMM(ζ1),COMM(ζ2),…,COMM(ζk))} and (4) a VRF proof confirming the identity of the k noise peers.

When a verifier receives a masked update from another peer, it can confirm that the masked SGD update is consistent with the commitments to the unmasked update and the noise by using the homomorphic property of the commitments [55]. A masked update is legitimate if the following equality holds:
COMM(Δwi+∑kζk)=COMM(Δwi)∗∏kCOMM(Δζk).
View Source

Once the verifier receives a sufficiently large number of updates R, it proceeds with selecting the best updates using Multi-Krum, as follows:

For every peer i, the verifier calculates a score s(i) which is the sum of euclidean distances of i's update to the closest R−f−2 updates. It is given by:
s(i)=∑i→j∥(Δwi+∑i,kζi,k)−(Δwj +∑j,kζj,k)|2,
View Sourcewhere i→j denotes the fact that (Δwj +∑j,kζj,k) belongs to the R−f−2 closest updates to (Δwi+∑i,kζi,k).

The R−f peers with the lowest scores are selected while the rest are rejected.

The verifier signs the COMM(Δwi) using its public key for all the accepted updates.

To prevent a malicious verifier from accepting all updates of its colluders in this stage, we require a peer to obtain signatures from the majority of verifiers before their update is accepted and disseminated for aggregation.

4.6 Aggregation Protocol
All peers with a sufficient number of signatures from the verification stage submit their SGD updates for aggregation into the global model. The update equation in SGD (see Appendix A, available in the online supplemental material) can be re-written as
wt+1=wt+∑i=1ΔwverifiedΔwi,
View SourceRight-click on figure for MathML and additional features.where Δwi is the verified SGD update of peer i and wt is the global model at iteration t.

The aggregation protocol enables a set of m aggregators, predetermined by consistent hashing, to compute ∑iΔwi without observing any individual updates. Biscotti uses a technique that preserves privacy of the individual updates if at least half of the m aggregators participate honestly in the aggregation phase. This guarantee holds if consistent hashing selects a majority of honest aggregators, which is likely when the majority of stake is honest. Biscotti achieves the above guarantees using polynomial commitments (described in Appendix C, available in the online supplemental material) and verifiable secret sharing [43] for aggregation of individual updates. We describe the details of the aggregation protocol in Appendix G, available in the online supplemental material.

4.7 Blockchain Consensus
Since subsets based on consistent hashing are globally observable by each peer and rely on the SHA-256 hash of the latest block in the chain, ledger forks should rarely occur in Biscotti. For an update to be included in the ledger at any iteration, the same noising/verification/aggregation committees are used. Thus, race conditions between aggregators will not cause forks in the ledger to occur as frequently as in e.g., BitCoin [31].

When a peer observes a new block through the gossip protocol, it can verify that the computation performed is correct by running the consistent hashing protocol for the latest block and verifying the signatures of the designated verifiers and aggregators for each new block.

In Biscotti, each verification and aggregation step occurs only for a specified duration. Any updates that are not successfully propagated in this period of time are dropped: Biscotti does not append stale updates to the model once competing blocks have been committed to the ledger. This synchronous SGD model is acceptable for large scale ML workloads which have been shown to be tolerant of bounded asynchrony [28]. However, these stale updates could be leveraged in future iterations if their learning rate is decayed [60]. We leave this optimization for future work.

SECTION 5Implementation
We implemented Biscotti in 4,500 lines of Go 1.10 and 1,000 lines of Python 2.7.12 and released it as an open source project.3 We use Go to handle all networking and distributed systems aspects of our design. We used PyTorch 0.4.1 [61] to generate SGD updates and noise during training. By building on the general-purpose API in PyTorch, Biscotti can support any model that can be optimized using SGD. We use the go-python v1.0 [62] library to interface between Python and Go. Since go-python is incompatible with Python 3, we were limited to using Python 2.7 for our implementation.

We use the kyber v.2 [63] and CONIKS 0.1.1 [64] libraries to implement the cryptographic parts of our system. We use CONIKS to implement our VRF function and kyber to implement the commitment scheme and public/private key mechanisms. To bootstrap clients with the noise commitments and public keys, we use an initial genesis block. We used the bn256 curve API in kyber for generating our commitments and public keys that form the basis of the aggregation protocol and verifier signatures. For signing updates, we use the Schnorr signature [65] scheme instead of ECDSA because multiple verifier Schnorr signatures can be aggregated together into one signature [66]. Therefore, our block size remains constant as the verifier set grows.

SECTION 6Evaluation
We had several goals in the evaluation of our Biscotti prototype. We wanted to demonstrate that (1) Biscotti is robust to poisoning attacks, (2) Biscotti protects the privacy of an individual client's data and (3) Biscotti is scalable, fault-tolerant and can be used to train different ML models.

For experiments done in a distributed setting, we deployed Biscotti across 20 Azure A4m v2 virtual machines, with 4 CPU cores and 32 GB of RAM. We deployed a varying number of peers in each of the VMs. The VM's were spread across six locations: West US, East US, Central India, Japan East, Australia East and Western Europe. Each experiment was executed for 100 iterations and the test error of the global model was recorded. To evaluate Biscotti's defense mechanisms, we ran information leakage and poisoning attacks on federated learning from prior work [13], [44] and measured their effectiveness under various attack scenarios and Biscotti parameters. We also evaluated the performance implications of our design by isolating specific components of Biscotti across varying committee sizes. For all our experiments, we deployed Biscotti with the parameter values in Table 1 unless stated otherwise.

TABLE 1 The Default Parameters Used for All Our Experiments, Unless Stated Otherwise

We evaluated Biscotti with both a logistic regression and softmax classifiers. We evaluate logistic regression with a Credit Card fraud dataset [67], which uses an individual's financial and personal information to predict if they will default on their next credit card payment. Our softmax classifier contains a 1-layer neural network with a binary cross entropy loss, and we evaluate it on the MNIST [68] dataset, a task that involves predicting a digit based on its image. We claim that the general-purpose PyTorch API allows Biscotti to support models of arbitrary size and complexity, as long as they can be optimized with SGD and can be stored in our block structure.

The MNIST and Credit Card datasets have 60,000 and 21,000 training examples respectively. We used 5-fold cross validation on the training set to determine training parameters for each model. The Credit Card logistic regression model uses a batch size of 10, learning rate of 0.01 and no momentum. The MNIST softmax model uses a batch size of 10, learning rate of 0.001 and momentum of 0.75.

The MNIST/Credit Card models were tested using a separately held-out test set of 10,000 and 9,000 examples respectively. For all experiments, the training dataset was divided equally among the peers unless stated otherwise. As a result, each client possesses 600 training examples for MNIST and 210 examples for the credit card dataset. Table 2 shows the details of our datasets. In local training we were able to train a model on the Credit Card and MNIST datasets with accuracy of 98 and 92 percent, respectively.

TABLE 2 The Dataset/Model Types Used in the Experiments
Table 2- 
The Dataset/Model Types Used in the Experiments
6.1 Tolerating Poisoning Attacks
In this section, we evaluate Biscotti's performance against a label-flipping poisoning attack [44]. We also investigate the different parameter settings required for Biscotti to successfully defend against a poisoning attack and evaluate how well it performs under attack from 30 percent malicious nodes compared to a federated learning baseline.

Biscotti requires each peer in the verification committee to collect a sufficient sample of updates before running Multi-Krum. We evaluate the effect of varying the percentage of total updates collected in each round by Multi-Krum against varying poisoning attack rates in MNIST to evaluate Multi-Krum's effectiveness. To ensure uniformity and to eliminate latency effects in the collection of updates, in these experiments the verifiers waits to collect all updates from all peers that are not assigned to a committee and then randomly samples a specified number of these updates. In addition, we also ensured that all verifiers deterministically sampled the same set of updates by using the last block hash as the random seed. This allows us to investigate how well Multi-Krum performs in a decentralized ML setting like Biscotti unlike the original Multi-Krum paper [42] in which updates from all clients are collected before running Multi-Krum.

To evaluate the success of an attack, we define attack rate as the fraction of target labels that are incorrectly predicted. An attack rate of 1 specifies a successful attack while a value close to zero indicates an unsuccessful one. Fig. 4 shows both the total execution time and the resulting attack rate when Biscotti is attacked by a varying number of poisoners. Although our theoretical guarantee ensures that Biscotti can withstand poisoning attacks from up to 30 percent poisoners, Biscotti performs well even against 35 percent poisoners. Beyond 35 percent, Multi-Krum is unable to prevent the poisoning attack from occurring, impacting both the final model performance and the total execution time. Multi-Krum is also impacted by the privacy parameter ε and the number of samples used when removing anomalies. Experiments detailing the impact of both parameters on Multi-Krum's poisoning deterrence are in Appendix J, available in the online supplemental material.

Fig. 4. - 
Biscotti's performance on the MNIST dataset with a varying number of poisoners, from 5 to 50 percent. Both the total execution time and the final attack rate are scaled such that the maximum is 1.
Fig. 4.
Biscotti's performance on the MNIST dataset with a varying number of poisoners, from 5 to 50 percent. Both the total execution time and the final attack rate are scaled such that the maximum is 1.

Show All

Biscotti versus Federated Learning Baseline. We deployed Biscotti and federated learning and subjected both systems to a poisoning attack while training on the Credit Card and MNIST datasets. Using the parameters from the above experiments, Biscotti sampled 70 percent of updates and used a value of 2 for the privacy budget ϵ.

We introduced 30 percent of the peers into the system with the same malicious dataset: for the Credit Card dataset they labeled all defaults as non-defaults, and for MNIST these peers labeled all 1 as 7 s. Figs. 5 and 6 show the test error when compared to federated learning. The results show that for both datasets, the poisoned federated learning deployment struggles to converge. In contrast, Biscotti performs as well as the baseline un-poisoned federated learning deployment on the test set.


Fig. 5.
Federated learning and Biscotti's test error on the CreditCard dataset with 30 percent of poisoners.

Show All


Fig. 6.
Federated learning and Biscotti's test error on the MNIST dataset with 30 percent of poisoners.

Show All

Biscotti requires a larger number of iterations to converge than un-poisoned federated learning. The convergence is slower for Biscotti because a small verification committee of 3 peers was used, which allows the poisoning peers to control a majority in the committee frequently and accept updates from fellow malicious peers. Biscotti finally converges as the honest peers gain enough stake over time, thereby reducing the influence of malicious peers in the system. Over the course of this experiment, where a constant +5 stake update function is used, the stake of the honest clients increases from 70 to 87 percent.

To demonstrate the effect of the committee size on convergence under attack, we re-ran the experiment on the MNIST dataset with a larger verification and aggregation committee size of 26 peers, which we analytically determine is large enough to provide protection against an adversary that controls 30 percent of the stake in the system (shown in Appendix K, available in the online supplemental material). Since peers in the verification or aggregation committees do not contribute updates in that particular round, 100 nodes cannot generate the 70 percent of updates needed to protect against Multi-Krum. Therefore, for this experiment we increased the number of nodes to 200. The results in Fig. 7 show that Biscotti, with a larger verification and aggregation committee size, converges in the same number of iterations as unpoisoned federated learning.

Fig. 7. - 
Federated learning and Biscotti's test error on the MNIST dataset with 200 total nodes, 30 percent poisoners, with a verification and aggregation committee size of 26.
Fig. 7.
Federated learning and Biscotti's test error on the MNIST dataset with 200 total nodes, 30 percent poisoners, with a verification and aggregation committee size of 26.

Show All

6.2 Performance, Scalability and Fault Tolerance
In this section, we evaluate the overhead of each stage in Biscotti and investigate the effect as the number of peers increase. We also measure Biscotti's performance as we scale the size of different committees and compare its performance against federated learning. Finally, we evaluate if client churn has any effect on Biscotti's convergence.

Performance Cost Break-Down. In breaking down the overhead in Biscotti, we deployed Biscotti over a varying number of peers in training an MNIST softmax model. We capture the amount of time spent in each of the major stages of our algorithm in Fig. 1: collecting the noise from the noising clients (steps ② and ③), verification via Multi-Krum and signature collection (steps ④ and ⑤) and secure aggregation of the SGD update (steps ⑥ and ⑦). Fig. 8 shows the average cost per iteration for each stage under a deployment of 40, 60, 80 and 100 nodes over 3 runs. During this experiment, the committee sizes were kept constant to the default values in Table 1.


Fig. 8.
Breakdown of time spent in different mechanisms in Biscotti (Fig. 1) in deployments with varying number of peers.

Show All

The results show that the cost of each stage is almost constant as we vary the number of peers in the system. Biscotti spends most of its time in the aggregation phase since it requires the aggregators to collect secret shares of all the accepted updates and share the aggregated shares with each other to generate a block. The noising phase is the fastest since it only involves a single round trip to each member of the noising committee while the verification stage involves collecting a predefined percentage (70 percent) of updates to run Multi-Krum in a asynchronous manner from all the nodes. The time per iteration also stays fairly constant as the number of nodes in the system increases.

Scaling Up Committee Sizes in Biscotti. We evaluate Biscotti's performance as we change the size of the noiser/verifier/aggregator sets. For this we deploy Biscotti on Azure with the MNIST dataset with a fixed size of 100 peers, and only vary the number of noisers needed for each SGD update, number of verifiers used for each verification committee, and the number of aggregators used in secure aggregation. Each time one of these sizes was changed, the change was performed in isolation; the rest of the committees used a set of 3. Fig. 9 plots the average time taken per iteration over 3 executions of the system.


Fig. 9.
The average amount of time taken per iteration with an increasing number of noisers, verifiers, or aggregators.

Show All

The results show that increasing the size of the noising, verifiers or aggregator sets increases the time per iteration. The iteration time increases slightly with noising committee since additional peers must be contacted to determine the total noise. Increasing the number of aggregators leads to a larger overhead because the secret shares are exchanged between more peers and recovering the aggregate requires coordination among more peers. Lastly, a large number of verifiers results in frequent timeouts in the aggregation stage. Verifiers wait for the first 70 updates and select 37 updates while aggregators wait for shares from the first 35 updates before initiating the aggregate recovery process. With an increased verifier set, the size of the intersection of updates accepted by a majority of verifiers frequently falls below 35, as each verifier runs Multi-Krum on a different set of updates. Hence, the aggregators wait until the update timeout is hit. Since the timeout is a constant value of 90 seconds, the verifier overhead does not increase significantly when increasing the verifier set from 10 to 26. However, our design could decrease this verifier overhead by decreasing the number of updates that aggregators wait for before starting their coordination.

Baseline Performance. We compare Biscotti to the original federated learning baseline [5] with an execution using 5 noisers, 26 verifiers and 26 aggregators. We analytically determine that these committee sizes provide a guarantee of protecting against an adversary that holds 30 percent stake from unmasking updates in the noising (Appendix I, available in the online supplemental material) and aggregation stages (Appendix K, available in the online supplemental material). These committee sizes also ensure convergence under poisoning from an attack by 30 percent of the nodes (Section 6.1).

We divide the MNIST dataset [68] into 200 equal partitions, each of which was shared with an honest peer deployed in one of 20 Azure VMs, with each VM hosting 10 peers. These peers collaborated on training an MNIST softmax classifier, and after 100 iterations both Biscotti and federated learning approached the global optimum. To ensure a fair comparison, the number of updates included in the model in each round are kept the same. In federated learning, we receive updates from 35 percent of the nodes selected at random for every round, while in Biscotti we include the first 35 percent verified updates in the block. The convergence for both systems are shown in Figs. 10 and 11, showing time and the number of iterations respectively. In this deployment, Biscotti takes about 13.8 times longer than Federated Learning (20.8 minutes versus 266.7 minutes), yet achieves similar model performance (92 percent accuracy) after the same 100 iterations.

Fig. 10. - 
Comparing the convergence of Biscotti to a federated learning baseline over time, while training an MNIST model over 200 nodes.
Fig. 10.
Comparing the convergence of Biscotti to a federated learning baseline over time, while training an MNIST model over 200 nodes.

Show All

Fig. 11. - 
Comparing the convergence of Biscotti to a federated learning baseline over the number of iterations, while training an MNIST model over 200 nodes.
Fig. 11.
Comparing the convergence of Biscotti to a federated learning baseline over the number of iterations, while training an MNIST model over 200 nodes.

Show All

We also evaluated the effect on Biscotti of an increased dataset size and model parameters in Appendix L, available in the online supplemental material. For dataset size, we increased the MNIST dataset to 600,000 and 6,000,000 training examples respectively by replicating the data available at each node. Our results shows that, despite increased dataset size, the Biscotti overhead stays at 14x. When increasing the model parameter size, we were able to train models with up to 117,540 parameters before the system failed. Biscotti can support larger models with better tuning of timeouts and a reduced communication overhead, which is discussed in Section 7.

Training With Node Churn. A key feature of Biscotti's P2P design is that it is resilient to node churn (node joins and failures). For example, the failure of any Biscotti node does not block or prevent the system from converging. We evaluate Biscotti's resilience to peer churn by performing a Credit Card deployment with 50 peers, failing a peer at random at a specified rate. For each specified time interval, a peer is chosen randomly and is killed. In the next time interval, a new peer joins the network, maintaining a constant total number of 50 peers. Fig. 12 shows the time to converge for varying churn rates. When a verifier or an aggregator fails, Biscotti defaults to the next iteration after a timeout, so this does not harm convergence, but does introduce variance in execution time. When the churn rate increases to 35 nodes/minute, the system does not converge, likely since 35 is the minimum updates required to create a block in our setup. Even with churn Biscotti makes progress towards the global objective; we found that Biscotti is resilient to churn rates up to 32 nodes per minute (1 node joining and 1 node failing every 1.875 seconds).


Fig. 12.
The impact of churn on model performance. Churn is injected by failing/joining random nodes at a pre-determined rate.

Show All

SECTION 7Limitations and Future Work
Multi-Krum Limitations. For Multi-Krum to be effective, it needs to observe a large number of honest samples in each round. This may not always be possible in a decentralized system with node churn. In addition, Multi-Krum could also reject updates from peers that have non-iid data e.g., a peer that only possesses one class in the model. We did not face this issue in our experiments because we partitioned the data uniformly. However, Biscotti is compatible with other SGD aggregation approaches [49], [50]. For example, in an earlier version of our design we used a version of RONI [69] to validate peer updates.

Deep Learning. Biscotti currently does not scale to large deep learning models with millions of parameters due to the communication overhead. Previous work in federated learning addressed this problem by reducing the model size updates through learning in a restricted parameter space or compressing model updates [70], [71]. There is also extensive research outside of federated learning in training more compact and efficient neural networks with techniques such as weight quantization [72], network pruning [73], knowledge distillation [74], and designs for resource-constrained settings [75]. Another strategy to increase scalability involves improved communication efficiency. This can be achieved through transfer learning [76] on a restricted parameter space or by better partitioning and rearranging of the tensor updates [77]. We leave the application of these techniques to Biscotti as future work.

Leakage From the Aggregate Model. Since no noise is added to the updates present in the ledger, Biscotti is vulnerable to attacks that exploit privacy leakage from the model itself [51], [52], [59]. Apart from differential privacy, these attacks can be mitigated by adding regularization like dropout [13], [51].

Stake Limitations. A client's stake plays a significant role in their chance of being selected as a noiser, verifier, or aggregator. We assume that a large stake-holder will not subvert the system because (1) they accrue more stake by participating, and (2) their stake is tied to a monetary reward at the end of training. However, a malicious client could pose as honest, accrue stake, and finally switch to acting maliciously to subvert our system.

SECTION 8Related Work
Securing ML. AUROR [21] and ANTIDOTE [78] are alternative techniques to Multi-Krum to defend against poisoning. AUROR has been proposed for the model averaging use case and uses k-means clustering on a subset of important features to detect anomalies. ANTIDOTE uses a robust PCA detector to protect against attackers on anomaly-detection models.

Other defenses like TRIM [79] and RONI [69] filter out poisoned data from a centralized training set based on their impact on performance on the dataset. TRIM trains a model to fit a subset of samples selected at random, and identifies a training sample as an outlier if the error when fitting the model to the sample is higher than a threshold. RONI trains a model with and without a data point and rejects it if it degrades the performance of the classifier. Recent work has also demonstrated the performance of robust aggregation schemes, such as the median or trimmed-mean [49] as a defense for federated learning. However, these techniques can be manipulated by adversaries [18], [48].

Finally, Baracaldo et al. [80] employ data provenance as a measure against poisoning attacks by tracking the history of the training data and removing information from anomalous sources.

Poisoning Attacks. Apart from label flipping attacks [44], which are handled by Biscotti, gradient ascent techniques [58], [79] are another popular way of generating poisoned samples one sample at a time by solving a bi-level optimization problem. Backdoor attacks have also been used to produce a classifier that misclassifes a manipulated image with certain pixels or a backdoor pattern [12], [17], [81]. Defending against such training attacks is a difficult and open problem.

Privacy Attacks. Shokri et al. [82] demonstrated a membership inference attack using shadow model training that learns if a victim's data was used to train the model. Follow up work [51] showed that it is quite easy to launch this attack in a black-box setting. In addition, model inversion attacks [59] have been proposed to invert class images from the final trained model. However, these are attacks on class-level privacy, which we do not protect against in Biscotti. These are only effective against a user's privacy if a class represents a significant chunk of a person's data, such as in facial recognition systems.

Privacy-Preserving ML Systems. Although a variety of recent work has proposed the use of blockchain systems for distributed learning [38], [39], [40], [83], [84], [85], [86], Biscotti addresses a unique point in the space of solutions. The existing prior work either relies on a trusted central authority, does not address poisoning attacks, does not handle privacy attacks, or are actually layer-2 blockchain solutions, which are machine learning applications built on top of an existing blockchain systems such as Ethereum [87]. We summarize previous work that combines blockchains with ML in Appendix M, available in the online supplemental material.

Other solutions that do not rely on blockchain use multi-party party computation [88], [89] or trusted execution environments [36] to encrypt the model parameters and the training data when performing multi-party ML. Biscotti does not rely on such techniques to provide privacy.

SECTION 9Conclusion
The emergence of large scale multi-party ML workloads and distributed ledgers for scalable consensus have produced two rapid and powerful trends. Biscotti's design lies at their confluence. To the best of our knowledge, this is the first system to provide privacy-preserving P2P ML through the design of a secure layer-1 distributed blockchain ledger. Unlike prior work, we do not rely on a centralized service, trusted execution environments or specialized hardware to provide defenses against adversaries. In our evaluation we demonstrate that Biscotti can coordinate P2P ML across 200 peers and produces a final model that is similar in utility to federated learning. We also illustrated its ability to withstand poisoning attacks and node churn (one node joining and one node leaving every 1.875 seconds). Our prototype is open source, runs on commodity hardware, and interfaces with PyTorch, a popular framework for machine learning: https://github.com/DistributedML/Biscotti.