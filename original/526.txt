Microservices are widely used for flexible software development. Recently, containers have become the preferred deployment technology for microservices because of fast start-up and low overhead. However, the container layer complicates task scheduling and auto-scaling in clouds. Existing algorithms do not adapt to the two-layer structure composed of virtual machines and containers, and they often ignore streaming workloads. To this end, this article proposes an Elastic Scheduling for Microservices (ESMS) that integrates task scheduling with auto-scaling. ESMS aims to minimize the cost of virtual machines while meeting deadline constraints. Specifically, we define the task scheduling problem of microservices as a cost optimization problem with deadline constraints and propose a statistics-based strategy to determine the configuration of containers under a streaming workload. Then, we propose an urgency-based workflow scheduling algorithm that assigns tasks and determines the type and quantity of instances for scale-up. Finally, we model the mapping of new containers to virtual machines as a variable-sized bin-packing problem and solve it to achieve integrated scaling of the virtual machines and containers. Via simulation-based experiments with well-known workflow applications, the ability of ESMS to improve the success ratio of meeting deadlines and reduce the cost is verified through comparison with existing algorithms.
SECTION 1Introduction
Today, cloud-based software development, such as SaaS (Software as a Service), has become increasingly popular because it improves business agility and lowers software maintenance costs [1]. However, the diversity of business needs and the elasticity in clouds require a software architecture with high scalability and rapid development. Compared with the traditional architecture, microservices shorten development cycles, reduce the inherent complexity, and improve scalability by constructing services with independent lifecycles and reducing the granularity of services [2]. Thus, microservices are popularly used for commercial software such as Netflix [3].

Generally, the applications deployed in clouds need to meet performance requirements, such as the response time, but should also reduce the cost of cloud resources as much as possible. Therefore, task scheduling and auto-scaling are the key problems to address in microservice-based applications in clouds. In the literature, task scheduling and auto-scaling have different methods to effectively utilize the computing resources in clouds. 1) In task scheduling, an application is described as a workflow, and the main topic is scheduling the tasks' execution on multiple instances [4], [5] to optimize the makespan. When instances are insufficient to meet the performance requirements, the algorithm will create new instances using cloud resources to achieve a trade-off between performance and cost [4], [5], [6], [7]. However, the scaling rules are usually simple and cannot meet more sophisticated scaling requirements, e.g., scaling in a two-layer structure consisting of containers and virtual machines (VMs). 2) Auto-scaling establishes rules by threshold or reinforcement learning [8], [9] or builds performance prediction models [10], [11], [12], [13] and determines the amount of resources to meet the performance criterion. Compared with task scheduling, auto-scaling algorithms focus on constructing scaling rules, but they simplify or ignore task scheduling. In fact, different scheduling strategies will lead to different demands for resources, requiring different scaling schemes. There is a difference between the predicted result of a scaling algorithm without combining scheduling and the actual amount of resources for scheduling. The difference may lead to performance degradation because of resource under-provisioning or an unnecessary extra cost because of over-provisioning [1]. Therefore, we are motivated to integrate auto-scaling with task scheduling for microservices in clouds, which can obtain the accurate resource demand for scaling and satisfy the service performance requirements while minimizing the service deployment cost.

Currently, most scheduling algorithms in clouds assume a single workflow execution and make scaling decisions based on the current workflow [4], [5] or a parameter set in advance [14], [15], ignoring the dynamic characteristics of streaming workloads. In a streaming workload, the scheduling algorithm needs to be executed repeatedly to serve continuously submitted workflows, and the algorithm cannot know in advance the characteristics of all workflows, such as the execution time of tasks. As the execution time of tasks will fluctuate at runtime, the optimal scaling decision made by the scheduling algorithm at the current moment, such as the configurations and numbers of new service instances, cannot guarantee global optimality. Therefore, the scheduling algorithm needs to make decisions for long-term optimization to ensure the system performance under a streaming workload.

In a microservices system, containers become the preferred technology to deploy microservice instances due to their fast start-up and low overhead. However, the container layer also brings new challenges to auto-scaling. 1) The cloud provider offers resources as VMs with different configurations, forming a two-layer resource structure with containers. It requires scaling on both the container layer and the VM layer simultaneously, whereas traditional scaling algorithms are only applicable to a single-layer structure (either VMs or containers). 2) Since containers in the same VM share images, the newly added container can be deployed on a VM that hosts the corresponding images, reducing the image pull time and the resulting start-up time of containers. Therefore, the scaling algorithm is required to make full use of existing images to improve the performance of scaling [16], and needs to take into consideration these two factors to scale VMs and then place instances on VMs.

To solve the identified problems, we propose the Elastic Scheduling algorithm for Microservices (ESMS), which integrates task scheduling and auto-scaling in clouds based on the most common on-demand model [4], [6], [16]. First, we define task scheduling for microservices as a cost optimization problem with deadline constraints and use a statistics-based strategy to determine the appropriate container configuration under a streaming workload. Then, a heuristic task scheduling algorithm based on the deadline distribution and urgency is proposed, which determines the scheduling scheme and the type and number of new containers, considering the impact of container images. Furthermore, to realize two-layer scaling of the VMs and containers, the deployment problem of new containers is described as the Variable-Sized Bin Packing Problem (VSBPP). We use a heuristic algorithm to solve the problem and to determine the scaling scheme of VMs and the mapping of containers on the VMs. Finally, we perform simulation-based experiments with real-world workflow applications (e.g., Montage) and Wikipedia access traces. ESMS is demonstrated to be able to reduce the rental cost and obtain the highest success ratio of meeting deadline constraints, compared with existing algorithms (e.g., ProLiS).

In this paper, the main contributions are as follows.

We propose an elastic scheduling algorithm for microservices, integrating task scheduling and auto-scaling. An urgency-based scheduling algorithm is used to obtain the scheduling scheme and accurately determine new instances to be scaled. The scaling scheme is then obtained by a two-layer scaling algorithm.

Aiming to improve the utilization under a streaming workload of microservices, we design a configurations solving algorithm based on statistical information. The resulting configurations is used to create new containers and assign the deadline to improve the accuracy of the deadline distribution.

To solve the two-layer scaling of VMs and containers, we consider the impact of container images on the initialization time of the containers in task scheduling. The auto-scaling problem is constructed as VSBPP to obtain the optimal scaling scheme of VMs and the deployment of containers, minimizing the cost of VMs.

By simulation experiments with real-world workflow applications, the proposed algorithm is verified to outperform three representative task scheduling and auto-scaling algorithms, in the ratio of meeting a deadline and the cost of the VMs.

The remainder of this paper is organized as follows. Section 2 summarizes the related work. Section 3 defines the system model and elastic scheduling problem for microservice-based applications in clouds. Sections 4 and 5 describe the proposed task scheduling and auto-scaling algorithms in detail, respectively. Section 6 presents the experimentation and evaluation. Section 7 concludes the paper.

SECTION 2Related Work
2.1 Task Scheduling in Clouds
As a well-known NP-complete problem, task scheduling has been intensively studied for many years. Different goals lead to different optimization ideas, including the response time [17], cost [18], security [47] and so on. Among the various goals, performance and cost have drawn the most attention. Different from traditional distributed systems, clouds are often assumed to be able to provide infinite resources [4], and a machine with more resources means a higher price. Therefore, the trade-off between performance and cost must be considered. Cloud providers charge users based on the number of time intervals used, and partial utilization of a time interval will be rounded to one full interval [4], [5], [6]. Task scheduling thus needs to avoid the waste of time intervals [14].

Existing scheduling algorithms in clouds can be mainly divided into deadline-constrained workflow scheduling and budget-constrained workflow scheduling.

In deadline-constrained workflow scheduling, the performance requirement is represented as the deadline of workflows, and the goal is to minimize the execution cost of workflows while meeting the deadline constraints. Abrishami et al. [5] defined Partial Critical Paths (PCPs) and proposed the IC-PCP algorithm to minimize the total cost. Wu et al. [4] extended the upward rank in HEFT [17] and defined the probabilistic upward rank. Two algorithms, i.e., ProLiS and LACO, were proposed to solve deadline-constrained workflow scheduling. Wu et al. [6] proposed a four-step algorithm, MSMD, to minimize the cost by reducing the number of VMs and instance hours. Thiago et al. [50] proposed the λ–granularity approach to speed up scheduling of deadline-constrained workflows.

Budget-constrained workflow scheduling minimizes the makespan of a workflow within a user-defined budget. Zheng et al. [18] extended the classic HEFT [17] and proposed the BHEFT, which selects the best possible service via an assessment of the spare budget. Bao et al. [16] modeled the performance of microservices and proposed the GRCP algorithm, which uses the greedy strategy to map tasks to existing or new instances. Qin et al. [49] proposed the remaining cheapest budget to meet budget constraints and guarantee the feasibility of solutions.

However, these studies focus on the scheduling of a single workflow, ignoring the more general streaming workload and multi-workflows [14], [15]. Cloud resources can be shared among several workflows, increasing resource utilization and reducing infrastructure costs [15], [19], [51]. Efforts [7], [20] and [21] studied multiple-workflow scheduling, but they are limited to constrained resource scenarios that are inapplicable to cloud computing. Rimal and Maier [15] proposed the CSWA algorithm for multitenant cloud computing, but they only considered one-time scheduling; that is, multiple workflows are submitted at one time, and the algorithm is executed with all the workflow characteristics known, ignoring general streaming workloads. In a streaming workload, several workflows are submitted at different timestamps, and the scheduling algorithm needs to be executed repeatedly without knowing the information of subsequent workflows. The algorithm should make full use of leftover resources and consider subsequent workloads when allocating resources for new instances. Note that task scheduling in workflows is different from that in stream data processing, where “tasks” are defined as replicas of a certain operator and are responsible for processing data [22]. Therefore, tasks in stream processing are similar to the service instances in microservice systems, and task scheduling corresponds to service instance placement in microservices [22], [23], [24].

Many task scheduling algorithms adopt simple strategies to scale out instances, e.g., creating a cheapest service instance [4], [5] or adopting a rule-based scaling strategy [14], [15]. A new instance will be created directly on the VM. However, in the two-layer structure consisting of containers and VMs, these strategies cannot solve problems such as how to determine the configurations of containers, which type of VMs should be created and which VM new containers should be placed at. Although both [8] and [16] described the two-layer resource structure of microservices, they did not discuss the related scaling problems and only considered the scaling of containers, ignoring the scaling of VMs.

2.2 Auto-Scaling in Clouds
According to the methodology, auto-scaling can be divided into three categories: rule-based methods, time series analysis, and model analysis.

Rule-based methods are widely used in industry [11], such as Amazon EC2 [9]. These methods require some thresholds or scaling rules based on prior knowledge to trigger the scaling actions responding to the dynamic workload. Zheng et al. [8] designed two rule-based autoscalers, which became a part of their SLA-aware microservices framework SmartVM.

Time series analysis uses a set of data points to discover repetitive patterns or to predict future values, such as the request volumes in the future or the periodic trends, typically by machine learning techniques. Kan [25] used a second-order Auto-Regressive Moving Average method to predict request volumes per second after a short period. Fernandez et al. [26] utilized five distinct statistical models to adapt four types of workloads.

In model analysis, mathematical or probabilistic models are constructed to predict performance and make decisions. The prime example is queuing theory. In [12], [13] and [27], queuing theory is used to model the response time of a microservice and the provision resources. In addition, reinforcement learning based on the Markov process, the regression model [16] and Petri Net-based analytical models [46] are also widely used techniques.

However, the above algorithms do not involve a specific scheduling method, which affects system performance. The results of these models may differ from the actual scheduling result and thus cannot accurately reflect the actual requirement for resources. In [15], the authors proposed several scheduling algorithms integrated with a rule-based scaling algorithm [14]. Scheduling and scaling algorithms are executed in turn every time requests are submitted. Evaluations show that the tardiness and average costs of different scheduling policies presented a significant difference, which conveys the necessity of integrating scheduling and auto-scaling.

In addition, these auto-scaling algorithms only consider the structure of single-layer resources and directly deploy service instances on the VM. The following two situations exist: 1) Amazon Auto Scaling [9] and certain algorithms [28] assume that instances are homogeneous and their configurations are set manually in advance. What the autoscaler needs to do is to adjust the number of instances. 2) Other studies, such as [26], consider heterogeneous resources in clouds because cloud providers offer various VMs with different configurations. Users can combine different types and amounts of VMs to generate optimal scaling plans. However, the high overhead and the long start-up time of VMs reduce the flexibility of these scaling algorithms.

Compared with VMs, containers become the choice for service instance deployment due to fast start-up time and low overhead. Recently, the deployment of microservices has mainly been based on containers. Existing container-related studies focus on performance analysis [29] or the case in which containers are used as a substitute for VMs [8], [25], [30], [31]. Existing container-related studies rarely consider the two-layer scaling of VMs and containers [1]. Heenisch et al. [32] analyzed the horizontal and vertical scaling of containers and VMs to offer a four-fold scaling model. However, there is no relevant performance model, and the capacity of an instance is represented by only one parameter, without considering the overall performance of the whole application. Guerrero et al. [33] discussed the deployment of containerized microservices in a multi-cloud environment involving a two-layer structure, but they only studied the initial deployment instead of auto-scaling at runtime. The initial deployment assumes that the configurations and numbers of containers will not change, different from the core problems of auto-scaling, which are whether to scale and how to scale.

Although the two-layer scaling of VMs and containers is similar to VM placement in data centers (DCs), their optimization objectives are different. Apart from performance-related objectives, VM placement in DCs mainly tries to minimize energy-related costs [34], [35], which are related to the energy cost of hosts and cooling systems; whereas scaling in clouds tries to reduce the rental cost related to the price models of cloud providers. The difference between the two cost objectives results in different models and solutions for the two problems. In addition, the characteristics of sharing images among containers are not available in VMs.

SECTION 3System Model and Problem Formulation
Fig. 1 shows a request sequence that contains all the user requests received in a microservice-based system. Since the scheduling algorithm needs to take time to perform a series of steps, the requests are divided into different execution periods, called scheduling periods. The scheduling periods are uniquely identified by a timestamp T. In each scheduling period, several requests can be scheduled, and service instances can be added or removed as needed.


Fig. 1.
System model.

Show All

Generally, a microservice system consists of three layers: the workflow layer, the microservice instance layer, and the VM layer. In the workflow layer, the structure of a microservice application is defined by a workflow. Each request triggers the execution of a workflow instance. Therefore, multiple requests in a scheduling period can be represented by multiple workflow instances, such as WF1 and WFl in Fig. 1. The microservice instance layer contains several microservice instances that process tasks. Each instance is deployed in a container; thus, the microservice instance layer is also called the container layer. The VM layer represents the VM resources occupied by the microservice application in clouds, and the VMs communicate through the provider's network. Compared with the single-layer structure of VMs, the introduction of the container layer enables fine-grained resource management. Lightweight containers make it possible to quickly create or remove service instances [36].

There are two optimization problems to be solved. 1) Scheduling tasks in workflows to microservice instances, satisfying the deadline constraint and reducing the resources used; 2) creating new containers and VMs and mapping new containers to VMs, reducing the total cost of renting VMs in clouds.

Most workflow scheduling algorithms in clouds only consider the first optimization problem and simplify or ignore how containers are created. These algorithms either assume that a service instance occupies the entire VM regardless of the container layer [4], [5], [6], [14], [17], or place several containers on the same VM and do not cap their occupied resources [16] so that the containers interfere with each other. The former completely ignores the container layer, whereas the latter results in unpredictable performance interference. Thus, these methods cannot be applied directly to real scenarios.

Similar to workflow scheduling algorithms, existing auto-scaling algorithms focus on the traditional VMs or simply use containers as the scaling unit [30], [31], [32]. These algorithms rarely consider the two-layer structure of VMs and containers but primarily study the scaling plan in the microservice instances layer, ignoring the second optimization problem. Some simple scheduling strategies are used in these methods, such as FCFS (First Come First Serve), which cannot resolve the dependencies between tasks and the fairness among multi-workflows well, so the first optimization problem is often poorly addressed.

The algorithms proposed in this paper will consider these two optimization problems in an integrated manner, achieving elastic scheduling for microservice-based applications in clouds.

3.1 System Model
Table 1 gives the primary variables and parameters. As shown in Fig. 1, each workflow corresponding to a request is described as a two-tuple WFl=(Vl,El), where Vl={ti} represents tasks in the workflow and El={ei,j} represents the dependencies among tasks. If there is an edge ei, j between ti and tj, ti can be executed only after ti is completed. Task ti is called the predecessor task of tj, and tj is the successor task of ti. A task without a predecessor task or a successor task is called the entry task tentry or the exit task texit, respectively. Two attributes, wi and datai, j, are used to describe the computation load of ti and the data transfer amount from ti to tj, respectively. Note that ti is a unique identifier, so two different workflow instances do not contain task ti at the same time.

TABLE 1 Summary of Variables and Parameters

In the microservice instance layer, each type of microservice is deployed as multiple service instances, e.g., there are two instances ms1, 1 and ms1, j of the first type of microservice. There is no functional overlap between different types of microservices [37]; thus, task t1 can only be processed by microservice instances of the corresponding type, that is, ms1, 1 and ms1, j. In general, each instance msi, j is deployed in a container ci, j, and the amount of resources owned by a container determines its processing speed, represented by si, j. Note that there is a one-to-one match between each instance and each container; thus, they do not distinguish each other in the following. As assumed in most workflow scheduling algorithms [4], [5], [6], [14], [15], [16], a service instance can only execute a single task in every instance time.

In clouds, computing resources are mainly provided as VMs, and microservice instances are deployed in VMs by containers. Cloud providers offer multiple types of VMs with different configurations and prices. Users are charged according to the number of time intervals and the type of VM, and each partial time interval consumed will be billed as a full time interval. For example, the time interval is one hour in Amazon EC2, and 3.4 hours are rounded to 4 hours. The VMs used by the system are represented by the set VM={vmi}. For VM vmi, its price is pricei and its lease duration is durationi. The bandwidth and network delay between two VMs vmi and vmj are described by bi, j and di, j, respectively.

3.2 Case Study
To illustrate the optimization problem due to the streaming workload, we consider a simple case shown in Fig. 2. We call the three scheduling periods T−ΔT,T, and T+ΔT according to the order of their timestamps, where period T−ΔT represents any scheduling period that occurs before timestamp T. Similarly, period T+ΔT is any scheduling period that occurs after timestamp T.

Fig. 2. - 
Microservice instances and tasks in the streaming workload.
Fig. 2.
Microservice instances and tasks in the streaming workload.

Show All

In Fig. 2, each dotted box indicates the duration of an instance running, and each solid line box indicates the duration of a task being executed by an instance. The height of each dotted box represents the processing speed of the instance, and the length of each solid line box is the execution time of the task. ms1,1 and ti represent the instance created in period T−ΔT and the assigned task, respectively. Similarly, ms2,1, tj and tk are the created instance and the assigned tasks in period T, respectively, and tl represents the task to be scheduled in period T+ΔT.

Since the resources in clouds are charged based on time intervals, VMs or containers are not shut down until a time interval is over. In period T, some VMs and service instances used in the previous period continue to run, such as ms1,1 in Fig. 2. Thus, utilizing these remaining resources and minimizing the cost are priorities. Meanwhile, when new service instances need to be created, newly added instances may continue to run until the period T+ΔT, such as ms2,1 in Fig. 2. The amount of resources these new instances have is what the scheduling algorithm must consider at timestamp T.

In scheduling period T in Fig. 2a, tasks tj and tk are waiting to be scheduled. Because instance ms1,1 continues to run, the algorithm will try to assign tj to instance ms1,1, utilizing the existing resource. The algorithm then creates a new instance ms2,1 to process tk because there is no instance of the corresponding type. In traditional scheduling algorithms, the speed of the newly added service instance is the minimum speed that enables the task to meet its sub-deadline, so the speed of ms2,1 is sufficient to just complete tk before its sub-deadline. This strategy ensures the minimum cost under the deadline constraint for a single workflow. However, in the streaming workload, the service instances not only need to process tasks in the current workflows but may also serve subsequent workflows. In the subsequent period T+ΔT, due to the fluctuation in the execution time of the task, unassigned task tl with a larger workload requires a longer execution time, which may cause tl to exceed its sub-deadline.

When task tl is scheduled, it may be found that the speed of the existing instance ms2,1 cannot meet its subdeadline, and a new instance with a higher speed must be created to complete tl in time, as shown in ms2,2 in Fig. 2b. However, this causes resource waste: ms2,1 is not used in the period T+ΔT, and an extra instance, ms2,2, is added, which increases resource consumption and cost.

The waste of instances in Fig. 2b is because the speed of ms2,1 does not meet the requirement of tl. Thus, if reasonable redundancy is added when creating the instance ms2,1, as shown in Fig. 2c, making its speed faster, tl can be assigned to ms2,1 to meet its subdeadline, and it is not necessary to create ms2,2, which improves resource utilization and reduces the cost. Therefore, when creating a new service instance, determining the configuration of the instance and its container is one of the key problems addressed in this paper.

To verify the feasibility of resource redundancy, we take two types of tasks, TmpltBank and Inspiral, in the LIGO workflow as experimental objects. Their execution time data are provided in [39]. We compare two strategies of creating new instances, the cheapest and redundancy strategies, and calculate their costs and the numbers of used instances. The cheapest strategy creates the service instance with the least amount of resources needed to meet the sub-deadline. The redundancy strategy is described in Section 4.1. Other experimental settings are the same as those described in Section 6, including the method of deploying new containers on VMs. The number and sub-deadline of TmpltBank are 240 and 8 seconds, respectively, and those of Inspiral are 480 and 24 seconds, respectively. The results are shown in Table 2. It can be observed that reasonable redundancy can reduce the number of new instances and costs.

TABLE 2 The Comparison of Two Strategies in the Creating of New Instances

3.3 Problem Formulation
When the capability of instances is insufficient, we can add new containers and service instances quickly by 1) creating a new container on an existing VM, such as c2, j, ms2, j in vm3; 2) or renting a new VM and then creating a new container on it, such as c5, j in vmk. Therefore, the elasticity of microservice-based applications involves two optimization problems: workflow scheduling and the integrated scaling of containers and VMs. This is a repeated process that needs to be optimized in every scheduling period. In scheduling period T, we define a task scheduling scheme M and a scaling scheme N:
M={mi,j,k,l|mi,j,k,l=ti∈(ti,WFl,msj,k,ST(ti,msj,k))}WFlandmsj,k∈MS(ti)(1)
View SourceRight-click on figure for MathML and additional features.
N={nj,k,x|nj,k,x=LST(cj,k,(cj,k,vmx,R(cj,k),vmx),LFT(cj,k,vmx))},(2)
View SourceRight-click on figure for MathML and additional features.where mi,j,k,l indicates that the task ti of the workflow WFl is allocated to the instance msj,k, starting from start time ST(ti,msj,k). nj,k,x means that the microservice instance msj,k is deployed in VM vmx, occupying the amount of resources R(cj,k) during the duration from the lease start time LST(cj,k,vmx) to the lease end time LFT(cj,k,vmx). The processing speed sj,k of cj,k depends on R(cj,k).

For scheduling scheme M, we need to determine the mapping between task ti to instance msj, k and its start time ST(ti,msj,k), where msj,k may be a new instance. For the scaling scheme N, we need to determine the mapping between container cj,k and VM vmx, where vmx may be a new VM. Because both msj,k and vmx may be newly created, the configurations of new containers R(cj,k), the types of new VMs (vmx) and the numbers of containers and VMs also need to be determined. Note that the scaling scheme N only affects new containers and VMs created in the current scheduling period. The existing instances and containers remain unchanged.

When task ti is assigned to microservice instance msj,k, its execution time and finish time are defined as:
ET(ti,msj,k)=wisj,k(3)
View Source
FT(ti,msj,k)=ST(ti,msj,k)+ET(ti,msj,k).(4)
View Source

LST(cj,k,vmx) and LFT(cj,k,vmx) can be determined by the start time and the finish time of all tasks on msj, k.:
LST(cj,k,vmx)=minti∈Sche(msj,k){ST(ti,msj,k)}(5)
View Source
LFT(cj,k,vmx)=maxti∈Sche(msj,k){FT(ti,msj,k)},(6)
View Sourcewhere Sche(msj,k) represents all the tasks that are assigned to instance msj,k. The value of ST(ti,msj,k) depends on the earliest start time (EST) of the task ti on the different instances. Because there are several instances that can process ti, the EST has different values on different instances, and ST(ti,msj,k) is equal to the EST of the selected instance. Given the goal of cost minimization, the task is not necessary to be assigned to the instance with the smallest EST, so ST(ti,msj,k) is greater than or equal to the smallest EST:
EST(ti,msj,k,T)=max{Avail(msj,k,T),maxtp∈pred(ti){AFT(tp)+TT(ep,i)}}(7)
View Source
TT(ep,i)=datap,ibx,y+dx,y,(8)
View Sourcewhere TT(ep,i) is the data transfer time from tp to ti. We assume that the instances that can process tasks ti and tp are deployed in containers on vmx and vmy and that the bandwidth and delay between the two VMs are bx,y and dx,y, respectively. AFT(tp) is the actual finish time of the predecessor task tp, and Avail(msj,k,T) is the timestamp when instance msj,k is ready to process the next task:
Avail(msj,k,T)=max{IT(msj,k)+T,maxti∈Sche(msj,k){AFT(ti)}},(9)
View Sourcewhere IT(msj,k,T) is the initialization time of instance msj,k in scheduling period T, which has four values according to the state of containers and VMs:
IT(msj,k)=⎧⎩⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪00IT(cj,k)IT(cj,k)+PT(cj,k)IT(cj,k)+PT(cj,k)+IT(vmx)msj,kisrunningstoring imagesno imagesnewvmxandmsj,k,(10)
View Source

If msj,k is not newly created, it can be used immediately at timestamp T;

If msj,k is newly created and created in a VM with its images, an additional start-up time [36] of the container is required;

If msj,k is newly created and there is no VM with its images, an additional pull time [36] of container images is added;

If both msj,k and vmx are newly created, it is necessary to add a new VM and then create a container on it, so a start-up time of vmx is added [38].

For scheduling scheme M and scaling scheme N, the response time (rtl) of each workflow and the cost of the VMs can be calculated by:
rtl=maxti∈WFl{AFT(ti)}−T(11)
View SourceRight-click on figure for MathML and additional features.
cost=∑vmx∈VMpricex∗⌈durationxinterval⌉,(12)
View Sourcewhere VM represents all leased VMs and pricex and durationx are the price and lease duration of each VM vmx, respectively; interval is the time interval of cloud providers, such as one hour in Amazon EC2.

The response time of each request is constrained by a deadline Dl. The optimization of the elastic scheduling for microservice applications is defined by minimizing the cost of the VMs while meeting the deadline constraints of all requests:
min.costs.t.rtl≤Dl,∀WFl.(13)
View Source

SECTION 4Urgency-Based Workflow Scheduling
Our ESMS consists of three parts: configuration solving, urgency-based workflow scheduling, and auto-scaling, as shown in Fig. 3. ESMS takes the workflows WF received in the scheduling period T as the input, obtaining the scheduling scheme M and the scaling scheme N. First, the algorithm calculates the cost-effective configuration (CE) of the container corresponding to each type of microservice, using the statistical information of the computation workload of tasks, which is completed offline. Then, every time the system receives workflows, the urgency-based workflow scheduling algorithm UWS is performed based on the CE. In the UWS algorithm, the deadline distribution allocates a sub-deadline for each task according to the CE. The scheduling urgency is calculated in the urgency calculation, and tasks are prioritized according to their urgency. Task mapping selects the appropriate service instance for each task in the order of priority. Finally, the set containing all newly created service instances and containers is used as the input of the integrated scaling algorithm IFFD. Here, we focus on the first two parts, and the detailed scaling algorithm is introduced in Section 5. The UWS and IFFD are performed in real-time.


Fig. 3.
Block diagram of ESMS.

Show All

To our knowledge, Netflix Conductor [52] models the application as a workflow and controls the execution order of tasks to be scheduled by the task queue. In addition, custom plugins can adjust the configuration and number of running containers by REST APIs provided by Kubernetes [53], and Kubernetes can combine with the Cloud API to implement the scaling of the VMs. Based on these two tools, the functions of ESMS are compatible with the existing tools.

As shown in Fig. 1, there is a streaming workload rather than a single workflow. Multiple workflows will be divided into the same period T and share service instances [15], [20]. Therefore, the scheduling algorithm is required to guarantee scheduling fairness [20]. What's more, the scheduling algorithm is repeatedly executed in continuous scheduling periods so that different periods are associated. Therefore, the specific questions that need to be addressed are summarized as follows:

How to ensure scheduling fairness.

How to make full use of the leftover instances and reduce the overall cost.

How to determine the configuration of the new instance and its container, considering the workloads of subsequent tasks.

Strategies aiming to answer these questions are added to the UWS, and its pseudocode is given in Algorithm 1. The UWS follows the basic structure of list scheduling: 1) the task ordering phase, calculating the heuristic information and sorting tasks; 2) the task mapping phase, assigning each task to the service instance. It is critical to choose which metric as the priority of the task during list scheduling; thus, we propose the scheduling urgency of the task as a basis for task ordering. For the first question, we adopt the frequently used merging strategy in multi-workflow scheduling [20]. For the second question, we consider the following two aspects in the task mapping: 1) the cost increase caused by assigning tasks to existing instances and 2) according to the effect of container images on existing VMs, some newly created containers will be deployed in advance. For the third question, we propose a statistics-based algorithm to determine the reasonable container configuration of each type of microservice, and the configuration is used as the basis for the deadline distribution and container creation.

4.1 Container Configurations
Since there is a one-to-one match between each service instance and each container, what should be determined is the configuration of the corresponding containers. For tasks of the same type in different workflows, the appropriate resource redundancy should be added to address the fluctuation in the execution time, such as the case shown in Fig. 2.

To this end, we first count the historical data of the computation workload of various tasks and then calculate the cost-efficient configuration of each type of microservice instance based on these data. Assuming there are h types of microservices, we obtain the container configuration scheme CE={cej|j=1,2,…,h}, where cej is a vector indicating the configuration of the service instance of the j-th type of microservice, such as the number of CPU cores. When the application needs to increase the number of service instances of the j-th microservice, the container configured as cej is preferentially created.

By analyzing the execution time of the tasks in several workflows provided by [39], we find that their distribution presents two modes: 1) the variance is small and it is normally distributed; 2) the variance is large and has a similar uniform distribution. For the former, we select the computation workload corresponding to the average execution time or the mean value plus 3σ according to the 3σ criterion. For the latter, the workload corresponding to the maximum execution time is selected. If tasks with a larger workload occur, a special strategy will be executed in the task mapping phase.

Because users do not pay for containers directly but for VMs, to correlate the configuration and price of containers with that of the VMs, we propose the following conversion strategies: 1) container configuration discretization, e.g., the discretizing unit of memory is 500 MB, and then, the amount of memory is a multiple of 500 MB. 2) container price conversion. The container price is determined based on the ratio of the container configuration and the VM configuration. Taking the VM provided by Amazon EC2 as an example, the price of an m4.large VM with 2vCPU and 8 GB is 0.10perhour,andacontainerwith1vCPUand2GBcosts0.05 per hour because it occupies half of the CPU and one-quarter of the memory; the higher ratio is used to determine the cost. The detailed steps for determining the configurations are as follows:

Initialize all cej in CE to the configuration with the least amount of resources. The least amount of each resource is equal to its unit resource after discretization, e.g., the least amount of memory is 500 MB.

Assuming that each instance of the j-th type of microservice (msj,k) is running on a container configured as cej, calculate the expected makespan of the workflow with the configuration scheme CE:
EFT(ti,msj,k,T)=EST(ti,msj,k,T)+ET(ti,msj,k)(14)
View Source
makespanl(CE)=maxti∈WFl{EFT(ti,msj,k,T)},(15)
View Source

where the value of T is zero and the speed of msj,k is the speed corresponding to the container with cej.
If the expected makespan is larger than the deadline, we replace the configuration of the j-th microservice, cej, with a configuration containing more resources, cej’. Usually, the amount of increase in the resource is equal to one discretizing resource unit. The new scheme is CEj=CE−{cej}+{ce′j}. The gain of replacing cej can be calculated as:
gainj=⎧⎩⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪makespanl(CE)−makespanl(CEj)cost(CE)−cost(CEj)makespanl(CE)−makespanl(CEj)cost(CEj)−cost(CE)ifcost(CEj)=cost(CE)ifcost(CEj)<cost(CE)otherwise(16)
View Source
cost(CE)=∑cj,kpricej,k∗⌈durationj,kinterval⌉,(17)
View Source

where durationj,k and pricej,k are the running duration and price of the containers, respectively.
Because there are h types of microservices, we replace the configuration for each type of microservice one by one and obtain several gains, {gainj|j=1,2,…,h}, where gainj corresponds to the gain that replaces the configuration of the j-th microservice. We select the scheme CEj with the largest gainj to replace the original CE, and then, the process returns to step 2. Note that the cost depends on the price and duration of the container. It is possible that the cost remains unchanged or decreases because the price increases but the duration decreases.

If the expected makespan meets the deadline constraint, the process stops, and the current CE is the cost-efficient container configuration scheme.

4.2 Deadline Distribution
In this paper, we adopt the distribution formula in [4] to calculate the sub-deadline of each task:
sdi=Dl×rank(tentry)−rank(ti)+ET(ti,ms∗j)rank(tentry)(18)
View Source
rank(ti)=ET(ti,s∗j)+maxtp∈succ(ti){TT(ei,p)+rank(tp)},(19)
View Source

The sub-deadline sdi of task ti is determined by its execution time and its rank. The meaning of the rank in HEFT [17] is the length of the expected execution time from ti to the exit task, which is similar to the critical path of the workflow. Therefore, it is reasonable to distribute the deadline based on the ratio of the ranks of ti and tentry.

In the original formula [4], the execution time is calculated by the speed of the fastest service instance. However, the goal is to minimize the cost while meeting the deadline constraint. Though the original formula can meet the deadline to execute tasks on the fastest instances, it also increases the unnecessary cost. Furthermore, tasks will be assigned to instances with different speeds; thus, a sub-deadline based on the same speed is inaccurate. Therefore, we choose the speed of the container configured as cej in CE to calculate the execution time. The service instance deployed in such a container is represented by ms∗j. Because the following strategies in the task mapping phase will create containers configured as cej as much as possible, the actual speed of the instances will be close to the speed used in the deadline distribution.

4.3 Task Ordering
To process the multi-workflows, we combine the workflows into a single workflow by adding a task as the predecessor task of all entry tasks and a task as the successor task of all exit tasks (Line 1 in Algorithm 1). The computational workload and the data transfer between the other tasks and the added tasks are zero. The strategy can ensure fairness in multi-workflow scheduling [20].

We calculate the scheduling urgency ui for each ready task in the merged workflow as the basis for task ordering. A ready task is one whose predecessor tasks are completed, and the entry task is always a ready task.
ui=sdi−XFT(ti)hop(ti),(20)
View Sourcewhere hop(ti) represents the number of unassigned tasks on the critical path from ti to the exit task and XFT(ti) is the expected finish time, which is defined as:
XFT(ti)=minmsj,k∈MS(ti){EFT(ti,ms∗j,T)},(21)
View Sourcewhere MS(ti) is a set of instances that can process ti. Because the selected instance is still unknown, the speed of the instance required to calculate the EFT is set to the speed of the instance msj*, which is similar to (18).

The smaller the difference between sdi and XFT(ti), the higher the risk of exceeding the sub-deadline. The larger the number of subsequent tasks to be scheduled, the greater the impact caused by the delay of ti because it directly leads to the delay of subsequent tasks, increasing the makespan of the workflow. Therefore, the smaller the value of ui, the higher the scheduling urgency of ti.

4.4 Task Mapping
According to the urgency, the ready task with the smallest ui is selected to be scheduled. The task mapping needs to consider how to make full use of the existing resources and how to create new service instances and containers:

If there exist instances that can complete the task on time, the instance with the lowest cost increase is selected;

If a new instance is needed, the container with the configuration in CE is created first;

If there is a task with a larger workload or it exceeds its sub-deadline, a new instance is created for meeting the deadline constraint;

If there are images required for creating the new instance in a certain VM, the instance is first deployed to the VM.

The details are as follows (Lines 5-34 in Algorithm 1):

Laxity(ti,msj,k) is calculated for all instances in MS(ti):
Laxity(ti,msj,k)=sdi−EFT(ti,msj,k,T).(22)
View Source

If there are some instances where Laxity(ti,msj,k) is nonnegative, indicating that they can complete the task before its sub-deadline, the cost increase incrCostj,k caused by assigning ti to msj,k is calculated, and the task will be assigned to the instance with the smallest incrCostj,k:
incrCost(ti,msj,k)=cost′−cost,(23)
View SourceRight-click on figure for MathML and additional features.where cost and cost’ are the cost before and after assigning the task, respectively.

If all the Laxity(ti,msj,k) values are negative, indicating that there is no instance that meets the sub-deadline, the minimum speed minSpeed of the new instance required to meet the sub-deadline is calculated, and the strategy is selected based on its value:
minSpeed=wisdi−IT(cj,k),(24)
View Source

If minSpeed is negative or greater than the maximum speed of the available VM, indicating that it is impossible to complete in time, we calculate the EFT of the two plans: a) assigning the task to an existing instance and b) creating an instance with the maximum speed and then assigning the task to it. The plan with a smaller EFT is selected.

If minSpeed is less than the speed of the container configured as cej, an instance and its container with the configuration cej are created, and the task is assigned to it.

If minSpeed is between the speed of the container configured as cej and the maximum speed of the available VM, an instance and its container whose speed is slightly larger than minSpeed are created, in which the speed is usually a multiple of the discretizing unit, and then, the task is assigned.

The above steps are repeated until all tasks are assigned to instances that are either existing instances or new instances.

Algorithm 1 UWS (CE, WF, T)
W ← merge workflows in WF;

newIns ← {};

Calculate rank and sub-deadline for each task in W based on CE;

Calculate the urgency for each task in W;

while there is an unassigned task do

ti ← the ready task with the minimum ui;

for each msj,k that can execute ti do

calculate Laxity(ti,msj,k);

end for

if some Laxity is nonnegative then

msj,k ← the microservice instance with the minimum incrCost and Laxity > 0;

else

if minSpeed < 0 or minSpeed > MAXSPEED then

msj,k1 ← the existing instance with minimum EFT;

msj,k2 ← a new instance with the maximum speed;

if EFT(ti,msj,k1)<EFT(ti,msj,k2) then

msj,k←msj,k1;

else

msj,k←msj,k2;

end if

else if minSpeed is less than the speed of a container with cej;

msj,k ← a new instance on a container with the configuration of cej;

else

msj,k ← a new instance whose speed is just larger than minSpeed;

end if

end if

if there are VMs that contain the image required by msj,k then

vmx ← the VM selected by the Best Fit manner;

Deploy msj,k on vmx in advance;

else

newIns←newIns+{msj,k};

end if

M←M+{mi,j,k,l};

end for

return M, newIns

In terms of time complexity, if there are m workflows and each workflow has n tasks, the total number of tasks is m∗n, and the complexities of computing EST,sdi, and ui are O(m2∗n2). Finding the ready task requires O(m∗n). When there are p instances, the complexity of the task mapping step is O(p∗m2∗n2). To determine the configuration of each container, it is necessary to calculate the expected makespan, whose complexity is equal to that of calculating the EST. If there are l types of configurations after discretization and h types of microservices, the complexity of computing CE is O(l∗h∗m2∗n2). Hence, the total complexity is O((l∗h+p)∗m2∗n2).

SECTION 5Auto-Scaling on Containers and VMs
As discussed in related work, most existing scaling algorithms deploy service instances to VMs directly. As shown in Fig. 4a, the scaling algorithm determines three instances to be added and the type of VMs running the three instances. If these algorithms are directly applied to the two-layer scaling, the number and types of the new containers can be determined by the previous strategies used for VMs, as shown in Fig. 4b. However, the newly created containers need to be further deployed to the VMs, and they are not allowed to occupy all the resources of a VM but share the VM with other containers, such as ms1,1 and ms2,1 deployed in vm1 in Fig. 4c. The scaling algorithm requires that the mapping of containers to VMs is determined, including determining which containers are deployed to the same VM and what type of VM to choose, which is a many-to-many mapping problem. Existing algorithms focus on the one-to-one mapping of the service instance and the VM, without a strategy to solve the abovementioned problems and the second optimization problem described in Section 3.


Fig. 4.
Scaling of service instances.

Show All


Fig. 5.
Workflow applications.

Show All


Fig. 6.
Different workload patterns.

Show All

Similar to the case in Section 3.2, the scaling algorithm needs to fully utilize the existing VMs and solve the second optimization combined with the impact of the container images. Therefore, the problems that need to be solved are as follows:

How to determine the number of resources occupied by the new container;

How to determine the type and number of the new VMs;

How to determine the mapping between the new containers and VMs.

The first problem has been solved by the scheduling algorithm. A two-layer scaling algorithm based on VSBPP is proposed to solve the other two problems.

As shown in (10), images stored in the VM will affect the initialization time of the container. Therefore, ESMS first tries to deploy the new container to the VM with the required images. Specifically, the VMs which store the required images are traversed, and a VM is selected by the best fit (BF) strategy, which selects the VM with the smallest difference between the remaining resources of the VM and the required resources of the container. This strategy is added to UWS because it will affect the actual finish time and expected finish time of tasks (Lines 27-29 in Algorithm 1). Note that the containers deployed in advance will not be deployed by the scaling algorithm.

From the result of the UWS, the new containers that need to be deployed are collected into the set newIns. First, we attempt to deploy newIns to the existing VMs. If the remaining resources of the existing VMs are insufficient, new VMs will be leased to contain newIns. First, all containers of newIns are sorted in ascending order of the amount of resources that they require. Then, the containers are deployed to the existing VMs by the BF strategy. If the resources of the existing VMs are not enough to deploy all new containers, we model the deployment of the remaining containers as a VSBPP.

5Definition 1.
Given container set newIns={msj,k} and set of VM types Type={Typei}, the size of each container is R(msj,k), and the size and price of each VM type is R(Typei)andpricei, respectively. We want to deploy each container to a VM so that the cost of leasing these VMs is minimized and the resources occupied by the containers do not exceed the total resources provided by each VM.

Different from the original VSBPP, the cost of leasing VMs is related not only to the type of VM but also to the time intervals, as shown in (12), and the time intervals vary with the type of VM. To solve the problem, we adopt a heuristic algorithm, IFFD [40], based on the FFD (first fit decreasing), where the containers and VMs are sorted in descending order of the resource amount that the containers require or VMs provide. A feasible solution is described as set B={Bx|Bx={nj,k,x}}, each element in which represents a VM and the containers deployed on it. These elements are arranged in order of the timestamp from when they were created; for example, B1 was created first, and B2 was created after B1.

In extreme cases, the number of new instances is equal to the number of tasks, m∗n. If the number of existing VMs is q and the number of VM types is r, the complexity of the first loop is O(m∗n∗q) and that of the second loop is O(r∗m∗n). The third loop travels through all tasks once, therefore requiring O(m∗n). Therefore, the complexity of IFFD is O((q+r)∗m∗n). Because the number of VMs (q) is less than the number of tasks (m∗n) and the number of VM types (r) is less than that of container types (l), the total complexity of ESMS is O((l∗h+p)∗m2∗n2).

To evaluate the complexity of ESMS, we list the complexity of three comparison algorithms used in Section 6: ProLiS [4], SCS [14], and IC-PCPD2 [5]. Their complexities are O((l+p)∗m2∗n2),O((x+l∗h∗m∗n)∗m∗n) and O((l+p)∗m2∗n2), respectively, where x is the length of the load vector in the SCS. The four algorithms have different complexities depending on the different values of l, p, h, and x, and the difference is not large.

SECTION Algorithm 2IFFD (newIns, Type)
N←∅

for each instance msj,k in newIns do

vmx ← the VM selected by the best fit manner;

N←N+{nj,k,x};

newIns←newIns−{msj,k};

end for

B1 ← Get a feasible solution of newIns by the FFD manner, assuming the type of VMs is Type1;

C(B1)←CalCost(B1);

for each Typei except Type1 do

vmx ← the latest created VM in Bi-1;

B* ← Allocate all msj,k on the vmx to VMs of type Typei by the FFD manner;

Bi←Bi−1−{vmx}+B∗;

C(Bi)←CalCost(Bi);

end for

i∗←argminC(Bi);

for each Bx in Bi* do

By ← reallocate all msj,k on the vmx to the VM that can contain all msj,k and reduce the cost;

N←N+By;

end for

return N;

SECTION 6Evaluation
In this section, we use the workflows provided by [39] to simulate the microservice-based applications. Simulation experiments are conducted to verify the performance of the proposed algorithms.

6.1 Experimental Settings
Based on the simulation tool provided in [4], we built a simulation platform implemented in Java that can read the workflows in the DAX format file and generate the request sequence of the workflows. The platform runs on a Windows 10 64-bit PC with i7 2.6 GHz CPU and 16 GB RAM, JDK 1.8.0.

Four workflow applications are used for simulation experiments: Montage, LIGO, GENOME, and SIPHT. These workflows are widely used in research of workflow scheduling [4], [5], [6], [42], and they cover different types of workflows: Montage is I/O intensive, GENOME is data intensive, and LIGO and SIPHT are CPU intensive; SIPHT is asymmetric, and the others are asymmetric. The details of the workflows are provided in DAX format files [4] that provide the name, computation workload, data transfer amount and dependencies between tasks. The computation workload of the task is defined as its execution time (in seconds) on a standard computing service. We use the workflow applications as the microservice-based application, where each type of task corresponds to a microservice and each microservice can only process one type of task. The execution cost of each task and other detailed information are available in [39] and [48].

In the experiments, we use 8 different types of VMs, as shown in Table 3. Similar to related works [4], [14], [42] in clouds, we use the ECU of the VM to represent its computing capacity and the resource provided by the VM and assume that the discretizing unit is 0.5 ECU. In addition, we set the boot-up time of the VM and container according to related works [36] and [38].

TABLE 3 Configurations and Prices of Virtual Machines

We choose two workflow scheduling algorithms in clouds, ProLiS [4] and IC-PCPD2 [5], and an auto-scaling algorithm in clouds, SCS [14], as the comparison algorithms. These algorithms also focus on cost optimization with deadline constraints. ProLiS is based on list scheduling, where each task is assigned to the cheapest VM meeting its sub-deadline. Since IC-PCP makes multiple tasks execute on the same instance, which is incompatible with the single function characteristic of microservices, we use IC-PCPD2 as the comparison algorithm. IC-PCPD2 distributes the deadline over individual tasks by PCPs and then assigns them to the VM that meets the sub-deadline. The SCS determines the number of VMs that need to be added by the load vector and then schedules tasks by the earliest deadline first (EDF) algorithm. Since the algorithms ignore the container layer, we modify them to adapt to the two-layer structure. The original algorithms determine the mapping between tasks and service instances (i.e., containers) and solve the integrated scaling of containers and VMs by the best fit with best-size (BFB) strategy. The BFB strategy deploys containers to existing VMs by the BF strategy. If the resources of existing VMs are insufficient, a VM whose resource amount is closest to the amount that the new container requires is leased.

To simulate the streaming workload, we randomly sample from Wikipedia access traces [43] to form three request sequences with workload patterns of stable, decreasing and increasing. Each request in the three sequences will create a workflow instance with a deadline constraint. To set a reasonable deadline, we calculate the average length of the critical path for each workflow and set a deadline factor to control the strictness of the deadline constraint, which ranges from [1.0, 2.0].
Dl=factor∗cpLength.(25)
View Source

6.2 Metrics
Although the objective is to minimize the cost, it is possible that some workflows cannot meet the deadline constraint. If there is no feasible solution that meets the constraint, it is meaningless to minimize the cost. Therefore, we set the following metrics:

The success ratio is defined as the ratio of the number of workflows that meet the deadline and the total number of scheduled workflows:
ratio=∑WFl∈WFsuccl|WF|succl={10rtl≤Dlrtl>Dl,(26)
View Source

Furthermore, scheduling and scaling schemes whose success ratio is 100 percent are called feasible solutions. The number of feasible solutions obtained is also used as a metric.

Cost is the cost of the VMs leased and calculated by (12).

Based on the two metrics above, the evaluation rules are as follows:

If the success ratio of the first scheme is higher than that of the second scheme, the first scheme is better;

If the success ratios of the two schemes are 100 percent, the scheme with the lower cost is better.

6.3 Results
We conduct several groups of experiments to test the performance of each algorithm with the different workflows, different workload patterns, and different workloads.

6.3.1 Different Workflow Applications
We choose a stable workload pattern and execute the algorithms with different workflow applications. The size of the workflow, i.e., the number of tasks in each workflow, is set to approximately 50 (the size of SIPHT is approximately 30 because of its unique structure). The success ratio and the cost under different values of the factor are shown in Figs. 7 and 8, respectively, and the number of feasible solutions is shown in Table 4.


Fig. 7.
Success ratio with different workflows and stable workloads.

Show All


Fig. 8.
Cost with different workflows and the stable workloads.

Show All

TABLE 4 Number of Feasible Solutions With Different Workflows

We observe that as the factor increases, the success ratios gradually increase, except for that of the SCS. As the result shows, the performance of ESMS is superior to the performance of the others. It can be found from the number of data points in Fig. 8 and the data in Table 4 that ESMS can obtain more feasible solutions than the others. Especially in the LIGO experiment, the success ratios due to ESMS are 18.31, 36.39 and 6.02 percent higher than those of ProLiS, SCS and IC-PCPD2, respectively. The success ratio of SCS fluctuates greatly, and there is no feasible solution. SCS is different from the other three algorithms. SCS first estimates the required number of instances by the load vector before the EDF scheduling algorithm. However, there is a slight difference between the predicted result of the load vector and the demand of the EDF scheduling algorithm, which results in some tasks being forced to queue. The resulting delay gradually accumulates during the execution of the workflows, eventually causing the workflow to time out. In addition, as factor increases, the cost-efficient type of the VMs changes, and their configuration also gradually decreases. Therefore, the success ratio does not increase even if the deadline factor is relaxed. The other three algorithms are based on task scheduling and can determine the resource demand by a scheduling scheme, so their success ratios are better. The result also demonstrates the effectiveness of integrating task scheduling and auto-scaling.

The performance of IC-PCPD2 is also unstable: it can obtain a high success ratio, low cost and number of feasible solutions close to ESMS in GENOME, but it cannot obtain any feasible solution in Montage. For SIPHT, Fig. 9a shows that the average success ratio of IC-PCPD2 is close to that of ESMS, but IC-PCPD2 has only three feasible solutions, as shown in Fig. 8d and Table 4. This is because most success ratios of IC-PCPD2 are higher than 99 percent but less than 100 percent. Although the difference in the success ratio in the other three workflows is small, the cost of ESMS is lowest.


Fig. 9.
The average success ratio and the average normalized cost with different workflows.

Show All

In terms of cost, we collect all feasible solutions from the experiments with different values of factor, normalize their cost and calculate the average cost, as shown in Fig. 9b. The cost due to ESMS is significantly lower than that of ProLiS, reducing the cost by 79.08, 6.80, 15.37, and 18.29 percent. Except that IC-PCPD2 has no feasible solutions in Montage, the cost of ESMS is 4.57, 0.58 and 11.39 percent lower than IC-PCPD2 in other three workflows.

In Fig. 8a, the cost of ProLiS is always high and does not decrease as the factor increases. This is caused by the unreasonable deadline distribution. In Montage, there is more data transfer between mAdd and mShrink, and ProLiS uses the fastest speed to distribute the deadline, resulting in the sub-deadline of mAdd being too small. As a result, all tasks executed before task mAdd are forced to be completed in a short time, requiring more instances and a higher speed, causing a higher cost. For example, in a certain workflow instance, the sub-deadline of mAdd is 10.5909, and the resulting makespan of the workflow is 53.7460, which is much smaller than the deadline of 71.9009. The ESMS distributes the deadline using the speed corresponding to CE, making the sub-deadline of mAdd 33.8911 and the makespan 69.6119, which is much closer to the deadline, reducing the unnecessary cost. In the other three workflows, the ESMS presents the lowest cost because reasonable redundancy is added to instances of each type, reducing the resource waste, as discussed in the case in Fig. 2 and Table 2.

ESMS has a higher success ratio due to the following two strategies in UWS: 1) the strategy of creating a new instance (Lines 13-20 in Algorithm 1) and 2) considering the impact of the container images (Lines 27-29 in Algorithm 1). In the former, when the task exceeds the sub-deadline, ESMS tries to complete it as early as possible, reducing the impact of the timeout task on the following tasks; although creating a new instance will increase the cost. In fact, even if a task exceeds its sub-deadline, the subsequent scheduling may make the workflow finish on time, which is also the main reason that IC-PCPD2 cannot obtain the feasible solution in Montage. When neither existing instances nor new instances can meet the sub-deadline, there is no corresponding strategy in IC-PCPD2, causing the success ratio to be less than 100 percent in many cases. In the latter, as shown in (10), whether the VM contains the images that the container requires will affect the initialization time of the container and then affect the finish time of the task. If a new instance is created, it should be deployed on the VM storing the required images, reducing the initialization time and the makespan of the workflow. Thus, ESMS achieves a higher success ratio.

According to Table 5, our ESMS uses the fewest machines while maintaining the lowest cost. Fewer machines can also reduce the complexity of operation and maintenance. In Montage, ProLiS uses the most machines. As analyzed before, its unreasonable deadline distribution leads to more and faster instances, which occupy more VMs. Certainly, the cost depends not only on the number of VMs but also on the configuration of the VMs. However, fewer VMs are better when the costs are close.

TABLE 5 The Number of VMs Used With Different Workflows

6.3.2 Different Workload Patterns
To evaluate the performance of the algorithms with different workload patterns, we access LIGO and GENOME (called L and G for short) with increasing and decreasing (called I and D for short) workloads. The two workflow applications can represent the two change patterns of the cost examined in Section 6.3.1. The size of the workflow is still set to 50. The results are shown in Fig. 10 and Table 6. In Table 6, the notation “(L, D)” means LIGO application with a decreasing workload.


Fig. 10.
Average success ratio and average normalized cost with different workload patterns.

Show All

TABLE 6 Number of Feasible Solutions With Different Workload Patterns

Combined with Figs. 7b, 7c, 8b and 8c, we can determine that the experiments with different workload patterns show similar results:

The success ratio of SCS is still random, and the number of feasible solutions is 0. Thus, there is no cost bar in Fig. 10b.

In the three experiments in LIGO, ESMS maintains the lowest cost and the highest success ratio.

In the three experiments in GENOME, ESMS outperforms ProLiS and SCS, and the performance of IC-PCPD2 is close to that of ESMS.

As Tables 4 and 6 show, ESMS can obtain more feasible solutions than the other algorithms.

To display the improvement of ESMS against the comparison algorithms, we list the improvement of the success ratio and cost with four workflow applications (called M, L, G and S for short) and three workload patterns (called S, D and I for short), including 12 groups, in Table 7. As seen from Table 7, in terms of the success ratio, the ESMS algorithm increases by 0.16-1.30 percent in general compared with ProLiS; in particular, the improvement is up to 18.31 percent in LIGO. With respect to the cost, the improvement is between 6.80 and 22.66 percent, and it reaches 85.84 percent in Montage. Compared with SCS, the increase in the success ratio is usually between 27.50 and 67.62 percent, except for the two special cases of 82.01 and 99.8 percent. In terms of the cost, SCS cannot be compared because it cannot obtain a feasible solution. As discussed above, IC-PCPD2 can obtain a performance close to ESMS in GENOME, but it cannot obtain a feasible solution in some cases, such as the three groups of Montage and Group 11. The performance improvement of ESMS in cost is 0.58-14.64 percent compared with IC-PCPD2, and the success ratio of ESMS is 0.00-7.40 percent higher than IC-PCPD2 in general, and in Montage, it can reach 72.97 percent.

TABLE 7 Improvement Percentages of ESMS Over Other Algorithms

To compare the performance of our strategy IFFD and the baseline BFB, the task scheduling algorithm is fixed as UWS and is combined with IFFD and BFB. Note that the “UWS+IFFD” in Table 8 is our ESMS. Because the task scheduling algorithm is the same and the impact of the container images is considered in UWS, the success ratios of the two algorithms in Table 8 are equal; thus, we only compare their costs. The results are listed in Table 8. Generally, our IFFD outperforms BFB in Montage and LIGO. Unlike simple bin packing problems, the VMs in clouds may be removed or their rental duration may be prolonged depending on scheduling schemes in subsequent scheduling. Therefore, the improvement of IFFD over BFB is not as great as that in the original bin packing problem.

TABLE 8 Costs of IFFD and BFB and the Improvement Percentages of IFFD Over BFB

Using the same scaling strategy (BFB), UWS+BFB obtain a higher success ratio than ProLiS and IC-PCPD2. This indicates that our task scheduling algorithm UWS can improve the success ratio.

6.3.3 Different Workloads
Different from the workload pattern, the workload means the request number in each scheduling period or the number of tasks in a workflow. To determine the impact of the workload on the performance of the algorithms, we select the second workload pattern and adjust the workload in the following ways: 1) given that the size of the workflow is 50, the number of requests increases from 131 to 2100; 2) given that the number of requests is 1051, the size of the workflow increases from 50 to 600.

The normalized success ratio and the cost of the four algorithms are shown in Fig. 11. Similar results with different sizes of workflows are shown in Fig. 12. Under each value of the number of requests and the size of the workflows, there are 11 groups of experiments with different factors, from 1.0 to 2.0. The eleven results are shown in a box or outliers in the boxplot.

Fig. 11. - 
Performance comparison with varying numbers of requests.
Fig. 11.
Performance comparison with varying numbers of requests.

Show All

The boxplot in Fig. 11a reflects that the success ratio of the solutions obtained by ESMS cluster around 100 percent, whereas those of ProLiS and IC-PCPD2 are more dispersed. In addition, the outliers in Fig. 11 reflect the lower success ratio of the algorithms when factor is small. Therefore, it can be found from these outliers that ESMS has a significantly higher success ratio than the others when the deadline constraint is strict. Fig. 11b also indicates that the cost of the solutions obtained by ESMS is the lowest. Except for the experiment with 262 requests, the cost of ProLiS is lower than that of IC-PCPD2. SCS can obtain a feasible solution when the number of requests is 525.

The results shown in Fig. 12 are similar to those of Fig. 11. Note that there is no feasible solution obtained by ProLiS when the workflow size is 200 and 400, and IC-PCPD2 has only one feasible solution when the size is 50.

Under the most extreme workload, i.e., there are 600 tasks in each workflow and the factor is 1, the success ratios of the four algorithms are 0.00, 0.00, 0.38, and 20.99 percent.

6.3.4 Runtime
To evaluate the overhead of algorithms, we test their runtime with the same workload pattern, the same size of the workflow and different numbers of requests, as shown in Fig. 13. Given the unstable performance of IC-PCPD2, we only compare ProLiS and ESMS.

Fig. 12. - 
Performance comparison with varying sizes of workflows.
Fig. 12.
Performance comparison with varying sizes of workflows.

Show All

Fig. 13. - 
Runtime of different algorithms with varying numbers of requests.
Fig. 13.
Runtime of different algorithms with varying numbers of requests.

Show All

When the number of requests is small (less than 525), the runtime is short, and the difference is small. As the number increases, the runtime increases, and each algorithm has its own superiority. When the number is 1051, the runtime of ProLiS is the shortest. When the number is 1573, the median runtimes are basically identical, and ProLiS and ESMS are similar. As the number further increases, the runtime of ProLiS is the longest and distributed, whereas that of the ESMS is shorter. In general, these algorithms have little difference in complexity.

SECTION 7Conclusion
In this paper, we proposed and developed an elastic scheduling algorithm, ESMS, for microservices in clouds, tackling task scheduling and auto-scaling in an integrated manner. The algorithm determines the container configuration based on the statistics of the workloads and is adaptive to the streaming workload. A VSBPP-based scaling algorithm is proposed to solve the two-layer scaling problem caused by the introduction of the container layer, and the impact of container images is considered. By simulation experiments using workflow applications, the ability of the proposed algorithm to improve the success ratio and cost for microservices in clouds is validated.

In the future, we plan to study how to construct a generalized performance model for a microservice-based system to accurately predict the response time of each microservice with different configurations. Second, this paper focuses on horizontal scaling in clouds. The combination of horizontal scaling and vertical scaling will be further studied. Third, considering the diversity of cloud resources, other billing models, such as Spot, will be one of our research directions.