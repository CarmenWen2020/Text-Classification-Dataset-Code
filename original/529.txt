With the widespread deployment of various cloud computing and service-oriented systems, there is a rapidly increasing demand for collaborative quality-of-service (QoS) prediction. Existing QoS prediction methods have made great progress in modeling users and services as well as exploiting contexts of service invocations. However, they ignore the completion of service requests/responses relies on the underlying network topology and the complex interactions between Autonomous Systems. To tackle this challenge, we propose a topology-aware neural (TAN) model for collaborative QoS prediction. In the TAN model, the features of users, services, and intermediate nodes on the communication path are projected to a shared latent space as input features. To jointly characterize the invocation process, the path features and end-cross features are captured respectively through an explicit path modeling layer and an implicit cross-modeling layer. After that, a gating layer fuses and transmits these features to the prediction layer for estimating unknown QoS values. In this way, TAN provides a flexible framework that can comprehensively capture the invocation context for making accurate QoS prediction. Experimental results on two real-world datasets demonstrate that TAN significantly outperforms state-of-the-art methods on the tasks of response time, throughput, and reliability prediction. Also, TAN shows better extensibility of using auxiliary information.

SECTION 1Introduction
Promoted by the popularity and wide applications of cloud computing, many app vendors, e.g., Google, Spotify, etc., and developers have published various cloud services online so that they can be called over the Internet. The number of cloud services has been increasing rapidly over the past two decades. Huge amounts of virtualized and standardized resources, software, and data are provided as cloud services. Users can access these cloud services via their APIs over the Internet from anywhere and at any time. Cloud applications can integrate these cloud services as external system components to implement new functionalities or offer new features [1].

Such cloud applications raise a series of challenging research problems. One of these problems is how to ensure their end-to-end quality of service (QoS). QoS can be described as a set of non-functional attributes (such as response time, throughput, reliability, availability, cost, and so on) that may impact the overall quality offered by a service [2]. QoS is not only a technical concern but also a business concern. From an application vendor's perspective, it plays an important role in gaining a competitive advantage in the cloud market and industrial IoT systems [3]. For example, Amazon estimates that every 100ms increase in latency cuts profits by 1% [4]. From the user's perspective, QoS is also an important differentiating point of different services. The system performance of a cloud application relies on the quality of its internal and external system components [5]. Compared with standalone applications, it is particularly challenging to ensure the performance of a cloud application when calling remote cloud services as its external system components because these cloud services are not managed in-house like internal system components [6]. An external system component with poor quality will undermine the system performance of the cloud application and in the worst-case scenarios, may even halt or crash the cloud application [7]. Thus, choosing and integrating high-quality cloud services is key to ensuring the system performance of a cloud application [8].

However, it is because of the huge scale of online cloud services that it is difficult or even impossible for developers to inspect, test, and evaluate all the candidate cloud services when they are looking to implement new functionality for their cloud applications. ProgrammableWeb,1 a cloud service portal, has already listed 23,000+ cloud services and 480+ categories. Thus, a quality-of-service evaluation approach is needed to help developers choose appropriate cloud services.

In recent years, collaborative QoS prediction has been intensively studied. It aims to predict the missing values based on the users’ historical experience of cloud services.[9]. Both memory-based and model-based QoS prediction approaches [10], [11], in combination with various spatial and temporal contexts of service invocations, have been explored and proposed [12], [13], [14]. The general idea is that the QoS of a specific user to a service can be predicted based on the experience of similar users or services[13]. Great progress has been made in modeling users and services as well as exploring contextual information of service invocations. However, existing QoS prediction approaches have not properly considered the Internet topology that often impacts a user's quality of a cloud service experience.

To our best knowledge, some QoS properties, such as response time, throughput, availability and reliability, are closely related to the Internet topology. They may vary significantly from user to user because of dynamic network conditions. When users in different regions request the same service, their service requests and corresponding responses may go through different paths over the Internet, then often results in differences in their QoS experiences. The Spearman correlation coefficient between the end-to-end response-times and the length of corresponding shortest paths, is about 0.3009, which is estimated based on the dataset of WS-DREAM as described in Section 5.1, proved this conclusion from a certain angle. Besides, users from the same region are likely to experience similar service availability and it is also correlated with network topology. For example, if the link between a user u and a service is congested and the service becomes unavailable, the users from the same region as u are not likely to be able to access the service. Fig. 1 intuitively depicts the roles of paths in service invocations. Each node represents an autonomous system, a user, or a service, and each edge represents its connection over the Internet. Both users’ requests and services’ responses go through such communication paths-starting from the source node and passing through a sequence of intermediate autonomous system nodes to the target node. Features of the paths (such as workload, delays, etc.) directly impact the message communication, and thus impact users’ QoS experiences. So it is particularly necessary to integrate the communication path with a prediction model to reduce the errors caused by users’ differentiated service invocation paths.


Fig. 1.
The topology of Internet meshed autonomous systems. What the arrow connected represents an invocation from a user to a service.

Show All

The consideration of network topology complicates QoS prediction significantly and calls for prediction techniques - beyond collaborative filtering - that can properly leverage the information hidden in sophisticated user-service invocation processes [14]. In recent years, neural networks, in particular deep neural networks, have been successfully applied to many tasks, such as rating prediction, click-through-rate prediction, and so on [15]. Neural network models can effectively extract features through multi-level representations because multiple hidden layers stacked amplify input features that have a significant impact on prediction results and suppress irrelevant features [16]. Besides, with a nonlinear modeling ability, a neural network model can approximate any complex function and construct highly-accurate prediction models for complex tasks [17]. In very recent years, there have been a few exploratory studies of collaborative QoS prediction based on neural networks [13], [18], [19]. The approaches proposed in these studies have demonstrated the superior ability of neural networks to model user-service interactions. However, these approaches have simply followed the basic idea of conventional QoS prediction approaches to model only the features of users and services, but not the path features discussed above. Cloud applications are often built to serve users from around the globe. Thus, the assurance of their system performance requires accurate QoS prediction for users in different regions.

To tackle the above challenge, this paper proposes a topology-aware neural (TAN) model for highly-accurate QoS prediction. In a service invocation, TAN takes not only the impacts of users and services but also communication paths between them into full consideration. It is worth noting that such communication paths are determined by the routing algorithms, which refers to the process of finding paths over a network that have a minimum distance or other cost metrics. Thus, at the heart of the process of service invocation is a mechanism to find the shortest paths. To achieve this, TAN approximates the shortest paths by using Dijkstra's algorithm [20]. Then, TAN obtains the sequence of nodes in the path, both end-nodes inclusive, to form the input features. In the path modeling layer, TAN employs bi-directional Long Short-Term Memory to capture the forward and backward dependence of a path and merge the features of the nodes on the path into a single vector representation for explicit modeling service invocations. And in the end-cross modeling layer, the complex interaction between the user and the service is captured through a convolution neural network for implicit modeling. After that, the gating mechanism [21] is utilized to fuse the path and end-cross features. Finally, QoS prediction is made by stacking the fully connected layers over the gating layer. With historical QoS data of service invocations, TAN can be trained to make QoS prediction for a given active user and a target service based on the network topology.

The main contributions of this paper include:

To the best of our knowledge, it is the first attempt to take into full consideration the network topology for QoS prediction.

We propose a specifically-designed neural network model named TAN for highly-accurate topology-aware QoS prediction. We also show TAN's potential in using side information.

In particular, TAN combines the implicit end-cross features and explicit path features with the gating mechanism innovatively to characterize service invocations.

Large-scale experiments are conducted on a widely-used real-world dataset to demonstrate its superior performance over state-of-the-art approaches. https://github.com/whale-ynu/TAN.

The remainder of the paper is arranged as follows. Section 2 reviews the related works. Section 3 discusses the principles and design of TAN. Section 4 illustrates the parameter optimization for TAN. Experimental results are presented and discussed in Section 5. Finally, Section 6 concludes this paper.

SECTION 2Related Work
We introduce the three main types of collaborative filtering methods for QoS prediction, including memory-based, model-based, and context-aware [22].

2.1 Memory-Based Methods
Memory-based methods predict QoS based on the similarity between users or services and can be categorized as user-based methods, item-based methods, and their combination. Shao et al. first propose a user-based QoS prediction approach [10]. After that, a series of following works focus on improving the accuracy of similarity computation. Chen et al. calculate the QoS values of different scales and use cosine similarity to measure the similarity between services [23]. Ma et al. consider the fluctuation of similarity with the growth of invocations and compute it based on the users’ original similarity [24]. In [25], a JacMinMax model is designed to avoid inaccurate similarity by setting a ratio of difference. From the perspective of similarity weight, Tang et al. design two presetting thresholds for adjusting similarity [26]. Besides, Zheng et al. provide a personalized framework for QoS ranking prediction based on users’ usage experience [27]. The advantages of memory-based methods is simplicity and interpretability. However, the selection of similarity measurements obviously affects the accuracy of QoS prediction [14]. More importantly, the performance can be cut dramatically when the data is sparse because it is hard for them to estimate similarities and capture potential patterns from data.

2.2 Model-Based Methods
Model-based methods which include matrix factorization, factorization machine, and neural network-based models have demonstrated their ability to achieve high accuracy when faced with the problem of data sparsity. Matrix factorization approaches refer to a group of algorithms where a user-item matrix is factorized into a product of two latent feature matrices. Luo et al. design a non-negative latent factor model [28]. Also, they propose an efficient missing QoS predictor on the principle of alternating direction method-based matrix factorization which enables fast model training and good representations of users/services [29]. Zhu et al. propose an adaptive matrix factorization method using online learning and an adaptive weight technique to make real-time QoS prediction [30]. He et al. develop a hierarchical matrix factorization model, which performs factorization on clustered local user-service matrices [31]. Factorization Machine (FM) is a machine-learning algorithm to incorporate second-order feature interactions [32]. On top of that, Xiao et al. design an attentional FM that learns the importance of each feature interaction [33]. He et al. propose a neural FM to adopt non-linear interactions instead of linear interactions in FM [34]. Wu et al. introduce user ID and service ID to construct embedding vectors and apply that to FM to predict QoS values [35]. Tang et al. incorporate the locations of services into classic FM [36]. Yang et al. propose a model considering both user and service locations in FM [37].

Factorization-based methods can learn implicit features from data, to overcome the defects of memory-based methods. However, this kind of method only considers the low-order and linear feature interaction, the high-order and nonlinear features entailed in the data are ignored, which limits the effect of representation learning. For this, some researchers have employed neural networks to make QoS predictions to introduce high-order and nonlinear modeling ability. Wu et al. propose a universal deep neural model to make QoS prediction of multi-attributes with contextual features [13]. Zhou et al. propose two universal Spatio-temporal context-aware collaborative neural models to make QoS prediction, considering invocation time and multiple spatial features on both service side and user side [14]. Apart from that, Wang et al. introduce a dynamic Bayesian network to make QoS predictions on updated QoS value time series [38]. Then, they propose utilizing long short-term memory (LSTM) to make QoS prediction [39]. Neural models have a stronger ability of modeling and feature selection, but they introduce more parameters and calculation requirements. Fortunately, the full utilization of context information associated with service invocations can help to alleviate this contradiction.

2.3 Context-Based Methods
There is much contextual information that can be leveraged to improve the accuracy of QoS prediction, such as location, time, and so on. Tang et al. take into account users’ longitudes and latitudes when identifying their neighbors [40]. Besides, Hu et al. incorporate users’ locations into Bayesian inference [41]. Yu et al. cluster users and services based on their locations [42]. Chen et al. further extend the factorization machine approach to leverage location information [43]. In [44], a strategy of two-level neighborhood selection is proposed by identifying the distance among users. Lee et al. incorporate users and services’ locations into a group and employ preference propagation to compute their similarity [45]. Wu et al. first adopt a density-peak algorithm for identifying hidden user clusters and then implement a prediction model by incorporating the achieved clusters [46]. Tang et al. propose integrating matrix factorization with the network map, where network distances among users are measured and neighborhoods of users are identified to enhance the prediction model [47].

As the QoS values can also fluctuate with invocation time, it is necessary to incorporate the temporal factor into QoS prediction, Yu et al. use the similarity between services in a specific time interval to approximate their average similarity [48]. Hu et al. believe that the QoS values are volatile over time and propose an auto-regressive integrated moving average (ARIMA) model to handle volatile QoS [49]. Zhu et al. represent time as a latent factor in a specific context and propose a context-aware model [50]. Luo et al. propose a latent factorization of tensors-based QoS predictor that can precisely represent the temporal information hidden in dynamic QoS data [51].

Besides, other contextual factors have also been considered in QoS prediction. Chen et al. argue that the physical environment of services is related to their quality. They cluster services based on this assumption [52]. Xu et al. think that the difference in the companies affect the QoS divergence and incorporate company affiliations into QoS prediction [53]. Wu et al. also propose a matrix factorization model relying on the configuration of hosts, service status, and internet conditions as well as users and services [12].

Although various kinds of contextual information have been employed and it is verified to be effective. However, there is still a lack of in-depth modeling of network topology, especially communication path. And how to develop neural prediction models to make full use of this key feature needs to be further explored.

SECTION 3Topology-Aware Neural Model for Highly Accurate QoS Prediction
The workflow of TAN is illustrated in Fig. 2. Given the network topology between a user and a service, the routing algorithm will select an optimal path comprised of a sequence of intermediate nodes, which is used for message communication between them. The features of the user and the service, as well as the intermediate nodes on the path, are represented by categorical variables as input features. However, using a bag of node features cannot sufficiently represent the interdependence between path nodes. Therefore, we set an explicit path modeling layer to capture the forward and backward interdependence between path nodes and to encode the entire path features into a single vector. Also, an implicit end-cross modeling layer is introduced to extract end-cross features between users and services to characterize the process of service invocation from the aspect of end-to-end interaction. The followings are a gating layer and a prediction layer to fuse features and achieve QoS prediction. Table 1 summarizes the symbols and notations used in this paper.

TABLE 1 Symbols and Notations


Fig. 2.
The basic architecture of the TAN model. It contains five major components: Input layer, implicit end-cross modeling layer, explicit path modeling layer, gating layer and prediction layer.

Show All

3.1 Input Layer
A service invocation refers to the process of interaction between users and services in the form of requests and responses. The entire process is completed by the message communication based on the computer network topology-a structure by which the autonomous systems (AS) are attached. In other words, the delivery of service requests and responses over the network depends on the complex interactions between thousands of autonomous systems that exchange routing information using a routing protocol. The routing protocol decides how a message gets from its source to destination and supplies the optimal path with the least cost, which usually refers to the shortest distance or the lowest load. In this paper, we only consider topology between ASs for simplicity, and in view of the information lack of link costs and network loads, the shortest distance (a.k.a the least routing-hot cost) is adopted to approximate the optimal paths when the network is free from congestion.

Let us assume that a service invocation occurs between user u0 to service s0 (as showed in Fig. 1). The Dijkstra's algorithm is utilized to select a communication path with the shortest distance. We can obtain the communication process as follows. First, the network is partitioned into many autonomous systems. When a service request is sent from the IP address of u0 to the IP address of s0, the source must know the AS number AS1 of u0 and the AS number AS4 of s0. Then, u0 will forward data packets to the specific border router R1 of its autonomous system. After that, the border router will select an optimal path and go down this path to the destination AS4. Applying this step to the topology network presented in Fig. 1, we can obtain the path as [AS1,AS2,AS3,AS4]. Finally, the border router R4 of AS4 forwards data packets to s0 according to the same routing algorithm. After this process, we can obtain the communication path as x=[u0,AS1,AS2,AS3,AS4,s0].

As set forth above, we represent all these features associated with a service invocation to corresponding dense vectors as the input of TAN. Formally, denote Rd a shared latent feature space, user i, service j and autonomous systems k will be projected into ui,sj,ask∈Rd where d is the dimensionality of latent feature vectors. Then, we have x=[u0,as1,as2,as3,as4,s0]. As for the implementation of TAN, the original feature vectors of users/services/ASs can be directly fetched through the identifiers, by using the interface of a deep learning library, such as embedding_lookup in Tensorflow.

3.2 Implicit End-Cross Modeling Layer
In the input layer, we can obtain the latent features associated with service invocations. Yet how to use these features effectively needs careful planning. The reason why collaborative QoS prediction methods are successful is that service invocations are abstracted as the feature interactions between users and services. By constructing a learning model, the useful patterns that affect the quality of service can be discovered from the data. Especially, the neighborhood effect (there are usually similar QoS values for users/services with similar contexts), can be fully exploited. Therefore, it is necessary to construct a End-cross Modeling Layer to model the interactions between users and services to capture this association pattern in QoS data.

Current techniques mostly model the interaction between users and services using the inner product of their latent feature vectors [13], [32]. However, the inner product only considers the linear correlation captured in the same feature dimension, while the high-order non-linearity between different dimensions is ignored. This leads to a deficient construction of end-cross features between users and services. To overcome this limitation, we employ the outer product to fully consider feature interactions between different dimensions and the convolution neural network (CNN) to extract high-order non-linear cross-features [54].

At first, we employ the outer product to generate an interaction map and to capture the dimensional correlations more sufficiently. The matrix M∈Rd×d generated by the outer product of two vectors is defined as follows:
M=ui∙sj.(1)
View SourceSuch a mechanism to capture the interactions between users and services may result in the redundancy of end-cross features and the risk of overfitting involved in a large number of parameters. Thus, we employ a convolution neural network (CNN) above the interaction map to address this issue by extracting high-order hidden features. In the CNN module, there are Lc convolution layers and T convolution kernels. Each convolution kernel can extract one type of latent feature from the input matrix. In each convolution layer, the input matrix of layer lc can be represented as a matrix Mlc, where lc is the identifier of the layer. When lc=0, the input matrix is the original user-service interaction map (i.e., M0=M), and the output of the layer lc−1 is used as the input of layer lc. Here, let ωlci denote the ith convolution kernel in the layer of lc. In this way, the feature extracted by the ith convolution kernel of layer lc can be described as follows:
Elci=relu(ωlci⊗Mlc−1+blc),(2)
View Sourcewhere Mlc−1 is the input matrix fed by the layer lc−1, ⊗ is the operation of convolution and blc is the bias. Now, the output matrix of layer lc (the input matrix of layer lc+1) can be represented as follows:
Mlc=[Elc1Elc2Elc3⋅⋅⋅ElcT],(3)
View Sourcewhere Elci is ith feature of layer lc.

The establishment and extraction of the cross-features mentioned above is highlighted in Figs. 3 and 4. In Fig. 3, given u⃗ i and s⃗ j with a shape of 1×4, after the outer product operation, we can obtain their interaction map with a shape of 4×4. In Fig. 4, given an input interaction map with a shape of 8×8×1 in the first layer, T as the number of kernels and 2 as the stride, we can obtain a feature map with the shape of 4×4×T as the output of layer 1 and the input of next layer after convolutional operations, so on and so forth. Finally, a 1×1×T vector can be generated as the end-cross features to be applied in the subsequent processes. To express the above-mentioned transformation conveniently, we rewrite it as: ve=cnn(ω∗,M). cnn(⋅,⋅) is the parameterized function to encode the interaction map M and ω∗ indicates the parameters of CNN module.


Fig. 3.
An example of outer product with 4 dimension features between a user and a service.

Show All


Fig. 4.
An example of feature extraction by convolution neural network.

Show All

3.3 Explicit Path Modeling Layer
End-cross modeling layer models the interactions between users and services, and extracts cross features that may impact the end-to-end QoS. In this case, the impacts of path features can also be captured, to a certain extent. This is because the pairing of a user and service implies the existence of a path. However, the role of the path will be limited because a lot of information about the path is missing (except the start and end points). We need a Path Modeling Layer to deal with the path features explicitly.

In the input layer, we have obtained the features of nodes on the path. However, isolated nodes cannot precisely characterize the mutual influence between them. It is thus necessary to model them as a single vector to aggregate and capture the path nodes’ interdependence. To achieve this, we introduce a component of Bidirectional Long Short Term Memory [56] into TAN. Long Short Term Memory (LSTM) is a unique recurrent neural network designed to deal with the vanishing of the gradients [55]. It consists of four components: input gate, output gate, forget gate and memory cell. All these gating units are intended to control the flow of information. In terms of structure, each input corresponds to a LSTM unit. Fig. 5 depicts the internal structure of the LSTM unit corresponding to one of the nodes in detail. Formally, the components of a LSTM unit are defined as follows:
itftcnctotht=sigmoid(Wixt+Wiht−1+bi),=sigmoid(Wfxt+Wfht−1+bf),=tanh(Wcxt+Wcht−1+bc),=ft⊙ct−1+it⊙cn,=sigmoid(Woxt+Woht−1+bo),=ot⊙tanh(ct),(4)
View SourceRight-click on figure for MathML and additional features.where sigmoid,tanh denotes the sigmoid and hyperbolic activation functions respectively, ⊙ is element-wise product, W∗ denote a set of weight matrices, b∗ denote a set of bias vectors, i,f,o,c,h represents input gate, forget gate, output gate, cell activation vectors and hidden unit, respectively.

Fig. 5. - 
The architecture of Long Short Term Memory Recurrent Neural Network [55].
Fig. 5.
The architecture of Long Short Term Memory Recurrent Neural Network [55].

Show All

Taking the sequence of nodes on the path as input (for example, x=[u0,as1,as2,as3,as4,s0]), each node corresponds to a LSTM unit. We define the length of the path as t=6. After t cycles of processing, the hidden state ht of the last LSTM unit represents the eventual sequence features, in which the interdependence between all nodes are taken into consideration. Formally, we have a function: ht→=lstm−→−(θ′,x) and θ′={Wi,Wf,Wc,Wo,bi,bf,bc,bo,x∗}.

Intuitively, a communication path involves both the forward and backward contexts, so a bi-directional LSTM is better to capture the bidirectional dependencies of elements in the sequence. Formally, we have: vp=ht←⊕ht→, where ht←=lstm←−−(θ′,x). It utilizes a forward LSTM to capture the forward contexts and utilizes a backward LSTM to capture the backward contexts. Then, their outputs are concatenated to completely describe the communication path.

3.4 Gating Layer
After path modeling and end-cross modeling, we have obtained two kinds of feature representations, i.e., ve and vp which characterize the end-to-end interactions and the communication paths, respectively. Inspired by the adaptive gate in the highway network [21], we employ a gating mechanism to combine the two types of features, and the fused features vg can be calculated as follows:
gvg=sigmoid(Weve+Wpvp),=g⊙ve+(1−g)⊙vp,(5)
View Sourcewhere g is a gate assigning the weight to the factor in each dimension of the two vectors, which provides a possibility to choose between the end-cross features and the path features dynamically. The sigmoid function ensures that the output values of g are from 0 to 1. The weight matrices We,Wp are used for linear transformation of ve and vp, respectively. By combining the features of end-to-end interactions and communication paths, TAN can already do well in the accurate QoS predictions.

3.5 Prediction Layer
The purpose of the Prediction Layer is to filter the fused features and realize the numerical prediction. By feeding the latent features of the whole process as the input to the fully-connected layers, the prediction are made as follows:
y1yL−1yL=relu(W1vg+b1),…,=relu(WL−1yL−2+bL−1),=WLyL−1+bL(6)
View Sourcewhere Wl and bl denote the weight matrix and the bias vector in the lth hidden layer, relu is the activation function, and there is no activation in the last layer, yL is the final prediction value of TAN. There totally are L sub-layers in this part.

3.6 Exploiting Side-Information
Beyond topological features and cross features, user-side features and service-dependent features also have a significant impact on end-to-end QoS. Therefore, we identify several features of this type and exploit them to enhance the prediction performance of TAN. As showed in Table 2, the average response time (also for throughput) observed from either user-side or service-side is a quantitative QoS value based on the historical data. It can be regarded as an expected value of the QoS attribute from a user i or service j, which can correct the prediction result and make it not drift too much. The number of operations, messages and ports, the size of the data are identified from the metadata of services. These characteristics can describe the complexity of services, so they involve in the cost of service execution.

TABLE 2 The Selected Side-Information

There are some possible ways to integrate TAN with side information. Here we provide one of the feasible solutions. Firstly, the side-information is quantified as z, taking RT as an example, zRT=[0.52,0.45,1,2,1,2700]; then a fully-connection layer without activation is employed to generate a linear transformation: z′, and finally it is integrated with TAN using another gating layer g′:
z′=Wz(z⊕bi⊕bj),g′=sigmoid(Wz′z′+WyyL),y′=g′z′+(1−g′)yL,(7)
View Sourcewhere ⊕ is the concatenation operator, Wz,bi,bj,Wz′,Wy are new parameters to be learned from the data. Specially, bi and bj are introduced to measure the potential QoS biases towards user i and service j. For simplicity, we name the new model as TAN++.

SECTION 4Model Learning
To estimate TAN's model parameters, we adopt both loss functions based on the least absolute deviations (L1) and the least square errors (L2). The L2 loss function is more sensitive to outliers in the training data compared to the L1 loss function because it will attempt to fit these outliers at the expense of additional samples [13]. Denote y(x) the true QoS value, and y^(x) the predicted QoS value given input x, we form the objective function of TAN with L1-loss:
minθ∑x∈X|y^(x)−y(x)|+λL1(θ).(8)
View Source

As a contrast, we form the objective function with L2-loss as follows:
minθ∑x∈X(y^(x)−y(x))2+λL2(θ).(9)
View Sourcewhere L1(θ) and L2(θ) are the regularization term with the balance factor λ employed to avoid overfitting of parameters in θ=[u∗,s∗,as∗,ω∗,W∗,b∗,b∗}.

As for model learning, a basic step is a gradient descent: select the initial value for a parameter θ, update the value of θ iteratively, and minimize the objective function until convergence. In each step of the iteration, the value of θ is updated in the negative direction of a gradient with respect to a training instance x. For the L1-loss based objective function, we have:
θ′=θ±ηdy^(x)dθ,(10)
View Sourcewhere η>0 is the learning rate (a.k.a the step size of gradient descent). And for the L2-loss based objective function, we have:
θ′=θ−η(y^(x)−y(x))dy^(x)dθ.(11)
View SourceFor each layer in a deep neural network, model parameters can be updated with a back-propagation algorithm that computes gradients using the chain rule [55]. TAN employs Adam, which performs first-order gradient-based optimization [57], in combination with mini-batch gradient descent to accelerate the training process.

SECTION 5Experiments
In this section, we conduct extensive experiments on two real-world datasets to demonstrate the advantages of TAN against state-of-the-art approaches. Then, to prove the necessity of each component of TAN, we design a series of ablation studies. Finally, we analyse the impacts of model parameters and their efficiency.

5.1 Datasets
To evaluate TAN's QoS prediction performance, similar to the state-of-the-art research[27], [30], [46], [58], our experiments focus on response time and throughput prediction, and further extend to reliability prediction. We use two WS-DREAM datasets collected and published by Zheng et al. [59], [60]. The first dataset contains a total of 1,974,675 real-world web service invocation results, which are collected from 339 users on 5,825 real-world web services on the PlanetLab platform. There are two QoS attributes in this dataset, i.e., response time and throughput . Also, it contains metadata of services. Afterwards, it was further extended by Tang et al. to associate users with regions, autonomous systems, and geographical coordinates as well as link services to autonomous systems, service providers, and regions [40]. To capture the topology information, we also use the IPv4 AS Links Dataset2 of the Internet at the level of autonomous systems. It is collected by the Center for Applied Internet Data Analysis and distributed in 2009 [61].

The second dataset contains 150,0000 service invocations on 100 Web services from 150 users. It builds for the evaluation and prediction of reliability. For each pair of users and services, its end-to-end reliability can be estimated by determining the number of successful responses in 100 service calls. We also enrich this dataset with contextual information as done by Tang et al [40]. Note that there are no available service metadata in the second dataset.

In the experiments, we further refine and extend these datasets. For each user or service, we need to obtain the autonomous system it belongs to and then check whether or not there is a corresponding AS in the topology. If the autonomous system is not given or a conforming item cannot be found in the topology, we will remove this invocation. After these processes, 327 users, 2,977 services, and 974,233 service invocations are retained in the dataset1. There remain 143 users, 57 services, and 8,151 records of reliability in the dataset2. The global topology consists of the autonomous systems that appear on the shortest paths from any user to any service and the links among all of them. The statistics of the two datasets are given in Table 3. We illustrate the users and services in the dataset on the world map based on their latitudes and longitudes in Fig. 6.

TABLE 3 Statistics of the Datasets


Fig. 6.
The distribution of the users and services on the world map.

Show All

Noted that for the QoS attributes of response time, throughput and reliability, the QoS data do not need to be normalized. On the one hand, these attributes have their own measures. Normalization will lose semantics of the measures, and specific characteristics of the QoS data. On the other hand, neural models allow fitting the QoS data without the need of data transformation, and thus forecasting the value of QoS attributes at different scales.

5.2 Evaluation Metrics
Mean absolute error (MAE), root mean squared error (RMSE), and normalized mean absolute error (NMAE) are used for measuring QoS prediction accuracy. Smaller values of these three evaluation metrics are desirable. MAE is defined as
MAE=∑x|y(x)−y^(x)|N,(12)
View SourceRight-click on figure for MathML and additional features.RMSE is defined as
RMSE=∑x(y(x)−y^(x))2N−−−−−−−−−−−−−−−√.(13)
View SourceNMAE is defined as
NMAE=MAEymax−ymin.(14)
View Sourcewhere y(x) and y^(x) are the observed QoS value and the predicted value for the input x, respectively. N is the number of test cases. ymax and ymin are the maximum QoS value and the minimum QoS value observed in a dataset, respectively.

5.3 Evaluation Methods
TAN is compared against state-of-the-art approaches that belong to the categories of neighborhood-based CF [10], [62], factorization-based approaches [12], [32], [37], [59], and neural-network-based approaches [13], [33], [34].

UPCC is a user-based collaborative model[10];

IPCC is an item-based collaborative model. [62];

UIPCC combines the user-based and item-based collaborative prediction approaches [62].

PMF is the basic matrix factorization approach for collaborative QoS prediction [59].

CSMF is a state-of-the-art matrix factorization approach that models the interactions of users-to-services and environment-to-environment [12].

FM combines the generality of feature engineering with the superiority of factorization models in estimating interactions between categorical variables [32]. It is employed in the context-aware QoS prediction in LBFM [37].

AFM is an extension of FM, which learns the importance of each feature interaction from data with a neural attention network [33].

NFM is an extension of FM to seamlessly combine the second-order features and the non-linearity of higher-order feature interactions [34]. A similar strategy is employed to develop a neural model for QoS prediction [13].

TAN2/1 is our TAN model in which the L2/L1 loss function is adopted.

TAN++ is our TAN++ model in which the L1 loss function is adopted.

The L2 loss function is adopted in all MF-based and FM-based models. UPCC, IPCC, UIPCC, and PMF leverage only the information from the user-service QoS matrix and no other information. On the contrary, full features (including users, services, users’ regions, users’ autonomous systems, services’ regions, and services’ IPs) are leveraged by CSMF, FM, AFM, NFM. All the methods are implemented in Python. Particularly, all factorization-based and neural-network-based methods are built and trained based on Tensorflow. The experiments are conducted on a computer configured with Intel Quad-Core i7 Processor, 32GB memory, and Ubuntu 18.04 with GPU support of Nvidia GTX 2080Ti.

5.4 Parameter Settings
To observe the impact of data sparsity, we randomly split the QoS dataset (a matrix) into a training set and a test set based on a specific ratio (e.g., MD= 2.5%, 5%, 7.5% 10%). For instance, MD (Matrix Density) =5% means that we randomly select 5-percent of the QoS entries to predict the remaining 95 percent of QoS entries. Then, for each method, the experimental results of three-folds are averaged and reported.

For neighborhood-based CF approaches UPCC, IPCC and UIPCC, PCC-based similarity is employed for the selection of the top-k neighbours of users or services. The neighborhood size of users is set as k = 10 and k = 50 for services [12]. For MF-based approaches, we set the regularization parameters according to Wu et al. [12]. For FM-based approaches, we follow the parameter settings in the source code given in [33], [34]. For TAN-based approaches, the value of λ in regularization is set as 0. The number of sub-layers in the prediction layer is fixed as 2 and the number of neurons in each sub-layer is 128. AdamOptimizer is the parameter optimizer and η is set to 0.01 initially. Besides, for PMF, CSMF, FM, AFM, NFM, TAN2, TAN1 and TAN++, we use the following configurations for training and testing: a) The maximum number of iterations is fixed at 100 and the mini-batch strategy with sizes from {64, 128, 256} is used; b) We compare the best prediction results in the iterative process.

5.5 Performance Comparison
We firstly conduct performance comparison on the tasks of response time and throughput prediction, and show the results in Table 4. For neighborhood-based methods, UPCC outperforms IPCC, and UIPCC's performance is somewhere in-between. It is consistent with the intuition that QoS evaluations on the user side are more accurate than on the service side [22]. In terms of PMF and CSMF, CSMF achieves higher accuracy than PMF in RMSE, considering that CSMF leveraging contextual information and modelling the interactions of users-to-services and environment-to-environment simultaneously. As compared with neighborhood-based methods, both PMF and CSMF outperform to neighborhood-based methods in many cases, particularly when the data are sparse. This confirms the effectiveness of matrix factorization in dealing with sparse data [12], [59].

TABLE 4 Performance Comparisons of QoS Prediction on Responsetime, Throughput and Reliability Using Different Matrix Density

And for factorization machines, FM does poorly. Although the first-order features and second-order cross-features are fully considered in FM, it has no additional feature selection ability, which leads to low efficiency in the task of QoS prediction. AFM can learn the importance of feature interactions from data with a self-attention mechanism and thus obtains lower prediction errors than FM. Also, since NFM can capture the non-linear and complex inherent structure of real-world data, it brings about a significant improvement over FM and AFM.

Regarding TAN2 which is optimized with L2-loss based objective function, we can find that TAN2 is superior to all the baselines in terms of MAE, RMSE, and NMAE on the response time prediction. Also, it outperforms most approaches except for PMF and CSMF on the throughput prediction. However, TAN2 is unable to compete with TAN1 where L1-loss based objective function is specially adopted for the QoS prediction. Obviously, the L1 loss is more robust than the L2 loss, as it is less-impacted by outliers which always exist in the QoS data.

TAN1 overwhelms all the models in both tasks. In the task of response time prediction, TAN1 improves the best values of baselines by an average of 33.78% for MAE. As for the metrics of NMAE and RMSE, these gains are 19.19% and 8.96%, respectively. In the task of throughput prediction, the gains are approximately 29.67%, 19.61%, and 1.03% in terms of MAE, NMAE and RMSE, respectively. Besides, with the increase of training samples, TAN1 yields better results. For example, the MAE of TAN1 is decreased from 0.4695 to 0.4005 as the MD is changed from 2.5% to 5%, and this trend is also reflected in other metrics.

As far as TAN++ is concerned, exploiting side information can naturally improve prediction accuracy. Compared with TAN1, in terms of MAE, RMSE and NMAE, the gains can reach 1.93%, 0.54% and 8.78% in the task of response time prediction, and 0.29%, 0.3% and 5.09% in the task of throughput prediction. As TAN++ is the fusion of TAN and side information, this shows that TAN has a good potential for expansion and can cooperate with other more powerful prediction models.

Finally, we focus on the results of reliability prediction. Owing to the lack of sufficient side information (in particular, the service metadata in the second dataset), we do not consider the results w.r.t TAN++. The end-to-end reliability significantly depends on the communication path, so TAN1 perform much superior to the baseline models. On the contrary, all baseline models are not provided satisfactory results of reliability prediction without exploration and exploitation of the path features. Considering that the volume of the reliability dataset is very small, which makes it difficult to train the memory-based prediction model, it is necessary to design a topology-aware QoS prediction model in real application scenarios.

5.6 Ablation Study
Since TAN combines explicit path features and implicit end-cross features, we explore the impact of each component by removing it from TAN in a controlled manner. Besides, to demonstrate the effectiveness of CNN, we replace the outer product and CNN with the traditional inner product in the implicit end-cross layer.

5.6.1 Impact of Path and end-to-end Interaction
In this section, we evaluate the impacts of the explicit path modeling layer and the implicit end-cross modeling layer. First of all, we train a TAN-E model without the explicit path modeling layer. At the same time, the gating layer is also removed. In this way, TAN captures only the users-to-services features to make QoS predictions. Similarly, we also remove end-cross features and only keep path features to build a TAN-P model. Finally, by comparing TAN-P, TAN-E with the original model TAN's prediction results, we analyze the impacts of different components.

As shown in Table 5, there is a significant difference in MAE between TAN and TAN-E. For response time prediction, TAN-E's MAE drop by 24.25% compared with TAN’s, while falling by 28.65% on the throughput prediction tasks. After removing the end-cross features, similar performance degradation is observed for TAN-P. However, the reduction in TAN-P's prediction accuracy is lower than that in TAN-E’s, where the decay is 9.16% and 11.45% on the two prediction tasks, respectively.

TABLE 5 Prediction Performances (MAE) of TAN-P, TAN-E and TAN

In total, TAN-E in which only end-cross features considered performs worse than TAN-P that especially models explicit paths. This is because that modeling of end-to-end interaction can only extract partial information from the data and is not sufficient to describe the complexity of the message communication paths. In contrast, explicit path modeling can effectively address the issue, so that there is a tolerable prediction accuracy for TAN-P. Certainly, it is not enough to provide high-precision prediction results only based on explicit path features. This is because the association patterns in QoS data, especially the neighborhood effect, whose great importance has been demonstrated in many previous studies of collaborative filtering [10][62], are not captured. So the end-to-end interaction modeling just can take the advantage of neighborhood effect and form a complementary relationship with explicit path modeling.

5.6.2 Comparison of Inner-Product and Outer-Product on End-Cross Modeling
Concerning the end-cross modeling of users and services, a combination of the outer product and the convolution neural network is adopted in this paper. To verify the significance of this design, we take the inner product operation to replace our End-Cross Modeling Layer for interactive modeling. This introduces a degraded version of TAN, referred to as TAN-I here.

According to Table 6, TAN can significantly diminish the MAE compared with TAN-I on both response time and throughput prediction, the corresponding decreases are up to 18.72% and 22.95%. That means the outer product in combination with CNN is more advantageous than the inner product when modeling feature interactions of users and services. It is due to only interactions between the same dimensions are captured in the inner product while the outer product in combination with a variety of convolution kernels can construct nonlinear interactions and combine the features of different dimensions. This allows the TAN model to have a higher capacity to capture useful features and further contributes to high prediction accuracy.

TABLE 6 Prediction Performances (MAE) of TAN-I and TAN

5.7 Parameter Analysis
We analyze the impact of the dimensionality of latent features and learn how the configurations of the prediction layer impact TAN's prediction accuracy. TAN++ is omitted as it is only one possible extension of TAN.

5.7.1 Impact of Dimensionality in Input Layer
The dimensionality d means the number of latent factors utilized to represent users, services, and ASs in the latent space Rd. For example, d=16 means that all users, services and ASs are represented by vectors of length 16. These vectors will be randomly initialized and then optimized during model training. Fig. 7 shows TAN's prediction accuracy with different combinations of d and MD.


Fig. 7.
Prediction performance of TAN with different dimensionality and matrix density. RT means response time and TP means throughput.

Show All

When the dimensionality d is fixed, TAN's prediction error, regardless of RMSE, MAE, NMAE, shows a gradually decreasing trend with the increase in the training data density. When we keep the data density constant, the trend will not always stay in the same direction as the dimensionality increases. In the response time prediction task, when the data density is in a relatively larger range from 5% to 10%, the increase in the dimensionality from 16 to 64 will result in better prediction accuracy. However, when the dimensionality further increases to 128, the prediction error begins to increase. In the cases of sparse data (e.g., MD=2.5%), all three evaluation metrics reach their minimum when the dimension is 16. Improving the dimensionality would lower the prediction accuracy. Similarly, these evaluation metrics show a tendency of first decreasing then increasing under all data densities. It reaches the optimum when the dimension is 64 on the task of throughput prediction. This observation is coinciding with the intuition that a relatively large dimension will produce better prediction results [59]. When too many parameters are flooded in TAN without sufficient training samples, the redundancy of latent features will inevitably lead to over-fitting, which will overreact to subtle fluctuations in the training data [13], [59]. It turns out that a medium-sized dimension (e.g., d=64) is the most suitable in cases of sparse data.

5.7.2 Impact of Prediction Layer
According to Eq. (6), there are at least two sub-layers in the prediction layer. So, different structures of sub-layers and neurons are set up to study their impact on QoS prediction in terms of MAE. The results are presented in Table 7. Note that the configuration in the format of {32,32} means that two sub-layers are set and there are 32 neurons in each sub-layer. There is no doubt that larger training data will greatly improve TAN's prediction accuracy. However, due to the problem of data sparsity, the increase of sub-layers is not conducive to error reduction given a relatively large number of neurons. For the configuration of two-layers, adding neurons can help reduce prediction errors. However, the profit is bounded[13]. In the cases of three-layers, extra neurons may lead to the risk of over-fitting. The best setting is two sub-layers with 128 neurons each sub-layers. 128 is just twice the optimal setting of d = 64, which is also equal to the number of neurons in the gating layer and the dimension of the output vector vp of bi-directional LSTM. This observation will simplify the setting of hyper-parameters in TAN. For example, we can always make the number of neurons in multiple hidden layers of bi-directional LSTM equal to the dimensionality of the input layer.

TABLE 7 Prediction Performance (MAE) of TAN With Different Configurations of the Prediction Layer

5.8 Analysis of Efficiency
5.8.1 Space Costs
For UPCC, IPCC and UIPCC, they only need to store the historical QoS data. As this kind of data is very sparse, the space cost for them is quite small. For model-based methods, their space cost is mainly reflected in the total number of model parameters. Denote Nu the number of users, Ns the number of services, Nas the number of autonomous systems, Nc the number of contextual features, L the number of full-connection layers, T the number of convolution kernels, the space complexity of selected prediction models is showed in Table 8. As for the factorization-based approaches, PMF has the least space overhead. CSMF, FM, AFM and NFM have the same space complexity considering that the number of users and services will be much larger than the number of contextual features (such as UR, UAS, SR, SAS). Based on the same observation, TAN has the same space complexity as other factorization-based methods.

TABLE 8 The Space Complexity of Selected Prediction Models

5.8.2 Computational Efficiency
Since a large number of similarity calculations are performed by UPCC, IPCC, and UIPCC, which inevitably lead to low efficiency, they are omitted in this section for simplicity. For model-based methods, the computational efficiency mainly depends on the dimensionality (d) of latent features, the size of training samples (MD), the batch size, the model size and the training strategies (such as optimization algorithm, learning rate and epochs).

Given the same configurations of d, MD and the batch size, Figs. 8a and 8b respectively show the comparison of training overhead and prediction overhead w.r.t selected methods. Obviously, TAN is inefficient compared with PMF, CSMF, FM, AFM and NFM when training the prediction models. On the one hand, the introduction of CNN and LSTM leads to the high time complexity of TAN. On the other hand, TAN has more parameters as compared with these factorization-based methods. However, TAN is competitive against prediction efficiency. It can predict nearly one million missing QoS records within one minute. Also, we can increase the batch size and use a smaller dimensionality (see Fig. 9) to improve the computational efficiency of TAN, and only lose small prediction accuracy (see Table 7).


Fig. 8.
The training overhead and prediction overhead w.r.t selected methods.

Show All


Fig. 9.
a) The computational overhead of TAN with batch size; b)The computational overhead of TAN with dimensionality.

Show All

5.9 Further Discussions About TAN
We have made a deep analysis of TAN, but it still faces some key challenges. Here, we will briefly summarize and discuss these issues.

The foremost challenge is from the dynamic nature of QoS. At present, TAN/TAN++ is only designed for the task of static QoS prediction, considering that the quality of service deployed in cloud center is mostly stable and will not fluctuate violently in a short time; Moreover, in the users’ perspective, QoS-aware web service selection focus more on the average QoS over a period of time than the QoS at a specific time[63]. As for the need for real-time Qos prediction, TAN needed to be extended to take into account the temporal factor. Specifically, QoS records can be divided into reasonable time slots, which can be characterized as vectors and combined with TAN. This solution has been verified to be effective in [14]. As TAN has good re-training efficiency, the new model can be re-trained regularly or on-demand to accommodate QoS variations.

Moreover, only the static shortest path is used to replace the end-to-end optimal path. Due to the dynamic nature of network routing, however, the shortest path may not be the actual path. It is necessary to study methods that can automatically identify dynamic paths. Of course, this requires more data support, such as massive routing traces and more QoS records.

When new users or services emerge, their representations will be cold-started and untrained. This will seriously affect the prediction accuracy. This cold-start problem can not be prevented by the current QoS prediction methods, and special countermeasures need to be studied to solve it. For example, using path approximation strategy and fine-tuning the prediction model with transfer learning.

SECTION 6Conclusion and Future Works
We have proposed TAN for the QoS prediction. It is the first attempt to consider representation learning of Internet topology for the QoS prediction. It constructs end-cross and path features, and models service requests and responses comprehensively. Experimental results demonstrate TAN's superiority and extensibility in the QoS prediction.

There are several directions for expanding TAN. It is worth exploring to solve the cold-start problem by using transfer learning. Cooperating with time to realize dynamic discovery of the paths and real-time QoS prediction is also a research direction with immense potential.