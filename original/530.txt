With growing Field Programmable Gate Array (FPGA) device sizes and their integration in environments enabling sharing of computing resources such as cloud and edge computing, there is a requirement to share the FPGA area between multiple tasks. The resource sharing typically involves partitioning the FPGA space into fix-sized slots. This results in suboptimal resource utilisation and relatively poor performance, particularly as the number of tasks increase. Using OpenCL's exploration capabilities, we employ clever clustering and custom, task-specific partitioning and mapping to create a novel, area sharing methodology where task resource requirements are more effectively managed. Using models with varying resource/throughput profiles, we select the most appropriate distribution based on the runtime, workload needs to enhance temporal compute density. The approach is enabled in the system stack by a corresponding task-based virtualisation model. Using 11 high performance tasks from graph analysis, linear algebra and media streaming, we demonstrate an average 2.8× higher system throughput at 2.3× better energy efficiency over existing approaches.
SECTION 1Introduction
Computing services, such as cloud data centres, look to achieve a higher return on cost of computing systems by efficient temporal and spatial sharing of resources amongst multiple tasks. In the recent time, the drive for enhanced performance for computationally demanding tasks have encouraged cloud service providers, such as Amazon, to integrate Field Programmable Gate Arrays (FPGAs) in their data centres [1]. Whilst FPGAs offer acceleration in high performance computing (HPC), this is typically restricted to a single application configuration (SAC) [1] and effective resource management for area-shared, multi-task execution still remains challenging. This has a higher significance, particularly with the migration of services to fog/edge under stricter resource and energy utilisation constraints.

Conventional software-programmable systems typically use discrete processing cores, abstracted as software threads offering microsecond latency context switching between tasks. This results in a highly flexible resource management and tasks scheduling model, to which a range of optimisations can be applied [2]. With FPGAs, however, this is complicated as support for mapping source code has to be considered spatially, i.e., how the devices, physical resources are shared.

Typically, frameworks are based on partial reconfigurable regions (PRRs) [3], [4] where the FPGA is partitioned into fix-sized rectangular slots. Spatial mapping constraints on modern FPGAs mean that the PRRs are designed to be largely homogeneous. Modern HPC tasks, however, are inherently heterogeneous where resource needs such as memory, computing, bandwidth, will change and have runtime varying workload sizes and throughput needs [5]. Mapping these independent HPC tasks with custom designed hardware and I/O onto PRR systems, typically results in a mismatch giving lower compute density and underutilisation of FPGA resources by up to 70 percent [3].

Custom task-specific partitioning and mapping (CPM), supported by the vendor tools, can be used to generate a single bitstream supporting multiple tasks acceleration functions for area-shared execution. The diverse computing requirements and workload sizes of dynamic task queues make the resource management and tasks scheduling an NP-complete problem, in addition to the long FPGA synthesis times. Thus, any effective approach needs to enable a runtime model that can offload appropriate designs onto FPGA to optimise overall performance, defined as the system throughput (STP) metric [6], a more effective measure of performance in multi-task workload processing. A full system stack is also needed for using FPGAs as a resource for optimum execution of dynamic workloads.

In this paper, a task-based implementation, optimisation and execution framework is proposed to tackle these challenges. Using OpenCL’s exploration capabilities, a large design space is generated and explored using machine learning to custom generate high compute density and runtime scalable accelerator functions. These are used in an effective manner in runtime, to suit the variable workload requirements, eventually leading to a higher STP for a range of heterogeneous tasks. The main contributions are given as:

A comprehensive runtime evaluation tool that provides early design analysis of the spatial and temporal constraints of various mapping schemes. It allows selection of the optimal under the operating conditions and objectives, as well as hinting towards areas for optimisation, e.g., area-shared multi-task execution, via functional emulation.

Machine learning based characterisation of area/throughput rate for different tasks and clustering of tasks for custom mapping and co-execution in an area-shared fashion to achieve high spatial compute density designs.

Multi-task design space exploration (DSE) and use of preemptive scheduling to enable an effective runtime resource allocation based on the size of each co-executed task, in achieving a high temporal density.

Realisation of a task-based virtualised resource allocation model to support this task-specific area sharing model at the higher system level.

Using HPC examples from graph analysis, linear algebra, media streaming and data mining, an average STP improvement of 2.8× at 2.3× better energy efficiency is achieved over PRR.

The paper is organised as follows. Section 2 discusses the motivation and background. Section 3 describes our proposed framework, whilst Section 4 gives details of the evaluation environment. Section 5 provides an analysis using the examples and conclusions are given in Section 6.

SECTION 2Background
FPGA-Based Computing Systems. With FPGAs integration in cloud and data centres, work is being carried out to allow seamless and efficient access to the FPGA resources from software environments. Among these, the authors in [7] treat the FPGA as an independent resource and develop a communication stack and associated hardware on the FPGAs to directly communicate with other CPUs and FPGAs in the cluster. Work in [8] uses middleware to provide an application-centric interface for developers and takes care of hardware development and integration for various vendors to enable portability and better productivity. Authors in [9] have developed a compilation framework, communication interfaces and runtime libraries to scale resources from sub-FPGA to multi-FPGAs per task requirements. Similarly, work in [10] takes a modular approach to system stack design where different components allow portability to integrate with varying versions of design tools and hardware and software layers.

FPGA as Reconfigurable Resource. Furthermore, with the use of an FPGA as a re-programmable source in dynamic environments, researchers have investigated various reconfiguration and task mapping schemes. The temporal sharing include the reconfiguration of FPGA with a single task (SAC) [7], [11] and the use of software programmable soft-cores [13]. For larger tasks, researchers have looked at providing transparent access to multiple FPGAs with each FPGA processing part of the task [14]. In addition, with increasing device sizes, research has explored space sharing, mostly via PRR [3], [4]. Finally, authors in [9] have looked to combine homogeneous PRR block-based design with supported interconnections that scale resources for a task on and across FPGAs transparently.

FPGA Space Sharing Constraints. This work is targeted at tasks that can benefit from sharing of FPGA space. Traditionally, this has been achieved via PRRs [3], [4], [15] as it mimics discrete computing resources in multi-core software programmable systems and provides independence in space and time. The independence in time suggests that a task in a single PRR slot can be independently reconfigured with a new task without affecting the processing in other PRR slots, enabling easy scheduling decisions such as task priorities. However, the flexibility in resource scaling per task and achieved resource mapping density suffers.

This is because the space domain programming and mapping of FPGA fabric via PRRs is challenging, since the distribution of the heterogeneous resources on modern tiled FPGA is unsymmetrical, particularly along the horizontal axis, as shown in Fig. 1. Furthermore, the FPGA uses multiple clock regions across both the vertical and horizontal axes and so crossing the region boundary requires custom logic which cannot be supported by modern runtime bitstream relocation schemes [16]. This limits relocation to homogeneous regions along the y-axis with a step size equal to height of clock region (Fig. 1), in line with work on PRR systems for independent tasks [3].


Fig. 1.
FPGA partitioning for PRR.

Show All

These mapping constraints restrict PRRs to be homogeneously designed regions, leading to underutilisation of resources. First, after omission of the static area for I/O interconnects, the homogeneous region along the y-axis can be as low as 60 percent area of the FPGA [3]. Second, modern data center workloads comprise a range of heterogeneous tasks with varying resource requirements [5], [10]. Within the boundaries of PRR, the actual area being allocated to heterogeneous task may be lower, namely 38 - 51% [17]. Overall, the effect is that the utilisation can be as low as 30% of the available resources, resulting in a lower system throughput and reduction in number of tasks that can be co-executed. Finally, due to the routing constraints surrounding fixed PRR slots, the frequency can drop by up to 24% [18].

Space Sharing Optimisations. Research has explored optimisation of resource utilisation of PRR-based designs to maximise throughput. Authors in [19] design variable sized slots and map a compute intensive task with a memory intensive task. Design in [20] allows 1,2 or 4 PRR slots to be used as a reconfigurable region for a task as a mean of mapping options. Authors in [15] reduce fragmentation within PRRs by optimising scheduling using policies such as compact placement and minimum conflicts between sharing tasks for variable sized PRRs. Research in [21] reduce the reconfiguration overhead associated with PRR usage, by prefetching the next task’s bitstream for next scheduled task and re-using the loaded bitstream when possible. Authors in [3] enable runtime support for elastic resource allocation per task if adjacent PRR slots are available, whereas those in [9] propose slots built using homogeneous blocks where a task can be allocated any number of blocks at runtime to create heterogeneous slots. However, even though these approaches improve on a basic PRR setup, they use the largely homogeneous PRR slots to map all heterogeneous tasks, which results in non-optimal resource utilisation.

The infrastructure in [22] is recent work that allows creation of amorphous subspaces on the FPGA which can both be partially and independently reconfigured for low-latency mode, whilst multiple subspaces can be combined and custom mapped for high throughput mode. The work gives large gains in throughput by increased resource utilisation when generating combined bitstreams, particularly by co-scheduling tasks with variable resource requirements. However, the work is targeted at the operating system (OS) design and providing system support to transparently transfer between the two modes. For the high throughput mode, it briefly suggests parallel creation of bitstreams (from tasks’ netlists) and place and route acceleration techniques to hide bitstream generation latency. However, the proposed use of a fair usage scheduler may create non-optimal mappings and frequent switching at runtime. In contrast, our work looks to first create a much larger design space and systematically explore it offline with the aim of generating higher density designs for high-throughput execution. Furthermore, we incorporate throughput-based scheduling and resource allocation to increase the usage of the high-throughput mode. Finally, we provide a thorough evaluation of both modes to enable informed decisions about changing modes as per the operating environment.

2.1 Design Philosophy
Consider a data computing environment where processing requests are received at regular intervals. The key differences for execution models of PRR and the proposed CPM framework are illustrated in Figs. 2a and 2b, respectively. The PRR approach generates bistreams and treats each incoming task requests independently. The system manager manages the incoming task queue via a virtualisation layer that abstracts the underlying hardware implementation from the user space. For each request, the virtualisation layer caters for the I/O and communicates with the resource manager which manages multiple regions and loads the bitstream for each execution.


Fig. 2.
PRR and CPM design flow and runtime.

Show All

In our approach, a major focus has been made to improve the resource utilisation by removing the need for each task to go into a predefined partial region, thus improving total execution time for the overall task set. This requires a number of major changes to the flow. First, a range of efficient swappable FPGA accelerators with a high compute density and varying areas are created for each task. The aim is to integrate the bitstreams supporting execution of multiple tasks, but rather than employing dynamic reconfiguration, these are loaded as a single multi-task bitstream onto the FPGA.

A lightweight scheduler, running at the same level as virtualisation layer, links task processing requests with corresponding optimum bitstream based on multi-task DSE and pre-emptive scheduling techniques. To enable intelligent co-scheduling, the scheduler processes task queues in batches, allowing reordering and processing in clusters to gain a higher throughput from CPM. With the order of jobs being second priority, a comparison may be made with other approaches for the epoch time at which execution of each task is completed, in order to evaluate the effect of prioritising throughput over the order of tasks.

Furthermore, the tasks implementation and runtime support are provided by the OpenCL framework as OpenCL is now recognised as an established high-level language for design as well as integration with software-based heterogeneous data centres [12]. The proposed approach fits well with the FPGA version of cloud computing model of Software as a Service (SaaS) where optimised bitstreams for standard computing tasks are stored as a library (Amazon Marketplace for Amazon FPGA Image (AFI) [1]) and users can request functional acceleration service with variable workload. Further optimisations can be made by complementing it with real-time data center workload characterisation [5].

We previously presented a comparative evaluation of partitioning schemes [23], an accuracy analysis of runtime evaluation tool [24] and spatial mapping optimisation via clustering [25]. This paper introduces a complete framework and includes the implementation of novel new components for temporal optimisation and integration with high-level virtualisation model. Building on all these modules, this work, for the first time, presents a complete system stack from early design analysis to deployment of dynamic heterogeneous HPC tasks.

SECTION 3Methodology
The high compute density mapping of Fig. 2b forms the core of framework enabling task-specific execution model and is explained in Fig. 3. From the list of heterogeneous HPC tasks to be executed, the OpenCL computing model is used to describe the functionality, thereby allowing the parallelism granularity to be defined using the general high-level synthesis parameters. The resulting design space is explored using dynamic hardware profiling, ensuring that the generated designs provide optimum speedup per resources utilisation. The eventual multiple hardware designs are then used to generate the dataset specifying the throughput achieved against resources utilised for each task.


Fig. 3.
High compute density mapping design methodology.

Show All

The comprehensive multi-task runtime evaluation tool starts with the single task DSE to gauge the performance of various partitioning schemes for dynamic workloads against various system design parameters. The performance analysis showed that although CPM provides higher spatial compute density than PRR, it is similar to SAC and may perform worse for dynamic workloads because of lower temporal utilisation. To address this, characterisation of tasks using the single task DSE along with machine learning based regression models is used to evaluate the weighted relationship between each on- and off-chip heterogeneous resource and FPGA throughput. The characterisation is used to divide all tasks into smaller clusters, so that tasks in a cluster complement others’ resource needs and can be co-executed on the FPGA.

Furthermore, to minimise the reconfiguration overhead, a multi-task bitstream may only be replaced with a new one after all the tasks being co-executed have finished. To reduce stalling by the longest running task, resource allocation per task is varied in the multi-task cluster as per the runtime workload size of each co-executed task. To enable this, the module generates a set of designs per cluster using a multi-task DSE, while trading off resource allocation (and throughput) between the cluster tasks. This permits variation in the execution time, Texec, of each task in processing respective workloads.

The generated multi-task designs are custom mapped on the FPGA to generate bitstreams supporting multiple task accelerator functions. The generated sets of bitstreams per cluster are then profiled on the hardware for throughput using metrics defined per corresponding workload for each task in the cluster. These high compute density mappings enable a high throughput hardware design which are then used by the higher level system stack.

3.1 Design Space Exploration
The DSE enables exploration of system optimisation strategies and the resulting area-throughput rate supports a benefit based approach where tasks can be allocated resources which profits them the most, i.e., memory, compute. The DSE is enabled by OpenCL’s capability to allow explicit description of parallel computing and scaling of hardware resources via multiple parameters [26]. A task’s kernel can be scaled over multiple compute units, CUs, where these implement coarse-grained parallelism described as work-groups. A work-group can be further spanned over work-items where multiple pipelines for these can be defined via the single instruction multiple data, SIMD, pragma.

A kernel may also be implemented as a single work-item. For some of these tasks, task-specific parameters such as the block size, number of rows, are used, as these define the parallelism of the defined parameter size. For some tasks, the unrolling pragma, U, can be used to unroll compute intensive part of the kernel which is identified using dynamic profiling based on an ’always active’ counter. The counter is coded in VHDL and passed to the OpenCL kernel as a software library via an Intel OpenCL Library feature. Eventually, parameters that provide the highest throughput scaling per unit area are selected.

3.2 Runtime Evaluation
A comprehensive multi-task runtime functional emulation tool (Fig. 4) allows fast early stage comparison. It enables multi-task DSE using the single task DSE and may be combined with analytical models [27], to significantly speed up clustering and resource allocation per task.


Fig. 4.
Runtime evaluation methodology.

Show All

Placement Checks. For PRR, the 2D area model treats mapping as a rectangle fitting problem and aims to find a region homogeneous in both size and spatial distribution of resources to map each incoming task [15]. For CPM, we implement a multi-dimensional model accommodating a dimension for each heterogeneous, on-chip resource i.e., logic, block random access memory (BRAM), digital signal processing blocks (DSPs). The mapping optimisations try to accommodate as many tasks as possible, whilst keeping total utilisation of all resources within the device limit. For PRR, each task’s configuration is treated independently while for CPM, a configuration waits for the longest running task and then selects a new multi-task configuration.

Memory Modelling. Even if the allocated on-chip resources in the multi-task environment are same as the single task, the achieved throughput may not be identical due to memory contention. To model and predict memory performance in multi-task processing, the tool employs ridge regression [28]. We generate a range of multi-task bitstreams and measure the actual performance to train the model with the accuracy presented in [24].

System DSE/Resource Management. The tool can be used to evaluate various resource management strategies. For CPM, a multi-task DSE estimates the effect on throughput while varying resource allocation per task in a cluster. For PRR, the tool implements optimisations that target segmentation (vacant regions on the FPGA at runtime) on top of basic homogeneous PRRs and which are important to compare PRR fairly with CPM. Among these, the first one checks if the adjacent PRR regions are free, and then attempts to fit a larger bitstream for the same task in this combined region to gain a speedup [3].

The second one targets partitioning FPGA into heterogeneous PRRs with different numbers of resources to increase mapping flexibility [29]. The tasks are then custom designed for one of the PRRs. Heterogeneous PRRs can be defined by including a different ratio of each heterogeneous resource type. However, in the current scenario, the device size is not big enough to benefit from such an approach, so heterogeneous PRRs are defined by varying the number of each type of resources while their relative ratios remain the same (Fig. 1). The optimisation uses heterogeneous PRRs to fit a smaller bitstream for tasks when none of the original bitstreams can be accommodated by a region [29].

Finally, the tool varies the vertical step size up to a single row on a continuous y-axis, for bitstream relocation. Although exhaustive, this can be achieved by generating multiple bitstreams equal to the number of rows within each clock region, by varying starting y-coordinates.

Configurations. Bitstreams parameters such as the coordinates of bounding boxes and heterogeneous resources usage from the DSE, are passed to the evaluation tool. The user can also specify a deviation in the Texec of tasks. A uniform distribution is used for random task generation and an input parameter varies the range of distribution for Texec.

Constraints. In PRR, the homogeneous/heterogeneous regions are fixed and the coordinates are provided by the user. In CPM, the total number of available heterogeneous resources and a realistic percentage of the maximum utilisation is provided as an input. To study the effect of various PRR constraints, the available area for the task mapping as well as bitstream relocation steps can be varied.

3.3 High Compute Density Bitstream Generation
We optimise system throughput by achieving a denser mapping of FPGA resources. The various modules include:

Characterisation and Clustering for Spatial Optimisation. Along with clustering, informed runtime decisions are enabled on heterogeneous resource allocation per task which requires characterisation of the DSE of each task rather than of any single bitstream. We perform regression modelling to find the weighted contribution of each type of resource towards throughput. This allows a benefit-based approach where a task which profits the most from a higher allocation of a certain resource type is clustered with a task which profits the least. Four different resources, including on-chip BRAM, DSPs, logic and off-chip bandwidth which represent the key FPGA resources, are considered.

Whilst ordinary least squares is one of the more commonly used linear regression methods, it is highly sensitive to random errors when variables are correlated, such as here where DSP blocks are linked to BRAM; ridge regression avoids this. The normalised values of on- and off-chip heterogeneous resources against maximum available for all bitstreams per task form the independent variables for the regression. The achieved throughput, measured on the actual hardware and normalised to maximum achievable for each task, becomes the dependent variable.

Regression provides the significance scores of each type of resource for each task to scale throughput. In addition, the normalised bandwidth utilisation for each task’s largest bitstream is used to cluster tasks for space sharing at a single time. Only bandwidth is used, as unlike other on-chip resources, it may become a bottleneck in the DSE for extremely memory intensive tasks and in the ridge regression model, consistent bandwidth usage may hide the fact that task has a high bandwidth dependence. In order to define clusters, each task is first represented in a multi-dimensional space where each dimension either represents the regression score of a resource or normalised bandwidth.

Finding the best combination of clusters is a global optimisation problem. We use a custom-designed optimisation function for its reduced complexity and validity for the considered scale of problem. It runs a set number of iterations where each iteration randomly selects the first task for each new cluster and then other tasks are searched such that they have maximum distance, and thus heterogeneity between them in the multi-dimensional space. The number of tasks in a cluster are defined manually by the system designer based on device size. The sum of mutual distances between tasks is used as a score of the cluster, while sum of all cluster scores defines iteration’s score. The iteration with highest score is chosen as the solution.

Resource Variation Per Task. Although CPM allows for higher spatial compute density, all cluster tasks are reconfigured as a single integrated bitstream. This means that the longest running task stalls the other tasks, unless reconfigured at the expense of reconfiguration overhead, and the resource utilisation by the tasks that finish early is suboptimal. To counter this, sets of multiple bitstreams per cluster with varying on-chip resources are created using the multi-task DSE enabled by the runtime evaluation tool and varying the high-level parameters. The clustering helps to avoid contention for the same type of resource and each task is allocated the resources it needs. A range of designs are produced with varying area-throughput rates for processing the respective workloads.

Integrated Bitsreams Generation. A PRR system is limited to pre-defined reconfigurable regions because of the runtime bitstream relocation needs, thus limiting the optimisation via clustering to only off-chip memory bandwidth. However, CPM can also benefit from optimisation of on-chip resource usage using custom FPGA mapping and resource allocation to tasks as per their heterogeneity, to generate a single bitstream offering multiple task acceleration functions.

The bitstream is generated by first using the OpenCL front-end to create HDL modules. Placement scripts modify the constraint files to map task modules to the corresponding PRR while for CPM, the area to be mapped is set to available area for task logic (Fig. 1). Finally an integrated bitstream is generated using the place and route tools, which are integrated with OpenCL back-end. In this work, no custom mapping optimisations over the vendor’s place and route tools are used by CPM for multiple modules. Both the PRR and CPM modules can be partially reconfigured independently of the static logic.

Hardware Profiling. All configurations are profiled while executing all tasks in the cluster to calculate real throughput for each bitstream of all tasks using workload-specific metrics, as described in Table 1. The workload-specific metrics encompass all of the compute, memory and control instructions to give a real measure of performance for time to solution.

TABLE 1 Use Cases Characteristics

With FPGA bitstream generation times taking hours for a single task, the full process can take a significant time. The offline process leading to bitstream generation for an initial set of tasks may be generated at the design time. For upcoming new tasks at runtime and if the system manager decides to update clustering and associated bitstreams, this may be achieved in parallel with tasks execution and updated once completed. Furthermore, the scope and frequency of clustering based optimisation may be limited to reduce design time as well as use of the simulator for faster pre-design analysis.

3.4 Runtime Scheduler
The runtime is designed to be lightweight such that its overhead on task execution time is negligible. For an incoming heterogeneous task queue with variable workload sizes, the runtime scheduler (Fig. 5) uses preemptive scheduling on the measured throughput of various multi-task bitstreams and selects the one that minimises the difference in the tasks Texec. Meanwhile, checkpoints allow for re-evaluation of the scheduling decisions and context switches. Eventually, this reduces stalling by a single long running task and improves temporal resource utilisation on top of high compute density mappings to achieve an higher overall throughput.


Fig. 5.
Runtime scheduler for bitstream selection as per workload sizes.

Show All

Algorithm 1 provides pseudo-code for the runtime selection of optimum bitstream. For each unscheduled task, nextTask, in the task queue, the algorithm first checks if the other tasks in the same cluster as nextTask also need to be scheduled. If not, it uses a single task bitstream for nextTask. Otherwise, it iterates through multiple bitstreams for that cluster to find the bitstream that minimises the difference of Texec (calculated as data size divided by throughput) for the tasks in the cluster for their respective workload sizes. The optimum bitstream and corresponding tasks are put in scheduling order. The estimate_Ti value provides estimated Texec for the ith bitstream for task in the brackets. Max_value on line 10 equals the sum of Texec for all tasks in SAC.

Algorithm 1. Runtime to Generate Scheduling Order, SO, and Associated Bitstreams, B, for Dynamic Task Queue, TQ
while TQ is not empty do

nextTask ← TQ[nextUnfinishedTask];

cluster ← taskClusters[nextTask] ;

otherTasksInCluster ← cluster - nextTask ;

if otherTasksInCluster not in TQ(unfinished) then

B[nextTask] ← singleBitstream[nextTask];

SO[nextTask] ← nextTask

end

else

ΔTexec ← max_value ;

for i in range(bitstreamsSet[cluster]) do

temp_ΔTexec ← estimate_Ti(nextTask) - estimate_Ti(otherTasksInCluster) ;

if ΔTexec > tempΔTexec) then

ΔTexec ← tempΔTexec ;

B[nextTask] ← bitstreams[i] ;

SO[nextTask] ← nextTask + otherTasksInCluster;

end

end

end

end

The preemptive estimation selects the bitstream such that the estimated difference in Texec for all of tasks in a cluster is the least amongst all bitstreams. This does not need cycle accurate estimation and even with allowable difference in the Texec of tasks, a higher throughput is possible, owing to the intelligent clustering and CPM and as long as the longest running task has the highest resource allocation in the cluster.

The runtime also makes use of lightweight scheduling decisions to re-evaluate the bitstream selection or perform context switches at discrete points in time, called checkpoints. Although checkpoints can be implemented at a finer granularity in OpenCL, such as at work-group level [3], the reconfiguration and host-accelerator communication overhead, may offset the acceleration gain. Instead, checkpoints are implemented at block level, namely an independent part of a whole workload that is batch processed, i.e., the computation is off-loaded to a FPGA and outputs written back to the host in batch. A block may be workload specific and is generated via workload distribution at a higher software level. All OpenCL workloads need to be processed as a set of blocks, iterated over the total workload, due to limited on-board DRAM.

Considering the runtime workload, the scheduler uses the preemptive estimation of Texec at the respective block size and the number of blocks in order to estimate total Texec of a workload. However, at the checkpoints after each block execution, the bitstream selection and scheduling decisions are re-evaluated, allowing for variation in the resource allocation per task. The scheduler evaluates the following equation to decide as to when to switch the bitstream
NewTexec=Tr(n)+WS(r)TH(n),(1)
View SourceRight-click on figure for MathML and additional features.where Tr(n) and TH(n) are the reconfiguration time and throughput of the new bitstream, respectively, while WS(r) is the remaining workload. The scheduler compares the NewTexec with the current configuration and chooses the improved one. Recent work in this domain has proposed the design of mechanisms for context-saving as well as run-time migration to a new reconfiguration on the same device or a new device for OpenCL-based tasks, while saving the already processed data [9], [30].

3.5 Virtualisation and Revenue Model
The conventional PRR-based approach targets infrastructure as a service (IaaS) and scales area per tasks with the number of PRR slots corresponding to number of resources [3], similar to the discrete cores-based distribution of resources in multi-processor systems. As resource-based division incurs a high cost of sharing in FPGA, a task-based functional acceleration stack targeting SaaS model is employed where functionality is offered to users as a service while the area scales with the associated throughput. This is similar to the Amazon model which provides AFIs containing pre-synthesised bitstreams of FPGA functionality while hiding the implementation details from users [1]. With CPM, the model can be extended to area shared multi-task execution with a higher system throughput facilitating higher revenue.

To enable the task-based model, we use the VineTalk virtualisation framework [12]. VineTalk reduces the efforts needed by application developers to deploy FPGA-based acceleration in data centres by handling the communication and control between an application and underlying accelerator. It does that by registering tasks as a set of supported functionalities accessible via easy to use interfaces while hiding the underlying hardware implementation details. It essentially exposes the FPGA as a virtual accelerator, VineAccelerator, available to applications as a task-based software API. The underlying libraries then manage the task queues and data buffers. Decoupled from this is the software controller and hardware facing API, which communicates with the underlying FPGA’s vendor runtime and manages bitstream loading and host-accelerator data transfers associated with each VineAccelerator.

We modify various layers to incorporate Vinetalk for area-shared CPM multi-task processing. At the application level, multiple tasks in a cluster can be integrated into a single call associated with a single VineAccelerator. In other words, each VineAccelerator represents a unique cluster and its definition then manages execution of independent tasks in a cluster. Furthermore, each VineAccelerator has access to multiple bitstreams, where each bitstream represents varying throughputs for each task in the cluster via an associated set of parameters. These are used by the scheduler to select the optimum bitstream at runtime and passed to the software controller. Finally, we implement interfaces and drivers for the hardware facing API for integration with Intel FPGAs.

SECTION 4Test Environment
We consider 11 HPC tasks belonging to various application domains and computing dwarfs such as sparse and dense linear algebra, graph analytics, structured grid computing and dynamic programming. We then identify high-level parameters for each task for DSE for OpenCL implementation. The parameters are evaluated to provide the maximum throughput per resource usage using hardware profiling. We also identify the key characteristics identified after profiling, that project a need for task characterisation on actual hardware and verify the selection of tasks for comprehensive evaluation. The DSE along with workload specific metrics used for each task in the throughput calculation, are summarised in Table 1.

4.1 FPGA and Host Platform
The runtime evaluation tool is written in Python 3. The high-level DSE is performed via the Intel OpenCL SDK for FPGAs v16.1 while constrained placement is achieved using Quartus Prime v16.1. The hardware profiling is performed for the Nallatech 385 board as the target FPGA system. The power is measured using on-board power sensors accessible via the memory-mapped device layer whilst the bandwidth is measured using the Intel FPGA Dynamic Profiler for OpenCL GUI. OpenCL runtime and independent command queues are used for each task, allowing parallel execution. Within a single command queue, non-blocking calls are issued for memory transfers as well as to multiple kernels of a single task, if needed. The runtime is executed on host comprising of an Intel Xeon E5530 chip running at 2.4 GHz.

4.2 System Throughput Metric
Assessing the system performance of a multi-task workload running in parallel on a single processing unit is challenging, as the absolute measure of individual task’s throughput does not provide an indication of system performance; the contribution to absolute processing time and total speedup may be influenced more by the tasks with larger workload sizes. Generic metrics such as FLOPS etc. may not provide a meaningful measure for all of the tasks being evaluated.

We use two different metrics for emulated and hardware results in order to allow a realistic and comprehensive assessment to be made. First, the emulation of large task queues comprising range of tasks provides the potential to estimate the total speedup, measured as execution time to process the whole workload or task queue for various partitioning schemes and SAC. To evaluate the compute density provided by various approaches in a multi-task environment, the STP metric [6] used is defined by
STP=∑i=1nNPi=∑i=1nCSPiCMPi,(2)
View Source

where NP is each task’s normalised progress defined by the number of clock cycles it takes in single task mode, CSPi, when the task has all of the resources of the FPGA available as compared to multi-task mode, CMPi, when it shares the space with other tasks. Here, n defines the number of tasks sharing the FPGA. The metric encompasses various system design parameters such as throughput variation with resource allocation per task, compute density variation against resource utilisation and system performance (including STP/Watt for energy efficiency) for various space partitioning schemes. It then provides a throughput relative to a baseline of SAC, which has an STP value of 1.

SECTION 5Results and Analysis
The scope of DSE is first explored with emulated results before analysis using clustering and runtime performance variation with design parameters. Whilst the results target the low-level system performance, we discuss the high-level virtualisation model using CPM.

5.1 Design Space Exploration
The DSE provides real area numbers as well as variation in throughput against resource utilisation for fair evaluation and comparison of mapping schemes. The scaled parameters and achieved speedup are summarised in Table 1. To measure speedup, the baseline Texec, corresponding to the lowest area bitstream, is defined by the serial pipelined benchmark implementation of the task. The maximum throughput is defined by the largest bitstream, limited by FPGA resources. We have generated 4−9 designs per task where each represents a point on the area-throughput curve and the maximum speedup ranges from 2× to 204× for tasks being considered.

5.2 PRR versus CPM
The evaluation tool provides the projected runtime gains of CPM against SAC and various PRR strategies. The tool also allows variation of system parameters and task constraints that affect the CPM throughput, particularly due to lower independence in time as compared to PRR and SAC. We iteratively apply different constraints to distinguish and highlight their effects while using all of the considered 11 heterogeneous tasks but with varying workloads.

Maximum Theoretical Gain by CPM. First, the maximum theoretical speedup achieved by CPM against various types of the PRR mapping, namely the continuous y-axis, heterogeneous PRRs and homogeneous PRRs, are analysed using a runtime evaluation tool. Here, we consider an ideal scenario for CPM where tasks sharing the space have same execution times. In total, there are 80 rows of the FPGA that can be configured as a single region or a set of two homogeneous regions of 40 rows each. Two more heterogeneous PRRs, namely 30 and 50 rows, are defined based on the sizes of generated bitstreams.

For CPM, we either use the same region as used for the PRR to maintain homogeneity, (Partial CPM) (P-CPM), or use all of the available area for the task logic after placement of the static modules, (Whole CPM) (W-CPM). The designs for W-CPM and P-CPM are created by using appropriate scaling parameters as well as the placement constraints for each realisation. Due to lower area utilisation, P-CPM takes lower bitstream generation time and has a slightly lower reconfiguration overhead. This analysis helps to differentiate between the speedup achieved by heterogeneous mapping in the same region, as well as the gains made by the availability of extra logic when mapping in a custom fashion.

Fig. 6 shows that for an ideal environment for CPM, it can achieve up to 4.1× higher throughput as compared to PRR, measured in terms of the total execution time for a set task queue size. Please note that out of this 4.1× gain, a 2× speedup is achieved via heterogeneous custom mapping whilst the rest is achieved by exploiting the higher resource availability. The results show that if the y-axis can be made continuous, a throughput gain of 1.8× can be achieved while heterogeneous PRRs can improve performance while making use of various optimisations mentioned in Section 3.


Fig. 6.
Speedup achieved by CPM versus PRR mapping.

Show All

Texec Variation. The speedup reported in Fig. 6 considers an ideal scenario for CPM by using similar Texec for all tasks sharing the FPGA at any time, however, this is not the case in dynamic environments. Next, the relative Texec of the tasks is varied, with reconfiguration only after all co-executed tasks have finished processing, thus allowing analysis of its effect on speedup. The speedup is given against baseline of SAC for both partitioning schemes.

The results in Fig. 7 depict a surprising trend, particularly for CPM versus PRR. Even with increasing range of Texec by up to 32× (beyond this range a reconfiguration overhead would become negligible for most tasks), the speedup with CPM decreases but remains higher than PRR by 2.7×. This is because on average, the device may be used by 3 or less tasks using CPM, as constrained by the size of the FPGA chip. Thus, a task may stall up to 2 tasks or a maximum of about 50 percent resources with an average much lower than that. Stalls by smaller tasks are overcome by the higher average compute density and gains made when the longest running task is not the smallest. Against SAC, the CPM follows a similar trend, however, the speedup falls below 1 as the range of Texec goes higher than 2 whilst PRR maintains an average speedup of 0.37×.


Fig. 7.
Speedup variation with variation in execution times (Tasks = 1024).

Show All

Reconfiguration Overhead. The analysis so far has not considered reconfiguration overhead, which can be significant if the tasks to be executed have smaller workloads in more dynamic environments. Furthermore, the reconfiguration overhead is directly related to the area being mapped. Thus, as the throughput increases with more resources when going from PRR to CPM and SAC, the gains may be offset by the higher reconfiguration overhead.

The next set of experiments evaluate the effect of reconfiguration overhead on total Texec to process a task queue against varying mean and range of Texec of the tasks. The experiments consider reconfiguration time for each scheme as proportional to the area to be reconfigured. The results are shown in Fig. 8 and include two different ranges (R) for each evaluated mean. Starting from the offset of reconfiguration overhead, the total Texec generally increases linearly with increase in mean Texec of tasks. However, the lower reconfiguration overhead plays a more significant role towards better performance for smaller tasks with lower mean Texec, while higher throughput is more significant for larger tasks.


Fig. 8.
Total execution time including the reconfiguration overhead for varying mean of tasks individual execution time (Tasks = 1024.

Show All

In the first test scenario, all tasks have similar Texec i.e range approaches 0 which, as we mentioned earlier, is an ideal scenario for CPM. However, even with the lowest throughput associated with PRR, it provides the best overall system performance by up to 1.2× owing to the lowest reconfiguration overhead. The lower throughput for PRR becomes the more significant factor towards total Texec with the increasing task size and the PRR becomes the worst performing for tasks taking more than 1 second per task.

The second set of experiments shows the benefits of space sharing when using an increased 8× range of Texec. Without considering reconfiguration overhead, CPM performed worse than SAC (Fig. 7). However, lower number of resources per task results in a lower reconfiguration overhead for CPM as compared to SAC. This results in CPM providing better overall performance by up to 1.14× even at a mean Texec of 10s. The overhead only offsets the performance loss due to lower throughput though, as CPM may perform worse than SAC with even higher range.

Although this work is focussed on identifying the use of CPM for higher throughput and optimising it for dynamic environments, the detailed comparison against various constraints in this section highlights the need for analysis of various schemes to suit the operating environment. Such analysis may also enable a system to switch mapping schemes at runtime as per variation in task dynamics, as proposed in [22].

As for the CPM, although it allows a space-shared model allowing scaling of resources at sub-device level, the degradation of performance against SAC suggested the need for further optimisations. A quick analysis also showed that apart from the underutilisation of resources in time by tasks with variable Texec, the runtime spatial utilisation by heterogeneous tasks is limited to 62, 49 and 71 percent on average for logic, BRAM and DSPs, respectively.

5.3 High Compute Density Mappings
To analyse the gains made by the proposed approaches for high density mappings, we first establish a baseline system throughput before analysing various optimisations.

5.3.1 Baseline System Throughput for CPM and PRR
The size of the device being used is small, whilst further constraints on area available are placed on it by PRR. This limits the area sharing to a cluster of 2 tasks. For CPM, up to 3 tasks can be accommodated at one time. However, based on practicalities and the need to keep the comparison fair, it makes sense to use 2 tasks per cluster for CPM as well. Using the DSE, the largest bitstreams per task are selected within the area constraints of the PRR and CPM.

We generate 10 random clusters of tasks for both PRR and CPM as a baseline. For evaluation of the STP, the data sizes for tasks in a cluster are chosen such that both tasks have a similar Texec. The results in Fig. 9a show that CPM with an average STP of 0.99 can provide an average 2.4× higher throughput as compared to PRR’s STP of 0.41 on the basis of a higher compute density in space but with consuming higher power. Fig. 9(a) shows that although the gain is less, the CPM provides 1.9× better energy efficiency (STP/W) on average over PRR.


Fig. 9.
STP and STP/W for CPM and PRR using: (a) unoptimised clustering, and (b) optimised clustering.

Show All

5.3.2 Clustering
Before describing the improved throughput due to clustering in Fig. 9(b), we briefly show the contribution of resources towards the tasks’ throughput using the DSE and Ridge Regression in Fig. 10, which forms the basis for clustering. The bias value in the figure representing the constant in the modelled linear equations relates to the baseline throughput. The clustering algorithm uses these models to create a set of 6 optimum clusters.


Fig. 10.
Ridge regression models for HPC tasks.

Show All

Fig. 9(b) shows gains for both the PRR and CPM, using a similar Texec for the tasks in a cluster. For PRR, the STP increase of 1.2× to 0.5 is mostly due to optimisation of the off-chip memory bandwidth utilisation. For CPM, the STP increase of 1.4× to new value of 1.4 corresponds to both the on- and off-chip resource optimisation. The gain of 3.3× and 2.8× for the throughput (STP) and energy efficiency (STP/W) between the CPM in Fig. 9(b) and the PRR in Fig. 9(a) respectively, is the maximum achievable via the proposed optimisations as compared to the existing area-shared schemes while an STP gain of 1.4× compared to SAC is achieved.

5.4 Texec Variation
Until now, the experiments have used custom data sizes which ensured a similar Texec for co-executed tasks which may represent a static configuration for long running tasks. However, for a more dynamic task queue, the limitation of CPM is that the execution is stalled by the longest running task, unless the scheduler decides to reconfigure the FPGA. In the next set of experiments, the Texec of tasks is varied relative to each other. The results are shown in Fig. 11 where the δTexec represents the difference in Texec for tasks in a cluster as a multiple.


Fig. 11.
STP variation for CPM with variation in Texec of tasks in cluster.

Show All

For each sample cluster, the middle cluster represents a similar Texec, that is δTexec≈1. Moving either side, the δTexec increases with the left side representing the first task in x-label taking the longer time to execute while the right side represents the second task, as shown by individual STP of tasks. Considering the average values, the results show that STP for CPM drops sharply initially but then stabilises. For example, from 1× to 2×, the drop is by 1.3× (from 1.36 to 1.04), however, from 4× to 8×, even when variation in Texec is 4× or more, the drop is only 1.06× (0.94 to 0.88).

The reason can be seen from the individual contribution of each task to the total STP. With an increase in Texec variation, the STP is increasingly defined by the longest running task and becomes independent of the variation in Texec. Furthermore, the individual STP contribution by the longest running task improves with increasing the Texec variation as it gets a higher share of the off-chip memory bandwidth. All of these factors reduce the effect of variation in Texec, but it still causes a significant drop in STP by 1.54×.

5.5 Runtime
To counter the drop in the STP with variation in Texec, the framework generates 3-4 bitstreams per cluster, with each bitstream trading off resources for each task against the other. We then evaluate a complete system using a queue of 36 randomly generated task workloads involving 3 workloads per task with variable sizes. The range of workload size per task is such that they take 1 - 60 mins (in line with study of real workloads processing times on three Google clusters in [36]) to process using bitstreams from Fig. 9(b). Each workload comprises a number of workload-specific blocks such as matrices, image frames, options, graphs, etc. Furthermore, for the sake of the experiments, each request is treated as an independent task and the module reuse strategy to avoid reconfiguration is not considered if the same task has multiple requests. If so, multiple requests for same task can be combined to form a single larger task.

The runtime scheduler then selects the optimum bitstream, that minimises the Texec variation in a cluster, using the profiled workload-specific metrics for throughput and the preemptive scheduling. As mentioned earlier, this estimation only chooses the best fit from the available set and does not require a cycle accurate estimation of Texec. The runtime can also use single task per bitstream (SAC), if needed. For the considered task queue, the scheduler reconfigured the bitstream at checkpoints only once while for 2 of the workloads, it used SAC.

The achieved instantaneous and average STP for 18 bitstreams of 2 tasks each used for execution of 36 tasks (SAC is also shown as a cluster, but represents two different bitstreams), is shown in Fig. 12. The figure also includes the corresponding Texec variation for bitstreams from Fig. 9(b), with an average of 9.6×. This set of experiments also includes the reconfiguration overhead, however, it is insignificant (less than 2s) for the size of workloads being considered. The results show that using the intelligent runtime selection of the bitstream, the STP can be improved by 1.3× as compared to Fig. 11, while reducing the power by 5 percent, on average. The actual processing time or STP for the PRR-based processing for the generated task queue could not be provided due to non-availability of the dynamic reconfiguration framework needed for the tasks with varying Texec. However, the STP would be similar to that reported in Section 5.3.2. The overall achieved STP is 2.8× higher than the base value using PRR in Fig. 9(a) while being 2.3× more energy efficient.


Fig. 12.
STP and STP/W for variable workloads via dynamic bitstream selection for 18 clusters that are used for execution of 36 tasks.

Show All

5.6 Discussion
The focus of this work has been to improve FPGA compute density and hence has not commented on the virtualisation overhead and the data transfer from the host to the FPGA via PCIe. Both the VineTalk and scheduler overhead depends on the workload size and number of batches required to complete processing. For a single batch, the combined overhead can be as low as 100μs, while excluding the reconfiguration overhead. For a higher number of batches (256), the overhead varies for different tasks and lies in the range of 0.3 - 0.9 percent of total execution time of tasks.

Furthermore, the cost of data transfer is essentially the same for all of the partitioning schemes. However, compared to SAC, multi-task processing provides up to 1.2× lower total Texec including the memory transfers from the host. In this, 1.18× is due to the higher compute density and is in the similar range as achieved STP for the set of evaluated workloads. The rest is provided by the time multiplexed memory transfers for multi-task processing, with the longer running task starting execution whilst the data is being transferred for other tasks.

The framework processes the task queue in batches where jobs in the batch may be reordered to maximise throughput. However, the order of jobs is the second priority and thus, the first task to be scheduled is selected from the order and the second task corresponds to the respective cluster. Using the runtime evaluation tool, it showed that although the PRR allows a strict order of processing to be followed; the lower compute density means that 100% of tasks finish execution later in terms of clock time compared to CPM. In a multi-FPGA data centre, multiple clusters can also be offloaded to different FPGAs to maintain adherence to the execution order. Tasks with strict deadlines can also be executed in SAC.

STP as a metric defines throughput as a comparison against SAC and the theoretical limit for maximum possible STP is defined by the number of tasks being shared i.e., 2 in this case. The evaluations show that the CPM provides up to a maximum of 1.4× improvement in the throughput which drops to 1.18× for the more dynamic and variable size workloads. This will improve with larger devices which will allow more space sharing than 2 tasks. PRR, however, this reduces the STP to 0.5×. There may be other benefits to using PRR particularly for more dynamic fine-grained workloads or faster integration of a new functionality. However, STP must be considered for evaluation of system performance and the CPM has the ability to provide a higher system throughput than SAC, while allowing benefits of area sharing. Even for larger workloads requiring multiple FPGAs, multiple instances can be generated on multiple FPGAs where each instance shares space with instances from other tasks to improve resource utilisation. Each instance may be treated as an independent sub-task by the scheduler to optimise temporal usage with changing throughput requirements from the main task.

SECTION 6Conclusion
A systematic framework is proposed for addressing the challenges of virtualisation based on space sharing of FPGAs and achieving higher system throughput. The framework proposes characterisation and clustering of tasks based on their heterogeneities in resource usage, which is then complemented by custom mapping and partitioning of tasks to maximise utilisation in space. A lightweight runtime scheduler integrated with a higher level virtualisation layer then makes use of off-line profiling of resource allocation variation per task to increase compute density in time. In doing so, the work projects the trade-offs of various space partitioning schemes, while improving the throughput and energy efficiency over existing methods.