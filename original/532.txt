Edge computing has been an efficient way to provide prompt and near-data computing services for resource-and-delay sensitive IoT applications via computation offloading. Effective computation offloading strategies need to comprehensively cope with several major issues, including 1) the allocation of dynamic communication and computational resources, 2) delay constraints of heterogeneous tasks, and 3) requirements for computationally inexpensive and distributed algorithms. However, most of the existing works mainly focus on part of these issues, which would not suffice to achieve expected performance in complex and practical scenarios. To tackle this challenge, in this paper, we systematically study a distributed computation offloading problem with delay constraints, where heterogeneous computational tasks require continually offloading to a set of edge servers via a limiting number of stochastic communication channels. The task offloading problem is formulated as a delay-constrained long-term stochastic optimization problem under unknown prior statistical knowledge. To solve this problem, we first provide a technical path to transform and decompose it into several slot-level sub-problems. Then, we devise a distributed online algorithm, namely TODG, to efficiently allocate resources and schedule offloading tasks. Further, we present a comprehensive analysis for TODG in terms of the optimality gap, the worst-case delay, and the impact of system parameters. Extensive simulation results demonstrate the effectiveness and efficiency of TODG.
Edge computing has been an efficient way to provide prompt and near-data computing services for resource-and-delay sensitive IoT applications via computation offloading. Effective computation offloading strategies need to comprehensively cope with several major issues, including 1) the allocation of dynamic communication and computational resources, 2) delay constraints of heterogeneous tasks, and 3) requirements for computationally inexpensive and distributed algorithms. However, most of the existing works mainly focus on part of these issues, which would not suffice to achieve expected performance in complex and practical scenarios. To tackle this challenge, in this paper, we systematically study a distributed computation offloading problem with delay constraints, where heterogeneous computational tasks require continually offloading to a set of edge servers via a limiting number of stochastic communication channels. The task offloading problem is formulated as a delay-constrained long-term stochastic optimization problem under unknown prior statistical knowledge. To solve this problem, we first provide a technical path to transform and decompose it into several slot-level sub-problems. Then, we devise a distributed online algorithm, namely TODG, to efficiently allocate resources and schedule offloading tasks. Further, we present a comprehensive analysis for TODG in terms of the optimality gap, the worst-case delay, and the impact of system parameters. Extensive simulation results demonstrate the effectiveness and efficiency of TODG.

SECTION 1Introduction
Due to the rapid development of wireless communications, mobile devices have become the information hub and accessing point to connect physical and cyber worlds. A large number of modern applications, such as activity recognition, interactive gaming, natural language processing, have been developed for mobile devices to provide intelligent and convenient services. However, these applications are usually computation-and-energy intensive and delay-sensitive. They are hardly executed on resource-constrained mobile devices and pose significant challenges in offloading them to the cloud with delay guarantees. To tackle these challenges, edge computing has been proposed as a promising solution to alleviate the computing burdens of mobile devices and reduce service delay [1]. It can leverage the computing capabilities of devices/infrastructures in the proximity of data sources to provide pervasive, prompt, and agile services via computation offloading at anytime and anywhere [2].

Nevertheless, the design of efficient computation offloading strategies in edge computing is a non-trivial task. Different from conventional cloud computing, where a device only needs to decide whether to offload its tasks to a cloud center, in the case of edge computing, the increase of user devices may complicate the offloading decisions, caused by the contention for the insufficient computational resources of each edge server [3]. If the computational resources of edge servers are not well coordinated for user devices, the performance would seriously degrade due to the overwhelmed offloading tasks. On the other hand, in edge computing, computation offloading must involve wireless communications between user devices and edge servers. The inherently limited and stochastic radio resources call for an effective radio resource allocation strategy; otherwise, the wireless network capacity may be quickly strained, causing low transmission efficiency and dissatisfaction with edge computing services [4]. Moreover, the delay guarantees of offloading tasks are essential for many applications, such as interactive gaming, object recognition, and rendering in smart driving [5]. However, the stochasticity of communication channels and computing power of edge servers make the delay control extremely difficult, especially for heterogeneous tasks with different delay requirements. In addition, in contrast to the centralized and powerful cloud server, edge servers are deployed in a distributed manner, each of which is often resource-limited and heterogeneous. Thus, it is of critical importance to develop distributed and computationally efficient algorithms for task offloading in the context of edge computing.

Recently we have witnessed significant progress in developing novel approaches to address the challenges in task offloading. In particular, there have been several works on various aspects, including designing energy-efficient offloading strategies [6], [7], [8], [9], jointly allocating communication and computation resources for performance improvement [10], [11], [12], [13], [14], lowering the response latency [5], [15], [16], and developing decentralized offloading methods [3], [17], [18], [19], [20], [21], [22]. However, most of these existing works mainly aim at tackling part of the aforementioned issues by weakening other restrictions. Therefore, we argue that the strategy, comprehensively taking the above issues into account, is a requisite for achieving effectual computation offloading in edge computing.

To bridge the gap, this paper systematically study a distributed task offloading problem with delay constraints in edge computing, where heterogeneous computational tasks (with different sizes, required resources, and response times) require continually offloading to a set of edge servers with different computing capabilities via a limiting number of random channels. Accordingly, we formulate the offloading problem as a delay-constrained long-term stochastic optimization problem under unknown prior statistical distributions. Clearly, it is quite tough to solve this stochastic optimization problem because of the inherent complexity of continually scheduling a large number of heterogeneous tasks and jointly allocating the communication and computational resources. To address this challenge, we first provide an approach to transform and decompose the original problem into three sub-problems. Then we develop an online algorithm, called TODG, solving these sub-problems in a distributed manner. In particular, by the “δ-periodic strategy”, TODG only needs to allocate channels every δ time slots, which can alleviate the computational cost during the system operation. We also provide a comprehensive performance analysis of TODG. It is demonstrated that TODG can achieve a trade-off between the near-optimal solutions and the computational cost. Besides, we rigorously show that TODG can well satisfy the delay constraints and quantify the impact of the delay requirements and the task buffer sizes on the system utility.

Our main contributions can be summarized as follows.

To the best of our knowledge, we are the first to systematically consider a distributed task offloading and resource allocation strategy for heterogeneous computational tasks with delay guarantees. We formulate the offloading problem as a delay-constrained long-term stochastic optimization problem under unknown prior statistical knowledge about the random task arrivals and the channel states as well as the computing power on edge servers.

We devise an online algorithm to solve the long-term stochastic optimization problem, namely TODG, which can be implemented in parallel among user devices and edge servers, and provide worst-case delay guarantees for all offloading tasks. In particular, we develop a δ-periodic strategy, enabling to carry out channel assignment every δ slots, which largely mitigates the computational cost and communication overhead induced by the complex computation in resource allocation.

We present a comprehensive analysis of the proposed algorithm. We characterize the optimality gap and the response latency, and quantify the impact of system parameters on the performance in terms of the buffer sizes, delay requirements, and the period of the δ-periodic strategy. Further, we provide extensive simulation results to showcase the efficacy of TODG.

The remainder of this paper is organized as follows. Section 2 briefly reviews the related work, and Section 2 introduces the system model and formulates the distributed task offload problem. We present the details of the proposed TODG algorithm in Section 4 and analyze the theoretical performance of TODG in Section 5. Finally, Section 6 shows the performance evaluation results, followed by a conclusion drawn in Section 7.

SECTION 2Related Work
As a key enabling technology, task offloading has attracted increasing research attention in edge computing recently [4]. Some early studies focus on making offloading decisions whether a mobile device should offload the task to an edge server or not [40], [41]. For example, Chen et al. [40] design a decentralized offloading game to make the offloading decision for minimizing the energy overhead. Dinh et al. [41] propose a computation offloading approach to determine the offloaded tasks and CPU frequency of a mobile device to minimize task execution time and energy consumption.

Some recent works study joint communication and computation resource allocation to improve the performance of task offloading from a system perspective [5], [10], [11], [12], [13], [14], [16], [37], [42], [43], [44]. Specifically, Ren et al. [42] propose a channel allocation and resource management approach to optimize offloading decisions and maximize the long-term network utility. Wang et al. [43] present a system-level resource management approach, including offloading decisions, channel allocation, and caching strategy, to maximize the network utility. However, all these works ignore the latency constraint in task offloading, which is significantly important for delay-sensitive applications and attracts increasing research attention [44]. To control the offloading latency, Kao et al. [23] propose a task partitioning method for one-task offloading, giving a near-optimal solution. Mao et al. [27] and Chen et al. [34] study the multitask offloading problems to optimize the execution delay and energy overhead. You et al. [22] design a threshold-based policy to manage offloading data volumes and channel access opportunities in a TDMA-based edge computing system. Alameddine et al. [33] design a joint computing resource allocation and task offloading approach, considering the heterogeneity in the requirements of the offloaded tasks. Leveraging SDN, Maswood et al. [35] present a cooperative three-layer fog-cloud computing framework to optimize the bandwidth cost and load balancing. Hekmati et al. [5] develops an energy-optimal task offloading algorithm, called OnOpt, which considers the stochastic wireless channels and exploits the Markov chain to obtain the optimal offloading decisions with hard deadline constraints. Wang et al. [16] focus on the task offloading problem in non-orthogonal multiple access (NOMA) based edge computing systems and propose an online-learning algorithm to determine the optimal task and subcarrier allocation decisions for minimizing the task execution delay.

Decentralized task offloading is another research trend in the study field of edge computing [3], [17], [18], [19], [20], [21], [22]. Jošilo et al. [3] propose an efficient decentralized algorithm for computing an equilibrium of task offloading game based on the variational inequality theory. Liu et al. [21] design a decentralized offloading algorithm based on Lyapunov optimization and primal-dual theory, which decomposes the original complex problem into a set of sub-problems that can be solved on a mobile device or an edge server separately. However, they only investigate the one-server system and aim at controlling the average delay instead of strict delay constraints. Based on the Stackelberg game theoretic approach, Zhang et al. [28] develop an iterative distributed offloading strategy for the hierarchical Vehicular Edge Computing system. Nevertheless, most of the existing distributed solutions are based on a strong assumption of fixed task volume or sufficient communication resources. In addition, by introducing a statistical computation and transmission model, Li et al. [37] propose a distributed task offloading algorithm to provide statistical delay guarantees with full consideration of stochastic communication resources. Different from one-round scheduling and statistical delay guarantees in [37], this paper focus on online offloading process with hard delay constraints.

We summarize the difference between this paper and the existing works in Table 1. Notably, this paper focuses on a more complex and practical scenario, where heterogeneous computational tasks with random arrivals require scheduling to different edge servers with varying delay constraints, and designs a distributed algorithm to optimize the long-term utility of the whole system.

TABLE 1 Comparison With Related Works

SECTION 3System Model
As illustrated in Fig. 1, we consider an edge computing system that operates in slotted time indexed by t∈{0,1,…}, where a set of heterogeneous user devices (denoted by N) can offload their computational tasks to the edge servers (denoted by M) via a limited number of communication channels (denoted by L) for faster response or lower energy consumption. In each time slot, the system allocates a certain number of channels and edge servers to a part of user devices, enabling them to offload the computational tasks from their local task buffers. Generally, the user devices can be classified into different types (denoted by K). The devices belonging to the same type will generate the same kind of tasks, such as image processing, video processing, etc. Accordingly, each edge server m∈M creates K virtual machines (VMs) corresponding to different types of tasks. Such that, the k-type tasks would be processed by the corresponding VM in the server. It is worth noting that in this paper, we do not assume any prior knowledge on the statistical distributions of all the stochastic variables (i.e., the task arrivals, the channel states, and the service rates on servers). In the following, we use superscript ‘u’ and ‘s’ to specify the variables on the user and server sides, respectively, and denote N=|N|, K=|K|, and L=|L| for simplicity. The key notations are summarized in Table 2.

TABLE 2 Key Notations
Table 2- 
Key Notations

Fig. 1.
Illustration of delay-constrained computation offloading for heterogeneous tasks with multiple channels and servers in edge computing.

Show All

3.1 Transmission Model
We let nk∈Nk denote the nth user device of the kth type, and L be a set of orthogonal channels that the user devices can dynamically access for task offloading in each time slot. Let a stochastic variable cnk,l,m(t) be the channel capacity of l between device nk and server m in t (i.e., the maximum amount of tasks that can be transmitted from nk to m by l), satisfying 0≤cnk,l,m(t)≤cmax for a positive constant cmax. A user device can access a channel at each slot to offload its tasks to a server or do nothing. Let a binary variable znk,l,m(t) denote nk's offloading strategy in t, i.e.,
znk,l,m(t)=0 or 1,(1)
View Sourcefor each k∈K, nk∈Nk, l∈L and m∈M. znk,l,m(t)=1 indicates that device nk can offload its tasks to server m via channel l; otherwise, znk,l,m(t)=0. Note that, a user device can only access one channel in a slot, so it follows that
∑l∈L∑m∈Mznk,l,m(t)≤1,(2)
View Sourcefor each k∈K and nk∈Nk. Further, to circumvent the interference, each channel can only be accessed by one device in the meanwhile, which implies that
∑k∈K∑nk∈Nk∑m∈Mznk,l,m(t)≤1.(3)
View SourceRight-click on figure for MathML and additional features.

Define snk(t) as the transmission rate for offloading tasks by device nk in slot t. Due to the power limitation, it is reasonable to assume that snk(t) is bounded, i.e.,
0≤snk(t)≤ξnk(t),(4)
View Sourcewhere 0≤ξnk(t)≤ξmaxnk is a bounded random variable over time slots representing the maximal amount of tasks that can be offloaded to the edge servers. Besides, snk(t) is also impacted by the channel state, captured by
snk(t)≤∑m∈M∑l∈Lznk,l,m(t)cnk,l,m(t),(5)
View Sourcewhich implies that snk(t) cannot exceed the channel capacity accessed by device nk in the current slot.

3.2 Task Buffer Model
Define stochastic variable aunk(t) as the amount of task arrivals on device nk in t, satisfying 0≤aunk(t)≤au,maxnk with au,maxnk>0. Due to the delay sensitivity and system stability, user devices can drop some out-of-date tasks, denoted by dunk(t), from its local task buffer, and we have
0≤dunk(t)≤du,maxnk,(6)
View Sourcewith a positive constant du,maxnk. Let Qunk(t) be the local task buffer of device nk with the following dynamics
Qunk(t+1)=max{Qunk(t)−snk(t)−dunk(t),0}+aunk(t),(7)
View Sourcewhere Qunk(0)=0.

We denote vm,k as the kth VM of server m, and asm,k(t) and um,k(t) as the amount of arriving and processed tasks on vm,k in slot t respectively. In particular, asm,k(t) can be expressed by
asm,k(t)≜∑nk∈Nk∑l∈Lznk,l,m(t)s^nk(t),(8)
View Sourcewhere s^nk(t)≜min{snk(t),Qunk(t} represents the actual departures of device nk in t. Due to the unpredictable states of edge servers, we assume that the processing rates of VMs are random. Let the bounded stochastic variable um,k(t) denote the amount of processed tasks on vm,k in t, and 0≤um,k(t)≤umaxm,k holds. Similarly, let dsm,k(t) denote the amount of dropped tasks on vm,k in slot t, satisfying
0≤dsm,k(t)≤ds,maxm,k,(9)
View Sourcewhere ds,maxm,k>0. Each vm,k also maintains a task buffer, denoted by Qsm,k(t), with the following queuing dynamics1
Qsm,k(t+1)= max{Qsm,k(t)−um,k(t)−dsm,k(t),0}+asm,k(t).(10)
View SourceFurther, since the task buffers in both devices and servers are finite, we impose the following constraints on Qunk(t) and Qsm,k(t) for each m∈M, k∈K, and nk∈Nk
Qunk(t)≤Qu,maxnk,Qsm,k(t)≤Qs,maxm,k,(11)
View Sourcewhere Qu,maxnk≥au,maxnk and Qs,maxm,k≥cmaxL are the buffer sizes for device nk and VM vm,k, respectively.

3.3 Delay-Constrained Model
hunk,t≜minT′{T′ ∣∣∣ ∑t′=tT′min{snk(t′)+dunk(t′),Qunk(t′)}output of Qunk in t′≥Qunk(t)+aunk(t), T′≥t}(12)
View SourceRight-click on figure for MathML and additional features.
hsnk,t≜minT′{T′ ∣∣∣ ∑t′=hunk,tT′min{um′,k(t′)+dsm′,k(t′),Qsm′,k(t′)}output of Qsm′,k in t′≥Qsm′,k(t)+asm′,k(t), T′≥hunk,t}(13)
View Source
For device nk, define hunk,t in (12) as the time slot when task aunk(t) leaves local task buffer Qunk. It indicates that in hunk,t, all the tasks before aunk(t) along with aunk(t) have just been offloaded or dropped. Thus, the queuing delay of aunk(t) in local device can be denoted as
τunk,t≜hunk,t−t+1.(14)
View SourceWhen aunk(t) is not dropped, we denote m′ as the server2, which aunk(t) is offloaded to, such that znk,l,m′(hunk,t)=13. Similar to hunk,t, define hsnk,t in (13) as the time slot in which task aunk(t) leaves the VM's buffer. Accordingly, the processing delay of aunk(t) on the server-side can be expressed as
τsnk,t≜{0hsnk,t−hunk,t, if aunk(t) is dropped locally,,otherwise.
View SourceRight-click on figure for MathML and additional features.There exists an acceptable worst-case delay for each device's tasks, denoted by τmaxnk, such that
τunk,t+τsnk,t≤τmaxnk, ∀t∈{0,1,…}.(15)
View SourceIt implies that each task should be executed before its deadline. Since the transmission delay can be seen as a constant, we neglect it for brevity.

3.4 Task Offloading Problem
The system's goal is to maximize the long-term utility among user devices while satisfying the worst-case delay constraints. To do so, we first define the average amount of arriving and dropped tasks on each user device below
a¯unk≜limT→∞1T∑t=0T−1E{aunk(t)},(16)
View Source
d¯unk≜limT→∞1T∑t=0T−1E{dunk(t)}.(17)
View SourceIn particular, we slightly abuse the notation and define dsnk(t) (distinct from dsm,k(t)) as the total amount of tasks of nk that have been offloaded to servers but are dropped in t. Similarly, we define the expected average of dsnk(t) below
d¯snk≜limT→∞1T∑t=0T−1E{dsnk(t)}.(18)
View SourceBased on that, we cast the task offloading problem as a long-term stochastic optimization problem, i.e.,
(P) max{d(t),z(t),s(t)}s.t. ∑k∈K∑nk∈Nkgnk(a¯unk−d¯unk−d¯snk),  (1)−(6),(9),(11)−(15),(19)
View Sourcewhere gnk(⋅) is a differentiable, concave, and non-decreasing utility function with a finite maximum first derivative denoted by βnk. We denote d(t)≜{dunk(t),dsm,k(t)∣m∈M,k∈K,nk∈Nk}, and the same applies to z(t) and s(t). This formulation aims to find good offloading and resource allocation policies that enable executing as many tasks as possible while satisfying the delay and stability constraints.

SECTION 4TODG: Distributed Task Offloading Scheme With Delay Guarantees
In this section, we develop an online algorithm for addressing the problem mentioned above in a distributed manner. We first transform original problem (19) into an easy-to-handle form, then decompose it into three slot-level sub-problems. After that, we provide approaches to solve the sub-problems in each time slot.

4.1 Problem Transformation and Decomposition
It is not easy to control dsnk(t) directly since tasks of different devices are mixed in the task buffers on servers. Thus, we derive the next lemma to decouple d¯snk from d¯unk and show (19) can be transformed equivalently to the following handy form with β=maxnkβnk.
max{d(t),z(t),s(t)}s.t. ∑k∈K∑nk∈Nkgnk(a¯unk−d¯unk) −β∑m∈M∑k∈Kd¯sm,k,  (1)−(6),(9),(11)−(15),(20)
View Source

Lemma 1.
If there exists an optimal solution {d∗(t),z∗(t), s∗(t)} of problem (20), it is also an optimal solution to problem (19).

Proof.
The proof is similar to [45]. We first show that by decoupling d¯snk from d¯unk and rewriting the objective function, the following problem is equivalent to original problem (19)
max{d(t),z(t),s(t)}s.t. ∑k∈K∑nk∈Nk(gnk(a¯unk−d¯unk)−βd¯snk),  (1)−(6),(9),(11)−(15),(21)
View Sourcewhere β≜maxnk{βnk}. It is easy to see that the optimal solution of problem (19) can be obtained at d¯snk=0 since tasks dropped from the edge server could just as easily have been dropped at the user side. On the other hand, since β is greater than or equal to the maximum derivative of gnk(⋅). Thus, for the objective (21), transmitting an extra unit of task (improving the utility gnk(a¯unk−d¯unk)) then dropping it on the server (leading to the βd¯snk penalties) is no better than dropping it directly on the user device. Thus, the above equivalent transformation holds.

Besides, from the definitions of dsnk(t) and dsm,k(t), we have ∑k∈K∑nk∈Nkdsnk(t)=∑m∈M∑k∈Kdsm,k(t), which implies ∑k∈K∑nk∈Nkd¯snk=∑m∈M∑k∈Kd¯sm,k. Combining these two conclusions, we complete the proof.

Moreover, the coupling of τunk,t and τsnk,t in (15) causes troubles in parallel computing between devices and servers. To handle the challenge, we introduce two types of virtual queues to decouple the control of τunk,t and τsnk,t and “relax” the delay constraints. More specifically, for each user nk and VM vm,k, we define delay state queues Zunk(t) and Zsm,k(t) to measure the delay in current task buffers as follows
Zunk(t+1)= max{Zunk(t)−snk(t)−dunk(t)+ζunk,0},(22)
View Source
Zsm,k(t+1)= max{Zsm,k(t)−um,k(t)−dsm,k(t)+ζsm,k,0},(23)
View Sourcewith parameters 0≤ζunk≤du,maxnk and 0≤ζsm,k≤ds,maxm,k. Intuitively, the arrivals ζunk and ζsm,k of Zunk(t) and Zsm,k(t) can be seen as penalties upon the tasks stuck in the task buffers at each slot. For example, if there is no departure of Qunk(t) in the current slot, i.e., snk(t)+dunk(t)=0, the delay of all tasks stored in Qunk(t) would increase by one slot. Accordingly, Zunk(t) will also increase by ζunk in slot t. Thus, a large backlog of Zunk(t) indicates that high latency happens in the current task queue Qunk(t). We demonstrate that the delay constraints can be well satisfied if appropriately selecting the parameters ζunk and ζsm,k in Lemma 2 after imposing the following assumption.

Assumption 1.
For any k∈K, nk∈Nk, and m∈M, dropping rates du,maxnk and ds,maxm,k are large enough such that
ds,maxm,k≥max{cmaxL,maxnk{2Qs,maxm,k/τmaxnk}},(24)
View Source
du,maxnk≥max⎧⎩⎨⎪⎪au,maxnk,2Qu,maxnkτmaxnk−maxm{2Qs,maxm,k/ds,maxm,k}⎫⎭⎬⎪⎪.(25)
View Source

Intuitively, Assumption 1 implies that even though the delay-constrained task arrivals outweighs the system's processing capacity, it is capable of dropping some tasks for system stability. It enables the system to stay in the solution space.

Lemma 2.
Suppose for each m∈M, k∈K, and nk∈Nk, Zunk(t) and Zsm,k(t) are bounded by
Zunk(t)≤Qu,maxnk,Zsm,k(t)≤Qs,maxm,k.(26)
View SourceRight-click on figure for MathML and additional features.Then, given Assumption 1, the delay constraint (15) can be satisfied if the following holds true for each m∈M, k∈K, and nk∈Nk
maxnk{2Qs,maxm,kτmaxnk}<ζsm,k,(27)
View SourceRight-click on figure for MathML and additional features.
2Qu,maxnkτmaxnk−maxm{2Qs,maxm,kζsm,k}≤ζunk.(28)
View Source

Proof.
For fixed ζunk and ζsm,k, we define the worst-case queuing delays on Qunk(t) and Qsm,k(t) are wunk and wsm,k respectively. Based on [46, Lemma 5.5], for each m∈M, k∈K, and nk∈Nk, it can be shown that
wunk=2Qu,maxnkζunk,wsm,k=2Qs,maxm,kζsm,k.(29)
View SourceTo satisfy (15), it requires that
⇔wunk+maxm{wsm,k}≤τmaxnk,2Qu,maxnkζunk+maxm{2Qs,maxm,kζsm,k}≤τmaxnk.(30)
View SourceDue to wunk,wsm,k>0, from (30), we have
2Qs,maxm,kζsm,k<τmaxnk, ∀m∈M,nk∈Nk,(31)
View Sourceby which we obtain (27). Plugging (27) into (30) and rearranging the terms, we complete the proof.

Remark
Lemma 2 implies that if we can bound Zunk(t)≤Qu,maxnk and Zsm,k(t)≤Qs,maxm,k, constraint (15) can be satisfied by setting sufficiently large ζsm,k and ζunk corresponding to (27)-(28). Besides, since Zunk(t) and Zsm,k(t) are bounded, the following holds
d¯unk≥ζunk−s¯unk≥ζunk−min{ξnk(t),cmax},(32)
View SourceRight-click on figure for MathML and additional features.
∑k,nkd¯snk=∑m,kd¯sm,k≥∑m,k(ζsm,k−u¯m,k).(33)
View SourceRight-click on figure for MathML and additional features.Therefore, although larger ζsm,k and ζunk result in lower delay for offloaded tasks (see (29)), inequalities (32)-(33) indicate that a further increase in ζsm,k and ζunk may cause an increase of the dropped tasks and lead to additional degradation of the system utility.

Based on Lemmas 1 and 2, original problem (19) can be transformed into the following one
(TP) max{d(t),z(t),s(t)}s.t. ∑k∈K∑nk∈Nkgnk(a¯unk−d¯unk) −β∑m∈M∑k∈Kd¯sm,k,  (1)−(6),(9),(11),(26).(34)
View Source

Next, we employ the dual-based drift-plus-penalty technique [47] to decompose problem (34) into slot-level sub-problems. Rather than optimizing the problem directly, the idea of this technique is to minimize the slot-level drift-plus-penalty function, composed by the one-slot utility function and the successive difference of the queue state measures (namely “Lyapunov drift”). By doing so, it enables maximizing the performance while implicitly controlling the system stability. More specifically, in each slot t, we define the drift-plus-penalty function Dϵ(t) as follows
Dϵ(t)≜−ϵ⋅(∑k∈K∑nk∈Nkgnk(aunk(t)−dunk(t))−∑m∈M∑k∈Kβdsm,k(t))+L(t+1)−L(t),(35)
View Sourcewhere ϵ≥0 is a weight parameter to balance the utility and latency, and L(t) is the Lyapunov function defined by
L(t)≜12∑k∈K∑nk∈Nk(Qunk(t)2+Zunk(t)2)12∑m∈M∑k∈K(Qsm,k(t)2+Zsm,k(t)2).(36)
View SourceHowever, minimizing Dϵ(t) directly is often computationally costly. A common alternative method is to minimize its upper bound, which is given by the following lemma.

Lemma 3.
Under any ϵ≥0, the upper bound of the drift-plus-penalty function Dϵ(t) can be expressed by
D^ϵ(t)≜C−ϵ⋅(∑k∈K∑nk∈Nkgnk(aunk(t)−dunk(t))−∑m∈M∑k∈Kβdsm,k(t))+∑k∈K∑nk∈Nk(Qunk(t)(aunk(t)−snk(t)−dunk(t))+Zunk(t)(ζunk−snk(t)−dunk(t)))+∑m∈M∑k∈K(Qsm,k(t)(asm,k(t)−um,k(t)−dsm,k(t))+Zsm,k(t)(ζsm,k−um,k(t)−dsm,k(t))),(37)
View Sourcewhere C is denoted by
C≜∑k∈K∑i∈Nk((au,maxnk)2+(ξmaxnk+du,maxnk)2)+∑m∈M∑k∈K((cmaxL)2+(umaxm,k+ds,maxm,k)2).(38)
View Source

Proof.
Note that for any Q≥0, b≥0, a≥0, we have
(max{Q−b,0}+a)2≤Q2+a2+b2+2Q(a−b).(39)
View SourceBased on (39), squaring the dynamics of Qunk(t) in (7) yields
≤(Qunk(t+1))2−(Qunk(t))2(au,maxnk)2+(ξmaxnk+du,maxnk)2+2Qunk(t)(aunk(t)−sunk(t)−dunk(t)).(40)
View SourceSimilarly, we have
≤(Qsm,k(t+1))2−(Qsm,k(t))2(cmaxL)2+(umaxm,k+ds,maxm,k)2+2Qsm,k(t)(asm,k(t)−um,k(t)−dem,k(t)).(41)
View SourceRight-click on figure for MathML and additional features.Squaring the dynamics of Zunk(t) in (22) and using the fact that max{a,0}2≤a2 and (ζunk(t)−snk(t)−dunk(t))2≤(ξmaxnk+du,maxnk)2, we have
≤(Zunk(t+1))2−(Zunk(t))2(ξmaxnk+du,maxnk)2+2Zunk(t)(ζunk(t)−sunk(t)−dunk(t)).(42)
View SourceRight-click on figure for MathML and additional features.Similar to (42), the following holds
≤(Zsm,k(t+1))2−(Zsm,k(t))2(umaxm,k+ds,maxm,k)2+2Zsm,k(t)(ζsm,k−um,k(t)−dsm,k(t)).(43)
View SourceRight-click on figure for MathML and additional features.Summing the squared differences in the queues yields the result.

Therefore, instead of optimizing long-term problem (34), we attempt to minimize the following dual problem in each slot
(DP) mind(t),z(t),s(t) D^ϵ(t),  s.t.  (1)−(6),(9).(44)
View SourceRight-click on figure for MathML and additional features.It is worth noting that we do not need to explicitly handle Constraints (11) and (26) in solving dual problem (44), simplifying the control process. Later, we will rigorously show that (11) and (26) can be satisfied by the proposed algorithm via properly setting weight parameter ϵ in Theorem 1.

Problem (44) can be decomposed into the following three sub-problems.

Dropping on user device. In each slot t, observing the current task arrivals, the buffer state, and the virtual queue backlog, user device nk decides the amount of dropped tasks (i.e., dunk(t)) via solving the following sub-problem
maxdunks.t. ϵ⋅gnk(aunk(t)−dunk)+(Qunk(t)+Zunk(t))dunk, 0≤dunk≤du,maxnk.(45)
View Source

Dropping on edge server. During slot t, based on the current buffer state and the virtual queue backlog, the amount of dropped tasks on VM vm,k (i.e., dsm,k(t)) depends on the solution of the sub-problem below
maxdsm,ks.t. (Qsm,k(t)+Zsm,k(t)−βϵ)dsm,k,  0≤dsm,k≤ds,maxm,k.(46)
View Source

Offloading decision. We solve the following sub-problem to obtain the offloading decision and the amount of transmitted tasks (i.e., snk and znk,l,m(t)) per slot, i.e.,
max{snk,znk,l,m}s.t. ∑k∈K∑nk∈Nk(Qunk(t)+Zunk(t))snk −∑m∈M∑k∈K∑nk∈Nk∑l∈LQsm,k(t)snkznk,l,m,  (1)−(5).(47)
View Source

4.2 Distributed Task Offloading With Delay Guarantees
This subsection provides a distributed Task Offloading with Delay Guarantees algorithm (TODG) to solve problem (34) by deriving the solutions to sub-problems (45)-(47). Note that (45) and (46) are both convex optimization problems, of which the closed-form expression can be easily found. However, solving problem (47) is a non-trivial task since (47) resembles a 3-dimensional matching problem, i.e., matching among user devices, channels and edge servers (see Fig. 1), which has been proven an NP-hard problem in the literature [48], [49]. Besides, the offloading decision variable znk,l,m(t) is also coupled with the amount of transmitted tasks snk(t), which exacerbates the complexity of the problem. Next, we provide the solutions to each sub-problem separately.

4.2.1 Dropping on User Device
Let du∗nk(t) denote the optimal solution of sub-problem (45) for each k∈K and nk∈Nk, then the expression of du∗nk(t) can be derived from the theorem below.

Lemma 4.
Suppose that gnk(0)=0. If there exists d0∈R such that −ϵ⋅g′(aunk(t)−d0)+Qunk(t)+Zunk(t)=0 holds, du∗nk(t) can be expressed by
du∗nk(t)=⎧⎩⎨⎪⎪[aunk(t)−g′−1nk(Qunk(t)+Zunk(t)ϵ)]du,maxnk0du,maxnk,ϵ>0,,ϵ=0,(48)
View SourceRight-click on figure for MathML and additional features.where g′nk(⋅) denotes the first-order derivative of gnk(⋅) and [x]ab≜min{max{x,b},a}. Otherwise, we have
du∗nk(t)=argmaxd∈{0,du,maxnk}{ϵ⋅gnk(aunk(t)−d)+(Qunk(t)+Zunk(t))⋅d}.(49)
View Source

Proof.
For convenience, in slot t, we denote
fnk(d)≜ϵ⋅gnk(aunk(t)−d)+(Qunk(t)+Zunk(t))d.(50)
View SourceWhen g′−1nk(⋅) is defined at (Qnk(t)+Znk(t))/V, we discuss the optimal solutions under two cases. If ϵ>0, the first-order stationary point d~nk(t) of fnk(⋅) is
d~nk(t)≜aunk(t)−g′−1nk(Qunk(t)+Zunk(t)ϵ).(51)
View SourceLet du∗nk(t) denote the optimal solution of sub-problem (45) for each k∈K and nk∈Nk, then the expression of du∗nk(t) can be derived from the theorem below.

Lemma 4 indicates that the optimal dunk(t) depends on the task queue and delay state queue backlogs. A small value of Qunk(t)+Zunk(t) implies that the available task buffer is sufficient, and the current latency is relatively low on the local device, so it is unnecessary to drop tasks. On the contrary, when Qunk(t)+Zunk(t) is large, it will be better to drop some outdated tasks for system stability and rapid response.

4.2.2 Dropping on Edge Server
Clearly, sub-problem (46) is a linear programming problem. For each m∈M and k∈K, let ds∗m,k(t) denote the optimal solution of (46), then we have
ds∗m,k(t)={ds,maxm,k0, if Qsm,k(t)+Zsm,k(t)>βϵ,, else.(52)
View SourceSimilar to Lemma 4, edge servers will dynamically adapt the dropping rates based on the current queue states.

4.2.3 Offloading Decision
Aiming at optimizing sub-problem (47), we first attempt to transform it into a bipartite matching problem. Then, we provide a decentralized method to resolve it efficiently. To begin with, we derive the following theorem to find the optimal amount of s from the local buffer.

Lemma 5.
For each k∈K, nk∈Nk, l∈L, and m∈M, let s∗nk(t) and z∗nk,l,m(t) denote the optimal solution of problem (47) in slot t. Then, the following holds
s∗nk(t)=∑m∈M∑l∈Lz∗nk,l,m(t)⋅min{ξnk(t),cnk,l,m(t)}.(53)
View Source

Proof.
Note that if user device nk is not selected to offload tasks in slot t, i.e., z∗nk,l,m(t)=0 for all l∈L, then s∗nk(t)=0 must hold. Besides, if there exists m′∈M and l′∈L such that z∗nk,l′,m′(t)=1, it is easy to see that Qunk(t)+Zunk(t)−Qsm′,k(t)>0; otherwise, z∗nk,l′,m′(t)=0 is a better or equal solution. Denote h(s,z) in slot t as
h(s,z)≜∑k∈K∑nk∈Nk(Qunk(t)+Zunk(t))snk−∑m∈M∑k∈K∑nk∈Nk∑l∈LQsm,k(t)snkznk,l,m.(54)
View SourceThen, we have ∂h(z∗(t),s(t))/∂s∗nk(t)>0. Combined with (4) and (5), the proof is completed.

Lemma 5 shows that if a user device is selected to offload tasks during a slot, it will transmit as many tasks as possible to the corresponding edge server from its local task buffer. Based on that, we can transform sub-problem (47) into the following one to obtain z∗nk,l,m(t), i.e.,
max{znk,l,m}s.t. ∑m∈M∑k∈K∑nk∈Nk∑l∈L(Qunk(t)+Zunk(t) −Qsm,k(t))min{ξnk(t),cnk,l,m(t)}⋅znk,l,m,  ⎧⎩⎨⎪⎪⎪⎪∑l∈L∑m∈Mznk,l,m≤1,∑k∈K∑nk∈Nk∑m∈Mznk,l,m≤1,znk,l,m=0 or 1.(55)
View SourceRight-click on figure for MathML and additional features.It is easy to see that the optimal solution to the above problem can be roughly seen as a “maximum weight matching” over N×L×M, with (Qunk(t)+Zunk(t)−Qsm,k(t))⋅min{ξnk(t),cnk,l,m(t)} being the weight of tuple (nk,l,m). Despite (55) resembling a 3-dimensional matching problem, we argue that it is equivalent to a bipartite matching in the next lemma.

Lemma 6.
In slot t, for each k∈K, nk∈Nk, and l∈L, if there exists mnk,l∈M such that z∗nk,l,mnk,l(t)=1, we have
mnk,l=argmaxm∈M{(Qunk(t)+Zunk(t)−Qsm,k(t))⋅min{ξnk(t),cnk,l,m(t)}}.(56)
View SourceRight-click on figure for MathML and additional features.

Proof.
Based on Lemma 5, the result can be easily obtained by contradiction.

Lemma 6 implies that the matching between user devices and channels suffices to determine the optimal solution of (55). That is, problem (55) can be rewritten as a maximum weight bipartite matching problem over the source set N and destination set L as follows
max{znk,l}s.t. ∑k∈K∑nk∈Nk∑l∈Lwnk,l(t)znk,l,  ⎧⎩⎨⎪⎪⎪⎪⎪⎪⎪⎪∑l∈Lznk,l≤1,∑k∈K∑nk∈Nkznk,l≤1,znk,l=0 or 1,k∈K,nk∈Nk,l∈L.(57)
View SourceRight-click on figure for MathML and additional features.The weight wnk,l(t) between node nk and l is denoted by
wnk,l(t)≜{ϕnk,l(t)−∞, if ϕnk,l(t)>0,, else,(58)
View Sourcewhere
ϕnk,l(t)≜(Qunk(t)+Zunk(t)−Qsmnk,l,k(t))⋅min{ξnk(t),cnk,l,mnk,l(t)}.(59)
View SourceBased on that, z∗nk,l,m(t) can be derived via the optimal solution z∗nk,l(t) of (57), i.e.,
z∗nk,l,m(t)={1,0, if z∗nk,l(t)=1 and m=mnk,l, else.(60)
View SourceNext, the key point is how to efficiently solve the maximum bipartite matching problem across time slots. One feasible solution is to employ the well-known Kuhn-Munkres method to find the maximum matching within O(max{N,L}3) iterations [50]. However, it is a computationally costly and highly centralized way, where all the information (including queue and channel states) requires sending to a central controller. Thus, it may be unworkable in some practical scenarios. Instead, building on the recent advance in multi-robot applications, we argue that the optimal matching can also be achieved in a distributed manner via the Multi-Robot Assignment algorithm [51]. More specifically, each user device can send its corresponding weight (i.e., wnk.l(t) in (58)) to only one of the connectable edge servers. After that, each edge server can carry out the local matching in parallel while exchanging necessary information with the adjacent servers as in [51]. Nevertheless, although it can alleviate the high computational complexity of the centralized methods via parallel computing, it may lead to relatively large communication cost, i.e., in the worst case requiring O(H⋅max{N,L}2) communication rounds, with H being the maximum hops among servers (see [51, Corollary 4] for more details).

Periodic Strategy. To tackle these issues, we provide a δ-periodic strategy to mitigate the computational and communication cost incurred by the Multi-Robot Assignment approach. That is, we allow only running the matching (60) every δ slots, i.e., in t∈Tδ≜{0,δ,2δ,…}; otherwise, we keep the channel allocation the same as that in the last slot. More specifically, in slot t∉Tδ:

The devices selected in the last slot occupy the same channels and only need to decide the target servers;

The devices not selected in the last slot remain idle for the current slot.

Algorithm 1. Online Task Offloading With Delay Guarantees Algorithm (TODG)
Input: ξunk(0), Qunk(0), Qum,k(0), Zunk(0), Zum,k(0), cnk,l,m(0) for all k∈K, nk∈Nk, l∈L, m∈M

Output: s(t), z(t), d(t) for all slot t

Select {ζunk}, {ζsm,k} and ϵ according to (28), (27), and (63) respectively;

for t=0 to T do

// User side

for user nk∈Nk for all k∈K do

if t∈Tδ then

Compute wnk,l(t) between user nk and channel l for all l∈L according to (58);

Send {wnk,l(t)} to one of the connectable servers and receive the channel allocation decision z∗nk,l(t);

end

Compute the offloading decision {znk,l,m(t)} according to (61);

if existing znk,l′,m′(t)=1 then

Send snk(t) tasks to server m′ via channel l′ according to (53);

end

Compute the amount of dropped tasks dunk(t) according to (48)-(49);

Update the task queue Qunk(t+1) and delay state queue Zunk(t+1) according to (7) and (22) respectively;

end

// Server side

if t∈Tδ then

Edge servers compute the optimal channel allocation decisions {z∗nk,l(t)} in a distributed manner with the Multi-Robot Assignment algorithm;

end

for server m∈M do

for VM k∈K do

Compute the amount of dropped tasks dsm,k(t) according to (52);

Update the task queue Qsm,k(t+1) and delay state queue Zsm,k(t+1) according to (10) and (23) respectively;

end

end

end

In a nutshell, the δ-periodic strategy computes the offloading decisions by
znk,l,m(t)=⎧⎩⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪z∗nk,l,m(t), z∗nk,l(t−1),  0, if t∈Tδ,else if m=m^nk,l(t)and m^nk,l(t)≠idle,else,(61)
View Sourcewhere z∗nk,l,m(t) is derived from (60), and mnk,l(t) is expressed by
m^nl,l(t)={mnk,l, idle, if ϕnk,l(t)>0,else.(62)
View SourceAlthough the devices still need to decide the target servers mnk,l(t) in each slot, the corresponding computational and communication cost is negligible, because Qsm,k(t) is a scalar, and the complexity of computing mnk,l(t) is only O(M). The idea behind the δ-periodic strategy is based upon the observation that the weights {wnk,l(t)} in (58), determined by the channel capacities and queue backlogs, generally would not change too sharply in adjacent slots. Thus, intuitively the error posed by the periodic strategy between the optimal solution of (47) is acceptable. In Section 5, we rigorously quantify the induced error and show the δ-periodic strategy can achieve a trade-off between the near-optimal system utility and the computational cost. It is worth noting that the periodic strategy also leads to a variant of the standard analytical techniques due to violating the optimality of the sub-problems’ solutions. The sketch map of the periodic strategy is provided in Fig. 2.

Combining the proposed solutions of the sub-problems, we summarize the details of TODG in Algorithm 1.

SECTION 5Performance Analysis
In this section, we analyze the performance of TODG. First, we establish the delay and system stability guarantees. Then, we characterize the optimality gap and study the impact of the system parameters on the performance.

We provide the following lemma to show how the parameters affect the system queue lengths.

5Lemma 7.
Suppose Assumption 1 holds. Given ϵ≥0, TODG can achieve Qunk(t)≤βϵ+au,maxnk and Qsm,k(t)≤βϵ+cmaxL for each k∈K, nk∈Nk, m∈M, and t∈N.

5Proof.
We first prove Qunk(t)≤βϵ+au,maxnk for all slots. It is easy to see that this holds for t=0 since Qunk(0)=0 for all k∈K and nk∈Nk. Suppose this is true for a particular time slot t. We show that it also holds for t+1. If Qunk(t)≤βϵ, Qunk(t+1)≤βϵ+au,maxnk holds because it can increase by at most au,maxnk in any time slot. If Qunk(t)>βϵ, based on Lemma 4 and Assumption 1, dunk(t)≥aunk(t). Hence, queue Qunk(t) cannot increase in the next time slot, i.e., Qunk(t+1)≤Qunk(t), thereby yielding the result.

Based on (52), similarly we have Qsm,k(t)≤βϵ+cmaxL for all slots. Thus, the proof is completed.

Lemma 7 indicates that the task queue sizes are bounded by ϵ, so the weight parameter can implicitly control task queues. Combined with Lemma 2, we derive the following theorem.

5Theorem 1.
Suppose that ζunk and ζsm,k satisfy (27)-(28), and the following holds
ϵ≤min{ϵu,ϵs}/β,(63)
View Sourcewhere ϵu and ϵs are denoted as
ϵu≜mink,nk{Qu,maxnk−max{au,maxnk,ζunk}},(64)
View Source
ϵs≜minm,k{Qs,maxm,k−max{cmaxL,ζsm,k}}.(65)
View SourceGiven Assumption 1, if for each m∈M, k∈K, and nk∈Nk, ζsm,k and ζunk satisfy (27) and (28) respectively, (11) and (15) hold during all slots.

5Proof.
Similar to the proof in Lemma 7, it is easy to show that Zunk(t)≤βϵ+ζunk and Zsm,k(t)≤βϵ+ζsm,k for each k∈K, nk∈Nk, m∈M, and t∈N. Based on (64) and (65), we have
Qunk(t)≤Qu,maxnk,Qsm,k(t)≤Qs,maxm,k,(66)
View SourceRight-click on figure for MathML and additional features.
Zunk(t)≤Qu,maxnk,Zsm,k(t)≤Qs,maxm,k.(67)
View SourceRight-click on figure for MathML and additional features.Combined with Lemma 2, we finish the proof.

Theorem 1 demonstrates that the buffer size constraints of and the task response time (i.e., (11) and (15)) can be well satisfied by appropriately selecting the values of parameters ζsm,k, ζunk, and ϵ for TODG. It is beneficial to implicitly handle the constraints on the worst-case delay, which is hard to satisfy via only slot-level decisions.

To analyze the optimality gap for TODG, we provide the following lemma to characterize the error induced by the δ-periodic strategy.

5Lemma 8.
For any p,q∈N and q<δ, under the δ-periodic strategy, the solution found by TODG in slot t satisfies that
D^(t)−D^∗(t)≤qG, if t=pδ+q,(68)
View SourceRight-click on figure for MathML and additional features.where D^∗(t) denotes the optimal value of dual problem (44), and G≜2cmax⋅maxk,nk{Qu,maxnk}⋅min{N,L}.

5Proof.
Because the optimal solutions of sub-problems (45) and (46) can be found, the following holds
=D^(t)−D^∗(t)∑m∈M∑k∈K∑nk∈Nk∑l∈L(Qunk(t)+Zunk(t)−Qsm,k(t))⋅min{ξnk(t),cnk,l,m(t)}⋅(z∗nk,l,m(t)−znk,l,m(t)).(69)
View SourceIf q=0, it is easy to see that D^(t+1)−D^∗(t+1)=0. Based on (61), |z∗nk,l,m(t)−znk,l,m(t)|=1 if and only if Qunk(t)+Zunk(t)−Qsm,k(t)>0. Let i^l∈N and i∗l∈N denote the device that is assigned with channel l by the δ-periodic and the optimal strategy in slot t respectively, i.e., zi^l,l,mi,l(t)=1 and z∗i∗l,l,mi,l(t)=1 (mi,l is defined in Lemma 6). Then, for q>0, (69) can be rewritten as
≤≤D^(t)−D^∗(t)∑l∈Lϕi∗l,l(t)−ϕi^l,l(t)2qcmax⋅maxk,nk{Qu,maxnk}⋅min{N,L},(70)
View Sourcethereby (68) holds.

Lemma 8 shows that the cumulative error in the decomposed sub-problems would linearly increase with the computation period δ. We note that the gap G in Lemma 8 is derived in a rare worst case, i.e., all the transmission rates of selected user devices suddenly become zero while the other devices enjoy the maximum channel capacity in the current slot. However, the channel and task queue states commonly do not fluctuate so sharply between adjacent slots. Thus, it is reasonable to expect that the error incurred by the δ-periodic strategy is often much smaller than the theoretical gap.

Based on Lemma 8, we are ready to establish the optimality gap for TODG.

5Theorem 2.
Let U¯opt and U¯ denote the objective values in (34) corresponding to the optimal and our solutions respectively. As in Theorem 1, suppose Assumption 1, and ϵ≤min{ϵu,ϵs}/β are satisfied and all the stochastic variables are independent and identically distributed (i.i.d) over time slots, then the following holds
U¯opt−U¯≤C+(δ−1)G/2ϵ,(71)
View Sourcewhere C is defined in (38).

5Proof.
Based on [46, Theorem 5.1], it is easy to show that for any fixed σ>0, there exists a stationary and randomized policy that can choose feasible control actions z~(t), s~(t), and d~(t) independent of current queue backlogs in each slot t, and satisfy that
∑k∈K∑nk∈Nkϵ⋅E{gnk(aunk(t)−d~unk(t))}−∑m∈M∑k∈Kβϵ⋅E{d~sm,k(t)}≥U¯opt−σ,(72)
View SourceRight-click on figure for MathML and additional features.
E{a~unk(t)}≤E{s~nk(t)}+E{d~unk(t)}+σ,(73)
View SourceRight-click on figure for MathML and additional features.
E{ζunk(t)}≤E{s~nk(t)}+E{d~unk(t)}+σ,(74)
View SourceRight-click on figure for MathML and additional features.
E{a~sm,k(t)}≤E{u~m,k(t)}+E{d~sm,k(t)}+σ,(75)
View SourceRight-click on figure for MathML and additional features.
E{ζsm,k(t)}≤E{u~m,k(t)}+E{d~sm,k(t)}+σ,(76)
View SourceRight-click on figure for MathML and additional features.for each k∈K, nk∈Nk, and m∈M. It should be note that in [46, Theorem 5.1], they define a set of auxiliary queues to obtain (72). However, because the feasible region of dunk(t) is deterministic, we can directly derive (72) via the stationary and randomized policy without additional virtual queues. Then, based on Lemma 8, combining (35), (37) and (72)-(76), and taking δ→∞, we have
E{L(t+1)−L(t)−ϵ⋅(∑k∈K∑nk∈Nkgnk(aunk(t)−dunk(t))−∑m∈M∑k∈Kβdsm,k(t))∣∣Θ(t)}≤C+qG−ϵ⋅U¯opt,(77)
View SourceRight-click on figure for MathML and additional features.for t=pδ+q as in Lemma 8, where Θ(t) denotes the current queue states. Taking expectations over Θ(t) on both sides of (77) and summing over t∈{0,…,pδ−1} yield
E{L(pδ−1)−L(0)}−∑t=0pδ−1∑k∈K∑nk∈Nkϵ⋅E{gnk(aunk(t)−dunk(t))}+∑t=0pδ−1∑m∈M∑k∈Kβϵ⋅E{dsm,k(t)}≤(C−ϵ⋅U¯opt)pδ+(δ−1)δpG/2.(78)
View SourceUsing the fact L(0)=0 and rearranging the terms, (78) can be written as
1pδ∑t=0pδ−1ϵ(∑k∈K∑nk∈NkE{gnk(aunk(t)−dunk(t))}−∑m∈M∑k∈KβE{dsm,k(t)})≥ϵ⋅U¯opt−(C+(δ−1)G2).(79)
View SourceRight-click on figure for MathML and additional features.Taking a limit as q→∞, and using Jensen's inequality, we have the following
∑k∈K∑nk∈Nkgnk(a¯unk−d¯unk)−β∑m∈M∑k∈Kd¯sm,k≥U¯opt−C+(δ−1)G/2ϵ,(80)
View Sourcethereby completing the proof.

Theorem 2 shows that the achievable system utility of TODG has a controllable gap regarding the period δ and the weight parameter ϵ between the optimal network utility. In particular, although a relatively large period δ can mitigate the high computational cost caused by the task scheduling and channel allocation, it may also result in performance degradation to the system utility. Further, combined with Lemmas 2 and 7, the weight parameter ϵ achieves a trade-off between latency and utility, i.e., a larger ϵ provides a lower optimality gap, but may increase the task queue sizes, thereby leading to a poor response time in average. Note that even though we derive the result of Theorem 2 in the i.i.d. case, it can be generalized to the non-ergodic case via [46, Theorem 4.13], which is outside the scope of this paper.

5Remark
Theorem 2 also quantifies the impact of the task buffer sizes and the delay requirements on system utility. It is not difficult to see that
ϵu>mink,nk{(1−2τmaxnk)Qu,maxnk}(a),(81)
View Source
ϵs>minm,k{(1−maxnk{2τmaxnk})Qs,maxm,k}(b).(82)
View SourceRight-click on figure for MathML and additional features.Due to the fact that τmaxnk≥2 for all k∈K and nk∈Nk, the following holds
Uopt−U∗<C+(δ−1)G/2min{(a),(b)},(83)
View SourceRight-click on figure for MathML and additional features.which implies that large task buffers may improve system utility, while small delay requirements may cause the opposite.

SECTION 6Performance Evaluation
In this section, we provide further insights into the algorithm performance via extensive simulation experiments.

Simulation Setup. We consider an edge computing system consisting of M=3 edge servers. Each edge server creates K=3 virtual machines (VMs), i.e., each server can serve K=3 heterogeneous types of computing requests (e.g., image processing, data compression, and mathematical calculation). The number of total user devices N is an integer from [3,100], with that of each type Nk being the same. We set the number of channels L as 20. We set the length of a time slot as 1 second. The transmission rate cnk,l,m(t) Mbps of channel l between device nk and edge server m in slot t is uniformly distributed between [0,1]. Task arrival rate aunk(t) Mbps of nk across time slots is drawn from continuous uniform distributions. Different types of devices have varying statistic characteristics, i.e., the arrival rates of the three types of tasks follow U(0.8,1), U(0.4,0.6), and U(0,0.4), respectively. Note that the proposed algorithm can offer an efficient solution without any prior knowledge about the stochastic processes. that is, these random variables can have other different stochastic characteristics without affecting the performance of our algorithm. We let the processing rate um,k(t) of VM vm,k follow U(0,3). The parameters’ details are summarized in Table 3.

TABLE 3 Parameters in Simulation
Table 3- 
Parameters in Simulation
Baselines. We compare our proposed algorithm (TODG) with a centralized stochastic control algorithm (SCA) proposed in [44] for heterogeneous task offloading. We also consider a greedy control algorithm (GA), which selects min{N,L} user devices in each slot with the shortest task queues. Each user will send as many tasks as possible to the edge server with a minimal task backlog. Due to the lack of channel allocation mechanisms in SCA and GA, we set that they randomly assign channels for the selected user devices.

Implementation. We implement the code in MATLAB R2019b on a server with two Intel® Xeon® Golden 5120 CPUs and one Nvidia® Tesla-V100 32G GPU.

System Utility and Delay Under Different δ and ϵ. To validate the achievable system utility as demonstrated in (83), we fix N=45 and run the experiments under different periods δ and weight parameters ϵ. We repeat the experiments ten times and plot the results in Fig. 3. It can be seen from Fig. 3a that the achieved system utility by TODG becomes larger as ϵ increases. More specifically, the system utility increases sharply with the increase of ϵ at the beginning, and then the increasing speed decreases when ϵ gets large. The underlying rationale is that the lower bound of the system utility is a concave function with respect to ϵ, as shown in (71). However, Fig. 3c also illustrates that the response delay increases linearly with ϵ, so the weight parameter ϵ enables a trade-off between the utility and latency. On the other hand, we study the impact of period δ on performance. Combining Figs. 3a and 3b, it is easy to see large δ would lead to a utility degradation, especially for a small ϵ value, which indicates the correctness of the analytical results in Theorem 2. Besides, Figs. 3b and 3b also imply that the decreasing rate of the utility induced by δ-periodic strategy is relatively low to δ, while the delay can remain stable as δ increases. For example, even though we make the optimal offloading decisions every 15 slots, the utility only decreases to 9.46, and the average maximal delay is 77.2. By comparison, the maximal utility and delay achieved by SCA are 9.28 and 223 in this setting, as shown in Fig. 5. Therefore, the δ-periodic offloading strategy is expected to reach a high system utility with delay guarantees and inexpensive computational cost.


Fig. 2.
Periodic strategy. Here, q is a non-negative integer. The δ-periodic strategy enables the system to carry out channel allocation every δ slots. Within a period, each device occupying a channel only needs to decide the transmission rate and target server.

Show All

Fig. 3. - 
System utility and delay under different $\delta$δ and $\epsilon$ε.
Fig. 3.
System utility and delay under different δ and ϵ.

Show All

Fig. 4. - 
System stability and delay requirements.
Fig. 4.
System stability and delay requirements.

Show All

Fig. 5. - 
Comparison of utility and delay among algorithms under different buffer sizes.
Fig. 5.
Comparison of utility and delay among algorithms under different buffer sizes.

Show All

System Stability and Delay Requirements. Figs. 4a and 4b show the dynamics of the task queues on user devices and edge servers under different task buffer sizes over 2000 slots. As illustrated in Figs. 4a and 4b, under fixed buffers, TODG can well guarantee the system stability, and the task backlogs are kept at a low level to provide a short response latency. We also evaluate the impact of the delay constraints on the system utility. For ease of exposition, we set the delay constraints to be the same among user devices, i.e., τmaxn=τmax for all n∈N. It can be easily seen from Fig. 4c, strict delay requirements would lead to a utility degradation. The reason is that if the delay requirements outweigh the scheduling and processing capability of the system, it will drop the outdated tasks, which will exacerbate the degradation when the delay constraints are tight. In addition, we study the impact of the penalty parameter ζ on performance. Similarly, we set all ζun=ζsm,k=ζ for convenience. As shown in Fig. 4d, with the increase of ζ, we obtain a lower response delay but also lead to a worse system utility, which validates the results of Lemma 2 and Remark 4.1. Meanwhile, it is not difficult to see that the role of ζ is opposite to ϵ. Actually, their relationship has been given by Theorems 1 and 2.

Comparison of Performance Among Different Algorithms. In order to compare the performance among TODG and baseline algorithms, we vary the task buffer sizes on both user devices and edge servers and show the corresponding system utility and task maximal response delay in Fig. 5. As illustrated in Fig. 5, TODG outperforms SCA and GA with fixed buffer sizes while significantly reducing the response delay. The reason is that TODG enables more effectively exploiting the stochastic features of communication resources and computational capabilities on edge servers. Meanwhile, by jointly scheduling different types of tasks, TODG will prioritize the tasks with high delay requirements. Moreover, it can be seen from Fig. 5a that large buffer sizes can bring a performance increase of TODG since it offers more flexibility for task scheduling. However, due to the lack of effective scheduling mechanisms to reduce response delay, SCA and GA can only achieve limited system utility with larger task buffers, resulting in worse latency caused by the longer queuing time.

Scalability of TODG. To evaluate the scalability of TODG, we vary the number of servers M from 3 to 30 and task types K from 3 to 9, while fixing the number of each type's devices as Nk=10 and channels as L=20. As Fig. 6 shows, with the increase of M, TODG outperforms the baseline algorithms, and the gap becomes larger. It indicates that TODG can make full use of the insufficient communication bandwidth and limited computational resources, especially for large-scale networks. Besides, even with sufficient edge servers, GA and SCA hardly improve their performance, because it is bottlenecked by the limited channel capacities (see Fig. 8c for more details).


Fig. 6.
Comparison of utility under different network scales.

Show All

Fig. 7. - 
Scalability of TODG.
Fig. 7.
Scalability of TODG.

Show All

Fig. 8. - 
Comparison of system utility among algorithms under different parameters.
Fig. 8.
Comparison of system utility among algorithms under different parameters.

Show All

As illustrated in Fig. 7, the utility achieved by TODG first increases sharply with larger M, then the increasing rate slows down. The reason is that a certain number of edge servers suffice to provide computing resources for the local tasks. In addition, fixing δ=1, we report TODG's average running time in each slot under different N, K and M in Fig. 7. Recall that N=10K in this series of experiments. The results validate the theoretical computation complexity of TODG, which is provided in Sec. 4.2.3. Notably, due to the periodic strategy, we do not need to carry out channel assignment in each slot, and thus the running time can be significantly reduced.

Impact of Parameters. We further study the impact of different parameters on the system utility. We first run the simulation experiments under varying numbers of user devices. As illustrated in Fig. 8a, TODG achieves better performance with more user devices. On the contrary, since SCA and GA cannot exploit the limited and stochastic communication and computational resources, the contention among user devices would hinder the further improvement of system utility. Especially for GA, the contention even causes a performance decrease because it neglects the fairness among devices. Then, we vary the number of task types K from 3 to 30, while fixing the number of each type's devices as Nk=10 and channels as L=20. For convenience, let the task arrival of each type and the processing rate of each VM follow U(0,1) and U(0,3), respectively. As Fig. 8b shows, with the increase of K, TODG outperforms the baseline algorithms, and the gap becomes larger. After that, we evaluate the performance under different transmission rates and plot the results in Fig. 8c. As shown in Fig. 8c, TODG substantially outperforms the baseline algorithms in the cases of poor channel capacities, which indicates that TODG can fully utilize the limited communication resources. Besides, we vary the processing capabilities of edge servers to show the corresponding impact on the performance. It can be seen from Fig. 8d that TODG can vastly improve the performance of SCA and GA, especially with powerful edge servers, which implies the importance of effective task scheduling and channel allocation. Due to the inefficient utilization of communication resources, a large number of tasks cannot be transmitted to edge servers timely. Thus, despite more powerful servers, the system utility of SCA and GA cannot be further improved. In contrast to SCA and GA, since TODG enables exploiting the communication resources, the bottleneck of TODG is the processing capabilities on edge servers. After eliminating this limitation, TODG shows its great advantages of effective task scheduling.

SECTION 7Conclusion
In this paper, we have proposed a distributed online task offloading algorithm, called TODG, which jointly allocates resources and schedules the offloading tasks with delay guarantees while also achieving inexpensive computational cost. We further provide comprehensive theoretical insights into TODG and particularly show it can balance the near-optimal system utility and computational complexity. Extensive simulation results validate the effectiveness of TODG and demonstrate that TODG outperforms the baseline algorithms, especially in the cases with poor channel conditions. There are many interesting directions for future work. First, it is of interest to consider the task migration problem in high mobility scenarios into TODG. Second, our simulation results indicate that the optimality gap is much smaller than the theoretical bound. It is intriguing to get a more deep understanding of this phenomenon. Moreover, it remains largely open to incorporate the learning methods (e.g., online learning) into the task scheduling for edge computing.
