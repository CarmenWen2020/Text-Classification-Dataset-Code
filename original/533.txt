In Mobile Edge Computing (MEC), many tasks require specific service support for execution and in addition, have a dependent order of execution among the tasks. However, previous works often ignore the impact of having limited services cached at the edge nodes on (dependent) task offloading, thus may lead to an infeasible offloading decision or a longer completion time. To bridge the gap, this article studies how to efficiently offload dependent tasks to edge nodes with limited (and predetermined) service caching. We formally define the problem of offloading dependent tasks with service caching (ODT-SC), and prove that there exists no algorithm with constant approximation for this hard problem. Then, we design an efficient convex programming based algorithm (CP) to solve this problem. Moreover, we study a special case with a homogeneous MEC and propose a favorite successor based algorithm (FS) to solve this special case with a competitive ratio of O(1)O(1)<; inline-graphic xlink:href="zhao-ieq1-3076687.gif"/>. Extensive simulation results using Google data traces show that our proposed algorithms can significantly reduce applications' completion time by about 21-47 percent compared with other alternatives.

SECTION 1Introduction
The Internet of Things and widespread use of mobile devices are driving the development of many delay-sensitive and resource-intensive applications, such as virtual/augmented reality, face recognition and data stream processing [2], [3], [4]. Currently, these applications are processed or performed on either mobile devices or on a cloud platform. On one hand, mobile devices have too little computational resource for many applications [5]. On the other hand, running resource-intensive applications on a cloud platform often requires massive data be transferred between mobile devices and remote servers in the cloud, leading to unpredictable communication delay [6] [7]. As a result, Mobile Edge Computing (MEC) has emerged as a promising solution to overcome the above disadvantages [8], [9], [10], [11], [12].

However, there still exist many challenges in MEC. We take the face recognition application as an example. Basically, a face recognition application can be divided into five dependent tasks: object acquisition, face detection, preprocessing, feature extraction and classification [13]. When these tasks are offloaded onto edge nodes, we need to take the following factors into considerations.

Service caching. Task execution may require the support of specific services. That means tasks can only be offloaded onto edge nodes configured with corresponding services. For example, tasks “feature extraction” can only be offloaded onto the edge nodes configured with trained machine learning model.

Dependency. There may be dependencies between tasks. For example, the output of task “feature extraction” is the input of task “classification”. Thus, task “classification” can start only if task “feature extraction” has completed.

Actually, both service caching and dependency will impact the feasibility and performance of task offloading [12]. If we do not consider service caching or dependency when offloading these tasks, the applications may not be performed successfully [3], [14]. Existing works on service caching often focus on the problem of joint optimization of service placement and task offloading in MEC [3], [5], [15], [16]. In fact, service placement/update may incur higher operation cost than task execution, and hurt the system stability [16]. For example, object database and trained machine learning models require a non-trivial amount of data and are time-consuming if we migrate these services [16]. Thus, service placement often occurs at long-term time scale. If we jointly update the service placement and task offloading at long-term time scale (e.g.,[5], [15]), due to task dynamics and uncertainty [17], [18], the offloading solutions may lead to computation congestion on some edge nodes. Different from the previous works [3], [5], [15], [16], we assume that services have been placed/cached on edge nodes according to the existing methods such as [5], [15]. We will consider the impact of service caching on the applications’ performance (e.g., the completion time) of dynamic task offloading.

Due to the limited memory resource, only a subset of services can be cached on an edge node [2]. The status of service caching (i.e., where the services are hosted) will influence the decisions of task offloading. We give an example as shown in Fig. 1. Three tasks need to be offloaded onto two edge nodes. Task 1 must finish and send corresponding data to task 2 before task 2 can start. Task 3 is independent of tasks 1 and 2. For simplicity, the processing delay for any task on any node edge is set as 1 and the communication delay is set as 0.5 for data transmission from task 1 to task 2 if these two tasks are offloaded onto different edge nodes. If we do not consider the constraint of service caching, the offloading solution with the shortest completion time is shown in the left plot of Fig. 1 and the total task completion time (referred to as makespan hereafter) is 2. However, if only edge node 1 caches required services for task 1 and only edge node 2 caches required services for task 2, the offloading strategy of the left plot is infeasible/inefficient due to edge node 1 does not cache the services required by task 2. In this case, the feasible solution is shown in the right plot of Fig. 1 and the makespan is 2.5. Thus, this paper focuses on offloading dependent tasks with service caching.


Fig. 1.
A Motivation example. We assume that three tasks need to be offloaded as illustrated in the top plot. The optimal offloading solution is shown in the left plot without considering service caching. The right plot is the offloading solution if only edge node 1 caches services for task 1 and only edge node 2 caches services for task 2.

Show All

We should note that most of the previous work that considered task dependency did not consider the impact of service caching on the task offloading [14], [19], [20], and thus can not be directly applied to the case studied in this work. Work [21] only considered a single edge node that assists a user in executing a sequence of dependent tasks with service caching, so it is difficult to apply to the scenarios with multiple edge nodes. The most related work is GenDoc [22], which jointly considered the problem of dependent task offloading and service caching placement with the objective of application completion time minimization. However, GenDoc does not consider the computing resource constraints on edge nodes when offloading tasks to edge nodes. In fact, mobile edge nodes are resource-sensitive and GenDoc may cause irrational use of limited computing resources.

The main contributions of this paper are as follows:

We formally define the problem of offloading dependent tasks in MEC while considering service caching (ODT-SC), and prove its NP-hardness. We also analyze that the ODT-SC problem cannot be solved using a constant approximation algorithm in polynomial time.

We present a convex programming based algorithm, called CP, for the ODT-SC problem (e.g., for heterogeneous scenarios). CP transforms this problem into a convex optimization problem and offloads tasks according to the solution of this convex optimization.

We design a favorite successor based algorithm, called FS, for the special case (i.e., homogeneous edge nodes), and prove that FS can achieve an approximation ratio of O(1).

We conduct extensive simulations using real-world applications (from [23]) and data traces (from [24]) to show that CP and FS help reduce applications’ completion time by about 21-47 percent compared with other alternatives.

The rest of this paper is organized as follows. Section 2 discusses the related works. Section 3 defines the problem of offloading dependent tasks in MEC while considering service caching and proves that there exists no approximation algorithm with a constant factor for this hard problem. In Section 4, we propose an efficient convex programming based algorithm to solve this problem, called CP. Section 5 focuses on the special case of homogeneous edge nodes and proposes an approximation algorithm with bounded approximation ratio. The simulation results are presented in Section 6. We conclude the paper in Section 7.

SECTION 2Related Works
The emergence of resource-consuming and delay-sensitive mobile applications, such as 3-D games, augmented reality, and autonomous driving, has spurred a growing need for low-delay access to computing resources [8]. To address these challenges, mobile edge computing (MEC), envisioned as a new computing paradigm, has received an increasing amount of attentions in recent years [2], [8], [9], [10], [11].

In MEC, task offloading is the main research issue in recent years due to its necessarity and importance [8]. Mao et al. [10] proposed a low-complexity algorithm to minimize the weighted makespan of multiple independent tasks through jointly optimization of tasks offloading and resources allocation. Tong et al. [11] proposed an edge computing architecture according to the distance between the edge nodes and users, and designed an optimal offloading scheme for minimizing the makespan by using a workload placement algorithm. Many works also devoted to minimizing the overall cost of task offloading [25], [26]. Neto et al. [25] proposed a lightweight mobile computation offloading framework to minimize overall execution overhead. Huang et al. [26] designed a Deep-Q Network based task offloading and resource allocation algorithm to minimize the overall offloading cost in terms of computation cost, energy cost, and delay cost.

The above works assumed that the edge nodes could process any tasks in the system. However, as the mobile applications become increasingly complicated and require the support of various services, the tasks can be processed by these edge nodes with the required services. As a result, since edge nodes cannot be equipped with all services due to limited memory and computing resource constraints [3], [16], these works can not be applied directly to scenarios with service-aware tasks. This challenge can be solved by considering service caching conditions. Existing works on service caching often focus on the problem of service placement or joint optimization of service placement and task offloading [3], [5], [15], [16]. In fact, service placement/update may incur higher operation cost than task execution, and hurt the system stability [16]. Thus, service placement often occurs at long-term time scale. On the contrary, tasks offloading often occurs at short-term time scale due to task dynamics and uncertainties [17], [18]. That means, making placement/offloading decisions simultaneously may decrease system stability and increase operational cost. Based on this consideration, Farhadi et al. [16] separated the time scales of service placement and request scheduling, and proposed a two-time-scale solution for joint optimization of service placement and request scheduling under storage, communication, computation, and budget constraints.

All aforementioned works focus on offloading independent tasks. As modern applications in MEC become increasingly complex, a mobile application may consist of a number of dependent tasks. Thus, offloading dependent tasks is necessary for many practical applications in MEC. Since it is complex by considering precedence constraints and data transfer requirement in MEC, only some works focus on the dependent task offloading problem in MEC such as [14], [19], [20], [21]. Sundar et al. [14] proposed a heuristic algorithm to schedule dependent tasks with the objective of overall application execution cost minimization while considering application completion deadline constraints. Hermes et al. [19] designed a polynomial time approximation algorithm for offloading dependent tasks to minimize the makespan under resource constraints. Fan et al. [20] studied the dependent task offloading problem to minimize the overall cost of all applications with each application’s completion time constraints. However, these works do not consider the service caching constraints at the same time.

In fact, many tasks may pose a dependent execution order and in addition, require service support for execution. Thus, considering both service caching and tasks dependency is significant for task offloading. To the best of our knowledge, only a few works considered both two constraints. GenDoc [22] jointly considered the problem of dependent task offloading and service caching placement with the objective of application completion time minimization. However, GenDoc does not consider the processing resource constraints, which may cause irrational use of limited processing resources.

SECTION 3Preliminaries and Problem Definition
In this section, we first introduce the system model, including task dependency model and network model. We then formally define the dependent tasks offloading with service caching (ODT-SC) problem, and prove there exists no constant approximation algorithm for this problem.

3.1 System Model
Task Dependency Model. We assume that one or several application(s) (e.g., face recognition, virtual reality) need to be executed at some point in time. These application(s) can be divided into many tasks, each of which can only be executed by one edge node. For each local device that contains task(s), we insert a dummy task as the precedent task of all tasks on this local device. The dummy task must be executed on this local device and the execution time is zero. Note that, task execution may require the support of various resources (e.g., storage, CPU, network I/O) and corresponding services (e.g., machine learning model) [27]. We can restrict the required services so that the dummy task can only be executed on the local device where it is located.

We use V={v1,v2,…,vn} to denote the set of tasks (including dummy tasks), where n=|V| is the number of tasks. According to Alibaba’s data of 4 million applications [22], [28], more than 75 percent of the applications consist of dependent tasks. That is, modern mobile applications usually contain multiple dependent tasks [14]. For example, a face recognition application can be divided into five dependent tasks: object acquisition, face detection, preprocessing, feature extraction and classification [13]. Given the precedence constraints among these tasks, we use a directed acyclic graph (DAG) G=(V,E) to denote the dependency among tasks, where V denotes the task set and E is the set of edges representing the precedence constraints. More specifically, there is an edge from task v to task v′ if and only if there exists data transmission from task v to task v′ (i.e., task v′ can start only if task v is completed and the corresponding data is transmitted to task v′). We use the parameter avv′ to denote the amount of data that are required to be transferred from task v to task v′. Besides, a sink node of the DAG is a node such that no edge emerges out of it.

Network Model. A typical MEC network contains a set of edge nodes, a remote cloud node and a set of local devices. The cloud node has powerful processing capacity and can cache all services, but it is far away from edge users (local devices), which means that the communication delay of transferring tasks from local devices to the cloud is large. On the contrary, the processing capacity of local devices is weak and only a few services can be cached due to the memory size constraint, but tasks can be executed directly on local devices, i.e., the communication delay can be ignored. For ease of description, we can regard local devices as special edge nodes with low processing capacity, and the cloud as a special edge node with a long transmission distance. We use set M={m1,m2,…,ml} to represent these execution nodes (including the cloud node and local devices), where l=|M| denotes the number of nodes. These nodes interconnect with each other through various network connections (e.g., local-area network [29]). The communication delay per unit data from nodes m to m′ is denoted by cmm′ (cmm′=0 if m=m′). In this way, if a task is offloaded to an edge node or the remote cloud, we can use the communication delay between this task and the corresponding dummy task to represent the offloading delay of this task.

In MEC, on the one hand, service caching on edge nodes will consume various resources of edge nodes, such as storage and computing resources. On the other hand, compared with the remote cloud, the storage and computing resources of edge nodes are relatively small. For example, the storage space of a small mobile base station is about 200GB, and the storage space required for a service is about 20-100GB [3]. As a result, we cannot load all services on each edge node, but only a subset of services. Let Mv represent the set of edge nodes that meet the service constraints for task v∈V. That means task v∈V can only be executed by an edge node in Mv. Moreover, each edge node has limited resources (e.g., CPU cycles, storage, computing, I/O [19] [30] [31]). We assume that node m∈M has C(m) resources. According to the attributes of edge nodes and tasks, similar to works [14], [19], [22], [32], through long-term statistics and analysis, if task v∈V is executed on node m, then rvm resources need to be allocated to the task v and the execution time is tvm. For ease of description, we take the resource constraint of the storage space as an example in this paper. Note that, it is easy to extend to multiple resource constraints (e.g., consider both storage, CPU and I/O resource constraints) [19] [31]. Table 1 summarizes some key notations.

TABLE 1 Key Notations
Table 1- 
Key Notations
3.2 Problem Definition
In MEC, there may contain many applications that need to be processed in time. Given a set of available nodes and a set of applications (each application consists of multiple dependent tasks), we define the problem of offloading these dependent tasks with service caching (ODT-SC). We first construct a DAG according to the dependencies among tasks, as described in Section 3.1. We then use a binary variable zmv to denote whether task v∈V is offloaded onto edge node m∈M or not. Let variable tv denote the start time for task v∈V. A feasible offloading solution should satisfy the following conditions:

All Tasks should be Offloaded: Each task should be offloaded onto exactly one edge node. That means, for each task v∈V, ∑m∈Mzmv=1.

Service Constraint: Task v∈V can only be offloaded onto the edge node configured with corresponding required services, i.e., the edge node in Mv. That means, ∑m∈Mvzmv=1,∀v∈V.

Dependency Constraint: For any task pair ⟨v,v′⟩∈E, task v′ can start iff all precedent tasks are completed and the required data is transferred to the edge node mv′. That is, for each task pair ⟨v,v′⟩∈E, tv+∑m∈Mzmvtvm+∑m∈M∑m′∈Mcmm′avv′zmvzm′v′≤tv′.

Execute Tasks in Sequence: For any pair of tasks v,v′∈V, if both tasks are offloaded onto the same edge node m∈M (i.e., mv=mv′=m), then tv+tvm≤tv′ or tv′+tv′m≤tv. That means, each edge node can only perform one task at a time instance and tasks cannot be interrupted during the execution [14].

Processing Resource Constraints: The processing resource constraint should be satisfied for every edge node, which can be formulated as ∑v∈Vzmvrvm≤C(m),∀m∈M.

The makespan of these tasks is denoted by T=max{tv+∑m∈Mzmvtvm,v∈V}. We aim to find a feasible offloading solution with a minimum makespan. Thus, the ODT-SC problem can be formulated as follows:
minTs.t.⎧⎩⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪∑m∈Mzmv=1,∑m∈Mvzmv=1,tv+∑m∈Mzmvtvm+∑m∈M∑m′∈Mcmm′avv′zmvzm′v′≤tv′,If tv≥tv′ and zmv=zmv′=1         then: tv−tv′≥tv′m,∑v∈Vzmvrvm≤C(m),tv+∑m∈Mzmvtvm≤T,tv≥0,zmv∈{0,1},∀v∈V∀v∈V∀⟨v,v′⟩∈E∀v,v′∈V,m∈M∀m∈M∀v∈V∀v∈V∀m∈M,v∈V.(1)
View Source

The first set of equations represents that each task should be offloaded onto exactly one edge node. The second set of equations denotes the services constraint, i.e., task v∈V can only be offloaded onto the edge node in set Mv. The third set of inequalities represents the dependency constraint, i.e., task v′ can start iff all precedent tasks are completed and transferred corresponding data to the task v′. The fourth set of constraints guarantees that all tasks on the same edge node will be executed in sequence. More specifically, if tasks v∈V and v′∈V are offloaded onto the same edge node m∈M and without loss of generality we assume task v starts later than task v′, i.e., tv≥tv′. In this case, we need to ensure that task v can start only if task v′ is completed, i.e., tv−tv′≥tv′m. The fifth set of inequalities represents the processing resource constraints. Our objective is to minimize the maekspan, i.e., minT.

3.3 Complexity Analysis
In this section, we prove that ODT-SC is one of the most difficult NP-hard problems for which there exists no approximation algorithm with a constant factor.

Theorem 1.
ODT-SC is one of the most difficult problems in NP-hard class: even finding a k-approximation algorithm (k is a constant) to solve ODT-SC is NP-hard.

To prove this theorem, we first give the following definition.

Definition 1 (Travelling Salesman Problem (TSP) [33]).
Given a set of cities and the distance between every pair of cities, the problem is to find the shortest route on which each city is visited exactly once and return to the starting point. In other words, given an undirected complete graph G(B,A) and the weight of each edge in A, the objective is to find an optimal Hamiltonian cycle.

Proof.
In the following, we first prove that TSP is a special case of the ODT-SC problem and then show that finding a k-approximation algorithm for TSP is NP-Hard.

We consider an arbitrary TSP instance Λ. There are a set of cities C={1,2,…,h}, with h=|C|, and the distance is denoted by dij between each pair of cities i,j∈C. Now, we construct a special case of the ODT-SC problem. Assume there are h identical edge nodes equipped with all required services, denoted by M={1,2,…,h}, which are one-to-one correspondence with the cities in set C. The available processing resources of edge node m∈M are the same, denoted by α. h+1 tasks are required to be offloaded onto these h edge nodes, denoted by V={v1,v2,…,vh+1}, and the DAG for the tasks is : v1→v2→...→vh+1. The execution time tvm=t for any task v∈V on any edge node m∈M. Task v1 and task vh+1 require α/2 processing resources, respectively, while the other tasks require α processing resources each. The data volume that required to be transmitted avv′=1 for each edge ⟨v,v′⟩ on the DAG. The communication delay per unit data between edge nodes i,j∈M is equal to the distance between cities i and j (i.e., cij = dij). The objective is to find a feasible offloading with a minimum makespan. For this special case, obviously, tasks v1 and vh+1 will share one edge node while each of the other edge nodes will execute only one task. The start time tvi=tvi−1+t+cmvi−1mvi for each task vi∈V−{v1} and tv1=0. Thus, we can obtain the maskspan:
T=tvh+1+t=tvh+t+dmvhmvh+1+t=(h+1)t+dmv1mv2+dmv2mv3+...+dmvhmvh+1.(2)
View Source

That means, for each task vi∈V, the selection of edge node mvi turns into the selection of ith visited city. This is exactly the TSP instance Λ. Thus, each TSP instance is a special case of the ODT-SC problem.

Previous works have proved that finding a k-approximation (k is a constant) algorithm for TSP is NP-Hard [33], [34]. We give a briefly proof for completeness. Let G1(B,A1) be any graph, where B denotes the vertex set and A1 denotes the edge set. Let b=|B| represent the number of vertices. We construct the complete graph G(B,A) such that A={⟨p,q⟩|p,q∈B} and define the weight/length of each edge ⟨p,q⟩∈A as:
dpq={1,kb,⟨p,q⟩∈A1otherwise.(3)
View Source

The TSP problem is to find a shortest Hamiltonian cycle in graph G(B,A). Assuming there exists a k-approximation algorithm, we denote the optimal solution as OPTHC and the approximation solution as AHC. Obviously, there exists Hamiltonian cycle in G1(B,A1) if and only if:
AHC≤k⋅OPTHC=kb.(4)
View Source

These is no Hamiltonian cycle in G1(B,A1) if and only if:
AHC≥OPTHC≥kb+b−1>kb.(5)
View SourceConsequently, if the solution AHC≤kb, then Hamiltonian cycle exists in G1(B,A1). If the solution AHC>kb, then there is no Hamiltonian cycle in G1(B,A1). That means, we can judge whether there is Hamiltonian cycle in any graph G1(B,A1) according to the solution of the approximation algorithm (i.e., in polynomial time). However, the Hamiltonian cycle problem is NP-complete, which cannot be solved in polynomial time [35] unless P=NP. Therefore, the assumption is false. As a result, finding a k-approximation algorithm for TSP is NP-Hard. Considering that TSP is a special case of ODT-SC, we can conclude that finding a k-approximation algorithm (k is a constant) to solve ODT-SC is NP-hard.

The above analysis shows the hardness of the ODT-SC problem. Thus, in this paper, we first design algorithms to solve the general ODT-SC problem in Section 4 and then design an approximation algorithm with bounded approximation factor for the homogenous scenario in Section 5.

SECTION 4Algorithms Design for ODT-SC
We first propose a rounding based algorithm to solve ODT-SC in Section 4.1. Although this method is non-trivial, it cannot provide satisfactory performance as shown in simulation section. Then, we propose convex programming based algorithm (CP), which will be described in Section 4.2.

4.1 Rounding Based Algorithm
In this section, we propose a rounding based algorithm for ODT-SC, called Rounding. Specifically, the Rounding algorithm offloads dependent tasks through three major steps: 1) relaxing the constraints of ODT-SC for computing the potential edge node mv and execution time tv for each task v∈V; 2) determining the scheduling order of tasks to meet the dependency constraints; and 3) edge node selection for scheduling each task in order with the aim of minimizing the makespan.

Relaxing the Constraints of ODT-SC. By Eq. (1), ODT-SC is very difficult to be solved directly. Specifically, the third set of inequalities in Eq. (1) is quadratic and the fourth set of constraints in Eq. (1) contains conditional statement. To eliminate these difficulties, we first define binary variables umm′vv′ for any two tasks v,v′∈V and any two edge nodes m,m′∈M that satisfy:
zmv+zm′v′−12≤umm′vv′≤zmv+zm′v′2,∀v,v′∈V,m,m′∈M.(6)
View Source

Considering that zmv and zm′v′ are both binary variables, it follows zmv⋅zm′v′=umm′vv′ and we can modify the third set of inequalities in Eq. (1) as follows:
=≤tv+∑m∈Mzmvtvm+∑m∈M∑m′∈Mcmm′avv′zmvzm′v′tv+∑m∈Mzmvtvm+∑m∈M∑m′∈Mcmm′avv′umm′vv′tv′,∀⟨v,v′⟩∈E.(7)
View Source

We then let χ represent a large number and xvv′∈{0,1},∀v,v′∈V. In this way, we can modify the fourth set of constraints in Eq. (1) as the following inequalities:
tv−tv′χ<xvv′,∀v,v′∈V(8)
View Source
χ(3−zmv−zmv′−xvv′)+tv−tv′≥tv′m,∀v,v′∈V,m∈M.(9)
View Source

More specifically, for any two tasks v and v′, there are two cases: 1) Both tasks v and v′ are offloaded onto the same edge node m∈M. Without loss of generality, we assume that task v starts later than task v′, i.e., tv≥tv′. In this case, both zmv and zmv′ are equal to 1 and Eq. (8) guarantees xvv′=1. Thus, χ(3−zmv−zmv′−xvv′)=0 and Eq. (9) can be simplified to tv−tv′≥tv′m, which guarantees that task v cannot start before task v′ is finished. 2) Tasks v and v′ are offloaded onto different edge nodes, which means zmv and zmv′ cannot be equal to 1 simultaneously. Under this case, 3−zmv−zmv′−xvv′ is larger than 0 and Eq. (9) holds regardless of the values of tv and tv′ (i.e., there is no constraint between tv and tv′). Thus, the above two sets of inequalities guarantee that all tasks on the same edge node will be executed in sequence and tasks offloaded onto different edge nodes can be performed simultaneously if necessary. It means that we transform the fourth set of constraints in Eq. (1) into the above two sets of inequalities. In the end, we relax all binary variables. Specifically, ODT-SC assumes that each task can only be performed onto exact one edge node. By relaxing this assumption, each task i∈V is permitted to be splittable and performed onto several edge nodes. To sum up, we formulate the problem as Eq. (10).

Since Eq. (10) is a linear program, we can solve it in polynomial time with a linear program solver such as PuLP[36]. Assume that the optimal solutions for Eq. (10) are denoted by zˇmv, tˇv, xˇvv′ and uˇmm′vv′, ∀v,v′∈V,∀m,m′∈M, and the optimal result is denoted by Tˇ. As Eq. (10) is a relaxation of the ODT-SC problem, Tˇ is a lower-bound result for this problem. According to these optimal solutions obtained by Eq. (10), we can get the potential edge node and execution time for each task v∈V, which will be used for the following operations.
minTs.t.⎧⎩⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪∑m∈Mzmv=1,∑m∈Mvzmv=1,zmv+zm′v′−12≤umm′vv′≤zmv+zm′v′2,tv+∑m∈Mzmvtvm+∑m∈M∑m′∈Mcmm′avv′umm′vv′≤tv′,tv−tv′χ<xvv′,χ(3−zmv−zmv′−xvv′)+tv−tv′≥tv′m,∑v∈Vzmvrvm≤C(m),tv+∑m∈Mzmvtvm≤T,tv≥0,zmv∈[0,1],xvv′∈[0,1],umm′vv′∈[0,1],∀v∈V∀v∈V∀v,v′∈V,m,m′∈M∀⟨v,v′⟩∈E∀v,v′∈V∀v,v′∈V,m∈M∀m∈M∀v∈V∀v∈V∀m∈M,v∈V∀v,v′∈V∀v,v′∈V,m,m′∈M.(10)
View SourceRight-click on figure for MathML and additional features.

Determining the Execution Order of Tasks. We sort all tasks by the increasing order of their starting time in a scheduling list Π. Tie-breaking is done randomly for simplicity. Based on the fourth set of inequalities in Eq. (10), it can be easily shown that the increasing order of tˇv for v∈V preserves the dependency constraints. Thus, we schedule tasks one by one in the order of scheduling list Π in the next step to preserve the dependency constraints.

Edge Nodes Selection. We offload task v∈V to edge node m∈M based on the the optimal solution zˇmv using the randomized rounding method[37]. More specifically, for each task v∈V, we choose one edge node m∈M to set zmv=1 and set the other values as 0, which means that we will offload task v to edge node m, with the probability of zˇmv. Thus, for each task v in scheduling list Π, we offload task v to edge node m∈M with the probability of zˇmv. In this way, we can determine the offloading schedule for all tasks. However, due to the randomized rounding processing, the selected edge node m∈M for task v∈V may break processing resource constraints.

4.2 Convex Programming Based Algorithm
In this section, we present a convex programming based algorithm for ODT-SC, called CP. The workflow of CP is shown in Fig. 2. Similar to Rounding, CP offloads dependent tasks through four major steps: 1) Relaxing the ODT-SC problem to construct a convex optimization program. 2) Using progressive rounding method to obtain feasible solutions. 3) Computing weight for each task according to the feasible solutions. 4) Offloading tasks based on the weight values.

Fig. 2. - 
Workflow of the CP algorithm. CP can be divided into four steps: relaxing the ODT-SC problem to construct convex program, using progressive rounding method to solve this program, computing weight for each task according to the solutions and offloading tasks according to the weights.
Fig. 2.
Workflow of the CP algorithm. CP can be divided into four steps: relaxing the ODT-SC problem to construct convex program, using progressive rounding method to solve this program, computing weight for each task according to the solutions and offloading tasks according to the weights.

Show All

Relaxing the ODT-SC Problem. We find that the Rounding algorithm cannot achieve good results due to introducing too many variables. Thus, in this section, to eliminate the complexity of Eq. (1), we first leverage the definition of binary variable zmv, with ∀v∈V and ∀m∈M, to give the following modification:
tv+∑m∈Mzmvtvm+∑m∈M∑m′∈Mcmm′avv′zmvzm′v′=tv+∑m∈Mzmvtvm+∑m∈M∑m′∈Mcmm′avv′max[zmv+zm′v′−1,0].(11)
View Source

Then we assume that edge nodes can perform tasks in parallel and each task v∈V is permitted to be splittable and can be executed on several edge nodes. In this way, we derive the following convex optimization problem:
minTs.t.⎧⎩⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪∑m∈Mzmv=1,∑m∈Mvzmv=1,tv+∑m∈Mzmvtvm+∑m∈M∑m′∈Mcmm′avv′⋅max[zmv+zm′v′−1,0]≤tv′,∑v∈Vzvmrvm≤C(m),tv+∑m∈Mzmvtvm≤T,tv≥0,zmv∈[0,1],∀v∈V∀v∈V∀⟨v,v′⟩∈E∀m∈M∀v∈V∀v∈V∀m∈M,v∈V.(12)
View Source

Since Eq. (12) is a convex optimization problem, we can solve it in polynomial time with a convex programming solvers such as CPLEX [38]. Assume that the optimal solutions for Eq. (12) are z˜mv and t˜v, ∀v∈V,m∈M, and the optimal objective value is T˜.

Progressive Rounding. This step obtains integer solutions zˆmv for each v∈V and m∈M using the progressive rounding method[39]. More specifically, in each iteration, we first solve Eq. (12) and obtain fractional solutions z˜mv. Then we choose some tasks v∈V with large value of max{z˜mv,m∈M} and use randomized rounding method [37] to derive an integer solution zˆmv for these chosen tasks. We fix the integer solution in zˆmv (i.e., fix these rounding solutions as known quantities) and solve Eq. (12) again. In this way, we can obtain a feasible solutions zˆmv after several iterations. That means, we get the offloaded edge node mv for each task v∈V (i.e., zˆmvv=1) while assuming edge node can simultaneously execute tasks in this step.

Computing Weights. This step computes the weight for each task. More specifically, we first insert an end task at the bottom of the DAG and connect this task with all sink nodes of the DAG. We then denote the weight of each link ⟨v,v′⟩∈E as w⟨v,v′⟩=tvmv+cmvmv′avv′ and the weights of links connected with the end task are set as 0. In this way, we compute the maximum distance from each task v∈V to the end task, denoted by W(v). It can be easily shown that the descending order of their distance to the end task preserves the dependency constraints and a larger value means potential longer execution time. Thus, we sort all tasks in descending order of their distance to the end task and keep them in a list Π.

Offloading Tasks. We offload tasks following the order of list Π in this step. We define some concepts/variables to facilitate the description of this part. We use Pred(v) and Succ(v) to denote the set of immediate predecessor and successor tasks of task v∈V, respectively, which can be obtained according to the DAG. Let R(m) denote the rest processing resources for edge node m∈M, initialized as C(m). Mv(R) denotes the set of edge nodes that satisfy both processing resource and service constraints for task v∈V, formally Mv(R)=Mv⋂{m|R(m)≥rvm,m∈M}. Let T(v,m) represent the first date at which there is a larger idle time slot on edge node m∈M than tvm, initialized as 0. Note that, the idle time slot may be between two already-offloaded tasks on edge node m or after the time all tasks offloaded onto edge node m are completed. Moreover, for each task v∈V, the actual offloaded edge node, the actual start time and the actual finish time are denoted by mv, tv¯ and fv¯, respectively, and are initialized to 0. With these definitions, if task v is offloaded onto edge node m, we can calculate the earliest start time EST(v,m) and the earliest finish time EFT(v,m):
EST(v,m)=max(T(v,m),maxv′∈Pred(v)(fv′¯+cmv′mav′v))(13)
View Source
EFT(v,m)=EST(v,m)+tvm.(14)
View Source

In each iteration, we choose the first task v in list Π for offloading. We first compute the earliest finish time EFT(v,m) for each edge node m∈Mv(R) by Eq. (14) and then offload task v onto edge node m with minm∈Mv(R)EFT(v,m). After task v is offloaded, we record the actual offloaded edge node mv. Besides, we update the actual start time tv¯, the actual finish time fv¯ and the rest processing resources R(mv) by Eqs. (15), (16) and (17), respectively.
tv¯=EST(v,mv)(15)
View Source
fv¯=tv¯+tvmv(16)
View Source
R(mv)=R(mv)−rvm.(17)
View Source

We repeat these operations until all tasks have been offloaded. The CP algorithm is formally described in Algorithm 1.

SECTION 5Favorite Successor Based Algorithm for the Practical Homogeneous Scenario
The above section has proposed the CP algorithm to solve the general ODT-SC problem (i.e., heterogeneous scenario). In many practical scenarios, since most of the edge nodes are placed/purchased at the same time by providers, the hardware specifications of edge nodes are similar [40], [41], [42]. Thus, this section studies the offloading problem in homogeneous scenarios. We assume that all edge nodes have similar processing capacity and all links have similar transmission rate. In other words, we use ev to denote the execution time tvm if task v∈V is offloaded onto edge node m∈M (i.e., ev=tvm) and use c to denote the communication delay per unit data cmm′ for each pair of edge nodes m,m′∈M (i.e., c=cmm′).

SECTION Algorithm 1.CP: Convex Programming Based Algorithm for ODT-SC
Step 1: Relaxing ODT-SC Problem

Construct a convex optimization program in Eq. (12)

Obtain the optimal solution z˜mv, t˜v

Step 2: Progressive Rounding

Derive an integer solution zˆmv by progressive rounding for each v∈V, m∈M

Step 3: Computing Weights

Compute W(v) for each task v∈V

Sort all tasks v∈V in descending order of their distance to the end task and saved in list Π

Step 4: Offloading Tasks

for each task v∈V do

Obtain Pred(v) according to DAG

Initialize variables tv¯ and fv¯ to 0

end for

for each edge node m∈M do

Initialize variables R(m) to C(m)

end for

while offloading list Π≠∅ do

Select the first task v from the list Π

Update Mv(R)=Mv⋂{m|R(m)≥rvm,m∈M}

Compute EFT(v,m) with Eq. (14) for m∈Mv(R)

Offload task v on m with minm∈Mv(R)EFT(v,m)

Record the offloaded edge node as mv

Update tv¯, fv¯ and R(mv) with Eq. (15), Eq. (16) and Eq. (17), respectively

Delete task v from offloading list Π

end while

MEC decreases the task offloading delay by placing computing resources in close proximity to the local devices. On the one hand, with the development of the 5G technology, the data transmission rate has been greatly improved [3]. On the other hand, the processing capacity of edge nodes is limited. Thus, for the typical applications, e.g., virtual/augmented reality (VR/AR), cognitive assistance and mobile gaming, the task execution delay is much greater than the transmission delay [2]. Thus, in the practical scenarios discussed in this section, we assume the minimum processing delay is greater than the maximum communication delay [41]. This section presents an approximate algorithm with bounded approximation factor for offloading tasks in the practical homogeneous scenarios.

5.1 Favorite Successor Based Algorithm Description
In a dependent task set V, each task v∈V may have several precedent tasks (i.e., predecessors) and several succedent tasks (i.e., successors). The predecessor/successor of task v that offloaded onto the same edge node as task v does not consume communication delay for data transmission between different edge nodes. Thus, how to select predecessor/successor of task v to offload onto the same edge node as task v is important during scheduling dependent tasks. Based on this consideration, we first give the definitions of favorite successor and predecessor for this problem.

Definition 2 (Favorite Successor [43]).
For any task v∈V, if task v′∈Succ(v) satisfies tv′¯<tv¯+ev+cavv′, then task v′ is called the favorite successor for task v.

Definition 3 (Favorite Predecessor [43]).
For any task v∈V, if task v′∈Pred(v) satisfies tv′¯+ev′+cav′v>tv¯, then task v′ is called the favorite predecessor for task v.

According to the above definitions, we prove that each task v∈V has at most one favorite successor/predecessor and the favorite successor/predecessor v′ must be offloaded onto the same edge node as task v. Specifically, if there are two favorite successors v′ and v′′ of task v, then both tasks v′ and v′′ should be offloaded to the same edge node as task v. Otherwise tv′¯(tv′′¯)≥tv¯+ev+c⋅avv′(c⋅avv′′), which contradicts the definition of favorite successor. Without loss of generality, assume that tv′¯≥tv′′¯. In this way, tv′¯ will be performed at least at time tv¯+ev+ev′′≥tv¯+ev+c⋅avv′. That is, tv′¯≥tv¯+ev+c⋅avv′, which contradicts the definition of favorite successor. Thus, each task v∈V has at most one favorite successor. Similarly, we can prove that each task has at most one favorite predecessor.

Based on the definition of favorite successor, we present a favorite successor based algorithm to solve the special case (FS). Specifically, FS offloads tasks through two major steps: 1) obtain favorite successor without considering services and the number of edge nodes constraints. The results can reflect the dependency priority of the DAG. 2) favorite successor based offloading while considering the number of edge nodes and service constraints.

Obtaining Favorite Successor. We first attempt to offload tasks to edge nodes without considering services and the number of edge nodes constraints. In this situation, we only need to consider the dependency constraint and we formulate this problem as follows:
minTs.t.⎧⎩⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪tv+ev+cavv′yvv′≤tv′,∑v′∈Succ(v)yvv′≥|Succ(v)|−1,∑v′∈Pred(v)yv′v≥|Pred(v)|−1,tv+ev≤T,tv≥0,yvv′∈{0,1},∀⟨v,v′⟩∈E∀v:⟨v,v′⟩∈E∀v:⟨v′,v⟩∈E∀v∈V∀v∈V∀⟨v,v′⟩∈E,(18)
View Sourcewhere binary variable yvv′ denotes whether task v′ is the favorite successor of task v. Specifically, yvv′=0 represents that task v′ is the favorite successor of task v, otherwise yvv′=1. The first set of inequalities indicates the dependency constraints. Specifically, if yvv′=0, then this set of inequalities turns into tv+ev≤tv′ (i.e., no communication delay between task v and task v′ if v′ is the favorite successor of v). Otherwise, it turns into tv+ev+cavv′≤tv′ (i.e., we need consider communication delay between task v and task v′ if v′ is not the favorite successor of v). The second and third sets of inequalities indicate that any task has at most one favorite successor/predecessor. For example, if task v∈V has more than one favorite successor, then ∑v′∈Succ(v)yvv′<|Succ(v)|−1, which contradicts the second inequality. The fourth set of inequalities means the task offloading delay, i.e., task v can be executed only when it has been offloaded from the local device to the edge node. Our objective is to minimize the makespan, i.e., min T.

To solve this program in polynomial time, we relax the sixth set of constraints by setting yvv′∈[0,1]. In this way, we can obtain the optimal solutions y˜vv′ (∀⟨v,v′⟩∈E) with a linear programming solver such as PuLP [36]. The second set of inequalities indicates that at most one successor v′ of task v∈V can satisfy y˜vv′<0.5. Specifically, if there exists two successors v′ and v′′ of task v∈V such that y˜vv′<0.5 and y˜vv′′<0.5, then we have ∑v′∈Succ(v)yvv′<|Succ(v)|−1, which contradicts with the second set of inequalities. Thus, for each ⟨v,v′⟩∈E, let yˆvv′=0 if y˜vv′<0.5, and let yˆvv′=1 otherwise. In this way, we get the integer solutions, that is, the favorite successor (if exists) for each task.

Favorite Successor Based Offloading. In this step, we leverage the results obtained by the first step and the list offloading algorithm[23] to offload tasks. That is, we offload tasks one by one with the earliest start time while trying to assign the favorite successor of task i to the same edge node as task i. During the offloading process, we first introduce some definitions for the sake of convenience. We use VR to denote the set of tasks whose all predecessors have been offloaded, initialized as the set of tasks without predecessor. In other words, VR denote the set of tasks that can be offloaded at this time. Moreover, let lm denote the last offloaded task on edge node m∈M at this time and fm denote the favorite successor of task lm, both initialized as none. We use Avail(v)=maxv′∈Pred(v)(tv′¯+ev′+cav′v) to denote the available time that task v can be executed on any edge node. That is, task v can be executed only after all its predecessors have been executed and the corresponding data has been transmitted.

Then we compute the earliest start time EST(v,m) with Eq. (13) for each task v∈VR and edge node m∈Mv. For each edge node m∈Mfm, if EST(fm,m)<T(fm,m)+calmfm, edge node m can execute the favorite successor fm earlier than other edge nodes. Thus, we try to reserve edge node m for executing task fm. For each task v∈VR⋂Succ(lm) and v≠fm and m∈Mv (i.e., v is a potential competing successor), if 1) Avail(v)≥EST(fm,m) or 2) there is an edge node m′≠m such that EST(v,m′)≤Avail(v) and if m′∈Mfm′ and EST(fm′,m′)<T(fm′,m′)+calm′fm′, v∉Succ(lm′) or v=fm′, we defer the earliest staring time of task v on edge node m, that is, EST(v,m)=EST(fm,m)+efm. Then we choose edge node m′ with minv′∈VR,m′∈Mv′EST(v′,m′) to offload task v′. After task v′ is offloaded, we record the actual offloaded edge node mv′ and update tv′¯ and fv′¯ with Eqs. (15) and (16), respectively. Besides, we update the last offloaded task lmv′ as task v′ and update the unoffloaded ready set VR. We repeat this iteration until all tasks have been offloaded. The FS algorithm is formally described in Algorithm 2.

Algorithm 2. FS: Favorite Successor based Algorithm for Homogeneous Scenario
Step 1: Obtaining Favorite Successor

Construct a linear program based on Eq. (18)

Obtain the optimal solution y˜vv′

Derive an integer solution yˆvv′ for each ⟨v,v′⟩∈E

Record the favorite successor (if exists) for each task based on the integer solution

Step 2: Favorite Successor based Offloading

for each task v∈V do

Obtain Pred(v) and Succ(v) according to DAG

Initialize variables tv¯, fv¯ and Avail(v) to 0

end for

for each edge node m∈M do

Initialize the last offloaded task lm to none

end for

Compute the set VR of unoffloaded tasks that all predecessors are offloaded

while VR≠∅ do

for each task v∈VR and edge node m∈Mv do

Compute EST(v,m) with Eq. (13)

end for

for each edge node m∈M that the last offloaded task lm have a favorite successor fm∈VR do

if m∈Mfm and EST(fm,m)<T(fm,m)+calmfm then

for each task v∈VR⋂Succ(lm) and v≠fm and m∈Mv do

Avail(v)=maxv′∈Pred(v)(tv′¯+ev′+cav′v)

if 1) Avail(v)≥EST(fm,m)

or 2) there is an edge node m′≠m such that EST(v,m′)≤Avail(v) and if m′∈Mfm′ and EST(fm′,m′)<T(fm′,m′)+calm′fm′, v∉Succ(lm′) or v=fm′ then

Update EST(v,m)=EST(fm,m)+efm

end if

end for

end if

end for

ESTmin=minv∈VR,m∈MvEST(v,m)

Choose one task v′∈VR to offloaded onto edge node m′∈Mv′ which satisfying EST(v′,m′)=ESTmin

Use mv′ to record the offloaded edge node

Update tv′¯ and fv′¯ with Eq. (15) and Eq. (16), respectively

Update the last offloaded task lmv′=v′

Update the unoffloaded task set VR that all predecessors are offloaded

end while

5.2 Performance Analysis
This section analyzes the approximate performance of FS. If we do not consider the services and the number of edge nodes constraints, we can offload tasks satisfying the favorite successor requirement. We first give the approximation ratio for this problem.

Theorem 2.
If we do not consider edge node constraints and offload tasks according to the integer solutions obtained by Eq. (18), we can finish all tasks in a makespan at most 43 times of the optimal makespan.

Proof.
Let Two denote the actual makespan and twov denote the actual start time of task v∈V using the integer solutions to offload. Let Tlp denote the makespan obtained by linear program, which is the lower-bound of the optimal makespan (denoted as Topt). We denote the weight of link ⟨v,v′⟩∈E as w⟨v,v′⟩=ev+cavv′yvv′.

Since we assume the system contains an unlimited number of edge nodes, we can offload any task once it receives all data from successors. Thus, Two equals to the longest path in the DAG. It means:
TwoTlp≤max⟨v,v′⟩∈E(wwo⟨v,v′⟩wlp⟨v,v′⟩)=max⟨v,v′⟩∈E(ev+cavv′yˆvv′ev+cavv′y˜vv′).(19)
View SourceIf yˆvv′=0, then ev+cavv′yˆvv′ev+cavv′y˜vv′≤1. Otherwise y˜vv′≥0.5, which means:
ev+cavv′yˆvv′ev+cavv′y˜vv′≤ev+cavv′ev+0.5cavv′≤43.(20)
View SourceThe last inequality holds because we assume that ev≥cavv′ for this special case. Hence we conclude that:
TwoTopt≤TwoTlp≤max⟨v,v′⟩∈E(ev+cavv′yˆvv′ev+cavv′y˜vv′)≤43.(21)
View Source

We then give some features of the proposed FS algorithm.

Lemma 3.
Let l′=minv∈V(|Mv|) (i.e., for any task, at least l′ edge nodes are configured with required services) and l′′=l−l′. We use T[0,tv¯] to denote the accumulate idle time on all edge nodes before the actual start time tv¯ of task v. Then we have :
T[0,tv¯]≤(l′−1)twov+l′′tv¯.
View Source

Proof.
In the worst case, the other l′′ edge nodes have not been configured with any service and all tasks require the support of service(s). Thus, all tasks can only be offloaded onto l′ edge nodes while leaving other l′′ edge nodes idle. That means, the accumulate idle time on l′′ edge nodes equals to l′′tv¯. If we can show at most (l′−1)twov accumulate idle time on l′ other edge nodes, the proof is finished. This becomes the identical parallel machines scheduling with dependent tasks problem[43], which has been illustrated by previous works such as [43], [44], [45].

Lemma 4.
Let Tfsl denote the actual makespan by using the FS algorithm. Then we have:
T[0,Tfsl]≤(l′−1)Two+l′′Tfsl.
View Source

Proof.
Let task v be the last completed task by the FS algorithm, by applying Lemma 3, we have
T[0,Tfsl]=T[0,tv¯]+T[tv¯,Tfsl]≤(l′−1)twov+l′′tv¯+(l−1)ev=(l′−1)(twov+ev)+l′′(tv¯+ev)≤(l′−1)Two+l′′Tfsl.(22)
View Source

Now, we give the approximation performance of our proposed FS algorithm.

Theorem 5.
Let Toptl to denote the optimal makespan for offloading on l edge nodes. We have TfslToptl≤ll′+43.

Proof.
We know that the accumulated idle time plus the whole tasks execution time is equal to the total time slices. By applying Theorem 2 and Lemma 4, we have:
lTfsl⇒Tfsl=T[0,Tfsl]+∑v∈Vev≤(l′−1)Two+l′′Tfsl+∑v∈Vev≤l′−1l′Two+∑v∈Vevl′≤4(l′−1)3l′Topt+ll′Toptl≤43Toptl+ll′Toptl.(23)
View Source

The penultimate inequality holds because Toptl≥∑v∈Vevl. Thus, we conclude that FS can achieve an approximate ratio of ll′+43, where l is the number of edge nodes and l′=minv∈V(|Mv|) (i.e., for any task, at least l′ edge nodes are configured with required services).

SECTION 6Performance Evaluation
This section evaluates the performance of the proposed algorithms by comparing with state-of-the-art methods over multiple application scenarios using real-world applications (from [23]) and data traces (from [24]).

6.1 Performance Metrics and Methodology
We mainly focus on the comparison of makespan in this section, which is one of the most important metrics for the task offloading problem. We compare CP and FS with the following existing approaches.

The first one is GenDoc [22]. It derives an efficient dynamic programming based algorithm to find the optimal dependent tasks offloading scheme with fixed service caching. The key characteristic of GenDoc is that one task might be placed and executed on multiple edge nodes repeatedly to avoid communication delay and achieve the objective of makespan minimization. However, this method may consume huge processing resources on edge nodes.

The second one is the Individual Time Allocation with Greedy Offloading (ITAGS) algorithm [14], which aims at minimizing the communication and computation costs while satisfying makespan constraint. Specifically, ITAGS first uses a binary-relaxed version of the original problem to allocate a completion deadline for each individual task, and then greedily optimizes the offloading of each task subject to its time allowance. For fair comparison, we modify the objective of ITAGS to makespan minimization while satisfying processing resources constraints. In this way, ITAGS can solve the same problem proposed in this paper.

The third one is the rounding based algorithm, which is illustrated in Section 4.1. We use Rounding to denote this method.

The last one is the traditional algorithm, denoted as Greedy. This algorithm picks tasks starting from the top of the DAG to keep dependency. Then it offloads each picked task to the edge node with the earliest finish time while satisfying service and resource constraints.

6.2 Simulation Settings
In this section, we introduce the simulation settings, including the generation methods of DAGs, task set settings, and the scenario settings for simulations.

6.2.1 DAG Generation
Similar to [14], [23], we generate DAGs with respect to real-world structures, namely Gaussian Elimination (GE)[46] and Fast Fourier Transform (FFT) [47]. For the GE structure, given the dimension η of a graph, the number of tasks in a GE structure is η2+η−22 [23]. For the FFT structure, we divide this structure into two parts: recursive calls and the butterfly operation. The number of FFT points θ determines the number of tasks in a FFT structure. There are 2⋅(θ−1)+1 recursive call tasks and θlog2θ [23]. Both generated structures are well known and used in real-world scenarios. In the following simulations, we generate dependent task set based on the above two structures, denoted by GE Structure and FFT Structure, respectively.

6.2.2 Task Set Settings
Similar to [17] [48], we use the data traces of google clusters [24] to generate task sets. The google cluster track contains hundreds of thousands of jobs (applications). Each job consists of one to thousands of tasks and each task has various parameters specified, including resource requests (e.g., storage and computing requirements) and service requests (e.g., can only be offloaded onto nodes equipped with corresponding services). For other information not included, we use the following methods to generate for our simulations. Specifically, for the general ODT-SC problem, to emulate the heterogeneous environment (e.g., the processing delay of the same job will vary on different edge nodes), we scale the processing time collected from [24] with a factor uniform distribution in (1,10). The communication-to-computation ratio is uniformly randomized in (0.1,10). In other words, for each task pair ⟨v,v′⟩∈E, the communication delay for data transmission from task v to task v′ is generated through multiplying the processing time for task v with a random number in (0.1,10). Moreover, the required processing resources rvm is drawn uniformly in (1,10) for each task v on edge node m. For each task, the percentage of edge nodes, configured with required services, is denoted by Ω. We set Ω as 50 percent and the number of edge nodes as 10 by default.

6.2.3 Simulation Scenario Settings
The simulations are performed under two scenarios. Specifically, the first scenario is applied to the heterogeneous environment, i.e., the general ODT-SC problem. We evaluate the performance of CP, GenDoc, ITAGS, Rounding and Greedy under this scenario. The second scenario is applied to the homogeneous environment, i.e., the special case introduced in Section 5. We compare FS with CP, GenDoc, ITAGS, Rounding and Greedy under this scenario.

We divide the simulations into six groups and each group of simulations contains the above two scenarios. Basically, we first run 10,000 random test cases to evaluate the overall makespan performance among all algorithms. Then we simulate the mean makespan of these algorithms over a wide range of parameters in the number of tasks, the percentage of edge nodes performing a task, the communication-to-computation ratio and the number of edge nodes.

6.3 Simulation Results
In order to demonstrate the effectiveness of our proposed algorithms, we run six sets of simulations for each DAG structure (e.g., GE and FFT structures) and each simulation scenario (e.g., heterogeneous and homogeneous scenarios).

Overall Performance Comparison. In the first set of simulations, we first randomly generate 200 DAGs using GE and FFT algorithms with the number of tasks from 5 to 500. For each DAG, we generate 50 task set settings according to Section 6.2.2. Thus, we generate totally 10,000 test cases. We perform CP, GenDoc, ITAGS, Rounding and Greedy on these test cases and evaluate the overall makespan performance. The results are shown in Fig. 3. We observe that CP can reduce mean makespan by about 21, 27, 35 and 47 percent compared with ITAGS, GenDoc, Rounding and Greedy, respectively. Besides, as shown in Fig. 3b, over 60 percent of test cases will be completed within the makespan of 6000 by CP, while only less than 50 percent of test cases will be completed within the makespan of 6000 by other algorithms. Thus, in the heterogeneous scenarios, CP achieves better overall makespan performance compared with other benchmarks. That is because our proposed CP algorithm has considered the service caching when making offloading decisions and the algorithm is well-designed (e.g., progressive rounding and computing weights).


Fig. 3.
Overall performance for heterogeneous scenario.

Show All

Similarly, for each DAG, we generate 50 different random settings that meet the requirements of the homogeneous scenarios. Overall, we generate totally 10,000 test cases for homogeneous scenarios. Note that, both our proposed CP and FS algorithms can be applied in homogeneous scenarios. Thus, we test FS, CP, GenDoc, ITAGS, Rounding and Greedy on these 10,000 test cases. As shown in Fig. 4, FS reduces the mean makespan by about 11, 20, 25, 35 and 45 percent compared with CP, GenDoc, ITAGS, Rounding and Greedy, respectively. It means that FS can achieve better makespan results than CP in homogeneous scenarios. That is because CP is specifically designed for homogeneous scenarios and is more efficient than CP in homogeneous scenarios. We usually perform the CP algorithm for heterogeneous scenarios and execute FS for homogeneous scenarios.


Fig. 4.
Overall performance for homogeneous scenario.

Show All

Comparison on the Number of Test Cases in Different Rankings. Similar to the first set of simulations, we generate totally 10,000 random heterogeneous test cases and evaluate CP, ITAGS, Rounding and Greedy on these test cases. For each test case, we sort all the algorithms in the ascending order of their makespans. The first algorithm, i.e., with the shortest makespan, is marked as Rank 1 and the algorithm with the second shortest makespan is marked as Rank 2. Similarly, we mark all algorithms for each test case. In the end, we perform 10000 test cases and count the number of times that each algorithm is marked as Rank 1/2/3/4/5. The simulation results are shown in Fig. 5a. We observe that CP produces the minimum makespan in 45.61 percent (4,561 out of 10,000) of test cases. By comparison, ITAGS, GenDoc, Rounding and Greedy are in Rank 1 in 1912, 1856, 1639 and 32 test cases, respectively. Our proposed CP algorithm has considered the service caching constraints and adopted well-designed algorithm steps. As a result, CP achieves the shortest makespan in most test cases and produces the longest makespan only in a few test cases. Similarly, we test FS, GenDoc, ITAGS, Rounding and Greedy on 10,000 random homogeneous test cases. As shown in Fig. 5b, FS produces the shortest makespan in 58.85 percent (5,885 out of 10,000) of test cases and the longest makespan on very little small portion (less than 1 percent) of the test cases. By comparison, GenDoc is in Rank 2 for most test cases, ITAGS is in Rank 3 for most cases and Greedy produces the longest makespan for most cases. The results indicate our proposed FS algorithm outperforms other state-of-the-art solutions on most test cases.

Impact of the Number of Tasks on Makespan. The third set of simulations investigates the mean makespan by changing the number of tasks. We execute each simulation 100 times and average the numerical results. The results are shown in Figs. 6 and 7. Fig. 6 shows the results for the general ODT-SC problem with different DAG structures. As the number of tasks increases, the mean makespan increases for all algorithms. CP can always achieve lower mean makespan compared with the other four algorithms. For example, when there are 400 tasks in the FFT structure, the mean makespan under CP is 5,783 while 7982, 8010, 8345 and 9834 under ITAGS, GenDoc, Rounding and Greedy, respectively. In other words, CP can decrease mean makespan by about 28, 29, 31 and 42 percent compared with ITAGS, GenDoc, Rounding and Greedy, respectively. Besides, GenDoc is in Rank 2 when the number of tasks is not more than 300 and in Rank 3 when the number of tasks is more than 300. That is because GenDoc may consume more processing resources and encounter resource constraints as the number of tasks increases.


Fig. 5.
Number of test cases versus rank of makespans.

Show All


Fig. 6.
Mean makespan versus number of tasks for heterogeneous scenario.

Show All


Fig. 7.
Mean makespan versus number of tasks for homogeneous scenario.

Show All

Fig. 7 gives the results for homogeneous scenarios with different DAG structures. We observe that our proposed FS algorithm always outperforms four benchmarks. Basically, FS achieves the shortest mean makespan while CP is in Rank 2, GenDoc is in Rank 3, ITAGS is in Rank 4, Rounding is in Rank 5 and Greedy produces the longest mean makespan. For example, when there are 300 tasks in the GE structure, our proposed FS algorithm can reduce the mean makespan by about 10, 25, 30, 38 and 49 percent compared with CP, GenDoc, ITAGS, Rounding and Greedy, respectively. That is because FS is specifically designed for homogeneous scenarios and can be more efficient than other algorithms designed for the general ODT-SC problem.

Impact of the Value of Ω on Makespan. The fourth set of simulations shows the mean makespan by changing the value of Ω, i.e., for each task, the percentage of edge nodes that are configured with required services. The results are shown in Figs. 8 and 9, where the horizontal axes are the value of Ω. As the value of Ω increases, the mean makespan decreases under all algorithms and CP/FS always achieve lower mean makespan than other algorithms. Fig. 8 shows the results for the general ODT-SC problem with different DAG structures. We observe that CP always achieves the lower makespan compared with the other algorithms. For example, for each task, if we only install required services on 40 percent of edge nodes in the GE structure, CP will achieve mean makespan of 6368, while ITAGS, Rounding, GenDoc and Greedy can achieve mean makespans of 10024, 10045, 11103 and 13141, respectively. That means CP reduces the mean makespan by about 36.4, 36.7, 43 and 51.5 percent compared with ITAGS, Rounding, GenDoc and Greedy, respectively. When calculating the offloading scheme, CP will take the service caching conditions on each node into account. Thus, compared with other benchmarks, CP will make full use of the limited services on each node and achieve lower makespan.


Fig. 8.
Mean makespan versus the value of Ω for heterogeneous scenario.

Show All


Fig. 9.
Mean makespan versus the value of Ω for homogeneous scenario.

Show All

The results for homogeneous scenarios with different DAG structures are shown in Fig. 9. Regardless of the proportion of the deployed services, FS always achieves lower makespan compared with the other algorithms. For example, when the value of Ω is 0.2 in Fig. 9b, FS reduces the mean makespan by about 13.1, 27.1, 27.1, 36.9 and 44.9 percent compared with CP, GenDoc, ITAGS, Rounding and Greedy, respectively. Besides, GenDoc performs better than ITAGS in homogeneous scenarios, especially when the value of Ω increases.

Impact of the Communication-to-Computation Ratio on Makespan. The fifth set of simulations evaluates the mean makespan by changing the communication-to-computation ratio. Specifically, we investigate the impact of inter-nodes communication time on mean makespan. As the communication-to-computation ratio increases, the communication delay will much impact the mean makespan. The results are shown in Figs. 10 and 11. Fig. 10 presents mean makespan under different communication-to-computation ratio for homogeneous scenarios with different DAG structures and CP always achieves the minimum makespan. For example, when the ratio is 0.2 in the GE structure, CP can reduce the mean makespan by about 29, 34, 36 and 52 percent compared with ITAGS, GenDoc, Rounding and Greedy, respectively.


Fig. 10.
Mean makespan versus communication-to-computation ratio for heterogeneous scenario.

Show All


Fig. 11.
Mean makespan versus communication-to-computation ratio for homogeneous scenario.

Show All

The impact of the communication-to-computation ratio on makespan for homogeneous scenarios is shown in Fig. 11. FS achieves minimum makespan than other algorithms all the time and GenDoc performs better as the communication-to-computation ratio increases. That is because GenDoc may place one task on multiple edge nodes to avoid the communication overhead.

Impact of the Number of Edge Nodes on Makespan. The last set of simulations illustrates the impact of the number of edge nodes on mean makespan. As shown in Figs. 12 and 13, with the increasing number of available edge nodes, the mean makespan of all tasks decreases for all algorithms. That is because more edge nodes can provide more computing resources for tasks. For heterogeneous scenarios, CP achieves less mean makespan than the other algorithms. For example, when there are 20 edge nodes with the GE structure, CP reduces the mean makespan by about 26, 34, 38 and 52 percent compared with GenDoc, ITAGS, Rounding and Greedy, respectively. Fig. 13 shows the results for homogeneous scenarios. Regardless of the number of edge nodes in MEC, FS always gets the smaller mean makespan than other algorithms. For example, when there are 10 edge nodes with the FFT structure in MEC, FS can reduce the mean makespan by about 10, 17, 18, 28 and 40 percent compared with CP, GenDoc, ITAGS, Rounding and Greedy, respectively. By obtaining the favorite successor for each potential node, FS can achieve better makespan performance than other algorithms.

Fig. 12. - 
Mean makespan versus number of edge nodes for heterogeneous scenario.
Fig. 12.
Mean makespan versus number of edge nodes for heterogeneous scenario.

Show All


Fig. 13.
Mean makespan versus number of edge nodes for homogeneous scenario.

Show All

From these simulation results, we can draw some conclusions. First, CP reduces the mean makespan by about 21-47 percent compared with the other algorithms in heterogeneous scenarios. Second, FS can achieve better performance than CP and reduce mean makespan by about 20-45 percent compared with the other alternatives in homogeneous scenarios. Third, our proposed CP/FS substantially outperform other algorithms, over a wide range of parameters including the number of tasks, the value of Ω, the communication-to-computation ratio and the number of edge nodes.

6.4 Small-Scale Testbed
Implementation. The prototype system consists of one central controller, four edge nodes, one local device and one remote cloud, as shown in Fig. 14. The central controller, running on a server with a core i9-10900 processor, 64GB RAM, 2TB hard disk and up to 1900Mbps wireless NIC, executes the proposed algorithms to determine the task placement and scheduling scheme. To solve the convex programming problems on the central controller, we embed the API provided by CPLEX 12.3. The local device, running on a server with a core i5-3470 processor, 8GB RAM, 1TB hard disk and up to 1900Mbps wireless NIC, caches all the task requests. For the four servers who are performing the edge nodes, each of them has an Intel i7-8700 CPU, 16GB RAM, 1TB hard disk and up to 1900Mbps wireless NIC. We use a server equipped with a core i9-10900 processor, 64GB RAM, 2TB hard disk and up to 1900Mbps wireless NIC as the remote cloud. All seven servers are connected through a 3200Mbps router. Note that, in order to estimate the high communication delay between the remote cloud and the local device (or edge nodes), we place the server that acts as the remote cloud far away from the router. Due to the different distance and random noise between these servers and the router, the actual-measured network speed limit between the five servers (acting as edge nodes and the local device) is between 5-13.5 MB/s, while the actual-measured network speed limit between the server acting as the remote cloud and other nodes is between 0.6-2.7 MB/s. To run tasks, we install Hadoop 3.3.0 on our testbed.

Experimental Inputs and Results. We run three Word-Counting applications simultaneously with input size of 1GB, 2GB, and 3GB, respectively on our testbed in Hadoop 3.3.0. Each of the applications consists of several mappers and reducers. Apparently, every reducer can start only after its corresponding mappers complete, which indicates the inherent dependence. In order to simulate the service caching constraints, only 2 edge nodes and the remote cloud can execute mappers.


Fig. 14.
Prototype system

Show All

In the experiment, we run CP, FS, GenDoc, ITAGS, Rounding and Greedy on the testbed and record the makespan performance. The makespans of different algorithms are shown in Fig. 15. We observe that CP can reduce makespan by about 7.5, 16.4, 20.5, 21.8 and 20.7 percent compared with FS, GenDoc, ITAGS, Rounding and Greedy, respectively. Note that, for the completeness of experiments, we run the FS algorithm in this heterogeneous scenario. The experimental results show that our CP algorithm performs better in the heterogeneous scenario than other algorithms including FS. In addition, due to the small scale of the experiment, the percentages of performance improvement of our algorithms are smaller than the performance improvement in large-scale scenarios. Through the experimental results, we believe that our proposed algorithms can achieve satisfactory performance in real scenarios.


Fig. 15.
Makespans of different offloading algorithms

Show All

SECTION 7Conclusion
In this paper, we have studied the problem of offloading dependent tasks with service caching to minimize the makespan (ODT-SC). We have proved that there exists no constant approximation algorithm for ODT-SC. A convex programming based algorithm has been designed to solve this problem. Moreover, we have studied the special case for the ODT-SC problem (i.e., homogeneous scenario) and proposed an approximate algorithm with bounded approximation factor to solve this practical case. Extensive simulation results have shown the high efficiency of our proposed algorithms.