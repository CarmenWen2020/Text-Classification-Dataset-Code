Load balancing is directly associated with the overall performance of a parallel and distributed computing system. Although the relevant problems in communication and computation have been well studied in data center environments, few works have considered the issues in an Internet of Things (IoT) edge scenario. In fact, processing data in a load balancing way for the latter case is more challenging. The main reason is that, unlike a data center, both the data sources and the network infrastructure in an IoT edge system can be dynamic. Moreover, with different performance requirements from IoT networks and edge servers, it will be hard to characterize the performance model and to perform runtime optimization for the whole system. To tackle this problem, in this work, we propose a load-balancing aware networking approach for efficient data processing in IoT edge systems. Specifically, we introduce an IoT network dynamic clustering solution using the emerging deep reinforcement learning (DRL), which can both fulfill the communication balancing requirements from IoT networks and the computation balancing requirements from edge servers. Moreover, we implement our system with a long short term memory (LSTM) based Dueling Double Deep Q-Learning Network (D3QN) model, and our experiments with real-world datasets collected from an autopilot vehicle demonstrate that our proposed method can achieve significant performance improvement compared to benchmark solutions.
SECTION 1Introduction
Edge computing has shown to be an efficient solution for data processing in Internet of Things (IoT) scenarios [1], [2]. However, with the advances of IoT technologies in recent years, data flows in IoT networks have become significantly complex and started to challenge the performance of the current IoT edge systems. For example, IoT data becomes increasingly larger (e.g., images and video stream), and the data can come from heterogeneous and dynamic IoT devices rather than homogeneous static ones in a conventional case.

To perform data communication in IoT networks and data computing on edge servers in a highly efficient way, in this work, we introduce a load-balancing aware network control method for IoT edge systems. The main motivation is that load balancing plays a critical role in the runtime and operation of a distributed computing system [3]. Specifically, it can avoid network congestion and server overwhelming through effective data distribution, and consequently improve the overall performance of a system.

In fact, various advanced load balancing strategies have been proposed for data center systems in the context of cloud and high-performance computing. However, few studies have ever considered the relevant issues in communication and computation in an IoT edge scenario. Moreover, although the problems on data collection and parallel computing have been widely studied in the domains of communications and computing respectively, almost all the relevant works in the two fields are independent of each other. Generally, the research on optimizing load balancing in an IoT edge system meets two main challenges:

Different Requirements: The data collection of IoT networks and the parallel computing of edge servers have different requirements. On the one hand, most data collection solutions in IoT focus on increasing communication efficiency, such as the total number of hops in multi-hop communication. On the other hand, most parallel computing solutions in edge servers focus on balancing the collected data size.

Dynamic Networks: The data collection process should not only consider the topology of the IoT network, but also the dynamic data generated by the network. For example, when mobile objects move into the sensing range of some IoT devices, the networking strategies should adapt the data collection schedule to the features of data flow, so that the amount of data collected in edge servers and communication load in network clusters can be balanced respectively.

In this paper, we propose a load-balancing aware networking approach to address the two challenges above. Specifically, we introduce an IoT network dynamic clustering solution with deep reinforcement learning (DRL) for an IoT edge system, which enables both communication balancing and computation balancing in IoT networks and edge servers respectively. In general, the main contributions of our work are summarized as follows.

We propose a DRL-based clustering solution for IoT edge systems with mobile objects. In each network cluster, the data from cluster member devices is collected and forwarded to the edge servers. The data size depends on whether mobile objects are located inside the sensing range of the IoT devices. The DRL model dynamically controls network clustering to adapt to the position of mobile objects, and to balances the communication load and the computation load for IoT networks and edge servers respectively.

To improve the proposed DRL model, we leverage the data collected from a real IoT scenario to train the DRL model via a Long Short Term Memory (LSTM) model. In detail, given a few real-world data samples, we generate as much simulation data as needed via an LSTM model for training the DRL model.

We further improve the performance of the DRL model by predicting the location probability distribution of mobile objects. Compared to existing DRL solutions, the input of our DRL model includes not only the current system state, but also the predicted states of mobile objects by LSTM.

Our solution is extensively evaluated based on the data collected from a real-world autopilot vehicle system. The experimental results show that the proposed solution can achieve much better performance than existing solutions.

The remainder of this paper is organized as follows. The related work is discussed in Section 2. The system model is presented in Section 3. The DRL-based solution is proposed in Section 4. The experimental results are presented in Section 5. Finally, we conclude the paper in Section 6.

SECTION 2Related Work
As an emerging technology, DRL [4] is very suitable to cope with complex optimization problems with high-dimensional state spaces. The general workflow of a DRL model includes observing system state, estimating the reward of possible actions, and deciding on which actions should be taken under the current state. The target of the DRL model is to maximize the long term reward by selecting a series of actions in the training environment [5].

In fact, DRL has been widely used for complex resource management in IoT [6], [7]. For example, the work [8] presents a resource allocation mechanism based on DRL in vehicle-to-vehicle communications. The solution makes each agent optimize sub-band and transmission power for satisfying the latency constraints using distributed local information. [9] proposes a self-adaptive DRL approach to control the communication flow in a wireless mesh network. The solution increases the optimization upper bound of the DRL model by adaptively changing the clustering pattern of wireless mesh networks. The work [10] proposes a semisupervised DRL model for managing the data of smart city IoT applications. Its key property is using both labeled and unlabeled data to improve the performance of DRL model. [11] proposes a quality of experience model to evaluate the qualities of IoT and a DRL-based resource allocation solution to improve the accuracy of the quality of experience model. [12] utilizes DRL for multi-access control and battery prediction of energy harvesting IoT systems. It proposes a two-layer DRL model to simultaneously optimize the access control considering the battery and channel states and minimize the battery prediction error. Moreover, to optimize the power transmission control in wireless networks, the work [13] develops distributed dynamic power allocation scheme in IoT networks by DRL.

In recent years, with the growing popularity of edge computing, how to perform efficient data processing in an IoT edge computing system has received significant attention from both academia and industry [14]. In a typical IoT edge system, IoT devices generate a large amount of data, and offloads the computing-intensive workload to the servers at the edge of the wireless network. Among various advanced approaches on IoT edge data processing, DRL has shown to be a competitive solution. Specifically, it has demonstrated its strong capability on solving various complex optimization problems in IoT edge networks like scheduling and coordinating. For example, the work [15] proposes a DRL solution to optimize task offloading decisions and wireless resource allocations considering the states of wireless channels. The work [16] utilizes DRL for computation off-loading and multi-user scheduling in IoT edge systems, which aims to minimize the long-term delay and power consumption. The work [17] presents a DRL-based solution to generate resource allocation decisions for optimizing network latency and power consumption, and the work [18] designs an IoT-based energy management system in edge computing infrastructure using DRL.

Although all the above DRL solutions can efficiently manage network resources and consequently to speed up data processing, they are not designed for dynamic IoT network topology. Moreover, none of them has ever studied the problem of load balancing as we have described. In comparison, our DRL solution can dynamically reconstruct IoT network clustering of edge servers when mobile devices move in the network. Therefore, it can significantly improve the load balancing of data communication and computation in IoT networks and edge servers respectively.

SECTION 3System Model
IoT devices usually generate large amounts of data that require further processing. However, it is always hard for them to process the data in an efficient way, due to hardware limitations [19]. Moreover, transferring data from IoT to a remote cloud server involves significant communication costs, which could result in communication delay [20] and bring in privacy problems [21]. To remedy these issues, edge computing has become an attractive solution in recent years [22]. In this work, we use a typical edge computing system architecture [23] [14], the model of which is illustrated in Fig. 1. In the model, the IoT network consists of randomly deployed homogeneous end devices, which are responsible for environment sensing and data transmission. In the meantime, the edge servers, which are physically close to the IoT network, are used to process the collected data from the IoT devices.


Fig. 1.
The model of an IoT edge system. The mobile object moves in the area of the IoT network. The IoT network is partitioned into clusters. The IoT devices send data to their corresponding edge servers.

Show All

Generally, to collect data from IoT devices to edge servers, the network is partitioned into clusters [24], [25], [26]. Each edge server is responsible for collecting and processing data from the member nodes of a corresponding cluster. In each cluster, the IoT devices form a mesh network and transmit data packets to the edge server by multi-hop communication. Each IoT device senses its physical surrounding area periodically. If the IoT device does not detect any mobile object, it periodically sends a message with standard payload (called a “standard-message”) to the edge server of its cluster. If a mobile object is detected, the IoT device sends a message that includes sensing data on top of the standard payload (called a “sensing-message”). Without loss of generality, we assume the size of a sensing-message is larger than the size of a standard-message. For example, suppose vehicles move in the deployment area of the IoT network and each IoT device carries a proximity sensor and a surveillance camera. If no vehicles are detected in proximity, IoT devices send only standard-messages with no sensing data. If vehicles are detected, the surveillance camera on the IoT device captures an image, and the IoT device sends a sensing-message containing the image to the cluster server. The aim of the system control is to dynamically vary the clustering for balancing the data size collected on edge servers and communication load on clusters, while mobile objects move in the IoT area. The main parameters used in our system design and implementation are described in Table 1.

TABLE 1 Main Parameters Used in the System Design and Implementation

An example of how a mobile object affects the balancing of network clustering is shown in Fig. 2. To simplify the explanation, we only consider the balancing of the data size collected to the edge servers. Suppose the size of the standard-message sent from the IoT devices that do not detect the mobile vehicle is x, and the size of a sensing-message sent from the IoT devices that detect the mobile vehicle is 2x. To balance the data size collected in the edge servers, as the mobile object moves from the location in Figs. 2a and 2b, the clustering pattern should also be changed accordingly. Clearly, it would be more difficult to find the clustering pattern for simultaneously balancing communication load within clusters and data storage in edge servers.


Fig. 2.
An example of how a mobile object affects the balancing of network clustering. To keep the size of the collected data balanced in the edge servers, the clustering solution varies as the mobile object moves from the location in (a) to (b).

Show All

SECTION 4DRL Solution
In this section, we present the design of the DRL model, which is responsible for controlling the clustering of the network (Section 4.1). Then we use LSTM model to train the DRL model (Section 4.2) and improve the performance of the DRL model (Section 4.3).

4.1 DRL Model
We choose Dueling Double Deep Q-Learning Network (D3QN) model as the DRL solution. D3QN consists of double DQN [27] and dueling DQN [28], which are the extensions of natural Deep Q-Learning Network (DQN) [29]. This choice is because, among all the DQN derivative models [30], D3QN has a relatively simpler structure and higher performance. Our solution does not aim to find the optimal load balancing of communication and storage. Instead, we use the D3QN model to find a dynamic clustering solution for sub-optimal load balancing.

4.1.1 Actions
Suppose edge servers Z={z1,...zi,...zn} with i∈[1,n] are deployed in an IoT network with M nodes. The network is partitioned to clusters C={c1,...ci,...cn} with i∈[1,n]. Each edge server resides in a cluster. |Z|=|C|. zi is responsible for collecting data from ci.

We partition the network into clusters as follows. First, we define a location oi called cluster-core for cluster ci. In the initialization, the cluster-core oi is set at the location of zi. Second, a network node that is closest to oi is selected as the cluster head hi of ci. Third, we partition the network to clusters C={c1,...ci,...cn} with i∈[1,n] based on the selected cluster heads. We use Multiplicatively Weighted Voronoi Clusters (MWVC) [31] to partition the network as this clustering solution can efficiently cope with the non-uniform density nodes in IoT networks.

We define the actions of a cluster-core as A, which includes seven moving actions: “Up, Down, Left, Right, Stay, Shrink and Expand”. The actions “Up, Down, Left, Right, Stay” are used to control the location of cluster-core. The actions “Shrink and Expand” are used to control the multiplicative weight of each cluster head, which further affects the size of the cluster. While mobile objects move in the network, the D3QN model selects an action on a cluster-core at each time step. After that, we partition the network to clusters based on the new locations of cluster-cores. An example of moving cluster-cores and network clustering is shown in Fig. 3.


Fig. 3.
Use cluster-core to partition the IoT network. The action of cluster-core includes Up, Down, Left, Right, Stay, Shrink and Expand.

Show All

4.1.2 States
We define the input state data st of D3QN model as an array value including the state data of the IoT network and of the mobile object. Specifically, st contains the adjacency matrix of the IoT network, the current cluster ID of each IoT device, the state (coordinate values) of the mobile object at time t, and the predicted state of a mobile object at time t+1. A detailed explanation about the predicted state of a mobile object is presented in Section 4.2.

4.1.3 Reward
Load balancing is a widely used solution for optimizing the performance of IoT networks [32], [33] and edge servers [34], [35]. For the dynamic system environment as we study in this work, the D3QN model is used to simultaneously balance the size of data collected by each edge server as well as the communication load in each network cluster. Without loss of generality, in our system, we assume that the communication capability of each IoT device is the same, and the computing and storage capability of each edge server is the same. Moreover, the data transmitted from IoT devices to edge servers include ”standard-message” and ”sensing-message”, which has been explained in Section 3. In such scenarios, the reward value of the D3QN model consists of two parts: one is used to describe the communication load balancing and the other is for the workload balancing, which can be indicated by the size of data received by each edge server. Specifically, the two balancing metrics are defined as:

Balancing of Communication Load in Clusters: Define the total number of communication hops for collecting data to edge server Z={z1,...zi,...zn} as B={b1,...bi,...bn} with i∈[1,n] respectively. To reward balanced communication hops in clusters, we define the reward value as
Γ=1n∑i=1n⎛⎝1−∣∣bi−B¯¯¯¯∣∣B¯¯¯¯⎞⎠.(1)
View Source

Balancing of Data Size in Edge Servers: Define the data size collected to each edge server Z={z1,...zi,...zn} as D={d1,...di,...dn} with i∈[1,n] respectively. To reward balanced data size in edge servers, we define the reward value as
Δ=1n∑i=1n⎛⎝1−∣∣di−D¯¯¯¯∣∣D¯¯¯¯⎞⎠.(2)
View Source

The reward value of the D3QN model at time t is defined as rt=Γt×Δt, where Γt and Δt are the values of Γ and Δ at time t respectively. By this reward design, the DRL model could explore a strategy to balance the load of communication and computing at the same time. The cumulative discounted reward of the D3QN model at time t is defined as Rt=∑∞τ=tγτ−trτ where γ∈[0,1] is the discount factor.

It should be noted that we focus on an intelligent method to enable load-balancing aware network control in this work. In fact, the above Equations (1) and (2) can be generalized to define DRL reward for the case that the IoT devices and edge servers are heterogeneous. Taking the computing balancing of edge servers for example, instead of using the absolute value of parameters, we can use the rate value of available resources in each server, such as the utilization rate of CPU resources [36]. Then, we can define the load balancing reward of edge servers as Ω=1n∑ni=1(1−∣∣wi−W¯¯¯¯¯¯∣∣W¯¯¯¯¯¯), in which wi represents the rate of available computing resources in edge server i and W={w1,...wi,...wn} with i∈[1,n]. Additionally, many other metrics can be used to evaluate the load balancing of IoT networks and edge servers, such as transmission delay, packet dropping rate, and data processing throughput.

4.1.4 Q-Value
Define Q(st,at) as the maximum expected reward by following strategy π, which is expressed as maxπE[Rt|st,at,π]. We build the D3QN model to find the policy producing the maximum Rt. Compared with natural DQN, Double DQN [27] decouples the selection of an action from the generation of target Q value, and Dueling DQN [28] changes the neural network structure comparing to natural DQN. To merge these two approaches, we first use Dueling DQN to calculate the Q value, in which its Q estimation is separated to two streams, including the value function Qv and the advantage function Qa, as follows.
Q≡Qv(st)+[Qa(st,at)−1|Qa|∑at+1Qa(st,at+1)].(3)
View Source

After that, we use the equation of Double DQN to calculate Qv and Qa separately, which is expressed as follows.
Qv≡rt+γQv[st+1,argmaxat+1Qv(st+1,at+1)](4)
View Source
Qa≡rt+γQa[st+1,argmaxat+1Qa(st+1,at+1)].(5)
View Source

4.2 Training DRL by LSTM
To apply the DRL model to a real mobile IoT edge system, we need a training environment that is as similar to the real-world environment as possible. In the real-world environment, some system states are static, such as the location of fixed IoT devices. These parameters can be easily and accurately simulated. Meanwhile, some other system states are dynamic. Take the scenario that vehicles transport cargo for example, the moving pattern of the vehicles depends on multiple factors, such as locations of cargos and traffic conditions on the road. In general, the more variety the training environments has, the more adaptive the DRL model will be, in the presence of the changes of the deploying environment. However, building a simulation training environment covering a wide variety is outside the scope of this paper.

Since the location tracking data collected from real-world mobile objects is limited, it becomes hard to train an effective DRL model in our case. To cope with this issue, based on a limited amount of location tracking data, we use an LSTM [37] model to produce simulating tracking data of the mobile objects, which is further used in the training of the DRL model. LSTM is a recurrent neural network (RNN) architecture [38]. Among various deep learning models, we select LSTM model, because it represents a typical solution that can catch the short-term and long-term locations and velocities of mobile objects.

The simulating tracking data of mobile objects is produced in the following steps. First, we discretize the tracking location of the vehicle by slicing the whole IoT deployment area into square grids. In this way, the position of a moving vehicle falls inside these square grids. In each square, we use the square center to represent the location of the mobile vehicle. This operation is shown in Fig. 4. Second, we define the moving direction of the mobile object Y in the discretized moving trace, including Up, Down, Left, Right, Up-Right, Down-Right, Up-Left, Down-Left, Stay. In this way, the LSTM model can learn the moving pattern of a mobile object based on the trace data. Third, we use the LSTM model to produce a series of moving direction data. Specifically, we feed an object's location tracking data to the LSTM model. The LSTM model uses the current location of a mobile object as input, and predicts the next moving direction. The predicted state with the highest probability of LSTM prediction model is selected as the simulating state of mobile objects, i.e., simulated location tracking data. The data flow of training is shown in Fig. 5, in which the current state is st, the next possible states are {s0t+1,...,sgt+1} with probability {P0,...,Pg} respectively. Then the simulating mobile object moves to the predicted location, and the LSTM model predicts the moving direction again. The series of the predicted moving direction data is used as the simulated moving trace of the mobile object. This process continues until the produced location tracking data is enough for training the DRL model.


Fig. 4.
Discretize the moving location of a mobile object.

Show All


Fig. 5.
LSTM model produces simulating data of mobile objects by learning and predicting the tracing data.

Show All

4.3 Decision-Making by DRL and LSTM
The LSTM prediction model is not only used to produce training data for the DRL model, but also for the input state for the decision-making of the DRL model. Precisely, we use the predicted moving directions of the mobile object as part of the input state of the DRL model.

A naive approach to connect the LSTM model and the DRL model is as follows. The output of LSTM model is the probability distribution of moving directions. The output prediction with the highest probability of LSTM model is selected as the input state of DRL model. However, the LSTM output state with the highest probability does not always make the DRL model output the action with the highest reward value. Take Fig. 6 for example. The current state is st. Suppose the LSTM model predicts the next state with the highest probability P3 is s3t+1, and the DRL model outputs action a3 with input state s3t+1. Meanwhile, suppose the LSTM model predicts the probability of next state s0t+1 and s1t+1 are P0 and P1 respectively. The DRL model outputs the same action a1 with input states s0t+1 and s1t+1. Although P0<P3 and P1<P3, the expected reward of action a1 could be larger than the reward of a3, i.e., (P0+P1)Q1>P3Q3.

Fig. 6. - 
An example of the naive approach to connect LSTM and DRL, in which the output states with the highest probability of the LSTM model is selected as the input state of the DRL model.
Fig. 6.
An example of the naive approach to connect LSTM and DRL, in which the output states with the highest probability of the LSTM model is selected as the input state of the DRL model.

Show All

Therefore, we should select an action that could produce the highest expected reward value in all the possible actions. In our solution, we input all the states with probability to the DRL model, instead of only the state with the highest probability. Then, the DRL model selects an action that could produce the highest expected reward value in all the possible actions. Our solution is shown in Fig. 7, in which ai represents the possible action in {a0,...,ah}, Qi is the Q-value of action ai, and Pi is the probability of the input state to the DRL model. The action selection is based on the following formula.
a=argmax(∑aiQi×Pi).(6)
View Source


Fig. 7.
Our solution to connect LSTM model and DRL model in the decision-making stage.

Show All

4.4 Workflow of Our Solution
The workflow of training the DRL model is shown in Fig. 8a. First of all, we collect the location tracking data of the mobile objects in the real-world environment. Then, we use these data for training the LSTM model. After that, the trained LSTM model produces simulating location tracking data. We build a training environment by the simulating the states of edge servers, IoT devices, and mobile objects. Finally, the DRL model is trained in this simulation environment.


Fig. 8.
The workflow of system-training and decision-making.

Show All

The workflow of decision-making is shown in Fig. 8b. The LSTM model predicts the probability of the next states for the mobile object. The states of edge servers, IoT devices, and the predicted states of mobile objects are input to the DRL model. We select the action with the maximum expected reward as the final output to the real-world environment.

4.5 Computational Complexity Analysis
We have combined LSTM and DRL together in our solution. To evaluate the complexity of our DRL approach, we perform a theoretical analysis of the computational complexity of the LSTM and D3QN models based on floating point operations (FLOPs), which is widely used to measure the computational complexity of deep learning models. Specifically, our analysis is based on the main parameter values we have used in our implementation, as reported in Table 2.

TABLE 2 The Parameters of D3QN Model and LSTM Model

In our DRL solution, the LSTM model has two fully connected hidden layers after the LSTM layer for output predicted directions as explained in Section 4.1. Denote the dimension of the two hidden layers are H1 and H2 respectively. The total number of FLOPs of the LSTM layer per computing step is (Il+Dl)×Dl×4×2, where Il and Dl are the input and the hidden dimension of the LSTM layer respectively [39]. The FLOPs of the first and the second hidden layers are Dl×H1l and H1l×H2l respectively, where H1l and H2l are the dimensions of the first and second hidden layers respectively. Therefore, the total FLOPs of the LSTM model per computing step is (Il+Dl)×Dl×4×2+Dl×H1l+H1l×H2l.

The FLOPs of the D3QN model mainly come from its fully connected deep neural network (DNN) model, which includes three hidden layers. Denote the input and output dimension of the neural network as Id and Ud respectively. The dimension of each hidden layer are H1d, H2d, and H3d respectively. Therefore the total FLOPs per computing step is (Id×H1d+H1d×H2d+H2d×H3d+H3d×Ud)×2 in the D3QN model [40], [41]. According to the D3QN action design in Section 4.1.1, Ud equals |C|×|A|, in which C and A represent the partitioned clusters and D3QN actions respectively. According to the D3QN state design in Section 4.1.2, Id equals M2+M+M+2+|Y|, in which M2, M, M, 2, and |Y| represent adjacency matrix of the network nodes, cluster ID of network nodes, data size produced by network nodes, coordinates of the mobile object, and the moving directions of the mobile object respectively.

From the analysis above, we can see that the computational complexity of LSTM is far less than D3QN. Therefore, compared to D3QN solution, combining LSTM to D3QN (Section 4.3) will only slightly increase the computational complexity in the process of decision making.

SECTION 5Experimental Results
In this section, we evaluate the performance of our DRL solution by a simulation environment and the moving trace of real-world vehicles.

5.1 Experimental Settings
We build a simulation environment, including edge servers, IoT devices, and mobile vehicles. In the simulation environment, the edge servers and IoT devices are randomly deployed in a 15m×15m area. The IoT devices monitor the neighbor area, and send the data to the edge servers for processing. The transmission range and detection range of each IoT device are both 3m. The communication speed between IoT devices is 10 KB/s. Each IoT device sends standard-message every 1 second. The size of a standard-message is STmesg=1 KB. Vehicles move in the area at a speed of 1m/s. If a vehicle is in the detection range of IoT devices, the IoT devices send sensing-messages to the edge servers. The sensing range of IoT devices for detecting nearby vehicles is 6m. The size of a sensing-message is SNmesg=3 KB.

To control the network clustering, we take actions on the cluster-core of the clusters as explained in Section 4.1. The moving step length of cluster-core is 1m. The expanding and shrinking rate of MWVC are 0.8 and 1.25 respectively. To calculate the convergence time length of training the DRL model, in each time step, we calculate the deviation value of the testing results over the next 400 epoch time steps. If the deviation value is smaller than 3%, then we consider the result starts to converge at that time step.

5.2 Experimental Models
To evaluate the performance of our approach, we build two simplified DRL models and two static clustering methods as benchmark solutions.

LL-D3QN: This is the complete version of our solution presented in Section 4, including D3QN model, LSTM-based training and decision-making.

L-D3QN: This solution uses both the D3QN model (Section 4.1) and the LSTM prediction (Section 4.3). Compared with the solution LL-D3QN, this solution does not train the D3QN model by the simulation data of LSTM model (Section 4.2). A limited amount of real-world location tracking data is used for training the D3QN model.

D3QN: This solution only uses the D3QN model. Compared with the solution L-D3QN, this solution only uses D3QN for decision-making.

Static Clustering with MWVC (SC-MWVC): This is the benchmark solution. The cluster-cores are fixed at the location of the edge servers. The IoT network is partitioned into MWVC clusters in initialization. After that, the clusters are fixed in the whole round of testing.

Static Clustering with Balanced Clustering (SC-METIS): We adopt the widely used METIS algorithm [42] for partitioning network into clusters. The aim of a balanced partitioning [43] is to partition a graph into a required number of subsets, in which the subsets have equal size and the number of edges linking different subsets is minimal.

5.3 Performance of D3QN Model
Before testing the complete DRL solution, we first evaluate the performance of the D3QN model, which does not include the LSTM model and is the key component in our DRL solution. In the test, we simulate the movement of mobile objects, including fixed route, random direction route, and random reflection route.

5.3.1 Fixed Route
In the first test, we randomly deploy 80 IoT devices and 4 edge servers. A mobile object moves between two randomly selected locations. We compare the D3QN model with the benchmark solution SC-MWVC. The testing results are shown in Fig. 9a. The converged reward value of D3QN is around 40% higher than SC-MWVC. The static clustering solution SC-MWVC has a constant result value, because the clustering pattern is fixed in the whole round of testing.


Fig. 9.
Testing results in scenarios with (a) a vehicle moving between two randomly selected locations; (b) a vehicle moving in a random direction mobility model; (c) multiple vehicles moving in a random reflection mobility model.

Show All

5.3.2 Random Direction Route
For the second test, we randomly deploy 20 IoT devices and 3 edge servers. A mobile object moves according to a random direction mobility model [44]. Specifically, we set a random location and moving direction for the mobile object in the initialization. Then, if the mobile object arrives at the border of the deployment area, it selects a new random direction. The testing results are shown in Fig. 9b. The converged reward value of D3QN is around 5 times higher than SC-MWVC. At the same time, the convergence speed of the D3QN model is slower than the testing result in Fig. 9a. This is mainly because the movement route of this test is more complicated.

5.3.3 Random Reflection Route
In the third test, we evaluate the performance of the D3QN model in the scenario with multiple mobile objects and random direction mobility model. We randomly deploy 20 IoT devices and 3 edge servers. The mobile objects move according to a random direction mobility model. When the mobile object arrives at the border of the deployment area, the reflecting direction of the input direction is named as μ. In this test, the new moving direction is randomly selected in the range [μ−15∘,μ+15∘]. We test the D3QN and the SC solutions with one and three mobile objects respectively. The testing results of the D3QN and SC-MWVC are shown in Fig. 9c. Compared with the SC-MWVC solution, our D3QN achieves much higher reward values. As the number of mobile objects increases, the reward values of both D3QN and SC-MWVC decrease. Because it is more difficult to find a balanced clustering with more mobile objects.

5.4 Performance of Our DRL Solution
We compare the performance of DRL and benchmark models in this section. The experimental data is from a real-world transportation scenario in a warehouse, where an autopilot vehicle automatically takes cargo from one location to another as shown in Fig. 10a. To obtain location tracking data of an autopilot vehicle, we deploy a vehicle in a 15m×15m warehouse for cargo transportation, and record the location of the vehicle every 1 second for around 8 hours. Fig. 10b shows the diagram of the real-world deployment scenario. Fig. 10c shows the moving trace of the vehicle in the warehouse. After collecting data, we discretize the moving trace by 0.025m×0.025m squares as explained in Section 4.2. We split the discretized location data into two parts, which are used for training and testing the DRL model respectively.


Fig. 10.
The experiment is based on the data collected from the scenario that an autopilot vehicle transports cargo automatically in a warehouse.

Show All

5.4.1 Size of Training Data Set
We evaluate the performance of the solutions by varying the size of the training data. The network has 80 IoT devices and 4 edge servers. We calculate the average and deviation value of 10 rounds of tests. In each round of test, we randomly reset the location of IoT devices and edge servers, while keeping the other experimental settings the same. The experimental results of using 100%, 50%, and 25% training data set are shown in Fig. 11.

Fig. 11. - 
Reward values and convergence time steps of the solutions in various percent of training data.
Fig. 11.
Reward values and convergence time steps of the solutions in various percent of training data.

Show All

Compared with SC-MWVC, the reward of LL-D3QN improves 205%, 197%, and 198% in 25%, 50%, and 100% training data set respectively. In addition, the rewards of LL-D3QN are quite stable, which are 0.68, 0.66, and 0.66 in 25%, 50%, and 100% training data set respectively. This shows that LL-D3QN is resilient against various sizes of the training data set, while the performance of the other solutions can be negatively affected. Meanwhile, the result shows that the performance of D3QN is lower than LL-D3QN. When there are 25% of training data, the reward value of LL-D3QN is 31% higher than D3QN. This shows that the LSTM model can effectively improve the performance of D3QN. The reward of SC-METIS improves 87% on average compared to SC-MWVC in 25%, 50%, and 100% training data set respectively. This is because the METIS clustering solution is designed for balanced partitioning, which can effectively increase the balancing reward.

The convergence time of L-D3QN is on the same level as D3QN. This shows that the LSTM model does not significantly increase the computational complexity of the D3QN model, which validates the analysis of computational complexity in Section 4.5. In comparison to D3QN and L-D3QN, the convergence time for training LL-D3QN model is much higher. The convergence time of LL-D3QN is 103%, 109%, and 110% higher than D3QN in 25%, 50%, and 100% training data set respectively. When there are 100% of training data, the convergence time of LL-D3QN increases 122% and 110% compared to L-D3QN and D3QN respectively. The main reason is that LL-D3QN trains the D3QN model by the simulation data of the LSTM model.

5.4.2 Number of Edge Servers
We evaluate the performance of our solution for various numbers of edge servers. The network has 80 IoT devices, and only 25% of the training data set is used for training. The other experimental settings are the same as the experiment of Section 5.4.1. The experimental results of 2, 3, and 4 edge servers are shown in Fig. 12.


Fig. 12.
Reward values and convergence time steps of the solutions in various numbers of edge servers.

Show All

Compared to SC-MWVC, LL-D3QN performs significantly better in all three sets of tests. Compared with SC-MWVC, the reward of LL-D3QN improves 262%, 194%, and 205% in 2, 3, and 4 edge servers respectively. We found that the reward values of LL-D3QN decrease as the number of edge servers increases. When there are 2 edge servers, the reward of LL-D3QN reaches 0.89, which is 21% and 31% higher than 3 and 4 edge servers respectively. The main reason is that it becomes more difficult to find balanced clustering as the number of edge servers increases. Meanwhile, the improvement from D3QN and L-D3QN to LL-D3QN increases slightly as the number of edge servers increases. The reward of LL-D3QN is 12%, 18%, and 31% higher than D3QN when there are 2, 3, and 4 edge servers respectively. The same as the testing results in Section 5.4.1, LL-D3QN has the highest convergence time in 2, 3, 4 edge servers, which is 112%, 79%, and 103% higher than D3QN.

5.4.3 Number of IoT Devices
We evaluate the performance of our solution for various numbers of IoT devices. The network has 4 edge servers, and 25% of the training data set is used for training. The other experimental settings are the same as the experiment of Section 5.4.1. The experimental results of 20, 40, and 80 IoT devices are shown in Fig. 13.


Fig. 13.
Reward values and convergence time steps of the solutions in various node numbers.

Show All

As the number of IoT devices increases, the reward value of LL-D3QN increases. The reward values of LL-D3QN are 0.51, 0.57, and 0.68 when there are 20, 40, and 80 IoT devices respectively. The main reason is that, as the IoT device number increases, the state space of the clustering pattern increases, which further improves the converged reward value of the DRL model. At the same time, the improvement from SC-MWVC to LL-D3QN increases 403%, 475%, and 205% respectively. The same as the experiments in Sections 5.4.1 and 5.4.2, LL-D3QN keeps the highest convergence time in 20, 40, 80 IoT devices, which is 95%, 75%, and 103% higher than D3QN.

5.4.4 Ratio of Message Sizes
In our final experiment, we evaluate the performance of our solution with respect to various values of SNmesg/STmesg. The network has 4 edge servers and 20 IoT devices. 25% of the training data set is used for training the models. The other experimental settings are the same as the experiment of Section 5.4.1. The experimental results with SNmesg/STmesg equaling 2, 3, and 4 are shown in Fig. 14.


Fig. 14.
Reward values and convergence time steps of the solutions in various SNmesg/STmesg.

Show All

The reward values of LL-D3QN are the highest compared to the other solutions. The reward values of LL-D3QN are 0.60, 0.51, and 0.40 when SNmesg/STmesg are 2, 3, and 4 respectively. At the same time, as SNmesg/STmesg increases, the improvement from SC-MWVC to LL-D3QN decreases. The improvement from SC-MWVC to LL-D3QN increases 528%, 403%, and 290% when SNmesg/STmesg are 2, 3, and 4 respectively. This is mainly because a higher SNmesg/STmesg value is easier to cause an unbalanced load of communication and storage, and it is harder to find a balanced clustering pattern by the DRL model. The same as the previous experiments, LL-D3QN has the highest convergence time when SNmesg/STmesg equals 2, 3, 4, which is 136%, 103%, and 223% higher than D3QN.

SECTION 6Conclusion
This paper presents a DRL-based method for load balancing aware networking in IoT edge systems. Specifically, through the dynamic optimization of IoT network clustering using a DRL model, our proposed approach can fulfill the load balancing requirements from both IoT networks and edge servers. We have implemented our solution in a real-world autopilot vehicle transportation scenario, and our experimental results have demonstrated that our method can achieve significant performance improvement in load balancing compared to benchmark solutions.