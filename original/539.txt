Integrating user ends (UEs), edge servers (ESs), and the cloud into end-edge-cloud computing (EECC) can enhance the utilization of resources and improve quality of experience (QoE). However, the performance of EECC is significantly affected by its architecture. In this article, we classify EECC into two computing architectures types according to the visibility and accessibility of the cloud to UEs, i.e., hierarchical end-edge-cloud computing (Hi-EECC) and horizontal end-edge-cloud computing (Ho-EECC). In Hi-EECC, UEs can offload their tasks only to ESs. When the resources of ESs are exhausted, the ESs request the cloud to provide resources to UEs. In Ho-EECC, UEs can offload their tasks directly to ESs and the cloud. In this article, we construct a potential game for the EECC environment, in which each UE selfishly minimizes its payoff, study the computation offloading strategy optimization problems, and develop two potential game-based algorithms in Hi-EECC and Ho-EECC. Extensive experiments with real-world data are conducted to demonstrate the performance of the proposed algorithms. Moreover, the scalability and applicability of the two computing architectures are comprehensively analyzed. The conclusions of our work can provide useful suggestions for choosing specific computing architectures under different application environments to improve the performance of EECC and QoE.
SECTION 1Introduction
1.1 Motivation
The vigorous development of Internet of Everything (IoE) and artificial intelligence technologies has given rise to an intelligence era for human society [1]. For example, Hikvision has developed surveillance cameras that no longer only obtain videos and images as in the past, but have the ability to recognize and track objects. Mobile phones (e.g., Huawei, Apple, and Xiaomi) have also evolved from traditional communication devices to important carriers running various applications, such as electronic payment, home management, virtual reality, and other intelligent applications. Intelligent applications require the support of powerful computing. However, the resources possessed by user ends (UEs), such as mobile phones and Internet of Things (IoT) devices cannot run intelligent applications efficiently.

Deploying computing resources near the network edge is considered a promising solution to the above issue. Edge computing (EC) cannot only provide low-latency services for UEs but also guarantee the data security of UEs [2]. EC has been widely studied in many directions, such as computation offloading [3], caching [4], resource allocation [5], [6], and privacy protection [7]. However, the above work ignores an important fact, that is, edge servers (ESs) do not have the same ability to handle computation-intensive tasks as the cloud [8].

Although cloud computing (CC) has sufficient resources to support the requirements of computation-intensive tasks [9], it cannot solve the issue of long delay caused by data transmission [10]. Due to the shorter transmission distance and higher transmission rate between UEs and ESs, EC can reduce data transmission delay and is suitable for providing services for handling latency-sensitive tasks. Therefore, cooperation between EC and CC can better meet various user demands. Edge-cloud computing has been studied in various work [11], [12], [13], [14]. However, when the network is unstable or the resource competition between UEs is tight, it is better for a UE to rely on its own ability to handle some tasks. Therefore, UEs, ESs, and the cloud can be integrated into end-edge-cloud computing (EECC), which cannot only enhance the utilization of resources but also improve quality of experience (QoE) while ensuring quality of service (QoS).

Although some work has verified the effectiveness of EECC [15], [16], [17], its performance is significantly affected by its architecture, which has not been studied. Motivated by the above reality, we classify EECC into the following two types of architectures according to the visibility and accessibility of the cloud to UEs:

Hierarchical end-edge-cloud computing (Hi-EECC). As shown in Fig. 1, Hi-EECC is a three-tier architecture, and the cloud is invisible to UEs. In Hi-EECC, UEs can offload their tasks only to ESs. When the resources of ESs are exhausted or the QoS demands of UEs cannot be satisfied by the ESs, the tasks are uploaded to the cloud by the ESs. The service provided by the cloud is transparent for UEs.


Fig. 1.
Illustration of Hi-EECC.

Show All

Horizontal end-edge-cloud computing (Ho-EECC). As shown in Fig. 2, Ho-EECC is a two-tier architecture. Since the cloud resources are visible and accessible to UEs, both the cloud and ESs are in the second layer of Fig. 2, i.e., Edge-Cloud layer. In Ho-EECC, the cloud does not need to rely on ESs to provide services to UEs. UEs can request ESs and the cloud directly according to their own preferences.


Fig. 2.
Illustration of Ho-EECC.

Show All

In this paper, we investigate the computation offloading strategy optimization problems in Hi-EECC and Ho-EECC, and analyze the impact of the architectures on UEs and ESs. We comprehensively study the scalability and applicability of the two computing architectures in terms of the energy consumption of UEs, time consumption of UEs, resource utilization rate of ESs, application type, and user scale.

1.2 Our Contributions
To the best of our knowledge, this paper is the first work to optimize computation offloading strategy for UEs, and investigate the performance of EECC in different computing architectures. The contributions are as follows.

The computation offloading strategy optimization problems in Hi-EECC and Ho-EECC are investigated, and the impact of the two computing architectures on UEs and ESs is analyzed in detail.

Considering the selfishness of UEs, we construct a potential game for the EECC environment, in which each UE selfishly minimizes its payoff. We also develop two potential game-based algorithms according to the characteristics of computing architectures. The existence of Nash equilibrium, the convergence of the algorithms, and the performance of the algorithms are theoretically analyzed in detail.

Extensive experiments with real-world data are conducted to demonstrate the performance of the proposed algorithms. The scalability and applicability of two computing architectures are comprehensively analyzed in detail. Three important conclusions are presented for choosing specific computing architectures in the different application scenario.

The remaining content is outlined as follows. In Section 2, the related work is reviewed. System models are detailed in Section 3. The EECC game is formulated in Section 4. The potential game-based computation offloading algorithms are described in Section 5. The convergence and performance of the algorithms are theoretically demonstrated in Section 6. We conduct extensive experiments using real-word data to verify the proposed algorithms and theorems, and to analyze the two computing architectures in Section 7. Section 8 provides the conclusions of this paper and our future work.

SECTION 2Related Work
The computation offloading strategy optimization problem in EC has consistently been a hot research topic in industry and academia, and has been investigated extensively. Wu et al. [18] studied the problem in a multi-channel wireless interference environment, and proposed a distributed algorithm to minimize the total delay of all UEs. You et al. [19] optimized the offloading strategy by minimizing the weighted sum of energy consumption of UEs under the constraint of computation delay, and they considered the time-division multi-access and orthogonal frequency-division multi-access communication modes. Chen et al. [20] improved the long-term performance of EC by using the Lyapunov optimization technique, and proposed an online algorithm without requiring future information about user demands. Hu et al. [21] proposed a greedy-based pruning algorithm to select UEs that should offload their tasks to ESs and developed a non-cooperative game-based iteration algorithm to determine the final strategy.

However, the computing capacity of EC is limited relative to the cloud. It is necessary to integrate EC and CC into a collaborative computing architecture, thus improving the performance of the architecture and introducing higher levels of flexibility for various demands of UEs. Some work has studied the computation offloading optimization problem in the edge-cloud computing architecture. For example, Ren et al. [11] investigated the computation offloading optimization problem in the hierarchical edge-cloud computing architecture and developed a convex-based algorithm to decide the task slipping strategy. Shah-Mansouri et al. [12] developed a potential game-based algorithm to optimize the strategy in the horizontal edge-cloud computing architecture. Du et al. [13] considered the communication cost between co-resident and non-co-resident tasks, and designed an algorithm to obtain a suboptimal strategy. Fantacci et al. [14] formulated the problem as a queueing system model and determined the strategy by maximizing the rate of UEs whose QoS can be satisfied.

It is also necessary for UEs to perform some computations locally, which copes with wireless network problems, such as network disconnection and instability. Moreover, UEs can process real-time tasks (such as emergency stop and failure recovery) that are very sensitive to delay [22]. Very little work has studied the computation offloading strategy optimization problem in EECC. For example, Hong et al. [22] studied the multi-hop computation offloading strategy optimization problem. Peng et al. [23] optimized the offloading strategy based on the strength Pareto evolutionary algorithm. Sun et al. [24] developed a hierarchical heuristic approach to make offloading decisions. Wang et al. [15] investigated the application of EECC to an underwater acoustic sensor network.

The effectiveness of EECC in improving the overall performance of the computing architecture has been widely demonstrated [15], [16], [17]. However, the performance of different EECC architectures has not been studied. Specifically, there is no work that investigates the computation offloading optimization problem under the different computing architectures, and summarizes how to choose a specific architecture of EECC for the different application scenario. To fill this research gap, this paper develops potential game-based algorithms for UE optimizing offloading strategies, provides a performance analysis of the two types of EECCs under the different application scenario, and analyzes the impact of various factors on the cost of UEs and the resource utilization of ESs. The main observations concluded from the experiments can provide some useful suggestions for improving the QoE of UEs with various requirements. Section 7 explains these interesting observations in detail.

SECTION 3Models
3.1 System Model
Fig. 3 depicts the scenario studied in this paper. Table 1 lists the parameters and their definitions in this paper. We assume that there is a group of UEs N that can be served by a group of ESs M. We use UEn (n∈[1,|N|]) and ESm (m∈[1,|M|]) to represent the nth UE and mth ES, respectively. Additionally, there is a cloud that can provide computing resources to UEs. However, the visibility and accessibility of the cloud to the UE is different between Hi-EECC and Ho-EECC. In this paper, the demand of UEn not only reduces its cost but also requests the service delay to be less than the deadline determined by the UE. Because the resources of ESs are limited, the demands of UEs may not be satisfied when the requirements exceed the resource capacity of ESs. As shown in Fig. 1, in Hi-EECC, UEs can offload their tasks only to ESs to reduce their cost. A task of UEn is offloaded to ESm when the cost of UEn being served by the ES is less than the local execution cost. However, if UEn’s deadline cannot be guaranteed, the task will be further uploaded to the cloud by ESm. In other words, the service provided by the cloud in Hi-EECC is transparent for UEs.

TABLE 1 Summary of Notations and Definitions
Table 1- 
Summary of Notations and Definitions

Fig. 3.
Illustration of the scenario studied in this paper.

Show All

As shown in Fig. 2, in Ho-EECC, UEs can offload their tasks to ESs or the cloud according to their requirements. In addition, offloading decisions made by a UE should not only reduce its cost but also ensure that the service delay is less than its deadline.

By using high-speed wireless communication technology, continuous service can be provided for UEs with mobility. However, the distance between communication entities has become the main factor that affects the data transmission rate. To reflect this reality, we assume that there is a set of service areas I and use i∈[1,|I|] to represent the ith service area. Moreover, if UEn and ESm are in the same service area, the ES can respond to the request of UEn. As shown in Fig. 3, there are six service areas in the studied scenario. Since UE1, UE2, and UE3 are in the 1st service area, they can initiate requests only to ES1 in Hi-EECC. However, UEs can initiate requests to the cloud in Ho-EECC.

In EECC, the heterogeneous characteristics of UEs and ESs have always been challenges for optimizing computation offloading strategies. In this paper, UEn is specified by fn, γn, an, σn, and σ′n, where fn,γn,an,σn, and σ′n are the computing capability of UEn (i.e., CPU frequency, which is quantified by the number of cycles per second), the resource allocation weight parameter of UEn, and a task of UEn, the energy consumption (Joule, J) per second for UEn processing an, and the energy consumption per second for UEn uploading an, respectively. Since 1 Watt = 1 J/s, σn,σ′n are CPU execution and data transmission power of UEn. The resources of an ES are allocated proportionally to UEs. We use γn>0 to denote the proportion of resources that UEn can obtain from ESm among all UEs that send a request to the ES, which can be determined by the payment level of UEn [25]. Moreover, each UE has a task to be completed. The task of UEn is further defined as an≜(δn,ωn,ζn,dn). For an, δn is the data size of an, which is measured by the number of bits. ωn represents the number of CPU cycles needed to complete an. ζn∈I is the location of UEn when an is executed. dn denotes the maximum service delay that UEn can tolerate, i.e., the QoS requirement of the UE. ESm is specified by f~m, r~m, and ζ~m, where f~m is the computing capability of the ES, which is quantified by the number of CPU cycles per second. r~m is the data transmission rate of ESm. ζ~m∈I represents the location of ESm. The computing capability of the cloud is denoted as f^, which is quantified by the number of CPU cycles per second. We use SNi≜{UEn|ζn=i} and SMi≜{ESm|ζ~m=i} to denote the set of UEs and the set of ESs in the ith service area, respectively. Moreover, we assume that ESs within the same service area are the same, i.e., f~m=f~m′ and r~m=r~m′ for all ESm, ESm′∈SMi [26]. However, ESs in different service areas are heterogeneous. Furthermore, we use λn∈{0,1}, λ~n,m∈{0,1}, and λ^n∈{0,1} to represent the offloading decision of an. If an is executed locally, λn=1. Otherwise, λn=0. If an is offloaded to ESm, λ~n,m=1. Otherwise, λ~n,m=0. Similarly, λ^n=1 means that an is executed by the cloud. Otherwise, λ^n=0. In Hi-EECC, for UEn∈SNi, since an can be executed by one entity at a time, we have the following two constraints:
1=λn+∑m=1|M|λ~n,m=λn+∑ESm∈SMiλ~n,m.(1)
View SourceAccordingly, in Ho-EECC, we have
1=λn+∑m=1|M|λ~n,m+λ^n=λn+∑ESm∈SMiλ~n,m+λ^n.(2)
View Source

3.2 Computation Model
3.2.1 Local Computation Model
The computing capability of UEn is quantified by the number of CPU cycles per second, i.e., fn. Thus, the local computation delay of an is
tn=ωnfn.(3)
View SourceThe energy consumption for UEn executing an is calculated by the classic model used in [27], [28], i.e.,
en=σntn,(4)
View Sourcewhere σn can be obtained through the measurement approach [29], [30].

3.2.2 Edge Computation Model
The computing capability (i.e., computing resources) of ESm is represented by f~m and will be distributed proportionally to all UEs that request the ES. In this paper, the computing resource of ESm allocated to UEn is
fn,m=γn∑|N|v=1γvλ~v,mf~m=γn∑UEv∈SNiγvλ~v,mf~m,(5)
View Sourcewhere v∈[1,|N|], ∑UEv∈SNiγvλ~v,m is the sum of resource weight parameters of all UEs that request ESm and γn/(∑UEv∈SNiγvλ~v,m) is the resource proportion that UEn can obtain from ESm. According to the above equation, the computation delay of an executed by ESm is formulated as
t~n,m=ωnfn,m.(6)
View SourceUEn focuses on minimizing its energy consumption and does not care about the cost of ESs. Therefore, the energy consumption of UEn is zero when ESm is executing an.

3.2.3 Cloud Computation Model
Compared with ESs, the cloud has sufficient resources to respond to the requests of UEs. Thus, we assume that the cloud server can handle an infinite number of tasks in parallel. The computation delay of an executed by the cloud can be formulated as
t^n=ωnf^.(7)
View SourceRight-click on figure for MathML and additional features.Similarly, the energy consumption of UEn is zero when the cloud is processing an.

3.3 Communication Model
3.3.1 Communication Model between UEn and ESm
Similar to the computing resource allocation policy, the communication resources (i.e., the data transmission rate) of ESm are distributed proportionally to all UEs that request the ES. Thus, the data transmission rate of ES_m allocated to UE_n is \begin{equation*} r_{n,m}=\frac{\gamma _n}{\sum _{v=1}^{|\mathcal {N}|}\gamma _{v}\tilde{\lambda }_{v,m}} \tilde{r}_m =\frac{\gamma _n}{\sum \limits _{ \text{UE}_{v} \in \text{SN}_i} \gamma _{v}\tilde{\lambda }_{v,m}} \tilde{r}_m. \tag{8} \end{equation*}
View SourceBased on the above equation, the communication delay of a_n between UE_n and ES_m can be formulated as \begin{equation*} \tilde{t}^{\prime }_{n,m} = \frac{\delta _n}{r_{n,m}}. \tag{9} \end{equation*}
View SourceThe energy consumption of UE_n offloading a_n to ES_m is \begin{equation*} \tilde{e}_{n,m} = \sigma ^{\prime }_n \tilde{t}^{\prime }_{n,m}. \tag{10} \end{equation*}
View Sourcewhere \sigma ^{\prime }_n can be obtained by the long-term experience [31].

3.3.2 Communication Model between UE_n and the Cloud
The service mode of the cloud depends on the computing architecture type of EECC. Therefore, we formulate the communication models between a UE and the cloud in Hi-EECC and in Ho-EECC, respectively.

In Hi-EECC, UE_n cannot directly request the cloud. ES_m requested by UE_n decides whether to offload a_n to the cloud. In addition, a high-speed fiber communication link between ESs and the cloud is a necessary infrastructure in Hi-EECC. It ensures the flexibility and scalability of ESs, thereby providing UEs with high-quality services. Thus, we assume that the data transmission rate between ESs and the cloud is the same and is represented by \hat{r}. The communication delay for ES_m offloading a_n to the cloud can be formulated as \begin{equation*} \hat{t}^{\prime }_{n,hi} = \frac{\delta _n}{\hat{r}}. \tag{11} \end{equation*}
View SourceTherefore, the transmission latency between UE_n and the cloud is \tilde{t}^{\prime }_{n,m} + \hat{t}^{\prime }_{n,hi}.

In Ho-EECC, UE_n can directly request the cloud. Moreover, the communication latency between UE_n and the cloud consists of two parts, i.e., the wireless communication delay and the wired communication delay [27]. We assume that the communication resources of the cloud are sufficient, i.e., the data transmission rate allocated to each UE is the same. Thus, the communication delay of a_n between UE_n and the cloud is \begin{equation*} \hat{t}^{\prime }_{n,ho} = \frac{\delta _n}{\hat{r}^{\prime }_{1}} + \frac{\delta _n}{\hat{r}^{\prime }_2}, \tag{12} \end{equation*}
View Sourcewhere \hat{r}^{\prime }_{1} and \hat{r}^{\prime }_{2} are the wireless data transmission rate and wired data transmission rate, respectively. Accordingly, the energy consumption of UE_n offloading a task to the cloud can be formulated as \begin{equation*} \hat{e}_{n} = \sigma ^{\prime }_n \hat{t}^{\prime }_{n,ho}. \tag{13} \end{equation*}
View Source

3.4 Cost Model
Based on the previous definitions, for UE_n \in \text{SN}_i, the delay of a_n in Hi-EECC can be formulated as \begin{align*} T_{n,hi} = &\ \lambda _{n}t_n + \sum _{\mathrm{ES}_{m} \in \text{SM}_i}\tilde{\lambda }_{n,m}(\tilde{t}_{n,m}+\tilde{t}^{\prime }_{n,m}) \\ & + \hat{\lambda }_{n}(\tilde{t}^{\prime }_{n,m}+ \hat{t}^{\prime }_{n,hi} + \hat{t}_n). \tag{14} \end{align*}
View SourceThe energy consumption of UE_n in Hi-EECC is \begin{equation*} E_{n,hi} = \lambda _{n}e_n + \sum _{\mathrm{ES}_{m} \in \text{SM}_i}\tilde{\lambda }_{n,m}\tilde{e}_{n,m} + \hat{\lambda }_{n}\tilde{e}_{n,m}. \tag{15} \end{equation*}
View SourceAs shown in the above equation, the energy consumption for UE_n uploading tasks to the cloud is also \tilde{e}_{n,m}. The reason is that when ESs upload tasks to the cloud, the UE does not incur any energy consumption. The delay of a_n in Ho-EECC can be formulated as \begin{align*} T_{n,ho} = &\ \lambda _{n}t_n + \sum _{\mathrm{ES}_{m} \in \text{SM}_i}\tilde{\lambda }_{n,m}(\tilde{t}_{n,m}+\tilde{t}^{\prime }_{n,m}) \\ &+ \hat{\lambda }_{n}(\hat{t}_n+\hat{t}^{\prime }_{n,ho}). \tag{16} \end{align*}
View SourceThe energy consumption of UE_n in Ho-EECC is \begin{equation*} E_{n,ho} = \lambda _{n}e_n + \sum _{\mathrm{ES}_{m} \in \text{SM}_i}\tilde{\lambda }_{n,m}\tilde{e}_{n,m} + \hat{\lambda }_{n}\hat{e}_{n}. \tag{17} \end{equation*}
View Source

In this paper, the cost of UE_n is formulated as a weighted sum of the energy consumption of UE_n and the time consumption of a_n. Therefore, the cost of UE_n in Hi-EECC is \begin{equation*} C_{n,hi}= \phi _n T_{n,hi} + (1-\phi _n) E_{n,hi}, \tag{18} \end{equation*}
View SourceRight-click on figure for MathML and additional features.where 0 \leq \phi _n \leq 1 is the weighted parameter of a_n’s delay, which can represent the individual preference for energy consumption and time consumption. Similarly, the cost of UE_n in Ho-EECC is formulated as \begin{equation*} C_{n,ho}= \phi _n T_{n,ho} + (1-\phi _n) E_{n,ho}. \tag{19} \end{equation*}
View SourceRight-click on figure for MathML and additional features.

SECTION 4A Potential Game Formulation
Due to the limited resources of ESs, there is a competitive relationship between UEs. All UEs have their own preferences and attempt to determine the most beneficial strategy for themselves. It is a considerable challenge to satisfy all UEs with a centralized method. Fortunately, game theory provides an efficient way to resolve the issue. Next, we construct a game for UEs and ESs, in which each UE selfishly minimizes its energy consumption and time consumption.

In EECC, there are |\mathcal {N}| players (i.e., UEs) in a game and all UEs seek to minimize their cost. The nth player derives a strategy between itself and ESs, i.e., \boldsymbol{\lambda _n}= (\lambda _{n}, \tilde{\lambda }_{n,1}, \dots, \tilde{\lambda }_{n, |\mathcal {M}|}) \in K_n \subseteq \mathbb {R}^{|\mathcal {M}|+1}, where K_n is all possible offloading strategies of UE_n. We use \Lambda to represent the offloading strategy set of all UEs, i.e., \Lambda = (\boldsymbol{\lambda _1}, \boldsymbol{\lambda _2}, \dots, \boldsymbol{\lambda _{\mathcal {|N|}}})^T \in \mathcal {K} = K_1 \times K_2 \times \ldots \times K_{|\mathcal {N}|}, where \mathcal {K} is all possible offloading strategy sets of UEs. \Lambda _{-n} represents the offloading strategy set of |\mathcal {N}|-1 UEs except for UE_n, i.e., \Lambda _{-n} = (\boldsymbol{\lambda _1}, \dots, \boldsymbol{\lambda _{n-1}}, \boldsymbol{\lambda _{n+1}}, \dots, \boldsymbol{\lambda _{\mathcal {|N|}}})^T. Each UE has a payoff function C(\boldsymbol{\lambda _n}, \Lambda _{-n}) \in \mathbb {R} in EECC game, where C(\boldsymbol{\lambda _n}, \Lambda _{-n}) represents the cost of UE_n adopting \boldsymbol{\lambda _n} when \Lambda _{-n} is given. The game is called the EECC game.

Before offloading a task to ES_m, UE_n should ensure that its cost can be reduced. That is, UE_n should first assess the feasibility of ES_m \in \text{SM}_{i}. The feasibility of ES_m for UE_n can be assessed by the following theorem.

4Theorem 1.
If UE_n \in \text{SN}_i offloads its task to ES_m \in \text{SM}_i, that is, ES_m is an available ES for UE_n, then \begin{equation*} \sum _{v \ne n}^{|\mathcal {N}|}\gamma _{v}\tilde{\lambda }_{v,m} =\sum _{\mathrm{UE}_{v} \in \text{SN}_i - \lbrace \text{UE}_n\rbrace }\gamma _{v}\tilde{\lambda }_{v,m} \leq B_n, \tag{20} \end{equation*}
View Sourcewhere B_n = (b_n-1)\gamma _n, and \begin{equation*} b_n = \frac{\tilde{f}_m \tilde{r}_m \omega _n \bigl (\phi _n+(1-\phi _n)\sigma _n\bigl)}{f_n\phi _n(\omega _n \tilde{r}_m + \delta _n \tilde{f}_m)+(1-\phi _n)\delta _n f_n \tilde{f}_m\sigma ^{\prime }_n}. \tag{21} \end{equation*}
View Source

4Proof.
If UE_n offloads its task to ES_m, then \tilde{C}_{n,m} \leq C_{n,l}, where \begin{equation*} \tilde{C}_{n,m} = \phi _n (\tilde{t}_{n,m}+\tilde{t}^{\prime }_{n,m}) + (1-\phi _n)\tilde{e}_{n,m}, \tag{22} \end{equation*}
View Sourceand \begin{equation*} C_{n,l} = \phi _n t_n+(1-\phi _n)e_n. \tag{23} \end{equation*}
View SourcePlugging Equations (3), (4), (5), (6), (9), and (10) into \tilde{C}_{n,m} \leq C_{n,l}, we have \begin{align*} \phi _n \Bigg (\frac{\omega _n}{f_{n,m}} + \frac{\delta _n}{r_{n,m}}\Bigg) + (1-\phi _n) \sigma ^{\prime }_n \frac{\delta _n}{r_{n,m}} \\ \leq \phi _n \frac{\omega _n}{f_n} + (1-\phi _n) \sigma _n \frac{\omega _n}{f_n}. \tag{24} \end{align*}
View SourceThat is, \begin{align*} \phi _n \Bigg (\frac{\omega _n \sum _{v=1}^{|\mathcal {N}|}\gamma _{v}\tilde{\lambda }_{v,m}}{\gamma _n \tilde{f}_m} + \frac{\delta _n \sum _{v=1}^{|\mathcal {N}|}\gamma _{v}\tilde{\lambda }_{v,m}}{\gamma _n \tilde{r}_m} \Bigg) \\ + (1-\phi _n) \sigma ^{\prime }_n \frac{\delta _n \sum _{v=1}^{|\mathcal {N}|}\gamma _{v}\tilde{\lambda }_{v,m}}{\gamma _n \tilde{r}_m} \\ \leq \phi _n \frac{\omega _n}{f_n} + (1-\phi _n)\sigma _n\frac{\omega _n}{f_n}. \tag{25} \end{align*}
View SourceRearranging the above inequality, we have \begin{align*} \frac{\sum _{v=1}^{|\mathcal {N}|}\gamma _{v}\tilde{\lambda }_{v,m}}{\gamma _n }\frac{\phi _n \omega _n \tilde{r}_m + \delta _n \phi _n \tilde{f}_m + (1-\phi _n) \delta _n \sigma ^{\prime }_n \tilde{f}_m}{\tilde{f}_m \tilde{r}_m} \\ \leq \frac{\phi _n \omega _n + (1-\phi _n)\omega _n\sigma _n}{f_n}, \tag{26} \end{align*}
View Sourcei.e., \begin{equation*} \frac{\sum _{v=1}^{|\mathcal {N}|}\gamma _{v}\tilde{\lambda }_{v,m}}{\gamma _n } \leq b_n, \tag{27} \end{equation*}
View Sourcewhere \begin{equation*} b_n = \frac{\tilde{f}_m \tilde{r}_m \omega _n \bigl (\phi _n+(1-\phi _n)\sigma _n\bigl)}{f_n\phi _n(\omega _n \tilde{r}_m + \delta _n \tilde{f}_m)+(1-\phi _n)\delta _n f_n \tilde{f}_m\sigma ^{\prime }_n}. \end{equation*}
View SourceRearranging the above inequality, we can easily obtain \begin{equation*} \sum _{v \ne n}^{|\mathcal {N}|}\gamma _{v}\tilde{\lambda }_{v,m} = \sum _{\mathrm{UE}_{v} \in \text{SN}_i - \lbrace \text{UE}_n\rbrace }\gamma _{v}\tilde{\lambda }_{v,m} \leq (b_n - 1)\gamma _n. \tag{28} \end{equation*}
View SourceRight-click on figure for MathML and additional features.Thus, we have the theorem.

Next, we provide the definition of the potential game [32]. Then, we introduce a potential function to transform the EECC game into a potential game [33].

4Definition 1.
A game is called a potential game, if there is a potential function \Phi _{\Lambda _{-n}}(\boldsymbol{\lambda _n}) that satisfies \begin{equation*} C(\boldsymbol{\lambda _n}, \Lambda _{-n}) < C(\boldsymbol{\lambda ^{\prime }_n}, \Lambda _{-n}) \Leftrightarrow \Phi _{\Lambda _{-n}}(\boldsymbol{\lambda _n}) < \Phi _{\Lambda _{-n}}(\boldsymbol{\lambda ^{\prime }_n}), \end{equation*}
View Sourcefor UE_n \in \mathcal {N}, \boldsymbol{\lambda _n} \in K_n, \Lambda \in \mathcal {K}, and \Lambda _{-n} \in \Pi _{v \ne n}K_{v}. \Phi _{\Lambda _{-n}}(\boldsymbol{\lambda _n}) is a potential function of \boldsymbol{\lambda _n} when \Lambda _{-n} is given.

As shown in Equations (5), (6), (8), (9), (10), and (14)-(19), when the offloading strategies of |\mathcal {N}|-1 UEs except for UE_n are given, the cost of UE_n depends on the ES it chooses and its \gamma _n. Since we assume that ESs within the same service area are the same, if \tilde{\lambda }_{n,m}=1, the number and types of requests responded by ES_m (i.e., \sum _{n=1}^{|\mathcal {N}|} \gamma _n\tilde{\lambda }_{n,m}) determine the cost of the UE. Based on Theorem 1, we construct a potential function for the EECC game in the following theorem.

4Theorem 2.
If all ESs in the ith service area are the same, i.e., \tilde{f}_m = \tilde{f}_{m^{\prime }} and \tilde{r}_m = \tilde{r}_{m^{\prime }} for all ES_m, ES_{m^{\prime }} \in \text{SM}_i, then the EECC game is a potential game with the following potential function: \begin{align*} \Phi _{\Lambda _{-n}}(\boldsymbol{\lambda _n}) =&\ \frac{1}{2}\sum _{n=1}^{|\mathcal {N}|}\sum _{v \ne n}^{|\mathcal {N}|}\sum _{m=1}^{|\mathcal {M}|}\gamma _n \gamma _{v} \tilde{\lambda }_{n,m} \tilde{\lambda }_{v,m} \\ & +\sum _{n=1}^{|\mathcal {N}|}\gamma _n B_n \lambda _{n}. \tag{29} \end{align*}
View SourceRight-click on figure for MathML and additional features.

4Proof.
According to the definition of a potential game, we should prove that the potential function increases or decreases with an increase or decrease in C(\boldsymbol{\lambda _n}, \Lambda _{-n}). To demonstrate the above property of \Phi _{\Lambda _{-n}}(\boldsymbol{\lambda _n}), we consider the following three cases. Let \boldsymbol{\lambda _n} = (\lambda _{n}, \tilde{\lambda }_{n,1}, \dots, \tilde{\lambda }_{n, |\mathcal {M}|}) and \boldsymbol{\lambda ^{\prime }_n} = (\lambda ^{\prime }_{n}, \tilde{\lambda }^{\prime }_{n,1}, \dots, \tilde{\lambda }^{\prime }_{n, |\mathcal {M}|}) be two offloading strategies of UE_n, where \boldsymbol{\lambda _n} \ne \boldsymbol{\lambda ^{\prime }_n}.

Case 1: Suppose that \tilde{\lambda }_{n,m} = 1, \tilde{\lambda }^{\prime }_{n,m^{\prime }} = 1, and \tilde{C}_{n,m^{\prime }} < \tilde{C}_{n,m}, where m \ne m^{\prime }. We know that the UEs that initiate requests to the same ES can affect each other. Moreover, since adjusting the offloading strategy among ESs does not affect other UEs that perform their tasks locally, we have \sum _{n=1}^{|\mathcal {N}|}\gamma _n B_n \lambda _{n} =\sum _{n=1}^{|\mathcal {N}|}\gamma _n B_n \lambda ^{\prime }_{n}. Based on Equation (29), since \tilde{\lambda }_{n,m} = 1, \tilde{\lambda }^{\prime }_{n,m^{\prime }} = 1, and \lambda _{n} + \sum _{m=1}^{|\mathcal {M}|}\tilde{\lambda }_{n,m}=1, we have \begin{align*} &\Phi _{\Lambda _{-n}}(\boldsymbol{\lambda _n}) - \Phi _{\Lambda _{-n}}(\boldsymbol{\lambda ^{\prime }_{n}}) \\ =&\ \frac{1}{2}\gamma _n \tilde{\lambda }_{n,m} \sum _{v \ne n}^{|\mathcal {N}|}\gamma _{v} \tilde{\lambda }_{v,m} + \frac{1}{2}\gamma _n \tilde{\lambda }_{n,m} \sum _{v \ne n}^{|\mathcal {N}|}\gamma _{v} \tilde{\lambda }_{v,m} \\ &+ \frac{1}{2}\sum _{v^{\prime } \ne n}^{|\mathcal {N}|}\gamma _{v^{\prime }}\tilde{\lambda }_{v^{\prime },m} \sum _{v \ne n, v \ne v^{\prime }}^{|\mathcal {N}|}\sum _{m=1}^{|\mathcal {M}|} \gamma _{v}\tilde{\lambda }_{v,m} \\ &- \frac{1}{2}\sum _{v^{\prime } \ne n}^{|\mathcal {N}|}\gamma _{v^{\prime }}\tilde{\lambda }_{v^{\prime },m^{\prime }} \sum _{v \ne n, v \ne v^{\prime }}^{|\mathcal {N}|}\sum _{m^{\prime }=1}^{|\mathcal {M}|} \gamma _{v}\tilde{\lambda }_{v,m^{\prime }} \\ &- \frac{1}{2}\gamma _n \tilde{\lambda }^{\prime }_{n,m^{\prime }} \sum _{v \ne n}^{|\mathcal {N}|}\gamma _{v} \tilde{\lambda }_{v,m^{\prime }} - \frac{1}{2}\gamma _n \tilde{\lambda }^{\prime }_{n,m^{\prime }} \sum _{v \ne n}^{|\mathcal {N}|}\gamma _{v} \tilde{\lambda }_{v,m^{\prime }} \\ =&\ \gamma _n \sum _{v \ne n}^{|\mathcal {N}|}\gamma _{v} \tilde{\lambda }_{v,m} - \gamma _n \sum _{v \ne n}^{|\mathcal {N}|}\gamma _{v} \tilde{\lambda }_{v,m^{\prime }}. \tag{30} \end{align*}
View SourceIf \tilde{C}_{n,m^{\prime }} < \tilde{C}_{n,m}, based on Equation (22), we have \begin{align*} \phi _n (\tilde{t}_{n,m^{\prime }}+t^{\prime }_{n,m^{\prime }}) + (1-\phi _n)e_{n,m^{\prime }} \\ < \phi _n (\tilde{t}_{n,m}+\tilde{t}^{\prime }_{n,m}) + (1-\phi _n)\tilde{e}_{n,m}. \tag{31} \end{align*}
View SourcePlugging Equations (5), (6), and (10) into the above inequality, we have \begin{align*} \phi _n \Bigg (\frac{\omega _n}{f_{n,m^{\prime }}} + \frac{\delta _n}{r_{n,m^{\prime }}}\Bigg) + (1-\phi _n) \sigma ^{\prime }_n \frac{\delta _n}{r_{n,m^{\prime }}} \\ < \phi _n \Bigg (\frac{\omega _n}{f_{n,m}} + \frac{\delta _n}{r_{n,m}}\Bigg) + (1-\phi _n) \sigma ^{\prime }_n \frac{\delta _n}{r_{n,m}}. \tag{32} \end{align*}
View SourceRearranging the above inequality, we have \begin{align*} \frac{\sum _{v=1}^{|\mathcal {N}|}\gamma _{v}\tilde{\lambda }_{v,m^{\prime }}}{\gamma _n }\frac{\phi _n \omega _n \tilde{r}_{m^{\prime }} + \delta _n \phi _n \tilde{f}_{m^{\prime }} + (1-\phi _n) \delta _n \sigma ^{\prime }_n \tilde{f}_{m^{\prime }}}{\tilde{f}_{m^{\prime }} \tilde{r}_{m^{\prime }}} \\ < \frac{\sum _{v=1}^{|\mathcal {N}|}\gamma _{v}\tilde{\lambda }_{v,m}}{\gamma _n }\frac{\phi _n \omega _n \tilde{r}_m + \delta _n \phi _n \tilde{f}_m + (1-\phi _n) \delta _n \sigma ^{\prime }_n \tilde{f}_m}{\tilde{f}_m \tilde{r}_m}. \tag{33} \end{align*}
View SourceRight-click on figure for MathML and additional features.If all ESs in the ith service area are the same, we have \tilde{f}_m = \tilde{f}_{m^{\prime }} and \tilde{r}_m = \tilde{r}_{m^{\prime }}, i.e., \begin{equation*} \sum _{v=1}^{|\mathcal {N}|}\gamma _{v}\tilde{\lambda }_{v,m^{\prime }} < \sum _{v=1}^{|\mathcal {N}|}\gamma _{v}\tilde{\lambda }_{v,m}. \tag{34} \end{equation*}
View SourceRight-click on figure for MathML and additional features.Since \gamma _n > 0, it can be easily found that \begin{equation*} \gamma _n\sum _{v \ne n}^{|\mathcal {N}|}\gamma _{v}\tilde{\lambda }_{v,m^{\prime }} < \gamma _n\sum _{v \ne n}^{|\mathcal {N}|}\gamma _{v}\tilde{\lambda }_{v,m}. \tag{35} \end{equation*}
View SourceTherefore, \Phi _{\Lambda _{-n}}(\boldsymbol{\lambda _n}) - \Phi _{\Lambda _{-n}}(\boldsymbol{\lambda ^{\prime }_{n}}) > 0.

Case 2: Suppose that \lambda _{n} = 1, \tilde{\lambda }^{\prime }_{n,m^{\prime }} = 1, and \tilde{C}_{n,m} < C_{n,l}. Based on Equation (29), we have \begin{equation*} \Phi _{\Lambda _{-n}}(\boldsymbol{\lambda _n}) - \Phi _{\Lambda _{-n}}(\boldsymbol{\lambda ^{\prime }_{n}}) =\gamma _n\sum _{v \ne n}^{|\mathcal {N}|}\gamma _{v}\tilde{\lambda }^{\prime }_{v, m} - \gamma _n B_n \lambda _{n}. \tag{36} \end{equation*}
View SourceBased on Theorem 1, we obtain \Phi _{\Lambda _{-n}}(\boldsymbol{\lambda _n}) - \Phi _{\Lambda _{-n}}(\boldsymbol{\lambda ^{\prime }_{n}}) > 0.

Case 3: Suppose that \tilde{\lambda }_{n,m} = 1, \lambda ^{\prime }_{n} = 1, and C_{n,l} < \tilde{C}_{n,m^{\prime }}. Similar to case 2, we can also easily obtain that \Phi _{\Lambda _{-n}}(\boldsymbol{\lambda _n}) increases or decreases with the increase or decrease in C(\boldsymbol{\lambda _n}, \Lambda _{-n}).

4Remark 1.
In this paper, we construct a potential function for the EECC game, but do not formulate specific potential functions for either Hi-EECC or Ho-EECC. On the one hand, the design is restricted by the potential game theory. Potential game theory requires that all servers be homogeneous [32]. However, although we assume that ESs within the same service area are the same, ESs in different service areas are heterogeneous. In addition, there are huge differences between the cloud and ESs.

On the other hand, the design takes into account the characteristics of Hi-EECC and Ho-EECC. As mentioned above, the way that the cloud responds to UEs is determined by the computing architectures. For UE_n \in SN_{i}, UE_n can temporarily ignore the existence of the cloud and determine a preliminary strategy between itself and ESs (i.e., all ES_m \in SM_{i}). In Hi-EECC, the deadline unsatisfied task of UE_n is further uploaded to the cloud by the ESs, the final best strategy that satisfy its QoS demand can be obtained. In Ho-EECC, by comparing the preliminary strategy with the strategy of cloud processing, the final best strategy with less cost can be derived.

In the EECC game, the nth player derives a strategy between itself and ESs, i.e., \boldsymbol{\lambda _n}= (\lambda _{n}, \tilde{\lambda }_{n,1}, \dots, \tilde{\lambda }_{n, |\mathcal {M}|}). Since UEs can initiate the requests only to the ESs that in the same service area as the UEs, and the ESs within the same service area are the same, we can construct a potential function, i.e., Equation (29), to transform the EECC game into a potential game. Thus, regardless of the specific computing architectures, according to Theorem 2, we can develop Algorithm 1 to determine the preliminary strategies of all UEs. Then, based on the characteristics of Hi-EECC and Ho-EECC, we can further develop different algorithms (i.e., COAHi and COAHo) to readjust the offloading strategies obtained from Algorithm 1 to obtain the final best strategies for all UEs, so that the potential game theory can solve the strategy optimization problem in the heterogeneous scenario.

It should also be noted that a potential game may have many potential functions. However, for a potential game, different potential functions do not affect the quality of the strategy [32], [33]. Therefore, we do not formulate other potential functions or explore the impact of these functions on the performance of the proposed algorithms and two computing architectures.

Since there are competitive relationships between UEs, the strategy of a UE affects the cost of other UEs. Thus, we must determine a best strategy set that can be accepted by all UEs, i.e., Nash equilibrium. We now present the definition of Nash equilibrium.

4Definition 2.
A strategy set \Lambda^{*} = ({\boldsymbol{\lambda^{*}_{1}}}, {\boldsymbol{\lambda^{*}_{2}}}, \dots, {\boldsymbol{\lambda ^{*}_{\mathcal{|N|}}}})^T is a Nash equilibrium of the EECC game, i.e., no UE can unilaterally change its strategy to further reduce its cost, if \begin{equation*} C({\boldsymbol{\lambda^{*}_{n}}}, {\Lambda^{*}_{-n}}) \leq C({\boldsymbol{\lambda_{n}}}, {\Lambda^{*}_{-n}}), \text{ for all } {\boldsymbol{\lambda_{n}}} \in K_n, \tag{37} \end{equation*}
View Sourceholds for all UE_n \in \mathcal {N}.

As shown in Definition 2, Nash equilibrium is the state in which all UEs find the best offloading strategies toward each other. It should be noted that not every game has a Nash equilibrium. Fortunately, if a game can be formulated as a potential game, there is at least one Nash equilibrium of the game [33]. Moreover, according to the finite improvement property, the Nash equilibrium of the game can be obtained after a finite number of iterations [32]. This motivates us to develop an iteration algorithm to find the Nash equilibrium of the EECC game. We present the algorithm in Section 5.1 and analyze the finite improvement property in Section 6.1.

SECTION 5Potential Game-Based Algorithms in Hi-EECC and Ho-EECC
5.1 Algorithms in Hi-EECC
According to Theorem 2, we develop an iteration offloading algorithm, i.e., Algorithm 1, for optimizing the offloading decisions of UEs between itself and ESs. We first initialize the strategies of UEs, i.e., \boldsymbol{\lambda _n} = (1,0,\dots,0)_{|\mathcal {M}|+1}, for all UE_n \in \mathcal {N} (Line 1). Then, we can calculate the initial potential function of each UE (Line 3). Next, we iterate every UE and make a new offloading decision with less cost (Lines 4-19). If no UE can unilaterally change its strategy to further reduce its cost, the game is over, i.e., the final strategy set \Lambda ^* is regarded as a Nash equilibrium (Lines 21-25). Moreover, to control the time complexity of the algorithm, we can define the maximum iteration number \Pi to limit the number of iterations, thus obtaining an acceptable strategy set of UEs. In Section 6.1, Theorem 4 analyzes the convergence of the algorithm in detail.

In Hi-EECC, UE_n makes offloading decisions depending on whether its cost can be reduced. However, if \tilde{\lambda }_{n,m}=1 and the delay served by ES_m exceeds d_n, the ES will offload a_n to the cloud for executing. It incurs the cost of ES_m for the ES uploading tasks to the cloud. In this paper, we define the cost of ES_m as the price paid for the time required by the cloud to complete the tasks. The cost of ES_m is \begin{equation*} C_{m,c}(\Lambda) = \sum _{n=1}^{|\mathcal {N}|}\frac{\omega _n}{\hat{f}}\Bigg (1-\sum _{m=1}^{|\mathcal {N}|}\tilde{\lambda }_{n,m}- \lambda _n\Bigg)\kappa _m, \tag{38} \end{equation*}
View Sourcewhere \kappa _m is the correlation parameter between the computation delay for the cloud completing a_n and ES_m’s cost.

Algorithm 1. Nash Equilibrium Calculating Algorithm
Input: \gamma _n, \omega _n, \zeta _n, d_n, \delta _n, \sigma _n, \sigma ^{\prime }_n, and f_n, for all UE_n \in \mathcal {N}. \tilde{f}_m, \tilde{r}_m, and \kappa _m for all ES_m \in \mathcal {M}. \hat{f}, \hat{r}, and \Pi.

Output: \Lambda ^*.

Initialize \Lambda \leftarrow \big ((1,0,\dots,0),\dots,(1,0,\dots,0)\big)_{|\mathcal {N}|}^T;

while \pi < \Pi do

Calculate \Phi _{\Lambda _{-n}}(\boldsymbol{\lambda _n}) for all UE_n \in \mathcal {N} based on Equation (29);

for UE_n \in \mathcal {N} do

\boldsymbol{\lambda _n} \leftarrow (1,0,\dots,0)_{|\mathcal {M}|+1};

i \leftarrow \zeta _n;

for ES_m \in \text{SM}_{i} do

if ES_m is an available ES for UE_n then

\boldsymbol{\lambda ^{\prime }_n} \leftarrow (0,\dots,0)_{|\mathcal {M}|+1};

\lambda ^{\prime }_{n,m+1} \leftarrow 1;

Calculate \Phi _{\Lambda _{-n}}(\boldsymbol{\lambda ^{\prime }_n}) based on Equation (29);

if \Phi ^{\prime }_{\Lambda }(n) < \Phi _{\Lambda }(n) then

\boldsymbol{\lambda _n} \leftarrow \boldsymbol{\lambda ^{\prime }_n};

\Phi _{\Lambda _{-n}}(\boldsymbol{\lambda _n}) \leftarrow \Phi _{\Lambda _{-n}}(\boldsymbol{\lambda ^{\prime }_n});

end if

end if

end for

Update the offloading strategy of UE_n between itself and ESs, i.e., {\boldsymbol{\lambda^{*}_{n}}}\leftarrow {\boldsymbol{\lambda_{n}}};

end for

\pi \leftarrow \pi + 1

if no UE can unilaterally change its strategy to further reduce its cost then

break;

end if

end while

return {\Lambda^{*}} = ({\boldsymbol{\lambda^{*}_{1}}}, {\boldsymbol{\lambda^{*}_{2}}}, \dots, {\boldsymbol{\lambda ^{*}_{\mathcal{|N|}}}})^{T}.

Let us suppose that UE_n and UE_{v} request ES_m for processing their tasks. Furthermore, the deadlines of a_n and a_{v} cannot be satisfied by ES_m. Thus, the two tasks could be uploaded to the cloud. However, while the ES uploads a_n to the cloud, the resources originally allocated to UE_n will be released. The released resources can provide service to UE_{v} and thus may satisfy the QoS demand of UE_{v}. Therefore, the ES should also optimize the tasks that are uploaded to the cloud to reduce its cost. The optimization objective of ES_m can be formulated as the following problem: \begin{align*} & \mathrm{P1}: \min _{\Lambda } C_{m,c}(\Lambda) \\ & s.t.\ C1: T_{n,hi}\leq d_n, \text{ for all UE} _n \in \mathcal {N}, \\ & C2: \sum _{m=1}^{|\mathcal {M}|}\tilde{\lambda }_{n,m} + \hat{\lambda }_{n} = 1, \text{ where } \tilde{\lambda }_{n,m}, \hat{\lambda }_{n} \in \lbrace 0, 1\rbrace, \tag{39} \end{align*}
View Sourcewhere C1 ensures that QoS demands of all UEs should be satisfied. C2 means that a task can be executed by only one entity. It is clear that P1 is an NP-hard problem [34].

Remark 2.
In Hi-EECC, when a_n is executed by the cloud, the cost of UE_n is \begin{equation*} \hat{C}_{n} = \phi _n (\hat{t}_n + \hat{t}^{\prime }_{n,hi} + \tilde{t}^{\prime }_{n,m}) + (1-\phi _n)\tilde{e}_{n,m}. \tag{40} \end{equation*}
View SourceBased on Equations (22) and (40), if offloading a_n to the cloud, the following inequality should be true: \begin{equation*} \tilde{C}_{n,m} - \hat{C}_{n} = \phi _n (\tilde{t}_{n,m}-\hat{t}_n - \hat{t}^{\prime }_{n,hi}) \geq 0. \tag{41} \end{equation*}
View SourceOtherwise, the QoS demand of UE_n cannot be met. Therefore, this paper has an implicit requirement that the cloud has sufficient resources to meet the demands of UEs, i.e., \hat{f} \gg \tilde{f}_m. Hence, for a_n, if \tilde{\lambda }_{n,m} = 1 and T_{n,hi}> d_n, offloading the task to the cloud by ES_m should not increase the cost of UE_n.

Moreover, if a_n is offloaded to the cloud, we have \begin{equation*} \sum _{n=1}^{|\mathcal {N}|}\gamma _n\tilde{\lambda }_{n,m} > \sum _{v \ne n}^{|\mathcal {N}|} \gamma _{v}\tilde{\lambda }_{v,m}. \tag{42} \end{equation*}
View SourceAccording to Equations (6), (9), (10), and (18), we can easily determine that if a_n is offloaded to the cloud, the cost of other UEs that request ES_m decreases.

Algorithm 2. Deadline Guaranteeing Algorithm
Input: The offloading strategy set \Lambda obtained from Algorithm 1. \hat{\boldsymbol{\lambda }}=(0,\dots,0)^T_{|\mathcal {N}|}. \gamma _n, \omega _n, \zeta _n, d_n, \delta _n, \sigma _n, \sigma ^{\prime }_n, and f_n, for all UE_n \in \mathcal {N}. \tilde{f}_m, \tilde{r}_m, and \kappa _m for all ES_m \in \mathcal {M}. \hat{f}, \hat{r}.

Output: New offloading strategy set \Lambda, \hat{\boldsymbol{\lambda }}, and the cost of ES_m C_{m,c}(\Lambda).

\mathcal {R} \leftarrow \lbrace \text{UE}_n|\ \text{for all UE} _n \in \mathcal {N}, \text{where} \ \tilde{\lambda }_{n,m}=1 \ \text{and} \ T_{n,hi}> d_n \rbrace ;

while \mathcal {R} \ne \emptyset do

UE_{n} \leftarrow \arg \max _{\mathrm{UE}_{n}\in \mathcal {R}} \lbrace \gamma _{n}\rbrace or \arg \min _{\mathrm{UE}_{n}\in \mathcal {R}} \lbrace \omega _{n}\rbrace;

Update the cloud offloading decision, i.e., \hat{\lambda }_n \leftarrow 1;

Update the offloading decision of UE_n between itself and ESs, i.e., \boldsymbol{\lambda _n};

\mathcal {R} \leftarrow \mathcal {R}-\lbrace \text{UE}_n\rbrace;

Calculate T_{n,hi} for all UE_n \in \mathcal {R};

\mathcal {R}^{\prime } \leftarrow \lbrace \text{UE}_n| \tilde{\lambda }_{n,m}=1 \ \text{and} \ T_{n,hi}\leq d_n \rbrace ;

\mathcal {R} \leftarrow \mathcal {R}-\mathcal {R}^{\prime };

end while

Calculate C_{m,c}(\Lambda) based on Equation (38);

return \Lambda, \hat{\boldsymbol{\lambda }}, C_{m,c}(\Lambda).

According to Remark 2, ESs can safely upload tasks to the cloud. To solve P1, we propose Algorithm 2 based on the greedy policy. Specifically, the algorithm reschedules the tasks based on \omega _n or \gamma _n. The reason is that uploading tasks with larger \gamma _n will increase the released resources such that more UEs’ demands can be satisfied. In addition, uploading tasks with smaller \omega _n will directly help reduce the cost of ES_m. We can obtain two task offloading rescheduling schemes by using the greedy policy. The scheme with the lowest cost is regarded as the final strategy. For simplicity, we use \hat{\boldsymbol{\lambda }} to represent the cloud offloading decisions of all UEs, i.e., \hat{\boldsymbol{\lambda }} = (\hat{\lambda }_1, \hat{\lambda }_2, \dots, \hat{\lambda }_{\mathcal {|N|}})^T \in \hat{K} \subseteq \mathbb {R}^{|\mathcal {N}|}, where \hat{K} is all possible cloud offloading strategy sets of UEs.

Algorithm 2 shows the two rescheduling schemes based on different greedy policies. We first iterate every ES_m \in \mathcal {M} and identify the deadline unsatisfied tasks (Line 1), i.e., \mathcal {R}. The task with the maximum \gamma _n among all UE_{v} \in \mathcal {R} is offloaded to the cloud (Lines 2-6). However, after a task is offloaded to the cloud, it is necessary to check whether the current resources can meet the QoS demands of other original deadline unsatisfied tasks. Thus, we update the computation and communication delay of the remaining tasks in \mathcal {R} (Line 7), and remove the UEs whose demands can be satisfied from \mathcal {R} (Lines 8-9). The above process is iteratively operated, until \mathcal {R} = \emptyset. C_{m,c}(\Lambda) can be obtained after the rescheduling processing (Line 12). Similarly, we can obtain a rescheduling scheme based on UE_{n} \leftarrow \arg \min _{\mathrm{UE}_{n}\in \mathcal {R}} \lbrace \omega _{n}\rbrace. It is easy to know that the time complexity of the algorithm is O(|\mathcal {N}|).

Algorithm 3. Computation Offloading Algorithm in Hi-EECC (COAHi)
Input: \gamma _n, \omega _n, \zeta _n, d_n, \delta _n, \sigma _n, \sigma ^{\prime }_n, and f_n, for all UE_n \in \mathcal {N}. \tilde{f}_m, \tilde{r}_m, and \kappa _m for all ES_m \in \mathcal {M}. \hat{f}, \hat{r}.

Output: \Lambda _o, \hat{\boldsymbol{\lambda }}_o.

Obtain \Lambda through Algorithm 1;

Initialize the cloud strategies of UEs \hat{\boldsymbol{\lambda }}=(0,\dots,0)^T_{|\mathcal {N}|};

for ES_m \in \mathcal {M} do

Obtain \Lambda _1, \hat{\boldsymbol{\lambda }}_1, and C_1 through Algorithm 2 based on UE_{n} \leftarrow \arg \max _{\mathrm{UE}_{n}\in \mathcal {R}} \lbrace \gamma _{n}\rbrace;

Obtain \Lambda _2, \hat{\boldsymbol{\lambda }}_2, and C_2 through Algorithm 2 based on UE_{n} \leftarrow \arg \min _{\mathrm{UE}_{n}\in \mathcal {R}} \lbrace \omega _{n}\rbrace;

if C_1 < C_2 then

Update \Lambda and \hat{\boldsymbol{\lambda }} according to \Lambda _1 and \hat{\boldsymbol{\lambda }}_1, respectively;

else if C_1 \geq C_2 then

Update \Lambda and \hat{\boldsymbol{\lambda }} according to \Lambda _2 and \hat{\boldsymbol{\lambda }}_2, respectively;

end if

end for

Obtain the final strategies of UEs between itself and ESs, i.e., \Lambda _o \leftarrow \Lambda;

Obtain the final cloud offloading strategies of UEs, i.e., \hat{\boldsymbol{\lambda }}_o \leftarrow \hat{\boldsymbol{\lambda }};

return \Lambda _o, \hat{\boldsymbol{\lambda }}_o.

We first develop Algorithm 1 to determine a preliminary decision set between UEs and ESs. However, Algorithm 1 aims only at minimizing the cost of UEs, and does not consider QoS requirements of the UEs. The decisions obtained by Algorithm 1 may cause QoS requirements of some tasks to not be met. Thus, we then develop Algorithm 2 to determine whether to continue uploading these unsatisfied tasks to the cloud to meet their demands, that is, to solve P1. Algorithm 3 is developed based on Algorithms 1 and 2, and is the algorithm for making offloading strategies in Hi-EECC, which is named COAHi. The initial decision set between local processing and offloading to ESs is obtained from Algorithm 1 (Line 1). Then, we reschedule the tasks between ESs and the cloud by using Algorithm 2 (Lines 3-5). Finally, we update \Lambda and \hat{\boldsymbol{\lambda }} according to the rescheduling decision set with less cost and obtain the final offloading strategies of UEs (Lines 6-14). The time complexity of Algorithm 3 is derived in Corollary 1. It should be noted that since we readjust the decisions obtained by Algorithm 1, the original Nash equilibrium of UEs is broken. Based on Remarks 1 and 2, although the final offloading strategy set obtained by Algorithm 3 is not a Nash equilibrium of the EECC game, the set consists of the best strategies of all UEs.

5.2 Algorithms in Ho-EECC
In Ho-EECC, UEs can directly offload their tasks to the cloud. Thus, UE_n can first make an offloading decision by using Algorithm 1. Then, the cost of the decision is compared with the cost of cloud processing. Therefore, the final offloading strategy can be obtained. As mentioned above, in contrast to the decision-making method in Hi-EECC, UE_n directly determines the offloading strategy. Hence, before making a decision, the UE should not only ensure that its cost can be reduced, but also ensure that its QoS demand can be guaranteed. In addition to Theorem 1, the feasibility of ES_m for UE_n should be checked using the following theorem.

Algorithm 4. Computation Offloading Algorithm in Ho-EECC (COAHo)
Input: \gamma _n, \omega _n, \zeta _n, d_n, \delta _n, \sigma _n, \sigma ^{\prime }_n, and f_n, for all UE_n \in \mathcal {N}. \tilde{f}_m, \tilde{r}_m, and \kappa _m for all ES_m \in \mathcal {M}. \hat{f}, \hat{r}_1, \hat{r}_2, and \Pi.

Output: \Lambda _o, \hat{\boldsymbol{\lambda }}_o.

\mathcal {N}^{\prime } \leftarrow \mathcal {N};

Obtain \Lambda through Algorithm 1;

Initialize the cloud strategies of UEs \hat{\boldsymbol{\lambda }}=(0,\dots,0)^T_{|\mathcal {N}|};

for UE_n \in \mathcal {N}^{\prime } do

Calculate C_{n,ho} based on \Lambda, \hat{\boldsymbol{\lambda }}, and Equation (19);

\hat{\lambda }_n \leftarrow 1;

\boldsymbol{\lambda ^{\prime }_n} \leftarrow (0,\dots,0)_{|\mathcal {M}|+1};

Calculate C^{\prime }_{n,ho} based on \Lambda, \hat{\boldsymbol{\lambda }}, and Equation (19);

if C^{\prime }_{n,ho} < C_{n,ho} then

\hat{\lambda }_n \leftarrow 1;

\boldsymbol{\lambda _n} \leftarrow \boldsymbol{\lambda ^{\prime }_n};

\mathcal {N}^{\prime } \leftarrow \mathcal {N}^{\prime } - \lbrace \text{UE}_n\rbrace;

end if

Update \boldsymbol{\lambda _n} for all UE_n \in \mathcal {N}^{\prime } through Algorithm 1, and obtain new offloading strategy set \Lambda;

end for

Obtain the final strategies of UEs between itself and ESs, i.e., \Lambda _o \leftarrow \Lambda;

Obtain the final cloud offloading strategies of UEs, i.e., \hat{\boldsymbol{\lambda }}_o \leftarrow \hat{\boldsymbol{\lambda }};

return \Lambda _o, \hat{\boldsymbol{\lambda }}_o.

Theorem 3.
ES_m \in \text{SM}_{i} is an available ES for UE_n \in \text{SN}_{i} when the ES satisfies Theorem 1 and the following inequality: \begin{equation*} \sum _{v \ne n}^{|\mathcal {N}|}\gamma _{v}\tilde{\lambda }_{v,m} \leq B^{\prime }_n, \tag{43} \end{equation*}
View Sourcewhere \begin{equation*} B^{\prime }_n = \frac{d_n \gamma _n \tilde{f}_m \tilde{r}_m}{\omega _n \tilde{r}_m + \delta _n \tilde{f}_m} - \gamma _n. \tag{44} \end{equation*}
View Source

Proof.
If a_n is offloaded to ES_m for executing, the computation and communication delay of a_n should satisfy the following inequality: \begin{equation*} \tilde{t}_{n,m} + \tilde{t}^{\prime }_{n,m} \leq d_n. \tag{45} \end{equation*}
View SourcePlugging Equations (6) and (9) into the above inequality, we have \begin{equation*} \frac{\sum _{n=1}^{|\mathcal {N}|}\gamma _n\tilde{\lambda }_{n,m}}{\gamma _n} \Bigg (\frac{\omega _n}{\tilde{f}_m} + \frac{\delta _n}{\tilde{r}_m} \Bigg) \leq d_n, \tag{46} \end{equation*}
View Sourcei.e., \begin{equation*} \sum _{v \ne n}^{|\mathcal {N}|}\gamma _{v}\tilde{\lambda }_{v,m} \leq \frac{d_n \gamma _n \tilde{f}_m \tilde{r}_m}{\omega _n \tilde{r}_m + \delta _n \tilde{f}_m} - \gamma _n. \end{equation*}
View SourceThus, we reach the conclusion.

Algorithm 4 is developed based on Algorithm 1, and describes the processing for making task offloading strategies for UEs in Ho-EECC, which is named COAHo. It should be noted that although Algorithms 3 and 4 both call Algorithm 1, the criteria for checking the availability of ESs is different. In Algorithm 3, the availability of ESs is checked by Theorem 1. In Algorithm 4, the availability of ESs is checked by Theorem 3. The initial strategy set \Lambda is obtained through Algorithm 1 (Line 2). Then, the current cost of UE_n is compared with the cloud execution cost of the UE. If the cloud execution cost is less than the current decision cost, UE_n will reschedule its task to the cloud (Lines 4-13). As some UEs are uploaded to the cloud, the resources originally occupied by these UEs are provided to other UEs. Therefore, the strategy should be updated again through Algorithm 1 (Line 14). If none of the UEs can benefit from the update process, the algorithm ends (Lines 16-18). Similar to COAHi, since we readjust the decisions obtained by Algorithm 1, the original Nash equilibrium of UEs is broken. Moreover, although the final offloading strategy set obtained by Algorithm 4 is not a Nash equilibrium of the EECC game, the set consists of the best strategies of all UEs.

SECTION 6Performance Analysis
6.1 Convergence of Algorithms
The finite improvement property of the potential game ensures that the game approaches a Nash equilibrium after the finite iteration [32]. Next, we analyze the convergence of the proposed algorithms. Let \Gamma _{max} \triangleq \max \lbrace \gamma _n | \text{ for all} \ \text{UE}_n \in \mathcal {N} \rbrace , \Gamma _{min} \triangleq \min \lbrace \gamma _n | \text{ for all} \ \text{UE}_n \in \mathcal {N} \rbrace , and B_{max} = \max \lbrace B_n | \text{ for all} \ \text{UE}_n \in \mathcal {N} \rbrace. Furthermore, \gamma _n and B_n are assumed to be non-negative integers.

Theorem 4.
For Algorithm 1, the maximum number of iterations for UE_n determining an offloading strategy is \begin{equation*} \Pi _{max} \leq \frac{|\mathcal {N}|^2 \Gamma _{max}^2}{2 \Gamma _{min}} + \frac{|\mathcal {N}|\Gamma _{max}B_{max}}{\Gamma _{min}}. \tag{47} \end{equation*}
View Source

Proof.
Based on Equation (29), we have \begin{align*} \Phi _{\Lambda _{-n}}(\boldsymbol{\lambda _n}) &\leq \frac{1}{2}\sum _{n=1}^{|\mathcal {N}|}\sum _{n=1}^{|\mathcal {N}|}\Gamma _{max}^2 + \sum _{n=1}^{|\mathcal {N}|}\Gamma _{max}B_{max} \\ & \leq \frac{1}{2} |\mathcal {N}|^2 \Gamma _{max}^2 + |\mathcal {N}|\Gamma _{max}B_{max}. \tag{48} \end{align*}
View SourceLet \boldsymbol{\lambda _n}= (\lambda _{n}, \tilde{\lambda }_{n,1}, \dots, \tilde{\lambda }_{n, |\mathcal {M}|}) and \boldsymbol{\lambda ^{\prime }_n}= (\lambda ^{\prime }_{n}, \tilde{\lambda }^{\prime }_{n,1}, \dots, \tilde{\lambda }^{\prime }_{n, |\mathcal {M}|}) be two offloading strategies of UE_n, where \boldsymbol{\lambda _n} \ne \boldsymbol{\lambda ^{\prime }_n} . Then we prove that if UE_n updates its strategy from \boldsymbol{\lambda _n} to \boldsymbol{\lambda ^{\prime }_n}, we have \begin{equation*} \Phi _{\Lambda _{-n}}(\boldsymbol{\lambda _n}) - \Phi _{\Lambda _{-n}}(\boldsymbol{\lambda ^{\prime }_n}) \geq \Gamma _{min}. \tag{49} \end{equation*}
View SourceCase 1: we suppose that \tilde{\lambda }_{n,m} = 1 and \tilde{\lambda }^{\prime }_{n,m^{\prime }} = 1, where m \ne m^{\prime }. According to Equations (30) and (35), we have \begin{align*} &\Phi _{\Lambda _{-n}}(\boldsymbol{\lambda _n}) - \Phi _{\Lambda _{-n}}(\boldsymbol{\lambda ^{\prime }_n}) \\ &=\gamma _n\Bigg (\sum _{v \ne n}^{|\mathcal {N}|}\gamma _{v}\tilde{\lambda }_{v,m} - \sum _{v \ne n}^{|\mathcal {N}|}\gamma _{v}\tilde{\lambda }_{v,m^{\prime }}\Bigg) > 0. \tag{50} \end{align*}
View SourceSince \gamma _n is assumed to be an integer, we have \begin{equation*} \sum _{v \ne n}^{|\mathcal {N}|}\gamma _{v}\tilde{\lambda }_{v,m} - \sum _{v \ne n}^{|\mathcal {N}|}\gamma _{v}\tilde{\lambda }_{v,m^{\prime }} \geq 1. \tag{51} \end{equation*}
View SourceIt can be easily obtained that \begin{equation*} \Phi _{\Lambda _{-n}}(\boldsymbol{\lambda _n}) - \Phi _{\Lambda _{-n}}(\boldsymbol{\lambda ^{\prime }_n}) > \Gamma _{min}. \tag{52} \end{equation*}
View Source

Case 2: we suppose that \lambda _{n} = 1 and \tilde{\lambda }^{\prime }_{n,m} = 1. According to Equations (36), we have \begin{equation*} \Phi _{\Lambda _{-n}}(\boldsymbol{\lambda _n}) - \Phi _{\Lambda _{-n}}(\boldsymbol{\lambda ^{\prime }_n})= \gamma _n\sum _{v \ne n}^{|\mathcal {N}|}\gamma _{v}\tilde{\lambda }_{v,m} - \gamma _n B_n > 0. \tag{53} \end{equation*}
View SourceSimilarly, since \gamma _n is assumed to be an integer, we obtain \begin{equation*} \Phi _{\Lambda _{-n}}(\boldsymbol{\lambda _n}) - \Phi _{\Lambda _{-n}}(\boldsymbol{\lambda ^{\prime }_n}) \geq 1. \tag{54} \end{equation*}
View SourceAccordingly, we also obtain \Phi _{\Lambda _{-n}}(\boldsymbol{\lambda _n}) - \Phi _{\Lambda _{-n}}(\boldsymbol{\lambda ^{\prime }_n}) \geq \Gamma _{min}. Based on Equations (48) and (49), we know that the maximum number of iterations for a UE making an offloading strategy by using Algorithm 1 is \begin{equation*} \Pi _{max} \leq \frac{|\mathcal {N}|^2 \Gamma _{max}^2}{2 \Gamma _{min}} + \frac{|\mathcal {N}|\Gamma _{max}B_{max}}{\Gamma _{min}}. \end{equation*}
View SourceBased on the above analysis, we have the theorem.

According to Theorem 4, we can derive the time complexity of Algorithm 3, and have the following corollary.

Corollary 1.
Since Algorithm 3 calls Algorithm 1 once and Algorithm 2 twice, it can be easily derived that the time complexity of Algorithm 3 is O(|\mathcal {N}|^2).

Theorem 5.
For Algorithm 4, the maximum number of iterations for UE_n making an offloading strategy is \begin{equation*} \Pi _{max} \leq \frac{ |\mathcal {N}|^3 \Gamma _{max}^2}{2 \Gamma _{min}} + \frac{|\mathcal {N}|^2\Gamma _{max}B_{max}}{\Gamma _{min}}. \tag{55} \end{equation*}
View SourceRight-click on figure for MathML and additional features.

Proof.
Compared with Algorithm 1, after obtaining the offloading strategies between UEs and ESs, Algorithm 4 then reschedules the tasks between the ESs and the cloud to obtain the offloading strategies with less cost. Thus, in Algorithm 4, UE_n performs Algorithm 1 no more than |\mathcal {N}| times to make all offloading strategies of UEs. Based on Theorem 4, we reach the conclusion.

6.2 Performance of Algorithms
Although performance evaluation is not the focus of game theory, it is interesting to investigate the performance of potential game-based algorithms. To analyze the performance of the algorithms proposed in this paper, we investigate the price of anarchy (PoA) in system-wide cost, which quantifies the efficiency ratio of the worst-case Nash equilibrium strategy over the optimal strategy obtained through the centralized methods [35]. In this paper, the system-wide cost of UEs is the total cost of all UE_n \in \mathcal {N}, i.e., \sum _{n=1}^{|\mathcal {N}|} C_n in Hi-EECC and \sum _{n=1}^{|\mathcal {N}|} C^{\prime }_n in Ho-EECC. In Hi-EECC, PoA is defined as \begin{equation*} \text{PoA} = \frac{\sum _{n=1}^{|\mathcal {N}|} C_n(\boldsymbol{\overline{\lambda }_n})}{\max _{\boldsymbol{\lambda _n}} \sum _{n=1}^{|\mathcal {N}|} C_n(\boldsymbol{\lambda^{*}_{n}})}, \tag{56} \end{equation*}
View Sourcewhere \boldsymbol{\overline{\lambda }_n} is an optimal offloading strategy of UE_n obtained from a centralized algorithm.

Theorem 6.
In Hi-EECC, for the EECC game, PoA satisfies \begin{equation*} 0 \leq \text{PoA} \leq \frac{\sum _{n=1}^{|\mathcal {N}|} \min \lbrace C_{n,l}, \tilde{C}_{n,m}^{min}, \hat{C}_{n}^{min}\rbrace }{\sum _{n=1}^{|\mathcal {N}|} \max \lbrace C_{n,l}, \tilde{C}_{n,m}^{max}, \hat{C}_{n}^{max}\rbrace } \leq 1, \tag{57} \end{equation*}
View Sourcewhere \begin{equation*} \qquad\qquad\tilde{C}_{n,m}^{max} = \frac{\sum _{n=1}^{|\mathcal {N}|}\gamma _n}{\gamma _n} \Bigg (\phi _n\bigg (\frac{\omega _n}{\tilde{f}_m}+\frac{\delta _n}{\tilde{r}_m}\bigg) + (1-\phi _n)\frac{\delta _n\sigma ^{\prime }_n}{\tilde{r}_m}\Bigg), \tag{58} \end{equation*}
View Source\begin{align*} \hat{C}_{n}^{max} =&\ \frac{\sum _{n=1}^{|\mathcal {N}|}\gamma _n\delta _n}{\gamma _n\tilde{r}_m}\bigl (\phi _n + (1-\phi _n)\sigma ^{\prime }_n \bigl) \\ &+ \phi _n\Bigg (\frac{\omega _n}{\hat{f}} + \frac{\delta _n}{\hat{r}}\Bigg), \tag{59} \end{align*}
View Source\begin{equation*} \quad\qquad\tilde{C}_{n,m}^{min} = \phi _n \Bigg (\frac{\omega _n}{\tilde{f}_m} + \frac{\delta _n}{\tilde{r}_m}\Bigg)+ (1-\phi _n)\frac{\delta _n\sigma ^{\prime }_n}{\tilde{r}_m}, \tag{60} \end{equation*}
View Sourceand \begin{equation*} \qquad\quad\hat{C}_{n}^{min} = \phi _n \Bigg (\frac{\omega _n }{\hat{f}} +\frac{\delta _n}{\tilde{r}_m} + \frac{\delta _n}{\hat{r}} \Bigg) + (1-\phi _n)\frac{\delta _n\sigma ^{\prime }_n}{ \tilde{r}_m}. \tag{61} \end{equation*}
View Source

Proof.
Since \boldsymbol{\overline{\lambda }_n} is the optimal offloading strategy and \boldsymbol{\lambda^{*}_{n}} is one Nash equilibrium of the game, it is easily found that 0 \leq \text{PoA}\ \leq 1.

For UE_n, the resources allocated by ES_m are satisfied as follows \begin{align*} \frac{\gamma _n}{\sum _{n=1}^{|\mathcal {N}|}\gamma _n}\tilde{f}_m \leq f_{n,m} \leq \frac{\gamma _n}{\tilde{f}_m}, \tag{62} \end{align*}
View Source\begin{align*} \frac{\gamma _n}{\sum _{n=1}^{|\mathcal {N}|}\gamma _n}\tilde{r}_m \leq r_{n,m} \leq \frac{\gamma _n}{\tilde{r}_m}. \tag{63} \end{align*}
View SourceLet f_{n,m}^{min} = \gamma _n\tilde{f}_m/\sum _{n=1}^{|\mathcal {N}|}\gamma _n, f_{n,m}^{max} = \tilde{f}_m, r_{n,m}^{min} = \gamma _n\tilde{r}_m/\sum _{n=1}^{|\mathcal {N}|}\gamma _n, and r_{n,m}^{max} = \tilde{r}_m. Based on Equations (6), (9) and (18), for UE_n offloading its task to ES_m, the cost of the UE is satisfied, i.e., \begin{align*} \tilde{C}_{n,m} & \leq \phi _n \Bigg (\frac{\omega _n}{f_{n,m}^{min}} + \frac{\delta _n}{r_{n,m}^{min}}\Bigg) + (1-\phi _n)\frac{\sigma ^{\prime }_n\delta _n}{r_{n,m}^{min}} \\ & = \frac{\sum _{n=1}^{|\mathcal {N}|}\gamma _n}{\gamma _n} \Bigg (\phi _n\bigg (\frac{\omega _n}{\tilde{f}_m}+\frac{\delta _n}{\tilde{r}_m}\bigg) + (1-\phi _n)\frac{\delta _n\sigma ^{\prime }_n}{\tilde{r}_m}\Bigg) \\ & = \tilde{C}_{n,m}^{max}, \tag{64} \end{align*}
View SourceRight-click on figure for MathML and additional features.and \begin{align*} \tilde{C}_{n,m} & \geq \phi _n \Bigg (\frac{\omega _n}{f_{n,m}^{max}} + \frac{\delta _n}{r_{n,m}^{max}}\Bigg) + (1-\phi _n)\frac{\sigma ^{\prime }_n\delta _n}{r_{n,m}^{max}} \\ & = \phi _n \Bigg (\frac{\omega _n}{\tilde{f}_m} + \frac{\delta _n}{\tilde{r}_m}\Bigg)+ (1-\phi _n)\frac{\delta _n\sigma ^{\prime }_n}{\tilde{r}_m} \\ & = \tilde{C}_{n,m}^{min}. \tag{65} \end{align*}
View SourceRight-click on figure for MathML and additional features.For UE_n offloading its task to the cloud, the cost of the UE is satisfied, i.e., \begin{align*} \hat{C}_{n} & \leq \phi _n \Bigg (\frac{\omega _n}{\hat{f}} + \frac{\delta _n}{r_{n,m}^{min}} +\frac{\delta _n}{\hat{r}}\Bigg) + (1-\phi _n)\frac{\sigma ^{\prime }_n\delta _n}{r_{n,m}^{min}} \\ & = \phi _n \Bigg (\frac{\omega _n}{\hat{f}}+ \frac{\delta _n}{\hat{r}}\Bigg) + \frac{\sum _{n}^{|\mathcal {N}|}\gamma _n\delta _n}{\gamma _n\tilde{r}_m}\bigl (\phi _n + (1-\phi _n)\sigma ^{\prime }_n \bigl) \\ & = \hat{C}_{n}^{max}, \tag{66} \end{align*}
View Sourceand \begin{align*} \hat{C}_{n} & \geq \phi _n \Bigg (\frac{\omega _n}{\hat{f}} + \frac{\delta _n}{r_{n,m}^{max}} + \frac{\delta _n}{\hat{r}}\Bigg) + (1-\phi _n)\frac{\sigma ^{\prime }_n\delta _n}{r_{n,m}^{max}} \\ & = \phi _n\Bigg (\frac{\omega _n }{\hat{f}} +\frac{\delta _n}{\tilde{r}_m} + \frac{\delta _n}{\hat{r}}\Bigg) + \frac{(1-\phi _n)\delta _n\sigma ^{\prime }_n}{ \tilde{r}_m} \\ & = \hat{C}_{n}^{min}. \tag{67} \end{align*}
View SourceFor UE_n executing its task locally, the cost of the UE is certain, i.e., C_{n,l}=\phi _n \omega _n/f_n+(1-\phi _n)\sigma _n\omega _n/f_n. Based on the above, we obtain \begin{equation*} \sum _{n=1}^{|\mathcal {N}|} C_n(\boldsymbol{\overline{\lambda }_n}) \geq \sum _{n}^{|\mathcal {N}|} \min \lbrace C_{n,l}, \tilde{C}_{n,m}^{min}, \hat{C}_{n}^{min}\rbrace, \tag{68} \end{equation*}
View Sourceand \begin{equation*} \sum _{n=1}^{|\mathcal {N}|} C_n(\boldsymbol{\lambda^{*}_{n}}) \leq \sum _{n}^{|\mathcal {N}|} \max \lbrace C_{n,l}, \tilde{C}_{n,m}^{max}, \hat{C}_{n}^{max}\rbrace . \tag{69} \end{equation*}
View SourceTherefore, we have \begin{equation*} \text{PoA} \leq \frac{\sum _{n=1}^{|\mathcal {N}|} \min \lbrace C_{n,l}, \tilde{C}_{n,m}^{min}, \hat{C}_{n}^{min}\rbrace }{\sum _{n=1}^{|\mathcal {N}|} \max \lbrace C_{n,l}, \tilde{C}_{n,m}^{max}, \hat{C}_{n}^{max}\rbrace }. \tag{70} \end{equation*}
View SourceThus, we have the conclusion.

In Ho-EECC, PoA is defined as \begin{equation*} \text{PoA} = \frac{\sum _{n=1}^{|\mathcal {N}|} C^{\prime }_n(\boldsymbol{\overline{\lambda }^{\prime }_n})}{\max _{\boldsymbol{\lambda_{n}}} \sum _{n=1}^{|\mathcal {N}|} C^{\prime }_n(\boldsymbol{\lambda^{*}_{n}})}, \tag{71} \end{equation*}
View Sourcewhere \boldsymbol{\overline{\lambda }^{\prime }_n} is an optimal offloading strategy of UE_n obtained from a centralized algorithm.

Theorem 7.
In Ho-EECC, for the EECC game, PoA satisfies \begin{equation*} 0 \leq \text{PoA} \leq \frac{\sum _{n=1}^{|\mathcal {N}|} \min \lbrace C_{n,l}, \tilde{C}_{n,m}^{min}, \hat{C}^{\prime }_{n}\rbrace }{\sum _{n=1}^{|\mathcal {N}|} \max \lbrace C_{n,l}, \tilde{C}_{n,m}^{max}, \hat{C}^{\prime }_{n}\rbrace } \leq 1. \tag{72} \end{equation*}
View Source

Proof.
Based on Equations (16), (17), and (19), in Ho-EECC, the cost of a_n executed in the cloud is \begin{equation*} \hat{C}^{\prime }_{n} = \phi _n \Bigg (\frac{\omega _n}{\hat{f}} + \frac{\delta _n}{\hat{r}^{\prime }_{1}} + \frac{\delta _n}{\hat{r}^{\prime }_2} \Bigg) + \sigma ^{\prime }_n(1-\phi _n)\Bigg (\frac{\delta _n}{\hat{r}^{\prime }_{1}} + \frac{\delta _n}{\hat{r}^{\prime }_2} \Bigg). \tag{73} \end{equation*}
View SourceAs shown in the proof of Theorem 6, we know that the minimum cost of UE_n responded to by ES_m is \tilde{C}_{n,m}^{min}. Moreover, the maximum cost of a_n executed by ES_m is \tilde{C}_{n,m}^{max}. Therefore, we have \begin{equation*} \sum _{n}^{|\mathcal {N}|} C^{\prime }_n(\boldsymbol{\overline{\lambda }^{\prime }_n}) \geq \sum _{n}^{|\mathcal {N}|} \min \lbrace C_{n,l}, \tilde{C}_{n,m}^{min}, \hat{C}^{\prime }_{n}\rbrace, \tag{74} \end{equation*}
View SourceRight-click on figure for MathML and additional features.and \begin{equation*} \sum _{n}^{|\mathcal {N}|} C^{\prime }_n(\boldsymbol{\lambda^{*}_{n}}) \leq \sum _{n}^{|\mathcal {N}|} \max \lbrace C_{n,l}, \tilde{C}_{n,m}^{max}, \hat{C}^{\prime }_{n}\rbrace . \tag{75} \end{equation*}
View SourceRight-click on figure for MathML and additional features.Based on the above inequalities, we obtain \begin{equation*} 0 \leq \text{PoA} \leq \frac{\sum _{n}^{|\mathcal {N}|} \min \lbrace C_{n,l}, \tilde{C}_{n,m}^{min}, \hat{C}^{\prime }_{n}\rbrace }{\sum _{n}^{|\mathcal {N}|} \max \lbrace C_{n,l}, \tilde{C}_{n,m}^{max}, \hat{C}^{\prime }_{n}\rbrace } \leq 1. \end{equation*}
View SourceThus, we have the conclusion.

SECTION 7Experimental Evaluation
In this section, extensive experiments with real-world data are conducted to demonstrate the convergence and performance of the proposed algorithms. The comparison between the developed algorithms (i.e., COAHi and COAHo) is actually the comparison between Hi-EECC and Ho-EECC. The scalability and applicability of Hi-EECC and Ho-EECC under the influence of various factors are also comprehensively studied. We present three important conclusions for choosing specific computing architectures in the different application scenario through the experimental analysis.

7.1 Parameter Configuration
In the experiments, we assume that there are six service areas, i.e., |\mathcal {S}|=6. UEs and ESs are randomly located in one of the service areas. Different numbers of UEs and ESs are generated to evaluate the proposed algorithms. Most parameters used in the experiments are real-world values obtained from other work. Specifically, the computing capacity of UE_n is randomly taken from \lbrace 0.5, 0.8, 1 \rbrace GHZ [29], [36]. The computing power of ES_m is randomly assigned from \lbrace 5, 6, 8, 9\rbrace GHz [37]. Furthermore, the computing resource of the cloud is \hat{f} = 10 GHz [29]. The communication resource of ES_m is \tilde{r}_m = 9.97R Mbps [38], where R \in [5,10] is a random integer variable and reflects the heterogeneity of different ESs. The communication resource of the cloud is \hat{r} = 99.7 Mbps. In Ho-EECC, without loss of generality, let \hat{r}^{\prime }_1 = \hat{r}^{\prime }_2 = 1.52 Mbps [27]. In addition, \sigma _n, \sigma ^{\prime }_n, and \phi _n are randomly taken from \lbrace 0.1, 0.3, 0.5, 0.7, 0.9\rbrace. \gamma _n is randomly assigned from \lbrace 1, 2, 3, 4, 5\rbrace.

To reflect the heterogeneity of UEs, we assume that UEs can execute three kinds of tasks: facial recognition [39], video game [40], and video transcoding [41]. Since it is difficult for us to directly obtain the workload (i.e., \omega _n) of a task, we introduce the processing density (represented by \nu _n), which is quantified by the number of cycles per bit [27]. The number of CPU cycles required to complete a task can be calculated through \omega _n = \delta _n \nu _n [27]. The processing densities of the above tasks use the real-world measurement data, i.e., facial recognition: 2339 cycles/bit [39]; video game : 2640 cycles/bit [40]; and video transcoding: 1000 cycles/bit [41]. Moreover, the data size of a task is randomly assigned from \lbrace 1, 2, 3, 4, 5\rbrace MB.

7.2 Experimental Results and Analysis
7.2.1 The convergence of algorithms
Fig. 4 shows the average number of game rounds required for Algorithm 1 to find a Nash equilibrium, and the average number of waiting time slots needed for COAHi and COAHo to determine an offloading strategy in the two computing architectures. During the iteration process, the algorithms allow only one UE to update its strategy at a time, while other UEs are in a waiting state. In this paper, the time required for UE_n to determine an offloading strategy is represented by the number of waiting time slots. In reality, a time slot is very short and at the time scale of microseconds [29]. Therefore, as shown in Figs. 4a and 4b, as the number of UEs (i.e., N=|\mathcal {N}|) increases, the average number of waiting time slots increases. As shown in Fig. 4a, in Hi-EECC, because the resources that UEs can obtain from ESs are very limited, as the number of UEs increases, the competition between UEs intensifies, so the number of game rounds and waiting time slots increases rapidly. However, as shown in Fig. 4b, UEs can obtain sufficient resources from the cloud to meet their own demands in Ho-EECC. Therefore, the number of game rounds does not change with the increase in the number of UEs. Compared with Hi-EECC, this is the reason why the average number of waiting time slots in Ho-EECC is less. The explanation is proved again by Fig. 5a. We can also know from the experiment that the change in the number of ESs (i.e., M=|\mathcal {M}|) does not affect the convergence of the algorithms, which is consistent with Theorems 4 and 5.


Fig. 4.
Average number of game rounds for obtaining a Nash equilibrium, and average number of waiting time slots needed by UEs to determine an offloading strategy in two computing architectures. (a) Hi-EECC. (b) Ho-EECC.

Show All


Fig. 5.
Comparison between COAHi and COAHo in the number of tasks executed by different entities (a), and resource utilization rate of ESs (b).

Show All

7.2.2 The impact of N and M
Fig. 5a shows the number of UEs that execute applications locally (i.e., Local: COAHi and Local: COAHo), upload tasks to the ESs (i.e., ES: COAHi and ES: COAHo), and responded by the cloud (i.e., Cloud: COAHi and Cloud: COAHo) in the two computing architectures. The figure shows that as the number of UEs increases, increasingly more UEs submit their tasks to the cloud. In addition, in Hi-EECC, although ESs are trying their best to satisfy more UEs, increasingly more UEs still choose to perform tasks locally. Moreover, in Hi-EECC and Ho-EECC, since the resources of ESs are limited, as the resources of ESs are exhausted, the number of requests that the ESs can respond to reaches the upper limit. A comparison of the resource utilization rate of ESs between the two computing architectures is depicted in Fig. 5b, i.e., Rate: COAHi and Rate: COAHo. The resource utilization rate of ESs refers to the ratio of the number of ESs responding to UEs’ requests to the total number of ESs, i.e., \sum _{m=1}^{|\mathcal {M}|} \mathbb {I} \bigl \lbrace \sum _{n=1}^{|\mathcal {N}|}\tilde{\lambda }_{n,m} \geq 1\bigl \rbrace /|\mathcal {M}|, where \mathbb {I}\lbrace \cdot \rbrace = \lbrace 0,1\rbrace is an indicator function. \mathbb {I}\lbrace \cdot \rbrace = 1 when the input parameter of the function is true. Otherwise, \mathbb {I}\lbrace \cdot \rbrace = 0. The resource utilization rate of ESs can help select the appropriate computing architecture in the different application scenario, thereby reducing the overhead required to maintain the ESs running. As shown in Fig. 5b, COAHi performs better when N \leq 100, and COAHo performs better when N \geq 200. Moreover, as shown in Figs. 6a and 6b, COAHi performs better when N \leq 300, and COAHo performs better when N \geq 500. It can be seen from the above figures that low-latency data transmission offsets the resource shortcomings of ESs. However, the resources of ESs are unable to cope with the large-scale user scenario. Based on the above discussions, we can reach our first important conclusion. In terms of cost, delay, and energy consumption of UEs, as well as the resource utilization rate of ESs, Hi-EECC is more suitable for the small-scale user scenario, while Ho-EECC is more suitable for the large-scale user scenario.


Fig. 6.
Comparison between COAHi and COAHo in terms of cost (a), and delay and energy consumption (b).

Show All

7.2.3 The performance of algorithms
To evaluate the performance of COAHi, we use the following five schemes as the baselines. (1) RanHi: UEs randomly determine an offloading decision. (2) ClHi: All UEs’ tasks are executed by the cloud. (3) EsHi: All UEs request ESs to execute their tasks. (4) LEsHi: UEs determine the offloading decision between itself and ESs. (5) LClHi: All UEs request ESs to process their tasks. The deadline unsatisfied tasks are further uploaded to the cloud by the ESs. The difference between EsHi and LClHi is in whether to upload the deadline unsatisfied tasks to the cloud. In Ho-EECC, we use the same baselines. However, the differences are that UEs must consider their deadline when making a strategy, and UEs can directly request the cloud. To distinguish the two computing architectures, the relevant benchmarks are named RanHo, ClHo, EsHo, LEsHo, and LClHo. It should be noted that when all UEs can determine offloading decisions only between itself and ESs, both LEsHi and LEsHo determine the decisions by using Algorithm 1. Since the Nash equilibrium is not unique, the strategies of LEsHi and LEsHo have some random differences in terms of cost, delay, and energy consumption of UEs. Moreover, as mentioned above, the comparison between the developed algorithms and the baselines is actually the comparison between computing architectures such as Hi-EECC, Ho-EECC, EC, CC, and edge-cloud computing.

Fig. 7 compares the performance of the algorithms in the different scenario in detail. As shown in Fig. 7, although algorithms EsHi and ClHo perform better in terms of cost, delay, and energy consumption, many UEs’ QoS demands cannot be satisfied. The cost of ESs using different schemes is shown in Fig. 8. It can be seen from the figures that COAHi and COAHo perform better than the baselines. Thus, through the above experiments, we find that the proposed algorithms perform better in terms of the cost, delay, energy consumption, and QoS demand of UEs. Moreover, compared with EC and CC, EECC shows unique advantages. UEs can handle some real-time tasks based on their own resources. ESs can provide UEs with low-latency and low energy consumption services for latency-sensitive tasks. The cloud can provide services for UEs to process their computation-intensive tasks. The end, edge, and cloud are complementary to one another and can more flexibly adapt to various user requirements.


Fig. 7.
Comparison between the proposed algorithms and baselines in terms of cost, delay, energy consumption, and the number of deadline unsatisfied UEs in two computing architectures. (a)-(d) Hi-EECC. (e)-(h) Ho-EECC.

Show All


Fig. 8.
Comparison of the cost of ESs between different algorithms.

Show All

7.2.4 The impact of application type
As shown in Equations (3), (6), and (7), we know that \omega _n affects the computation delay of tasks. As shown in Equations (9), (10), and (12), we know that \delta _n affects the communication delay of tasks. Based on the data size and workload, we classify the application into communication-intensive tasks and computation-intensive tasks. To better reflect the effectiveness of the algorithms, and evaluate the adaptability of two computing architectures to different application types, we introduce two multipliers \alpha and \beta to increase the data size and workload of tasks, respectively. As shown in Figs. 9a, 9b, and 9c, if the data size is expanded to \alpha times the original data size, the performance of COAHi gradually becomes better than that of COAHo. As mentioned above, Ho-EECC is more suitable for the large-scale user scenario. When the user scale is fixed, the increase in the data size directly prolongs the transmission delay of applications. The long distance and low transmission rate between UEs and the cloud weaken the advantage of Ho-EECC. As shown in Fig. 9d, the UEs that originally initiated requests to the cloud began to request ESs to perform their tasks. Based on the above discussions, we can reach our second important conclusion. For communication-intensive tasks, the cost of UEs in Hi-EECC is less than the cost of UEs in Ho-EECC. That is, whether in the large-scale user scenario or small-scale user scenario, we can conclude that Hi-EECC is a better choice for communication-intensive tasks. The reason for this phenomenon is that the communication delay dominates the cost of UEs. Obviously, uploading tasks to ESs closer to UEs is more in line with the UEs’ demands.


Fig. 9.
Comparison between COAHi and COAHo in terms of cost (a), delay (b), energy consumption (c), and the number of UEs responded to by different entities (d) when \alpha takes different values.

Show All

As shown in Figs. 10a, 10b, and 10c, as the workload is expanded to \beta times the original workload, we can see that the performance of COAHi is better than that of COAHo in the small-scale user scenario. However, as shown in Figs. 10e, 10f, and 10g, it can be seen that the performance of COAHo is better than that of COAHi in the large-scale user scenario. Based on the above discussions, we can reach our third important conclusion. For computation-intensive tasks, we can conclude that Hi-EECC is a better choice for UEs in the small-scale user scenario, and Ho-EECC is a better choice for UEs in the large-scale user scenario. As shown in Figs. 10d and 10h, with an increase in workload, UEs tend to initiate requests to the cloud regardless of the computing architectures. The reason for this phenomenon is that computation delay dominates the cost of UEs. The resources of ESs are unable to meet the demands of large-scale users. Hence, we can again confirm that EECC can improve the resource utilization of UEs, ESs, and the cloud to better serve users with different demands.


Fig. 10.
Comparison between COAHi and COAHo in terms of cost (a), delay (b), energy consumption (c), and the number of UEs responded to by different entities (d) when \beta takes different values.

Show All

SECTION 8Conclusion and Future Work
In this paper, we construct a potential game for the EECC environment, in which each UE selfishly minimizes its payoff, and investigate the computation offloading strategy optimization for UEs in Hi-EECC and Ho-EECC. Accordingly, we develop two potential game-based algorithms, i.e., COAHi and COAHo, to determine the best offloading strategies for all UEs. The scalability and applicability of Hi-EECC and Ho-EECC under the influence of various factors are also comprehensively studied. We present three important conclusions for choosing specific computing architectures in the different application scenario through the experimental analysis. The main conclusions are as follows: (1) In terms of cost, delay, and energy consumption of UEs, as well as the resource utilization rate of ESs, Hi-EECC is more suitable for the small-scale user scenario, while Ho-EECC is more suitable for the large-scale user scenario. (2) For communication-intensive tasks, whether in the large-scale or small-scale user scenario, the cost of UEs in Hi-EECC is lower than the cost of UEs in Ho-EECC; that is, Hi-EECC is a better choice for UEs. (3) For computation-intensive tasks, Hi-EECC is a better choice for UEs in the small-scale user scenario, and Ho-EECC is a better choice for UEs in the large-scale user scenario.

The assumption that ESs in the same service area are homogeneous is a limitation of this paper, which is an issue left for our future research. Moreover, the paper opens many research topics in EECC. In our future work, we will first study the impact of mobility on the cost of UEs and the service mode of EECC. The combination of Hi-EECC and Ho-EECC is also an interesting direction worthy of investigation.