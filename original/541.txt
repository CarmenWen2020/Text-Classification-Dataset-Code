Practical Byzantine Fault Tolerance (PBFT) consensus mechanism shows a great potential to break the performance bottleneck of the Proof-of-Work (PoW)-based blockchain systems, which typically support only dozens of transactions per second and require minutes to hours for transaction confirmation. However, due to frequent inter-node communications, PBFT mechanism has a poor node scalability and thus it is typically adopted in small networks. To enable PBFT in large systems such as massive Internet of Things (IoT) ecosystems and blockchain, in this article, a scalable multi-layer PBFT-based consensus mechanism is proposed by hierarchically grouping nodes into different layers and limiting the communication within the group. We first propose an optimal double-layer PBFT and show that the communication complexity is significantly reduced. Specifically, we prove that when the nodes are evenly distributed within the sub-groups in the second layer, the communication complexity is minimized. The security threshold is analyzed based on faulty probability determined (FPD) and faulty number determined (FND) models, respectively. We also provide a practical protocol for the proposed double-layer PBFT system. Finally, the results are extended to arbitrary-layer PBFT systems with communication complexity and security analysis. Simulation results verify the effectiveness of the analytical results.
SECTION 1Introduction
The consensus mechanism/algorithm, which orders transactions and guarantees the integrity with consistency of the blockchain across geographically distributed nodes, is of importance to blockchains, which is the backbone of groundbreaking decentralized ledger technology and cryptocurrency. It provides secure, accountable, and immutable and low-cost solutions. Thus, it has shown great potential in various sectors such as financial services, energy trading, supply chain management, Internet of Things (IoT), etc [1].

The consensus algorithms largely determine the performance of distributed system especially for blockchains, such as transaction throughput, latency, node scalability, security level, etc. Depending on application scenarios and performance requirements, different consensus algorithms can be considered. In the case of a permission-less public blockchain, nodes are allowed to join or leave the network without permission and authentication; therefore proof based algorithms such as Proof-of-Work (PoW) [2], Proof-of-Stake (PoS) [3], and their variants are commonly used in many public blockchain applications, where the cryptocurrency such as Bitcoin is the most well-known one. Proof based algorithms are designed with excellent node scalability performance through nodes competition, which is essential to deal with the double-spending problem. However, they could be very resource demanding. For instance, recently published estimates of bitcoin’s electricity consumption are wide-ranging, on the order of 20-80 TWh annually, or about 0.1%−0.3% of global electricity consumption [4]. Also, these consensus mechanisms have other limitations, such as long transaction confirmation latency and low throughput. For instance, the Transaction Per Second (TPS) is generally limited to 7 in Bitcoin and about 15 in Ethereum, while the transaction confirmation delay is typically as considerable as 10 minutes in Bitcoin and 15 seconds in Ethereum [5]. It is worth pointing out the computational requirement of Proof-based consensus varies from one to another, the notable examples of non-computing Proof-based consensus is InterPlanetary File System (IPFS), a distributed file system uses the concept of proof-of-space/ space-time [6]. Though, Proof-based consensus has been mostly seen in the applications of public blockchain, it has a limited generic distributed system and blockchain coverage, as it is still incrementally resource demanding, and the search of voting-based consensus for blockchain and new generation distributed system is imminent.

Unlike the public blockchain, the private and consortium blockchains prefer to adopt lighter consensus protocols such as PBFT, Paxos [7] and Raft [8], [9] to reduce the amount of computational power and improve the transaction throughput [10]. They have been widely used in general distributed systems for data synchronization, meanwhile, their property is also critically important to the application scenarios of the blockchain-enabled IoT ecosystem, which is typically composed of low cost and low power devices. Though some private chain suitable consensus only enables the Crash Fault Tolerance (CFT) [8], as it does not protect the integrity of transactions from malicious attacks, but they are acceptable for private blockchain where the nodes are trusted.

1.1 PBFT Applied to Blockchain
To protect distributed systems from malicious users, Practical Byzantine Fault Tolerance (PBFT) was proposed in [11] as an improved and practical protocol based on original Byzantine Fault Tolerance (BFT) [9], [12]. Comparing to the Proof based consensus such as PoW, where the security threshold is 51 percent, i.e., absolute secure transaction can be achieved if the malicious user(s) occupies no more than half of the overall resource, PBFT requires the number of malicious users under 33 percent of total participants to ensure the system immune from the malicious attacks [11].

PBFT is favoured for private and consortium chains, thanks to the lower complexity and low energy consumption, which is particularly important for wireless IoT applications [13]. A promising advancement of PBFT can be found in Hyperledger development [14], part of Hyperledger business blockchain frameworks, which has been adopted by tech giants like IBM or Wall Street Fintech, such as J.P. Morgan [15].

1.2 Motivations
Though PBFT has shown good performance in terms of latency, resource requirement and nodes complexity, the node scalability, which is a metric to measure how well the network reflects the capacity of the system to handle the increasing number of nodes, is a bottleneck of PBFT since it relies on heavy inter-node communications. Thus, from the communication complexity perspective, the PBFT based blockchain hardly scales up to 100 nodes [16].

Variant PBFT-based consensuses have been proposed to solve the problem of poor scalability. For example, PBFT with short-lived signature [17], minimizes the consensus time for signature verifying. Another significant evolution path is sharding, where shards have their own consensus group; hence the transactions can be confirmed within a short time because of the smaller size network [18], [19]. However, the trade-off includes increased communication costs and lowered security levels. For instance, every shard keeps its own data, which is not shared with other shards. In the case of users requesting contracts on several shards, inter-node communications go up quickly. Meanwhile, putting segments of data into different repositories without proper redundancy and recoverability is risky. Losing control of any individual shard will interrupt the blockchain completely, either causing untraceable and irreversible records to future records or forking the chain from the breaking point [20].

The communication complexity issues in the PBFT system is due to the exhaustive peer to peer communication among the nodes. To reduce the cost of communication, intuitively, one can construct a hierarchical multi-layer PBFT by refraining the communication within their layers or sub-groups. The sub-consensus can be performed per group, and the overall consensus can be defined as exceeds the number of groups achieved the sub-consensus. This multi-layer PBFT system model is initially proposed in [21], providing a brief analysis of communication complexity based on a particular case. However, there are still many challenges to be addressed before this idea can be applied in a real system. First, the complexity analysis derived in the [21] cannot be applied to generic situations as it is developed under the premise that the number of nodes in each sub-group is equal. To better represent the practical situation, detailed analysis should be provided considering various node allocation scenarios. Also, the security analysis in [21] is derived under the hypothesis that faulty nodes only exist in the bottom layer, which does not apply to real situations where faulty nodes are randomly distributed into all layers. Therefore, new security analysis should also be provided to verify the reliability of multi-layer PBFT system. Finally, to ensure liveness and safety of the network, a new complete protocol is also needed.

1.3 Contributions and Organizations
In this paper, we first propose a double-layer PBFT system, as shown in Fig. 1, where we have m replicas in the first layer and n sub-layer replicas in the second layer, which give a total number of 1+m+mn nodes in the network. Then we extend this system model to a more general X-layer case (X>2,X∈I) to analyze and compare communication complexity in multi-layer systems. The security analysis shows that the security performance is improved over time, and the consensus algorithm accommodates at most ⌊n3⌋×⌊m2⌋ faulty nodes to ensure the absolute safety of the system under malicious attacks. Meanwhile, only m2+mn2 inter-node messages, instead of O(Z2) messages in a traditional single-layer PBFT network consensus, are required.


Fig. 1.
Topology of the proposed double-layer PBFT system. (Note that we give double-layer system as an example here, the idea proposed in this paper can be extended to arbitrary-layer PBFT).

Show All

However, it is inevitable that the scalability is improved at the cost of a longer delay as the consensus-reaching process goes through more than one layer. As a result, the proposed multi-layer PBFT would not be appropriate for systems such as some financial scenarios in consortium blockchain which requires both node scalability and low latency transaction. On the contrary, in other applications such as blockchain-enabled scenarios with massive IoT devices connection, where scalability is a bottleneck but not latency, multi-layer PBFT provides a solution with greatly improved node scalability while protecting the system from malicious attacks [22], [23], [24]. For instance, blockchain can be used to play an important role for tracking and managing all elements involved in supply chain from raw materials to the final products to ensure the quality of the manufacturing [25]. Given the fact that there are many players within the system, traditional single layer PBFT based blockchain is hard to scale up. With multi-layer PBFT, the suppliers could be divided into groups and layers and consensus can be reached among them to provide reliable information for different parties and ensure the efficiency of manufacturing.

Note that, in such hierarchical scenarios, the consensus is reached in a network with biased rights in different layers. Indeed, we have seen the most consensus models in blockchain have treated their nodes with the equality, however, it is also common to find examples of inequality in real-life for upper level scalability and availability; for example, the practice of liquid democracy, has shown a good influence on social stability and performing the best interests of represented members, in which case, the participants are still in active power after the upper-tier delegate is elected. Another common example of master/slave (leader/follower) model of data storage has produced promising results regarding the availability and I/O performance. Moreover, a lack of flexibility and liquid can be added with the proposed dynamic multi-layer design. The multi-layer PBFT and the advanced system together form a rolling, robust and flexible consensus for not only digital distributed systems, but also prompts real-life impacts. From this point of view, the proposed PBFT consensus serves as a viable solution to the current society to a trade-off between an efficient but low secure, centralized architecture and a highly secure but low efficient distributed one.

To summarize, this paper makes the following contributions

This paper first introduces a novel double-layer PBFT model. This model is scalable since it reduces the inter-node communications to C≈1.9Z43, comparing to the traditional PBFT system of O(Z2).

Second, the analytical security performance of the proposed system is derived. It proves that under certain conditions, the maximum number of faulty nodes can increase from ⌊m3⌋ to ⌊n3⌋×⌊m2⌋.

Third, a new double-layer PBFT protocol is introduced, based on which consensus can be reached among nodes in different layers.

Finally, a general X-layer PBFT model is proposed, which is proven to have the minimum communication complexity reduced to linear C=16Z−163 when the network depth is maximized to Xmax. Additionally, the security threshold is derived.

The rest of the paper is organized as follows. Section 2 reviews the related work. In Section 3, the double-layer PBFT model is proposed. Then, the communication complexity is analyzed and compared to the original PBFT. In Section 4, the security threshold is derived based on the double-layer system. A general X-layer system is proposed and analyzed in Section 5. Section 6 proposes the protocols for double-layer PBFT system and Section 7 concludes the paper.

SECTION 2Related Work
Variant consensus algorithms have been designed for permissioned blockchain to provide safety under trustless environment with different performances [26]. Practical Byzantine Fault Tolerance (PBFT) [11] is one of the most popular State Machine Replication (SMR) technology, which provides 13 optimal tolerance under malicious attacks with low latency. In recent years, with the growing interest in blockchain, many new protocols based on SMR are proposed. In The Next 700 BFT Protocols, the author present a method to simplify the designing of new protocols by introducing Abortable Byzantine faulT toleRant stAte maChine replicaTion (Abstract) as a new way to illustrated BFT [27]. Results show that the proposed protocol by using Abstract provides a better performance in terms of both latency and throughput.

Apart from the application in permissioned blockchain, the concept of quorum certificate is also borrowed in Ethereum Casper to provide safety for Ethereum 2.0 [28]. It is an extra mechanism on top of PoS and serves as a finality gadget. The Casper does not generate the block but determines the sequence of blocks on chain. The designated replicas (validators in Ethereum) votes for a parent-children relationship for two collections of blocks and form a quorum certificate granted by more than 23 of replicas. Considering that Ethereum 2.0 updates rapidly, it is difficult to draw a conclusion on the application of shards and Casper. The sharding may sacrifice safety and increase communication complexity, and the actual implementation of Casper in Ethereum 2.0 also affects it.

One problem PBFT protocols may encountered is the difficulty to implement. BFT-SMART is so far the most popular open-source library for BFT-SMR application based on Java, filling the gap between the literature work and practical implementation [29]. Evaluation shows that the throughput of BFT-SMART reaches more than 80,000 TPS. Also, although it is simpler than other BFT implementing systems, it still contains 13.5K lines of Java code, which is much more complicated than the implementing systems for Paxos [26].

Another bottleneck of PBFT is its scalability. As mentioned in the introduction, the communication complexity limits the performance of protocols. Former researchers have also proposed various solutions. The HotStuff leverages threshold signature to reduce communication complexity [30]. In each phase, the primary broadcasts messages and each replica responses a valid message with a partial signature. The primary collects them responses with partial signature more than 23 replicas and combine into a digital signature. This signature is broadcasted by replica again, which serves as a quorum certificate. Unlike HotStuff, the multi-layer PBFT proposed improves node scalability by reorganizing network structure, where the threshold signature can also be applied to further reduce the communication complexity. Moreover, it is noticeable that as the network scales up, the primary in HotStuff has to collect and broadcasts an increasing number of messages and combines more partial signatures. This workload can be barked down by implementing our tree-like structure. Also, the pace of HotStuff is affected by the primary since it waits for the aggregation of partial signature to advance.

SECTION 3Communication Complexity of Double-Layer PBFT
3.1 Original Single-Layer PBFT
Before introducing the system model of double-layer PBFT, the protocol and communication complexity of original single-layer PBFT [11] are briefly analyzed in this subsection. Fig. 2 shows the protocol diagram of the original single-layer PBFT. As an example, we assume there is one primary node and three state machine replicas. The consensus is triggered by a client sending a request to the nodes’ header (Replica 0). Then consensus will be operated among the nodes, and if an agreement is reached among the replicas, the new record will be committed to the blockchain, vice versa. The whole consensus process includes three stages pre−prepare, prepare and commit as shown in Fig. 2. On receiving the request from the client, the primary node (i.e., Replica 0) broadcasts a pre−prepare message to the other nodes. In prepare and commit stages, all replicas send messages to check the validity of received messages. In each stage, a minimum number of consistent messages are required for stepping into the subsequent stage.

Fig. 2. - 
Single-layer PBFT consensus processing [11].
Fig. 2.
Single-layer PBFT consensus processing [11].

Show All

The consensus is technically reached when the commit phase is successful among the majority of non-faulty nodes. Specifically, the client must receive at least f+1 replies (f denotes the number of faulty nodes in the group) from the nodes to validate the final consensus with a total number of 3f+1 replicas. This ensures that at least one non-faulty replica replies to this operation. In the case of the client fails to collect f+1 replies, the client may resend the request to primary for retry. Upon receiving the same request again, if the consensus is already reached on the commit phase, replicas just resend the final stage messages. If the consensus is not reached, the network goes over the protocol again.

From Fig. 2, we can see that PBFT is a communication demanding protocol. Given the total node number Z, the original single-layer PBFT requires O(Z2) times of inter-node communications to reach consensus. Obviously, the system is not scalable since the complexity burden is non-affordable when Z is large (i.e., thousands).

In the next, a scalable multi-layer PBFT system and protocol is proposed to reduce the communication complexity. The performance analysis and protocol design of a double-layer system will be introduced first, and then the arbitrary-layer system will be proposed in Section 5.

3.2 Protocol Overview of Double-Layer PBFT
In the double-layer PBFT protocol, scalability is improved by recursively inserting the PBFT consensus algorithm between commit and reply phases as the sub-layer algorithm. The higher layer forms a certificate, which proves that the consensus was reached. With this certificate, a node in the first layer initiates a PBFT consensus reaching process among second layer nodes.

As illustrated in Fig. 1, there are m replicas and a primary node in the first layer. Each replica in the first layer forms a consensus group with n sub-layer replicas in the second layer. The primary invokes a new operation by multicasting pre−prepare messages to replicas in the first layer with information about this operation. The replica accepts valid pre−prepare requests and step into prepare phase by multicasting prepare messages within the first layer. If one replica receives no less than 2f identical prepare messages from other first-layer replicas, that operation is prepared on this replica. A prepared replica then multicasts commit messages among first-layer replicas.

Similarly, this operation is committed after collecting 2f identical commit messages. At this point, this particularly first-layer replica is considered as the primary node in its consensus group and starts consensus reaching by multicasting new preprepare messages to its sub-layer nodes. For instance, in Fig. 1, a committed node r1 sends pre−prepare messages to node r2 and other replicas in ConsensusGroup0. Second-layer replicas repeat the process mentioned above until the commit phase. Finally, all committed replicas in the second layer send reply messages to primaries, and primaries reply to the client. This protocol will be stated in detail in Section 6.

3.3 Communication Complexity Analysis of Double-Layer PBFT
The double-layer PBFT model is proposed in Fig. 1, where the first-layer leader controls m replicas, each of which serves as a primary node of the n sub-layer replicas in the second layer. Therefore, there are 1+m+mn nodes in the system. Note that here we assume each sub-group in the second layer has the same number of nodes, and a generic case will be discussed in Proposition 4. Based on this, the communication complexity can be calculated as the following proposition.

Proposition 1.
For a double-layer PBFT system with m replicas in the first layer and n sub-layer replicas in each sub-group, the communication complexity C to reach consensus is
C=(m+1)2+m(n+1)2.(1)
View Source

Proof is derived in Appendix, which can be found on the Computer Society Digital Library at http://doi.ieeecomputersociety.org.ezproxy.auckland.ac.nz/10.1109/TPDS.2020.3042392.

In the next, we aim to find the optimal setup of a double-layer system with given Z nodes to provide the lowest communication cost. When the overall number of nodes Z is given to form a double-layer PBFT, we can assign either larger groups (i.e., smaller m and larger n) or a larger number of groups (i.e., larger m and smaller n). Proposition 2 gives the best grouping algorithm in terms of minimizing the communication complexity.

Proposition 2.
For a double-layer system containing Z nodes in total, the minimum communication complexity can be achieved when n equals to the nearest integer to the real positive root of following equation:
n3+3n2+n=2Z−1,  (3≤n≤Z−43).(2)
View Source

Proof is derived in Appendix, available in the online supplemental material.

Note that, Proposition 2 is based on the assumption that the system is full, in other words, the number of sub-layer replicas in each sub-group is equal.

Proposition 2 provides with a pragmatic method to design a double-layer PBFT system with minimum C. In the next, we try to find a direct function of C versus Z so that the communication complexity of any double-layer system can be estimated analytically.

Proposition 3.
When m and n are fixed by optimal allocation and the system is full, the relationship between communication complexity C and total node number Z can be written as
C≈1.9Z43.(3)
View Source

Proof is derived in Appendix, available in the online supplemental material.

Fig. 3 compares the analytical and estimated results from Propositions 2 and 3 respectively and validates the availability of Equation (3).


Fig. 3.
Comparison of analytical and estimated results of communication complexity for double-layer PBFT.

Show All

From Fig. 3 we can see that compared with original single-layer PBFT, where C is quadratic of Z, the communication complexity is greatly reduced in double-layer PBFT systems. For example, the communication complexity of a system with 1000 nodes is reduced by two orders of magnitude, from 106 to 2×104. Unfortunately, the communication complexity of the double-layer PBFT system is still not linear against the node number; instead, it is of 43 order.

In the following proposition, we focus on the setup of the systems that are not full. In other words, the number of sub-layer replicas in each sub-group may be different.

Proposition 4.
If the double-layer system is not full, the communication complexity reaches a smaller value when the vacancies are equally distributed into the sub-groups. The minimum value can be reached by distributing vacancies to the minimum number of sub-groups.

Proof is derived in Appendix, available in the online supplemental material.

Through double-layer PBFT, complexity can be significantly lowered compared to the original single-layer PBFT. However, the proposed system may cause a longer delay. In addition, with such a topology, the security performance should be analytically investigated to guide the actual system deployment.

SECTION 4Security Analysis
4.1 Security Threshold Analysis
In the double-layer PBFT, nodes in both layers participate in the consensus reaching process. The first layer is a classic PBFT model that tolerates no more than ⌊m3⌋ faulty nodes based on the conclusion Z>=3f+1 [11]. In the second layer, as there are m PBFT consensus groups, we need to analyze the threshold of consensus-reached sub-groups required to ensure the security and Liveness of the whole system. During the consensus-reaching process, as the leader of each sub-group directly send post−reply to the client, each consensus sub-group is regarded as a whole. While any individual node in PBFT systems can be divided into three categories, including consensus reached, not reached and faulty node. A consensus network may only be in two situations: consensus reached and not reached. In other words, a consensus sub-group would also be in two situations, either consensus reached or not. In this case, a system tolerates at most ⌊m2⌋ failed groups to reach consensus, i.e., the security threshold of consensus-reached sub-groups is ⌊m2⌋.

To facilitate distributed systems and blockchain in different environments, we analyze the success rate in two models under malicious attacks. The faulty probability determined (FPD) model is used when the probability of every single faulty node is fixed, and the faulty number determined (FND) model is used when the number of faulty nodes in the system is fixed. In these two models, we are given different initial conditions to analyze the security performance of the system. More specifically, we assume the faulty nodes in the FPD model are independent with each other, and they have the same faulty probability. Conversely, in the FND model, the probability of whether one node is faulty depends on other nodes because the total number of faulty nodes is fixed. In addition, these two models have different application scenarios. The FND model, which is more similar with the traditional PBFT, is suitable for small systems where the number of faulty nodes can be easily estimated. However, it is more appropriate to use FPD model to evaluate the performance of large systems where node failure is estimated by probability. For example, in manufacture, we are often given the reject rate of one product instead of the specific number. Finally, though FPD and FND models have many differences, the security performances of them approach the same as the system scaling up, which will be shown in Section 4.2. In addition, the frequently used notations are summarized in Table 1.

4.1.1 Faulty Probability Determined
Let’s assume Pf is the faulty probability of each node. To find out the relationship between the success rate PP and Pf, we shall first define two important conditions, under which consensus can be reached.

no more than ⌊m3⌋ faulty nodes in the first layer (EVENT A)

no more than ⌊m2⌋ groups fail in the second layer (EVENT B)

In addition, Event A and Event B are not independent. If one replica in the first layer is faulty, it will be impossible for the corresponding sub-group to reach consensus. Therefore, we have PP=P(A)×P(B|A). Assume that there are i faulty nodes in the first layer and 0≤i≤⌊m3⌋. According to the cumulative distribution function [31], we can get
P(A)=∑⌊m3⌋i=0Cim(1−Pf)(m−i)Pif.(4)
View SourceThe value of P(B|A) depends on the value of P(A). Equation (4) indicates there are already i faulty nodes in the first layer. It means i out of m groups in the second layer share no chance to reach consensus as they have a faulty leader. Therefore, there can be at most ⌊m2⌋ failed groups in the second layer. We assume there are j groups, which do not have a faulty leader, fail to reach consensus. 0≤j≤⌊m2⌋−i. We have
P(B|A)=∑j=0⌊m2⌋−iPjg(1−Pg)(m−i−j).(5)
View SourcePg represents the probability of a group, with a non-faulty leader, failing to reach consensus. We assume there are g faulty nodes in one single group. To make this group fail, ⌊n3⌋+1≤g≤n since PBFT group tolerates up to ⌊n3⌋ faulty nodes. Therefore, we have
Pg=∑ng=⌊n3⌋+1CgnPgf(1−Pf)n−g.(6)
View SourceWe can get the function of the system consensus success rate PP against Pf as follows:
PP=∑⌊m3⌋i=0(Cim(1−Pf)(m−i)Pif∑j=0⌊m2⌋−iPjg(1−Pg)(m−i−j)).(7)
View Source

To verify the closed-form expression derived, a simulation is performed based on random sampling in MATLAB. In the simulation process, we take the faulty probability Pf and node number m, n as the input and use a random array consisting m+mn numbers to represent the status of nodes. Each number in this random array is either 1 or 0, representing faulty and non-faulty node respectively. On top of that, we set that each array element has a probability of Pf to be 1 (faulty), otherwise it is 0 (non-faulty). In this case, by counting the faulty nodes in each layer and sub-group and comparing the results with the thresholds, we can determine whether the consensus can be reached or not. Then, the above mentioned process is repeated over 10000 times and the simulation success rate for one value of Pf can be obtained by taking the ration of success times to the total repeating times. Finally, by increasing Pf from 0 to 1, we can easily get the simulation curve and compare it to the analytical result of the closed-form expression. The simulation design of the FND model is similar while the only difference is that FND takes the faulty node number instead of faulty probability as the input.

Note that, the purpose of the simulation is to examine the correctness of the derivations. Therefore, the complex peer-to-peer communication process is temporarily ignored in the simulation performed. However, we are also working on a system simulation, which takes every communication and view change into consideration, to further test the multi-layer PBFT system.

Fig. 4 shows clearly that the two curves match well, which verifies the effectiveness of the analytical result.


Fig. 4.
Analytical and simulation results for success rate in FPD model. (m=n=30).

Show All

4.1.2 Faulty Number Determined
In the FND model, we assume that there are K faulty nodes in the whole system and aim to find the relationship between K and the success rate PN. Meanwhile, we can use the same assumption of event A and event B to calculate PN, PN=P(A)×P(B|A).

Unlike the FPD model, P(A) and P(B|A) are calculated using the hypergeometric model [32] since the FND model is based on the prerequisite of a fixed number of faulty nodes. Thus, we have
P(A)=1CKm+mn∑⌊m3⌋i=0CimCK−imn.(8)
View SourceAlso, there should be at most ⌊m2⌋−i failed sub-groups with non-faulty leaders, which means at most ⌊m2⌋−i sub-groups have more than ⌊n3⌋ faulty nodes. However, in the FND model, the number of faulty nodes in each group affects situations in the other groups so that it will be extremely complicated to consider m groups together. Therefore, a simplified binomial distribution model on the group level is adopted, assuming every group has the same faulty probability of Pg2.

Pg2 represents the probability of a group with a non-faulty leader failing in the second layer. It can be calculated as follows:
P(B|A)≈∑j=0⌊m2⌋−iPjg2(1−Pg2)(m−i−j),(9)
View Source
Pg2=∑ng=⌊n3⌋+1CgnCK−gmn−n−1CKm+mn−1.(10)
View SourceThen we can get the probability PN against K as
PN=1CKm+mn∑⌊m3⌋i=0(CimCK−imn∑(⌊m2⌋−i)j=0Pjg2(1−Pg2)(m−i−j)).(11)
View SourceIn Equation (11), ∑⌊m3⌋i=0CimCK−imn requires K−i>0 since the combinatorics of a combination number must be positive. When K−i>0, i.e., K<⌊m3⌋, the success rate can be simply calculated as
PN=1CKm+mn∑Ki=0CimCK−imn.(12)
View Source

The curves in Fig. 5 show that, in the high success region where (PN>0.85), the analytical curve matches the simulation curve. When the success rate of the system is lower, there is a slight difference between our calculation and simulation results. The rationale behind this is that a part of the model (group level in the second layer) is simplified from hypergeometric distribution to the binomial distribution by using Pg2 as a failure rate for every group in Equation (10). However, the error is negligible and will approach zero as the total number of the nodes in the system increases since the hypergeometric distribution approaches the binomial distribution in large systems [32].


Fig. 5.
Analytical and simulation curves for success rate in the FND model. (m=n=30).

Show All

4.2 Fault Tolerance Evaluation
Comparing the FPD model and the FND model with different network sizes, Fig. 6 shows that the curves of the two models gradually concur as m and n increase.


Fig. 6.
FPD and FND models’ analytical results with different m and n.

Show All

From Fig. 6, it can also be observed that as m and n get larger, the slope of the success rate curve approaches infinity around x=13 in both models. If we increase m and n to 500, the curve shows a more obvious trend to step around Pf=13, as in Fig. 7. The rationale behind this is that for a scaled-up system, faulty nodes are approximately evenly distributed. In this case, the proportion of faulty nodes in the whole system and that in each sub-group are very close. In other words, with the proportion of faulty nodes in the whole system reaching around 13, the corresponding proportion within each consensus group also approximates 13, which is the security threshold in original single-layer PBFT. Therefore, the success rate steps to zero when the proportion of faulty nodes or the faulty probability exceeds 13. Therefore, we have the following proposition.


Fig. 7.
Analytical result of FPD model with m=n=500.

Show All

Proposition 5.
The system tolerates a larger proportion of faulty nodes when scaling up. The fault tolerance of double-layer PBFT converges to 13 when Z goes to infinite.

Fig. 6 only compares the situations where m and n are multiples of 3. We choose these special circumstances because when the m and n are multiples of 3, the system provides better security performance compared with others. This can be explained by considering the threshold of faulty nodes within each group. For example, two PBFT groups with 3f+1 and 3f+3 nodes can both tolerate f faulty nodes at most based on the conclusion in [11]. However, the ratio of maximum faulty nodes to the total nodes in the first group is bigger than that in the second one. Therefore, assigning m and n to be multiple of three makes the ratio reaches its maximum. Bringing these groups together, the whole system also shows better security performance. Moreover, when m is a multiple of 6, the security performance is even better since we require at least half of the sub-groups to reach consensus. This means two systems with m=18 and m=19, for example, both tolerate 9 failed sub-groups at most. In this way, systems with an even integer m hold a larger ratio of faulty nodes. Based on these, we have the following Remark.

Remark
By assigning m and n to be multiples of 6 and 3 respectively, the fault tolerance performance can be improved.

4.3 Advanced System and Security Optimization
In this subsection, the advanced system is proposed as an ideal and ultimate situation where the nodes in the two layers are classified. We assume that nodes in the first layer are always non-faulty, while nodes in the second layer have a probability of Pf to be faulty. Note that, this is built under the assumption that, by implementing view change whenever vulnerability is detected in the first layer, after a long period, the nodes left in the first layer are the ones with higher reliability. These nodes show more stable performance and remain faithful for a certain period of time. The consensus success rate of the advanced system can be calculated as
PA=∑j=0⌊m2⌋Pjg3(1−Pg3)(m−j),(13)
View Source
Pg3=∑ng=⌊n3⌋+1CgnPgf(1−Pf)n−g,(14)
View Sourcewhere Pf represents the faulty probability of second-layer nodes and PA represents the consensus success rate of the advanced system. Comparing the analytical result with the simulation results in Fig. 8, we can see that the two curves concur. This proves the analytical result is valid.


Fig. 8.
Analytical and simulation curves for success rate in advanced system. (m=n=30).

Show All

In the practical systems, it is always worth to know what the determined security threshold is, i.e., to achieve 100 percent success rate, what is the maximum faulty nodes. Unlike the single-layer PBFT, the double-layer PBFT can be vulnerable if the faulty nodes are randomly distributed. This is because that the first layer is made up of one PBFT group consisting of a limited number of nodes, and the system has no chance to reach consensus if more than one-third of them are faulty nodes. In this case, as long as there are more ⌊m3⌋ faulty nodes in the system, some specific faulty node distributions will cause the system to fail. In other words, the first-layer nodes have more weights than the second for the final decision. However, as we mentioned in the introduction, this topology is very useful in decentralized systems to balance efficiency and security performance.

Fortunately, the determined security threshold can be improved over the operation time. In the advanced system where there is no faulty node in the first layer, the success rate stays at 100 percent until the number of faulty nodes increases to ⌊n3⌋×⌊m2⌋. Note that the intermediate states exist but not presented. Therefore, we have the following proposition.

Proposition 6.
To achieve 100 percent success rate, the maximum number of faulty nodes tolerated increases from ⌊m3⌋ to ⌊n3⌋×⌊m2⌋ if the advanced system can be achieved.

However, this does not indicate that the consensus rate falls immediately to 0 when this threshold is exceeded. As shown in Fig. 8, the curve does not show a rapid decline until the probability of faulty nodes reaches around 0.3. To guarantees 100 percent success of consensus, number of faulty nodes cannot be more than 1/6 of overall nodes. However, in practical deployments, especially with wireless communication uncertainty in large systems, it is very costly (and even impossible) to achieve a 100 percent success of consensus. Thus, fault tolerance of the double-layer system depends on the reliability requirement in different scenarios. For example, in 5G where the reliability requirement is lowered to 99.999 percent [33], the statistical fault tolerance is much higher than 16 according to Fig. 8.

SECTION 5X-Layer PBFT System
5.1 Communication Complexity Analysis of X-Layer System
X-layer PBFT system represents a general situation where nodes in a network are allocated to more than 2 layers. The consensus algorithm of ith (i≤X) layer is inserted between the commit and reply phase in i−1th layer so that communication complexity can be further lowered. Suppose in an X-layer system, where every layer is full, the number of nodes in each subgroup in the ith of X-layer is mi+1 (m0=1). The total number of nodes ZX in this X-layer system is
ZX=1+∑a=1X(∏i=1ami).(15)
View SourceThe communication complexity CX of this X-layer PBFT system is
CX=∑i=1Xmi−2mi−1(mi+1)2,(16)
View Sourcewhere m−1 is defined to be 1.

In this case, the minimum communication complexity of X-layer system can also be transformed into a typical optimization problem, which aims to solve the minimum value of Equation (16) under the restriction of Equation (15). The method is similar to Proposition 2, which is omitted here. Fig. 9 compares the communication complexity of systems with the same number of nodes but different network depth. It shows that lower communication complexity can be obtained by dividing a system into more layers.


Fig. 9.
Communication complexity of PBFT and multi-layer systems.

Show All

Therefore, we infer that the minimum communication complexity CXmin will be reached if a system is divided into maximum network depth Xmax, i.e., by allocating the minimum number of 3 replicas into every sub-group. In other words, the Xth layer contains 3X nodes in total. For given Z, Xmax can be expressed as
Xmax=⌊log3(2Z+1)⌋−1.(17)
View SourceTo analyze the communication complexity of this limiting case, we suppose that Z is an integer that satisfies Z=1+3+32+⋯+3Xmax. This is to say, there are exactly 3 replicas in each sub-group in each layer. In this case, CXmin can be calculated as
CXmin=∑i=1Xmax3i−1(3+1)2(18)
View Source
=16×1−3Xmax1−3(19)
View Source
=16Z−163.(20)
View SourceSo far, we have analyzed the communication complexity of multi-layer PBFT when the network depth is maximized. During this process, we make an assumption about the number of replicas in each sub-group to explore the what the communication complexity would approach when X continues to increase. The analytical result shows the communication complexity of the Xmax-layer system is lowered to 16Z−163, a linear relationship with the node number Z. Therefore, we have the following proposition.

Proposition 7.
The communication complexity is further lowered by increasing network depth and is in the range of 1.9Z43≥C≥16Z−163. In other words, multi-layer PBFT system provides better node scalability compared with the original PBFT.

However, it is worth mentioning that other performances such as security and latency serve as a trade-off when reducing the communication complexity. Detailed analysis is provided in Sections 5.2 and 5.3 as follows.

5.2 Security Analysis of X-Layer System
Like the double-layer system, the security performance of the X-layer PBFT system can also be optimized over operation time by a proper protocol. In this case, the threshold TX to maintain the success rate at 100 percent is
TX=⌊mX3⌋×⌊∏X−1i=1mi2⌋.(21)
View SourceRight-click on figure for MathML and additional features.

From Equation (21), it can be seen that TX<16 and it drops as X increases. In other words, by increasing the network depth, the security performance is weakened while the communication complexity is improved. This is because increasing layer number leads to fewer node number in each layer, which makes the system more vulnerable to randomly distributed faulty nodes. In the practical scenarios, this trade-off should be considered when designing multi-layer PBFT systems.

5.3 Latency Analysis
Another trade-off when reducing the communication complexity is latency, which will be analyzed in this subsection. In the proposed multi-layer PBFT, we construct a hierarchical structure to limit the peer-to-peer communication within each group or layer. Meanwhile, it is inevitable that the system confirmation delay is prolonged since the consensus reaching process is carried out in each layer successively.

In fact, the confirmation delay would keep increase with increasing network depth X. Assuming that each layer takes average tavg seconds to reach consensus and propagate to the next layer, the average propagation delay tapd holds a linear relationship with the network depth X, i.e., tapd=Xtavg. Fortunately, if the protocol is used in the Internet, we can use parallel routes and distribute different information through different paths. In this way, though the consensus-reaching process still goes through different layers, the delay within groups is reduced, contributing to a lowered overall latency [34].

Conclusively, compared with original PBFT, the multi-layer PBFT sacrifices certain system delay while providing low complexity. Even though, compared with other consensus algorithms such as PoW that has good scalability, the latency of multi-layer PBFT is significantly lower. Therefore, the proposed system can be regarded as a trade-off between system delay and scalability of the existing protocols. Table 2 is provided to compare the performances of different protocols and the proposed multi-layer PBFT. As such, Considering the different demands of practical scenarios, different consensus protocols could be adopted to provide optimal performance accordingly.

TABLE 1 Frequently Used Notations

TABLE 2 Performance Comparisons of the Proposed and State-of-the-Art Consensuses

SECTION 6The Protocol
In this section, we propose a practical protocol dedicated to the double-layer PBFT, where replicas in the first layer are denoted as r1i (superscript 1 for layer number, subscript i for replica index). r1i act as leaders for corresponding second-layer replicas (r2i). One leading r1i and its corresponding r2is, all together, form a consensus group, resulting in a tree-like topology structure.

When each group reaches consensus, group members reply to their leader instead of the client. Then the leader collects replies and sends it to the client on behalf of that consensus group. The client accepts the results only agreed by more than half consensus groups. The protocol overview is illustrated in Fig. 10. Meanwhile, there is a group configuration GP that describes the allocation group members and their leader. It will be updated when the network structure is changed. In brief, the new protocol inserts successive pre−prepare, prepare, and commit phases before starting the commit phase in the upper layer.

Fig. 10. - 
Implementation flow chart for double-layer PBFT model.
Fig. 10.
Implementation flow chart for double-layer PBFT model.

Show All

6.1 Consensus Flow
6.1.1 The Client
A client c sends a request message [o,t,c]request to primary. This request invokes an operation o with timestamp t. Timestamps are ordered by time, so the stamp of later operation contains higher values. The request is sent to the replica. The identity of the replica is extracted from the view number contained in replies from previous operations. On receiving the request, primary multicasts messages using the protocol stated below.

All group leaders reply results to the client directly. The poset−reply has the form [o,t,c,i,r,rc1,ν,GP]poset−reply where ν is the current view number, i is the replica number, r is the result of the execution, rc is the reply certificate and GP describes the replicas allocation.

Assuming there are a number of m replicas in the first layer and n in the second, and the number of faulty replicas in a consensus group is fg (to distinguish from f). If leaders have received fg+1 matching valid replies from the same consensus group, this group is said to have reached consensus. The network reaches consensus when more than half of the groups have the same replies. The client only accepts results replied from group leaders when at least half of them are consistent.

6.1.2 First-Layer Protocol
In the first layer, the primary and m replicas form a consensus group. When primary p receives a request M=[o,t,c]request from client, it authenticates the request and client’s identity. Then primary assigns a sequence number α to M. After that, the primary steps into pre−prepare phase by multicasting [M,d,α,ν]pre−prepare1 where d is the digest of M (superscript is to distinguish pre−prepare1 in the first layer from pre−prepare2 in the second layer). Primary multicasts messages only among r1. Thus, only r1 reacts on pre−prepare1. The propagation is restricted since the protocol runs layer by layer.

For pre−prepare, prepare and commit messages, a replica r1i accepts the one with the same view ν; the authenticity is then verified; α is between watermark h and H. The watermark is introduced to ensure a weak synchronization and defined in Section 6.4.

With conditions above, a replica i in the first layer accepts a pre−prepare1 message from primary only when there is none different request with the same view ν and sequence number α is accepted. Then it multicasts [d,α,i,ν]prepare1 messages to all r1i in the first layer. It records both pre−prepare1 and prepare1 messages to its log. During the prepare phase, each r1i replica collects 2f messages with matching sequence number α, view ν, and request M. With the received pre−prepare1 messages, they form prepared−certificate1, which indicates a particular r1i replica has prepared the request.

For prepared r1i, it multicasts [d,α,i,ν]commit1 and waits for more than 2f+1 matching commit1 messages with the same view, sequence and digest from different r1 replicas. Received messages form commit−certificate1 (cc1) and this request is said to be committed on replica r1i. Then the replica pauses the execution and initiates another round of protocol in the second layer as described in Section 6.1.3.

The committed replicas send [o,t,i,r,ν]reply1 to their group leader, i.e., the primary in the first layer. The primary confirms that this group has reached consensus by checking more than half of group members reply consistent reply1 messages, including itself. The group leader collects reply1 and forms a reply-certificate rc1. After that, this primary replies to the client with [o,t,c,i,r,rc1,ν,GP]poset−reply. Notice that it is not necessary for primary to reply to the client on behalf of consensus, but we require primary to do so to keep the algorithm the same on all replicas in case a massive deployment.

6.1.3 Second-Layer Protocol
A committed r1i multicasts new pre−prepare message to r2 within the same consensus group, where another round of PBFT protocol is implemented. All group members reply to the leader in a similar manner in Section 6.1.2 when the request is committed again. For a replica r1p which acts as primary, it multicasts a similar [M,d,α,ν,cc1]pre−prepare2 to r2i replicas in same consensus group, where cc1 is the commit−certificate1. The ν, α, and M are inherited from the previous process. A replica r2i in consensus group will accept the request if the condition mentioned in the first-layer protocol is satisfied, in addition to the presence of cc1.

On receiving valid pre−prepare2 message, the pre-prepared r2i multicasts [d,α,i,ν]prepare2 messages to all r2i in same consensus group. It adds both pre−prepare2 and prepare2 messages to its log. In the prepare2 phase, each r2 replica collects 2f messages with matching sequence number α, view ν, and request M. With received pre−prepare2 message, it forms a quorum prepared certificate2, which indicates that this r2i replica has prepared the request.

Then the prepared replicas r2i and their r1i leader multicast [d,α,i,ν]commit2 and collect 2f+1 matching commit2 messages with the same view, sequence and digest form different r2i replicas. These commit2 form commit−certificate2, and this request is said to be committed. Replica then executes the message which has been committed. After the execution, all group members reply to the result to their group leader, and the leader replies to the client in a similar manner in Section 6.1.2. The pseudocode for protocol are described in Algorithms 1, 2 and 3.

Algorithm 1. Primary Normal-Case Pseudocode
while valid request1 received=True do

if client identity authenticated=True then

m←n.

multicasts pre−prepare1 to r1.

end if

end while

while valid prepare1 received=True do

if number of valid prepare1>2f then

forms prepared−certificate1.

multicasts commit1 to r1.

end if

end while

while valid commit1 received=True do

if number of valid commit1>2f then

forms commit−certificate1.

end if

end while

while valid reply1 received=True do

if number of valid reply1>half of members then

forms rc1.

reply client with post−reply1.

end if

end while

6.2 Faulty Primary Elimination
6.2.1 Faulty Primary Detection
The most commonly applied condition for initiating a view-change is by detecting whether the primary is responding, i.e., the replicas keep a timer which will be reset each time a new request is received. However, a faulty primary that assigns different pre−prepare to different replicas will not trigger time-out. Thus, we present a possible mechanism without a timer to detect faulty primary nodes that multicast random messages during prepare phase. Since one replica may skip several operations when the connection is lost, we want the detection to be independent without the prerequisite of total order and continuous sequence number α, i.e., replicas accept discrete α as long as it is consistently increasing/decreasing. To facilitate understanding of the mechanism, we first assume that the primary is not faulty. Since there are at most f faulty replicas, the collected 2f+1 messages contain f faulty messages in the worst situation. And the rest f+1 messages must be obtained from non-faulty replicas; thus, those f+1 messages are identical. Those faulty messages (or the digest of the messages) could be unmatched with each other. Or, the faulty messages are identical, but those faulty messages are different from those matching messages among non-faulty nodes. For simplicity, we say that in the latter case, there are two versions of messages in 2f+1 collections, one kept among non-faulty replicas, and another version is kept among faulty replicas.

Algorithm 2. r1i Normal-Case Pseudocode
while valid pre−prepare1 received=True do

multicasts prepare1 to p r1.

end while

while valid prepare1 received=True do

if number of valid prepare1>2f then

forms prepared−certificate1.

multicasts commit1 to r1.

end if

end while

while valid commit1 received=True do

if number of valid commit1>2f then

forms commit−certificate1.

multicasts pre−prepare2 to subordinate r2.

end if

end while

while valid prepare2 received=True do

if number of valid prepare2>2f then

forms prepared−certificate2.

multicasts commit2 to r2.

end if

end while

while valid commit2 received=True do

if number of valid commit2>2f then

forms commit−certificate2.

reply primary with reply1.

end if

end while

while valid reply2 received=True do

if number of valid reply2>half of members then

forms rc2.

reply client with post−reply2.

end if

end while

Algorithm 3. r^2_i Normal-Case Pseudocode
while valid pre-{prepare^2} received=True do

multicasts {prepare^2} to r^1 r^2 in same consensus group.

end while

while valid {prepare^2} received=True do

if number of valid {prepare^2}>2f then

forms prepared-certificate^2.

multicasts commit^2 to r^2 in same consensus group.

end if

end while

while valid commit^2 received=True do

if number of valid commit^2>2f then

Forms a quorum commit-certificate^2.

Send reply^2 to group leader.

end if

end while

When primary is non-faulty and each replica collects 2f+1 messages, the number of different versions n_v falls in the interval 2 and f+1. That is because, in the worst case mentioned above, each faulty replica multicasts different versions of messages, resulting in f different versions among faulty replicas. Besides, there is an additional version kept by f+1 non-faulty replicas. At the presence of faulty primary that multicasts random messages, the messages in pre-prepare that received by non-faulty replicas differ. In other words, n_v exceeds the upper bound f+1, and the primary is detected to be faulty. This condition, along with time out, triggers the view-change phase.

6.2.2 View Change
In conventional PBFT [11], replicas invoke view change in prepare phase. Changes are made to adapt to our multi-layer model. As shown in Fig. 1, a replica is the primary of its sub-layer replicas. Thus, in this protocol, replicas in a specific layer detect their faulty primary in the upper layer and invoke cross-layer view change. Each replica, which suspects the primary to be faulty, multicasts a view-change messages with the stable checkpoint to new primary (the new primary may be determined by election mechanism based on current view number). The new primary decides whether to lunch new-view.

Suppose a replica is in layer L as a member for group K, and notice the group leader is the group member of group in layer L-1. If it suspects the group leader is faulty for not responding or deliver pre-prepare messages with invalid sequence number, it stops accepting requests and starts view change that moves the view of this group from \nu into \nu +1 by multicasting [\nu +1,\alpha,C,P,i]_{view-change}, where \alpha is the sequence number for the latest checkpoint for this replica and C is the 2f matching certificate for this checkpoint. P is the collection of prepare-certificate (2f+1 matching prepare requests) for each pre-prepare request that higher than \alpha. i is the identification of the sender.

If the new primary p in new view \nu +1 has received 2f-1 valid view-change messages from other group members. It multicasts [\nu +1,GP,\gamma,O]_{new-view}, where the \gamma is a set of 2f matching viewChange-certificates, and O is the set of pre-prepare messages than need to be multicasted for they are failed to reach consensus in last view \nu. The sequence of pre-prepare in O is ranging from the latest checkpoint know to the new leader and the latest \alpha in P. GP is the update of itself. describes how p allocate its group member in layer L-1 to the rest members in group K. Member replicas accept and execute a valid new view. Those mentioned above are similar to original view-change protocol.

However, taking over the leader of group K in layer L implies that this replica becomes the member of a group in layer L-1 (for instance, group J). Thus, the primary p multicasts [join,\nu,\gamma,i]_{join} to group J after the view-change. The \nu is view number for group J since it should remain unchanged if there is no view change in J. p extracts view in J from the commit-certificate that passed by the original group leader from group K. Matching \nu and \gamma prove the validity. Then this replica directly commits the current operation since it must have been committed in layer L-1, otherwise the consensus protocol will not be executed in layer L. This join message informs the member in group J that there is a change of replica and group J update local GP.

Also, a member in layer L is the leader in layer L+1. To reallocate its former members (since p is no longer their leader), it multicasts [v+1,G,\gamma,i,r]_{redistribute} to replica r where G is the new designated group in layer L-1 for replica r. The replica r then redirects itself to group G and update local GP. The replicas governed by group K in layer deeper than L-1 are reaching consensus in seam sequence since they are (indirectly) lead by group K. The effect of view change is limited.

To reduce the extra communication complexity and latency introduced by this modified protocol, the redistribute messages are embedded into GP in new-view message. Then it is extracted by current members of group K and pass to their former group in layer L+1 by piggybacking in pre-prepare. Those replicas in layer L+1 redirect themselves before responding to new pre-prepare. Thus, only the join message attributes to extra communication expenditure which, in the double-layer case, the size m of layer 1.

To facilitate the assessment of view change complexity of our double-layer scenario, suppose that the number of replicas in the first layer is m, and for each group in the second layer is n (as depicted in Fig. 1). Note that the view change can be triggered from either the first layer or the second layer. First, we consider the case that the view change is triggered in the first layer. As described in the protocol, the first part of our view change process runs within this layer (which is similar to original PBFT). For this part, the complexity is O(m^3). As one of the members in the first layer is selected as the new primary, the extra complexity is introduced by multicasting the redistribute messages to its group in the second layer. Since the certificate is generated in the first layer, the complexity of this part is O(m^2n). The total complexity is O(m^3+m^2n).

Then we consider that a view change is triggered in one of the groups in the second layer. Similarly, the complexity is O(n^3) for the first part. The extra complexity is introduced by multicasting the join messages to the first layer. However, the certificate is now generated in one of the groups in the second layer. For this reason, the additional complexity is O(n^2m). Then the total complexity is O(n^3+n^2m).

For comparison, the complexities are O((m+mn)^3) for the original PBFT and O(m+mn) for HotStuff when considering the same total number of replicas. It can be seen that in both first layer or second layer triggered cases, the complexity of proposed double layer PBFT is not as good as HotStuff, however, it reduces the complexity in large scale when compared with original PBFT.

6.3 Operation Synchronization
The precondition for entering the next phase does not require synchronization across all replicas. Due to the loss of connection or other reasons, one replica may skip some operations. Though the consensus is still reached, the sequence number \alpha is not necessarily continuous.

But for clients who also access data from the network, operations are preferably synced on each replica. The asynchronous replica can be detected by the discrete sequence number and the reached watermark. A replica may extract the missing operations by inquiring them from other replicas in real-time or at a constant interval.

Once the replica decides to extract a missing operation from the rest of the network, it multicasts [n\alpha,i,\nu ]_{extract-request}. If there are no fewer than 2f+1 valid extract-reply, the replica accepts the reply as its missing operation. For replica who received extract-request, it replies [O,\alpha,i,\nu ]_{extract-reply} if the request is valid.

6.4 Garbage Collection
The data recorded on replicas increase its size as the protocol run. To discard unnecessary operations that have already reached a consensus, we implement Castro’s [11] garbage collection.

Replicas multicast checkpoint messages with the sequence number of latest committed operations. Sequence number with f+1 checkpoint (including its own) is seated as low watermark h. To prevent a replica who encounters with the transmission delay from going too far. A logic size L is set that the replica only executes operation between h and H=L+h.

6.5 Safety and Liveness
The group-wide operation is inconsistence with the original PBFT. The network is weakly synchronised that the time t for a messaged been received after sending does not go infinity. The byzantine replica is assumed unable to subvert cryptography. The safety and liveness are retained within/across groups with modified view change protocol. Suppose there are no more than \frac{1}{3} of the nodes in a group are faulty, and the network is weakly synchronised that the time t for a messaged been received after sending does not go infinity.

Since the protocol requires more than \frac{2}{3} replicas to communities before advanced into next operation (to ensure the number of responses from non-faulty replicas is always greater than that from faulty), there is at least one non-faulty replica overlap for two consecutive operations. All non-faulty replicas agree on each other, then they agree on total order of operations, providing safety. Also, the bound of faulty replicas indicates the protocol always collects sufficient responses to proceed for liveness. The consensus in upper layer is the precondition to invoke protocol in sub-groups. Thus, safety is guaranteed across groups. The modified view change protocol replaces faulty group leader. Thus, liveness is guaranteed across groups. This consistency with PBFT in safety and liveness within and across groups leads to consistency for the whole network.

SECTION 7Conclusion
A scalable multi-layer PBFT mechanism is proposed to reduce the communication complexity of the original single-layered PBFT. This paper proves that the communication complexity of the proposed double-layer PBFT system is significantly reduced to a minimum of C\approx 1.9Z^{\frac{4}{3}} at system’s maximum optimized capacity. To reach the minimum communication complexity, the optimal values of m and n are proposed in Section 3.3. Moreover, the analytical results of the security threshold show that the success rate sinks significantly when the proportion of faulty nodes exceeds \frac{1}{3} of the total. Also, the threshold which keeps success rate at 100 percent rises from \lfloor \frac{m}{3}\rfloor to \lfloor \frac{n}{3}\rfloor {\times }\lfloor \frac{m}{2}\rfloor in advanced model. These results show that the security performance of the double-layer system is largely determined by the first layer and is improved over operation time. Latency performance is also a trade-off. The confirmation delay increases with increasing network depth. Finally, we expand the double-layer system to the multi-layer. We have compared security performance and communication complexity between double-layer and multi-layer systems. Results show that communication complexity can be further lowered to a minimum of \frac{16Z-16}{3} if the network depth is maximized to X_{max} at the expense of certain security performance degradation.

This paper provides guidance for multi-layer PBFT system design and performance analysis, which would serve as a foundation for future research. Meanwhile, there are also limitations to be improved. For example, a system model to differentiate nodes in the first and second layer could be proposed as a trade-off between FND/FPD and the advanced model. Also, experimental evaluation of Proposition 7 would give a further insight into the performance tradeoffs to help with the system design in different application scenarios. Another potential topic is the deployment of multi-layer PBFT system. It is worth to mention that some scenarios such as in financial, are sensitive to both latency and scalability, thus more advanced research should be conducted to solve the issue.

