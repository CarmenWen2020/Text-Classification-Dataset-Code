Efficient application scheduling is critical for achieving high performance in heterogeneous computing environments. The application scheduling problem has been shown to be NP-complete in general cases as well as in several restricted cases. Because of its key importance, this problem has been extensively studied and various algorithms have been proposed in the literature which are mainly for systems with homogeneous processors. Although there are a few algorithms in the literature for heterogeneous processors, they usually require significantly high scheduling costs and they may not deliver good quality schedules with lower costs. In this paper, we present two novel scheduling algorithms for a bounded number of heterogeneous processors with an objective to simultaneously meet high performance and fast scheduling time, which are called the Heterogeneous Earliest-Finish-Time (HEFT) algorithm and the Critical-Path-on-a-Processor (CPOP) algorithm. The HEFT algorithm selects the task with the highest upward rank value at each step and assigns the selected task to the processor, which minimizes its earliest finish time with an insertion-based approach. On the other hand, the CPOP algorithm uses the summation of upward and downward rank values for prioritizing tasks. Another difference is in the processor selection phase, which schedules the critical tasks onto the processor that minimizes the total execution time of the critical tasks. In order to provide a robust and unbiased comparison with the related work, a parametric graph generator was designed to generate weighted directed acyclic graphs with various characteristics. The comparison study, based on both randomly generated graphs and the graphs of some real applications, shows that our scheduling algorithms significantly surpass previous approaches in terms of both quality and cost of schedules, which are mainly presented with schedule length ratio, speedup, frequency of best results, and average scheduling time metrics.
SECTION 1Introduction
Diverse sets of resources interconnected with a high-speed network provide a new computing platform, called the heterogeneous computing system, which can support executing computationally intensive parallel and distributed applications. A heterogeneous computing system requires compile-time and runtime support for executing applications. The efficient scheduling of the tasks of an application on the available resources is one of the key factors for achieving high performance.

The general task scheduling problem includes the problem of assigning the tasks of an application to suitable processors and the problem of ordering task executions on each resource. When the characteristics of an application which includes execution times of tasks, the data size of communication between tasks, and task dependencies are known a priori, it is represented with a static model.

In the general form of a static task scheduling problem, an application represented by a directed acyclic graph (DAG) in which nodes represent application tasks and edges represent intertask data dependencies. Each node label shows computation cost (expected computation time) of the task and each edge label shows intertask communication cost (expected communication time) between tasks. The objective function of this problem is to map tasks onto processors and order their executions so that task-precedence requirements are satisfied and a minimum overall completion time is obtained. The task scheduling problem is NP-complete in the general case [1], as well as some restricted cases [2], such as scheduling tasks with one or two time units to two processors and scheduling unit-time tasks to an arbitrary number of processors.

Because of its key importance on performance, the task scheduling problem in general has been extensively studied and various heuristics were proposed in the literature [3], [4], [5], [6], [7], [8], [9], [10], [11], [13], [12], [16], [17], [18], [20], [22], [23], [27], [30]. These heuristics are classified into a variety of categories (such as list-scheduling algorithms, clustering algorithms, duplication-based algorithm, guided random search methods) and they are mainly for systems with homogeneous processors.

In a list scheduling algorithm [3], [4], [6], [7], [18], [22], an ordered list of tasks is constructed by assigning priority for each task. Tasks are selected in the order of their priorities and each selected task is scheduled to a processor which minimizes a predefined cost function. The algorithms in this category provide good quality of schedules and their performance is comparable with the other categories at a lower scheduling time [21], [26]. The clustering algorithms [3], [12], [19], [25] are, in general for an unbounded number of processors, so they may not be directly applicable. A clustering algorithm requires a second phase (a scheduling module) to merge the task clusters generated by the algorithm onto a bounded number of processors and to order the task executions within each processor [24]. Similarly, task duplication-based heuristics are not practical because of their significantly high time complexity. As an example, the time complexity of the BTDH Algorithm [30] and the DSH Algorithm [18] are O(v4); the complexity of the CPFD Algorithm [9] is O(e×v2) for scheduling v tasks connected with e edges on a set of homogeneous processors.

Genetic Algorithms [5], [8], [11], [13], [17], [31] (GAs) are of the most widely studied guided random search techniques for the task scheduling problem. Although they provide good quality of schedules, their execution times are significantly higher than the other alternatives. It was shown that the improvement of the GA-based solution to the second best solution was not more than 10 percent and the GA-based approach required around a minute to produce a solution, while the other heuristics required an execution of a few seconds [31]. Additionally, extensive tests are required to find optimal values for the set of control parameters used in GA-based solutions.

The task scheduling problem has also been studied by a few research groups for the heterogeneous systems [6], [7], [8], [10], [11], [13], [14]. These algorithms may require assigning a set of control parameters and some of them confront with the substantially high scheduling costs [6], [8], [11], [13]. Some of them partition the tasks in a DAG into levels such that there will be no dependency between tasks in the same level [10], [14]. This level-by-level scheduling technique considers the tasks only in the current level (that is, a subset of ready tasks) at any time, which may not perform well because of not considering all ready tasks. Additionally, the study given in [14] presents a dynamic remapper that requires an initial schedule of a given DAG and then improves its performance using three variants of an algorithm, which is out of the scope of this paper.

In this paper, we propose two new static scheduling algorithms for a bounded number of fully connected heterogeneous processors: the Heterogeneous Earliest-Finish-Time (HEFT) algorithm and the Critical-Path-on-a-Processor (CPOP) algorithm. Although the static-scheduling for heterogeneous systems is offline, in order to provide a practical solution, the scheduling time (or running time) of an algorithm is the key constraint. Therefore, the motivation behind these algorithms is to deliver good-quality of schedules (or outputs with better scheduling lengths) with lower costs (i.e., lower scheduling times). The HEFT Algorithm selects the task with the highest upward rank (defined in Section 4.1) at each step. The selected task is then assigned to the processor which minimizes its earliest finish time with an insertion-based approach. The upward rank of a task is the length of the critical path (i.e., the longest path) from the task to an exit task, including the computation cost of the task. The CPOP algorithm selects the task with the highest (upward rank + downward rank) value at each step. The algorithm targets scheduling of all critical tasks (i.e., tasks on the critical path of the DAG) onto a single processor, which minimizes the total execution time of the critical tasks. If the selected task is noncritical, the processors selection phase is based on earliest execution time with insertion-based scheduling, as in the HEFT Algorithm.

As part of this research work, a parametric graph generator has been designed to generate weighted directed acyclic graphs for the performance study of the scheduling algorithms. The graph generator targets the generation of many types of DAGs using several input parameters that provide an unbiased comparison of task-scheduling algorithms. The comparison study in this paper is based on both randomly generated task graphs and the task graphs of real applications, including the Gaussian Elimination Algorithm [3], [28], FFT Algorithm [29], [30], and a molecular dynamic code given in [19]. The comparison study shows that our algorithms significantly surpass previous approaches in terms of both performance metrics (schedule length ratio, speedup, efficiency, and number of occurrences giving best results) and a cost metric (scheduling time to deliver an output schedule).

The remainder of this paper is organized as follows: In the next section, we define the research problem and the related terminology. In Section 3, we provide a taxonomy of task-scheduling algorithms and the related work in scheduling for heterogeneous systems. Section 4 introduces our scheduling algorithms (the HEFT and the CPOP Algorithms). Section 5 presents a comparison study of our algorithms with the related work, which is based on randomly generated task graphs and task graphs of several real applications. In Section 6, we introduce several extensions to the HEFT algorithm. The summary of the research presented and planned future work is given in Section 7.

SECTION 2Task-Scheduling Problem
A scheduling system model consists of an application, a target computing environment, and a performance criteria for scheduling. An application is represented by a directed acyclic graph, G=(V,E), where V is the set of v tasks and E is the set of e edges between the tasks. (Task and node terms are interchangeably used in the paper.) Each edge (i,j)∈E represents the precedence constraint such that task ni should complete its execution before task nj starts. Data is a v×v matrix of communication data, where datai,k is the amount of data required to be transmitted from task ni to task nk.

In a given task graph, a task without any parent is called an entry task and a task without any child is called an exit task. Some of the task scheduling algorithms may require single-entry and single-exit task graphs. If there is more than one exit (entry) task, they are connected to a zero-cost pseudo exit (entry) task with zero-cost edges, which does not affect the schedule.

We assume that the target computing environment consists of a set Q of q heterogeneous processors connected in a fully connected topology in which all interprocessor communications are assumed to perform without contention. In our model, it is also assumed that computation can be overlapped with communication. Additionally, task executions of a given application are assumed to be nonpreemptive. W is a v×q computation cost matrix in which each wi,j gives the estimated execution time to complete task ni on processor pj. Before scheduling, the tasks are labeled with the average execution costs. The average execution cost of a task ni is defined as
wi¯¯¯¯¯=∑j=1qwi,j/q.(1)
View Source

The data transfer rates between processors are stored in matrix B of size q×q. The communication startup costs of processors are given in a q-dimensional vector L. The communication cost of the edge (i,k), which is for transferring data from task ni (scheduled on pm) to task nk (scheduled on pn), is defined by
ci,k=Lm+datai,kBm,n.(2)
View Source

When both ni and nk are scheduled on the same processor, ci,k becomes zero since we assume that the intraprocessor communication cost is negligible when it is compared with the interprocessor communication cost. Before scheduling, average communication costs are used to label the edges. The average communication cost of an edge (i,k) is defined by
ci,k¯¯¯¯¯¯¯=L¯¯¯¯+datai,kB¯¯¯¯,(3)
View Sourcewhere B¯¯¯¯ is the average transfer rate among the processors in the domain and L¯¯¯¯ is the average communication startup time.

Before presenting the objective function, it is necessary to define the EST and EFT attributes, which are derived from a given partial schedule. EST(ni,pj) and EFT(ni,pj) are the earliest execution start time and the earliest execution finish time of task ni on processor pj, respectively. For the entry task nentry,
EST(nentry,pj)=0.(4)
View Source

For the other tasks in the graph, the EFT and EST values are computed recursively, starting from the entry task, as shown in (5) and (6), respectively. In order to compute the EFT of a task ni, all immediate predecessor tasks of ni must have been scheduled.
EST(ni,pj)=max{avail[j],maxnm∈pred(ni)(AFT(nm)+cm,i)},(5)
View Source
EFT(ni,pj)=wi,j+EST(ni,pj),(6)
View SourceRight-click on figure for MathML and additional features.where pred(ni) is the set of immediate predecessor tasks of task ni and avail[j] is the earliest time at which processor pj is ready for task execution. If nk is the last assigned task on processor pj, then avail[j] is the time that processor pj completed the execution of the task nk and it is ready to execute another task when we have a noninsertion-based scheduling policy. The inner max block in the EST equation returns the ready time, i.e., the time when all data needed by ni has arrived at processor pj.

After a task nm is scheduled on a processor pj, the earliest start time and the earliest finish time of nm on processor pj is equal to the actual start time, AST(nm), and the actual finish time, AFT(nm), of task nm, respectively. After all tasks in a graph are scheduled, the schedule length (i.e., overall completion time) will be the actual finish time of the exit task nexit. If there are multiple exit tasks and the convention of inserting a pseudo exit task is not applied, the schedule length (which is also called makespan) is defined as
makespan=max{AFT(nexit)}.(7)
View Source

The objective function of the task-scheduling problem is to determine the assignment of tasks of a given application to processors such that its schedule length is minimized.

SECTION 3Related Work
Static task-scheduling algorithms can be classified into two main groups (see Fig. 1), heuristic-based and guided random-search-based algorithms. The former can be further classified into three groups: list scheduling heuristics, clustering heuristics, and task duplication heuristics.


Fig. 1. Classification of static task-scheduling algorithms.
Show All

List Scheduling Heuristics. A list-scheduling heuristic maintains a list of all tasks of a given graph according to their priorities. It has two phases: the task prioritizing (or task selection) phase for selecting the highest-priority ready task and the processor selection phase for selecting a suitable processor that minimizes a predefined cost function (which can be the execution start time). Some of the examples are the Modified Critical Path (MCP) [3], Dynamic Level Scheduling [6], Mapping Heuristic (MH) [7], Insertion-Scheduling Heuristic [18], Earliest Time First (ETF) [22], and Dynamic Critical Path (DCP) [4] algorithms. Most of the list-scheduling algorithms are for a bounded number of fully connected homogeneous processors. List-scheduling heuristics are generally more practical and provide better performance results at a lower scheduling time than the other groups.

Clustering Heuristics. An algorithm in this group maps the tasks in a given graph to an unlimited number of clusters. At each step, the selected tasks for clustering can be any task, not necessarily a ready task. Each iteration refines the previous clustering by merging some clusters. If two tasks are assigned to the same cluster, they will be executed on the same processor. A clustering heuristic requires additional steps to generate a final schedule: a cluster merging step for merging the clusters so that the remaining number of clusters equal the number of processors, a cluster mapping step for mapping the clusters on the available processors, and a task ordering step for ordering the mapped tasks within each processor [24]. Some examples in this group are the Dominant Sequence Clustering (DSC) [12], Linear Clustering Method [19], Mobility Directed [3], and Clustering and Scheduling System (CASS) [25].

Task Duplication Heuristics. The idea behind duplication-based scheduling algorithms is to schedule a task graph by mapping some of its tasks redundantly, which reduces the interprocess communication overhead [9], [18], [27], [30]. Duplication-based algorithms differ according to the selection strategy of the tasks for duplication. The algorithms in this group are usually for an unbounded number of identical processors and they have much higher complexity values than the algorithms in the other groups.

Guided Random Search Techniques. Guided random search techniques (or randomized search techniques) use random choice to guide themselves through the problem space, which is not the same as performing merely random walks as in the random search methods. These techniques combine the knowledge gained from previous search results with some randomizing features to generate new results. Genetic algorithms (GAs) [5], [8], [11], [13], [17] are the most popular and widely used techniques for several flavors of the task scheduling problem. GAs generate good quality of output schedules; however, their scheduling times are usually much higher than the heuristic-based techniques [31]. Additionally, several control parameters in a genetic algorithm should be determined appropriately. The optimal set of control parameters used for scheduling a task graph may not give the best results for another task graph. In addition to GAs, simulated annealing [11], [15] and local search method [16], [20] are the other methods in this group.

3.1 Task-Scheduling Heuristics for Heterogeneous Environments
This section presents the reported task-scheduling heuristics that support heterogeneous processors, which are the Dynamic Level Scheduling Algorithm [6], the Levelized-Min Time Algorithm [10], and the Mapping Heuristic Algorithm [7].

Dynamic-Level Scheduling (DLS) Algorithm. At each step, the algorithm selects the (ready node, available processor) pair that maximizes the value of the dynamic level which is equal to DL(ni,pj)=ranksu(ni)−EST(ni,pj). The computation cost of a task is the median value of the computation costs of the task on the processors. In this algorithm, upward rank calculation does not consider the communication costs. For heterogeneous environments, a new term added for the difference between the task’s median execution time on all processors and its execution time on the current processor. The general DLS algorithm has an O(v3×q) time complexity, where v is the number of tasks and q is the number of processors.

Mapping Heuristic (MH). In this algorithm, the computation cost of a task on a processor is computed by the number of instructions to be executed in the task divided by the speed of the processor. However, in setting the computation costs of tasks and the communication costs of edges before scheduling, similar processing elements (i.e., homogeneous processors) are assumed; the heterogeneity comes into the picture during the scheduling process.

This algorithm uses static upward ranks to assign priorities. (The authors also experimented by adding the communication delay to the rank values.) In this algorithm, the ready time of a processor for a task is the time when the processor has finished its last assigned task and is ready to execute a new one. The MH algorithm does not schedule a task to an idle time slot that is between two tasks already scheduled. The time complexity, when contention is considered, is equal to O(v2×q3) for v tasks and q processors; otherwise, it is equal to O(v2×q).

Levelized-Min Time (LMT) Algorithm. It is a two-phase algorithm. The first phase groups the tasks that can be executed in parallel using the level attribute. The second phase assigns each task to the fastest available processor. A task in a lower level has higher priority than a task in a higher level. Within the same level, the task with the highest computation cost has the highest priority. Each task is assigned to a processor that minimizes the sum of the task’s computation cost and the total communication costs with tasks in the previous levels. For a fully connected graph, the time complexity is O(v2×q2) when there are v tasks and q processors.

SECTION 4Task-Scheduling Algorithms
Before introducing the details of HEFT and CPOP algorithms, we introduce the graph attributes used for setting the task priorities.

4.1 Graph Attributes Used by HEFT and CPOP Algorithms
Tasks are ordered in our algorithms by their scheduling priorities that are based on upward and downward ranking. The upward rank of a task ni is recursively defined by
ranku(ni)=wi¯¯¯¯¯+maxnj∈succ(ni)(ci,j¯¯¯¯¯¯+ranku(nj)),(8)
View Sourcewhere succ(ni) is the set of immediate successors of task ni, ci,j¯¯¯¯¯¯ is the average communication cost of edge (i,j), and wi¯¯¯¯¯ is the average computation cost of task ni. Since the rank is computed recursively by traversing the task graph upward, starting from the exit task, it is called upward rank. For the exit task nexit, the upward rank value is equal to
ranku(nexit)=wexit¯¯¯¯¯¯¯¯¯¯.(9)
View SourceRight-click on figure for MathML and additional features.

Basically, ranku(ni) is the length of the critical path from task ni to the exit task, including the computation cost of task ni. There are algorithms in the literature which compute the rank value using computation costs only, which is called static upward rank, ranksu.

Similarly, the downward rank of a task ni is recursively defined by
rankd(ni)=maxnj∈pred(ni){rankd(nj)+wj¯¯¯¯¯¯+cj,i¯¯¯¯¯¯},(10)
View SourceRight-click on figure for MathML and additional features.where pred(ni) is the set of immediate predecessors of task ni. The downward ranks are computed recursively by traversing the task graph downward starting from the entry task of the graph. For the entry task nentry, the downward rank value is equal to zero. Basically, rankd(ni) is the longest distance from the entry task to task ni, excluding the computation cost of the task itself.

4.2 The Heterogeneous-Earliest-Finish-Time (HEFT) Algorithm
The HEFT algorithm (Fig. 2) is an application scheduling algorithm for a bounded number of heterogeneous processors, which has two major phases: a task prioritizing phase for computing the priorities of all tasks and a processor selection phase for selecting the tasks in the order of their priorities and scheduling each selected task on its “best” processor, which minimizes the task’s finish time.


Fig. 2. The HEFT algorithm.
Show All

Task Prioritizing Phase. This phase requires the priority of each task to be set with the upward rank value, ranku, which is based on mean computation and mean communication costs. The task list is generated by sorting the tasks by decreasing order of ranku. Tie-breaking is done randomly. There can be alternative policies for tie-breaking, such as selecting the task whose immediate successor task(s) has higher upward ranks. Since these alternate policies increase the time complexity, we prefer a random selection strategy. It can be easily shown that the decreasing order of ranku values provides a topological order of tasks, which is a linear order that preserve the precedence constraints.

Processor Selection Phase. For most of the task scheduling algorithms, the earliest available time of a processor pj for a task execution is the time when pj completes the execution of its last assigned task. However, the HEFT algorithm has an insertion-based policy which considers the possible insertion of a task in an earliest idle time slot between two already-scheduled tasks on a processor. The length of an idle time-slot, i.e., the difference between execution start time and finish time of two tasks that were consecutively scheduled on the same processor, should be at least capable of computation cost of the task to be scheduled. Additionally, scheduling on this idle time slot should preserve precedence constraints.

In the HEFT Algorithm, the search of an appropriate idle time slot of a task ni on a processor pj starts at the time equal to the ready_time of ni on pj, i.e., the time when all input data of ni that were sent by ni’s immediate predecessor tasks have arrived at processor pj. The search continues until finding the first idle time slot that is capable of holding the computation cost of task ni. The HEFT algorithm has an O(e×q) time complexity for e edges and q processors. For a dense graph when the number of edges is proportional to O(v2) (v is the number of tasks), the time complexity is on the order of O(v2×p).

As an illustration, Fig. 4a presents the schedules obtained by the HEFT algorithm for the sample DAG of Fig. 3. The schedule length, which is equal to 80, is shorter than the schedule lengths of the related work; specifically, the schedule lengths of DLS, MH, and LMT Algorithms are 91, 91, and 95, respectively. The first column in Table 1 gives upward rank values for the given task graph. The scheduling order of the tasks with respect to the HEFT Algorithm is {n1,n3,n4,n2,n5,n6,n9,n7,n8,n10}.


Fig. 3. A sample task graph with 10 tasks.
Show All

TABLE 1 Values of Attributes Used in HEFT and CPOP Algorithmsfor Task Graph in Fig. 3
Table 1- 
Values of Attributes Used in HEFT and CPOP Algorithmsfor Task Graph in Fig. 3
Fig. 4. - Scheduling of task graph in Fig. 3 with the HEFT and CPOP algorithms. (a) HEFT Algorithm (schedule length = 80). (b) CPOP Algorithm (schedule length = 86).
Fig. 4. Scheduling of task graph in Fig. 3 with the HEFT and CPOP algorithms. (a) HEFT Algorithm (schedule length = 80). (b) CPOP Algorithm (schedule length = 86).
Show All

4.3 The Critical-Path-on-a-Processor (CPOP) Algorithm
Although our second algorithm, the CPOP algorithm shown in Fig. 5, has the task prioritizing and processor selection phases as in the HEFT algorithm, it uses a different attribute for setting the task priorities and a different strategy for determining the “best” processor for each selected task.

Task Prioritizing Phase. In this phase, upward rank (ranku) and downward rank (rankd) values for all tasks are computed using mean computation and mean communication costs (Steps 1-3). The CPOP algorithm uses the critical path of a given application graph. The length of this path, |CP|, is the sum of the computation costs of the tasks on the path and intertask communication costs along the path. The sum of computation costs on the critical path of a graph is basically the lower bound for the schedule lengths generated by the task scheduling algorithms.

The priority of each task is assigned with the summation of upward and downward ranks. The critical path length is equal to the entry task’s priority (Step 5). Initially, the entry task is the selected task and marked as a critical path task. An immediate successor (of the selected task) that has the highest priority value is selected and it is marked as a critical path task. This process is repeated until the exit node is reached (Steps 6-12). For tie-breaking, the first immediate successor which has the highest priority is selected.

We maintain a priority queue (with the key of ranku+rankd) to contain all ready tasks at any given instant. A binary heap was used to implement the priority queue, which has time complexity of O(logv) for insertion and deletion of a task and O(1) for retrieving the task with the highest priority. At each step, the task with the highest ranku+rankd value is selected from the priority queue.

Processor Selection Phase. The critical-path processor, pCP, is the one that minimizes the cumulative computation costs of the tasks on the critical path (Step 13). If the selected task is on the critical path, then it is scheduled on the critical-path processor; otherwise, it is assigned to a processor which minimizes the earliest execution finish time of the task. Both cases consider an insertion-based scheduling policy. The time-complexity of the CPOP algorithm is equal to O(e×p). Fig. 4b shows the schedule obtained by the CPOP algorithm for Fig. 3, which has a schedule length of 86. Based on the values in Table 1, the critical path in Fig. 3 is {n1,n2,n9,n10}. If all critical path tasks are scheduled on P1, P2, or P3, the path length will be 66, 54, or 63, respectively. P2 is selected as the critical path processor. The scheduling order of the tasks with respect to CPOP algorithm is {n1,n2,n3,n7,n4,n5,n9,n6,n8,n10}.


Fig. 5. The CPOP algorithm.
Show All

SECTION 5Experimental Results and Discussion
In this section, we present the comparative evaluation of our algorithms and the related work given in Section 3.1. For this purpose, we consider two sets of graphs as the workload for testing the algorithms: randomly generated application graphs and the graphs that represent some of the numerical real world problems. First, we present the metrics used for performance evaluation, which is followed by two sections on experimental results.

5.1 Comparison Metrics
The comparisons of the algorithms are based on the following four metrics:

Schedule Length Ratio (SLR). The main performance measure of a scheduling algorithm on a graph is the schedule length (makespan) of its output schedule. Since a large set of task graphs with different properties is used, it is necessary to normalize the schedule length to a lower bound, which is called the Schedule Length Ratio (SLR). The SLR value of an algorithm on a graph is defined by
SLR=makespan∑ni∈CPMINminpj∈Q{wi,j}.(11)
View SourceThe denominator is the summation of the minimum computation costs of tasks on the CPMIN. (For an unscheduled DAG, if the computation cost of each node ni is set with the minimum value, then the critical path will be based on minimum computation costs, which is represented as CPMIN.) The SLR of a graph (using any algorithm) cannot be less than one since the denominator is the lower bound. The task-scheduling algorithm that gives the lowest SLR of a graph is the best algorithm with respect to performance. Average SLR values over several task graphs are used in our experiments.

Speedup. The speedup value for a given graph is computed by dividing the sequential execution time (i.e., cumulative computation costs of the tasks in the graph) by the parallel execution time (i.e., the makespan of the output schedule). The sequential execution time is computed by assigning all tasks to a single processor that minimizes the cumulative of the computation costs.
Speedup=minpj∈Q{∑ni∈Vwi,j}makespan.(12)
View SourceRight-click on figure for MathML and additional features.If the sum of the computation costs is maximized, it results in a higher speedup, but ends up with the same ranking of the scheduling algorithms. Efficiency, the ratio of the speedup value to the number of processors used, is another comparison metric used for application graphs of real world problems given in Section 5.3.

Number of Occurrences of Better Quality of Schedules. The number of times that each algorithm produced better, worse, and equal quality of schedules compared to every other algorithm is counted in the experiments.

Running Time of the Algorithms. The running time (or the scheduling time) of an algorithm is its execution time for obtaining the output schedule of a given task graph. This metric basically gives the average cost of each algorithm. Among the algorithms that give comparable SLR values, the one with the minimum running time is the most practical implementation. The minimization of SLR by checking all possible task-processor pairs can conflict with the minimization in the running time.

5.2 Randomly Generated Application Graphs
In our study, we first considered the randomly generated application graphs. A random graph generator was implemented to generate weighted application DAGs with various characteristics that depend on several input parameters given below. Our simulation-based framework allows assigning sets of values to the parameters used by random graph generator. This framework first executes the random graph generator program to construct the application DAGs, which is followed by the execution of the scheduling algorithms to generate output schedules, and, finally, it computes the performance metrics based on the schedules.

5.2.1 Random Graph Generator
Our random graph generator requires the following input parameters to build weighted DAGs.

Number of tasks in the graph, (v).

Shape parameter of the graph, (α). We assume that the height (depth) of a DAG is randomly generated from a uniform distribution with a mean value equal to v√α. (The height is equal to the smallest integral value not less than the real value generated randomly.) The width for each level is randomly selected from a uniform distribution with mean equal to α×v√. A dense graph (a shorter graph with high parallelism) can be generated by selecting α>>1.0; if α<<1.0, it will generate a longer graph with a low parallelism degree.

Out degree of a node, (out_degree).

Communication to computation ratio, (CCR). It is the ratio of the average communication cost to the average computation cost. If a DAG’s CCR value is very low, it can be considered as a computation-intensive application.

Range percentage of computation costs on processors, (β). It is basically the heterogeneity factor for processor speeds. A high percentage value causes a significant difference in a task’s computation cost among the processors and a low percentage indicates that the expected execution time of a task is almost equal on any given processor in the system. The average computation cost of each task ni in the graph, i.e., wi¯¯¯¯¯, is selected randomly from a uniform distribution with range [02×wDAG¯¯¯¯¯¯¯¯¯¯¯¯], where wDAG¯¯¯¯¯¯¯¯¯¯¯¯ is the average computation cost of the given graph, which is set randomly in the algorithm. Then, the computation cost of each task ni on each processor pj in the system is randomly set from the following range:
wi¯¯¯¯¯×(1−β2)≤wi,j≤wi¯¯¯¯¯×(1+β2).(13)
View Source

In each experiment, the values of these parameters are assigned from the corresponding sets given below. A parameter should be assigned by all values given in its set in a single experiment and, in case of any change on these values, it is written explicitly in the paper. Note that the last value in the out_degree set is the number of nodes in the graph which generate fully connected graphs for the experiments.

SETV={20,40,60,80,100},

SETCCR={0.1,0.5,1.0,5.0,10.0},

SETα={0.5,1.0,2.0},

SETout_degree={1,2,3,4,5,v},

SETβ={0.1,0.25,0.5,0.75,1.0}.

These combinations give 2, 250 different DAG types. Since 25 random DAGs were generated for each DAG type, the total number of DAGs used in our experiments was around 56K. Assigning several input parameters and selecting each parameter from a large set cause the generation of diverse DAGs with various characteristics. Experiments based on diverse DAGs prevent biasing toward a particular scheduling algorithm.

5.2.2 Performance Results
The performance of the algorithms were compared with respect to various graph characteristics. The first set of experiments compares the performance and cost of the algorithms with respect to various graph sizes (see Figs. 6 and 7). The SLR-based performance ranking of the algorithms is {HEFT, CPOP, DLS, MH, LMT}. (It should be noted that each ranking in this paper starts with the best algorithm and ends with the worst one with respect to the given comparison metric.) The average SLR value of HEFT on all generated graphs is better than the CPOP algorithm by 7 percent, the DLS algorithm by 8 percent, the MH algorithm by 16 percent, and the LMT algorithm by 52 percent. The average speedup ranking of the algorithms is {HEFT, DLS, (CPOP=MH), LMT} (Fig. 6b).


Fig. 6. (a) Average SLR and (b) average speedup with respect to graph size.
Show All

Based on these experiments, the HEFT algorithm outperforms the other algorithms for any graph size in terms of SLR and speedup. The CPOP algorithm outperforms the related work in terms of average SLR; for various graph sizes, it cannot give higher speedup values than the DLS algorithm. With respect to average running times (see Fig. 7), The HEFT algorithm is the fastest and the DLS algorithm is the slowest one. On average, the HEFT algorithm is faster than the CPOP algorithm by 10 percent, the MH algorithm by 32 percent, the DLS algorithm by 84 percent, and the LMT algorithm by 48 percent.


Fig. 7. Average running time of algorithms with respect to graph size.
Show All

The next experiment is with respect to the graph structure. When α (the shape parameter of the graph) is equal to 0.5, i.e., the generated graphs have greater depths with a low degree of parallelism, it is shown that the performance of the HEFT algorithm is better than that of the CPOP algorithm by 8 percent, the MH algorithm by 12 percent, the DLS algorithm by 16 percent, and the LMT algorithm by 40 percent. When α is equal to 1.0, the average SLR value of the HEFT algorithm is better than that of the CPOP algorithm by 7 percent, the MH algorithm by 14 percent, the DLS algorithm by 7 percent, and the LMT algorithm by 34 percent. When α is equal to 2.0, the HEFT algorithm is better than the CPOP algorithm by 6 percent, the MH algorithm by 15 percent, the DLS algorithm by 8 percent, and the LMT algorithm by 31 percent. For all three different graph structures, the HEFT algorithm gives the best performance.

Quality of schedules generated by the algorithms with respect to various CCR values was compared in another experiment. The performance ranking of the algorithms when CCR≤1.0 is {HEFT, DLS, MH, CPOP, LMT}. When CCR>1.0, the performance ranking changes to {HEFT, CPOP, DLS, MH, LMT}. The CPOP algorithm gives better results for graphs with higher CCRs than the graphs with lower CCRs. Clustering of the critical path on the fastest processor results in better quality of schedules for the graphs in which average communication cost is greater than average computation cost.

Finally, the number of times that each scheduling algorithm in the experiments produced better, worse, or equal schedule length compared to every other algorithm was counted for the 56250 DAGs used. Each cell in Table 2 indicates the comparison results of the algorithm on the left with the algorithm on the top. The “combined” column shows the percentage of graphs in which the algorithm on the left gives a better, equal, or worse performance than all other algorithms combined. The ranking of the algorithms, based on occurrences of best results, is {HEFT, DLS, CPOP, MH, LMT}. However, the ranking with respect to average SLR values was: {HEFT, CPOP, DLS, MH, LMT}. Although the DLS algorithm outperforms the CPOP algorithm in terms of the number of occurrences of best results, the CPOP algorithm has shown slightly better average SLR value than the DLS algorithm.

TABLE 2 Pair-Wise Comparison of the Scheduling Algorithms

5.3 Application Graphs of Real World Problems
In addition to randomly generated task graphs, we also considered application graphs of three real world problems: Gauss elimination algorithm [3], [28], Fast Fourier Transformation [29], [30], and a molecular dynamics code given in [19].

5.3.1 Gaussian Elimination
Fig. 8a gives the sequential program for the Gaussian elimination algorithm [3], [28]. The data-flow graph of the algorithm for the special case of m=5, where m is the dimension of the matrix, is given in Fig. 8b. Each Tk,k represents a pivot column operation and each Tk,j represents an update operation. In Fig. 8b, the critical path is T1,1T1,2T2,2T2,3T3,3T3,4T4,4T4,5, which is the path with the maximum number of tasks.


Fig. 8. (a) Gaussian elimination algorithm, (b) task graph for matrix of size 5.
Show All


Fig. 9. (a) Average SLR and (b) efficiency comparison for the Gaussian elimination graph.
Show All

For the experiments of Gauss elimination application, the same CCR and range percentage values (given in Section 5.2) were used. Since the structure of the application graph is known, we do not need the other parameters, such as the number of tasks, out_degree, and shape parameters. A new parameter, matrix size (m), is used in place of v (the number of tasks in the graph). The total number of tasks in a Gaussian elimination graph is equal to m2+m−22.

Fig. 9a gives the average SLR values of the algorithms at various matrix sizes from 5 to 20, with an increment of one, when the number of processors is equal to five. The smallest size graph in this experiment has 14 tasks and the largest one has 209 tasks. The performances of the HEFT and DLS algorithms are the best of all. Increasing the matrix size causes more tasks not to be on the critical path, which results in an increase in the makespan for each algorithm.

For the efficiency comparison, the number of processors used in our experiments is varied from 2 to 16, incrementing by the power of 2; the CCR and range percentage parameters have the same set of values. Fig. 9b gives efficiency comparison for Gaussian elimination graphs when the matrix size is 50. The HEFT and DLS algorithms have better efficiency than the other algorithms. When the number of processors is increased beyond eight, the HEFT algorithm outperforms the DLS algorithm in terms of efficiency. Since the matrix size is fixed, an increase in the number of processors decreases the makespan for each algorithm. As part of this experiment, we compared the running time of the algorithms with respect to the various numbers of processors (by keeping the matrix size fixed). The results indicate that the DLS algorithm is the slowest algorithm among them, although it performs as well as the HEFT algorithm. As an example, when the matrix size is 50 for 16 processors, the DLS algorithm takes 16.2 times longer than the HEFT algorithm to schedule a given graph. When the performance and cost results are considered together, the HEFT algorithm is the most efficient and practical algorithm among them.

5.3.2 Fast Fourier Transformation
The recursive, one-dimensional FFT Algorithm [29], [30] and its task graph (when there are four data points) is given in Fig. 10. In this figure, A is an array of size m which holds the coefficients of the polynomial and array Y is the output of the algorithm. The algorithm consists of two parts: recursive calls (lines 3-4) and the butterfly operation (lines 6-7). The task graph in Fig. 10b can be divided into two partsthe tasks above the dashed line are the recursive call tasks and the ones below the line are butterfly operation tasks. For an input vector of size m, there are 2×m−1 recursive call tasks and m×log2m butterfly operation tasks. (We assume that m=2k for some integer k). Each path from the start task to any of the exit tasks in an FFT task graph is a critical path since the computation costs of tasks in any level are equal and the communication costs of all edges between two consecutive levels are equal.


Fig. 10. (a) FFT algorithm, (b) the generated DAG of FFT with four points.
Show All

For the FFT-related experiments, only the CCR and range percentage parameters, among the parameters given in Section 5.2, were used, as in the Gauss elimination application. The number of data points in FFT is another parameter in our simulations, which varies from 2 to 32 incrementing by the power of 2. Fig. 11a shows the average SLR values for FFT graphs at various sizes of input points. One can observe that the HEFT algorithm outperforms the other algorithms in most of the cases. Fig. 11b presents the efficiency values obtained for each of the algorithms with respect to various numbers of processors with graphs of 64 data points. The number of processors used varied from two to 32, incrementing by the power of 2. The HEFT and DLS algorithms give the most efficient schedules in all cases.


Fig. 11. (a) Average SLR and (b) efficiency comparison for the FFT graph.
Show All

When the running times of the algorithms for scheduling FFT graphs are compared with respect to both the number of data points and the number of processors used (see Fig. 12), one can observe that the DLS algorithm is the highest cost algorithm. Note that the number of processors is equal to six in Fig. 12a and the number of input points is equal to 64 in Fig. 12b.


Fig. 12. Running times of scheduling algorithms for the FFT graph.
Show All

5.3.3 Molecular Dynamics Code
Fig. 13 is the task graph of a modified molecular dynamic code given in [19]. This application is part of our performance evaluation since it has an irregular task graph. Since the number of tasks is fixed in the application and the structure of the graph is known, only the values of CCR and range percentage parameters (in Section 5.2) are used in our experiments. Fig. 14a shows the performance of the algorithms with respect to five different CCR values when the number of processors is equal to six. On the average, the SLR ranking is {HEFT, DLS, CPOP, MH, LMT}. The efficiency comparison of the scheduling algorithms is given in Fig. 14b, in which the number of processors is varied from two to seven with an increment of 1. Since there are at most seven tasks in any level in Fig. 13, the number of processors in the experiments is bounded up to seven processors. It was also observed that the DLS and LMT algorithms take a running time almost three times longer than the other three algorithms (HEFT, CPOP, and MH). When these results are combined, the HEFT algorithm is the most practical and efficient algorithm for this application.


Fig. 13. The task graph of the molecular dynamics code [19].
Show All


Fig. 14. (a) Average SLR and (b) efficiency comparison for the task graph of a molecular dynamics code.
Show All

SECTION 6Alternate Policies for the Phases of the HEFT Algorithm
We proposed three substitute policies (shown as A1, A2, and A3) for the task prioritizing phase of the HEFT algorithm, which is based on upward rank in the experiments. In A1, the priority value is equal to the summation of the upward and downward ranks. In A2, the right part of the + sign gives the latest execution finish time of an immediate predecessor task of ni, which is already scheduled. A3 is similar to A2, except that it considers the communication cost. In the experiments, it has been observed that the original priority policy gives better results than these alternates by 6 percent.

A1: priority(ni)=ranku(ni)+rankd(ni),

A2: priority(ni)=ranku(ni)+maxnj∈pred(ni)AFT(nj),

A3:

priority(ni)=ranku(ni)+maxnj∈pred(ni){AFT(nj)+cj,i}.
View SourceRight-click on figure for MathML and additional features.
Another extension that may improve performance is to take immediate child tasks into account. As an example, the HEFT algorithm generates an output schedule of length 8 for the task graph given in Fig. 15; if task A and its immediate child (task B) are scheduled on the same processor, which minimizes the earliest finish time of task B, the schedule length decreases to 7. To consider this extension, we can modify the processor selection phase of the HEFT algorithm as follows: For each selected task, one of its immediate child tasks is marked as the critical-child based on one of the three policies given below. If the other immediate predecessors of the critical child are already scheduled, then the selected task and its critical child are scheduled on the same processor that minimizes the earliest finish time of the critical child; otherwise, the selected task is scheduled to the processor that minimizes its earliest finish time, as in the HEFT algorithm. The three critical child selection policies (B1, B2, and B3) use either communication cost or upward rank or both.

B1: critical_child(ni)=maxnc∈succ(ni)ci,c,

B2: critical_child(ni)=maxnc∈succ(ni)ranku(nc),

B3:
critical_child(ni)=maxnc∈succ(ni){ranku(nc)+ci,c}.
View SourceRight-click on figure for MathML and additional features.

The original HEFT algorithm outperforms these alternates for small CCR graphs. For high CCR graphs, some benefit has been observed by taking critical child tasks into account during processor selection. When 3.0≤CCR<6.0, B1 policy slightly outperforms the original HEFT algorithm. If CCR≥6.0, B2 policy outperforms the original algorithm and others alternates by 4 percent.

Fig. 15. - Scheduling of a task graph with the (a) HEFT algorithm and (b) an alternative method.
Fig. 15. Scheduling of a task graph with the (a) HEFT algorithm and (b) an alternative method.
Show All

SECTION 7Conclusions
In this paper, we presented two new algorithms, called the HEFT algorithm and the CPOP algorithm, for scheduling application graphs onto a system of heterogeneous processors. Based on the experimental study using a large set (56K) of randomly generated application graphs with various characteristics and application graphs of several real world problems (such as Gaussian elimination, FFT, and a molecular dynamics code), the HEFT algorithm significantly outperformed the other algorithms in terms of both performance and cost metrics, including average schedule length ratio, speedup, frequency of best results, and average running time. Because of its robust performance, low running time, and the ability to give stable performance over a wide range of graph structures, the HEFT algorithm is a viable solution for the DAG scheduling problem on heterogeneous systems. Based on our performance evaluation study, we also observed that the CPOP algorithm has given either better performance and better running time results than existing algorithms or comparable results with them.

Several alternative policies were studied for task prioritizing and processor selection phases of the HEFT algorithm. A new method was introduced for the processor selection phase which tries to minimize the earliest finish time of the critical-child task of each selected task.

One planned future research is to analytically investigate the trade-off between the quality of schedules of the algorithms, i.e., average makespan values, and the number of processors available. This extension may come up with some bounds on the degradation of makespan given that the number of processors available may not be sufficient. We plan to extend the HEFT Algorithm for rescheduling tasks in response to changes in processor and network loads. Although our algorithms assume a fully connected network, it is also planned to extent these algorithms for arbitrary-connected networks by considering the link contention.