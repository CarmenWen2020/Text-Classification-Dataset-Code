A novel paradigm named Wireless Powered Mobile Edge Computing (WP-MEC) emerges recently, which integrates Mobile Edge Computing (MEC) and Wireless Power Transfer (WPT) technologies. It enables mobile clients to both extend their computing capacities by task offloading, and charge from edge servers via energy transmission. Existing studies generally focus on the centralized design of task scheduling and energy charging in WP-MEC networks. To meet the decentralization requirement of the near-coming 6G network, we propose an online learning algorithm for computation offloading in WP-MEC networks with a distributed execution manner. Specifically, we first define the delay minimization problem by considering task deadline and energy constraints. Then, we transform it into a primal-dual optimization problem based on the Bellman equation. After that, we design a novel neural model that learns both offloading and time division decisions in each time slot to solve the formulated optimization problem. To train and execute the designed algorithm distributivity, we form multiple learning models decentralized on edge servers and they work coordinately to achieve parameter synchronization. At last, both theoretical and performance analyses show that the designed algorithm has significant advantages in comparison with other representative schemes.

SECTION 1Introduction
The development of the Sixth Generation of Mobile Communications (6G) and the Internet of Things (IoT) technology enables ultra-low latency and computation-intensive applications, such as virtual reality and three-dimensional videos, to become blooming. However, mobile clients are always limited on computing capabilities and battery lifetime due to the constraint of production costs and technologies [1]. Recently, Mobile Edge Computing (MEC) and Wireless Power Transfer (WPT) are regarded as two promising technologies to relieve the conflict between application requirements and limited terminal resources [2]. With MEC, the computing capabilities of mobile clients can be virtually enhanced by offloading computation-intensive tasks to edge servers. WPT allows mobile clients to charge their batteries over the air, where the Radio Frequency (RF) signals are transmitted from edge servers to mobile clients. After those signals are received, they can be converted into electricity for devices.

Currently, a new paradigm, named Wireless Powered Mobile Edge Computing (WP-MEC), is introduced by the integration of MEC and WPT technologies. Mobile clients in WP-MEC networks can not only offload their computation tasks to edge servers with the purpose of reducing task competition delay, but also charge from edge servers to sustain their energy supply [3]. Generally, due to the characteristic of half-duplex, offloading and charging processes for mobile clients can not be run simultaneously. Thus, an inevitable issue exists in WP-MEC networks: how to make reasonable time division and resource allocation for both task offloading and energy charging to maximize the utilities of mobile clients. Time division and resource allocation have mutual impacts on each other. On one hand, the time division can influence the amount of harvested energy of mobile clients, and task scheduling decisions are dependent on harvested energy. On the other hand, task scheduling results in the previous time slot inversely impact the time division of the subsequent time slot.

Existing studies always formulate the computation offloading issue in WP-MEC networks by Mixed Integer Programming (MIP). Optimization methods, including convex optimization [4] and approximation method [5], are utilized to solve such issues. However, most of those algorithms are executed in a centralized manner, and require full network information to make scheduling decisions. According to the report in [6], near 100% geographical wireless coverage will be provided by 6G, where distributed management is needed to meet use cases with millisecond information update rates. Followed that, distributed computation scheduling algorithms in WP-MEC networks are called for to meet the coming 6G network. However, it is difficult to construct such a distributed system due to the following challenges:

First, there are many studies investigating the scheduling algorithm for computation offloading with single edge server in WP-MEC networks. It has been revealed that even the optimization problem of computation offloading with single edge server in a centralized manner is NP-hard [4], not to mention multiple edge servers with distributed scheduling. When there is one edge server, it merely needs to determine its own energy transmission and task offloading periods based on the system status. When there are several edge servers and a set of mobile clients in the system, those servers not only need to take the requirements of devices into consideration, but also their energy transmission time should be unified. This is because concurrent energy pulses can increase received energy levels of mobile clients in a short period [7]. However, it is difficult to reach a consensus on energy transmission time for all edge servers in the distributed environment.

Second, to schedule dynamic offloading requirements of mobile clients in an online manner for minimizing the long-term task completion delay, it is promising to design a Deep Reinforcement Learning (DRL) based scheduling algorithm with the advantage of its strong learning ability and fast convergence speed. Nevertheless, edge servers acting as learning agents need to make resource scheduling decisions independently, since they do not know policies of other agents beforehand. Meanwhile, their different decisions may result in multiple offloading choices for some clients while none for others, heavily impacting users’ utilities. Thus, resource allocation policies of edge servers should be carefully designed.

To solve the above challenges, we propose an online learning algorithm for distributed computation offloading in wireless powered mobile edge computing networks, named OLD-COP, with the purpose of minimizing the long-term average task completion delay for mobile clients. To the best of our knowledge, this work is the first to provide a distributed design of computation offloading with multiple edge servers in WP-MEC networks so far. Our contributions can be summarized as follows:

We first formulate the distributed computation offloading issue as a delay minimization problem by considering task deadline and energy constraints. To make it solvable, we transform it into a primal-dual optimization problem based on Bellman equation. Specifically, states, actions, state transition and rewards are formally defined based on the formulated Markov Decision Process (MDP).

To solve the defined optimization problem, we design a novel neural model, including three learning components, i.e., policy, value and dual networks, which learns both time division and offloading decisions in each time slot. To train and execute the designed learning model distributively, we reform the whole learning model into multiple copies, which can be decentralized on edge servers and coordinated to achieve parameter synchronization.

Comprehensive theoretical analyses are conducted both from slot-based and long-term perspectives. We prove that the upper bound of the achieved system performance exists, and the algorithm convergence can be guaranteed with certain conditions even in the distributed environment.

Performance evaluation based on the city map of Manhattan is conducted to demonstrate the effectiveness of the designed algorithm in terms of average task completion delay, average task completion ratio and so on.

The rest of this paper is structured as follows: in Section 2, we review the related work; we present the system model and define the studied problem in Section 3; in Section 4, we formulate the task scheduling problem based on MDP; we design the online learning algorithm for distributed computation offloading in WP-MEC networks in Section 5, followed by performance evaluation in Section 6; finally, we conclude this work in Section 7.

SECTION 2Related Work
We review the state-of-the-art for computation offloading and energy harvesting in WP-MEC networks in this section. Currently, the studies on WP-MEC can be categorized into the following two kinds based on the number of involved edge servers.

The majority of researchers focus on the first category, i.e., computation scheduling with one edge server. The authors in [2] investigate binary offloading by jointly optimizing system time allocation and individual task offloading mode with the purpose of maximizing the sum computation rate of mobile clients. An online resource allocation algorithm based on sliding-window is proposed in [8], to minimize energy consumption in a system-wide area. The remote task execution, energy beamforming at the access point, task offloading and computing for mobile clients are jointly optimized. An online resource allocation strategy is proposed in [9], with the purpose of maximizing the fairness among different mobile users, where Lyapunov optimization technology is leveraged. The trade-off between response delay and energy efficiency in WP-MEC networks is investigated in [3]. Random task arrivals and channel conditions are considered, and a stochastic optimization problem is formulated. Similar with [9], Lyapunov optimization technology is also utilized.

With the purpose of maximizing the profit of the operator in multi-user WP-MEC networks, an iterative algorithm based on Lagrangian dual method is proposed in [10] by optimizing power allocation of mobile clients and data sizes of offloaded tasks. Both binary and partial computation offloading modes in WP-MEC networks are designed in [11], with the purpose of maximizing the computation efficiency. Alternative optimization and iterative algorithms are designed by jointly optimizing offloading power and time, computing frequency as well as energy harvesting time. However, the above studies are based on one edge server along with multiple mobile clients, and merely centralized scheduling policies are designed.

In addition, many studies leverage DRL technology in WP-MEC networks, such as [12], [13], [14]. Aiming to minimize the long-term system cost, an online resource management scheme based on DRL is proposed in [12]. Both task offloading and server provision can be learned by a designed post-decision state based algorithm. In [13], the unmanned aerial vehicle is utilized as the server to charge mobile clients and harvest data from them. An on-board deep Q-network is designed for the minimization of data packet loss of mobile clients, where charging and data collection decisions are learned to derive the optimal solution. A DRL-based online framework is proposed to learn binary offloading decisions according to time-varying wireless channel conditions in [14]. Although the above studies are efficient in the environment with one edge server and multiple mobile clients, few ones focus on distributed computation offloading with multiple edge servers and mobile clients.

The other category is scheduling with more than one edge server. There are a limited number of studies related to this topic due to its complexity. The authors in [15] consider computation offloading scheduling with multiple edge servers in WP-MEC networks, aiming to maximize the computation completion ratio. The formulated optimization problem is proved to be NP-hard, and a centralized approximation algorithm is designed to improve the system performance. A joint resource allocation and task offloading algorithm is proposed in [16], where a Mixed Interger Non-linear Problem (MINLP) is formulated to maximize task offloading gains of mobile clients. Although they focus on computation scheduling with multiple edge servers, their algorithms can not be conducted in a distributed manner.

To realize distributed computation offloading in WP-MEC networks with online scheduling, we design an algorithm based on DRL technology. A novel neural model is constructed, learning both offloading decisions and time division strategies in each time slot. Furthermore, the whole learning model can be reformed into single copies, which can be decentralized on edge servers and coordinated to achieve parameter synchronization. To the best of our knowledge, we are the first to investigate distributed resource scheduling in WP-MEC networks with the coexisting of multiple servers and mobile clients.

SECTION 3System Model and Problem Definition
In this section, we first present the system model. Then, we define the optimization problem. The main notations are listed in Table 1.

TABLE 1 Main Notations

3.1 System Model
The objective of this paper is to efficiently allocate network resources to minimize the long-term expected completion delay of tasks generated by mobile clients. As shown in Fig. 1, we consider a 2D area, where M edge servers, denoted by M={1,...,j,...,M}, and N mobile clients, represented by N={1,...,i,...,N} coexist. Each edge server is installed on one Base Station (BS), and BSs can communicate with mobile clients based on Orthogonal Frequency Division Multiple Access (OFDMA) technology. The subchannel matching algorithm in [17] can be applied in our system. Mobile clients randomly move around in the area, and can harvest energy through WPT from edge servers. They can also offload their tasks to edge servers for computation, and tasks are processed in sequence on each server. The time horizon is divided into a set of time slots with equal length T. For simplicity and without loss of generality, we consider that mobile client i generates a task in each time slot with probability ρi, i∈N. The task generated by mobile client i in time slot t can be represented by eti={ζti,cti,dt,maxi}, where ζti is the task size; symbol cti is the required CPU cycle for task computation, and can be computed by cti=ςζti, where ς represents the required CPU cycles of each bit. Symbol dt,maxi is the task deadline. We assume that the task is atomic, and can not be split into smaller ones. Thus, binary offloading decisions can be made.

Fig. 1. - 
The illustrative system model.
Fig. 1.
The illustrative system model.

Show All

Mobile client i is with CPU frequency fli, and edge server j is with fsj. The mobile client leverages half-duplex transmission, thus WPT and task offloading can not be simultaneously conducted [18]. Similar to [19], we design a harvest-then-offload algorithm for task scheduling. That is, mobile clients first harvest energy from servers, and then offload their tasks to remote servers. We utilize τt (τt<T) to represent the duration of the energy harvesting period, and T−τt denotes that of the task scheduling period in time slot t. In the following two subsections, we specify the detailed delay and energy computation models.

3.2 Task Completion Delay
For a task generated by a mobile client, it can either be offloaded to edge servers or processed locally. For the delay to complete the task, three components are included:

3.2.1 Transmission Delay
If task eti is offloaded to edge server j, the transmission delay from mobile client i to edge server j in time slot t can be computed by dt,lij=ζti/Υtij, where Υtij is the transmission rate between mobile client i and edge server j, and can be computed based on Shannon formula, i.e.,
Υtij=Blog2(1+Pijhtijσ2j).(1)
View SourceSymbol B is the channel bandwidth, and σ2j is the noise power of server j. Variable Pij is the transmission power from mobile client i to edge server j, and htij is the wireless channel gain. If task eti is processed locally, there is no transmission delay.

3.2.2 Computation Delay
If task eti is processed locally, the computation delay can be obtained by dt,oi=cti/fli. If task eti is offloaded to edge server j, the computation delay can be obtained by dt,oij=cti/fsj.

3.2.3 Waiting Delay
Since tasks are processed sequentially on edge servers, waiting delay can be caused even after energy harvesting. Herein, waiting delay refers to the duration for a task from the time it is generated until that it starts to be processed locally or by the edge server, i.e.,
dt,wi=τt′+Δt′+(t′−t)T,t′≥t,(2)
View Sourcewhere t′ represents the time slot in which task eti is processed. Variable Δt′ is the waiting delay in the processing queue of the targeted edge server in time slot t′ after energy harvesting.

We utilize binary value αti to denote whether task eti is processed locally. When it is processed on client i, variable αti=1, and vice versa. Binary value βtij denotes whether task eti is offloaded to server j in time slot t. Consequently, we can obtain the total completion delay for task eti by:
dti=αtidt,oi+(1−αti)∑j=1Mβtij(dt,lij+dt,oij)+dt,wi.(3)
View SourceSince the sizes of computation results are generally small, the delay for returning them back to mobile clients can be neglected [20].

3.3 Energy Harvesting and Consumption
In the scenario of energy harvesting with a set of transmitters and multiple receivers, all those energy transmitters on edge servers send energy pulse simultaneously to mobile clients [7]. These energy waves can be combined either destructively or constructively. In the destructive mode, the harvested energy from the combined energy wave is lower than that of the individual energy wave. Conversely, the harvested energy from the combined energy wave is higher than that of the individual energy wave in the constructively mode. Since transmission frequencies of edge servers are fixed in our system, edge servers can be regarded as a virtual transmitter similar to [5] and [21]. Then, all edge servers transmit RF power to mobile clients during the first τt duration of each time slot.

In this situation, the harvested energy by each mobile client is related to the transmission frequency of those transmitters and the distance between the transmitter and the receiver. Since our focus is to reasonably make time division and task offloading decisions, similar with [5] and [11], we adopt a linear energy harvesting model in our system for simplicity and without loss of generally. Thus, the harvested energy by client i in time slot t can be expressed by Et,hi=μPih^tiτt, where μ is a ratio between 0 and 1, referring to the energy conversion efficiency for mobile clients, and h^ti is the downlink channel gain from the virtual transmitter to client i. The received transmission power from the virtual transmitter by client i can be regarded by Pi.

After energy harvesting, the generated tasks of mobile clients can be scheduled, and the consumed energy for task scheduling includes the following two aspects:

3.3.1 Transmission Energy
The consumed energy for task eti transmitted from mobile client i to edge server j can be computed by Et,lij=Pijdt,lij.

3.3.2 Local Computation Energy
According to [15], if the task is processed locally on the client, its consumed energy for computation can be obtained by Et,oi=κi(fli)3dt,oi, where κi is the coefficient related to energy efficiency on mobile client i. As a result, total consumed energy Et,ci can be expressed by:
Et,ci=αtiEt,oi+(1−αti)Et,lij.(4)
View Source

3.4 Problem Definition
In time slot t, there are mainly two periods, i.e., one is for energy harvesting, and the other is for task scheduling and processing. In the period of energy harvesting, all edge servers transmit RF power to mobile clients, and all clients charge for energy. After that, the generated tasks are scheduled for computation. Mobile clients decide whether and where to offload their tasks based on the network condition. An offloading request generated by the mobile client can be broadcast to reachable edge servers. After receiving the offloading requests from mobile clients, edge servers learn to decide the scheduling sequences for those offloaded tasks and send the process sequences to mobile clients. Then, tasks can be scheduled by mobile clients based on the received results. To satisfy the users’ computation requirements, we define the considered problem as a delay-minimization problem, presented as follows:

Objective:

Our objective is to minimize the average task completion delay over all time slots, i.e.,
P1:minlimt→∞1Nt∑ν=0t∑i=1Ndνi,(5)
View Source
s.t.dti≤dt,maxi,(5a)
View Source
Et,ci≤Et,hi+Et−1,ri−Eti.((5.b))
View Source

Input:

Edge server set M, mobile client set N, task set {eti}i∈N, CPU frequency sets {fli}i∈N and {fsj}j∈M, channel bandwidth B, wireless channel gain {htij}i∈N,j∈M, noise power {σ2j}j∈M, transmission power {Pij}i∈N,j∈M, energy conversion efficiency μ, received power {Pi}i∈N, and energy efficiency coefficient κi.

Output:

1) Length τt of energy harvesting period in time slot t;

2) Offloading decisions αti and βtij for task eti, i∈N, j∈M and t∈{1,2,3....}.

Constraints:

For client j, its generated tasks should be scheduled and processed before deadline dt,maxi, i.e., Equation (5a). If equation (5a) can not be satisfied, the task is discarded. Then, the consumed energy for completing task eti should satisfy Equation (5.b). Symbol Et−1,ri is the remaining energy of mobile client i in time slot t−1, and Eti is the consumed energy before processing task eti in time slot t. That is to say, the consumed energy of client i in time slot t can not exceed the remaining energy in time slot t. If Equation (5.b) can not be satisfied, the task can not be scheduled in time slot t.

It is rather challenging to achieve the above objective even in the centralized network environment. This is because not only task scheduling decisions but also time division strategies for energy harvesting and task scheduling affect the system performance, making the delay minimization problem be NP-hard [15], not to mention solving the optimization problem in a distributed manner. On one hand, edge servers should reach a consensus on the energy harvesting period based on the decentralized environment. On the other hand, the task offloading requirement of mobile clients should be scheduled to the most suitable edge servers for delay minimization, especially based on local observations of edge servers. Thus, we intend to find policy π that can be executed in a distributed manner to achieve the objective, while satisfying constraints of task deadline and device residual energy.

SECTION 4MDP Optimization Problem Formulation
In this section, we model the defined delay minimization problem as an MDP, which can be represented by tuple (S,A,R,P,γ). Symbol S is the state space, and A is the action space. Symbol R represents the reward, and P is the state transition probability. Symbol γ∈(0,1) is the discount factor. Policy π is utilized to select actions based on each state. In the following, we provide the detailed introduction for the formation of elements in the considered MDP.

4.1 States
The state of an MDP can be represented by S≜{st=(Sm,Sc)}, t∈{0,1,2,...}, where two components are included in each state:

Sm denotes the states of mobile clients, where Sm={(xti,yti,Zti,Cti,Δdt,mini,Δdt,maxi)}i∈N. Variables xti and yti are the coordinates of mobile client i in time slot t; symbol Zti is the total data amount of local tasks for client i in time slot t, and Cti is the total required CPU cycles for computing tasks of client i in time slot t; symbol Δdti is the remaining life cycles of tasks for client i in time slot t. Correspondingly, symbols Δdt,mini and Δdt,maxi are the minimum and maximum left life cycles of tasks for client i in time slot t, respectively. Thus, we can obtain total data amount Zti=∑tν=0(ζνi)Δdνi>0, and total required CPU cycle Cti=∑tν=0(cνi)Δdνi>0, where Δdνi is the left life cycle of task eνi. We define function Γ(X)=(X)Δdνi>0. When Δdνi>0 holds, Γ(X)=X. Otherwise, Γ(X)=0.

The information related to communication is included in Sc, where Sc={(Pij,Υtij)}i∈N,j∈M.

4.2 Actions
The action space can be represented by A≜{at=(τt,αti,βtij)}i∈N,j∈M. That is, not only the energy harvesting period, but also the task scheduling result should be learned.

4.3 State Transition
The state transition probability distribution is denoted by P:S×A×S→[0,1], and ρ0:S→[0,1] is the distribution of initial state s0. State st can be transferred into st+1 by taking action at based on probability P(st+1|st,at).

4.4 Rewards
The immediate reward received by servers after taking action at at state st can be represented by rt:S×A→R. Since our objective is to minimize the average task completion delay, we define rt=−∑tν=0∑Ni=1ρi(dνi)Δdνi>0.

4.5 Optimization Problem Formulation
We need to find policy π that can minimize the average task completion delay from a long-term perspective. In other words, we prepare to maximize the accumulative reward defined in the last subsection. Then, our optimization problem can be formulated by:
P2:V∗(st)=maxat[rt+γEst+1|st,at[V(st+1)]],(6)
View Source
s.t.Constraints (5a) and (5b),
View Sourcewhere V∗(st) is the optimal state value of the formulated MDP. Thus, optimal policy π∗ can be founded by π∗=argmaxat[rt+γEst+1|st,at[V∗(st+1)]]. However, the above formulated optimization problem can not be solved directly by applying DRL-based approaches, such as traditional gradient based policies. The reasons are as follows:

First, although we merely need to determine the action for each state in the formulated MDP, the elements in the action contain integer variables while the output of the neural network is possibilities related to actions, making the actual action hard to determine. In addition, the elements in the defined actions are coupled with each other, and the change of one element has heavy impacts on others. For example, duration τt of energy harvesting period affects task scheduling decisions αti and βtij. Actions can not be directly derived from a neural network model. Second, the corresponding constraints make the optimization problem rather complex. The delay and energy conditions should be both satisfied, and need to be transformed into a solvable form. Third, in our considered decentralized environment, rewards are affected by actions of multiple servers. However, there is no centralized management to train policies, and servers need to train their own policies independently.

SECTION 5Online Learning for Distributed Computation Offloading
To solve the formulated optimization problem P2, we propose an online learning algorithm which can be trained and executed in a distributed manner. We first transfer problems P2 into P3 in Section 5.1, which can be solved by each edge server independently with their trained policies. After that, we present the whole online algorithm in Section 5.2, where three neural networks are specified on initialization, action execution, batch data collection and network training. At last, we provide a comprehensive algorithm analysis in Section 5.3 s.

5.1 Problem Transformation
In this subsection, we first present action, reward and value function reformation, and then process the constraints in the original optimization problem. At last, we formulate the solvable optimization problem for each server.

5.1.1 Action Reformation
In our considered multi-server environment, it is difficult to resolve problem P2 directly, since each edge server chooses actions independently in a decentralized manner. Thus, we first redefine the action of each server, which can formulate their joint actions for the formulated MDP.

From Section 4.2, we can observe that the defined original action includes three elements: energy harvesting period τt, task offloading decision αti and task scheduling decision βtij. For the first element, all servers should reach a consensus on it, due to the half-duplex character of mobile clients. For the remaining elements, servers make their own decisions without the awareness of others’ decisions. Because the three elements are coupled together, we can not solve them simultaneously. In addition, it is difficult for the learning agents on edge servers to make suitable offloading decisions for all clients. Herein, we consider that all clients send offloading requests for all their tasks, based on which learning agents learn their policies. Then, we provide the following definition:

Definition 1 (Estimated task processing sequence).
It is defined by the processing sequence of all offloaded tasks of mobile clients, when they are all processed by edge server j in time slot t. It can be represented by Qtj={Qtji}Ni=1, where Qtji is the processing sequence for offloaded tasks of mobile client i, and can be computed by Qtji={qt′ij,et′i,ιt′ij}tt′=0. Symbol qt′ij is the processing sequence number of task et′i, and ιt′ij is the estimated completion time of task et′i.

Following that, we utilize atj to represent the action for edge server j in time slot t, and define atj=(τtj,Qtj), j∈M, where τtj is the duration of energy harvesting period learned by edge server j. The reasons for utilizing sequence Qji instead of task scheduling decision βtij are: 1) task scheduling decisions should be made based on the full network state, whereas servers are not aware of others’ policy, resulting in partial observation; 2) their unawareness in the decentralized environment may cause multiple offloading selections for some clients but no choice for others. Thus, to balance the offloading choices among different clients, we consider that each server broadcasts the processing sequence for all clients in each time slot. Based on all received sequences, mobile clients can always make proper offloading decision αti and choose optimal edge server βtij, depending on their own energy consumption and delay constraints.

5.1.2 Constraint Relaxation and Reward Reformation
Since edge server j chooses action atj independently, its received reward is different from the whole reward. To make the reward of each server be consistent with the whole reward, we define the reward of edge server j by:
rtj=−∑ν=0t∑i∈N,αti=0,βtij=1ρi(dνi)Δdνi>0−1M∑ν=0t∑i∈N,αti=1ρi(dνi)Δdνi>0.(7)
View SourceThus, we can obtain rt=∑Mj=1rtj. In this way, each server can train their own policies locally, without the need of knowing policies of others.

For the constraints in problem P2, they are difficult to meet during the execution process of the neural network. Therefore, we add penalty ltj to reward rtj when edge server j has expired tasks. Then, the updated reward becomes r^tj=rtj−ltj, where penalty ltj can be computed by ltj=λtjnt. Symbol λtj is an adjustment parameter, and nt is the total number of expired tasks for all mobile clients in time slot t. Then, reward rt can be represented by rt≜∑Mj=1r^tj, and the prove can be found in the following theorem:

Theorem 1 (Consistency of the reward function).
Reward function r^tj of edge server j is consistent with the whole system reward rt.

The proof can be found in Appendix A of Supplementary File, which can be found on the Computer Society Digital Library at http://doi.ieeecomputersociety.org.ezproxy.auckland.ac.nz/10.1109/TPDS.2021.3129618.

5.1.3 Solvable Optimization Problem Formulation
For DRL-based algorithms, the balance between exploration and exploitation is significant to help the agent to learn good policies. With the purpose of encouraging exploration and avoiding early convergence to a suboptimal result, entropy regularization is utilized to form the soft bellman optimality by augmenting the standard value function with a discounted entropy regularizer [22]. Thus, the value function in our work can be represented by:
Vξ(st)= maxπ(st,⋅)(Eat∼π(st,⋅)(rt+γEst+1|st,atVξ(st+1))+ ξH(π,st)),(8)
View Sourcewhere parameter ξ is a control variable that scales the degree of regularization and entropy H(π,st)=−∑at∈Aπ(st,at)logπ(st,at). When ξ=0, the above equation degrades to the standard value function as shown in Equation (6). According to Proposition 3 in [23], when optimal pair (V∗ξ,π∗ξ) is the unique pair of (V,π) for Equation (8), the following equation is satisfied:
V(st)=rt+γEst+1|st,at[V(st+1)]−ξlogπ(st,at).(9)
View SourceInspired by Equation (9), a natural way to find the optimal pair of (V∗,π∗) is to resolve the following problem:
minV,πEst,at(rt+γEst+1|st,at[V(st+1)]−ξlogπ(st,at)−V(st))2.(10)
View Source

Although the above optimization problem encourages exploration for the training process, it is still hard to solve. This is because its inner conditional expectation requires two independent sampling of st+1, which is difficult to obtain in practice. Consequently, a primal-dual form of optimization problem in Equation (10) has been introduced in [23], which can be expressed by:
minV,πmaxρEst,at,st+1[(δ(st,at,st+1)−V(st))2]−ηEst,at,st+1[(δ(st,at,st+1)−ρ(st,at))2],(11)
View Sourcewhere parameter η∈[0,1] is a control variable to balance the trade-off between the two parts of Equation (11), and ρ(st,at) is a global dual variable. Function δ(st,at,st+1) can be computed by:
δ(st,at,st+1)=rt+γV(st+1)−ξlogπ(st,at).(12)
View Source

In our considered system, edge servers make decisions independently, thus policy π(st,at) can be factorized by:
π(st,at)=∏j=1Mπj(st,atj).(13)
View SourceIn addition, we make rt≜∑Mj=1r^tj to relax the constraints in problem P2. Then, function δ(st,at,st+1) can be reformed by:
δ(st,at,st+1)=∑j=1Mr^tj+γV(st+1)−ξlog∏j=1Mπj(st,atj)=∑j=1M(r^tj+γMV(st+1)−ξlogπj(st,atj)).(14)
View SourceWe define δj(st,atj,st+1)=r^tj+γMV(st+1)−ξlogπj(st,atj), and δ(st,at,st+1)=∑Mj=1δj(st,atj,st+1). Following that, the optimization objective in problem P2 can be transformed into:
minV,{πj}j=Mj=1maxρEst,{atj}Mj=1,st+1[(∑j=1Mδj(st,atj,st+1)−V(st))2]−ηEst,{atj}Mj=1,st+1[(∑j=1Mδj(st,atj,st+1)−ρ(st,at))2].(15)
View SourceRight-click on figure for MathML and additional features.

We can observe that parameters π and at have been transformed into parametric forms. To make the optimization problem in Equation (15) solvable in a fully distributed way, parameters V and ρ also need to be parameterized into M elements corresponding to M edge servers. As a result, we consider that edge server j keeps a local copy of global parameters V(st) and ρ(st,at) by Vj(st) and ρj(st,atj), respectively. That is, there is a consensus update process for edge servers to make their local copies the same with that in [24], i.e., V1(st)=...=Vj(st)=Vj+1(st)=...=VM(st). Then, edge server j intends to independently solve the following optimization problem:
P3:0s.t.minVj,πjmaxρjEst,atj,st+1[(δj(st,atj,st+1)−1MVj(st))2]−ηEst,atj,st+1[(δj(st,atj,st+1)−1Mρj(st,atj))2],V1(st)=...=Vj(st)=Vj+1(st)=...=VM(st).(16)
View Source

5.2 The Whole Algorithm
In our considered system, mobile clients send computation offloading requests to edge servers. After receiving those requests, edge servers can schedule them by solving problem P3. That is, each edge server acts as a learning agent, and makes scheduling decisions for mobile clients without knowing the policies of others. Then, edge servers broadcast the scheduling results (task processing sequences) to mobile clients, and those clients choose the best scheduling strategies among the results of those servers to minimize their task completion delays. To solve problem P3, each edge server maintains and trains its constructed neural networks locally. The training process of edge server j can be specified as follows:

5.2.1 Neural Network Initialization
For edge server j, it needs to construct three neural networks corresponding to three optimization parameters ρj, Vj and πj in problem P3, i.e., dual network with optimization parameter ωj, value network with ϕj and policy network with θj. The dual network is utilized to train dual parameter ρj(st,atj), and the policy network trains agent policy πj(st,atj). The value network is utilized to predict state value Vj(st). Thus, policy πj(st,atj) is equivalent to πθj(st,atj), and state value Vj(st) is equivalent to Vϕj(st), similar with dual parameters ρj(st,at) and ρωj(st,at). Then, problem P3 is also equivalent to the following problem:
P4:s.t.minϕj,θjmaxωjL(ϕj,θj,ωj)=Est,atj,st+1[(δj(st,atj,st+1)−1MVj(st))2]−ηEst,atj,st+1[(δj(st,atj,st+1)−1Mρj(st,atj))2],ϕ1=...=ϕj=ϕj+1=...=ϕM,ω1=...=ωj=ωj+1=...=ωM.(17)
View Source

5.2.2 Action Execution
In time slot t, edge server j chooses action atj based on its policy πj. First, edge server j inputs system state st into the policy network, and its output is πj(st,atj). Based on the output possibilities, edge server j converts them into processing sequence Qtj of mobile clients. Then, the mobile client makes the proper offloading decision and selects the proper edge server for computation offloading. That is, for task eti, client i compares the estimated delay for completing eti based on processing sequence {Qtji}Mj=1, and estimated local processing delay d^t,oi. The above process can be formed as an optimization problem for client i by:
P5:s.t. j^=argmin{ιtij}Mj=1,inequation (5b),(18)
View Sourceand
P6:s.t. Di=argmin{ιtij^,d^t,oi},inequation (5b).(19)
View SourceThe above two problems can be easily solved by the method of exhaustion. If Di=ιtij^, then αi=0 and βij=βij^, and server j^ can be selected for processing task eti. If Di=d^t,oi, then αi=1 and local computing is selected.

5.2.3 Batch Data Collection
In order to learn good scheduling results by training policy, value and dual networks, we collect decision trajectories of those edge servers in mini-batches. The system state, predicted state value and outputs of dual and policy networks are recorded together in batches for each edge server. Then, edge servers train their local neural networks by minimizing their losses based on the collected batch data.

5.2.4 Model Training
Solving problem P4 is not easy by DRL, since both dual and primal parameters need to be approximated based on neural network training. Thus, three steps are utilized here to train the three neural networks. First, we solve the inner dual problem, i.e.,
P7:s.t.maxωjL(ϕj,θj,ωj),ω1=...=ωj=ωj+1=...=ωM.(20)
View Source

To get the learned value of ωj, we need to train the dual network. Since the first part in the expression of L(ϕj,θj,ωj) can be regarded as a constant in the training process of the dual network, we set the corresponding loss function by:
L(ωj)=−ηEst,atj,st+1[(δj(st,atj,st+1)−1Mρωj(st,at))2].(21)
View Source

We utilize the gradient descent method with momentum [25] to train the dual network, since it can accelerate the gradient descent process. The involved moment is a velocity vector accumulated in the direction of persistent reduction for L(ωj) across a batch of time slots. To get the approximated value of ωj, we collect state-action pairs of edge server j with batch size B, and train the dual network based on the momentum optimizer. Similar to [24], we conduct a consensus update for ωj by:
ωtj=∑i=1MWjiωti−χtmtωjztωj−−−√,(22)
View Sourcewhere Wji is a mixing matrix for edge server j and others, and χt is the momentum coefficient. Variables mtωj and ztωj are moment vectors.

Algorithm 1. Pseudo-Code of Training Processes for Edge Servers
Input: Batch size B; initial policies with policy parameters {θj}Mj=1, dual parameters {ωj}Mj=1 and value parameters {ϕj}Mj=1

Output: Learned policy {πθj}Mj=1.

for round k=1,2,.. do

Sample a mini-batch of state transition (st,{atj}Mj=1,at+1,{rtj}Mj=1) with size B.

for edge server j=1,2,...,M do

Solve Problem P7 by the following steps:

Compute gradient g(ωj) of optimization parameters ωj in equation (21).

Update moment vectors mtωj and ztωj related to the dual network by:

mωj=o1mt−1ωj+(1−o1)(−g(ωj))

zωj=o2zt−1ωj+(1−o2)g(ωj)⊙g(ωj)

Update parameter ωj by equation (22).

end for

Sample a mini-batch of state transition (st,{atj}Mj=1,at+1,{rtj}Mj=1) with size B.

for edge server j=1,2,...,M do

Solve Problem P8 by the following steps:

Compute gradients g(θj) and g(ϕj) of parameter θj and ϕj in equation (16).

Update moment vectors mtϕj and ztϕj related to the value network by:

mϕj=o1mt−1ϕj+(1−o1)(−g(ϕj))

zϕj=o2zt−1ϕj+(1−o2)g(ϕj)⊙g(ϕj)

Update parameter ϕj by:

ϕtj=∑Mi=1Wjiϕti−χtmtϕjztϕj√

Update parameter θj by the Adam optimizer.

end for

end for

After that, we train the value and policy networks to solve the following problem:
P8:s.t.minϕj,θjL(ϕj,θj,ωj),ϕ1=...=ϕj=ϕj+1=...=ϕM.(23)
View SourceRight-click on figure for MathML and additional features.

Similar to the dual network, we also utilize the momentum optimizer to train the value network and the Adam optimizer for the policy network [26]. Compared with the momentum optimizer, Adam is based on adaptive estimates of lower-order moments for gradients. Then, gradients of the optimization parameters for Problem P8 can be derived by the partial derivative of ϕj and θj. After that, a consensus update process is conducted for parameter ϕj. For parameter θj, it does not need to be consistently updated with others, since each edge server trains its policy independently. For the output of the policy network, the first element is regarded as the action related to energy harvesting period, and others are related to the action for server selection. The training process for edge servers and the whole process of OLD-COP algorithm can be found in Algorithms 1 and 2, respectively.

5.3 Algorithm Analysis
In this subsection, we provide comprehensive theoretical analysis for the designed OLD-COP algorithm. We regard edge servers as learning agents, to solve the formulated optimization problem in a decentralized manner, with the output of the learned energy harvesting period and the scheduling queue for task processing. First, we provide the following theories for task scheduling:

Theorem 2 (Upper bound of task completion delay).
When τt is fixed, the actual completion time of task eti is no more than the best scheduled result broadcast by edge servers in time slot t, when the remaining energy of mobile client i is sufficient for the scheduling of task eti, i.e., ιt,aclij^≤ιtij^, where:
ιtij^=⎧⎩⎨⎪⎪⎪⎪⎪⎪⎪⎪τt+dt,lij^+dt,oij^,qtij^=1;τt+max{ιti′j^,dt,lij^}+dt,oij^,1<qtij^≤|Qtj|,qti′j^=qtij^−1.(24)
View SourceSymbol j^ is the optimal server for task processing of mobile client i, and qtij^ is the estimated processing sequence of task eti by edge server j^. Variable |Qtj| is the total number of tasks in Qtj.

The proof can be found in Appendix B of Supplementary File, available in the online supplemental material.

Theorem 3 (Upper bound of the average task completion delay).
When mobile devices always have sufficient energy to schedule offloaded tasks, the upper bound of the average task completion delay achieved by the designed algorithm can be obtained by:
dmax=limT→∞1TN∑t=0T∑i=1N[αtidt,oi+(1−αti)(ιtij^−τt)+dt,wi].(25)
View Source

The proof can be found in Appendix C of Supplementary File, available in the online supplemental material.

In addition, we can prove that the designed online learning algorithm can converge to a stationary solution. First, we provide the following definition:

Definition 2 (Lipschitz continuous gradient).
If function f(z) is lower bounded, differentiable and has a Lipschitz continuous gradient, the following inequation is founded for L>0, i.e., ∥▽f(z1)−▽f(z2)∥≤L∥z1−z2∥, when parameters z1 and z2 satisfy z1,z2∈Rn, and Rn is n-dimensional Euclidean space.

The original definition and detailed information of Lipschitz continuous gradient can be found in [27].

Algorithm 2. Pseudo-Code of the OLD-COP Algorithm
Input: State of mobile clients Sm and state of communication-related information S3c.

Output: The average task completion delay E(dti).

for time slot t=0,1,2,.. do

Get state st.

for mobile client i=1,2,...,N do

Broadcast offloading requests for all tasks.

end for

for server j=1,2,...,M do

Get action atj by the learning model based on Algorithm 1.

Compute energy harvesting period τt.

Compute task scheduling period T−τt.

Broadcast estimated processing sequence Qtj to mobile clients.

end for

for mobile client i=1,2,...,N do

Solve problems P4 and P5.

Offload tasks that need to be processed remotely to corresponding edge server j^∈M.

Process tasks that need to be processed locally.

Compute average task completion delay E(dti).

end for

end for

For problems P5 and P6, they are defined for each edge server. From the systematic perspective, optimization problems are:
P9:s.t.maxωL(ϕ,θ,ω),ω1=...=ωj=ωj+1=...=ωM,(26)
View Sourceand
P10:s.t.minϕ,θL(ϕ,θ,ω),ϕ1=...=ϕj=ϕj+1=...=ϕM,(27)
View Sourcewhere ω={ω1,...,ωM}, θ={θ1,...,θM}, and ϕ={ϕ1,...,ϕM}. In the following, we analyze the algorithm convergence based on problems P6 and P7.

Theorem 4 (Algorithm convergence).
Consider that three neural networks, i.e., dual, policy and value networks, are trained for totally B2 time slots, and the size of mini-batches for training is B. The designed online learning algorithm converges to the stationary solution of problem P7 with rate O(1B2) based on the following conditions:

The function approximators, utilized in the neural network training for vj(st), πj(st,atj) and ρj(st,at), have Lipschitz continuous gradients, i.e., ∥ΔϕL(ϕ,θ,ω~)−ΔϕL(ϕ,θ,ω∗)∥≤c1/B and ∥ΔθL(ϕ,θ,ω~)−ΔθL(ϕ,θ,ω∗)∥≤c2/B, where ω~ is the obtained result by solving problem P6;

The stochastic gradients of g(ϕ), g(ρ) and g(ω) estimated by each step are bounded by ϱ2.

The proof can be found in Appendix D of Supplementary File, available in the online supplemental material.

SECTION 6Performance Evaluation
6.1 Simulation Setup
To demonstrate the feasibility of OLD-COP, our experiments are conducted based on Tensorflow 2.1 and Python 3.6. As illustrated in Fig. 3, we utilize the city map of Manhattan for performance validation. We select an area with 1 km × 1 km squared by red lines, where servers are uniformly distributed in the area without movements. Actually, walkers or riders holding mobile terminals can be viewed as mobile clients [28]. They move along roads in the considered area based on Manhattan mobility model [29]. The simulated time horizon is time-slot-based because of the high repeatability of individuals’ activities. The settings of simulated parameters are based on [15] and [19], and can be found in Table 2.

TABLE 2 Simulation Parameters


Fig. 2.
The structure of the designed algorithm.

Show All

Fig. 3. - 
The city map of Manhattan.
Fig. 3.
The city map of Manhattan.

Show All

In addition, we consider the Rayleigh fading channel model, where the wireless channel gain between edge server j and mobile client i can be computed by htij=Φ0X−ϖijhFijhSij. Generally, ϖ is between 2 and 4, and we define it by 3. Herein, Φ0=−20dB, and Xij is the distance between edge server j and mobile client i. Symbols hFij and hSij are channel gains caused by small-time-scale channel fading and large-time-scale shadowing, respectively, and are i.i.d vectors of CN(0,1). We leverage multi-layer perceptions for the distributed online learning, where four fully connected layers are constructed for policy, dual and value networks, respectively. Softmax activation function [30], Momentum Optimizer [25] and Adam Optimizer [26] functions are utilized to train neural networks. Since there is no existing study considering distributed computation offloading among multiple edge servers and mobile clients in WP-MEC networks, we compare the designed OLD-COP algorithm with the following four schemes:

CoCoRaM [15]: It is a centralized task scheduling algorithm with a set of edge servers and mobile clients. An approximation algorithm is designed for reasonable time allocation and task scheduling in WP-MEC networks.

Energy-Aware Scheduling (EAS) [31]: This algorithm is leveraged in our system. Thus, the time division is the same with our algorithm. Whereas, for task scheduling process, mobile clients always select the edge server that can minimize their consumed energy.

Random Computation Offloading (RCO): Time division is also set the same with our algorithm, while mobile clients randomly select edge servers for task offloading.

Local Computing (LC): All tasks are processed on mobile clients without offloading, and the duration of energy harvesting is set by the length of each time slot.

6.2 Simulation Results
6.2.1 Performance based on Different Numbers of Mobile Clients
Fig. 4a illustrates the performance of average task completion delay for CoCoRaM, EAS, RCO, LC and the designed algorithm, OLD-COP, with different numbers of mobile clients. We notice that the average task completion delay of OLD-COP is lower than that of the other four algorithms. This is because our algorithm aims at minimizing the average task completion delay by designing the online scheduling algorithm. CoCoRaM algorithm is a centralized scheduling algorithm in WP-MEC networks, and solves a generalized assignment problem to schedule tasks by fixing the energy harvesting period. Then, it utilizes an approximation algorithm to derive the energy harvesting period. However, merely the minimum energy for local computation of each client is considered and the long-term performance is not considered during the optimization process. Thus, its corresponding performance is worse than that of the designed algorithm. Although EAS has the same time division method with ours, it attempts to minimize the consumed energy of mobile clients first. RCO randomly selects edge servers for offloading without reasonable scheduling, and RC algorithm merely depends on the computation capability of mobile clients, which can cause a large delay. When the number of mobile clients increases, average task completion delays of the five algorithms all become large. This is because more mobile clients lead to more offloaded tasks in the system. Consequently, more tasks consume more computation resources and lead to larger delays.


Fig. 4.
Performance with different numbers of mobile clients.

Show All

The performance of average task completion ratio based on different numbers of mobile clients is illustrated in Fig. 4b. Herein, task completion ratio refers to the ratio of tasks that can be successfully processed compared to those generated in each time slot. We can find that the performance on average task completion ratio of OLD-COP is higher than those of EAS, RCO, LC algorithms, while lower than that of CoCoRaM algorithm. This is because CoCoRaM algorithm maximizes the task completion ratio by solving a defined optimization problem in a centralized manner. Whereas, our designed algorithm aims to minimize task completion delay by a designed learning algorithm in a distributed manner. Thus, for task scheduling in our designed algorithm, mobile clients always select edge servers with the minimum delays for their tasks. The performance of consumed energy of mobile clients with different numbers of mobile clients is shown in Fig. 4c. It is obvious that EAS algorithm has the minimum energy consumption. This is because mobile clients scheduled by EAS algorithm always select edge servers that can minimize their energy consumption for task offloading. To maximize the task completion ratio, more energy is consumed for task transmission and processing in CoCoRaM algorithm. Thus, the consumed energy of mobile clients for OLD-COP is lower than that of CoCoRaM algorithm, while higher than that of EAS algorithm.

6.2.2 Performance based on Different Numbers of Edge Servers
The results of average task completion delay based on different numbers of edge servers are presented in Fig. 5a. It is obvious that the performance of the five algorithms is similar with that of Fig. 5a, i.e., average task completion delay of OLD-COP is the minimum, CoCoRaM algorithm second, and those of EAS, RCO and LC are higher. When the number of edge servers becomes big, the performance of OLD-COP, CoCoRaM, EAS, RCO and LC algorithms also increases. This is because when there are more edge servers, more computation resources can be utilized for task processing. Then, average task completion delay can be reduced with more computation resources. However, the performance of LC algorithm has almost no change when the number of edge servers becomes big. This is because it processes tasks locally without offloading, so that the computation resources of mobile clients have no change.


Fig. 5.
Performance with different numbers of edge servers.

Show All

Fig. 5b illustrates the performance of average task completion ratio with different numbers of edge servers. We can find that when the number of edge servers becomes big, average task completion ratios of OLD-COP, CoCoRaM, EAS, RCO and LC algorithms also increase. The reason is that, on one hand, a bigger number of edge servers result in more energy sources for mobile clients, and they can harvest more energy. On the other hand, more computation resources can be utilized, speeding up the process of task processing. The performance of consumed energy of mobile clients for the five algorithms is illustrated in Fig. 5c. When the number of edge servers becomes big, the consumed energy of mobile clients decreases. This is because when there are more edge servers, mobile clients have higher chances to offload their tasks to nearer edge servers, which reduces the consumed transmission energy of tasks.

6.2.3 The Impact of Task Generation Possibilities
Fig. 6 illustrates the impact of task generation possibilities on the system performance. The trend of average task completion delay is illustrated in Fig. 6a. It shows that average task completion delay increases when the task generation probability becomes large. For example, when the task generation probability is 0.5, average task completion delays of OLD-COP, CoCoRaM, EAS, RCO and LC algorithms are 0.0084s, 0.017s, 0.034s, 0.099s and 0.183s, respectively. When the task generation probability is 0.7, average task completion delays of the five algorithms are 0.013s, 0.024s, 0.047s, 0.117s and 0.194s, respectively. The reason is that, more tasks can be generated when the task generation probability becomes large. Following that, there are more tasks need to be scheduled in the system. Thus, when the number of edge servers is fixed, average task completion delay becomes large.


Fig. 6.
Performance with different task generation possibilities.

Show All

Fig. 6b is the trend of average task completion ratio with different task generation possibilities. We can observe that when the task generation probability becomes large, the trend of average task completion ratio drops. For instance, when the task generation probability is 0.5, average task completion ratios of OLD-COP, CoCoRaM, EAS, RCO and LC algorithms are 0.57, 0.70, 0.44, 0.22 and 0.0098, respectively. When the task generation probability increases to 0.7, average task completion ratios of the five algorithms are 0.43, 0.61, 0.22, 0.13 and 0.0092, respectively. This is because when more tasks exist in the system, more computation and communication resources are consumed to process those tasks. However, some tasks can not be processed in time, resulting in lower average task completion ratios. In addition, the trend of average task completion ratio is similar with that in Fig. 4b, which has similar reasons for the five algorithms.

6.2.4 The Impact of Energy Harvesting Efficiency
System performance with different energy harvesting efficiencies is illustrated in Fig. 7. We can observe that, for average task completion delay shown in Fig. 7a, the designed algorithm, OLD-COP, has the best performance compared with the other four algorithms, since our algorithm intends to minimize average task completion delay by designing reasonable online scheduling algorithms. However, CoCoRaM algorithm does not consider the long-term performance and merely tries to approach to the optimal scheduling in each time slot. Although it is a centralized approach, the performance of average task completion delay is still worse than that of our algorithm. When the energy harvesting efficiency becomes large, average task completion ratios of the five algorithms gradually become small due to the sufficient harvested energy.


Fig. 7.
Performance with different energy harvesting efficiencies.

Show All

Fig. 7b shows the performance trend of average task completion ratios for the five algorithms. When the energy harvesting efficiency increases, average task completion ratios of the five algorithms become large. For example, when the energy harvesting efficiency is 0.6, average task completion ratios for OLD-COP, CoCoRaM, EAS, RCO and LC algorithms are 0.612, 0.752, 0.514, 0.313 and 0.0098, respectively. When the energy harvesting efficiency increases to 0.8, average task completion ratios of the five algorithms are 0.651, 0.793, 0.558, 0.358 and 0.011, respectively. The reason is that more harvested energy can support more tasks for both local computing and remote transmission.

6.2.5 The Performance of Execution and Convergence Time
Fig. 8a illustrates the execution time of the five algorithms. It is observed that the execution time of the designed algorithm, OLD-COP, is the maximum, and EAS tightly follows it. The execution time of LC algorithm is the minimum. This is because OLD-COP algorithm includes three neural networks, and periodically trains them based on collected state-action trajectories, consuming much time. Since EAS algorithm has the same method to learn time division with our method by training neural networks, its execution time is similar with ours. However, CoCoRaM, RCO, and LC algorithms do not have the training process. LC algorithm totally depends on local computing and merely needs to put tasks into local computing queues. RCO randomly selects available edge servers, while CoCoRaM algorithm produces approximate optimal computation schedules with given WPT time for WP-MEC networks, thus its time complexity is higher than that of RCO and LC algorithms. When the number of mobile clients becomes big, the execution time of the five algorithms also becomes long. This is because more clients can generate more tasks, and more tasks consume more time for scheduling.


Fig. 8.
Performance of execution and convergence time.

Show All

Fig. 8b is the convergence time of the designed algorithm. Since our algorithm and EAS depend on the neural network training, and EAS utilizes the same training method with ours, we merely evaluate the convergence time of ours. We can observe that its convergence time is acceptable, compared with its total execution time. When the number of mobile clients becomes big, the convergence time of the designed algorithm also increases. This is because more clients can generate more tasks, and the algorithm execution time becomes longer, correspondingly.

SECTION 7Conclusion
In this paper, we proposed an online learning algorithm for computation offloading in WP-MEC networks with distributed execution. Our objective is to minimize the average task completion delay of mobile clients in a long-term perspective. First, we defined the delay minimization problem by considering task delay and energy constraints. Then, we transformed it into a primal-dual optimization problem based on the Bellman equation. To solve the formulated optimization problem in an efficient way, we designed a novel neural model, which can learn time division and task offloading decisions simultaneously. To further make the learning model trained and executed in a distributed manner, we formed multiple learning models that can be decentralized on edge servers and coordinated to achieve parameter synchronization. At last, both theoretical analyses and performance results showed that our designed algorithm has a significant advantage in terms of average task completion delay and algorithm convergence speed.