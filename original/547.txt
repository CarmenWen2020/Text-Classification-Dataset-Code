The ever-expanding scale of cloud datacenters necessitates automated resource provisioning to best meet the requirements of low latency and high energy-efficiency. However, due to the dynamic system states and various user demands, efficient resource allocation in cloud faces huge challenges. Most of the existing solutions for cloud resource allocation cannot effectively handle the dynamic cloud environments because they depend on the prior knowledge of a cloud system, which may lead to excessive energy consumption and degraded Quality-of-Service (QoS). To address this problem, we propose an adaptive and efficient cloud resource allocation scheme based on Actor-Critic Deep Reinforcement Learning (DRL). First, the actor parameterizes the policy (allocating resources) and chooses actions (scheduling jobs) based on the scores assessed by the critic (evaluating actions). Next, the resource allocation policy is updated by using gradient ascent while the variance of policy gradient is reduced with an advantage function, which improves the training efficiency of the proposed method. We conduct extensive simulation experiments using real-world data from Google cloud datacenters. The results show that our method can obtain the superior QoS in terms of latency and job dismissing rate with enhanced energy-efficiency, compared to two advanced DRL-based and five classic cloud resource allocation methods.
SECTION 1Introduction
Cloud computing has rapidly developed as one of the most prevailing computing paradigms [2]. In cloud computing, resource allocation is regarded as a process of allocating computing, storage, and networking resources to meet the requirements of both users and cloud service providers (CSPs). Many problems in cloud resource allocation have emerged with the ever-increasing scale and dynamics of cloud datacenters, such as irrational resource provisioning and slow response to changes. These problems not only degrade the Quality-of-Service (QoS) but also cause high energy consumption and maintenance overheads [3]. Therefore, it has been a high-priority objective to design an adaptive and efficient solution for resource allocation in cloud datacenters. However, it is a highly challenging task due to the dynamic system states and various user demands in cloud computing [4], as described below.

The complexity of cloud datacenters. There are a large number of different types of servers in cloud datacenters, which provide various computing and storage resources including central processing units (CPUs), memories, and storage units. Therefore, it is challenging to manage and coordinate such heterogeneous resources efficiently in cloud computing [5].

The diversity of demands from users. Jobs coming from different users demand heterogeneous resources (e.g., CPUs, memories, and storage units) and different durations (e.g., minutes, hours, and days) [6]. Such diversity of user demands intensifies the difficulty of resource allocation in cloud datacenters.

The excessiveness of energy consumption. Large energy consumption not only causes huge operation overheads but also results in extensive carbon emissions [7]. In Google cloud datacenters, the average CPU utilization of servers is only around 20% [8]. Such energy waste occurs when irrational resource allocation schemes are used. However, it is hard to satisfy diverse user demands while maintaining cloud datacenters with high energy-efficiency.

The dynamics of cloud systems. In cloud datacenters, system states such as resource usage and requests are changing frequently. Effective resource allocation is expected to continuously meet the requirements of user jobs under such dynamic cloud environments. However, it is difficult to build an accurate model for resource allocation in response to dynamic cloud environments. Therefore, these dynamics have caused huge challenges to adaptive resource allocation in cloud datacenters [9].

Many classic solutions for cloud resource allocation are based on rules [10], heuristics [11], and control theory [12]. Although these solutions can solve the problem of cloud resource allocation to some extent, they commonly use the prior knowledge of cloud systems (e.g., state transitions, demand changes, and energy consumptions) to develop corresponding strategies of resource allocation. Thus, these solutions might work well in a specific application scenario, but they are unable to fully fit in the cloud environment with dynamic system states and user demands. For example, job scheduling can be easily executed by using rule-based strategies for meeting instant user demands. However, they only consider the current job characteristics (e.g., resource demands and job durations) to obtain short-term benefits. Therefore, they are unable to adaptively fulfill the dynamic demands of user jobs with a long-term perspective, and it might result in excessive job latency and serious resource wastes due to irrational resource allocation. Besides, numerous iterations may be needed to find feasible resource allocation plans with these solutions, which leads to high computational complexity and resource overheads. Therefore, they are unable to effectively address the complicated problem of resource allocation in dynamic cloud environments.

Reinforcement learning (RL) [13] has emerged as a promising approach for handling resource allocation problems with high-adaptiveness and low-complexity. However, traditional RL-based methods suffer from the problem of high-dimensional state space when dealing with complex cloud environments [14]. To address this problem, deep reinforcement learning (DRL) [15] was proposed to extract low-dimensional representations from high-dimensional state spaces using deep neural networks (DNNs) [16]. Although there are some DRL-based methods focused on the problem of cloud resource allocation [17], [18], [19], [20], most of them use the value-based DRL (e.g., deep Q-networks (DQN) [15] and double Q-learning (DQL) [21]), which may lead to low training efficiency when dealing with a large action space. This is because the value-based DRL learns a deterministic policy by calculating the probability of each action. However, in a cloud datacenter, jobs may arrive constantly and thus the action space may be considerably large to continuously meet the requirements of scheduling jobs. Therefore, it could be hard for the value-based DRL to approach the optimal policy with quick convergence. By contrast, the policy-based DRL (e.g., policy gradient (PG) [13]) learns a stochastic policy and can better deal with the large action space in a cloud datacenter by directly outputting actions with the probability distribution, but it might reduce the training efficiency caused by the high variance generated when estimating the policy gradient.

As a synergy of value-based and policy-based DRL algorithms, advantage actor-critic (A2C) [13] was designed to address the above issues. In A2C, the actor chooses actions based on the scores assessed by the critic, where the variance of policy gradient is reduced with an advantage function. However, the A2C adopts a single-thread training manner and thus underutilizes computational resources. Meanwhile, strong data correlation may occur when using the A2C because similar training samples are generated when there is only a single DRL agent interacting with the environment, which would cause unsatisfactory training results. To address these problems with A2C, an asynchronous advantage actor-critic (A3C) algorithm with low-variance and high-efficiency was proposed in [22]. The A3C uses multiple DRL agents to interact with the environment simultaneously, making full use of computational resources and thus improving the learning speed. Meanwhile, the data collected by different DRL agents are independent of each other, and thus the A3C breaks the data correlation.

In light of the A3C algorithm's advantages, we develop an A3C-based resource allocation scheme for cloud datacenters with heterogeneous resources, diverse user demands, large energy consumption, and dynamic environments. The main contributions of this paper are summarized as follows.

A unified model of resource allocation is designed for a cloud datacenter with dynamic system states and heterogeneous user demands. In the proposed model, the QoS (job latency and dismissing rate) and energy-efficiency (average energy consumption of jobs) are regarded as optimization goals. Furthermore, the state space, action space, and reward function for cloud resource allocation are defined and formulated as a Markov decision process (MDP), which are used in the proposed DRL-based cloud resource allocation scheme.

An actor-critic DRL (A3C) based resource allocation method is proposed to efficiently approach the optimal policy of job scheduling in a cloud datacenter. Specifically, DNNs are utilized to handle the problem of high-dimensional state space in a cloud datacenter. Moreover, the training efficiency of the proposed method is greatly improved with the asynchronous update of policy parameters among multiple DRL agents.

The extensive simulation experiments using real-world trace data from Google cloud datacenters are conducted to validate the effectiveness of the proposed method. The simulation results demonstrate that the proposed method can achieve the better QoS, higher energy-efficiency, and faster convergence compared to five classic resource allocation algorithms and two advanced DRL-based resource allocation methods.

The rest of this paper is organized as follows. In Section 2, the related work is introduced. Section 3 describes the system model of resource allocation in a cloud datacenter. In Section 4, the proposed A3C-based cloud resource allocation method is presented in detail. In Section 5, the proposed method is evaluated by simulation experiments with real-world datasets. Section 6 concludes this paper.

SECTION 2Related Work
Resource allocation in cloud computing has attracted much research attention, while many studies have contributed to solving this important problem. In this section, we review the related work from two aspects, including the classic and DRL-based solutions for cloud resource allocation.

2.1 Classic Approaches for Cloud Resource Allocation
Resource allocation problem is omnipresent in cloud computing and many methods have been proposed to enhance resource utilization with rational resource provisioning and effective cost control. For example, Zahid et al. [10] proposed a ruled-based language for CSPs, in order to improve the QoS compliance of high-performance computing (HPC) clouds. Through using probabilistic thresholds, a system model was designed in [23] for accomplishing the switching between different operating levels of cloud services. Johnson's rule and genetic algorithm were combined in [24] for solving the multiprocessor scheduling problem in cloud datacenters. By using a rule-based control approach, a power-aware job scheduler was designed in [25] for improving power-efficiency and meeting power constraints. Wang et al. [25] also compared the pros and cons of baseline scheduling algorithms, such as the longest job first (LJF), shortest job first (SJF), and round-robin (RR). Furthermore, Samal and Mishra [26] analyzed the variants of RR algorithms for load balancing in cloud computing. Based on the heuristics, the problem of cloud resource reservation was solved in [11] for meeting user demands while reducing resource costs. Grandl et al. [27] designed Tetris, a cluster scheduler with packing heuristics, in order to match task requirements with resource availability and improve cluster efficiency. Avgeris et al. [12] proposed a hierarchical resource allocation framework with an admission control mechanism, and it can support mobile users to choose edge servers for executing their tasks with less response time and computational costs. Haratian et al. [28] designed an adaptive resource management framework for meeting the QoS requirements, where the decision-making of cloud resource allocation was executed by a fuzzy controller in each iteration of control cycles. Through utilizing the feedback-control theory, a Big-Data MapReduce system was developed in [29] for reducing the costs of cluster reconfigurations.

Overall, most of these work focused on rule-based strategies, heuristics, and control theory for cloud resource allocation. The rule-based strategies or heuristics need to establish different rules for fulfilling dynamic system states and user demands in cloud datacenters. Thus, not only the application scopes of them are limited but also high overheads of rule settings are generated. Besides, the control-theory based solutions require numerous feedback iterations, and thus they usually lead to high computational complexity and unnecessary resource overheads. To address these important challenges, DRL has emerged as an adaptive and efficient decision-making method for solving the complicated problem of cloud resource allocation.

2.2 DRL-Based Methods for Cloud Resource Allocation
Deep reinforcement learning (DRL) combines reinforcement learning (RL) and DNNs, and it emphasizes the decision-making process of choosing actions according to different system states in the dynamic environments, in order to maximize the long-term rewards. Two great milestones have witnessed the vigorous development of DRL-based algorithms. One is the application of deep Q-networks (DQN) on the Atari 2600 platform [15]. The other is the Alpha Go that defeated the Go world champion by integrating Monte Carlo Tree Search (MCTS) with DNNs [30]. Moreover, DRL-based methods have recently been applied to the problem of resource allocation in cloud computing. For instance, Tong et al. [17] combined the Q-learning algorithm with DNNs to handle the scheduling problem of directed acyclic graph (DAG) tasks in the cloud environment. The DQN algorithm was adopted in [18] to allocate compute-intensive jobs, in order to reduce the energy consumption of cloud datacenters. By using the DQN algorithm, a hierarchical framework was designed in [19] for adaptive resource allocation, aiming to reduce power consumption in cloud datacenters. Also, Zhang et al. [31] adopted the DQN algorithm, in order to achieve adaptive provisioning and configuration for cloud resources. Different from these work using value-based DRL algorithms, Mao et al. [20] leveraged a policy-based DRL algorithm (i.e., policy gradient (PG)) to handle the resource allocation problem in cloud datacenters. Subject to resource constraints in wireless systems, Eisen et al. [32] utilized a PG-based method to find the near-optimality of resource allocation. Based on the PG algorithm, a QoS-aware scheduler was developed in [33], aiming to improve the QoS when scheduling DNN inference workloads in cloud computing. Moreover, a PG-based actor-critic approach for user scheduling and resource allocation was designed in [34], aiming to maximize the energy-efficiency in heterogeneous networks. Besides, an actor-critic based DRL method for cloud resource allocation was developed in our previous work [1] that only considered the optimization of job latency but not energy-efficiency. Moreover, the training efficiency of these actor-critic based DRL methods can still be improved because they did not take advantage of asynchronous update mechanism.

In general, most of these work depends on value-based DRL methods for cloud resource allocation. It is difficult for them to approach the accurate optimal policy when dealing with a large action space. Although there exists a small amount of research using policy-based DRL methods to address this problem, the high variance is generated when they estimate the policy gradient. Besides, the above DRL-based methods reveal drawbacks in training efficiency, thus numerous iterations are needed for optimizing the scheduling policy. To address these essential problems, we propose an A3C-based resource allocation method in cloud datacenters. Different from the GA3C [35] that aims to enhance the performance of the A3C algorithm by leveraging the GPU computational power, our method focuses on the adaptation and application of the A3C algorithm in cloud resource allocation.

SECTION 3System Model
A unified model of resource allocation is designed, aiming to improve the QoS and energy-efficiency in dynamic environments of cloud datacenters with various user demands and ever-changing system states. For the clarity of presentation, we consider the scenario of a single cloud datacenter with a set of servers, donated by V={v1,v2,…,vm}, where m indicates the number of servers. Each server provides multiple types of resources (e.g., CPUs, memories, and storage units), donated by Res={r1,r2,…,rn}, where n indicates the number of resource types. As shown in Fig. 1, a DRL-based resource controller is embedded in the resource allocation system (RAS). The RAS generates policies of job scheduling based on the resource requests of different user jobs and current state information of the cloud datacenter (e.g., number of servers, resource usage, and energy consumption). According to the policies delivered by the DRL-based resource controller, the job scheduler assigns jobs from the job sequence to servers. Specifically, the jobs are generalized as data processing jobs [36], such as the training jobs of deep learning (DL) models for image processing and speech recognition. For different jobs, they exhibit various resource requests according to their purposes. Therefore, each job consists of a specific job duration (e.g., minutes, hours, or days) and the request for different types of resources (e.g., CPUs and memories). During the process of resource allocation, the information collector records the usage of different resources and current energy consumption (measured by an energy agent) in the cloud datacenter. Referring to the above information, the DRL-based resource controller will generate policies of job scheduling accordingly. The major notations involved in the proposed model are listed in Table 1.

TABLE 1 Major Notations Used in the Proposed Model


Fig. 1.
Model of resource allocation in a cloud datacenter.

Show All

Considering that there are a set of all jobs that are expected to be processed, denoted by Jtotal={j1,j2,…,jp}, where p indicates the total number of jobs, a set of jobs that are waiting in the job sequence, denoted by Jseq={j1,j2,…,jq}, where q indicates the number of jobs waiting in the job sequence, and q≤p. When a job from Jtotal arrives, it will first enter Jseq. If the available resources are enough, this job can be processed immediately. Otherwise, this job will wait in the job sequence for scheduling. Following the first-in-first-out (FIFO) policy, the jobs in Jseq will be dropped when the job sequence is full. Therefore, the actual completion time of a job is obtained by calculating the time interval from entering the job sequence to the end of processing, denoted by Tjfinish−Tjenter. Fig. 2 illustrates an example of job scheduling. We assume that there is a server with 100 computing units of CPU resources. The jobs j1, j2, and j3 request 50, 30, and 40 units of CPU resources, respectively, which arrive at timesteps t1, t2, and t3, and are completed at timesteps t4, t5, and t6. In the proposed model, the time instances (i.e., timesteps) are the arrival times and completion times of jobs at servers. Whenever a job arrives or is completed, a state transition occurs. More specifically, when j1 and j2 arrive, there are enough CPU resources to process these two jobs immediately. Therefore, the actual completion time of j1 and j2 (i.e., t4−t1 and t5−t2) are equal to their expected job durations (i.e., d1 and d2). However, j3 can only be processed until j1 is completed because the CPU resources are currently inadequate. Therefore, the actual completion time of j3 (i.e., t6−t3) is longer than its expected job duration (i.e., d3).


Fig. 2.
An example of job scheduling.

Show All

The numerical discrepancy among different values of job latency may lead to excessive computation time during gradient descent, therefore, the normalization is used to improve the training speed and convergence of the algorithm. Therefore, Lnormal is defined as the normalized average job latency, which normalizes the job latency of all successfully completed jobs and then takes their average.
Lnormal=∑j∈completed jobs((Tjfinish−Tjenter)/dj)Number of completed jobs,(1)
View Sourcewhere Lnormal≥1 and dj is the duration of a job.

Moreover, a constraint is added on the length of the job sequence Jseq, denoted by seqLen, which is used to avoid the QoS degrading caused by the excessive number of jobs that are staying in the waiting status. Therefore, disRate is defined as the job dismissing rate, which calculates the rate of dismissed jobs when the job sequence is full.
disRate=1−Number of completed jobsTotal number of jobs,(2)
View Sourcewhere 0≤disRate≤1.

Besides, the energy consumption generated in cloud datacenters commonly depends on their resource usage, and thus it is a feasible way to reduce energy consumption by enhancing this metric. This is because fewer servers tend to be switched on when the existing servers have high resource usage. Through experimental measurements, existing studies [19], [37], [38] have shown that the energy consumption of a server is proportional to its resource usage. Based on these studies, the total energy consumption of a cloud datacenter is formulated as
Etotal=∑t=0T∑v∈V(k⋅Pmax+(1−k)⋅Pmax⋅Urest),(3)
View SourceRight-click on figure for MathML and additional features.where Pmax is the maximum energy consumption of a server when it is fully utilized, the fraction k is used to calculate the energy consumption of an idle server, Urest is the resource usage of all servers by timestep t, and T is the timestep when the last job is completed.

Different from most of the existing work that regards the total energy consumption as a performance metric, we consider the energy-efficiency during the job scheduling process, which is measured by the average energy consumption of all successfully completed jobs as
Ejob=EtotalNumber of completed jobs.(4)
View Source

To improve the QoS (i.e., Lnormal and disRate) and energy-efficiency (i.e., Ejob), a DRL-based resource allocation method is proposed to execute the job scheduling in a cloud datacenter. Specifically, we regard the RAS as a DRL agent and the cloud datacenter as the environment. At each timestep, the DRL agent chooses an action of scheduling jobs by interacting with the environment. Accordingly, the state space, action space, and reward function in DRL are defined as follows.

State space: In the state space S, the state st∈S consists of the resource usage of all servers and resource requests of all arrived jobs by timestep t. On one hand, Urest=[[u1,1,u1,2,…,u1,n],[u2,1,u2,2,…,u2,n],…,[um,1,um,2,…, um,n]] indicates the usage of different types of resources on all servers by timestep t, where um,n is the usage of the nth resource type on the server vm. On the other hand, Orest=[[o1,1,o1,2,…,o1,n],[o2,1,o2,2,…,o2,n],…,[oj,1,oj,2,…,oj,n]] indicates the occupancy requests of all arrived jobs for different types of resources by timestep t, where oj,n is the occupancy request of the latest arrived job j for the nth resource type, and Djobt=[d1,d2,…,dj] denotes the durations of all arrived jobs by timestep t. Therefore, the state of a cloud datacenter by timestep t is defined as
st=[sVt,sJt]=[Urest,[Orest,Djobt]],(5)
View Sourcewhere sVt=Urest and sJt=[Orest,Djobt] are used to represent the states of all servers and arrived jobs for the clarity of presentation. The state space changes when jobs arrive or are completed, and the dimension of the state space depends on the situation of servers and arrived jobs, calculated by (mn+z(n+1)), where m, n, and z are the number of servers, resource types, and arrived jobs, respectively.

Action space: At timestep t, the action at adopted by the job scheduler is to select and execute jobs from the job sequence, according to a policy of job scheduling delivered by the DRL-based resource controller. The policy is generated based on the current system state, and the job scheduler assigns jobs to a specific server for execution. Once a job is scheduled to an appropriate server, the server will automatically allocate corresponding resources according to the resource request of this job. Therefore, the action space only indicates whether a job will be processed by a server or not, which is defined as
A={at|at∈{0,1,2,…,m}},(6)
View Sourcewhere at∈A. When at=0, the job scheduler does not assign the job at timestep t and the job needs to wait in the job sequence. Otherwise, the job will be processed by a specific server.

State-transition probability matrix: The matrix indicates the probabilities of the transition between two states. Taking Fig. 2 as an example, at timestep t0, there is no job to be processed and the initial state s0=[0,[[0],[0]]], where the three ”0” items represent the CPU usage of the server, occupancy requests of jobs, and job durations, respectively. At t1, the job j1 is scheduled immediately since the available resources are sufficient. After taking this action, the state evolves to s1=[50,[[50],[d1]]], where the first ”50” item indicates the CPU usage of the server, the second ”50” item represents the occupancy request of j1 for CPU resources, and d1 is the duration of j1. Similarly, after taking the action of scheduling j2 at t2, the state evolves to s2=[80,[[50,30],[d1,d2]]]. Specifically, the state-transition probability matrix is denoted as IP(st+1|st,at), which indicates the probabilities of transiting to the next state st+1 when taking an action at at the current state st. The values of the transition probabilities are obtained by running the DRL algorithm, which outputs the probabilities of taking different actions at a state.

Reward function: The reward function is used to guide the DRL agent (RAS) to learn better policies of job scheduling with higher discounted long-term rewards, aiming to improve the system performance of cloud resource allocation. Therefore, at timestep t, the total rewards Rt consist of two parts including the rewards of the QoS (denoted by RQoSt) and the energy-efficiency (denoted by Renergyt), which is defined as
Rt=RQoSt+Renergyt.(7)
View Source

Specifically, RQoSt reflects the penalties (hence negative) for different types of latency at timestep t including Tj,waitt, Tj,workt, and Tj,misst (as described in Table 1), which is defined as
RQoSt=−∑j∈Jseq(w1⋅Tj,waitt+Tj,worktdj+w2⋅Tj,misst),(8)
View Sourcewhere w1 and w2 are used to weight the penalties. Since RQoSt is a negative value, a job that has a longer duration tends to wait for a shorter time. This is sensible for a cloud system with the objective of profit maximization as a job with a longer duration can lead to higher profits [39].

Moreover, Renergyt reflects the penalty for energy consumption at timestep t, which is defined as
Renergyt=−w3⋅∑j∈JseqEj,exect,(9)
View Sourcewhere Ej,exect is the energy consumption of executing a job by timestep t, and w3 is used to weight the penalty.

During the optimization process of cloud resource allocation, the DRL agent first chooses an action at (scheduling jobs) under the current system state st (resource usage and resource requests) of the environment (cloud datacenter). Next, the DRL agent receives rewards Rt (the QoS and energy-efficiency) and steps to the next state st+1. This process is illustrated by an MDP, as shown in Fig. 3.


Fig. 3.
An example of the MDP process modeling cloud resource allocation.

Show All

Due to the uncertainty of system states, the problem of cloud resource allocation is formulated with model-free DRL. Based on the discrete-time-based MDPs with a large action space, an A3C algorithm is utilized to explore adaptive and efficient resource allocation in dynamic environments of cloud datacenters.

SECTION 4Adaptive and Efficient Resource Allocation Using A3C in Cloud Datacenters
This section presents the proposed effective resource allocation method based on the asynchronous advantage actor-critic (A3C), which can achieve superior QoS and energy-efficiency in cloud datacenters. The proposed method adopts an actor-critic based DRL framework with asynchronous update (A3C) to accelerate the training process. Specifically, the A3C-based method incorporates both value-based and policy-based DRL algorithms. On one hand, the value-based DRL determines the value function by using function approximators and adopts the ϵ-greedy to balance the exploration and exploitation. Therefore, the DRL agent utilizes existing experiences to choose good actions of job scheduling whilst exploring new actions. On the other hand, the policy-based DRL parameterizes the policy of job scheduling and directly outputs actions with probability distributions during the learning process without storing their Q-values. Thus, the DRL agent can efficiently choose actions under a large action space.

The key steps of the proposed A3C-based cloud resource allocation method are shown in Algorithm 1. Based on the definitions of state space (in Eq. (5)), action space (in Eq. (6)), and reward function (in Eq. (7)), the actor's network Vπθ and critic's network Qπθ are first initialized with weights and biases. Next, the actor's and critic's learning rate γa and γc, and the TD error discount factor β are initialized.

SECTION Algorithm 1.The A3C-Based Resource Allocation in Cloud Datacenters
Initialize: The actor's network Vπθ and critic's network Qπθ with weights and biases.

Initialize: The actor's and critic's learning rate γa and γc, reward decay rate λ, TD error discount factor β, counter temp=0, and update step u.

for each training epoch n=0,1,2,…,N do

Receive the initial state s0, where s0=env.observe();

for t=0,1,2,…,T do

Select the action at of scheduling jobs based on the current system state st of the cloud datacenter, where st=[sVt,sJt] (defined in Eq. (5)) and at∈A (defined in Eq. (6)): at=actor.choose_action(st);

Execute the scheduling action at, receive the reward Rt (QoS and energy-efficiency) and the next state st+1, where Rt=RQoSt+Renergyt (defined in Eq. (7)): Rt,st+1=env.step(at);

Calculate the discounted long-term rewards: Rdisc=R0+λR1+...+λt−1Rt−1;

Calculate the advantage function in the critic, where Qw(st,at)=Rdisc+λtVπθt(st+1): Aπθt(st,at)=Qw(st,at)−Vπθt(st);

Minimize the TD error: δπθt=Rt+βVπθt(st+1)−Vπθt(st);

Update the state-action value function parameter: wt+1←wt+γcδπθtt∇wQw(st,at);

Calculate the policy gradient in the actor by using the advantage function: ∇θtJ(θt)=Eπθt[∇θtlogπθt(st,at)Aπθt(st,at)];

Update the scheduling policy: θt+1←θt+γa∇θtJ(θt);

Update the state: st=st+1;

Update the counter: temp=temp+1;

if temp % u==0 then

Call Algorithm 2 to asynchronously update policy parameters in each DRL agent;

end

end

end

The optimization objective of the proposed A3C-based resource allocation method is to obtain the most rewards. Therefore, the instant reward Rt (defined in Eq. (7)) is accumulated by using a probability distribution as
J(θt)=∑st∈Sdπθt(st)∑at∈Aπθt(st,at)Rt,(10)
View Sourcewhere dπθt(st) is the stationary distribution of MDPs modeling cloud resource allocation under the current policy πθt of job scheduling.

After the initialization, the training process for optimizing cloud resource allocation begins. To improve the optimization objective, the policy parameters of job scheduling are updated continuously.

In one-step MDPs, the policy gradient of the objective function is defined as
∇θtJ(θt)=Eπθt[∇θtlogπθt(st,at)Rt].(11)
View Source

When it comes to multi-step MDPs, the instant reward Rt is replaced by the long-term value Qπθt(st,at), and the policy gradient theorem is defined as

Theorem 1. Policy Gradient Theorem [13]: For any differentiable policy πθt(st,at) and any policy objective functions, the corresponding gradient is defined as
∇θtJ(θt)=Eπθt[∇θtlogπθt(st,at)Qπθt(st,at)].(12)
View Source

Based on this theorem, a Temporal Difference (TD) learning [13] is adopted, which estimates the state-values accurately and guides the update of policy parameters.

Fig. 4 illustrates the framework of the proposed A3C-based cloud resource allocation method. Through taking advantage of both policy-based and value-based DRL, the proposed method is able to handle a large action space and reduce the variance when estimating gradient.


Fig. 4.
Framework of the A3C-based cloud resource allocation method.

Show All

In each DRL agent, the critic's network estimates the state-action value function Qw(st,at)≈Qπθt(st,at) and updates parameter w. Moreover, the actor's network guides the update of policy parameters θt of the job scheduling policy πθt based on the evaluated values from the critic's network. The corresponding policy gradient is defined as
∇θtJ(θt)=Eπθt[∇θtlogπθt(st,at)Qw(st,at)].(13)
View Source

Next, a state-value function Vπθt(s) is used to reduce the variance when estimating the gradient, which is only related to the state and does not change the gradient. Thus, the policy gradient is redefined as
∇θtJ(θt)=Eπθt[∇θtlogπθt(st,at)Aπθt(st,at)],(14)
View Sourcewhere Aπθt(st,at)=Qπθt(st,at)−Vπθt(st) is the advantage function. Moreover, Vπθt(s) is updated by the TD learning, where the TD error is defined as
δπθt=Rt+βVπθt(st+1)−Vπθt(st).(15)
View Source

To improve the training efficiency, multiple DRL agents work simultaneously and update their policy parameters of job scheduling asynchronously, as shown in Algorithm 2. Specifically, a certain number of DRL agents are initialized with the same local parameters of neural networks (i.e., the scheduling policy) and interact with their corresponding environments of cloud datacenters. For each DRL agent, the gradients are accumulated periodically in the actor's and critic's networks and the asynchronous update is executed for the parameters in the global network by using gradient ascent via RMSProp optimizer [22]. Next, each DRL agent pulls the latest parameters of the actor's and critic's networks from the global network and uses them to replace the local parameters. Based on the updated local parameters, each DRL agent will continue to interact with its corresponding environment and independently optimize its local parameters of scheduling policy. Note that there is no coordination among these DRL agents during the local training process. The A3C-based method will be kept training by using the asynchronous update mechanism among multiple DRL agents until the results converge.

SECTION Algorithm 2.The Asynchronous Update of Policy Parameters of Job Scheduling in Each DRL Agent
Initialize: The global and local parameters (θ and θ′) for actor's networks, the global and local parameters (w and w′) for critic's networks.

for i=temp−u,temp−u+1,…,temp do

Accumulate gradients in the actor: dθ←dθ+∇θ′logπθ′(si,ai)(Ri−Vw(si));

Accumulate gradients in the critic: dw←dw+∂(Ri−Vw(si))2/∂w′;

end

Update global parameters by using gradient ascent via RMSProp: θ=θ+γadθ, w=w+γcdw;

Synchronize local parameters: θ′=θ, w′=w;

Reset gradients: dθ←0, dw←0;

SECTION 5Performance Evaluation
In this section, we first describe the settings and datasets in our simulation experiments. Next, we evaluate the performance of the proposed method and conduct comparative experiments with other baselines.

5.1 Settings and Datasets
The proposed model of cloud resource allocation is implemented based on the TensorFlow 1.4.0. A cloud datacenter is simulated with 50 heterogeneous servers, where the fraction k of energy consumption for an idle server is set to 70% and the maximum energy consumption Pmax of a server is set to 250 W [37]. Therefore, the energy consumption of a server is distributed between 175 W and 250 W with the increase of resource usage from 0% to 100%. Moreover, the real-world trace data from Google cloud datacenters [8] is used as the input of our proposed model. The datasets contain the resource usage data of different jobs over 125,000 servers in Google cloud datacenters during May 2011. More specifically, 50 servers are first randomly extracted from Google datasets over 29 days, where each server consists of around 100,000 job traces. Next, several essential metrics are extracted from each job trace, including machine ID, job ID, start time, end time, and the corresponding resource usage. For example, Figs. 5 and 6 depict the per-day and per-minute resource (CPU and memory) usage of a server, and they reflect the ever-changing resource demands of jobs at different times. The job duration is assumed to be known before the scheduling. This assumption is reasonable because users commonly specify the requirements of their jobs (including resource usage and job duration) when they want to utilize cloud resources to execute their jobs, which enables cloud datacenters to allocate the resources correspondingly. In addition, the length of job sequence is set to 1000.


Fig. 5.
Varying CPU demands of jobs at different times.

Show All


Fig. 6.
Varying memory demands of jobs at different times.

Show All

During the training process, 10 DRL agents are used to implement the asynchronous update of policy parameters. In each DRL agent, the job trace data is fed in the proposed model by batches, where the batch size is set to 64. As for the design of DNNs, two fully-connected hidden layers are built with 200 and 100 neurons, respectively. Moreover, we set the maximum number of epochs as 1000, the reward decay rate λ as 0.9, and the critic's learning rate γc as 0.01. Based on the above settings, extensive simulation experiments are conducted to evaluate the performance of the proposed A3C-based cloud resource allocation method.

To analyze the effectiveness and advantage of the proposed method for cloud resource allocation, extensive comparative experiments are conducted. On one hand, the performance of two advanced DRL-based methods (i.e., PG [20] and DQL [21]) are assessed. On the other hand, five classic algorithms are also evaluated as follows.

Random. Jobs are executed by a random order of job durations.

Longest job first (LJF) [25]. Jobs are executed by a decreasing order of job durations.

Shortest job first (SJF) [25]. Jobs are executed by an increasing order of job durations.

Round-robin (RR)[26]. Jobs are executed fairly in a circular order, where time slices are employed and assigned to each job in equal portions.

Tetris [27]. Jobs are executed based on their resource demands and the availability of system resources at the moment they arrive.

5.2 Convergence Results
To evaluate the convergence of the proposed A3C-based cloud resource allocation method, the impact of two essential parameters is investigated, including the TD error discount factor β and the actor's learning rate γa.

First of all, the value of TD error discount factor β is changed with the constant actor's learning rate γa=0.001. As shown in Fig. 7, higher total rewards and faster convergence (around 400 training epochs) are achieved when β is set to 0.9. This is because the proposed method with β=0.9 makes better use of recent rewards to guide the actor's network and thus better actions are chosen along with the right direction for higher total rewards. Therefore, β=0.9 will be used in the following experiments.


Fig. 7.
Convergence versus different TD error discount factors.

Show All

Next, the constant TD error discount factor β=0.9 is used to analyze the convergence of our proposed method with the different values of actor's learning rate γa. As shown in Fig. 8, when γa is set to a large value (e.g., 0.1 or 0.01), high total rewards can be obtained in few training epochs. However, the algorithm converges to the local optimum in this case, and thus it can no longer learn a more optimized policy. By contrast, when γa is set to 0.001, it only takes around 400 training epochs to achieve higher total rewards than the above two cases. When γa decreases to 0.0001, the learning curve always fluctuates strongly with the increase of training epochs, and it is hard to reach a smooth convergence in this case. Thus, γa=0.001 is more suitable for the next experiments than other values.


Fig. 8.
Convergence versus different actor's learning rates.

Show All

5.3 Comparison Under Single-Objective Optimization
In this subsection, the proposed A3C-based cloud resource allocation method is first evaluated by several performance metrics, including the total rewards, QoS (normalized average job latency and job dismissing rate), and energy-efficiency (average energy consumption of jobs), under different cases with various average system loads. Next, the proposed method is compared with some classic resource allocation methods, including LJF, Tetris, SJF, and RR, under single-objective (QoS) optimization.

As shown in Fig. 9, the total rewards (represent the QoS) generally declines with the increase of average system load. In this case, the proposed method can always achieve higher total rewards than other methods even if the average system load becomes higher. By contrast, other classic methods present comparable performance only when the average system load is less than 1.2. Especially, when the average system load is over 2.0, the performance of these classic methods is only slightly better than the random scheme. The LJF method even performs worse than the random scheme when the average system load is over 2.4. This is because that a large number of jobs are waiting to be processed when the average system load is high but the LJF method always schedules the job with the longest job duration in priority, which results in the excessive waiting of many jobs and seriously degrades the scheduling performance. By contrast, the proposed method always maintains excellent performance. The results verify the advantage of the proposed method in scheduling jobs under complicated environments with high system loads.


Fig. 9.
Total rewards of different resource allocation methods with various system loads under single-objective optimization.

Show All

As shown in Figs. 10a and 10c, the proposed method obtains both the lowest normalized average job latency and job dismissing rate among all these methods. Especially, the performance gap becomes larger with the increase of average system load. This also verifies the strong adaptiveness of our proposed method in dynamic cloud environments with changeable average system loads. Besides, the average job energy consumption is also measured in this case, and the comparisons are conducted among these methods under the case of single-objective optimization. As shown in Fig. 10b, the proposed method leads to the higher average energy consumption of jobs when the average system load stays low (i.e., less than 2), although more energy consumption can be reduced when the average system load is over 2.


Fig. 10.
Comparison of performance metrics among different resource allocation methods under single-objective optimization.

Show All

5.4 Comparison Under Multi-Objective Optimization
In this subsection, the comparative experiments are conducted between the proposed method and other classic methods for cloud resource allocation under multi-objective (QoS and energy-efficiency) optimization. As shown in Fig. 11, the total rewards (represent the weighted sum of the QoS and energy-efficiency) degrade with the increase of average system load. This is because the growing and changeable demands from user jobs increase the complexity of cloud resource allocation. In this case, the proposed method obtains much higher total rewards than other classic resource allocation methods when the average system load is high. Especially, when the average system load is over 1.6 with more complicated system states, the performance improvement achieved by the proposed method becomes more obvious. This is because the proposed method is of good ability to find a better trade-off between the QoS and energy-efficiency during the job scheduling process.

Fig. 11. - 
Total rewards of different resource allocation methods with various system loads under multi-objective optimization.
Fig. 11.
Total rewards of different resource allocation methods with various system loads under multi-objective optimization.

Show All

As shown in Figs. 12a and 12c, the proposed method outperforms other resource allocation methods in terms of both normalized average job latency and job dismissing rate. This verifies that the proposed method has the excellent stability of maintaining the superior QoS in the cases of both single-objective and multi-objective optimizations. Moreover, Fig. 12b depicts the energy-efficiency of different resource allocation methods, where the proposed method can always attain the lowest average energy consumption of jobs among these methods with the increase of average system load. Thus, the proposed method makes up for the defects that occur in Fig. 10b under the case of single-objective optimization. This is because that the energy-efficiency is integrated in our DRL-based scheduling method, and thus both the QoS and energy-efficiency are well considered during the job scheduling. The above results demonstrate the advantageous performance of our proposed method in improving the QoS and energy-efficiency.


Fig. 12.
Comparison of performance metrics among different resource allocation methods under multi-objective optimization.

Show All

5.5 Comparison Among Different DRL-Based Methods
In this subsection, the performance comparison between the proposed A3C-based method and two advanced DRL-based methods for cloud resource allocation is conducted under multi-objective optimization, where the average system load is 1.2. As shown in Fig. 13, the proposed method can always achieve higher total rewards than the other two DRL-based methods during the training process of resource optimization. Moreover, the learning curve of the proposed method tends to converge after around 200 training epochs. However, the PG-based and DQL-based methods respectively require around 800 and 400 training epochs to reach a relatively-smooth convergence. As shown from Figs. 14a, 14b, 14c, and 14d, the proposed method can achieve the better QoS (normalized average job latency and job dismissing rate) and higher energy-efficiency (average energy consumption of jobs) compared to the other two DRL-based methods. Therefore, the above results demonstrate the excellent performance and high training efficiency of the proposed method. This is because the proposed method is able to effectively avoid large variance by using the advantage function when estimating the policy gradient. Meanwhile, the efficient convergence can be achieved by using the asynchronous update mechanism among different DRL agents.


Fig. 13.
Total rewards of different DRL-based methods under multi-objective optimization when the average system load = 1.2.

Show All


Fig. 14.
Comparison of performance metrics among different DRL-based methods.

Show All

Finally, the detailed performance metrics of the proposed A3C-based method and other classic methods for cloud resource allocation are exhibited when the average system load is 1.2. As shown in Table 2, the proposed method reduces over 3% normalized average job latency and 27% job dismissing rate than the DQL-based DRL method, which performs best among other methods in terms of these two metrics. The PG-based DRL method results in the worst energy-efficiency (average energy consumption of jobs) among all these methods. This is because the PG-based DRL generates high variance when estimating the policy gradient. Therefore, it cannot achieve a good load balancing among different servers. Consequently, it will result in high loads and low utilization on servers, causing excessive energy consumption. By contrast, around 9% of the average energy consumption is saved by using the proposed method compared to the RR method, which leads to the lowest average energy consumption of jobs among other methods. Moreover, the proposed method can achieve excellent QoS and energy-efficiency simultaneously. This is because that the training efficiency of the proposed method is greatly improved by using the asynchronous update of policy parameters among multiple DRL agents. Therefore, the proposed method can efficiently approach the global optimal and well guarantee the QoS and energy-efficiency simultaneously.

TABLE 2 Performance Metrics (Normalize Average Job Latency, Average Energy Consumption of Jobs, and Job Dismissing Rate) With Average Load = 1.2

SECTION 6Conclusion
In this paper, we first formulate the resource allocation issue in cloud datacenters as a model-free DRL problem with dynamic system states and various user demands. Next, we propose an A3C-based resource allocation method to effectively schedule jobs for improving the QoS and energy- efficiency in cloud datacenters. The extensive simulation experiments using real-world trace data from Google cloud datacenters demonstrate the effectiveness of the proposed method in achieving adaptive and efficient resource allocation. More specifically, the proposed method outperforms the classic resource allocation methods (i.e., LJF, Tetris, SJF, RR, PG, and DQL) in terms of the QoS (normalized average job latency and job dismissing rate) and energy-efficiency (average energy consumption of jobs). Moreover, the proposed method works better than the others with the increase of average system load, and it can achieve higher training efficiency (faster convergence) than two advanced DRL-based methods (i.e., PG and DQL). The simulation results show the great value of the proposed method for improving resource allocation in cloud datacenters.

In our future work, we plan to first extend the proposed model to consider the jobs’ priority order. Specifically, we will need to redefine the state space, action space, and reward function, taking into account the jobs’ priority order. For example, the actions of scheduling the high-priority jobs can lead to better rewards, thus guiding the algorithm to learn scheduling policies considering the jobs’ priority order. Next, we intend to design a new query-aware database parameter tuning method using an advanced DRL model built on this work. Through feeding the features of query information, the DRL model could learn the relations among database states, queries, and configurations to realize the automatic parameter tuning. Moreover, we will try to improve the generalization of the proposed DRL-based resource allocation scheme by developing an automatic data augmentation technique, which aims to regularize policies and value functions with respect to various state transitions and thus allows the DRL agent to capture task invariances and learn useful behaviors when the environment changes.