Abstract
Deep learning base solutions for computer vision made life easier for humans. Video data contain a lot of hidden information and patterns, that can be used for Human Action Recognition (HAR). HAR can apply to many areas, such as behavior analysis, intelligent video surveillance, and robotic vision. Occlusion, viewpoint variation, and illumination are some issues that make the HAR task more difficult. Some action classes have similar actions or some overlapping parts in them. This, among many other problems, is the main reason that contributes the most to misclassification. Traditional hand-engineering and machine learning-based solutions lack the ability to handle overlapping actions. In this paper, we propose a deep learning-based spatiotemporal HAR framework for overlapping human actions in long videos. Transfer learning techniques are used for deep feature extraction. Fine-tuned pre-trained CNN models learn the spatial relationship at the frame level. An optimized Deep Autoencoder was used to squeeze high-dimensional deep features. An RNN with LSTM was used to learn the long-term temporal relationships. An iterative module added at the end to fine-tune the trained model on new videos that learns and adopt changes. Our proposed framework achieved state-of-the-art performance in spatiotemporal HAR for overlapping human actions in long visual data streams for non-stationary surveillance environments.

Introduction
With the rise of digital gadgets and networking capabilities, social interaction platforms have become an integral part of our life. As of Jan 2012, more than 60 h of video data were uploaded to YouTube, and as of May 2019, more than 500 h of video content were uploaded to YouTube every minute [1, 2]. By the year 2022, 85% of internet traffic will be from video data streams [3]. This large number of video data led us to explore different application areas, understand and utilize them in making human life easier. Video data contain more information than static images. They contain multiple frames and an additional temporal component. The human action recognition task includes the identification of various activities performed by humans in video data streams. The human action recognition problem mainly focuses on the questions that are: ‘What actions to detect and recognize?’ and ‘Where the actions are being performed in the video data stream?’. There is a local context in video data-streams, that is: what action is being performed in a frame? This local context comprises spatial information of action. There is another global context for video data streams, that is: how to make sense from continuous data streams for action recognition? This global context covers the temporal information associated with an action, sub-action, or event. So, there is a lot of hidden information in a video data stream that helps in action recognition, including spatial–temporal information and their relationship. The human action recognition problem can be decomposed into several categories: human to human actions, human and objects interactions, body motion of humans only, sub-activities or atomic actions, gestures, group activities, events, and behaviors. Atomic actions or activities like walking and running are less complex and are easier to recognize. However, activities like cooking are a combination of many sub-activities which makes them complex and difficult to recognize.

Most of the video datasets present for action recognition consists of an actor-centric scene in which the actor can act freely. The application areas of human action recognition are wide. Some of the application areas are fraud detection, suspicious or abnormal behavior detection, behavior monitoring for elderly persons, intelligent video surveillance, human–computer interaction systems, and robotic vision. Even though a lot of research has been done, but there are still some problems in Human Action Recognition (HAR). Challenges in action recognition include dynamic nature and variation in new data streams, occlusion, viewpoint variation, background clutter issues, illumination, scale changes, and computational cost [4]. Inter- and intraclass activities' similarities also make the task more challenging. Different people perform the same action differently. The activities between different classes look similar because they share different information. Further, some parts of these activities may be overlapping or similar and make classification difficult. For sports activities, skipping rope and jumping for football headshot have some overlapping actions among them. As both the activities have the same action pose in their initial frames. Capturing long context in non-stationary visual data streams is also a difficult task. Sometimes, the extension of periodic temporal interval differentiation of 2 distinct activities is much harder to distinguish.

With the rise of deep learning architectures, researchers proposed different solutions to design deep learning approaches to the spotlight for video classification and recognition tasks [4, 5]. Neural network-based methods can automatically extract features from visual data streams. Convolutional neural network (CNN) learns features directly such that its initial layers extract the local features and the last layers of CNN learn the global features in visual data streams that describe high-level semantics information. In recent years, researchers proposed many different deep learning architectures for action recognition in videos. In this paper, we propose a spatiotemporal HAR framework in visual data streams. Capturing long context and similar or overlapping actions recognition are the two main issues that make the task of human action recognition more challenging. Many studies showed that the use of a pre-trained CNN model reduces the computational cost for training and achieved high accuracy in image classification, video summarization, and fire detection. Therefore, we have utilized the concept of transfer learning and extensively investigated seven pre-trained CNN models. By using transfer learning concepts, we used these pre-trained CNN models for deep feature extraction. These deep features extract the frame-level local and global features from video data and then learn the spatial relationships for each frame. These frame-level deep features are high-dimensional data. Because in continuous data streams, an action is consisting of a sequence of motion patterns. Therefore, we designed an optimized Deep Autoencoder (DAE) to squeeze these high-dimensional deep features. For capturing long context and learn the hidden temporal information in visual data streams, we used and Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM). The proposed framework learns the hidden spatial and temporal information and their relationship for capturing long context and also accurately classification of overlapping actions. The main contributions of this paper are summarized as:

We propose an efficient and optimized human action recognition framework for visual data streams that utilize the knowledge of seven pre-trained CNN models for deep features extraction for frame-level spatial information extraction.

An optimized deep autoencoder is designed that squeezes the high-dimensional deep features and learn to represent frame-level actions in a small latest space

We used a Recurrent Neural Network (RNN) with LSTM to learn temporal information from the low-dimensional features plane

We propose an iterative fine-tuning module in which the previously trained model adopts the changes in the new video.

The remaining of the paper is structured as follows: Sect. 2 briefly describes the literature review of related work in action recognition. Section 3 presents the proposed framework in detail. In Sect. 4, the performance of our proposed model for all 18 overlapping groups with discussion is presented. In the last, Sect. 5 concludes the proposed framework with final remarks and also presents some future directions.

Literature review
Karpathy et al. [4] proposed a framework using multiple consecutive frames to fuse hidden temporal information in 2D pre-trained CNN. For single-frame, single Conv net architecture was used, and at the last stage, it fuses information from all frames. Simmoyan and Zisserman [5] proposed an extension of the previous work by Karpathy et al. by proposing two-stream networks. Their model overcomes the issues of learning motion features, faced by deep learning using stacked optical-flow-vectors of motion features. The long-range temporal context learning was not present because the predictions were generated by taking the averages of predictions for sampled video clips. This method also required computed optical-flow vectors in advance. Donahue et al. [6] presented a Recurrent Convolutional Network by placing a Long Short-Term Memory (LSTM) decoder block after the encoder convolution block. They use an end-to-end training architecture for action recognition. The comparison of RGB and optical-flow information with 16 frame samples from clips as input to architecture was also presented. The long-range temporal context challenge remained unsolved. Tran et al. [7] developed a 3D Convolutional Network (C3D) for spatial and temporal features learning in video data. They also presented the use of de-convolutional layers for video data streams. Although their proposed architecture worked better than existing methods, long temporal context learning was still unsolved. Feichtenhofer et al. [8] achieved improved performance in action recognition in videos by using two-stream architecture without any substantial rise in the size of training parameters. For actions like brushing hair and teeth-brush, the first spatial network extracted spatial information that whether there are hair or teeth involved in an activity.

Wang et al. [9] proposed a two streams network architecture Temporal Sequence Network (TSN) and performed well on long temporal context learning. Girdhar et al. [10] introduced an aggregation mechanism of learnable features by using max pooling or average pooling. Bag of action (visual) words from each of the two streams were then encoded from the output. Zhu et al. [11] proposed a novel architecture for pre-generation of optical flow for every frame sample in videos using an unsupervised methodology. The field of flow for two consecutive frames generated using CNN. After that, from the generated field-of-flow a single frame, the next consecutive frame can be reconstructed such that the existing frame and the new generated frame difference can be minimized using inverse warping technique. Diba et al. [12] developed a Temporal Transition Layer (T3D) an extension to Inflated 3D Convolutional Neural Network (I3D) utilizing a 3D-DenseNet deep architecture. To get diverse temporal depths they stacked multi-temporal-depth pooling layer after the last dense blocks in DenseNet. Girdhar et al. [13] presented an attention pooling mechanism for human action recognition. The human interaction with other humans of objects was detected by adding human pose information as extra supervision for the model. Zheng et al. [14] proposed a novel attention network architecture for local and global knowledge awareness. The authors also proposed a Global Attention Regularization (GAR) mechanism. Long et al. [15] introduced an attention clusters-based local-features integration mechanism for the classification of video data. The authors showed that it is not always necessary to use long-term temporal information. This model achieved slightly higher performance for capturing spatial relationships.

Girdhar et al. [16] proposed a novel Transformer Network (TN) architecture for action classification. The proposed model almost fails to learn similar or overlapping and long temporal context actions in video data. Yu et al. [17] developed an action recognition framework for atomic and interacting spatiotemporal actions. The proposed Discriminative Deep Model Long Short-Term Memory (D2M LSTM) used a feature fusion technique for recognizing similar/overlapping actions in real-time settings. Liang et al. [18] presented a sub-action detection and representation learning multi-model mechanism. Long visual data streams were decomposed into multiple segments containing overlapping sub-actions. Nazir et al. [19] proposed a Deep Residual Network base mechanism for extracting motion and long temporal features. After that, a multi-kernel-based Support Vector Machine (SVM) was used to perform action learning from extracted deep features. The method of Dense Trajectories improved the proposed model performed very well. Zhang et al. [20] presented a supervised learning Minimum Effort Temporal-Activity-Localization (METAL) mechanism for un-trimmed video data. Yang et al. [21] introduced the progressive learning mechanism for spatiotemporal action recognition.

The spatial refinement module refines the localization of a person acting while simultaneously the temporal extension module increases the length of temporal context. Ji et al. [22] developed a 3D Convolutional Neural Network (CNN) for detecting the ending of human actions. Information from a few successive frames was extracted for spatiotemporal dimensions. The main focus of this work was on the detection of ending actions that further can help in the differentiation of similar human actions.

Proposed framework
This section presents the proposed system in detail, as presented in Fig. 1 The seven Deep Learning Pipeline (DLPL) proposed in our system containing a pre-trained CNN from one of the: DenseNet201, InceptionV3, ResNet101V2, ResNet152V2, VGG16, VGG19, Xception, with fine-tuning performed on them. Next, the pre-trained model is followed by a Deep Autoencoder (DAE) and then, at last, an RNN with LSTM. These 3 networks make a DLPL, while the experimental settings and results of these DLPL for all of the 18 groups are provided in this work. Table 1 presents the details for the proposed Deep Learning Pipelines (DLPL).

Fig. 1
figure 1
The proposed spatiotemporal human action recognition (HAR) framework

Full size image

Table 1 Proposed deep learning pipelines (DLPL)
Full size table

Data pre-processing
In data pre-processing, we grouped those action classes that have some similar portion or have some overlapping actions between them in the UCF-101 dataset. The total number of groups containing overlapping actions is 18 for the UCF-101 dataset and each group consists of action classes ranging from 2–5. All 18 groups with overlapping or similar action classes are given in Table 2. After grouping overlapping action classes, frames from each of the videos are extracted and then reshaped according to input shapes required for each of the seven pre-trained CNN models. The frames were rescaled into dimensions (224, 224, 3). For InceptionV3 and Xception, the frames were rescaled into dimension (299, 299, 3) for DenseNet201, ResNet101V2, ResNet152V2, VGG16 and VGG19 pre-trained CNN models. We used standard train-test split-list-1 provided on the UCF website.

Table 2 Overlapping and similar action classes groups
Full size table

Transfer learning
Transfer learning is a technique in machine learning that learns useful information and hidden patterns in one specific domain, and then, utilizing and transferring this gained knowledge to another different but related domain. In transfer learning, we exploit what the model has learned for one task can be further generalized to another related problem. Here, training a model for one task can become the starting point for the training of another model for a related task. These pre-trained models in transfer learning are less computational and fine-tuning these pre-trained models are very fast as compared to training a model from scratch. Another benefit of using transfer learning is that they need only a small amount of data because pre-trained models have already been trained on a very large dataset for a long time and with a lot of optimization strategies. Among many other strategies to use transfer learning, one of them is to remove the last classification layers and add new layers then train and fine-tune the pre-trained model and extra layers added at the end on the new dataset. Here, domain  includes a feature-space  and a Marginal Probability Distribution (MPD) . Here,

For a domain , where

It includes a label  and a predictive objective function  where for a new instance of x, the  is predicting . The task is represented as:

This task  then learned from each training pair from  and . The training domain 
 and the learning task represented as 
. The target domain represented as 
 and the learning task represented as 
 while for the transfer learning problem, 
 and 
 should be fulfilled. Now, the goal of transfer learning is to utilize the knowledge acquired in 
 and in 
, and improve the learning of the given predictive target function 
 in 
 [23, 24].

Deep features extraction
Video data have a huge amount of useful information hidden in visual data streams. These include color intensity changes, edges and shapes, motion and texture patterns, and long-term temporal contextual information. Deep learning-based methods have proven to be more useful in video and speech content analysis [25], images [26], and also in extraordinary representational capabilities [27, 28]. To take benefits of transfer learning, we used seven pre-trained CNN models: DenseNet201, InceptionV3, ResNet101V2, ResNet152V2, VGG16, VGG19, and Xception. These models are used for extracting deep features at the frame level that learn spatial patterns and relationships in visual data streams. These models were trained on the ImageNet dataset of around 1.2 million color images of 1000 different classes [29]. Deep features are the features in high-dimensional space that represent both local as well as global features of spatial information in video frames. Traditional computer vision methods for feature extraction required hand-engineering and are slow and less efficient as compared to new deep learning-based methods. These methods include Harris Corner Detection (HCD), Shi-Tomasi Corner Detection (STCD), Scale Invariant Feature Transform (SIFT), Speeded-Up Robust Features (SURF), Features from Accelerated Segment Test (FAST), Binary Robust Independent Elementary Features (BRIEF), Oriented FAST and Rotated BRIEF (ORB) [30,31,32,33,34].

In this work, we performed extensive experiments on pre-trained CNN models. We removed the last few Fully Connected Layers (FCL) at the end of these models and added some extra layers, so that, the model can be trained for our dataset. After adding the layers at end of the network, we fine-tuned the model, so that, it can be used for action recognition tasks from videos. To extract deep features, the output before the last fully connected SoftMax layer is captured and provided to the next network for representation learning. For a convolution layer of (5 × 5) kernel size, their output is a (150 × 150) dimension for 200 feature maps and using the same pooling with stride value of 1. For and input image of size (150, 100, 3) and RGB image with 3 color channels, total parameter would become (5 × 5 × 3 + 1) × 200 = 15,200 where 1 is used for bias. Every 200 feature maps consist of a (150 × 100) number of neurons and it calculates the weighted sum for every neuron with (5 × 5 × 3) = 73 inputs. For 32-bit float values, this becomes a total of (200 × 150 × 100 × 32) = 96 million bits (12 MB) for each one of the instances in the dataset. At the prediction stage, one layer releases the RAM when its computation is completed for the next layer but for training purposes, every computation performed in a forward pass must be preserved for a reverse pass.

To detect the position of a specific object from a 2D image, the weighting function can be written as follows:


 
 

(4)

here  denoted the position of in time  and  is the convolutional kernel. The position of the object can be calculated as a Simple Moving Average (SMA) using convolution. The kernel for SMA can be written as:


 
 

(5)

here is the window size for a simple moving average. Convoluted image for the 2D image can be computed as follows:


  
 
 

(6)

here  is the 2D image on which the convolution is being performed,  is the 2D smoothing kernel. The output of a single neuron in the convolution layer can be computed as:


 
 
 
 

(7)


 
 

(8)

For a feature map,  of a convolutional layer  is 
 with  row and  column, 
 is the vertical stride, while 
 is the horizontal stride, 
 is the height of the receptive field, and 
 is the width of the receptive field. Further, 
 is the previous layers’ number of the feature map, 
 is the neuron output at previous layer for 
 row, 
 column and 
 feature map or channel. Biasness is represented by 
 and 
 is represented as the connection weight between a neuron in feature map  and feature map 
 for the row  and column  of layer .

DenseNet201 CNN model
Densenet201 [35] is a deep convolutional neural network (CNN) architecture that has 201 layers. The pre-trained densenet201 model is trained on the ImageNet dataset of more than one million color images from 1000 different classes. This pre-trained model takes an input image of size 224 by 224. The DenseNet resolved the vanishing gradient problem for deep neural networks. Vanishing gradient is the problem that arises in deep networks because the path to reach from input to output becomes too large. In this case, the gradient calculation in opposite direction becomes so small that it vanishes before reaching another end.

DenseNet resolved this problem by simplifying connectivity among layers by maximizing gradient (information) flow. It introduced the concept of feature reuse by connecting each layer with every other subsequent layer of the network. This strategy reduces the redundant feature maps and the resulting model requires fewer parameters. In DenseNet of  layers, the  layer of the network accepts the feature maps from all previous layers as input. So, the layer transition becomes:


 

(9)

where 
 is the nonlinear transformation, 
 denotes the output of  layer and 
 denotes the concatenated feature maps.

The 
 is a composite function of 3 successive operations: batch-normalization (bn), a rectified linear-unit (ReLU), and (3 × 3) convolution layer. For down-sampling, the network was divided into several densely connected blocks, where each block consists of several layers, and also there is a connection from each layer to all the subsequent layers. The layer between each dense block is named as transition layers that comprise a batch-normalization, a (1 × 1) convolutional layer, and at the end a (2 × 2) average pooling layer. To reduce the computational cost and the total number of input feature maps, a (1 × 1) convolutional layer was used before every (3 × 3) convolutional layer as a bottleneck layer. Each of these bottleneck layers generates a 4 k feature map. For further reduction of the feature maps, the subsequent transition layer of a dense block generates  feature-maps output  feature-maps in a dense block. Here,  is the model compression factor. There is no change in the number of transition layer feature-maps when . In densenet architecture, the value of 0.5 was set for . For the ImageNet dataset, the DenseNet model has 4 dense blocks with (224, 224, 3) input shape images. There were 2 k convolutions of dimension (7 × 7) in the initial convolution layer with a stride value of 2. The value of  is 32 at the initial layer. Figure 2 illustrates the detailed architecture of the DenseNet model we used.

Fig. 2
figure 2
DenseNet201 architecture with extra layers added at the end for fine-tuning on UCF-101 dataset

Full size image

InceptionV3 CNN model
Inceptionv3 [36] is the third improvement of Inception architecture and it is also a complex 48 layers deep convolutional neural network architecture. The pre-trained Inception model is trained on the ImageNet dataset for 1000 different classes. This pre-trained model takes an input image of size (299, 299) with 3 (RGB) color channels. Inception resolved the problem of overfitting in deeper networks and also selecting the right filter size is also another problem along with the expensive computational requirements. Inception made the network architecture ‘wider’ instead of just making the network deeper by stacking more and more layers by introducing the use of multiple small kernels. In the initial Inception architecture, it used three kernels with different sizes as (1 × 1), (3 × 3), and (5 × 5) together with max-pooling. The output from each of these was then concatenated and forwarded to the next inception module. An auxiliary classifier with batch-normalization and SoftMax was deployed after 2 inception modules. The final loss was then weighted-sum of the auxiliary loss calculates on the same labels and the real loss. Alteration of dimensions using small filters also had several drawbacks, such as loss of some information. This issue was solved in Inception version 2 and version 3. This goal was achieved by factorizing a (5 × 5) convolution into 3 convolutions of size (3 × 3). Furthermore, another factorization of (n × n) was used into 2 (1 × n) and (n × 1) convolutions. Another factorization of (7 × 7) convolution into 3 (3 × 3) convolutions was used. This setting made it possible to use fewer parameters at training time, which in turn made Inceptionv3 a more robust network architecture. Figure 3 illustrates the detailed architecture of the Inceptionv3 Model we used.

Fig. 3
figure 3
InceptionV3 architecture with extra layers added at the end for fine-tuning on UCF-101 dataset

Full size image

ResNet101V2 and ResNet152V2 CNN model
Resnet101v2 and Resnet152v2 [37] are improved versions of deep residual networks. In this work, we used 2 variants of residual network version 2. Resnet101 is 101 layers deep and ResNet152 is 152 layers deep convolutional neural network architectures. Pre-trained models of these networks trained on ImageNet with 1000 classes are used for experiments. Both network models take images of input sizes of (224, 224) with 3 (RGB) color channels. Resnet used the idea of skip connections also names fast forward connections. It is the concept of feeding earlier layers to further deeper layers before applying the ReLU activation. This skip connection or shortcut makes the network less risky of vanishing gradient problems. The residual or identity block in Resnet contains both a forward connection and a fast-forward connection. In Resnet, there are multiple residual units stacked together. These units can be represented as:

here 
 is the input and 
 is the output of  unit.  is the number of residual units’ amount.  represents a residual function and  is a Relu activation, 
 is the weight and bias for 
 residual unit. While 
 is the underlying identity mapping. The goal is to lean the  residual-function for 
, using 
 which was achieved using a skip-connection. The improvement to v2 is also to develop a path inside a residual block and also in the whole network for information propagation purposes. For 
 and 
, both identity mappings, the information could be broadcast within the whole network. The above 2 settings improved network performance very well. For a deeper unit  and a shallow unit  , we have:


 
 

(13)

 is the summation of 
 and all preceding residual functions’ output. The back-propagation operation can be represented as:


 
 
 
 
 
 
 

(14)

 is the loss function. Figure 4 illustrates the detailed architecture of the resnetv2 model we used.

Fig. 4
figure 4
ResNet101V2 and ResNet152V2 architecture with extra layers added at the end for fine-tuning on UCF-101 dataset

Full size image

VGG-16 and VGG-19 CNN models
A (VGG) [5] is a deep convolutional neural network. We used 2 variants of VGG network architecture, and for VGG-16, it has 16 layers deep, and VGG-19 is 19 layers deep architecture. We used a pre-trained model and weights of these models trained on the ImageNet dataset. Both pre-trained models take the input of (224, 224, 3) RGB images. To start the VGG model take the images as input and pass them over to a stack of convolutional layers with a filter with (3 × 3) receptive field were used for convolution. Stride and padding values were set to 1 and for spatial pooling 5 max-pooling layers were also used while for max-pooling, the stride size was set to 2. The stack of convolution layers was then followed by 3 Fully Connected Layers (FCL) and the 2 FCL have 4096 channels each. The last fully connected layer outputs the 1000 classes with a SoftMax at the end for the ImageNet dataset. Images were input into 2 consecutive convolutional layers resulting in 64 filters of (3 × 3). After that, a max-pooling layer with a stride value of 2 was added. The second block also contains 2 convolutional layers of 128 filters of (3 × 3) receptive field. Then again, the same max-pooling layer was added as previously. The third block contains 3 convolutional layers of 256 filters of (3 × 3) receptive field for VGG-16. For VGG-19, there were 5 convolutional layers in the third block with the same filters and receptive field as VGG-16. Then a max-pooling with the same configuration as the first max-pooling was added. For the 4th block there were 3 convolutional layers for VGG-16 and VGG-19 there were 5 convolutional layers with 512 filters of receptive filed of (3 × 3). Then , a max-pooling is applied as previously. The next block configuration remains the same as the 4th block and a max-pooling at the end. At last, 3 fully connected layers as described earlier with 50% dropout before the first and second fully connected layers were added with a SoftMax at the end. ReLU activation was used for both models for each convolution operation. Figure 5 illustrates the detailed architecture of the VGG model we used.

Fig. 5
figure 5
VGG16 and VGG19 architecture with extra layers added at the end for fine-tuning on UCF-101 dataset

Full size image

Xception CNN model
eXtreme Inception (Xception) [38], inspired by original Inception architecture is a deep convolutional neural network with 71 layers. We used a pre-trained model and weights of this model trained on the ImageNet dataset. This pre-trained model takes the input of (299, 299, 3) RGB images. Depth-wise separable convolutions (n × n channel-wise spatial convolution) and a pointwise (1 × 1) convolution were introduced instead of Inception modules. It also use several skip connections similarly to Resnet. The data are input into the entry flow block, then it circulates in the middle flow block 8 times and at last, it goes through the exit flow block. Figure 6 illustrates the detailed architecture of the Xception model used in this research.

Fig. 6
figure 6
Xception architecture with extra layers added at the end for fine-tuning on the UCF-101 dataset

Full size image

Representation learning
For representation learning, we designed a deep autoencoder network.

Deep autoencoder
A neural network designed in a way that it can lean the representation from input data to reconstruct them to its output as close as possible to the original input is called an autoencoder. This process of reconstruction follows the learning process, not the remembering process to generate its output from input data in an unsupervised manner. A neural network with this architecture having multiple hidden layers is named Deep Autoencoder. The autoencoder consists of two parts: an encoder, whose responsibility is to learn the smallest representation for the input data that can further be used by the decoder, that reconstruct the output closest to original input data with the help of that internal representation generated by the encoder. The internal representation that an encoder generates is called latent space. The latent space comprises a probability distribution generated by an encoder that is then fed into the decoder. The goal of the decoder is then to generate output data from the latent space and generate the samples from the probability distribution that is close to the original input. This concept made it possible for us to use the autoencoders for Generative Modeling. The main interesting part for us in deep autoencoders is the latent space, not the output of the decoder. Usually, the latent space contains useful features in a smaller dimension than the original input. Learning this small dimension makes it possible to use to get the most salient features from the given training data. This low dimension latest space learning can be described as:

where  is penalized with the use of a loss function  to learn the representation similar to  input data. The model sometimes fails to learn the model when the learned latent space dimensions are smaller or equal to the input data dimension. For such a problem regularization mechanism with a loss, the function works better allowing the autoencoder to have other abilities as well rather than just remembering the input and copying to output. For certain tasks such as classification, a sparsity factor can be added to the autoencoder in addition to the output reconstruction error:

where  is the sparsity penalty applied to the encoder’s output  and  is the decoder output. The log-likelihood for the regularization can be described as follows:


 
 

(17)

 is the latent variables and  is the visible variables and 
 is the joint distribution with 
 is the prior distribution of the model over the latent variables.



The absolute sparsity penalty, Laplace-prior is:


 
 

(18)


 
 
 

(20)

where  is constant, while  is a hyperparameter term. The generalized autoencoder with sparsity added can be described and follows:


 

(22)


 

(23)


 
 

(24)


 

(25)

where the number of neurons in input  and the number of neurons in the output 
 are equal, while  is the patch.  is the input to autoencoder.

So, for a 5 × 5 patch, there are 25 pixels in there and each of these 25 pixels is the input to 25 neurons.  is the weight from input to hidden neurons in encoder and 
 is the counterweights of  in the decoder, b is the bias, and  is the loss function.

Our proposed deep autoencoder model first encodes the input data by multiplying and adding weights and biases with a nonlinear sigmoid or ReLU function. This encoded data are then passed through the decoder by using the same dimensions in reverse order. The mean-squared error was reduced by adjusting weights through a backpropagation mechanism.


 

(26)


 

(27)

here the input  is passed to the first hidden layer. All the successive layers get their input from their respective preceding hidden layer. The number for encoding is , 
 is the data, 
 is the weight and 
 is the biasness for the layer . The proposed deep autoencoder was trained for 100 epochs, L2 regularization was applied to prevent overfitting and 0.5 value of sigma  was used with sparsity regularization. Finally, L2 and sparsity regularization with mean-squared error (MSE) was used as a cost function. Figure 7 illustrates the Deep Autoencoder (DEA) architecture.

Fig. 7
figure 7
The architecture of deep autoencoder (DAE)

Full size image

Temporal context learning
For learning long-term temporal context, we used the Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM). It learns the temporal information from the sequence of frames.

Recurrent neural network (RNN)
RNN is a type of neural network that uses previous data for learning. The architecture of the RN model is presented in Fig. 8. The output from the preceding step is forwarded to the next step. The ‘memory’ in RNN keeps track of what has been learned previously. It performs a similar task for each input at every hidden layer by using the same parameters, which makes the parameters less complex. It keeps the weights and biases the same for all the layers that in turn results in the conversion of independent activations into the dependent activations. We input one time-step of input at a time to the network. The current state is computed using both the current input and the previous network state. This current internal state called hidden-state 
 then becomes the 
 for the next time-step. After competing for all time-steps similarly, the output is computed by the last state and then compared original output, and the error rate is computed. For updating weights and biases for the next iteration, this error is then backpropagated to the network. This whole mechanism of RNN allowed us to learn temporal features from the sequence of frames from visual data streams. The simplified RNN equations are as follows:


 

(28)

here  is the time-step, 
 is activation, 
 is output,  and  re activation functions. 
 are the temporal coefficients. Hidden state in RNN can be updated using:

where 
 is the current hidden state, 
 is the previous hidden state, 
 is the current input and 
 is the fixed function having trainable weights. For calculating error, through backpropagation, we use the following:

here 
 is the prediction of the network for a specific time-step, 
 is the actual output and 
 is the error for each time step. The total error then becomes:

Fig. 8
figure 8
RNN architecture

Full size image

The difference between traditional backpropagation and backpropagation through time is that to calculate the total error, errors at every time step are summed together.

Let  is a weight vector.


 
 
 
 
 
 
 
 

(34)

Long short-term memory (LSTM)
LSTM is one of the most successful algorithms to solve the vanishing gradient or exploding gradient problem in RNN which is presented in Fig. 9. An LSTM recurrent unit keeps track of the previously learned knowledge and tries to forget only irrelevant data. A vector named as internal-cell-state is then preserved by every LSTM recurrent unit. LSTM used 4 gates forget-gate which are responsible for ‘forget’ previous data amount using Sigmoid activation function. Input-gate or update-gate are responsible for the amount of information that needs to be stored in an internal cell-state and use the Sigmoid activation function. Input-modulation-gate or relevance is responsible for modulating information on which input-gate operates using a Hyperbolic-Tangent activation function. Output-gate is responsible for generating output from internal-cell-state  and uses a Sigmoid activation function.


 
 
 
 
 
 
 
 
 

(35)

Fig. 9
figure 9
LSTM architecture

Full size image

In this case, the gradient equation contains 
 instead of 
 for LSTM backpropagation. The simplified equations for LSTM look like this:


 

(36)


 

(37)


 

(38)


 

(39)



(40)

here  is a memory cell, 
 is the output activation at time-step , 
 is the candidate for replacing , 
 is the previous activation value, 
 is the current input,  is the weight,  is the bias. 
 is the update gate, 
 is the forget gate and 
 is the output gate. The value of these 3 gates resides between 0 and 1. The details for the parameters are presented in Table 3.

Table 3 Parameters setting for our HAR model
Full size table

Here,  is the sigmoid activation function and  is the hyperbolic tangent activation function, 
 is the updated memory cell value and 
 is the activation at time-step , and ‘*’ denotes the element-wise multiplication.

Iterative model training for new data streams
Due to the changing nature of video data over time, the previously trained model is not capable of adopting changes in video data and unable to correctly learn actions from new data streams. The proposed framework will update itself over new data, iteratively. It will adopt the variation in its surrounding environments. To retrain the model iteratively those data streams will be used whose predicted confidence score is greater than the certain threshold so that it can adopt varying conditions in its surrounding. By considering environmental effects, deployment settings, and user requirements thresholds can be selected. The previously trained model will be fine-tuned when enough data will be collected, so the model will adapt itself to the variations in the environment in new data. This model then can be applied to many other domains, patient activity monitoring, fraud, and abnormal behavior detection in the video for real-time surveillance. The detailed fine-tuning module architecture is shown in Fig. 10.

Fig. 10
figure 10
Iterative fine-tuning module for adopting changes in new videos

Full size image

Experimental results and discussion
In this section, we presented the results and a brief discussion of the experiments we performed on the UCF-101 actions dataset. We used a total of 48 action classes separated into 18 groups. We used ‘Python 3’ programming language on a Windows 10 system. The system specifications are: 
, ,  and . All of the deep learning architectures were implemented using TensorFlow 2.x framework with Keras API pre-integrated with TensorFlow. We evaluated the proposed system performance using five evaluation metrics: loss, accuracy, precision, recall, and f1-score.

Dataset
For experimentation and evaluation purposes, we used UCF-101 [39] dataset. UCF-101 is a benchmark action recognition dataset and it consists of ‘13,320’ videos and has 101 different action classes. These videos are collected from YouTube and available in ‘.avi’ video format. UCF-101 is a challenging and complex dataset because the actions are categorized into human-to-human interactions, support actions, the interaction of humans with different objects, musical instruments playing, and the single person performing actions that involve other parts of the human body. There are 450–2400 s duration of clips in an action class for all classes and on average of 25 frames-per-second with 3 channels makes the dataset almost comparable to ImageNet dataset [29, 39].

That is why the computational cost of training a simple 2D Convolutional Neural Network (CNN) on the UCF101 dataset is very high. UCF-101 dataset characteristics summary is given in Table 4. One frame sample of each 101 action class in the UCF-101 dataset is presented in Fig. 11.

Table 4 Summary of UCF-101 dataset characteristics
Full size table

Fig. 11
figure 11
One frame sample of each 99 action classes in the UCF-101 dataset [39]. The border color of each frame sample denotes the class category: blue color represents the human and object interactions, green color for sports actions, red color for body motion

Full size image

Performance evaluation
We evaluated the proposed system performance using five evaluation metrics: loss (categorical-cross-entropy), accuracy (overall performance), precision (positive predictions accuracy), recall (sensitivity or actual positive samples coverage), and f1-score.


 
 

(42)


 
 

(45)

here  represents the true-positive (number of classes correctly classified as positive),  is the true-negative (classes classified as negative, those are negative),  represents false-positive (classes classified as positive but are not positive),  is the false-negative (classes classified as negative but are not negative). We have applied categorical cross-entropy loss, which is a combination of SoftMax activation and cross-entropy loss.

The formulae are given below:


 
 

(46)


 
 

(47)

here 
 is the SoftMax activation, 
 is the CNN score and 
 is the ground-truth for every input class  in  number of output classes in multiclass classification, 
 is the input vector, and 
 here represents the standard exponential function for the output vector.

Results discussion
The overlapping classes in group 1 include ApplyEyeMakeup, ShavingBeard, and ApplyLipstick. The best performance achieved for group 1 is 0.9918 accuracy for DLPL-7, as shown in Table 5. The minimum accuracy achieved for group 1 was 0.9701 using DLPL 2. Group 2 contains three overlapping classes including BoxingPunchingBag, BoxingSpeedBag, and Punch. The best accuracy of 0.856 was achieved by DLPL 2 and the lowest accuracy score was 0.9623 for DLPL 7, as shown in Table 6. For group 3 with three classes: BalanceBeam, FloorGymnastics, StillRings, the accuracy of 0.9848 was achieved by DLPL 2 and the lowest accuracy of 0.9526 was achieved by DLPL 7, as shown in Table 7. Group 4 contains 2 overlapping classes include CliffDiving and Diving. A 0.9921 performance accuracy by DLPL 1 and the lowest accuracy of 0.9337 by DLPL 6, as shown in Table 8. Group 5 contains 2 overlapping classes including BlowDryHair and Haircut and we achieved the best performance of 0.9582 and the lowest 0.9241 accuracies, as shown in Table 9. The accuracy was less because these classes look more similar and both only contain a small part of the human body and face, while both actions were performed on hair. Group 6 contains 5 overlapping classes which include HighJump, LongJump, FloorGymnastics, JavelinThrow, and PoleVault. The lowest accuracy of 0.9215 and the best accuracy of 0.9385 were achieved, as shown in Table 10. The low accuracy was due to the factor that five classes are overlapping or similar actions in them. Therefore, the accuracy was less as compared to other groups. Group 7 has two overlapping classes including JumpingJack and JumpRope. The best accuracy of 0.9911 was achieved by DLPL 2, and the lowest accuracy of 0.9372 was achieved by DLPL 3, as shown in Table 11. Group 8 has three overlapping classes including BodyWeightSquats, CleanAndJerk, and Lunges. We achieved the best accuracy of 0.9896 accuracies by DLPL 4 and low accuracy of 0.9719 for DLPL 2, as shown in Table 12. Group 9 contains 2 overlapping classes including BandMarching, and MilitaryParade. We achieved 0.9811 of best accuracy by DLPL 5 and minimum accuracy of 0.9529 for DLPL 3, as shown in Table 13. Although the accuracy was good for these classes containing multiple actors, and for long video sequences, it reduces the performance because of multiple actors performing the same action. For UCF-101, it performed well because the videos were trimmed very well and due to the short length of the videos. Group 10 consists of 2 overlapping classes including BreastStroke and FrontCrawl. The accuracy was high with top accuracy of 0.9873 and the lowest accuracy of 0.9642, as shown in Table 14. The accuracy was high due to a sudden change in point-of-view for breast-stroke action make the two actions different at a later stage in videos.

Table 5 Performance for proposed deep learning pipelines for overlapping group 1
Full size table

Table 6 Performance for Proposed Deep Learning Pipelines for Overlapping Group 2
Full size table

Table 7 Performance for proposed deep learning pipelines for overlapping group 3
Full size table

Table 8 Performance for proposed deep learning pipelines for overlapping group 4
Full size table

Table 9 Performance for proposed deep learning pipelines for overlapping group 5
Full size table

Table 10 Performance for proposed deep learning pipelines for overlapping group 6
Full size table

Table 11 Performance for proposed deep learning pipelines for overlapping group 7
Full size table

Table 12 Performance for proposed deep learning pipelines for overlapping group 8
Full size table

Table 13 Performance for proposed deep learning pipelines for overlapping group 9
Full size table

Table 14 Performance for proposed deep learning pipelines for overlapping group 10
Full size table

Group 11 consists of 4 overlapping classes including CliffDiving, Diving, ParallelBars, and HandstandPushups. Our proposed model achieved the best accuracy of 0.9384 and the lowest 0.9215 for this group, as shown in Table 15. The accuracy was low as compared to other groups because it has four overlapping classes. The classes are similar to each other, thus making it hard to distinguish between them. Group 12 has 2 classes including BandMarching and PlayingDaf while achieved top accuracy of 0.9810 for DLPL 5 and the lowest accuracy of 0.9587 for DLPL 3, as shown in Table 16. The small number of classes in an overlapping actions group and less similarity make it possible to achieve high accuracy.

Table 15 Performance for proposed deep learning pipelines for overlapping group 11
Full size table

Table 16 Performance for proposed deep learning pipelines for overlapping group 12
Full size table

Group 13 consists of 4 classes including playing guitar, PlayingCello, PlayingSitar, and PlayingViolin. The accuracy was comparatively low because of the number of similar classes. The top accuracy of 0.9429 for DLPL 6 was achieved and the lowest accuracy was 0.9212 for DLPL 3, as shown in Table 17. Group 14, 15, and 16 each consist of 2 classes: PlayingPiano and Typing, RockClimbingIndoor and RopeClimbing, Rafting and Rowing, respectively. The accuracy of these groups is good enough because a similar number of classes are two, as shown in Tables 18, 19 and 20, respectively. The more similar classes in a group, the performance of the respective group decreases, respectively. Group 17 continuing 3 classes: Shotput, 'HammerThrow, ThrowDiscus achieved higher accuracy of top 0.9879 for DLPL 1 and the lowest accuracy of 0.9654 for DLPL 3, as shown in Table 21. The last group 18 consists of 2 overlapping classes: SkateBoarding and Skiing achieved top accuracy of 0.9873 for DLPL 2 and the lowest accuracy of 0.9535 for DLPL 3, as shown in Table 22.

Table 17 Performance for proposed deep learning pipelines for overlapping group 13
Full size table

Table 18 Performance for proposed deep learning pipelines for overlapping group 14
Full size table

Table 19 Performance for proposed deep learning pipelines for overlapping group 15
Full size table

Table 20 Performance for proposed deep learning pipelines for overlapping group 16
Full size table

Table 21 Performance for proposed deep learning pipelines for overlapping group 17
Full size table

Table 22 Performance for proposed deep learning pipelines for overlapping group 18
Full size table

Certain factors affect the performance of the proposed system. The number of overlapping and similar classes in a group has more impact on the performance. With the increase in the number of classes, the performance decreases because it is difficult for the model to distinguish between multiple classes that have overlapping and similar actions in them. Although our proposed system worked better than existing methods for temporal action recognition because large videos have an action for a long interval which makes the action recognition more difficult. Multiple actors performing actions also reduced the performance of accuracy action recognition. Our proposed models were trained on video data streams learning both spatial and temporal relationships for intervals of time. The confidence score of the model was changing due to the camera motion, actors’ differences, and viewpoint changes. However, these problems were faced for few classes. For most of the classes, the proposed system worked very well. The overfitting issue was handled by experimenting with the different number of frames in a video and also by adjusting dropout regularization. It is also noted that although pre-trained CNN models varied in performance with the use of deep autoencoder and RNN with LSTM also leave their impact on the performance. The difference in the performance of pre-trained CNN models was relatively little, largely due to frame-level spatial information extraction but reaching towards the end of LSTM and learning temporal relation between these sequence of video frames the performance change for each of the DLPL was becoming smaller.

We used 10 iterations for CNN, 100 for DAE, and 50 iterations for RNN with a batch size of 32, 64, and 128. Although the proposed system worked well on some groups it is also noted that certain factors still have their impact on performance. Video streams that are long action sequences in them are causing less performance for some classes. Video streams that have complex actions in them, containing multiple actions were the problem that has the most impact on less performance for complex actions classes.

Another factor that we observed in our experiments was the group size containing overlapping actions. With the increase in the number of overlapping action classes in a group, it was challenging for the system for classifying all of them correctly. Utilizing previously learned knowledge by using Transfer Learning techniques helped us in improving the baseline performance. The need for data requirements for training deep architectures was reduced by using transfer learning techniques. The frame-level features were extracted using pre-trained CNN models. These pre-trained CNN models extract the spatial information from frames by learning what action is performed in a frame and where the action is performing in the given frame. This feature extraction module cannot achieve higher accuracy because it performed its operations on extracted frames from videos. Thus, for learning the temporal relationship between these frames the RNN with LSTM was a good choice.

The use of RNN makes it possible to learn an action performed for an interval of time from a sequence of frames by passing information from one time step to the next, transferring knowledge from the earlier frames to preceding frames. As autoencoder is a type of feed-forward network in which an encoder is a feed-forward network and a decoder is also a feed-forward network. Depth in autoencoder has a significant effect on the performance of autoencoder. Instead of using a single layer for the encoder and a single layer for the decoder we used a deep autoencoder. In DAE we have one input layer after and two deep layers encoder, and a latent space representation after these layers. The decoder is the reverse of the encoder. This deep architecture allowed us to utilize the benefits of depth in feed-forward networks by approximation of function mapping from input to latent space representation, given enough hidden units, achieving higher accuracy. This deep autoencoder architecture worked much better in learning a generalization of latent space representation than shallow or linear-autoencoders. The generation of latent space representation reduced the need for higher computational resources and reduced the need for training data amount. Another factor that we observed in our experiments was the group size containing overlapping actions. With the increase in the number of overlapping action classes in a group, it was challenging for the system for classifying all of them correctly. Our proposed models were trained on video data streams learning both spatial and temporal relationships for intervals of time. The confidence score of the model was changing due to the camera motion, actors’ differences, and viewpoint changes. However, these problems were faced with fewer classes. For most of the classes, the proposed system worked very well.

The overfitting issue was handled by experimenting on a different number of frames in a video and also by adjusting dropout regularization. It is also noted that although pre-trained CNN models varied in performance but with the use of deep autoencoder and RNN with LSTM leave their impact on the performance also. The difference in the performance of pre-trained CNN models was relatively better due to frame level spatial information extraction but reaching towards the end of LSTM and learning temporal relation between these sequence of video frames the performance change for each of the DLPL was becoming small. Time required to extract deep spatial features from 1 frame and from 25 frames using CNN models then using DAE to squeeze those feature maps into low-dimensional feature plane and using RNN with LSTM to learn temporal relationship and classify them into action classes is given in Table 23.

Table 23 Processing time for 1 frame and for 25 frames using proposed DLPLs
Full size table

A comparison of the proposed system with other methods is presented in Table 22. Column 3 in the comparison Table 24 shows the average accuracy results where we have achieved an overall accuracy of 0.9603. We compared the performance of the proposed system with the hand-crafted feature extraction and traditional machine learning methods like Caetano et al. [40], Dalal et al. [41], Shi et al. [42], Peng et al. [43], and Cai et al. [44]. Among all hand-crafted feature extraction methods, Peng et al. [43] achieved the best accuracy of 0.8790 on the UCF-101 action dataset. Our proposed system outperforms these methods in performance by 0.0813 which is an 8.13% increase in inaccuracy. Our proposed method also outperformed the neural network-based [4, 8, 45,46,47,48,49,50] human action recognition methods by 0.017 which is a 1.7% increase in accuracy. We have achieved a positive prediction (precision) score of 0.9566, a sensitivity (recall) score of 0.9564, and an f1-measure score of 0.9565. These results illustrate the effectiveness of our proposed deep learning pipeline system for Human Action Recognition (HAR) on the UCF-101 human actions dataset.

Table 24 Comparison of the proposed system with other methods
Full size table

Conclusion and future work
In this article, we proposed a deep learning-based framework for recognizing actions that are overlapping or have similar action parts in them. The Deep Learning Pipelines (DLPL) proposed in this work containing one pre-trained CNN model followed by a Deep Autoencoder (DAE) and an RNN with LSTM. For learning frame-level spatial information seven pre-trained models with fine-tuning were utilized. Deep features extracted from these CNN models are of high dimensional. Among many built-in benefits, the DAE helped in squeezing these high-dimensional feature maps into low-dimensional feature maps that reduced the computational requirements for the proposed system. An RNN was added after DAE learned the long-range temporal actions for a sequence of continuous video frames. An iterative module was added at the end used to fine-tune the trained action recognition framework on the new video data streams for online non-stationary environments. Our proposed system achieved state-of-the-art performance on Human Action Recognition (HAR) for overlapping actions in long-range temporal visual data streams. There are still some challenges and directions for future work. Learning actions from multi-view-points, complex actions containing multiple sub-actions, and the development of generalized, unbiased, and standard data are still problems at large.