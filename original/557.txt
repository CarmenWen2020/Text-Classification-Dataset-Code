Abstract
Cloud storage systems are increasingly adopting NoSQL database management systems (DBMS), since they generally provide superior availability and performance than traditional DBMSs. To the detriment of better consistency guarantees, several NoSQL DBMSs allow eventual consistency, in which an operation is confirmed without checking all nodes. Different consistency levels for an operation (e.g. read) can be adopted, and such levels may distinctly affect system behaviour. Thus, the assessment of a system design taking into account distinct consistency levels is important for developing cloud storage systems. This work proposes an approach based on reliability block diagrams and generalized stochastic Petri nets to evaluate availability and performance of cloud storage systems, considering redundant nodes and eventual consistency based on NoSQL DBMS. Experimental results demonstrate system configuration may influence unavailability from 1 s to 21 h in a year, and performance can be impacted by up to 17.9%.

Introduction
Over the past few decades, cloud storage has played an important role in changing the way data is stored and accessed. Indeed, such mechanism has become fundamental for several systems, as data can be accessed without restrictions on geographical barriers and details about the storage infrastructure are concealed from users [25].

Cloud storage has also been adopted for big data applications, which are characterized by fast generation and manipulation of large amount of data [46]. In this context, traditional database technologies, such as relational database management systems (DBMS), have performance issues in dealing with large data volume [10]. Due to better performance, NoSQL (Not only SQL) DBMSs have been adopted by prominent service providers (e.g. Amazon), offering database as a service (DBaaS) [1] as a storage mechanism.

The flexibility of data schema and horizontal scalability attributes that facilitate the adoption of NoSQL in cloud storage [28]. Several NoSQL DBMSs, such as Cassandra,Footnote 1 adopt eventual consistency to improve system performance and availability. In other words, a temporary inconsistency between redundant servers (nodes) is accepted, but eventually the system will reach a consistent state [39] (i.e. all nodes are updated).

According to CAP theorem [7, 15], distributed systems may guarantee only two of the following properties: (i) consistency, (ii) availability, and (iii) partition tolerance. Cloud storage must be highly available and scalable, and eventual consistency improves availability and partition tolerance [34]. NoSQL DBMSs usually utilize quorum-based consistency [45], which allows eventual consistency be associated with an operation (e.g. read) and a minimum number of nodes. The choice of consistency level impacts availability and performance of storage systems [45] and, thus, the assessment of a system design taking into account distinct consistency levels is important for evaluating cloud storage services.

This paper proposes a technique for evaluating availability and performance of cloud storage services based on NoSQL DBMS. The approach utilizes reliability block diagrams (RBD) and stochastic Petri nets (SPN) to estimate availability and operation latency, considering distinct consistency levels and redundant nodes (i.e. cluster size). In this work, these models are validated using Cassandra and Riak DBMS, and experimental results demonstrate the consistency level may have a great influence on a system’s non-functional attributes.

The rest of this paper is organized as follows. Section 2 provides an overview of important concepts. Section 3 introduces related work and Sect. 4 describes the adopted methodology. Section 5 presents the proposed models and Sect. 6 describes experimental results. Finally, Sect. 7 concludes this work.

Background
This section presents fundamental concepts to better understand this paper.

Cloud storage
Cloud storage is an online data management infrastructure based on cloud computing paradigm, in which data storage providers hide details about service architecture and storage mechanisms to the respective users [31, 43]. Indeed, cloud storage is a term that may encompass several forms for data storage, such as data as a service (DaaS) and database as a service (DBaaS)

Multi-tenancy is an important feature of cloud storage services, since computing resources are shared to multiple and simultaneous tenants in the cloud, allowing reduced costs and service flexibility [6].

Multi-tenancy can be adopted in different layers. For instance, a service provider may offer to each tenant a DBMS on an isolated virtual machine, which results in sharing resources at the virtualization level. Another option allows tenants to share a common DBMS, in the sense that data from different tenants are kept in the same DBMS. In both cases, providers must ensure the services behave uniquely for each tenant, and access to other tenants’ data should be forbidden, unless explicitly desired. In addition, predicting the workloads expected from different tenants is quite important for the functioning of DBaaS [2].

Availability
Service failure can cause performance degradation or even service interruption due to the unavailability of a requested resource. The failure may have distinct sources, such as hardware, software, network interconnection, and system failures can impair the users’ preference for the service [12].

Cloud computing services are usually characterized by massive user utilization, and assuring high availability for these systems is a challenge. Redundant servers are commonly utilized to meet availability requirements [11].

Availability is the probability of a system being under a functioning condition. It considers the alternation of operational and nonoperating states [27]. Steady-state availability (A) is commonly adopted, and the following equations are taken into account


 
 

(1)

MTTF is the mean time to failure and MTTR is the mean time to repair, such that

M(t) is the cumulative distribution function representing the probability that a repair will occur within time t. R(t) is the reliability function, which is the probability of a system performing its functions without failures for a period of time t. Downtime is given by .

Consistency in NoSQL DBMS
Consistency in DBMS may be defined as a single valid state for all database instances, in the sense that every redundant database server has the same data [17, 30]. Thus, system performance might be affected, as DBMS needs to confirm that a write operation was executed on each redundant server. For read operations, a DBMS must guarantee that the returned data are up-to-date. Similarly, availability may also be influenced by the consistency policy.

For greater performance and availability, NoSQL DBMSs usually adopt eventual consistency, in the sense that not all redundant servers (replicas) may have immediately the newest data. Thus, a database replica may only return its accessible data, which may not be the newest. Eventually, all redundant servers will be consistent [37]. To mitigate this inconsistent interval, NoSQL DBMSs allow the adoption of different consistency levels, which define the minimum number of redundant servers that confirm an operation, which is tuned according to application requirements [19].

Eventual consistency may also improve availability, since a DBMS requires the number of operational servers be equal to the consistency level. With a lower consistency level, the database system needs less servers to enforce the consistency.

Figure 1 presents a DBMS composed of 3 redundant servers and 3 different consistency levels. The cloud represents external requests. A server is responsible for receiving an operation request and for coordinating communication with other redundant nodes. This server is named as coordinator and any node can assume this function (which is dependent on the system load balancing) [21].

Fig. 1
figure 1
Example of consistency levels (one, two and three)

Full size image

Assuming consistency level ONE, the confirmation of an operation (e.g. write) requires only a single server. For level TWO, two nodes must confirm the operation and, in the case of a read operation, the newest data of both servers are considered.

Similarly, level THREE requires the confirmation of all nodes and, for a read operation, the newest data are guaranteed to be always accessible (strong consistency) [18].

The condition for strong consistency is given by the following in equation (5) [24]:

N is the number of redundant servers; W is the number of servers with the newest data due to a write operation; and R is the number of adopted servers for a read operation.

Quorum-based consistency
Representative NoSQL DBMSs, such as Cassandra [42], Riak [5] and Amazon DynamoDB [22], adopt a quorum-based technique for controlling database consistency among replicas. More specifically, a DBMS confirms an operation execution (e.g. write operation) if and only if a sufficient number of servers (quorum) recognize the successful completion of the operation [14]. Quorum is strongly related to eventual consistency, since the adopted quorum is the consistency level of the operation.

Assume RF is the number of redundant servers (i.e. replication factor); R represents the read quorum; and W denotes the write quorum. The system is operational as long as the number of active servers (Up) is greater than or equal to max(W, R). For instance, consider ,  and . To be operational (), the storage system tolerates at most two simultaneous node failures.

Related work
Over the last decade, prominent researches have been carried out to assess consistency and performance of NoSQL DBMS that can be utilized in cloud storages. Despite their importance, availability is usually not taken into account on these works.

Huang et al. [21] evaluate the performance of eventual consistency using Cassandra. The authors show the size of write queue may considerably influence the consistency of NoSQL DBMSs based on quorum technique. Yao and Wang [44] developed an approach to estimate the probability to access the newest data. Despite showing the relation between consistency and latency, the authors do not verify system availability.

In [13], the authors evaluate the consistency mechanism of five NoSQL DBMSs: Redis, Cassandra, MongoDB, Neo4j and OrientDB. Experiments are carried out on a simulator, and results indicate Cassandra DBMS provides the highest availability. The work does not further validate the experiments on a real system. Anatoliy et al. [3] perform experiments using different levels of consistency and workload to assess the impact on the performance of Cassandra NoSQL DBMS. Their results demonstrate distinct levels of consistency can significantly affect system performance, and the ideal level depends on the workload. Availability is not assessed.

Singla et al. [40] present a technique to deal with data consistency in social networks using Cassandra DBMS. Experimental results indicate throughput can be improved, but they do not consider system failures. In [35], the authors adopt Petri nets to model Cassandra clusters, but focusing only on performance. In [20], the authors propose a coloured Petri net (CPN) model for performance analysis of a NoSQL DBMS using quorum-based consistency. The model estimates system throughput, but does not take into account system failures.

In [16], the authors present a data integration technique using checkpointing to provide fault-tolerance in NoSQL servers. Results demonstrate system failures can be mitigated, but that work does not estimate availability. Pankowski [36] presents an algorithm for data replication on NoSQL DBMS, taking into account consistency and availability. The method is based on quorum technique. System availability is obtained using the average availability of all nodes, but that work does not validate the approach.

Different from previous works, this paper proposes RBD and GSPN models for evaluating availability and performance of cloud storage systems using NoSQL DBMS. Our approach provides an additional support to system designers for assessing the influence of distinct consistency levels and redundancy mechanisms on cloud storage. Table 1 provides an overview of related work compared to our technique, taking into account system modelling (Modelling), performance, availability and consistency.

Table 1 Comparison works
Full size table

Methodology
This section presents the adopted methodology (Fig. 2) for assessing cloud storage systems using the proposed models (Sect. 5).

Fig. 2
figure 2
Methodology

Full size image

The first step concerns to understand the system for evaluation, such as its components and interactions. Next, the designer should define the metrics (e.g. availability) for assessing a new implementation or modification on a existing system. Data collection on a real system (e.g. prototype) is then carried out, as the model parameters require values representing the time execution of some activities. If data collection is not feasible, the designer should consider values from other works, datasheets (when available) or even adopt desired values for an initial assessment. Afterwards, model creation is performed according to the defined metrics. For instance, if the designer is only interested on availability, a RBD may be sufficient for system assessment.

Model validation is optional, as this activity may depend on data collected on a real system. Without loss of generality, this task verifies if the models provide estimates that represent the behaviour of the system in which data were obtained. If the model does not provide close estimates, then the designer should review the model and carry out (if necessary) additional measurements. Finally, experiments are performed to assess different system designs and configurations.

Availability and performability models
This section presents the availability and performability models for evaluating cloud storage services. RBD models are adopted to estimate the availability of storage service, whereas a GSPN model performs a joint assessment of availability and latency (i.e. performability) of the system.

Reliability block diagrams (RBD) are combinatorial models, in which the structural relation between system components is utilized to assess system availability or reliability. Graphically, a RBD represents system components using blocks and their logical relations using arcs [26, 41]. If the failure or recovery of a system component is not influenced by any other system component, a RBD model can be adopted. However, RBD models have constraints to represent complex relations between system components. This work adopts series and k-out-of-n arrangements, which are evaluated using closed-form equations.

Generalized stochastic Petri nets (GSPN) are state-based models, which may represent complex relations between system components. More specifically, GSPN is a prominent Petri net (PN) extension, which is a bipartite directed graph. Places (represented by circles) denote local states and transitions (depicted as rectangles) represent actions. Arcs (directed edges) connect places to transitions and vice-versa. An inhibitor arc is an arc type that depicts a small white circle in one edge, instead of an arrow, and they usually denote the unavailability of tokens in places. Tokens (small filled circles) may reside in places, which denote the state (i.e. marking) of a PN. The semantic of a PN is defined in terms of a token game, in the sense that tokens are created and consumed according to transition firings.

GSPN allows the association of exponential distribution to timed transitions (represented by white rectangles), or zero delays to immediate transitions (depicted as thin black rectangles). Timed transitions may have different concurrency degrees: single server semantics (ss) or infinity server semantics (is). In general, is is adopted to represent parallel activities, considering that the firing rate of a transition is linearly increased according to its enabling degree. In ss, the firing rate is kept constant. For model evaluation, the state space of GSPN models may be translated into continuous-time Markov chains (CTMC). State-based models may suffer from state space explosion, but simulation techniques may be adopted as an alternative to the Markov chain generation. The reader is referred to [4] for more details.

The following operators are adopted for GSPN:  estimates the probability of the inner expression (exp);  denotes the number of tokens in place p; and  represents the average number of tokens in place p. Besides, the proposed models have been conceived for stationary analysis, and all timed transitions adopt exponential distribution and single-server semantics (except when indicated).

Cloud storage system
Our modelling approach considers a cloud storage system using NoSQL DBMS and quorum-based consistency (Sect. 2). The system is composed of N physical servers to host virtual machines (VM), and each VM has one DBMS (one-to-one relationship). RF denotes the number of NoSQL DBMS replicas and CONS is the consistency level (). A physical server can execute any VM, and, in case of a server failure, VMs are migrated to a working server. The migration is considered to have no impact on availability. The system is operational if K servers () and CONS replicas (or VMs) are operational.

Availability models
Figure 3a, b depicts, respectively, the RBD models for the physical server and DBMS replica. Both models adopt series arrangement, in which the system is available if all components are operational. Assuming n components, system availability/reliability (
) is estimated as 
, in which 
 denotes the availability/reliability of component i.

Fig. 3
figure 3
RBD models: a physical server; and b DBMS replica

Full size image

The physical server (Fig. 3a) is composed of hardware (HW), operating system (OS) and hypervisor (HYP). The DBMS replica model (Fig. 3b) contemplates virtual machine (VM) and NoSQL DBMS (NSQ). Each component is a block, and, from the respective MTTF and MTTR, the component availability is obtained.

Using the equations described in Sect. 2.2, the model may also be reduced to a single block [8] with an approximate MTTF and MTTR. For Fig. 3a, the reduced block is denominated PhysicalServer and its availability is 
. Similarly, for Fig. 3b, the single block is called VMDBMS and its availability is 
.

For obtaining the final system model (Fig. 4), a k-out-of-n arrangement is required. For this case, availability/reliability is estimated as 
. p denotes the availability/reliability of a component.

Fig. 4
figure 4
RBD models

Full size image

As K out of N physical servers and CONS out of RF database replicas need to be operational, system availability (
) is obtained as follows:


 

(6)

Performability model
Figure 5 depicts the proposed GSPN model, which is composed of three building blocks: availability, client arrival and consistency.

Fig. 5
figure 5
Performability model

Full size image

The availability block contemplates the physical servers (physical) and DBMS replicas (replica). Place ppUP and pUP denote the number of physical servers and replicas that are operational, respectively. Initially, all physical servers (N) and replicas (RF) are working. Transition tFailure and tpFailure denote the failures, and tRestore and tpRestore represent the maintenance. Transition delays are the MTTF and MTTR of each subsystem, which are estimated using the RBD models depicted in Fig. 3. Besides, these transitions adopt infinite-server semantics. Place ppDown represents the failure of a server, and, similarly, place pDown denotes the failure of a VM/replica.

For DBMS replicas, immediate transition tInFail has the guard expression (), which verifies if K physical servers are unavailable. If true, all DBMS fails, since the minimum amount of physical servers are not operational. Besides, the inhibitor arc from place ppDown to tRestore does not allow the recovery of DBMS replicas, unless K physical servers are available.

The model also adopts constant CONS, which is a positive integer denoting the consistency level. Immediate transitions tInCoord and tInQueue represent the failure of a read or write operation due to lack of servers to guarantee the consistency level. Inhibitor arcs also perform this verification.

Client arrival block represents the periodic user requests, which are indicated by the firing of timed transition tClient. If there is space in the queue (), a client request is accepted by the system (place pCoord).

In the consistency block, transition tCoord represents the coordinator communication with the nodes (including itself) for executing an operation and adopt infinite-server semantics. An inhibitor arc with weight  guarantees the transition will be only enabled if there are enough operational servers to meet the consistency level. Whenever a server fails and the consistency level cannot be guaranteed, a failure response is returned (transition tRespFail). If the amount of DBMS nodes required by the consistency level perform the operation, a successful response is returned (transition tRespSuc).

The performability model is adopted to estimate availability and latency between successful operations. Using the proposed model, availability is given by . Throughput is estimated as , in which resp is the time associated with transition tRespSuc (successful response). Thus, latency is 
.

Experimental results
This section presents experimental results to demonstrate the practical feasibility of the proposed modelling technique. Initially, model validation is presented and, next, results related to availability and latency are described.

This work has adopted the following tools for model evaluation: Mercury [38] and TimeNET [47]. The former is an integrated modelling environment, which allows the creation of RBD, fault tree, Markov chain and SPN models. TimeNET is a specialized tool for evaluating the family of stochastic Petri net models (including GSPN). We adopt Mercury for evaluating RBD models and TimeNET for GSPN models, as the latter provides faster results concerning numerical evaluation.

Model validation
Two validations were performed to demonstrate the accuracy of the results provided by our RBD and GSPN models. We have adopted an environment based on XenServer 7.1 platform to provide virtual machines (VM) for each NoSQL DBMS and a client system. All VMs utilize Debian 9.0 operating system (OS) and a dual core CPU. Regarding RAM memory, the client VM has 4GB and each DBMS has 8GB.

This work adopts Cassandra DBMS for RBD validation and Riak DBMS for GSPN validation. These DBMSs have good documentation and features that make them feasible to collect data for experiments. A single DBMS could be adopted, but the idea is to demonstrate the conceived models can be utilized for distinct DBMSs with quorum-based consistency.

RBD model
For the RBD model, the validation takes into account a series arrangement, in which one physical server and one DBMS are adopted (Fig. 4). The model is compared to a real system using fault injection, and Table 2 depicts the components and the respective MTTFs and MTTRs. These values are reduced by a factor of 100 for validation purposes, and the exponential distribution is assumed.

Algorithm 1 shows the fault injection script of a component and the respective recovery. A time to failure is randomly generated, the system execution is interrupted (i.e. unavailable) and then restored after a time to repair (also randomly generated). Distinct threads running Algorithm 1 were adopted for the physical server and DBMS.

figure a
Algorithm 2 represents the script for the monitor that records the system state (i.e. available or down). A loop performs periodic communications with the DBMS. If there is a change in the state of the service, this new state is recorded. We wanted to carry out experiments for a period equivalent to roughly one year. Taking into account our acceleration factor of 100, we needed to execute the experiments for about 97.3 h.

figure b
We utilized the technique described in [23] for calculating a 95% confidence interval for system availability: [0.997319, 0.998306]. Using Mercury tool, the RBD model estimates availability as 0.997732. Since such a value is contained in the confidence interval, there is no statistical evidence to refute the equivalence between the proposed model and the real system.

Table 2 MTTF and MTTR of model components
Full size table

Performability model
For the sake of validation of model in Fig. 5, this work utilizes a replication factor with 2 machines and consistency level 2. We have not considered server failures and read operation has been taken into account. Yahoo! Cloud Serving Benchmark (YCSB) [9] has been utilized, assuming an uniformly distributed workload for the client.

The time between requests was set to 11.8ms, and 30 samples were collected on the system for read operation. Such sample size is related to central limit theorem [33], since we are concerned with mean values. Table 3 depicts the obtained mean values for each activity represented by timed transitions in the GSPN model.

Table 3 Transition times for performability model validation
Full size table

Next, the model was evaluated using stationary analysis [29], and the latency between successful responses was estimated as 13.928ms. Since this value is contained in the 95% confidence interval [13.912,14.232] obtained with the measurements, there is no statistical evidence to indicate the model does not represent the real system.

Experiments
Two experiments were carried out to evaluate availability and latency. Both experiments are based on a design of experiments (DoE) [32] approach with a 
 factorial design.

The following sections present results using rank of effects. Effect is the change in response due to a change in the factor level. All ranks are presented in descending order, taking into account the absolute values of all effects. Besides, this work considers main effects and second-order interactions, since higher order interactions are usually negligible [32].

Availability
The experiment takes into account the following factors () and levels (): (i) consistency level (cons)—1, 3; (ii) replication factor (rf)—3, 5; (iii) physical server (n)—1, 3; (iv) hardware MTTF (
)—8760 h, 17520h; and (v) hardware MTTR (
)—0.835 h, 1.67 h. 
 and 
 represent the hardware of physical server, and their values may vary due to better hardware components or skilled maintenance crew. Regarding the software components, Table 4 depicts the adopted values (which are fixed).

Table 4 MTTF and MTTR of experiments
Full size table

Table 5 presents the rank of effects considering downtime (DT), as it provides a better visualization for small values related to availability variation. Assuming one year as the time span, downtime (in hours) is .

Table 5 Rank of main and interaction effects (downtime)
Full size table

Factors cons, rf and their interaction are responsible for most availability variation, more than 9 h a year, considering the adopted levels. Although 9 h seem small compared to 8760 h, they may generate fines due to violations of quality of service agreements or loss of users. The number of physical servers has a significant impact on availability, almost 2 h. For the adopted levels, hardware MTTF (
) and MTTR (
) have a smaller contribution, about 30 min in a year.

Table 6 provides the results of each treatment. Downtime values less than 1 min are represented as < 0.01. Availability is considerably reduced when strong consistency () is taken into account, in which no redundant machine is accessible for fast replacement of a failed node. On the other hand, additional physical machines improve availability. The only treatments with lower availability are those that adopted strong consistency. Besides, Fig. 6 depicts an effect plot indicating the change of availability for the consistency levels, in which downtime may vary from approximately one hour per year (level 1) to 10.52 h (level 3).

Fig. 6
figure 6
Effect plot—availability

Full size image

Table 6 Results for availability
Full size table

Latency
This experiment considers a DoE with 4 factors () and 2 levels (), taking into account read operation: (i) consistency level (cons)—1, 3; (ii) successful response time (resp)—0.9, 1.8; (iii) coordinator communication (cc)—1.7, 3.4; and (iv) replication factor (rf)—3, 5. For the physical server, MTTF and MTTR are 1115h and 0.47h, respectively. For the DBMS replica, the values are 1440h for MTTF and 0.5h for MTTR. The time between client requests is also kept constant: 11.8ms.

For resp and cc, the levels are the same as the values utilized in validation and a 50% reduction is taken to account to denote a better system capacity. cons and rf assess the influence of consistency level and number of servers on system performance.

Table 7 presents the rank for main and interaction (e.g. ) effects, considering the latency between successful operations. Coordinator communication (cc) has the greatest effect. Consistency level (cons) also has an important contribution to performance, as the amount of servers required for an operation has a prominent influence on system performance. Due to the importance of cc and cons, their interaction () has the largest effect among all interactions.

Table 7 Rank of main and interaction effects (latency)
Full size table

Replication factor (rf) and its interactions do not play a significant role in determining system performance. However, redundancy is important for system availability, which also depends on the required consistency level. As with the availability experiments, the effects rank may vary depending on the values adopted in the model.

Table 8 presents the successful latency for each DoE treatment. The lowest values are associated with consistency 1, since less communication between nodes is required for executing an operation. For a better visualization, Fig. 7 depicts an effect plot indicating the change of performance for each consistency level.

Table 8 Results for read latency
Full size table

Fig. 7
figure 7
Effect plot—latency

Full size image

Reduced coordinator communication and response times also improve latency and, for the adopted levels, a system designer should assess the procurement of better machine components and network capacity.

Discussion
Experimental results demonstrate the importance of assessing availability and performance of distinct consistency levels on cloud storage systems. In our experiments, system unavailability can vary from less than 1 s to 21 h a year. Consistency (cons) and replication factor (rf) have the greatest influence, but have different directions. More specifically, more servers improve availability, but a higher consistency level decreases availability. Another interesting situation is related to physical servers (n), in which the highest level () provides the highest availability (0.999999999). However, considering  and , unavailability is greater than 19 h. Besides, our models allow the evaluation of different time to failure and repair, which also have influence on availability.

Regarding performance, latency may vary up to 17.9%, which can be prominent depending on system requirements. Consistency has an important influence on performance, but replication factor does not have a remarkable impact considering the adopted values. The reason is associated with the parallel execution of each replica. Coordinator communication (cc) is the most important factor, as this server is responsible for controlling the request with DBMS replicas. These results can assist system designers, for instance, to make decisions about the best configuration for implementing or modifying a storage system.

Conclusion
Cloud storage services have changed the way data is stored and accessed, and high performance and availability are fundamental non-functional requirements of such services. NoSQL DBMS has been commonly adopted in cloud storage, as eventual consistency improves performance and availability. Distinct consistency levels can be utilized, but they may differently impact the behaviour of cloud storage services.

This work presented analytical models based on RBD and GSPN to evaluate cloud storage services using NoSQL DBMS. The models estimate availability and latency for distinct consistency levels and DBMS replicas. The proposed models allow to make decisions before modifying or implementing the real system or prototype, which can be costly. A limitation of our approach is the separated evaluation of operation type (read or write) as well as the requirement for obtaining values for the model parameters (e.g. MTTF or MTTR). If data cannot be collected on a system, the designer may adopt estimates from other works or perform experiments with desired values.

Experimental results demonstrate consistency is an important factor, which cannot be neglected in the design of cloud storage systems. Unavailability can vary from 1 s to more than 21 h in a year, while system performance is impacted by up to 17.9%.

As a future work, we are developing techniques to assess the impact of consistency in the energy consumption of cloud storage services. Indeed, energy consumption has been a concern over the years due to the rising costs and environmental sustainability. Therefore, a designer would have an additional tool for cloud storage assessment.