Abstract
Traditional classification algorithms work well on general small-scale microarray datasets, but for large-scale scenarios, general machines are not capable of supporting the operation of these algorithms anymore for the memory and time costs. In this paper, we design a new application framework to perform the computation of at the fastest speed. First, the synthetic minority over-sampling technique is used to sample a few classes of sample for obtaining the balanced data. Then, a large-scale algorithm for 
-SVM based on the stochastic gradient descent method is proposed and used for microarray classification. Also, We give a simple proof of the convergence of stochastic gradient descent algorithm. Next, various large-scale algorithms for support vector machines are performed on the microarray datasets to identify the most appropriate algorithm. Finally, a comparative analysis of loss functions is done to clearly understand the differences. The experimental results show that the stochastic gradient descent algorithm and the squared hinge loss is an attractive choice, which can achieve high accuracy in seconds.

Introduction
Microarray is valuable for association and linkage studies to isolate chromosomal regions related to cancer or genetic variations. Microarray datasets generated from experiments on DNA, RNA, and protein microarrays are very large (not for samples, but the number of features), which poses challenges for data mining and analysis [1]. A good number of techniques for microarray analysis have been discovered and proposed, including aggregation and normalization, feature selection [2], class discovery and prediction [3].

Class prediction for microarray is the main interest and the final step in data analysis, which has been explored by many researchers and experts using various machine learning algorithms [4,5,6]. For example, Shah et al. [6] proposed a hybrid laplacian score-convolutional neural network (LS-CNN) and applied it to microarray classification. For the training set, the accuracy of the model in predicting cancer is almost always 100%, while the prediction results of the test set are not presented in the article. Guyon et al. [3] used the support vector machine (SVM) based on recursive feature elimination (RFE) to select key features from raw gene data and achieved high performance in terms of accuracy for the test set. To further improve the classifier performance, Vafaee Sharbaf et al. [7] used genetic algorithm (GA) to get the fine-tuned results of multi-layer perceptron (MLP), SVM, and K-nearest neighbor (KNN). However, the parameter optimization itself imposes a huge time overhead.

For the microarray classification, there is an obvious problem that microarray data sets are generally unbalanced, which is defined as data sets with significant differences in the number of samples each class. It can seriously impair the performance of SVM, even though the prediction accuracy is shown to be good, it is likely to be an illusion. In other words, SVM is susceptible to frequency bias. To reduce the adverse effects, the common approaches used are class-weight, over-sampling (SMOTE [8], etc.), under-sampling [9]. Class-weight method is the simplest way to place more emphasis on the minority classes. Over-sampling and under-sampling are distinct from balancing the data in that the former increases the number of minority samples and the latter decreases the number of majority classes.

Moreover, with the explosive growth of data dimensions, the computational inefficiency of SVM becomes more apparent. To improve the capability of solving large-scale data, various novel algorithms called large-scale support vector machines solvers have been proposed over the past decade. They fall into two general classes: solvers for primal form and solvers for dual form. For dual form problems, the advent of sequential minimal optimization (SMO) [10] has changed the perception of traditional solvers for SVM. Subsequently, 
 [11], Liblinear (based on the coordinate descent) [12] and bundle method for risk minimization (BMRM) [13] were proposed for solving a large-scale problem.

In the case of primal problem, the empirical loss should be minimized and the regularization term balances the accuracy and noise tolerance. According to the requirements, some variants of gradient descent (GD) are used to solve the large-scale problem, including the stochastic gradient descent (SGD) tricks [14,15,16], NORMA [17], Pegasos [18], SGD-QN [19], Mini-batch SGD [20], and these techniques have presented simple and effective advantages. Nevertheless, simple SGD-based solvers are vulnerable to the curse of kernelization and lose their edge when faced with a non-linear classification problem. To overcome this drawback, Wang et al. proposed the adaptive multi-hyperplane machine (AMM) [21] model and applied SGD instead of SMO to obtain the optimum, which have shown a promising high efficiency in large-scale and non-linear data sets. Meanwhile, Djuric et al. [22] combined the learning vector quantization (LVQ) algorithm with AMM and proposed the growing AMM (GAMM) model trained by SGD, which has demonstrated that GAMM is comparable to non-linear SVMs classifiers and higher computational efficiency. To address the kernelization problem, Wang et al. [23] introduced the maintenance budgeting approach to the primal SGD and controlled the number of support vectors to make SGD applicable to the kernel SVM, called budgeted SGD (BSGD), which achieved faster computational speed for large-scale kernel SVM.

In this paper, the synthetic minority over-sampling technique (SMOTE) is used to over-sampling the minority classes of the microarray data set. Due to the advantage of computing in “feature space” rather than “data space”, the cost of space and time is negligible. And SGD is applied to obtain the optimal of 
-SVM with linear kernel for reducing the computational complexity. The empirical results show that this new approach achieves an impressive computational time and space while ensuring the classification accuracy.

The paper is structured as follows: Related work is presented in Sect. 2. The methodology of based-SGD SVM is shown in Sect. 3. The experimental results on four microarray data sets are reported in Sect. 4. Conclusions and future work are given in Sect. 5.

Related works
Processing of imbalanced data
Imbalance refers to the very large differences in sample sizes of different classes, and the imbalanced distribution of sample classes is mainly found in classification-related modeling problems [8, 9]. In addition to the fact that classes with small sample sizes contain little feature information and it is difficult to propose valid information, general machine learning models are influenced by classes with large sample sizes and ignore classes with small sample sizes. This makes the classification accuracy unreliable. Empirical evidence shows that if the difference in sample size between classes is greater than 10 times, it needs to be alerted and dealt with promptly.

SMOTE is an over-sampling method and used to over-sampling minority classes by creating synthetic examples. As we can see from Fig. 1, the specific calculation steps are as follows:

i.
K nearest neighbors for the minority class sample is calculated.

ii.
Random linear interpolation of N samples randomly selected from K nearest neighbors.

iii.
Creating a new minority class sample relies on the following formula.

Fig. 1
figure 1
The strategy for the synthesis of minority class samples

Full size image

Large-scale algorithms for SVM
After analyzing the large-scale SVM algorithms that have emerged in recent years, we give a preliminary classification of them according to the formulas of the optimization problem, which are presented in Fig. 2.

With the explosive growth of data, many researchers have proposed various strategies for large-scale SVM training. There are four main categories of large-scale SVMs, which are primal formula, dual formula, geometric formula, and parallelizing SVM. The idea of SMO [10] is to decompose the large-scale quadratic programming problem (QP) into the simplest subproblems. We only need to optimize a pair of Lagrange multipliers (
), which means the computational complexity is only 
.

However, we seek algorithmic complexity that can be asymptotically linear or linear. (Sub)SGD-based variants such as Pegasos [18], BSGD [23], etc. have been proposed for obtaining the solution of primal SVM, and they can achieve the art-of-the-state accuracy and efficiency. Moreover, some researchers and practitioners have noted the geometric characteristics of SVMs. They have introduced geometric theory into SVMs for large-scale data sets. The core vector machine (CVM) is the most interesting algorithm for achieving linear training time 
 [24]. Fast kernel density estimate (FastKDE) is an approximation method using a simple sampling strategy, and the time complexity is 
 [25]. Reducing samples through convex hull vertices selection (CHVS) selects the major samples to constitute a convex hull for reducing the size of samples [26]. Fast convex-hull vector machine (FCHVM) first extracts the boundary vectors and forms the convex hull vectors by integrating the obtained boundary vectors. This strategy is a better application to ncRNA datasets [27]. Subsequently, other methods based on the ideas of core sets or reducing the training set were proposed. As the development of distributed system (DS) infrastructures such as Hadoop and Spark, the parallel computing based on multiple machines has been valued and implemented by the machine learning pioneers. According to the idea of divide and rule, Graf et al. proposed the cascade SVM for the ultra-large-scale data set. In the beginning, the data sets are decomposed into small subsets [28]. Then, each subset is trained by a single SVM, which is solved by the general algorithms such as interior point method (IPM), SMO. Finally, each SVM is used as a filter to eliminating non-SVs. Hence, the resulting SVs are applied to train the global model. Although the running time is reduced, the model becomes unstable. We can understand the detailed large-scale algorithm and its complexity from Table 1.

Fig. 2
figure 2
The preliminary classification for large-scale SVM based on the formula of the optimization problems

Full size image

Table 1 The comparison of some large-scale algorithms on runtime
Full size table

Methodology
Primal formula of SVM
Given a training set 
, 
, 
. To get an ideal hyperplane 
, we need to solve the following optimization problem.


 
 
 
 

(1)


 

(2)

where 
 is slack variables. And  is a regularization parameter. Equations 2–3 are the constrained conditions making it difficult to solve the objective function Eq. 1. In fact, we can obtain the unconstrained optimization problem by introducing a penalty term, which is inspired by the external penalty method.


 
 
 
 
 

(4)

where  is regularization parameter scaling 
, and 
 is called hinge loss function, 
 is called squared hinge loss. The modified Huber loss is defined


 
 

(5)

Fig. 3
figure 3
The generally used loss function for SVM is hinge loss, which penalizes errors linearly

Full size image

The generally used loss function for SVM is hinge loss, which penalizes errors linearly. And the squared hinge loss is commonly applied to replace the hinge loss since it is first-order indifferentiable at . Modified Huber can both generate sparse solutions to improving training efficiency at 
 and perform probability estimation. Also, it penalizes for the samples linearly at 
, which means there is less interference from anomalies (Fig. 3).

Stochastic gradient algorithm for SVM
Gradient Descent (GD) is a classical iterative optimization algorithm for finding the minimum of a convex differentiable function f(w). Due to the direction of  points the greatest decrease, the following is to repeatedly compute


 
 

(6)

where 
 is the learning rate determining the speed of convergency and the performance of the model, w is the vector that represents the weights concerning the input data. The shortcoming of GD is that the entire data set needs to be computed for each iteration, which consumes a large amount of computing costs.

However, SGD only needs to find the gradient from a single randomly selected sample.


 
 

(8)

where I is an indicator function determining 
, , and 
 are randomly chosen.

figure a
Convergence analysis of SGD for 
-SVM
The convergence of stochastic gradient algorithm descent has been studied in various stochastic approximation paper. It is known that SGD converges when the learning rates 
 satisfies
 and
. The stochastic optimization problem is Eq. 4, by defining 
, 
 is a random variable, the empirical risk minimization is


 
 
 
 

(9)

The expected risk is


 
 

(10)

And the unbiased estimator of (10) is 

The objective function (4) is smooth and convex if the squared hinge loss is used for SVM. Hence,  has a Lipschitz continuous gradient. It is essential to ensuring convergence of many gradient decent based algorithms.

Assumption 1
(Lipschitz continuous gradient) The objective function 
 is differentiable. The gradient function of f is 
 and Lipschitz continuous if there exists a constant  such that, 
,


 

(11)

Lemma 1
With the hinge loss, squared hinge loss, and modified Huber loss,  is convex function i.e., 
,


 

(12)

Proof
For a general max function,  
 
, for ,


 
 
 
 
 


Specially, modified Huber loss is convex function when 
, and it is still convex function due to the formula is a affline function.

Assumption 2
(-Strong convexity) The objective function 
 is a -strongly convex, i.e., there exists a constant  such that 
,


 
 

(13)

Under Assumption 1,


 

(14)

From the paper, we know that there exists constants  such that 
.

Lemma 2
The Eq. 4with hinge loss, squared hinge loss, or modified Huber loss satisfy Assumption 2.

Proof
Under Lemma 1, 
,


 

By appending the 
 regularization term 
 
, and holding the Assumption 2, we can obtain this conclusion. 

Theorem 1
(Fixed Stepsize). Under Assumption 1, 2, and Lemma 1, consider Algorithm 1 with a fixed stepsize, 
 


 
 
 

(15)

where 
 is the unique minimizer, 
 and 
.

Proof
By using Eq. 13,


 
 
 
 
 
 

Here, according to the conditions, 
 can be replaced by 
,


 
 
 
 
 
 
 

By subtracting 
 from both sides and taking total expectations,


 
 

By subtracting 
 
 from both sides,


 
 
 
 
 
 

 

Theorem 2
(Diminishing Stepsize Sequence). Under Assumption 1, 2, and Lemma 1, consider Algorithm 1 with a stepsize sequence such that 
 
 
, and 
 
. Then, ,

for


 
 

(17)

where 
.

Proof
By induction, ,

,


 
 
 
 
 
 
 
 
 


The framework for classification of microarray
The classification procedure of microarray is presented in Fig. 4, which is composed of four main phases:

i.
Preprocessing. The mean value of each feature is applied for the missing value imputation in the case of a small number of missing values, and the input feature matrix is normalized in the range [0, 1].

ii.
Over-sampling. SMOTE is used to over-sampling minority classes.

iii.
Model Training. The SGD-based 
-SVM model is constructed for the training sets.

iv.
Prediction. The test set is used to validate the proposed classifier.

Fig. 4
figure 4
The framework of proposed methods for classification of the microarray

Full size image

Experiments
In this section, experiments are performed on the framework of the proposed methods of microarray classification methods. The programs are processed in Jupyter Notebook (python3.7) and run on Windows 10, 16 G memory, Intel®Core
 i5-8500CPU@ 3.00 GHz.

Performance evaluation
The confusion matrix is a table that visually describes the performance of a classifier. For a binary classification problem, the confusion matrix is shown in Table 2. And For the multi-class classification problem, we only need to expand the binary class confusion matrix outward by rows and columns. Given a confusion matrix C with s classes, the performance parameters of ith class are the summation of rows or columns of the matrix C.


 
 
 
 

Table 2 Confusion matrix
Full size table

Data sets
The properties of the microarray data we used are summarized in Table 3. And the detailed class names, tags corresponding to the classes, and the number of each class are presented in Tables 4, 5, 6, and 7. All data sets are taken from NCBI (https://www-ncbi-nlm-nih-gov.ezproxy.auckland.ac.nz/).

Table 3 Profile of microarray data
Full size table

Table 4 LEukemia classes, tags corresponding to classes, number of samples in each class
Full size table

Table 5 Affymetrix 250K StyI SNP classes, tags corresponding to classes, number of samples in each class
Full size table

Table 6 Colon biopsies cancer classes, tags corresponding to classes, number of samples in each class
Full size table

Table 7 MDS cancer classes, tags corresponding to classes, number of samples in each class
Full size table

Comparation of large-scale algorithms for SVM
As we have discussed above, various large-scale algorithms are performed for SVM. But the detailed difference in computing microarray data sets among these methods is indistinct. Moreover, it takes experiments to know which method is more effective for the classification computing of microarray. Therefore, an experiment is performed for comparison.

Table 8 Comparison of some general large-scale algorithms for SVM
Full size table

Table 8 contains six major methods and nine minor approaches, including LibSVM (with linear and RBF kernel functions), SGD-based SVM, Pegasos (with linear and RBF kernel functions), AMM (with a batch method and linear kernel function, and online method with RBF kernel function), BSGD (with strategies of Removal and Merging), mrPSVM [33]. Due to randomness, the result and computing time are different for each computing. Hence, we calculate a total of 20 times and take the average value for them.

Based on Table 8, we can see that LibSVM has the highest computational time cost, especially with the RBF kernel function. Indeed, the classification accuracy is acceptable. To further improve the model performance, we use the particle swarm optimization (PSO) to optimize the SVM. It can be seen that the accuracy of PSO-SVM has improved somewhat, otherwise, the time cost has increased dramatically. Meanwhile, using mapreduce on 5 PCs, the computation time of mrPSVM is reduced to 124 s, while the accuracy is reduced as well. Nonetheless, all the latter algorithms achieve a significant improvement over LibSVM and mrPSVM in terms of computational time and space. It is reliable that SGD-based SVM, Pegasos, AMM, BSGD are independent on the number of samples but depend on the dimensionality of datasets. Unfortunately, the performance of these algorithms degrades somewhat as the dimensionality increases, especially when making classification predictions on SNP datasets (238,230), where the accuracy is highly unsatisfactory.

Although the classification accuracy is lower than LibSVM (with linear kernel function) in LEukemia, the computing efficiency is impressive. Although the computing time is higher than Pegasos, AMM, and BSGD in SNP and MDS, the accuracy is more acceptable. Comprehensive experiments show that SGD-based SVM can achieve art-of-the-state computing time and accuracy. Finally, the SGD-based 
-SVM is performed as a classifier for microarray classification.

Comparation of the loss functions
To clearly understand the differences between the hinge loss function, the squared hinge loss function, and the modified Huber loss function in practical applications, a very necessary experiment is implemented on the microarray datasets. The result is presented in Table 9.

Table 9 Comparison of SGD-based SVM with hinge loss function, the square hinge loss function, and the modified Huber loss function
Full size table

As with the above experiment, we conducted 20 experiments and took their average. The difference in computational efficiency and accuracy between these loss functions is not significant. However, by careful comparison, we can see that the accuracy of the squared hinge loss function is higher than the other loss functions on all data sets, while the computation time of the modified Huber loss function is shorter than the above two. Since the time difference is not significant, and for higher accuracy, we choose to use the squared hinge loss for SGD-based SVM to make the following experiments.

Classification results of microarray
All of the microarray data sets are trained by SGD-based SVM with the squared hinge loss function.

Results of LEukemia and SNP
Fig. 5
figure 5
The over-sampling technique SMOTE is used to increase the minority class samples of the microarray data sets, including LEukemia and SNP

Full size image

Fig. 6
figure 6
The confusion matrix of LEukemia (GSE13159)

Full size image

As we can see from Tables 4 and 5, the number of classes with the largest sample size (448, 242) is over 10 times that of the class with the smallest sample size (13, 15). The classes are called minority classes when the number of samples is less than 100 in LEukemia data. And the classes are called minority classes if the number of samples is less than 50 in SNP data. The strategy is to increase the minority classes of LEukemia with a sample size of less than 100 to 100 and the minority classes of SNP with a sample size less than 50 to 50. We can see the detailed results from Fig. 5.

LEukemia is a multi-class dataset with 18 classes, each of which has a different sample size. The maximum sample size for class 15 is 448, and the minimum sample size for class 1 is only 13. This demonstrates that the sample size of the class 15 is approximately 34 times that of the class 1, which means that the dataset is heavily unbalanced. Therefore, before classifying the dataset, it needs to be balanced immediately using the relevant techniques (over-sampling or under-sampling techniques). Due to the small sample size, the synthetic minority over-sampling technique (SMOT) [8] is used here, which is one of the most popular over-sampling methods. Then, the balanced data are classified in the proposed framework, and the performance is described in Fig. 6. The rows represent the predicted class, the columns are the true class, the green fonts on the far right are the precision, the green fonts on the bottom are the recall rate and overall score (accuracy), respectively. The overall accuracy is 93.1%, which means that this multi-class data set can be classified with a high score, and demonstrates the excellent performance of this proposed method.

Affymetrix 250K StyI SNP has the highest number of features (238,230 features) among the four data sets. The computational time rises as the number of features increases, and the accuracy seems to decrease, which is especially evident for this data set. However, the SGD-based SVM model can deal with this data set quickly, and this multi-class data set can be classified with desirable accuracy (83.7%). The result is shown in Fig. 7.

Fig. 7
figure 7
The confusion matrix of SNP (GSE19399)

Full size image

Results of colon and MDS
Fig. 8
figure 8
The confusion matrix of the Colon (GSE11223). And the confusion matrix of the MDS (GSE15061)

Full size image

There are 202 samples in the Colon data set, where the training set contains 161 samples and the test set contains 41 samples. We can see from Fig. 8a, the recall rate is 78.3%, which indicates that 21.7% of the positive classes (labeled 0) in the data set are predicted to be negative classes (labeled 1). The precision is 94.7%, which demonstrates that 94.7% of the samples with positive predictions are positive samples.

There are 870 samples in the MDS data set, where the training set contains 619 samples and the test set contains 261 samples. We can see from Fig. 8b, the overall classification accuracy is 93.1%, which is higher than [33]. It is important to emphasize that the model not only obtains a high accuracy but also the computation time is only within a few seconds.

Comparison with related works
To further evaluate the effectiveness of our proposed work, we compare it with the state-of-the-art mthods such as mrPSVM, mrKNN, mrPNN, and SET-MLP on LEukemia (GSE13159) and MDS (GSE15061). To the best of our knowledge, the best performance in terms of accuracy for Leukemia is 87.66% in the literature [36]. Liu et al. combined the sparse evolutionary training (SET) method with MLP to improve the computing efficiency and accuracy. The state-of-the-art method performance for MDS (GSE15061) is 84.14% in the literature [33].

The comparative results are shown in Table 10. Our proposed method gives 93.07% and 93.10% for Leukemia and MDS, respectively. This indicates that the work proposed in this paper has significant effitiveness over other related works.

Table 10 Comparison with related works in term of accuracy
Full size table

Conclusions and future work
Several large-scale support vector machine algorithms are used to minimize the running time and improve the computational efficiency of microarray classification, and the sgd-based support vector machine is selected as the microarray classifier. To overcome the negative impact of unbalanced data on the model and to reduce the time cost incurred because of balancing data, the efficient SMOTE algorithm is employed to balance the data. The experimental results show that the classification performance of the balanced data is significantly improved. Although the implementation of the square hinge loss function on the model increases the running time, it improves the performance in terms of accuracy. Compared with the traditional SMO algorithm, the large-scale SVM algorithm significantly improves the computational efficiency while ensuring the classification accuracy, indicating the success of the large-scale algorithm in microarray classification. In addition, although parallel computing can significantly improve storage capacity and computing power, it involves problems of resource sharing, data consistency, debugging difficulty, etc., and the performance requirements of the machine are not reduced, but increased. In conclusion, large-scale algorithms can avoid most useless computations and obtain optimal solutions.

However, it is due to the randomness that the accuracy and computation time of each calculation may be different so that the large-scale algorithms yield unstable results. Future work is likely to be directed toward improving the stability of large-scale algorithms and guaranteeing the effectiveness of approximation.

