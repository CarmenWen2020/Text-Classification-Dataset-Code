Abstract
This research is to design an effective prefetching method required for hybrid main memory systems consisting of dynamic random-access memory (DRAM) and phase-change memory (PCM) components, which can be especially used for big data applications and massive-scale computing environment. Conventional prefetchers perform adequately for regular memory access patterns. However, graph processing applications show extremely irregular memory access characteristics, causing some difficulty in predicting accurate prefetching operation. Therefore, an effective dynamical prefetching algorithm based on the regression method is proposed in this study. We have designed an intelligent prefetch engine that can identify any dynamic accessing characteristics in memory accessing sequences. Specifically, it can select regular, linear, or polynomial regression predictive analysis based on the memory access sequence characteristics, and also dynamically determine the number of pages required for any selected prefetching. We also present a DRAM-PCM hybrid memory structure that can reduce the energy consumption and resolve the thermal issue that hampers conventional DRAM memory systems. Experimental results indicate that the performance can increase by around 40%, compared to that of conventional DRAM memory structures.

Introduction
With the rapid development of big data analysis and the increasing volume of generated data, the efficiency requirement for database management has become increasingly urgent. Considering the demand of low latency performance, several in-memory-based database systems such as SPARK have become a popular trend for data processing.

As shown in Fig.1, in-memory configuration offers nearly the best power consumption and performance compared to other parallel disk systems. To obtain a fast response [1], in-memory processing allocates all the data needed in the main memory to minimize the hard disk accessing time [2]. However, this worsens the bottleneck effect of the memory capacity. Unfortunately, in real database systems, the capacity of the main memory is quite limited. When the memory space that the database requires is beyond the main memory’s capacity, the data in main memory will be constrained to move to an auxiliary storage, which is a considerable drawback for in-memory processing. Various studies regarding prefetching between memory layers aimed to prevent this problem, but in real-world circumstances, accessing the auxiliary storage layer is inevitable. A few studies have focused on prefetching between the main-memory layer and the auxiliary storage layer.

Furthermore, in most commercial database applications, data are positively associated with large-scale graphs. Although several studies have focused on data analysis and mining, there is still great potential for research in graph computing [3, 4].

Fig. 1
figure 1
Power consumption versus throughput in various system structures

Full size image

Therefore, in this study, a dynamic recognition prefetch engine associated with a dynamic random-access memory and phase-change memory hybrid memory is proposed. The prefetch operation is performed dynamically between the main-memory layer and the auxiliary storage layer with a machine learning technology. Specifically, any memory request from the last-level cache will be preprocessed first, then a regression calculation will be performed. Finally, the prefetcher will determine whether to perform prefetching. Moreover, the main-memory layer includes a dynamic random-access memory, and phase-change memory modules that can provide exceptional cost efficiency and performance [4, 5].

Studies have indicated that the performance can be increased by 40% compared to the performance of conventional dynamic random-access memory (DRAM) memory structure. Moreover, the performance can be increased by 3% compared to that of the state-of-the-art prefetch algorithm. The major contributions of our study are summarized as follows:

(1)
A highly cost-effective dynamic random-access memory and phase-change memory (DRAM-PCM) hybrid memory architecture with a prefetch buffer to optimize the performance of the main-memory layer and reduce the total cost for large-scale data processing.

(2)
An efficient machine-learning-based prefetch engine that can intelligently identify any prominent feature of the memory access sequences used to cope with the irregular access pattern of graph processing.

(3)
A dynamical prefetching algorithm is used to dynamically determine the quantity of prefetching one at a time.

In Sect. 2, we review the relative studies regarding the latest prefetch mechanisms and analyse their shortcomings in graph processing. Section 3 presents the structure and algorithm of the dynamic recognition prefetch engine proposed in this study. In Sect. 4, we present the simulated experiments conducted on the proposed scheme regarding performance and energy consumption. Finally, we summarize this study in Sect. 5.

Related works
In this section, we will introduce the necessity of improving the prefetching algorithms that are wildly used. In fact, due to low-latency requirements and hardware constraints, classic prefetchers usually use relatively low-complexity algorithms. However, with the development of hardware technology and the improvement of machine learning (ML) algorithms, many researchers have tried to use ML technology for address prediction in recent years. Therefore, one of the goals of this article is to try to combine the efficiency of classic algorithms with the wide applicability of ML algorithms. At the same time, when designing the ML algorithm, we selected a mechanism with moderate complexity and better fit and used statistical error analysis to optimize the prediction results.

Due to the rapid development of the central processing unit (CPU), in modern Von Neumann machines, the hardware factor that limits system performance is no longer the calculation speed of the CPU but the delay when the memory miss occurs. In another word, the memory system has become a bottleneck for the performance of an entire modern system. Thus, prefetchers are used to predict and pre-load data, which can then be accessed by the system to use directly instead of waiting to read data from a lower-level storage.

Theories for predicting data are generally based on time and spatial locality. Prefetching schemes such as the global history buffer (GHB), program counter/delta correlation (PC/DC) and stride can compare differences between data addresses. They are highly accurate and efficient for regular access patterns [6, 7]. However, one of the most distinguishing characteristics of graph processing is irregular memory access patterns [8]. In addition, both SMS and Sarsa prefetchers do not perform adequately when workloads have either a low spatial locality or low semantic locality [9, 10].

Fig. 2
figure 2
Trace map of misses from last-level cache using a triangle count (TC) and b kcore as workloads

Full size image

Figure 2 presents the irregularity of memory requests in graph processing. The x-axis denotes the data accessing sequence in time, and the y-axis indicates the address value. According to the figure, access patterns of graph processing cover a massive range of memory space without several visible regular characteristics.

To deal with massive irregular patterns, some aggressive methods may be needed. However, aggressive prefetching methods come with risks of misprediction, which could lead to the failure of the entire system. Zhuang et al. and JT Yun et al. proposed a request table method to reduce the risk of inaccurate prefetching by distinguishing and discarding bad prefetching results [11, 12]. Moreover, JT Yun et al. took a further step considering machine intelligence by proposing a simple linear regression prefetcher with preprocessing.

In the structure proposed by JT Yun et al., the data addresses requested from the last-level cache to the main memory are recorded in the request table. It preprocesses the data in the request table, divided into three groups according to the address value. Among the three groups of A, B, and C, the most extensive data sample size is regarded as a hot zone. Then, a  address space (256 pages) is used as a threshold to denoise the data in the hot zone and remove address values exceeding 256 pages [13]. At this point, the preprocessing is complete, after which a simple linear regression is performed on the hot zone’s denoising data to obtain the predicted value.

Although this type of algorithm is simple to compute, it contains several potential problems. First, the preprocessing method is exceedingly radical and straightforward. As shown in Fig. 2, the graph processing access patterns are evenly and widely distributed in each address range. After simply dividing the data into three groups, the differences in data sample size between the groups may not be apparent. That is, the divergences between the hot and cold zones may not be apparent. Therefore, the method cannot ensure that the next access address will be in the hot zone. Because it uses this as a basis to perform linear regression calculation, the result obtained has a high risk of misprediction.

Second, the model uses a simple linear regression to make predictions. It considers the most recent access request as a reference and returns a single prediction result. In an actual operation, the prefetcher preloads the data in the address space of the calculated prediction result and 4 KB after it (i.e., 1 page). According to the property of the linear regression method, if the access pattern conforms to the linear regression characteristics, the next actual access address value A should obey the Gaussian distribution with the predicted result A as the mean and  as the variance as follows:

Selecting only the prediction result A and the address space of the following 4 KB presents an apparent error, which may lead to mispredict the actual data that we have already found.

To address these issues, we designed a full mapping mechanism that can intelligently select the prefetch mode by using the history table method. We applied a secondary table mechanism (without threshold) that matches a global and a local history table to comprehensively and efficiently collect memory requests. By using a few initial input values as training data, we learned and marked the access pattern as the regular or irregular mode. Thus, we can combine the advantages of both conventional regular prefetch algorithms and aggressive methods like machine intelligence algorithms. And also, we provides a scheme for dynamic prefetching based on the Gaussian distribution and confidence intervals to deal with the error for linear regression.

Last, in terms of machine intelligence algorithms, the model given by JT Yun. only used simple linear regression as its predictive method. When the access pattern’s linear feature is fragile (linearly independent), the model cannot provide correct prefetch results. Therefore, our study uses the polynomial regression scheme to ensure that linear regression will be performed when the access mode is linearly related, and the polynomial regression algorithm will be performed when it is linearly independent [13, 14].

Architecture and algorithm implementation
In this section, we will introduce the architecture of our model and the specific implementation process. After the analysis in previous, we have clarified the three key issues required in our model.

(1)
An intelligent identification mechanism can quickly identify whether the memory access sequence is regular. And call the corresponding prefetch engine according to the recognition result.

(2)
An efficient machine-learning-based prefetch engine with moderate complexity that can cope with the irregular access pattern of graph processing.

(3)
A dynamical prefetching algorithm that works with the ML prefetch engine. The dynamic engine uses the method of statistical error analysis to dynamically determine the quantity of prefetching one at a time.

To achieve the above functions, we need all prefetch algorithms to be executed by the hybrid main memory management unit (Sect. 3.1), independently from the operating system. Therefore, this model will not occupy system resources and cause additional overhead.

Overall architecture
Our proposed model consists of six major modules: a prefetch buffers to store prefetch pages, a prefetch controller to manage the prefetch engine, a prefetch engine, a history table to analyse addresses recently accessed to main memory, a DRAM-PCM hybrid main memory, and a hybrid main memory management unit to manage hybrid main memory as shown in Fig. 3.

Fig. 3
figure 3
Overall architecture

Full size image

We choose PCM as the non-volatile memory combined with DRAM because of its excellent performance. PCM chips exhibit relatively excellent reading and writing speeds and writing durability. It can also provide large storage space without highly integrated, which solves the traditional DRAM systems’ thermal problem.

The prefetching engine we designed is not limited to DRAM-PCM hybrid memory. The hybrid memory structure proposed in this research is suitable for the combination of DRAM and various non-volatile memory materials. This article only uses PCM as a representative for experiments and discussion. In fact, as the core of this study, the prefetching algorithm also shows an excellent versatility on the conventional pure DRAM memory structure (Sect. 4.5).

History table
In our proposed model, the first step is to classify and preprocess the memory requests. Because the access patterns of the graph processing will cover an expansive memory space, comprehensive statistics on all requests will cause significant overhead and latency. Therefore, we applied a secondary table mechanism that matches the global and local history tables to comprehensively and efficiently collect memory requests.

The first-level table is a global table with 128 entries that records the higher 46 bits of the fetch address. The global table will monitor all the memory space and make updates recorded entries following the first in first out (FIFO)’s principle according to time locality.

The second-level table with 512 entries will correspond to each entry in the first-level table and record the lower 18 bits of the corresponding address, that is, the offset of each entry in the global table. The second-level table follows the spatial locality property and will update in the least recently used (LRU) mode as the local table. Grouping data according to high bits of addresses can not only monitor the entire address space, but also effectively reduce the amount of calculation for regression analysis.

Regular pattern prefetching
After preprocessing the data, we analyse each entry in the global table. Owing to an apparent feature of regular pattern prefetching, which is low overhead, we first determine whether memory access sequences are regular patterns. We adopt a method similar to comparing the offset differences in GHB-PC/DC and stride algorithms to analyse the data. If the delta between the request sequences has prominent regular characteristics, prefetching will be processed as shown in Algorithm 1. We set data size compared by the regular prefetch algorithm to 3 to 5. When the sample size is only 1 or 2 groups, it is impossible to judge whether it contains a pattern, so the prefetch engine will conservatively select the next page to prefetch. The regression algorithm can capture the characteristics of the access sequence better with a sufficient amount of data. Thus, when the data size exceeds 5, the procedure will be handed over to the regression algorithm.

figure a
Regression analysis
If there is no prominent regular characteristic in the memory access sequence, we switch the prefetch mode to regression analysis. Regression analysis is a statistical analysis method widely used in the field of machine learning. It fits the most appropriate hypothetical curve to existing data and predicts the data’s potential trend through this curve [14, 15]. When applied to machine learning, the existing data are used as the training set; as the number of data increases, the regression parameters are continuously learned and modified to dynamically generate prediction results [16]. Regression analysis methods are divided into unary regression and multiple regression analyses.

In this study, we use the least-squares method for unary regression analysis. We consider the time sequence corresponding to each offset in the local table as X and the specific address value as Y, and then sort out a set of (x, y) pairs as the training data for machine learning algorithms. The unary regression analysis includes linear regression and nonlinear regression analysis. In the design proposed by Yun et al., a prefetching scheme of unary linear regression was implemented as follows [13]:


 

(2)

where X is an independent variable vector, and Y is a dependent variable vector. Meanwhile,  indicates the regression coefficient vector.

However, in actual experiments, we found that the irregularity of memory access sequences is significantly complicated, and the fitting line of linear regression can only cover a part of the memory accesses. Several memory requests that do not conform to linear characteristics remain. Therefore, we adopted a univariate polynomial regression analysis method. When the memory access sequence is linearly independent, the data are fitted to a hypothetical curve to predict the next memory access address as follows:

To prevent excessive overhead and over-fitting, we limit the highest power of the polynomial regression to the second power and calculate the coefficient matrix by the Gaussian-Jordan elimination method [17].

In addition, the method calculates the linear correlation coefficient and confidence interval. If a significant linear correlation exists, then the linear regression will continue to be selected in the next prefetching. If the linearity is irrelevant, a quadratic polynomial fitting will be performed, and the prediction result will be calculated. The pseudo code is shown in Algorithm 2.

figure b
As shown in Algorithm 2, when there is no prominent regular characteristic in access sequence, the regression prefetch engine will be activated. We classify and discuss the prefetch modes according to the number of samples. When the sample size is less than 3, it is difficult to determine the memory access pattern. Therefore the prefetch engine will directly prefetch the next page of memory data at the current address. In fact, in the previous stage of regular prefetching, this issue should have been covered. A judgment condition is added here as an auxiliary guarantee to avoid additional overhead.

When the sample size reaches 3, we will perform the first training. First, check the linear correlation coefficients of these 3 data to determine whether they have linear correlation characteristics. If they are linearly correlated, record the dataset as linear regression and save the fitting coefficients. If the samples are linearly independent, the polynomial fitting method will be used to predict the next address.

After the initial training, when the miss occurs again, we only need to calculate the loss function between the new memory address and the fitting function to know how the parameters of the current training model need to be adjusted.

Dynamical prefetching
In linear regression analysis, the prediction result should obey the characteristics of the Gaussian distribution. We choose to dynamically increase the number of prefetched memory pages based on the confidence interval. When the prefetcher performs linear regression with the highest power of 1, we calculate the 
 
 
 interval, which has a confidence of 66.7%. Then, we compare this interval with the 12 KB address space (3 pages) and consider the intersection. As a result, the dynamical prefetcher could pre-load 13 pages depending on the confidence interval.

All prefetched memory pages are stored in an independent prefetch buffer. Before it is stored in the prefetch buffer, a redundancy check will be performed to ensure that there is no duplicate data in the DRAM-PCM main memory. The dynamic prefetching procedure will be performed following Algorithm 3.

figure c
Result and discussion
GraphBig
In actual commercial database workloads, graphs play a key role because big data applications that consist of entities with internal links naturally form a graph.

For further investigating graph-like data processing, the Linked Data Benchmark Council (LDBC) released a series of social network benchmark (SNB) [18]. The workloads (person-knows-person relationship) were generated by LDBC-SNB Data Generator, and they contain the connection information of 1000K of people. It mimics real-world graphs such as social networking services.

Moreover, Lifeng Nai et al. from IBM proposed the GraphBig suite for running graph processing kernels [19].

Simulation configurations
The workload benchmarks we choose to evaluate our proposed method are gathered the CPU access trace using PinTool [20]. The workloads are listed in Table 1. We used a trace-driven simulator to simulate the first, second, and third levels of cache as well as the DRAM-PCM hybrid main memory. The proposed system configuration and simulation parameters are listed in Tables 2 and 3 [21, 22].

Table 1 Graph processing workloads
Full size table

Table 2 Proposed system configuration
Full size table

Table 3 Simulation parameters
Full size table

Optimal history table and DRAM size
To obtain the best system performance, we need to find the optimal history table and memory size configuration.

The first step is to determine the size of the history table. In theory, if the local history table records more request entries, the more accurate the prefetching result will be. But if the local history table is too large, storing the whole history table will cause unnecessary overhead. The calculation process will also become more complicated and increase the risk of overfitting. Moreover, according to experiments, when the local history entries increased from 64 to 512, the hit rate increased accordingly. However, when the entries continue to grow, the hit rate seems to reach saturation, and it may even decrease due to overfitting. Therefore, based on the result shown in Fig. 4, we settled the number of global history entries in the history table at 512.

Fig. 4
figure 4
Hit rate of different local history entries

Full size image

The second step is to find the appropriate ratio and capacities of the DRAM and PCM memory in our model. To this end, we conducted a series of experiments. We set the memory capacity in our proposed model as 128MB/256MB/256MB DRAM and 1GB/2GB/3GB PCM memories, respectively.

The results of the experiments are shown in Fig. 5. The x-axis represents memory capacity configurations, and the y-axis shows execution time. For workloads, such as Spath, TC), increasing memory space can reduce its execution time. But for other workloads (such as Dcentr, BFS), continuously increasing memory does not linearly reduce the execution time. On the contrary, increasing memory will increase energy consumption as well. .Considering the trade-off of reducing the execution time and ensuring less energy consumption. We selected 256MB DRAM and 2GB PCM as the optimal choice.

Fig. 5
figure 5
Normalized execution time for various memory configurations

Full size image

Experimental result
Figures 6 and 7 present the comparison results of the execution time and energy consumption.

Fig. 6
figure 6
Normalized execution time for various GraphBig workloads with different prefetching methods

Full size image

Fig. 7
figure 7
Normalized energy consumption for various GraphBig workloads with different prefetching methods

Full size image

We measured the execution times and energy consumptions of different prefetching algorithms under different workloads, compared the conventional DRAM memory structure without prefetching as the benchmark value, and calculated the normalized execution time and energy consumption.

According to the resulting figures, our model reduced the execution time by a maximum of 50% compared to the traditional DRAM structure, and the energy consumption was reduced by 21%. Our model also reduced the execution time by 40% and the energy consumption by 18%, on average. Compared to the model proposed by JT Yun, which is state-of-the-art, it excelled in performance by approximately 3% in both execution time and energy consumption.

Figure 8 presents the PCM lifetime under different prefetching algorithms.

Fig. 8
figure 8
PCM lifetime for various GraphBig workloads with different prefetching methods

Full size image

The system life calculation formula is as follows [23]:

where S is the memory size, B is the traffic of the writes, and W is the limitation of cell endurance. According to Fig. 8, in the best case, our model improves the lifetime of PCM by 36% at maximum. And in the worst case, our model can still improve by at least 2%. The last group of data named “AVG” in Fig. 8 shows the average lifetime of the PCM of each algorithm under all different workloads. Compared with other algorithms, our model improves the lifetime of PCM by 624% on average.

The versatility to conventional DRAM memory structure
The prefetching algorithm used in this research is based on the DRAM-PCM hybrid memory structure. But this scheme cannot only be applied to DRAM-PCM hybrid memory but also has good versatility for other memory structures. Especially in the conventional pure DRAM memory structure can also show excellent performance.

To this end, we used the same experimental equipment to build a simulation environment for the traditional DRAM memory structure. In the conventional DRAM analysis environment, we used the same setting parameters as the DRAM-PCM hybrid memory to achieve the purpose of the control experiment. Namely: 17MBbuffer, 128+2046MBDRAM main memory.

Fig. 9
figure 9
Normalized execution time and energy consumption

Full size image

We can see in Fig. 9. Using the prefetching algorithm designed in this article in the conventional pure DRAM main memory structure can save about 50% of the execution time at the maximum, and the minimum saving time can also reach 25%. This performance result is similar to that when using the DRAM-PCM hybrid memory structure.

In terms of energy consumption, since the PCM memory chip with lower energy consumption is not applied, the total energy consumption is higher than that of the DRAM-PCM hybrid memory structure. However, compared to the conventional DRAM main memory without any prefetching algorithm, it is still able to optimize energy consumption by 2% to 18%.

Conclusion
In summary, this article provides an effective prefetch model for the DRAM-PCM hybrid main memory structure that can dynamically prefetch multiple pages based on the machine learning algorithm of polynomial regression. In our model, the memory request from the last-level cache will be preprocessed first, followed by a regression calculation. Finally, the prefetcher will determine whether to perform prefetching, as well as the number of prefetched pages according to the confidence interval of the calculation.

The experiments show that this model has sufficient adaptability to irregular storage models. Compared with the performance of conventional DRAM memory structure, the performance of our model increases by 40%; compared with the latest prefetch algorithm, it also increases by 3%.