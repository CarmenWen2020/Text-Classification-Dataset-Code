Abstract
Automated guided vehicles (AGVs) are Internet of Things robots that navigate automatically as guided by a central control platform with distributed intelligence. Different methodologies have been proposed for AGV visual tracking applications. However, vision-based tracking in AGVs usually confronts the problem of time delay caused by the complexity of image processing algorithms. To balance the trade-off among algorithm complexity, hardware cost and performance, precision and robustness are usually compromised in practical deployment. This paper proposes a prototype design of a visual tracking system. Edge computing is implemented which migrates computation intensive image processing to a local computer. The Raspberry Pi-based AGV captures the real-time image through the camera, sends the images to the computer and receives the processing results through the WiFi link. An improved Camshift algorithm is developed and implemented. Based on this algorithm, the AGV can make convergent prediction of the pixels in the target area after the first detection of the object. Relative coordinates of the target can be located more accurately in less time. As tested in the experiments, the system architecture and new algorithm lead to reduced hardware cost, less time delay, improved robustness and higher accuracy in tracking.

Introduction
Autonomous guided vehicles (AGVs) are automatic load machines which are able to carry items from one place to another. With the development of modern factory environment, ever-growing demands for AGVs have been raised. This leads to a need of AGVs with increased efficiency and reliability [1]. Different methodologies have been proposed for AGV applications. Especially, the AGV visual tracking application has been a hot research topic because of the development of vision-based signal processing technology. Meanwhile, AGVs are also Internet of Things (IoT) robots. According to Ericsson’s report, it is predicted that there will be more than 24.9 billion IoT devices by 2025 [2]. Rapid growth on the usage of AGVs is expected. To compensate limited computation power of AGV with balanced time delay, edge computing presents a natural solution. However, the field of AGV visual tracking with edge computing still has many problems to deal with. Due to the time delay, influence of the environment and image noise, real-time tracking technology is still a challenging issue. Although researchers have made huge achievements in solving the problem of visual tracking theoretically, practical implementation of AGV visual tracking still has many gaps to fill [3].

Mean shift algorithm has been widely used as a light-weighted target tracking algorithm with density-based clustering [4]. As an improvement in the mean shift algorithm for target tracking, Camshift (Continuously Adaptive Mean Shift) algorithm updates the search window size according to the target’s size and accurately locates the target with size change [5]. It has been used in the area of human motion pursuing and face tracking. However, when calculating the histogram distribution of target template, Camshift algorithm does not use kernel function for weighting, which would result in every pixel in the target area having the same weight in the target model [6]. Therefore, the antinoise ability of Camshift algorithm is lower than that of mean shift tracking algorithm. At the same time, Camshift algorithm does not define candidate targets and directly uses the target template for tracking [7]. In addition, Camshift algorithm refers to the Hue component of the HSV color space, which depends only on the color information in the target tracking. In this case, when the target and the background colors are close, Camshift will be automatically executed. The tracking window will be expanded and sometimes even exceeds the whole video tracking window size, leading to inaccurate target positioning, continuous tracking error and the loss of the target [8]. Meanwhile, how to implement Camshift algorithm in AGV visual tracking at an edge computing environment effectively requires careful consideration.

This paper proposes an improved Camshift algorithm for AGV visual tracking with edge computing system architecture. A Raspberry Pi-based AGV prototype is constructed. It captures the real-time image through the camera and is regarded as the end node. The images are transmitted back to the server computer for processing which is regarded as the edge. The edge adopts the method of dynamic visual tracking based on the improved Camshift algorithm to carry out multi-scale detection on collected images in order to track a specified target. By locking the target in the real-time image, its relative coordinates can be obtained. The edge then sends back the maneuver instructions to the end node.

The paper is organized as follows. Section 2 presents the hardware design and proposed improved Camshift algorithm. Section 3 details the experimental setup. The analysis of the experimental results is provided. Section 4 provides further discussions on the design issues. Conclusions are drawn in Section 5.

Hardware and software design of the prototype
AGV prototype
Automated guided vehicles (AGVs) are Internet of Things (IoT) robots that navigate automatically as guided by a central control platform with distributed intelligence [9]. In this design, Raspberry Pi with a camera is used to build the prototype of AGV. Although the prototype has the functionality less powerful than a practical industrial AGV, it is adopted to test the design concepts and algorithms.

In this paper, the model of the Raspberry Ri is 3B+ [10]. It was regarded as the CPU of the AGV prototype. L298N is a special drive IC, which is a part of H bridge IC. The input end of L298N can be directly connected with microcontroller unit (MCU) that is the reason why it is very convenient to be controlled by MCU [11]. At the time that the DC motor is driven, the stepper motor can be directly controlled for forwarding and reversing by changing the logic level of the input. In order to avoid the interference of the motor to the single-chip microcomputer, this module adds an optocoupler for photoelectric isolation, so that the system can work stably and reliably. Therefore, the L298N is particularly suitable for the driving module of intelligent car and was adopted in the AGV prototype.

HC-SR04 [12] is an ultrasonic sensor for obstacle avoidance. It realizes ranging through triggered return signal in the AGV prototype. Eight square waves of 40kHz are automatically sent by the module. It then detects the signal returns. Through the periodic measurements, ranging and obstacle avoidance are achieved.

Edge computing environment
An ideal opportunity for the development of AGV visual tracking application is provided by the emergence of edge computing environment [13, 14]. In this paper, the AGV prototype is regarded as the end node. The platform (a laptop computer used in the design) is regarded as the edge. Because of limited computation power of Raspberry Pi, its microcontroller cannot afford intensive image processing in a short time, which would cause unacceptable delays between two continuous frames. In this case, a highly efficient server is a reasonable solution to replace the Raspberry Pi as a controller. Raspberry Pi captures real-time images and encodes images by Unicode. The encoded information is then transmitted to PC server by UDP protocol. Once the target information is extracted and determined with a threshold value, internal driving instructions of the AGV prototype are transmitted back to the Raspberry Pi from PC by UDP protocol. As a connection-less transport layer protocol, UDP does not offer the reliability and simply sends out datagrams that the application passes to the IP layer [15]. By avoiding the setup of a connection between the client and the server before transmitting the datagram, UDP is able to maintain a quick speed that suits the needs of this application.

In our application, the edge environment can be divided into four layers: hardware layer, operating system (OS) layer, data evaluation layer and service migration layer which can be seen in Fig. 1 [16]. The hardware layer contains the designed AGV prototype, sensors, Raspberry Pi, camera and other hardware parts in this design. The laptop computer can be regarded as an operating system (OS) layer which plays a role of monitoring the usage of all kinds of resources from the hardware layer [16]. The data evaluation layer is used to assess consumption statement of edge node. The edge evaluation is conducted according to designed improved Camshift algorithm. The service migration layer is used to migrate edge services. It is regarded as the designed program in the system and is the core of the edge environment. If a practical AGV system architecture is relatively simple, e.g., with limited AGV end nodes and tracking precision requirement, service may be localized in a single platform. In this case, the service migration layer is limited to data migration only.

Fig. 1
figure 1
Edge computing environment

Full size image

Camshift algorithm
The original Camshift (Continuously Adaptive Mean Shift algorithm) is reviewed in this section. The improved Camshift algorithm is detailed in Sect. 2.4. Camshift is an improvement in the mean shift algorithm. It updates the search window size according to the target size and accurately identify the target with size change [17]. Meanwhile, it is also able to track the target changing size in video streaming. The basic principle is related to the color information of the moving item in each video frame. This information works as characteristics of the input image to execute mean shift operation in every input image, respectively. After that, the target center and search window size of the previous frame are taken as the initial values of mean shift algorithm of next frame. In this way, continuous target tracking can be realized. Because the position and size of the search window are close to previous state, and the moving target is usually near this area, the search time is shortened. Therefore, if Camshift algorithm is used on two sets of pictures, it obtains two sets of corresponding center points. Theoretically, these points are at the same location in the world coordinate system. That means these two sets of points could be used to calculate the space coordinate [18].

The Camshift algorithm adopts the HSV (hue, saturation and value) color mode instead of the RGB model with primary colors of R (red), G (green) and B (blue). With a RGB color model, morphological characteristics of the images cannot be reflected fully, which only shows deployment of color from the principle of optics [12]. The reason why Camshift algorithm converts the image from the RGB color space to the HSV is that the HSV color model is less affected by changes in the illumination intensity than the RGB color model [8]. H component is used in the algorithm to set up a histogram model of the tracking target. It normalizes the obtained result to 0∼255. By replacing the value of each pixel in the image with the probability pair in which the color appears, the color probability distribution map is produced. An appropriate threshold is then set to binarize the color probability map.

By iterations, the Camshift algorithm updates the search window size based on the orientation [21,22,23]. The direction information of the target is reflected by the second moments ,  and  of the search window as formulated below:

By defining the intermediate variables a, b and c, it is able to calculate the length and width of the next frame search window in a closed form and they are given as

The direction angle, length and width are computed as:

The initial size of the search window for the next frame is given as [8, 19–22]:



(10)



(11)

Camshift algorithm improves the second defect of mean shift tracking algorithm. During the period of tracking, it updates the search window size according to the target size and accurately locates the target with size change [17]. However, when calculating the histogram distribution of the target template, Camshift algorithm does not use kernel function for weighting, which would result in every pixel in the target area having the same weight in the target model. Therefore, the antinoise ability of Camshift algorithm is lower than that of mean shift tracking algorithm. At the same time, Camshift algorithm does not define candidate targets and directly uses the target template for tracking. In addition, Camshift algorithm utilizes H component of the HSV color space and target histogram framework, which heavily relies on the color of the target information in tracking. In this case, when the target color and the background color are close, Camshift will automatically be executed and the tracking window will be expanded. Sometimes this expanded tracking window may exceed the whole video frame size, leading to inaccurate target positioning, continuous tracking errors and the loss of the target. An improved Camshift algorithm is proposed and detailed in Sect. 2.4.

Integrating Kalman filter algorithm with Camshift and windowing
Kalman filter algorithm is regarded as one of the most famous Bayesian filter theories which is a linear optimal status estimation method [24]. For the tracking of a real-time target, the Kalman filter algorithm ensures a tracking solution to be optimal for different real-time tracking objects if essential parameters are defined. The Kalman filter can ensure ideal estimates based on the data available given confirmable and statistical properties of the system parameters and measurements [25]. Popular variations of Kalman filters include extended Kalman filter (EKF) and un-scented Kalman filters (UKF).

In this paper, conventional Kalman filter is applied to predict the tracking target. The tracking algorithm takes into account the dynamic nature of the background. It decreases negative effects of the environment such as light and similar background in order to improve the tracking performance. It is been combined with the Camshift algorithm for AGV visual tracking in a real-time processing system at the edge computing environment. The key algorithm is summarized as follows.

Step The initial values of the parameters are defined in advance. For example, the Gaussian white noise covariance of measurements and estimates are supposed to be stable at the time while the frames in the tracking process are changing. This is compatible with the concept of the Kalman filter algorithm [26].

Step By manually selecting a designated tracking target, we obtain the coordinates of the central point for the selected tracking area (x, y), frame window width w and length h.

Step 3: Start the measurement. Use Kalman filter to predict the location of the target window. Current frame of the tracking process is defined as k. Based on the previous frame of tracking process, the frame of the tracking process can be predicted.

Step 4 With frames being predicted, it is time to collect the current measured frames. According to the converged predictions, images are truncated to the predefined the window size (w and h). Combining predicted frames and the measured frames with windowing, Camshift is applied to calculate optimal location and size of the target at the time of k.

Step 5 Update the covariance under the state of k. The system then operates recursively from k to k+1.

Section 3 provides more details on the implementation of this improved Camshift algorithm.

Experimental results
Working process of the system
The prototype system contains a camera, Raspberry Pi 3B+, pre-designed program and sensors for obstacle avoidance. A laptop is regarded as the platform in the simulated edge environment. Images are collected by the camera of the AGV node. The images are transmitted to the platform (edge) through wireless data transmission (WiFi). At the edge, these images are analyzed by improved Camshift algorithm. Decisions on tracking actions are then sent to the AGV end node to control its movement. The whole process can be summarized in Fig. 2.

Fig. 2
figure 2
Working process of the system

Full size image

Target tracking experiment
The experiment involves detection and identification of a specific object. The rigid object is used to characterize the constant characteristics of the pixel when moving. The convergence area of target pixel is calculated for each frame in the video stream. The central area of the target pixel aggregation is then obtained achieving real-time tracking.

At the beginning of the tracking function, the PC sever needs to set the receiving mode and wait for the Raspberry Pi to transmit the image, which is shown in Fig. 3.

Fig. 3
figure 3
The edge waits for the image from end node

Full size image

The accuracy and robustness of designed improved Camshift algorithm is tested by experiments with the AGV prototype. Another AGV is regarded as the tracking target (the one in blue mask). The third AGV with a similar blue color is added to the scenario as the disturbance imitating dynamic environmental factors.

Except the combined Kalman filter with Camshift algorithm, the improved algorithm also uses a selecting window method. A selected tracking area is determined which is based on the tracking target. This area is selected in the first frame. Once this area is defined at the initial stage, the rest of the area would be painted white or dark which decreases the interference from the background environment. The adjusted algorithm with simple preprocessing works well for this single target AGV tracking problem. Figures 4 and 5 demonstrate graphically the different tracking performance of original Camshift and improved Camshift tracking schemes. Figure 4c and d shows inaccurate tracking scenarios. Figure 5 demonstrated accurate tracking during the course of movement. The intermediate steps with windowed images are enclosed at the bottom for illustration.

Fig. 4
figure 4
Inaccurate target tracking with original Camshift algorithm

Full size image

Fig. 5
figure 5
Accurate target tracking with improved Camshift algorithm

Full size image

More numerical analysis results are detailed in Table 1, Table 2 and Fig. 6. Table 1 shows the true and predicted coordinate points at the center of the frames in 15 frames. It demonstrates the accuracy of the improved Camshift algorithm. The indicated x and y coordinate points are the central points of the tracking area in the frames. To ensure the tracking area being a rectangle of pixels, values of predicted pixel points are rounded down. According to the data in Table 1, it can be concluded that the improved Camshift algorithm has a high accuracy in tracking.

Table 1 True point versus predicted points with improved Camshift
Full size table

Table 2 Time delay of Camshift versus improved Camshift
Full size table

Fig. 6
figure 6
Time delay of Camshift and improved Camshift

Full size image

The experiments also demonstrated reduced time delay and improve stability in tracking as shown in Fig. 6 and Table 2. Traditional Camshift algorithm processes the whole image captured by the camera. The calculated information entropy of the image is about 6.49856. The designed improved Camshift algorithm with the windowing function focuses on a portion of the image. The updated image after preprocessing has a reduced entropy of 1.80633. Windowing saves the processing time by reducing the convergence time and leads to less time delay. The reduced entropy leads to the stability of the time delay among frames as shown in Fig. 6: The time delay of original Camshift and improved algorithm are illustrated. Table 2 compares the difference of the two algorithms numerically with 20 frames included.

As shown in Table 2, the original Camshift algorithm has an average time delay of about 6.6406 ms. The improved Camshift algorithm has an average time delay of 5.9879 ms. It achieves an average of 12.94 % less time delay. Meanwhile, the improved Camshift is more stable with 51.74% reduced variation of the time delay as shown in Fig. 5.

Discussion
The key features and improvements in the experiments are summarized below.

Essential parameters are first defined which include statement numbers, AGV’s speed, observed quantity, system measurement matrix, state transition matrix and noise covariance.

Enabling the camera, the program based on improved Camshift algorithm starts. In the traditional Camshift algorithm, the next frame is detected from the detection center of the previous frame by mean shift algorithm. However, the target is in a moving state, and when the background color overlaps with the target color, the tracking may shift. Therefore, the improved algorithm uses Kalman filter to predict the coordinates of the moving target and to predict the position of the target in the next frame.

By setting a selection window around the predicted coordinates, the background pixels around the target can be removed, which reduces the possibility of false tracking transfer. Combining Kalman filter and windowing algorithm result in reduced tracking time delay of 12%. It improves the stability and robustness of target tracking with performance gain of 51.74% due to the reduction in complex background interference.

From the test results, it can be demonstrated that the color information of the target can be used as the condition of convergence area after the edge recognizes the target. The coordinates of the target in the image can be locked through the convergence calculation in each frame. Corresponding instructions can be sent to the end node. In addition, by comparing the size of the identified target area with the preset value, the Raspberry Pi AGV can maintain a specified distance from the target.

As the speed of the target AGV varies along the movement, the distance between the prototype tracking AGV and the target, the shape and color of the tracking target, and the complexity of the background can affect the tracking performance. To further clarify the performance variation, additional experiments are conducted. The tracked target object is a blue AGV as shown in Fig. 5. The tracked object has a preset original speed. Deformation and scale variation will occur during the tracking progress. The tracking AGV therefore has its maximum achievable speed of target that will vary for different preset tracking distances based on the improved Camshift algorithm. Tests have been made regarding different tracking distances and preset window sizes. The results are shown in Table 3.

Table 3 The test data in tracking
Full size table

In the experiments, the maximum speed of target is 40 cm/s, the frame per second is 30 f/s and the processing time is close to 0.06 s. The max speed in testing needs to be less than one half the actual width. The calculation formula is given below.



(12)



(13)

Therefore, a small default window can be set to make the tracking distance longer when fast moving objects need to be tracked. Moreover, if it needs to drive on a specified route, a large default window should be set up for close tracking.

Conclusions
A prototype AGV tracking system was built and tested in the edge computing environment. A new tracking algorithm was proposed which was based on the improved Camshift algorithm combining Camshift, Kalman filter and windowing algorithm. The designed system tackles several unsolved problems in the original Camshift algorithm for AGV tracking including adaptive color tracking, interference cancelation under dynamic background and maintenance of constant tracking distance. The experimental results demonstrated reduced tracking time delay of 12%. The stability and robustness of target tracking has a performance gain of 51.74%. The system architecture provides a solution to balance limited computation power of individual AGVs and the requirement of computation-intensive image processing for AGV tracking. The experimental results also revealed the challenges encountered in the current AGV tracking system design including speed limit and complexity in the color/shape of the target, which worth further investigation.