Abstract
High-precision point cloud maps have drawn increasing attention due to their wide range of applications. In recent decades, point cloud maps are normally generated by simultaneous localization and mapping (SLAM) methods, which favor real-time performance over high precision. These methods generally focus on trajectory accuracy resulting in the unclearness of map accuracy. Therefore, to build a high-precision point cloud map and evaluate the mapping performance directly, this study proposes a tight coupling mapping method that integrates the error-state Kalman filter (ESKF), the general framework for graph optimization (g2o), and the point cloud alignment. An ESKF and a g2o are both used to improve the precision of the mapping process. Also, experiments based on a mobile mapping backpack prototype are conducted to verify the proposed method. Targets in the environment and a high-precision reference point cloud map are used to directly evaluate the map performance. The results indicate that the generated point cloud map is sufficiently precise and can reach the centimeter level.

Introduction
In recent years, building a high-precision point cloud map has elicited considerable research interest [1]. Studies of the mapping technique have revealed broad prospects for applications in autonomous driving [2], building information modeling [3], simultaneous localization and mapping (SLAM) [4], and semantic information extraction [5].

Light detection and ranging (LiDAR) is a typical sensing system used in mapping methods [6] to provide accurate and dense point clouds corresponding to object surfaces and environments [7]. In general, a point cloud in a full revolution of the spinning LiDAR is defined as a frame. A mapping process is to splice LiDAR frames obtained at different times and poses (positions and attitudes). For this reason, the basis of mapping is a point cloud alignment algorithm, which can provide transformation between different frames. To further improve the performance of mapping, optimization methods are needed to adjust the raw transformation information. In this paper, the ability of an offline mapping method to build high-precision point cloud maps is explored based on an optimization method considering the error-state Kalman filter (ESKF) and the general framework for graph optimization (g2o).

The main contributions of this paper are:

The construction of a novel tight coupling mapping framework to integrate the ESKF, g2o, and point cloud alignment. This paper attempts to use the ESKF and the g2o simultaneously and tests this combination on a newer mobile mapping backpack prototype platform. The experiment results show that the map generated by the proposed framework can reach the centimeter level.

The use of the ESKF to provide an initial estimation for the point cloud alignment. The experiment results demonstrate that this option provides more accurate initial estimations compared to traditional methods, such as the direct inertial measurement unit (IMU) integration and the normal distributions transform (NDT).

In the remainder of this paper, related works are first introduced. Then, the complete workflow of the tight coupling mapping framework that integrates the ESKF, g2o, and point cloud alignment is demonstrated in detail. Finally, experiments based on a mobile mapping backpack prototype are conducted to verify the feasibility of the proposed method. The experiment results show the accuracy of the built point cloud map can reach the centimeter level.

Related work
A point cloud alignment method is an essential part of mapping [8]. The NDT and the iterative closest point (ICP) are two of the most classic point cloud alignment algorithms. The NDT is a popular method for generating low-resolution point cloud alignments with large pose variations [9]. Among ICP variants, the generalized iterative closest point (GICP) is a generalization of the ICP algorithm which combines the Point-to-Plane ICP algorithm and Plane-to-Plane ICP algorithm [10, 11]. The GICP is chosen in this paper. One reason is that it is more robust compared with the traditional ICP, while another reason is that it has relatively higher accuracy compared with the NDT when an accurate initial estimation can be provided [12]. There are also other methods using features, such as lines or planes for the point cloud alignment [13]. In theory, the method proposed in this paper is compatible with them all, and thus this paper uses these point cloud alignment methods directly.

The optimization framework is the focus of this paper. The extended Kalman filter (EKF) [14] and the general framework for graph optimization (g2o) [15] have been identified as general map optimization methods. In a traditional EKF-based optimization method, IMU data are used to predict the state vector—the pose and velocity of the platform—while the point cloud alignment results are used to correct this state vector [16]. In g2o-based optimization implementation, poses are optimized as the vertices of the graph, and the edges of the graph are the constraint condition [17]. Each vertex corresponds to a six-degrees-of-freedom pose [18]. Here, edge constraints may represent either pre-integration parameters, such as odometry displacement, IMU integration, or the result yielded by the point cloud alignment [19]. A factor graph (FG), a newly emerging optimization method, is a nonlinear optimization method to integrate diverse information. Different from an EKF (a Markov chain) which only considers the last previous state information, the FG effectively considered all the historical information [20]. This paper endeavors to combine it and a g2o to make up for this shortcoming. Additionally, the ESKF, a widely used variant of the EKF, is selected by this paper [21, 22] because it outperforms the traditional EKF in terms of robustness and linear approximation [23, 24]. Table 1 compares different optimization methods regarding real-time, historical information, and nonlinear, where ✓ means yes, ✗ means no, ☆ means satisfactory, and ★ means good. Also, SLAM algorithms are traditional methods to build point cloud maps in the field of robotics [25]. These methods generally center on trajectory accuracy rather than map accuracy. Thus, it is needed to evaluate the mapping performance directly.

Table 1 Comparison of different optimization methods
Full size table

Methods
General design
The framework of the tight coupling mapping method developed to integrate the ESKF, g2o, and point cloud alignment is shown in Fig. 1. Edges are defined as the transformations between two adjacent frames or between the current frame and the local map. Vertexes are defined as the pose of each frame in the local map.

Fig. 1
figure 1
Framework of the proposed method. The arrows indicate information flow directions

Full size image

The GICP and the NDT are chosen as the point cloud alignment methods for testing the proposed framework. Other alignment methods can be also easily deployed to replace these alignment methods in the proposed framework. The point cloud alignment process provides the transformation information, which indicates the edge. In addition to the edges between adjacent frames, edges are also created between the frames and the local map to provide redundant observations for the g2o optimization process.

The vertexes are used as measurements of the ESKF. Here, the use of ESKF instead of direct IMU integration is advantageous because the former can limit the error accumulated during IMU integration, and thus yields more accurate predictions. Some point cloud alignment methods use the identity transformation matrix as the default value if the initial estimation is not provided. However, the identity transformation matrix is not as accurate as an ESKF estimation [26].

A g2o is a useful general framework for graph optimization based on nonlinear error functions. In this paper, the g2o is used to optimize the graph generated by the proposed method. The initial vertex is fixed in g2o optimization. Construction of the local map is based on the vertex poses outputted by g2o, and only key vertexes are used in the local map building process to save the computer memory and speed up the alignment calculation. The key vertex is defined by distance and rotation. For example, if a new vertex is far from the previous key vertex, i.e., outside the circle with a center at the previous key vertex and a radius of 1 m, or rotates over 45 degrees compared to the previous key vertex, it will be added as a key vertex.

Coordinate systems and transformations
The coordinate systems are defined as follows: the local north-east-down coordinate system Fn with a subscript n, the body-fixed front-right-down coordinate system Fb with a subscript b, and the g2o optimization coordinate system Fg with a subscript g. Figure 2 (a) shows these coordinate systems.

Fig. 2
figure 2
a Reference coordinate systems; b The blue and orange circles represent the poses of the LiDAR and the IMU at different moments, respectively. The number in the circle means LiDAR frame number. Red solid arrows represent the transformation relationship between different LiDAR frames. Green dotted arrows indicate the transformation relationship between the LiDAR and the IMU, which are constant

Full size image

As shown in Fig. 2 (b), the outputs of g2o are each frame transformation with respect to Frame 1 in the coordinate system Fg. Frame 1 is the origin of the coordinate system Fg, so these outputs are poses of Frame i in the coordinate system Fg. Frame 1 also serves as a link to transform these vertexes into the coordinate system Fn.

In the first step of obtaining the pose \left[ {{\text{C}}_{{\text{i}}}^{{\text{n}}} {\text{t}}_{{\text{i}}}^{{\text{n}}} } \right] of Frame i in the coordinate system Fn, the g2o outputs the pose \left[ {{\text{C}}_{{\text{i}}}^{{\text{g}}} {\text{t}}_{{\text{i}}}^{{\text{g}}} } \right] of Frame i in the coordinate system Fg. The {\text{C}}_{{\text{i}}}^{{\text{X}}} is the rotation matrix and {\text{t}}_{{\text{i}}}^{{\text{X}}} is the position in the coordinate system FX, where X is the coordinate system n, b, or g.

Second, {\text{t}}_{{\text{i}}}^{{\text{g}}} and {\text{C}}_{{\text{i}}}^{{\text{g}}} are transformed into the coordinate system Fb.

\begin{gathered} {\text{t}}_{i}^{b} = {\text{C}}_{{\text{L}}}^{{\text{I}}} {\text{t}}_{i}^{g} + {\text{t}}_{{\text{L}}}^{{\text{I}}} \hfill \\ {\text{C}}_{i}^{b} = {\text{C}}_{{\text{L}}}^{{\text{I}}} {\text{C}}_{i}^{g} \hfill \\ \end{gathered}

(1)

where {\text{C}}_{{\text{L}}}^{{\text{I}}} and {\text{t}}_{{\text{L}}}^{{\text{I}}} denote the rotation matrix and the translation vector between the LiDAR and the IMU, respectively. These values are constant and can be derived from the calibration between the LiDAR and the IMU.

Finally, {\text{t}}_{{\text{i}}}^{{\text{b}}} and {\text{C}}_{{\text{i}}}^{{\text{b}}} are transformed into the coordinate system Fn.

\begin{gathered} {\text{t}}_{i}^{n} {\text{ = C}}_{b}^{n} {\text{t}}_{i}^{b} \hfill \\ {\text{C}}_{i}^{n} {\text{ = C}}_{b}^{n} {\text{C}}_{i}^{b} \hfill \\ \end{gathered}

(2)

where {\text{C}}_{{\text{b}}}^{{\text{n}}} is obtained using the attitude and heading reference system, which is a common system in the context of the IMU [27].

Implementation of the ESKF
An ESKF consists of a state equation and a measurement equation. In this paper, the 15-dimensional state vector x is chosen as

x = \left[ {\begin{array}{*{20}c} {\delta p} & {\delta \theta } & {\delta v} & {\delta \beta_{a} } & {\delta \beta_{g} } \\ \end{array} } \right]^{{\text{T}}}

(3)

where \delta {\text{p}} is the position error, \delta \theta is the attitude angle error concerning the quaternion, \delta {\text{v}} is the velocity error, \delta \beta_{a} is the accelerometer bias error, and \delta \beta_{{\text{g}}} is the gyroscope bias error. The superscript T represents the transpose operation.

The state equation can be expressed as

\left\{ {\begin{array}{*{20}c} {\delta \dot{p} = \delta v} \\ {\delta \dot{v} = - {\text{R}}\left[ {a_{m} - \beta_{a} } \right]_{ \times } \delta \theta - {\text{R}}\delta \beta_{a} - {\text{R}}a_{n} } \\ {\delta \dot{\theta } = - \left[ {\omega_{m} - \beta_{g} } \right]_{ \times } \delta \theta - \delta \beta_{g} - \omega_{n} } \\ {\delta \dot{\beta }_{a} = n_{a} } \\ {\delta \dot{\beta }_{g} = n_{g} } \\ \end{array} } \right.

(4)

where \delta {\dot{\text{p}}} is the derivative of the position error \delta {\text{p}}. Here, \delta {\dot{\text{v}}}, \delta \dot{\theta }, \delta \dot{\beta }_{a} and \delta \dot{\beta }_{{\text{g}}} are just like the relationship with \delta {\dot{\text{p}}} and \delta {\text{p}}, whose superscript \cdot indicates derivatives. Furthermore, a_{m} is the output of the three-dimension accelerometer, R is the rotation matrix and \omega_{m} is the output of the three-dimension gyroscope. a_{n} and \omega_{n} are the white Gaussian noise of the accelerometer and the gyroscope, respectively, while n_{a} and n_{{\text{g}}} are the Gaussian random walk noise of the accelerometer and gyroscope, respectively. The gravity is regarded as a constant in the equations, which differs from the traditional method in which this parameter is considered as an estimated state. \left[ \right]_{ \times } refers to the following matrix,

[v(x,y,z)]_{ \times } = \left[ {\begin{array}{*{20}c} 0 & { - z} & y \\ z & 0 & { - x} \\ { - y} & x & 0 \\ \end{array} } \right]

(5)

By using a Taylor series expansion to a first-order approximation, the discrete form of the state equation can be obtained as

x_{k + 1} = {\text{F}}_{k} x_{k} { = }\left[ {\begin{array}{*{20}c} {{\text{I}}_{3 \times 3} } & {0_{3 \times 3} } & {{\text{I}}_{3 \times 3} \Delta t} & {0_{3 \times 3} } & {0_{3 \times 3} } \\ {0_{3 \times 3} } & {{\text{R}}_{k}^{{\text{T}}} \left\{ {\left( {\omega_{m,k} - \beta_{g,k} } \right)\Delta t} \right\}} & {0_{3 \times 3} } & {0_{3 \times 3} } & { - {\text{I}}_{3 \times 3} \Delta t} \\ {0_{3 \times 3} } & { - {\text{R}}_{k} \left[ {a_{m,k} - \beta_{a,k} } \right]_{ \times } \Delta t} & {{\text{I}}_{3 \times 3} } & { - {\text{R}}_{k} \Delta t} & {0_{3 \times 3} } \\ {0_{3 \times 3} } & {0_{3 \times 3} } & {0_{3 \times 3} } & {{\text{I}}_{3 \times 3} } & {0_{3 \times 3} } \\ {0_{3 \times 3} } & {0_{3 \times 3} } & {0_{3 \times 3} } & {0_{3 \times 3} } & {{\text{I}}_{3 \times 3} } \\ \end{array} } \right]x_{k}

(6)

where {\text{I}}_{3 \times 3} is the identity matrix of size 3, 0_{3 \times 3} is the zero matrix of size 3, \Delta t is the discrete-time interval, subscript k indicates the value is at time-step k, and Fk is the state transition matrix at time-step k. The detailed derivation process can be found in Solà [28].

The measurement vector z of the ESKF is chosen as

z = [\begin{array}{*{20}c} {\delta p_{M} } & {\delta \theta_{M} } & {\delta v_{M} } \\ \end{array} ]^{{\text{T}}}

(7)

where the subscript M denotes the sensor measurement. The measurement vector z is computed by subtracting the sensor measurements from the predicted values. Its calculation formulas are as follows,

\left\{ {\begin{array}{*{20}c} {\delta p_{M} { = }\,p^{ + } - p_{{\text{L}}} } \\ {\left[ {\begin{array}{*{20}c} 2 \\ {\delta \theta_{M} } \\ \end{array} } \right]{ = }\,2\left( {q_{{\text{L}}}^{*} \otimes q^{ + } } \right)} \\ {\delta v_{M} { = }\,v^{ + } - v_{{\text{L}}} } \\ \end{array} } \right.

(8)

where {\text{p}}^{ + }, q^{ + } and {\text{v}}^{ + } are the estimated position, quaternion, and velocity from the state equation, respectively, and {\text{p}}_{{\text{L}}}, q_{{\text{L}}} and v_{{\text{L}}} are the position, quaternion, and velocity from the vertexes in the g2o, respectively. \delta q is the quaternion error, defined as

\delta q = q_{LiDAR}^{*} \otimes q^{ + }

(9)

where \otimes represents the multiplication of the quaternion and the superscript * represents the conjugate operation. When the rotation is very small [29], the relationship in Eq. 10 can be used. Combining Eqs. 9 and 10 yields Eq. 8.

\delta q \approx \left[ {\begin{array}{*{20}c} 1 \\ {\frac{1}{2}\delta \theta_{M} } \\ \end{array} } \right]

(10)

The discrete measurement equation can be expressed as

z_{k + 1} = {\text{H}}_{k + 1} x_{k + 1} { = }\,\left[ {\begin{array}{*{20}c} {{\text{I}}_{3 \times 3} } & {0_{3 \times 3} } & {0_{3 \times 3} } & {0_{3 \times 3} } & {0_{3 \times 3} } \\ {0_{3 \times 3} } & {{\text{I}}_{3 \times 3} } & {0_{3 \times 3} } & {0_{3 \times 3} } & {0_{3 \times 3} } \\ {0_{3 \times 3} } & {0_{3 \times 3} } & {{\text{I}}_{3 \times 3} } & {0_{3 \times 3} } & {0_{3 \times 3} } \\ \end{array} } \right]x_{k + 1}

(11)

where Hk+1 is the observation matrix at time-step k + 1.

Workflow of the proposed method
The workflow of the proposed method is shown in Algorithm 1.

Algorithm 1
Workflow of the proposed method.



Note that IMU data is required throughout the process to enable the ESKF to provide initial estimations.

Experiment and discussion
Experiments based on a mobile mapping backpack prototype are performed to verify the validity of the proposed method. The mapping backpack prototype consists of two Velodyne Puck LiDARs, a MicroStrain 3DM-GX5-25 IMU, and a data collection system. The backpack prototype is shown in Fig. 3. Detailed information about the system has been introduced in our previous study [30], and only the related sensors are shown here.

Fig. 3
figure 3
Mapping backpack prototype

Full size image

The process noise Q is configured to a fixed value according to the IMU noise parameters in Table 2, and the measurement noise R is set according to the precision of the used point cloud alignment method.

Table 2 Noise parameters of the MicroStrain 3DM-GX5-25 IMU
Full size table

The first experimental setting is an indoor office. The IMU and LiDAR data are recorded by the data collection system in the backpack prototype, and all of the data are tagged with timestamps managed by a precise synchronizer.

Firstly, the initial estimation results from the proposed method are compared to those of the traditional IMU integration and the NDT to determine the performance. In this part, the ESKF is used to refer to the ESKF module in the proposed method, which provides the initial estimation. The experiment runs on a 6-Core Intel® Core™ i7-8700 CPU @ 3.20 GHz PC. Single-thread computation is deployed in the experiments, and the maximum number of NDT iterations is 35. The point cloud dataset contains a total of 4100 frames; each frame contains more than 20,000 points, and the maximum distance is constrained to 10 m. Figure 4 displays Frame 3090 in the dataset, which contains 36,283 points.

Fig. 4
figure 4
Point cloud of frame 3090 in the dataset. The color of the point cloud is rendered according to the intensity

Full size image

As shown in Fig. 5, the mean runtime of the ESKF is 0.106 ms, compared to 0.067 ms for the traditional IMU integration and 0.158 s for the NDT; the latter is more than 1000 times the runtime of the ESKF.

Fig. 5
figure 5
Runtimes of the traditional IMU integration, the ESKF, and the NDT

Full size image

The enhanced line simplification (ELS) alignment result can offer centimeter-level accuracy [30] and is used as a reference for comparing the predicted poses of the traditional IMU integration, the NDT, and the ESKF. The results are shown in

Table 3. The bold numbers in Table 3 indicate the best performance. Regarding the mean absolute errors (MAE) and root mean square errors (RMSE), as shown in Table 3, the ESKF more accurately predicts the pose when compared to the NDT and the traditional IMU integration method, except for the position on the Z-axis. This result may be attributed to periodic vibrations in the z-axis direction. The experiment platform is a backpack worn by a person, whose natural walking causes these periodic vibrations. The estimated position difference is affected by the velocity. The traditional IMU integration directly integrates the acceleration to calculate the velocity without correction. Its resulting estimated position differences diverge and are not in the same magnitude as those estimated using the NDT and the ESKF. In summary, the results indicate that the proposed method performs well in terms of providing initial estimations, which can improve the alignment accuracy.

Table 3 Comparison of the statistics for the traditional IMU integration, the ESKF, and the NDT
Full size table

The original GICP alignment mapping result is shown in Fig. 6. The result of the tight coupling mapping method, which fuses the ESKF, the g2o, and the GICP point cloud alignment, is shown in Fig. 7. The figures intuitively demonstrate that the proposed method improves the mapping accuracy.

Fig. 6
figure 6
Original GICP alignment mapping results. a Overall view; b Inside view of the floor, where the center curve is the trajectory. The color of the point cloud is rendered according to the intensity

Full size image

Fig. 7
figure 7
Mapping results of the proposed method using the GICP. a Overall view; b Inside view, where the center curve is the trajectory. The color of the point cloud is rendered according to the intensity

Full size image

To numerically evaluate the accuracy of the proposed method, targets are laid out on the walls and the floor. Each target is the same size as a piece of A4 paper and is marked with two 14.4 \times 10.1 cm black rectangles. The pictures of the targets are shown in Fig. 8.

Fig. 8
figure 8
Pictures of the targets. a Picture of a target; b Targets in the point cloud map, which are shown in the red boxes. The color of the point cloud is rendered according to the intensity

Full size image

By comparing the length and the width of the targets in the point cloud map and the true value, a distance error histogram can be generated to evaluate the quality of the point cloud map. Figure 9 demonstrates that the distance error is between -4 cm and 4 cm. The fitting normal distribution is X ~ N(0.05,1.242).

Fig. 9
figure 9
Distance error histogram of the proposed method using the GICP. The red curve is the fitting normal distribution

Full size image

Another framework that fuses the ESKF, the g2o, and the NDT point cloud alignment is also tested. Figure 10 presents the mapping result of the original NDT alignment, and Fig. 11 presents the mapping result of the proposed method using the NDT.

Fig. 10
figure 10
The mapping result of the original NDT alignment. The color of the point cloud is rendered according to the intensity

Full size image

Fig. 11
figure 11
Mapping results of the proposed method using the NDT. a Overall view; b Inside view, where the center curve is the trajectory. The color of the point cloud is rendered according to the intensity

Full size image

Figure 12 demonstrates a nearly 99% distance error between -4 cm and 4 cm; here, the fitting normal distribution is X ~ N(0.32,1.322). These results indicate that the point cloud map generated by the proposed method is precise and can reach the centimeter level, and that the proposed method effectively improves the mapping accuracy. Because the NDT alignment method is less accurate than the GICP, the mapping result using the GICP is better than that obtained using the NDT, which suggests that the performance of the proposed method is affected by the accuracy of the used alignment method.

Fig. 12
figure 12
Distance error histogram of the proposed method using the NDT. The red curve is the fitting normal distribution

Full size image

To further prove the accuracy of the method, the proposed method is compared to the state-of-the-art SLAM algorithm LIO-SAM [31], which uses the FG for optimization. The SLAM benchmarks generally only provide the ground truth of the trajectory, which is not suitable for directly evaluating the performance of the mapping. The benchmark “ISPRS BENCHMARK ON MULTISENSORY INDOOR MAPPING AND POSITIONING” [32], which has the truth point cloud map as a reference, does not record IMU data, so it cannot be used either. Therefore, an experiment imitating the benchmark is supplemented. The FARO Focus M 70 Laser Scanner is used for the reference, whose ranging error is \pm 3 mm. Figure 13 shows the FARO Focus M 70 Laser Scanner and the experiment environment, an outdoor garden. Figure 14 shows the reference point cloud map provided by the FARO Focus M 70 Laser Scanner.

Fig. 13
figure 13
Picture of the experiment environment and the FARO Focus M 70 Laser Scanner

Full size image

Fig. 14
figure 14
Reference point cloud map provided by the FARO Focus M 70 Laser Scanner

Full size image

The point cloud maps built by the proposed method and the LIO-SAM are compared to the reference point cloud map directly. To keep the density of these point cloud maps the same, they are downsampled using a voxel size of 0.1 m. Figure 15 shows the distance error between the LIO-SAM point cloud map and the reference, while Fig. 16 indicates the distance error between the point cloud map built by the proposed method and the reference. In these two figures, different colors represent different distance errors as shown in the color bar. The white color indicates that the corresponding point of the point is not found in the reference map. Figure 17 shows the histograms of distance error of the LIO-SAM and the proposed method. The mean error of the LIO-SAM is 8.3 cm, while the mean error of the proposed method is 5.8 cm. Compared with the LIO-SAM, the accuracy of the proposed method is improved by almost 30%. Besides, the 82.64% distance error of the proposed method point cloud map is within 10 cm, while the value of the LIO-SAM is 64.16%. The experiment shows that the performance of the proposed method is better than the state-of-the-art method LIO-SAM and proves that the point cloud map generated by the proposed method can reach the centimeter level once again.

Fig. 15
figure 15
The distance error between the LIO-SAM point cloud map and the reference. Different colors represent different distance errors as shown in the color bar. The white color indicates that the corresponding point of the point is not found on the reference map

Full size image

Fig. 16
figure 16
The distance error between the point cloud map built by the proposed method and the reference. Different colors represent different distance errors as shown in the color bar. The white color indicates that the corresponding point of the point is not found on the reference map

Full size image

Fig. 17
figure 17
The histogram of the distance error. a The LIO-SAM; b The proposed method

Full size image

Conclusion
High-precision point cloud maps are essential to many fields, such as autonomous driving, building information modeling, and robotics. To solve the problems that current mapping methods favor real-time performance over high precision and generally focus on trajectory accuracy rather than directly evaluating map accuracy, a tight coupling mapping method that integrates the ESKF, g2o, and point cloud alignment is proposed and mapping performance is evaluated directly by this paper. In this paper, an ESKF is used to provide initial estimation for the point cloud alignment. A g2o is then applied to make up for the lack of historical information in the ESKF and to optimize the result of the point cloud alignment. Experiments are conducted to demonstrate the performance of this method by using the IMU and the LiDAR data collected by the mobile mapping backpack platform.

The results prove the benefits of using the ESKF to provide initial estimation and the good mapping performance of the novel tight coupling mapping framework to build high-precision point cloud maps. The experiment results demonstrate that the initial estimation process runtime of the proposed method is much smaller than that of the NDT; meanwhile, the proposed method yields a more accurate pose estimation. It could feasibly be used to generate the initial estimation for a point cloud alignment. Additionally, the bad mapping result of the sole GICP and NDT method suggests that the proposed method is more robust than the sole point cloud alignment. Moreover, the mapping results show that the proposed mapping method can provide a centimeter-level point cloud map and the performance of the proposed method is better than that of the state-of-the-art method LIO-SAM.

In the next step of the research, the proposed mapping method will be tested further in more scenarios to verify its universal applicability. Pedestrian dead reckoning (PDR) will also be added to the proposed framework, as this approach can help to improve the estimation for the position. Thus, this addition may increase the applicability of the proposed method to scenes involving point cloud degeneracy.

Abbreviations
EKF:
Extended Kalman filter

ESKF:
Error-state Kalman filter

ELS:
Enhanced line simplification

FG:
Factor graph

g2o:
General framework for graph optimization

GICP:
Generalized iterative closest point

ICP:
Iterative closest point

IMU:
Inertial measurement unit

LiDAR:
Light detection and ranging

NDT:
Normal distributions transform

SLAM:
Simultaneous localization and mapping

0_{3 \times 3} :
The zero matrix of size 3

a_{m} :
The output of the three-dimension accelerometer

a_{n} :
The white Gaussian noise of the accelerometer

{\text{C}}_{{\text{L}}}^{{\text{I}}} :
The rotation matrix between LiDAR and IMU

{\text{C}}_{{\text{i}}}^{{\text{X}}} :
The rotation matrix of Frame i in the coordinate system FX

Fk :
The state transition matrix at time-step k

F b :
The body-fixed front-right-down coordinate system

F g :
The g2o optimization coordinate system

F n :
The local north-east-down coordinate system

Hk + 1 :
The observation matrix at time-step k + 1

{\text{I}}_{3 \times 3} :
The identity matrix of size 3

k :
Subscript k indicates the value is at time-step k

M :
Subscript M denotes the sensor measurement

n_{a} :
The Gaussian random walk noise of the accelerometer

n_{{\text{g}}} :
The Gaussian random walk noise of the gyroscope

p^{ + } :
The estimated position from the state equation

p_{{\text{L}}} :
The position from the vertexes in the g2o

q^{ + } :
The estimated quaternion from the state equation

q_{{\text{L}}} :
The position from the vertexes in the g2o

R:
The rotation matrix

T:
The superscript T represents the transpose operation

{\text{t}}_{{\text{L}}}^{{\text{I}}} :
The translation vector between LiDAR and IMU

{\text{t}}_{{\text{i}}}^{{\text{X}}} :
The position of Frame i in the coordinate system FX

{\text{v}}^{ + } :
The estimated velocity from the state equation

{\text{v}}_{{\text{L}}} :
The velocity from the vertexes in the g2o

x_{k} :
The state vector is at time-step k

z_{k} :
The measurement vector is at time-step k

\delta p :
The position error

\delta q :
The quaternion error

\delta {\text{v}} :
The velocity error

\delta \beta_{a} :
The accelerometer bias error

\delta \beta_{{\text{g}}} :
The gyroscope bias error

\delta \theta :
The attitude angle error concerning the quaternion

\Delta t :
The discrete-time interval

\omega_{m} :
The output of the three-dimension gyroscope

\omega_{n} :
The white Gaussian noise of the gyroscope

\cdot :
The superscript \cdot indicates derivatives

\otimes :
The multiplication of the quaternion

* :
The superscript * represents the conjugate operation

