Abstract
Power Side-channel attacks are a serious class of attacks which targets the vulnerabilities in physical implementation of a design. One of the main challenges from the view of designers in the automated design flow is the lack of enough metrics, tools, and methods to automatically measure the level of security during the designing stages. Besides, current tools do not provide any hints or reports to the engineers about the locations or sources of side-channel vulnerabilities at pre-silicon deign stages. In this research, we will propose a framework called “PATCH”, which uses a statistical flow to precisely find the source nodes of the power side-channel leakage on any arbitrary register-transfer level (RTL) design. PATCH conducts security assessments on the design and reports its security status and vulnerable nets to the designer. This will provide flexibility to designers in order to apply required changes at early stages of the design process. Our results showed that PATCH can localize the sources of leakage in an efficient manner to be applicable in the ASIC design flow. In addition, it can optionally be accompanied with our Injection tool to automatically remediate leakage of information caused by vulnerable nets.

Introduction
Side-channel information is known as a critical source of information leakage in the context of secure hardware design. There are many types of side-channels such as power consumption of crypto cores [1], variations in execution time of different instructions in a microprocessor [2], electromagnetic emanations of a smart-card [3] and even acoustic noise heard from electronic components of a personal computer during its operation [4]. Attackers and eavesdroppers may find a mapping between observed side-channel information and secret data that is processed by our hardware using these channels of information leakage. Such action is generally called a side-channel attack.

Among different types of side-channel attacks, we mainly focus on the power analysis attacks in the current research due to the wide range of researches and studies done on this sort of attack, and its related countermeasures in the literature. Power analysis attacks include several different attacks including Simple Power Analysis (SPA) [1], Differential Power Analysis (DPA) [1] and Correlation Power Analysis (CPA) [5].

Researchers offered different countermeasures for power side-channel attacks. All of these efforts try to eliminate or weaken the relationship between the side-channel information and the sensitive data, and they mainly fell in two groups [19]:

The first group of countermeasures reduces or eliminates the emission of side-channel information, which is called Hiding countermeasures.

The second group tries to eliminate or reduce the dependency between the sensitive secret information and the side-channel information, which is called Masking countermeasures.

It is worth noting that, there are some other techniques such as re-keying [12] that can be used individually or as a complement for above countermeasures.

Although there are many researches on the side-channel security and the possible countermeasures for power attacks but, when it comes to a real application, very large-scale integration (VLSI) designers cannot apply available countermeasures on their designs with ease. Hardening methodologies are usually customized and tailored for specific circuits or algorithms and fitting them to an arbitrary hardware design during the design flow and manufacturing process is a tedious and complicated task.

Besides, another main challenge for semiconductor design houses is the fact that managing design side-channel security requires its own expert. It means that during the design process, side-channel security experts should work collaboratively with VLSI designers during the whole design process to fix the possible security vulnerabilities. This impose several overheads in terms of time-to-market and design fees to them [29].

Typically, VLSI designers cannot determine the exact sources and amount of side-channel leakage in their circuits during the design stage. They have not enough tools, metrics or knowledge to evaluate the side-channel leakage during different steps of the canonical design process. Current available commercial and industrial tools do not help them to quantify the amount of security gained in terms of losing other cost parameters such as silicon area, performance, and power consumption after applying different countermeasures. Usually, after applying side-channel countermeasures to our hardware, we have a larger silicon area, higher power consumption, and probably less performance [28]. This is one of the reasons that topic of side-channel leakage assessment at design stage gained popularity among vendors that are developing Electronic Design Automation (EDA) tools and the design houses [26].

Another problem with custom security solutions is the fact that usually they should be applied manually and no EDA tools support the automatic enhancement of security for a custom design. This manual process is laborious, error-prone and time-consuming. Since computer-aided design (CAD) tools are not security-aware, there are even cases that they introduce some vulnerabilities to the design, themselves [25,26,27].

Normally, VLSI designers apply countermeasures partially only on blocks of the design that are processing sensitive information in order to reduce the overhead of security countermeasures while keeping a reasonable cost-security trade-off. By providing power side-channel-aware tools, we can help them to apply countermeasure more wisely on finer-grained elements of a design such as registers or net to avoid unnecessary loss of valuable resources while having the same level of the security. If the design tool provides some hints or can localize the sources of information leakage, it will be much easier for designers to achieve the better balance among security and other costly parameters. In such a situation, the designer can apply the countermeasures more precisely to the parts that leak more information and at a finer level of granularity to reduce the overheads of the hardening process.

Our Contributions In this research, we proposed a design flow and corresponding software framework composed of several software tools and scripts which is called PATCH, to locate the exact sources of leakage in an arbitrary hardware-design at early stages of the design process. In our method, the nodes with the high levels of information leakage in a design are located via a statistical approach and they are reported to the users. This information can guide them to concentrate on the high-leakage points inside their circuit and improve the security of the design against side-channel attacks more efficiently. This information helps the designers to narrow down the countermeasures to a level that have higher efficiency in terms of security and lower overheads in terms of design parameters.

PACTH has helpful features for VLSI designers that are listed below:

Designers can use it to localize side-channel leakage from early stages of design process. PATCH analyzes security of an RTL design before going through the next stages of design procedure such as placement and routing. This will give enough flexibility to designer to apply required modification to the design at basic steps of the flow and this save their time to a large extent.

Instead of proposing a decoupled procedure and set of tools for side-channel leakage assessment, PATCH mostly uses the traditional commercial tools to assess security (besides of some custom tools and scripts which developed by authors). This way, the leakage assessment and localization process overlaps with normal design process in several steps and this causes saving of time during the design procedure and allows to leverage the power and accuracy of industrial tools for leakage assessment.

It can assess vulnerability in fine granularity. PATCH reports the list of vulnerable nets in the gate-level net-list of the design (comparing to some other frameworks that report vulnerability in granularity level of design blocks [29]).

PATCH is scalable in terms of performance and design cost trade-off. Its performance for side-channel assessment and remediation of vulnerabilities is acceptable and it can be used for designs with large scales. As mentioned already, some of its steps is overlapped with the steps in the conventional design process. In our software setup, we did not leverage the parallelism but the most of the steps inside PATCH framework can take advantage of the parallelism to increase its performance. In terms of design cost, we can keep the balance between the gained security and imposed expenses by modification of the thresholds parameters of the PATCH mechanism to localize the vulnerable nets.

Application of PATCH is not limited to implementation of cryptographic functions. It can be used to assess vulnerabilities of any arbitrary RTL design such as a micro-processor. It has been shown that information may leak from modules of the non-cryptographic designs, such as integer division unit of a commercial CPU [30].

As an interesting option, PATCH is accompanied with a tool called Injection to remediate the vulnerabilities and apply local countermeasure automatically on the nets that leak power side-channel information. Remediation of the design is done as a complementary and an optional step to show efficiency of the internal algorithm used in PATCH framework and also as a proof-of-concept.

The outline of this paper is as follows. In Sect. 2, we will review the side-channel assessment methods. Section 3 describes the proposed mechanism to pinpoint the vulnerable nets and also brings in the details about our framework (PATCH) and Sect. 4 demonstrates the experimental results. Finally, Sect. 5 concludes this research and talks about our future works.

Literature review
In order to automatically asses side-channel leakage of a design and harden a design against side-channel leakage, we should focus on two concerns, which will be discussed in the next two sub-sections. The first concern is how should we measure the amount of side-channel leakage. So in the first sub-section, we will talk about metrics which proposed for side-channel leakage assessment. The next concern would be how to use these metrics to automatically analyze a design from side-channel security perspective and then apply required modifications and remediations. So, in the second sub-section EDA-based design-flows, frameworks and methodologies which tried to assess and improve side-channel security of the design will be reviewed and compared to our work.

Side-channel assessment metrics
There are several metrics that are proposed for the assessment of side-channel leakage. Among them, Minimum Trace to Disclosure (MTD) is the most widely known metric in the context of side-channel analysis. MTD is the number of traces required to reveal secret information (such as cryptography key) during a side-channel attack. It cannot be measured before conducting a real attack on the device but there are some methods that bound its limits based on the parameters such as SNR [19]. Estimation of this metric depends highly on attack type and expertise of the test person. Therefore, it is hard to quantize the level of security for a device using this metric and compare different designs by their MTD. By the way, it is a metric, which is widely used in side-channel literature for reporting the level of security since it gives us a sense of the required effort to reveal secret information of our device in a real attack scenario.

Among metrics, there is another class of metrics that measure the security of a design against the specific type of side-channels. For example, SVF [9, 10] and CSV [11] that measure the amount of leakage through the access patterns of cache memory but they cannot quantify information leakage through other desired types of side-channels. SAVAT [12] is another metric, which evaluates the ratio of information leakage through electromagnetic emission among different instructions of a microprocessor. We must note that the author of [9] claimed that SVF could be applied to assess leakage through any type of side-channels, but its evaluation depends on access pattern detection that is not applicable for all types of side-channels and they only demonstrated case studies for cache access patterns through their publications.

Another suggested assessment methodology that gained enough popularity in the literature is the T-Test methodology [13] which is a member of a larger group of test methodologies called Test Vector Leakage Assessment (TVLA) methodologies. This methodology proposed for the first time in a non-invasive attack testing workshop to suggest a methodology of side-channel leakage testing that its result does not depend on specific attack and expertise of people in the test lab. Side-channel leakage of a DUT (device under test) can be examined using the T-test methodology without performing any real attacks. This methodology divides the power traces into two different groups of given input data on DUT. Using Welch’s T-test, the ‘T’ parameter can be evaluated for the two groups of collected power traces.

If the ‘T’ value crosses the specified boundary, it means that there is a significant and detectable difference between traces collected from two groups of input data. Consequently, traces collected from our DUT can give the attacker enough information to understand that traces belong to which group of input data. In this condition, the DUT demonstrates the leakage of information and it failed to pass a side-channel security assessment test.

It must be considered that T-test or generally speaking TVLA methodologies are univariate and they cannot detect leakage due to multivariate data dependencies in the [18].For example an RSA design which may suffer from SPA leakage may pass the T-test despite being vulnerable to other attacks. Therefore [18] suggest to conduct additional analyzes besides of T-test. However, in case we use a T-test in the correct way, it is a convenient and powerful tool for the assessments of information leakage.

Our previous research [20] was an initial step in our roadmap toward goals we are going to cover in this research. In that work, we have reviewed metrics that are used for side-channel leakage assessment in the literature. Then we proposed some features for metrics which can be applicable in EDA tools for assessment of side-channel leakage. Analyzing a case-study and using T-test methodology, our results showed that security status of a design may vary from part to part in a design and this is detectable in pre-silicon stages of design process. So in this work we will deeply focus on pinpointing the sources of the leakage and optionally helping the designer to remediate the security status of the design at early stages of VLSI design process as it will be described later in Sect. 3.

Automated side-channel leakage assessment and enhancement tools
Assessment of side-channel leakage in real devices has been focused on many researches prior to this work. However, one of the major challenges for hardware designers is the lack of tools at pre-silicon stage, which consider side-channel security during the design process. This issue was addressed in several researches. Authors of [21] proposed an EDA-based methodology to perform power analysis on pre-silicon gate-level netlist. They built a new leakage model to speed up the detection of sensitive information leakage and integrated their model in an EDA tool to enable designers with power analysis capabilities during design stages. In [22], side-channel leakage of an open-source 32-bit RISC CPU is analyzed using available commercial EDA tools. They have analyzed the effectiveness of side-channel countermeasure at software level and concluded that software countermeasure cannot be applied without considering the micro-architectural details of underlying platform that executes the software codes and applications. They have also recommended to integrate side-channel analysis in EDA tools in order to reduce engineering effort for the security assessment process. Authors of [25] tried to make a secure implementation of an open-source RISC-V processor by localizing its sources of leakage and verified the result by implementing it on an FPGA board. The interesting part of this research is the results about the side-effects of EDA tools on side-channel security. According to the result of this research, Floating Point Unit (FPU) and branch-prediction module play an important role in leakage of AES key although it seems that no data dependency between FPU and branch-prediction unit with the key in the AES encryption module these modules. This dependency is observed due to translations which happened in EDA tools to improve the design considering other design metrics except the side-channel security. Therefore, EDA tools will be a key player and they side-channel security-aware EDA tools should be used for security-critical systems and application. [24] is another work which suggests a methodology for side-channel security assessment of the RTL designs and it introduces a procedure to localize the source of leakage using simulation tools.

Although side-channel-aware automation design flows and frameworks seem to be a vital element in the design of secure chips, to our knowledge, there are few EDA frameworks that targeted side-channel aware design flow.

Adaptable Module and Autonomous Side-Channel Vulnerability Emulator (AMASIVE) is an FPGA-based assessment framework which is introduced in [6]. AMASIVE is fed with an adaptable model of the attacker and an acceptable threshold for information leakage. The attacker model defines how much the computational power of the attacker is, what are his abilities in the logical deduction to access secret information, and finally how much his time limits to access the sensitive information is. Using this information, AMASIVE can detect if the attacker can access sensitive information in the defined constraints or not. If the answer was true, AMASIVE shows the path which the attacker took to access sensitive information and which nodes in our design have lost their confidentiality. In an extension of this research [7], authors have enhanced their framework so that it can automatically harden the vulnerable parts in the design with countermeasure which fits the detected vulnerabilities in earlier stages. Although not explicitly claimed, the AMASIVE framework is intended to harden designs, which are going to be implemented on FPGA platforms. In order to find vulnerable points in the design, AMASIVE needs a collection of power traces, which experimentally extracted during an attack to real FPGA device and then it tries to improve the design protection against side-channel attacks in higher layers of abstraction. In the next step, it implements the hardened design on the real device as the final output. This process fits well with the FPGA design flow due to its reconfigurability. However, adopting this flow for ASICs designs is so costly and expensive. ASIC fabrication only for the purpose of collecting power traces, adds a lot of time and expenses to the process of ASIC manufacturing which makes the flow suggested by AMASIVE unacceptable for chip designers in current competitive global markets.

CASCADE is another framework for SCA evaluation during the designing period which introduced in [8]. It proposed a framework which can be integrated with the canonical standard-cell ASIC design flows such as Synopsys Design Compiler or Mentor Graphics QuestaSim.

Authors of [8] analyzed different designs and assessed the level of their security against power attacks for them. They claimed that CASCADE can pinpoint the vulnerable nodes with a precision of a single gate. However, this is not demonstrated anywhere in this research. Against AMASIVE, CASCADE could not automatically insert countermeasures in source design and points which detected as leaky in the design and it can only report the level of security for the design.

Another effort for automated assessment and improvement of ASIC design cycle in terms of side-channel leakage is addressed in [23]. An algorithm called Karna is used on that research which is based on EDA tools and analyzes the design during VLSI design work-flow and it improves the security of design without any overhead to reduce the amount of leakage. The design will be evaluated for security status in the stage which ASIC placement is finished and before proceeding to VLSI design routing stage. If design does not meet the security requirements specified by the designer, Karna splits the design into an  grid and then extracts the list of gates in the design which leak information above the desired threshold. In the next step, by replacing the gates which have positive slack with other possible alternatives in the technology specific standard-cell library, it will change the pattern of dynamic power trace.

Besides of all its interesting ideas, Karna has serious limitations. Firstly, it evaluates the security status of a placed netlist and is limited to restrictions of that fixed placement. Second limitation is that it try to change the power pattern of the circuit by swapping the gate with another gate with same functionality but with different parameters such as area or threshold voltage. However, cell libraries usually contain a few alternatives for gate with same functionality but with different parameters. Finally, it only replaces the cells with enough-positive slack to avoid the timing failure of the design. Therefore, many of gates cannot be swapped with their alternatives because they belong to a critical (or near-critical) path of the design.

As explained in previous paragraphs, current automated tools and frameworks have some sort of shortcomings and drawbacks when trying to help the ASIC designers to manage security vulnerabilities that are present in their design. AMASIVE is not suited for ASIC design flow and CASCADE only evaluates the level of security at design time. It does not provide any hints to the designer to locate the source of leakage nor provides any facilities to improve the level of security against side-channel attacks. Karna is only work that locates the source of information leakage at gate level and uses this information to improve the leakage status of the design but it has its own shortcomings.

In this research, we try to fill the current gap by introducing the PATCH framework. Besides of evaluating the security of ASIC designs, in order to provide useful hints to designers, this framework pinpoints the sources of information leakage in the design and quantizes the level of information leakage for each of the leakage sources in early stages of VLSI design flow. As a second priority, it also provides the tools to automatically insert countermeasure at points which detected as sources of information leakage.

PATCH: a framework for pinpointing the vulnerabilities
In this section, we describe our mechanism to localize the vulnerable points in a design. Vulnerable points are defined as the nodes in the netlist of design that have more influence in the leakage of sensitive information through side-channel than other nets or nodes in the design.

It is worth noting that vulnerable nodes can be identified after recognizing the moments of time in which the information leakage of the input design crosses the acceptable boundaries. We will pay particular attention to the leakage of information in these moments and look for finer elements inside the design which caused the leakage. Since leakage in a moment of time may be caused by the propagation of several transitions in previous moments, we will look for leakage in temporal neighborhoods of moments in which leakage is observed. We call these temporal neighborhoods as “Sensitive Regions”.

After extraction of the sensitive regions, we will look for nets that have a systematic effect on the unacceptable amount of leakage that observed in the sensitive regions. Detection of this systematic effect is done in this research by calculation of the correlation of power usage of each net only in the sensitive regions and power usage of the whole design, again in the sensitive regions. This way we ignore the behavior of a net outside sensitive regions and we can elaborate its systematic effect in information leakage which crossed the acceptable threshold.

In the next step, nets with the highest correlation value are marked as key players in the leakage of information. Then we will try to reduce leakage caused by these nets by applying some local countermeasures and the process of leakage assessment is repeated again for updated design and it will continue until information leakage reaches to an acceptable level. The proposed algorithm to localize vulnerable nets in netlist of a given design and remediate their information leakage is brought in Algorithm 1. A visual demonstration of this flow is also brought in Fig. 1. There are 7 steps (labeled from 1 to 7 in this figure) which must be accomplished to find the list of leakiest nets in our design. The Step 8 (i.e. the countermeasure insertion) is separated from other steps in a dotted gray box since we want to emphasize that the countermeasure insertion step will be researched deeply in our future works and we only brought some demonstrative samples for this step in this research. As the first step in this flow, RTL design is synthesized to the gate level using the target standard-cell library. In the Step 2, delay and design constraint specifications of synthesizing design are extracted. Then, the design is simulated and the dynamic behavior of design will be investigated while it is fed with different input values. Captured signal transitions are fed to the next tool in the Step 4 and it extracts time-based power consumption of synthesized design per each input vector. Besides, transitions, delay and design constraint information are required in the power estimation stage. This information is shown by the blue arrow in Fig. 1.

After all, the design will be analyzed using the collected power traces and different analyzers such as CPA, DPA or T-test in the Step 5 and then the moments in which designs show the highest levels of information leakage are identified. Actions must be taken to remediate the behavior of design during these time intervals, which called “sensitive regions”. To do this, the power consumption of each net in design will be estimated in the proposed flow by choosing a suitable power model such as Hamming distance or Hamming weight in the Step 6. In the next step, i.e. Step 7, the nets that have the highest correlation with the total power consumption of design in sensitive regions are determined. As already explained, these nets are the main candidates for applying local countermeasures since their behavior leads to the leakage of information during times which are highlighted as sensitive regions.

Insertion of suitable local countermeasures is done in the Step 8. However, as we noted already this task is not focused on this research and our main goal in this work is the localization of leakage source not looking for the ways to remediate it. So, we highlighted the Step 8 in a dotted gray box to highlight this fact.

After this brief review of the process for pinpointing the vulnerabilities, we will bring details for steps that need more clarification and at first, we will talk about synthesizing our design from RTL level to gate level.

Fig. 1
figure 1
Process flow diagram of PATCH framework to localize the leakage sources in an RTL design (Step 8 is optional and not focused primarily in this work)

Full size image

Step 1: gate level synthesis
The assessment process is started by synthesizing an RTL description into a gate-level netlist using a standard-cell library targeted for the final fabrication of our ASIC design. The measurements and simulations are performed at gate level due to several reasons as follows:

First of all, this step, i.e. gate-level synthesis is at the beginning of the design process and the designer has the opportunity to remediate vulnerabilities before getting deeper into the next step of the design process such as placement and routing. This helps him/her to save time and avoids repeating time-consuming actions that are done to proceed to the next steps of the design process if our analysis shows that the design needs changes to increase the level of its security.

Secondly, designers have a better understanding of components and functionality of design modules at higher levels of abstraction and they can find better fixes for each vulnerability due to something which we call it readability of a design. At higher levels of abstraction, the design is still readable and the designer can relate different parts of a design to each other and this helps to find better remedies for security vulnerabilities when the design is investigated at higher levels of abstraction.

Finally, a large group of side-channel vulnerabilities detected in this stage, are vulnerabilities which detected due to strong data dependencies between internal secret information and physical side-channel information. Proceeding to the next steps of the design process and physical variations in the design does not eliminate such vulnerabilities because of their systematic nature. So it is better to handle them during the early stages of the design process before moving toward the next steps. However, this benefit itself, is a double-edged sword. Since our design is still in higher levels of abstraction, the results of our simulation for physical parameters, such as timing and power consumption, are imprecise and some vulnerabilities that may show up in the next steps, may be missed. This fact suggests that vulnerability assessment in the next stages of the design process is crucial for designers but they must consider vulnerabilities in higher levels of abstraction first, then proceed to the next steps. As mentioned already, handling detected vulnerabilities in higher levels of abstraction and during the first stages of the design process, requires fewer efforts and costs.

In the PATCH framework, the Synopsys Design Compiler [14] is used for synthesizing RTL design into the gate-level standard-cell design.

Step 2: extraction of delay and constraint specification
When the design is synthesized from RTL into a lower level of abstraction (i.e. its standard-cell gate-level description), it will be affected by the physical specification of the standard-cell library. This way some details will be added to our design and make it more specific compared to its RTL description. Since this design is going to be simulated, it will be necessary to capture these detailed changes to use them during simulations for timing and power consumption in the next steps of the security assessment procedure. We accomplish this task by saving this information after synthesizing our design in the Design Compiler. This detailed information is saved in files with SDF and SDC format. Files with SDF format include information about the timing and delays of cells, gates, and modules used in the design and files with SDC format include information about constraints such as load capacitances, environment condition which will be used during our simulation in the next steps.

Step 3: simulation of the nets transition
At this stage, we will try to simulate the dynamic behavior of our gate-level design while feeding it with real inputs. This task is done since the dynamic power consumption of CMOS-based hardware is dominated by the number of transitions at output values of its internal transistors and gates. We use Mentor Graphics’ ModelSim [15] for this task. Generating a testbench will be necessary to feed our design with different values. This testbench is generated such that it can feed the design input value with different statistical distributions. We usually use uniformly distributed random values in our testbenches but other distributions for security assessment may be required for example in the case of conducting a semi-fixed non-specific T-test [16]. Information of captured signal transitions is saved in VCD (Value Change Dump) format is an exchangeable format among tools that need this type of information including Synopsys Design Compiler and Synopsys PrimeTime PX.

Power consumption estimation
The power consumption of a circuit depends on its static and dynamic components. The dynamic component of CMOS circuits directly relates to the number of switching or toggles at the output of CMOS gates. To estimate the dynamic power of CMOS gates different power models such as Hamming weight or Hamming distance have been introduced. Simulation tools such as Synopsys Design Compiler or Synopsys PrimeTime PX use power models to estimate power usage. Besides, they also have information about the timing, fan-out, and capacitance of gates which helps to have more precise power estimations comparing to use Hamming distance without considering parameters such as load capacitance or internal delay of gates. Side-channel attacks usually use dynamic power components of a circuit for side-channel attacks since it shows big differences per different circuit inputs and we can leverage these variations for a successful attack.

Step 4: estimation of power usage in the whole design
In the PATCH framework, firstly different inputs are fed to our synthesized circuit and their dynamic power usage at different moments of time is captured. As explained earlier, this activity is accomplished using a testbench generated by our framework in an HDL language (VHDL or Verilog) for a given design. Then signal transitions are captured in files with VCD format using ModelSim. These files which contain information about toggling of nets during different time samples help us to estimate instantaneous power usage of our design. VCD files are fed to Synopsys PrimeTime PX for time-based power estimation. VCD files are accompanied by delay file (.SDF), technology library, the synthesized design itself and design constraint file (.SDC) which also have information about parameters such as load capacitances. The output of this stage would be a time-based power report of our design during the simulation runtime. This way, we can extract power trace per each input value which was fed already to the design by the HDL testbench.

Step 6: estimation of power usage for individual nets
Besides evaluating total power consumption for the whole design, we also need to evaluate the power usage of each net individually during simulation runtime. However, due to a large number of nets and fine granularity of nets in a design, it is not efficient to use available commercial tools for power estimation per each net and these tools have no prebuilt tools to report power usage per each net and their smallest granularity to report power is at the scale of design modules (which consist of many cells and nets). So we developed a custom tool that parses VCD files and keeps track of toggling per each net separately and estimates power usage per each net at different time samples using the Hamming distance power model. This tool is composed of codes developed in C language and some scripts in MATLAB.

Step 5: analyzing design and extraction of sensitive regions
After collecting power traces of the design in Step 4, we should start analyzing our design to find moments of time in which information leaked from our design. This will help the designers to pinpoint design vulnerabilities in the next steps. To do analysis, TVLA methodologies such as T-test can be used or side-channel attacks such as DPA or CPA can be conducted. The attack scenario is the same as attacking real physical devices but there are few differences. Conducting a successful attack targeting a simulated design requires fewer power traces comparing with its real physical implementations. This benefit is due to the nature of the process used for capturing simulated power trace.

We use CPA, DPA, and T-test in the PATCH framework but the tests and attacks are not limited to these options and it can be extended to any other desired test or attack methods that are available in the literature for the analysis of power side-channel information.

If the result of tests shows that the input design leaks sensitive information, we will create a set of time windows that consists of neighborhoods around moments of time in which our design leaked information more than an acceptable threshold. For example, assume a design is given that implements AES-128 block cipher and it takes 500ns for encryption of each plaintext. Also assume that during the analysis of the design by T-test, it is observed that T value crosses the acceptable boundaries in 123ns and 465ns. We call these moments of time as sensitive moments and then we define neighborhoods around sensitive moments with the desired radius such as 10ns and call them sensitive regions. In this situation,  and  are the sensitive regions defined for sensitive moments of 123ns and 465ns. A set of sensitive regions such as  is finally created and will be passed to the next stages in the PATCH framework. As you can see, this set contains temporal neighborhoods in which the security breach occurred. This set shows wherein the power traces should be inspected carefully for the leakage of information and it helps the designer to narrow down his/her investigation for localization of the leakage source.

Step 7: find nets for local countermeasure insertion
When sensitive regions are found, it is time to look for a countermeasure to remediate this situation. In this subsection, a countermeasure is proposed to reduce the level of leakage in a design. We have two aims for suggesting a countermeasure. The former target is that we can evaluate the proposed assessment method by measuring the leakage level before and after the application of the countermeasure and the latter goal is highlighting the benefits of an assessment process for the designers in order to improve the security of the design.

Our countermeasure methods will mainly fall into a group of hiding countermeasures. This kind of method reduces the SNR of signals which leak sensitive information and make it harder for the attacker to find useful information in power traces. In order to keep a balance between security and other design parameters, some modules are inserted partially into the design netlist as a sort of countermeasure. To do this smart task, PATCH should find candidate locations in the design for the insertion of countermeasure modules.

In this framework, we will insert modules in the path of nets considering this rule of thumb: If the power consumption of a net has a high correlation with the power consumption of whole design in sensitive regions, so this net has a major effect on leakage of information in these regions and its transition behavior must be modified somehow.

In other words, leakage of information in a sensitive region is due to the summation of several components together in that region temporally and locally. Then we should look for components that boosted leakage in these regions.

Therefore, we analyze power traces collected for design in the sensitive regions and compare them with power traces of each specific net at the same time intervals. Figure 2a demonstrates a power trace of the design in which hypothetical sensitive regions are highlighted in red in this trace. These are the time slots in the power trace which lead to the information leakage. Figure 2b shows a power trace of three different nets N1, N2 and N3 considering only their signal transitions in sensitive regions. By calculating Pearson correlation of power traces of the design and power trace of each specific net in sensitive regions, and extracting the nets which have the highest correlation value, we can reach to the list of candidate nets which led to the unacceptable amount of information leakage.

Fig. 2
figure 2
a The sensitive regions of a normal power trace of a net inside the netlist of design are highlighted. b Power trace are masked and only switching activities inside sensitive regions will be considered for analysis of different nets

Full size image

Due to the desired level for the security improvement, the different number of candidate nets can be selected for countermeasure insertion. If many nets are selected, we will have increased area and power overheads and if few of them are selected, countermeasure insertion will not change the security status of our design that much and the designer must adjust this number due to the cost and security trade-offs.

PATCH already modeled power consumption of each net in the previous step (i.e. Step 6) and during Step 7, it will generate a list of nets that are the best fit for the countermeasure insertion. At this point, the designer have required information and he/she decide about the best method to apply local countermeasures.

Step 8: injection: local countermeasure insertion (optional)
Insertion of countermeasures can be accomplished optionally in Step 8. However, we are not going to focus on countermeasure techniques and the effectiveness of possible solutions as local countermeasures in this work. This is left as a part of our future works since this task itself is complex enough and deserves enough attention as an individual research activity.

After reviewing the process of security analysis and pinpointing vulnerabilities, we are going to review the results which observed during a case-study in the next section.

Experimental results: PATCH in practice
In order to evaluate the PATCH framework in a real scenario, we selected an unprotected AES implementation and tried to localize its vulnerabilities using the PATCH. Then, we applied efficient countermeasures at detected vulnerable points to harden the design in order to increase its security against SCA. As our results showed, the PATCH framework could find and report the vulnerable points and also it could improve the security level of our test design. According to our results, one of the factors which affects the level of security improvement is the number of countermeasure modules that we inserted in unprotected design. In the following subsections, the framework setup is described and then the experiments will be illustrated in detail. We described the experimental setup and then illustrated the correctness of the proposed vulnerability analysis. In the last subsection performance of PATCH framework is reviewed.

Framework setup
The input design that is selected as a case study for our analysis is an unrolled pipelined implementation of AES [17]. In this implementation, the results of each round are stored in a register during each clock cycle and passed to the next round using these registers.

We conducted a CPA attack on the initial step of AES where the XOR of plaintext and key is calculated to measure MTD for this unprotected design. We could recover the first byte of the encryption key for this unprotected design by analysis of 9400 power traces. Feeding results of the CPA and the design itself to the PATCH, it reported a list of nets sorted by their level of vulnerability. PATCH analyzed power traces to generate this report and found that leakage of information happens at the time interval of 6ns to 10ns from the start of encryption for each plaintext and highest leakage peak detected at 8ns.

So it forms a sensitive region set equal to . Afterward, it analyzed VCD files (simulation output data) to extract the list of transitions for each net in our design and using the Hamming distance power model, it estimated power usage of each net of our design individually. Then it calculated the correlation between power usage of each net and total power usage of the design in the sensitive regions. Finally, by sorting the nets according to their correlation value, PATCH listed the most vulnerable nets of our design. We can set a threshold for PATCH to modify its sensitivity to information leakage when generating a list of vulnerable nets. Typically, the number of nets which marked as vulnerable will be increased while we are changing threshold value to achieve higher levels of sensitivity.

In the next step, we have tried to verify the validity of the reported list of vulnerable nets generated by PATCH. In order to do this, countermeasures were inserted locally on each detected vulnerable net in our design and then MTD is calculated for the hardened design to see if any changes in the level of security observed or not. As a local countermeasure to remediate the amount of information leakage for each vulnerable net, a cost-efficient time-shifting module is inserted in the path of each vulnerable net. This module is shown in Fig. 3a. In this module, at each clock cycle, a different input of the multiplexer is routed to its output due to the value of pseudo-random linear-feedback shift register (LFSR). Each multiplexer input has the different number of buffer gates in its path so in each clock cycle, the input signal traverses a different path with different delays. The level of variation in the delays can be controlled by the size of the LFSR counter, the number of multiplexer inputs and the number of buffer gates in each multiplexer input. Figure 3b demonstrates the insertion process of a variable delay module in the path of a sample net such as “NET_113”.

Fig. 3
figure 3
a A module that gets an input signal and imposes a variable delay to it in each clock cycle, this module has 4 different delay values during different clock cycles. b insertion of random delay module in the path of a sample net (NET_113). The top part shows the net in untouched netlist of the design and the bottom part shows this net after insertion of a variable delay module in its path

Full size image

We developed a custom tool to insert countermeasures automatically to our design which is called Injection in this paper. Injection gets the list of vulnerable nets and the design source in HDL format as its inputs and it will generate an HDL design with applied countermeasures. Figure 4 demonstrates the process of modification of an HDL netlist by Injection. In this figure, module R00022 is an instance of our random delay module which its detailed internal structure is previously demonstrated in Fig. 3-a. In its Verilog implementation, we also have an initial seed input and a reset input. LFSR value is set to initial seed value after the reset pin is set. This seed value is used to make an initial difference between randomized delays of the individual variable delay modules after the reset signal are triggered for the whole design at its startup phase. The seed value for each delay module is set in the Verilog source code by Injection. This tool generates random seed values for different modules with uniform random distribution. The generation of random seed values is done by Injection so this offline task has no side effects on the performance of our input hardware design.

PATCH framework can be configured to accomplish different tasks by using different toolsets. Due to this reason and a variety of tools from a configuration to a configuration, we listed tools which used in our setup in Table 1.

Fig. 4
figure 4
a A segment of AES-128 implementation including its netlist and Verilog source. b modification of the same segment after insertion of a variable delay module in the path of net “n1067”

Full size image

Table 1 List of software tools we used in our custom setup of PATCH framework
Full size table

Vulnerability analysis and remediation
As described earlier, we proposed an automatic flow to evaluate the security of a crypto-system during the designing period and we localized the sources of information leakage. Using PATCH, we generated a list of vulnerable nets with four different sensitivity thresholds. The order of these four threshold values is set such that the level of security is in ascending order. The number of nets labeled as vulnerable corresponding with these four thresholds in ascending order was 71, 697, 1223 and 1883 respectively. These nets are referred to as vulnerable nets in this manuscript.

By inserting a variable delay module per each of vulnerable net, a new hardened design is generated. Due to its generality in side-channel literature, we used MTD to assess the level of security for each of these designs.

MTD to reveal the first byte of the key for four modified designs varied from 10,700 to 50,500 (in comparison with the unprotected design with MTD equal to 9,400). Our results are brought in Table 2. As it can be observed, area overhead ranges from  up to  while gained MTD improvement ranges from  up to  in comparison with original unprotected design. It must be noticed that the real MTD of protected and unprotected designs would be much higher after their fabrication since there are no environmental noises and misalignments in the design-time simulations. Beside, process variations and optimization of CAD tools may increase the MTD values that are experimented in the practice.

Table 2 Comparing the original design with four protected designs with different numbers of inserted countermeasure modules
Full size table

Figure 5 depicts the gained MTD improvement versus imposed area overhead. Among the four designs, all of them have increased MTD when we increased the number of gates and occupied more silicon areas. However, the ratio of this improvement is not the same for all. For example, in the 3rd modified design, using 1223 delay modules, MTD increased by  while for the 4th design, using 1883 gates MTD improved by . It is obvious that by increasing the number of delay modules and placing them in the right place, MTD of modified design will improve however it comes at the cost of increased silicon area.

Fig. 5
figure 5
MTD improvement versus imposed area overhead in modified designs

Full size image

We also executed a T-test for unprotected design and 4th protected design which had 1883 delay modules and the highest level of protection among other alternatives. T-test started failing for unprotected design with 40,000 traces but it failed for the protected design when we used at least 55,000 power traces. Result of T-test for these designs are shown in Fig. 6.

Results of Table 2 for CPA attacks and results of our T-tests demonstrate that using the PATCH framework we can pinpoint leakage source in our design and then after we can improve the security status of our case study design using this information. It must be noted that we are not going to focus on the effectiveness of countermeasure methods and the process to insert them in a netlist. However, we want to emphasize that the PATCH framework is able to pinpoint the parts of the design that leak information more than others. This way, the designer can make some changes in his source design or netlist to improve the security of his/her design and optimize it in terms of security to fulfill security requirements defined for this design.

Fig. 6
figure 6
T-test results (a) for original unprotected design with 40,000 traces (b) for protected design with 55,000 traces

Full size image

Performance analysis
The process of design evaluation for the security and also design optimization may be repeated several times by engineers to fulfill security requirements. Therefore, it is important that our security evaluation framework be efficient and scalable enough to make its application feasible and beneficial for the designers during the design process. To benchmark the performance of the PATCH, we ran the PATCH framework on a desktop computer with Intel Core i5-6600 CPU and 16GBs of RAM. Table 3, brings the time consumed to complete steps of the security assessment process in PATCH. Times reported for Step 3, Step 4 and Step 5 are related to the number of input plaintexts which was 10,000 plaintext for the reported results. According to the results,  of the total time is spent in Step 3 and Step 4 which is the time to run two commercial tools (i.e. ModelSim and Prime Time PX). The runtime of these parts can be optimized if we use techniques for parallelism but since they look fair enough, we did not try to leverage these techniques.

Table 3 Time spent on running of different steps in the PATCH framework
Full size table

As it can be observed in Table 3, the total time for analysis of our design and its modification is around 25 minutes. Among these steps, some of them have overlapped with normal steps which must be passed in normal VLSI design flow such as Step 1, Step 2, Step 3 and Step 4. If we exclude their time from time spent to run the PATCH framework, the total time which must be spent specifically for the security process in PATCH would be something less than 6 minutes. According to these results, the performance of the PATCH framework looks quite reasonable and it can be easily scaled to process larger and more complex commercial designs in practice. As we noted already, reported times are for the case that we run all tasks without any parallelism and parallel implementation of PATCH can improve the execution time considerably.

Table 4 Comparing researches in the context of automated leakage assessment and remediation
Full size table

Comparison with other frameworks
In this subsection, we are going to compare PATCH with other state-of-the-art researches that referred in this work. This comparison is summarized in Table 4. Among all frameworks and methods, it is only AMASIVE [6, 7] which is not applicable for ASIC design-flow. Karna [23] is interesting from the point that it can modify design without imposing silicon-area overhead by changing standard-cells that have positive slack. This is not feasible in all practical scenario and another disadvantage of Karna is the fact that it is applicable on ASIC designs after mapping stage and before routing stage. Designer usually has not many options and has not enough flexibility to change the design at this stage.

Cascade [8] can conduct leakage assessment at gate-level-synthesis and physical-synthesis stages and it can detect leakage using different distinguishers such as test-vector leakage assessment (TVLA), difference-of-means (DoM) and etc. However, Cascade have no means to automatically remediate points in the design which marked as vulnerable.

RTL-PSC [7] is assesses the leakage at RTL-level. It uses functional-simulation to measure the amount of leakage using KL-divergence and success-rate (SR) metric. Since it works in RTL-level it is applicable both on ASIC and FPGA design flow. Since RTL-PSC is technology-independent, the precision of this framework is lower comparing to technology-dependent approach which considers gate-level and physical-level parameters of design-cells. Beside, RTL-PSC, only reports leakage at granularity of design blocks. It does not report leakage for cells or nets inside a design block which is the reason for its acceptable performance in this assessment framework.

PATCH is can detect vulnerability at granularity of a net inside a gate-level netlist of a design. It is performance is comparable with Cascade and is in acceptably range to be used for commercial applications. Since it uses standard-cell technology library used for design fabrication as its input, its precision is much better than methods which simulate the design at higher-level of abstractions (such as RTL-level or architectural level). However, detection of leakage at gate-level still provides enough flexibility to the designer to revise or modify design to overcome detected issues.

PATCH, has also some drawbacks. Currently it only evaluates the leakage at gate-level while it can do assessment at the next stages of design-flow including mapping and routing. In addition, currently countermeasure insertion is an optional step in our framework and its quality can be improved considerably which we left this as part of our future research. Since PATCH is not limited, we can leverage metrics which used in other frameworks such as KL-divergence and Success-Rate in our framework to improve its precision in detection of vulnerable points in the design.

Conclusion
In this paper we suggested a methodology to localize the sources of information leakage in generic RTL designs that described in HDL formats. We implemented this methodology in our framework which called “PATCH” to help the VLSI designers for security assessment and remediation of their ASIC designs. This framework reports a list of nets that are mainly in charge of unacceptable levels of information leakage and using our custom tool, which called Injection, designers can insert their custom countermeasures in the path of each leaky net. The PATCH framework is designed such that it uses commercial tools that are commonly used by VLSI designers but it is not limited to this special configuration. Our results and analysis showed that our methodology can help the designers to improve the security status of their design but in the price of more silicon area.

As a part of our future work, we are going to look for other heuristic methods that keep the best balance between security and other design objectives. Moreover, we plan to leverage parallelism in the PATCH framework to increase its performance to make it more comfortable and handy for the VLSI designers. In addition, studies on efficiency of different possible countermeasures and automation of the countermeasure insertion process are left as parts of our future researches. Current research is a basic step towards that goal.

