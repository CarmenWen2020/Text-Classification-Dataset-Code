Abstract
Rapid and exponential development of textual data in recent years has yielded to the need for automatic text summarization models which aim to automatically condense a piece of text into a shorter version. Although various unsupervised and machine learning-based approaches have been introduced for text summarization during the last decades, the emergence of deep learning has made remarkable progress in this field. However, deep learning-based text summarization models are still in their early steps of development and their potential has yet to be fully explored. Accordingly, a novel abstractive summarization model is proposed in this paper which utilized the combination of convolutional neural network and long short-term memory integrated with auxiliary attention in its encoder to increase the saliency and coherency of generated summaries. The proposed model was validated on CNN\Daily Mail and DUC-2004 datasets and empirical results indicated that not only the proposed model outperformed existing models in terms of ROUGE metric but also its generated summaries had higher saliency and readability compared to the baseline model according to human evaluation.

Introduction
Considering the rapid and extensive development of internet technology, people are daily confronted with a large amount of textual information like papers, news, reports, and blogs. Due to the fact that these large amounts of information are generally unstructured, fast, and accurate selection of efficient information is changed to a prominent challenge that needs to be solved. To this end, automatic text summarization, which aims to generate a shorter version of text covering the main idea of the source text, is known as an effective solution [1, 2]. Automatic text summarization commonly aims to compress the input text in a lossy manner as well as preserving the key concepts [3]. Notably, text summarization not only plays a key role in natural language processing and information retrieval but also can help readers to justify the concept of a text as well as reducing the consumed time in retrieving the information [4, 5].

Text summarization is commonly classified into two categories of extractive and abstractive models [6]. Extractive text summarization techniques are generally based on traditional approaches where the summary is generated by selecting the important sentences of the source text and combining them for composing a coherent summary. On the other hand, abstractive text summarization considers the semantic of the source text and tries to resemble human-written summaries where the new sentences are generated according to the source document [7].

Extractive summarization techniques were first introduced and obtained remarkable results while they only focused on mining the core information from the source text and utilized them as the summary [8]. More recently, by enhancement of computer performance and development of deep neural networks, abstractive summarization techniques have been at the center of attention and made considerable progress while they try to generate the summary by mapping the input sequence to another output sequence [4]. The current effective abstractive summarization techniques are commonly implemented based on the encoder‚Äìdecoder frameworks where various deep neural networks such as Recurrent Neural Network (RNN) [9], Long Short-Term Memory (LSTM) [10], Convolutional Neural Networks (CNN) [11] can be used as candidate models to implement encoder or decoder.

Although numerous text summarization models, especially deep learning-based methods [3, 9, 12,13,14,15], have obtained considerable results in recent years, existing models were not able to resolve all problems and they are still confronted with some limitations. In fact, a good summary must be brief as well as containing salient information. It must also meet the syntactic structure requirement besides being semantically coherent. However, by considering the current abstractive text summarization models, it is observed that the generated summaries generally have poor saliency and are not in accordance with the syntactic structure requirement [2, 12, 16]. Abstractive models have also difficulties in meeting the synthetic structure requirements [2]. Moreover, existing encoder‚Äìdecoder models only take one word as an input in each time step and do not consider phrases [12]. However, assuming phrases as input may lead to better efficiency because they are composed of several words for expressing the overall meaning and can help to produce natural sentences by learning the collocation of phrases [17].

On the other hand, the decoder does not need to receive all contents of the source text to generate a summary and only important information is required. For instance, humans first read and comprehend the text to generate the summary. Then, they outline important information as well as filtering the irrelevant ones. Finally, the summary is written considering the extracted information. However, current abstractive models are not able to highlight principal information in the source text and they consider the whole source text as the input of the encoder. In this regard, if the source text includes remarkable irrelevant information, the encoder takes all the source text as input and therefore is highly influenced by irrelevant information and cannot be able to efficiently represent the semantic of text [5].

Having the mentioned issues in our mind, a novel abstractive text summarization model is proposed in this paper to not only decrease the impact of irrelevant information but also improve the coherence and saliency of the generated summary as well as paying more attention to synthetic structure. The proposed model leveraged the combination of CNN and LSTM to form the encoder and despite other existing models where hard attention is commonly used in the decoder, auxiliary attention is used in the encoder to imitate human brains in generating the summary. The main contributions of this paper can be categorized as follows.

The proposed model uses the combination of CNN and LSTM in the encoder to be able to make use of phrases as input which can not only lead to the generation of more natural sentences as summaries but also improve the overall performance of text summarization.

Despite other presented attention mechanisms, the proposed model utilizes auxiliary attention in the encoder and before the decoder to simulate the human brain in summarizing by outlining the important information and filtering the information that is not useful in generating the summary.

The proposed model also uses the combination of CNN and LSTM besides the auxiliary attention in the encoder. Based on the empirical results, the proposed model has higher efficiency compared to the state of the arts while its generated summaries are semantically and syntactically acceptable.

The remainder of this paper is organized as follows. Related studies with a focus on deep learning-based abstractive models are reviewed in Sect. 2. Section 3 presents the proposed model and introduces its key ideas. The proposed model contextualization and empirical results are provided in Sect. 4. Conclusion and remarks on future research direction are mentioned in Sect. 5.

Related work
Considering the vast quantities of textual data that are currently available and must be summarized to retrieve efficient knowledge within a sensible period of time, a lot of attention has been paid to automatic text summarization models that are broadly classified into extractive and abstractive [6, 7, 12, 13, 18]. In the early days of research in the field of text summarization, most studies focused on extractive approaches where summarization is conducted by extracting key sentences from source documents and using them to generate summaries [6, 7, 19,20,21,22]. Traditional extractive text summarization approaches were highly dependent on human-generated features and utilized computational techniques, such as genetic programming, fuzzy logic, neural networks, or machine learning to produce the summary [7]. In this regard, Cheng and Lapata [23] proposed a model for single document summarization. His proposed data-driven model employed neural network and continuous sentence features besides a hierarchical document encoder and attention-based decoder to generate the summary. In a similar line of research, Litvak and Last [24] proposed supervised and unsupervised approaches for identifying the key concepts in the source documents to generate a summary. Both of their approaches were based on an extension of systematic representation that took advantage of structural document features to enhance the efficiency of the traditional vector space model for text summarization.

Kami-Fai Wong et al. [25] also proposed a learning-based model that combined different features of sentences that were categorized as surface, relevance, content, and event. Notably, surface features considered the external aspect of sentences while relevance features measured the relatedness of a sentence with other sentences. The content features also measured sentences according to the content transmitting words and the event features indicated sentences based on their containing events.

By the extension of deep learning and its remarkable efficiency in various natural language processing tasks, it was also considered as an attractive solution for extractive summarization [2, 4, 12, 17]. In this regard, Cao et al. [26] utilized a convolutional neural network for encoding the sentences and then ranked them by extracting the semantic information to generate the final summary. However, their proposed model did not consider the content while extracting the keywords.

Ren et al. [27] also proposed a model based on convolutional and attention-based recurrent neural networks for the summary generation that considered the content to find valuable sentences. Sukritiand and Nidhi [28] also proposed a deep learning-based extractive text summarization model that contained principal steps as feature extraction, feature enhancement, and summary generation. In the following, Nallapati et al. [22] proposed a model named SummaRuNNer which considered summarization as a sequence classification problem. They employed two-level recurrent neural networks where the first one was performed for word-level and the second one was used to compute the hidden representation of sentence-level for generating the summary.

Although extractive text summarization models obtained considerable results and their generated summaries could meet the syntactic structure requirement, their generated summaries were not semantically coherent [6]. On the other hand, humans generally tend to paraphrase the documents in their own words and prefer summaries that are concise, coherent, and salient. As a result, the study of text summarization has changed its direction and mainly focused on abstractive text summarization in recent years to meet the mentioned needs [17]. Abstractive text summarization is harder than extractive text summarization but it is more acceptable because its generated summary is an appropriate representation of human-generated summaries. In spite of the fact that the primary abstractive text summarization approaches were based on machine learning and required hand-crafted features, the emergence of deep learning has also made a great revolution in this field [4, 18].

In this regard, Rush et al. [29] solved the problem of long document summarization using a deep neural network. They utilized a bag of word based encoders and a convolutional neural network with an attention mechanism as a decoder to generate summaries. Chopra et al. [30] also proposed an attention-based feed forward neural network for generating headlines of news articles. Following a similar line of research, Lopyrev [31] utilized attention-based long short-term memory network for generating headlines from the text of news articles. They utilized recurrent neural networks as the substitute of the neural language model in the decoder which yielded considerable enhancement.

Deep learning-based text summarization models are generally suffering from short sentences and repeated words. Rare words are commonly known as another key problem in abstractive text summarization because some words that rarely happen might be assumed unimportant while they are important in human view in constructing the summary. Accordingly, researchers started to overcome the repeatability and rare word problems [18]. In this regard, Nallapati et al. [9] implemented an attention-based encoder‚Äìdecoder using recurrent neural network and their model obtained the state of the art performance on two various datasets. Coverage mechanism was also proposed to solve the repeatability problem. To this end, Suzuki and Nagata [32] evaluated the upper bond features of the target word in the encoder and controlled the output word in the decoder to decrease the repeatability of summaries. More recently, Paulus et al. [33] performed reinforcement learning and applied attention mechanism on the decoder to create summaries.

To overcome the rare word problem, Vinyals et al. [34] utilized the pointer mechanism in each encoder and decoder. The concept of pointer mechanism with different configurations was also employed by Gu et al. [35], Gulcehre et al. [36], and Nallapati et al. [9] to address the rare word problems. See et al. [37] also used an identical mechanism in their model which led to the great improvement in the summary generation.

In the following, Song et al. [3] proposed a summarization model that took advantage of convolutional neural network and long short-term memory network besides pointer mechanism to generate a summary. Yao et al. [38] also used attention-based long short-term memory network with a probabilistic generation model and reinforcement learning to produce summaries. Moreover, Li et al. [39] also employed seq2seq encoder‚Äìdecoder to generate the abstractive summary. By utilizing the generative model, they made their model able to consider the latent structure and discriminative state information of summary. Wang and Len [10] also proposed a summary-aware attention in social media for short text abstractive summarization. They utilized source hidden states and attended summary vectors to compute summary-aware attention weight.

Xiang et al. [40] presented an attentional encoder‚Äìdecoder framework where Bi-LSTM was used in both encoder and decoder to encode the input text and generate the summary. Huan et al. [41] also proposed a variational neural decoder text summarization model that used a combination of variational RNN and variational autoencoder to capture complex semantic representation.

Following a similar line of research, Fan et al. [42] used a convolutional sequence to sequence model to perform abstractive summarization. They also proposed a mechanism that enabled the readers to control the important aspects of the generated summary by specifying high-level attributes. Hsu et al. [43] presented a unified model that employed the combination of abstractive and extractive summarization. They also utilized sentence-level attention to modulate word-level attention and inconsistency loss function to penalize the inconsistency between two attention levels. Li et al. [44] proposed a neural abstractive summarization framework that leveraged actor-critic approaches from reinforcement learning where typical attention and maximum likelihood were, respectively, employed as actor and critic.

Moreover, Celikyilmaz et al. [15] proposed deep communicating agents in an encoder‚Äìdecoder model to overcome the problem of long document summarization. Based on their model, long text is divided into subsections and each of them is encoded by a specific agent and then reinforcement learning is utilized to perform training. Zhou et al. [45] extended sequence to sequence model for text summarization. Their proposed model includes a sentence encoder, selective gate network, and attention-equipped decoder where selective gate network provides a sentence-level representation by controlling the flow of information from encoder to the decoder.

By the development of pre-trained language models, they have been also utilized for the task of text summarization. In this regard, Zhang et al. [46] used Bidirectional Encoder Representations from Transformers (BERT) to encode the input sequence into context representations. There were two stages in their decoder where a Transformer-based decoder was used in the first stage to generate a draft output sequence and then each word of the draft sequence was masked and fed to BERT in the second stage. Finally, by combining the input sequence and the draft representation generated by BERT, a transformer-based decoder was utilized to predict the refined word for each masked position. Aksenov et al. [13] also examined conditioning the encoder and decoder of a transformer-based neural model on the BERT language model. Additionally, they proposed a new method of BERT-windowing that allowed chunk-wise processing of long text. They also introduced 2-dimensional convolutional self-attention to investigate how locality modeling can affect the summarization ability of the transformer. Liao et al. [47] also proposed an aggregation mechanism based on the transformer model to address long text representation challenges where history information can be reviewed to make the encoder hold more memory capacity.

Overall, it is worth mentioning that although numerous deep learning-based models have been proposed for the task of abstractive text summarization, they are still in their early steps of development and there is a big gap between machine-generated and human-generated summary which makes this field very attractive with great potential for researchers. Considering the current abstractive summarization models, it can be said that not only the generated summaries have poor saliency but also they are not in accordance with syntactic structure. Moreover, generated summaries commonly include some irrelevant information, and generating new n-grams which is the main purpose of abstractive summarization is not efficiently covered by previous studies.

Methodology
Problem definition
Abstractive text summarization aims to generate summaries of a text including its main idea without copying its important sentences. Assume that ùëã={ùë•1,ùë•2,‚Ä¶, ùë•ùëõ} is the input document, where ùë•ùëñ‚ààùëã refers to ith token and ùëõ is the length of input text. Givenùëã, the proposed model aims to learn a function ùëì(ùë•) that maps ùëã to another sequence of tokens ùëå={ùë¶1,ùë¶2,‚Ä¶, ùë¶ùëö}, where ùëå is the summary with the length of ùëö while ùëö<ùëõ. Notably, ùëì is implemented using the proposed encoder‚Äìdecoder network that is described in the following section.

Architecture of the proposed model
The details of the proposed model are introduced in this section. The proposed model consists of four parts including, word embedding, encoder, auxiliary attention, and decoder. The flow diagram of the model is also depicted in Fig. 1 where the input is the source text and the generated summary is the output. Firstly, documents are segmented into sentences, and sentences are tokenized to access words. Each word is then embedded in a word vector. Secondly, the word vectors are entered into an encoder aiming to obtain an understanding of the source text. The encoder of the proposed model consists of a combination of CNN and LSTM. Thereafter, auxiliary attention is applied to the encoder to emphasize more important words via decreasing the interference of useless information to the decoder. The auxiliary attention is only performed along with the training process where the semantic of the reference summary is compared with the semantic of the source text. Noteworthy, the auxiliary attention is not applied during the test process. Finally, the output of the encoder and auxiliary attention is fed to the decoder to generate the final summary. More details about each part of the model are provided in the following section.

Fig. 1
figure 1
Flow diagram of the proposed model including embedding layer, encoder, auxiliary attention and decoder

Full size image

Word embedding
Deep neural networks generally require vectorized representation as input to have a clear understanding of input words. Due to the fact that the proposed abstractive text summarization model utilizes deep neural networks, the input source must be first converted to a vector representation. Bag Of Word (BOW) is the most frequently used technique that is commonly employed in text summarization to transform the input text to vectors. However, this technique, besides its simplicity, suffers from remarkable drawbacks while it provides sparse data in a high dimensional vector space and ignores the semantic relation between different words in the text. Therefore, building an effective representation of the input source for text summarization is essential. In this regard, Skipgram [48] model, which revealed its efficiency in various natural processing tasks [49,50,51], is used in the proposed model to generate word vectors of the input text. Skipgram is a supervised learning algorithm that can not only work on any raw text but also requires less memory compared to other vector representations as well as it can efficiently represent less frequent words. Moreover, it is able to represent semantically similar words with neighboring points in the same vector space that can have a great impact on summarization. However, the sub-linear relationships are not explicitly defined in Skipgram and there is little theoretical support behind such characteristics [48].

The diagram of the Skipgram model is depicted in Fig. 2. As it is clear, it is a shallow two layers neural network which learns vector representation of a word based on its context. In other words, Skipgram trains the word vectors ùë•1,ùë•2,‚Ä¶ùë•ùëõ by maximizing the average log probability using the following equation.


1ùëõ‚àëùëñ=1ùëõ‚àë‚àíùëê‚â§ùëó‚â§ùëê,ùëó‚â†0logùëù(ùë•ùëñ+ùëó|ùë•ùëñ)

(1)

Fig. 2
figure 2
Skipgram model structure

Full size image

Here, ùëê  represents the content value that must be estimated for each word. The probability value of ùëù(ùë•ùëú|ùë•ùêº)  is also calculated using the Softmax function as follows. Notably, Softmax is not a traditional activation function and can produce multiple outputs for an input array. Therefore, Softmax can be used to build neural network models that can classify more than two classes instead of a binary class solution. However, Softmax cannot produce more than one label and therefore it cannot be utilized for multi-label classification.


ùëù(ùë•ùëú|ùë•ùëñ)=exp(ùë£‚Ä≤ùëáùë•ùëúùë£ùë•ùêº)‚àë|ùë£|ùë•=1(ùë£‚Ä≤ùëáùë•ùë£ùë•ùêº)

(2)

Here, ùë£ùë• and ùë£‚Ä≤ùë•, respectively, refer to the input and output vector representations of word ùë• and |ùë£| is the total number of vocabulary. In other words, this model assumes two vector representations for each word that are randomly assigned in the beginning. Along with the training process, these vectors are updated to maximize the value of (Eq. 1). At least, the final embedding vector for each word is obtained by averaging or joining these two vectors.

Encoder
Encoder generally tries to imitate the human brain in reading and understanding the text. Commonly, recurrent neural network due to its ability in representing the serialized data is used to implement the encoder. Considering the fact that the general recurrent neural network has also remarkable problems, such as gradient vanishing and long short-term dependency [3, 52], variants of long short-term memory network have been recently employed in various studies as the encoder.

However, LSTM only takes a word as the input in each time step and it is not able to consider phrases as input. Owing to the fact that there is a need to generate sentences that have high saliency in text summarization, considering word interaction and phrases as input may have a great impact on generating the summary. Accordingly, we decided to combine CNN and LSTM aiming to create an encoder that can take phrases as input. Convolutional neural network is utilized for two reasons. Firstly, single-layer CNN can generate low-level features. Secondly, it has been efficiently utilized for sentence-level classification.

While the goal of using CNN is to provide phrases as the input of LSTM, firstly a window with a specified size of ùëõ must be considered to be able to extract phrases. Hence, for each word ùë•ùëñ in the sentence that must be entered into the LSTM, embedding vectors of ùëö words before and after ùë•ùëñ are concatenated to form an input matrix ùê¥. Considering that ùëë is the dimensionality of embedding vectors and  ùëõ is the size of the window, the dimensionality of the input matrix would be 2ùëö+1√óùëë and it can be denoted by ùê¥‚ààÓàæ2ùëö+1√óùëë.

To produce new features, convolutional operation must be performed on the input matrix. Noteworthy, in order to apply filters on input matrix ùê¥‚ààÓàæ2ùëö+1√óùëë, the filter width must be equal to the dimensionality of embedding vectors to maintain the sequential structure of input text and therefore only filter height can be varied. Considering ùê¥‚ààÓàæ2ùëö+1√óùëë  as the input matrix, convolutional filter ùêπ‚ààÓàæ‚Ñé√óùëë is performed on   ùê¥ to generate submatrix as new feature  ùê¥[ùëñ : ùëó]. While the convolutional operation is repeatedly applied on matrix ùê¥,  ùëÇ‚ààÓàæ2ùëö+1‚àí‚Ñé+1√óùëë  is generated as the output sequence using the following equation.


ùëÇùëñ=ùë§ ‚ãÖ ùê¥[ùëñ:ùëñ+‚Ñé‚àí1],

(3)

where =1,‚Ä¶,2ùëö+1‚àí‚Ñé+1, ùë§ is the weight matrix and ‚ãÖ is the convolutional operator performed on the input matrix and convolutional filter to obtain the feature maps ùê∂‚ààÓàæ2ùëö+1‚àí‚Ñé‚Ñé+1. To achieve feature maps, bias term (ùëè‚ààÓàæ) and activation function (ùëì) are also added to ùëÇùëñ as bellows.

While employing various filter sizes leads to the generation of various feature maps, a pooling function is required to induce a fixed size vector by capturing the most important features from each feature map. By performing the pooling function, obtained features are concatenated to form the feature vector  ùëúùëñ that can be used as the input of LSTM. As a result, instead of using word embedding of each word as the input of LSTM from left to right, (ùëú1,ùëú2‚ãØùëúùë°)  obtained from the convolutional neural network are fed to LSTM as input in each time step. The forward propagation of LSTM can be computed using the following equations.


ùëìùë°=ùúé(ùëäùëì.[‚Ñéùë°‚àí1,ùëúùë°]+ùëèùëì)

(5)


ùëñùë° =ùúé(ùëäùëñ.[‚Ñéùë°‚àí1,ùëúùë°]+ùëèùëñ)

(6)


ùëúùë°=ùúé(ùëäùëú.[‚Ñéùë°‚àí1,ùëúùë°]+ùëèùëú)

(7)


ùê∂ÃÉ ùë°=tanh(ùëäùê∂.[‚Ñéùë°‚àí1,ùëúùë°]+ùëèùê∂)

(8)


ùê∂ùë°=ùëìùë°√óùê∂ùë°‚àí1+ùëñùë°‚àóùê∂ÃÉ ùë°

(9)

Here, ùëäùëì,  ùëäùëñ, ùëäùëú, and ùëäùê∂ , respectively, refer to the weight matrices of forget gate, input gate, output gate, and cell state. ùëúùë° shows the current input of the sequence which is obtained from the convolutional layer in the proposed model and ‚Ñéùë°‚àí1 also is the input from the previous step. ùê∂ùë° is also the cell state while ùúé  is the sigmoid function. The whole process in each LSTM cell aims to calculate the hidden state for the next cell which is at once part of the output of the current cell and the input of the next cell. Finally, ‚Ñéùëñ=(‚Ñé1,‚Ä¶,‚Ñéùëõ) are obtained as the hidden states of each time step while ùëõ is the length of the input text.

Auxiliary attention
Existing encoders generally read and comprehend the source text and then generate the summary. However, they are not able to outline principal information or certify the correctness of encoded information [37, 53]. On the other hand, humans are able to read and comprehend the text as well as highlighting the important information while they want to generate a summary. In other words, humans prefer to focus on important information of text in writing a summary rather than considering the whole source text. Inspired by the process of humans' brains in generating summary, we proposed an auxiliary attention to be applied on the encoder in contrast to the existing attention mechanisms that are commonly utilized in decoder along with the summary generation [10, 37, 39, 53]. The reason behind using auxiliary attention is to provide only prominent information for the decoder by decreasing the interference of useless information which can yield to the generation of more accurate summaries.

As previously mentioned, a series of hidden states ‚Ñéùëñ=(‚Ñé1,‚Ä¶,‚Ñéùëõ) are generated after applying the encoder. In this step, these hidden states are concatenated to obtain ùêª to accurately represent the entire text. The next step is extracting important features. To this end, ùêª and ‚Ñéùëñ at each time step are fed to a multi-layer perceptron to achieve ùëîùëñ which is a weight vector that represents the importance of the ùëñth input (Eq. 11). Thereafter, a dot product operation is carried out between ‚Ñéùëñ and ùëîùëñ and finally ‚Ñé‚Ä≤ùëñ is obtained which represents the information that is extracted at time step ùëñ (Eq. 12). Here, ùëäùë† and ùëâùëñ are the learnable parameters while ‚äô  is the dot product.


ùëîùëñ=ùúé(ùëäùë†.ùêª+ùëâùëñ.‚Ñéùëñ+ùëè)

(11)

It is worth mentioning that although multi-layer perceptron was used to extract information, the correctness of extracted information is not guaranteed and it is necessary to strengthen the important information as well as ignoring or weakening the useless or unnecessary information. Considering the fact the semantics of source text and reference summary in supervised learning must be consistent, we decided to compare their semantics to certify the semantic accuracy of extracted information. In this regard, these new states ‚Ñé‚Ä≤ùëñ=(‚Ñé‚Ä≤1,‚Ä¶,‚Ñé‚Ä≤ùëõ) are summed in a new vector as ùëâùë† which aims to show the semantic of the source text (Eq. 13). Notably,ùëâùë† is then used as the input of the decoder for generating the summary. On the other hand, the reference summary is also encoded using LSTM and hidden states of ùëüùëñ=(ùëü1,‚Ä¶,ùëüùëõ) are obtained. These hidden states are also summed into a new vector as ùëâùëü which represents the semantic of the reference summary (Eq. 14).

Thereafter, cosine similarity is used to measure the similarity between ùëâùë† and ùëâùëü that, respectively, refers to the semantic of the source text and reference summary (Eq. 15). Computing cosine similarity between these two vectors can help us to determine if the extracted information is accurate or not. Larger similarity score indicates that the extracted information is more correct.


cos(ùëâùë†,ùëâùë°)=ùëâùë†.ùëâùë°||ùëâùë†||.||ùëâùë°||

(15)

To enhance the performance of multi-layer perceptron in extracting more important information, the similarity score is fed back to the network. Along with the training process, the similarity score must be maximized to ensure that the most accurate information is extracted. Negative log-likelihood must be also minimized to obtain the highest similarity score. It must be noted that the auxiliary attention is only applied to the training process because the reference summary is available. Due to the fact that the reference summary is not available during the test process, auxiliary attention can not be performed. The schematic structure of the proposed encoder containing CNN, LSTM, and auxiliary attention is depicted in Fig. 3.

Fig. 3
figure 3
Schematic structure of the proposed encoder containing CNN, LSTM, and auxiliary attention

Full size image

Decoder
The goal of the decoder is to transform the source text into a series of hidden states and then revert these hidden states to actual word sequences and generate a summary. To this end, LSTM is also utilized in the decoder to generate the final summary (Eq. 16). Here, ùë†ùë° and ùë¶ùë°, respectively, refer to the hidden state and input of LSTM at each time step. Notably, the decoder is initialized with ùëâùë† which indicates the principal information of the source text.


ùë†ùë°=LSTM(ùë†ùë°‚àí1,ùë¶ùë°),ùë†0=ùëâùë†

(16)

At each decoding step, the hidden state ùë†ùë° must be updated based on the previous hidden state and input tokens. To generate words, conditional probability distribution which determines the probability of generating the ùëñth word must be computed using the following equation.


(<ùë¶(1),‚Ä¶, ùë¶(ùëö)>|‚ü®ùë•(1),‚Ä¶, ùë•(ùëõ)‚ü©)=‚àèùë°=1ùëöùëÉ(ùë¶(ùë°)|<ùëâùë†,ùë¶(1),‚Ä¶, ùë¶(ùë°‚àí1))

(17)

Specifically, ùëÉ is the vocabulary distribution that can be computed using ùëÉvocab,ùë°=Softmax (ùë§ùë†ùë°+ùëè), where ùëÉvocab,ùë° is a vector whose dimensionality is equal to the vocabulary size |ùëâ| and softmax (ùë£ùë°)=exp(ùë£ùë°)‚àëùúèexp(ùë£ùúè) for each ùë£ùë°  of vector ùë£. Notably, after generating each word in each time step, the same word is used as the input of the next decoding step to generate the next word. As a result,  ùëÉvocab,ùë°(ùë§) determines the probability of generating the target word ùë§ in the vocabulary |ùëâ|.

Experiments
This section includes details of the experiments besides the used datasets, evaluation metrics, implementation process, and empirical results.

Dataset
CNN/Daily Mail dataset [9] was used in the experiments for training and evaluation of the proposed model. This dataset includes 312,085 documents with multi-sentence summaries. Each document contains about 29.74 sentences and 766 words on average. The corresponding summaries also contain about 3.72 sentences and 53 words on average. This dataset generally contains 286,817 training samples, 13,368 validation pairs, and 11,487 test pairs.

DUC-2004 dataset [54] was additionally utilized for the evaluation of the proposed model. It is worth mentioning that although it is a sort of old and small dataset, it contains many manually generated summaries and has been used as a standard dataset in various academic research and most previous studies have conducted their experiments on this dataset. As a result, we also decided to use this dataset in the experiments. This dataset only includes 500 documents and each document contains four manual reference summaries while each of them includes 75 bytes on average. As it is clear, DUC-2004 dataset is a too small dataset for training a deep neural network, and therefore it was only used in the experiments to test the proposed model.

Evaluation metrics
ROUGE
Recall Oriented Understanding for Gisting Evaluation (ROUGE) [42] is a metric that is commonly utilized for text summarization where the lexical units overlap, such as unigram, bigram, word sequence, and lowest common subsequence between generated summary and reference summary is measured. ROUGE is calculated using the following equation (Eq. 18).


ROUGE - ùëÅ=‚àëùëÜ‚àà{Refrences~Summaries}gramùëõ‚ààùëÜ‚àëCountmatch(gramùëõ)‚àëùëÜ‚àà{Refrences~Summaries}gramùëõ‚ààùëÜ‚àëCount(gramùëõ)

(18)

Here, ùëÅ refers to the n-gram length. Moreover, Countmatch(gramùëõ) represents the maximum number of n-grams co-occurring in the generated summary and reference summary while Count (gramùëõ) shows the number of n-grams in the reference summary.

Notably, separate scores are commonly reported using the ROUGE toolkit for various n-grams. However, among various reported scores, unigram-based (ROUGE-1) and bigram-based (ROUGE-2) scores indicated the best agreement with human judgment and accordingly we calculated them for evaluating the proposed model. Based on existing studies that utilized ROUGE recall as the evaluation metric, we also employed it in the evaluations of this paper. Pyrouge packageFootnote 1 and official ROUGE scriptFootnote 2 were utilized to obtain ROUGE scores.

Human evaluation
Although ROUGE metric has been extensively utilized to evaluate text summarization models, it is only able to evaluate the literal similarity between generated summary and reference summary and therefore cannot consider the saliency of the generated summary. In order to prove that the proposed model is able to generate more salient and informative summaries, we carried out more evaluations with the help of human experts [16]. To this end, various sets of randomly selected summaries (including reference summary, summary generated by the proposed model, summary generated by the model of See et al. [37]Footnote 3) were assigned to different evaluators to grade each summary based on its saliency. Noteworthy, the evaluators did not know which summary was produced by the proposed model and which one was produced by the model of See et al. [37] and they were only aware of the reference summary. Table 1 presents the scoring criteria that were utilized by evaluators to score the summaries where relevance is considered as a metric to express the informatively of summaries. At least, the results provided by various evaluators were collected and the mean value was computed that could be used for comparison.

Table 1 Scoring criteria of saliency metric
Full size table

Besides saliency, readability is another important factor that can be measured to show the superiority of the proposed model in generating summaries. Similar to saliency evaluation, we assigned various sets of three randomly selected summaries to evaluators and asked them to score summaries based on syntax and grammar. Table 2 illustrates the readability scoring criteria. Noteworthy, the evaluators were not aware of the details of the summary and they were just told which summary was the reference summary. Finally, the results provided by various evaluators were collected and the mean value was computed. It must be noted that while all of the proposed experiments were conducted in Iran and our official language is not English, we asked professional English teachers to score the summaries so as to guarantee the specialty of evaluators and being sure about the obtained results.

Table 2 Scoring criteria of readability metric
Full size table

Baselines
In order to illustrate the efficiency of the proposed model, it is necessary to provide a comprehensive comparison between the proposed model and the existing state of the arts. While we conducted the experiments in two various datasets (CNN/Daily Mail and DUC-2004), existing methods that were used as baselines to be compared with the proposed model are divided into two groups considering their employed dataset.

Group A: Models conducted their experiments on CNN/Daily Mail dataset.

Words-lv2k-temp-att: It is a model that utilized a feature-rich encoder and pointer mechanism, respectively, for word embedding and overcoming the rare word problems [9].

Controlled summarization: It is a model that enabled users to determine some high-level features to control the shape of the generated summary [55].

Pointer-Generator‚Äâ+‚ÄâCoverage: It is a model that not only utilized the pointer mechanism to overcome the rare words problem but also introduced an extended vocabulary coverage to solve the repeatability problem [37].

ML‚Äâ+‚Äâintra-attention: It is a model where the attention mechanism was used inside the decoder to overcome the repeatability problem [33].

ATSDL: It is a model that used phrase extraction in the pre-processing step to provide the encoder with phrases as input rather than words besides using a pointer mechanism to solve the rare words problem [3].

End2end w/inconsistency loss: It is the combination of extractive and abstractive summarization models for generating a summary [43].

Attentive information extractor: It is a model that used an attentive information extractor in the encoder and a traditional attention mechanism in the decoder to generate the summary [40].

DCA MLE‚Äâ+‚ÄâSEM‚Äâ+‚ÄâRL: It is a model that utilized deep communicating agents in the encoder and decoder and tried to address the problem of long documents in generating summary using reinforcement learning [15].

DCA MLE‚Äâ+‚ÄâSEM: It is a model that utilized deep communicating agents in the encoder and decoder without reinforcement learning [15].

BERT‚Äâ+‚ÄâCopy transformer: It used BERT as the encoder and decoder while a new method of BERT-windowing was also employed to allow chunk-wise processing of long text [13].

Group B: Models conducted their experiments on DUC-2004 dataset.

ABS‚Äâ+‚Äâ: It is a model where CNN and neural language model were, respectively, utilized as encoder and decoder [29].

AC-ABS: It is a model that utilized an actor-critic framework to increase the efficiency of traditional abstractive summarization [44].

Words-lv5k-1sent: It is a model based on attentional encoder‚Äìdecoder [9].

SEASS: It is a model where selective encoding is utilized to develop the seq2seq model for text summarization [45].

C2R‚Äâ+‚ÄâAtten: It is a model where CNN and RNN were, respectively, used as encoder and decoder [30].

Model configuration
Text pre-processing
Pre-processing is known as one of the prominent steps in natural language processing and there are many tools such as CoreNLP and NLTK that can be used to perform this step and reduce word redundancy. Although this is a primary and easy step, it is necessary and helpful for the following steps. In this regard, CoreNLPFootnote 4 was used in our implementation to pre-process the source text since it supplies a bunch of linguistics analytical tools for processing the text content. Commonly, text pre-processing includes tokenization, morphological reduction, and coreference resolution. For instance, words like ‚Äúbe‚Äù, ‚Äúam‚Äù, ‚Äúis‚Äù, ‚Äúare‚Äù, ‚Äúwas‚Äù, ‚Äúwere‚Äù, and ‚Äúbeen‚Äù are considered as different phrases without performing morphological reduction. As a result, morphological reduction is required to merge these phrases into one. Additionally, there are numerous demonstratives and pronouns in the source text that may lead to ambiguity along with the training process. Therefore, coreference resolution is required to eliminated ambiguity before the training process. To better illustrate the details of pre-processing, a sample of the source text and its pre-processed result is depicted in Fig. 4.

Fig. 4
figure 4
Source text pre-processing steps

Full size image

Hyperparameters
While pre-processing was completed and tokens were extracted, Skipgram was used to convert tokens to vector representations. In this step, the dimensionality of word vectors and window size were, respectively, assumed as 200 and 3. Word vectors were also updated using a learning rate of 0.1.

As previously mentioned, the encoder of the proposed model consists of CNN and LSTM. In CNN, filter size and the number of filters were considered as hyper-parameter. In the experiments, the filter size of (3, 4, 5) and 128 filters led to the best results. The hidden state size of LSTM was also set to 256. It is worth mentioning that the size of the window utilized in the encoder to extract phrases from input was also set to 3.

Considering the fact that the average length of reference summary in CNN\Daily Mail and DUC-2004 datasets were, respectively, about 53 words and 75 bytes, the maximum step of the decoder was set to 100 for CNN\Daily Mail dataset and 25 for DUC-2004 dataset. Moreover, the batch size was set to 16 in the experiments, and the beam search algorithm with the size of 4 was used to generate the summary. AdaGrad update rule with the learning rate of 0.01 was also employed for optimization. Training was also conducted in 50 epochs. It must be said that all the experiments were implemented using Python 3 and TensorFlow 2.1.0

Results
As previously mentioned, we conducted the experiments using two different datasets so as to be able to compare the efficiency of the proposed model with a wide range of existing models. It is worth mentioning that although applying attention mechanism on encoder‚Äìdecoder deep learning-based summarization models is not a quite new solution for overcoming the rare word problem as well as increasing the performance, current abstractive summarization models commonly used attention mechanism in the decoder to extract key information along with summary generation. However, the proposed model applies auxiliary attention in the encoder to provide informative information for the decoder rather than encoding all the input sequences. Furthermore, existing encoder‚Äìdecoder models generally used CNN or LSTM in their encoder while the proposed model takes advantage of the combination of CNN and LSTM for encoding input sequence which enables the encoder to uses phrases as input rather than words which can have a great impact on generating more coherent summaries. The empirical results on CNN\Daily Mail and DUC-2004 datasets based on the ROUGE metrics are summarized in Table 3. Higher ROUGE value indicates that summaries generated by the proposed model are more similar to the reference summaries.

Table 3 Performance comparison of the proposed model against other existing models
Full size table

As it is obvious, the proposed model outperforms the state of the art models on both datasets. Specifically, the proposed model obtained ROUGE-1 of 42.93, ROUGE-2 of 20.78, and ROUGE-L of 39.63 on CNN/Daily Mail dataset. The proposed model also outperformed the state of the art models on DUC-2004 dataset and obtained ROUGE-1 of 34.38, ROUGE-2 of 11.24, and ROUGE-L of 29.78. Overall, considering the obtained results (Table 3), it can be concluded that the proposed model has about 3% improvement on average compared to the best existing abstractive summarization model.

In order to provide a comprehensive analysis of the efficiency of the proposed model, two samples of the source text and their reference summaries, summaries generated by Pointer-Generator‚Äâ+‚ÄâCoverage [37], and the proposed model are illustrated in Fig. 5. As it is clear, the proposed model is able to capture more significant information existing in the source text which clarifies that its generated summaries have greater saliency.

Fig. 5
figure 5
Abstractive summarization examples

Full size image

Due to the fact that the ROUGE metric can only calculate the repeatability of n-gram and does not analyze the quality of a generated summaries, we also performed human evaluation to examine the saliency and readability of the proposed model. To this end, 100 samples were randomly chosen (based on the procedure mention in Sect. 4.2.2) and assigned to three expert evaluators to scores them based on the scoring criteria mentioned in Table 1 to prove that the proposed model has higher saliency in generating the summaries compared to the baseline model. The results of the saliency evaluation are summarized in Table 4. It can be seen that the average saliency score of the proposed model is about 6% higher than the model of See et al. [37]. Higher saliency score of the proposed model confirms that it was able to generate more informative summaries which can be due to the fact that the proposed model can decrease the interference of irrelevant information in the source text using auxiliary attention.

Table 4 Results of saliency evaluation
Full size table

To evaluate the readably of the proposed model, 100 samples were also randomly chosen and assigned to three expert evaluators to scores them based on scoring criteria mentioned in Table 2 to prove that the generated summaries of the proposed model have greater readability compared to the baseline model. The results of the readability evaluation are summarized in Table 5. It can be seen that the average readability score of the proposed model is about 7% higher than the model of See et al. [37] and therefore it can be concluded that the proposed model obtained a higher score for both syntax and grammar compared to the baseline model.

Table 5 Results of readability evaluation
Full size table

Therefore, it can be concluded that not only the proposed model was able to generate summaries with higher readability but also its generated summaries were composed of natural sentences rather than just words that can be owing to the utilization of the combination of CNN and LSTM in the encoder that enables the model to use n-grams as input.

Finally, we decided to visualize the attention weight ùëéùëñ to prove the efficiency of the proposed model in extracting more informative information from the source text. To this end, a sample text was randomly selected and its weight heat map visualization is depicted in Fig. 6. As it is obvious, important words in the source text were accurately selected which clarifies that auxiliary attention mechanism can successfully determine the significance of each word in the source text. While this key information is obtained before the decoder, the interference of unimportant information to the decoder can be efficiently reduced. Accordingly, the generated summaries include more important information and have higher saliency.

Fig. 6
figure 6
Weight heat map illustration

Full size image

Discussion
Attention mechanism is commonly applied on abstractive summarization models to extract important information along with summary generation. However, the proposed model leverages the attention mechanism in the encoder to obtain principal information while the source text is encoded. It is also able to take phrases rather than words as input which makes it capable of considering both semantics and syntactic at the same time. Results of the experiment (Sects. 4, 5) revealed that not only the proposed model has higher efficiency based on the ROUGE metric compared to other existing models but also it can decrease the interference of irrelevant information which leads to the generation of summaries that are more accurate and salient. Considering the readability evaluation, it was also observed that the summaries generated by the proposed model have higher readability.

However, owing to the fact that the main target of abstractive summarization is generating summaries with higher saliency besides generating more novel n-grams that are available in the reference summary, we carried out statistical analysis to explore the abstractive ability of the proposed model. To this end, we measured the percentage of new n-gram of the proposed model generated summaries in comparison to the gold summaries and the summaries generated by the model of See et al. [37] on both CNN/Daily Mail and DUC2004 datasets. The results are illustrated in Fig. 7 where a larger percentage indicates stronger abstraction.

Fig. 7
figure 7
Percentage of new n-grams on CNN/Daily Mail and DUC2004 dataset

Full size image

As it can be seen, although the number of the new n-grams generated by the proposed model is not equal to the reference summary, it is remarkably higher compared to the model of See et al. [37] which proves the efficiency of the proposed model that can be related to the utilization of CNN and LSTM in the encoder which makes the encoder able to take phrases as input. It is worth mentioning that the lower number of new n-grams generated by the proposed model indicates that some of the contents in the generated summary were directly taken from the source text. However, based on the human evaluation, the saliency of the proposed model generated summaries was confirmed. Considering the DUC-2004, it is clear that the number of new generated n-gram was generally less than CNN/Daily Mail which can be due to the short length of summaries in this dataset. In fact, the process of generating summaries was already over in this dataset while the proposed model massively started to generate new n-grams.

Moreover, to measure the abstractive ability of the proposed model, the probity of generating new words (ùëÉgen‚àà[0,1])  was calculated at time step ùë° from the context vector  ‚Ñé‚àóùë°, decoder state ùë†ùë°, and decoder input ùë•ùë° using the following equation, where ùë§ùëá‚Ñé‚àó, ùë§ùëáùë†, ùë§ùëáùë•, and ùëèptr are learnable parameters and ùúé is the sigmoid function.


ùëÉgen=ùúé(ùë§ùëá‚Ñé‚àó ‚Ñé‚àóùë°+ùë§ùëáùë†ùë†ùë°+ùë§ùëáùë•ùë•ùë°+ùëèptr)

(19)

To this end, ùëÉgen value was recorded at the beginning and end of the training process. It was found that the ùëÉgen value was equal to 0.21 at the beginning and then increased and converged to 0.59 at the end of the training process. It clarifies that the model could only copy the words from the source text at initial training steps and then learned to generate. Noteworthy, ùëÉgen value was very low during the test process (only 0.13) which shows that the model only received word-by-word supervision from the reference summary during the training step and it is not performed along with the test process. Finding a solution for this problem without influencing the efficiency of the proposed model can be considered as possible future work. Reinforcement learning can be also utilized along with the training process to improve performance.

Additionally, although it was observed that the proposed model is able to extract key information in the encoder (Fig. 6), it is obvious that it was not able to filter all irrelevant information correctly. To overcome this issue, hard attention mechanism can be also applied in the decoder. However, it must be taken into consideration that applying hard attention may lead to the loss of valuable information.

Considering the fact that the training time of deep neural networks is highly related to the hardware that they are implemented on, namely modern GPUs can significantly reduce the training time, it cannot be considered as a fair measure for comparing the efficiency of deep learning-based models and it has been rarely explored as a metric for evaluation. As a matter of fact, it is necessary to mention that choosing an optimal model for abstractive summarization is not possible while the definition of "optimal" is not well-defined and there is always a tradeoff between the model complexity (training and test speed) and its performance. However, considering the fact that one CNN and three LSTMs were adopted in the proposed model, its training speed was a bit slow. To fill this lacuna, various tricks like discarding source documents with the length of over 500 words were utilized. Moreover, the maximum length of input text was set to 250 for CNN/Daily mail dataset in the first steps of the training process and it was then changed to 400 and 100 while the model began to converge, which efficiently increased the training speed.

However, in order to provide an analysis of the time complexity of our proposed method, we decided to plot the learning curves corresponding to ROUGE-L of the proposed model compared to the model of See et al. [37] on CNN\Daily mail and DUC 2004 dataset as illustrated in Fig. 8. As it is obvious, not only our proposed model has higher ROUGE-L value per epochs but also it obtained the maximum accuracy only after about 20 epochs on both datasets, and therefore it was converged faster compared to the model of See et al. [37].

Fig. 8
figure 8
Learning curves of the proposed model and the model of See et al. [37]. in term of ROUGE-L per epochs

Full size image

Another set of experiments were also carried out to investigate the influence of the size of the dataset on the efficiency of the proposed model. In this regard, 150,000, 200,000, and 250,000 samples of the CNN\Daily mail dataset were randomly chosen as the training set while the test set was considered unchanged. Thereafter, the proposed model and the model of See et al. [37] were individually trained on the chosen training sets. The obtained results are reported in Table 6. As it is obvious, by enhancing the number of training samples, the ROUGE-L value is enhanced which not only indicates the importance of the dataset size but also proves that the proposed model is able to efficiently employ complementarity of heterogeneous features to increase the overall summarization efficiency.

Table 6 ROUGE-L values on various randomly chosen training samples
Full size table

Briefly, despite the potential of the proposed model in generating summaries, it also suffers from the mentioned weaknesses. However, considering the results of experiments and considering the evaluations conducted by anonymous experts, it was confirmed that its generated summaries have higher saliency and readability, and overcoming the mentioned issues can be considered as interesting future studies.

Conclusion
In this paper, a novel abstractive text summarization model is proposed that takes advantage of the combination of convolutional neural network and long short-term memory network with pre-trained word vectors in encoder besides the auxiliary attention to overcome the existing key problems as well as increasing the saliency and coherency of the generated summaries. In this regard, the proposed model utilized the combination of convolutional neural network and long short-term memory to enable the model to take n-grams as input rather than text. The auxiliary attention consisting of the multi-layer perceptron and similarity module is also employed to obtain principal information of the source text by reducing the interference of irrelevant information in the encoder and before the decoder. Therefore, the decoder can generate summaries just based on important information rather than the entire input.

The proposed model was validated on CNN\Daily Mail and DUC-2004 datasets and the empirical results proved that the proposed model outperformed the state of the art approaches on both datasets in terms of ROUGE metric. Due to the fact that the ROUGE metric can only calculate the repeatability of n-gram and does not analyze the quality of a generated summaries, we also performed human evaluation to examine the saliency and readability of the proposed model. According to the analysis and obtained results, the proposed model has also higher saliency and readability compared to the baseline model.

However, it must be mentioned that despite the superior performance of the proposed model, its inability in providing word to word supervision in both training and test steps, which is the main purpose of abstractive text summarization, besides its inefficiency in filtering all irrelevant information can be considered as its limitations. However, employing reinforcement learning with hard attention can be considered as possible future research to overcome these issues and enhance performance. Leveraging other representation techniques, like BERT and RoBERTa, to provide significant features as the input of the encoder is also worth exploring.

