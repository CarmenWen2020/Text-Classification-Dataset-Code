Abstract
The rumors, advertisements and malicious links are spread in social networks by social spammers, which affect users’ normal access to social networks and cause security problems. Most methods aim to detect social spammers by various features, such as content features, behavior features and relationship graph features, which rely on a large-scale labeled data. However, labeled data are lacking for training in real world, and manual annotating is time-consuming and labor-intensive. To solve this problem, we propose a novel method which combines active learning algorithm with co-training algorithm to make full use of unlabeled data. In co-training, user features are divided into two views without overlap. Classifiers are trained iteratively with labeled instances and the most confident unlabeled instances with pseudo-labels. In active learning, the most representative and uncertain instances are selected and annotated with real labels to extend labeled dataset. Experimental results on the Twitter and Apontador datasets show that our method can effectively detect social spammers in the case of limited labeled data.

Introduction
Online social networks are beneficial for news propagation and interpersonal communication and have gradually become an essential tool in people’s daily life [1, 2]. However, with its influence increasing, online social network has overrun with social spammers [3]. Social spammers refer to a mass of accounts operated by software robots or human. For political or economic purpose [4], they publish vast quantities of false information or follow other users. By manipulating topics or increasing user influence, their goal is to influence online public opinion. Besides, social spammers may also push advertisements, implement phishing and spread malicious software, which are potential risks to information security. Moreover, to evade social network’s defensive system, social spammers adopted various strategies to disguise, such as copying normal user’s profile, using algorithms to generate original content and simulating the publishing time mode of human. All of these make it more difficult to detect social spammers.

Although there are challenges in social spammer detection, many researchers have proposed solutions. Supervised detection methods need massive labeled instances, including normal users and social spammers, to classify unlabeled instances. These methods aim to extract features that can effectively distinguish the users, such as relationship graph features [5], profile features [6, 7], behavior features [8], content features [9, 10] and multiple comprehensive features [11]. However, there are abundant unlabeled users on social network, but the number of labeled training users is limited. Moreover, manually annotating a large number of users is a complex and error-prone task, which requires a lot of human efforts and time. Thus, it is difficult to implement. Unsupervised detection methods often have lower accuracy and higher false positive rates. And their robustness is poor in the presence of noise [12].

To solve the problems of the above methods, we propose a combination of semi-supervised learning algorithm (SSL) and active learning algorithm (AL) to detect social spammers. We use co-training algorithm which is one of the semi-supervised learning algorithms. Semi-supervised learning algorithm trains the classifier with the help of unlabeled data, and the final classifier is better than that of training with only labeled data. In this way, it solves the problem of supervised detection methods. However, [13] shows that although the semi-supervised learning algorithm selects the most confident instances, it is still possible to get false pseudo-labels. Eventually, the accumulation of these noises will cause performance degradation of the classifier. Therefore, the active learning algorithm is presented to deal with the noise accumulation of semi-supervised learning. Active learning selects informative unlabeled instances through some sampling strategies and gets their real labels through queries. These newly labeled instances are then added into the training set. Classifiers are retrained with the training set and thus improve the performance [14]. In this paper, we combine active learning in two parts of semi-supervised learning: firstly, selecting the initial labeled dataset for semi-supervised learning and secondly selecting informative instances to update labeled dataset of semi-supervised learning iteratively.

The main contributions of this paper are as follows:

First, we propose a novel active co-training social spammer detection method, which improves social spammer detection performance by using both high confidence unlabeled instances and high information unlabeled instances. To the best of our knowledge, this is the first study to propose a combination of semi-supervised learning and active learning for social spammer detection.

Second, the experimental results on the real-world Twitter dataset and Apontador dataset show that our method can effectively detect social spammers with a small number of labeled data.

The rest of this paper is organized as follows. Related work is reviewed in Sect. 2. In Sect. 3, the proposed detection framework is presented. Section 4 introduces the experimental results and evaluation. Section 5 concludes this paper and plans future work.

Related work
For social spammer detection, the major methods can be divided into three types: supervised detection methods, unsupervised detection methods and semi-supervised detection methods.

(1)
Supervised detection methods

Supervised detection methods train the classifier by extracting various features of social network users. Benevenuto et al. [15] detected Youtube spammers based on profile features and video features. With the evolution of spammer strategy, Cheng et al. [5] discovered that users’ relationship was difficult to change and proposed a spammer detection method based on relationship graph features. Chen et al. [8] proposed an information diffusion model based on social spammers’ behavior features. And the commonality in interactive behaviors of various spammers makes this method general. Zhang et al. [6] combined the Bayesian model and genetic algorithm for the ever-changing spammers and used a threshold optimization model to detect social spammers more accurately. Shen et al. [11] integrated multiple types of features, using isomorphism theory to complement data, and then used non-negative matrix factorization (NMF) to calculate the consensus matrix of users’ features for spammer detection. Supervised detection methods require plenty of labeled instances to train the classifier. However, in the real world, there are few labeled instances and plenty of unlabeled instances. It is not easy to implement the task of manual annotating.

(2)
Unsupervised detection methods

Unsupervised detection methods mainly use the topological relation of social network to classify users. Ye et al. [16] used a typical spammer account as a seed to expand the fan relationship layer by layer. Based on the high aggregation of spammer relationships, they used a community detection algorithm to discover spammer groups. Li et al. [17] used directed links between social network users to construct a relationship matrix and calculate the PageRank value to detect social spammers. Tan et al. [18] designed an unsupervised spammer detection method that combines social relationship graphs and user link graphs. This method first determined some normal users through social relationship graphs and then identified spammers based on the link graphs of these users. Wang et al. [19] used clickstream to model user behavior and then used similarity graphs to cluster users. This graph clustering-based method needed to know the number of users in advance. Compared with supervised detection methods, although unsupervised detection methods do not require any labeled instances, they have a high false positive rate and are not as robust as supervised detection methods [12].

(3)
Semi-supervised detection methods

Semi-supervised detection methods use labeled data to train an initial model and then select the most confident unlabeled instances according to the current model. These instances are annotated with pseudo-labels and added to the labeled dataset. The model enhances its performance by retraining with the updated labeled dataset iteratively. Li et al. [12] proposed a social spammer detection framework which combined semi-supervised learning algorithm with trust propagation algorithm. It calculated all unlabeled users’ scores on the social graph and selected confident users as the new training data. But it would be complicated when the network topology is large. Chen et al. [20] proposed a semi-supervised clue fusion model, which fused the clues by a linear weighted summation function to detect spammers in Sina Weibo by using a small size of labeled training instances. Wu et al. [21] proposed a semi-supervised collaborative learning approach for social spammer and spam message detection in Sina Weibo, using the inherent correlation between them to improve detection performance.

In summary, Li et al. [12] proposed a novel semi-supervised social spammer detection method based on trust propagation algorithm. However, the trust propagation algorithm is not suitable for large-scale network topologies. Therefore, the following researches adopted methods of combining semi-supervised learning algorithm with user features. But there is still the problem of noise accumulation in semi-supervised detection methods. To tackle this problem, we propose a novel social spammer detection method, which further improves the performance of social spammer detection by combining active learning algorithm with semi-supervised learning algorithm.

Active co-training social spammer detection method
We propose the Active Co-training Social Spammer Detection (ACTSSD) method, and its framework is shown in Fig. 1. There are three key parts of the proposed framework, including co-training algorithm, seed selecting and labeled dataset updating. Co-training algorithm is shown in the blue box. Active learning is used in both seed selecting and labeled dataset updating. It is combined in following two aspects: first, before the co-training iteration, using the seed selection algorithm to create the initial labeled dataset L, and second, during the co-training iteration process, using the two-layer active learning sampling strategies to update the labeled dataset L by choosing high informative instances. In this section, we first briefly describe the active learning process and then introduce the three key parts of ACTSSD in turn.

Fig. 1
figure 1
Framework of ACTSSD

Full size image

Active learning process
In addition to semi-supervised learning, active learning is another method that uses a small number of labeled instances and a large number of unlabeled instances to build the high-performance classifier. The main difference between the two is as follows. Semi-supervised learning automatically assigns pseudo-labels to confident unlabeled instances, while active learning selects informative unlabeled instances and submits them to supervisors for annotation. Because these two methods have inherent similarities and complement each other, their combination can select instances with different confidence levels. As a result, the performance of classifier is maximized.

The active learning algorithm is modeled by the following five components [22], as shown in formula (1).

From formula (1), C is one or a group of classifiers; Q is a query strategy that is used to select unlabeled informative instances from U, such as uncertainty sampling; S is a supervisor who can annotate unlabeled instances with real labels; L is the labeled dataset for training; U is the unlabeled dataset. At the start of active learning iteration, the initial classifier may begin with a small number of labeled data in L. The process of active learning is shown in Fig. 2.

Fig. 2
figure 2
Framework of ACTSSD

Full size image

Symbol definition
For the convenience of expression, some symbols and their meanings are listed in Table 1. Users on network are expressed as feature vectors in the form 
, where 
 is the feature vector of user i and n is the total number of users. 
 is the labeled dataset, where 
 is the user’s label (+1 denotes the social spammer, -1 denotes the normal user). 
 is the unlabeled dataset. l is the number of labeled users, and  is the number of unlabeled users. In real situation, . Active learning calculates the uncertainty and representativeness of user. The uncertainty is expressed as H(x), and the representativeness is expressed as I(x). The set of candidate users selected by the first layer of active learning sampling is expressed as 
.

Table 1 Some symbols and their meanings
Full size table

Seed selection algorithm
Original co-training algorithm composes initial labeled dataset L by random selection, whereas ACTSSD algorithm uses a seed selection algorithm to improve the quality of the initial labeled dataset L. The more informative the labeled dataset is, the more effective and robust the co-training classifiers are. Instances by randomly selected may not be representative and diverse, especially when there are only a small number of instances [23].

In order to get high-quality initial labeled dataset, we use the representativeness sampling strategy in active learning expressed as 
 which is a variant of density-weighted methods [22]. First, the unlabeled dataset U is clustered by using the k-means algorithm. Second, information density of instances in each cluster is calculated by formula (2).


 
 
 

(2)

where 
 is the similarity between instances. The sampling strategy 
 hopes to get the most representative instances in each cluster. So we choose Euclidean density, which has a clear geometric meaning. Instances at each cluster center have the largest Euclidean density. Correspondingly, we use Euclidean similarity to calculate Euclidean density, as shown in formula (3).

Here d is the Euclidean distance, as shown in formula (4).

By sorting the information density of instances in each cluster in descending order and selecting the top instances in proportion, the most representative instances are obtained. These representative instances form the seed set 
.

The seed selection algorithm is described in Algorithm 1.

figure a
Labeled dataset update process
The ACTSSD framework has two modules that need to be updated with the labeled data: active learning process and co-training process. First of all, the seed set 
 is obtained by Algorithm 1.

In each iteration, two kinds of unlabeled instances will be selected to update the labeled dataset. Active learning algorithm selects informative instances and manually annotates them with real labels; co-training algorithm selects confident instances and automatically annotates them with pseudo-labels.

(1)
Updating labeled dataset by active learning

In each iteration, a few instances selected by active learning algorithm are added into labeled dataset L. Because these informative instances have real labels, they can reduce the noise of co-training and further improve the performance of classifiers. The ACTSSD framework uses a two-layer active learning sampling strategy to select the informative instances based on representativeness and uncertainty.

The first layer uses the representativeness sampling strategy 
 described in Sect. 3.3. Unlabeled instances are sorted according to information density I(x), and Top-M unlabeled instances are selected as candidate instances for the second layer sampling, as shown in Formula (5).


 
 

(5)

The second layer uses the uncertainty sampling strategy 
. The initial classifier is trained by 
. In each iteration, the classifier is retrained with labeled data updated by active learning. Uncertainty sampling uses a probabilistic classifier to directly estimate the posterior probability of unlabeled instances and select instances that are most difficult to be distinguished by classifier. Information entropy is the most commonly used method to measure the uncertainty of an instance. It considers the model’s judgment results for all categories of instance x, as shown in Formula (6).


 
 

(6)

where 
 denotes the predicted label of instance x and 
 denotes the probability that instance x is predicted to be a specific label. The second layer selects instances with the highest uncertainty from 
. More specifically, instances with the largest entropy are selected. Then, they are manually annotated to form 
 which are used to update the labeled dataset L.

(2)
Updating labeled dataset by co-training

The co-training algorithm divides user features into two sufficient and redundant views [24], that is, each of which is sufficient for learning and conditionally independent of the other (presented in Sect. 3.5). Two initial classifiers are trained by 
 on two different views. In each iteration, each classifier calculates the confidence of unlabeled instances in 
 and selects instances with the highest confidence to annotate with pseudo-labels. Then, these instances are added into the training set of another classifier. The classifiers will be retrained with the updated labeled dataset iteratively.

Co-training
On social network, user features can be divided into two sufficient and redundant attribute sets: behavior attributes and content attributes, which just fit the requirements of co-training. Compared with self-training algorithm that only uses single classifier, co-training algorithm contains two classifiers which can enhance each other. It has been proved that disagreement-based semi-supervised learning can effectively reduce the bias generated by self-training [24], such as co-training and tri-training. Nevertheless, the tri-training algorithm uses an implicit confidence measurement, which has limitations combining with the active learning algorithm.

User features
Inspired by [23, 25], we consider four categories of user features: profile features, behavior features, neighbor features and content features, defined as follows:

Profile features Profile features are extracted directly from user’s profile, usually including the number of followees, the number of followers, the ratio of followers/followees, user screen name, account age, etc.

Behavior features Behavior features measure the user’s active level on social network, usually including the number of posts, the frequency of posts, the number of reposts, the number of comments, the number of replies, etc.

Neighbor features Neighbor features are extracted from neighbor users, usually including the number of neighbor users’ followees, the number of neighbor users’ followers, the number of neighbor users’ posts, the number of neighbor users’ comments, etc.

Content features Content features called text features are extracted from the text published by users, usually including the number of words, the number of URLs, the number of spam words, the number of special characters, the number of topics, etc.

The first three features belong to behavior attributes, and the last one belongs to content attributes [23]. After dividing user features according to these two attribute sets, each user can be independently described by a feature vector in behavior attributes or content attributes.

Co-training in ACTSSD
The framework of co-training algorithm in ACTSSD is shown in Fig. 3. Different from original co-training algorithm, it uses additional labeled users 
 selected by active learning in each iteration. User features are divided into two views 
 and 
 according to behavior attributes and content attributes in Sect. 3.5.1. The initial labeled dataset L is obtained by the seed selection algorithm in Sect. 3.3.

Fig. 3
figure 3
Framework of co-training algorithm in ACTSSD

Full size image

 is obtained by separating the features according to 
. In each view, classifier 
 is trained with labeled dataset 
. The buffer pool 
 is composed of some instances randomly selected from unlabeled dataset U. From 
, each of the two classifiers 
 selects 
 positive instances and 
 negative instances whose classification confidence are higher than the threshold t. All of these instances are added into another classifier’s training set with pseudo-labels. At last, some instances are taken from U to replenish 
. Moreover, in each iteration, the labeled instances 
 selected by active learning algorithm are also used to update both the labeled datasets 
 and 
. So far, one iteration is completed. In the next iteration, the classifier 
 will be retrained with the updated labeled dataset 
. If the two classifiers have not been changed or reach the preset number of iterations, the training will be stopped. The algorithm of co-training in ACTSSD is given in Algorithm 2.

figure b
Experiments and evaluation
In this section, we conduct two parts of experiments: comparative experiments and parameter evaluation experiments. There are four comparative experiments. Firstly, the ACTSSD algorithm is compared with supervised learning algorithms to verify whether ACTSSD can achieve similar effects to supervised learning algorithms with only a small number of labeled instances. Secondly, the ACTSSD algorithm is compared with the original co-training algorithm to verify the effectiveness of our method which combines co-training with active learning. Thirdly, compare the ACTSSD algorithm with the detection method based on active learning in [26] to verify whether the ACTSSD algorithm performs better than the active learning algorithm alone. Fourthly, compare the ACTSSD algorithm with other social spammer detection method to verify whether the ACTSSD algorithm has better performance. The parameter evaluation experiments analyze the parameter sensitivity of ACTSSD. According to the effects of parameters on the experimental results, the optimal value of each parameter is given.

Experiments
Dataset
In this paper, we use the Twitter dataset [27] and Apontador dataset [28] provided by Benevenuto. Considering the audience of the two social platforms, we conduct all the experiences on Twitter dataset. In addition, to verify the performance of ACTSSD, we also conduct two comparative experiments on Apontador dataset in Sects. 4.2.1 and 4.2.2.

Twitter dataset contains 1065 labeled users, including 710 normal users and 355 social spammers. There are 26 different features and 62 fields. Some numeric-related features extend their maximum, minimum, mean and median to form the total 62 fields. All the features are divided into behavior attribute sets and content attribute sets. The user features of Twitter dataset are listed in Table 2.

Table 2 User features of Twitter dataset
Full size table

Apontador is a popular location based social network system in Brazil. Apontador dataset contains 7076 labeled data, including 3538 spams and 3538 non-spams. Among them, spam tips are published by spammers, which aim at advertising unsolicited messages on tips about locations. There are 49 different features and 60 fields. Some numeric-related features extend their maximum, minimum, mean, median and standard deviation to form the total 60 fields. All the features are divided into behavior attribute sets and content attribute sets. The user features of Apontador dataset are listed in Table 3.

Table 3 User features of Apontador dataset
Full size table

Evaluation measures
We adopt precision, recall and F1-measure as the evaluation measures. These evaluation measures can be calculated by formulas (7)–(9). All the above measures are mainly for spammers.


 
 

(9)

Among them, TP denotes the number of users who are correctly predicted as social spammers, FP denotes the number of normal users who are incorrectly predicted as social spammers, TN denotes the number of users who are correctly predicted as normal users, and FN denotes the number of social spammers who are incorrectly predicted as normal users.

Experimental setup
All the following experiments were implemented in Python with scikit-learn in the current work. The configuration of the underlying hardware used for the experiment is Intel(R) Xeon(R) CPU E3-1230 V2 @ 3.30 GHz and 8 GB RAM, running on Microsoft Windows 10 Professional Edition Operating System.

In the experiments, we choose 7 popular classification algorithms, including Random Forest (RF), Logistic Regression (LR), Decision Tree (DT), Naive Bayes (NB), Support Vector Machine (SVM), Gradient Boosting Decision Tree (GBDT), K Nearest Neighbor (KNN). For fair comparison, we use a common subset for testing. The size of this public testing set is 20% of all the data. The training set sizes of these algorithms are as follows. For ACTSSD algorithm and original co-training algorithm, we further divided the training set into the labeled dataset L and unlabeled dataset U. For supervised learning algorithms, the remaining 80% of data is all used for training in Twitter dataset, while the size of training set is equal to the size of L in Apontador dataset. It is important to note that each group of experiments was independently repeated 50 times, and the final results were averaged.

Experimental results and discussion
Comparison of ACTSSD and supervised learning algorithms
In this experiment, the labeled dataset L of the ACTSSD algorithm is obtained by seed selection algorithm in Sect. 3.3.

In Twitter dataset, the size of L is set to 50, and the iteration number of ACTSSD algorithm is 30. In each iteration, active learning selects one informative instance and adds into L. The experimental results are shown in Figs. 4, 5 and 6.

Fig. 4
figure 4
Precision of classification algorithms on Twitter dataset

Full size image

Fig. 5
figure 5
Recall of classification algorithms on Twitter dataset

Full size image

Fig. 6
figure 6
F1-measure of classification algorithms on Twitter dataset

Full size image

Looking at Fig. 4, it is apparent that among all of 7 models, the precision of ACTSSD algorithm is close to or higher than 90%, and higher than the supervised learning algorithms. Figure 5 shows that under most classification models, the recall of ACTSSD algorithm is higher than that of supervised learning algorithms. For example, the recall of ACTSSD under LR is 48% higher than that of the supervised learning algorithm. As shown in Fig. 6, under each classification model, the F1-measure of ACTSSD algorithm is higher than that of the supervised learning algorithms, and the F1-measure exceeds 80%.

Observing the experimental results, ACTSSD algorithm performs almost as same as supervised learning algorithms, or even better. In the training process, ACTSSD algorithm only uses a small number of labeled instances, while supervised learning algorithms need a large number of labeled instances. It shows that ACTSSD algorithm can effectively detect spammers in the case of limited labeled instances. Because ACTSSD makes full use of the high-confidence instances selected by co-training and the high-information instances selected by active learning, classifiers’ performance is continuously enhanced in the iterative process.

In Apontador dataset, the size of L is set to 400, and the iteration number of ACTSSD algorithm is 30. In each iteration, active learning selects 10 informative instances and adds them into L. The experimental results are shown in Figs. 7, 8 and 9.

Fig. 7
figure 7
Precision of classification algorithms on Apontador dataset

Full size image

Fig. 8
figure 8
Recall of classification algorithms on Apontador dataset

Full size image

Fig. 9
figure 9
F1-measure of classification algorithms on Apontador dataset

Full size image

From Fig. 7, we can see that among all of 7 models, the precision of ACTSSD algorithm is higher than that of supervised learning algorithms. For example, the precision of ACTSSD under DT is 14% higher than that of the supervised learning algorithm. As shown in Fig. 8, among all of 7 models, the recall of ACTSSD algorithm is higher than that of supervised learning algorithms. Figure 9 shows that under most classification models, the F1-measure of ACTSSD algorithm is close to or higher than 80%, and higher than the supervised learning algorithms.

Observing the experimental results, ACTSSD algorithm performs better than supervised learning algorithms. Although they have the same size training set, ACTSSD contains a part of instances with pseudo-labels, which are annotated automatically by its co-training part. Actually, ACTSSD uses fewer labeled instances but performs better than supervised algorithms.

In summary, the experimental results on two social platforms show that the classification performance of ACTSSD algorithm is comparable to that of supervised learning algorithms. ACTSSD algorithm can effectively detect social spammers in the lack of labeled instances.

Comparison of ACTSSD and original co-training algorithm
In this experiment, the labeled dataset L of ACTSSD algorithm is obtained by seed selection algorithm in Sect. 3.3, and the labeled dataset L of original co-training algorithm is obtained by random selection. In addition to the difference in the initial labeled dataset L, another difference is that ACTSSD algorithm gets some extra labeled instances selected by active learning in each iteration. This experiment is to verify the effectiveness of ACTSSD algorithm which combined active learning with co-training algorithm.

In Twitter dataset, for both ACTSSD algorithm and original co-training algorithm, the size of L of is set to 50 and the iteration number is 30. In each iteration of ACTSSD, active learning selects one informative instance and adds into L. The experimental results are shown in Figs. 10, 11 and 12.

Fig. 10
figure 10
Precision of classification algorithms on Twitter dataset

Full size image

Fig. 11
figure 11
Recall of classification algorithms on Twitter dataset

Full size image

Fig. 12
figure 12
F1-measure of classification algorithms on Twitter dataset

Full size image

As shown in Fig. 10, the precision of ACTSSD algorithm is close to or higher than the original co-training algorithm. From Fig. 11, we can see that among all of 7 models, the recall of ACTSSD algorithm is higher than that of the original co-training algorithm, with an average increase of 14%. Figure 12 shows that under each classification model, the F1-measure of ACTSSD algorithm is higher than that of the original co-training algorithm and exceeds 80%. Compared with the original co-training algorithm, ACTSSD algorithm has similar precision and greatly improved recall, which improves the detection performance of social spammers.

Observing the experimental results, it can be seen that the original co-training algorithm may perform well in terms of precision. Under all the classification models, the recall of the original co-training algorithm is less than 70%. This is because the normal users in this Twitter dataset are twice as many as the spammers. And the original co-training created the initial labeled dataset L by random selection. Since there are more normal users, the original co-training can classify normal users well. Accordingly, it contains fewer spammers that are not representative. It leads to poor performance of detecting spammers and low recall. ACTSSD algorithm greatly improves the recall by selecting instances with high representativeness and uncertainty, thereby increasing the F1-measure.

In Apontador dataset, for both ACTSSD algorithm and original co-training algorithm, the size of L is set to 400, and the iteration number is 30. In each iteration of ACTSSD, active learning selects 10 informative instances and adds them into L. The experimental results are shown in Figs. 13, 14 and 15.

Fig. 13
figure 13
Precision of classification algorithms on Apontador dataset

Full size image

Fig. 14
figure 14
Recall of classification algorithms on Apontador dataset

Full size image

Fig. 15
figure 15
F1-measure of classification algorithms on Apontador dataset

Full size image

As shown in Fig. 13, the precision of ACTSSD algorithm is higher than the original co-training algorithm. From Fig. 14, we can see that among all 7 models, the recall of ACTSSD algorithm is higher than that of the original co-training algorithm. For example, the recall of ACTSSD under NB is 15% higher than that of the supervised learning algorithm. Figure 15 shows that under each classification model, the F1-measure of ACTSSD algorithm is higher than that of the original co-training algorithm.

Observing the experimental results, it can be seen that the recall of original co-training algorithm no longer shows a significantly lower regularity as in Twitter dataset. That is because the number of spam and non-spam is balanced in Apontador dataset. The experimental results confirm the analysis of Twitter dataset. That is, the low recall of the original co-training algorithm is caused by data imbalance.

In summary, the experimental results on two social platforms show that it is effective to combine active learning algorithms with co-training in ACTSSD.

Comparison of ACTSSD and active learning algorithm
This experiment compares ACTSSD algorithm with the two-layer active learning detection method (DDTLS) in [26], using all four classification models in DDTLS. The results are shown in Table 4, and we bold the higher values of each evaluation measure.

Table 4 Experimental results of ACTSSD and DDTLS (%)
Full size table

Table 4 shows that except for Naive Bayes (NB) model, the precision of ACTSSD under the other three models is increased to over 90%. Under all four models, the recall and F1-measure of the ACTSSD algorithm are higher than those of the DDTLS algorithm.

ACTSSD algorithm uses the random forest (RF) as the basic classification model, and the DDTLS algorithm uses the support vector machine (SVM) which is the best in all four models. The comparison results are shown in Table 5, and we bold the higher values of each evaluation measure.

Table 5 Comparison of ACTSSD and DDTLS (%)
Full size table

The results show that the precision, recall and F1-measure of ACTSSD algorithm are higher than those of DDTLS algorithm, and the precision is increased by 10.48%. It shows that in addition to active learning, the co-training algorithm in the ACTSSD further improves the performance of classifiers by automatically annotating confident instances without increasing the labor and time costs.

In summary, ACTSSD algorithm has better performance than the active learning algorithm.

Comparison of ACTSSD and other detection method
This experiment compares ACTSSD algorithm with the detection method based on Laplacian feature score (LSSL-SSD) in [29]. The basic model is random forest (RF) with 100 base classifiers. We change the labeled dataset size from 10%, 20%, 40% of all the data as [29] does; the comparison results are shown in Table 6, and we bold the higher values of each evaluation measure.

Table 6 Comparison of ACTSSD and LSSL-SSD (%)
Full size table

Table 6 illustrates that under the condition the number of labeled instances accounts for 10%, 20% and 40% of the total instances; the precision, recall and F1-measure of ACTSSD are higher than those of LSSL-SSD. Moreover, when ACTSSD algorithm uses only 10% labeled instances, its performance is better than that of the LSSL-SSD algorithm using 40% labeled instances. This is because active learning in the ACTSSD algorithm selects informative labeled instances, which can greatly reduce manual annotating.

In summary, ACTSSD algorithm performs better than LSSL-SSD algorithm and can better detect social spammers and normal users.

Parameter sensitivity analysis
This part analyzes parameter sensitivity of ACTSSD. The undetermined parameters include the initial L size, the confidence of the semi-supervised iteration and the number of iterations. Random forest (RF) is the basic classification model in this experiment. The single-factor sensitivity analysis is conducted on Twitter dataset. Figures 16, 17 and 18 show the effects of the above three parameters on evaluation measures.

Fig. 16
figure 16
Effect of initial L size

Full size image

Figure 16 describes the effect of initial labeled dataset L on ACTSSD. In the process of increasing the initial L size from 50 to 500, F1-measure is almost unchanged. The small fluctuations in precision and recall are caused by slightly different numbers of normal users and social spammers selected by active learning in each experiment.

This shows that ACTSSD has a strong learning ability. It only needs a few initial labeled instances to get the equivalent performance to that training with a lot of initial labeled instances after iterative training.

Since the initial L size has little effect on the performance of the classifiers, in order to reduce the cost of manual annotation, we select as few initial instances as possible, that is, the initial L size is set to 50.

Fig. 17
figure 17
Effect of iteration confidence

Full size image

Figure 17 shows that as the confidence increases, the precision shows an increasing trend, and the recall shows a decreasing trend. It is important to note that the probability of a user being classified as a normal user or a social spammer by the model is certain. Comparing the probability of a user under the two classifications, if the probability of social spammer is higher, then the user is more likely to be a social spammer, and vice versa. The confidence determines the classification result of a user. When the probability of the user being classified as a social spammer is higher than the confidence, this user is annotated with a pseudo-label of social spammer. If the confidence is set to high, then fewer users are annotated as social spammers. The reason for the trends in Fig. 17 is that as the confidence increases, the possibility of predicting normal users as spammers is decreasing, and the possibility of predicting spammers as normal users is increasing.

The statistical results about pseudo-labels of test set instances from different confidence of classifiers are shown in Table 7. The results are consistent with theoretical analysis, that is, when the confidence increases, the number of unlabeled users predicted as spammers decreases.

Table 7 Pseudo-labels of test set instances from different confidence of classifiers
Full size table

It can be concluded that when the confidence is 0.8, the F1-measure takes the maximum value. Therefore, we set the confidence of classifiers in co-training iteration to 0.8.

Fig. 18
figure 18
Effect of iteration numbers

Full size image

Figure 18 shows that when the number of iterations is less than 30, the precision, recall and F1-measure monotonically increase with the number of iterations. When the number of iterations is bigger than 30, these three evaluation measures tend to be stable. This is because, at the beginning of the iteration, ACTSSD makes full use of the information of unlabeled instances to improve the performance of classifiers. After the number of iterations exceeds a certain value, the added instances have little effect on improving the performance. In this experiment, the number of iteration is set to 30, because the performance of classifiers can be fully enhanced without more unnecessary iterations.

Conclusion
In this paper, we proposed a novel active co-training social spammer detection method (ACTSSD), in which co-training is an algorithm of semi-supervised learning. Co-training annotates confident unlabeled instances with pseudo-labels. And active learning uses a two-layer sampling strategy based on representativeness and uncertainty to select informative instances for manual annotating. In each iteration of ACTSSD, newly labeled instances generated by the above two algorithms are to update the training set. The classifiers will be retrained iteratively, thereby improving the performance. Co-training solves the problem of limited labeled users in the real world; active learning deals with noise accumulation in the iterative process of co-training. The experimental results on the real Twitter dataset show that ACTSSD requires only a few labeled data to detect spammers effectively as supervised detection methods do with plenty of labeled data. Moreover, ACTSSD has better classification performance than separate co-training method or active learning method.

However, there are further improvements in this article. (1) In the experiment, we found that the precision of ACTSSD algorithm is sometimes slightly lower than original co-training algorithm. The possible reason is that the number of representative instances selected by the ACTSSD algorithm is difficult to preset in advance. Too few instances cannot achieve the overall coverage effect of the unlabeled dataset, thus providing insufficient information to the training set. While too many instances will weaken the utility of representative sampling and increase the cost of manual annotating. Therefore, in future research, the representative sampling of instances should be optimized to further improve classification performance. (2) This paper focuses on offline social spammer detection and does not realize spammers’ dynamic detection. In response to this problem, the follow-up research will use dynamic features of spammers and choose suitable algorithms to achieve online social spammer detection.