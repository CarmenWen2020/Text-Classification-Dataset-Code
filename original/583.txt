Abstract
This is a short communication where we present a theoretical model of a swarm of wireless robots that can be used for cellular-level diagnosis and treatment of a variety of life threatening diseases such as cancer. Based on this model, we illustrate a distributed position and orientation tracking algorithm that constructs digitized images from a set of pixels transmitted by the robots of the swarm model that are in motion. Simulation results are also presented.

Introduction
As technology continues to exert increasing influence in the medical field, two important goals in the treatment of cancer are becoming more attainable: accurate detection during cancer's early stages of development; and targeted drug delivery that would apply appropriate medication to a precise location. However, there are many challenges toward reaching those goals.

Current major cancer treatments are untargeted, e.g., chemotherapy, which contains toxins that could potentially cause life-threatening or distressing side effects, including febrile neutropenia, infections, mucositis, nausea, vomiting, and fatigue [2,3,4,5]. One of the most notable contributions to the field of targeted drug delivery is the recent work of Prof. Khizroev and his team [6]. They have developed externally controlled magneto-electric nanoparticles (MEN) that are used as carriers of drugs to cancer cells. The MENs distinguish cancer cells from the surrounding normal cells via application of a D.C. magnetic field in a specific strength range tailored to the cancer type. While this revolutionary technique is currently being tested, accurate detection of cancer cells at an early stage still remains a major challenge.

Detection techniques are generally classified into two categories: invasive and non-invasive.

In the area of invasive detection, endoscopy is a widely used technique that involves the insertion of a scope directly into the body to obtain a conventional image directly from the interior of an organ or vessel. It can also take samples when it observes an irregularity (a process known as biopsy). Tissue sampling continues to be the foremost evidence for diagnosis. A more advanced version, capsule endoscopy, allows obtaining direct conventional images within the complex vessels inside the body via capsules that contain cameras [7]. Although this technique presents reliable evidence for doctors’ diagnoses, the size of the camera system still limits its usage mostly to the digestive system area.

Imaging detection techniques fall in the non-invasive detection category, and include mammogram, computer-assisted tomography (CT) and magnetic resonance imaging (MRI). Unfortunately though, they all require the abnormality to be developed to at least the millimeter scale to be detectable. While that may seem like a small scale, it actually is not small enough in application; thus, the false-negative rate of major cancer detection techniques, such as mammograms for breast cancer, is as high as 20% [1]. Also, a CT scan can only detect lung cancer (a leading cause of cancer-related deaths per year) when it has a minimum size of 2 mm in diameter [8]. Even if it reaches that minimum perceptible size, the misdiagnosis rate is considered so unacceptably high that a laboratory test is always required to determine the final result of a diagnosis.

The millimeter limitation imposed by the above imaging techniques is due to the fact they operate by finding tissue density differences and are, therefore, not effective at revealing early-stage cancer cells that have identical density as normal cells. For example, CT constructs images by detecting differences in densities of tissues by their discrepancy in X-ray absorption rates. Furthermore, tomography delivers a very high dose of radiation to the patient, which itself has the potential of causing cancer and, therefore, is not recommended for repetitive use. Lower-dose radiation detection methods, such as mammograms, are highly influenced by the density of tissues, and that could generate false results at an even higher rate [1].

A fairly new technology, Pulsed Magneto-Motive Ultrasound (pMMUS), utilizes magnetic nanoparticles  to label cells and then obtain an image through B scan [9]. It is a promising technology with a resolution as high as 100 nm. However, the image it obtains is still a density map.

Another recent advancement is Optical Coherence Tomography (OCT), which utilizes light interferometry in microscale to retrieve structural data of the tissue of interest. Light, though, has a maximum penetration depth. Therefore, this imaging technology can only penetrate into tissues with a depth of 500 µm with a resolution of 10 µm [10, 11], thus limiting its usage to skin disease detection only. No work has been done for a body-wide, real-time, cellular-level imaging system before, yet the early detection of cancer eagerly awaits such a technique.

In this short communication, we present a theoretical model of a swarm of wireless robots that can be used for cellular-level diagnosis and treatment of a variety of life-threatening diseases such as cancer. Based on this model, we present a distributed position/orientation-tracking algorithm that constructs images from a set of pixels transmitted by moving robots.

The rest of the paper is organized as follows: In Sect. 2 we first introduce the theoretical architectural design of the multifunctional robots used in our proposed Swarm model of computation. Next, in Sect. 3, we present an imaging technique that combines a specialized localization method of robots with the image reconstruction method using a 3D reference model. Section 4 shows the simulation results produced by taking images from different parts of the human body and comparing images with the 3D reference model. Section 5 describes a method for auto detection of abnormalities. The conclusion and possible future expansions of the research are demonstrated in Sect. 6.

Swarm of robots computational model
In this section, we first introduce a Swarm of Robots Computation Model (SoRCM). Next, the theoretical architectural design of the multifunctional robots used in this model is presented. The imaging methodology presented in the next section is based on this model. A conceptual depiction of SoRCM is shown in Fig. 1A.

Fig. 1
figure 1
A Swarm of Robots Computational Model (SoRCM). B Structure of each of the Robots of SoRCM

Full size image

As shown in Fig. 1A, the SoRCM model represents a swarm of wireless robots that are monitored and controlled remotely via a computer. While SoRCM is an abstract model of computation, the following could serve as one possible early-stage design of each of the robots in the swarm. The design is made possible by currently available microscale and nanoscale robotics technologies. Clearly, the design can be improved as more advances are made in the field. Each robot is capable of imaging, sampling, and targeted drug delivery. The architecture of each robot is divided in several parts, as shown in Fig. 1B and as described below:

A.
PFCA Camera or Sensor

B.
Accelerometer

C.
Gravity Sensor

D.
Micro Grabber

E.
Iron–Cobalt Bead

F.
Bio-degradable Polymer Sac

Parts A, B, and C are the main components dedicated for our imaging methodology. The PFCA Camera or Sensor will capture discrete data. That, combined with the orientation data obtained from the Accelerometer and Gravity sensor, forms a pixel vector further used in the construction of a 3D image. The method for producing this image will be illustrated in detail in Section III.

Parts D, E, and F are used to elevate the robot’s functionalities by making the robot both diagnosis- and treatment-capable. The functionality of each component is discussed in each dedicated part.

A.
PFCA Camera or Sensor Our robot design implements a small camera called the Planar Fourier Capture Array (PFCA) [23]. The PFCA camera is a tiny pseudo-camera that can sense small pixels and gives the robot high-tech imaging capabilities. Unlike a typical camera, which has lenses or mirrors that take an entire analog image, the PFCA camera transmits a few digitized pixels, which an external computer can then assemble together to form an image. Every sensor in a PFCA camera is covered with layers of metal grating that diffract light. Thus, every sensor gathers incident light from only certain directions. The gathered light is subsequently analyzed using Fourier transforms, which help combine the information yielded by the many sensors into a single, low-resolution image. The PFCA camera could also be substituted with any kind of micro to nano sensor for different purposes; furthermore, any kind of discrete data could be assimilated into a map with our imaging system.

B.
Accelerometer The accelerometer mounted on the robot is used to keep track of the rotation of each robot. It provides real-time acceleration data measured along the x, y, and z directions in 3D space. The position data serves as a component for our imaging system, detailed in section III.B.

C.
Gravity Sensor The gravity sensor is used for position tracking calibration before injection. It gathers data about the tilt of the robot with respect to the gravity axis (direction that gravity exerts force on an object, used for calibration before injection to know exact initial condition). It is another component of our imaging system, detailed in section III.A

D.
Micro-grabber Our robot has a micro-grabber—a microscale, claw-like grabbing device, built upon the work of a research team at Johns Hopkins University headed by Prof. David Gracias [24] that has developed a star-shaped, tissue-collection microrobot. Each micro-grabber collects tissue with its “fingers.” Our robots are programmed to be heat-sensitive, such that the micro-grabber’s “fingers” will close when exposed to the specific temperature emanating from a certain area of the body. If at that moment the bot is appropriately positioned, its closing “fingers” will grasp a sample of tissue [20, 24]. The micro-grabber of our robot is composed of chromium, copper, and nickel, and includes a polymer trigger. The attributes of this polymer trigger will vary based on the stimulus to which the bot’s “grabbing” mechanism is programmed to respond. Upon activation, the polymer trigger expands, forcing the grabber to bend at the polymer joints and grasp a sample. The operator can send a robot fleet to the area in question and perform an exploratory biopsy, receiving immediate results from throughout the area. The downside to the proposed method of tissue collection is that the samples will be much smaller than the samples typically collected today. Again, however, because so many robots are deployed at once, we expect to collect enough of these small samples to be useful in subsequent analysis.

E.
Iron-Cobalt Bead The iron-cobalt bead (delineated in Fig. 1 as an orange oval) placed on the body of the robot is central to the bot’s capacity for operator-controlled movement. Navigation of the bot is accomplished by using MRI coils to move the robot’s magnetic iron-cobalt bead in a specific direction. Compared to other metal alloys, a bead of iron cobalt is extra-sensitive to magnetic forces because of the configuration of electrons in the material. Furthermore, it is best to coat the bead with graphite, because coating the bead with graphite will keep the bead from oxidizing during its journey.

F.
Bio-degradable Polymer Sac For a robot to carry out its targeted drug delivery (detailed in Sect. 3.2), it must be in possession of the drug appropriate to the patient’s situation. For this reason, our robots are designed to carry small amounts of the necessary medication within a biodegradable polymer sac (represented by a green oval in Fig. 1). This sac is deployed at the command of the operator and degrades as it moves toward its destination, ultimately releasing its contents onto the targeted area.

The imaging methodology
Having explained the structure of the robots in the proposed SoRCM model, we now focus on presenting an imaging algorithm that operates using the model. The flowchart shown in Fig. 2 is an overview of the algorithm.

Fig. 2
figure 2
Block diagram of proposed medical imaging technology

Full size image

In the first step, a 3D model of the human body is generated with the help of conventional MRI and image processing techniques that act as a position reference model. This reference model has a positional three-dimensional coordinate value, which indicates the location and shape of the tissues along with each one of the tissue’s placeholders for the pixel value that will be mapped to it. Next, the robots are carried by blood or fluid, and, thus, randomly travel through the body and gather pixels through their onboard sensors or cameras. Simultaneously, the orientation of each robot is obtained through the rotation tracking and initial calibration. Upon receiving the transmitted data from the robot, custom GPS position coordinates can be calculated from multiple receivers. With the pixel value, the orientation, and an initial point, a parser can be generated. Each information reception from every robot can be mapped to the 3D reference model with the pixel value in real time. After the 3D reference model is mapped with pixel values, an iterative image reconstruction is performed until all missing pixel values are received.

We separate this imaging methodology into five parts, as follows: In part A, we illustrate how to keep track of the orientation of each robot; in part B, we discuss how our custom GPS location tracking system can locate the position of each robot without adding any more components to the robot; in part C, we explain the 3D reference model used in our imaging system; in part D, we illustrate the mapping process that avoids mapping a pixel of an overlaying robot, rather than tissues, onto the reference model; in part E, we describe the imaging reconstruction method we use to smoothen and improve the image iteratively in real time as more and more data is collected.

A.
Application of magnetic field.

As the swarm of bots is injected inside the body, the initial orientation of the robots must be obtained and fixed. It is important that the orientations are fixed so that the accelerometers in the bots have the same initial calibration for further orientation tracking. Fixing the orientation of the bots is done by applying a significant magnetic field whereby the swarm of robots is lined up in the same direction as the magnetic field due to its onboard iron-cobalt bead, as shown in Fig. 3.

Fig. 3
figure 3
Swarm alignment using magnetic field

Full size image

The magnetic field aligns the robots in the x–y plane; however, the rotation of the bots is not fixed with the magnetic field. This causes a problem: If the rotation of the bots varies, then the orientation data given by the accelerometer won’t be accurate. Figure 4 illustrates the possible movement of the bot along the rotation axis. We solved this problem by using a gravity sensor.

Fig. 4
figure 4
Robot movement along rotation axis

Full size image

The gravity sensor is mounted on the robot (as shown in Fig. 5) and gives the initial rotation of the bot. With the gravity sensor, the information about the rotation of the bot is compared to the accelerometer data to get the final orientation.

Fig. 5
figure 5
Calculation of initial rotation using gravity sensor

Full size image

After the robots are injected in the body and start moving freely inside the body, the accelerometer provides the real-time acceleration data it measures along the x, y, and z directions as Ax, Ay, and Az, respectively. The depiction of pitch, roll, and yaw axes are shown in Fig. 6.

Fig. 6
figure 6
Depiction of pitch, roll and yaw axis

Full size image

The rotation angle received by the gravity sensor is either added to or subtracted from the roll of the accelerometer. The final orientation of the bot is calculated by the pitch, roll, and yaw calculation equations shown below [12, 13]. The pitch, roll, and yaw are updated in real time.


 
 


 
 


 
 

B.
Positioning technique.

The robot must be localized for it to perform core functionalities, such as image reconstruction, tissue sampling, and targeted drug delivery. We apply principles of the Global Positioning Service (GPS) to find the locations of the robots in the human body.

In a typical GPS system, the synchronized GPS target and satellite would generate the same sequence of pseudo-random code. At some time point, an object transmits its current code sequence to the satellite. The satellite compares its own sequence with the sequence it receives from the object so that it can determine the time used for transmission. The satellite then deduces the distance between the object and itself. By using at least four satellites that work together, and by using fundamental geometry calculations, the location of the object can be found.

We will apply the principles of GPS operation to find the location of our robots. Because our robots are so small in scale (microscale), our system utilizes a custom localization method that requires much less transmitting power than typical GPS systems.

In our application of the GPS, at least four receivers should be mounted around the human body. One serves as the origin point and the other three are placed relative to the origin as a three-dimensional basis. These receivers individually capture the signals transmitted by each robot. Each receiver records the local time when it receives a piece of data. The outside processing system receives signals from the robot, and is then able to set up equations based on three time intervals between receiving such signals. Using these equations, the outside processing system is able to obtain the relative coordinates of the robot.

Consider O, A, B, C as signal receivers with known coordinates: O(0, 0, 0), A(a1, a2, a3), B(b1, b2, b3), C(c1, c2, c3). Then consider T as a bot with unknown coordinates: T(x, y, z). When T generates a signal, receivers note a time: TO, TA, TB, and TC (Fig. 7).


 

Fig. 7
figure 7
Robot within grid for finding coordinate values

Full size image

With “s” as signal propagation speed,


 
 
 
 
 
 
 

The solution within the range bounded by the receivers (i.e., within the human body, since the robots can only be present within the body) is the location of the robots. The receivers can be portable since the coordinate system is just relevant to the receivers, the body, and the robots.

To distinguish between transmissions of different robots in the body, we append an additional identification number to every piece of data being transmitted. This identification number correlates to a specific robot inside the body and, thus, identifies with which robot the receiver is communicating. This method will not use much more power, nor will it significantly affect the processing time of the receivers. The accuracy of the measurements depends on the accuracy of the receiver’s time measurement.

After the processing system has calculated the GPS coordinates, the transmitted image data can then be used to form an image, which is explained further in section D.

III.
Construction of 3D model.

A 3D model of the human body and its organs needs to be developed to serve as a spatial reference model. A 3D reconstructed image is formed after pixels from the robot are mapped to corresponding points on the reference model. The 3D model can be developed using methods described in [14]. This works by taking large amounts of 2D images collected through MRI or CT of the human body and its organs, processing them using algorithms such as “Marching Cubes” or “Isosurface.” The following is a brief description of these two methodologies.

a.
Marching Cubes is a computer graphics algorithm. The algorithm proceeds through the scalar field, taking eight neighbor locations at a time (thus forming an imaginary cube), then determining the polygon(s) needed to represent the part of the isosurface that passes through this cube. The individual polygons are then fused into the desired surface. The applications of this algorithm are mainly concerned with medical visualizations such as CT and MRI scan data images.

b.
Isosurface is a three-dimensional analog of an isoline. In medical imaging, isosurfaces may be used to represent regions of a density in a three-dimensional CT scan, allowing the visualization of internal organs, bones, or other structures.

In some cases, the 3D image of the human body can be obtained directly, and then can be processed to extract pixel Cartesian points, which will facilitate the reconstruction.

The techniques should end up producing Cartesian points of each pixel boundary, which will be used for reconstruction later. The flowchart below (Fig. 8) depicts the flow of algorithm to generate a 3D reference model:

IV.
Mapping between Pixel Phasor and 3D model pixel.

Fig. 8
figure 8
Flowchart to obtain 3D reference model

Full size image

For the image reconstruction to be accurate, it is necessary that we use the points from the reference model to translate the GPS coordinates. Thus, we carry out a mapping of pixels through phasor and GPS coordinates to the location of the organ on the reference model. In order to map these pixels to the reference model, we must have the following pieces of information:

A set of orientation vectors providing the direction of mapping. (Part A)

A set of GPS coordinates providing information about robot locations. (Part B)

A set of 3D reference points, S depicting the ideal location of the body organ. (Part C)

To understand this mapping, consider a point P0[× 0, y0, z0] and orientation V[a, b, c] determined from the GPS coordinates. The equation of a line in 3D space with these attributes is given by equation [21] (Fig. 9):



Fig. 9
figure 9
Populating the sample set to find the closest point

Full size image

Here, “t” is a parameter describing the slope of the line in 3D space. Given the set S of reference points, we choose “t” to obtain a set of points P′ on the line such that the number of points in P′ and S is the same. The range P′ must be able to cover the maximum number of points in S. To satisfy this condition, let Q be the maximum of all points in S. We vary “t” from 0 to Q in steps of Q/(N − 1) where N is the number of points in S. This ensures that points in S are equally spaced.

After populating P with points using the scheme mentioned above, we do the following:

Pseudo code
For every point C in P′
     For every point V in S
          Find absolute distance D′ between C and V
     Find minimum D′ and denote it by D″
Find minimum D″ and denote it by D
Upon the algorithm’s termination, each point in S is linked to its closest point in P, given orientation V. This point in P is then used for reconstructing the image (instead of the GPS coordinate). The accuracy of this scheme depends on the density of set S, because the greater the number of points in the reference model, the more likely it is that the algorithm finds a point that is closer to the given coordinate.

E.
Image reconstruction.

There are different techniques for image reconstruction in practice, such as Linear Image Reconstruction, Tomographic Reconstruction, and Iterative Reconstruction. We will describe each method; however, we will find that iterative reconstruction is most suitable to our method.

1. Linear image reconstruction This reconstruction technique uses simple matrix-vector operations. Linear image reconstruction involves the inversion of the encoding matrix. However, if the dimensions of the matrix are large, as they often are, then direct inversion is complicated.

2. Tomographic reconstruction There are four types of reconstruction algorithms used in tomographic reconstruction: Fourier domain reconstruction algorithm, back projection algorithm, iterative reconstruction algorithm, and fan beam algorithm. These methods use Fourier transform, radon transform, multiple summation method, and line integrals, respectively, to reconstruct the image, and the technique is often used for X-ray imaging. This technique, however, is not ideal for real-time scenarios, as there are many computations required.

3. Iterative reconstruction Iterative reconstruction uses multiple iteration steps that allow for a better reconstruction. We start with an assumed image and generate projections. We then compare the projections with the original state image and update in real time the image with the pixel’s information based on the difference between the original image color and the pixel information color. The pixel information color is not so easily obtained: It requires that we make a weighted average of all pixels in a specific location. Iterative reconstruction uses statistical likelihood-based approaches, so it provides better noise profiles. When we do not have a large set of projections available for the image, using the probabilistic method helps recreate the entire image: This means that if the robot is unable to detect something exactly, it will use a probability to see where a point in the image is supposed to go. Furthermore, iterative reconstruction is especially helpful because it can reconstruct the image even when the data is incomplete, as it is in our method.

The most suitable method among the techniques that are discussed is the iterative reconstruction method. In our imaging system, we continuously collect multiple pixels and reconstruct them together in real time; thus, iterative reconstruction is most compatible for such a data-varying image reconstruction problem. Furthermore, iterative reconstruction is also better because it improves the noise sensitivity factor, which will reduce errors in our method. Moreover, because we have multiple sources of the same image unit, the iterative method can improve the reconstruction quality. Iterative reconstruction is a widely used method, as it is used for SPECT, PET, CT and MRI scans. For example, in real-time MRI, we can acquire data from multiple receiver coils, and by using proper sampling patterns, we improve the image reconstruction [17]. Because of the above properties, iterative reconstruction has proven to be much better than any conventional reconstruction [16].

We reconstruct the image from the pixel information and the position coordinates collected by the bots. Each bot will send us pixels that it captures with the PCFA camera that is fixed on the bot. We will also get the GPS coordinates associated with the position of the bots. After we are provided with the pixel information and the position coordinates associated to the bots, our parser can create the pixel matrix. From the pixel matrix, our image reconstruction algorithm creates several 2D image slices that are stacked together to form a 3D image. We do this to improve the quality of our image. By slicing our 3D image into 2D images, and then reforming it into a 3D image, we get rid of many black points or error regions that existed in our original, low-quality 3D image. The image reconstruction algorithm is generic and would work for all circumstances, given that we are provided with a pixel matrix (which in our case is done by the parser). This step comes before the iterative reconstruction technique that is used during the simulations in order to make the reconstruction easier.

This image is reconstructed iteratively later using one of the various techniques that are used in computed tomography. The technique that we use is Algebraic Reconstruction Technique (ART) [18] (see also Refs. [16, 17, 19]).

ART can be thought of as an iterative solver of a system of linear equations. The values of the pixels are considered to be variables collected in a vector x, and the image process is described by a matrix A. The measured angular projections are collected in a vector b. Given a real or complex m × n matrix A, and a real or complex vector b, respectively, the method computes an approximate solution of the system of linear equations, as in the following formula.


 

where i = k mod m + 1, ai is the ith row of the matrix A, bi is the ith component of the vector b, and λk is a relaxation parameter. The above formula gives a simple iteration routine.

Simulation results
Access to the source files for the above reference software can be obtained by contacting the lead author, Prof. Mary Mehrnoosh Eshaghian-Wilner.

As explained below, we have developed a user interface to facilitate the user's ability to run the diagnosis and visualize the images as they are reconstructed. Should the user find an anomaly in the organ’s reconstructed image that differs from the normal, there is an ability to halt the reconstruction process and examine the issue further.

The user first loads the 3D model of the human body into the software. Next, the user starts the process by loading the robots’ location coordinates after the robots are injected into the body. The images will then be reconstructed gradually with respect to the robots’ location and the pixel value. This entire flow is described below, with the help of a few simulations that are done with randomly generated data. This interface is used in the following manner:

1.
We initially construct a 3D reference model of the body, and the first plot in the interface displays the constructed 3D model of the body (Fig. 10).

2.
Next, we locate the robots injected inside the body using the Load GPS Coordinates button. This will be a continuous flow of discrete data, simulating the robots to continue moving inside the body.

3.
Finally, we reconstruct the image using the pixel data from the bots. The pixel data can be taken at any point in time. This will result in displaying the actual organs from the reference model and the reconstructed image from the pixel data in two different plots, as shown in Fig. 11. The user is then able to compare the reconstructed image easily to the reference model and find any anomalies that might exist. Figures 12 and 13 show the detected anomalies in the torso, leg, and esophagus, respectively [15].

Fig. 10
figure 10
Constructed 3D reference model

Full size image

Fig. 11
figure 11
Simulation of torso where we can see a lump

Full size image

Fig. 12
figure 12
Reconstructed image of torso and leg focused on the lump detected after comparing with the reference model

Full size image

Fig. 13
figure 13
Lump detected on the esophagus after comparing with the reference model

Full size image

We also prompt a way for auto-detection of any anomalies by comparing the image obtained by the robots with a normal 3D reference model. While this has not been programmed yet in our simulation, we describe in the next section how this auto-detection may be done. The auto-detection highlights the differences between normal cells and infected ones.

A 2D digitized detection of abnormalities
Our current simulation simply outputs the new reconstructed image and requires that a technician observe any abnormalities. However, the task of detecting the abnormalities can be automated using the following algorithm.

It should be noted that in order for this autodetection algorithms to work, the program must compare the reconstructed image and reference model plane-by-plane (as in a 2D method). The algorithms also require that the constructed images be digitized such that each coordinate has a 0 or 1 (i.e., 0 if the intensity is below a fixed threshold and 1 if it is above the threshold), where 0 is white and 1 is black.

We then compare the reference model image (represented by a simplistic example in Fig. 14A) and the reconstructed image (by another simplistic example in Fig. 14B).

Fig. 14
figure 14
A The reference model. B Reconstructed image

Full size image

We compare the two images pixel-by-pixel at each of the coordinates. When two pixels on the images are different at the same location, we output the differing pixels on an output image called the abnormality image (as shown in Fig. 15). If the corresponding pixels are the same, they are not output on the abnormality image.

Fig. 15
figure 15
Area that the robot detects as abnormal

Full size image

Once the comparisons are completed, the program will use the connected ones and labeling algorithms that we previously developed [22] to connect and label each of the abnormality groups as shown (Fig. 16).

Fig. 16
figure 16
Abnormality after the connected ones algorithm is implemented

Full size image

There are other properties that can be inspected with those images other than finding the connected ones and labeling them. For example, the distances between different segments can be calculated, and other geometric properties such as linear separability can be found [22].

Conclusion and future research
In this paper, we first presented a model comprising a swarm of multifunctional robots that are aimed to perform real-time, cellular-level imaging, target delivery, and micro-grabber sampling. Then, using the model, we presented an imaging technique that constructs images from imaging pixels sent by the swarm of robots. A novel component of this technique is a positioning and orientation-tracking algorithm. The presented imaging design assumes that a 3D model of the body will be provided. The purpose of a 3D model is to compare the reconstructed image with that of the reference model. Important future research would entail the elimination of the 3D reference model whereby images can be reconstructed entirely using radar information.