Abstract
The K-means algorithm is one of the most popular algorithms in Data Science, and it is aimed to discover similarities among the elements belonging to large datasets, partitioning them in  distinct groups called clusters. The main weakness of this technique is that, in real problems, it is often impossible to define the value of  as input data. Furthermore, the large amount of data used for useful simulations makes impracticable the execution of the algorithm on traditional architectures. In this paper, we address the previous two issues. On the one hand, we propose a method to dynamically define the value of  by optimizing a suitable quality index with special care to the computational cost. On the other hand, to improve the performance and the effectiveness of the algorithm, we propose a strategy for parallel implementation on modern multicore CPUs.

Previous
Next 
Keywords
K-means clustering

Adaptive algorithm

Unsupervised learning

Multicore CPUs

1. Introduction and related works
In the last thirty years, several theories, methodologies, and tools have been introduced to learn from data, that is to understand, comprehensively, complex phenomena through the analysis of large structured or unstructured datasets representing real problems. This wealth of knowledge has often changed its name over the years (for example, data mining or big data), and today is commonly known as data science [10].

One of the most used tools in this field is a class of unsupervised learning methods known as Clustering Algorithms, whose main aim is to collect similar data in the same group according to a precise metric [29].

An extensive literature is available in the field, giving an overall picture of the clustering approaches, in sequential computing environments as well for parallel architectures [1], [20], [23], [29], [31], [36], [39], [40]. Very often, they use different taxonomies for the description of the algorithms, but all of them consider the -means algorithm as one of the most useful computational tools.

It can be described as follows. Given a dataset consists of  elements in the -dimensional space 
, and an integer , the -means algorithm defines a partition 
 of the elements of  in  non empty subsets 
 called clusters, each of them with 
 elements, and where the elements showing some similarity according to a given criterion are assigned to the same cluster.

In its classical version, the -means algorithm identifies each cluster with a representative 
, called centroid, computed with the following vector operation: (1)
 
while the similarity of an element 
 with the centroids 
 can be measured using some metric in 
, but usually, the Euclidean distance 
is employed. With these definitions, the -means algorithm assigns each 
 to the cluster 
 that minimizes the distance from its centroid 
, that is: (2)
 

The following Algorithm 1 then provides an outline of the Basic -means Algorithm.


Algorithm 1: Basic -means Algorithm
(1) Define  clusters 
, assigning to them 
 elements 
.
(2) repeat
 (2.1) for each cluster 
 compute the centroids 
 as in (1):
 (2.2) for each 
 (2.2.1) search the cluster 
 as in (2)
 (2.2.2) assign 
 to 
 endfor
 until (no change in the reassignment)
A detailed study of the convergence of the -means algorithm is reported in [34], and several papers address the mathematical properties of the algorithm. They show, in particular, that the final result strongly depends on several factors, such as the choice of the initial partition in step (1) or the number of clusters  supplied as input data.

For example, regarding the choice of the initial partition, in [30], the  clusters are defined starting from a random selection of  points of  chosen as centroids. In [2], the clusters are identified by selecting their centroids as far away as possible among them. Similarly, in [19], a dataset preprocessing is proposed to find well-separated points with a high density of elements surrounding them. In general, all the methods proposed for this problem guarantee only a convergence to a local minimum.

The choice of the value of  is one of the main challenges in the design of this algorithm. Very often, in several real and large problems, the dataset does not show sufficiently clear patterns to define such a value as input data. If it is defined too small, there is a high chance that dissimilar objects will be gathered in the same cluster, while if it is set too large, there is the risk that related elements will be scattered in separate clusters.

For the above, several researchers introduce methods to set the value of  dynamically at run time, so to realize a trade-off between efficiency (a small value of ) and accuracy (a high affinity of elements in each cluster). As an example, in [3] the authors iteratively determine the values of  by merging two clusters when the distance between the centroids is smaller than an inter-clusters tolerance or by dividing the cluster with a standard deviation of his points (that is a measure of their affinity) greater than an intra-cluster threshold. A similar approach is described in [21], where the measures of the intra and inter-cluster distances (the so-called silhouette) are averaged. Lastly, in [4], the authors introduce a method based on the maximization of a function based on the sum of squares of the distances among elements belonging to the same cluster and different clusters.

Finally, in the last years, further efforts have been addressed toward parallel implementations of the -means algorithm in several high-performance computing environments. Significant algorithms are described, for example, in [11] for distributed memory architectures, in [22], [24], [33] for multicore CPUs and in [7], [8] for GPUs based systems. Almost all these studies exploit the role that a large amount of data can play in an implementation based on the data parallelism programming model.

Our work joins this research trend introducing a parallel adaptive -means algorithm, with new features respect to the other works. On the one hand, it defines the value of  dynamically by dividing only selected clusters with non-similar elements, so to reduce the number of displacements of the items among the clusters. On the other hand, it improves the performance of the algorithm, implementing it in a multicore CPUs based computing environment by using two different parallelization strategies.

The present paper is, therefore, organized as follows: in Section 2 we introduce the new parallel adaptive K-means algorithm describing the clusters generation procedure and the parallelization strategy; in Section 3 we report the implementation details of the algorithm with special care to the data structure organization; in Section 4 we show the results obtained from various experiments aimed to validate the new algorithm; in Section 5 we discuss the results, and in Section 6 we conclude the work.

2. A new parallel adaptive -means algorithm
This section has a double aim: from the one hand, we introduce a methodology aimed to dynamically define the number of clusters with a reduced computational cost of the algorithm, and, on the other hand, we propose a parallel implementation in multicore environments.

A widespread method to define the value of  without considering it as an input data is to execute the Basic -means Algorithm several times, with an increasing value of , until a given quality index, used as a measure for the goodness of the solution, satisfies the user requirements (e.g., [35]). To this aim, it is possible to find a large variety of quality indexes in the literature (see, for example, [15]). One of the most used index to determine the number of clusters existing in a data set is the Root-Mean-Square Standard Deviation (RMSSD) that represents a measure of the average affinity of the elements grouped inside the several clusters of the partition 
: (3)
 
When the number of clusters  increases, the (3) initially decreases but, when a good affinity among the elements in each 
 is reached, there are no further improvements. In other words, it is possible to increase the number of clusters until adding a new cluster does not give a significant reduction of 
. The previous approach is known as the “Elbow” method [18].

From the above it is, therefore, possible to design the following Iterative -means Algorithm (Algorithm 2) executing the Algorithm 1 several times with the value of  increasing at each step until the 
 index shows no further improvements:


Algorithm 2: Iterative K-means Algorithm
(1) Set the number of clusters 
(2) repeat
 (2.1) Increase the number of clusters 
 (2.2) Define  clusters 
, assigning to them 
 elements of .
 (2.3) repeat
 (2.3.1) for each cluster 
 compute the centroids 
 as in (1)
 (2.3.2) for each 
 (2.3.2.1) search the cluster 
 as in (2)
 (2.3.2.2) assign 
 to 
 endfor
 until (no change in the reassignment)
 (2.4) update 
 as in (3)
 until (the variation of 
 is smaller than a given threshold)
Let consider now the Computational Cost (CC) of the previous Algorithm 2, paying special attention to the kernels inside the iterative structure 2.3. At this regard, it is easy to show that the computation of the centroids 
 in the Step 2.3.1, by using the (1), needs 
whereas the (2), used for the search of the new cluster of each element 
 in the step 2.3.2.1, requires 
Finally, let consider the cost of step 2.3.2.2 of Algorithm 2. It is a critical section of the procedure because it strongly depends on the initial partition 
 of the dataset  defined at the beginning of each iteration (step 2.2). With an inappropriate initial partition, there is a high risk that a large number of displacement of elements 
 among the clusters 
 in step 2.3.2.2 occurs, before the stopping criterion of the iterative structure 2.3 is satisfied. For such reason, we propose a strategy designed to control such a number.

Our idea is based on the assumption that, at the iteration , the elements of  are already grouped according to their similarity, in the  clusters of the partition 
. Many of them do not need to be displaced so that it is possible to concentrate the attention only on the clusters showing still little affinity among the elements. A traditional way to compute the similarity of a set of 
 elements is through the standard deviation: 
 
Greater the value 
 is, more distant the elements 
 from centroid 
 are, and the cluster consists of dissimilar elements.

The key choice of our strategy is then based on the reuse of the partition 
 defined in the previous iteration , avoiding to start with a generic partition in step 2.2. More precisely, let 
 be the cluster with the largest standard deviation at the iteration , that is: (4)
 
the proposed algorithm divides only this cluster into two new subclusters 
 and 
, and defines the initial partition 
 as follows: (5) 
 

In this regard, we remark that the strategy of reorganizing only the subsets of partitions showing poor values of some quality index is an assessed approach in the design of the so-called adaptive algorithms in other scientific computing areas. For example, several adaptive algorithms for numerical integration [26], [27] or computational fluid dynamics [17], [38] refine only the subdomains of a partition where the discretization error is significant. Similar adaptive approaches are used to develop modified versions also of the -means algorithm (e.g., [5]).

From what has been said, we, therefore, propose the following Adaptive -means Algorithm:


Algorithm 3: Adaptive K-means Algorithm
(1) Set the number of clusters 
(2) repeat
 (2.1) Increase the number of clusters 
 (2.2) find the cluster 
 as in (4)
 (2.3) define the new partition of clusters 
 as in (5)
 (2.4) repeat
 (2.4.1) for each cluster 
 compute the centroids 
 as in (1)
 (2.4.2) for each 
 (2.4.2.1) search the cluster 
 as in (2)
 (2.4.2.2) assign 
 to 
 endfor
 until (no change in the reassignment)
 (2.5) update 
 as in (3)
 until (the variation of 
 is smaller than a given threshold)
Today, all the most powerful high-performance computing systems are based on multicore CPUs (e.g., www.top500.org) so that an efficient implementation of algorithms on these devices can be considered as the first level of a complex software stack, able to solve real-world problems in these environments with high performances [37]. In a multicore CPU,  computing units (the cores) share the same main memory, each of them has a private set of registers so that the operating system can dispatch on them concurrent threads, allowing the implementation of the algorithm based on the shared memory Multiple Program Multiple Data paradigm.

An analysis of Algorithm 3 shows that the -means algorithm is well suited for implementations on high-end parallel computing environments based on multicore CPUs. More precisely, we observe that it is possible to identify in it at least two forms of parallelism:

parallelism at clusters level.
In this case, the number of clusters  determines the degree of parallelism. More precisely the clusters 
 with  are distributed among the  threads. This approach finds a natural application in step 2.4.1.

parallelism at elements level.
In this case, the number of elements  determines the degree of parallelism. More precisely the elements 
 with  are distributed among the  threads. This approach finds a natural application in step 2.4.2.

The critical task in Algorithm 3 is then the step 2.4.2.2. In such instruction, some elements 
 are assigned to new clusters taking into account their distance from the centroids defined in (2). This displacement is the cause of a high risk of race condition when different threads attempt to access the data structures that are used to describe the clusters (see the next section for more details on the implementation issues). Such a step is then the only sequential task of Algorithm 3, and it may be the reason for the decay of the algorithm efficiency.

Let 
 and 
 respectively be the running time of the algorithm with  threads and the time required to run the sequential sections of the algorithm. Then the Serial Fraction is defined as 
, and the Amdhal law states that the Speed-up 
 and the Efficiency 
 can be respectively represented as follows: (6)
 
 
 
 
From (6), we observe that also for a moderate value of , the Speed-up and the Efficiency can be strongly degraded, so that it is of paramount importance to start with a suitable partition in the step 2.3 designed to reduce the total number of the displacements of the elements in the step 2.4.2.2.

3. Implementation details
To better describe the new parallel Adaptive -means Algorithm, in this section, we report some implementation details, with particular attention to the data structures that are used to describe the management of the clusters (see also Fig. 1).

To store the  elements 
, our implementation uses a 2-dimensional  static array , where each row represents a -dimensional element of the dataset. This choice is due to the greater efficiency in accessing the elements compared to other implementations based on dynamic data structures such as lists or trees. In any case, we recall that one of the most expensive steps of the algorithm is the displacement of the elements among the clusters (step 2.4.2.2). Each displacement has a not negligible cost of  memory accesses with a severe impact on the performance so that our algorithm leaves the physical order of the rows of  unchanged to reduce such a cost.


Download : Download high-res image (251KB)
Download : Download full-size image
Fig. 1. Organization of the main data structures in the parallel adaptive -means algorithm.

To describe the structure of each cluster 
, we used a pointers array , where each component points to an element of the dataset. In the array , each cluster is therefore defined as a set of contiguous items, each of them pointing to a row of  representing an element 
 of the cluster 
. The new partition 
 in step 2.4.2.2, is therefore achieved by exchanging only components of , each of them with a cost of  memory accesses. We remark that the array  is shared among all threads so that any change in it has to be carried out in a critical section with exclusive access by each thread. As the number of memory accesses in the step 2.4.2.2 decreases, then the value of  in (6) is reduced, with an improvement of the speed-up and efficiency of the algorithm.

In our implementation, each cluster 
 consists of 
 rows of  referenced by contiguous pointers in , and it is fully described by a Cluster Descriptor (
). It is a data structure containing:

•
: a cluster identifier

•
: the centroids of the cluster

•
: a pointer to the first of the contiguous items of  referencing the cluster elements

•
: the number of cluster elements

•
: the standard deviation of the elements of the cluster

Finally, a table of pointers called Cluster Table  provides direct access to the Cluster Descriptors 
.

With the described data structures organization, it is then possible to efficiently manage all information required in Algorithm 3.

4. Experimental results
We tested the accuracy and the efficiency of the proposed Adaptive -means Algorithm running several experiments using the HPC cluster available at the Department of Science and Technologies of the University of Naples Parthenope. This facility is a computing environment where each node is equipped with two Intel Xeon 16-core 5218 CPUs running at 2.3 GHz for a total of 32 computing cores per node, and 192 Gbytes of main memory. In this system, we implemented the Algorithm 3 in C language using the POSIX thread library for the thread management.

As a benchmark for our experiments, we used the following datasets with different dimension and from different application areas, taken from the University of California (UCI) Machine Learning Repository [12]:

Iris
[13]. This problem is based on a quite small but very popular dataset used for multivariate classification. The dataset contains  instances of iris flowers classified in  classes of 
 instances, each of them representing different types of iris plants. The items are described by  attributes representing respectively the width and length of petals and sepals.

Letters
[14]. In this case, the problem is represented by a larger dataset with  black-and-white rectangular images, each of them representing one of the  capital letters of the English alphabet. Each letter is described by  attributes (e.g., the dimension of the character, the number of the edges, the position of the black pixels in the image, and other graphical features).

Wines
[6]. This dataset contains  instances of Portuguese wines described by  attributes (e.g., acidity, sugar, sulfate, alcohol, and other organoleptic features). The wines are grouped in  classes according to their quality (from  to ).

Banknotes
[16]. This problem is described by a dataset containing  instances representing images that were taken from genuine and forged banknote-like specimens. Some mathematical transformations were used to extract  features from images, that are classified in  clusters (true or false).

Cardio
[9]. In this dataset,  fetal cardiotocographic reports (CTGs) have been processed and  diagnostic features measured (e.g., acceleration, pulsation, short and long term variability, and other physiological features.). We use the dataset to classify the elements concerning  morphologic patterns.

Clients
[32]. In this dataset, data are related to the direct marketing campaigns of a banking institution. The marketing campaigns were based on  phone calls to access if a bank deposit would be or not subscribed (that is  clusters). We classify the elements according to  attributes (age, job, education, marital status, and other individual features).

The first set of experiments is aimed to evaluate the effectiveness of the Adaptive -means Algorithm (Algorithm 3) with respect to the Iterative -means Algorithm (Algorithm 2), through a comparative analysis of the total execution time with  thread. Table 1 shows such results reporting, for each test problem, the total number of elements displaced among the clusters (disp) and the total elapsed time in seconds (time) for the generation of the reported number of clusters .

The second set of experiments is aimed to evaluate the performance of Algorithm 3 through the values 
 and 
 given by (6) up to 32 computing cores. For these experiments, we remark that, although each core can run more than one thread with the Intel Hyperthreading Technology, we only used one thread for each core. Table 2a, Table 2b report the total running time in seconds (time), the Speed-up (
), and the Efficiency (
) by using  and  threads for the six test problems.


Table 1. Performance of Algorithm 2 and Algorithm 3.

Problem		Algorithm 2	Algorithm 3
Disp	Time	Disp	Time
Iris	3	67	5.4e−4	59	5.8e−4
Letters	26	1 001 349	55.9	130 801	23.8
Wines	11	81 095	2.6	18 507	9.1e−1
Banknote	2	682	2.7e−3	682	2.9e−3
Cardio	10	25 158	4.3e−1	6553	2.5e−1
Clients	2	30 734	2.3	30 734	2.3
For a complete analysis, we also compared the performance of the Adaptive -means Algorithm (Algorithm 3) with the Iterative -means Algorithm (Algorithm 2) and other implementations of the k-means algorithm on multicore CPUs:


Table 2a. Performance of Algorithm 3 on the Iris, Letters and Wines problems.

Iris	Letters	Wines
Time	
Time	
Time	
2	3.4e−4	1.70	0.85	13.22	1.80	0.90	5.2e−1	1.74	0.87
4	2.5e−4	2.32	0.58	7.00	3.48	0.87	3.3e−1	2.72	0.68
8	2.9e−4	2.00	0.25	4.31	5.52	0.69	2.5e−1	3.60	0.45
16	3.3e−4	1.76	0.11	3.31	7.20	0.45	2.2e−1	4.16	0.26
32	3.6e−4	1.60	0.05	3.23	7.36	0.23	2.0e−1	4.48	0.14

Table 2b. Performance of Algorithm 3 on the Banknotes, Cardio and Clients problems.

Banknotes	Cardio	Clients
Time	
Time	
Time	
2	1.7e−3	1.70	0.85	1.4e−1	1.74	0.87	1.35	1.70	0.85
4	1.2e−3	2.44	0.61	9.2e−2	2.60	0.65	9.4e−1	2.44	0.61
8	1.3e−3	2.16	0.27	7.2e−2	3.44	0.43	9.9e−1	2.32	0.29
16	1.4e−3	2.08	0.13	6.7e−2	3.68	0.23	9.5e−1	2.40	0.15
32	1.3e−3	2.24	0.07	6.5e−2	3.84	0.12	8.9e−1	2.56	0.08
Pkmeans,
described in [33]. It is based on a parallel strategy at Cluster Level similar to that implemented in our Algorithm 3, but with a fixed value for the number of cluster . The algorithm has been run on a problem with  items with  attributes. The number of generated clusters is .

McKmeans,
described in [24]. It is based on the concept of transactional memory to guarantee thread safety indirectly. The algorithm has been run on a problem with  items with  attributes. The number of generated clusters is .

Algorithm 2 and Algorithm 3 are run on the “Letters” problems ( items,  attributes and  clusters). Table 3 reports the results of such an experiment.

Finally, the last set of experiments studies the reduction of the Root-Mean-Square Standard Deviation (3) when the number of clusters  increases. This trial represents a critical test because it allows the evaluation of the quality of the partition 
 generated by Algorithm 3, compared to that of the traditional -means algorithm. More precisely, we are interested in verifying that a smaller number of displacements in Algorithm 3 does not produce a distribution of the elements among the clusters of lower quality than that provided by Algorithm 2. To this aim, we compared the Root-Mean-Square Standard Deviations 
 of both Algorithm 2 and Algorithm 3 for the six selected test problems with a variable value of . The graphs in Fig. 2 report such results.


Table 3. Performance comparison of Algorithm 3 with other parallel implementation of the K-means algorithm.

Pkmeans [33]	McKmeans [24]	Algorithm 2	Algorithm 3
2	1.78	0.89	1.76	0.84	1.68	0.84	1.80	0.90
4	3.22	0.80	1.98	0.62	2.50	0.62	3.48	0.87
8	n.a.	n.a.	2.30	0.43	3.43	0.43	5.52	0.69
16	n.a.	n.a.	n.a.	n.a.	3.85	0.24	7.20	0.45
32	n.a.	n.a.	n.a.	n.a.	4.64	0.15	7.36	0.23
5. Discussion
From Table 1, we observe better effectiveness of the Algorithm 3 with respect to Algorithm 2, measured in terms of the number of elements displaced among the clusters and the total execution time, mainly for large values of  (that is the datasets related to the Letters, Wines, and Cardio problems). More precisely, when the number of iterations is large, we can better appreciate the effects of the adaptive strategy aimed to reuse the partition already defined at the previous iterations, with a smaller number of elements that need to be moved among the clusters. For a little value of  (namely Clients, Iris, and Banknotes problems), the experiments do not report significant benefits for Algorithm 3. In these cases, we register a similar number of displacements and about the same execution time because the adaptive strategy is applied only for a small number of iterations.

Parallel performance reported in Table 2a, Table 2b shows a strong dependence of the Speed-up and the Efficiency on the number of clusters. The higher the value of , the higher the Efficiency we get. This aspect can be explained through the two forms of parallelism, at clusters level and items level, we introduced in Algorithm 3. A significant  increases the concurrency and allows full usage of the  computing units. On the other hand, because the degree of concurrency is independent of the number of attributes, we do not observe significant differences among Speed-up and Efficiency for problems with different values for .

From Table 3, it is interesting to observe that, with the proposed adaptive strategy, Algorithm 3 shows significantly better Speed-up and Efficiency with respect to a parallel version of Algorithm 2. About this aspect, we have previously stated in Sections 2 A new parallel adaptive, 3 Implementation details, that the displacements of the items 
 among the clusters are implemented through a rearrangement of the pointers in the array  occurring in a sequential section of the algorithm. This step is, therefore, the cause of a significant Serial Fraction  in the (6), so that it has a disruptive impact on 
 and 
. With a smaller computational cost for this step in Algorithm 3, we then also reduce the Serial Fraction, with a significant improvement of the Speed-up and Efficiency.

Furthermore, regarding the performance comparison with other algorithms reported in the same Table 3, it is first evident that the Adaptive -means Algorithm (Algorithm 3) outperforms the McKmeans algorithm based on the transactional memory. We believe that the bad performance of the McKmeans algorithm is motivated by the sequential access to shared memory with high idle time. In essence, the transactional memory model is primarily designed to avoid race conditions, but it is not a guarantee of high performance. The Pkmeans algorithm, instead, uses a parallelization strategy at Cluster Level similar to those implemented in our Algorithm 3, with comparable performance values. In any case, we emphasize that the Pkmeans algorithm requires a fixed number of cluster , while our Algorithm 3 overcomes this problem with the adaptive procedure described in Section 3.

Finally, from the analysis of the values of the Root-Mean-Square Standard Deviation (3) reported in 2, we register a very similar behavior of the Iterative -means Algorithm (Algorithm 2) and the Adaptive -means Algorithm (Algorithm 3), very often with almost identical graphs. More precisely, in our experiments, we measured a difference of no more than  between the Root-Mean-Square Standard Deviation 
 related to the partitions generated by the two algorithms, asserting the effectiveness of the Adaptive -means Algorithm proposed in Section 2.

6. Conclusions
This paper describes our studies aimed to improve the performance of the -means algorithm in case the number of clusters  is not available as input data. This issue is a common situation in real applications so that traditional approaches are based on several runs of the algorithm with different values of  attempting to optimize some quality index, with a high risk to increase the computational cost. The method we introduced is based on an adaptive procedure aimed to minimize the number of displacements of the elements of the dataset among the clusters, preserving, at the same time, the clusters of the partition with small values of the standard deviation. Furthermore, the paper addresses the problem of the algorithm implementation in a multicore CPU based system, giving special attention to the reduction of the Serial Fraction so to improve Speed-up and Efficiency. Several experiments confirm these expectations: the proposed algorithm achieves better performance and efficiency with respect to other approaches, mainly for large values of the number of clusters , showing, at the same time, similar values of the Root-Mean-Square Standard Deviation, used as a measure of the global quality of the generated partition.

We already planned future works aimed to integrate, in a single hybrid implementation, the Adaptive -means Algorithm with other algorithms designed for different advanced computing environments, such as GPUs based or distributed memory computing systems [25], [28].

