Abstract
Building a distributed deep learning (DDL) system on HPC clusters that guarantees convergence speed and scalability for the training of DNNs is challenging. The HPC cluster, which consists of multiple high-density multi-GPU servers connected by the Infiniband network (HDGib), compresses the computing and communication time for distributed DNNs' training but brings new challenges. The convergence time is far from linear scalability (with respect to the number of workers) for parallel DNNs training. We thus analyze the optimization process and identify three key issues that cause scalability degradation. First, the high-frequency update for parameters due to the compression of the computing and communication times exacerbates the stale gradient problem, which slows down the convergence. Second, the previous methods used to constrain the gradient noise (stochastic error) of the SGD are outdated, as HDGib clusters can support more strict constraints due to the Infiniband network connections, which can further constrain the stochastic error. Third, the same learning rate for all workers is inefficient due to the different training stages of each worker. We thus propose a momentum-driven adaptive synchronization model that focuses on solving the above issues and accelerating the training procedure on HDGib clusters. Our adaptive k-synchronization algorithm uses the momentum term to absorb the stale gradients and adaptively bind the stochastic error to provide an approximate optimal descent direction for the distributed SGD. Our model also includes an individual dynamic learning rate search method for each worker to further improve training performance. Compared with previous linear and exponent decay methods, it can provide a more precise descent distance for distributed SGD based on different training stages. Extensive experimental results indicate that the proposed model effectively improves the training performance of CNNs, which retains high accuracy with a speed-up of up to 57.76% and 125.3% on the CPU-based and GPU-based clusters, respectively.

Previous
Next 
Keywords
Distributed deep learning system

Momentum-driven

Adaptive

Synchronization model

High-performance computing

1. Introduction
In recent years, we have witnessed the rapid revolution of deep learning (DL), which has made a great breakthrough in both industry and research areas such as computer vision, speech recognition, natural language processing, etc. [24]. With the scaling up of DL models and large volumes of data, training DL models has become a time-consuming task. High-performance computing (HPC) clusters, which consist of high-density GPU servers and high-speed networks, have been expected to improve the training performance. Thus, distributed DL systems build on clusters with high-density GPU servers and high-speed networks (HDGib clusters) have become an essential topic, as they can provide strong support for today's big data AI applications [23], [27].

Parameter server architecture based on the data parallelism and asynchronous parameter update strategy is widely used for scaling DNN training over large datasets and models. It has a better training performance guarantee, as it allows parameters to be updated from each worker individually and eliminates the global barrier for gradient aggregation from all workers. The parameters are updated with high frequency in this case compared to other training schemes. Ideally, such parallel training based on distributed systems can achieve a linear speed-up of the total training time (with respect to the number of workers) compared to a single worker. The total training time is defined as the time of DNN training achieves convergence with the target validation accuracy. Such linear scalability is often hard to achieve due to the communication overheads and the massive iterative optimization processes (caused by stochastic and asynchronous errors). Solving this scalability challenge is therefore a major goal of distributed DL research.

Consequently, a topic of major focus for many distributed DL researchers currently is the acceleration of communication through a wide spectrum of approaches. For example, gradient compression and quantization approaches [3], [34], [42] make efforts for communication reduction for distributed DNN training. This is also echoed by recent literature [1], [15]. They share the same goal—reducing the wall clock time for each iteration to improve the training performance. The communication reduction methods are less than optimal when training the DNN on the clusters with the high-speed network and can be further improved to accelerate the training procedure. This is because the gradient compression and quantization approaches, which are often lossy compression of the gradient, produce gradient errors at each iteration that need to increase the number of the training steps to guarantee the DNN model converges to the target accuracy. These methods are efficient for low throughput networks. For high-speed networks, blindly compressing the communication time leads to an increase in optimization steps, which limits the improvement gained from the high-performance platform. Another emerging method to reduce the communication overhead is the ring all-reduce approach [11], [21], [37]. It exchanges part of the gradient (usually 
 
 gradients, M is the number of workers and 
 is the number of gradient) between workers per communication round and needs 2M communication rounds to broadcast the updated gradient to all workers. Thus, the ring all reduce approach can reduce the communication overhead compared to the parameter server architecture. It is efficient for clusters with low throughput networks, but the total training steps (counted by the worker) that the DNN training converges to the target accuracy is larger than the parameter server approaches (this also can be proved by our evaluation results in Sec. 5.3). For the high-speed network (e.g., InfiniBand network), training DNN with the ring all-reduce method often leads to the network bandwidth idle for most DNN models, and the training performance can be further improved using the idle network bandwidth.

For the HDGib clusters, it is significant to explore a new optimization direction to further accelerate the distributed DNN training besides the communication reduction approaches. A new direction is to explore more solutions to find a more desirable descent direction and distance for distributed SGD at each training step, thus reducing the number of training steps that the DNN model required to achieve convergence with the expected validation accuracy. This new optimization direction is complementary to the communication reduction approaches, and they can be combined to accelerate the distributed DNN training. The significance of this optimization direction can be certified by some recent research works [17], [42], [43]. We also provide evaluation results and analysis based on the most popular DNN models and data sets in Section 3 to further confirm the necessity and significance of this direction.

For the DL applications on HDGib clusters, we analyze the reasons that affect the linear scalability of the distributed DNN training besides the communication overhead and identify three cruxes toward optimization. First, the stale gradient problem leads to an increase in the number of training steps to guarantee convergence of the DNN model with the target accuracy. The stale gradient is defined as the gradient evaluated at an older version of the model [10]. Additionally, the high-speed network further compresses the communication time and leads to the parameters are updated with high frequency, which increases the queuing delay for the gradient update of each worker and increases the staleness of the gradient. The gradient's error rate increase with the increment of the gradient's staleness, and it needs to increase the number of training steps to guarantee the convergence of the DNN training, thus limiting the performance improvement gained by the high-speed network. Second, the stochastic error of the gradient caused by the random sampling of the training data for SGD slows down the convergence of the distributed DNN training. This stochastic error can be reduced by collecting gradients from more workers for each training step, but it requires the support of a high-speed network. The stochastic error can be neglected when the number of workers used to collect the gradients for each iteration is larger than a threshold. However, it is not easy to indicate the number of workers that can constrain the stochastic error while guaranteeing system performance. Third, the same learning rate for all workers often leads to excessive or inadequate descent of the distributed SGD for a particular worker, potentially slows down the convergence. Thus, adopting the same learning rate for all workers is inefficient due to the different training stages, especially on HDGib clusters.

Based on the analysis above, our objectives toward building a distributed DL system on HDGib clusters are threefold: 1) to handle the large volume of stale gradients to reduce the number of training steps that the DNN models required to achieve convergence with the target validation accuracy, 2) to provide a constraint for the stochastic error for the distributed SGD, and 3) to dynamically determine the step size for each worker individually and adaptively. Thus, we expect that the error rate for the descent direction is constrained, and the stale gradients are well organized. The learning rate should dynamically adapt to different training stages to guarantee convergence speed. To achieve the above goals, we aggregate as many gradients and merge the stale gradients in momentum to reduce the gradient noise while guaranteeing system performance. We also propose a non-monotonic search strategy for the learning rate to further improve the training performance.

We summarize our contributions as follows:

•
A distributed progressive momentum algorithm is proposed to dynamically merge large volumes of stale gradients generated by HDGib clusters into the momentum term to constrain the descent direction's error rate adaptively. This algorithm avoids wasting resources to reduce the stale gradients' effect and assign them to contribute to model convergence. The momentum term also can avoid the local optimal problem for the DNN training.

•
An adaptive K-synchronization algorithm is proposed to constrain the stochastic error within a reasonable range without affecting the system performance when training DNNs on HDGib clusters and avoiding the local optimal problem. It provides strong support for stochastic gradient approximation, thus guaranteeing that the distributed SGD finds a correct enough descent direction at each training step. Our algorithm can achieve the same convergence results with less time as compared with the fully synchronous and asynchronous algorithm.

•
An adaptive Barzilai and Borwein (BB) approximation algorithm is proposed to individually adjust the learning rate of each worker. When compared with the global learning rate of the decay method, our searching method was found to be more suitable for HDGib clusters because of the different training stages of all the workers.

The document is organized as follows: We describe the design motivations for our synchronization model in Section 3 and provide a detailed description of the proposed algorithms in Section 4. In Section 5, we discuss our system design, experimental setup, and follow-up by presenting the benchmark results. We analyze the existing solutions for the synchronization models in Section 2 and conclude our work and propose directions for further research in Section 6.

2. Related works
Parameter server vs. all-reduce  Ring all-reduce approaches [11], [21], [37] are proposed to reduce the communication overhead of the distributed DNN training and can achieve the nearly linear scalability of the GPU and network throughput. It requires more training steps (counted by the worker) to achieve the convergence with the target accuracy for the DNN model and potentially suffers from the local optimal problem when training with the vanilla SGD (without the momentum term) since it is a kind of synchronous parameter update strategy. With the increase of the training scale (a large number of workers), the parameter update latency is also large, and it requires collecting the gradients from all workers for each parameter update round. A recent research work, “ZeRO-Offload” [31], proposes re-schedule the training task between the GPUs and CPUs for each worker by offloading the data and compute to CPUs from GPUs, which allows the huge DNN models (e.g., Bert) trained with a common GPU (e.g., NVIDIA V100) server. It uses the ring all reduce communication strategy to gather the gradient from workers to update the parameters, and the GPU throughput can achieve linear scalability. This work is complementary to ours when it uses the parameter server as the communication framework since it mainly focuses on optimizing the training task on each worker, and our work focuses on the optimization of the gradient aggregation among workers.

Distributed synchronization models  The most recent distributed DL systems are built based on three types of synchronization models: Bulk Synchronous Parallel SGD (BSP)-based models [6], which requires the server to collect the gradients from all workers and then aggregate them to update the parameter at each training step; Asynchronous Parallel SGD (ASP)-based models [8], which allows each worker to update the parameter individually; and Stale Synchronous SGD (SSP)-based models [16], which is a kind of asynchronous parameter update strategy, and it binds the stale gradients with a maximal delay time τ. BSP-based models provide the most precise descent guide, and ASP-based models show the best performance for distributed SGD. To achieve better performance for distributed DNNs' training, we should combine the advantages of the BSP-based models and ASP-based models. Previous works [5], [9] have studied the fastest-k SGD for a predetermined k and analyzed the convergence rate of fastest-k SGD with respect to the number of iterations. However, the performance of predetermined k models is highly sensitive to the determiner's decision. In [14], the authors try to adaptively determine the k. However, they ignore the effect of the stale gradients and stochastic error of the gradient for SGD. Some other ASP-based models, such as [26], [30], propose their algorithms based on the shared memory. Shared memory leads to the data consistency problem, and its scalability is poor. The work [30] also assumes the parameter update is sparse, and the objective function of the training task is convex. These assume are available for the traditional machine learning tasks but not suitable for deep learning. The parameter update is not sparse, and the objective function often is non-convex for the deep learning [35]. The [45] proposes to reduce the communication frequency to accelerate the DNN training. However, it is difficult to tune the communication frequency.

Adaptive learning rate  A previous work [4] presents a multi-GPU system for the distributed training of speech CNNs and acknowledging the need to modulate the learning rate in the presence of stale gradients. The authors propose an exponential penalty for stale gradients and show results for up to five workers. However, in large-scale distributed systems, the gradients' staleness can assume values of up to a few hundred [8] and the exponential penalty may reduce the learning rate to an arbitrarily small value, potentially slowing down the convergence. Differing from the exponential penalty and to avoid the excessive decay of the learning rate, [47] provide a linear decay for the learning rate 
 
, where τ is the staleness and 
 is the basic learning rate. It assumes that a large fraction of the gradients have staleness close to n, where n is the number of gradients the parameter server aggregates during each iteration, and only with a very low probability (≤0.0001), does τ exceed 2n. However, this idealized assumption of the system is unrealistic.

Clusters with different performance  The training performance is quite different with different kinds of clusters. It needs to explore the special synchronization models for each of them. A previous work [44] explores the training performance of a heterogeneously networked GPU cluster, and the work [19] designs a synchronization model for heterogeneous multi-GPU clusters. Limited work has tried to explore the training performance of HDGib clusters.

3. Background and motivations
3.1. Background
Our work focuses on the optimization of the synchronization model of distributed DNNs' training using the parameter server architecture and data parallelism to accelerate the training procedure, especially the distributed DNN training on HDGib clusters [40]. In recent years, HDGib clusters have been widely used to improve large DNN models' and datasets' training performance [25], [33]. It compresses the computing and communication time through the utilization of high-density GPU servers and high-speed networks that result in the high-frequency update of the parameters. The high-frequency update of the parameters increases the queuing delay for the parameter update of each worker that exacerbates the stale gradient problem and increases the gradient's error rate that needs to increase the training loops to guarantee the DNN model converges to the target accuracy.

3.2. Observation and analysis
In Fig. 1, we evaluate and show the speedup of distributed DNN training with the different number of workers and network bandwidths compared with that of a single worker based on the training steps that the DNN model required to achieve convergence with a particular validation accuracy. We see that the speedup for 32 workers is often less than 20× for the  network, and the speedup becomes even less for the  network, which is often less than 12×. The number of training steps that the DNN model required to converge is hard to achieve linear scalability for the distributed training. The performance (the speedup shown in Fig. 1) drops incredibly with an increase in the number of workers and the network bandwidth. From Fig. 1, we see that the speedup drops up to 37.94% for the  network compared to . We also compare the convergence behaviors for different network bandwidths ( and ) in Fig. 3 (a). We see that the distributed DNN training consumes more update rounds for the global parameter to achieve convergence compared to the single worker, especially for high-throughput networks. Furthermore, we compare the stale degree for different network bandwidths in Fig. 2, which shows the stale degree is higher for the  network than the  network. The stale degree is defined as the gap of training step between the fastest and other workers for each iteration. The step gaps between the fastest and slowest worker for each iteration are shown in Fig. 3 (b), which also indicates that the gap is higher for the  bandwidth than the  one.

Fig. 1
Download : Download high-res image (575KB)
Download : Download full-size image
Fig. 1. The comparison of the speed-up of the 10 Gbps and 100 Gbps network bandwidths. (For interpretation of the colors in the figure(s), the reader is referred to the web version of this article.)

Fig. 2
Download : Download high-res image (813KB)
Download : Download full-size image
Fig. 2. The comparison of staleness of the 10 Gbps and 100 Gbps network bandwidths.

Fig. 3
Download : Download high-res image (474KB)
Download : Download full-size image
Fig. 3. Comparison of the degradation in the accuracy and stale degree of the 10 Gbps and 100 Gbps network bandwidths and single nodes based on MobileNet/TinyImagenet.

Based on the above observation, we analyze why the models require more training steps to converge to the target accuracy in the distributed environment (especially for HDGib clusters) and found that the degradation of the acceleration is due to the inaccurate descent direction (gradient errors) of the distributed SGD. The gradient errors are involved in the optimization process mainly from two avenues: the gradients used to update the parameter are calculated on 1) the antiquated optimization step (the older version of the models) (stale gradients). The HDGib clusters also exacerbate the stale gradient problem due to the increase of the queuing delay for the parameter update on the server caused by the high-frequency update of the parameter; and 2) the stochastic data sets that are used to train (stochastic error).

3.3. Stale gradient problem
For the stale gradient problem, the momentum term is utilized to merge the stale gradients in our algorithm to accelerate the training procedure as well as avoid the local optimal problem.

3.3.1. Why use momentum?
Momentum in SGD is calculated using the historical gradients in the optimization process with the formula 
, where γ is the decay rate and 
 is the gradients calculated in the 
 iteration. Similarly, we found that the stale gradients are also calculated in the antiquated optimization steps (for example, the current optimization step is t, and the stale gradients are calculated in the step 
, which means that the gradient is calculated by the older version of the DNN models). The momentum is primarily proposed to keep the training performance stable and accelerate the training procedure [28]. The momentum is also proposed to avoid the local optimal problem [38]. Thus, our algorithm proposes to merge the stale gradient into the momentum term to improve the training performance of the DNNs.

To illustrate, we analyze the principle of it using cosine distance. Cosine similarity measures the similarity between two vectors of an inner product space [13] (). We generate the gradients' cosine distance between the 
 and 
 iterations, shown in the upper sub-figure of Fig. 4 with a blue line, and the cosine distance between the momentum term and gradients for the 
 iteration, which has been indicated with a red line. We see that the momentum term's cosine distance (with respect to current gradients) is lower than the stale gradients. It shows stable training performance and further accelerates the training procedure.

Fig. 4
Download : Download high-res image (448KB)
Download : Download full-size image
Fig. 4. Comparison of the cosine distance based on MobileNet.

3.4. Stochastic error
We use the classical stochastic optimization theory, “an effective test value should be greater than the stochastic error [36]”, to constrain the stochastic error and determine the number of workers used to collect the gradients for each optimization step.

3.4.1. Why is adaptive K effective?
The small sample of the training data set at each iteration produces a large stochastic error for the training of DNNs with SGD. A large stochastic error often results in the accuracy oscillation of the DNNs training, which needs to increase the training steps to guarantee the DNN model converges to the target validation accuracy. Increasing the training data set can reduce the stochastic error for each training step. In distributed DNN training, increase the training data set means collecting the gradients calculated by the same version (e.g., the latest version) of the DNN model from more workers. However, it is difficult to determine how many workers (the k value) are used to collect the gradients for each training step to constrain the stochastic error within an acceptable range. In addition, too large k, which means to collect gradient from many workers at each training step, also leads to the local optimal problem and results in the degradation of the validation accuracy for DNN models due to the extreme reduction of the stochastic error. Thus, it is significant to find a suitable k to constrain the stochastic error as well as avoid the local optimal problem for distributed DNN training.

We generate the evaluation results based on MobileNet and show them in Fig. 5 (a), indicating that the training loops for achieving the target accuracy decrease with an increase in the number of workers. However, the worker number can't increase without limitation mainly due to two reasons: 1) for a particular application, the stochastic error can be ignored when the number of workers is larger than a threshold; and 2) the degradation of the validation accuracy caused by the local optimal problem. For the MobileNet/TinyImagenet example shown in Fig. 5 (a), the stochastic error's effect on accuracy can be overlooked when the number of workers is larger than 3. Thus, an algorithm needs to be formulated that can adaptively determine the number of workers (k) during the training. We propose an adaptive k-synchronization algorithm to dynamically provide the conditions for gradient aggregation based on the classical stochastic optimization theory. We provide the conditions in Algorithm 1 and provide a detailed derivation for this algorithm in Section 4.2. To verify the correctness and validity of our algorithm, we also evaluate the conditions and show the results in Fig. 5 (b) and (c). We see from Fig. 5(b) that the gradient aggregation conditions (
 
) meet when , and do not meet when . We also generate the gradient aggregation conditions using (
 
) with a different number of workers (k), as shown in Fig. 5(b), where the conditions meet when . It matches the evaluation results in Fig. 5(a); the stochastic error's effect on accuracy can be disregarded, and the gradients are aggregated when the number of workers is greater than 3. All the above can verify the reliability of our algorithm.

Fig. 5
Download : Download high-res image (1MB)
Download : Download full-size image
Fig. 5. The conditions for the aggregation of gradients based on MobileNet/TinyImagenet.

Algorithm 1
Download : Download high-res image (297KB)
Download : Download full-size image
Algorithm 1. Momentum-driven adaptive synchronization model.

3.5. Why adaptive learning rate? Our approach: two dimension adaptive
Adaptive learning rate during the training  Traditional calculation methods neglect the characteristics of the loss function and often suffer from excessive decay (detailed explanation is given in Sec. 2) for the learning rate. Besides, it is difficult to estimate a reasonable initial learning rate, and an inappropriate learning rate can lead to poor local solutions [39].

Adaptive learning rate per worker  Due to the stochastic feature of the SGD, using the same learning rate for all workers at each iteration brings the stochastic error for the parameter update and affects the training performance. We compare the loss value of different workers based on the synchronous communication strategy on ResNet50 and ResNet101 in Fig. 6 and see that the loss value (indicate the training result of each worker) varies across different workers for each iteration even with the synchronous communication strategy. Thus, we adjust the learning rate per worker and let each worker define the individual learning rate based on their training results. Backtracking line search based on the Armijo condition is an efficient approach to finding a suitable learning rate for deep learning applications. However, the learning rate sequence searched by this method is monotonic, and it ignores the characteristics of the objective function (loss function). We explore an approximated Barzilai and Borwein (BB) method to fit a quadratic model to the loss function and calculate the learning rate based on the curvature of the quadratic model. To increase the robustness of the algorithm, we combine the backtracking line search method with our approximated BB method to calculate the learning rate for each worker individually at each training step.

Fig. 6
Download : Download high-res image (508KB)
Download : Download full-size image
Fig. 6. The comparison of the loss value for different workers on ResNet50 and ResNet101.

3.6. System performance analysis
We further analyze why various asynchronous models' performances are sensitive to and limited by network resources, especially for large CNN models. This coupling feature between performance and network resource results from the queuing model required by the asynchronous scheme. Workers update parameters from the server at high frequency while the network resources are limited. According to our evaluation based on the VGG16/Cifar100, there is 788 times parameters update within 505s with  network bandwidth. The size of the gradient for VGG16 is around . We expect that there are  times of parameter update per second, but the parameter update times are  per second in practice. When training DNN with  network, we expect that there is  times update per second, but the update times are only  times per second in practice. We further analyze this observation and found that the CPUs of the server becomes a bottleneck for the system when training DNN on GPU clusters with high-speed networks based on the parameter server architecture. This also can be demonstrated by our evaluation results in Fig. 15 in Sec. 5.6 and recent research work [18]. The CPU bottleneck is caused by the limited memory bandwidth. The most popular Optimizer (e.g., RMSProp, Adam, etc.) that is used to aggregate the gradient to update the parameter can easily exhaust the memory bandwidth of the modern CPUs since they require many times memory access (include the read and write) for applying the gradient update. For example, the Optimizer of Adam requires more than 10× memory access to update the parameter using the gradient [20].

Our work proposes to buffer k fast workers to reduce the stochastic error and merge the stale gradient to the momentum term before applying the parameter update at each iteration during the training. This also can reduce the parameter update frequency, thus reducing the CPU overhead for the server compared to the asynchronous gradient aggregation strategy theoretically.

4. Methodology and theoretical analysis
In this section, we give a detailed introduction to the Distributed Progressive Momentum Algorithm that addresses the stale gradient problem, the Distributed Adaptive K Synchronization Algorithm that is used to constrain the descent direction in distributed SGD to reduce the stochastic error and avoid the local optimal problem, and the Distributed Adaptive BB Approximation Algorithm that is used to search for the learning rate for each iteration on each worker adaptively.

4.1. Distributed progressive momentum algorithm
4.1.1. Initial definition for the momentum SGD
With the scaling-up of training data, we appreciate selecting data parallelism as our computation model and the most popular parameter server architecture as the basic framework of our distributed DNN training system. In this kind of system, the training data set D is naturally stored on each worker, 
 is defined as the model parameters at the 
 iteration (training step), and the parameters update rule is defined as equation (1) before the convergence criteria is met:(1)
 

The function  is defined to calculate gradients using full model parameters 
 on the worker m. The summation formula of  is used to calculate intermediate results for all workers. The function  aggregates the current model parameters 
 and the summation formula of  to generate the parameters for the next iteration.

Momentum in SGD is proposed to accelerate convergence speed and avoid the local optimal problem [29]. The momentum term (mc) for the 
 iteration is defined in Equation (2), where γ is defined as the decay coefficient for the momentum term and 
 is defined as the gradients from the 
 iteration on the full training data set. Thus, the gradients used to update the parameters for the 
 iteration is calculated as 
. The detained parameter update rule with momentum in Equation (1) is defined as Equation (3), where α is the learning rate.(2)
(3)
 

4.1.2. Distributed progressive momentum algorithm
When applying the above parameter update rule with the momentum (introduced in Equation (4)) in distributed DNN training, the gradients 
 are required to be collected from all workers synchronously for each training step. This is a strong overhead for the distributed system. We found that the momentum term is generated by the gradients calculated at the previous optimization points (training steps), which has the same feature as the stale gradients. Thus, we provide a distributed progressive momentum algorithm that can progressively merge the stale gradients into the momentum term to solve the stale gradient problem and avoid the local optimal problem, as well as reduce the synchronous overhead for the original momentum algorithm.

We assume that there are M workers, each of who makes additive updates to global model parameters at regular intervals. Each worker repeats the steps shown in the Worker part of the Algorithm 1. The symbol definition in algorithms is shown in Table 1.


Table 1. Symbols and definitions for algorithms.

Symbol	Definition	Symbol	Definition
Witer,i	Worker i's training step (i: Worker ID)	γ	Decay Coefficient for Momentum
Momentum term for Siter step	θ	Current Server's Parameter
∇LB(θ,Di)	Worker i's Gradient (i: Worker ID)	SIter	Server's Current training step
θ′	Updated Parameter on the Server	gj	Gradients on Server for j training step
αi	Learning Rate for Worker i	∇L(θ,D)	Current Gradient on Server
M	The number of Workers	k	k for Adaptive k synchronization model
According to the analysis in Sec. 3, the noise gradients are involved through two avenues: the stale gradients and the stochastic error (caused by the stochastic sampling for the training data). The distributed progressive momentum algorithm aims to solve the stale gradient problem as well as the local optimal problem. To constrain the stochastic error, servers collect gradients from k fast workers at each iteration and repeat the steps shown in Algorithm 1 (the Server part). During this collection period, several stale gradients from  slower workers are also received. Instead of dropping them or consuming a large number of computation resources to reduce their effect, we progressively merge them into the momentum term according to their iteration number. This algorithm is shown in Fig. 7. However, according to our experience, we find that it is challenging to set a reasonable k only based on our estimation. Therefore, we propose a distributed adaptive K-synchronization algorithm, which can provide a suitable k at each iteration dynamically during the training. We give a detailed analysis of this algorithm in Sec. 4.2.

Fig. 7
Download : Download high-res image (566KB)
Download : Download full-size image
Fig. 7. Distributed progressive momentum algorithm.

4.2. Distributed adaptive K-synchronization algorithm
According to the description in section 4.1, it is important to dynamically find a suitable k for each iteration in order to bound the stochastic error within a reasonable interval. Error bound, an inherent property of an optimization problem, has recently been revived in the development of algorithms with improved global convergence but without strong convexity. Our work utilizes the error bound algorithm to constrain the descent direction for the distributed SGD. Therefore, we focus on the problem of the form as the formula (4):(4) 
  
 
 

Where 
 is a set of data collected from a probability distribution of p. We assume that L and f are differentiable but possibly non-convex, and the domain Θ is convex. In distributed DL applications, each result of the function  measures how well a DL model with parameters θ match a particular data observation z, among which z is a training data set. The expectation over z measures how well the model fits the entire corpus of data on average.

When N is extremely large (even infinite), the calculation for 
 
 becomes a challenge because it is difficult to evaluate , and its gradient  makes classical gradients methods impossible. Therefore, an SGD algorithm is proposed to reduce the computational overhead for minimizing Equation (4) [32]. On each iteration, SGD selects a training batch 
 of data uniformly at random, and then calculates it using the following Equations (5) and (6):(5)
(6)
 
 

Where 
 denotes the step size (learning rate) used on the 
 iteration. The training data set B is randomly selected from the full data observation 
. According to the simple random sampling (SRS) rule, we can obtain Equation (7). Therefore, the calculation of gradients 
 can be explained as a “noisy” approximation of the true gradients.(7)

A small sample costs fewer resources, but with big noise, increasing the sample can reduce noise but cost more resources. Therefore, it is important to dynamically search for a suitable sample size (k) for each iteration to balance the noisy rate and resource consumption.

4.2.1. Distributed adaptive k-synchronization algorithm
A sufficient condition for 
 to be a effective descent direction is defined as per the following Equation (8):(8)
 (9)

According to the explanation of stochastic optimization, the effective descent direction is constrained by the approximate and true descent directions. The difference between the approximate and true descent directions is defined as noise or error. Bounding this error can guarantee the effectiveness of the approximate descent direction. In a word, the error between the approximate and true gradient needs to be less than both the approximate and true gradients (two error bounds are shown in (8)). Inspired by [7], we get the first error bound for the stochastic gradients 
 by analyzing the maximum value of stochastic error 
. The error bound is further constrained by the estimation of the true gradients (the second inequality in Equation (8)).

The stochastic error 
 is bounded by the function of the variance matrix trace  as following Equation (10). Thus, we can obtain the maximum value for the stochastic error, that is, 
 
 [7] (a detailed derivation has been provided in the Appendix).(10)
 

According to Equation (8), the collected gradients need to be larger than the stochastic error. Thus, we can re-formulate this error bound as the following Equation (11).(11)
 

To simplify the formula definition, we let the trace be defined as Equation (12):(12)
 
 

Finally, the first error bound in Equation (8) can be defined as Equation (13) [7].(13)
 

We further check the approximate gradients with the second error bound in (8). The true gradient is the average value of gradients generated through the whole training data set. Thus, to obtain the second error bound, we estimate the true gradients  using the random sample. According to the Theorem: Interval Estimate of Population Mean and the assumption that the sample variance is the unbiased estimation of the population variance, we can obtain the following probability distributions (also known as the ) (14):(14)
 
 

Where the μ denotes the population average value, the 
 
 is the sample average value; and the 
 is the sample variance. Then, we can get a confidence interval for the population mean with the confidence rate 
 as following (15).(15)
 
 
 
 
 
 

Where 
 
 is the coefficient in . Then, we set the 
 
 
 
 as the approximation of the population mean. In this interval estimate of the population mean method, the confidence interval can also be selected according to the requirement of applications. The inequality (16) can be obtained by applying the above theorem to our algorithm. The maximum value of the confidence interval is selected as the approximated value of the population mean to check the inequality 
 
. The adaptive K-synchronization algorithm is shown in Algorithm 1.(16)
 
 
 
 
 

4.3. Distributed adaptive BB approximation algorithm
4.3.1. Initial definition
Step size, also known as learning rate in deep learning applications, is a variable to control the descent step size on each iteration in SGD. The step size α determines the next descent point in the line of 
, as shown in Equation (17). In practical applications, compared with the simply learning rate decay method, exact line search can provide a maximum α in which α is chosen to minimize f along the ray 
 in Equation (18). However, most of the searching methods are time-consuming. Thus, we provide an adaptive BB approximation algorithm in an effort to find a large enough step size with less time.(17)
(18)

4.3.2. Distributed adaptive BB approximation algorithm
The largest step size can be obtained through the exact line search method, but the strong search overhead makes it inefficient. The backtracking line search method with Armijo condition is thus proposed to search for a large “enough” step size to reduce the search overhead [7]. The classic adaptive Barzilai and Borwein (1988) (BB) [2] method provides a guide for us to explore a non-monotonic step-size scheme that uses curvature estimates to propose a new step-size selection algorithm. It has been proven to be very efficient for solving nonlinear optimization problems, as it fits a quadratic model for the objective on each iteration. The optimal step size is explored for the local quadratic model [12]. The BB method is used along with a non-monotone line search as a convergence safeguard for non-quadratic problems.

Inspired by [7], we combine the backtracking line search method with Armijo condition (defined in Eq. (19), where ) and our BB approximation algorithm to search for the learning rate at each iteration on each worker. Spectral schemes for gradient descent were proposed by Barzilai and Borwein [2], who model the function f as the simple quadratic function and use the curvature of the quadratic model to estimate the learning rate [12]. Since training DNN model with SGD and its varies variant is a kind of stochastic problem, a stochastic variable ψ is introduced for the original quadratic function that provided by the Barzilai and Borwein [2]. Thus, the loss function  is given a quadratic approximation form as Equation (20), 
, where 
 
 and 
⁎
 [7], [12].(19)
(20)
 
⁎
 

This section of work aims to find a suitable α to minimize the 
. The iteration equation for parameters is 
 
 
. The α can be deduced by the curvature estimates as following Equation (21) [7] (a detailed derivation has been provided in the Appendix).(21)
 
 
 

For Equation (21) above, the curvature 
 on each iteration is estimated by the BB two-point step-size rule following Equation (22), in which  denotes the scalar product of the vectors a and b:(22)
 
 

In Equation (22) above, the gradient needs to be calculated in the current batch using parameters 
 and 
. This is not efficient and increases the computing overhead. Thus, we replace the 
 by 
 
. The variance of the gradient is calculated by Equation (23). As the sample is randomly extracted from the sample space, the gradient variance 
 equals each other. Thus, we can obtain the relationship between the batch size and gradient variance: The batch size increases a times and the gradient variance reduces a times. By deploying this property in our method, we can approximate the gradients as Equation (24) [22]:(23)
 
 
 
 
 
(24)
 

Equation (22) can be modified as Equation (25):(25)
 
 

There is a smoothing operation for the step size that makes the performance more predictable as demonstrated by the following Equation (26) in which 
 denotes the step size calculated by Equation (21), where  and N and B denote the full batch size and k times mini-batch size. In addition, when , we set 
 
. As there is no noise produced by the method in this case, we use the optimal step size for a deterministic method. The distributed adaptive BB approximation algorithm that used to search for the learning rate [7] is shown in Algorithm 2.(26)

Algorithm 2
Download : Download high-res image (165KB)
Download : Download full-size image
Algorithm 2. Distributed adaptive BB approximation algorithm.

5. Evaluation and results analysis
In this section, we give an introduction to the implementation and the evaluation method of our synchronization model in Sec. 5.1 and 5.2. The detailed analysis of the evaluation results, includes the speedup & accuracy analysis in Sec. 5.3, the linear scalability analysis in Sec. 5.4, the k value and training performance & accuracy analysis in Sec. 5.5, network bandwidth & convergence analysis in Sec. 5.6 and the separated performance improvement gained by the distributed adaptive momentum algorithm & learning rate analysis in Sec. 5.7, are introduced in the following sections.

5.1. The implementation of our synchronization model
We implement our work on top of the Tensorflow (version 1.15.0) with the parameter server architecture and the data-parallel mechanism that uses the Google-specific RPC (GRPC) library for communication between the server and workers. For the parameter server architecture, these are a master node and several worker and server nodes. During the training, workers execute the training procedure and calculate the gradients. The server collects the gradients from the workers and aggregates them to update the parameter. The master node records the nodes' status/roles and network communication information, saving the checkpoint. At the same time, the master node also takes part in the training procedure. Generally, the first worker is selected as the master node. The data (gradients and parameters) are only exchanged between the server and workers, and no data transferring between the workers for the parameter server architecture. Similar with different kinds of distributed Optimizer provided by the Google Team and researchers, our model wraps the distributed Optimizer class in Tensorflow and design a new distributed Optimizer, called AdapOptimizer, by rewriting the apply_gradient() function that is used to update the parameter on the server. We also write a learning rate searching function and execute this adaptive searching algorithm on each worker to reduce the computation overhead of the server. We compare the training performance of our model with the all-reduce algorithm which is implemented on top of Horovod (v0.22.1) + Tensorflow (v2.4.0) with MPI (v4.0.2) and Tensorflow with NCCL (v2.10.3).

Software stack  The training cluster is built with the CentOS and Slurm Workload Manager. We deploy the distributed DNN training with 32 workers based on the Tensorflow, where each worker is allocated with one dedicated GPU card. For each worker, the deep learning library CUDNN is used to accelerate the training on GPUs. The GRPC library is used for communication between the server and workers. The OpenCV is also used as the library to pre-process the training images.

Parallel programming model  During the training, each worker is assigned with one process. For GPU clusters (introduced in Sec. 5.2), each computing node is assigned with one process for one worker. For HDGib clusters (introduced in Sec. 5.2), each computing node runs 8 workers and each worker is assigned with one GPU card and a process. The GRPC library is used for communication. Thus, the parallel programming model we used for implementing the distributed DNN training is a type of Process Interaction that using the message passing scheme for communication.

5.2. The methodology for the performance evaluation
Testbed  To verify the efficiency of our proposed model, three different kinds of clusters have been used as the testbed. 1) CPU Cluster: 32 computing nodes, each with 3.40 GHz 4-cores Intel Core(TM) i5-7500 CPUs, 16 GB Memory; the network bandwidth is 1 Gbps. 2) GPU Cluster: 32 GPU servers connected by  network, each with 1 GeForce RTX 2080Ti GPU, 4 CPU cores with Intel Xeon Silver 4114 CPU @ 2.20 GHz, 28 GB Memory. 3) HDGib Cluster: 4 GPU servers connected by  network, each with 8 GeForce RTX 2080Ti GPUs, 20 CPU cores with Intel Xeon Silver 4114 CPU @ 2.20 GHz, 256 GB Memory.

Benchmark models and datasets  Five CNN models MobileNet, ResNet34, ResNet50, ResNet101, and VGG16 and three data sets Cifar10, Cifar100, and TinyImagenet are selected as our benchmark. The detailed information for CNN models is shown in Table 2.


Table 2. # parameters and sizes of CNN models.

Name	# Params	Model size	# layers	Grads size
MobileNet	4,200,000	18 MB	28	0.125 Gb
ResNet34	21,282,000	83 MB	34	0.634 Gb
ResNet50	25,610,269	98 MB	50	0.763 Gb
ResNet101	44,707,176	171 MB	101	1.332 Gb
VGG16	138,357,544	528 MB	16	4.123 Gb
Baselines: synchronization models  We compare our synchronization model with eight representative models: (i) Asynchronous Parallel SGD (ASP) [8], which is an asynchronous parameter update strategy and allows each worker to update the parameter individually; (ii) Bulk Synchronous Parallel SGD (BSP) [6], which requires the server collect the gradients from all workers and then aggregate them to update the parameter at each training step; (iii) Stale Synchronous Parallel SGD (SSP) [16], which is used to bound the stale gradients with a maximal delay time τ; (iv) Elastic Averaging SGD (EAO) [46], which periodically updates individual models trained over parallel workers; (v) DropWorker [5], which defined as dropping the stragglers during the training using the synchronous parameter update strategy; (vi) Horovod+Tensorflow_MPI, an open-source all-reduce communication library that uses MPI for executing the all reduce operations. (vii) AllReduce_NCCL, which implements the All-Reduce algorithm based on the Tensorflow and the NCCL is used as the communication library; (viii) AllReduce_GRPC, which implement the all-reduce algorithm based on the Tensorflow and using the GRPC as the communication library.

Hyper-parameter setting  The hyperparameters setting for the above models are shown in Table 3. Mini batch refers to the batch size of each worker. The staleness of SSP is 3. The drop rate for the DropWorker model is the drop percentage for stragglers. The interval for EAO means that the parameter update interval for each worker from the server. Lastly, “LR” and “DR” are the learning rate and the decay rate for the learning rate, respectively.


Table 3. Hyperparameter setting for Async(ASP), Sync(BSP), SSP, DropWorker, and EAO.

Model/datasets	ASP, BSP, SSP, DropWorker, EAO	DropWorker	EAO	Cluster	Network
Mini_Batch	LR/DR	Dropout	Drop Rate	Interval
ResNet34/Cifar10	32	0.01/0.99	0.8	50%	20 Steps	CPU	1 Gb/s
MobileNet/Cifar10	64	0.01/0.99	0.8	50%	20 Steps	CPU	1 Gb/s

ResNet50/TinyImagenet	32	0.1/0.99	0.8	50%	10 Steps	GPU	10 Gb/s
ResNet101/Cifar100	64	0.01/0.99	0.8	50%	10 Steps	GPU	10 Gb/s
VGG/Cifar100	32	0.001/0.99	0.8	50%	30 Steps	GPU	10 Gb/s

ResNet101/Cifar100	64	0.01/0.99	0.8	50%	10 Steps	GPU	100 Gb/s
VGG/Cifar100	32	0.001/0.99	0.8	50%	30 Steps	GPU	100 Gb/s
5.3. Speedup and accuracy
Computing and communication time ratio & speedup  The computing (“Cp”) and communication (“Cu”) times for different benchmarks are shown in Table 4, and we see that when deploying distributed DL applications on the GPU clusters, the computing time is compressed extremely, and Table 2 shows that the gradients' size for most of the models is around  or even larger with the  data type. We define the time ratio of communication/computation as the value of communication time divide by the computation time. According to our observations and analysis in the Introduction section (Sec. 1), the high-speed network bandwidth can compress the communication time to accelerate the training procedure, but it exacerbates the stale gradient problem that leads to the increasing of the training steps that the DNN models required to converge to the target validation accuracy. Thus, the speedup achieved by the high-speed network is limited. Our momentum-driven algorithm focus on addressing the stale gradient problem to improve the training performance for distributed DNN training with the high-speed network. From the Fig. 9, Fig. 10, Fig. 11, we see that our approach can gain more improvement for the training performance on HDGib cluster ( network bandwidth) compared to the GPU cluster ( network bandwidth). This further confirms our approach's effectiveness and verifies our statement that the high-speed network exacerbates the stale gradient problem. Besides, we found that the speedup achieved by the CPU cluster ( network bandwidth, training DNN model with CPU) is also high. We further analyze the root cause of this observation and found that the speedup achieved by our approach is not only affected by the network bandwidth but the time ratio of the communication/computation. The stale gradient problem becomes serious with the decreasing of the above time ratio. Thus, our approach can achieve higher speedup for CPU and HDGib cluster ( network bandwidth) that with lower time ratio of the communication/computation compared to GPU cluster ( network bandwidth) and the speedup is shown in Fig. 8, Fig. 9, Fig. 10, Fig. 11. Table 4 and Fig. 11 also show that our model achieves a significant reduction for the convergence steps only with a few extra computing times, especially for the large DNN models with the  network bandwidth, such as the VGG16. Thus, the overall convergence time for our model is less than that of other synchronization models.


Table 4. Computing and communication times per local step.

Model/datasets	ASP	BSP	SSP	Our Model	DropWorker	EAO
Cp	Cu	Cp	Cu	Cp	Cu	Cp	Cu	Cp	Cu	Cp	Cu
Cluster/Network	CPU cluster with network bandwidth 
ResNet34/Cifar10	23s	5.7s	23s	24.3s	23s	19.4s	23s	7.4s	23s	18.3s	23s	4.9s
MobileNet/Cifar10	4.36s	2.74s	4.36s	8.84s	4.36s	4.54s	4.36s	3.24s	4.36s	4.74s	4.36s	2.54s

Cluster/Network	GPU cluster with network bandwidth 
ResNet50/TinyImagenet	0.86s	4.14s	0.86s	9.14s	0.86s	7.14s	0.86s	5.14s	0.86s	12.54s	0.86s	8.64s
ResNet101/Cifar100	0.89s	8.11s	0.89s	15.11s	0.89s	12.11s	0.89s	9.11s	0.89s	6.81s	0.89s	4.31s
VGG16/Cifar100	0.98s	25.02s	0.98s	35.02s	0.98s	31.02s	0.98s	27.02s	0.98	34.02s	0.98s	22.02s

Cluster/Network	GPU cluster with network bandwidth 
ResNet101/Cifar100	0.89s	0.21s	0.89s	0.79s	0.89s	0.66s	0.89s	0.3s	0.89s	0.53s	0.89s	0.03s
VGG16/Cifar100	0.98s	2.06s	0.98s	5.41s	0.98s	3.4s	0.98s	2.7s	0.98	3.15s	0.98s	0.04s
All_reduce and communication library  We compare our work with the all-reduce algorithm that implemented based on different communication libraries, includes the GRPC, MPI, and NCCL, in Fig. 9, Fig. 10. From these figures, we see that our algorithm outperforms the all-reduce algorithm by  for Horovod+Tensorflow with MPI communication library,  for Tensorflow with NCCL, across three DNN models trained on GPU cluster with  network bandwidth. The speedup of wall clock time of each training step is  and  for Horovod+Tensorflow with MPI and Tensorflow with NCCL respectively compared to our work. From the above observations, we see that the MPI and NCCL communication library reduce the wall clock time for each training step, while the all-reduce algorithm requires large numbers of training steps (counted by worker) to achieve convergence for DNN models with the target validation accuracy. Thus, the total training time of the all-reduce algorithm is longer than our approach. Compared to the low bandwidth (e.g., ) networks, the speedup of the wall clock time for each iteration gained by the all-reduce with NCCL is less for the clusters with high-speed () network. Thus, the training performance is even poor for all-reduce on HDGib clusters compared to our algorithm. Our algorithm outperforms all-reduce that implemented on top of Tensorflow+NCCL by up to 86.7% for VGG16, which is shown in Fig. 10. In addition, the all-reduce is a kind of synchronous communication strategy. Thus, it potentially suffers from the local optimal problem that leads to the degradation of the validation accuracy for the DNN training compared to the asynchronous communication strategy, which is caused by the extreme reduction for the stochastic error (especially train DNN with the vanilla SGD that without the momentum term).

5.4. Linear scalability analysis
We train DNN models using different numbers of workers on the cluster with  network. In Fig. 12, we observe desirable improvement for the linear scalability of the convergence speed based on training step achieved by our model with the worker number compared to the ASP. We see that our model outperforms ASP by  for ResNet101 and  for MobileNet. Our synchronization model mainly focuses on addressing the stale gradients problem and constraining the stochastic error, and we observe that our synchronization model achieves more improvement for ResNet101 compared to the MobileNet from Fig. 12. Thus, we classify the above observations and see that the large DNN models (e.g. ResNet101 in Fig. 12) are more seriously affected by the stale gradient and stochastic error compared to the small DNN models (e.g. MobileNet in Fig. 12).

Fig. 12
Download : Download high-res image (258KB)
Download : Download full-size image
Fig. 12. The comparison of the speed-up for different numbers of workers on ResNet101 and MobileNet.

5.5. k value vs. training performance and accuracy
We investigate how the k value varied with the time during the training for different DNN models. In the experiment, we use the network with the  bandwidth and mini-batch size 32 (introduced in Hyper Parameter Setting in Sec. 5.2) to train each model. In Fig. 13, we observe that the k value is non-monotonic varied with the time during the training and bounded with a relatively small range (e.g.  for ResNet101 and  for MobileNet). This is reasonable because the k value is used to constrain the stochastic error adaptively during the training and also avoid the local optimal problem. Too large k value leads to the local optimal problem, and too small k value brings a large stochastic error that results in the increasing of the training steps to achieve convergence for DNN model with target accuracy, which affects the training performance. This also can be proved by the evaluation result shown in Fig. 14. From Fig. 14, we see that the validation accuracy for the training with fixed  (large k) is less than  (small k) and also less than our adaptive k algorithm. The degradation of the validation accuracy is caused by the local optimal problem due to the large k value. Since the k value that evaluated by our adaptive k algorithm concentrates on range , we compare our adaptive k algorithm with fixed  in Fig. 14 and see that the validation accuracy of  algorithm is similar with our algorithm. This result indicates that when training DNN with a small k value, it can avoid the local optimal problem. However, the number of training steps to achieve the convergence with the expected validation accuracy is larger than our algorithm for  (in Fig. 14(a)(c)) and the convergence time is also longer than our algorithm (in Fig. 14(b)(d)) for both ResNet101 and MobileNet models.

Fig. 13
Download : Download high-res image (440KB)
Download : Download full-size image
Fig. 13. The k value of our adaptive k synchronization algorithm for different kinds of benchmarks.

Fig. 14
Download : Download high-res image (1MB)
Download : Download full-size image
Fig. 14. The comparison of training performance for our adaptive k algorithm and fixed k with different k values based on the training steps and times.

5.6. Network bandwidth vs. convergence
We investigate the convergence behaviors for different synchronization models at different bandwidth levels. From Fig. 9, Fig. 10, we observe that our model gains more performance improvement by up to 56.52% with a  network compared with that of the  based on the training steps. This can verify our proposition in Section 3 that the high-speed network exacerbates the stale gradient problem, which affects the training performance extremely. Our model aggregates more workers (usually less than 5) for each training step to reduce the stochastic error. Thus, the time for each training loop is a bit longer than that of the ASP model. According to our evaluation, the communication time often occupies a significant portion of the overall training time (which is usually higher than 90%) for a  network while around 60% for a  network. Thus, the increase in communication overhead has a greater impact on training performance for a  network than a  one. These can be verified by comparing Fig. 9(d)(f) and Fig. 10(b)(d). We see the speed-up is higher by up to 91.16% for the  network than that of the  based on the training time, which is also shown in Fig. 11(c). Combining all the above analyses, we see that our work can solve the serious stale gradient problem with a little extra communication time. Thus, our work is more efficient for high-speed networks and can improve the training performance by up to 125.03%. In a (commercial) AI cloud equipped with more powerful computation devices (e.g., Tesla V100, up to 9× faster than GTX 2080Ti) and higher bandwidth network, a higher gain can be reached due to more serious stale gradient problem.

Network bandwidth usage and bottleneck analysis  Since the data exchanging occurs only between the server and workers and no data transferring between workers. Thus, we evaluate the network usage between the server and worker on clusters with  and  network bandwidth in Fig. 15. From this figure, we see that the network usage can achieve up to 90% and 44% for  and  network respectively. We further analyze the root cause for this observation and found that the network bandwidth and memory bandwidth of the server are the common bottlenecks for the distributed DNN training with parameter server architecture. Take the ASP as an example, both the memory and the network bandwidth are the bottlenecks for the  network. Thus, the network usage can achieve up to 90% (in Fig. 15(a)). For the , the network bandwidth usage only achieves 44% (in Fig. 15(b)). The degradation of the network bandwidth usage for  is caused by the limitation of the memory bandwidth. The memory bandwidth becomes the main bottleneck compared to the network bandwidth when training DNN with  network. The above analysis also can be proved by the recent work [18].

Fig. 15
Download : Download high-res image (411KB)
Download : Download full-size image
Fig. 15. The network usage for ResNet101 and VGG16 on GPU clusters with 10 Gbps and 100 Gbps bandwidth.

5.7. Separated performance improvement for momentum and learning rate
Our model consists of two parts of optimization modules: The momentum and adaptive-k part constrain the descent direction, and the learning rate part calculates the descent distance. To clarify each part's contributions, we generate different experiments based on two groups: synchronization model with the learning rate defined by our BB method and traditional decay method [41]. The speed-up for the above two groups' experiments is shown in Fig. 11 (b)(d). We see that our proposed BB method achieves up to 25% and 27.06% improvement based on the training steps and times compared to the decay method. We also show the learning rate calculated by our approximate BB method for different workers in Fig. 16 based on ResNet50/TinyImagenet (Fig. 16(a)) and ResNet101/Cifar100 (in Fig. 16(b)). From this figure, we see the learning rate varies across different workers and adjusts during the training procedure adaptively.

Fig. 16
Download : Download high-res image (473KB)
Download : Download full-size image
Fig. 16. The learning rate calculated by our approximate BB method based on the ResNet50 and ResNet101.

6. Conclusion and ongoing work
This paper provides an efficient synchronization model for distributed DL on HDGib clusters, improving the training performance of various kinds of CNN models and data sets while maintaining accuracy. It includes three parts of the works: the momentum-driven part, adaptive K synchronization part, and adaptive BB approximation part. The synchronization model is crucial for the convergence behaviors of the distributed DL. We analyzed various optimized SGDs in the DL theoretical area and the features of distributed systems and provided a distributed-friendly synchronization model. The results show that our proposed synchronization model can offer a fast and stable convergence guarantee for HDGib clusters. The convergence steps and times are less than those of other models with the same accuracy guarantee. In the future, we would like to explore the development of a probability search method for the step size, as it would be more suitable for the non-convex problem. We would also explore the distributed-friendly probability searching method in the context of the current research.


