Abstract
The unrelenting growth of data intensive applications has been raising the bar for performance of high speed networks. To cater for this growth, the core segments of networks today are based on 100 Gbps links. Analyzing the huge volume of traffic over these networks is a challenging yet essential task from the perspectives of network administration, network security and law enforcement. In spite of the availability of adequate network, compute, and memory resources, designing traffic analysis solutions performing at line rate for high speed traffic brings up several challenges. Traffic analysis solutions built on commodity compute platforms bring distinct advantages in terms of cost, adaptability, up-gradation and scalability. Keeping this in view, this paper explores the feasibility of designing high speed traffic analysis solutions that can handle 100s of Gbps on commodity compute platforms. It begins this process by analyzing the design issues in line rate handling of traffic on commodity servers with 100 GbE NIC and by bringing out an optimized and scalable packet processing pipeline. Leveraging this processing pipeline, NAPA-FP, a NUMA aware flow probe architecture, has been designed. With an implementation of this architecture on a commodity server, we show that line rate processing of Internet-like traffic from a 100 GbE interface can be achieved with a single NUMA node, by using a suitably configured packet processing path. We also show that its performance scales linearly to multiple 100 Gbps with additional NUMA nodes and NICs. In particular, the implementation on a 4-socket commodity server with  GbE NICs is able to process  Gbps Internet-like traffic at line rate using 3 NUMA nodes. The optimizations with respect to resource allocation, sharing and processing pipeline are reported with corroborating experimental results.

Previous
Next 
Keywords
Traffic analysis

Linear scaling

Commodity servers

Beyond 100 Gbps

1. Introduction
Managing and securing computer networks requires the ability to have knowledge of what is happening in these networks in real-time. This requirement can be met by having a network traffic analysis solution, which can look at the complete network traffic, record the data at various levels of abstraction and then let one glean important information by offering multi-level views to the data from the administration, network security and law enforcement perspectives. The administration perspective helps in troubleshooting, resource planning and forecasting; the network security perspective enables detection of policy violations and security incidents; and the law enforcement perspective helps in discovery of illegal activities. However, network traffic analysis has become a challenging task over the last decade mainly due to the fast pace of evolution of the Internet traffic in terms of bandwidth, protocols and application content. Over the last few years the bandwidths of core networks have evolved from single 10 Gbps to multiples of 10 Gbps to 100 Gbps to support rapidly increasing Internet traffic [21]. With respect to protocols, apart from a host of new protocols like Spdy [51], Quic [30] etc. various tunneling mechanisms have evolved in order to support new applications over legacy networks. Further, the content on the links has changed from text and image-based traffic to more complex streaming voice, video, peer-to-peer and mobile application traffic [49]. The dynamic and complex nature of traffic requires constant adaptation of the traffic analysis solutions as well.

The requirement of traffic analysis solution to be scalable and flexible is best met by software solutions that work on programmable commodity general purpose hardware platforms. However, traffic analysis has long been the realm of specialized hardware, either entirely custom-built or partially custom-built, based on FPGAs and network processors [6]. Custom-built hardware is generally more expensive than commodity hardware due to the initial development cost and the long development cycle. They also lack the flexibility and scalability. Their advantage is that it is relatively straightforward to achieve desired traffic processing performance by using special hardware circuitry designed for specific tasks. It is not straightforward to design a software solution based on commodity hardware to achieve comparable processing performance, in spite of the hardware having the required capacities. The main challenge is that the processing stack on a commodity platform including hardware, operating systems, networking stacks and application frameworks is designed to be generic to support a variety of applications. To meet the challenges of line rate processing, the traffic processing pipeline on commodity hardware has to be architected to optimize the IO, compute and memory operations with strategies for resource allocation and reuse with concurrency.

Based on the level of detail required, traffic analysis may require a combination of deep packet inspection, flow level analysis or simple collection of statistics. Recording of the entire traffic, however, is not preferred due to privacy concerns and huge storage requirements. The deep packet inspection capability in traffic analysis is limited by the visibility of information layers. Flow level analysis has been the preferred choice due to the visibility available up to the flow information in all traffic, reduced storage and computation costs of flow capture, and the availability of techniques for flow processing.

Commodity servers of today come packed with enough resources as a result of the significant advancements made over the last decade in terms of processing power, memory bandwidth and IO bandwidth. Introduction of multi-core and multi-processor technologies, improved cache and high speed processor interconnects have increased the processing power in a significant manner. The integration of multiple memory controllers in processors and the increase in the number of memory channels per controller have increased the accessible memory bandwidth per CPU core. Multi-processor systems with Non-Uniform Memory Access (NUMA) architecture have further augmented the memory access speeds by distributing the memory across multiple NUMA nodes. The introduction of PCI Express Generation 3, integration of IO Hub into the processor, introduction of Intel Data Direct IO and Non-Uniform IO Access (NUIOA) [25] have significantly increased the accessible IO bandwidth. The Receive Side Scaling (RSS) [36] available in latest NICs is another important feature that helps in achieving performance by distributing packet load across multiple CPU cores. The packet processing research community has been working towards enabling the development of traffic analysis solutions on these sophisticated commodity platforms that has resulted in specialized frameworks, architectures and techniques for packet processing. Most of these efforts have considered flow probe as a representative traffic analysis application covering all stages of packet processing. Some of the notable work on the design of flow probe software for commodity hardware are [11] [57] [56], which focus on processing a few multiples of 10 Gbps. Achieving 100 Gbps+ performance in a tightly linked packet processing pipeline on the commodity hardware with the Linux networking stack has proved challenging [48] and warrants specialized software architectures.

This work presents NUMA Aware Pipelined Architecture for Flow Probe (NAPA-FP), a software architecture that works on commodity servers. It is based on a parallel architecture that extracts optimum performance from the present day commodity servers and NICs for line rate processing. It leverages the multi-core, multi-processor NUMA architectures of servers and the RSS feature of NICs and combines together some of the known optimization techniques. A general design principle is presented for building a commodity hardware-based flow probe that is scalable in performance. A prototype implementation of flow probe based on this design principle is presented with experimental results on a commodity server. The scalability of performance with additional NICs and NUMA nodes is ascertained by experiments with a  Gbps probe.

The major contributions of this work are the following:

•
The hardware capabilities of commodity servers are evaluated and the feasibility of processing network traffic at 100 Gbps+ rates is established.

•
A complete processing pipeline architecture for realizing 100 Gbps flow probe on commodity servers is presented along with experimental validation.

•
The architecture is shown to scale the traffic processing capability linearly to handle multiple 100 Gbps by using multiple NUMA nodes.

•
Optimal techniques and parameter settings for engineering the performance are deliberated along with experimental results validated on variations of traffic patterns.

The rest of the paper is organized as follows. Section 2 describes the related works in this area. Section 3 analyses the budgetary requirements and feasibility of processing multiple 100 Gbps traffic on commodity servers. Section 4 describes the proposed architecture, NAPA-FP. Section 5 enumerates different optimization techniques that have been adopted to improve the application performance. Section 6 presents an evaluation of NAPA-FP and finally, Section 7 concludes the paper.

2. Related works
During the last decade, many researchers have been contributing to evolve techniques for high speed packet processing on commodity compute platforms with a focus on the performance of Linux in optimizing the usage of interface, compute and memory resources. Specific contributions have been made to identify bottlenecks and optimize the performance at different stages of network packet processing. These include distribution of packets on arrival, assignment of compute and memory resources, optimizing memory bandwidth usage, and efficient data structures and algorithms.

Prominent among the packet capture frameworks are PF_RING [9], Netmap [44], PacketShader [22] and Intel DPDK [26]. All these frameworks are shown to perform packet capture at line rate of 10 Gbps with little or no loss. Prominent strategies of these frameworks are 1. Replacing interrupt-driven drivers with polling-mode drivers 2. Bypassing Linux kernel stack using their own packet processing routines 3. Zero copying techniques and 4. CPU-pinning in order to avoid context switching. The implementations of these frameworks are available as open source and their adaptation depends on their usability aspect. Most of these frameworks exploit the RSS feature of the NICs to achieve data parallelism and performance. In their recent work, Barbette et al. propose a new load-balancing technique called RSS++ [1] to spread packet load in more optimized manner. This technique dynamically scales to number of allocated CPU cores to accommodate increasing input loads.

The work by Nathan Hanford et al. [23] elucidates different performance bottlenecks and proposes solutions for high-speed packet processing on a commodity server. Sebastian Gallenmuller et al. [19] compare different frameworks in terms of performance. Their paper describes different limiting factors and the steps taken to overcome these limitations. Rolf Neugebauer et al. [39] show that PCI-e, and interaction with the host system has an impact on the performance of network packet processing applications. The work of Guangdeng Liao and Bhuyan L. [32] points out packet processing overheads with respect to hardware architecture and operating system. It also proposes a new server architecture using a customized Direct Memory Access (DMA) for efficient payload movement.

For packet capture and processing beyond 10 Gbps on commodity compute platforms, Intel DPDK has been the choice of most of the works. G. Julian-Moreno et al. [28] present their work on 40 Gbps traffic capture and retention using commodity hardware. They are able to capture, timestamp and store 40 Gbps network traffic using a tailored network driver together with Non-Volatile Memory express (NVMe) technology and the Storage Performance Development Kit (SPDK) framework. In their work, they have used Intel XL710, 40 Gbps NIC card. P. Emmerich et al. [16] have developed FlowScope, a system for packet capture and selective dumping of packets from 100 Gbps network. In order to achieve 100 Gbps capture capability, they have used three Intel XL710, 40 Gbps NIC cards. Both these works use DPDK as the underlying framework for packet capture.

Haoda Wang et al. [50] evaluate the challenges associated with 100 Gbps packet capture and point out that packet sizes play a significant role in performance. It is relatively trivial to reach line rate with many threads using 1518 B packets but the NICs struggled to reach an aggregate speed of 20 Gbps with 64 B packets. Furthermore, they have compared some of the commodity packet processing frameworks and they conclude that Intel DPDK is the only framework that works beyond 40 Gbps. Xiaoban Wu et al. [53] focus on sketch-based network measurement for 100 Gbps links using multi-core processors. For their work they considered Mellanox ConnectX-4 with Intel DPDK for fast packet IO. In the experimental results of their work, even using the best combination of design options, the maximum performance obtained with 64 B packets is 20% packet drop.

A significant amount of work is reported for packet processing architectures on commodity hardware for different purposes such as flow processing [20] or virtual routers [15]. With the introduction of NetFlow [7] by Cisco, flow monitoring has become an effective way of monitoring high speed networks. Ntopng [11] by Luca Deri is a real-time high speed network flow probe that can be deployed in a commodity hardware. It's an open source software solution that has proved its effectiveness in 10 Gbps networks. Recently, this work has been extended to capture full network traffic at wire-speed up to 100 Gbps and write it to NVMe disk [8] [10]. Similarly, FloWatcher-DPDK [57] and FlowMon-DPDK [56] are two Intel DPDK-based flow probes that focus on monitoring flows at 10 Gbps with minimal resources and claim to scale horizontally for higher traffic rates. In both these works, the flows are captured, stored and analyzed on the same system. The current trend on flow analysis emphasizes the need of Machine Learning based techniques that may require adaptive construction of flow features [2] and allow export to enable fast computing.

Flow probe has been ported in different FPGA-based NICs. Zadnik et al. [55] make a first implementation on NetFPGA-1G (a VirtexR-2 platform). This version is publicly available in the NetFPGA-1G repository and supports only 1 Gbps link rate. Yusuf et al. [54] propose architecture for using reconfigurable hardware for network flow analysis. Their architecture processes multiple flows in parallel by mapping processor intensive tasks into FPGA. Rick Hofstede et al. [24] talk about evolution of different flow probe technologies. They also compare different open source and commercially available flow probes in terms of features and performance.

The present work takes lead from the reported works and brings out an assessment of resources required, their allocation and optimized use for design of 100 Gbps flow probe that provides a scalable performance with additional NUMA nodes and NICs. To this end, a commodity server based on NUMA architecture and running Linux operating system is considered to validate the design approach and to prove the targeted performance.

3. Feasibility of 100 Gbps processing on commodity servers
Before getting into the design, we establish the feasibility of processing 100 Gbps traffic on commodity servers. This is done by analyzing in detail both the resource requirements for 100 Gbps flow probe processing and the available capabilities of a representative commodity server.

3.1. Capabilities of representative commodity server
These servers support 100 GbE NICs and other peripheral interfaces with adequate IO speed and bandwidth along with high speed access to memory. They have PCI-e x16 Gen 3/4 interfaces for IO and multiple memory channels of DDR4 with clock speed of 2666 MHz and above. To address the latency issues, they are introduced with multiple levels of shared and dedicated cache. For the feasibility analysis and validation of our design, we have used a server with Intel Xeon Skylake Gold 6148 processor with 20 cores and clock speed of 2.4 GHz, which is a good representative of the number of cores and clock rates of the latest processors.

3.1.1. IO bandwidth
The NIC card is the input interface to the traffic and is interfaced to the processor through the PCI-e interface. It is necessary that the PCI-e interface has adequate bandwidth for transferring input traffic at 100 Gbps rate. Widely available PCI-e x16 Gen 3 provides 16 lanes with each lane providing 8 Giga Transfers per second (GT/s) per direction. Thus PCI-e x16 Gen 3 has a cumulative bandwidth of 128 GT/s and an effective bandwidth of 126 Gbps, considering 128b/130b encoding used in PCI-e Gen 3.

Neugebauer et al. [39] elucidate the performance implications of PCI-e when interacting with the host architecture and device drivers, and also recommend a set of measures to achieve performance. For better IO performance, Intel Skylake SP processor provides a feature called Direct Data Input Output (DDIO), and Non-Uniform Input Output Access (NUIOA) when leveraged, gives a better performance.

3.1.2. Memory bandwidth
The commodity servers provide multiple channels with memory modules that have sufficient cycle time, but the latency is a major bottleneck. The servers support up to six channels to be populated with DDR4 2666 MHz of memory that gives a theoretical bandwidth of 119.21 GB/s per processor. Therefore, local memory attached to the same processor can be accessed at this rate. The major concern is DDR4 latency, which is of the order of 100 ns that acts as a major bottleneck in achieving high speed packet processing performance [48]. Therefore, one of the design considerations is to restrict the data movement within the NUMA node and make efficient use of layered cache available with each processor. The processors in the latest commodity servers like Intel Xeon Skylake SP Gold 6148 have two memory controllers with six memory channels. These processors are provided with around 32 KB L1 data cache, 20 MB L2 cache and 27.5 MB of L3 cache which have the latencies of the order of 2 ns, 5 ns and 18 ns respectively. A design that can manage cache locality becomes critical to ensure nil packet drop.

3.1.3. CPU cycles
The commodity NUMA servers have multi-processor and multi-core configurations to enable parallel computation. As mentioned above, the server under consideration has Intel Xeon Skylake Gold 2.4 GHz processor with 20 cores, which means 48 Giga clock cycles are available per second per NUMA node. Parallelizing the processing to exploit the multiple cores and to utilize the combined clock cycles will be an important design consideration, as the cycle budget of an individual core might not be enough to process packets at 100 Gbps line rate.

3.1.4. NUMA node separation
As described in Intel Xeon Scalable Platform Product brief document [42], Intel Skylake SP processors have on-chip integrated memory controllers connected to the memory modules inserted in the local slots, and also an integrated PCI-e controller connecting directly to a subset of the PCI-e slots. These two features respectively called NUMA and NUIOA, provide separation among the NUMA nodes and help in establishing separate packet processing pipeline localized on NUMA nodes. It is an important design consideration to make distinctions between local and remote resources. Direct Data IO (DDIO) is an additional feature available in this NUMA architecture by which Ethernet controller can make the processor cache as the primary destination and source of IO data instead of main memory. DDIO feature is available only for local socket IO, where the IO device, the core that interfaces with the IO device, and the memory are on the same NUMA node.

3.2. Processing requirements for 100 Gbps flow probe
The resource requirements for processing a given rate of network traffic are mainly impacted by packet rate i.e. packets per second, which in turn is affected by the size of packets. In this subsection, we analyze the processing requirements for 100 Gbps flow probe with respect to worst case and average case scenarios of packet rate.

3.2.1. Worst case scenario of maximum packet rate
The worst case scenario for 100 Gbps traffic processing in terms of the number of packets to be processed is when all the packets (Ethernet frames) are of 64 B size and the packet rate is 148.8 Mpps (the maximum packet rate for a 100 GbE link). The time available for per packet processing in this case is 6.72 ns which translates to 16 compute cycles per core of a 2.4 GHz processor. This available number of compute cycles per packet can be increased by using more number of cores to parallelize the processing. Thus, by using 20 such cores, which is very common in commodity servers, we get 320 cycles for processing each packet which is more than the processing budget estimated by Brouer et al. [4].

3.2.2. Average case scenario of typical internet like traffic
As per Schulze et al. report on Internet Study [46], average packet size in typical TCP/IP network is 556 B which is 8.5 times the worst case scenario of 64 B. This average packet size translates to a processing budget of around 2800 cycles per packet that is 8.5 times the budget available for 64 B packets. This scenario allows more clock cycles to perform CPU intensive operations on packets.

4. NUMA aware pipelined architecture for flow probe: NAPA-FP
In this section, first we briefly describe a flow probe, explain the approach taken to arrive at NAPA-FP and our software architecture for a high speed flow probe. Subsequently, NAPA-FP and its modules are described in detail along with the design decisions and optimization techniques applied.

4.1. Brief description of flow probe
A flow is defined in RFC 3917 [43] as a set of IP packets passing through an observation point in the network during a certain time interval, such that all packets belonging to a particular flow have a set of common properties. These common properties may include packet header fields such as source IP, destination IP, source port, destination port and protocol, collectively called the five-tuples. Flow probe is the main component of a flow analysis system, which collects and records information pertaining to the flows in a network. It is deployed at the gateway of the network for which the flow analysis is to be done as shown in Fig. 2. The exact information collected from each flow, called flow features, depends on the objective of the analysis being carried out and also on the sophistication of the flow probe and its configuration. Flow features can include the basic ones like the values of five-tuples or derived ones like statistical parameters. These flow features are exported from the probe to the collector using technologies such as Netflow [5] or IPFIX [47].

Fig. 2
Download : Download high-res image (65KB)
Download : Download full-size image
Fig. 2. Flow probe deployment scenario in passive mode.

4.2. Approach to development of NAPA-FP
Realizing a flow probe that can perform at the order of 100 Gbps requires an efficient software architecture with meticulous choice of algorithms and techniques optimized for every packet processing stage. The end-to-end packet processing path includes various hardware and software suites that manage the NIC buffer, PCI-e bus, system memory, cache and CPU. In case of flow probe it additionally includes the export path to collector as well. As brought out in the previous sections, our work is based on the multi-processor NUMA architectures available in the commodity servers. The feasibility analysis of the representative commodity platform in section 3, shows that the hardware components have enough resources for processing 100 Gbps traffic on each NUMA node when managed efficiently.

Optimizing a network application to achieve a desired performance is a complex process as trade-offs between different parameters need to be considered. Therefore, design choice involves selection of a suitable commodity server, its configuration parameters, OS kernel, driver and application design. As mentioned earlier, there have been published works in the form of techniques, frameworks and principles for specific usage or use cases like [25] [29] [13] [34]. In NAPA-FP our effort has been to combine them to design an architecture for high speed flow probe that can enable Internet traffic analysis. The identified techniques, frameworks and tuning parameters specific to the proposed architecture, are explained in section 5.

4.3. Design challenges
The distributed memory across the CPU sockets in NUMA architecture provides differential memory access latencies depending on whether the accessed memory is local to the processing CPU or is remote i.e. attached to a different CPU. The design challenge will be to create packet processing pipelines that are local to a NUMA node and tuned to achieve maximum performance. The design has to ensure that the network packets from a NIC inserted in one of the PCI-e slots connected directly to the PCI-e controller of a CPU, are processed exclusively only on that CPU using only the memory local to that CPU. Then the NUMA node comprising CPU, memory and NIC can be considered as a self-contained logical unit in which the complete processing happens. Multiple such units can be replicated on all the NUMA nodes of a single server to linearly scale the packet processing performance to 100 Gbps+ traffic rates. The cost of L1 cache access is 3 cycles, L2 cache access is 11 cycles, L3 cache access is 32 cycles, local node memory access is around 140 cycles and remote node memory access is 300 cycles approximately [33]. Therefore, due consideration should be given to reduce the costly memory accesses while designing for optimum performance.

4.4. Design of NAPA-FP
At an abstract level, our approach is to have a processing engine running exclusively on a single NUMA node handling 100 Gbps traffic from a NIC. The processing engine comprises multiple independent processing pipelines. Each of these processing pipelines are functionally decomposed into three major stages. Processing Engine has a Pipeline Spawner that sets up multiple packet receive queues from the NIC and spawns processing pipelines corresponding to each of these queues. Fig. 3 shows the architecture of NAPA-FP.

Fig. 3
Download : Download high-res image (122KB)
Download : Download full-size image
Fig. 3. Architecture of NAPA-FP.

Fig. 4 is a representation of one linear processing pipeline. All the required resources are local to the pipeline and as such there is no dependency with other pipelines. Functionally, this pipeline consists of three stages namely Packet Capture, Flow Construction and Flow Export. These stages in pipeline are implemented as separate threads and data is transferred between these threads. Linked list queue and fixed ring buffer queue are the two data structures commonly used in similar situations. A ring buffer queue uses a fixed length buffer that is allocated a-priori with a continuous block of memory. NAPA-FP uses ring buffer queue in its design because it performs better compared to the linked list queues. This is due to the continuous memory block, which results in spatial locality and better cache performance. In NAPA-FP there are two logical rings namely packet metadata ring between packet capture and flow construction stages and flow metadata ring between flow construction and flow export stages. It may be noted that copying packets degrades performance significantly in any packet processing architecture. Hence NAPA-FP is designed such that there is only one copying from NIC buffer to DMA buffer, which is unavoidable.

Fig. 4
Download : Download high-res image (62KB)
Download : Download full-size image
Fig. 4. One linear processing pipeline (PP) of NAPA-FP.

4.4.1. Packet capture module
This module is responsible for receiving the packets from NIC and making the packet metadata available to the next stages of processing. As the general purpose networking stack provided by the Linux kernel cannot capture at 100 Gbps rates, we need to have a high speed packet capture mechanism. In NAPA-FP we have used DPDK [26] as a framework for packet capture. The reasons for choosing DPDK and the various optimizations and tunings applied to DPDK are described in section 5.

Mellanox ConnectX-5 has RSS feature, which distributes the incoming packets into multiple receive queues in a flow aware manner. DPDK exposes these queues to user space application. These receive queues are implemented as lockless ring buffers, labeled as DPDK RX_RING in Fig. 4. As will be shown in the experiments section, the number of receive queues required for capturing 100 Gbps Internet-like traffic has been empirically estimated to be six in our validation experiments. A linear processing pipeline corresponding to each of the receive queue is spawned, and in each of these pipelines there is a packet capture stage implemented as a thread responsible for receiving packets from DPDK RX_RING.

Once the packets are received, the packet capture module extracts the relevant details required for flow construction and these details are forwarded to the flow construction module sharing the packet metadata ring. A dedicated CPU core is reserved for each packet capture thread. This thread is pinned to the assigned core, which reduces context switching keeping data local to the CPU cache.

4.4.2. Flow construction module
The flow construction module is responsible for construction of flows and harvesting of flow information. It uses a hash table for maintaining flow states and collecting flow information. Information from each packet is updated in the hash table by indexing into the corresponding flow location in the table using a hash key. The hash key is computed as a function of five-tuples namely, source IP, destination IP, source port, destination port and protocol. The procedure to find and update flow information is given in Algorithm 1. The hash table is one of the most performance critical parts of NAPA-FP, as it has to keep track of the life of each flow. It needs to be designed to hold hundreds and thousands of flow records, do fast inserts, lookups, updates, and manage record expiration after active and inactive timeouts. We have introduced two optimizations with respect to the hash table design and the RSS hash key usages as described in the sub-section 5.9.

Algorithm 1
Download : Download high-res image (99KB)
Download : Download full-size image
Algorithm 1. Find and update flow record for each packet.

In the prototype implementation, the following flow information were harvested: source IP, destination IP, source port, destination port, protocol, flow start time, flow duration, total number of bytes, total number of packets, average packet size, maximum packet size and minimum packet size. This information harvesting process is configurable and can collect additional parameters. However, in such cases the impact on performance in terms of memory bandwidth and CPU cycles needs to be assessed.

This module consumes an entry from packet metadata ring and performs a lookup in the hash table. On success, the corresponding node in the table is updated with the packet information. On failure, a new session node corresponding to this five-tuple is created and flow parameters are updated with the details from the packet metadata. Upon meeting any of the following conditions, sessions are considered as completed:

1.
Received end of flow signal i.e. FIN or RST packet in case of TCP

2.
Flow is inactive for 15 seconds

3.
Flow is active for more than 15 minutes

The timeout values, used above, are based on the average inter-packet time and session duration collected from Internet traffic. Finished sessions are cleared from the hash table and the corresponding flow information is transferred to the flow exporter using flow metadata ring structure. In cases 2 and 3 above, more packets corresponding to the same flow could arrive and those will be treated as belonging to a new session. In such cases, fusing multiple flows into one needs to be addressed by the flow collector.
4.4.3. Flow export module
The flow export module is responsible for exporting raw flow records received from flow construction module. There is one flow export module corresponding to each linear pipeline. NAPA-FP provides support for both the prominent flow export protocols, Netflow v9 and IPFIX over UDP. This module receives flow records from flow construction and constructs export template accordingly. The export template and the flow records are transferred to the designated flow collector, the address of which is to be configured. Flow collector is a standard software, installed in a separate system, that accepts flow records and stores in a database for further analysis.

As mentioned previously and as will be shown in the experiments section 6, six receive queues are sufficient for 100 Gbps packet capture. A linear processing pipeline comprising three stages namely packet capture, flow construction and flow export is spawned corresponding to each receive queue. The design allocates a dedicated core to each of the stages of each pipeline, thus requiring a total of 18 cores for running six processing pipelines. The processor being considered, Intel Xeon Skylake SP Gold 6148 with 20 cores, or any similar mid-range processor satisfies this core requirement.

5. Techniques and framework applied in NAPA-FP
In this section, we describe a set of techniques that have been identified as essential for achieving performance. Some of these techniques and principles have been developed and presented independently of each other in the literature. In our architecture we combine these techniques together and prove their significance in the context of high-speed packet processing.

5.1. Use of data and task decomposition
Evolution of multi-core, multi-processor systems has given the key to enormous processing power for application developers. To exploit this processing power, applications need to be highly parallelized, where the jobs are divided into smaller sub-tasks that can be executed in parallel. Two types of decomposition are applied to achieve parallelism, namely data decomposition and task decomposition. NAPA-FP uses two levels of data decomposition of which first level is done by using multiple NICs for a coarse-grained load decomposition. The second level of data decomposition is done by harnessing RSS features of NICs. Besides distributing network packets across multiple CPU cores, RSS feature maintains flow affinity. In NAPA-FP a task is decomposed into multiple linear pipelines as per the number of processors and available CPU cores. Each pipeline is completely independent of the other and has multiple stages that work concurrently.

5.2. NUIOA aware processing
In a multi-processor system there could be a number of PCI-e slots of different configurations, belonging to different processors. Though a NIC card can be inserted in any of the PCI-e slots, best performance is guaranteed only when the association of the card to the CPU where the packet processing takes place is ensured. This behavior is called Non-Uniform Input Output Access (NUIOA) as was briefly introduced in section 3. Furthermore, the DDIO feature available in Intel Skylake SP processor, for directly copying packets from NIC to cache, is best exploited when this association is in place. NAPA-FP provides an automated detection of incorrect placement by probing the sys file system in Linux and thereby ensures that packet processing functions, pertaining to a particular NIC, are executed in the same CPU where the NIC is connected.

5.3. NUMA aware processing
NUMA aware processing is all about writing programs in such a way that the data processed by a NUMA node is completely stored in its local memory and access to the non-local memory is reduced to the maximum extent possible. In a multi-processor system with different NUMA nodes, NUMA aware code provides better performance. There are numerous works [31] [12] explaining how the performance of software on NUMA systems can be improved by adopting NUMA aware programming or processing. The works by Pilla et al. [41] and Broquedis et al. [3] clearly show that for better performance, all threads that share data among themselves should be bound to same NUMA node. Such binding reduces high latency remote memory access and usages of limited UPI bandwidth. The same concept is utilized in NAPA-FP to achieve better performance. Packets received on an interface are processed in NUMA node to which the NIC is bound. The complete processing of the packet from packet capture to flow export is handled on the same NUMA node. Thus, by design, NUMA affinity is enforced.

5.4. Limited use of global memory
NAPA-FP makes limited use of global memory in order to improve performance. All the data structures like hash tables and buffer pools are maintained independently by each thread or in the worst case independently by each NUMA node. Given the large number of parallel threads of processing that is required to achieve the adequate packet processing performance, resorting to global memory would have reduced the complexity of design. However, even at the cost of increased complexity, we have ensured that each thread works on its own local variables or pointers thereby reducing the overheads of locking and in turn increasing the performance. This avoidance of global memory as a policy in the design has also resulted in making the design easily re-configurable or adaptable to different hardware platforms or workloads. When global memory is used in parallel applications, it needs to be locked during every write access. Use of a lock is a costly operation which needs to be avoided in parallel programming. Global memory interferes with cache coherence and reduces cache efficiency, as this memory space is accessed by different threads running in different processing cores.

5.5. Processing bidirectional flows together
It may be noted that forward and reverse flows of a TCP connection have certain common parameters like IP addresses, port numbers and protocols. Addressing these flows together reduces memory footprint as separate data structure need not be maintained for each flow. Additionally it improves cache hit ratio as both flows have temporal locality which is essential for better performance. By default RSS hash function produces different output values for forward and reverse flows and hence maps them to different packet processing pipelines. To achieve the desired property, RSS hash function has to produce the same output values for both flows. This was accomplished in NAPA-FP by leveraging the technique of manipulating RSS seed, proposed by Shinae Woo et al. [52]. However, as the flow collector requires unidirectional flows, the forward and reverse flows are exported separately by the flow export module.

5.6. CPU core affinity and real-time priority of threads
By default the scheduling of different processes or threads is handled by the operating system. Though the scheduling algorithms can optimally allocate threads to different cores, in case of process intensive tasks, better performance can be achieved by judiciously binding threads to specific cores. This core affinity ensures that the context switch overheads are reduced and the processing resources are used productively to the maximum extent. This has been applied in NAPA-FP by according real-time priority to the packet capture threads and binding them to their exclusive cores. The remaining threads are also bound to a set of cores on the same NUMA node as that of the packet capture threads. Also, if two threads operating on the same data are made to have affinity to the hyper-threaded cores on the same physical core, it can lead to an improved cache hit ratio because of shared data cache. This has been implemented in NAPA-FP to ensure that the flow construction and flow export threads that work on the same set of flows are bound to the two hyper-threads of the same core.

5.7. Pre-allocation and reuse of memory buffers
In NAPA-FP flow construction module, pre-allocated memory blocks are used for creating flow record linked list. During program initiation phase, multiple blocks of pre-allocated flow record nodes are created and organized as a free list. As and when required, a node is taken from the free list and used in hash table. Once the flow processing is done the same node is added back to the free list so that it can be reused.

5.8. Contiguous memory to achieve cache locality
Use of contiguous memory gives better cache hit ratio provided it is accessed sequentially. In designing NAPA-FP we tried to exploit this concept to the maximum. Information is passed from the producer thread to the consumer thread using lock-free rings. These lock-free rings are pre-allocated in contiguous memory space and are accessed sequentially. Furthermore, in designing the hash table for flow construction, contiguous buffers along with chaining are used. This has an obvious performance benefit which is assessed in sub-section 5.9.

5.9. Improving hash table performance
As mentioned in the previous section, hash key computation is a per packet operation that involves compute and memory access. This compute and memory access is non-negligible for traffic rates in the order of 100 Gbps. As NIC computes RSS-hash value for each packet to distribute them among different receive queues, this value is available as packet metadata from DPDK. We chose to use this value instead of computing the hash value afresh.

Hash table is the most performance critical part of the processing pipeline. Any bottleneck in this data structure will have non-marginal penalty in the complete pipeline. So we studied different hash table designs and finally developed an efficient data structure that best suits our flow probe. For NAPA-FP, we require a hash table that gives good performance, especially when the number of elements stored in hash table is greater than its size (load factor > 1).

Here we propose a hybrid hash table combining both probing and chaining strategy. The basic idea is that in order to resolve collisions, corresponding to each hash bucket, keep a horizontal array of size (T) as reserve for the items. Whenever there is a collision, look for an empty slot and accommodate the item in the array. If an empty slot is not available then add the item in linked list. Keeping the items in continuous array gives cache locality. However, this array should not be very long. Value of T needs to be identified experimentally such that 80% of the items are in continuous memory and the remaining in the linked list. Each bucket maintains a bit vector (V) of size T-bits which indicates the presence of items in the horizontal array. While searching for an item in the hash table we need to probe the horizontal array followed by linked list. Instead of probing each and every slot in the horizontal array we need to probe only those positions where the bit vector is set. We named this strategy as Horizontal Probing and Chaining (HPC) Hash Table.

Fig. 5 shows the HPC hash table used in NAPA-FP. Each bucket contains a continuous block of flow record array, a pointer to flow record linked list and a bit vector (V). An item hashed to a bucket, refers to bit vector (V) and occupies the first empty slot in the flow record array. If the flow record array is full, then a node is added to flow record linked list and the item is inserted. If an item is added to nth position in the flow record array, the corresponding bit in V is set. Removing an item requires resetting its corresponding bit position in the bit vector.

Fig. 5
Download : Download high-res image (64KB)
Download : Download full-size image
Fig. 5. HPC hash table implementation for flow construction.

To avoid contention, separate HPC hash tables are maintained for each processing pipeline. Each node in the HPC hash table corresponds to a network connection and maintains two structures to harvest upstream and downstream statistics separately, as according to the definition of flows [43], the upstream and downstream are considered as separate flows.

5.10. Use of huge pages
Linux kernel maintains tables that are used to map virtual memory addresses to physical addresses. For every page transaction, the kernel needs to load related mapping table entries. With small size pages, more number of pages and corresponding mapping table entries need to be loaded. This reduces performance of the system. Linux allows page size to grow up to 1 GB. With huge pages, finding an address is faster as fewer entries are needed in the Translation Look-aside Buffer (TLB) to provide memory coverage. In NAPA-FP we use multiple huge pages of 1 GB size. Huge pages are reserved in each NUMA node at booting time and are subsequently used for packet buffers, DPDK ring and packet metadata ring.

5.11. Disable real-time throttling
In general a real-time thread would completely engage the CPU if the scheduling principles are followed. This is a desired behavior, but at the same time any bugs in real-time threads would completely block the system. To prevent this from happening, there is a real-time throttling mechanism which makes it possible to limit the amount of CPU power that the real-time threads can consume. In NAPA-FP the packet capture module needs to run as a real-time thread to minimize packet drops. To ensure this real-time behavior, we disable Real-time throttling feature of the kernel.

5.12. Disable P-state and C-state
It is often noted that there is a trade-off between power-efficiency and performance. Power management mechanisms on modern CPUs aim at reducing power consumption when the system is idle. There are two ways to decrease the power consumption, of a processor, firstly by powering down subsystems and secondly by voltage/frequency reduction. This is accomplished by using C-states and P-states respectively [45]. When C-states are enabled, subsystem is powered down provided it is not executing anything. On the other hand P-states describe the CPU state where the subsystem is actually running, but it does not require full performance so the frequency it operates is decreased. This shutting down of subsystem or throttling of frequency is controlled by software subsystems called governors, which monitor system load and manage accordingly. The adjustments of power are usually load-related and are not real-time. Governors are generally slow (μ) to adapt depending on the processor and operating system version. For low latency real-time operation, using power efficiency slows down performance. Therefore, in our system we have disabled both C-state and P-state in BIOS as well as in kernel.

5.13. Frameworks used in NAPA-FP
While selecting a packet IO framework, our objectives were (a) It should be easy to integrate with our software stack, (b) It should support our hardware i.e. it should have support of Mellanox ConnectX-5 card, and (c) It should be capable of achieving the required packet capture performance with the chosen IO framework and hardware. Considering all these objectives, DPDK packet IO framework from Intel is chosen for NAPA-FP development.

DPDK is a packet IO framework that offers direct access to network interface and high performance. It comprises three main components namely the Poll Mode Drivers (PMD), memory management module and Environment Abstraction Layer (EAL). PMD uses the memory rings on the NIC to directly send and receive packets without interrupts, thus achieving high CPU utilization. The memory management unit of DPDK utilizes huge pages for the buffer management through its own memory allocator known as mempool. The mbuf library provides functionality to store and process the raw packet data in the memory blocks allocated from a mempool. EAL provides a unified way to initialize the central DPDK components. Using DPDK in its default state doesn't give line rate packet capture performance on a 100 Gbps network interface. The use of hugepages is implemented by DPDK library itself. Various other performance enhancement mechanisms mentioned above in this section were also applied to achieve the targeted performance.

6. Evaluation of the proposed architecture
The evaluation of the proposed architecture focuses on three major aspects. First, we evaluate the individual components that make the different stages of processing pipeline. Second, we evaluate the complete system against our targeted traffic rate. Third, we demonstrate the impact of different optimization techniques on NAPA-FP architecture and performance. A comparison of NAPA-FP against two other solutions is presented towards the end of the section.

6.1. Server configuration
The configuration of the specific hardware that was used in development of NAPA-FP is given Table 1. It is a Supermicro server with four 2.4 GHz Intel Xeon Gold 6148 processors. Each of the processor has 20 cores with Intel hyper-threading that allows a single core to execute two independent threads simultaneously. Thus, there are 40 logical cores per processor. Each of the cores has separate 32 KB of L1 data and instruction cache, 20 MB L2 cache and 27.5 MB of shared L3 cache per processor.


Table 1. Server specifications.

Component	Specification
Processor	4 x Intel Gold 6148, Skylake SP
# Cores	20 Cores HT
L1 Cache	32 KB (I) & 32 KB (D) per core, 8-way
L2 Cache	1 MB per core, 11-way
L3 Cache	27.5 MB per processor, 16-way
UPI bandwidth	10.4 GT/s
RAM	48 GB per NUMA node Total 192 GB
RAM speed	2666 MHz
# 100 GbE NIC	Mellanox ConnectX-5
# 1G NICs	Intel NIC
PCI spec	Gen 3 (speed 8 GT/s)
For the development and validation of NAPA-FP, we have used the commodity 100 GbE NIC Mellanox ConnectX-5 [35], a dual port NIC that sits on PCI-e x16 slots. The two ports in this dual port NIC can work simultaneously at 100 Gbps only with PCI-e x16 Gen 4 slots, which supports 256 GT/s (16x16 GT/s) and an effective bandwidth of 252 Gbps. However, as the server being used has only PCI-e Gen 3 slots and each x16 slot supports only a maximum bandwidth of 128 GT/s and an effective bandwidth of 126 Gbps, only one port per NIC is used in the experiments. The server has three PCI-e x16 Gen 3 slots that are connected directly to three different processors. All the three slots are populated with Mellanox ConnectX-5 NICs for handling  Gbps. The operating system of this platform is RHEL 8.0 with kernel version 4.18. DPDK version 19.11.5 is used as the packet IO framework.

6.2. Test-bed setup
In order to validate NAPA-FP architecture and the design principles, we used the Spirent traffic generator to generate near realistic Internet traffic based on profiles created from actual Internet traffic. Traffic profiles were generated based on traffic of our center and data from a study done by Paul Emmerich et al. [17]. We observed the Internet traffic of our center for a period of one week to obtain the packet distribution as given in Table 2. This shows that most of the packets in the traffic are either small <129 B or big >1024 B and that 90% of the flows are smaller than 32 KB in size. It also shows that http and https constitute a major chunk of the Internet traffic. The study by Paul Emmerich et al. reveals that for a 10 Gbps link with 80% utilization, there is an average of 666K parallel flows. This amounts to 832.5K flows for 10 Gbps. Secondly, they also observed 960M flows in a period of 24 hours. This implies an average flow rate of approximately 11K flows per second. Based on these profiles, the traffic generator was able to generate stateful traffic nearly at line rate (99.21 Gbps) with configurable application layer content. Furthermore, it also gives a fine grained control on the generated traffic rate, which is essential to profile the performance of NAPA-FP.


Table 2. Packet distribution of our organization internet traffic.

Packet size (bytes)	Packet count (%)	Volume (%)
<= 64	16.78	3.01
65-128	38.20	4.25
129-256	1.60	2.64
257-512	1.69	3.39
513-1024	4.66	6.53
1025-1518	37.07	80.18
Fig. 6 shows the experimental setup used in our testbed. Two 100 Gbps ports of traffic generator are connected to each other via a 100 Gbps switch, Extreme Networks SLX 9240 [18]. One of these ports is used as a server, while the other is used as a client, and the generator was configured to generate total bidirectional traffic of 100 Gbps. The port mirroring feature on the switch was configured to mirror the ingress traffic from both P1 and P2 to the 100 GbE NICs on the flow probe server. As this switch supports configuring multiple port mirroring sessions, traffic from P1 and P2 could be made available to multiple 100 GbE NICs in the flow probe. Hence all the NICs in the experiment were receiving copy of the same traffic sessions generated between the two ports of the traffic generator. The same traffic being supplied to the multiple NICs is a valid test as the traffic from each NIC is handled independently on a separate NUMA node and there is no aggregation of the input traffic inside the flow probe server.

Fig. 6
Download : Download high-res image (77KB)
Download : Download full-size image
Fig. 6. Test-bed setup for performance validation of flow probe.

6.3. Evaluation of packet capture module
As mentioned earlier, our packet capture module is based on DPDK version 19.11.5. In this section, we describe the evaluation of performance of a single 100 GbE NIC card connected to NUMA Node 0. Maximum load on this module is generated by 64 B packets. So we tried to benchmark this module by generating 100 Gbps rate using 64 B packets (148.8 Mpps) from Spirent traffic generator. Most of the performance measurements are done in terms of Packet Drop Ratio (PDR), which represents the ratio of the number of packets dropped to the total number of packets sent. It may be noted that there are various parameters that need to be tuned to capture 100 Gbps using 64 B packets. Impact of some of the key parameters on packet capture performance is evaluated in this section.

As there are no cross-talks between different NUMA nodes in NAPA-FP by design, we expect the processing capability of the probe to scale by adding additional NICs which bind to different NUMA nodes. This was validated by experimenting with three 100 GbE NICs available at the time of experiment. When additional NICs were added and bound to a different NUMA node, the traffic processing performance went up from 100 Gbps to 200 Gbps and 300 Gbps respectively.

6.3.1. Number of CPU cores for packet capture
To start with, we evaluated the number of CPU cores required to capture 100 Gbps traffic with 64 B packet sizes (148.8 Mpps) on NUMA Node 0. Traffic load of 148.8 Mpps was generated using Spirent traffic generator. Bar chart in Fig. 7 shows the number of logical CPU cores and PDR while capturing 148.8 Mpps traffic. Results show that with 12 CPU cores we can capture 148.8 Mpps with PDR of 0.04. A sudden drop was observed in the PDR when the number of cores and receive queues was set to 12. Analysis of this sudden drop highlighted the role of Multi Packet Receive Queue (MPRQ) feature of the Mellanox NIC in the performance. MPRQ or Striding Receive Queue is a feature in Mellanox NIC that handles burst of received packets with little PCI access [37]. Instead of transferring one packet at a time from NIC FIFO to DMA buffers, MPRQ transfers multiple packets at one instance using a single large buffer. This saves PCI-e bandwidth by posting a single large buffer for multiple packets and thereby improves receive performance especially when the packet sizes are small, because a single buffer can accommodate multiple small packets. During experiments it was discovered that MPRQ was getting activated only when the number of receive queues were set to 12. This was due to the default value set in the Mellanox driver. We further experimented by modifying the driver to invoke MPRQ for lesser number of receive queues. With this new setting, it was possible to achieve line rate packet capture with only 7 Rx queues for 100 Gbps traffic comprising 64 B packets. However, we decided to retain the default MPRQ setting for all our flow probe experiments because of two reasons. First, we discovered that when MPRQ is enabled the RSS hash value is not always available [38] to the application layer, especially when MPRQ is enabled keeping other parameters and options the same. Second, MPRQ is beneficial only when average size of packets is small, whereas the flow probe is intended for Internet traffic with much larger average packet size.

Fig. 7
Download : Download high-res image (140KB)
Download : Download full-size image
Fig. 7. Number of CPU cores vs PDR (packet size 64 B and packet rate 148.8 Mpps).

We also evaluated the number of CPU cores required to achieve minimum PDR for 100 Gbps traffic with different packet sizes. Traffic load of 100 Gbps was generated with packets of different sizes like 64 B, 128 B and so on. Fig. 8 shows the minimum number of CPU cores required to capture this traffic with all the standard tuning measures enabled. Also, it shows the number of cores required to capture Internet traffic (IMix) as per the profile mentioned in sub-section 6.2.

Fig. 8
Download : Download high-res image (141KB)
Download : Download full-size image
Fig. 8. Number of CPU cores required to capture 100 Gbps traffic with different packet sizes and minimum PDR.

6.3.2. DPDK RX descriptor ring size
DPDK uses RX Descriptor Ring [14], a lock-free ring structure to transfer packet information from NIC queues to packet capture thread. It is a First In, First Out (FIFO) queue that stores elements on a fixed-length array. As burstiness of traffic needs to be accommodated by this ring, the size of the ring i.e. the number of slots in DPDK RX_RING needs to be adjusted with empirical studies of network traffic. We evaluated the impact of ring size to PDR. Our experimental results, as given in Fig. 9, show that if the number of RX descriptors is 4096 per ring, with 12 CPU cores we are able to capture 148.8 Mpps with a PDR of 0.04.

Fig. 9
Download : Download high-res image (134KB)
Download : Download full-size image
Fig. 9. Number of slots per DPDK RX_RING vs PDR (packet size 64 B, packet rate 148.8 Mpps, DPDK cores 12).

6.4. Evaluating flow construction module
To achieve cache locality and also to handle load factor > 1 we have proposed HPC hash table with the feature of horizontal probing and chaining. Here we compare our proposed hash table data structure with that of simple chaining and then estimate the array size parameter (T) for the proposed hash table. For these experiments traffic is generated as per the Internet traffic profile mentioned in sub-section 6.2.

6.4.1. Evaluation of proposed hash table
Here we compare HPC hash table with that of chaining scheme. Comparison is in terms of amount of time (micro seconds) required to search for 1000 random flows in the hash table. The size of hash table is 215 buckets and the size of horizontal array (T) is 32. As shown in Fig. 10, with the increase in number of concurrent flows and nodes to be handled, the HPC hash table gives an order of 2x performance compared to the chaining scheme.

Fig. 10
Download : Download high-res image (138KB)
Download : Download full-size image
Fig. 10. Time (usec) taken to probe for 1000 packets in Chaining and in HPC hash table. (For interpretation of the colors in the figure(s), the reader is referred to the web version of this article.)

Next we estimated the horizontal array size T for the HPC hash table, as proposed in sub-section 5.9, such that 80% of hashed items are in continuous memory and the remaining in the linked list. Value of T depends on number of simultaneous flows that needs to be maintained in the hash table and the number of collisions against each bucket. Corresponding to each processing pipeline there is one flow construction thread and one flow hash table. For the purpose of estimating T, let us assume that one processing pipeline is able to process 20 Gbps traffic. Thus this hash table needs to maintain an average of 1660K flows at any instance. Considering 1660K flows, we studied the amount of collisions against each bucket. Graph in Fig. 11 shows the cumulative distribution function (CDF) of the collision values. The graph shows that 80% of the hashed items have a collision value less than or equal to 86. This means that 80% of the hashed flows can be accommodated in the contiguous horizontal array if we set the horizontal array size T to 86. Then we validated that this value of T is giving optimum performance by experimenting with different T values 32, 64 and 86. From the graph in Fig. 12 it is observed that HPC hash performance increases with the increase of horizontal array size T, from 32 to 86.

Fig. 11
Download : Download high-res image (118KB)
Download : Download full-size image
Fig. 11. CDF of collision in HPC hash table with table size 215.

Fig. 12
Download : Download high-res image (134KB)
Download : Download full-size image
Fig. 12. Comparison of HPC hash table performance w.r.t. horizontal array size (T).

6.5. Assessing complete system
Here we are assessing the performance of NAPA-FP on a single NUMA node and subsequently on multiple NUMA nodes. A mix of http, https and other application traffic, as per Internet traffic profile is generated between two ports of the traffic generator. The summary of hardware and software configuration and tuning parameters used for NAPA-FP are shown in the Table 3.


Table 3. Hardware/software configurations used for NAPA-FP.

Configuration setting
BIOS settings
C-state	Disabled
P-state	Disabled
Turbo boost	Disabled

Mellanox ConnectX-5 settings
MPRQ	Enabled (not invoked due to driver settings)

Operating system
Version	RHEL 8.0, kernel 4.18, x86_64

DPDK settings
Version	19.11.5 LTS
Huge pages	1 GB pages - 4 Nos.
Ring Size	4096 slots
# RxQ	12 (64 B pkts.), 6 (Internet Traffic)
6.5.1. Assessing performance on single NUMA node
In this experiment we consider one NUMA node and increase the number of processing pipelines one at a time till 100% traffic is processed comfortably. As mentioned in sub-section 6.2, 99.21 Gbps is the maximum rate at which we were able to generate stateful traffic between two 100 Gbps ports of Spirent traffic generator. This experiment was repeated in different NUMA nodes independently and average of the results is shown in Table 4. In NAPA-FP design, one linear pipeline constitutes packet capture thread, flow extraction thread and a flow export thread. Also, each of these threads runs on a separate and dedicated logical core of the same NUMA node. Augmenting the system with one processing pipeline leads to utilization of 3 CPU cores. Table 4 shows that with six processing pipelines and 18 dedicated CPU cores on a NUMA node we are able to collect flow records from about 100 Gbps realistic traffic without any packet drop. Analyzing this data it may be observed that traffic processing rate does not increase proportionally with the increase in number of processing pipelines. We attributed this non-uniformity to LLC contention. As the number of processing pipelines increases, the number of hash tables also increases which results in increase of memory footprint and cache contention.


Table 4. Number of processing pipelines and maximum processed traffic as percentage of generated traffic.

No. of pipelines	Processed traffic
1	30.21%
2	53.70%
3	72.67%
4	85.19%
5	92.44%
6	100%
To ensure that all the processing pipelines run on a single NUMA node, we used core affinity for all the threads. This was verified during the experiments using the top command in Linux, which gives a dynamic real-time view of the CPU usage by various running processes and threads. The top output showed that the entire processing was happening on a single NUMA node and all the remaining three NUMA nodes were entirely idle.

6.5.2. Assessing performance on multiple NUMA nodes
The same experiment was repeated with 3 NICs attached to 3 NUMA nodes. This experiment is conducted by adding one processing pipeline at a time to all the three NUMA nodes. Experimental results in Fig. 13 prove that with 18 processing pipelines and 54 CPU cores, distributed evenly among the NUMA nodes, we are able to process  Gbps traffic. These results support our claim that NAPA-FP is completely NUMA aware. Here we would like to mention that in spite of a free PCI-e x16 slot being available, we could not insert the fourth NIC due to some form-factor constraints because of an adjacent SAS controller. However, the fourth NIC can very well be accommodated with a riser board and the traffic from this card can be processed on the fourth NUMA node that was totally idle during the experiments with three NICs. Hence the experiments validate our claim that NAPA-FP is scalable and capable of handling traffic from as many as even four NICs.

Fig. 13
Download : Download high-res image (167KB)
Download : Download full-size image
Fig. 13. Number of processing pipelines on 3 NUMA nodes vs maximum processed traffic in Gbps.

6.5.3. Effectiveness of various optimization techniques
A set of experiments were conducted to evaluate the effectiveness of various optimization techniques used in NAPA-FP. These were done by turning off the different optimization techniques in the design one by one and then comparing the resultant performance with the original performance. To disable NUMA and NUIOA awareness, packets from NICs attached to NUMA node 0, node 1 and node 2 are processed in node 1, node 2 and node 0 respectively. For enabling global memory a global packet counter variable is used. As and when a packet is received it is incremented by corresponding packet capture thread. With regard to C-state and P-state, as referred in sub-section 5.12, the enabling is done in BIOS. The results are plotted in the graph in Fig. 14. Limiting the use of global memory is shown to provide a performance benefit of approximately 1.5x. Similarly, disabling C-state and P-state gives a performance benefit of around 5%-10%. The graph also conveys that without NUMA aware processing, the maximum traffic that can be processed by the server almost flattens beyond 150 Gbps. This flattening is due to the excessive cross-NUMA communications that saturates the capacity of UPI links.

Fig. 14
Download : Download high-res image (154KB)
Download : Download full-size image
Fig. 14. Impact of turning off various optimization techniques used in NAPA-FP.

6.6. Comparison with other flow probes
As discussed earlier in section 2, FloWatcher-DPDK [57] and Flowmon-DPDK [56] are flow probe solutions that report results of performance on 10 Gbps using Intel NIC and DPDK framework. Both these solutions require horizontal scaling of resources to process higher traffic rates. However, no advanced versions of these solutions with adequate details for handling 100 Gbps traffic have been published. The nProbe Cento [10] solution using Napatech FPGA-based card can process 132 Mpps on 2xIntel E5-2687w (3.1 GHz per core) using 10 cores [40]. Though this solution gives performance comparable to that of NAPA-FP, it is based on FPGA-based special purpose NIC. The drawback of such hardware-based solutions has been highlighted in section 1. The same solution using Intel E810-based 100 GbE adapters has been proposed recently. According to the solution website [27], it can handle 52 Gbps with 64 B packets using six receive queues. However, the technical details are not available for a proper comparison with our solution.

NAPA-FP fills the gap in the available technical literature by enumerating in detail a scalable architecture for 100 Gbps+ flow probe. It establishes that flow capture, extraction and export is feasible on commodity servers at 100 Gbps and at the same time scalable to multi 100 Gbps traffic rates through a carefully designed NUMA aware processing pipeline. NAPA-FP has also systematically identified and presented a number of optimization techniques, for various stages of the processing pipeline, which combine together have helped in achieving the performance. The scalability is demonstrated by processing  Gbps Internet-like traffic at line rate using three NUMA nodes and  GbE NICs. To the best of our knowledge this is the first work demonstrating with experimental results the feasibility of achieving up to  Gbps performance using commodity servers.

7. Conclusion
The main contribution of this work has been to show that it is feasible to develop high speed traffic analysis solutions capable of handling 100s of Gbps on commonly available and comparatively less expensive commodity compute platforms. Generally high speed traffic analysis is considered the realm of specialized and expensive hardware and beyond the capabilities of general purpose commodity hardware. This work highlights that the latest general purpose hardware has enough processing power, and can be fully exploited by appropriate optimization techniques.

As a first step, we identified and combined together the relevant software design techniques and best principles for different stages of a packet processing pipeline, resulting in an architecture and principles for designing a commodity server-based flow probe. Most part of this architecture, named NAPA-FP and the design principles are generic to all the traffic analysis solutions like Intrusion Detection System (IDS) and Network Behavior Anomaly (NBA) and hence can be easily extended to develop such solutions. The results clearly demonstrate that carefully crafted software processing pipeline which extracts maximum performance from each of the stages can improve the performance manifold. We have also presented the results of experiments using a prototype implementation of flow probe based on NAPA-FP architecture running on a server with four NUMA nodes (each comprising a processor with 40 logical cores and 64 GB of RAM). The experiments looked at the effectiveness of the techniques identified as important for designing flow probes, by measuring the applicability of techniques for flow probe performance. The results clearly validate the merits of the proposed architecture and design principles. We have evaluated and validated that using the proposed design architecture, line rate traffic of 100 Gbps from suitably configured NIC port can be processed using a single NUMA node. Using the 3 NICs available in the server we could process  Gbps, which confirms that traffic processing on each NUMA node is independent in the proposed design and thus the performance is scalable with additional NUMA nodes and NICs.