Abstract
Spiking neural networks (SNNs) have been getting more research attention in recent years as the way they process information is suitable for building neuromorphic systems effectively. However, realizing SNNs on hardware is computationally expensive. To improve their efficiency for hardware implementation, a field-programmable gate array (FPGA) based SNN accelerator architecture is proposed and implemented using approximate arithmetic units. To identify the minimal required bit-width for approximate computation without any performance loss, a variable precision method is utilized to represent weights of the SNN. Unlike the conventional reduced precision method applied to all weights uniformly, the proposed variable precision method allows different bit-widths to represent weights and provide the feasibility of maximizing truncation effort for each weight. Four SNNs adopting different network configurations and training datasets are established to compare the performance of proposed accelerator architecture using the variable precision method with the proposed one using the conventional reduced precision method. Based on the experimental results, more than 40% of the weights require less bit-width when applying the variable precision method instead of the reduced precision method. With the variable precision method, the proposed architecture achieves 28% fewer ALUTs and 29% less power consumption than the proposed one using the reduced precision method.

Previous
Next 
Keywords
Spiking neural network

Approximate computing

Field programmable gate array

Hardware accelerator

1. Introduction
Neuromorphic systems [27] demonstrate an attempt for the next generation of computer architecture by building computational blocks as neurons. Spiking neural networks (SNNs) [19], the 3rd generation of the artificial neural networks (ANNs), are widely used to realize neuromorphic systems [13], [16], [32]. Unlike conventional ANNs that lack timing dynamics, SNNs employ biologically-realistic models of neurons to mimic information processing in biology, where information is encoded with a spatio-temporal pattern and processed in a massively parallel fashion. The unique biological behavior on SNNs makes them suitable candidates for effective implementation of neuromorphic systems due to its low energy consumption and event-driven paradigm [23].

Similar to conventional ANNs, SNNs incorporate a collection of neurons that are connected in an acyclic graph and organized in layers (as shown in Fig. 1(a)). One obvious difference is that neural activation in conventional ANNs depends on non-linear continuous functions that only operate on a single propagation cycle, whereas SNNs operate using spike trains, which are time-dependent sequences, rather than continuous values. The occurrence of spike is determined when the membrane potential of that associated neuron exceeds a certain threshold. When a neuron spikes, the potentials of all its fanout neurons are incremented by corresponded weights. For example, as shown in Fig. 1(b), incoming synaptic paths consist of three different spike trains (
, 
, and 
), which control the movement of the membrane potential. The amount of movement depends on the weights (
, 
, or 
) on that path spiked. Besides, changes in membrane potential depend on the spiking neuron models used in SNNs, including the Hodgkin-Huxley model [12], the Izhikevich model [14], and the leaky integrate-and-fire model (LIF) [2].

Fig. 1
Download : Download high-res image (226KB)
Download : Download full-size image
Fig. 1. An example of an SNN architecture and an SNN neuron. (a) Architecture of a 4-layered SNN, (b) Spiking behavior of an SNN neuron with 3 incoming synaptic paths.

In recent years, many researchers showed promising results to bridge the gap between ANNs and SNNs in computer vision tasks such as image classification [6], or object detection [5]. Nevertheless, one of the limitations of SNNs is that simulating them on hardware is computationally intensive. For example, a neuromorphic system described in [18] adopts an SNN model to simulate the functionality of the visual cortex. To process a single input image, the system requires billions of computations. On the other hand, in SNNs, the number of computations dynamically changes based on the frequency of spikes. This brings challenges to realize parallel computation on commercial platforms such as CPUs [21], or GPUs [24]. Recently, some researchers demonstrate successful attempts on realizing SNNs on a specialized hardware platform such as a customized neural chip [26] or a very-large-scale integration (VLSI) system [9]. Despite improved parallelism on specialized hardware, heavy computation intensity is still one of the concerns that affects hardware energy efficiency.

With the growth in the size of SNNs, the computation intensity is expected to be increased rapidly. As computation intensity has impacts on energy efficiency, efficient SNN implementations are desired to handle the computation intensity issue. One recent study [28] exploits the possibility of software-level approximate computing for efficient SNN implementation. Based on the frequency on each neuron spiked, connections to the neurons that appear to be spiked less frequently are cut-off and thus no computation is required for the cut-off neurons. Nevertheless, there are some limitations in [28]. First, the cut-off scheme may be completely varied for different SNN architectures or for different time steps. Therefore, the cut-off scheme is not generalized. Statistical analysis has to be performed prior to generating the corresponded cut-off scheme for each case. Second, realizing the cut-off scheme brings extra hardware complexity to handle both normal and cut-off scenarios. Another fruitful direction for efficient SNN implementation is to use fewer bits to represent weights [35]. Compared to the baseline model, a 4-layered SNN using a 32-bit single-precision floating-point format, this method shows that there is no performance degradation by truncating all weights to a fixed-point format with 6-bit integer and 7-bit fraction. Moreover, more recent studies demonstrate the feasibility of improving energy efficiency by employing specialized SNN accelerators. For example, [20] uses Intel's Lohi neuromorphic chip to realize energy-efficient SNN. However, the main drawback of [20] is that the SNN implemented on the Lohi chip cannot maintain the same accuracy compared to the software implementation.

Approximate arithmetic units [1], [29], [30] have been widely applied in digital system designs to achieve improved efficiency. It applies inexact computation for the least significant bits of the input data. Compared to the conventional exact arithmetic unit, it will consume less area and power and can compute faster [15]. For example, the authors in [22] apply approximate adders to reduce computational complexities of image processing algorithms implemented on field-programmable gate array (FPGA). Due to these advantages, many error-resilient applications use approximate arithmetic units to reduce their hardware cost. Neural networks, such as the SNN, are one of the error-resilient applications. The use of an approximate multiplier in a multi-layer perceptron has been explored in [17]. Additionally, one recent study [10] demonstrates the usefulness of applying approximate arithmetic units to a convolutional neural network (CNN). The use of approximate computing in SNN has been explored in [33][34]. A truncated multiplier is designed for SNN computation in [34]. In [33], the approximate adder is implemented by breaking down the long carry chain and it achieved reduced delay. However, for FPGA-based approximate adder, the design method with more resource reduction can be investigated.

In this paper, an energy-efficient accelerator architecture is proposed for SNNs, combining both concepts of approximate computing and the reduced precision method. First and foremost, the proposed architecture aims to improve the truncation effort on SNNs where different layers use different precision formats. Unlike one of previous studies [35], which employ the reduced precision method to truncate weights uniformly, a variable precision method is introduced to maximize the truncation effort on SNNs. As the neurons that are less likely to be spiked are more error-resilient [28], it is expected that the weights used to accumulate the membrane potentials of those neurons can be further truncated without losing overall accuracy. On that basis, in the proposed architecture, approximate arithmetic units are applied, which is appropriate to handle variable precision requirements while reducing computation intensity compared to exact arithmetic units. To identify the proposed variable precision that generalizes different SNN architectures, a series of statistical analysis is performed on four SNNs. Specifically, the proposed variable precision method is demonstrated on a 3-layered and a 4-layered SNNs trained on the MNIST dataset [38]. Moreover, a 4-layered and a 5-layered SNNs trained on the Fashion-MNIST dataset [37] are investigated. By utilizing approximate arithmetic units with proposed variable precision, the proposed FPGA based SNN accelerator is implemented on Intel's Arria-10 SoC Development Kit. Based on our experimental results, more than 40% of the weights requires fewer bits after applying the conventional reduced precision method. By adopting approximate arithmetic units, the proposed SNN accelerator consumes 29% less energy than the one using exact arithmetic units on FPGA.

The major contributions of the paper are summarized as follows:

•
A statistical analysis-based variable precision method to find the minimum numeric precision required by each layer of an SNN model; this reduces the SNN's hardware implementation cost.

•
Based on the analysis of several SNN models, the optimal numeric formats that fit most SNN models available in the literature are proposed.

•
A hardware architecture for an SNN implementation that utilizes approximate adders is proposed. Synthesis results show the proposed SNN hardware architecture achieves less energy and resource consumption compared to a baseline SNN design.

The rest of the paper is organized as follows: Section 2 presents the background information of spiking neural networks. The software simulation process to determine the variable precision required by SNN is presented in Section 3. In Section 4, the corresponding hardware accelerator architecture with approximate arithmetic units is presented. Section 5 shows the implementation results and their analysis. Finally, Section 6 concludes the whole paper.

2. Spiking neural networks: preliminaries
The architecture of the SNN is similar to ANN, where neurons are organized into layers. The neurons in-between adjacent layers are connected with a fully-connected method (as shown in Fig. 1(a)). As the ANN propagates spatial information in the layer-by-layer, the output of a neuron is generated right after receiving pre-synaptic input from neurons in the previous layer. In contrast, neurons in the SNN process information in the temporal domain, where a time series of spike trains describes the output patterns of neurons in SNN. There are three popular neuron models at present to describe the spiking behavior of SNNs, including the Hodgkin-Huxley model [12], the Izhikevich model [14], and the leaky integrate-and-fire model (LIF) [2]. It is known that LIF is the most commonly used neuron model due to its low computation complexity and simple hardware implementation. In the LIF model, the neuron updates its membrane potential (u) by the following equation:(1)
 
 where 
 is the membrane potential of the neuron i at time t and  denotes the pre-synaptic input which is determined by spiking activities of incoming synaptic paths at time t and synaptic weights. For example, as shown in Fig. 1(b), when the first two incoming synaptic paths are spiked at the same time, the pre-synaptic input is equal to the sum of the synaptic weights 
 and 
. The membrane potential u continuously updates until it exceeds a given threshold. Thereafter, the neuron i fires the spike and resets the membrane potential u.

In this paper, four baseline SNN models are used to justify the proposed variable precision method. A 3-layered (MNIST-3) and a 4-layered (MNIST-4) SNNs are trained and tested on the MNIST database [38]. A 4-layered (FASHION-4) and a 5-layered (FASHION-5) SNNs are implemented and evaluated on the Fashion-MNIST database [37]. Both databases are used as the classification benchmarks of previous SNN implementation [6], [11]. To match the input image size (28 × 28), the input layer is constructed by 784 neurons whereas the output layer consists of 10 neurons to match the number of classes on both databases. Depending on the total number of layers used in the model, one or many hidden layers are inserted in-between the input layer and the output layer. Each hidden layer consists of 1200 neurons. The model configurations and classification performances are shown in Table 1. All baseline models use the LIF neuron model, where the spike is determined by a binary value (0 or 1), the membrane threshold is set to 1, and 35 cycles to terminate membrane update. To train the baseline models in this study, we follow the training scheme as described in [6].


Table 1. Layer configurations and testing accuracies of baseline models.

Number of layers	Number of hidden layers	Neuron configuration	Testing accuracy (%)
MNIST-3	3	1	{784-1200-10}†	98.32
MNIST-4	4	2	{784-1200-1200-10}	98.44
FASHION-4	4	2	{784-1200-1200-10}	86.76
FASHION-5	5	3	{784-1200-1200-1200-10}	87.36
† denotes a 3-layered SNN, which the first layer consists of 784 neurons, the second layer consists of 1200 neurons, and the last layer consists of 10 neurons.

3. Proposed variable precision methodology
We focus on how to reduce computation intensity on SNNs by taking advantage of approximate computing. In this section, we propose a variable precision method that enables us to apply approximate computing for SNNs. To this end, the subsection 3.1 gives the concepts behind the proposed variable precision method; the subsection 3.2 describes the way to realize variable precision-based approximate arithmetic units; subsection 3.3 gives the details of identifying variable precision method on different baseline SNN models.

3.1. Precision requirements of SNNs
To address the computation intensity issue imposed by SNNs, a simple method is to reduce the bit-length of weights on SNNs. Instead of using a conventional 32-bit single-precision floating-point format as the default numerical precision for all weights of the baseline models, we first convert all weights to a fixed-point format. By following the reduced precision method as described in [35], we found that the weights of the baseline models can be truncated to 13-bit fixed-point numbers. As shown in Fig. 2, a fixed-point format with 6-bit integer and 7-bit fraction ensures all baseline models to achieve the same accuracy as the ones using 32-bit single-precision floating-point format. Compared to the default numerical precision format, the truncated weights require less memory footprint, computing speed, and resource utilization. Nevertheless, the truncation effort is not maximized as truncation is uniformly performed across all the weights. To increase truncation effort, one fruitful direction is to further reduce bit-width of weights in-between two adjacent layers while the remaining weights follow original reduced precision method. As shown in Fig. 3, without loss of MNIST-3 and MNIST-4 models' accuracies, the fraction bit-width of the weights in-between the first layer and second layer can be further reduced from 7-bit to 6-bit. Another example is shown in Fig. 4. For MNIST-3, weights connected to the second layer and the third layer can be truncated to 11-bit length (6-bit integer and 5-bit fraction) without reducing its inference accuracy. Similarly, the rest of the baseline models can use a fixed-point format with 6-bit integer and 6-bit fraction to represent the weights in-between the second layer and the third layer. While this method improves the truncation effort for the models employing the reduced precision method, it is still challenging to obtain a generalized numerical format that fits different SNNs' architectures. On the other hand, it is difficult to identify the optimal layer-wise numerical format as a single SNN model may produce several possible combinations as shown in Fig. 3 and Fig. 4.

Fig. 2
Download : Download high-res image (178KB)
Download : Download full-size image
Fig. 2. Inference accuracies of baseline models using fixed-point format with different fraction bit-width and 6-bit integer.

Fig. 3
Download : Download high-res image (175KB)
Download : Download full-size image
Fig. 3. Inference accuracies of baseline models where weights connected to the first layer and the second layer use different fraction bit-width and 6-bit integer and remaining weights use 7-bit fraction and 6-bit integer.

Fig. 4
Download : Download high-res image (177KB)
Download : Download full-size image
Fig. 4. Inference accuracies of baseline models where weights connected to the second layer and the third layer use different fraction bit-width and 6-bit integer and remaining weights use 7-bit fraction and 6-bit integer.

To maximize truncation effort for each weight, the proposed variable precision method considers the spiking sequence of each neuron. When a neuron generates a spike, all its fanout weights are used to update the membrane potentials of the neurons in the next layer. For a neuron spiked frequently, its fanout weights are more likely to be used for the accumulation of membrane voltages compared to a neuron spiked less often. As shown in Fig. 5, neurons in the second layer of MNIST4 generate fewer spikes when the weights are truncated from 13-bit (6-bit integer and 7-bit fraction) to 11-bit fixed-point numbers (6-bit integer and 5-bit fraction). This concludes that changing the spiking sequences of neurons results in changing model performance. To maintain the spiking sequence of each neuron to be similar as the one in the original model, the proposed variable precision uses 13-bit fixed-point format (6-bit integer and 7-bit fraction) to represent the weights connecting to the neuron spiked often while the numerical precisions of remaining weights are further reduced. The proposed variable precision is advantageous primarily due to the following two aspects: 1) truncation effort is maximized as the reduced operation is performed based on neuron-level instead of layer-level or model-level. 2) it is easy to be realized in hardware with approximated arithmetic units, where exact computation units are used for the neurons spiking frequently and approximated computation units are used for the neurons spiking less often.

Fig. 5
Download : Download high-res image (54KB)
Download : Download full-size image
Fig. 5. Number of spikes generated by neurons in the second layer when the weights of MNIST-4 using a fixed-point format with different fraction bit-width and 6-bit integer.

To identify the proposed variable precision method on SNNs, an activity level (α) is associated with each neuron. The activity level (α) is a ratio (a value between 0 and 1) to specify the significant number of spikes for a neuron actively contributing to the overall spikes in its corresponded layer. The significant number of spikes for a neuron can be obtained based on the histogram of the spiking sequences of its corresponded layer. For example, a histogram of spiking activities for the neurons in the second layer of MNIST4 is shown in Fig. 6. The dashed line indicates an activity level (α) of 0.2. In this case, α of 0.2 indicates that 20% of all neurons in the corresponding layer have the number of spikes equal to or less than the significant spike number of 3589. When the activity level is set, the proposed variable precision method is applied to all incoming weights. As shown in Fig. 6, the neurons with a large number of spikes fall on the right side of the dashed line represent 80% of total neurons spiked most frequently. When an α is set to 0.2, the incoming weights that connect to 80% of total neurons spiked most frequently are truncated to 13-bit fixed-point numbers with 6-bit integer and 7-bit fraction. For the rest of the neurons, the fraction bit-width of its incoming weights are further reduced to n-bit (n<7-bit). The details of identifying the variable precision method on different baseline SNN models are described in subsection 3.3.

Fig. 6
Download : Download high-res image (87KB)
Download : Download full-size image
Fig. 6. Histogram of neurons spiked in the second layer of MNIST-4. The dashed line indicates activity level (α) of 0.2.

Algorithm 1 presents the pseudo-code to apply the proposed variable precision method on an SNN. To identify the variable precision format of jth neuron in ith layer 
, the significant spike number s is obtained by placing a line of activity level α on the histogram of neurons spiked in ith layer. If the jth neuron spikes more than the significant spike number, the jth neuron uses 6-bit integer and 7-bit fractions. Otherwise, the jth neuron uses 6-bit integer and n-bit fractions (n<7-bit).

Algorithm 1
Download : Download high-res image (80KB)
Download : Download full-size image
Algorithm 1. Identifying variable precision format (VPF).

3.2. Approximate adder for proposed variable precision method
There are many approximate adder designs available in the literature [15][7]. These designs focus on the resource reduction at the gate level and can achieve great performance in application-specific integration circuit (ASIC) arithmetic unit design. However, for an FPGA device, as logic operations are realized by look-up table (LUT), these ASIC based approximate arithmetic units may not perform well when being mapped to FPGA devices [24]. To solve this problem, in [24][4][8], FPGA-specific approximate adder architectures are proposed. Those adders are designed based on LUT in Xilinx FPGA [31]. Either the logic to generate sum or the logic to generate output carry is simplified to reduce the amount of LUT. In this paper, our target device is Intel Arria-10 FPGA. Therefore, we follow the design method in [24] but modify it to fit into the adaptive look-up table (ALUT) in Intel FPGA [3].

The truth table of an exact adder to generate sum (S) and output carry (
) is shown in Table 2. Implementing this 1-bit adder in FPGA will consume two LUTs, as shown in Fig. 7(a), where one LUT is for the calculation of S and the other one is for 
. By modifying the truth table of the addition, the approximation computation can be realized. As shown in Table 2, for S, we still perform the exact computation. However, 
 is directly connected to input A without any other logic. By implementing this change, errors will only happen two of the eight cases. However, it will reduce the LUT consumption from 2 to 1 as shown in Fig. 7(b). In the proposed design, the architecture in Fig. 7(a) will be used as exact cell while that in Fig. 7(b) will be used as approximate cell.


Table 2. Truth table of exact and approximate adders.

Input operands	Exact addition	Approximate addition
A	B	Cin	Cout	S	Cout	S
0	0	0	0	0	0	0
0	0	1	0	1	0	1
0	1	0	0	1	0	1
0	1	1	1	0	0	0
1	0	0	0	1	1	1
1	0	1	1	0	1	0
1	1	0	1	0	1	0
1	1	1	1	1	1	1
Fig. 7
Download : Download high-res image (20KB)
Download : Download full-size image
Fig. 7. LUT usage of exact adder and approximate adder. (a) Exact adder, (b) Approximate adder.

3.3. Determining variable precision method
To identify the optimal variable precision method for different baseline models, the classification performances of baseline models are evaluated by considering various activity levels (α) and different bit-width combinations of variable precision method. Specifically, the activity levels (α) are in the range from 0.2 to 0.9 with a step size of 0.1. Based on the value of α, a 13-bit fixed-point format with 6-bit integer and 7-bit fraction is used to represent the weights that connect to the neurons having activity levels (≥α). The remaining weights are converted to fixed-point numbers with 6-bit integer and n-bit fraction (n = 4-bit, 5-bit, 6-bit). However, one obvious challenge is to optimize both activity level and bit-width combination of variable precision at the same time. To overcome this issue, a parameter named the hardware cost (J) is established to estimate the hardware cost of an SNN layer employing the proposed variable precision method. The hardware cost (J) of a layer is obtained by the following equation:(2)
 
 where 
 is the hardware cost of the layer i and 
 denotes the activity level of the layer i. Depending on the value of 
, 
 represents the fraction bit-width used for the weights that connect to the neurons having activity levels (
). The denominator is associated with the hardware cost of using a fixed-point format with 7-bit fraction. As described in subsection 3.2, the LUT consumption of an exact adder is 2. Hence, a 7-bit exact adder requires 14 LUTs. Similarly, the numerator represents the hardware cost of using the proposed variable precision. For neurons having activity levels (
), the hardware cost is equal to 
. As the incoming weights that connect to the rest of neurons have a fixed-point format with 
-bit fraction, the hardware cost of applying the exact adder is 
 while the hardware cost of the approximated part is 
. It is noted that 
 of 1 indicates that the activity level (α) is 0 and the incoming weights that connect to the neurons in the layer i use 13-bit fixed-point format with 6-bit integer and 7-bit fraction.

To determine the optimal activity levels (α) and fraction bit-widths (n) for different SNN models, different values of α and n are tested based on a layer-by-layer scheme. For an example of a 3-layered SNN, the variable precision method of the weights that connect to the first layer and the second layer is identified before determining the variable precision method for the weights between the second layer and the third layer. By following the layer-by-layer scheme, the baseline models using the proposed variable precision method are identified as shown in Table 3. In summary, without reducing classification accuracies of all baseline models, the weights that connect to the input layer and the first hidden layer follow the proposed variable precision method where n is 5-bit and α is 0.6. For the weights between two adjacent hidden layers, n is 5-bit while α is 0.5. For the weights that connect to the last hidden layer and the output layer, n is 4-bit while α is 0.6.


Table 3. Classification accuracies of baseline models using proposed variable precision method with different activity levels, α and fraction bits, n.

Model name	Weight connections	Fraction bit-width (n)	Activity level (α)	Hardware cost (J)
From	To	0.9	0.8	0.7	0.6	0.5	0.4	0.3	0.2
MNIST-3	Layer 1	Layer 2	4-bit	0.961†	0.978	0.991	0.997	0.999	1.000	1.000	1.000	0.914
5-bit	0.996	0.999	1.000	1.000	1.000	1.000	1.000	1.000	0.900
6-bit	1.000	1.000	1.000	1.000	1.000	1.000	1.000	1.000	0.929

Layer 2	Layer 3	4-bit	0.984	0.990	0.997	1.000	1.000	1.000	1.000	1.000	0.871
5-bit	0.998	0.998	1.000	1.000	1.000	1.000	1.000	1.000	0.900
6-bit	0.999	1.000	1.000	1.000	1.000	1.000	1.000	1.000	0.943
MNIST-4	Layer 1	Layer 2	4-bit	0.955	0.966	0.988	0.994	0.999	1.000	1.000	1.000	0.914
5-bit	1.000	1.000	1.000	1.000	1.000	0.900	1.000	1.000	0.900
6-bit	1.000	1.000	1.000	1.000	1.000	0.929	1.000	1.000	0.929

Layer 2	Layer 3	4-bit	0.859	0.918	0.950	0.978	0.990	1.000	1.000	0.936	0.936
5-bit	1.000	1.000	1.000	1.000	1.000	0.914	1.000	1.000	0.914
6-bit	1.000	1.000	1.000	1.000	1.000	0.943	1.000	1.000	0.943

Layer 3	Layer 4	4-bit	0.993	0.997	1.000	1.000	1.000	1.000	1.000	0.871	0.871
5-bit	0.989	0.995	1.000	1.000	1.000	0.900	1.000	1.000	0.900
6-bit	1.000	1.000	1.000	1.000	1.000	0.943	1.000	1.000	0.943
FASHION-4	Layer 1	Layer 2	4-bit	0.890	0.919	0.945	0.956	0.962	0.972	0.982	1.000	0.957
5-bit	0.988	0.989	0.993	1.000	1.000	1.000	1.000	1.000	0.914
6-bit	1.000	1.000	1.000	1.000	1.000	1.000	1.000	1.000	0.929

Layer 2	Layer 3	4-bit	0.908	0.961	0.980	0.990	0.994	1.000	1.000	1.000	0.914
5-bit	0.985	0.989	0.997	1.000	1.000	1.000	1.000	1.000	0.914
6-bit	0.992	0.994	1.000	1.000	1.000	1.000	1.000	1.000	0.950

Layer 3	Layer 4	4-bit	0.986	0.990	1.000	1.000	1.000	1.000	1.000	1.000	0.871
5-bit	0.990	0.997	1.000	1.000	1.000	1.000	1.000	1.000	0.900
6-bit	1.000	1.000	1.000	1.000	1.000	1.000	1.000	1.000	0.929
FASHION-5	Layer 1	Layer 2	4-bit	0.710	0.822	0.894	0.927	0.939	0.959	0.991	1.000	0.957
5-bit	0.979	0.994	0.996	1.000	1.000	1.000	1.000	1.000	0.914
6-bit	1.000	1.000	1.000	1.000	1.000	1.000	1.000	1.000	0.936

Layer 2	Layer 3	4-bit	0.669	0.844	0.946	0.968	0.984	0.991	1.000	1.000	0.936
5-bit	0.953	0.983	0.993	0.997	1.000	1.000	1.000	1.000	0.929
6-bit	0.998	1.000	1.000	1.000	1.000	1.000	1.000	1.000	0.943

Layer 3	Layer 4	4-bit	0.642	0.915	0.964	0.977	0.983	0.998	1.000	1.000	0.936
5-bit	0.953	0.970	0.980	0.995	1.000	1.000	1.000	1.000	0.929
6-bit	0.986	1.000	1.000	1.000	1.000	1.000	1.000	1.000	0.943

Layer 4	Layer 5	4-bit	0.981	0.992	0.993	0.994	1.000	1.000	1.000	1.000	0.893
5-bit	0.983	0.995	0.998	1.000	1.000	1.000	1.000	1.000	0.914
6-bit	1.000	1.000	1.000	1.000	1.000	1.000	1.000	1.000	0.929
† denotes normalized accuracy of MNIST-3 model when the weights between the first layer and the second layer use the proposed variable precision method with α of 0.9 and n of 4-bit.

Italics indicates optimal values of α that achieve the least hardware cost (J) when n is 4-bit, 5-bit or 6-bit.

Underline indicates the optimal combination of α and n that achieve least hardware cost (J) for the incoming weights of that layer, where the value of the least hardware cost is bold.

As a summary of the software simulation results, for the SNN models used in our experiments, the computation between layer 1 and layer 2 can be done with 60% 5-bit computation ( and ), while the computation between layer 2 and layer 3 and that between layer 3 and layer 4 can be done with 50% 5-bit computation ( and ). For the last sets of connections in the FASHION-5 model, although the precision can be reduced to 4-bit, we still consider to use 5-bit as the reduced precision in hardware implementation since the computations of other layers still need 5-bit computations and most of the SNN models available in the literature still use 3-layer or 4-layer configurations. According to Table 3, 60% of the computations can be done with 5-bit fraction. Therefore, to accomplish SNN operations, computations with both 5-bit and 7-bit fractions are required.

3.4. Variable precision method on convolutional spiking neural network
Recent studies [25], [36] demonstrated the flexibility of transforming convolutional neural networks (CNNs) into convolutional spiking neural networks (CSNNs). Compared to SNNs, CSNNs illustrated better classification performance on more challenge datasets such as CIFAR or ImageNet. Since convolution operation uses the same weight matrix slide over input data more than one time, each weight may be associated with server input points showing different spiking activities. Hence, the proposed variable precision method cannot be utilized to convert convolutional (Conv) weights directly. Nevertheless, since a typical CSNN consists of one or many fully connected (FC) layers, the FC weights can be converted to the proposed variable precision format. To reduce computation complexity in CSNNs, a reduced precision method can be applied to all Conv layers to minimize the size of the weights while the proposed variable precision method can be applied to all FC layers to enable approximate computing.

To identify the usefulness of the proposed variable precision method on CSNN, a series of experiments were performed. First, a 5-layered CSNN was trained by using the CIFAR-10 dataset. The CSNN consists of 2 Conv layers followed by 3 FC layers. Both Conv layers have kernel size of 3 × 3 and output 64 feature maps. A 2 × 2 maximum pooling layer is inserted after each Conv layer. The first FC layer and second FC layer contain 128 and 64 neurons respectively. The last FC layer consists of 10 neurons to match the number of output classes. The predicting outcomes of the CSNN are normalized via SoftMax. To simulate spiking activities of the CSNN, we use SNNToolBox [25], an open-source SNN simulation tool, to perform temporal pattern simulation. We set 35 cycles to terminate the simulation process of each sample. The trained CSNN achieved an accuracy of 0.7281 on the CIFAR-10 testing dataset. By following the reduced precision method as described in subsection 3.1, the weights of the CSNN can be truncated to 13-bit fixed numbers (6-bit integer and 7-bit fraction). Then, by following Algorithm 1, all weights associated with FC layers were converted to the proposed variable precision format. As shown in Table 4, without downgrading classification accuracy, 70% of weights connecting to the last Conv layer and the first FC layer can be further truncated to 11-bit fixed-point format with 5-bit fraction (n = 5-bit and α = 0.7). Regarding the rest of the weights associated with the second FC layer and the third layer, 70% of the weights can use 10-bit fixed-point format with 4-bit fraction. Therefore, regarding the CSNN architecture, the computations in FC layers can be approximated via the proposed variable precision method while a reduced precision method can be applied to Conv weights to reduce the computational complexity in Conv layers.


Table 4. Classification accuracy of a 5-layered CSNN using proposed variable precision method with different activity levels, α and fraction bits, n.

Weight connections	Fraction bit-width (n)	Activity level (α)	Hardware cost (J)
From	To	0.9	0.8	0.7	0.6	0.5	0.4	0.3	0.2
Last Conv	1st FC	4-bit	0.921†	0.949	0.968	0.977	0.988	1.000	1.000	1.000	0.914
5-bit	0.963	0.964	1.000	1.000	1.000	1.000	1.000	1.000	0.9
6-bit	0.980	1.000	1.000	1.000	1.000	1.000	1.000	1.000	0.943

1st FC	2nd FC	4-bit	0.980	0.996	1.000	1.000	1.000	1.000	1.000	1.000	0.828
5-bit	1.000	1.000	1.000	1.000	1.000	1.000	1.000	1.000	0.871
6-bit	1.000	1.000	1.000	1.000	1.000	1.000	1.000	1.000	0.936

2nd FC	3rd FC	4-bit	0.989	0.989	1.000	1.000	1.000	1.000	1.000	1.000	0.828
5-bit	0.996	1.000	1.000	1.000	1.000	1.000	1.000	1.000	0.886
6-bit	1.000	1.000	1.000	1.000	1.000	1.000	1.000	1.000	0.914
† denotes normalized accuracy of the 5-layered CSNN when the weights between the last convolutional (Conv) layer and the first fully-connected (FC) layer use the proposed variable precision method with α of 0.9 and n of 4-bit.

Italics indicates optimal values of α that achieve the least hardware cost (J) when n is 4-bit, 5-bit or 6-bit.

Underline indicates the optimal combination of α and n that achieve least hardware cost (J) for the incoming weights of that layer, where the value of the least hardware cost is bold.

4. FPGA implementation of SNN with approximate computing
For better performance and energy efficiency, a hardware accelerator is usually desired for SNN implementation. In this section, an FPGA based SNN accelerator architecture is proposed. Approximate arithmetic units are newly introduced in the proposed accelerator by utilizing the findings from software simulation.

The overall architecture of the proposed FPGA accelerator is shown in Fig. 8. The whole architecture is designed targeting Intel's Arria-10 SoC Development Kit. It contains 1) a neuron buffer module, 2) a weight buffer module which are used to temporarily store neuron values and weight values respectively, 3) a neuron status register module which is used to store the firing status of the neurons in the current layer, 4) a processing element module which is responsible for the main computation, 5) a controller module which coordinates the operations of all the modules. In addition, as the on-chip memory is not large enough to cache all the weights, an external DDR memory is used to store all the model parameters and part of them are loaded to the on-chip memory during computation as required. The processing element module is designed with approximate adder. As both 5-bit and 7-bit fractions are required for SNN computation, when implementing 5-bit operations, the fraction data still keeps 7-bit while the least significant 2-bit are computed with approximate cells as discussed in Section 3.2.

Fig. 8
Download : Download high-res image (65KB)
Download : Download full-size image
Fig. 8. Diagram of the proposed SNN accelerator architecture with approximate computing.

4.1. The processing element with approximate adders
4.1.1. Approximate adder for the proposed FPGA architecture
By using exact cell and approximate cell, multi-bit adders are built. The resource consumption of some adders is shown in Table 5, where prefix E represents exact adder and prefix A represents approximate adder, the first number represent the bit-width and the possible second number represents the number of least significant bits calculated with approximate cells. For example, for A16-4, the least significant 4-bit are calculated with approximate cells and the remaining 12-bit are calculated with exact cells. Two 13-bit designs are also implemented because they will be used in the proposed SNN accelerator architecture. As shown in Table 5, by using approximate cells, the resource (ALUT) consumption of approximate adders is reduced compared to the exact adders while the mean error distance (MED) is relatively small.


Table 5. Resource consumption and mean error distance of LUT adders.

Design	E16	A16-2	A16-4	A16-8	E13	A13-2
ALUTs	31	29	27	22	24	22
MED	-	1.52 × 10−5	2.25 × 10−5	3.86 × 10−5	-	2.86 × 10−6
Note: Ex = x-bit exact adder, Ax-y = x-bit approximate adder with y-bit approximated.

4.1.2. The processing element
At each clock cycle, 512 weight values, 
, are read from the weight buffer. Their corresponding input neuron status, 
, are read from the neuron status registers. The weight value will only be used during computation if the corresponding status is 1 (fire). So the first step in the processing element is to perform a parallel AND operations of weight values and their neuron status. After that logic operation, an adder tree is used to accumulate the 512 values. At each time, the computation is performed for only one output neuron. Multiple cycles of computations for one output neuron is required when the former layer has more than 512 neurons. For the computation of each neuron, the previous output neuron value will be read from the neuron value buffer. In the next several computation cycles, the newly added values will be accumulated to the old neuron value.

After accumulation finished, the final value of the neuron will be further evaluated by the check spike module to determine the status of the neuron. For the models used in this paper, a threshold value of 1 is used. If the value is less than this preset threshold, the neuron does not fire and its value will be saved to the neuron buffer for future accumulation. A value of 0 will be fed into the shift register. If the value reaches the threshold value, the neuron fires and its value will be reset to zero. The neuron status of 1 is fed into a shift register. At the same time, the initial neuron value 0 will be sent back to the neuron buffer to replace the old value. After the computations of all current layer neurons are accomplished, if any one of the neuron fires, all status values in the shift register will be transferred to the neuron status register for continuing the next layer's computation.

As shown in Fig. 9, a large adder tree is used in the processing element. This adder tree is composed of both exact adder and approximate adder. As analyzed in previous sections, 40% or 50% of the computation in each layer are required to be accurate. Therefore, in the hardware architecture 50% of the adders are implemented with exact adders while the others are implemented as approximate adders. A simplified example of the adder arrangement is shown in Fig. 10. In this example, we apply 6 approximate adders and two exact adders in level one. Then, for the following levels, if both inputs are from approximate adders, these two inputs will be added with approximate adders. Otherwise, if at least one of the input is from exact adder, an exact adder will be used. In this way, the computations that can be done with 5-bit fraction are performed with approximate adders and those can be done with 7-bit fraction are performed with exact adders. Due to the feasibility of approximate computation determined during training, we could rearrange the data/weight order, so that at each computation cycle, there is always both approximate and exact computations appeared and the exact computation occupies roughly 50% of all computations at each cycle.

Fig. 10
Download : Download high-res image (72KB)
Download : Download full-size image
Fig. 10. Example of adder tree with both exact adder and approximate adder.

4.2. The memory system
The memory system is composed of three main components as shown in Fig. 8: the external DDR memory, the on-chip memory (weight buffer and neuron buffer), and the registers (the neuron status register).

The baseline SNN architecture used in this paper is fully-connected. Therefore, the amount of weight parameters is large and it exceeds the total capacity of on-chip RAM (M20k for Intel FPGA). In the proposed design, the weight parameters are stored in the DDR memory. When they are required for the current computation, they will be loaded into the on-chip weight buffer and are then fed to processing elements. The amount of neuron values is relatively small. However, as the number of layers for various models or the number of neurons in various layers are different, to handle all these scenarios, the neuron values are also stored in DDR memory and loaded into the on-chip neuron buffer when needed. In the proposed design, the on-board 512 MB DDR4 memory is used. The DDR interface bandwidth is configured to 512-bit (64 Byte), which is the largest bandwidth available for the designated FPGA board. At each clock cycle, 512-bit data can be read from DDR memory.

The diagram of the weight buffer is shown in Fig. 11. In the weight buffer, the M20k is configured to 32-bit interface with a depth equal to 1024, as shown in Fig. 11. We implement 256 such M20k banks. Therefore, at each cycle, a total of 512 weight parameters can be sent to the processing element (32-bit interface can accommodate two 13-bit weight parameters). We apply double-buffer design method to the weight buffer so that the time spent on loading the next part of weight can be hidden by the computation time of the current part of weight. As half of each M20k bank is 512 in depth, we can cache up to  weight in this buffer. We need a total of 512 M20ks for this weight buffer.

Fig. 11
Download : Download high-res image (107KB)
Download : Download full-size image
Fig. 11. Diagram of the weight buffer in the proposed SNN accelerator.

The neuron buffer is composed of an M20k bank of 16-bit width and 2048 in depth, as shown in Fig. 12. This size of M20k bank (2048 16-bit memory cells) is large enough for caching one layer neuron values of most SNNs. If a layer of SNN has more than 2048 neurons, then only part of the neurons can be read and processed each time and the remaining ones will be processed in the next round.

Fig. 12
Download : Download high-res image (55KB)
Download : Download full-size image
Fig. 12. Diagram of the neuron buffer in the proposed SNN accelerator.

The neuron status register is designed based on the characteristics of SNN computation. For SNN computation, different from normal neural networks, a neuron will generate an output only when its value is greater than a certain threshold. When a neuron's value is larger than the threshold value, the neuron fires and it generates an output of 1. Otherwise, the output is always 0. If all neurons in a certain layer do not fire, then, there is no need to perform computation for the next layer since all of the outputs for this layer are zeros. For SNN, the final output is usually obtained after multiple computation cycles. In order to reduce the amount of computation, a neuron status register, which is composed of two sets of 2048-bit registers, as shown in Fig. 13, is added. At the beginning of the SNN computation, the input spikes at the current time is loaded from the DDR memory. Before each layer's computation, the neuron status (whether it fires or not) of the previous layer are evaluated first. The computation is only performed if there is any fired neuron in the previous layer.

Fig. 13
Download : Download high-res image (53KB)
Download : Download full-size image
Fig. 13. Diagram of the neuron status register in the proposed SNN accelerator.

4.3. The controller
The flow of the whole implementation is shown in Fig. 14. The controller needs to know the number of layers in the running SNN model so that it can properly coordinate the operation. The number of layers is one of the parameters to be written to the configuration registers. With configuration registers ready, the computation starts with reading the input spike. Then, the computation of layer 2 will be started (PE-LN with N=2). After this computation, the value of each neuron in layer 2 will be evaluated. If any one of them fires, then the computation will continue to be performed for the next layer. Otherwise, the computation for the remaining layers will be omitted and the operation will go back to the starting point to read the next set of input spikes.

Fig. 14
Download : Download high-res image (133KB)
Download : Download full-size image
Fig. 14. Working flow of the proposed SNN accelerator (LN represents layer N).

As a SNN model could have any number of layers, an inner loop is designed in the control flow. If any neuron in a layer fires, the operation will check whether the current layer is the last layer in the SNN model. If it is the last layer, the output of the SNN model is considered to be generated and the operation can be terminated. Otherwise, the computation for the next layer will be performed.

Every time a set of input spike is read, the cycle counter will be incremented by 1. Through the SNN training process, we found that for all cases in the database, a SNN output can be generated within 35 cycles. Therefore, we use reaching 35 cycles as the condition to terminate the whole operations even though there is no output neuron firing.

4.4. Datapath of the proposed SNN accelerator
With the architecture and the control flow of the proposed SNN accelerator discussed in former sections, we then use the MNIST-3 (784-1200-10) model as an example to explain the overall datapath of the SNN implementation of the proposed accelerator.

The MNIST-3 model has three layers with 784, 1200, and 10 neurons, respectively. The number of neurons in each layer and the number of layers are written to the configuration registers to control the number of clock cycles required for the computation.

At the beginning of the computation, the first set of input spike, which is 784 binary values are loaded to the neuron status register. Then, part of weight values are loaded from the DDR memory to the weight buffer. As the weight buffer can cache up to  weight values, we choose to load the weights to compute the first 256 neurons (in total ) in layer 2. For the first round of computation, since the current neuron values of layer 2 neurons are all zeros, there is no need to load neurons from DDR to neuron buffer.

After the above mentioned initial data loading completed, the computation can start. For the first computation round, the first 512 neuron status and the first 512 weights corresponding to the first layer 2 neuron are loaded from neuron status register and weight buffer, respectively, to the processing element. They are ANDed and then accumulated. The resulting value is sent back to the adder tree for further accumulation since the computation for the first neuron has not been completed after the first round. In the second round, the remaining 272 status and weights are loaded and computed. After this round of computation, the first neuron in layer 2 is completed. The result is sent to the check spike module to determine whether its value reaches the preset threshold value. If the value is larger than the threshold, then a 1 is fed into the shift register and a 0 is written back to the neuron buffer. Otherwise, a 0 is fed into the shift register and the result itself is written back to the neuron buffer for future computation. This process is repeated until all the 256 layer 2 neurons are computed.

As double-buffer is used in the proposed architecture, so that during the computation for the first 256 neurons, the weights for the next 256 neurons can be loaded to the other half of the weight buffer. As a result, there is no need to wait for data loading before the computation of the second set of 256 neurons. In addition, the next set of input spikes are loaded into the second set of neuron status register, , in case there is no layer 2 neuron firing after the first round of computation.

After all layer 2 neurons are computed, there will be 1200 status values in the shift register and 1200 new neuron values in the neuron buffer. The values in the shift register will be loaded back to neuron status register, . If any value in the registers is 1, then the computation will continue for the third layer. In this case, the neuron status in  and the weights loaded during the last set of computations (weights for the third layer) will be used for the coming computation. The newly computed neuron values (for layer 3) will be updated to the second half of the neuron buffer. The neuron values of layer 2 neurons (stored in the first half of the neuron buffer) will be written back to DDR during the computation of layer 3. If no layer 2 neuron fires after the first round of computation, the neuron status in  and the same weight values that used for the last set of computations (weights for the second layer) will be used for the coming computation. In this case, because the weights in the weight buffer are for the last set of 256 layer 2 neurons, the computation order for this round will be different from the first round.

These processes will be repeated until all outputs are generated. For other SNN models, similar control flow and datapath will apply.

5. Results and analysis
The proposed SNN accelerator architecture is implemented with Verilog HDL. The on-chip memory is designed with M20k IP. The interface between DDR and FPGA logic is designed with the Intel FPGA external memory interface (emif) IP. The whole design is then synthesized and placed and routed using Intel Quartus Prime 19.2 Pro. The target device is Intel's Arria-10 SoC Development Kit which has an Arria-10 SX (10AS066N3F40E2SG) FPGA device. Simulation of the proposed architecture is performed with extensive testing vectors (pre-trained SNN model and MNIST database) using Modelsim to verify its functionality. Post place-and-route simulation is also performed to obtain the signal activity file for accurate power consumption estimation. In order to show the efficiency improvement of the proposed architecture, a reference architecture is also implemented. The reference architecture has the same architecture as the proposed architecture except that all the adders are exact adders.

The implementation results of both architectures are shown in Table 6. For the accuracy, all the reference models can achieve 100% relative accuracy with respect to the accuracy with using 32-bit floating-point numbers. Both designs can achieve a clock frequency of 250 MHz. As only 2 bits are implemented with approximate cell for each 13-bit adder, the speed improvement of approximate adder is not significant. The throughput is calculated based on frequency and the average cycles needed to generate SNN output. The two designs have similar throughput due to the same achieved clock frequency. In terms of resource consumption, the proposed design with approximate adder consumes 28% less ALUTs than the reference design. For registers and M20Ks, due to similar datapath architectures of the two designs, they use the same registers and M20Ks. In terms of power, the proposed design can achieve 29% less power consumption. On one hand, the proposed design consumes less ALUTs due to the use of approximate adders. On the other hand, the logic of approximate adder is simpler which can help to reduce the signal toggle rate and thus to reduce the power consumption. When comparing the energy consumption per image, the proposed design also achieves an average 29% energy reduction in four reference SNN models.


Table 6. Implementation results of reference and proposed designs.

With all exact adder	Proposed
Frequency (MHz)	250	250
ALUTs	6917	4962
Registers	13050	13050
M20Ks	516	516
Dynamic Power (mW)	625.50	442.32

Throughout for Reference Networks (img/s)
MNIST-3	2939	2939
MNIST-4	1184	1184
FASHION-4	1184	1184
FASHION-5	742	742

Energy/img for Reference Networks (μJ)
MNIST-3	212.79	150.47
MNIST-4	528.05	373.41
FASHION-4	528.05	373.41
FASHION-5	843.29	596.33
6. Conclusion
In this paper, an energy efficient FPGA based spiking neural network (SNN) accelerator architecture is proposed and implemented using approximate adders. An analysis of layer-wise precision requirements of four different SNN models is performed to determine the feasibility of applying variable precision to different layers. Through software experiments, the precision combinations to maintain high accuracy are obtained. Based on the findings in our software experiment, a hardware accelerator architecture is proposed. In the proposed hardware architecture, approximate arithmetic units are used to handle variable precision requirements. The whole architecture is carefully tailored for the target FPGA device. Implementation results show that the proposed SNN accelerator can achieve 28% reduction in ALUT and 29% reduction in power and energy consumption. The proposed hardware architecture can be used to accelerate SNN computation. In addition, the working process presented in this paper can be applied to other neural network models or other applications when approximate computing is desired to be used.

In the future, the feasibility of applying approximate computing in larger scale SNN will be investigated. In addition, the approximate computing strategy for other spiking neuron models will be explored.