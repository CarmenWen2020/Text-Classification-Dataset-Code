Abstract
Latent Sector Errors (LSEs) happen at a significant frequency in the field and can impose a huge risk to data reliability. Disk scrubbing is a background process that reads disks periodically to detect LSEs timely, thus shortening the window of vulnerability to data loss. Nowadays, proactive error prediction, using machine learning techniques, has been proposed to improve storage system reliability by increasing the scrubbing rate for disks with higher error rates. Unfortunately, the majority of works incur non-trivial scrubbing costs and overlook the relationship between complete disk failures and LSEs.

In this paper, we attempt to maintain or improve data reliability at reduced scrubbing costs. In particular, we design a novel adaptive approach that enforces a lower scrubbing rate for healthy disks and a higher scrubbing rate for disks which are subject to LSEs. Besides LSEs that are specific to partial disk failures, we also adjust scrubbing rates according to complete disk failure rates, because disks typically develop LSEs before they finally fail. Moreover, a voting-based method that exploits the periodic characteristic of scrubbing is proposed to ensure prediction accuracy. Experimental results on a real-world field dataset have demonstrated the effectiveness of our proposed approach. Specifically, the results show that we can achieve the same level of reliability, in terms of Mean-Time-To-Detection (MTTD), as the traditional fixed-rate scrubbing scheme with almost 49% less scrubbing costs or we can improve the reliability by a factor of 2.4X without extra scrubbing costs. Compared with the state-of-the-art approaches, our method can achieve the same level of reliability with nearly 32% less scrubbing costs.

Previous
Next 
Keywords
Hard disk

Reliability

Latent sector error

Scrubbing

Machine learning

1. Introduction
Although hard disk drives are generally highly dependable and rarely fail, the devices replaced most frequently in data centers are hard disk drives [10], [28], [31], [34]. It has been estimated that 78% of the hardware replacements are due to disk failures [33]. In recent years, total storage-system capacities tend to increase faster than the disk storage-densities, hence the total number of disks in data centers keeps increasing. However, disk reliability has not been improved accordingly [13].

Disk storage systems can lose data for a variety of reasons, including failures at both the device level (i.e., complete disk failure [35]) and the block level (i.e., partial disk failure [11]), where individual sectors on a disk become unreadable. As sector errors are latent errors, a disk is typically not aware of sector errors and thus not able to report the errors until the affected sectors are read, making it challenging to protect against that type of errors [24]. There are two scenarios where Latent Sector Errors (LSEs) manifest and impact data reliability and availability: (1) if an LSE is found while the system is operating in degraded mode, e.g., one disk fails in RAID-5, data loss could occur. (2) if an LSE is encountered during a read request, there occurs temporary data unavailability.

Even worse, LSEs happen at a significant frequency in the field. Recent studies based on data from NetApp have reported that 5%–20% of nearline hard disk drives in NetApp’s storage systems develop sector errors within a period of 24 months [5]. More recently, an analysis of field data, conducted by Mahdisoltani et al. [24], reveals that two drive models among the seven most common hard disk models in a large production installation have 11% and 25% of their disks affected by sector errors, respectively.

Nowadays, disk scrubbing [2], [26], [30] and intra-disk redundancy [9], [12], [30] are two commonly deployed approaches to protect disks against LSEs. Disk scrubbing periodically scans disks, and intra-disk redundancy deploys erasure codes. Though lots of scrubbing schemes have been proposed, most of them focus on improving reliability without taking cost-efficiency into account. Besides designing complex scrubbing schemes, there are also efforts in an attempt to predict LSEs using machine learning methods [24]. Mahdisoltani et al. [24] propose increasing the scrubbing rate when errors are alarmed by the prediction results so as to enhance the data reliability. Unfortunately, their methods have to pay non-trivial extra scrubbing costs to reach higher data reliability.

However, it is very challenging to apply proactive error prediction to guide scrubbing in a working environment. The first challenge is that scrubbing is not free. Although disk scrubbing enhances the reliability of storage systems, the cost of scrubbing comes in multiple aspects, e.g., energy consumption and performance impact. On one hand, the higher the rate of scrubbing, the more reliable the storage system will be. On the other hand, a higher rate of scrubbing will cause a higher cost of scrubbing. However, the budget for data protection is always limited [6], [36], thus making it imperative to build more cost-efficient methods to protect data from being lost. The second challenge is that disk scrubbing is related with not only LSEs but also complete disk failures. On one hand, as mentioned above, LSEs which occur while the system is operating in degraded mode may lead to data loss. On the other hand, disks typically develop LSEs before they finally fail [28].

In our research, we present scrub unleveling, a new approach to improving the cost-efficiency of disk scrubbing using machine learning techniques. Our approach has three distinguishing features. Instead of designing complex scrubbing schemes, scrub unleveling only interacts with the scrubbing scheduler by instructing with an appropriate scrubbing rate. Instead of finding an optimal scrubbing rate for all disks, we attempt to adjust the scrubbing rate for each individual disk based on their health statuses. Instead of sacrificing data reliability for low scrubbing cost, or vice versa, we aim to achieve low scrubbing cost and high data reliability at the same time.

The main contributions of our work are four-fold:

•
We propose a novel scrub unleveling scheme, using machine learning methods, to adapt the scrubbing rate according to the disks’ health statuses. In addition, we collectively study the impacts of complete disk failure and partial disk failure on scrubbing and propose a failure-aware scrubbing scheme.

•
Based on the periodic characteristic of scrubbing, we design a novel voting-based algorithm to ensure prediction accuracy.

•
Experimental results on real-world datasets demonstrate that, by applying our proposed schemes, we can achieve both lower scrubbing cost and higher data reliability than the state-of-the-art method.

This paper is an extended version of our conference paper [14], and we make the following new contributions over the conference version:

•
We provide a more detailed quantitative estimation of scrubbing costs and MTTDs under the scrubbing guided by an LSE predictor, and demonstrate that reducing scrubbing costs and MTTDs are promising.

•
We have clarified the motivation of adaptive+ scheme and built simulations of running system to measure scrubbing counts and MTTDs.

•
We have added experiments to demonstrate the effectiveness of down-sampling to data imbalance issue in latent sector error prediction, experiments to analyze the robustness of latent sector error prediction, and experiments to verify the effectiveness of proposed voting-based method.

The remainder of this article is organized as follows. Section 2 describes the background of LSEs and scrubbing as well as motivations of our design. Section 3 gives a theoretical analysis. Section 4 presents the design of scrub unleveling. Section 5 discusses the dataset and preprocessing. Section 6 presents the simulation results about the effects of scrub unleveling on data reliability and scrubbing cost. Section 7 describes the related work, and Section 8 concludes our work.

2. Background and motivation
Before disks finally fail, they typically develop LSEs [28]. This characteristic explains why some spare sectors on a disk are reserved and mapped to the logical block numbers (LBNs) of failed sectors [5]. Although LSEs are not as fatal as complete disk failures, they can cause temporary data unavailability as well as permanent data loss in the worst case [6]. Therefore, besides protecting against complete disk failure, paying attention to LSEs is also essential. Together with intra-disk redundancy, scrubbing empowers a system in most cases to be recovered and resume normal operation [5].

In the following text of this section, we will discuss the benefit in data reliability brought by scrubbing as well as proactive error prediction guided scrubbing. Also, we argue that fixed rate scrubbing is suboptimal due to the varying disk failure rates along the lifetime of hard disk drives.

2.1. Benefit of proactive error prediction
Scrubbing aims to detect LSEs as soon as possible. Without scrubbing, as shown in Fig. 1(a), a sector error will be found by a read request. As a result, the quality of service is impacted. Moreover, data in the affected sector will be lost if the error is encountered when no redundancy data is available [5], [24].

The metric of interest when it comes to LSEs is not the traditional Mean-Time-To-Failure (MTTF) [31] or the Mean-Time-To-Data-Loss (MTTDL) [6], but instead the Mean-Time-To-Detection (MTTD) [24], [30]. MTTF measures the disk lifetime before complete disk failure and does not give a measure of its resilience to LSEs. MTTDL is a systemic measure, and not applicable to the study of errors in a single disk [26]. MTTD indicates the time from when an error occurs till the error is detected [24], [30].

As shown in Fig. 1(b), we can detect a sector error by scrubbing, thus shortening the window of vulnerability to data loss (i.e., MTTD). Furthermore, as shown in Fig. 1(c), we first predict the impending sector errors and then increase the scrubbing rates for disks that are predicted to suffer from sector errors. Therefore, in this case, together with a proactive error prediction, we can further shorten the MTTD by applying the prediction results to guide the scrubbing rate.

2.2. Constant scrubbing rate is suboptimal
The NetApp study [5] shows that the percentage of disks that have LSEs increases almost linearly with time, while most storage systems use constant scrubbing rate without taking the varying LSE rates into account [24]. As a result, scrubbing at a constant rate will be either not cost-effective or insufficient along the disk lifetime. This characteristic inspires us to predict the occurrence of LSEs (partial disk failure) and to adjust the scrubbing rate according to the prediction results.

As mentioned above, disks typically develop LSEs before they finally fail. When a disk fails, the redundancy of data associated with those on the failed disk will be reduced. To maintain high reliability, we need to scrub disks which are subject to a higher risk of disk failures because of their higher sector error rates. Fig. 2 shows the lifecycle failure pattern, following a “bathtub curve” [15], [31], for hard disk drives. According to this failure pattern, the first year of operation is characterized by infant mortality. In years 2–5, the failure rates are approximately in a steady state, and then, after years 5, wear-out starts to kick in. That is to say, LSEs occur unevenly. As a result, the fact of varying complete disk failure rates renders the scheme with a constant scrubbing rate suboptimal. In other words, scrubbing at a fixed rate without considering varying LSEs rates is insufficient for both infant and aged disks, while it is over-provisioned for disks in their steadily operating period. Therefore, there is a need to encode the disk failure characteristics into the schedule of scrubbing to better protect reliability.


Download : Download high-res image (108KB)
Download : Download full-size image
Fig. 2. The bathtub-like lifecycle failure pattern renders a fixed-rate scrubbing suboptimal. The black curve shows the lifecycle failure pattern for disks and the green line indicates the fixed-rate scrubbing. As we can see, scrubbing at a fixed rate is insufficient in both the disk infant mortality period and the wear-out period, while it is over-provisioned in the useful-life period.

2.3. Scrubbing is not free
Although disk scrubbing enhances the reliability of storage systems, the cost of scrubbing comes in multiple aspects, e.g., energy consumption and performance degradation. It is apparent that the higher the rate of scrubbing, the more reliable the storage system will be. On the other hand, a higher rate of scrubbing will bring a higher cost of scrubbing. However, the budget for data protection is always limited [6], [36], thus making it imperative to build more cost-efficient methods to protect data from being lost.

3. Theoretical analysis
In this section, we quantitatively estimate the impact of the scrubbing rate on the reliability and the scrubbing cost. Moreover, we also estimate the impact of introducing error prediction. The symbols used in the rest of this paper are listed in Table 1.

To simplify our discussion, we make the following assumptions as in [23], [26], [30].


Table 1. Symbols and definitions.

Symbol	Definition
Scrubbing interval (day)
Scrubbing rate (1/day)
Factor of scrubbing rate acceleration
Factor of scrubbing rate deceleration
# of predicted negatives
# of predicted positives
# of true positives
# of true negatives
# of false positives
# of false negatives
# of total faulty disks
# of total healthy disks
False negative rate
False positive rate
MTTD under fixed-rate scrubbing scheme
MTTD under adaptive rate scrubbing scheme
MTTD under adaptive+ rate scrubbing scheme
Improvement of MTTD under adaptive
rate scrubbing scheme
Scrubbing cost under fixed-rate scrubbing scheme
Scrubbing cost under adaptive rate scrubbing scheme
Scrubbing cost under adaptive+ rate scrubbing scheme
Increase of scrubbing cost under adaptive
rate scrubbing scheme
•
The failure time follows a uniform distribution, and the mean of the uniform distribution is equivalent to the average value of the distribution interval.

•
The impact of the normal read operation on MTTD is ignored. As 87% of all latent sector errors in nearline disks are discovered by disk scrubbing [5], the normal read operation’s impact on MTTD is negligible.

3.1. MTTD
In traditional fixed-rate periodical scrubbing, the scrubber runs continuously at a slow rate in the background so as to limit the impact on foreground traffic. Given a scrubbing interval , for instance, a disk is being scrubbed at a rate of  [26], [30]. So the average time running with failure is . Then the MTTD under fixed-rate scrubbing (i.e., without error prediction) is calculated as follows: (1)
 

This formulation implies, the higher the scrubbing rate, the higher the reliability. Note that lower MTTD means a higher level of reliability.

In scrub unleveling, we accelerate the scrubbing rate by a acceleration factor  () if an error is predicted, otherwise decelerate the scrubbing rate by a deceleration factor  (). We use  and  to represent the number of true positives and the number of false negatives, respectively. The time for detection of one true positive is 
 
 with an accelerated scrubbing rate of . So the total time for detection of all true positive () is proportional to the number of true positive disks, and the result is 
 
. Similarly, the total time for detection of all false negative () is 
 
 with decelerated scrubbing rate . Then the MTTD of the scrub unleveling scheme (denoted as adaptive, and we use them interchangeably in the following discussion) is calculated as follows: (2)
 
 
 
 
 
where  and 
 
.

As Eq. (2) implies, MTTD is directly proportional to FNR, which refers to the fraction of error disks that are mis-classified as no-error ones and are not related to FPR, which refers to the fraction of no-error disks that are mis-classified as error ones, because no-error disks do not impact the reliability.

Compared with fixed-rate scrubbing, the improvement of MTTD under adaptive rate scrubbing is calculated as follows: (3)
 
 
 
 

One of our targets is to improve MTTD, i.e., . Therefore, as Eq. (3) indicates, the requirement of  is 
 
. For instance, if we set  to , we need to adjust  under 
 
 
 to get improvement in MTTD. As shown in Section 6.1, i.e., the results of proactive error prediction, this requirement of FNRs is easy to achieve.

3.2. Scrubbing cost
In a traditional fixed-rate periodic scrubbing, the scrubbing cost is proportional to both the total number of disks and the scrubbing rate, since a higher scrubbing rate means more scrubbing overhead. Under the assumption of the scrubbing rate being  and the total number of disks being , the scrubbing cost, within timespan , of standard periodical scrubbing (i.e., without error prediction) is calculated as follows: (4)

In the scrub unleveling scheme, we apply accelerated scrubbing in response to predicted positive (i.e., ) and apply decelerated scrubbing rate in response to predicted negative (i.e., ). Then the scrubbing cost, within timespan , under the adaptive scheme is calculated as follows: (5)where  and .

Compared with fixed-rate scrubbing, the increase of scrubbing cost under adaptive rate scrubbing is calculated as follows: (6)
 
 
 
where .

Our other target is to cut down scrubbing cost, i.e., . Therefore, as Eq. (6) indicates, the requirement is 
 
 
. For instance, if we set  to , we need to adjust 
 
 under 
 
 to reduce scrubbing cost. As the fact that error disks are much fewer than no-error ones and high accuracy of machine learning based sector error predictor, resulting in  is much more than , thus this requirement is also easy to achieve.


Download : Download high-res image (152KB)
Download : Download full-size image
Fig. 3. System architecture of Scrub Unleveling. A Self-Monitoring, Analysis and Reporting Technology (SMART) monitor collects and aggregates the SMART values. The centralized LSE predictor predicts the presence of LSEs and forwards prediction results to rate controller which guides the scrubbing scheduler to adaptively adjust the scrubbing rates.

4. The design of scrub unleveling
In scrub unleveling, we adjust scrubbing rates for each individual disk according to its health statuses. Fig. 3 shows the architecture of scrub unleveling. A monitor collects and aggregates the attribute values reported by SMART. A centralized LSE predictor reads the SMART attribute values, calculates the probabilities of LSEs using machine learning methods, and notifies the predictions about impending LSEs. The rate controller receives prediction results and guides the scrubbing scheduler to adaptively adjust the scrubbing rates.

4.1. LSE predictor
SMART is a monitoring system implemented inside most modern disks and monitors and reports various indicators of disk reliability [24]. Based on the SMART data collected by SMART Monitor, LSEs Predictor attempts to predict whether a disk will have LSEs within a given time interval. We formulate the problem of predicting future errors as a binary classification problem and then use a variety of machine learning methods to train classifiers. As in practice, a common rule of thumb is to scan the entire disk once every two weeks [4], [5], [24], [26], we attempt to predict LSEs two weeks into the future, i.e., whether there will be LSEs within the next 14 days.

The predictor performs predictions every day, but the scrubbing is issued once every scrubbing period. Given the periodic characteristic of disk scrubbing, we take a voting-based algorithm, shown in Algorithm 1, to combine the results in the current scrubbing interval. In detail, when making a prediction for a disk, we check all  samples in the current scrubbing interval  and raise an alarm if more than  samples are classified as erroneous, otherwise healthy. Different from the voting-based algorithm proposed by [21], where the windows for voting are fixed, the windows in our method are determined by the disk scrubbing intervals. That is to say, if the current scrubbing interval is , the current window for voting is set to .


Download : Download high-res image (262KB)
Download : Download full-size image

Download : Download high-res image (226KB)
Download : Download full-size image
4.2. Scrubbing rate controller
Scrubbing a storage system requires balancing the costs of the scrubbing against the benefits that the scrubbing will provide. In this paper, we define the benefits as data reliability and the costs as energy consumption of scrubbing which is proportional to scrubbing time. Since scrubbing operations are low priority tasks and the impact of scrubbing on performance is negligible [23], we ignore the cost generated by the performance degradation.

In Section 3, we have analyzed the positive impact brought by error prediction guided scrubbing. Besides LSEs, varying complete disk failure rates also render a constant scrubbing rate suboptimal. According to the “bathtub-like” lifecycle failure pattern [31], scrubbing at a fixed rate is insufficient during the disk infant mortality period and the wear-out period, while it is over-provisioned in the useful-life period. Therefore, we encode the disk failure characteristics into scrubbing scheduling to protect data against LSEs better.

As shown in Algorithm 2, we combine the proactive error prediction guided scrubbing and the bathtub-aware scrubbing together. During the infant mortality period and the wear-out period, we use a high scrubbing rate , and we switch to a low scrubbing rate  () during the useful-life period. In addition, we accelerate the rate by a acceleration factor of  () if an error is predicted, otherwise decelerate the rate by a deceleration factor of  (). At the same time, considering that most disks at most time are free of sector errors, we introduce a decelerated factor  for useful-life period. When disks are at their useful-life period, the scrubbing rate is decelerated by a factor of .

Therefore, the MTTD under the adaptive+ scheme is calculated as follows: (7)
 
 
 
 
where  indicates the total power-on hour of disk , and  and  mean the prediction result of disk  is a true positive and a false negative, respectively. Moreover,  and  indicate that the power-on hour is one year and five years, respectively.

The scrubbing cost under the adaptive+ scheme is calculated as following: (8)where  and  mean disk  is predicted as positive and negative, respectively.

5. Dataset description and preprocessing
5.1. Dataset
To evaluate the proposed schemes, we use the real-world dataset from Backblaze [3], which spans a period of 12 months from January 2017 to December 2017. From this dataset, we select three drive models, Seagate’s ST4000DM000, ST8000DM002, and ST8000NM0055, which are affected by LSEs most significantly, i.e., the number of disks affected by LSEs is among the most in this period. The definition of a sector error event is defined as in [24]: a disk is considered to have a sector error if its raw value of SMART 5, which indicates the total number of reallocated sectors, increases.

As shown in Table 2, erroneous disks indicate disks that have at least one sector error in the year of 2017. For erroneous disks, samples in a period of two weeks before actual sector error are labeled as an error, otherwise no-error. For no-error disks, we label the total samples as no-error. We note that the interval of two weeks is the commonly used scrubbing period in storage systems [4], [5], [24], [26].


Table 2. Overview of evaluation dataset.

Drive model	Class	No. drives	No. samples
ST4000DM000	no-error	34,830	11,739,000
error	328	8693
ST8000DM002	no-error	9745	3,378,084
error	223	6527
ST8000NM0055	no-error	14,299	2,455,296
error	175	4583

Table 3. The 12 selected SMART attributes.

Attribute ID	Attribute name	Attribute type
1	Real_Read_Error_Rate	Normalized
3	Spin_Up_Time	Normalized
4	Start_Stop_Count	Raw
5	Reallocated_Sector_Count	Raw
7	Seek_Error_Rate	Normalized
9	Power_On_Hours	Normalized
10	Spin_Retry_Count	Normalized
12	Power_cycle_Count	Raw
187	Reported_Uncorrect	Normalized
194	Temperature_Celsius	Normalized
197	Current_Pending_Sector	Raw
198	Offline_Uncorrectable	Raw

Download : Download high-res image (515KB)
Download : Download full-size image
Fig. 4. False positive rates (x-axis) versus false negative rates (y-axis) when predicting sector reallocation (SMART 5).

5.2. Data preprocessing
5.2.1. Re-sampling
As shown in Table 2, the dataset is highly imbalanced, i.e., error disks are much fewer than no-error ones. However, imbalanced dataset is harmful to canonical machine learning algorithms which assume that the number of samples in considered classes is roughly similar [18]. To solve this problem, under-sampling is a commonly used method which under-samples the majority class and renders balanced dataset. Therefore, we first under-sample the no-error samples and construct training sets with different ratios of the no-error to the error samples, ranging from 1:1 to 50:1. Secondly, we build random forests models on these training sets and compare their accuracy. In the final training set, the ratio is set to 3:1 which leads to the best prediction accuracy. And the detailed experimental results are shown in Fig. 5 in Section 6.1.2.

5.2.2. Feature selection
We employ feature selection to remove redundant and irrelevant features and select relevant features. This preprocessing can not only reduce the time of model training and prediction but also enhance the prediction performance [35]. In our dataset, each disk reports 24 SMART attributes, and each attribute contains a raw value and a normalized value. Instead of factoring in all the SMART attributes values into the prediction model, we use correlation coefficients and select 12 features that correlate most with LSEs. The selected SMART attributes are listed in Table 3.

5.2.3. Normalization
Since different SMART attributes have diverse value intervals, to ensure a fair comparison among them, we apply data normalization. The Z-score normalization [8] used in our approach is calculated as follows: (9)
 
where  is the original value of a feature and  and  are the mean and standard deviation of the feature in our dataset, respectively. We also experiment with Min–Max normalization [19], [20], [24], but find Z-score results in better prediction performance.

6. Experimental results
6.1. Results of proactive error prediction
To evaluate the models, we divide the dataset randomly into training and test sets for each disk model. The training set consists of 70% of all error and no-error disks, and the remaining 30% of the disks are in the test set.

To build the LSE prediction, we have evaluated six different machine learning methods, including logistic regression (LR), random forests, support vector machines (SVM), classification and regression trees (CART), backward propagation neural networks (BP) and Gradient Boosting Decision Tree (GBDT). For LR we experimented with L2 regularization and learning rate of 0.01. For random forests we experimented with different numbers of trees, and settled on using 200 trees for the results included in the paper. For SVM we used the LIBSVM library [7], and experiment with linear kernel. For BP, we use 3-layer BP with 64 nodes in the hidden layer. Both hidden and output layers use the ReLU function as the activation function. We set the maximum number of iterations to 2000, the learning rate to 0.01 and adopt Adam [17] for optimization. For GBDT, we use 100 trees and learning rate of 0.1.

6.1.1. FPR versus FNR
The quality of the predictions is shown in Fig. 4, in which each line corresponds to a different machine learning method. The -axis shows the FPRs which fall in the range of [0,0.2] and the -axis shows the FNRs. As we can see, errors can be predicted with a high enough accuracy to guide lighter-weight proactive actions [24]. E.g., when limiting the FPR to 10%, for instance, we can correctly predict 96%, 94%, and 85% of all errors for ST4000DM000, ST8000DM002, and ST8000NM0055, respectively. Comparing different machine learning methods, the random forests consistently outperform other classifiers. One possible reason is that random forests have few parameters to tune, making them easy to train [24]. For other classifiers, a considerable amount of tuning is required, making them harder to achieve reasonably high accuracy.

6.1.2. Robustness of predictions
Imbalanced training set results in suboptimal prediction accuracy, therefore we down-sample the no-error samples with different down-sampling ratio and render re-balanced training sets. Then Random Forests models are trained on these training sets and AUC (Area Under ROC Curve) value is measured. As shown in Fig. 5, the highest prediction accuracy is achieved when the ratio of the no-error to the error samples is set to 3:1.

In reality, one practical problem is that prediction models will most likely be used in small and medium-sized data centers [21], [24]. As shown in Fig. 6, to evaluate the effectiveness of prediction models applied to smaller systems, we train random forests with 10%, 25%, 50%, and 75% of the training data from ST4000DM000. As expected, prediction performance decreases for smaller datasets. However, the result is still promising.

6.1.3. The effectiveness of voting-based method
In Section 4.1, we introduced the voting-based method in which if over half of instances in current window is classified as failure, the corresponding disk is considered as error. Note that non-voting method if any instance in current window is classified as failure, the disk is regarded as error. Here we conduct experiments to evaluate the effectiveness of voting-based method. The result of the experiment using Random Forests is shown in Table 4. It is clear that voting-based method could improve the prediction performance.


Table 4. Voting-based Versus Non-voting.

Drive model	Metrics	Voting-based	Non-voting
ST4000DM000	FDR	90.1%	91.5%
FAR	3.6%	9.5%
ST8000DM002	FDR	86.2%	86.9%
FAR	3.8%	9.5%
ST8000NM0055	FDR	74.4%	76.2%
FAR	3.3%	9.1%
6.2. Simulating scrub unleveling
Based on prediction results, we build simulators to evaluate the effectiveness of our scrub unleveling schemes. We choose the random forests as the predictor, as they consistently provide the best predictions. In the adaptive scheme, we set the base scrubbing rate as  in the whole lifetime of the disk. In the adaptive+ scheme, we set the base scrubbing rate as  in both the disk infant mortality period and the wear-out period, and set the base scrubbing rate as  in the useful-life period. In all simulations, we set  to one full disk scrubbing bi-weekly which is a commonly used scrub rate in practice. Given the bathtub characteristic of life cycle failure pattern for disks, we set , i.e., one full disk scrubbing per week.

The choice of the scrubbing rate depends on the system’s sensitivity to added scrubbing cost. To this end, we experiment with a range of  and  values. Since sequential fixed-rate scrubbing is the common approach used in production, we use fixed-rate scrubbing in our simulation. It is worthy to note that our scheme is compatible with other complex scrubbing approaches, e.g., staggered scrubbing [26], since our scheme only interacts with the scrubbing scheduler with an appropriate scrubbing rate.

6.2.1. Scrubbing cost
Fig. 7(a) shows scrubbing cost under different accelerated scrubbing modes. Each line corresponds to a different set of acceleration factors  and deceleration factors . And each point on every single line corresponds to a different (FPR, FNR), i.e., a different prediction result, which determines the proportion of time spent in accelerated mode. Note that we either accelerate or decelerate the rate based on the results of LSE prediction. That is to say, the sum of the proportion of time spent in accelerated mode and that in decelerated mode is 1.

As shown in Fig. 7(a), the increase of scrubbing cost is proportional to the fraction of time spent in the accelerated scrubbing mode. Moreover, the lowest increase is achieved when  and , i.e., all of the time is spent in decelerated mode. On the other end of each line, the highest increase is achieved when  and , i.e., all of the time is spent in accelerated mode. As we can see, the accelerate rate  determines the highest increase of scrubbing cost. That is to say, the higher the rate of the scrubbing the higher the scrubbing cost we need to pay. Although low scrubbing cost is our goal, we also need to take into account data reliability, i.e., MTTD, in our scenario.

6.2.2. MTTD
Fig. 7(b) shows the improvement of MTTD under different fractions of time spent in accelerated mode. Each line corresponds to a different set of acceleration factors  and deceleration factors . As we can see, a lower deceleration factor , while the acceleration factor  keeps the same, will lead to less improvement in reliability. We also observe that even if we limit the fraction of time the system spends in the accelerated scrubbing mode to 5% (i.e., the storage system spends 95% of the total time in the decelerated scrubbing mode), the improvement of MTTD is still promising.

The inevitable false negatives in the error prediction could cause some LSEs to be found in a delayed manner, because scrubbing rates of these disks are decelerated, resulting in less improvement of MTTD. As shown in Fig. 7(b), however, the improvement of MTTD is greater than the loss of MTTD. Because, on the one hand, FNRs are very low in our prediction as shown in Section 6.1. On the other hand, the improvement of MTTD demands a moderate level of FNRs.

6.2.3. Cost versus MTTD
Fig. 7(c) shows the relationship between the improvement of MTTD and the increase of scrubbing cost. In this figure, fixed-rate scrubbing is indicated by point (0,1) where the increase of scrubbing cost is 0, and the improvement of MTTD is 1. As we can see, when compared with fixed-rate scrubbing scheme, if the deceleration factor  is set less than 1, we have the chance to achieve improvement of MTTD with less scrubbing cost. If the deceleration factor  is set to 1, we will pay additional scrubbing cost to improve MTTD. The reason why we can achieve improvement of MTTD when decelerating scrubbing rate is that the healthy disks are the majority and they have no impact on MTTD.

Fig. 9 shows a comparison between our schemes and the state-of-the-art approach. The state-of-the-art approach uses an accelerated scrubbing rate in error disks and keeps a default scrubbing rate in no-error disks, which is a special case of our proposed approach in which we accelerate the scrubbing rate in error disks while decelerate the scrubbing in no-error disks, i.e.,  and . According to [24], we simulate the state-of-the-art approach, where MTTD is improved at extra scrubbing cost, by setting (, ) to (2, 1). For our schemes, we simulate them with (, ) set to (2, 0.5) where MTTD can be maintained or improved at reduced scrubbing cost. From the results, we observe that adaptive+ scheme can still achieve higher improvement in MTTD when extra scrubbing cost is zero. Specifically, compared with fixed-rate scrubbing, we achieve the same level of reliability with almost 49% less scrubbing cost, or improve the reliability of storage systems by a factor of 2.4X in terms of MTTD without extra scrubbing cost. Moreover, compared with the state-of-the-art approach (i.e., proactive error prediction proposed by Mahdisoltani et al. [24]), our method can achieve the same level of MTTD with nearly 32% less scrubbing cost. The reason is twofold: (1) we also reduce the rate instead of only increasing the rate from a default baseline, (2) we consider both partial disk failure and complete disk failure. Therefore, scrubbing resources are allocated more efficiently, i.e., saving unnecessary scrubbing resources on healthy disks to disks at risk.


Download : Download high-res image (299KB)
Download : Download full-size image
Fig. 8. Simulating the scrub unleveling with three different decelerated factors for useful-life period. The left subfigure shows the normalized scrubbing counts, the middle subfigure shows the normalized MTTD, and the right subfigure shows the sum of normalized scrubbing counts and normalized MTTD with weights of 0.5 and 0.5, respectively.

6.2.4. Scrubbing count versus MTTD
The aforementioned simulations are inferred from the analytical cost model where scrubbing cost is proportional to scrubbing time. To simulate a running system, we introduce scrubbing counts, which indicate the total start–stop counts of scrubbing for the disks in test dataset within simulation time, to assess scrubbing cost which we assume to be proportional to scrubbing counts. That is to say, a higher scrubbing count means a higher scrubbing cost. Similarly, MTTDs indicate the total mean time for detecting LSEs for the disks in test dataset within simulation time. Specifically, by using the LSE predictor with (, ) of (3.6%, 90.1%), we evaluate scrubbing counts and MTTDs under different scrubbing schemes. For each scrubbing scheme, we simulate different , , and  which indicates the decelerated factor for the useful-life period. Note that  is only valid for adaptive+ scheme.

Fig. 8 shows the normalized scrubbing count (left), normalized MTTD (middle), and the sum of normalized scrubbing count and normalized MTTD with weights of 0.5 and 0.5 (right), respectively. As we can see, for the same (, ), if we increase the , the scrubbing count increases, the MTTD decreases, and the sum decreases. So when (, ) is fixed, it is worthwhile to increase  properly. Similarly, it is worthwhile to decrease  when (, ) is fixed and it is worthwhile to increase  when (, ) is fixed. Compared with the adaptive+ scheme, when the weights for scrubbing count and MTTD are the same, adaptive scheme is better. However, with more parameters to be adjusted, the adaptive+ scheme is more flexible than the adaptive scheme when the weights are different. The set of results reveals that we should set a higher  if MTTD is more critical to the system and set a lower  if scrubbing cost is more of a concern.

7. Related work
7.1. Reducing cost
Scrubbing was first proposed for fault detecting in memory by Saleh [29]. Kari’s dissertation [16] seemed to be the first comprehensive analysis of LSEs in the disk and presented four scrubbing algorithms in which extra load caused by scrubbing was considered and dealt with. Though LSEs can lead to much loss, the cost of these protection strategies is also essential because the budget for data protection is always limited. Prior works on minimizing the cost of scrubbing fall into two classes, those which submit scrubbing during idle times [2], [25] and those which try to piggyback scrubbing on workload operations [1], [32]

By treating the scrubbing as low priority background activities and scheduling them efficiently during idle times, Mi et al. [25] and Amvrosiadis [2] both demonstrated that the user performance could be maintained within predefined bounds while the data reliability was improved.

For the usage of scrubbing in large archival storage systems, opportunistic scrubbing has been proposed by Schwartz et al. [32]. In such systems, disks may remain powered off for long periods, so we need to keep the disk powered down and minimize the number of power-ups. On the other hand, to deal with LSEs, we must detect them early enough to be able to use the redundancy built into the storage system [32]. So they adopted an opportunistic strategy where scrubbing actions piggybacked on normal read accesses instead of powering on disks solely to check them, i.e., scrubbing when a disk was powered up for another operation. Moreover, they demonstrated that opportunistic scrubbing gains most of the benefits of deterministic scrubbing with no additional power up/down cycles. Amvrosiadis [1] proposed performing scrubbing opportunistically based on data cached in memory, which significantly alleviates the I/O overhead of scrubbing.

7.2. Improving MTTD
To timely detect LSEs, staggered scrubbing was proposed by Oprea et al. [26] and aimed to exploit the fact that errors happen in bursts. Rather than sequentially reading the disk from the beginning to the end, the idea was to quickly probe different regions of the disk, hoping that if a region of the disk had a burst of errors, then they would find one in the probe and immediately scrubbed the entire region. Furthermore, by adjusting the scrubbing rate according to disk’s age and history of LSE development, they proposed a strategy with adaptive scrubbing rates. By combining staggered scrubbing and accelerated scrubbing, Schroeder et al. [30] proposed accelerated staggered scrubbing in which once an error is encountered in a region the entire region is scrubbed at an increased scrubbing rate. However, the non-sequential and accelerated read will incur extra overhead. Since scrubbing process consumes energy, the higher the scrubbing is, the more energy it costs.

Without the help of effective disk failure prediction, [27] attempted to predict when a disk failure was likely to result in data loss. Then an expedited scrubbing is issued to the whole array whenever a disk failure was detected. However, they only considered the complete disk failure and did not take the characteristics of LSEs into account.

More recently, instead of designing complex scrubbing schemes, there are also efforts in an attempt to predict LSEs using machine learning methods. Mahdisoltani et al. [24] proposed increasing the scrubbing rate when errors were alarmed by the prediction results to enhance the data reliability in terms of Mean-Time-To-Detection (MTTD). They also demonstrated that sector errors could be predicted ahead of time with high accuracy by using machine learning techniques [24].

Liu et al. [22] attempted to strike an optimal scrubbing rate by keeping a balance between data loss cost, scrubbing cost, and disk failure rate. Their results demonstrated that the scrubbing rate is proportional to the data price and the annual failure rate, while is inversely proportional to the scrubbing cost. However, there is no optimal scrubbing rate for all of the disks because of the variations within disks’ healthy statuses.

Previous works trade scrubbing cost for data reliability or vice versa. In this paper, by applying proactive error prediction to guide the scrubbing rate, we can achieve lower scrubbing cost together with higher data reliability. There are three differences between our method and previous works: (1) Instead of finding an optimal scrubbing rate for all of the disks, we attempt to adjust the scrubbing rate for each individual disk based on their health statuses. (2) Rather than sacrificing scrubbing cost or data reliability, we aim to achieve low scrubbing cost and high data reliability at the same time. (3) Besides LSEs, we also adjust scrubbing rates according to the “buthtub curve” disk failure pattern.

8. Conclusion
In storage systems, disk scrubbing has to be performed to maintain high data reliability in response to LSEs. However, the presence and severity of LSEs exhibited by individual disks bear wide variations, causing existing scrubbing schemes to be cost-ineffective. To address this problem, we propose a new scheme, scrub unleveling, to achieve high reliability at low scrubbing cost. By using the results of LSE prediction, we enforce a lower rate scrubbing to healthy disks and a higher rate scrubbing to disks subject to LSEs. Furthermore, we propose an adaptive+ scheme in which we accelerate scrubbing rate in the infant period and the wear-out period, and decelerate scrubbing rate in useful-life period.

To evaluate the effectiveness of scrub unleveling, we conduct mathematical analyses, simulations and experimental measurements. Experiments with a real-world dataset demonstrate the superiority of our method and experimental results on a small-size dataset confirm the robustness of the proposed prediction. Compared with the traditional fixed-rate scrubbing scheme, we can achieve the same level of reliability, in terms of MTTD, as the former with almost 49% less scrubbing costs or improve the MTTD of storage systems by a factor of 2.4X without extra scrubbing costs. Moreover, compared with the state-of-the-art approaches, our method can achieve the same level of MTTD with nearly 32% less scrubbing costs.

