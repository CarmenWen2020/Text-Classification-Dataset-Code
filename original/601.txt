1. Introduction
Context and motivation of the problem
In the context of digitalization and Internet-of-Things (IoT) infrastructures, fog/edge computing platforms are emerging distributed processing infrastructures that can address scalability issues of the cloud computing paradigm. In contrast to the latter, which assumes that data is globally gathered and analysis is performed into the cloud, edge computing refers to having data and analysis in the edge nodes; fog computing is broader and it refers to having data and analysis in intermediate processing nodes, i.e. an abstraction between the edge and the cloud.

Processing sensor data is an important problem that can benefit from such computing infrastructures. For coping with the rate and volume at which data is produced by certain types of sensors, efficient approximation approaches that summarize the data and process the summaries are required, for example in the spirit of the synopses methods introduced in [8]. The importance of data summarization and summary processing is more pronounced for processing data from high-rate sensors, such as LIDAR sensors.

LIDAR (LIght Detection And Ranging) is an accurate sensing method that gives a 3D scan of the environment in which it operates. LIDAR reads the distances by reflecting pulsed laser lights and measuring the reflected lights. A LIDAR sensor typically has a rotating head that emits several layers of laser beams, resulting in a high resolution 3D scan of the environment, possibly containing hundreds of thousands of points, by one full rotation of the head. LIDAR sensors can produce readings with rates of several MBps. A point cloud refers to the output of a LIDAR sensor [42].

The high resolution, accuracy, and 3D nature of the point clouds produced by a LIDAR sensor make such a sensor an attractive choice for autonomous detection and localization of objects (e.g., in environments such as industrial areas or motorways). The ability of detecting and localizing objects autonomously can be useful in numerous applications and use-cases, e.g. in autonomous driving vehicles [19].

It is likely that more than one LIDAR sensors are needed in order to scan and create detailed 3D models of complex and large environments [1]. This is needed, for instance, when LIDAR laser beams from a single sensor might not be able to reach some objects because of occlusion. Moreover, even for an object which is not occluded, the LIDAR reading can be limited due to the object’s disposition and the LIDAR’s location; i.e., LIDAR beams might not hit some portions of the object. Engaging several LIDAR sensors for gathering data from different points of view can help us overcome issues such as occlusion and partial views of objects. Moreover, it can increase robustness by adding redundancy to deal with failure cases. This opportunity can enhance resiliency and availability, e.g. for automated vehicles in production environments, for checking safety risks such as geofence detection in environments where robots and humans co-exist, to mention just a couple of possible uses.

In the following, we discuss the challenges that are introduced by a multi-LIDAR setup, in the problem of detection and localization of objects, utilizing fog and edge computing in data processing.

Challenges relative to the state-of-the art
Detection and localization of objects using data from several LIDAR sensors is a resource-demanding problem. Given the state-of-the-art, a method that can be considered as the baseline, is to (i) first gather the point clouds from the different sources (edge nodes), commonly connected through a shared wireless channel, to a fog node; (ii) afterwards apply an object detection algorithm, commonly a clustering algorithm [40], [41], on the union of sources’ point clouds, labeling each point accordingly. However, this approach is not practical due to the cumulative data volumes, resulting in: (i) prohibitive processing costs (at least linear in the point clouds’ size) even for state-of-the-art parallel clustering approaches [3], [12], [29], [35], [35], [40]; (ii) prohibitive communication bandwidth requirements; for example, simultaneous transmission of six or more high-resolution point clouds can take multiple seconds using a shared wireless communication medium of common rates [7], [26]; (iii) prohibitive latency, as a combined consequence of the above.

To cope with the aforementioned limitations, continuous data processing at the edge (i.e. performing distributed detection and localization for each LIDAR source) can be beneficial to overcome the aforementioned limitations. However, to that end, new aspects imply challenges in identifying a fast, continuous and distributed approach: (i) efficient representation of the local results, and (ii) efficient combination of the local results in order to generate a global result; furthermore, (iii) the representation of the local results should be accurate enough to facilitate obtaining high-quality object detection and localization and, at the same time, should be compact enough to minimize the communication footprint and the complexity of applications such as geofencing.

Contributions
We propose MAD-C, a multi-stage approximate distributed cluster-combining method for obstacle detection and localization, using point-clouds from multiple LIDAR sensors; MAD-C efficiently exploits the available decentralized processing capacity at the edge nodes and prevents saturation of the communication network. A key component of our algorithm is an efficient summarization method. First, MAD-C clusters each point cloud at each edge node attached to a LIDAR sensor, orthogonally building on known methods for fine-grained point-cloud clustering. Each local cluster corresponds to an object in the sensed environment from the perspective of the respective sensor (i.e. occlusion can have significant influences). Then, each edge node computes a local, constant size geometrical summary of each object and MAD-C lets the processing nodes combine their findings with those of other nodes, until a complete view is reached. The following points summarize the findings regarding the properties of MAD-C.

•
We show that the aforementioned summaries are computable in a continuous way, with constant overhead in time and space and can be combined in an order-insensitive concurrent fashion, in time that depends only on the number of objects and sensors instead of their point-clouds’ sizes; thus MAD-C can exploit the data parallelism in the problem. We further provide a detailed analysis of the asymptotic expected completion time of MAD-C, considering both the communication and processing overhead, over a binary tree or a flat tree (star) network topologies connecting the edge devices.

•
We provide an extensive experimental study of MAD-C, on an actual IoT testbed and on simulations, with real-world and synthetic data and varying number of nodes, to cover a wide spectrum of scenarios, including very demanding cases. Based on the results, we can conclude that the common view produced by MAD-C is close in accuracy to that of the aforementioned baseline. We also observe significant improvements in processing and communication time, which are the more important aspects for fog/edge architectures and for the usage of the algorithm in time-sensitive applications.

•
Furthermore, as a usage of MAD-C, we show how to leverage its properties in order to efficiently answer queries regarding geofences, e.g. to detect objects inside restricted areas in an environment.

An overview of MAD-C and results from a preliminary experimental study via simulations were first introduced in [27]. Here we provide a more detailed description of the algorithm, including procedures to tune key parameters, as well as the analytical study of its completion time. Furthermore, we provide a more extensive empirical evaluation of MAD-C with larger data volumes, over networks of various sizes, enabling to understand the scaling properties of MAD-C. Moreover, MAD-C is evaluated not only in simulation, but also in an IoT test-bed, comprising of representative fog/edge type devices. In addition, in the present work we explain how MAD-C’s summarizations can be used in other applications. Specifically, we present and analyze an application of MAD-C for the geofencing problem.

The paper is organized as follows: We cover preliminaries in Section 2. We present MAD-C and its algorithmic implementation aspects in Section 3. We analyze MAD-C’s time complexity in Section 4. We present extensions and examples of further usages of MAD-C in Section 5. We provide an empirical evaluation of MAD-C in Section 6. We present the related-work and conclusions in Section 7 and Section 8, respectively.

2. Preliminaries
This section describes the system model, the problem description, and the baseline solution. It also provides some background for self-containment of the paper.

2.1. System model and problem description
We assume K  asynchronous, interconnected nodes in the system. A processing unit and a LIDAR sensor at a known location and pose in the environment are associated with each node. In our model, nodes can be perceived as fog/edge devices.

Let 
 denote the point cloud (a set of points in the 3D space) that the LIDAR sensor associated with the th node, with a full rotation of its spinning head, collects. Also, let 
 be the number of the points in 
. The th node’s processing unit can process 
 locally as well as communicate raw and processed data to other nodes.

A (local) view refers to an individual 
, and the merged point cloud is the union of all the  point clouds. We consider that the views are collected at the same time, i.e. that the combined point clouds constitute a consistent snapshot of the scene. For simplicity and w.l.o.g, we assume views are expressed in the same coordinate system; otherwise, pre-processing can transform them into a canonical system: depending on each LIDAR’s disposition, a rotation matrix and a translation can be applied on its point-cloud.

The latter can be performed in conjunction with reading the points and filtering away the ground points [23], in constant time per point, as the pre-processing step. Let us introduce an example scenario in which MAD-C is deployed.

Example.
Fig. 1(a) shows a scene in which three LIDAR sensors are installed at 
, 
, and 
. Fig. 1(b–d) respectively visualizes the view of each of the 3 LIDAR sensors. Fig. 1(e) shows the merged point cloud. Notice that (i) there is at least one object missing in each local view and (ii) the views are complementary regarding the objects that are not occluded; e.g. they display almost non-overlapping segments of the car. Therefore, engaging more nodes to collect point clouds can result in higher accuracy.

We assume the existence of a spanning tree for the nodes to communicate and aggregate data [44]. Each node knows its children and its parent. Let the sink node be the root of the connection topology tree, in charge of generating a global view from data from all the nodes. We mainly consider a shared communication medium with low-bandwidth capacity (e.g. relying on wireless communication). We first present our methods under the spanning tree and no-message-loss assumptions, for ease of the presentation. Later on, we generalize our approach using known results in distributed systems.

The goal is to generate a continuous stream-compliant solution that utilizes the distributed network of nodes in the system to generate, at the sink node, a map that:

•
enumerates the objects;

•
for each object, provides a representation (e.g. volumetric and/or expressed as a set of points);

•
is based on the information from all the  local point clouds,

•
ensures high quality of detection.

Besides detection and localization of objects, this map should be usable in scenarios that require to check conditions relative to the environment, e.g. crossing of objects and dynamically defined borders, aka geofences. We elaborate on the precise definition of the geofence problem in the designated section, for ease of reference and for facilitating the reading of the paper.

The properties of interest in a solution to the problem are: (i) low time and communication complexity and (ii) high accuracy of the map. Regarding the former, we estimate the number of processing steps and the amount of information that needs to be communicated among the nodes. Regarding the latter, we use rand Index, commonly used to compare two clusterings (e.g. [45]). Rand index measures how much two clusterings of a sample set agree, based on the ratio of the number of pairs of elements that are either clustered together or separately in both clusterings, to the total number of pairs of elements.

2.2. Background and baseline
Performing cluster analysis on a 
 is a helpful technique in order to group the points in 
 based on the scene object that each point belongs (e.g., as discussed in [19]). To that end, there are several suitable clustering algorithms that can be applied on  point clouds to detect the scene objects, e.g. [3], [12], [32], [40].

Euclidean clustering.
As a means of detecting scene objects in a given 
, this algorithm partitions 
 into an a priori unknown number of clusters such that each cluster has at least  number of points, and within each cluster, each point lies in -radius neighborhood of at least another point in the same cluster, where  and  are two predefined values. Non-clustered points are identified as noise [40]. Using a kd-tree [5], a data structure for efficient indexing and neighborhood search queries, the Euclidean clustering algorithm’s expected and worst-case time complexities are respectively  and 
, see [40, Ch. 4], on a point cloud with  points.

Euclidean clustering is an established and widely applied method [41]. Moreover, regarding the problems that concern multiple sources of point clouds, the common practice in the literature is to perform processing on the merged point clouds, for example [43]. Simplicity and no data loss (i.e. taking every single reading into account) are the advantages of processing the merged point cloud. Given the aforementioned background, we aim for a solid baseline that provides a reliable ground-truth solution for the problem in Section 2.1 and is an established point of reference for comparison with our proposed methods. Therefore, we consider the following baseline:

Baseline.
The th node gathers 
 (via its attached LIDAR sensor) and, at the same time, listens/waits for the incoming data (point clouds) from its children (if any). It merges (in the sense of set union) 
 with all the point clouds from its children and transmits the result to its parent. This procedure continues for all the nodes until the sink node holds the merged point cloud from all the  nodes. Finally, the sink node performs the Euclidean clustering algorithm on the merged point cloud.

Observation 1

Using a kd-tree for neighborhood search, the expected and worst-case computational complexities of the baseline method are respectively  and 
, where 
, i.e., the size of the merged point cloud.

3. The MAD-C algorithm
Here we describe MAD-C and how it meets the challenges and goals explained in the previous sections.

3.1. The key idea of MAD-C
In a nutshell, each node  in MAD-C clusters 
 locally and forwards compact summaries of its local clusters. Those summaries get combined with the ones of other nodes along a spanning tree, up to the sink node, which then can deliver the set of global objects. Compared to the baseline presented in Section 2.2, MAD-C drastically reduces the volume of data to be transmitted, while it pipelines and distributes the analysis.

Questions arising are: how to efficiently (i) generate local maps, i.e. summaries of the local clusters in the local views; and (ii) gradually combine the maps in a deterministic fashion, despite network asynchrony. Below we elaborate on these questions and in the following subsections we explain how we treat them.

Consider two local clusters 
 and 
 from two distinct nodes. According to the outcome of the baseline, 
 and 
 constitute a bigger cluster if they overlap (or have points in the  neighborhood of each other). In that case, 
 and 
 should be merged. But how can we determine if 
 and 
 meet the conditions to be merged without calculating pairwise distances among all the points in 
 and 
 (i.e. the cost of a global solution)? It certainly is highly desirable if such a decision can be made with  time complexity.

Commonly used tree-based nearest neighbor searches cannot provide a solution with  time complexity to the aforementioned problem, for instance check [30]. Furthermore, simply considering the distances between the centroids of 
 and 
 is not enough, because the geometrical volume that the two clusters take is also important. Therefore, we propose the MAD-C summarization technique that captures centroid, orientation, and size of each local cluster.

3.2. Generating local maps by efficient summarization of local clusters
Ideally, the summary of a local cluster  (i) uses small space (not growing with the number of points in ), (ii) can be built incrementally as new points are added in , (iii) can be shared with peers as soon as all ’s points are found, and (iv) expresses the geometrical volume that  occupies, to allow for comparisons and merging with overlapping or nearby clusters.

We identified that bounding ellipsoids satisfy the aforementioned requirements. Therefore, we pursue to fit a bounding ellipsoid for summarizing each local cluster. But how can we efficiently characterize the bounding ellipsoid corresponding to a local cluster? The contour surfaces of a three-dimensional Gaussian distribution are ellipsoids, where a contour surface is the set of all equi-probable points, and a three-dimensional Gaussian probability distribution is fully characterized by a mean vector 
 (the centeroid of the distribution) and a covariance matrix 
 (the spread of the data points) [21].

The family of ellipsoids corresponding to the contour surfaces of a 3-variable Gaussian distribution are characterized by 
, where  is a scaling factor, which we call the confidence step. All the ellipsoids in the family are centered at . With a given , each of the three eigen-vectors of  defines the direction of a principal axis of the ellipsoid with the length being equal to the square root of the corresponding eigenvalue multiplied by  [21].

Observation 2

The Gaussian fit through maximum likelihood estimation [21], allows to calculate a bounding ellipsoid incrementally by calculating  (’s number of data points), 
 (cumulative vector sum of ’s data points) and 
 (cumulative sum of outer products of ’s data points). As soon as all the points in  are identified,  and  of the corresponding bounding ellipsoid  are calculated as  and 
, respectively.

Observation 3

The representation of a bounding ellipsoid, in terms of  and , summarizing a local cluster  takes constant storage size, independent of . Furthermore,  and  can be calculated incrementally as points are being added to , with constant computational overhead per point.

Example.
Figs. 2(a), 2(b), and 2(c) respectively show the local maps corresponding to Figs. 1(b), 1(c), and 1(d). Note that the ellipses displayed in Fig. 2 symbolically illustrate bounding ellipsoids summarizing local clusters. The delimiting boxes displayed in Fig. 2 are explained in Section 3.4.

Definition 1

A map  is a set of objects. An object  is a set of (bounding) ellipsoids. Two objects are similar, if there exists at least a pair of geometrically overlapping ellipsoids (one from each object).

Stage one.
The th MAD-C node applies the Euclidean clustering algorithm (or alternatively any relevant distance based clustering method) on 
, producing local map 
. This is done distributedly, i.e. concurrently with other nodes. Procedure  in Algorithm 1 outlines MAD-C’s first stage for the th node. Lines 4–10 in Algorithm 1 show how incremental ellipsoid fitting is pipe-lined into the local clustering, with constant overhead per point (see Observation 3). Moreover, lines 11–20 in Algorithm 1 show how objects in 
 are initialized by the ellipsoids whose  and  are computed using Observation 2. We explain how to find the value of the confidence step, , in Section 3.4. The highlighted lines in Algorithm 1, correspond to algorithmic implementation details that are discussed in Section 3.4.

MAD-C’s next stage is explained in the following subsection, where we study how the maps get combined by merging the similar objects in order to generate a global map.

3.3. Towards a global map: Combining maps
In MAD-C’s second stage, each none-leaf node  receives maps from its children. It updates 
 by combining it with its children’s maps and, if it is not the sink node, forwards the result to its parent.

Notice that if the combining was performed over the actual local clusters rather than their summaries, two local clusters (each one with at least  points) would become one if at least a pair of points (one from each) is found within distance . Similarly, objects in 
 and each child map 
 are compared to detect if they are similar (see Definition 1), i.e. if they contain ellipsoids that geometrically overlap. If so, those pairs of objects are merged; i.e. the union of their ellipsoids is recognized as one object in the resulting map. In Section 3.4 we explain how (i) to integrate  in an ellipsoid’s representation, and (ii) to check in constant time if two ellipsoids geometrically intersect.

Notice also that applying the baseline on the merged point cloud (excluding the local noise points) results in clusters that each contain one or more local clusters from the local views. The latter holds because the points constituting a local cluster will still be clustered together (possibly with points from other local clusters) when the Euclidean clustering algorithm is applied on merged point cloud. In the same manner, the objects returned by MAD-C’s sink node are sets of ellipsoids, where each ellipsoid corresponds directly to a local cluster. In other words, both methods provide a clustering of the local clusters.

Observation 4

Let  be the set of all local clusters (from different point clouds). Based on the above discussion, both baseline and MAD-C perform clustering on the elements of . In order to measure how much MAD-C is able to capture the clustering behavior of the baseline, we apply a clustering similarity measurement (such as rand index) on the respective clustering outcomes of MAD-C and the baseline on the elements in .

Stage two.
In this stage, the th MAD-C node executes procedure  shown in Algorithm 2, distributedly and in parallel with other nodes. Lines 1–4 in Algorithm 2 show that for every child node , the th node receives the child map 
 and updates 
 by combining it with 
. If th node is not the sink node, when all the  operations are finished, 
 is transmitted to the th node’s parent. Lines 5–13 in Algorithm 2 show how the  operation is performed on 
 and 
: all pairs of similar objects (one from 
 and one from 
) are identified and merged as one object in the resulting map (the highlighted lines are implementation details covered in Section 3.4).

Lemma 1

Operation  on maps containing ellipsoids with unique identities, satisfies the reflexive, symmetric and associative properties.

The above follows from line 10 in Algorithm 2, ensuring that if 
 and 
 have intersecting ellipsoids, then 
 and 
 will be merged regardless of the order of execution.

Example.
Fig. 2(d) shows the map resulting from  (
, 
). Fig. 2(e) shows the  result of the latter and 
.

3.4. Algorithmic implementation aspects of MAD-C
In this section, we cover three implementation-related algorithmic aspects of MAD-C.

Determining if two ellipsoids geometrically overlap.
Given a pair of ellipsoids 
, the method described in [2] determines in constant time whether they intersect. It first characterizes 
 respectively as 
 and 
, where  and  are 4 × 4 matrices that (i) are derived using their centroids and covariance matrices by extending with a default row and column; and (ii) can be used to detect if there is at least an admissible eigenvector (one that does not have a zero in the fourth dimension) of 
 that satisfies both equations; in the latter case 
 overlap.

Aura: Integrating  in ellipsoids’ representations.
If the minimum distance of pairs of points from two objects is less than , then they are grouped together by the Euclidean clustering algorithm. We target the same behavior for the ellipsoidal models, adding an aura  around them. To do so, we simply increase the lengths of the main axes by this constant. Algorithm 3 shows how to update the covariance matrix of an ellipsoid for that purpose.

Determining the value of , i.e. the confidence step.
As discussed in Section 3.2, MAD-C summarizes local clusters by bounding ellipsoids. Observation 2 shows how to find the parameters  and  for an ellipsoid characterized by 
, where  (the confidence step) is a scaling factor on the size of the bounding ellipsoids. The appropriate value for  is data-dependent, hence harder to estimate in a data-agnostic way. For instance, consider the bounding ellipsoids in the example in Fig. 2. With a too small , the bounding ellipsoids get too small to correctly cover the local clusters. On the other hand, with a too big , the bounding ellipsoids erroneously span over other local clusters as well. Therefore, to find a proper value for , the analyst that deploys MAD-C can apply a two-step empirical method. In the first step, the effectiveness of bounding ellipsoids for summarizing clusters can be validated for a wide range of  values. Afterwards, the clustering accuracy of MAD-C can be estimated for the  values that result in the most effective bounding ellipsoids. Finally, an  value for sample data that correspond to the deployment environment of the application, that results to the desired accuracy, can be chosen. In the following, we discuss how the effectiveness of bounding ellipsoids and the accuracy of MAD-C can be estimated.

•
Measuring Effectiveness of Bounding Ellipsoids. For measuring the effectiveness of bounding ellipsoids, we consider the fact that MAD-C utilizes bounding ellipsoids as models summarizing clusters. To that end, recall and precision [36], two well-known model evaluation metrics, can be leveraged. For any particular local cluster  and its corresponding bounding ellipsoid , precision measures the ratio of the number of points correctly covered by  to the total number of points covered by . On the other hand, recall measures the ratio of the number of correctly covered points by  to the total number of points in .

•
Measuring Accuracy. For estimating the accuracy of MAD-C, we consider the fact that both MAD-C and the baseline perform clustering on the set of local clusters (see Observation 4). Therefore, to measure the agreement between the clustering outcome of MAD-C and that of the baseline, rand index (see Section 2) can be employed.

The delimiting box heuristic.
Checking if two ellipsoids overlap (i.e. are similar) is not always necessary; such checks can be saved if e.g. they occupy distant areas of the scene. We propose delimiting boxes of objects to reduce the number of ellipsoid comparisons in determining if two objects are similar (see Definition 1). An object’s delimiting box is an axis-aligned rectangular shape that encapsulates all the ellipsoids corresponding to that object as shown in line 12 in Algorithm 2. Similarly, an ellipsoid’s delimiting box is the smallest axis-aligned circumscribed rectangle encapsulating that ellipsoid. Therefore, an ellipsoid’s delimiting box is characterized by one closed interval on each axis as shown in line 18 in Algorithm 1. This heuristic does not lead to any false negatives because if the delimiting boxes corresponding to objects 
 and 
 do not overlap, then the two objects are not similar. On the other hand, 
 and 
 are not necessarily similar if their delimiting their boxes overlap — in that case a pairwise comparison between the ellipsoids in the two objects has to be performed to determine if they are similar or not. The exact saving due to this heuristic is largely data-dependent and hence harder to estimate asymptotically, in a data-agnostic way. In Section 6 we will empirically compare the actual number of comparisons with and without this heuristic. Please note that applying the delimiting box heuristic does not affect the validity of the properties discussed in Lemma 1.

4. MAD-C’s completion time analysis
In this section, we aim to characterize the asymptotic completion time behavior of the sink node in MAD-C.

4.1. Assumptions, notations, and definitions
We assume the existence of a global clock just for the ease of exposition. Furthermore, we assume, for the local clustering, every MAD-C node leverages the Euclidean clustering algorithm employing a kd-tree for -neighborhood search (see Section 2.2). We assume that all MAD-C nodes start working at . Let MAD-C nodes be numbered , and let  be the index of the sink node. In a binary tree topology, let  and  respectively be the indices of the left and right child of the th node (if applicable). For , let 
 be the number of points in 
 and 
 be the maximum value of 
s. Let 
 denote the number of nodes in the sub-tree in which the th node is located, including the th node itself. For instance, 
 is . Let  denote the number of actual objects in the environment, as detected by the baseline introduced in Section 2.2. Finally, let  denote the number of ellipsoids in map . Table 1 summarizes the notations that we use.

Assumption 1

For the sake of the analysis, we assume that the asymptotic number of local clusters in each local view is .

The above is a sensible assumption for common cases because, in each local view, while some environmental objects might be entirely occluded, others might split up into smaller ones, thus detecting  number of local clusters on average. As might be expected, this assumption is data-dependent, and there can be rare situations where the occlusion due to certain arrangement of objects in an environment increase or decrease the asymptotic number of detected local clusters.


Table 1. Table of notation.

Number MAD-C nodes
Number of actual objects in the environment (as determined by the baseline)
Confidence step
The th node’s local point cloud
The th node’s working map
The number of objects represented in 
 
 		Total number of ellipsoids in 
Number of points in the merged point cloud (
)
Number of points in 
Maximum number of points in all the point clouds (
)
The number of nodes in the sub-tree in which the th node is located
A path in the topology tree that starts from the th node (a leaf node) and ends at the th node
The completion time of the th MAD-C node
The amount of time that the th MAD-C node spends on local clustering and creating ellipsoidal models
The amount of time that the th MAD-C node spends on combining its map with those of its children’s
The amount of time that the th MAD-C node spends on transmitting its map to its parent
The amount of time that the th MAD-C node spends on waiting to receive all the maps from its children
The execution time of each computation step
The transmission time of each unit of data
Definition 2

Regarding the th MAD-C node, let 
 be the amount of time that the node spends on local clustering and creating the ellipsoidal summarizations. Moreover, let 
 be the total amount of time that the node spends on waiting to receive maps from its children (if applicable). Furthermore, let 
 be total amount of time that the node spends on combining its map with those of its children’s (if applicable). Finally, let 
 be the amount of time that the node spends on transmitting its map to its parent (if applicable). All in all, the completion time of the th MAD-C node, 
, is characterized as the following: 
.

As the completion time of the th MAD-C node has computation-related and transmission-related factors, let the execution time of each computation step and the transmission time of each unit of data be denoted by 
 and 
, respectively. Note that for the latter to be a constant there is a simplifying assumption that each node has dedicated bandwidth.

4.2. Characterizing the asymptotic behavior of components of the completion time
In this subsection, we present the asymptotic behavior of MAD-C’s completion time components. Using the latter, we will derive asymptotic bounds on the sink node’s expected completion time.

Characterizing the asymptotic behavior of 
Lemma 2

The expected asymptotic behavior of 
 is
, choosing the Euclidean clustering algorithm with the kd-tree for -neighborhood search as the local clustering algorithm and assuming that the resulting kd-tree is balanced and the value of  is small enough to return constant number of points per each nearest neighbor search query.

Proof

The expected asymptotic time complexity of the th node’s local clustering is 
 as 
 number of nearest neighbor search queries is performed each with 
 complexity. Moreover, based on Observation 3, the asymptotic amount of time that the th node spends on fitting bounding ellipsoids is 
. Considering the two contributing terms, the argument in Lemma 2 is proven.  □

Characterizing the asymptotic behavior of 
In order to analyze the asymptotic behavior of , we need to characterize the execution time of the  operation first. Note that in our analysis, we do not consider the effect of the delimiting box heuristic presented in Section 3.4 because its effect is data-dependent. Nevertheless, in practice, as we will see in the empirical evaluation section (Section 6), the delimiting box heuristic reduces the number of ellipsoid comparisons performed by the  operation.

Lemma 3

The number of ellipsoid comparisons that are performed by the  operation on 
 and 
 is bounded by 
 and 
 from below and above respectively.

Proof

 checks the similarity (see Definition 1) of 
 pairs of objects because the asymptotic number of objects in each map is  based on Assumption 1.

For any given pair of objects, the similarity check immediately returns as soon it realizes whether the objects are similar or not. Therefore, in the best case, it returns with  ellipsoid comparisons for every pair of objects. In the worst-case, however, the similarity check compares every pair of ellipsoids in 
 and 
, in every pair of objects in 
 and 
, giving the following upper bound on the number of ellipsoid comparisons: 
  □

Corollary 1

.

The above corollary is derived from Lemma 3 and noticing that comparing two ellipsoids requires  computation steps, as discussed in Section 3.4.

Lemma 4

Let  be the  result of two arbitrary maps 
 and 
. The number of ellipsoids in  equals the sum of number of ellipsoids in 
 and 
, i.e. 
.

Proof

For any ellipsoid  in either 
 or 
, there exists an object 
 in the resulting map 
 that  is a member of 
. Conversely, the objects in 
 are composed of ellipsoids from either 
 or 
. Therefore, regardless of how the  operation merges the similar objects in the two maps, the total number of ellipsoids in 
 is always the sum of number of ellipsoids in 
 and 
. □

The following lemma presents bounds on the asymptotic behavior of  for MAD-C nodes with a flat-tree (i.e. star) or binary tree connection topology.

Lemma 5

For any leaf node , 
 is zero. Otherwise, assuming a star or binary tree connection topology, the following bounds hold on 
: 
.

Proof

For any leaf node, 
 is zero because the leafs do not perform any combine operations. We proceed with the proof in two cases, when the connection topology is a (i) star, and (ii) a binary tree.

Star Topology. With a star topology, only the combine time corresponding to the sink node is non-zero. Therefore, let us derive 
. The sink node sequentially combines 
, its map, with those of its children’s. Suppose, w.l.o.g., that the sink node performs the  operations in the following order: 
, , 
. Initially, according to Assumption 1,  
  holds for all nodes. However, as 
 gets updated by each  operation,  
  changes after each update. More specifically, after the th  operation,  
 , check Lemma 4. Therefore, applying the lower and upper bounds in Corollary 1, the best-case and worst-case execution time of the th  operation are 
 and 
, respectively. Summing up the lower bounds for different values of , we get 
 (note that the latter is a tighter lower bound than the one introduced in Lemma 5). On the other hand, summing up the upper bounds for different values of , leads us to claimed upper bound, as follows: 

Binary Tree Topology. Let us start with deriving the lower bound. A non-leaf MAD-C node performs two combine operations, one on its left child’s map (
) and one on its right child’s map (
). Applying the lower bound in Corollary 1, 
 is a lower bound on the execution time of the th node’s combine operations.

Now, let us derive the upper bound. Suppose, w.l.o.g., that the th node first combines 
 with 
 and then with 
. The worst-case execution time of the first combine operation is bounded by 
 
 because at the time of combining  
  is , and  
  is . However, after the first combine operation,  
  is 
 
 (based on Lemma 4). Consequently, applying the upper bound in Corollary 1, the worst-case execution time of the second combine operation is bounded by 
 
, concluding the upper bound 
 on 
.  □

Corollary 2

The following bounds hold on the combine time of the sink node in MAD-C for a star or binary tree connection topology: 
.

Characterizing the asymptotic behavior of 
The following lemma presents the expected transmission time for any MAD-C node.

Lemma 6

 is zero. For , the asymptotic behavior of 
 is 
 
).

Proof

The first argument (
) holds because the sink node does not perform any transmissions. We prove the second argument using induction.

Base case. According to Assumption 1, the asymptotic number of local clusters in any local view is . Accordingly, the asymptotic number of bounding ellipsoids that a leaf node transmits to its parent is . According to Observation 3, the representation of each bounding ellipsoid takes constant size in space. Therefore, the transmission time for a leaf node asymptotically takes 
). Considering that 
 is 1 for a leaf node, the base case is proven.

Inductive step. Suppose that the argument holds for children of the th node, indexed by 
. The th node initially contains  ellipsoids in its map. However, after it performs combine operations on the maps from its children, the asymptotic number of ellipsoids in its map is  
  
), based on Lemma 4. On the other hand, 
 is equal to 
  
+1. Therefore, the expected asymptotic transmission time of the th node is 
 
). By mathematical induction, the argument is proven. □

Characterizing the asymptotic behavior of 
Unlike the other three time components, 
, the th node’s waiting time, cannot be analytically characterized on its own because the amount of time that a node waits is execution dependent. Our next best alternative to an analytical characterization is deriving upper and lower bounds on 
.

Lemma 7

If the th node is a leaf node, then 
 is zero. In general, the following bounds hold: 
, where  is a path in the topology tree that starts at the th node, which is a leaf node, and ends at the th node.

Proof

The lower bound is an optimistic estimation which holds when all the th node’s children’s maps are available before the th node has finished its local clustering task. On the other hand, the upper bound is a pessimistic estimation that holds when the th node finishes its local clustering task very early and has to wait until the th node finishes its local clustering task. Additionally, the th node has to wait until all the nodes in the path  finish their combine operations and transmit the results until the th node receives the map from its slowest child. Therefore, the path  that maximizes 
 determines an upper bound on 
.  □

Suppose 
 is the path that starts from the leaf node indexed by 
 and reaches the th node and maximizes the upper bound in Lemma 7 for the th MAD-C node.

Lemma 8

The following bound holds for the th MAD-C node: 
.

Proof

 
 
 
 
 

Lemma 9

.

Proof

The lower bound in Lemma 9 is clear, so we prove the upper bound in Lemma 9 as the following: 

4.3. Characterizing the completion time 
Definition 2 explained that the th node’s completion time comprises four components. Employing the characterizations of the components (covered in Lemmas 2, 6, 5, and 9), we aim to derive lower and upper bounds on the expected completion time of the sink node.

Theorem 1

The following bounds hold on the expected completion time of MAD-C’s sink node with a star or a binary tree connection topology: 
.

Proof

Based on Lemma 6, 
. Therefore, considering Definition 2 and the linearity of expectation, 
 is equal to 
. We first derive the lower bound on 
. 

In the last statement, the best-case asymptotic behavior of 
 (see Lemma 5) is used as an asymptotic lower bound on 
. This proves the lower bound in Theorem 1. We now prove the upper bound on 
. 

In practice, we expect that the number of points in each local point cloud (possibly containing hundreds of thousands of points) to be much larger than the number of nodes () and the number of actual objects (). Therefore, with small enough 
 (i.e. fast enough transmission links), the asymptotic terms containing 
 and 
 are the dominating factors in the expected completion time of MAD-C’s sink node. In other words, we expect that either the local clustering of the sink node or one of its descendants to be the dominant factor in MAD-C’s completion time.

Observation 5

The longest local clustering dominates MAD-C’s completion time in practice.

5. Extensions and examples of further usages of MAD-C
In this section, we describe how MAD-C can form the core of a set of approximations for extended usage. We also explain how MAD-C’s ellipsoidal models can be further employed for efficiently processing geofence queries regarding the fusion of multiple point clouds.

5.1. Extensions
Versatile communication methods for MAD-C nodes.
Lemma 1 implies that the maps and the  operation, satisfy the properties of conflict-free replicated data types [37]. Consequently, the network topology and timing asynchrony do not affect the final map at the sink node. Moreover, the  operations can be executed using non-atomic multicasting, similar to gossiping or selective flooding, guaranteeing eventually consistent final outcome and inherent fault-tolerance properties [44]. Therefore, the spanning tree assumption in Section 2.1 can be lifted and besides the sink node, also any other node can construct the global map.

MAD-C-ext For delivering data point labels.
MAD-C can be extended into providing a clustering label for each point in the merged point cloud, similar to what the baseline in Section 2.2 does. The extension is as follows: Consider a leaf MAD-C-ext node indexed as . In addition to 
, the th node transmits 
 and the corresponding local clustering labels. Its parent node, indexed as 
, after having performed the regular tasks of a MAD-C node, relabels the points in the union of point clouds from its own and its children based on the updated 
. It then, in turn, transmits 
 along with its merged point cloud and clustering labels to its parent. As the process continues, finally the sink node holds the final merged point and its corresponding clustering labels.

Observation 6

The clustering accuracy of MAD-C-ext compared to the baseline in Section 2.2 can be computed by applying a clustering similarity measurement (such as rand index) on the clustering outcomes of MAD-C-ext and the baseline.

5.2. Geofencing with the fusion of LIDAR point clouds
MAD-C’s ellipsoidal models can be leveraged to approximately but efficiently answer queries regarding the merged point cloud. Queries regarding geofencing are considered useful, considering that a geofence defines a predetermined perimeter in an environment marking a hazardous area for instance. We explain how the summaries produced by MAD-C, without needing to have access to the original gathered point clouds, make it possible to answer dynamically changing geofence queries. Let us give the useful definitions before we formally define the problem.

Definition 3 Plane

A plane in a 3D space is characterized by 
, where 
 is known as the plane’s normal vector, and 
 is an arbitrary point on the plane. Any point that satisfies the characterization equation is on the plane. The plane’s positive/negative side contains all the points that make the characterization equation greater/less than zero.

Definition 4 Geofence

A geofence is the area enclosed by the intersection of positive sides of a finite number of planes. For a geofence , let    be the number of enclosing planes.

Note that a geofence can have an arbitrary shape and size, but with a large enough number of planes any arbitrary shape can be approximated with the desired accuracy. Also note that Definition 4 allows a geofence to be much more general than just a regular perimeter, e.g., a polyhedron can be a geofence according to Definition 4. We suppose that the intersection of the positive sides of planes in  is non-empty.

Note that, in the literature, most geofencing problems concern positioning systems such as global navigation satellite system (GNSS) to determine whether an object has entered/exited boundaries of a geofence, for example [46]. However, not much work exists on geofencing with LIDAR data. In the following, we describe the requirements of the problem.

The geofence-crossing problem with LIDAR data.
Given a geofence  and a clustering  of  data points, find out the clusters that violate , i.e. the clusters that fall inside or cross the geofence at one or more of its boundaries.

We first outline a baseline approach to address the geofencing-crossing problem, before presenting a solution based on MAD-C’s ellipsoidal models.

A baseline solution to the geofence-crossing problem with LIDAR data.
For each cluster , one can check every point in  to see whether it falls inside or crosses the geofence . If at least one such point is found, then the whole cluster  is identified as falling insider or crossing . Notice that the worst-case number of processing steps required by the straightforward solution to the geofence-crossing problem is   ). The latter holds because, in the worst-case, every point has to be checked against all the    number of planes in , and it takes constant number of processing steps to determine to which side of a plane any given point falls.

Adopting MAD-C as a solution to the geofence-crossing problem with LIDAR data
We discuss here how MAD-C’s ellipsoidal summaries can be employed to efficiently find out the objects that are located inside a geofence or cross it at one or more of its boundaries, i.e. the objects that violate the geofence.

Definition 5

An object 
 violates a geofence , if at least one ellipsoid in 
 violates . An ellipsoid  violates a geofence , if, for every plane  in ,  either falls on the positive side of  or crosses .

Based on Definition 5, we need to be able to determine the relative position of an ellipsoid with respect to a given plane  which is either of the three following possibilities: (i) The ellipsoid is on the negative side of . (ii) The ellipsoid is on the positive side of . (iii) The ellipsoid intersects . In the following we study how to determine the relative position of an ellipsoid  with respect to a given plane .

Definition 6

Given a plane  and an ellipsoid , let points 
 and 
 respectively represent the lower and upper bounds of the orthogonal projection of all ’s points on the normal vector of .

Lemma 10

An ellipsoid  neither crosses nor falls inside the positive side of  if and only if both 
 and 
 are on the negative side of .

Proof

Consider Fig. 3, where the relative position of ellipse  and the thick line that symbolically represents a plane are of interest. The thin line shows the plane’s normal vector () passing through an arbitrary point 
 on the plane. The point denoted by 
 shows the orthogonal projection of ’s centroid on the normal vector. The following three cases determine ’s relative position with respect to the plane:

•
If both 
 and 
 are on the positive side of , then  is also located on the positive side of .

•
If 
 is on the positive side of , but 
 is on the negative side of , then  intersects .

•
If both 
 and 
 are on the negative side of , then  is also located on the negative side of . □



Download : Download high-res image (218KB)
Download : Download full-size image


Download : Download high-res image (161KB)
Download : Download full-size image

Download : Download high-res image (76KB)
Download : Download full-size image


Download : Download high-res image (181KB)
Download : Download full-size image
Lemma 11

All the points that fall within/on the boundaries of ellipsoid  characterized by 
, are members of the following set and vice versa (the members of the following set fall within/on the boundaries of ): 

Proof

We note that 
 is equal to 
 
 
 because  is a symmetric positive-definite matrix. Accordingly, we can characterize the points that fall within/on the boundaries of  as the following: 
 
 
 
 

We now present a theorem that shows how the values of 
 and 
 are calculated.

Lemma 12

For a given ellipsoid  characterized by centroid vector  and covariance matrix , and a given plane  with normal vector  passing through 
, 
 and 
 are determined as the following: 
 
 
.

Proof

The orthogonal projection of any point  on the normal vector is 
 
. Applying the latter on the representation of  given in Lemma 11, we derive 
, the set of points corresponding to the orthogonal projection of the points in  on the normal vector passing through 
: 
 
 
 
Since  can arbitrarily be chosen from a ball with radius one, we can derive the following bounds on 
: 
 
 
 
 
With reference to the above inequalities, we conclude that 
 
 
.  □

Now that we know how to determine the relative position of an ellipsoid with respect to any given plane, we proceed with explaining our novel method for determining the objects that violate a given geofence .

.
Given a map 
 filled with MAD-C’s objects, we iterate through each object 
, as shown in line 2 in Algorithm 4, in the map to determine which ones violate the geofence . To that end, if at least one ellipsoid  in 
 violates , then 
 is marked as violating  as shown in lines 3 and 4 in Algorithm 4.

Operation .
In order to find out if an ellipsoid  violates a geofence , for every plane  in , 
 and 
 are calculated using Lemma 12 as shown in lines 10 and 11 in Algorithm 4. If the conditions in Lemma 10 (corresponding to line 12 in Algorithm 4) hold for at least a plane  in , then  neither crosses nor falls inside ; therefore,  does not violate  (see Definition 5). On the other hand, corresponding to line 14 in Algorithm 4, if  crosses or falls inside every  in , then  violates .

Lemma 13

In the worst-case,  asymptotically requires  
   ) processing steps.

Proof

In the worst-case, operation  has to be called for every ellipsoid in 
 — remember from Definition 1 that  
  denotes the total number of ellipsoids in 
. Each time, the maximum number of times that the for loop in line 7 in Algorithm 4 gets executed is   . Therefore, considering that 
 and 
 are evaluated in constant number of processing steps, in the worst-case,  asymptotically requires  
   ) number of processing steps.  □

6. Empirical evaluation
We empirically evaluate MAD-C and MAD-C-ext (introduced in Section 5.1) from different perspectives. In order to perform a thorough evaluation, we conduct experiments using both a proof-of-concept implementation on an IoT test-bed, as well as a virtual-machine-based simulation. The simulation environment gives us flexibility in adjusting the parameters of the experiments to study scalability and aspects related with larger networks and more data, while the experiments in the IoT test-bed give the actual results that could be expected in a real-world MAD-C setup.

Concretely, the evaluation provides a study on (i) estimating the value of the confidence step (), (ii) the completion time and scalability aspects of MAD-C and MAD-C-ext compared with the baseline, and (iii) the components of MAD-C’s completion time. The study concerns the influence of the following parameters: the number of nodes (), the number of scene objects (), and the topology of the inter-connected nodes, i.e., a star or a binary tree connection topology, as motivated in the analysis section.

Regarding (i), we use the procedure explained in Section 3.4 to determine the value of . Regarding (ii), we study the completion time of MAD-C in accordance with Theorem 1. In the same fashion, we present the completion time of MAD-C-ext, reflecting the overheads that it introduces to MAD-C. Regarding (iii), considering MAD-C’s time components introduced in Definition 2, we study  (the combine time) in accordance with Lemma 5, and  (the transmission time) in accordance with Lemma 6. We complement the study of the combine time by evaluating the effect of the delimiting box heuristic (presented in Section 3.4) on the number of ellipsoid comparisons. To that end, we find the average number of ellipsoid comparisons with and without the delimiting box heuristic.

6.1. Evaluation setup
We implemented MAD-C and MAD-C-ext in C++1 and used the GNU scientific library [15] for matrix algebra. The functionalities of network communication are implemented using the Boost. Asio library [28]. For the baseline and local clusterings, we employed the Euclidean clustering (cf. Section 2) algorithm in the Point Cloud Library [41], with  and  respectively set to 0.35 and . With these values, the baseline reasonably detects all objects in the scenes and provides a reliable ground-truth. For time measurements, we used elapsed real time.

We indexed the nodes from  to . The node indexed by  is the sink node in both topologies. Furthermore, in a binary tree topology,  and  are respectively the indices of the left and right children of the th node, while height of a node denotes the number of edges on the longest path from the node to a leaf. In the following, we give the setup details regarding the execution of MAD-C in the virtual-machine-based simulation and the IoT test-bed.

Virtual-machine-based simulated MAD-C.
On a 2.1 GHz Intel(R) Xeon(R) E5-2695 (with 3.3 GHz maximum turbo frequency) server supporting 72 threads (including hyperthreading), we emulated a range of networks sizes, ranging from 7 to 20 LIDAR sensors (i.e. MAD-C nodes), representing a reasonably high number of nodes in a realistic MAD-C deployment. Each emulated node was run inside an Oracle VM VirtualBox machine [34]. As the operating system, each virtual node ran an Ubuntu 18.04, and it was assigned 2 GB of memory and 7 hyper threads. For networking among the nodes, the virtual machines operated under the host only adapter option in the VirtualBox. The networking bandwidth for each simulated MAD-C node was limited to 100 Mbps via the VirtualBox settings.

MAD-C in an iot test-bed consisting of resource-constrained nodes.
These experiments, acting as both a proof-of-concept study and a validation study of the simulated systems, were run on a test-bed consisting of five ODROID-XU3 devices. Each ODROID-XU3 device is a single-board computer equipped with a Samsung Exynos 5422 Cortex-A15 2.0 GHz quad core and Cortex-A7 quad core CPUs with 2 GB of memory. Each ODROID-XU3 ran an Ubuntu 18.04 as the operating system and was connected to a switch with 100 Mbps bandwidth per port.

6.2. Evaluation data
We used both real and synthetic LIDAR data sets. We generated synthetic LIDAR data sets corresponding to several scenarios, each with different characteristics in order to ensure an unbiased evaluation. The scenarios were generated by the webots simulator [31], which simulates real-world LIDAR sensors (Velodyne HDL-32E, in our case) and 3D scenes. Our first synthetic scenario resembles a factory environment with Automated Guided Vehicles, lifting arm cranes and related objects, where four LIDAR sensors are placed at the corners of the scene and one in the middle as shown in Fig. 4(a). Moreover, we generated random scenes filled with a variety of objects, as small as cubic boxes (with lengths of 80 cm) to objects as big as cars, over an area of 
, where seven LIDAR sensors are placed in different locations as shown in Fig. 4(b). Each object is randomly rotated around its vertical axis to vary the angle with which it is exposed to the LIDAR sensors. Under the aforementioned settings, we generated 300 synthetic scenarios with 10 scene objects, 300 synthetic scenarios with 50 scene objects, and 300 synthetic scenarios with 100 scene objects, respectively denoted by syn-10, syn-50, and syn-100.

Regarding real-world LIDAR data, we utilized the Ford Multi-AV Seasonal Data Set [1], gathered by vehicles driving through the greater Detroit area, including a university campus, the DTW airport, and residential communities. Four Velodyne HDL-32E LIDAR sensors, mounted on the four corners on top of each vehicle, gathered the LIDAR data. We randomly chose a subset of the Ford Multi-AV Seasonal Data Set and split each merged point cloud into 20 overlapping partitions, reminiscent of a twenty-LIDAR sensor setup in which the sensors perceive the environment in a way that there is both redundancy and occlusion, and capture overlapping and complementary measurements. We also utilized the point clouds in the KITTI data set [17], gathered by a Velodyne HDL-64E LIDAR sensor mounted on a car driving around in urban and rural areas, see Fig. 4(c) for an example. We randomly chose 43 point clouds from the KITTI data set. Since the LIDAR point clouds in the KITTI data set were gathered by a single source, we only used them to show the effectiveness of bounding ellipsoids in modeling the local clusters.


Download : Download high-res image (924KB)
Download : Download full-size image
Fig. 5. Average recall and precision with one standard deviation (confined within the zero to one interval) show the effectiveness of bounding ellipsoids in summarizing clusters as a function of the confidence step in different data sets with and without aura.


Download : Download high-res image (330KB)
Download : Download full-size image
Fig. 6. Accuracy of MAD-C and MAD-C-ext in rand index. 6(a) shows the clustering accuracy of MAD-C for the factory scene with different values of the confidence step and different number of nodes (i.e. ). Fig. 6, Fig. 6 respectively show the clustering accuracy boxplots of MAD-C for the synthetic scenes and the Ford Multi-AV Seasonal Data Set for different number of nodes, with confidence step 1.5.

6.3. Evaluation results
We start presenting the results by estimating the value of the confidence step.

6.3.1. Determining  (the confidence step)
As outlined in Section 3.4, to determine the value of  to use, the analyst that deploys MAD-C, needs to jointly tune, for representative data of the application, (i) the effectiveness of the bounding ellipsoids in summarizing the local clusters and (ii) the clustering accuracy of MAD-C, both measured for different values of  and in conjunction with the aura (cf. Section 3.4). To that end, we measured the effectiveness of the bounding ellipsoids in summarizing the clusters for a wide range of  values. Afterwards, based on the aforementioned measurements, we narrowed down the search spectrum for the suitable values of , that result in appropriate accuracy, for sample data of each of the study cases.

The effectiveness of bounding ellipsoids in summarizing local clusters.
Plots in Fig. 5 show the average recall and precision values with one standard deviation error bars (contained within the zero to one interval) for the different data sets, as  varies between 0.1 to 3.5. The first, the second, and the third columns in Fig. 5 respectively correspond to the merged point clouds in the synthetic scenes, the KITTI scenes, and the merged point clouds in the Ford Multi-AV Seasonal Data Set. The first row in Fig. 5 (5(a), 5(b), and 5(c)) shows the results without the aura, and the second row (5(d), 5(e), and 5(f)) shows the results with the aura, introduced in Section 3.4.

Based on the results in Fig. 5, with a too small value for the confidence step, clusters are partly covered (i.e. low average recall) or not covered at all (i.e. low average precision). This is expected, because a small confidence step shrinks the volume of the bounding ellipsoids (remember from Section 3.2 that the confidence step is a scaling factor on the size of the bounding ellipsoids). This problem gets mitigated as the confidence step increases; however, with a too large value for the confidence step the precision drops again because the bounding ellipsoids expand so much that they erroneously start to cover parts of other clusters as well. As the results show, adding the aura results in achieving higher recall and precision values even with relatively small values of . The latter is expected because the aura expands the principal axis of each bounding ellipsoid by the constant .

Furthermore, the results in Fig. 5 show that adding the aura generally improves the effectiveness of bounding ellipsoids in summarizing the clusters because the recall and precision retain close to 1 values with more choices of . Based on the aforementioned observations, we narrowed down the interval for the  to .

Clustering accuracy.
Fig. 6(a) shows the clustering accuracy of MAD-C and MAD-C-ext on the factory scene when the number of nodes varies from two to five and the confidence step is  and 2.4. We observe that rand index values are maximized when the confidence step is 1.4 and 1.8. Fig. 6(b) shows the clustering accuracy of MAD-C and MAD-C-ext when confidence step is 1.5 for the synthetic scenes with two, three, four, and five nodes, where each boxplot summarizes statistics measured for syn-10, syn-50, and syn-100 data sets. Fig. 6(c) shows the clustering accuracy of MAD-C and MAD-C-ext for the Ford Multi-AV Seasonal Data Set. Figs. 6(a), 6(b), and 6(c) show that with an appropriate value of the confidence step, the clustering behavior of MAD-C is similar to that of the baseline (see Observation 4). Figs. 6(a), 6(b), and 6(c) also show that MAD-C-ext achieves high clustering accuracy (see Observation 6).

6.3.2. Completion time of MAD-C, MAD-C-ext, and the baseline
The boxplots in Fig. 7 show the completion time (in seconds) of MAD-C and the baseline for the synthetic scenes with the star and binary tree connection topologies. The first, second, and the third columns respectively correspond to results with syn-10, syn-50, and syn-100 data sets. The first row (7(a), 7(b), and 7(c)) shows the results obtained on the IoT test-bed with five nodes. The second row (7(d), 7(e), and 7(f)) shows the results obtained in our simulation with five nodes, and the third row (7(g), 7(h), and 7(i)) shows the simulation results with seven nodes. Remember from Section 6.1 that the virtual-machine-based simulation and the IoT test-bed use different hardware platforms. The latter explains the discrepancies (in absolute times) between the simulation results with five nodes and the results of the IoT test-bed (with five nodes). Nonetheless, we observe that the ratio of completion times is consistent between the two cases. The statistical results show that MAD-C is 2–5 times faster than the baseline in the experiments with the synthetic scenes.


Download : Download high-res image (759KB)
Download : Download full-size image
Fig. 7. Completion time of MAD-C and Baseline in different setups for syn-10, syn-50, and syn-100 data sets. (a–c) The IoT test-bed with five nodes. (d–f) Simulation with five nodes. (g–i) Simulation with seven nodes. Note that the simulation and the IoT test-bed use different hardware platforms.


Download : Download high-res image (306KB)
Download : Download full-size image
Fig. 8. Completion time of MAD-C and Baseline in different setups for the Ford Multi-AV Seasonal Data Set. The lower and upper limits as well as the median values are presented for each boxplot.

The boxplots in Fig. 8 show the completion time (in seconds) of simulated MAD-C and simulated baseline for the Ford Multi-AV Seasonal Data Set with the star and binary tree connection topologies. 8(a), 8(b), and 8(c) respectively show the simulation results for 7, 15, and 20 nodes. In Fig. 8, the lower and upper limits as well as the median values are presented for each boxplot to enhance readability as the scale on the -axis is logarithmic. The results show that MAD-C is about 5–30 times faster than the baseline, depending on the number of nodes.

Regarding scalability aspects, the results statistically show that, as the number of nodes increases, the increase in MAD-C’s completion time is drastically lower than the increase in the baseline’s completion time, as expected. For example, regarding the synthetic data set, as the number of nodes increases from five to seven, MAD-C’s maximum completion time increases about 7% on syn-100 data set, considering the star connection topology. Under the same conditions, the maximum completion time of the baseline increases about 38%. Moreover, the median completion time of MAD-C remains about 0.5 s with 7, 15, and 20 nodes, but the median completion time of the baseline is approximately 3, 9, and 15 s, respectively. Besides, the results show that the completion time of MAD-C and the baseline increase on data sets containing higher number of objects, respectively confirming the analysis in Theorem 1 and Observation 1 (note that the synthetic point clouds that contain higher number of scene objects contain more number of points as well).

The growth in baseline’s completion time (with respect to the number of nodes) is explained by the fact that it gathers and centrally processes all the point clouds, see Observation 1. On the other hand, Observation 5 is the critical key in understanding why MAD-C’s completion time does not increase as rapidly as the baseline’s does with increasing number of nodes: the length of the longest local clustering (the dominant factor in the MAD-C’s completion time, as shown in the previous section) does not grow with increasing number of nodes. As we will see in the following, transmission and combine times are negligible for MAD-C nodes.

Finally, the boxplots in Fig. 9 show the completion time of simulated MAD-C-ext and the simulated baseline for the Ford Multi-AV seasonal Data Set with the star and binary tree connection topologies. Fig. 9(a), Fig. 9(b), and Fig. 9(c) respectively show the results for 7, 15, and 20 nodes. In Fig. 9, the lower and upper limits as well as the median values are presented for each boxplot to enhance readability as the scale on -axis is logarithmic. The results show that MAD-C-ext is more than three times faster than the baseline, despite the fact that MAD-C-ext transmits the raw point clouds. The savings in completion time of MAD-C-ext is due to distributing the workload among the nodes and not performing the clustering task centrally. Moreover, as presented in the following, performing combine tasks takes negligible time compared to the total completion time.

6.3.3. Combine time of MAD-C nodes
The boxplots in Fig. 10 show the combine time of MAD-C nodes (in seconds) on syn-10, syn-50, and syn-100 data sets for the star and binary tree connection topologies. For the star topology, the boxplots show the results corresponding to the root node, which is the only node performing any  operations. For the binary tree topology, the boxplots also show the results of nodes at height one (remember that leaf nodes do not perform any  operations). Figs. 10, 10(a), 10(b), and 10(c) show MAD-C’s combine time for the IoT test-bed (with five nodes), the virtual-machine-based simulation with five nodes, and the simulation with seven nodes, respectively.

The boxplots in Fig. 11 show the combine time of MAD-C nodes (in seconds) on the Ford Multi-AV Seasonal Data Set for the star and binary tree connection topologies. Fig. 11(a), Fig. 11(b), and Fig. 11(c) respectively show the results for 7, 15, and 20 virtual-machine-based simulated nodes, differentiating the nodes based on their height in the respective connection hierarchy.

The results in Fig. 10, Fig. 11 show that, as expected, the combine time increases with increasing number of objects () and increasing number of nodes (). Moreover, for a constant , we observe that the sink node in the star topology has higher combine time than the sink and intermediate nodes in the binary tree topology. This is expected because in the star topology, only the sink node performs  operations; however, the workload gets distributed among the sink and intermediate nodes in the binary tree connection topology.

Comparing MAD-C’s completion times in Fig. 7, Fig. 8 with MAD-C’s combine times in Fig. 10, Fig. 11, we observe that the amount of time that a MAD-C node spends on combining maps is negligible compared to MAD-C’s total completion time.

The following discussion examines the effect of the delimiting box heuristic (see Section 3.4) on the average number of ellipsoid comparisons made by the  operations.

The effect of the delimiting box heuristic.
Table 2 shows the average number of ellipsoid comparisons made by the  operations on syn-10, syn-50, and syn-100 data sets, with various number of nodes. The highlighted entries show the results when the delimiting box heuristic was employed, and the non-highlighted entries show the results without the heuristic. These results show that, with the delimiting box heuristic, the asymptotic number of ellipsoid comparisons follows a smaller growth function than the worst-case asymptotic bound presented in Lemma 3. Therefore, in practice, the execution time of the  operation is lower than the bounds presented in Corollary 1.


Download : Download high-res image (347KB)
Download : Download full-size image
Fig. 11. MAD-C’s combine time for the Ford Multi-AV Seasonal Data Set.


Table 2. Average number of ellipsoid comparisons made by the  operations with/without the delimiting-box heuristic.



6.3.4. Transmission time of MAD-C and baseline nodes
The boxplots in Fig. 12 show the transmission time (in seconds) of MAD-C and baseline nodes on syn-10, syn-50, and syn-100 data sets for the star and binary tree connection topologies. For the star topology, the boxplots show the aggregate results of all the leaf nodes. For the binary tree topology, the boxplots also show the aggregate results of the intermediate nodes. The first row in Fig. 12 (12(a), 12(b), and 12(c)) shows MAD-C results respectively on the IoT test-bed, the virtual-machine-based simulation with five nodes, and the virtual-machine-based simulation with seven nodes. The second row in Fig. 12 (12(d), 12(e), and 12(f)) shows the results of the baseline in the same order. As explained in Section 6.1, the nodes in the IoT test-bed are connected to a switch, but the virtual nodes in the simulation use the host machine’s loopback interface with software limited bandwidth. The latter explains the discrepancies between the simulation results with five nodes and the results of the IoT test-bed (with five nodes).

The boxplots in Fig. 13 show the transmission time (in seconds) of MAD-C and baseline nodes on the Ford Multi-AV Seasonal Data Set for the star and binary tree connection topologies, differentiating the nodes based on their height in the respective connection hierarchy. Fig. 13(a), Fig. 13(b), and Fig. 13(c) respectively correspond to 7, 15, and 20 simulated MAD-C nodes. Similarly, Fig. 13(d), Fig. 13(e), and Fig. 13(f) correspond to 7, 15, and 20 virtual-machine-based simulated baseline nodes, respectively.

The statistical results show MAD-C vastly cuts down on the required transmission time. For instance, comparing the results of MAD-C and the baseline on the IoT test-bed, MAD-C nodes are about 40 time faster than their baseline counterparts. Moreover, MAD-C’s savings in transmission time becomes more significant with increasing number of nodes. For example, shown in Fig. 13(f), MAD-C nodes are about two orders of magnitude faster than the baseline nodes in the virtual-machine-based simulations with 20 nodes.


Download : Download high-res image (771KB)
Download : Download full-size image
Fig. 12. Transmission time of MAD-C and baseline nodes in different setups for syn-10, syn-50, and syn-100 data sets.


Download : Download high-res image (691KB)
Download : Download full-size image
Fig. 13. Transmission time of MAD-C and baseline nodes in different setups for the Ford Multi-AV Seasonal Data Set.

Based on the results in Fig. 12, for a MAD-C node, the transmission time increases almost linearly with the number of objects (). Moreover, the results in Fig. 12, Fig. 13 indicate that the transmission time of a node  increases with 
, which is the number of nodes in the sub-tree of that given node (note that the number of nodes, the topology of the tree, and the height of a given node  can determine 
). These observations are in accordance with the asymptotic behavior of the transmission time provided in Lemma 6.

Based on the results, we expect that advantages of MAD-C over the baseline in saving transmission time become even more pronounced in setups where the communication medium is slow and subject to failures such as collision, i.e. the issues that take place in wireless communication.

6.3.5. Summary of the results
In the evaluation of MAD-C, firstly we conducted experiments to empirically estimate the value of the confidence step. Then, we studied the completion time of MAD-C and MAD-C-ext and compared the results to that of the baseline. We observed the scalability of MAD-C and MAD-C-ext for varying number of nodes using real and synthetic data sets. We notably observed 2 to 30 times faster completion time with MAD-C compared to the baseline, with differences becoming more pronounced as the network sizes, the data sizes and the complexity of the sensed environment increase. Furthermore, we empirically observed that the local clustering is the dominant cost factor in MAD-C as the transmission and combine times are negligible, confirming our analytical studies. We conclude that MAD-C’s completion time can be further improved by employing a faster local clustering algorithm.

7. Related work
Clustering is a commonly used method to detect objects in a point cloud [33], [40]. Among the relevant clustering algorithms that can be applied on point clouds are [3], [12], [40]. As point clouds are typically large in volume, the study of clustering algorithms that utilize parallelization to tackle the challenges of clustering a large volume of data is important. For example, the method in [35] achieves parallelization by using graph algorithmic concepts. Other methods such as [6] and [9] utilize graphics processing units to achieve parallelization. As we explained in the paper, MAD-C works on top of the result of a clustering algorithm. Therefore, in deployment, MAD-C can orthogonally utilize such commodities if they are available.

Combining readings from multiple sensors is commonly recognized as sensor fusion. For instance, the authors in [10] propose a method for 3-D model reconstruction of objects using multiple calibrated camera views.

As mentioned in Section 1, MAD-C and results from a preliminary experimental study via simulations were introduced in [27]. The present paper adds the analytical study of the completion time behavior of MAD-C. Furthermore, it provides a significantly more extensive empirical evaluation of MAD-C with binary tree and star tree connection topologies. Besides, MAD-C is evaluated here not only in simulation, but also in an IoT test-bed, comprising of representative fog/edge type devices. Furthermore, MAD-C is evaluated here using a more extensive data set. In addition, the present work explains how MAD-C’s summarizations can be used in other applications, having as an example to efficiently answer geofence queries.

Data summaries offer opportunities to efficiently deal with big volume of data generated in a streaming fashion, (cf. e.g. [16]). MAD-C is indeed a distributed algorithm that processes streaming data through data summaries.

Variants of Octrees [11], voxel grids [40], and bounding boxes [18], [24] are tools used for efficient processing of point clouds. Our work, based on bounding ellipsoids, offers new advantages because they have a compact representation, they can be calculated incrementally (as points are being added in the clusters) and they facilitate efficient operations.

Geometric alignment of two point clouds can be performed by ICP [39] when the relative location and pose of the sources is not known. In our work, however, we know the exact location and pose of all LIDAR nodes.

Distributed versions of some center-based clustering algorithms (for instance K-Means, K-Harmonic-Means, and Expectation–Maximization) are proposed in [13]. The basic iterative idea is to have the distributed nodes compute sufficient statistics for their local data and then have the local sufficient statistics aggregated to attain the global sufficient statistics which gets broadcasted back to the distributed nodes. Enough iterations of the explained procedure take place until some predetermined performance criteria are met.

DBDC [25], density-based distributed clustering, is a client–server approach to distributed density based clustering. Firstly, each node locally performs DBSCAN. Afterwards, a small number of representatives are chosen to summarize each local cluster. The representatives from each site are then transmitted to a central coordinator for further processing, where the central coordinator creates a global clustering based on the representatives using DBSCAN with adjusted parameters. The global clustering model is subsequently transmitted to the local sites, where cluster relabeling takes place according to the global clustering model. Unlike MAD-C, there are no guarantees on the size of summaries in DBDC, i.e. the number of representatives for each local cluster can become large. Whereas DBDC only works with a client–server topology, MAD-C operates with any given connection topology. MAD-C is a distance-based clustering method, but DBDC is a density-based clustering method; therefore, they are complementary approaches.

8. Conclusions
MAD-C is an approximate method for distributed obstacle detection and localization in the fusion of several point clouds generated by different nodes, where each node has a LIDAR sensor and a processing unit. Each MAD-C node, distributedly and in parallel with other nodes, first applies a distance-based clustering algorithm on its local point cloud and summarizes each local cluster into a constant-sized representation. The summarization can be pipe-lined into the local clustering process with constant computational overhead per data point. After the summarizations, the nodes share their summaries and collaboratively combine them in an order-insensitive concurrent fashion, to generate a global summary corresponding to the fusion of all local point clouds. Compared to a baseline method that centrally gathers and clusters all the point clouds, MAD-C drastically reduces the amount of information that needs to be shared among nodes. It also distributes the computational complexity among several nodes. One important usage of MAD-C is to efficiently detect objects inside a geofence, for instance an enclosed hazardous area specified in the environment.

As we saw analytically and empirically, MAD-C has smaller completion time than the baseline: MAD-C drastically cuts down on transmission volume, and its processing overheads are marginal. Regarding scalability, we saw that MAD-C’s benefits become more pronounced as the number of nodes and the complexity of the scenes increases. We also noticed and discussed that the local clustering is the dominant cost factor in MAD-C’s completion time. Therefore, we expect that MAD-C can be modified into an even faster method if faster local clustering techniques (e.g., parallelization using GPUs) are employed. Moreover, we discussed how to adopt MAD-C’s summaries in order to efficiently identify objects in certain areas of the environment for geofencing purposes. Other potential uses of the approach can include synergies of MAD-C with spatio-temporal monitoring applications that involve distributed clustering and connectivity, e.g. [4], [14], [20], [38] or other methods targeting continuous, efficient, approximate 2-tier data analysis, e.g. [22].