Abstract
The alternating direction method of multipliers (ADMM) is a powerful operator splitting technique for solving structured convex optimization problems. Due to its relatively low per-iteration computational cost and ability to exploit sparsity in the problem data, it is particularly suitable for large-scale optimization. However, the method may still take prohibitively long to compute solutions to very large problem instances. Although ADMM is known to be parallelizable, this feature is rarely exploited in real implementations. In this paper we exploit the parallel computing architecture of a graphics processing unit (GPU) to accelerate ADMM. We build our solver on top of OSQP, a state-of-the-art implementation of ADMM for quadratic programming. Our open-source CUDA C implementation has been tested on many large-scale problems and was shown to be up to two orders of magnitude faster than the CPU implementation.

Previous
Next 
Keywords
Graphics processing unit

GPU computing

Quadratic programming

Alternating direction method of multipliers

1. Introduction
Convex optimization has become a standard tool in many engineering fields including control [17], [37], signal processing [30], statistics [12], [22], [44], finance [8], [9], [15], [29], and machine learning [16]. In some of these applications one seeks solutions to optimization problems whose dimensions can be very large. For such problems, classical optimization algorithms, such as interior-point methods, may fail to provide a solution.

In the last decade operator splitting methods, such as the proximal gradient method and the alternating direction method of multipliers (ADMM), have gained increasing attention in a wide range of application areas [6], [10], [36]. These methods scale well with the problem dimensions, can exploit sparsity in the problem data efficiently, and are often easily parallelizable. Moreover, requirements on the solution accuracy are often moderate because of the noise in the data and arbitrariness of the objective. This supports the use of operator splitting methods, which return solutions of a medium accuracy at a reasonable computational effort.

Graphics processing units (GPUs) are hardware accelerators that offer an unmatched amount of parallel computational power for their relatively low price. They provide far greater memory bandwidths than conventional CPU-based systems, which is especially beneficial in applications that process large amounts of data. It is thus no surprise that the use of GPUs has seen many applications in the area of machine learning, ranging from training deep neural networks [14], [25], [27] to autonomous driving [28]. Many software tools for machine learning, including PyTorch [40], TensorFlow [1], Theano [43], and CNTK [39], have native support for GPU acceleration. However, there has been a general perception that GPUs are not well suited for the needs of numerical solvers for linear programs (LPs) and quadratic programs (QPs) [19].

This paper explores the possibilities offered by the massive parallelism of GPUs to accelerate solutions to large-scale QPs. We build our solver on top of the ADMM-based OSQP solver [42]. The authors in [35] have demonstrated that GPUs can be used to accelerate the solution to the linear system arising in their method. We follow a similar approach to accelerate OSQP by replacing its direct linear system solver with an indirect (iterative) one, which we implement on the GPU. Moreover, we perform all vector and matrix operations on the GPU, which further improves the performance of our implementation. While the authors in [3], [13] use GPUs to solve LPs and QPs in batches, i.e., they solve numerous different problems within one operation, our solver is designed for solving a single but large-scale problem at a time.

Outline
We introduce the problem of interest in Section 2 and summarize the algorithm used by the OSQP solver in Section 3. We then present in Section 4 an alternative method for solving the linear system arising in OSQP. We give a short summary of general GPU programming strategies in Section 5, followed by implementation details of the proposed GPU-based solver in Section 6. Section 7 demonstrates the performance of our solver on large-scale numerical examples. Finally, Section 8 concludes the paper.

Notation
Let  denote the set of real numbers, 
 the -dimensional real space, 
 the set of real -by- matrices, and 
 (
) the set of real -by- symmetric positive (semi)definite matrices. We denote by  and  the identity matrix and the vector of all ones (of appropriate dimensions), respectively. For a vector 
, we denote its th element by 
, the Euclidean norm by 
, and the 
 norm by 
. For a matrix 
, we denote the -norm of 
 by 
. The gradient of a differentiable function 
 evaluated at 
 is denoted by . For a nonempty, closed, and convex set 
, we denote the Euclidean projection of 
 onto  by 
. The Euclidean projections of 
 onto the nonnegative and nonpositive orthants are denoted by 
 and 
, respectively.

2. Problem description
Consider the following QP: (1) 
 	
 
 where 
 is the optimization variable. The objective function is defined by a positive semidefinite matrix 
 and a vector 
, and the constraints by a matrix 
 and vectors  and  so that 
, 
, and 
 for all . Linear equality constraints can be encoded in this way by setting 
.

2.1. Optimality and infeasibility conditions
By introducing a variable 
, we can rewrite problem (1) in an equivalent form (2) 
 	
 
 The optimality conditions for problem (2) are given by [42] (3a)(3b)
(3c)(3d)
 where 
 is a Lagrange multiplier associated with the constraint . If there exist 
, 
, and 
 that satisfy (3), then we say that  is a primal and  is a dual solution to problem (2).

Problem (1) need not have a solution. If there exists 
 such that (4)
then problem (1) is infeasible and we say that 
 is a certificate of primal infeasibility. Similarly, if there exists 
 such that (5)
 for all , then the dual of problem (1) is infeasible and we say that 
 is a certificate of dual infeasibility. We refer the reader to [5, Prop. 3.1] for more details.

3. OSQP solver
OSQP is an open-source numerical solver for convex QPs. It is based on ADMM and was shown to be competitive to and even faster than commercial QP solvers [42]. An iteration of OSQP is shown in Algorithm 1. The scalar  is called the relaxation parameter, and  and 
 are the penalty parameters. OSQP uses diagonal positive definite matrix , which makes 
 easily computable. In step 6 of Algorithm 1 it evaluates the Euclidean projection onto the box 
, which has a simple closed-form solution 
where  and  operators should be taken elementwise.


Download : Download high-res image (122KB)
Download : Download full-size image
If problem (2) is solvable, then the sequence 
 generated by Algorithm 1 converges to its primal–dual solution [5], [42]. On the other hand, if the problem is primal or dual infeasible, then the iterates 
 do not converge, but the sequence 
always converges and can be used to certify infeasibility of the problem. In particular, if the problem is primal infeasible then 
 will satisfy (4), whereas 
 will satisfy (5) if it is dual infeasible [5, Thm. 5.1].

3.1. Termination criteria
For the given iterates 
, we define the primal and dual residuals as 
 The authors in [42] show that the pair 
 satisfies optimality conditions (3c)–(3d) for all  regardless of whether the problem is solvable or not. If the problem is also solvable, then the residuals 
 and 
 will converge to zero [5, Prop. 5.3]. A termination criterion for detecting optimality is thus implemented by checking that 
 and 
 are small enough, i.e., (6)
where 
 and 
 are some tolerance levels, which are often chosen relative to the scaling of the algorithm iterates [10, §3.3].

Since 
, termination criteria for detecting primal and dual infeasibility are implemented by checking that 
 and 
 almost satisfy infeasibility conditions (4), (5), i.e., 
and 
 for all , where 
 and 
 are given tolerance levels.

3.2. Solving the KKT system
Step 4 of Algorithm 1 requires the solution to an equality-constrained QP, which is equivalent to solving the following linear system: (7) 
  
  
 from which 
 can be obtained as 
We refer to the matrix in (7) as the KKT matrix.

OSQP uses a direct method that computes the exact solution to (7) by first computing a factorization of the KKT matrix and then performing forward and backward substitutions. The KKT matrix is symmetric quasi-definite for all  and 
, which ensures that it is nonsingular and has a well-defined 
 factorization with diagonal  [18]. Since the KKT matrix does not depend on the iteration counter , OSQP performs factorization at the beginning of the algorithm, and reuses the factors in subsequent iterations.

3.3. Preconditioning
A known weakness of ADMM is its inability to deal effectively with ill-conditioned problems and its convergence can be very slow when data are badly scaled. Preconditioning is a common heuristic aiming to speed-up convergence of first-order methods. OSQP uses a variant of Ruiz equilibration [24], [38], given in Algorithm 2, which computes a cost scaling scalar  and diagonal positive definite matrices  and  that effectively modify problem (1) into the following:  
 	
 
 where the optimization variables are 
, 
 and 
, and the problem data are 


Download : Download high-res image (96KB)
Download : Download full-size image
3.4. Parameter selection
OSQP sets  and 
 by default and the choice of these parameters does not seem to be critical for the ADMM convergence rate. However, the choice of 
 is a key determinant of the number of iterations required to satisfy a termination criterion. OSQP sets a higher value of 
 that is associated with an equality constraint, i.e., (8)
 where 
. Having a fixed value of 
 does not provide satisfactory performance of the algorithm across different problems. To compensate for this sensitivity, OSQP adopts an adaptive scheme, which updates 
 during the iterations based on the ratio between norms of the primal and dual residuals [46].

The proposed parameter update scheme makes the algorithm much more robust, but also introduces additional computational burden since updating  changes the KKT matrix in (7), which then needs to be refactored. Updating 
 is thus performed only a few times during the runtime of the algorithm.

4. Preconditioned conjugate gradient method
An alternative way to solve the equality-constrained QP in step 4 of Algorithm 1 is by using an indirect method. As observed in [42], eliminating 
 from (7) results in the reduced KKT system (9)
from which 
 can be obtained as 
. Note that the reduced KKT matrix is always positive definite, which allows us to use the conjugate gradient (CG) method for solving (9).

4.1. Conjugate gradient method
The CG method is an iterative method for solving linear systems of the form (10)where 
 is a symmetric positive definite matrix. The method computes the solution to the linear system in at most  iterations [31, Thm. 5.1]. However, when solving large-scale linear systems, one aims to terminate the method after  iterations, which yields an approximate solution to (10).

Solving (10) is equivalent to solving the following unconstrained optimization problem: 
 
since its minimizer can be characterized as 

4.1.1. Conjugate directions
A set of nonzero vectors 
 is said to be conjugate with respect to  if 
Successive minimization of  along the conjugate directions 
, i.e., evaluating (11a)
 
(11b)
 produces 
 that minimizes  over 
, where 
 is the expanding subspace spanned by the previous conjugate directions 
 [31, Thm. 5.2]. The minimization in (11a) has the following closed-form solution: 
 
where 
 is the residual at step .

4.1.2. Conjugate gradient
There are various choices for the conjugate direction set 
. For instance, the eigenvectors of  form a set of conjugate directions with respect to , but are impractical to compute for large matrices. The cornerstone of the CG method is its ability to generate a set of conjugate directions efficiently. It computes a new direction 
 using only the previous direction 
, which imposes low computational and memory requirements. In particular, a new direction 
 is computed as a linear combination of the negative gradient 
 and the previous direction 
, (12)
The scalar 
 is determined from the conjugacy requirement 
, leading to 
 
The first conjugate direction is set to the negative gradient, i.e., 
. Combining the successive minimization (11) and the computation of conjugate directions (12) yield the CG method.

4.2. Preconditioning
Since the CG method is a first-order optimization method, it is sensitive to the problem scaling. To improve convergence of the method, we can precondition the linear system by using a coordinate transformation 
where 
 is a nonsingular matrix. Applying the CG method to the transformed linear system yields the preconditioned conjugate gradient (PCG) method, which is shown in Algorithm 3 [31, Alg. 5.3]. It turns out that  need not be formed explicitly, but rather acts through 
.


Download : Download high-res image (91KB)
Download : Download full-size image
In general, a good preconditioner should satisfy  and at the same time make the linear system  easy to solve [20, §11.5]. One of the simplest choices is the diagonal or Jacobi preconditioner, which contains the diagonal elements of , making 
 easily computable.

More advanced choices include the incomplete Cholesky, the incomplete , and polynomial preconditioners. The incomplete preconditioners produce an approximate decomposition of  with a high sparsity, so that solving  is computationally cheap. The family of polynomial preconditioners include the Chebyshev and least-squares polynomial preconditioners, both of which require a bound on the spectrum of  [4].

5. GPU architecture and programming strategies
GPUs have been used for general-purpose computing for more than two decades [11]. They come in many different variations and architectures, but we will restrict our discussion to the latest NVIDIA Turing-based architecture. Most of the concepts also apply to older NVIDIA GPUs; for further details, we refer the reader to [32].

A GPU consists of an array of several streaming multiprocessors (SMs), each of which contains multiple integer and floating-point arithmetic units, local caches, shared memory, and several schedulers. The on-board RAM of the GPU is called global memory, in contrast to the shared memory that is local to each SM. While an SM of the Turing generation has  of shared memory, the global memory is much larger and is typically in order of . Compared to the shared memory, it has a much higher latency and a lower bandwidth, but is still much faster than system RAM; bandwidths of  are not uncommon for the GPU global memory, whereas system RAM is limited to –.

The main challenge in using GPUs is to leverage the increasing number of processing cores and develop applications that scale their parallelism. A solution designed by NVIDIA to overcome this challenge is called CUDA, a general-purpose parallel computing platform and programming model.

5.1. CUDA architecture
CUDA is an extension of the C programming language created by NVIDIA. Its main idea is to have a large number of threads that solve a problem cooperatively. This section explains how threads are organized into cooperative groups and how CUDA achieves scalability.

5.1.1. Kernels
Kernels are special C functions that are executed on a GPU and are defined by the __global__ keyword. In contrast to regular C functions, kernels get executed  times in parallel by  different threads, where each thread executes the same code, but on different data. This concept is known as single instruction, multiple data (SIMD). The number of threads is specified when calling a kernel, which is referred to as a kernel launch.

5.1.2. Thread hierarchy
While kernels specify the code that is executed by each thread, the thread hierarchy dictates how the individual threads are organized. CUDA has a two-level hierarchy to organize the threads, a grid-level and a block-level. A grid contains multiple blocks and a block contains multiple threads. A kernel launch specifies the grid size (number of blocks) and the block size (number of threads per block).

The threads within one block can cooperate to solve a subproblem. The problem needs to be partitioned into independent subproblems by the programmer so that a grid of thread blocks can solve it in parallel. Each block is scheduled on one of the available SMs, which can happen concurrently or sequentially, depending on the number of blocks and available hardware. If there are enough resources available, then several blocks can be scheduled on a single SM.

The threads within a single block have a unique thread index that is accessible through the built-in variable threadIdx, which is defined as a -dimensional vector. This allows threads to be indexed in one-, two-, or three-dimensional blocks, which allows for a natural indexing in the problem domain. Similarly, blocks within a grid have a unique block index that is accessible through the variable blockIdx. It is also defined as a -dimensional vector and allows for one-, two-, or three-dimensional indexing.


Download : Download high-res image (279KB)
Download : Download full-size image
Fig. 1. Numerical performance of the axpy routine run on CPU and GPU. Left: The average memory throughput (in ). Right: The average floating-point performance (in ).

5.1.3. Accelerating numerical methods
Numerical methods make extensive use of floating-point operations, but their performance is not solely determined by the system’s floating-point performance. Although GPUs offer magnitudes larger floating-point power than CPUs, it is the memory bandwidth that limits the performance of many numerical operations [45]. Fortunately, GPUs also offer an order of magnitude larger memory bandwidth, but utilizing their full potential is not an easy task since the parallel nature of the GPU requires different programming strategies.

Listing 1 implements the simple Basic Linear Algebra Subprograms (BLAS) routine axpy () on the CPU. The code uses a simple for loop to iterate through the elements of  and . A GPU implementation of axpy is shown in Listing 2. The code looks very similar to the CPU version, but has two important differences. First, the for loop is replaced by a simple if condition. Instead of one thread iterating through a loop element-by-element, there is a thread for each element to be processed, which is a common pattern in GPU computing. Second, a thread ID is used to determine the data element which each thread is operating on. A global thread ID is calculated from a local thread index and a block index. The if condition disables threads with a thread ID larger than the number of elements. This is necessary since threads are launched in blocks and the total number of threads usually does not match the number of elements, while the cost of few idle threads is negligible.


Download : Download high-res image (48KB)
Download : Download full-size image

Download : Download high-res image (60KB)
Download : Download full-size image
Fig. 1 compares the achieved memory throughput and the floating-point performance of the CPU and GPU implementations of the axpy operation; see Section 7 for the hardware specifications. The plots are obtained by performing the axpy operation for various sizes of vectors and measuring the time required to run these operations. Knowing how much data is moved and how many floating-point operations are required for the axpy operation, we can compute the average memory throughput and the average floating-point performance, both of which depend linearly on the size of vectors; this is why the shapes of the two plots look the same. For large vector sizes the simple GPU implementation is approximately  times faster. Note, however, that small problems cannot be accelerated well with GPUs, as there is not enough work to keep the GPU busy and a kernel launch and data transfer come with a constant overhead that cannot be amortized. The GPU reaches the maximum memory throughput of , which is  of its theoretical peak of , whereas the peak value of the floating-point performance is around , which is less than  of its theoretical peak of ; the peak numbers are specifications of the NVIDIA GeForce RTX 2080 Ti GPU, which was used in our numerical tests. This shows that the performance of axpy is clearly limited by the memory bandwidth.

5.1.4. Segmented reduction
A reduction is an operation that takes a vector 
 and an associative binary operator , and returns a scalar  [26], 
This abstract formulation allows us to formulate many operations as a reduction, among others the sum of elements, the maximum value of elements, the 
 norm, the 
 norm etc. The only difference between reduction and segmented reduction is that the latter reduces individual segments of  and outputs a vector that computes reduction over the segments. There exist very efficient parallel implementations for both reduction and segmented reduction [7], [33], and thus any problem that can be reformulated as one of them can be easily accelerated by a GPU.

5.2. CUDA libraries
There exist multiple libraries shipped with the CUDA Toolkit that implement various functions on the GPU [34]. We summarize in the sequel the NVIDIA libraries used in this work.

•
Thrust is a CUDA


library based on the

Standard Template Library (STL). It provides a high-level interface for high-performance parallel applications and all essential data parallel primitives, such as scan, sort, and reduce.
•
cuBLAS is a CUDA implementation of BLAS, which enables easy GPU acceleration of code that uses BLAS functions. We use only level-1 cuBLAS API functions that implement the inner product, axpy operation, scalar-vector multiplication, and computation of norms.

•
cuSPARSE is a CUDA library that contains a set of linear algebra subroutines for handling sparse matrices. It requires the matrices to be in one of the sparse matrix formats described in the next section.

5.3. Sparse matrix formats
5.3.1. COO
The coordinate (COO) format is one of the simplest sparse matrix formats. It is mainly used as an intermediate format to perform matrix operations, such as transpose, concatenation, or the extension of an upper triangular to a full symmetric matrix. It holds the number of rows m, the number of columns n, the number of nonzero elements nnz, and three arrays of dimension nnz: Value, RowIndex, and ColumnIndex. The cuSPARSE API assumes that the indices are sorted by rows first and then by columns within each row, which makes the representation unique.

The 4 × 5 matrix given below: (13)
 
has the following COO representation:  
  
  
  Note that we use the zero-based indexing in the example above and throughout the paper.

5.3.2. CSR
The compressed sparse row (CSR) format differs from the COO format only in the RowIndex array, which is compressed in the CSR format. The compression can be understood as a two-step process. First, we determine from RowIndex the number of nonzero elements in each row, which results in an array of length m. Then, we calculate the cumulative sum of this array and insert a zero at the beginning, which results in an array of length m+1. The obtained array is denoted by RowPointer since it points to the beginning of a row in Value and ColumnIndex arrays.

The RowPointer array has the property that the difference between its two consecutive elements, is equal to the number of nonzero elements in row k. Noting that RowPointer[0] = 0 and applying the property above recursively, it follows that Matrix  given in (13) has the following CSR representation:  
  
  
  The CSR format is used for Sparse Matrix–Vector multiplication (SpMV) in cuSPARSE.

5.3.3. CSC
The compressed sparse column (CSC) format differs from the CSR format in two ways: the values are stored in the column-major format and the column indices are compressed. The compressed array has dimension n+1 and is denoted by ColumnPointer. Matrix  given in (13) has the following CSC representation:  
  
  
  The CSC format is not used directly for computations in cuSPARSE. However, we can interpret the CSC representation of a matrix  as the CSR representation of 
 using the following mapping: 

6. GPU acceleration of OSQP
Profile-driven software development is based on identifying major computational bottlenecks in the code, as performance will increase the most when removing these [11]. This section identifies and analyzes computational bottlenecks of OSQP when solving large-scale QPs, and shows how we can remove them by making use of GPU’s parallelism.

6.1. OSQP computational bottlenecks
Given a QP in the form (1), we denote the total number of nonzero elements in matrices  and  by . Profiling the OSQP code reveals that for large-scale problem instances all operations whose execution time scales with  represent a potential bottleneck since  is typically much larger than the number of QP variables  and constraints .

As shown in Section 3.1, evaluating termination criteria requires several sparse matrix–vector multiplications. Performing these computations in each ADMM iteration can slow down the solver considerably. Hence, OSQP evaluates these criteria every  iterations by default so that the overall computational burden is reduced. This means that the algorithm can terminate only when the iteration counter  is a multiple of . We discuss in Section 6.2 how to represent the problem matrices in the GPU memory so that sparse matrix–vector multiplications can be performed efficiently on the GPU.

The main computational bottleneck is using a direct linear system solver to tackle the KKT system (7). When  is very large, the computational cost of factoring the KKT matrix becomes prohibitively large. This issue also limits the number of parameter updates, which can improve convergence rate of the algorithm, but require the KKT matrix to be refactored. Furthermore, in each ADMM iteration we need to evaluate forward and backward substitutions, which cannot be fully parallelized. Section 6.3 describes an efficient GPU implementation of the PCG method that avoids matrix factorizations.

Profiling reveals that the matrix equilibration procedure described in Algorithm 2 is also demanding for large-scale problems, where the main bottlenecks are computing the column-norms in step 5 and matrix scaling in step 7. The matrix scaling requires pre- and post-multiplying  and  by diagonal matrices, which is equivalent to scaling rows and columns of  and . We discuss in Section 6.4 how to parallelize these operations on the GPU.

6.2. Representation of matrices
OSQP represents matrices  and  in the CSC format. Moreover, since  is symmetric, only the upper triangular part of  is actually stored in memory. The preferred way of storing matrices in the GPU memory is using the CSR format since it has a superior SpMV performance on the GPU. However, when using  in the CSR format, computing 
 is around  times slower than computing  [34]. This inefficiency can be avoided by storing both  and 
 in the CSR format, though this doubles the memory requirements.

Similarly, storing only the upper triangular part of  is memory-efficient, but computing  in that case is much slower than when the full  is stored [34]. Therefore, we store the full  in the CSR format since it improves the SpMV performance.

We also store vectors , , , as well as the ADMM iterates in the GPU memory, and perform all matrix and vector operations on the GPU. This reduces considerably the size of memory transferred between the system and the GPU memory.

6.3. Reduced KKT system
As discussed in Section 4, we can avoid factoring the KKT matrix by solving the reduced KKT system (9) with the PCG method, which only evaluates matrix–vector multiplications and can be easily parallelized. Although OSQP uses the parameter matrix of the form 
, where 
 is set as in (8), numerical tests show that this choice of  makes the PCG method converge slowly. This can be understood by looking at the effect of  on the reduced KKT matrix. Since  appears in the term 
, setting it as in (8) has the effect of scaling the rows of  by different values, which effectively increases the condition number of the matrix.

The convergence rate of the PCG method can be improved by using 
 instead. This choice will in general result in more iterations of Algorithm 1, but will reduce the number of iterations of Algorithm 3 considerably. The linear system (9) now reduces to (14)
Note that the coefficient matrix above need not be formed explicitly. Instead, the matrix–vector product 
can be evaluated as 

6.3.1. Preconditioner
We use the Jacobi preconditioner, for which solving  amounts to a simple diagonal matrix–vector product. The diagonal of the Jacobi preconditioner for (14) can be computed as 
Note that we need not compute the full product 
, but only its diagonal elements, 
where 
 denotes the th column of .

6.3.2. Parameter update
Once  and 
 are available, computing  becomes extremely easy. This makes the parameter update computationally cheap since we only need to update the preconditioner . This allows us to update 
 more often than is done in OSQP. Our numerical tests perform well when 
 is updated every  iterations.

6.3.3. Termination criteria and warm starting
The solution to (14) need not be carried out exactly for Algorithm 1 to converge [10, §3.4.4]. This fact can be used to motivate an early termination of the PCG method, meaning that we solve (14) only approximately at first, and then more accurately as the iterations progress. This can be achieved by performing a relatively small number of PCG iterations to obtain an approximate solution, and using warm-starting by initializing 
 in Algorithm 3 to the solution 
 computed in the previous ADMM iteration.

Finding a good termination criterion for Algorithm 3 is essential for reducing the total runtime of ADMM. If the PCG method returns solutions with low accuracy, then ADMM may converge slower, or even diverge. On the other hand, if the PCG method solves the subproblems with unnecessarily high accuracy, this may increase the total runtime of ADMM. The SCS solver [35] sets  in Algorithm 3 as a decreasing function of the ADMM iteration counter . We adopt a different strategy in which  is determined based on the ADMM residuals. In particular, we use 
 
where 
 and 
 are the scaled primal and dual residuals. Parameter  ensures that  is always lower than the geometric mean of the scaled primal and dual residuals. We set  and 
. Since we use the 
 norms for ADMM residuals, we use the same norm in step 3 of Algorithm 3. As  depends on the ADMM residuals, which are computed when evaluating ADMM termination criteria, we evaluate these criteria after every  ADMM iterations.

6.4. Matrix equilibration
6.4.1. Computing column norms
The CSR representation of a sparse matrix allows for efficient computation of its row norms since the RowPointer array defines segments of the Value array corresponding to different rows. Since we also store the matrix transpose, we can efficiently compute column norms of a matrix since they are equivalent to the row norms of its transpose.

A naive approach would be to have one thread per row computing its norm, but this approach is not the most efficient. First, the workload may be distributed poorly among the threads since one row can have zero elements, and another can have many. Second, the memory is accessed almost randomly as each thread iterates through its row, which can considerably deteriorate performance.

A more efficient way of computing the row norms of a matrix in the CSR format is to represent the operation as a segmented reduction, where the segments are defined by the RowPointer array and, in the case of the 
 norm, the associated binary operator is given by 


Table 1. The main differences between OSQP and cuOSQP implementations of Algorithm 1.

OSQP (CPU)	cuOSQP (GPU)
Evaluating step 1 of Algorithm 1	– linear system (7)	– linear system (9)
– 
 factorization	– PCG method (Algorithm 1)
Parameter matrix 	– 
– 
– 
 set according to (8)	
Updating 
– rarely	– every 10 iterations
Storing data matrices	– CSC format	– CSR format
– upper triangular 	– full 
– only 	– both  and 
Checking termination	– every 25 iterations	– every 5 iterations
6.4.2. Matrix post-multiplication
Matrix post-multiplication refers to evaluating the product , where 
 is a diagonal matrix stored as an -dimensional vector, and 
 is a general sparse matrix in the CSR format. The ColumnIndex array can be used to determine the diagonal element of  that multiplies each element of ,


Download : Download high-res image (18KB)
Download : Download full-size image
This operation can be performed by many threads concurrently and independently. As the memory read and write access to the array Value is fully coalesced, all memory addresses can be combined into a larger transaction. However, the read access from D can be partly coalesced, but this does not impact the performance too much.
6.4.3. Matrix pre-multiplication
Matrix pre-multiplication in the product  is conceptually easier to implement since all elements in a row are multiplied with the same diagonal element of . However, it is not obvious how to determine the row index corresponding to an element of the Value array since the matrix  is represented in the CSR format. We address this issue by computing RowIndex from the RowPointer array in advance, although it increases the memory usage. The code that evaluates the matrix pre-multiplication is thus


Download : Download high-res image (16KB)
Download : Download full-size image
6.5. cuOSQP
Table 1 summarizes the main differences between OSQP and our GPU implementation of Algorithm 1. Although our implementation requires two times more memory to store the problem matrices, it does not need to store any matrix factorizations. Moreover, we can reduce the memory requirements by using the single-precision floating-point representation, which also leads to faster computations (see Section 7.4).

We refer to our CUDA C implementation of the OSQP solver as cuOSQP. The code is available online at

https://github.com/oxfordcontrol/osqp/tree/cuda-1.0

and its Python interface at
https://github.com/oxfordcontrol/cuosqp

cuOSQP uses cuBLAS, cuSPARSE, and Thrust libraries, which are included within the CUDA Toolkit. Note that a custom implementation of linear algebra could improve efficiency of the solver even further. However, relying on CUDA libraries not only saves the development time, but also ensures that our code is portable to various GPUs and operating systems. We have tested our code on both Linux and Windows machines, and have run it on a GeForce RTX 2080 Ti (launched in 2018) and a GeForce GTX 970 (launched in 2014).
7. Numerical results
We evaluate performance of cuOSQP and compare it against both single- and multi-threaded versions of OSQP (version 0.6.0), which was shown to be competitive to and even faster than commercial QP solvers [42]. Our main goal is to demonstrate how a parallel GPU implementation can improve performance of an optimization solver for large-scale problems. The sizes of benchmark problems range from 
 to 
 nonzero elements in  and . We use the default parameters for both solvers. By default, we use the single-precision floating-point representation with cuOSQP, but we also compare the single- and double-precision variants in Section 7.4.

All numerical tests were performed on a Linux-based system with an i9-9900K @ 3.6 GHz (8 cores) processor and 64 GB of DDR4 3200Mhz RAM, which is equipped with the NVIDIA GeForce RTX 2080 Ti GPU with 11 GB of VRAM.

7.1. OSQP benchmark problems
We use the set of benchmark problems described in [42, Appendix A], which consist of QPs from  problem classes, ranging from standard random problems to applications in control, finance, statistics, and machine learning. The problems are available online at [41] and are summarized in the sequel.

•
Control. The problem of controlling a linear time-invariant dynamical system can be formulated as the following constrained finite-time optimal control problem:  
 
 
 
 
 
 
 

•
Equality. This class consists of the following equality- constrained QPs:  
 
 

•
Huber. Huber fitting or the robust least-squares problem performs linear regression under the assumption that there are outliers in the data. The problem can be written as 
where the Huber penalty function 
 is defined as 
 

•
Lasso. The least absolute shrinkage and selection operator (lasso) is a well-known technique aiming to obtain a sparse solution to a linear regression problem by adding an 
 regularization term in the objective. The problem can be formulated as 

•
Portfolio. Portfolio optimization is a problem arising in finance that seeks to allocate assets in a way that maximizes the risk-adjusted return. The problem has the following form:  
 where 
 represents the portfolio, 
 the vector of expected returns,  the risk-aversion parameter, and 
 the risk covariance matrix.

•
Random. This class consists of the following QP with randomly generated data:  
 
 

•
SVM. Support vector machine (SVM) problem seeks an affine function that approximately classifies two sets of points. The problem can be stated as 
where 
 is the set label and 
 the vector of features for the th point.

All instances were obtained from realistic non-trivial random data. For each problem class we generate  different instances for  dimensions giving a total of  problems. As a performance metric, we use the average runtime across  different problem instances of the same size.

Fig. 2, Fig. 3 show the computation runtimes achieved by OSQP and cuOSQP. The figures show that OSQP is faster than cuOSQP for problem sizes of the order up to 
. However, for larger problem instances cuOSQP is significantly faster. Furthermore, the slope of the runtimes achieved by OSQP is approximately constant, whereas for cuOSQP it is flatter for smaller problems and increases for larger. This behavior is expected since smaller problems cannot fully utilize the GPU, and the kernel launch and data transfer latencies cannot be amortized. Moreover, the main focus of cuOSQP is on large-scale problems and thus we have not optimized it for small problem sizes.

Fig. 4 shows the number of ADMM iterations needed to satisfy the termination condition (6). Updating 
 every  iterations helps decrease the total number of ADMM iterations for the problem classes Equality, Lasso, and SVM, which explains the obtained speedup shown in Fig. 2, Fig. 3. For the Control, Portfolio, and Random classes the benefit is not apparent, while for the Huber class updating 
 less frequently seems to work better; in fact, our numerical tests indicate that the smallest number of iterations is achieved when 
 is kept constant.

7.2. QDLDL
When compared to OSQP’s default single-threaded linear system solver QDLDL [21], the maximum speedups achieved by cuOSQP range from  to  times. The largest reduction in runtime is achieved for the Equality class, where OSQP takes  to solve the largest problem instance, while cuOSQP solves it in . The second largest reduction is achieved for the SVM class with a reduction from  to .

For some problems, one can observe that OSQP runtimes do not necessarily increase with the problem size. This behavior comes from computing the permutation of the KKT matrix prior to its factorization, which is performed by the AMD routine [2], whose runtimes do not depend only on the number of nonzero elements in the KKT matrix.

7.3. MKL Pardiso
Apart from its single-threaded QDLDL linear system solver, OSQP can be interfaced with Intel MKL Pardiso [23], a multi-threaded parallel direct sparse solver. By default, Intel MKL Pardiso uses the maximum number of CPU cores available, which results in its best performance [23]; hence, in our numerical tests the solver uses  cores. Fig. 2, Fig. 3 show that the computation runtimes increase monotonically with the problem size when using MKL Pardiso. Also, for smaller problem sizes OSQP is faster when using QDLDL, but for larger problems using MKL Pardiso reduces its runtimes significantly. However, the maximum ratio of runtimes achieved by OSQP and cuOSQP is still between 3.7 and  times, depending on the problem class.

7.4. Floating-point precision
Fig. 5 shows the average computation times when running cuOSQP on the Portfolio benchmark class for both single- and double-precision floating-point representations. The penalty in computation times when using double- over single-precision is less than  times over all problem sizes. Moreover, our numerical results suggest that for other problem classes this penalty is even smaller (data not shown). This is counter-intuitive at first since the GPU used in our tests has  times higher single-precision floating-point performance than in double-precision. However, most numerical methods that we use, especially SpMV, are memory-bound operations, which means that the computation times are limited by the memory bandwidth. Hence, we expect that the achieved speedups would be even larger for GPUs with higher memory bandwidths, such as NVIDIA V100 or V100s models.

8. Conclusions
We have explored the possibilities offered by the massive parallelism of GPUs to accelerate solutions to large-scale QPs and have managed to solve problems with hundreds of millions nonzero entries in the problem matrices in only a few seconds. Our implementation cuOSQP is built on top of OSQP, a state-of-the-art QP solver based on ADMM. The large speedup is achieved by using the PCG method for solving the linear system arising in ADMM and by parallelizing all vector and matrix operations. Our numerical tests confirm that GPUs are not suited for solving small problems for which the CPU implementation is generally much faster. Our open-source implementation is written in CUDA C, and has been tested on both Linux and Windows machines.

Our implementation stores all problem data and ADMM iterates in the GPU memory. While this design choice reduces the size of memory transferred between the system and the GPU, its drawback is that the size of problems are limited by the available GPU memory. One possible extension would be to use the unified memory approach, which merges the system memory with the GPU memory, and automatically transfers data on demand between the two memory spaces. Alternatively, we could use multiple GPUs to solve problems whose data could not fit on a single GPU. The main challenges with a multi-GPU approach include the distribution of the workload across multiple devices and ensuring synchronization between them.