Abstract
The paper presents a new method of optimization by the Layered Algorithm (LA). The proposed algorithm reduces the initial area to the sub-area containing the optimum. The proposed technique is based on the classification of the optimized function values (data sampled by sensors). The classification method uses a two-dimensional three-state Cellular Automata (CA). The CA classifies all area points ascribed to the CA cells based on their values. Specification of the categorization layers to the data gives a possibility to identify the different levels areas. Consequently, after analysis, a sub-area containing the optimum can be designated. In this paper, the proposed algorithm is applied to find the location of the airborne contaminant source by analyzing the concentration of released substances reported by mobile sensors distributed over the domain of interest. The Gaussian dispersion model simulation of the contaminant dispersion in the urbanized area is applied to generate the data used to verify the efficiency of the proposed Layered Algorithm. The LA successfully estimates the sub-area of the considered domain where the contamination source is located, taking to account data from sensors solely.

Keywords
Area optimization
Cellular automata
Gaussian dispersion model
Classification
Sensors


1. Introduction
The paper is devoted to the area of optimization problems. The goal is to indicate, from an initial domain, the sub-area in which an optimum is placed. This issue is realized using a new approach to apply classification methods using cellular automata (CA). In a classification problem, we wish to determine to which class belong new observations. The classification is based on the training data set containing observations whose class is known. The binary classification deals with only two classes, whereas multiclass classification observations can belong to one of several classes. The well-known classifiers are neural networks, support vector machines, k-NN algorithm, decision trees, etc. The idea of using cellular automata in the classification problem was described by Maji et al. [9], Povalej et al. [11] and recently by Fawcett [4]. Fawcett designed a heuristic rule based on the von Neumann neighborhood (so-called voting rule); moreover, he tested its performance on different data sets. Results of Fawcett's study indicated his method, based on CA, as better than the other compared methods, like as (a) J48, a decision tree induction algorithm, (b) k-NN, a nearest-neighbor learning algorithm, (c) SMO, an implementation of support vector machines. Such binary classification problem was also analyzed by Piwonska et al. [10], where GA was considered as a tool to select CA rules with von Neumann neighborhood. Recently, in the papers [12], [13] the Fawcett's method modified into a probabilistic form was proposed and analyzed. These modifications were examined on the different sets of data, and the obtained results show its higher effectiveness for classification (lower number of incorrect classifications) and better accuracy (the shortest scattering range).

The proposed algorithm is verified on a vital task: the localization of the airborne contamination source based solely on the registered contamination level. An efficient algorithm that can successfully complete such a task would be of great importance for the emergency response groups existing in all cities whose goal is a quick reaction to any threats to people and the environment. Nowadays, chemicals are used in most industry areas, making the transport and storage of toxic materials pose a constant risk of releasing them into the atmosphere. In the cases when the source of the failure resulting in releasing the toxin into the atmosphere is known, the emergency responders can quickly undertake all necessary actions to minimize the consequences of such release. The more challenging situations when the sensors, distributed over a region of interest, report the dangerous substance's non-zero concentration, which source is not known. In such cases, it is crucial to have a system able to quickly estimate the most probable location of the contamination source based solely on the concentration data reported from the sensor network. The algorithms that can cope with this task can be divided into two categories. The first algorithms are based on the backward approach, but those are dedicated to open areas or continental-scale problems. The second type of algorithms is based on the forward approach. In this case, the appropriate dispersion model parameters are sampled (among them source location) to choose the one giving the smallest distance measure between the model outputs and sensors measurements in the considered spatial domain. Such an inverse problem has no unique analytical solution but might be analyzed with probabilistic frameworks, as the Bayesian approach, where all searched quantities are modeled as random variables. In [2], [3], [17], [7] authors presented the reconstruction of the airborne contaminant source utilizing the Bayesian approach in conjunction with Markov Chain Monte Carlo (MCMC), Sequential Monte Carlo (SMC), and Approximate Bayesian Computation (ABC) algorithm. A comprehensive literature review of past works on solutions of the inverse problem for atmospheric contaminant releases can be found in (e.g. [5]).

In practical application, the contaminant source must be found quickly, and very often, the only information is the sparse-point contamination registered by stationary or mobile sensors.

This class of problems seems to be a potential area of application for the newly presented algorithm named Layered Algorithm (LA), which is based on binary classification using three-state 2D cellular automata. LA is introduced in this paper to solve the problem of reconstructing the airborne contaminant source. To verify the algorithm, the synthetic data generated by the Gaussian dispersion model are used as an input.

This paper is organized as follows. Section 2 describes two-dimensional CAs and binary classification problems. In section 3 are presented binary classification methods based on 3-state CA. Section 4 presents the area optimization algorithm's construction and widely describes an example of its work. Section 5 introduces the results of the LA verification with the benchmark functions. The details of the proposed algorithm's testing in the localization of the contaminant source based on the concentrations data reported by the sensors distributed randomly over the domain are presented in Section 6. The need and the potentials ways of the LA parallelization are discussed in Section 7. The last section concludes the paper and is a study of the proposed algorithm's application possibilities and plans for future work.

2. Two-dimensional cellular automata and binary classification problem
CAs and their potential to efficiently perform complex computations are described by S. Wolfram in [18]. In this paper, a two-dimensional CA is considered. CA is a rectangular grid of  cells, each of which can take on k possible states. After determining the initial states of all cells (i.e., the initial configuration of a CA), each cell changes its state according to a rule - transition function TF, which depends on the states of cells in a neighborhood around it. In this work a finite CA with the periodic boundary conditions is used. The CA cells transition is usually done synchronously, although asynchronous mode is used too. Two types of neighborhood are commonly used. First, the von Neumann neighborhood (the four cells orthogonally surrounding the central cell), which can be described as:(1)
 where 
 denotes the state of a cell at position  in the two-dimensional cellular grid, at time step t. The second standard is the Moore neighborhood (the eight cells around the central cell), which can be described as:(2)

A CA's evolution is usually presented using so-called ‘space-time diagrams’ displaying a grid of cells at subsequent time steps, with each state marked with a different color.

The state of the data in classification problem should be in  space. Suppose that  data-points 
, where  and  are given as a training set from two classes: class 1 and class 2. When each of 
 data-points is known as one of two classes, the set is classified. On the other hand, when even one of the data-points is not one of two known classes, we are dealing with the classification problem. Moreover, to answer the question of which class 1 or class 2 unclassified data points belong, the classification method should be applied. In CA, the data space of such task should be mapped from  into the grid of  cells (in this paper  for simplicity). Each of the cells can take one of 3 states: state 1 (classified to class 1), state 2 (classified to class 2), or state 0 (unclassified, class 0). Classifier - the CA rule will analyze the unclassified cells and changes their states into one of two knowns.

3. Data mining using cellular automata
The classification methods based on two-dimensional three-state cellular automata was applied for classifying all of the points of the area based on data received as the input data (sensors sampling the optimized function). The goal was designation of the sub-area containing optimum. For this purpose, three kinds of classifier were studied. The first classification method was proposed in [4]. The applied rule of CA known as , and their successor  are defined as:

•
classify as  0, if number of  1 neighbors + number of  2 neighbors = 0,

•
classify as  1, if number of  1 neighbors > number of  2 neighbors,

•
classify as  2, if number of  1 neighbors < number of  2 neighbors,

•
classify with uniform randomness from { 1,  2}, if number of  1 neighbors = number of  2 neighbors.

The difference between the  and  form is that the  classifier is looking for unclassified states and changes its states into one of the classified states. In contrast, the  classifier during his work analyzes each state and updates the state according to the upper formula.
The next research step was a modification of Fawcett's rule into two patterns: partially and fully probabilistic in  forms (see [12], [13]). The proposed modification should cause a more accurate classification of binary data, especially for large CA grids. In this paper, we also analyze the nonstable forms of Fawcett's rules modification in two kinds of neighborhoods (von Neumann and Moore), while Fawcett's analyzed his rules with only von Neumann's neighborhood. The proposed Partially Probabilistic modification is following:

•
classify as  0, if number of  1 neighbors + number of  2 neighbors = 0,

•
classify as  1, with probability ,

•
classify as  2, with probability ,

where probabilities are calculated from classified neighbors in the neighborhood, i.e.:
 
 where . It means that an unclassified cell will change into a classified state with a probability calculated from known states in the neighborhood and appropriate state.
The Full Probabilistic modification of Fawcett's rule is following:

•
classify as  0, with probability ,

•
classify as  1, with probability ,

•
classify as  2, with probability ,

where probabilities are calculated from each class of neighbors in the neighborhood, i.e.: 
 
, where . It means that unclassified cell could change into a classified state or stay unclassified with probability calculated from a state of cells in the neighborhood.
4. Structure of the Layered Algorithm
4.1. Description of the algorithm
The Layered Algorithm can be used as an optimization tool, which from the entire considered space will select the sub-space for which assumed criteria are fulfilled the best. This sub-space with high probability contains the optimum. Presented in this paper algorithm uses the contaminant concentrations reported by the set of sensors monitoring the analyzed area. These data are the input for the classification method based on two-dimensional three-state cellular automata, which classifies all points of the area to designate the sub-area containing optimum. The subsequent layers are categorized based on the sampled value (concentration level). The steps of the algorithm are presented below.

The Layered Algorithm construction:

1.
Downloading input parameters:

•
CA size,

•
Number of sensors,

•
Method of data classification (including type of neighborhood);

2.
Preparation of the cellular automaton:

•
Mapping optimizing area into CA grid,

•
The random distribution of sensors in cells of CA grid - CA cells with included sensors are the cells to be classified (class 1 or class 2), other CA cells are unclassified (class 0);

3.
Searching for solutions:

•
Creation of layers with the use of ascending ordered values sampled by the sensors, except repeated values. On each layers works a separate CA, specifying classes of CA cells with sensors.

Let: 
, be ascending ordered concentrations sampled by the sensors, except repeated values. Each 
 creates a separate layer for the algorithm.

FOR 
 DO

{

FOR   DO

{

If (value from sensor is >= 
 )

THEN the sensor is in a positive state (class 1)

ELSE the sensor is in a negative state (class 2)

}

}

•
Classification of data for each layer. During processing, CA is specifying classes 1 or 2 for unclassified (class 0) CA cells. For classification, classifiers described in one of upper subsections are applied.

4.
Elaboration of received results:

•
Designation of optimal sub-area. In each of analyzed layers the common areas classified as class 1 are labeled.

4.2. Example of the algorithm operation
As an example, let's analyze problem of searching a optimum of the Sphere Function [16]. Characteristic of the Sphere function:

•
Formula: 
,

•
Inverted function: 
⁎
,

•
Search domain:  (used domain: ),

•
Global minimum/maximum 
,

•
Minimal (Maximal) value 
 (
⁎
).

Since the above function has one global minimum, and the proposed algorithm is searching for a maximum, the function  was inverted into 
⁎
, 
 is the maximal value of function in the analyzed domain. The global optimum (maximum) is placed in the middle of the analyzed area, i.e., in the center of the CA grid. For such defined function, the parameters of the optimization algorithm were specified as follows: Threshold is equal to 10, Step of threshold is equal to 5, and the number of sensors is equal to 10. A lower threshold step and a higher number of sensors result in a more detailed specification of the sub-area containing optimum and a more accurate algorithm result. The continuous search domain was converted into the discrete space, a CA grid of size . The sensors sample the value of the sphere function at a considered point assigned to CA cell. The data received from the sensors were categorized into each layer created using threshold and its step. The data reported by sensors are categorized as a state 1 (class 1) when the value is higher than a threshold, determining the Layer, and as a state 2 (class 2) when the value is lower. Initially, the data at the sensors' location (CA cells) were classified in each layer, while other cells not containing the data were unclassified (state 0). With the use of  classification rule, CA classified the unclassified cells (in class 0) into one of the states: class 1 or class 2, in each layer, see Fig. 1.

Fig. 1
Download : Download high-res image (117KB)
Download : Download full-size image
Fig. 1. An example of classification process with the use of n4_V1_stable_FP rule in 2D CA with size 300 × 300 for Sphere Function [16]: (a) randomly located sensors in the CA grid, (b) temporary CA state - an early classification stage, (c) temporary CA state - advanced classification stage, (d) final CA state - after classification.

Fig. 1 presents the classification in one layer. White areas are cells classified as state 2 (class 2), graphite areas are cells classified as state 1 (class 1), and gray areas are state 0 (unclassified) cells. Panels in Fig. 1 presents: (a) randomly located sensors in the CA grid; (b) an early classification stage, (c) an advanced stage of the classification, and (d) the almost completed classification.

When the same process is applied to each layer, areas become classified (see Fig. 2). In Fig. 2 we can see areas in state 1 on selected layers, 1, 6, 7, 11, 12, 13, respectively. In the first layer, the whole area was classified as the state 1 (state 1 - graphite color, see Fig. 2(a)), and for the higher level, the state 1 area decreases (see Fig. 2(b)-(e)). The last panel presents a result when less than 15% of the area was in state 1 (see Fig. 2(f)).

Fig. 2
Download : Download high-res image (107KB)
Download : Download full-size image
Fig. 2. An example of area optimization by layers, effects of classification in selected layers with use of n4_V1_stable_FP rule with Moore neighborhood in 2D CA with size 300 × 300 for inverted Sphere Function [16], Threshold = 10, Step of threshold = 5, Number of sensors = 10; Effect of classification: (a) layer 1, (b) layer 6, (c) layer 7, (d) layer 11, (e) layer 12, (f) layer 13.

The output of the proposed optimization algorithm is a common part of state 1 areas in all layers. The expected are: at first, the analyzed problem's solution is placed in the sub-area indicated by the algorithm; a second, the pointed sub-area is a small part of the whole analyzed space. In the ideal situation, the algorithm's output can be a single CA cell representing a small area/point. The result of area optimization by layers algorithm using a binary classification based on three state 2D CA is presented in Fig. 3.

Fig. 3
Download : Download high-res image (42KB)
Download : Download full-size image
Fig. 3. The result of the work done by the layered algorithm of area optimization by layers and binary classification with the use of three state 2D cellular automata. Methods of solutions measures represented by common part of state 1 areas of: (a) all layers, (b) all layers except the last layer, and (c) three last layers except the last layer.

Fig. 3 presents the result of the work done by the algorithm of area optimization by layers and binary classification using three state 2D cellular automata. Three methods for measuring the optimization results were applied. The first one estimates the common part of state 1 areas in all layers (Fig. 3(a)), the second one − the common part of state 1 areas in all layers except the last layer (Fig. 3(b)), and the third one − the common part of state 1 areas in the three last layers except the last layer (Fig. 3(c)). For each method, the following characteristics (assessment criteria) were calculated: classification error, accuracy error, and relevance.

The classification error is a ratio between the size of the area containing the probable extreme and the entire analyzed space. This value is given in [%]. The smaller the classification error, the better the result of the algorithm is.

The accuracy error is calculated as the distance from the known most distant point to the center of the circle described on the sub-area pointed out by the algorithm. After inscribing the estimated sub-area in a circle, the distance between two points in space is calculated: center of the circle and global optimum of analyzed space.

For one test, relevance is a simple YES / NO value indicating whether the global optimum of analyzed space is inside the designated area. The percentage relevance value was calculated for a series of tests, which determines how many tests confirmed the YES value. The listed assessment criteria estimated for the presented example with the use of the three solution measures are following: (1) Classification error is equal to 14,04%, 16,04% and 14,08% for Method 1, 2 and 3 respectively; (2) Accuracy error is equal to 1,29; 0,22 and 1,3 for Method 1, 2 and 3 respectively; (3) Relevance for each method is equal to YES.

To summarize, each method is characterized by a similar size of the selected optimal area. Also, for each method, the solution of the analyzed problem is included in the designated sub-area. The distance between the center of the circle, and the global optimum of the analyzed space for the presented example is shortest for method 2.

5. Verification of the Layered Algorithm with Selected Benchmark Functions
As a preliminary examining stage, tests were conducted for the proposed algorithm with the use of benchmark functions (see, e.g., [16], [6]) were conducted. The first version of the algorithm was tested on three classical functions: Sphere, Booth, and Matyas Function. Results presented in papers [14], [15] show a promising quality of the proposed algorithm in finding an optimal value for the tested benchmark functions. However, presented in these papers, the area optimization algorithm proved to be not fast enough. Therefore the algorithm was upgraded to the version presented in this paper. The paper [8] presents the next series of tests on the new version of the Layered Algorithm (LA) with the use of several functions, such as: Sphere, Adjiman, Bohachevsky, and Drop-Wave Function taken from benchmark functions (see [1]). In both test cases, the algorithm was characterized by a low Classification error and Accuracy error and a relatively high Relevance. It means that both versions of the algorithm provided as a result, a small sub-area containing the analyzed function's optimal value.

6. Verification of the Layered Algorithm's effectiveness in the localization of a contaminant source
6.1. Generation of testing data
The Layered Algorithm occurred to work quite well with the benchmark functions. In this section, algorithm effectiveness in the practical application, i.e., the localization of an airborne contaminant source, is presented. To verify the proposed algorithm's operation, synthetic data of the contaminant concentration over the simulation domain was generated using the Gaussian dispersion model (e.g. [19]). The Gaussian plume model is the most common air pollution model. It is based on a simple formula describing the three-dimensional concentration field generated by a point source under stationary meteorological and emission conditions. Despite many simplifications, the Gaussian plume model is used up to now by many researchers. In this model for uniform steady wind conditions, the concentration 
 of the emission (in micrograms per cubic meter) at any point 
 meters downwind of the source, 
 meters laterally from the centerline of the plume, and z meters above ground level can be written as follows:(3)
 
 
 
 
 
 
 
 where U is the wind speed directed along the x axis, Q is the emission rate or the source strength, and H is the effective height of the release equal to the sum of the release height and plume rise 
. In the equation (3) 
 and 
 are the standard deviations of concentration distribution in the crosswind and vertical direction and depends on 
. These two parameters were defined empirically for different stability conditions by Pasquill and Gifford (e.g. [19]). Gaussian dispersion plume model was used to generate a map of contaminant spread in a given area. We restrict the diffusion to the stability class C in an urban area (Pasquill type stability for the rural area). The release rate was assumed to change with time within the interval 
 
 
, which resulted in the change of the sensors' concentration in subsequent time intervals. The wind was directed along the x-axis with an average speed 
 
. The sample distribution of the contaminant within the considered domain presents Fig. 4.

Fig. 4
Download : Download high-res image (102KB)
Download : Download full-size image
Fig. 4. The release source location and the spatial distribution of concentrations at 10 sensors positions within the sample domain.

6.2. Test assumptions
Presented in section 4 the Layered Algorithm was examined in the sense of its efficiency in optimization of the area where the contaminant source was placed using the spatial distribution of contaminant concentration. These data were generated by the Gaussian plume model for an urbanized area. In the model, the assumed domain was the square . The contaminant source was placed in the position  within the domain. The contaminant concentration was measured in a regular grid with one sensor on 1 km2. The collected data has been passed to the LA algorithm to verify if it can find airborne contaminant source within the domain.

In the conducted experiments, the applied CA was two-dimensional with size . The larger the size of the CA, the more accurate results we could obtain, assuming a denser data grid. For the data classification in the LA algorithm, two classifiers: nonstable Partially Probabilistic with Moore neighborhood (), and Fawcett stable with von Neumann's neighborhood () were applied. These classifiers were selected and described in [14], [15], [8] as the best from the proposed classifiers. Tests presented in this paper were conducted for a varying number of input data (sensors data) ({10, 15, ..., 95, 100}), more extensive than used in earlier cited papers.

Each test contained 100 restarts of the algorithm with a different random sensors' spatial setup. Next, assessment criteria for each of the three solution measures were calculated and averaged for 100 restarts. So, each test for a varying number of sensors contained 100 single runs of the algorithm. The average values of classification error, accuracy error, and relevance for varying numbers of sensors from the conducted experiments were estimated.

6.3. Estimating the location of an airborne contaminant source
First tests revealed that the LA could not correctly point out the contaminant source position in the assumed data configuration. The sub-area selected by the LA was small, but the actual source location was distant from the pointed area. The main reason was connected with the fact that the maximal concentration was not in the contaminant source position. It is due to the wind that carries contaminant from its source accordingly to wind direction. Consequently, in the source location, the concentration measured by the sensors was lower than at the point shifted accordingly to the wind direction. As a result, the LA algorithm could not correctly designate the sub-area with the actual source.

So, it was necessary to estimate the displacement vector. The wind in the selected model was blowing parallel to the x axis with speed equal to 
 
, while simulation time was 1000 s. Thus, the displacement vector is equal to .

The shifting of the selected sub-area pointed first by the LA using the estimated displacement vector () gave a satisfying result. After analyzing the calculation method of the final selected sub-area, the best results were obtained for Method 1, where the selected areas from all layers were taken into account. This method gave the best classification error and accuracy error, as the smallest areas and distances are the most important. Relevance values are the best for Method 3, in which only the last three layers were taken into account. In this method and similarly to Method 2 (all layers except the last one), the selected areas were more extensive than for Method 1. Nevertheless, the difference between relevance in Method 1 and other methods is not significant because it did not exceed 3% for each applied sensor number. The measures for Method 1 of calculation of the final selected sub-area are visualized in Fig. 5.

Fig. 5
Download : Download high-res image (187KB)
Download : Download full-size image
Fig. 5. Comparison of average values of (a) Classification error (in [%]), (b) Accuracy error (real distance) and (c) Relevance (in [%]) for two classification methods: partially probabilistic nonstable with Moore neighborhood, and Fawcett stable with von Neumann neighborhood versus the varying number of sensors reporting the contaminant concentration.

Fig. 5 presents the measures of the LA results in the task of contaminant source localization for two classifiers: partially probabilistic nonstable with Moore neighborhood and Fawcett stable with von Neumann neighborhood. We can see that with the growing number of sensors, the quality of results is increasing. The average classification errors were calculated from 100 classification tests for each number of sensors distributed randomly within the analyzed area. The classification error is not higher than 12%, assuming that only 10 sensors are reporting the contamination and is very fast going down with the growing number of sensors (the input data for LA). Starting from 50 sensors, the classification error is not higher than 2%, while from 90 is not higher than 1%. Similar trends and values characterize both classifiers for the number of sensors greater than 20. The analogous relationship is observed for average accuracy error with values going down from more than 2000 m for 10 and 15 sensors to 140 m for 100 sensors (see Fig. 5(b)). In contrast, the higher relevance values are better. So, in Fig. 5(c), we can observe the growth of the relevance with the increasing number of sensors. The relevance for only 20 sensors equals almost 50% and is growing practically with the linear trend to 100% for 100 sensors. A similar behavior is observed for both classifiers. However, the behavior of Fawcett stable classifier with the von Neumann neighborhood seems to be more stable and smooth.

7. The need and possibilities of the Layered Algorithm parallelization
The inspiration for the LA development was its application in actual problems like finding the location of the contamination source based on the sparse contaminant distribution. In practice, an appropriate algorithm should indicate the area in which the source is located quite quickly. The goal is to implement the LA in the localization system working in real-time, which means that the probable location should be indicated in minutes/seconds after the first registration of the dangerous toxin concentration by the sensors. What is more, the area suspected of being the contaminant source should be as small as possible. The highest impact on the LA computational time and resources requirements have size of the scanned area, number of sensors data, and a CA size. A large number of sensors data allow to increase the number of layers and by this improve the quality of the results, but with the number of layers, the computation time increases. The increase of the CA cells allows decreasing the size of the area assigned to a single CA cell, which results in a higher resolution of the LA results. However, it significantly increases the requirement for resources and computational time.

The LA tests presented in this paper were carried out on a limited number of sensors data with the 10000 CA cells in LA configuration. The tests started from the number of sensors equal to 10 (it means one sensor per 10 km2) and ended with 100 sensors (one sensor per 1 km2). The tests were conducted on a standard computer with Intel Core i7-8850H CPU @2.60 GHz 2.59 GHz with 16 GB RAM. The computational time for data analysis by the LA in a sequential version was in average approximately 2 minutes 30 seconds for a single run with 10 sensors. In comparison, for 100 sensors single run took in average 9 minute 40 seconds. An increase in the data number significantly increases the computation time. One must remember that LA is a probabilistic algorithm that has to be run in multiple iterations to supply the solution probability. Thus the 100 iterations of LA with 10 sensors increase the computational time up to about 5 hours and for 100 sensors for about 20 hours. Such a long computational time is not acceptable if it is to be implemented in a real-time working localization system. The solution is to develop the parallel/distributed version of the LA.

The LA was developed in a way that it can be easily parallelized. Separate processors can successfully realize the process of data classification in layers. In the LA initialization phase, the sensors distribution within the scanned domain and the reported data (concentrations) must be shared or sent to working processors. Then the classification process does not require communication between the processors. After finishing the classification, the common part of the classified areas can be determined gradually as each processor finishes the classification. The other obvious method of parallelization is a realization of the LA iterations by separate workers.

The presented method of LA parallelization should reduce the computational time. The almost linear speedup is expected, which will enable LA operation in real-time. The future work will present the LA parallel version and its performance.

8. Conclusions
This paper proposes the Layered Algorithm based on area optimization by layers and binary classification using three-state two-dimensional cellular automata. Cellular automata classify all initial area points according to the assumed criterion to designate the sub-area containing the optimum. The values ascribed to the cellular automata cells determine the layers. Such, approach allows specifying the areas in the different levels, and in consequence, after analysis, the sub-area containing the optimal point can be selected. The proposed algorithm's efficiency was verified with the localization of the airborne contamination source based solely on the spatial distribution of the registered contamination. The Layered Algorithm input parameters were the contaminant concentrations reported by the sensors distributed over the assumed domain. The data were generated by the Gaussian dispersion model simulating the contaminant dispersion within the domain. The Layered Algorithm's goal was to indicate the sub-area in which the contamination source was located. The interpretation of the obtained results was introduced in conjunction with the algorithm's assessment criteria, like classification error, accuracy error, and relevance. The algorithm results' measures corresponding to the algorithm run with each of three classification methods for different setups, i.e., the varying number of sensors (input data) were presented.

To solve the localization problem, two kinds of classifiers were used:    with Moore neighborhood, and also   with von Neumann's neighborhood. These classifiers were selected and described in our previous papers [14], [15], [8] with the use of the classical benchmark functions. In this paper, these classifiers were analyzed in detail and applied to solve a real problem, i.e., localizing the contamination source position based on the sparse point concentrations.

The conducted studies revealed that the proposed Layered Algorithm works well for the stated localization task. The obtained results measures are quite good. The reasonably high relevance value, equal to 100% was obtained for some number of sensors, randomly distributed within the considered area. Furthermore, the classification and accuracy errors are low. The classification error is smaller than 1% of the analyzed area. The minimal accuracy error is 140 m, which means the maximal distance between the point of the selected by the algorithm sub-area and the middle of the circle encircled on a selected sub-area. Performed tests show that the proposed method could be used in different optimization problems starting from simple ones, like optimizing n-dimensional functions, to more complicated tasks like designating the sub-area containing airborne toxin source based on the data given by a restricted number of mobile sensors. Conducted experiments show that the proposed algorithm can significantly reduce the scanned area. It can be especially useful when the number of sensors is restricted, or we have mobile sensors reporting the contamination varying in time and space. In that case, the efficient algorithm is required to estimate the spatial distribution of contaminants, and consequently, point out the area where the source is located. Furthermore, the detailed studies of the relevance of results obtained by the proposed algorithm indicate that it can be used as an optimization tool, which indicates the sub-area including the optimum.

The future work will focus on algorithm testing for different domain setups and contaminant dispersion scenarios to ensure that it will be a good alternative for methods currently applied in the problem of the airborne contaminant source localization, especially in the case of mobile sensors.