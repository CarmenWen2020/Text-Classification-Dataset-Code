Abstract
Network-on-chip (NoC) is an integral part of many-core microprocessors. Performance analysis of network-on-chip directly affects the performance of the microprocessor. In this paper we propose a mathematical model to represent packet flow in an NoC as an open feed-forward queuing network. We study the performance of NoC by varying different parameters that includes packet injection rate, packet size, buffer size and number of virtual channels. We also discuss how different flow control algorithms, injection processes, traffic patterns can be incorporated in our model. Apart from the speedup achieved by our model, we also demonstrate that our model can be used to explore various configurations of NoC with minimal error.

Previous
Next 
Keywords
Network-on-chip

Performance analysis

Many-core processor

Stochastic modelling

1. Introduction
In multi-core and many-core processors packet switched networks are used as an alternative to traditional bus based communication. The performance of the on-chip network directly affects the performance of the processor [2]. The efficiency of on-chip communication among cores depends on the network topology, routing, flow control, allocation, scheduling algorithms, traffic pattern and router micro architecture [17]. A major step in the design space exploration of the NoC is finding the configuration (combination of traffic pattern, injection process, routing algorithm...etc.) of NoC suitable for different traffic scenarios like best effort and guaranteed throughput [31]. The NoC metrics like delay and throughput must be evaluated quickly and fed back to the NoC model for optimizing the router design. Simulators and analytical models have been used extensively for the performance analysis of NoC [10]. Time taken to carry out the performance evaluation of NoC based on analytical model is much less as compared to that of the simulation time especially when the number of cores is large [21], [27], [32]. Further analytical models offer rich insight into the communication capability of the NoC leading to the design of newer routing algorithms, arbitration policies and flow control algorithms. Our aim is to develop an analytical model that can provide faster results as well as to accurately model the operation of NoC as a mathematical model.

In this section we will give a brief overview of NoC. A detailed account on basic concepts of NoC can be found in the textbooks [10], [41] and synthesis lectures on computer architecture [17]. NoC is characterized by its network topology, routing algorithm, flow control and allocation algorithms. The network topology represents the connections between router and channels in the network. Some example topologies are Ring, Mesh, Torus and fat-tree. Mesh is the most commonly used topology for NoC [10]. In a mesh network each router has five input ports and five output ports as shown in Fig. 1. North, East, South, West ports are connected to the neighbouring nodes and local port is connected to the local processor core.

Traffic pattern determines how packets are distributed among the nodes of the network. Traffic patterns can be realistic or synthetic. Realistic traffic patterns replicate real applications. Synthetic traffic patterns are generated using mathematical models. Some examples of synthetic traffic patterns are uniform, shuffle, nearest neighbour and hot-spot. The flow control algorithm controls the allocation and de-allocation of buffers and channels to packets. Router resources can be allocated at packet level or flit1 level. Allocators or arbiters assign the limited buffer capacity and output channel bandwidth to the incoming packets. Examples of resource allocation algorithms are round robin and separable allocation algorithm.

A generic router micro-architecture consists of the following components: buffers, routing logic, allocators, and a crossbar (or switch). An example input-buffered router architecture is shown in Fig. 2. Note that it is the processor that is the ultimate end point (source and destination) of the traffic. There are two ways to define a node. One way is to define the node as the router. Then the Node (router) maintains a queue and forwards the traffic, but cannot source or sink the traffic. Nodes are passive in this sense. We then need to model separate agents to source and sink. The other way is to model a node as a combination of processor and router, so that a node can not only maintain a queue, forward the traffic, but can also potentially source or sink traffic. We use the later definition of a node in this paper.

We refer to the combination of processor core, caches, network interface and router together as a node. Packets are injected into the network by the processor core of source node and received by processor core of the destination node through number of intermediate nodes. Since the flits of the packet are stored in the buffer before transmitting to the next node, each node of the network can be represented as a queue. A packet finishing service at a node may seek service at the next node in the route or exit the system if the current node is the destination node.


Download : Download high-res image (88KB)
Download : Download full-size image
Fig. 2. A typical router architecture of an NoC.

These characteristics of a packet flow are similar to a queuing network. A packet with  routers in between source and destination nodes can be modelled as a queuing network with  queues. Therefore we model the packet flow in an NoC using Open Feed-Forward Queuing Network. Feed-forward network preserves the operation of NoC by restricting the flow of the packets in only one direction avoiding the feedback. Open networks allow external packet arrivals and packet departures. Further to keep our model as general as possible we do not make any assumption on the arrival and service characteristics of the queue. Motivated by this analogy in this paper we develop an analytical model for NoC using generalized Open Feed-Forward Queuing Networks [16], [20] and carryout performance analysis of NoC.

2. Literature survey
Queuing theory is a powerful tool for the performance analysis of computer networks as well as on-chip networks. Guan et al. [13] have proposed a queuing theoretic analysis to find the routing delay for hyper cube and mesh networks. Kleinrock et al. [15] have developed a queuing model for wormhole routing with finite buffer size. Guz et al. [14] have explored the possibility of non-uniform link capacities in NoC and created a link capacity allocation algorithm. In [11] Ahsen et al. have proposed a method to achieve double data rate and showed that this NoC achieves higher throughput than single data rate networks. Scalability and power consumption are two main problems of NoC’s, Junghee et al. [23] have introduced a new flow-control mechanism that can improve both scalability and power consumption of on-chip networks. Prasad et al. [29] have introduced a new topology ZMesh, used M/G/1 queues for channel modelling. Nikitin et al. [26] have modelled the NoC as constant service time system using M/D/1 queue analysis thus eliminating the need to assume intermediate packet distribution rates as Poisson. Kiasari et al. [21] have proposed a generalized performance queuing (PQ) model based on G/G/1 queue which supports any arbitrary traffic pattern. Ogras et al. [27] have proposed a new router model for NoC that allows to derive a closed form expression to find the average number of packets at each buffer under Poisson traffic.

Bogdan et al. [6] have discussed the limitations of queuing theory and Markovian based traffic approaches and proposed a statistical physics based method for traffic characterization. Bogdan et al. [7] have suggested a mean field random graph based method to capture the non-stationary and multifractal characteristics of NoC traffic. Bogdan et al. have also applied their analysis for mathematical modelling of workloads for data-centres-on-chip(DCoC) [5]. Qian et al. [30] have provided a detailed review of existing and future models for NoC analysis. They have discussed various scenarios for traffic modelling from simple Markovian models to complex multifractal approaches. A Support Vector Regression (SVR) based Latency Model for Network-on-Chip (NoC) has been proposed by Qian et al. [32]. Xiao et al. [42] have introduced a novel self optimizing and self programming computing system (SOSPCS) framework based on machine learning and neural networks for efficient mapping in heterogeneous systems. Most of the literature mentioned in this paragraph was dedicated to finding the alternatives to queuing based approaches like statistical physics, mean field analysis etc. However in this paper we explore the queuing approach further by using generalized queues which can model the traffic using different injection processes that can exhibit non-stationary, self-similarity and long range dependence characteristics.

Although there have been a number of attempts in the literature to analyse the performance of NoC using queuing theory, almost all of the work concentrates on modelling the router or channel as a stand-alone queue and analyses the queue independently [21], [27]. On the other hand a packet is routed through a number of routers and therefore a flow is affected by the inter dependency between the queues maintained by the routers along the path of the packet flow. So a packet flow can only be accurately modelled using Network of queues. There has been very little attempt in the literature to capture the inter dependencies between the queues of a packet flow in an NoC. Further most of the analytical models in the literature base their analysis on assuming Poisson injection process for the arrival of packets. This makes the analysis easier. However if the arrival process is not Poisson and the service time is not exponentially distributed then we need to model the NoC’s as generalized open feed-forward queuing networks. The present paper attempts to fill the gap.

The aim of the paper is to analyse the performance of NoC by modelling it using a generalized open feed-forward queuing network. We analyse the effect of injection process, packet length, buffer size, the number of virtual channels on packet latency.

Following are the unique contributions of our work that differentiates our work from the rest.

•
We model the NoC as a generalized feed-forward queuing network. Though many models use queuing theory, most of them model individual nodes only, and do not consider NoC as a whole and model it as network of nodes.

•
Unlike most of the models we do not invoke Markovian approximation. Our model is generalized and can therefore handle arbitrary injection processes (such as Gamma, Weibull, Pareto) and service characterization.

•
Most of the analytical models in the literature concentrate on one or two aspects of NoC for performance analysis. We evaluate the performance of the NoC with respect to a wide variety of parameters such as number of virtual channels, traffic patterns, injection process, routing algorithms, packet size and buffer size.

•
Most of the analytical models in the literature consider wormhole router architecture, while we use virtual channel router architecture.

Rest of the paper is organized as follows: In Section 3 we present the system model of Network-on-Chip from the perspective of network of queues approach. In Section 4 we derive the equation to calculate expected service time at a node. In Section 5 we model NoC as generalized network of queues and analyse it. We present the analytical and simulation results under various configurations in Section 6. In Sections 6.1 to 6.9 we deal with various traffic patterns, routing algorithms, injection processes, packet size, buffer size and run time analysis. Final conclusions are given in Section 7.

3. System model
In this section we present the system model used for the performance analysis of NoC. An  network-on-chip on a many-core processor consists of 
 routers connecting 
 processor cores for faster communication. We consider an NoC with following characteristics which is commonly used in the NoC literature [9], [21], [27]:

•
 network with Mesh topology

•
DOR (Dimension Order Routing)

•
Uniform traffic pattern

•
Packets are transferred flit by flit.

•
Once a packet reaches destination node, the packet will be immediately transferred to the corresponding processor core without any delay.

Extension to other traffic patterns, routing algorithms, topology and packet generation process are discussed in the subsequent sections. Injection rate is measured as packets/cycle2 and packet latency is measured in cycles. We define the NoC metrics as follows: Packet Latency, denoted as , is time elapsed from the instant the first flit of the packet is injected into the network at the source node until the last flit of the packet is received at the destination node. The terminology used in the manuscript is defined in Table 1.

Each node  in the  mesh network is characterized by three random processes. Random processes 
, 
, 
 represent the injection process, aggregate arrival process, service time process respectively at node .


Table 1. Terminology.

Variable	Description
Buffer size (flits)
Packet size (flits)
Total number of nodes(
)
Number of neighbours to node 
Number of Virtual Channels at node 
Injection process at node 
Aggregate Arrival process at node 
Service process at node 
Service time at node 
Injection rate to node 
SCV of injection process
Aggregate arrival rate at node 
SCV of aggregate arrival process
Service rate at node  (
 
)
Service rate at node  averaged over all flows
SCV of service process
Packet latency
•
Injection process 
 governs the rate at which packets are injected into the network by the local processor core of the node .

•
Aggregate arrival process 
 at a node  characterizes the overall rate at which packets from different flows pass through that node.

•
Service process 
 governs the rate at which packets are serviced at a node .

•
Service time at a node  is the duration from the time a packet arrives at the node (i.e., stored in the buffer) to the time at which the packet leaves the node (i.e., transferred through the channel to the next node).

We can find the aggregate arrival rate 
 at each node  using the following traffic flow equation [34]. (1)
where 
 is the routing probability (probability that a packet goes to node  from node ) for . 
 is the probability that a packet leaves the network from node  after completing service (i.e after reaching the destination node). 
 

In an  mesh network there will be 
 nodes. The corner nodes will have only two neighbours, edge nodes will have three neighbours and centre nodes will have four neighbours. From this information we can define the adjacency matrix that enables us to find the routing probabilities. For an  mesh network the adjacency matrix  is an  matrix. The value of each element 
 is  or  as given below . (2)
 

Sum of a row in adjacency matrix  gives the number of neighbours 
 at node . (3)

We assume that there are large number of flows in the network at any given time. Each flow takes a route which is independent of the route taken by the other flows. Therefore large number of independent flows are statistically multiplexed through each node. As a result we assume that a packet that comes to a node is equally likely to be routed to any one of its neighbours or sinked to the local core. Therefore Routing probability 
 is given by (4)
 
 
 

For example in a 4 × 4 mesh network shown in Fig. 3 the routing probability for a corner node (1, 4, 13, 16), centre node (6, 7, 10, 11), edge node (2, 3, 5, 8, 9, 12, 14, 15) will be 
 
 
 
 respectively.

We use synthetic benchmarks for our performance analysis. Injection process and traffic pattern are the two key components in a synthetic benchmark. The traffic pattern determines spatial distribution of traffic i.e., destination node for a given source node. Injection process defines the temporal distribution of traffic i.e., the time at which packets are injected into the network. The traffic pattern can be specified using a matrix for source and another for destination. For example a mesh network of size 3 × 3 with uniform traffic pattern can have the source and traffic matrices as follows: 
 
 


Download : Download high-res image (248KB)
Download : Download full-size image
Fig. 3. A 4 × 4 NoC with three flows 
, 
, 
.

 is the source matrix with the element 
 giving the label for the nodes in the  coordinate position.  is the traffic matrix that gives the label of the destination for the corresponding source node in the source matrix. In other words traffic flows from 
 to 
.

We use the generalized  queue to model individual nodes. The arrival process and service process are general. According to KLB (Kraemer and Langenbach-Belz) formula the average waiting time at node  can be calculated using (5)
 
where 
 
 and 
 

For more details on waiting and service time calculations of G/G/1 queuing model readers can refer to [22], [35].

The NoC can be abstracted as a graph , where  is the set of all routers. The cardinality of  is .  is the set of all edges 
. A flow is a set of nodes, that represents the path taken by the packets between two communicating processors. Each flow is characterized by the source node, destination node, and intermediate nodes (determined by the routing algorithm) along with the number of channels between the nodes. In graph theoretic terminology each flow  is a path in the graph  representing the NoC. Let  denote the set of flows in the network. Each flow  has a variable number of hops , having  nodes numbered  along the flow as shown in Fig. 4.

4. Service time model
In this section we model each node as a server and calculate the service time at an arbitrary node of the network. Service time is sum of waiting time and transfer time. Waiting time is the time the packet has to spend in the buffer before completing the transfer. Transfer time is the time required to transmit all flits of a packet from a node. We assume the width of the channel is equal to the size of the flit. So one flit can be transferred through the channel in one cycle. As the number of packets injected into the network increases multiple flows will try to acquire the same channel, which forms contention among the flows. When there is no contention it takes  cycles to transfer the  flits of the packet from one node to the next node and there is no waiting time, so service time is equal to the transfer time. Therefore service rate of a packet when there is no contention is given by 
 
 and the service time is  cycles.

4.1. Service time calculation
Service time with contention can be calculated as follows: Consider the flow shown in Fig. 4. The example flow consists of  nodes or  hops from source to destination. Each node has 
 virtual channels and the buffer size is  flits. Nodes in the flow are numbered from 1 to  in ascending order from source to destination. A buffer is allocated and reserved for a flow until all the flits of the packet are transferred from that node. When a packet  reaches a node ,  flits of the packet are stored in the buffer (since the size of the buffer is ). The buffer will be released to other packets only after all  flits of the packet  are transferred from node  (since the size of the packet is m). If  is less than  then only  flits of the packet can be transferred at a time from previous node or local network interface to the buffer. To complete the transfer of packet from the current node to next node the data has to be brought to the buffer 
 
 times. Once the channel is available and the buffer in the next node  is free, packet transfer from current node  will be initiated.

If there is contention the packet transfer may be interrupted after a few flits are transferred, so to complete the packet transfer from node  the outgoing channel has to be acquired multiple times which incurs additional latency for the packet.

Since the packet size is  flits and buffer size is  flits the packet has to be spread among 
 
 buffers if  and 1 buffer if  to complete the transfer from node . We denote packet to buffer size ratio 
 
 as 
. Expected service time at a node  
 is determined by using blocking probability and average service time at node . (6)

 ( blocks) is the probability that a packet is blocked  times at node  before the packet is transferred completely from node  to . Since the packet size is  and buffer size is , the maximum number of times a packet is blocked at a node is 
.

When there is no blocking it takes  cycles to transfer the whole packet. The first term in Eq. (6) corresponds to this. If the packet is blocked 
 times it takes 
 cycles to transfer the whole packet as shown in the last term of Eq. (6). Let us assume that a flow  has  hops then we can write the equation at each node of the flow  as shown below in Eqs. (7) to (9). (7)
(8)
(9)
 Let (10)
Then Eqs. (7) to (9) can be written as shown below (11)
 Substituting 
 in 
, we will get 
(12)
 Similarly after substituting remaining equations 
, 
, …, 
 in Eq. (12) we will get the following expression (13)
Finally we can write the expected service time for a flow at node  as (14)
where 
 is the effective number of hops the packet will be spread across the network, 
 is the minimum of 
 and 
. 
where 
 is the remaining number of hops from current node  to destination node of the flow, 
. Therefore the service time calculated using Eq. (14) will be specific to a flow   . In particular it is a function of nodes in the flow and number of hops in the flow.

4.2. Calculation of blocking constants
To calculate the blocking constant 
 given by Eq. (10) we have to find the blocking probability. To this end we invoke an approximation and use the formula for the blocking probability of an M/M/1/K queue. Here K is the waiting space of the finite-buffer queue [22] and is equal to 
, the number of virtual channels at a node  in our model. Blocking probability 
 at node  is [35], (15)
 
 
 where 
 is the server utilization factor at node . Probability that a packet is blocked  times before the complete transfer of packet from node  to node  is given by (16)

To find the blocking probability 
 at node  using Eq. (15) we have to calculate 
 
. Effective arrival rate 
 at node  can be found using Eq. (1).

Expected service rate 
 at node  is the weighted average of expected service rate of all the flows waiting for service at node  given by (17)
 
In Eq. (17) the summation is done over the set of all flows  that pass through the node . The symbol 
 denotes the process of averaging over all flows that get multiplexed through a node. Note that 
 is a function of the flow , although not explicitly mentioned.

5. Generalized network of queues model
In a queuing network packets corresponding to different flows are transferred through many queues before reaching the destination which results in aggregated flows. The characteristics of the aggregated arrival process will be different from the injected process. For example in the network shown in Fig. 3 there are three flows 
, 
, 
.

Assume 
 and 
 are steady state probability density functions of injection and departure processes respectively at node . Then the density function of aggregate arrival process at node 2 will be the convolution of departure process at node 1 
 and injection process at node 2 
. There is no known solution for the analysis of network of queues with dependent arrival or service times in the literature. There are three methods proposed in the literature to address this problem [34]: Exact methods, Approximation methods and simulation techniques.

Exact methods like Jackson’s method are valid only under certain simplifying assumptions. Simulation techniques are time consuming as the size of the network increases. Further they do not offer insight in to the dynamics of network of queues. Approximation methods like diffusion approximations, mean value analysis and decomposition methods have been proposed in the literature. In this paper we have adopted the decomposition method for solving the generalized network of queues model for NoC.

In generalized network of queues model injection, arrival and service processes are characterized using first two moments of the random processes i.e., expected value or mean and variance. The second moment is used to calculate the square coefficient of variation. For example let 
 be the SCV or variability of service process at node . 
 is obtained from expected value and variance as follows: 
 
 
. Let 
 be the variability of aggregate arrival process at node , 
 can be found using the asymptotic method [37] (18)
 
 
where 
 is the SCV (squared coefficient of variation) or variability of injection process at node , 
 can be calculated using the equation3 
 

, 
, where 
 is the proportion of arrivals to node  that came from node  i.e., 
 
.

 is the expected service rate at node  that can be calculated using Eq. (17). Effective arrival rate 
 is calculated using Eq. (1).

Recall the graph theoretic abstraction of NoC given at the end of system model. Averaging over set of all the flows that pass through node  we get (19)
 
 
By using the expected value and square coefficient of variation of aggregate arrival process and service process we can find the waiting time of the packet at a node  using Eq. (5).

Packet latency for an arbitrary flow is given by (20)
 
where 
 
 
is the expected number of visits by different flows to node i.


Table 2. Network parameters used for Booksim simulator.

Parameter	Value
Network dimensions	3  3 nodes
Topology	Mesh
Per VC buffer size	4 flits
Packet length	20 flits
Credit delay	2 cycles
VC allocation delay	1 cycle
Routing delay	1 cycle
Switch traversal delay	1 cycle
Switch allocation delay	1 cycle
6. Results and analysis
This section is organized as follows: In Section 6.1 we validate our model using simulation results. In Section 6.2 we show how our model can be adopted for different traffic patterns. In Section 6.3 we discuss how an adaptive routing algorithm can be included in our model. In Section 6.4 we show how the performance of the NoC is affected by different injection processes. Effect of buffer size and packet size on the packet latency is explored in 6.5. We report the speedup achievable by our analytical model as compared to the simulation run time in Section 6.6. The distribution of packet latency is explained in Section 6.7. We plot the throughput obtained from both analytical model and simulation, explain them in Section 6.8. Prediction accuracy of our model is explained in Section 6.9.

6.1. Validation
In this subsection we validate our analytical results for the generalized model by comparing it against simulation results obtained using Booksim simulator. BookSim [18] is a cycle-accurate simulator developed for NoCs at Stanford university. BookSim can be used to study many aspects of NoC design including topology, routing, flow control, router micro-architecture and quality-of-service. The analytical results for the packet latency given by Eqs. (1), (17), (5), (20) have been computed numerically using MATLAB. The parameters used for numerical computation and simulation are shown in Table 2. We have used synthetic traffic patterns in all our experiments. Fig. 5 shows numerical results computed using generalized model and simulation results obtained using Booksim simulator. In Fig. 5(a)–(d) we plot latency vs. injection rate for different number of Virtual Channels. The close match between the analytical and simulation results validates our model. Further we can observe the following, (i) For 
  1 the packet latency rises exponentially at injection rate of 0.015 as the queue loses stability beyond this range. (ii) For values of 
 greater than 2 there is no significant improvement in the latency with increase in 
 for the given configuration.

It can be seen that there is a fairly good match between the simulation and analytical results in Figs. 6, 7, 8 and also in Fig. 5(c) and (d). The deviation between the analytical and simulation results occurs only after the saturation point as shown in Fig. 5(a), (b). After the saturation point (0.011, 0.02, 0.022, 0.024 for 
 respectively) the delay increases exponentially.

As the injection rate increases there will be more in flight packets in the network, which increases contention and leads to saturation [40]. As the injection rate increases, packet flows are subjected to merging at different routers along the path resulting in exponential reduction of available bandwidth [10]. In queuing theory the queues are stable only until the saturation point. The performance of NoC can be estimated correctly only in this stable region of the queues. Further as mentioned in Section 5, the generalized network of queue models cannot be solved exactly. We have invoked the decomposition approximation for solving the result. This may be a reason for the deviation between analysis and simulation after saturation.

6.2. Traffic pattern
We use synthetic traffic to evaluate the performance of NoC. Most of our analysis is based on uniform random traffic pattern, where a packet generated at node  has equal probability to select a node  as its destination, where  and . Two commonly used classes of synthetic traffic patterns are bit permutation and digit permutation. Bit permutation based traffic patterns like Shuffle and Transpose traffic pattern obtain bits of the destination address by manipulating bits of the source address. In digit permutation based traffic patterns like Tornado and Neighbour traffic patterns entire destination address is obtained by manipulating the source address. Detailed information on traffic patterns can be found in [10], [17], [41]. Most of the synthetic traffic patterns are modelled to closely represent real applications. For example Transpose traffic pattern can be used to model matrix transpose operations and corner-turn operations, Neighbour traffic pattern can be used to model fluid dynamics simulations [10] and Shuffle traffic pattern can be used to model FFT and sorting applications [39]. Formally the traffic patterns can be defined as follows.

For an  mesh network with  nodes, assume the nodes  to  have a binary address of size 
 bits. Let  to  denote the source address and  to  denote the destination address. For bit permutation based traffic patterns  must be a power of 2.

6.2.1. Shuffle traffic pattern
In shuffle traffic pattern bits of the destination address are obtained from the source address using the mapping  where  to . In Fig. 6 we have shown the variation of average packet latency for a 4 × 4 mesh network with shuffle traffic pattern keeping the number of virtual channels equal to 4.

6.2.2. Tornado traffic pattern
In Tornado traffic pattern the address of destination node is obtained using the formula 
 
. The average packet latency in a 3 × 3 mesh network with Tornado traffic and number of virtual channels equal to 4 is shown in Fig. 7.

6.2.3. Neighbour traffic pattern
In Neighbour traffic pattern the address of the destination node can be obtained using the formula . Applications that exploit the locality in the network can be modelled using Neighbour traffic pattern. The average packet latency in a 3 × 3 mesh network with neighbour traffic and number of virtual channels equal to 4 is shown in Fig. 8. Fig. 6, Fig. 7, Fig. 8 demonstrate that our analysis can be adopted to model different traffic patterns.

6.3. Routing algorithm
In our initial analysis we have used XY Dimension order routing algorithm. Our analysis can be extended to other routing algorithms as follows. Changes in routing algorithm and traffic pattern will be reflected in the routing probability matrix . There are two types of routing algorithms: oblivious and adaptive. Oblivious or deterministic routing algorithms do not consider the state of the network such as whether channel is congested or not, and whether the buffer is full or not. In other words the packets are routed through a predetermined path by deterministic routing algorithms.

Adaptive routing algorithms may route the packets through paths to avoid congested or failed channels. Adaptive routing algorithms provide path diversity.

A simplest adaptive routing algorithm is the turn model routing algorithm that prevents dead-lock by reducing the number of turns in the path but at the same time provides flexibility in the path taken by the packets. The XY routing algorithm along with three turn model routing algorithms is illustrated in Fig. 9. In turn model routing 6 turns are allowed whereas in DOR only 4 turns are allowed. Detailed information on routing algorithms can be found in [10]. In the west-first routing algorithm south-to-west turns and north-to-west turns are restricted in the sense that all west turns must be completed first.


Download : Download high-res image (76KB)
Download : Download full-size image
Fig. 9. Turn models (a) XY (b) West-first (c) North-last (d) Negative-first (solid lines for permitted turns and dashed lines for prohibited turns).

Consider the packet flow 
 from node 5 to node 16 for a 4 × 4 network shown in Fig. 10. For example in XY dimension order routing a packet is routed in the X-dimension first and then Y-dimension. If XY dimension order routing is used then the packet is first transferred from node 5 to node 8 through nodes 6 and 7, then packet is transferred from node 8 to 16 through node 12 taking 5 hops in total. Whereas in the case of west-first adaptive routing, the algorithm considers the state of channels and routes the packets through less congested channels as shown in Fig. 10 with a dotted line. The channels with thick black line are congested channels, channels with dashed lines are restricted channels in case of west-first routing.

If an adaptive routing algorithm like west-first turn model is used the routing matrix can be derived as follows: If West-first turn model routing is used the packet avoids the congested channels as shown in Fig. 10. So the probability that a packet goes from node  to node  depends on the source and destination. For the example discussed above the probabilities at node 5 are 
, 
, 
. That is the packet is equally likely to take one of the following three channels 5 to 1 or 5 to 6 or 5 to 9. Let us assume that the channels 5 to 6 are selected. Similarly at node 6 the probabilities are 
 (since west turns cannot be taken), 
 (channels from 6 to 7 are congested), 
, 
. These routing probabilities can be used to calculate the aggregate arrival rate using Eq. (1), which can be further used to find the packet latency using Eqs. (17), (20).


Download : Download high-res image (137KB)
Download : Download full-size image
Fig. 10. An example packet flow 
. Dashed lines show restricted turns for the West-first routing, Thick black lines show the congested channels.

Our model can be used for arbitrary routing algorithms and traffic pattern by deriving the routing matrix as mentioned above. In Fig. 11, Fig. 12 we plot the average packet latency against the injection rate for West-first routing and XY Dimension Order Routing. Fig. 11 compares the routing algorithms for a 5 × 5 mesh network with uniform random traffic pattern and 4 virtual channels. For uniform traffic pattern the average packet latency is less in case of dimension order routing as compared to the West-first adaptive routing which was also reported in Ref. [33]. Fig. 12 compares the routing algorithm for a 4 × 4 mesh network with shuffle traffic pattern and 4 virtual channels.

We have also plotted the injection rate versus packet latency for XY–YX routing algorithm as shown in Fig. 13. We also compared the results of XY routing algorithm and XY–YX routing algorithm in Fig. 14. In XY–YX routing algorithm one of XY or YX DOR routing algorithms is used randomly for each packet flow. This leads to optimum load balancing. The disadvantage with XY–YX routing algorithm is, it may lead to deadlock which can be avoided by using more than one virtual channel.


Download : Download high-res image (136KB)
Download : Download full-size image
Fig. 11. Comparison of using XY DOR and West-first routing: Injection rate vs. Packet latency for a 5 × 5 mesh network with Uniform random traffic pattern and 
  4.


Download : Download high-res image (135KB)
Download : Download full-size image
Fig. 12. Comparison of using XY DOR and West-first routing: Injection rate vs. Packet latency for a 4 × 4 mesh network with shuffle traffic pattern and 
  4.

6.4. Injection process
We further explore the effect of injection process on the packet latency. We have considered the following injection processes: Gamma, Pareto, Weibull and Geometric process. The motivation for exploring these injection processes are as follows: Studies have shown that many applications generate traffic that are self-similar in nature or the governing probability distribution of the random process are heavy tailed [4]. In other words the traffic has burstiness at all time scales which causes more congestion and cannot be modelled using Poisson approximation. Further it is shown that Generalized Pareto Distribution (GPD) and Weibull distribution can act as good mathematical models for such realistic self similar traffic [8]. Pareto distribution is used to model distribution of tsunami sizes [12], in cyber physical systems the likelihood of penetration through a point in a connected system [38]. Weibull distribution is used to describe the size distribution of a particle population formed by fragmentation crushing [19], in statistical theory of the strength of materials [25] and in reliability, survival studies to analyse life time or response data. Gamma distribution is used in reliability analysis to fit failure data [24], to represent shocks in modelling of graceful deterioration [36], to model rain rate distribution at higher rates [1]. The Gamma distribution can also be modelled as the sum of exponential random variables. Geometric distribution is encountered in those traffic models wherein an application generates a packet once per iteration of a loop until a condition is met. Geometric distribution can also be used for population modelling, econometrics and return of investment research. Figs. 15, 16 show the numerical results computed using Generalized model for Gamma, Generalized Pareto Distribution, Weibull and Geometric distributions as injection processes respectively.

In Fig. 15(a) we have plotted packet injection rate vs. average packet latency using Gamma distribution as injection process. Gamma distribution is characterized by the shape parameter  and scale parameter . The values of the parameters for the plots in Fig. 15(a) are 
, ; 
, ; 
, , where 
 is the injection rate that spans the -axis. We observe that as the value of shape parameter  increases packet latency decreases. The scale parameter  has no significant effect on the packet latency.

Fig. 15(b) shows packet injection rate vs. average packet latency wherein the injection process is governed by generalized Pareto distribution. GPD is characterized by the following three parameters; (i) Tail index or shape parameter , (ii) scale parameter , (iii) threshold or location parameter . The values of the parameters of the GPD for the results plotted in Fig. 15(b) are 
, , ; 
, , ; 
, , ; 
, , . If ,  GPD acts as exponential distribution. Other parameters being constant we observe the following (i) Packet latency increases as  increases, (ii) packet latency increases as  increases, (iii) Packet latency decreases as  increases.

Numerical results for Weibull distribution as injection process are shown in Fig. 16(a). Weibull distribution is characterized by the scale parameter  and shape parameter . The values of the parameters used in the simulation for the results plotted in Fig. 16(a) are 
, ; 
, ; 
, . The shape parameter  has no effect on the packet latency. However the packet latency increases as the scale parameter  increases.


Download : Download high-res image (231KB)
Download : Download full-size image
Fig. 15. Injection rate(
) vs. Packet latency using Gamma distribution as injection process for 
  2.

6.5. Effect of packet and buffer size on packet latency
Fig. 17 shows the effect of buffer size on packet latency for various number of virtual channels. For this experiment we have used Geometric distribution as the injection process with injection rate 
  0.001, packet size  10 and 20 flits for Fig. 17(a) and (b) respectively. As the buffer size increases the latency decreases up to a particular point, after which there is no effect of increasing buffer size. The optimal buffer size for 
 and  is 3 flits as shown in Fig. 17(a), similarly for 
 and  the optimal buffer size is 6 flits as shown in Fig. 17(b).

Fig. 18 shows the effect of packet size on the packet latency. For this experiment we have used Geometric distribution as the injection process. Buffer size used is 4 and 8 for Fig. 18(a) and (b) respectively. We observe that as packet size increases the overall packet latency increases exponentially. For a given packet size as the buffer size increases latency decreases as shown in Fig. 18, which has been already demonstrated in Fig. 17.

6.6. Run time analysis
In Fig. 19 we compare the simulation run time of Booksim simulator against the corresponding time taken for numerical computation of average packet latency using our analytical model for NoC of varying size. As the size of the network increases from 2 × 2 to 16 × 16 the simulation time required to obtain the performance metrics increases exponentially whereas the run time of analytical model increases only linearly with size. For example for a 10 × 10 mesh network simulation using Booksim simulator takes 12 h to give the result, whereas numerical computation takes only 38 min. (Both calculations are done on a PC containing Intel core i5-6500 processor with 6 MB cache, 4 cores, 4 threads and 8 GB RAM.) This gives a speedup of about 20 for one configuration of NoC. Now consider the various degrees of freedom on the design configurations such as different traffic pattern, routing algorithms, size of NoC, packet generation process, packet and buffer size, number of virtual channel etc. If the design engineer wants to carry out performance evaluation of 20 different configurations of the NoC then the speedup becomes very significant.

6.7. Distribution of packet latency
In this section we have plotted the distribution of packet latency obtained from analysis and simulation for one example configuration of the NoC. It can be seen that as compared to the analytical results the probability distribution obtained through simulation has less variance. However both distributions have the same shape and mean value. Fig. 20 shows the normalized histogram of packet latency obtained through numerical computation of analytical results and simulations.

6.8. Throughput
In this subsection we have carried out simulation as well as numerical computation for the throughput of the packets and it is shown in Fig. 21. Even though the size of the buffer is limited, there is no concept of dropping packets in network on chip. The packets will wait in the source queue at the Network Interface until a free buffer is available in the router, then the packet will enter the network through the local channel. Since packets are not lost (in infinite capacity nodes), throughput the number of packets delivered per unit time is equal to the injection rate. The capacity of the NoC will fix the maximum supported throughput or injection rate. At low traffic injection rates the packet delivery rate or throughput is directly proportional to the packet injection rate [28], throughput becomes stable after the saturation point. We have calculated the saturation throughput by extracting the saturation point from the packet latency data plotted in Fig. 5(d). The saturation throughput is equal to the packet injection rate at which the delay increases exponentially in the packet latency plot. It happens at about 0.022 in Fig. 5(d). So ideal theoretical curve is a line with slope  1, till it reaches saturation value, and then remains flat. The simulation curve asymptotically reaches the saturation throughput value after saturation.

6.9. Prediction accuracy
Apart from the speedup achieved by the analytical model one would be interested to know the prediction accuracy of our model. The closeness of results shown in Figs. 5(a)–(d), 6, 7, 8 compared to Booksim simulator further strengthens our analysis. Not only our model produces acceptable results but also our analytical model is significantly faster compared to the simulation as discussed in Section 6.6. The performance analysis of NoC for different configurations can be evaluated with minimal error using our model.

To this end we define error as the difference between the analytical results and the simulation results. We have computed the standard error of the mean between the analytical result and the simulation result. The standard error of the mean is defined as standard deviation of the error divided by square root of number of samples [3]. Fig. 22 shows the standard error of mean for various sizes of mesh NoC (from 2 × 2 to 10 × 10) and for various values of 
. It can be seen that for NoCs with 36 routers the error is within 10% and it is less than 15% for NoCs having up to 64 routers.

7. Conclusion
We have demonstrated that an NoC can be accurately modelled as an Open feed-forward network of queues. Our model is quite general in analysing any traffic pattern and characteristics as long as the first two moments of the injection, arrival and service process are known. We have studied the effect of the number of virtual channels, traffic patterns, routing algorithms, injection processes, buffer size and packet size on the performance of Network-on-chip. Our model offers significant speedup over simulation methodology and can be used in the optimization loop of NoC design. Our model can be extended to handle different routing algorithms, traffic patterns as demonstrated in Section 6. In future, our analysis can be extended to the study of simultaneous execution of heterogeneous workloads by using multi-class queuing networks to model the NoC.

