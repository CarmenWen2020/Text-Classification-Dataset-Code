Abstract
Collaborative Deep Learning (CDL) utilizes the strong feature learning capability of neural network and the model fitting robustness to solve the problem that the performance of Recommender System drops dramatically when the data is sparse. However, it makes the model training become difficult to maintain when Recommender System faces a large amount of data, and a variety of unpredictable problems will arise. In order to solve the above problems, collaborative deep learning and its parallelization methods were studied in this study, and an improved model CDL-I (CDL with item private node) aiming at item content optimization based on collaborative deep learning was proposed, which improved SDAE on the basis of CDL, added private network nodes; in case of sharing the network parameters of the model, private bias terms were added for each item. As a result, the network may learn the item content parameters in a more targeted manner, thereby enhancing the detection performance of the model on item content in Recommender System. Furthermore, the algorithm was parallelized by splitting the model, and a parallel training CDL-I method was also proposed, which was transplanted to the Spark distributed cluster. The parameters of each part of the model were trained and optimized in parallel to enhance the scale and scalability of data that the model could process. The experiments on multiple real datasets have verified the effectiveness and efficiency of the proposed parallel CDL-I algorithm.

Previous
Next 
Keywords
Collaborative Deep Learning (CDL)

Recommender System

Parallel computing

Spark

1. Introduction
1.1. Motivation
Nowadays, due to the rapid growth of information and the rapid generation of mass data, Recommender System plays a vital role in obtaining effective information in a timely manner, and it has changed the way people interact with information [19]. Through identification, distinguishing and sorting of mass data, Recommender System helps people to filter out the most interesting and useful parts from massive information, which greatly simplifies the process of information selection [10]. As the support of Recommender System, machine learning algorithm is based on personalized recommendation, and most of the systems are based on collaborative filtering methods for recommendation. However, the biggest problem faced by collaborative filtering is the cold start [41]. When a user or an item is a newly added object, due to its insufficient historical information, the system often fails to analyze and operate it well, resulting in a significant reduction in recommended performance. In addition, this method has two significant shortcomings: data sparsity and model scalability. Extremely sparse scoring matrices lead to extremely low available information rate and calculation operations are mostly invalid operations. If the Recommender System is based on the collaborative filtering model, the computational cost has super-linear increase with the number of users and items [2], [26].

With the further enrichment of information, the expansion speed of data gets faster and faster, and it is foreseeable that the improvement of hardware performance will not be able to meet the rapid expansion of the amount of data and become the bottleneck of contemporary complex operations. The multiple iteration characteristics of Recommender System also put forward more demanding requirements for computing performance. Gradually, people begin to use GPU with many-core systems, single-machine multi-device parallelism, and distributed data processing platforms to expand computing capability to meet computing needs [40]. The distributed platform with its enormous expansion space will definitely be used as the basic component, integrating multi-card parallelism, and helping computing science develop further. When deep learning is applied to recommendation scenarios and faced with massive amounts of computing data, CDL (Collaborative Deep Learning) based on distributed computing platforms will inevitably become a feasible choice to solve problems such as insufficient computing resources, huge amounts of data and massively complex operations, and become an important solution in the scenario of complex recommendations for massive amounts of data. At the same time, the data on the Internet is getting richer and richer, and the redundant information is becoming more and more, it is difficult to extract effective information from it. It has important practical significance to study deep learning-based recommendation problems and their algorithms, and introduce parallel learning methods.

1.2. Research contributions
The improvement of this paper was made based on the CDL algorithm [35] and its model. The model CDL-I (CDL with Item Private Node, CDL-I) was proposed, and the parallel research on the CDL algorithm was conducted. Relying on the Apache Spark distributed computing platform, CDL algorithm was parallelized. Results show that it has improved the use degree of the Recommender System for data information and the scalability of the Recommender System in the era of big data.

CDL constructs a multi-layer Bayesian model by combining the deep representation learning of the auto-encoder for the item content and the feedback information of the collaborative filtering scoring matrix. Based on CDL, this paper improved the rough detection of the item content and proposed a CDL-I model. Combined with multiple sets of experiments on the real data set, it proves that CDL-I improves compared to traditional recommendations. Then this paper introduces the parallel improvement of the algorithm and the effect after being ported to the Spark platform. The contributions of this paper can be briefly summarized as follows:

(1) CDL is improved and the CDL-I model is proposed. Taking into account the differences in item content categories, an item privatization node is introduced for CDL for differential training. CDL-I retains unique bias weight for each item, which improves the performance of the algorithm in content detection. When recommending different items to users, the recommendation results are improved based on the privatization bias amount of the combined item.

(2) On the basis of CDL-I, the parallelization of algorithm is studied, experimented and analyzed. A parallel CDL-I training method was proposed. The model is split and trained in parallel, and the training time is shortened under the same data scale. At the same time, data parallel is adopted for the model and the data, and the sub-models are collaboratively calculated to improve the model capacity and algorithm scalability.

(3) Combined the Spark distributed computing platform, this paper implements parallel training of the algorithm, and obtains a series of experimental data based on objective data sets. By analyzing and comparing the data, the researchers verified the effectiveness of the model improvement, and proved the improvement of parallel research on the algorithm performance, thereby reflecting the significance of distributed computing in the Recommender System, and obtaining a highly scalable parallel CDL-I model.

2. Related work
Recommendation algorithms mainly include recommendations based on collaborative filtering (CF), demographic-based recommendations, and content-based recommendations [7], where collaborative filtering is one of the most widely used recommendation methods at present, and this technology has attracted widespread attention from researchers in different directions. The essence of collaborative filtering is to use the history of other users to assist in recommending the judgment of target users [27]. Collaborative filtering technologies are mainly divided into two types: neighborhood-based collaborative filtering method and model-based collaborative filtering methods [5]. In different collaborative filtering, matrix factorization (MF) is the most popular method of model-based collaborative filtering. Matrix factorization obtains a low-rank user feature matrix and a low-rank product feature matrix through user–product scoring matrix factorization, and the obtained low-rank matrix product is used to fit the original scoring matrix. For unobserved user–item pairs Make predictions. Some improvements have been applied to this method, such as weighted matrix factorization method [31], non-negative matrix-based matrix factorization method [14], and matrix locality-based matrix factorization method [13]. These methods all show the effect on the Recommender System. In the method based on matrix factorization, the important factors are the learned user feature matrix and product feature matrix. Good features can bring better prediction results.

With the popularity of social media, more and more researchers have begun to study the socialized Recommender System that utilizes social relationships between users. Effective use of social network user relationship information can improve the effectiveness of recommendations [29], [30]. Hao Ma proposed an interpretable probability factor analysis model by sharing the same low-rank matrix of users, combining the user–product scoring matrix with the users’ social trust network [16]. Similarly, Hao Ma introduced a socialization-based regularization term into the optimization function of matrix factorization. This regularization term makes the feature factors of friends who have a high degree of similarity with the target user in predicting the target user be paid more attention [17]. The socialization regularization term effectively utilizes the users’ social relationship and improves the recommendation effect.

In recent years, deep learning technologies have been proven to learn good data feature representations in natural language processing, computer vision, and speech recognition [6]. Applying deep learning technology to the Recommender System for feature learning can learn more representative user features and product features. With further study of deep learning, researchers apply deep learning to the Recommender System. Among them, Aaron et al. applied deep learning to music recommendation in 2013. In the recommendation scene for music as an item, deep learning technology was used to detect and express the content of the item, and generated recommendation results by combining with traditional classification technology and recommendation technology [20], which pushed the combination to a new climax.

There are some problems in the personalized Recommendation System, mainly including sparseness problem, cold start problem, and scalability problem. Deep learning can solve cold start and sparseness problems in the personalized recommendation area to a certain extent. By discovering high-level abstract features in the data, inferring the internal connections between items, between users, and between users and items, the researchers alleviated the limitations of Recommender System. Zhang built distributed vectors through neural network with users and items, and used the distributed expression of user vectors and item vectors to train neural networks, thereby improving the system recommendation effect [39]. Liang proposed a self-encoding neural network model for probabilistic scoring for unsupervised feature learning. The users’ implicit features are discovered through the self-encoding network, and implicit vector expressions preferred by users are generated. Combining with the nearest neighbor model collaborative filtering to provide personalization Recommendation [15], Salakhutdinov proposed a collaborative filtering algorithm based on Boltzmann machine, and got better recommendation effect than Netflix system [25]. In order to effectively utilize external information, Wang proposed a hierarchical Bayesian model called Collaborative Deep Learning (CDL), used deep learning to learn content information and combined it with collaborative filtering to obtain a scoring matrix [34]. Similar to the work of Aaron et al. combining the recommendation based on music content with deep learning, the industry uses DBN to extract content features to improve the accuracy of recommendation results, and has made satisfactory progress [8], [33]. The industry has also taken users’ behavior as serialized information or the entire recommendation process as serialized information, and conducted research on learning and detection for serialized data by using RNN [3], [4], [38]. There are also studies using self-coding neural networks to reduce dimensions for the feature matrix, and then achieve the improvement of data filling or recommendation algorithm operation efficiency [21], [28].

Existing work proves that the features learned by Deep Learning in the Recommender System are representative. It is effective to solve the problem of data sparseness in the Recommender System, and it can improve learning performance. In the application, the data of the Recommender System is complex and sparse, and it is very difficult for the collaborative filtering method based on matrix filtering to learn effective feature information. The current research results have shown the effectiveness of deep learning-based recommendation algorithms in practical data applications.

3. Background
3.1. CDL-based recommendation algorithm
CDL-based recommendation is the result to use CDL (Collaborative Deep Learning) for recommendation and improving it. CDL is an algorithm paradigm that combines deep learning algorithm and collaborative filtering algorithm and applies to the Recommender System, improves the performance of Recommender System, and improves recommendation results [35]. Different from previous research, neural network is used as an independent algorithm for content recognition of recommended items for feature extraction; CDL provides a new idea for training content of items and user scoring matrix together.

The improvement of recommendation algorithm based on CDL WAs introduced in this paper. As a case of CDL paradigm, it used stacked noise reduction auto-encoder machine and probability matrix factorization as the active structure of the algorithm, respectively, and could collaboratively learn and calculate the content of recommended items and the scoring matrix, thus giving recommendations.

3.1.1. Stacked denoising auto-encoder
The Stacked Denoising Auto-Encoder (SDAE) is a feedforward neural network that learns from the raw data and output data of superimposed noise and obtains the input data without superimposed noise [18], as shown in Fig. 1.

The features that SDAE derives from input data of superimposed noise and input data without superimposed noise are almost the same, but the features learned from superimposed noise data are more robust. Generally, the implicit layer is constrained as a bottleneck in the middle of the entire system [22], and the input layer 
 is the original data of superimposed noise. An SDAE solves the optimization problem of removing noise, namely: (1) 
 
where,  is regularization parameter, 
 is Frobenius norm, and 
 is the output of  layer network.


Download : Download high-res image (147KB)
Download : Download full-size image
Fig. 1. SDAE schematic diagram.


Download : Download high-res image (189KB)
Download : Download full-size image
Fig. 2. Scheduling of Spark.

SDAE can obtain more robust identity functions and feature expressions, and feature extraction is more robust and has strong generalization performance. In contrast, the SDAE learning process is slow, and faces the problems of gradient explosion and gradient dispersion. Therefore, when a single-layer AE is sufficient to obtain the required features, it tends to be a shallow network structure.

3.1.2. Matrix factorization
The traditional collaborative filtering-based method is to infer similar patterns through users’ history and then recommend related items without using the content information of the items. In contrast, there is a kind of models in collaborative filtering being called implicit factor model, which has achieved great success [23], [24], [32]. Compared with the model using the neighborhood algorithm, it can achieve better performance [1], [9], [11]. In this paper, we mainly focus on matrix factorization related algorithms in implicit factor models.

In the implicit factor model, matrix factorization-based algorithms perform well in the Recommender System. These algorithms are based on matrix factorization and represent users and items into a shared low-rank space. The factorization is done for the scoring matrix through the existing matrix items, and the implicit vector obtained through factorization is used for matrix multiplication operation and can fill in missing data. With continuous learning and adjustment of implicit vectors, more and more accurate missing filled values are obtained.

Before using the matrix factorization model for recommendation, first calculate relevant parameters of the model, that is, the user vector and the item vector in the implicit space. Usually, the existing users’ scoring data is used to form a matrix to perform matrix factorization, and the parameters of each implicit vector are calculated by optimizing Eq. (2). (2) 
 
where, 
, 
, 
 and 
 are regularization parameter respectively.

MF can achieve good predictive performance in the Recommender System, but it is somewhat insufficient [12].

(1) The learned low-dimensional space vectors have poor interpretative performance.

(2) MF is very sensitive to the cold start problem. When a specific user or a specific item lacks historical performance data, MF cannot obtain the necessary startup information and the model cannot work.

(3) Failure to consider the relationship between users, resulting in data loss.

In addition, in the Recommender System, matrix factorization often uses probabilistic models. When the prior experience of experts is introduced into the algorithm, the algorithm model is upgraded to a probabilistic model with states. Generally, the experts’ experience is added into the priori condition as implicit vector. This matrix factorization algorithm with priori probability is called Probabilistic Matrix Factorization (PMF). PMF solves the model by factorizing the existing scoring matrix, adding a prior condition to the implicit vector, and perform necessary constraints on the data, and estimating the maximum posterior distribution of missing values under the existing data condition.

3.2. Distributed platform for parallelization
The distributed memory computing framework proposed by Apache Spark not only retains the scalability, fault tolerance, and compatibility of MapReduce [36], but also makes up for the shortcomings of MapReduce in these applications. Due to the use of memory-based cluster computing, Spark in these applications is about 100 times faster than MapReduce [37].

The distributed system requires a unified scheduling role, which is generally called the manager, and other computing nodes are called workers. In general, during the running of parallel programs, workers calculate relatively independent parallel steps, and the manager uniformly schedules, manages, and counts statistics. This process requires multiple mechanisms to provide protection. The distributed platform is a set of data processing solutions that are organically combined with various systems. Spark can be deployed in a Hadoop cluster environment, use Yarn for resource management, and has the ability to directly access the HDFS file system. Spark runs but is different from the MapReduce intermediate process and the calculation results need to read and write HDFS. Spark saves the calculation results in memory, so that it is not necessary to read and write HDFS frequently, which greatly reduces IO operations, improves algorithm operation efficiency, and greatly reduces algorithm operation time. In addition, Spark is operated and optimized based on the directed acyclic graph DAG, which greatly improves the IO efficiency of the platform, and at the same time controls the data recovery cost after a node abnormality in a distributed environment. The Scheduling of Spark is shown in Fig. 2:

4. Improvement of CDL-based recommendation algorithm
Based on the original CDL, a model CDL-I model with a private bias node of the item is proposed in this paper. It adds a unique bias unit to the input layer of SDAE for each item. Under the sharing of network parameters of the model, add unique bias values for each item, so that the network can learn content parameters of different items more specifically.

The training and learning of CDL-I uses the algorithm that is similar with CDL. It needs to learn  items from the data set. During this process, the auto encoder learns and extracts the content. The obtained content vector is integrated with the bias vector as item information of the Recommender System. Use matrix factorization for scoring prediction, sort the calculated scores, and recommend from high to low scores. In addition, during the model training process, the data is sampled for positive samples, which increases the gap between positive and negative samples, improves model performance, and evaluates the model by recommending Top-N results.

4.1. Problem definition and model improvement
Similar to the use of implicit feedback of training data in Literature [35] to improve the performance of Recommender System, this paper defines a full data set containing  items, which are represented by  dimensional matrix 
. The j line in 
 represents the content vector of the corresponding item, which is represented by  dimension vector 
. For  users, the scoring of  items can be represented by  dimensional scoring matrix 
. When the i user has positive feedback on the  item, let 
; otherwise it is 0.

In the self-encoding neural network, 
, 
 and 
 respectively denote the input with noise, the input of raw data, and the output of the  layer network of auto-encoder. 
 and 
 are​  dimensional matrix, 
 is a 
 dimensional matrix. The  line similar to 
 and 
 is represented as 
. 
 and 
 are the weight matrix and the bias vector of the  layer network respectively. 
 represents the  column of the weight matrix 
. The total number of layers of the neural network is . 
 is short for weight matrix and bias vector.

4.1.1. Probability graphical model of PMF
In this paper, the initialization of PMF model is completed by a series of operations. For Matrix Factorization applied to collaborative filtering, it can be defined as probability graphical model [24] as shown in Fig. 3. The model generation process of PMF is designed by probability graphical model, and Table 1 is obtained by initial sampling of vector parameters using Gaussian distribution. Where, 
 is the standardized parameter of 
, which is used to measure the confidence of the corresponding score. In this paper, the value of 
 is determined by Eq. (12).

When  is 1, the maximum posterior estimation of the model is consistent with Eq. (2). When  is larger, the higher the reliability of the score is. In the actual observation, if the score is 0, there may be two situations. User  is not interested in project  or has not given a score feedback. In the experiment, we use the consistency strategy to determine whether the score is 0. We describe as making the value of  satisfy Eq. (12), in which , where  and  are super parameters, and adjust the model to predict the value. After building the PMF model, this paper uses the coordinate descent method to learn the model parameters, which can be used to fit the model after the model converges.


Download : Download high-res image (43KB)
Download : Download full-size image
Fig. 3. Probability graphical model of PMF.


Table 1. Definition of PMF model.

Probabilistic matrix factorization
(1) For each user , initialize the implicit user vector as 
(2) For each item , initialize the implicit item vector as 
(3) For the item score of each user, initialize the feedback value as 
4.1.2. Probability graphical model of CDL-I
Similar to literature [35], this paper uses SDAE as an integral part of the algorithm and adds private item nodes to the model. The process of model construction can be described as follows:

(1) Operate (1), (2) and (3) for each layer of SDAE

(1) The weight matrix of the current layer is recorded as 
, and each array of the weight matrix is sampled to make it obey the distribution shown in Eq. (3). (3)
(2) For the deviation vector of layer , sample and make it obey the distribution described in Eq. (4) (4)
(3) For data  of each row in data matrix  of layer , sample according to Eq. (5). (5)
(2) For each piece of input data, perform initialization operation. The operation process shall be completed in accordance with (1), (2) and (3) respectively.

(1) Initialize the model input to obey the distribution Eq. (6). (6)
(2) Initialize the bias node of item to comply with Eq. (7). The weights of private node are unique to the item and are not shared in the network. (7)
(3) Create the implicit bias vector of the item according to Eq. (8), and the implicit vector of the item is obtained from Eq. (9). (8)
(9)
 (3) Create an implicit vector for the user, and initialize each user  according to Eq. (10). (10)
(4) Create the initial value of scoring matrix, score each user–item, and sample according to Eq. (11). (11)
Here, 
, 
, 
, 
, 
 is the super parameter of the model, and 
 is the confidence of each user–item scoring matrix in the scoring matrix. Similar to [32], in this paper,  is assigned according to Eq. (12). (12)
 As a channel to contact user scoring and content information, 
 can learn the efficient expression of implicit relationship between items with the use of 
. Take efficiency into account, set 
 to infinity. The Probability Graphical Model of CDL-I is shown in Fig. 4.

4.2. Description of the CDL-I algorithm
After constructing the model according to Fig. 3, Fig. 4, the authors obtained an initialization model that adds the prior probability set according to experience. In this paper, the training algorithm consistent with CDL was used to learn and optimize each part, and finally the entire model was optimized. After convergence of the model, the learned model was obtained. Results show that CDL-I improves the model of CDL, and the algorithm is solved using the same methods and steps as CDL. In the CDL-I training, when the user and item implicit vectors are given, coordinate gradient descent is used to adjust the weight of PMF and the bias vector corresponding to item ; after the PMF is adjusted, SDAE is optimized according to the PMF results. Then, strip the adjusted item bias vector to get the implicit expression of the item, and get the corresponding output of SDAE, and use the back propagation to adjust the neural network. The objective function of CDL-I in this paper uses the same objective function as CDL.

The solution of the algorithm is to calculate the maximum posterior probability of the probability map model under given data, which is equivalent to calculating the joint maximum likelihood value of the objective function, where the setting parameter 
 is infinite, and the adjustment strategy for PMF model training is as shown in Eqs. (13), (14). (13)
(14)
 where, 
 is the column vector consisting of all the user ’s scoring. 
 is the confidence parameter for each pair of user–item scoring matrices in the scoring matrix.

In the CDL algorithm, the parameter update of SDAE part follows traditional back-propagation method. The weight matrix update rule is shown in Eq. (15), and the bias vector update is shown in Eq. (16). (15)
(16)
 The prediction steps for the algorithm is shown as Eq. (17). After getting approximation, the equation shown in (18) is obtained. (17)
(18)
 Based on the previous theoretical basis, the main steps of CDL-I algorithm can be obtained as follows. The item private nodes are added before model training, the initialization nodes between different items meet the independent and identical distribution conditions, and the node weights are calculated together with the model. The node update rule follows Eq. (15).



Download : Download high-res image (358KB)
Download : Download full-size image
5. CDL-I parallelization on spark
The parallel training of CDL-I adopts the mixed mode of data parallel and model parallel, and divides the model into SDAE and PMF for distributed training under the spark framework. In the process of parallel training model, the machine learning algorithm based on data parallel often needs communication interaction to complete parameter exchange. This function is logically abstracted as a part called parameter server. As a logical concept, parameter server can exist in the form of independent model storage; it can be attached to a training terminal node, or it can be distributed in several calculation nodes after model splitting, or even it can be a node where model and data coexist. The parameter server schedules each node, stores and forwards the model parameters, updates the parameters with formulas, distributes the updated parameter increments to each node, accepts parameter correction for each training node, updates and iterates the model, and starts further training at the new starting point.

In CDL-I, PMF and SDAE are based on data parallel mode respectively to ensure that there is a complete model on the node, but as the whole model, CDL-I exists in the cluster training in a distributed way. In addition, SDAE which is based on the data parallel mode is modularly encapsulated and is sliced on data. Each distributed node maintains a complete network system. After a certain number of iterations or PMF reaching a specific state, the parameters are updated. During the initialization phase of the algorithm, the data is distributed according to its location. At the same time, to avoid the abnormal increase in model training time caused by data skew, the data partition is randomly fragmented and distributed after the data is loaded into memory to ensure that all computing nodes have approximately balanced load. In addition, during the data read-in phase, the data operation characteristic of Spark was used to perform the Shuffle operation within the partition to ensure the randomness of data of the single model copy.

CDL-I parallel training process is described in detail, as shown below. The parallel diagram is shown in Fig. 5. Only the PMF training is performed and the weights are adjusted by using a batch of data. When a batch of data is trained, PMF parameters are constant, and SDAE is still trained and learned using the current data, which reduces the data to memory overhead and reduces the number of IO. Although it has a certain impact on calculation accuracy, it improves the efficiency of model training.

When model training starts, manager first performs a map operation and distributes initialization parameters to each worker node, including model scale configuration, regularization parameters, and random number seeds. After that, each worker reads the data independently, and uses the random number seed to initialize the corresponding parameters. The same random seed ensures that the initial state of the neural network on each worker is consistent. Each worker then uses its own data to shard the training model, SDAE reads the item content information data, and PMF uses user scoring data. The model training process is continuously iterated to ensure that at least each sample in the obtained data segment is calculated once.


Download : Download high-res image (234KB)
Download : Download full-size image
Fig. 5. CDL-I parallel diagram.

Each worker uses a part of the data to perform model training, and then performs a Reduce operation to fuse the calculation results of each node. The reduction operation is defined as that the worker uploads the calculated model parameters to the manager, then manager averages each parameter, and distributes the fusion parameters of the corresponding model to SDAE and PMF respectively. And nodes perform self-renewal. One iteration of the system is completed so far.

Iterate the system to ensure that all workers are fully trained, at least to ensure that all data in the training set are involved in the training of the system model. Finally, a trained model is obtained on the manager. The algorithm flow is shown below.



Download : Download high-res image (541KB)
Download : Download full-size image
In the above Step (4) of the algorithm, the parameter model is fused with the averaging method for selecting network parameters in this paper, that is, carrying out averaging operation for the parameter matrix of each model copy on Master; the result set  of step (5) includes SDAE and PMF and corresponding bias nodes. Due to the highly sparse characteristic of the recommendation data, in response to the erroneous information and invalid calculation caused by the sparse data, the model was sparsely optimized during the parallelization of CDL-I. In the input and output layer of neural network training, sparse operation is performed. Data and corresponding nodes of network is screened through the sparse indicator function, and targeted model training is performed, which not only avoids the error information caused by zero data filling of unknown data, but also reduce the amount of calculations for network training under the condition where the same model with the same convergence is achieved.

6. Experimental evaluation
The cluster used in the experiment has 6 nodes. The task submission configuration is 48 Cores, Driver-memory is 30 GB, Executor-memory is 30 GB, and the data is read by HDFS. Spark version is 2.0.1.

6.1. Data set and preprocessing
Similar to CDL, this paper uses 3 data sets for experiments, namely CiteULike-a, CiteULike-t and Netflix. CiteULike-a and CiteULike-t is from [6], [22], [30], which are separately taken from CiteULike. CiteULike is a document tagging website that allows users to create personalized personal library. When a document is selected by a user, the user’s personal library will contain the summary, title, key words and other information of the document. The data set used in this article is the result of collection and filtering. However, the user data includes deletion, selection and filtering. When a user collects less than three documents in his personal library, it is not included in the data set. CiteULike-a includes 5551 users and 16,980 literature as item recommendation set; CiteULike-t includes 7947 users and 25,975 literature as recommendations set. In the experiment, when user u has collected the literature , set pair () to 1, which is similar to the corresponding score of “like” in the scoring matrix. For the user–item​ scoring pair, CiteULike-a contains 210,537 collection operations. After converting to the scoring matrix, the data sparsity is 0.218%; CiteULike-t contains 142,807 collections, and the corresponding scoring matrix contains only 0.065% of the data. Netflix, from Netflix prize, is the recommended Grand Prix data set. The data set includes users’ ratings for movies. For Netflix, this paper follows the method of Literature [42] to convert user scoring into implicit feedback operation. This paper only considers positive feedback, that is, user–item pairs with scoring of 5 points. After excluding users with less than 3 times of positive evaluation and movies without content information, 407,261 users and 9228 movies are retained. The final dataset contains the scoring of 15,348,808 users and the data sparsity is 0.408%. In order to make the data reach the massive level, this paper replicates the user dimension, expands the data, and then obtains massive data.

6.2. Comparative analysis of CDL-I performance
In the recommendation based on Top-N, this paper uses the Mean Average Precision (MAP) and recall rate (Recall) of the evaluation index optimized for Top-N recommendation. The definition of Mean Average Precision is divided into two parts, namely accuracy and average accuracy. The definition is given by Eqs. (19), (20). After calculating the average accuracy rate, the authors calculate the average value of AP@ for all users to get MAP. (19)
 
(20)
 
 Eq. (11) provides the definition of recall rate in this paper. (21)
 
where, 
 is the recommendation result given in the Top-N recommendation; 
 represents the user’s adoption of the item in the test set, and it refers to the item set where the users give positive feedback in the scoring.  represents an indicator function. It is 1 when the item is adopted in Top-k, otherwise it is 0.

Four models of SDAE structure M1, M2, M3, and M4 are used on the measurement of MAP@300, to be tested comparing with CDL-I. The network activation function of M1 is configured as the implicit layer Identity, the output layer Sigmoid, and the loss function Logistic. The network activation function of M2 is implicit layer Sigmoid, output layer Sigmoid, loss function Logistic. The network activation function of M3 is implicit layer Identity, output layer Identity, loss function Square. The network activation function of M4 is implicit layer Sigmoid, output layer Identity, loss function Square. The comparison results are shown in Table 2.

Experiments show that CDL-I has obvious advantage over SDAE in terms of recommendation accuracy. The main reasons are:


Table 2. SDAE and CDL performance in Top-N recommendations.

Algorithm	CiteULike-a	CiteULike-t	Netflix
CDL-I	0.514	0.453	0.312
M1	0.416	0.373	0.223
M2	0.407	0.279	0.167
M3	0.323	0.191	0.118
M4	0.388	0.225	0.149
(1) The training of CDL-I combines item content information. In the training of discovering the implicit relationship between users and items, more essential and accurate item information has been grasped. Combined with portraits of collaborative filtering for users, model parameters can be adjusted in a targeted manner, thereby improving the accuracy of model recommendation.

(2) CDL-I aims at Top-N for optimization in the optimization of the objective function. After the data is subjected to binary noise reduction processing, the information retained by the data is more conducive for CDL-I to recommend, but not for SDAE to make up the data.

To sum up the above two points, it can be found that CDL-I has obvious advantages in the accuracy of recommendation. It makes full use of the auxiliary information of item content to improve the performance of recommendation.

In addition, CDL-I is composed of SDAE, and the performance of neural network will also have an impact on the final performance of the model. Combined with the previous assumptions, this paper conducts a comparative test on CDL-I model composed of SDAE of different network layers, and the performance results are shown in Fig. 6. Experiments show that the increase of the number of hidden layers has a positive effect on the performance of the model. However, if the number of layers continues to increase, the performance of the model does not improve as expected. It can be concluded that the hidden relationship between users and items in the recommendation system is relatively simple, and the five-layer network can be fully fitted. When the hidden layer reaches two layers, the model’s performance is nearly stable.

The comparison experiment of the recall rate of CDL-I is shown in Fig. 7. The data in the figure are averaged from the experimental data of 10 groups. The experiments show that CDL performs better than SDAE under the same N conditions. CDL-I can approach the convergence value earlier in the changing process of N, which shows that the improvement of CDL can improve the algorithm recommendation performance based on better extraction of item content, and can obtain relatively better recommendation performance in the same situation. Source data experiments include comparison of sampling density of item content vectors, comparison of results for different model structures, and comparison of results under different data sets. To sum up, CDL-I can improve the CDL recommendation results.


Download : Download high-res image (346KB)
Download : Download full-size image
Fig. 6. Performance of CDL-I.

6.3. Experiment and analysis of CDL-I parallelization
6.3.1. Analysis of sequential experimental results
The algorithm of this paper is based on Spark and Scala. Spark has data loading operation, so it can optimize the calculation and storage of massive data. Accordingly, the experimental performance under single-computer computing resource is analyzed in this section. The implementation of pure language linear libraries and the advantages and disadvantages of realizing Spark-based distributed platform optimization in the case of limited computing resources is compared. In the experiment, the processing time of data of different scales and the time overhead of one iteration of the model during training are calculated. The experimental results are shown in Fig. 8. Scala Data represents the time overhead for processing data of different scales using Scala scripts, and Spark Data represents the time overhead for processing data using a single Spark node. Scala Model and Spark Model represent the time overhead of the algorithm implemented by Scala and the Spark parallel algorithm respectively.

Experiments show that the algorithm based on Spark has a slight loss in accuracy, and the performance in the dimensions such as recommendation accuracy and rating prediction has an accuracy loss of 2%–5% as compared to the sequential version. In the error definition of the industry, this range can be accepted as the margin of error. In terms of speed, the optimization of Spark platform shows advantages. With the same amount of data and the same resource overhead, Spark has 1.3 times speed improvement over the sequential algorithm. In terms of model scalability, compared with the memory overflow and calculation time exponential increase of the algorithm of the sequential version, Spark-based algorithms have stable time overhead. In resource application, Spark makes full use of disk operations to greatly expand the scale of data that can be handled at the cost of a certain amount of computing time. Experiments show that under the condition of fixed resources, the algorithm based on Spark has strong improvement in the amount of data that can be processed compared to the single-machine algorithm, which greatly improves the computing power provided by single-machine computing.


Download : Download high-res image (107KB)
Download : Download full-size image
Fig. 8. Calculation time under different data volumes.

6.3.2. Analysis of parallel experiment results
The performance of Spark parallel algorithm and sequential algorithm is compared under the sequential condition in the previous article. Experiments have proved that the performance of Spark-based parallel algorithms has far surpassed the performance of sequential simple algorithm. In order to fully verify the scalability of data scale of parallel algorithm, experiments under Spark cluster conditions are conducted in this paper. At the same time, it is compared with the performance of sequential algorithm, stand-alone Spark algorithm and algorithms under different cluster scale. The experimental results are shown in Fig. 9.

Sampling or copying from the data in the experiment ensures that the data scale is controlled, and that different cluster scales are tested under the same data conditions during the experiment. It can be seen that when the calculation amount of the algorithm is huge, multi-node computing has obvious performance advantages over single-node computing, and different nodes have a certain proportional relationship under the same data scale. As the amount of data continues to increase, it can be seen that when the amount of data reaches 300,000, the time overhead of a stand-alone system has increased by leaps and bounds, and as the amount of data continues to increase, the system time overhead shows an uncontrollable rapid increase. When the data scale reaches the single-node processing limit, the system time overhead becomes immeasurable.


Download : Download high-res image (127KB)
Download : Download full-size image
Fig. 9. Performance comparison of parallel algorithms.

Compared with the sequential algorithm, firstly, the upper limit of the data amount that the distributed algorithm can process is much larger than that of the sequential algorithm. Secondly, the time overhead of the distributed algorithm is significantly better than that of the single-node algorithm. As the number of nodes increases, the time overhead advantage gradually becomes prominent. Finally, it is concluded from the comparison of figure line that during the gradual increase in the amount of data, the system time overhead steadily increases with the increase in the amount of data, and has a certain linear trend. Compared with the explosive growth of time in Scala model, it has obvious advantages.

7. Conclusion
When the traditional collaborative filtering algorithm is faced with new entries, the performance of the algorithm drops sharply, especially there is no historical data or historical data is scarce, and the amount of information is insufficient. In addition, the traditional recommendation algorithm in dealing with massive data is very difficult, combined with deep learning model, the calculation is much more than before. Through in-depth study of collaborative deep learning and its parallelization, this paper propose CDL-I model by improving SDAE on the basis of CDL. By adding private network nodes to the item and adding private bias nodes to each item in the case of network parameter sharing, the network can learn item content parameters more pertinently and it improves the detection performance of the model in the recommendation system. In addition, in order to deal with the increasing scale of data, we split the CDL-I model and propose a parallel training method. The parameters of each part of the model are trained and optimized in parallel, which could enhance the data scale that the model can handle. The future research will focus on improving the training efficiency of the model.