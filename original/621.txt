Abstract
Mobile devices (MDs) and applications are receiving extensive popularity and attracting significant attention. Mobile applications, especially for artificial intelligence (AI) applications, require powerful computation-intensive resources. Hence, running all the AI applications on a single MD introduces high energy consumption and application delay, as it has limited battery capacity and computation resources. Fortunately, the emerging edge-cloud computing (ECC) architecture pushes the computation resource to both the network edge and remote cloud to cope with challenging AI applications. Although the advantage of ECC greatly benefits various mobile applications, data security remains an important open issue in this scenario, which has not been well studied. This paper focuses on the profit maximization (PM) problem for security-aware task offloading in an ECC environment, i.e., considering the tasks from MDs with different service demands, edge nodes should decide them to be processed on the edge node or the remote cloud with a security guarantee. Specifically, we first construct the security model to measure the time overhead for each task under various scenarios. We then formulate the PM problem by jointly considering the security demand and deadline constraints of tasks. Finally, we propose a genetic algorithm-based PM (GA-PM) algorithm, the coding strategy of which considers the task execution location and execution order. Moreover, the crossover and mutation operations are implemented based on the coding strategy. Extensive simulation experiments with various parameters varying demonstrate that our GA-PM can achieve better performance than all the comparison algorithms.

Keywords
Edge-cloud computing (ECC)
Task offloading
Security constraint
Profit maximization (PM)
Genetic algorithm (GA)

1. Introduction
The Internet and wireless communication techniques greatly benefit people from all over the world to communicate with each other conveniently. Recently, the emerging 5G cellular network provides higher network bandwidth, and mobile devices (MDs) and applications are growing rapidly and receiving extensive popularity [17]. Although MDs are small, lightweight, and portable, they can be equipped with powerful and high-performance computation capability, bringing great convenience to people's daily life [14]. Attributed to the high-performance MDs, various mobile services and applications are emerging. Especially with the advance of artificial intelligence (AI) technique, more and more AI applications are developed, e.g., Internet of Medical Things (IoMT) [31], deep neural network (DNN) computing [5], emotion recognition [12], and so on. However, such AI applications are typical computation-intensive applications; running all of them on a single MD will incur unacceptable execution time and high energy consumption.

Mobile edge computing (MEC) pushes various resources, e.g., computation, storage, and network, to the network edge [26], [16], [7]. Thus, MDs can offload their computation-intensive AI applications to the proximal edge nodes, reducing battery consumption and task completion time [38], [34], [21], [23]. Nevertheless, based on the statistics from Cisco, the number of worldwide MDs is about 11.6 billion by 2021 [33]. Moreover, in the real-world scenario, an edge node is only deployed with small-scale physical servers but will serve a large number of MDs simultaneously. Therefore, the MEC edge node will suffer from the insufficient resource problem, which is a challenge to process computation-intensive AI applications. Fortunately, the emerging edge-cloud computing (ECC) architecture also allows the edge node to offload computation-intensive tasks to the remote cloud by backhaul network [5], [2], [9]. In this case, the edge node and remote cloud jointly cope with challenging AI applications for resource-limited MDs.

However, offloading tasks from the edge node to the remote cloud should concern the distance. This is because the longer distance produces longer data transmission time, causing longer delays [15]. Moreover, running mobile applications on the nearby edge node or the remote cloud can easily suffer from security and privacy problems [37]. For example, offloading a neural network task to the remote cloud may cause malicious data privacy attacks. Both the data and the application itself are the sources of security threats and may be attacked by malicious cloud users or external attackers. Like the cloud environment, the edge node also experiences serious security problems [29], [28]. A report from 3GPP says that both MDs and edge nodes will be attacked maliciously from the perspectives of physical, protocol, privacy, and data in the wireless network environment [10]. Network attackers adopt various malicious ways to attack the privacy data of mobile users. Based on the above descriptions, guaranteeing data security is crucial for mobile users, as data is also a kind of important intellectual property. Although many research efforts have been made for bridging data and network security in cloud computing [39], [18], [6] and MEC [19], [13], it is also necessary to deploy essential security protection mechanisms for the ECC environment.

Similar to the existing works, we also plan to deploy confidentiality and integrity services to form a joint security protection mechanism to cope with a variety of threats and attacks (such as alteration and spoofing) in the ECC environment. Using security services is a simple but very useful way to protect mobile users' data. However, applying security services will incur additional time overhead, i.e., the execution time of security services. For instance, to protect the data from alteration attacks, the cryptographic algorithm of confidentiality service will be used, and the execution process of data encryption introduces an amount of time overhead.

This paper proposes a profit maximization (PM) algorithm from the edge node perspective in the ECC environment. The edge node needs to offload the tasks received from MDs to the remote cloud when its task queue is congested because of resource limitation. In this case, the edge node applies an optimal task offloading strategy to decide how many tasks should be executed on the edge node and how many tasks should be offloaded to the remote cloud. Before offloading tasks to the cloud, the appropriate security services should be performed due to the security issue, which incurs a certain time overhead. Additionally, the completion time of mobile AI applications is another significant concern for mobile users. Thus, our objective is to maximize the profit with the constraints of security and deadline. The major contributions are listed below.

•
The security model, including the execution time on the number of CPU cores and data size, security level, and security time, is generated to measure tasks with different data sizes, security demands, and computation capacities. We then formulate the PM problem by jointly considering the security demand and deadline constraints.

•
We propose a genetic algorithm-based PM (GA-PM) algorithm and devise a coding strategy that considers the task execution location and execution order. Moreover, the corresponding crossover and mutation operations are implemented based on the coding strategy. According to the analysis, the time complexity of the GA-PM algorithm is polynomial.

•
Extensive simulation experiments with various parameters varying (such as task arrival rate, number of servers, data size, and security demand) demonstrate that our proposed GA-PM achieves better total profit than the comparison algorithms.

This paper is organized as follows: Section 2 summarizes the existing work about task offloading, security, and profit optimization. Section 3 introduces the ECC architecture with security services, presents the security model, and formulates the PM problem. Section 4 elaborates on the GA-PM algorithm design, including encoding, initial population, crossover, mutation, population selection, and the time complexity analysis. Section 5 presents the experimental results and discussion. Finally, Section 6 concludes this paper and gives our future research.

2. Related work
With the development of wireless communication techniques, MEC is becoming a promising computing paradigm. Scholars and researchers focus on various challenges [26], [16], [24]. For task offloading in the MEC, it has been extensively researched on energy and delay optimization. For example, Dinh et al. [8] propose a computational offloading framework to minimize the joint cost of energy and delay. Yan et al. [38] research the task offloading and resource allocation by considering both the energy consumption and execution time. Chen et al. [4] investigate the task offloading problem in the multi-eNB environment to minimize latency and energy consumption. Li et al. [21] investigate the multi-user offloading problem and propose a Lyapunov-based optimization algorithm to achieve optimal bandwidth allocation. Lu et al. [23] propose a DRL-based lightweight task offloading strategy for large-scale heterogeneous MEC, which integrates the LSTM network into a deep Q-network to improve learning performance. Li et al. [20] research the service provision problem of task offloading in the UAV-based MEC environment, which is addressed by DRL to maximize the system throughput. Li et al. [22] introduce an opportunistic computation offloading method, called OPPOCO, to optimize the tradeoff between cloudlet-assisted computation service (CCS) mode and remote cloud (RC) model. However, the above works concentrate on the energy and delay problem but do not take the data security issue into consideration.

Using security services to protect privacy data has been extensively adopted in cloud computing and MEC. To address the security problem of sensitive data in the cloud, Zeng et al. [39] introduce a security-aware workflow scheduling strategy, called SABA, to minimize the completion time with the budget constraint. Li et al. [18] develop a security-aware workflow scheduling on achieving optimal service selection in multi-dimensional security space. Chen et al. [6] bring security service and task duplication to form a security scheduling framework for optimizing makespan and cost. Wang et al. [35] integrate the VM resource availability and security services to guarantee the security and efficiency of workflow execution. Many studies also focus on the security problem in the MEC. Huang et al. [13] first present a security and energy-efficient scheduling scheme for service workflows, by genetic algorithm, to minimize energy consumption. Then, a security-aware computation offloading algorithm is proposed to optimize the joint cost of energy consumption, latency, and task dropping. Li et al. [19] introduce a security and performance-aware resource allocation for enterprise multimedia data. The architecture of enterprise multimedia security and two heuristic algorithms are devised to minimize energy consumption with security guarantees. Alabdulatif et al. [1] propose a privacy-preserving architecture for extensive data analysis in the cloud, where the fully homomorphic encryption (FHE) is deployed to eliminate several privacy and security concerns. All the related works discussed above seem to optimize the cost, time, and energy consumption from cloud users' and mobile users' perspectives. However, in the ECC environment, the edge node pays attention to the profit instead of the above factors.

Additionally, to assure normal operation, it is essential to gain profit. Hence, how to increase profit is a critical issue for the edge node, which has been studied from various perspectives in many works. For example, to decrease the end-to-end (E2E) delay, Sun and Ansari [30] develop a PM strategy by reasonably placing user's Avatars to maximize the joint gain of E2E delay and migration cost. Wang et al. [36] design a PM algorithm with an incentive mechanism for edge-cloud architecture implemented by a multi-round auction mechanism. The purpose is to motivate edge-cloud to provision mobile services with a market-based pricing model. Mei et al. [27] design an intermediate role, known as a cloud broker, for cloud users and cloud providers. The function of a cloud broker is to buy computation resources from cloud providers with low prices and sell them to cloud users at high prices, thus, maximizing the profit. To encourage MDs to participate in collaborative resource sharing with idle computation resources, Sun et al. [32] present an incentive mechanism with a joint pricing mechanism and resource allocation strategy to maximize task profits publishers in the device-to-device communication. Hao et al. [9] introduce a video caching and processing approach to meet mobile users' video traffic requirements and provide maximum profit for video service providers. Chen et al. [3] put forward a PM strategy for task offloading in the ECC environment. Four sub-methods, i.e., clustering, classification, profit formulation, and scheduling strategy, are devised to form integration to achieve profit optimization. Mahmud et al. [25] propose a profit-aware placement policy for latency-sensitive IoT (Internet of Things) applications in the fog-cloud environment, where the integer linear programming (ILP) method is used to improve profit and QoS. Although the above approaches focus on the PM problem from various perspectives and computing environments, they do not consider the security problem, which is very important. Hence, this paper proposes a GA-PM algorithm for offloading edge node tasks to the remote cloud with security and execution time constraints in the ECC environment.

3. Architecture, model, and problem
We first present the system architecture of ECC with security services. Then, the security model is introduced. Finally, we formulate our PM problem. The major notations are given in Table 1.


Table 1. Major notations.

Symbols	Semantics
The number of tasks at time slot τ
ti	The i-th task
Wi	The workload of task ti
Di	The data size of task ti
The deadline constraint of task ti
The start time of task ti
The end time of task ti
The transmission time of task ti
The execution time of task ti
STi	The security time of task ti
sdi	The security demand of task ti
xi	The execution location of task ti
yi	The execution order of task ti
ηi	The profit of task ti
λ	The task arrival rate
Tslot	The duration of each time slot
3.1. System architecture
The system architecture of task offloading with security services is shown in Fig. 1, consisting of three components: mobile devices, edge nodes, and remote cloud. Every mobile user carries MDs to enjoy mobile services. Nowadays, mobile users prefer to use various AI applications on MDs. Although the up-to-date MDs are equipped with powerful computation capacity, the limited resource is still a great challenge to process all the computation-intensive tasks. Hence, MDs would like to submit a part of or all the tasks to the edge node by 4G/5G cellular network to improve the quality of service (QoS). Edge node provisions edge-server-oriented service with low delay and battery consumption. This is because the edge node lies in proximity to end devices and has much more computation resources than MDs. Nevertheless, if the edge node is congested by too many task requests, it can also offload some tasks to the remote cloud. The remote cloud is far away from the edge node and mobile users, but it can offer unlimited computation resources and cope with challenging AI applications.

Fig. 1
Download : Download high-res image (374KB)
Download : Download full-size image
Fig. 1. The system architecture of task offloading in the ECC environment.

At the beginning of each time slot τ, MDs submit AI tasks to the edge node with execution time constraints, where the length of each time slot is denoted by 
. Thus, the number of tasks arrived at the edge node is denoted by 
. The task arrival process  follows the Poisson distribution with parameter . Here, let  denote the index set and 
 represent the task set at time slot τ. Note that the MDs in the ECC environment have different preferences, i.e., the submitted tasks are heterogeneous with different workload 
, the size of input data 
, deadline constraint 
, and profit 
. When λ is small, the edge node can process all the tasks by itself under the computation capacity 
 and obtain all the tasks' profits. However, when λ is too large, the edge node will send a part of tasks to the remote cloud due to insufficient computation resources. Cloud computing is a kind of commercial paradigm. Executing tasks on the cloud should pay for corresponding prices. We assume that the price of a unit resource in the remote cloud is p. In this case, the profit gained by the edge node is less than 
 if task 
 is offloaded to the remote cloud. Hence, the task queue of the edge node is divided into two parts: the offloading queue and the local queue. Thus, at the beginning of time slot τ, the edge node will use the GA-PM algorithm to offload tasks to obtain maximum profit.

3.2. Security model
The data is the critical intellectual property, and it maybe suffers from malicious threats, e.g., alteration and snooping in the ECC environment. Generally, confidentiality and integrity services are applied for coping with security threats [18], [6]. Specifically, the confidentiality service, consisting of IDEA, DES, AES, Blowfish, and RC4, is used to implement data encryption. In contrast, the integrity service containing Tiger, RipeMD160, SHA1, RipeMD128, and MD5, is applied to check data integrity [13].

To build our security model, we run all the security services on the Dell R530 server (2.2 GHz 8-core CPU). Table 2 and Table 3 give the corresponding execution time for confidentiality and integrity services, respectively. Note that they are collected by running these security services on a single CPU, and the size of protected data is 1 MB [13]. Based on Table 2 and Table 3, we measure the security level for each algorithm or function.


Table 2. The confidentiality services.

k	Algorithms	
: Security time (s)	
: Security level
1	IDEA	0.85	1.0
2	DES	0.72	0.85
3	Blowfish	0.48	0.56
4	AES	0.45	0.53
5	RC4	0.27	0.32

Table 3. The integrity services.

k	Functions	
: Security time (s)	
: Security level
1	Tiger	0.13	1.0
2	RipeMD160	0.10	0.77
3	SHA-1	0.09	0.69
4	RipeMD128	0.08	0.62
5	MD5	0.06	0.46
For simplicity, we use the abbreviations cd and ig to stand for confidentiality and integrity services, respectively. For the confidentiality service, we normalize the security level range in []. The security levels are measured on the execution time. For example, we identify that the slowest algorithm has the highest security, and hence we allocate 1.0 to IDEA as its security level. Then, the security level of the other confidentiality services is calculated as below.(1)
 where 
 is the service time of the k-th encryption algorithm, where . Similarly, according to Table 3, we give Tiger the level 1.0, and the levels of the other confidentiality services are calculated by(2)
 where 
 is the service time of the k-th hash function.

Next, we study the impact of the CPU size 
 on security time overhead, where the data size is set to 1 MB. For adding security services to the private data with a multi-core CPU, we divide the task data into multiple equally sized data blocks, and the number of blocks is equal to that of CPU cores. For example, if we use the dual-core CPU to encrypt 1 MB of data, the data is divided into two equal parts, i.e., each data block is 0.5 MB. Thus, the dual-core CPU can be completely exploited. The corresponding results are shown in Fig. 2, wherewith the increase of 
, the service time of all the security services decreases. Furthermore, the service time is inversely proportional to the 
, i.e., if the value of 
 is doubled, the security time is halved.

Fig. 2
Download : Download high-res image (331KB)
Download : Download full-size image
Fig. 2. The security time overhead under the different number of CPU cores.

Fig. 3 shows the results of time overhead on different data sizes, i.e., varying D from 1 to 10 MB with an increment of 1 MB, where the experiment is conducted on a single CPU. It can be observed that the time overhead of all the security services grows linearly with D. That means the service time is in direct proportion to the data size, i.e., the time required is double when the size of data is double. Then, we can reason out the following Eq. (3), i.e.,(3)
 where 
 is the security time on a single CPU with 1 MB data size, and C is the computation capacity of a multi-core CPU. 
 can also be regarded as the baseline time overhead and can be easily found in Table 2 and Table 3. According to Eq. (3), we can calculate all the time overhead for different tasks under various computation conditions.

Fig. 3
Download : Download high-res image (348KB)
Download : Download full-size image
Fig. 3. The security time overhead under different data sizes.

In the ECC environment, both the edge and cloud are the attack targets for malicious users. Suppose the attack times follow the Poisson distribution, and then the risk probability for a task under the security level 
 is modeled as an exponential distribution [18], i.e.,(4)
 where 
 represent the security demand, and 
 is the risk coefficient. Parameter 
 represent the number of attacks per unit time, and the edge node and remote cloud may have different risk coefficients. Thus, the joint risk probability by considering two security services is represented by(5) 
 

For guaranteeing the security of tasks submitted by MDs, their security demands should satisfy the following condition.(6)
 Then, the required security level is just larger than or equal to 
, that is(7)

In this case, 
 and , indicating that using the security service with level 
 can guarantee data security.

3.3. Problem formulation
We formulate the PM problem for security-aware task offloading. The aim of our GA-PM is to maximize the total profit of edge nodes at every time slot τ while guaranteeing the security and meeting the completion time constraint of each task.

Each task 
 can be processed by the edge node or offloaded to the remote cloud. Let 
 be called the execution location of task 
, i.e.,(8)
  where 
 denotes task 
 is executed on the edge node, and 
 represents task 
 is offloaded to and executed on the remote cloud. Thus, let vector 
 denote the tasks' execution locations at time slot τ.

Apart from the execution location, the tasks' execution order is another important concern. Due to the deadline constraints, different execution orders may cause different profits. For example, we assume there are two tasks, 
 and 
, with the same workload 
, but with different deadlines, i.e., 
 s and 
 s. Suppose these two tasks are executed on the edge node, and the computation capacity of the edge node is 
 GHz. Thus, the processing time is 1 s for tasks 
 and 
. Case 1: If the execution order is 
, we find that all the tasks can be finished within the deadlines, gaining total profit 
. Case 2: However, if the execution order is 
, task 
 cannot be completed within the deadline. Only profit 
 is obtained by the edge node. Obviously, 
. This simple example indicates that the execution order has a direct impact on the edge node's profit. We use the vector 
 to denote the tasks' execution order at time slot τ, and a constraint condition should be met that 
, i.e., any two tasks cannot have the same order.

Next, we analyze the end time of each task. If task 
 is executed on the edge node, i.e., 
, its execution time is 
. Then, its end time is 
, where 
 is the start time of task 
. Note that 
 is calculated according to the execution order , and the 
 of 
 is the end time of task 
, i.e., 
. Since running GA-PM algorithm also consume a certain amount of time, the start time of the first task in  is 
 instead of 
 (), where 
 is the actual algorithm running time. The detailed running time analysis can be found in Section 5.6. Then, the profit of completing this task 
 is expressed by(9)
  This indicates that if a task cannot be finished before the deadline, the edge node receives zero profit.

If a task is scheduled to the remote cloud, security services should be conducted to guarantee data security before offloading it. So, four steps will be experienced by task 
, which are adding security services, transmitting data, stripping security services, and executing tasks. The above process is shown in Fig. 4. Specifically, the responsibility of the edge node is to add security services to tasks and transmit them to the remote cloud. In contrast, the cloud is responsible for stripping security services and executing tasks. Then, the end time of task 
 consists of five components, which is given by(10)
 where 
 is the security time consumed by the edge node, 
 is the transmission time (B is the bandwidth between the edge and cloud), 
 is the security time consumed by the remote cloud, and 
 is the execution time of task 
 on the remote cloud. Unlike the edge node with the fixed computation resource, the remote cloud provides unlimited computation resources for its external users, i.e., the edge node can apply for the any number of resources. So, for a task 
, 
 and 
 are the fixed values. In order to ensure 
, the following condition should be satisfied.(11)
 Let 
 be the minimum cloud resource required by the edge node for task 
, which can just meet the deadline constraint. In this case, 
, and 
. Combined with Eq. (11), we can calculate the minimum 
 as follows:(12)
 

Fig. 4
Download : Download high-res image (110KB)
Download : Download full-size image
Fig. 4. The task execution process by offloading a task from the edge node to the remote cloud.

Thus, the cost pays for the cloud is 
, where 
. Then, the profit of completing task 
 on the remote cloud is represented by(13)
  Note that 
 indicates that completing task 
 takes too much cost rather than obtains profit. So, in this case, the edge node prefers to drop this task instead of finishing it.

Based on the above analysis, the total profit of the edge node at time slot τ is the sum of all the tasks' profits Ψ. The objective of this paper is to maximize the above total profit Ψ at each time slot τ and meet the corresponding constraints, which are described as follows:(14a)
 
(14b)
(14c)
(14d)
(14e)
 where Eq. (14b) is the execution location constraint, Eqs. (14c) and (14d) are the execution order constraint, and Eq. (14e) is the deadline constraint for each task 
 at time slot τ.

4. Algorithm design of GA-PM
Finding an optimal solution for our PM problem with an acceptable time is highly challenging, as it is an NP-hard problem. Therefore, we develop an approximate algorithm with low time complexity. This paper devises a GA-based PM algorithm to achieve the optimal objective. GA is a very famous evolutionary computational technique developed by J. Holland [11] and has been extensively researched and applied in various areas. It is a random search method that evolves from the natural evolution process and uses a number of chromosomes to search the problem space. Typically, GA comprises of the following steps: encoding, population initialization, crossover, mutation, and population selection. Next, combined with the PM problem, we describe the above steps in detail to form our GA-PM algorithm.

4.1. Encoding
We have already presented in Section 3.3 that the total profit of the edge node is highly correlated with the tasks' execution location and execution order. Then, we devise our encoding strategy as follows:(15a)(15b) where profit  can be regarded as the fitness function related to  and . For each chromosome in GA, its encoding represents all the tasks' execution position and order, shown in Fig. 5, where suppose . In this case, a chromosome denotes the vector (). Based on this vector and Eq. (14a), we can compute the total profit and evaluate this chromosome. However, the search space of execution location and execution order is different. So, the crossover and mutation operations should be processed separately.

Fig. 5
Download : Download high-res image (22KB)
Download : Download full-size image
Fig. 5. The coding strategy.

4.2. Population initialization
To speed up the search process for the optimal solution, we apply random initialization to initialize each population individual (i.e., chromosome). We first introduce the random initialization procedure of tasks execution location, and its pseudocode is given in Algorithm 1. To initialize the location of each task 
, we generate a number of ϵ in the range [] randomly. If this , then we set location 
; otherwise, 
 is set to 1.

Algorithm 1
Download : Download high-res image (23KB)
Download : Download full-size image
Algorithm 1. Random initialization of execution location.

The pseudocode of execution order initialization is shown in Algorithm 2. First, we copy the index set  as S. For each task 
, a random integer ϵ is generated in the range []. Then, the execution order 
 of this task 
 is set to 
, where 
 is the ϵ-th element in current set S. Thus, if 
 has been used, it will be removed from the set S. This initialization process can ensure any two task orders are different.

Algorithm 2
Download : Download high-res image (31KB)
Download : Download full-size image
Algorithm 2. Random initialization of execution order.

4.3. Crossover
Crossover operation is one of the most critical genetic operators, and the aim is to integrate two current chromosomes and form two new chromosomes. The role of crossover is to produce descendants, maybe with better fitness values. In this way, the individuals in the population can explore the unknown solution space. According to the coding strategy, we perform the crossover operation for execution location  and order , respectively, where we use the classical single-point crossover.

The order of task execution should follow the rule that 
. Hence, the crossover operation should not violate this restriction, which is shown in Fig. 7. Let 
 and 
 denote two order vectors. Similarly, first, a cut-off point is generated randomly, which split each vector into two parts. By swapping the substring, two temporal individuals 
 and 
 are obtained, i.e., 
, and 
. Note that this process is different from that of crossover for execution location. Next, each temporal individual is scanned from the beginning to the end, and we remove the duplicate numbers that have been already appeared. This operation will not cause any order number conflict. The corresponding pseudocode of this crossover is shown in Algorithm 4.

Fig. 7
Download : Download high-res image (128KB)
Download : Download full-size image
Fig. 7. The crossover operation of execution order.

Algorithm 4
Download : Download high-res image (49KB)
Download : Download full-size image
Algorithm 4. Crossover for execution order.

4.4. Mutation
The mutation is another fundamental operation of GA, which is crucial to improve the quality of the population. By slightly changing chromosomes, the mutation operation can enhance the fitness values and prevent early algorithm convergence. We also devise our mutation operation for execution location  and order , respectively.

The mutation operation of execution location is shown in Fig. 8. Let X denote a location vector. We generate an integer number ϵ in the range [], where ϵ is the mutation point. In our problem, we set 
, i.e., if original 
, then it is changed to 1; otherwise, 
 is set to 0, producing a new location vector 
. This process is too simple that there is no need to describe the algorithm procedure.

Fig. 8
Download : Download high-res image (37KB)
Download : Download full-size image
Fig. 8. The mutation operation of execution location.

Like crossover of execution order, mutation operation should not break the order constraint either. The mutation for task execution order is shown in Fig. 9. Let Y denote an ordered vector. We first generate an integer number 
 in the range [], where 
 is the mutation point. To maintain the order constraint, we randomly select another integer 
 in [], where 
. After that, we swap 
 and 
, yielding a new execution order 
. Due to its simplicity, we do not need to give the pseudocode of this mutation process.

Fig. 9
Download : Download high-res image (60KB)
Download : Download full-size image
Fig. 9. The mutation operation of execution order.

4.5. Population selection
Let m denote the population size and M represent the maximum iteration number of GA. According to the crossover and mutation operations described above, the original m individuals reproduce another m new individuals. In the population selection stage, we choose m dominated individuals from  individuals, the function of which is to guide the search towards the optimal direction. The selection criterion is the total profit of edge node, i.e., individual a is said to dominate individual b, if and only if 
. Then, GA continues performing crossover and mutation based on the selected m dominated individuals and repeats this iteration until the times are up to M. The outcome of GA-PM algorithm is the final population with the optimal or maximum solution.

4.6. Complexity analysis
Algorithm 5 shows the pseudocode of the GA-PM algorithm, where we successfully combine the PM problem with GA. The time complexity for the initializing population is , where . The time complexity of crossover for execution location and order is , while the time complexity for mutation is . The time complexity of population selection is also  that each individual should be evaluated. Above all, the overall time complexity of our GA-PM algorithm is .

Algorithm 5
Download : Download high-res image (30KB)
Download : Download full-size image
Algorithm 5. GA-PM algorithm.

5. Experimental simulation
This section is to validate the performance of the proposed GA-PM algorithm. We first describe the experimental parameters used in the simulation program. Then, five groups of experiment analysis are given in detail, which are the impacts of task arrival rate λ, the number of servers N, data size D, deadline constraint 
, and security demand sd, and the running time experiment of GA-PM algorithm, respectively.

5.1. Parameters setting
Let 
 represent the number of time slots, i.e., the edge node will receive 
 task batches from MDs. The length of each time slot is set to 
 seconds. Suppose the edge node has the number of  physical services, and each server has an eight-core CPU with a maximum frequency 2.2 GHz. The remote cloud provisions the computation resource with a price 
, which is based on the Google CPU instance [40]. The bandwidth between the edge and cloud is  Gbps, i.e., the data transmission rate is about . We also assume edge and cloud have the same security demands that 
, and risk coefficient 
, which are only for the purpose of the experiments. In the GA iteration process, we define individual size , maximum iteration number . Moreover, the mutation probability and crossover probability are set to 10% and 50%, respectively.

At the beginning of each time slot τ, the tasks arrived at the edge node follow the Poisson distribution with parameter . Due to the heterogeneous MDs, the data size, workload, deadline constraint, and profit of each task obey the uniform distribution in , , , and 
$, respectively. Note that the deadline 
 of each task is less than or equal to 
, which means every task should be processed in a one-time slot τ.

The performance metrics evaluated in our experiments are as follows:

•
Total profit (TP): The total profit of the edge node, including the profits gaining from tasks execution on the edge node and the remote cloud.

•
Profit on edge (PoE): It only records the profit owned by executing the tasks on the edge node.

•
Profit on cloud (PoC): It consists of the profit produced by executing the tasks on the remote cloud.

Note that TP is the sum of PoE and PoC. The following comparison algorithms are also simulated.

•
No Security (NoSec): The tasks offloaded to the cloud do not add any security service and hence will suffer from the risk with the probability calculated by Eq. (5). If malicious users attack a task, the edge node gains no profit even if this task has been successfully completed.

•
Edge execution (EdgeEx): All the arrived tasks are executed on the edge node. In this case, security services are not required.

•
Cloud execution (CloudEx): All the tasks arrived at the edge node are first offloaded to the remote cloud with security services and then processed by cloud servers.

All the comparison algorithms are implemented based on GA, where we set security demand 
, execution location 
, and execution location 
 for NoSec, EdgeEx, and CloudEx, respectively.

5.2. Impact of task arrival rate λ
The simulation results on three performance metrics, such as TP, PoE, and PoC, by varying the λ from 20 to 200 with an increment of 20 are shown in Fig. 10, where we keep the other parameters fixed, for example, , , and . Note that the larger λ means the more tasks will arrive at the edge node at each time slot.

Fig. 10
Download : Download high-res image (374KB)
Download : Download full-size image
Fig. 10. The performance impact of task arrival rate λ.

It can be observed from Fig. 10a that the TPs of all the algorithms increase with the arrival rate λ. This is because with more tasks arrived, the edge node and remote cloud will process more tasks, incurring higher profit. Moreover, we can see that GA-PM obtains better TP than the other comparison algorithms, as our proposed algorithm considers the tasks execution location and order to maximize the total profit. EdgeEx has the second-highest profit when . When the arrival rate λ is a small value, GA-PM performs slightly better than EdgeEx. This is because the small λ generates few tasks, and hence, the edge node almost can process all the arrived tasks. However, with larger λ and hence more tasks arrived, the edge node with limited computing capacity cannot execute all the tasks. For example, when , the increase of EdgeEx's TP is not obvious as the workload is overloaded for the edge node in this case. Although NoSec applies the same method to schedule tasks, it behaves less TP than GA-PM. The rationale is that without the security protection mechanism, the tasks executed on the remote cloud may be failed due to malicious attacks, causing profit loss. CloudEx has the least TP. This is because CloudEx offloads all the tasks to the remote cloud, which requires extra security service time and data transmission time, wasting the task execution time and hence reducing the TP.

Fig. 10b represents the PoEs of all the algorithms. Similarly, as λ increases, the PoEs of all the algorithms except CloudEx increase. We find that EdgeEx has the largest PoE in many cases. However, when , the PoE of EdgeEx increases slowly. This is due to the fact that 1) EdgeEx processes all the tasks on the edge node; 2) the computing resource of the edge node is insufficient under the large λ. NoSec has the second-highest PoE and performs better than our GA-PM. This is because, without any security protection, NoSec tries to execute more tasks on the edge node instead of offloading them to the remote cloud. The PoE of CloudEx is always zero, as it offloads all the tasks to the remote cloud.

We can see from Fig. 10c that the PoCs of all the algorithms except EdgeEx increase with λ. When , CloudEx exhibits the best PoC; however, when , GA-PM has the largest PoC. This can be explained that with small λ and hence few tasks arrived at the edge node, CloudEx can almost process all the tasks by means of offloading tasks to the remote cloud. In contrast, GA-PM will deploy fewer tasks to the remote cloud, as the tasks executed on the cloud require the extra execution cost. However, with larger λ and hence more tasks arrived, CloudEx cannot process all the tasks due to the deadline constraint. In this case, GA-PM offloads more tasks to the cloud by paying for the execution cost, gaining the corresponding profit and showing a clearer advantage. In a word, GA-PM tends to execute more tasks on the edge node unless the deadlines of some tasks are so tight that they cannot be met. Without the security services, a part of NoSec's profit is lost due to malicious attacks, resulting in less PoC than GA-PM. The PoC of EdgeEx is always zero, as it executes tasks only on the edge node.

5.3. Impact of number of servers N
This section discusses the performance impact of the number of servers N. Fig. 11 shows the corresponding results by varying N from 2 to 20 with an increment of 2, where we set , , and . Note that the larger N, the higher the computation capacity of the edge node.

Fig. 11
Download : Download high-res image (376KB)
Download : Download full-size image
Fig. 11. The performance impact of the number of servers N.

Fig. 11a shows the TPs of all the algorithms, where GA-PM performs the best. With the increase of N, the TPs of all the algorithms except CloudEx go up. This is because, with high computing capacity, GA-PM, NoSec, and EdgeEx can process more tasks on the edge node. Specifically, when , the TP of EdgeEx increases rapidly. This is due to the fact that under the fixed task arrival rate λ and with more servers deployed, more tasks can be processed at the edge node. However, when , the TP of EdgeEx increases slowly as the computing resource becomes saturated when N is further increased. Since some tasks' profit is lost due to malicious attacks, NoSec always has less TP than GA-PM. The curve of CloudEx is flat with D as CloudEx processes all the tasks on the remote cloud, and hence it is independent of D.

We find in Fig. 11b that with the increase of N, the PoEs of all the algorithms except CloudEx increase. As explained above, the larger number of servers, the higher the computation capacity of the edge node, which brings higher PoE. The PoE of EdgeEx first increases rapidly when , and then its PoE slightly goes up when . We also can observe that NoSec has higher PoE than GA-PM, as it tries to execute more tasks on the edge node. GA-PM algorithm can balance all the tasks between the edge and cloud to maximize the TP. Since CloudEx processes all the tasks on the remote cloud, its PoE is always zero.

We find from Fig. 11c that the PoCs of GA-PM and NoSec decrease with the growth of N. This is because with more physical servers deployed, GA-PM and NoSec can process more tasks on the edge node and tend to offload fewer tasks to the remote cloud, gaining more PoE but less PoC. Due to the lack of security protection, NoSec performs worse than GA-PM. We can also observe that CloudEx is independent of N. This is because server size only has an impact on the task execution on the edge node. Since EdgeEx processes all the tasks on the edge node, its PoC is always zero.

5.4. Impact of data size D
This section validates our GA-PM algorithm on the data size by varying D from 2 to 20 MB. The corresponding results are shown in Fig. 12, where we keep the other parameters fixed, for example, , , and . Note that the larger D will take a longer time to transmit the data from the edge to the cloud.

Fig. 12
Download : Download high-res image (377KB)
Download : Download full-size image
Fig. 12. The performance impact of data size D.

Fig. 12a indicates a decrease in the TPs of all the algorithms except EdgeEx with D. We first find that the curve of EdgeEx is flat. This is because different data sizes directly impact the security time and data transmission time. However, EdgeEx executes all the tasks on the edge node, and hence it is independent of D. We can also see that when , GA-PM has a larger TP than EdgeEx. This is because, in this case, GA-PM can still offload some tasks to the remote cloud and gain profit. However, when , executing tasks on the remote cloud cannot get corresponding profit, and hence, GA-PM and EdgeEx almost have the same TP. With larger D, the data transmission time becomes larger, suffering from more malicious attacks, and leading to the TP of NoSec decreases. CloudEx exhibits the lowest TP with D. Moreover, the TP of CloudEx decreases rapidly when . The reason is that the size of data directly impacts task offloading, i.e., with larger D, CloudEx will spend more time on security time and data transmission time. Hence, it can process only few tasks, leading to less TP.

Fig. 12b gives the PoEs of all the algorithms. We can see that EdgeEx has the largest PoE as it puts all the tasks execution on the edge node, and the curve of EdgeEx is flat as it is independent of D. However, the PoEs of GA-PM and NoSec slightly increase with the increase of D. This can be explained that with larger D, GA-PM and NoSec try to process more tasks on the edge node. Due to CloudEx executes all the tasks on the remote cloud, its PoE is always zero.

Fig. 12c shows the PoCs of all the algorithms, wherewith the increase of data size D, the PoCs of all the algorithms except EdgeEx decrease. We can see that when , CloudEx performs better than GA-PM; however, when , the PoC of CloudEx decreases rapidly, and GA-PM has the largest PoC. This is because the larger D, the more security time and data transmission time. In this case, a large number of tasks cannot be offloaded to the remote cloud due to deadline constraints. Similarly, with a larger D, a task will be easy to suffer from malicious attacks, resulting in the reduction of NoSec's PoC. The curve of EdgeEx's PoC is always flat as it executes all the tasks on the edge node.

5.5. Impact of security demand sd
We examine the performance impact of security demand sd. The results with sd varying from 0 to 1.0 with an increment of 0.1 are shown in Fig. 13, where we keep the other parameters fixed, for example, , , and  MB. Note that the larger sd indicates that we need a higher security level to ensure data security.

Fig. 13
Download : Download high-res image (556KB)
Download : Download full-size image
Fig. 13. The performance impact of security demand sd.

Fig. 13a indicates the relationship between the security demand sd and the TPs of all the algorithms. We can see that with the increase of sd, the TP curves of all the algorithms except EdgeEx give a downward trend, which proves that security demand has a direct impact on the TP. This is because to protect data security, the greater sd needs a higher security service level. As introduced in Section 3.2, the higher security level causes more security time overhead. In this case, adding security services for offloading tasks to the remote cloud needs more time, and stripping security services by the remote cloud also needs more time. We can also find that varying sd has the most impact on CloudEx as it offloads all the tasks to the remote cloud. The curve of EdgeEx's TP is flat, which indicates EdgeEx is independent of sd. It is clear that GA-PM achieves the best TP when . However, when , GA-PM has only a slightly higher TP than EdgeEx. This is because, with larger sd, GA-PM tends to process more tasks on the edge node. Although adding security services introduces security time overhead, the TP of GA-PM is still larger than that of NoSec. Note that when , GA-PM and NoSec have the same TP.

We first find from Fig. 13b that the PoEs of GA-PM and NoSec increase with sd. The reason for NoSec is that the larger sd incurs a higher risk probability as it does not adopt any security service. Hence, NoSec tries to execute more tasks on the edge node, leading to larger PoE. Similarly, with larger sd and hence more security time, GA-PM also processes more tasks on the edge node, resulting in higher PoE. EdgeEx has the most PoE and is independent of sd, as it processes tasks only on the edge node. By offloading all the tasks to the remote cloud, the PoE of CloudEx is zero.

Fig. 13c gives the PoC results of all the algorithms. We find that the PoCs of GA-PM, NoSec, and CloudEx decrease with the increase of sd. For NoSec, the larger sd means the tasks are more vulnerable to network attacks. Nevertheless, for GA-PM and CloudEx, the larger sd directly impacts the security time, leading to reducing task execution on the remote cloud. Since EdgeEx processes all the tasks on the edge node, it has zero PoC.

Fig. 13(d) shows the relationship between security demand sd and risk probability 
. For a specified 
, a larger sd leads to a higher risk probability. So, the edge node should apply a higher level of security service to protect data security. Besides, the larger sd incurs more security time. Hence, the TPs of all the algorithms except EdgeEx decrease with sd. Moreover, for NoSec, the larger sd indicates the tasks offloaded to the remote cloud will suffer from more malicious attacks. As described before, if a task is attacked, the edge node gains no profit even if this task has been successfully completed. So, with the increase of sd, NoSec will experience more attacks and hence loss more profit. The corresponding result is shown in Fig. 13(e).

5.6. Analysis of running time
To examine the running time of our GA-PM algorithm under the different number of task sizes, we scale the task arrival rate λ from 20 to 200, while we keep  and . The statistical execution time is shown in Fig. 14. We can observe that the running time of GA-PM increases with λ. Moreover, we find that the running time almost goes up linearly as λ increases. According to the analysis in Section 4.6, the time complexity of the GA-PM algorithm is . Hence, this observation confirms the theoretical analysis.

Fig. 14
Download : Download high-res image (25KB)
Download : Download full-size image
Fig. 14. The running time of GA-PM with different arrival rates λ.

6. Conclusions and future work
This work presents a security-aware task offloading method for the edge node to maximize the total profit in the ECC environment. Specifically, due to malicious attack issues, we first build a security model, investigating the impact of time overhead on the number of CPU cores and data size. Then, we combine security protection with the task offloading to generate our PM problem. We develop a GA-PM algorithm to maximize the total profit for the edge node, and a corresponding coding strategy is devised. Moreover, the crossover and mutation operations are implemented based on the coding strategy, which considers the execution location and execution order of arrived tasks. The time complexity of the GA-PM algorithm is . Extensive simulation results show that our GA-PM algorithm always behaves better than all the comparison algorithms on all the parameter variations.

In future work, we focus on the tasks with workflow structure, i.e., the tasks are not independent but precedence-constrained, which is generally modeled by a directed acyclic graph (DAG). Scheduling workflow tasks in the mobile environment is very difficult, which remains an open issue.

