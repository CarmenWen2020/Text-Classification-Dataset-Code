Abstract
Kernel matrix-vector product is ubiquitous in many science and engineering applications. However, a naive method requires  operations, which becomes prohibitive for large-scale problems. To reduce the computation cost, we introduce a parallel method that provably requires  operations and delivers an approximate result within a prescribed tolerance. The distinct feature of our method is that it requires only the ability to evaluate the kernel function, offering a black-box interface to users. Our parallel approach targets multi-core shared-memory machines and is implemented using OpenMP. Numerical results demonstrate up to 19× speedup on 32 cores. We also present a real-world application in geo-statistics, where our parallel method was used to deliver fast principle component analysis of covariance matrices.


Keywords
Kernel method
Matrix-vector multiplication
Covariance matrix
Fast multipole method
Shared-memory parallelism

1. Introduction
We consider the problem of computing kernel matrix-vector products, where the kernel function  is non-oscillatory, translation invariant, and sufficiently smooth (NOTIS) ( can be singular when ). For example, a NOTIS function can be  or exp().

The problem can be formulated mathematically as evaluating(1)
 
 where 
 and 
 are the target and the source data points in a cubical domain, respectively, and 
 is the weight associated with 
. In many applications, the two sets of points 
 and 
 may overlap. Algebraically, Eq. (1) can be written as the following matrix-vector multiplication:(2) where 
 and 
 are two vectors, and K is an N-by-N matrix with 
. Such type of computation arises in many science and engineering fields, such as kernel methods in statistical learning and machine learning [13], [18], data assimilation methods in geosciences [5], [21], particle simulations and boundary integral/element methods in computational physics [14], [15], [27], dislocation dynamics simulations in material science [6], [38], etc. To compute ϕ in Eq. (2), a naive direct evaluation requires 
 operations, which is prohibitive when N is large.

1.1. Related work
One special but important instance of Eq. (1) is when the data points 
 and 
 lie on a regular grid. In this case, Eq. (1) can be evaluated exactly (up to round-off errors) using the fast Fourier transform (FFT), which requires  operations. Although the FFT can be extended to handle non-uniform data distributions [3], [26], but its efficiency decreases for highly irregular distributions in three dimensions (3D).

The fast multipole method (FMM) is a general framework that has been successfully applied to non-uniform data distributions. In the FMM, we partition the problem domain hierarchically into subdomains of different scales and exploit the multi-scale decomposition in the evaluation of Eq. (1). In particular, we evaluate exactly the calculation associated with adjacent subdomains at the finest scale; and we evaluate approximately the calculation associated with non-adjacent subdomains at every scale. Overall, the procedure requires  operations with a provable accuracy. While the FMM has been derived for specific kernels that appear frequently in computational physics [10], [11], [14], [16], [36], the derivation may be tedious, difficult or even impossible for an arbitrary NOTIS kernel function.

Therefore, black-box algorithms have been developed, which require only the evaluation of a given kernel function. Such methods can be classified into two groups. The first consists of methods that approximate the kernel function (away from the origin) with polynomials, such as Legendre polynomials or Chebyshev polynomials [6], [7], [9], [12]. The other group consists of methods that compute the so-called equivalent densities or the so-called skeletons for every subdomain to efficiently represent the contained source points and their weights [22], [23], [35], [37]. Theoretically, this approach is justified by the potential theory for kernel functions that are fundamental solutions of non-oscillatory elliptic partial differential equations.

1.2. Contributions
In this paper, we present a parallel implementation of the black-box method in [6], [9] for evaluating Eq. (1) using  memory and computation. The key idea is using Lagrange interpolation to construct approximations of the kernel function  when x and y are distant. Unlike fast algorithms that have been developed for specific kernel functions, our method applies to a wide range of functions. Successful stories include applications of our method in dislocation dynamics simulations [6] and aquifer characterization [20]. In those two applications, no fast algorithm exists for the two kernel functions—the Green's function in anisotropic elasticity and the isotropic exponential function. In particular, our method requires only a black-box routine to evaluate the kernel function, and thus can be integrated easily with other codes. For example, our package was used in an iso-geometric boundary element method to obtain a solver of  complexity [27]. Other examples of using (an earlier version of) our code are in the elastic formulation of the displacement discontinuity method for the simulation of micro-seismicity [8], [31], [32].

To evaluate Eq. (1), our method follows the general FMM machinery as follows. First, the problem domain containing 
 and 
 is partitioned in a hierarchical fashion (see Fig. 1(a)), and the partitioning is associated with a tree data structure where every tree node represents a subdomain in the hierarchy. Second, a post-order traversal of the tree is performed, and the “multipole coefficients” associated with every tree node is computed as a compact representation of the source points and corresponding weights that the subdomain contains. This step is often called an upward pass. Third, at every level of the hierarchy, the “local coefficients” of every tree node is computed using the “multipole coefficients” of its interaction list (two nodes are in each other's interaction list if their parents are adjacent but they are not; see Fig. 1(b)). Fourth, a pre-order traversal of the tree is performed to “accumulate” the local coefficients of all nodes to those at the leaf level. This step is often called a downward pass. Finally, the contribution from adjacent subdomains are evaluated exactly for all leaf nodes. Technically speaking, multipole and local coefficients are terminologies used in the original FMM [14]. Here we adhere to the same terms for their counterparts in our algorithm, which we will rigorously define in Section 2.3.

Fig. 1
Download : Download high-res image (179KB)
Download : Download full-size image
Fig. 1. (a) hierarchical decomposition of the problem domain and the associated tree structure. The tree translation operators (M2M, M2L and L2L) are introduced in Section 2.3. (b) a subdomain α at the fourth level in the hierarchy, its neighbors  and its interaction list .

Our parallel algorithm is based on three observations of the above procedure: (1) the computation of multipole coefficients and local coefficients during the upward pass and the downward pass is embarrassingly parallel for tree nodes at the same level; (2) the computation of local coefficients based on multipole coefficients is embarrassingly parallel for all nodes; and (3) the exact evaluation of the contribution from adjacent subdomains is embarrassingly parallel for all leaf nodes. While our parallel algorithm does not exploit task-level parallelism [1], which usually requires a well-designed runtime system to implement, our results show satisfactory parallel speedups.

To summarize, we present parallel black-box FMM in 3D (PBBFMM3D) to compute the kernel matrix-vector multiplication in Eq. (1) for NOTIS kernels. The algorithm provably requires  memory and operations. Our parallel implementation is based on OpenMP1 and targets multi-core shared-memory machines. Nowadays, on-node parallelism is becoming increasingly important because of the wide adoption of multi-core architectures. Our method also serves as the basis for a multi-node distributed-memory solution, which can be implemented on top of our code using the so-called local essential tree and space-filling curve [33], [34]. Extensions to many-core architectures (e.g., GPUs) was considered in [2], [29], which used blocking schemes to reduce memory movement and to improve the arithmetic intensity (flop-to-word ratio). Our code is publicly available at https://github.com/ruoxi-wang/PBBFMM3D

1.3. Outline
The rest of this paper is organized as follows. Section 2 describes the algorithm focusing on the data dependency and the parallelism. Section 3 describes the software architecture and the user interface. Section 4 presents the accuracy and the running time of PBBFMM3D. Section 5 draws the conclusion.

2. Parallel black-box algorithm
In this section, we present the black-box algorithm for evaluating Eq. (1). In particular, we focus on the data dependency and illustrate our parallel strategy. Although our code deals with 3D problems, we use 1D and 2D examples here for pictorial illustration.

2.1. Hierarchical domain decomposition
As with other multilevel methods, our approach is based on a hierarchical decomposition of the problem domain to achieve linear complexity. Specifically, a cubical domain is divided into eight subdomains through binary partitioning along every coordinate. Then, every subdomain is divided recursively until the number of data points in every subdomain is less than a prescribed constant. This hierarchical domain decomposition is naturally associated with an octree tree data structure, where the root stands for the entire domain, and the other nodes stand for subdomains at different levels in the hierarchy; see Fig. 1(a). For the rest of the paper, we use the terms subdomain and tree node interchangeably.

Note the hierarchical decomposition is generally non-uniform ( is adaptive). But we choose to use a uniform-tree data structure in PBBFMM3D for the ease of parallel implementation, which is reasonably efficient as long as the point distribution is not extremely irregular. With a uniform tree, some leaf nodes may end up having few data points. In PBBFMM3D, empty nodes are skipped in the algorithm, and the overhead of using a uniform tree compared with an adaptive one is from processing nodes that have only a few points.

Given the tree , the parent  and the children  of a node α are naturally defined. We also define the neighbors  and the interaction list  of a node α as below; see Fig. 1(b) for an pictorial illustration.

Definition 2.1 Neighbors and interaction list

Given a hierarchical tree structure,

•
: the adjacent nodes (subdomains) of α at the same level in the hierarchy including α itself. The number of neighbors is generally 
, where d is the spatial dimension.

•
: the non-adjacent nodes (subdomains) at the same level in the hierarchy whose parents are neighbors of . The number of nodes in the interaction list is generally 
, where d is the spatial dimension.

2.2. Separation of variables (low-rank approximation)
The key idea of our algorithm is to approximate the kernel function  through polynomial interpolation when the target point x and the source point y are distant. In this section, we focus on the situation where x and y are inside two non-adjacent subdomains, respectively. Technically speaking, x and y are well-separated. For the following discussion, we need a set of p interpolation nodes 
⁎
⁎
⁎
 on the real line. Then we can form interpolation nodes in 3D with tensor products:
⁎
⁎
⁎
⁎
 where i and 
 are 1D-index and 3D-index of the  grid, respectively, e.g., 
, 
, and 
.

Definition 2.2 Lagrange basis polynomials

The p-th order Lagrange basis polynomials in 1D are
⁎
⁎
⁎
⁎
 
 The p-th order Lagrange basis polynomials in 3D are tensor products of the p-th order Lagrange basis polynomials in 1D:
⁎
⁎
⁎
⁎
 where 
, and 
⁎
⁎
⁎
⁎
.

Definition 2.3

p-th order polynomial interpolant of 
Denote 
 as the p-th order polynomial interpolant of , which is obtained through Lagrange interpolation on x and y, respectively (x and y are well-separated):(3)
 
⁎
⁎
 
 
⁎
⁎
⁎
⁎
 where 
⁎
⁎
.

Observe that constructing the approximation 
 requires only evaluations of the kernel function 
⁎
⁎
.

The PBBFMM3D code offers two options for interpolation nodes, namely, Chebyshev nodes
 
 and equally spaced nodes
 
 With the Chebyshev nodes, the approximation in Eq. (3) is nearly optimal among polynomials of the same order. More importantly, the error decays as 
 if the kernel function  is analytic and bounded in the “Bernstein ellipse” of foci 1 and -1 with semimajor and semiminor axis lengths summing to ρ [30]. With equally spaced nodes, the matrix 
⁎
⁎
 in Eq. (3) is a block-Toeplitz-Toeplitz-block matrix, which has a reduced memory footprint and can be applied in 
 time using the FFT [6]. Note the Lagrange interpolation of high degree over equally spaced nodes does not always converge, even for smooth functions, which is known as Runge's phenomenon. In practice, we find low-order approximations sufficiently accurate in many applications. The scheme of Lagrange interpolation on uniform nodes can be stabilized by fitting a polynomial of degree  using least-squares.

2.3. Black-box FMM algorithm
To evaluate Eq. (1), our algorithm has the following four stages.

1.
Upward pass. A post-order traversal of  is performed to compute the “multipole coefficients” of every subdomain, which encodes information of the source points and their weights contained in the subdomain. For every leaf node α in , the particle-to-moment (P2M) translation is executed:
 
⁎
 where 
 and 
⁎
 are the source points and interpolation nodes in α, respectively. For every non-leaf node α in , the moment-to-moment (M2M) translation is executed:
 
 
⁎
⁎
 where 
⁎
 and 
⁎
 are the interpolation nodes in α and β, respectively.

2.
Far-field interaction. The “local coefficients” of every node is computed using the “multipole coefficients” of its interaction list. For every node α in , the moment-to-local (M2L) translation is executed:
 
 
⁎
⁎
 where 
⁎
 and 
⁎
 are the interpolation nodes in α and β, respectively.

3.
Downward pass. A pre-order traversal of  is performed to “accumulate” the “local coefficients” at leaf nodes. This is effectively a “transpose” of the upward pass. For every node α in , the local-to-local (L2L) translation is executed:
 
⁎
⁎
 where , 
⁎
 and 
⁎
 are the interpolation nodes in α and β, respectively. For every leaf node α in , the local-to-particle (L2P) translation is also executed:
 
⁎
 where 
 and 
⁎
 are the target points and interpolation nodes in α, respectively.

4.
Near-field interaction. The contribution from neighbors is evaluated exactly. For every leaf node α, the particle-to-particle (P2P) translation is executed:
  
 
 where 
 and 
 are the target and the source points in α and β, respectively.

To summarize, the contribution from neighbors is calculated exactly; the contribution from the interaction list is approximated using the polynomial interpolant in Eq. (3); and the contribution from the remaining leaf nodes are approximated through coarser levels in the tree. A diagram illustrating the above process is shown in Fig. 2. Although our algorithm is presented using scalar operations for ease of illustration, it is implemented using BLAS2 and BLAS3 subroutines.

Fig. 2
Download : Download high-res image (188KB)
Download : Download full-size image
Fig. 2. A one-dimensional FMM example with one target point and three source points. The calculation of Eq. (1) is divided into three parts: (1) P2P translation from the neighbor (gray), (2) P2M→M2L→L2P translations from the interaction list (blue), and (3) P2M→M2M→M2L→L2L→L2P translations from the remaining leaf nodes (red). (For interpretation of the colors in the figure(s), the reader is referred to the web version of this article.)

Below, we present a theorem for the computational cost and memory footprint of the above algorithm. Our primary focus is showing the complexity in terms of the number of points N, and thus we present the computational cost of one M2L translation as 
, the cost of a naive matrix-vector product. For the same reason, we use an 
 estimate for the storage of every M2L translation operator. Given the M2L translation is a bottleneck in the FMM, we implement acceleration techniques in PBBFMM3D, which is discussed in Section 2.5.

Theorem 2.1 Computational cost and memory footprint

The computational cost and memory footprint of the black-box FMM algorithm are both .

Suppose every leaf node has at most 
 points (typically ). The number of leaf nodes and the number of tree nodes are both 
. In the above algorithm, every node requires constant amount of work (independent of N) at every stage:

1.
Upward pass. P2M: 
 work for every leaf node. M2M: 
 work for every non-leaf node, where every non-leaf node has at most 8 children in 3D. Note the M2M is a 3D tensor-vector multiplication.

2.
Far-field interaction. M2L: 
 work for every node, where the interaction list has at most 
 nodes in 3D.

3.
Downward pass. L2L: 
 work for every node. L2P: 
 work for every leaf node. Note the L2L is a 3D tensor(transpose of the tensor in M2M)-vector multiplication.

4.
Near-field interaction. P2P: 
 work for every leaf node, where every node generally has at most 
 neighbors in 3D.

Therefore, the computational complexity of the entire algorithm is .
Regarding the memory footprint, every tree node stores 
 “multipole coefficients” and 
 “local coefficients”, which sums up to 
. Since the kernel function is translational invariant, we need to precompute only 
 M2L translation operators at every level for the -level tree structure, which is 
 memory in total. Therefore, the memory footprint is 
.

2.4. Parallel algorithm
In this section, we analyze the parallelism in each of the four stages in the FMM algorithm, and we have implemented a parallel algorithm using the OpenMP API for shared-memory machines.

1.
Upward pass. As stated earlier, the upward pass is a post-order traversal of , so a parallel post-order tree traversal using OpenMP tasks is implemented in PBBFMM3D.

2.
Far-field interaction. The M2L translations are independent for all nodes. In addition, the translation typically requires the same amount of work for every node. Therefore, the OpenMP “parallel for” directive is employed on the loop over all nodes for M2L translations in PBBFMM3D.

3.
Downward pass. As stated earlier, the downward pass is a pre-order traversal of , so a parallel pre-order tree traversal using OpenMP tasks is implemented in PBBFMM3D.

4.
Near-field interaction. The P2P translations are independent for all leaf nodes. However, the translations between pairs of neighbors are work-heterogeneous due to the non-uniform distribution of the target and the source points. In PBBFMM3D, the OpenMP “parallel for” directive is employed on the loop over all leaf nodes for P2P translations, and different scheduling policies can be used based on any prior knowledge of the point distribution.

Here our parallel algorithm focuses on parallelizing each stage of the FMM algorithm, and haven't exploit the concurrency across different stages. In principle, the near-field interaction stage does not depend on the others except for updating the results. So the work-heterogeneous P2P translations can be prioritized. Furthermore, notice the M2L translation of a node can happen as soon as its “multipole coefficients” have been computed. However, implementing these ideas efficiently typically requires a task-based runtime system, for which we refer interested readers to [1], [2].

2.5. Acceleration techniques
Since the far-field interaction and the near-field interaction usually dominate the entire computation, we introduce the acceleration techniques used in PBBFMM3D.

Homogeneous kernel  A kernel function is homogeneous if
 for  and m is typically an integer. For example,  is a homogeneous kernel function of degree -1. Since the interpolation grids are fixed relative to the problem domain, the M2L translation operators of different levels differ only by a scaling constant. Hence, we store these operators for only the leaf level.

Symmetry and skew-symmetry  A kernel function is symmetric if and is skew-symmetric if In many applications, the source 
 and the target 
 are the same set of points. Therefore, the kernel matrix K becomes symmetric or skew-symmetric if the kernel function is such. This implies only one of the two P2P/M2L translation operators needs be stored between a pair of tree nodes that are either neighbors or in each other's interaction list.

Fast M2L translation  Recall the definition of an M2L translation operator 
⁎
⁎
 in Eq. (3), where 
⁎
 and 
⁎
 are interpolation grids in a pair of tree nodes that are in each other's interaction list. In PBBFMM3D, there are two options for the interpolation grids: Chebyshev nodes or equally spaced nodes. With Chebyshev nodes, an SVD-based compression of the M2L operator is employed following the approach in [9] since the translation operator is observed to be numerically low rank. With equally spaced nodes, the M2L operator is a block-Toeplitz-Toeplitz-block matrix [6], where the 
-by-
 matrix has only 
 unique entries and can be applied to a vector in 
 time using the FFT.

Multiple right-hand-sides  In some applications, Eq. (1) needs to be evaluated with multiple weight vectors associated with the same set of source points. In PBBFMM3D, all weight vectors are grouped into a matrix as the input of the FMM algorithm, which allows using cache-friendly BLAS3 operations in our algorithm.

3. Software description
In this section, we briefly discuss the software architecture of PBBFMM3D focusing on the black-box feature of the algorithm and the C++ and Python interfaces. More details can be found in the documentation at https://github.com/ruoxi-wang/PBBFMM3D. The code is written in C++ with the OpenMP API, and requires some standard linear algebra libraries including the BLAS,2 the LAPACK3 and the FFTW3 library.4 The Boost Python Libraries5 is required for using the Python interface.

The PBBFMM3D has the following three main classes as shown in Fig. 3.

•
Class H2_3D_Tree sets parameters and creates the hierarchical partitioning of the problem domain. The parameters include (1) “Domain size”: side length of a cubical problem domain, (2) “Tree level”: the number of levels in the hierarchical partitioning, (3) “Interpolation type”: Chebyshev nodes or equally spaced nodes, (4) “Interpolation order”: the number of interpolation nodes used in Eq. (3), (5) “SVD truncation error”: error from the compression of the M2L translation operators, which is by default the prescribed accuracy of the entire computation. The two key member functions are the following. Function PrecomputeM2L() precomputes the M2L operators, and BuildFMMHierarchy() creates the hierarchical partitioning and builds the corresponding data structure.

•
Class H2_3D_Compute stores the information regarding the source and the target points including (1) “Target”: position of target points 
, (2) “Source”: position of source points 
, (3) “Weight”: weights 
 associated with the source points, and (4) “Number of weights”: number of weights associated with every source point. The five key member functions include FMMDistribute(), which assigns the source and the target points to leaf cells in the tree, and four functions correspond to the four translation stages described in Section 2.

•
Class kernel_NAME defines the kernel function. The member function SetKernelProperty() sets the homogeneous and the symmetric properties of the kernel function. The member function EvaluateKernel() takes two data points x and y and returns the value of .

Fig. 3
Download : Download high-res image (309KB)
Download : Download full-size image
Fig. 3. Three main classes in PBBFMM3D. The H2_3D_Tree class stores information regarding the hierarchical partitioning of the problem domain. The H2_3D_Compute class implements the FMM algorithm, and the kernel_NAME class describes the kernel function.

C++ & python interfaces  Listing 1, Listing 2 show the core lines of a basic example using the C++ interface and the Python interface, respectively. The example evaluates Eq. (1) with the standard Gaussian kernel. The first line creates an object of Class kernel_Gaussian, which implements the standard Gaussian function. The class inherits from class H2_3D_Tree and takes input parameters. The second line creates the hierarchical partitioning of the problem domain. The last line does the computation and stores results in the output variable.

Listing 1
Download : Download high-res image (76KB)
Download : Download full-size image
Listing 1. C++ interface. “tree” is an object of class kernel_Gaussian, which implements the standard Gaussian kernel. It also stores input parameters for the hierarchical partitioning. “compute” takes the positions of the target points, the source points, the associated weight vector(s), and the number of weight vectors. It evaluates Eq. (1) and outputs the results.

Listing 2
Download : Download high-res image (85KB)
Download : Download full-size image
Listing 2. Python interface for the example in Listing 1.

Customized kernel  While some commonly used kernel functions are already implemented in PBBFMM3D, defining a new kernel function is straightforward as shown in Listing 3. It requires implementing only two methods as follows. The EvaluateKernel() method takes a pair of source and target points and returns the function value, and the SetKernelProperty() method tells the homogeneous degree of the kernel and whether it is symmetric (see Section 2.5).

Listing 3
Download : Download high-res image (140KB)
Download : Download full-size image
Listing 3. Example of defining the e−‖x−y‖ kernel. The function is symmetric but not homogeneous. Here, “vector3” is a structure of three floating point numbers representing coordinates in 3D.

4. Numerical results
In this section, we present numerical experiments with PBBFMM3D to show the accuracy, the sequential running time and the parallel scalability. A real-world application in geostatistics is also presented, where PBBFMM3D was used to speed up the calculations with a covariance matrix. We focus on two particular kernel functions here:  and , where , the Green's function for the Laplace equation in 3D, is frequently used in computational physics; and  is a popular choice as the covariance function for a Gaussian process.

All experiments were performed on a Linux server with 192 GB of RAM and the Intel Xeon Platinum 8280 (“Cascade Lake”) with 56 cores on two sockets (28 cores/socket). The code was compiled with GCC 8.3.0, which implements version 4.5 of the OpenMP standard, and the code was linked with the Intel MKL library6 version 2020.1.217 and the FFTW3 library version 3.3.8.

Parameters and notations 

•
N: the number of source/target points, which are randomly generated using the uniform distribution in the unit cube.

•
p: the order of interpolation, which determines the accuracy of our algorithm. Note the number of interpolation nodes is 
 in Eq. (3) as a tensor product of p nodes in each dimension.

•
unif: using equally spaced nodes in Eq. (3).

•
cheb: using Chebyshev nodes in Eq. (3).

•
tree_level: the number of levels of the hierarchical partitioning of the problem domain.

•
SVD accuracy: the compression accuracy of the M2L translation operator, which is chosen to be the same as the prescribed accuracy.

•
r: notation of the Euclidean distance between a pair of source point x and a target point y, i.e., 
.

4.1. Accuracy & sequential running time
In this section, we focus on two parameters N and p. First, we fix 
 and increase p to show the accuracy of PBBFMM3D. (We refer to [6] for the precomputation time and the memory footprint.) Then, we fix  and increase N to show the sequential running time.

Fig. 4 shows the (relative) error of evaluating Eq. (1) as a function of p, where the kernel functions are  and , respectively. The error is defined as
 
 where 
 stands for the approximation constructed implicitly in PBBFMM3D. Since both kernel functions are analytic (away from the origin), the error of our interpolation using either Chebyshev nodes or uniform nodes decays exponentially.

Fig. 4
Download : Download high-res image (182KB)
Download : Download full-size image
Fig. 4. Relative error vs. interpolation order p, where N = 104 and . Both unif and cheb lead to exponential decay of errors.

Fig. 5 shows the scaling of the sequential running time with respect to the number of points N. We see that the time increases linearly as N increases, as opposed to the quadratic increase of evaluating Eq. (1) naively.

Fig. 5
Download : Download high-res image (185KB)
Download : Download full-size image
Fig. 5. Sequential running time vs. number of points (N = 104,8 × 104,82 × 104, and 83 × 104). The interpolation order p is fixed at 4.

4.2. Parallel scalability
In this section, we present the parallel running time of PBBFMM3D on up to 32 cores. We focus on the kernel function  and use Chebyshev nodes in Eq. (3) with . Table 1 reports the parallel running time. To show the parallel scalability, we chose the number of cores to be a power of 2. But that is not required in PBBFMM3D. The baseline (1 core) is the serial version used in [6], [9]. As the table shows, we obtained approximately 19× speedup on 32 cores for 
.


Table 1. Parallel running time of PBBFMM3D. The kernel function is 1/r, and we used Chebyshev interpolation with p = 4. We set the number of tree levels to be 5, 6 and 7 for the three increasing problem sizes, and the errors are 2.10e-5, 2.08e-5, and 2.10e-5, respectively (independent of the number of cores used).

N	Time (seconds)
1 core	2 cores	4 cores	8 cores	16 cores	32 cores
82e+4	5.74e+0	3.32e+0	1.83e+0	9.93e-1	5.84e-1	4.44e-1
83e+4	4.72e+1	2.58e+1	1.40e+1	7.87e+0	4.40e+0	3.49e+0
84e+4	4.04e+2	2.15e+2	1.14e+2	6.37e+1	3.72e+1	2.13e+1
Fig. 6(a) shows the strong scalability of PBBFMM3D, i.e., the running time using a sequence of increasing number of cores for a fixed problem size. We provided a breakdown of the running time into the four stages: upward pass, far-field interaction, downward pass, and near-field interaction. We see that the running time for all stages nearly halved as the number of cores doubled.

Fig. 6
Download : Download high-res image (258KB)
Download : Download full-size image
Fig. 6. Parallel scalability and breakdown of the total time. The four stages in the algorithm are defined in Section 2. Strong scalability results correspond to a fixed problem size of 84 × 104 points. The base cases for weak scalability are 210 × 104 (∼10 million) points on 1 core and 4 cores, respectively.

Fig. 6(b) shows the weak scalability of PBBFMM3D, i.e., the running time for a fixed problem size per core. So we increased the number of particles proportionally to the number of cores. As the figure shows, the time spent on each stage in the FMM only increased by a small amount when the problem size increased by 8× (the number of cores also increased by 8×). The ideal runtime would stay unchanged due to the linear complexity of PBBFMM3D. Our implementation achieved an efficiency of 73% to 86% (serial time/parallel time).

4.3. Application in Gaussian processes
Gaussian random field (GRF) theory [25], [28] has been widely used in interpolation and estimation of spatially correlated unknowns. For example, GRF methods can be used for estimating the permeability of the underground soil and rock, which is of critical interest to hydrogeologists and petroleum engineers [19], [24]. In GRF methods, a covariance matrix is required as the prior information of the underlying unknown field. However, practical applications typically require large numbers of unknowns, so dimension reduction techniques such as the principle component analysis are required.

To obtain the top-k principle components, we employ PBBFMM3D to compute the truncated eigenvalue decomposition of the covariance matrix. In particular, we use a randomized method (Algorithm 5.3 in [17]) to calculate the top-k eigenvalues and their associated eigenvectors, and the randomized algorithm requires evaluating Eq. (1)  times (same target and source points but with  different weights). While the original method requires 
 operations, we accelerate the method with PBBFMM3D and arrive at the optimal  complexity, where N is the number of data points.

In the next experiment, we randomly generated data points in the unit cube and employed the kernel function 
, one-dimensional Matérn kernel with smoothness 1/2. The goal was to compute the top-100 eigen-pairs of the covariance matrix, where 120 matrix-vector products (packed into one matrix-matrix product) were evaluated twice in the randomized algorithm. In the original randomized method, matrix-matrix products are computed by calling the dgemm subroutine in the Intel MKL library (with multi-threading on 32 cores). In the PBBFMM3D-accelerated method, the number of Chebyshev nodes was , and the number of tree levels were 3, 4 and 5 for the three increasing problem sizes. We measured the error 
, where 
 and 
 are the eigenvalues computed using the original method and the PBBFMM3D-accelerated method, respectively. The errors for 
 and 
 are 
 and 
, respectively.

Fig. 7 shows the running time of the original method and our accelerated method. In Geostatistics applications, the original randomized method with exact matrix-matrix products is the common practice, and therefore we set that as our baseline. This allows us to show the best speedup and scaling one can achieve from leveraging PBBFMM3D. In the original method, we need to evaluate all entries of the covariance matrix and compute matrix-matrix products, both of which scale as 
. It is worth noting that when 
, forming the entire covariance matrix requires 3.2 TB of memory.

Fig. 7
Download : Download high-res image (139KB)
Download : Download full-size image
Fig. 7. Comparison between the original randomized algorithm (Algorithm 5.3 in [17], denoted as ‘Exact’) and the PBBFMM3D-accelerated randomized algorithm. Both methods computed the top-100 eigen-pairs of the same covariance matrix, where they evaluated 120 matrix-vector products (packed into one matrix-matrix product) twice. Timing of the original method was not available when N = 64 × 104 because forming the entire covariance matrix requires 3.2 TB of memory. The error of our accelerated method was at the order of 10−4.

In the PBBFMM3D-accelerated method, we need to evaluate only  entries of the covariance matrix. As Fig. 7 shows, our method scales linearly with respect to the problem size N, and thus the speedup over the original method becomes more pronounced when N increases.

5. Conclusions
We have introduced PBBFMM3D, a black-box algorithm/software for evaluating kernel matrix-vector multiplication on shared-memory machines. The target kernels are non-oscillatory translation invariant functions that are sufficiently smooth away from the origin. The user needs only provide a function routine that returns the kernel value given a source point and a target point, if the kernel function is not already implemented. (The user can also specify the degree of homogeneity and whether the kernel is symmetric or skew-symmetric to active specific optimizations.) Our algorithm requires  memory and work, where N is the number of data points.

A parallel algorithm is presented in this paper and implemented using OpenMP for shared-memory machines. We have presented parallel scalability results on up to 32 cores and achieved at most 19× speedup. We have also presented an application in geostatistics, where we accelerated a random algorithm for computing the truncated eigen-decomposition of covariance matrices.