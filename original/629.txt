Abstract
The technological evolution of the smart grids is going to take its shape in the form of a new paradigm called the Internet of Energy (IoE); which is considered to be the convergence of internet, communication, and energy. Like other evolved technologies, the IoE inherits security vulnerabilities from its constituents that need to be addressed. Intrusion Detection Systems (IDS) have been used to counteract malicious attacks. Among the types of IDS, anomaly-based IDS that employ mostly machine learning algorithms are considered to be the promising one, owing to their capability of detecting zero-day attacks. However, using complex algorithms to detect attacks, the existing anomaly-based IDS designed for IoE require considerable amount of time. It is tempting to reduce the training and testing time in order to make the IDS feasible for the IoE architecture. In this paper, we propose a hybrid anomaly-based IDS that can be installed at any networked site of the IoE architecture, such as Advanced Metering Infrastructure (AMI), to counteract security attacks. Our proposed system reduces the overall classification time of detection compared to the existing hybrid methods. The proposed solution uses a combination of K-means and Support Vector Machine, where the K-means centroids are used in a unique training method that reduces the training and testing times of the Support Vector Machine without compromising classification performance. We choose the best value of “k” and fine-tuned the SVM for best anomaly detection. Our approach achieves the highest accuracy of 99.9% in comparison with the existing approaches.

Previous
Next 
Keywords
Intrusion detection

Anomaly-based intrusion detection

Machine learning

Smart grid

Internet of Energy

1. Introduction
The recent technological advancements have converted the traditional electric grid to a Smart Grid (SG) thereby using the information and communication technology. One step ahead of this, a new paradigm has emerged on the technology horizon, called the Internet of Energy (IoE). The IoE is believed to remedy the two main global problems of carbon emissions and energy crisis. The IoE is the innovative version of the amalgamation of the SG and the Internet. In other words, it is the convergence of Internet, communication, and energy [7]. The IoE is a platform that is used to integrate communication and power flows, thereby ensuring the seamless integration of power generation, distribution, operations, end-users, service providers, and regulators. It is believed that the IoE will also accommodate the renewable energy sources in order to support economic and environmental efficacies. An abstract form of the IoE is depicted in Fig. 1. One of the key enablers for IoE is the Internet of Things (IoT) [1], [37], [41], specifically the sensing and actuating abilities of IoT will be used for the IoE energy domains. The Internet architecture provided by the IoT technology enables the dissemination of information among various smart devices, such as smart home appliances, smart metres, and utility companies; and facilitates to monitor and control the operations of IoE.


Download : Download high-res image (423KB)
Download : Download full-size image
Fig. 1. Abstract view of the internet of energy architecture.

The Advanced Metering Infrastructure (AMI) is an important entity of the IoE paradigm that facilitates the bidirectional data communication between the potential customers and the utility companies. The AMI is composed of three entities, i.e. smart metre, data concentrators, and AMI headends. Smart metres are responsible for the metring of electricity consumption of the electrical appliances. The data concentrators work in a specific geographical area and aggregate power consumption related data from smart metres installed at that region. The data concentrators then forward the data to the AMI headends. The headends act as a central server at the utility company and are responsible for the reception, storage and management of the information sent by the data concentrators. The aggregated data at the headends enable the utility company in managing the resources and taking right decisions regarding the power generations, transmission and dissemination.

The AMI employs and use various communication networks to exchange data, as shown in Fig. 2. We discuss these networks briefly as follows. Home Area Networks (HANs) is a personal area network that is usually built across personal devices of a person at home, such as smart appliances, using Bluetooth or Zigbee. The smart metre at HAN connects to data concentrators via another communication network, called Neighborhood Area Networks (NANs), using WiFi, cellular-3G or 4G, or WiMAX technologies. The data concentrators then use Wide Area Networks (WANs) to communicate with the headends at the utility company’s network. WANs may use WiMAX, cellular-3G or 4G, or satellite technologies. The Field Area Networks (FANs) are then used by the utility company to process and maintain the operations.

The advantages of the ensued IoE architecture are numerous; however, the IoE being a critical infrastructure for a country will inherit the security vulnerabilities of the SG and the IoT and may induce more unknown threats to the new system. The IoE architecture may be targeted by cyber attackers for the availability, integrity, and confidentiality services to be compromised [15]. For instance, Denial of Service (DoS) or Distributed Denial of Service (DDoS) attacks may be mounted to cause detrimental damages like disruption of the system or its subsystems, or power outage [4], [8], [14], [50]. Cyber attackers create network of compromised nodes, called botnets, in order to upsurge the scale of the attack. The attackers usually employ spoofed or masquerade identities to launch these attacks which makes the source of the attack undetectable [2]. One example of such an incident happened in Ukraine, where a successful cyberattack was launched on a Ukrainian power substation ensuing power outage in the region for almost 225,000 population [19]. Furthermore, bogus data can be injected into the network to disrupt the system and even to modify the smart metre data [9], [10], [29], [31]. False identities can be created on a single physical device, called Sybil attack [3], [25], to perform other malicious actions for monetary and non-monetary gains.

The hybrid nature of IoE architecture that comprises of various ICT and industrial components and the diversity of communication taking place among these components generating huge volume of data by each subsystem hinder the use of conventional security systems. Therefore, it is evident that the presence of an Intrusion Detection Systems (IDS) is crucial for the smooth operation of IoE thereby ensuring the main security requirements of availability, integrity, and confidentiality [44], [45]. The IDS monitor network traffic and look to detect any attacks in real-time. Monitoring network traffic provides the ability to flag any attack up to a network administrator or security practitioner for further analysis and counter measures to be carried out. IDS monitor network traffic and raise alerts for any suspected attacks, this is made difficult by increasing quantities of data as well as new attacks being constantly created as a consequence of the increase in valuable data [12], [18], [20], [34], [39].


Download : Download high-res image (187KB)
Download : Download full-size image
Fig. 2. Advanced Metering Infrastructures (AMI) architecture [11].

Various machine learning based anomaly-based IDS have been proposed to protect the critical infrastructure of IoE and SG. Some of them are designed to protect the whole ecosystem of IoE or SM, such as [35] and [51], while others are designed to target protecting the critical individual components of IoE or SM, such as [23] and [6]. However, anomaly-based IDS have one main drawback, i.e. the complexity of the algorithms involved adds to computational and therefore time complexity; this is a serious problem when the volume of network traffic increases. Similarly, some components of the IoE and SG are resource-constrained and may not feasible for them to use these machine learning based IDS which are resource hungry and consume considerable amount of computing and storage resources. In this paper, we propose an anomaly detection-based IDS that employs machine learning algorithms to detect the zero-day (unknown) malicious attacks. Our proposed IDS can be installed at networked site of the IoE architecture, such as AMI which enables the communication between energy consumers and the utility companies [36]. This research work tries to tackle the aforementioned drawback of the anomaly-based IDS by reducing the processing time of data in anomaly detection whilst not compromising the detection accuracy of the system.

This paper specifically makes the following contributions:

•
We propose an anomaly detection-based IDS that employs machine learning algorithms to detect zero-day (unknown) malicious attacks using k-means based Support Vector Machine (SVM).

•
The proposed system significantly reduces the time complexity by using the optimal value of “k” for K-means as well as using the elbow method.

•
We apply SVM to detect anomalies and fine-tune the hyperparameters using the grid search method for better anomaly detection.

•
We tackle the problem of processing large amounts of network traffic and our proposed approach achieves the highest accuracy of 99.9% in comparison with existing approaches.

This paper is structured as follows, Section 2 described related work of the anomaly-based IDS solutions from 2010 onwards, in Section 3 the proposed KSVMeans is described and broken down into its individual modules before being evaluated and analysed in Section 4 through a series of experiments and comparisons to the recent research. In Section 5 the paper is concluded.

2. Related work
Anomaly-based IDS are usually considered to be the best option (as compared to the signature-based IDS) because of their ability to detect zero-day attacks. In this section, we will discuss the existing anomaly-based IDS in general and also the ones that have been proposed for the IoE and SG architectures.

In IDS, machine learning algorithms are commonly used to perform statistical analysis and pattern matching in an attempt to determine an anomaly. The goal of the algorithm is for the system to learn complex behaviours in the network traffic without the need for a human to have any kind of interaction. Two main types of machine learning algorithms are supervised and unsupervised learning. Supervised learning is where the system is trained using labels, meaning the data has already been tagged with the correct outcome and needs sorting into the appropriate class based on the labels. Unsupervised learning is where the system is trained without any knowledge of the required outcome, this method typically looks to group the data based on commonalities in the data, this is typically called clustering and can be used before classification to assign labels. Some of the popular machine learning based IDS are discussed below.

Bayesian Networks are used in anomaly-based IDS, a popular one is known as Bayesian Networks that use probability distribution for a series of variables to create a network. It has been used to reduce false positives by looking at the probability of features appearing in a particular attack [5], which improved detection rate of Root to Local attacks (R2L) to 85.35%. Naïve Bayes was also used as a classifier in a hybrid IDS with a K-mean clustering algorithm [32], using the labelled output of the K-Mean to later classify data based on the probability they will fall into a cluster by using the features of the network traffic, producing accuracy results of 92.12%. In [27], an extension of the Naïve Bayes is proposed called the Hidden Naïve Bayes [22] that adds a hidden layer looking at the combined influence of features, relaxing the independence assumption of the Naïve Bayes. This improves the accuracy of the Naïve Bayes by 15%. Using probability to predict network attacks certainly has promising applications due to the links between certain features and attacks.

In [26], the Decision Tree (DT) ability to process large amounts of data efficiently is used to sort data into groups so that a SVM can classify the smaller subsets of data. In [33], a similar method is proposed; however, an SVM is placed on each node in the DT. In [43] a combination of DT and Neural Network is proposed to create a NueroTree where the DT classifies data based on the output of a Neural Network, this produces a 6% accuracy improvement on a standalone C4.5 DT. The ability for DTs to group data using a simplistic and easy to understand approach provides a great opportunity for anomaly detection, this ability to efficiently achieve this task means employing more computationally intense algorithm such as the SVM can be used to work on smaller less complex subsets.

In [24], the authors use Genetic Algorithm (GA) and generate rules to detect network attacks with a fitness score assigned to how well they do at anomaly detection, new generations are created using the results from the produced rules, when tested on multiple specific attacks the detection rates were consistently above 90%. The GA was used as a feature reducer in [46], where the GA was used to find the optimal weight of a feature when looking for high accuracy in the anomaly detection in network traffic, that was later used to influence the distance in a K-Nearest Neighbour classifier to improve DoS detection with results of 97.4% accuracy for unknown and 78% accuracy for known attacks. A GA was also used alongside the previously mentioned NeuroTree [43], this method uses the feedback from the NeuroTree to create new generations of feature subsets, these new features will then be used on the new NeuroTree and this process will continue giving feedback to the GA to produce better feature subsets. The GAs overall ability to find the optimum solution in a large search space means there is an obvious use for the GA in the feature selection, an important step in making anomaly detection efficient.

In [16], the Deep Belief Network (DBN) is used alongside an SVM as a feature reduction measure, by using multiple layers to find correlations in the features of the network traffic, new features can be created to reduce the overall feature set and help the SVM perform more efficiently, this method can be seen in Fig. 3 where the original feature set X is reduced to h1 and then h2 before being supplied to the SVM.

A similar method to this is proposed in [40]; this method however uses the NSL-KDD dataset, specifically used in IDS research and produces accuracy of 92%, a 4% improvement on a standalone SVM. The ability of the DBN to deal with large complex datasets and produce more simplistic models means this algorithm can be used on large high dimensional network traffic datasets.


Download : Download high-res image (148KB)
Download : Download full-size image
Fig. 3. DBN-1SVM approach proposed in [16].

A method proposed in [48], called the Triangle Based Nearest Neighbour Approach (TANN), uses the K-Nearest Neighbour(K-NN). TANN uses the K-mean to cluster into 5 clusters representing the 5 attack types in the KDD Cup 99 Dataset (Normal, DoS, U2R, R2L and Probing). The data point being classified and the 5 centroids from the K-mean clusters are then used to create a series of triangles that are used to create new features that lower the dimensionality of the classification problem, as shown in Fig. 4.

The same K-mean and K-NN combination is used in the Centre and Nearest Neighbour Approach (CANN) [30], in this method the same 5 clusters are created and the distance to all centroids and data point in the same cluster is calculated, the sum of which is then used to create a new distance-based feature to again reduce the dimensionality of the classification problem. This improves training and testing times over a standaloneK-NN and produces accuracy and detections rates of over 99%, with the TANN approach producing only 84.67% accuracy. The CANN does however have false alarm rates of 2.6%, more than a standalone K-NN. The simplicity of a K-NN makes this algorithm very useful when dealing with problems as complex as network traffic. However, the fact that each new data point to be classified needs to measure the distance to all other points makes this algorithm very memory and time intensive on large datasets. Using this classifier with a clustering algorithm can be harnessed on smaller datasets, for example running the K-NN on individual K-mean clusters.

In [48] and [30] the K-means algorithm is used with aK-NN classifier to reduce dimensionality by creating single features based on the centroids created by the K-mean and the distances measured in the K-NN. In [42] the algorithm is used to label data so that an SVM can classify the data in a supervised manner. In [38], the K-means is used alongside an SVM with 4 clusters created and then the features reduced based on the cluster to improve the performance of the SVM. TheK-means simplicity makes it a very popular algorithm for clustering as the decision over clusters is done by calculating the distance to the number of centroids, therefore the complexity lies in the proportion of k and not the size of the dataset unlike the K-NN. Current research in IDS does not utilize the K-means use of cluster to produce tight clusters, our proposed KSVMeans uses the K-means cluster centroids to represent the cluster rather than the labelled output of each cluster, thereby lowering the size of the training dataset for a classifier.


Download : Download high-res image (129KB)
Download : Download full-size image
Fig. 4. TANN [48] method showing 5 K-mean clusters and unknown data point Xi.

As discussed before in this section, the SVM has been used as a classifier in combination with various other algorithms, in [26] and [33] it is used with a C4.5 DT to classify the small subsets of data produced on the leaf nodes. In [42] the SVM is used with the K-means clustering algorithm to classify the network traffic into normal and anomalous traffic based on the labelled output of the K-means. SVMs use different kernels to produce different outputs, for example the linear kernel is used to create a straight separation between two classes, whereas a Radial Based Function (RBF) kernel is more flexible and is not confined to a perfectly straight line. This flexibility allows the SVM to be adaptable to almost any problem, however it does struggle with large complex datasets.

An IDS has been proposed in [35] for the complete ecosystem of SG. Their proposed architecture consists of three components: SVM, ontology knowledge base, and a fuzzy risk analyzer. The IDS is distributed across host and networked based components of the SG. The SVM model is used to detect attacks and trained for 30 h on a dataset of 3600 records of attacks. The ontology knowledge base includes KDD 1999 dataset and simulated experiments from the authors. A fuzzy logic-based technique is used to mitigate the false positives and to determine a risk value for each component in the SG environment.

The authors in [6] targeted the Supervisory Control And Data Acquisition (SCADA) systems in SG for intrusion detection and compared four ML algorithms (i.e. Random Forest, SVM, K-NN, and K-means) for anomaly detection using a Modbus/TCP dataset thereby dividing it into three sub-datasets each having different set of records of normal and malicious data. The four ML algorithms are trained on specific features extracted that concern only the TCP/IP stack.

An anomaly based IDS is proposed in [51] for the IoE architecture which is based on state uncertainty evaluation model. Deep learning is used to detect attacks based on the deviation of estimated or predicted state interval. The main focus is on the injection attacks. The authors claimed promising results from their performance evaluation.

Jokar et al. [23] proposed an IDS for the ZigBee-based Home Area Network (HAN) in the SG architecture. The IDS is the amalgamation of specification and anomaly-based models. Various features from Physical and MAC layer have been collected and utilized from the network traffic; hence, attacks related to these two layers are detected only. The system is composed of various agents that monitor network behaviour. There is also a central Intrusion Prevention System (IPS) that analyzes the network traffic for specific features to be used for attack detection. The IPS is a centralized entity which continuously communicates with the set of agents. In case of any abnormal behaviour, the IPS uses a reinforcement learning technique to respond with an appropriate action. The system evaluation includes only theoretical analysis of six attacks against the network.

In this paper we proposed a scheme, called KSVMeans, that uses the SVMs ability to classify data and combines it with the K-means efficient clustering and use of mean centroids to create a very small training set to train the SVM in order to reduce computational complexity and time. When looking at all recent research, it is evident that it tends to neglect time complexity of IDS in a large number of cases and focuses on accuracy. Due to the IDS requirements of being used in a real-time network on a SG or IoE architecture, such as AMI, the time complexity is a very important aspect of any anomaly-based IDS. Because of this, the proposed solution to anomaly-based IDS presented in this paper, the KSVMeans, looks at the trade-off between accuracy and time complexity, and looks to propose a solution that is fast and not time intensive as well as not losing accuracy when compared to recent anomaly-based IDS research.

3. The proposed approach
3.1. System overview
In our proposed solution, a machine learning algorithm is developed to detect attacks, specifically DoS and Probing attacks in network traffic whilst reducing the processing time. TheK-means algorithm is a clustering algorithm which can produce tight clusters from unlabelled large datasets, such as network traffic. The SVM on the other hand can be extremely accurate when dealing with smaller lower dimensional datasets. For this reason, we see the use of the K-means algorithm on the dataset creating 3 smaller clusters which will represent normal data, DoS data and Probe data as a logical first step to the data processing before classification. The focus of this research is on DoS and Probe traffic and therefore we cluster the U2R and R2L traffic into the normal cluster due to the similarities between them as discussed in [32]. The centroid of each cluster is then used as the symbolic set of features for that attack group, these 3 centroids are then used to train 2 SVMs. The first SVM will separate normal traffic from attack traffic, whilst the second SVM will then separate the attack traffic into DoS and Probe traffic, finishing with 3 classes of traffic representing normal, DoS and probe traffic. This is a promising solution to the problem of spotting anomalies in network traffic due to the dual processing complexity reduction of reducing classifier training down just to 2 records, as opposed to a percentage of the training data used in most methods that can reach hundreds of thousands of records. The proposed KSVMeans model is split into 3 modules, the first is the pre-processing module where the data is normalized. The second module is the K-means clustering module and finally the classification module using an SVM. A diagram of the modules can be seen in Fig. 5, which shows the full solution diagram, starting on the left with the KDD Cup ’99 dataset being split 80/20, with 20% of traffic being used for K-means clustering to produce 4 centroids for SVM1 and 5 centroids for SVM2.

3.2. Normalization
Due to the various scales that are involved in the high dimensional complex dataset that is produced using network traffic, it is important to normalizethe data so that there are no differences in the scales. If this was not done, then the scales which used large numbers would have more influence than say a binary feature whose maximum value would be a 1. This is done by using the following function (Eq. (1)): (1)
 
where x  (
, 
) is a list of values for each feature, 
 is the feature being normalized and 
 is the normalized value. Doing this for every feature will put them all on the same scale and make them directly comparable without one feature being weighted higher than another. It is important that the normalization takes place before the clustering stage to stop the clusters being biased towards certain features.

3.3. K-means clustering module
Clustering is the unsupervised classification of patterns into smaller subsets, clustering effectively organizes a collection of data into logical clusters based on the features available to the clustering algorithm. The K-means algorithm [13] is considered to be one of the simplest algorithms which can be used for unsupervised learning tasks such as data clustering. The algorithm works by first pre-defining a value for k that specifies the number of clusters that the data will be sorted into. The clusters are then created by assigning data to the nearest cluster by measuring the distance to the centroid (mean data point) using the Euclidian Distance function. For any two instances with n-features, for example X  (x1,…,x
) and Y  (y1,…,y
) the Euclidian Distance function is shown in Eq. (2). After the data points are assigned to a cluster the mean is recalculated and the process continues until the centroid remains stable, this process can be seen in illustrative form in Fig. 6 [48]. (2)
 The goal of the K-means can therefore be described as finding k clusters in a given dataset N, where the algorithm must first find k number of data points to represent the dataset in relation to the k required clusters. Therefore, the algorithm is used to cluster N data points into k clusters represented by 
 containing 
 data points. So as to minimize the within cluster sum-of-squares, in other words the distance from each point to the centroid of its cluster [21], as shown in Eq. (3): (3)
where 
 is a feature vector representing the  data point and 
 is the geometric centroid that is the mean of data points in cluster 
.

The proposed KSVMeans technique uses hard clustering, that is, every data point or vector in the vector space is assigned to one cluster only and will have no membership of any other cluster. The alternative to this is fuzzy clustering where vectors are assigned a percentage of membership to each cluster, the KSVMeans does not use this option as there will never be a case where network data should be considered as part of more than one cluster. The KSVMeans will also be configured with a value of 3 for the variable k. That is the K-means algorithm in the KSVMeans will try and cluster the data into 3 distinct groups. An alternative to this can be to use a similarity metric to analyse the clustering quality depending on the number of clusters selected. The similarity metrics used to evaluate the clustering quality can be seen in Eqs. (4), (5) [53]: (4)
μ
 
(5)
μ
μ
 where 
 is the average distance of each member i to μ
 which is the centroid of the same cluster 
, and 
 is the minimum distance between any two centroids, 
 is the input element j of member i, 
μ
 is input element number j of centroid μ
, N is the total number of vectors and F is the number of dimensions a vector has (see Eq. (6)) [53]: (6) 
  
 
 
The minimum validity ratio can be used to get the best number of initial clusters, where “0” is the validity ratio for evaluating different number of clusters. Considering the domain knowledge and the knowledge of the problem, the KSVMeans will use a pre-set value of 3 for k, representing the normal data, probe data and DoS data.


Download : Download high-res image (451KB)
Download : Download full-size image
Fig. 6. The K-means process.

These 3 centroids used in the initiation of the K-means algorithm can be chosen, or randomly generated. Random centroids may not produce the desired result as their selection is random with no knowledge of the data being processed. Alternatively, if the centroids are chosen by an expert they may be chosen poorly and cause the algorithm to not cluster into the desired categories. The KSVMeans uses randomly generated centroids as the value of k is low and therefore there is enough vector space for the centroids not to converge indefinitely, this approach also shows that the algorithm working with the real data rather than being guided by the configurators knowledge which may not be accurate, leaving the algorithm to calculate the best clusters.

K-means Steps

1.
Choose the initial values of k and the centroids to represent the initial 3 clusters.

2.
Assign each vector to the closest centroid.

3.
Using the new membership of each cluster re-calculate the centroid for each cluster.

4.
If there is no reassignment of vectors or there is a minimal decrease in the squared error then stop, else go to step 2.

Once this process has been finished there should be 3 clusters representing DoS, Probe and normal data with 3 centroids (one in each clusters) representing the mean of each cluster, the features of which are considered the average for that cluster, for example the DoS centroid will contain 41 features, each of which will be the average value of that feature from all other DoS data points in that cluster. That is, where the dataset X, contains data points represented by 
 with 41 features {
, 
, 
,…, xi40, 
}, the centroid C will also contain 41 features, where 
 will be the mean of all 
 in the corresponding cluster. As these centroids contain the features which are symbolic to their clusters, we ignore all data points and use only the centroid of each cluster to train the SVMs in the next module, leaving us with 1 data record containing 41 features which represents the whole of that attack type in the training of the SVM.

3.4. SVM classification module
SVMs are supervised learning methods that are used in machine learning to analyse data to be classified into separating groups. They are binary classifiers which mean that they are separated into two separate groups at a time. This is done by finding the best separating hyperplane which can also be known as the decision boundary that makes the decision over whether a particular datum is considered normal or an anomaly. An example is shown in Fig. 7. This shows the best separating hyperplane which is the line labelled A, with the two B lines representing the distance of the separating hyperplane to each nearest data point known as support vectors on both sides. The idea is to create the widest street between the two classes. Firstly, we need to decide a rule to use as the decision boundary. The decision rule for the example in Fig. 7 would be as follows; we adopt the SVM for the following formulation: (7)where w is the vector in Fig. 7, constrained to be perpendicular to the hyperplane, u is an unknown vector in the dataset which is being classified and b is a constant. If the dot product  b is more than 0 then the data u is in the square class. As the goal is to classify data into one of two classes we will use this to calculate if the data is above or below a given threshold. Due to the separating hyperplane being the line with the biggest gap between alternate classes we end up with 2 lines each side of the separating hyperplane, which can be called the support lines on which our support vectors will lie, shown as C in Fig. 7. These support lines will be on either side of the separating hyperplane, equal distance from it. The goal is often to score more than 1 or  to determine the class in which to classify the data, commonly described as positive class and negative class. Using the decision rule earlier we can therefore determine that a positive vector x represented as 
 can be classified using Eq. (8): (8)
And a negative vector x represented by 
 can be classified using Eq. (9): (9)
Adding a variable 
 such that 
 is equal to 1 for a positive vector 
 and  for a negative vector 
. Multiplying both of the previous Eqs. (8), (9) by 
 makes this process more mathematically convenient as both now equal, see Eq. (10): (10)
Therefore (Eq. (11)): (11)
And for support vectors this will equal 0 (Eq. (12)): (12)
It is also important to make sure that the two support lines have the maximum width either side of the separating hyperplane whilst remaining equal distance from the hyperplane. This can be done using a support vector from each hyperplane support line, represented by 
 from the  support line and 
 from the  support line. Therefore, the width will equal (Eq. (13)): (13)
 
We want to minimize the weight vector which will in turn maximize the margin between support lines. To do this we use the Lagrange Multipliers to find an extremum of the function, this will give us a new expression from which we can derive the maximum, i.e. to get the maximum width between the support lines, to get this we use the Lagrange Multipliers (Eq. (14)): (14)
 
where 
 is a Lagrange Multiplier. We use this value of F to differentiate between variables, firstly w using Eq. (15): (15)
 
 which shows that the value of w used earlier can be derived from a linear sum of the samples used (Eq. (16)): (16)
 which we can put into our earlier decision rule seen in Eq. (13), we get the result in Eq. (17). (17)
Another variable we need to differentiate F is b, shown below (Eq. (18)): (18)
 
 which implies (Eq. (19)): (19)
If we substitute these findings in Eqs. (16), (19) into the earlier Eq. (14) we get (Eq. (20)): (20)
 
 This can be used to get the maximum distance between the two support lines. We can use these equations to measure the separating hyperplane and the support lines using the support vectors, which tend to be the hardest to classify and lie the closest to the separating hyperplane on either side. We can then assign new unknown, unclassified and unlabelled data to either the positive or negative side representing normal or attack data depending on the output of Eq. (11). We will ultimately end up with two classes separated by the hyperplane, this SVM method is used in 2 separate SVMs in the KSVMeans, and these are discussed later in this section in more detail.

SVMs have the option of using multiple different kernels, which is a function provided to the machine learning algorithm to classify the data, different kernels are capable of producing different results. The two main methods used in SVM research are the hyperplane-based approach proposed by Williams et al. [52], where the best separating hyperplane is found between classes in feature space, specifically the best separating hyperplane is the line with the biggest margin to support vectors at either side that successfully separates the data into the correct classes. This is the method discussed above and the method used in the KSVMeans. Another method was proposed by Duin and Tax [47], this method proposes the Support Vector Domain Description (SVDD), a one-class SVM which uses the sphere with the smallest radius approach, where instead of finding a separating hyperplane the goal is to find a minimum radius with data outside the radius being anomalies. Both of these approaches can be seen in Fig. 8, showing the input space (on the left) with the mapping to both kernel functions (on the right), showing the SVDD and the Proximal Support Vector Machine (PSVM).

A comparison of these two methods on various datasets is given in [16]. This can be used as a guideline of how kernels perform in both accuracies, i.e. indicated by the Area Under Curve (AUC), and performance in training and testing speeds. In Table 1 the two approaches are shown that are tested in a linear kernel method and a non-linear kernel method, called the Radial Based Function (RBF). This research gives a good idea of how the methods perform, and how they perform on different kernels, looking at the averages on Table 1 we can see in terms of testing and training times, the linear methods outperform the RBF kernel. The best performing algorithm in training was the linear-PSVM with a training time of 0.0421, whereas the best testing time was the linear-SVDD with a testing time of 0.0016, although this is only a marginal improvement over the linear-PSVM which had a testing time of 0.0058. Despite performing less on performance times, the RBF kernel outperformed the linear kernel on accuracy, with results of 0.8838 and 0.8850 for the RBF-PSVM and RBF-SVDD, respectively. This is a clear improvement over the linear methods which had an accuracy of 0.8125 for the linear-PSVM and 0.8137 for the linear-SVDD. A linear kernel is a straight line that will separate the normal data from anomalies like discussed previously, this kernel is used in the KSVMeans and is backed up by the positive performance results shown in Table 1, alternatively an RBF kernel, also seen in Table 1 and also used as a baseline when testing the KSVMeans does not stick to a straight line and can curve slightly if needed.


Download : Download high-res image (166KB)
Download : Download full-size image
Fig. 8. Input space mapped to SVDD and PSVM approaches comparison [16].

Analysing the results of this research, it is clear that the linear methods provide a better processing time whilst the RBF methods show improved performance. Considering the KSVMeans has the primary goal of processing time reduction, the linear kernel will be used in the KSVMeans solution. The simplicity of the training data using in the proposed solution [16].


Table 1. Comparison of PSVM and SVDD method in both linear and non-linear kernels.

Kernel	Hybrid	Method	Metric	Datasets								Avg.
Forest	Banana	Adult	GAS	OAR	DSA	Smiley	HAR	
Linear		PSVM	AUC	0.83	0.77	0.81	0.89	0.87	0.78	0.75	0.80	0.8125
AUC
0.07	0.11	0.08	0.08	0.12	0.06	0.09	0.15	0.0887
Train	0.0046	0.0121	0.0073	0.0857	0.0722	0.0156	0.0605	0.0791	0.0421
Test	0.0018	0.0029	0.0036	0.0063	0.0067	0.0085	0.0086	0.0082	0.0058
SVDD	AUC	0.83	0.78	0.81	0.89	0.87	0.79	0.74	0.80	0.8137
AUC
0.05	0.13	0.07	0.03	0.09	0.07	0.13	0.11	0.0850
Train	0.0743	0.2505	0.1880	0.3175	0.2764	0.8314	1.9246	1.6288	0.6864
Test	0.0019	0.0012	0.0015	0.0027	0.0004	0.0013	0.0025	0.0016	0.0016
RBF		PSVM	AUC	0.97	0.92	0.86	0.91	0.90	0.85	0.79	0.87	0.8838
AUC
0.02	0.03	0.05	0.04	0.05	0.03	0.08	0.09	0.0488
Train	0.0254	0.0464	0.0611	0.0606	0.0475	0.1516	1.0071	0.3548	0.2193
Test	0.0091	0.0085	0.0286	0.0132	0.0090	0.0275	0.0976	0.0763	0.0337
SVDD	AUC	0.97	0.92	0.87	0.91	0.91	0.84	0.78	0.88	0.8850
AUC
0.02	0.04	0.02	0.04	0.06	0.05	0.05	0.07	0.0350
Train	0.0192	0.0201	0.0423	0.0411	0.0165	0.0664	0.5422	0.2562	0.1255
Test	0.0085	0.0986	0.0186	0.0123	0.0136	0.0243	0.0885	0.0828	0.0434
The classification module of the KSVMeans is split into 2 SVMs, these are SVM1 which issues to classify normal traffic from attack traffic containing DoS and Probe data, the second SVM, called SVM2 is used to classify DoS traffic from Probe traffic. Each SVM is described below. The three centroids outputted by the K-means algorithm are calculated to create new centroids which will be used to train the SVMs, Fig. 9 shows how these new centroids are calculated, with the original centroids on layer A and the new centroids on layer B. When two centroids are combined, the result is the mean of the two contributing centroids.

The classification module is split into 2 SVM’s, the first SVM which is called SVM1 classified the clustered labelled instances: normal+U2R traffic as normal and others as R2L, DoS and Probe traffic. The second SVM is the same as the first SVM in working but it classified the data from 2 clusters: Normal, U2R and R2L and DoS and Probe including the features used with the only difference being the centroids used to train the second SVM.


Download : Download high-res image (95KB)
Download : Download full-size image
Fig. 9. The output of centroids being combined to create the final classification centroids.

SVM1 – Normal+U2R vs. DoS vs. Probe vs. R2L

This SVM will use the KSVMeans classification features on the training centroids and will classify all 20% of the testing data provided. The Normal+U2R centroid and the DoSProbe and R2LProbe centroid are used to train this SVM, this is because we are trying to separate normal traffic from DoS and probe traffic, and therefore to classify DoS and Probe, we calculate the mean of these centroids and use that to train the SVM. This use of centroids will be tested in the experimentation section later in this paper. Any traffic that is classified as normal will be moved to a normal class and removed from the process, however the attack traffic is moved to the second SVM.

SVM2 – Normal+U2R+R2L vs. DoS vs. Probe

The second SVM is trained using the DoSProbe centroid and the Normal, U2R and R2L centroid, the reason the NormalU2R and R2L centroid is used is because the NormalU2R and R2L share similarity. Therefore, to add a slightly bigger difference without losing the effect of the U2R and R2L centroid features it is combined with the normal centroids to create a bigger margin between the two classes. This provides better results when tested. Several configuration-based decisions had to be made when configuring the SVM in the KSVMeans, these are discussed in detail below.

The use of centroids outputted from a K-means algorithm to train an SVM classifier has not been used before in anomaly-based IDS, to the best of our knowledge. This makes the centroid training method unique to this research in the classification of network traffic. We retain the information on all the features as we still have the mean of the features of all data points that were grouped in the clusters. However, we can now train the SVM classification algorithm using only 2 training records. This is opposed to training an SVM on the labelled output of the K-means clusters, which is typically a percentage of the total dataset, in the case of the KSVMeans that would be 80% training data of the KDD Cup 99 dataset.

4. Experimentation and results
This section explains the experimental analysis and results of our intrusion detection approach. We also provide a comparison with existing studies carried out for intrusion detection. We use the KDD’99 dataset for experimentation. We apply K-means and SVM algorithm to detect the normal traffic and intrusion attacks (i.e., DoS, Probe, R2L, and U2R).

4.1. Experimental setup
The proposed solution was tested using python and MATLAB, in this software the proposed solution was created using 4 MATLAB files, a K-means clustering class, a normalization class and 2 SVM classification classes. The proposed solution only covers DoS and Probe detection, however the SVMs can be easily adapted to detect any kind of attack by changing the centroid combination. The proposed solution clusters the data in the first K-means class and outputs four centroids (Normal, DoS, R2L and Probe). These are saved to a master centroid matrix whilst an SVM1 centroid matrix calculates the mean of the DoS and Probe centroids and makes this the centroid against which normal data will be classified. The normal data is then saved to a normal data matrix and the attack data is saved to an attack data matrix to be input to the SVM. The master centroid matrix is then referred again to produce the centroids for the second SVM which will be the probe centroid and the mean of the DoS and normal centroids. Fig. 5 shows the use of different centroids for classification the impact had on SVM1 and SVM2.

4.2. Dataset: KDD Cup ‘99
The KDD Cup ’99 dataset [49] is a dataset produced for researchers working at network intrusion detection. This dataset is labelled and produced using military network simulations exploiting three target machines running multiple OS and applications. The network traffic was collected using a TCPdump style packet sniffer over a 7-week period and the data contains normal traffic as well as the 4 attack types discussed earlier in the report and used several times in the research discussed (DoS, Probe, U2R and R2L). The KDD dataset contains 41 features per connection [17].

All the experiments carried out in this section were done using the full KDD Cup ’99 dataset containing 1,048,576 rows of data and 41 features with an additional 42nd feature containing the label for the data. This dataset was chosen due to its frequent use in anomaly-based IDS research. This makes the results comparable with other solutions as well as allowing other researchers to test against this method themselves with a familiar dataset.

4.3. Measuring metrics
Having already discussed the basic tasks and architecture of the IDS, we now discuss the process of evaluating our proposed IDS that carries out analysis of traffic captured. A common way to describe the effectiveness of these approaches is by knowing how many situations the system gets correct, or incorrect classifications of a given piece of data. These can be described as:

1.
False Positives (FP): If the system produces a false positive, then it has classified a harmless piece of data as being an attack. This can cause wasted resources analysing this situation which is ultimately not worth analysing.

2.
False Negatives (FN): A false negative in a system will mean that some malicious activity in the network has been deemed to be harmless, meaning that the attack will be able to penetrate the network unnoticed.

3.
True Positive (TP): A true positive will mean the system has correctly identified an attack as malicious and the correct actions can be taken to mitigate this.

4.
True Negative (TN): A true negative is when the system correctly identifies the network traffic as having no malicious content, ignoring it and moving on to the next section of traffic to analyse.

It is common in research to analyse proposed methods using Accuracy, Recall, F-Score, Detection Rate and False Alarm Rate (Error Rate), these can be calculated using the formulas given below: (21)
 
(22)
 
(23)
 
(24)
 
(25)
 
 The ideal is to maximize the true positives and true negatives whilst having as little false negatives and false positives as possible. However, this is often impossible with it becoming more about a trade-off between having more false positives or more false negatives in an attempt to maximize​ true positives and true negatives.

4.4. Experiments
To test the KSVMeans to see how the proposed algorithm performs, several experiments have been carried out using the proposed solution to see its effectiveness. These experiments are described below one by one with the results discussed. The results are compared to several baselines used such as the SVM and K-NN algorithms as well as comparing the results to other research carried out in various state-of-the-arts.

4.4.1. Experiment to Assess Different Values of K in the K-means
In this experiment, we look at the impact of different value of K in the K-means algorithm, in the proposed solution several methods were considered, looking at an initial clustering of ,  and , results shown in Tables 2, 3, and 4, where in , a normal cluster is created and 4 clusters representing the 4 attack types. In  the same clusters are created however U2R and R2L traffic are clustered into their own group because of their similarities, and in  normal traffic and U2R/R2L traffic are all grouped into 1 clusters due to the similarities in these types of data as discussed in [32]. To gauge which of these would perform the best clusters for the first SVM, a 100% of the KDD-Cup ’99 dataset was clustered using each of these settings. Each configuration was used for 5 iterations of the same clustering algorithm, an unweighted feature is added to the starting centroids which corresponds to the cluster it represents.

For example, in  the starting centroids would represent the Normal  U2R/R2L cluster, with an unweighted feature value of 0, a DoS attack cluster with an unweighted feature of value 0.5, the same as the scale added to the end of the centroid. The unweighted value is also added to the training data, for example all DoS data will have the unweighted feature 0.5. The features are unweighted so that they do not have any bearing on the final clustering. However, as the centroid is calculated using the mean of all the data points in that clusters. Therefore, it will contain the mean of the unweighted feature, giving a clear indication of how much of the data is confusing with other clusters. That is, if the DoS cluster end up with mainly Normal U2R/R2L traffic in it, this unweighted feature on the final centroid will get closer to 0 and further away from the desired 0.5.

The results of this experiment are shown in Fig. 10 where a comparison of K-means duration and in Fig. 11, where the accuracy of the clustering is compared with respect to varying k. The results indicate that the lower number of initial clusters configured by the K-means algorithm improves the time complexity and accuracy of the clustering process. As shown in Tables 2, 3 and 4, The K  3 configuration provides the best time duration with a time of 0.288398, over 2 times faster than the next best time of K  4 with a time of 0.708446. The worst was the  configuration which had 0.817078 average time duration. This algorithm duration being lower with less clusters is due to the lowered amount of computations involved. This does not appear to have an effect on accuracy as  which also shows the highest accuracy with 90.53%, the second-best accuracy being the  with 75.3% and the third highest with an accuracy of 53.58% on . This makes  the highest in accuracy and lowest in processing time meaning the chosen configuration of  is the best option for this method.


Table 2. Results for  clustering test.

Iteration	Finishing X value for cluster N (normal traffic)	Finishing X value for cluster D (DoS traffic)	Finishing X value for cluster P (probe traffic)	Finishing X value for cluster U (U2R traffic)	Finishing X value for cluster R (R2L traffic)	-means clustering accuracy (%)	-means time duration (s)
Start/Ideal	0	0.25	0.5	0.75	1	100%	–
1	2.0584e−04	0.25	−.4992	0.1346	0.4997	88.99%	0.94017
2	2.16617e−05	0.25	0.5010	0.0114	0.3561	85.20%	0.71999
3	1.8021e−04	0.25	0.5002	2.2910e−04	0.1348	57.76%	0.71003
4	0.0011	0.2623	0.4989	0.0018	0.4998	59.38%	0.91553
5	2.6617e−05	0.25	0.5010	0.0114	0.3561	85.19%	0.79967
Averages	0.0003	0.25246	0.30038	0.031886	0.3693	75.30%	0.817078

Table 3. Results for  clustering test.

Iteration	Finishing X value for cluster N (normal traffic)	Finishing X value for cluster D (DoS traffic)	Finishing X value for cluster P (probe traffic)	Finishing X value for cluster UR (U2R/R2L traffic)	K-means clustering accuracy (%)	K-mean time duration (s)
Start/Ideal	0	0.33	0.66	1	100%	–
1	0.0032	0.3264	0.6591	0.0356	60.23%	0.69877
2	0.0012	0.3473	0.6591	0.0022	59.67%	0.80112
3	2.2861e−04	0.3300	0.6592	0.1770	64.62%	0.65734
4	0.0016	0.3455	0.6599	0.6599	94.68%	0.66767
5	0.0022	0.3475	0.6591	0.0020	60.71%	0.71733
Averages	0.0016	0.3393	0.6592	0.1753	53.58%	0.708446

Table 4. Results for  clustering test.

Iteration	Finishing X value for cluster N (normal traffic)	Finishing X value for cluster D (DoS traffic)	Finishing X value for cluster P (probe traffic)	K-means clustering accuracy (%)	K-mean time duration (s)
Start/Ideal	0	0.5	1	100%	–
1	5.8856e−04	0.5282	0.9938	97.24%	0.31188
2	1.4329e−05	0.1060	0.5313	63.73%	0.28447
3	5.8856e−04	0.5282	0.9938	97.24%	0.28113
4	5.8856e−04	0.5282	0.9938	97.24%	0.23091
5	5.8856e−04	0.5282	0.9938	97.24%	0.33360
Averages	4.74e−04	0.4437	0.9013	90.53%	0.288398
4.4.2. Experiment analysing the results of the KSVMeans whentrained on different size training data
In the second test, the difference in dataset sizes was looked at by comparing 10%, 20% and 30% training dataset sizes. The aim of this test was to see the effect on the training set size to the accuracy and speed of the K-means algorithm as well as the effect on the overall performance of the proposed KSVMeans solution. Fig. 13 shows the performance accuracy of the K-means algorithm on each of these datasets, the highest accuracy was the 20% training data with an accuracy of 97.24% and the 30% training dataset with 94.77%. The 10% training dataset had an accuracy of 89.21% showing it to be the worst accuracy out of the three. Fig. 14 shows the duration of each training data size, the 10% training data had the lowest time with 0.382229 s followed by 20% with 0.6 s and then 30% with 0.745625 s. This shows that more training data will take longer to cluster. Combining these two results show the most consistent size of training data is 20% with it being 1st in accuracy and 2nd in time duration, meaning the proposed KSVMeans solution is using the optimum amount of training data to produce the best results.

Fig. 12 depicts the optimal value of K. The elbow method runs k-means clustering on the given dataset for a range of values of k, in this case it is 14 and then for each value of k computes an average score for all clusters. By default, the distortion score is computed, the sum of square distances from each point to its assigned centre. The value of having least sum of square of distance is chosen as optimal value of k. By looking at our previous and this elbow method it is clear that 3 is the optimal value.


Table 5. Overall effect of training data size on kSVMean approach.

SVM1 – Normal vs. DoS/Probe	SVM2 – DoS vs. Probe
10% SVM1	20% SVM1	30% SVM1	10% SVM2	20% SVM2	30% SVM2
Train time (s)	0.224883	0.199560	0.317010	0.070000	0.07117	0.065809
Test time (s)	0.012136	0.010807	0.0011079	0.010458	0.010082	0.010635
Accuracy	96.21%	97.26%	95.99%	96.41%	96.14%	51.08%
Detection rate	98.27%	98.22%	99.62%	99.57%	99.67%	97.75%
Error rate	2.20%	0.02%	0.47%	18.39%	14.13%	30.42%
TP	566 017	577 491	555 939	447 185	434 247	238 022
FP	9948	10 411	2129	1911	1427	5479
FN	29 781	18 307	39 859	15 034	16 331	234 472
TN	442 830	442 367	450 649	8481	8669	12 535
Table 5 shows the statistics from the 2 SVMs in the KSVMeans IDS solution when trained on the different percentages of data. The ideal is low training and testing times, high accuracy, detection rate, true positives, true negatives, low error rate, false positives and false negatives. The best result for each SVM and each statistic is highlighted in bold, this shows the SVM1 has more favourable statistics when trained on the output of theK-means tested on 20% training data. The worst performance on SVM1 is the 10% training data which does not perform better than the 20% and 30% on any of the measured performance statistics. On SVM2, network traffic separated from each other, the 20% training data has the most favourable statistics once again, performing best on 4 metrics. Comparing the testing of different size datasets on the K-means and the KSVMeans as a whole show that using 20% training data gives the best in both clustering performance and overall classification performance, with good clustering results and overall accuracy results show that 20% training data is highly efficient.

4.4.3. The effects of the proposed centroid training method
In this test, the main aim of the KSVMeans is tested, the main aim of the solution is to create an algorithm which is fast and efficient whilst maintaining accuracy. In this test, we looked at testing the training and testing times of the proposed solution. This method uses a KNN trained on the proposed centroids training method as a baseline, represented as ‘K-means+KNN-Centroid’. We also use an SVM trained on the full 20% output of the K-means, represented as ‘K-meansSVM-20%’, as well as a KNN trained on the full output which is shown as‘K-means+KNN-20%’. The test was run using the normal configurations for the KSVMeans algorithm, including the previously discussed 20% training data and tested on the full KDD Cup ’99 dataset with 1 048 576 records. The results of this experiment can be seen in Table 6 where the averages from training and testing of the 2 SVMs were taken over 10 iterations to produce the results and are used to calculate a total classification time as well as the accuracy of both classifiers combined to show performance as well as time complexity.

In Table 6, we can see the significant time difference between methods that use the K-means output to train the SVM and those that just use the centroid training method proposed in this research. The slowest classifier was the K-meansKNN hybrid which was trained using the labelled output from the K-means clustering algorithm. This method took 18 min and 20 s. This length of time is no surprise as the K-NN is known for not being efficient on large datasets [28]. The K-means+SVM hybrid trained on the labelled output of the K-means also struggled with the slowest training time of the whole test showing the problem the SVM has with training using large datasets [16]. One of the main contributions of this research, the use of centroids to train classifiers is shown to be massively beneficial in this experiment and when compared to the previous methods that are trained using the labelled K-means output, they perform substantially better. The proposed solution of using a K-means+SVM hybrid using the centroid of the K-means to train the SVMs shows a duration after both SVMs of 0.34674 s, this is a number that is 1367 times faster than the K-meansKNN-20% method and 637 times faster than the K-measnSVM-20% hybrid, showing that this approach has significant improvements on some current research in this area. Unfortunately, most recent research in the area of anomaly-based IDS has neglected training and testing time as discussed in the problem requirements, so there is no direct time comparison between this proposed method and other solutions, however this massive improvement over the standard hybrid baselines is extremely promising.

The K-meansK-NN-Centroid method was also able to classify the whole KDD Cup ’99 dataset in under 1 s, 1220 times faster than the 20% labelled data KNN baseline, showing the centroid training method proposed in this research can be used to train other classifiers, although in this test the KNN still has overall classification times that were 2.6 times slower than the KSVMeans. Not only this, but the accuracy of the proposed solution is 97.20%, i.e. 4.6% higher than the K-means+KNN-20% and 8.8% higher than the K-means+SVM-20% method, showing that performance is not compromised. The increase in duration over time through the classification process can be seen visually in Fig. 15, Fig. 16. In Fig. 15 the 2-labelled output trained methods are compared to the proposed KSVMeans solution over the 4 steps of classification. In Fig. 16 the two centroid methods are compared.


Table 6. Table summarizing the total classification times for each method.

Method	SVM1 training	SVM 1 testing	SVM2 training	SVM2Testing	Total time	Overall classification accuracy
K-meanKNN-20%	0.90953	892.83645	0.44499	203.99745	1098.18842 (18 min 20 s)	92.59%
K-meanSVM-20%	44.35614	123.78795	3.30258	49.53744	220.98411 (3 min 30 s)	88.43%
K-meanKNN-Centroid	0.03682	0.61941	0.01355	0.23369	0.90347 s	97.20%
Proposed solution	0.04579	0.23159	0.01517	0.05419	0.34674 s	97.20%
4.4.4. Experiment comparing the linear kernel to the RBF kernel
In this experiment, the kernel of the SVM in the KSVMeans was changed from the proposed linear to an RBF kernel. Running the KSVMeans algorithm on 20% training data using the full KDD Cup ’99 dataset the following results were produced for the KSVMeans and the RKSVMeans (KSVMeans using RBF kernel). Table 7 provides a side by side comparison; this test was run only on the first SVM as this is the SVM which handles the most data and will give a good indication of how the two kernels perform in comparison with each other.

These results show that the RBF kernel is definitely competitive and should not be completely written off in use in this method, however due to the importance of time complexity in this solution the Linear method originally proposed is considered to be the correct choice, despite the RBF kernel performing better in training times, this gap is only 0.0001 of a second, whereas the testing results show that the proposed linear method performs over 23 times faster, with times of 0.010807 compared to the RBF kernels 0.253106 s. The accuracy of the proposed linear kernel is higher than the RBF kernel, whilst the detection rate and error rate are marginally better in the RBF, however overall the performance is relatively consistent. From this we can conclude that the performance impact of the RBF kernel is not significant enough to use this kernel based on the linear kernel being over 23 times faster in testing.


Table 7. A side by side comparison of the proposed linear KSVMeans and theRKSVMeans using an RBF kernel.

Testing netric	RKSVMeans	KSVMeans (proposed)
Train time (s)	0.199451	0.199560
Test time (s)	0.253106	0.010807
Accuracy (%)	95.39%	97.26%
Detection rate (%)	99.41%	98.22%
Error rate (%)	0.01%	0.02%
True Positive (TP)	550 690	577 491
False Positive (FP)	3228	10 411
False Negative (FN)	45 108	18 307
True Negative (TN)	449 550	442 367
4.4.5. Fine tuning of SVM
We tune the hypermeter of SVM to get the best detection results. We use Grid Search approach to select the best parameters of SVM. Below Table 8 shows the parameters of SVM than can be trained for better results.

We use the above setting for both SVM1 and SVM2 and achieve the highest accuracy of 99.9% using SVM2.


Table 8. Hyper parameters of SVM.

Parameter	Fine tuned
alpha	1e−08
epsilon	0.1
L1_ratio	0.15
Learning rate	Optimal
Loss	Hinge
Max iteration	1000
Penalty	L1
Tolerance	0.001
Validation fraction	0.1
Cross validation sets	5
power_t	0.5

Table 9. Showing the results of experimenting with complete dataset.

Classifier	Accuracy	Precision	Recall	F-score	Response time
SVM1	99.1	0.991	0.991	0.992	0.032372
SVM2	99.9	0.999	0.998	0.999	0.009457
4.4.6. Results of KSVMeans
In this section, we present the results of two approaches: (1) labelled instance of each cluster given as input to SVM (2) Bi-cluster instances given as input to SVM using 100% of the KDD Cup 99 dataset. The training dataset is adjusted accordingly to remain 80% of the overall dataset. The configuration of the KSVMeans remained as that described in the solution overview in Section 3.

We look at the results of 100% of the KDD Cup 99 dataset. The training dataset is adjusted accordingly to remain 80% of the overall dataset. The configuration of the KSVMeans remained as that described in the solution overview in Section 3. It is worth noting that the U2R and R2L data has been left in the datasets to provide realism, these will be clustered with the normal data in the following experiments and be considered as normal due to their similarities. By removing all U2R and R2L data we would have created an artificial dataset that does not represent the most realistic network dataset at our disposal.

Table 9 shows the results from the 2 different experiments when using 100% of the KDD Cup 99 dataset. The results show that we achieve the accuracy, precision, recall, and f-score of 99.1%, 0.991%, 0.991%, and 0.992% for SVM1 respectively. The response time for test prediction is noted as 0.032372. This can be seen in Fig. 17 where the confusion matrix is displayed showing SVM1 approach results. It can be seen that there are a few instances that are confused with others. The rationale behind this improvement is that the U2R class is already clustered with Normal class that shares a lot of similarities. DoS, Probe, and R2L instances are separated using our KSVMeans configuration. We achieve the accuracy, precision, recall and f-score of 99.9%, 0.999%, 0.998%, and 0.998% for SVM2 respectively. The response time for test prediction is noted as 0.009457 which is much lower than SVM1. This can be seen in Fig. 18, Fig. 19 where the confusion matrix is displayed showing SVM2 approach results. It can be seen that there are a few instances that are confused with others in both figures. The rationale behind this improvement is that the U2R and R2L classes are first clustered with Normal class and then it fed to SVM. Similar is the case of DoS and Probe clusters that are separated using our KSVMeans configuration. These results would be expected as the K-means algorithm uses a proportional subset of the full dataset to produce the centroids that train the SVMs, therefore if the overall dataset size is less than the training data subset that is 60% of the full dataset will be less, this means the SVMs will be trained using less information. The conclusion from this experiment is that the large and accurately clustered dataset used the more accurate the KSVMeans solution will be, this should not be a problem when looking at network IDS where the dataset is always going to be large due to the heavy network traffic.

4.4.7. Comparison to researched solutions
Finally, we will compare the algorithm against other proposed research to see how the algorithm performs. Table 10 shows some methods and their baselines from other research compared to the KSVMeans and the baselines used in this paper. The proposed KSVMeans was tested on the full KDD Cup ’99 dataset. This will give a good indication of where our results stand in the current research on network anomaly-based IDS as all the proposed methods from other research were tested on the same dataset. The results for the baselines and KSVMeans are an average of the results over the 2 SVMs. We achieve an accuracy of 99.9% using our SVM2 approach which is promising in comparison with other state-of-the-art approaches.


Table 10. Comparison between proposed solutions.

Accuracy	Detection rate	Error rate
NueroTree [43]	–	79.73%	20.27%
Decision Stump	–	92.1%	7.9%
C4.5 Decision Tree	–	92.27%	7.73%
Naïve Bayes	–	89.21%	10.79%
Random Forrest	–	88.98%	11.02%
Random Tree	–	89.11%	10.89%
REP Tree	–	98.38%	1.62%
CANN [30]	99.46%	99.28%	2.95%
SVM	95.37%	98.97%	4%
K-NN	99.89%	99.92%	0.32%
TANN [48]	93.87%	93.39%	28.69%
K-NN	95.14%	98.97%	4.02%
SVM	95.01%	99.01%	3.88%
K-meanK-NN	99.01%	99.27%	2.99%
K-meanSVM [38]	87.01%	88.71%	–
K-mean	76.62%	76.32%	–
SVM	64.34%	100%	–
Naïve Bayes [5]	94.37%	89.70%	–
K-meanNaïve Bayes [32]	99.60%	99.80%	0.50%
K-meanK-NN	93.55%	98.68%	4.79%
Auto encoder [18]	87.2%	–	–
KSVMeans (proposed)	99.93%	99.9%	0.1%
K-meanKNN_20%	88.43%	86.96%	22.94%
K-meanSVM_20%	92.65%	98.18%	24.44%
5. Conclusion
Cyberattacks are becoming a detrimental problem in an Internet environment, so as in the IoT and IoE. This research reviewed the current state of the anomaly IDS research landscape and drawn conclusions based on findings. This research proposed the KSVMeans, a solution to the problem of real time network detection. We showed that the KSVMeans can be used at any networked site of the IoE architecture, such as Advanced Metering Infrastructure (AMI). This method was tested using the KDD Cup ’99 dataset and produced extremely positive results in training and testing times, performing 1367 times faster than a K-means  K-NN hybrid baselines on the same task. Improving training and testing times usually comes with a trade-off of accuracy and detection rate, despite this usually being the case, the KSVMeans produced accuracy results of 99.9%, and error rate results of 0.1%. These were compared with the best recent research in this field that has been tested on the same KDD Cup ’99 dataset and was shown to improve upon accuracy and detection rate, meaning the algorithm is competitive as well as time efficient, a rare combination in the field of anomaly IDS.

